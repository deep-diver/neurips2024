[{"type": "text", "text": "Sequential Probability Assignment with Contexts: Minimax Regret, Contextual Shtarkov Sums, and Contextual Normalized Maximum Likelihood ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Idan Attias ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ziyi Liu University of Toronto & Vector Institute kevind.liu@mail.utoronto.ca ", "page_idx": 0}, {"type": "text", "text": "Ben-Gurion University & Vector Institute idanatti@post.bgu.ac.il ", "page_idx": 0}, {"type": "text", "text": "Daniel M. Roy University of Toronto & Vector Institute daniel.roy@utoronto.ca ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the fundamental problem of sequential probability assignment, also known as online learning with logarithmic loss, with respect to an arbitrary, possibly nonparametric hypothesis class. Our goal is to obtain a complexity measure for the hypothesis class that characterizes the minimax regret and to determine a general, minimax optimal algorithm. Notably, the sequential $\\ell_{\\infty}$ entropy, extensively studied in the literature (Rakhlin and Sridharan, 2015, Bilodeau et al., 2020, Wu et al., 2023), was shown to not characterize minimax regret in general. Inspired by the seminal work of Shtarkov (1987) and Rakhlin, Sridharan, and Tewari (2010), we introduce a novel complexity measure, the contextual Shtarkov sum, corresponding to the Shtarkov sum after projection onto a multiary context tree, and show that the worst case log contextual Shtarkov sum equals the minimax regret. Using the contextual Shtarkov sum, we derive the minimax optimal strategy, dubbed contextual Normalized Maximum Likelihood (cNML). Our results hold for sequential experts, beyond binary labels, which are settings rarely considered in prior work. To illustrate the utility of this characterization, we provide a short proof of a new regret upper bound in terms of sequential $\\ell_{\\infty}$ entropy, unifying and sharpening state-of-the-art bounds by Bilodeau et al. (2020) and $\\mathrm{W}\\mathbf{u}$ et al. (2023). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sequential probability assignment is a fundamental problem with connections to information theory [Ris84; MF98; XB00], machine learning [CL06; Vov95; RST15; FKLMS18; Sha20], and portfolio optimization [Kel56; Cov74; Cov91; CO96; Fed91]. In the original non-contextual setup, the learner aims to assign probabilities to a series of labels, which are revealed sequentially. The goal is to offer probabilistic forecasts over the label set such that the probability assigned to any observed sequence is comparable to that assigned by the best model in any fixed class of models. ", "page_idx": 0}, {"type": "text", "text": "The celebrated work of Shtarkov [Sht87] characterized minimax regret for context-free sequential probability assignment in terms of what is now known as the Shtarkov sum, and subsequently described the minimax algorithm, Normalized Maximum Likelihood (NML). NML represents the ideal probabilistic forecast in the sense of minimax regret, providing a benchmark for universal coding and prediction strategies. While often not used directly due to its computational complexity, NML has guided the design of practical algorithms and informed the development of efficient approximation methods. The principles underlying NML have inspired advances in both information theory and online learning, establishing fundamental limits and serving as a critical benchmark. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we study the problem of sequential probability assignment with contexts, which has been analyzed in recent works (e.g. [RS15; BFR20; WHGS23]) under the framework of online supervised learning formalized by Rakhlin, Sridharan, and Tewari [RST10]. In this setup, the problem is modeled as a $T$ -round game between a learner and the nature: On each round $t=1,\\dots,T$ , the learner observes a context $x_{t}$ from nature and predicts a distribution $\\hat{p}_{t}$ over some finite label space $\\boldsymbol{\\wp}$ . Then nature reveals a label $y_{t}\\in\\mathcal{V}$ and the learner incurs a logarithmic loss $\\ell(\\hat{p}_{t},y_{t})\\,=\\,\\bar{-}\\log(\\hat{p}_{t}(y_{t}))$ , where $\\hat{p}_{t}(y_{t})$ is the probability assigned to label $y_{t}$ by $\\hat{p}_{t}$ . The performance of the learner is measured by the regret with respect to a class $\\mathcal{F}$ of experts, defined as the difference between the total loss of the learner and that of the best expert in $\\mathcal{F}$ . The value of primary interest is the minimax regret, that is, the worst-case regret by the best learner over arbitrarily adaptive data sequences. The minimax regret serves as a benchmark for all algorithms and as a target for studies of adaptivity. Our goal is to address several fundamental questions: ", "page_idx": 1}, {"type": "text", "text": "Can we find a natural complexity measure of $\\mathcal{F}$ that characterizes the minimax regret, enabling us to analyze the minimax regret in new ways? And can we identify, in view of this complexity measure, a general, minimax optimal algorithm? ", "page_idx": 1}, {"type": "text", "text": "Notably, the sequential covering number of $\\mathcal{F}$ , a well studied measure of complexity, has been shown not to characterize the minimax regret on its own [RS15; BFR20; WHGS23]. This fact distinguishes sequential probability assignment and log loss: while sequential covering numbers enable a tight analysis in online learning problems with convex Lipschitz losses, like absolute loss [RST15] and square loss [RS14a], they do not yield minimax rates for log loss on some classes. Tackling such classes evidently requires new techniques. ", "page_idx": 1}, {"type": "text", "text": "Main contributions. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "1. We introduce a new complexity measure, which we call the contextual Shtarkov sum, that serves as a natural generalization of the Shtarkov sum from the context-free setting. We show that the minimax regret is characterized by the worst-case contextual Shtarkov sum.   \n2. We derive the minimax optimal algorithm, dubbed contextual Normalized Maximum Likelihood (cNML), using a data-dependent variant of the contextual Shtarkov sum, thereby generalizing NML from the context-free setting.   \n3. We apply contextual Shtarkov sums to the study of sequential entropy bounds on minimax. Doing so, we provide a short proof of a new regret upper bound in terms of sequential entropy that unifies and even improves on state-of-the-arts bounds by [BFR20] and [WHGS23]. Our results extend beyond the binary label setting studied by recent work to arbitrary finite label sets. ", "page_idx": 1}, {"type": "text", "text": "Related work. Sequential probability assignment has been studied extensively. Various aspects of this problem have been investigated, including sequences with and without side information (contexts), parametric and nonparametric hypothesis classes, and stochastic or adversarial datagenerating mechanisms. This problem has a long history in the machine learning community, see [CL06, Ch. 9] and the references therein. In the information theory community, this problem is also known as universal prediction [MF98], where the regret is also referred to as redundancy with respect to a set of codes. This has been studied in both stochastic and adversarial settings [Fre96; Ris86; Ris96; Sht87; XB97; DS04; MF98; OS04; Sha06; Szp98], where the focus was primarily on the parametric classes of experts. Closely related topics include universal source coding [Kol65; Sol64; Fit66; Dav73], data compression with arithmetic coding [Ris76; RL81; ZL77; ZL78; FMG92], and the minimum description length (MDL) principle [Ris78; Ris84; Ris87; BRY98; Gr\u00fc05; HY01]. Lastly, this topic is intimately tied with sequential gambling and portfolio optimization, as pointed out by [Kel56; Cov74; Cov91; CO96; Fed91]. ", "page_idx": 1}, {"type": "text", "text": "A classical result in context-free sequential probability assignment is that the minimax regret is equal to the log contextual Shtarkov sum [Sht87], and the minimax algorithm is the well-known Normalized Maximum Likelihood. When the set of contexts is known in advance to the forecaster, namely, a fixed design setting, the minimax regret is equivalent to the log Shtarkov sum of the function class when projected onto the known set of contexts [JSS21; WHGS23]. ", "page_idx": 1}, {"type": "text", "text": "A long line of work has focused on controlling minimax regret for rich classes in terms of covering numbers. [CL99; OH99] upper bounded the regret in terms of the (non-sequential) uniform covering number of the class. However, this complexity measure proved to be insufficient for obtaining optimal rates. [RS15] improved regret upper bounds by proposing a sequential covering measure. Thereafter, by utilizing the self-concordance property and the curvature of the log loss, [BFR20] further improved the upper bound in terms of the sequential covering number for nonparametric Lipschitz classes, through a non-constructive proof. [WHGS23] proposed a Bayesian algorithmic approach in order to upper bound the regret using a global notion of sequential covering. Notably, both the global and local sequential covering numbers do not fully characterize the regret, and the algorithm in [WHGS23] is not minimax optimal. By relaxing the worst-case analysis, [BHS23] studied this problem within the smoothed analysis framework, where nature is not fully adversarial but constrained. ", "page_idx": 2}, {"type": "text", "text": "Online learning with respect to arbitrary hypothesis classes and the zero-one loss, in the realizable case, is known to be characterized by the Littlestone dimension [Lit88]. The agnostic case was addressed by [BPS09; ABDMNY21]. Understanding sequential complexities in online learning with Lipschitz losses was extensively studied by [RST10; RS14a; RS14b; RST15]. Note that the logarithmic loss is neither Lipschitz nor bounded. Recently, [AHKKV23] characterized online regression in the realizable case, for any approximate pseudo-metric, such as the $\\ell_{p}$ loss. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. For a positive integer $K$ , let $[K]:=\\{1,2,\\dots,K\\}$ . For a finite set $\\kappa$ with $|{\\boldsymbol{\\kappa}}|=K$ , we use $\\Delta(K)$ to denote the set of all distributions on $\\kappa$ . We may identify $\\kappa$ with $[K]$ (under arbitrary enumeration of elements in $\\kappa$ ) and treat elements of $\\Delta(K)$ as vectors in $\\mathbb{R}^{K}$ . For a vector $p\\in\\mathbb{R}^{\\check{K}}$ and $i\\in[K]$ , let $p(i)$ be the $i$ -th coordinate of $p$ . Let $\\Delta^{+}(\\mathcal{K})=\\{p\\in\\Delta(\\mathcal{K}):p(k)>0,\\forall\\bar{k}\\in\\mathcal{K}\\}$ . For a general finite sequence $(a_{i})_{i=1}^{N}$ , we will use $a_{n;m}$ to denote the sub-sequence $(a_{n},\\ldots,a_{m})$ for any $n\\leq m$ and the empty sequence for $n>m$ . For any set $\\boldsymbol{\\mathcal{A}}$ , let $\\mathcal{A}^{*}=\\cup_{k\\geq0}\\mathcal{A}^{k}$ be the set of all finite length sequences over $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 2}, {"type": "text", "text": "Sequential probability assignment and minimax regret. Let $\\mathcal{X}$ be the context space and $\\boldsymbol{\\wp}$ be the finite label space. In each round $t\\in[T]$ during the game of sequential probability assignment, the learner receives a context $x_{t}\\in\\mathscr{X}$ from nature and assigns a probability distribution $\\hat{p}_{t}\\in\\Delta(\\mathcal{N})$ to the possible labels. Then nature reveals the true label $y_{t}\\in\\mathcal{V}$ and the learner incurs a loss $\\ell(\\hat{p}_{t},y_{t})=$ $-\\log(\\hat{p}_{t}(y_{t}))$ . Throughout, the learner is required to predict nearly as well as the best expert from an expert class, which is modeled as an arbitrary hypothesis class $\\bar{\\mathcal{F}}\\subseteq\\{(\\mathcal{X}\\times\\mathcal{Y})^{*}\\times\\mathcal{X}\\overset{}{\\rightarrow}\\Delta(\\mathcal{Y})\\}$ . More formally, the goal of the learner is make their regret with respect to $\\mathcal{F}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F};\\hat{p}_{1:T},x_{1:T},y_{1:T})=\\sum_{t=1}^{T}\\ell(\\hat{p}_{t},y_{t})-\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\ell(f(x_{1:t},y_{1:t-1}),y_{t}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "as small as possible for all sequences $\\mathbf{x}$ and $\\mathbf{y}$ generated by nature, possibly in an adversarial manner. Here $f(x_{1:t},y_{1:t-1})\\in\\Delta(\\mathcal{V}))$ can be understood as the prediction made by expert $f$ at round $t$ using past observations $(x_{1:t-1},y_{1:t-1})$ as well as the fresh context $x_{t}$ . The main focus is to study the minimax regret $\\mathcal{R}_{T}(\\mathcal{F})$ , which can be written as the following extensive form ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal R_{T}(\\mathcal F)=\\operatorname*{sup}_{x_{1}}\\operatorname*{inf}_{\\hat{p}_{1}}\\operatorname*{sup}_{y_{1}}\\cdot\\cdot\\cdot\\operatorname*{sup}_{x_{T}}\\operatorname*{inf}_{\\hat{p}_{T}}\\operatorname*{sup}_{y_{T}}\\mathcal R_{T}(\\mathcal F;\\hat{p}_{1:T},x_{1:T},y_{1:T}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $x_{t}\\in\\mathcal{X},\\hat{p}_{t}\\in\\Delta(\\mathcal{Y})$ and $y_{t}\\in\\mathcal{P},\\forall t\\in[T]$ . In light of this formulation, we can see that the minimax regret concerns both the learner and the nature to be adaptive, meaning that their actions can rely on the revealed history so far. ", "page_idx": 2}, {"type": "text", "text": "Remark 2.1 (Sequential vs non-sequential experts) Experts $f$ as mappings from $({\\mathcal{X}}\\times{\\mathcal{Y}})^{*}\\times{\\mathcal{X}}$ to $\\Delta(\\mathcal{Y})$ are sometimes called fully sequential experts [WHGS23] due to their ability to predict based on the past history. However, the literature (e.g. [RS15; BFR20; WHGS23]) often considers the more limited notion of non-sequential experts, modeled as $\\mathcal{F}\\subseteq\\{\\mathcal{X}\\to\\Delta(\\mathcal{Y})\\}$ , reflecting the fact that prediction made by each expert $f$ is simply $f(x_{t})$ in each round $t$ . In contrast, our results are more general as our novel techniques can be applied to the more flexible sequential experts. ", "page_idx": 2}, {"type": "text", "text": "Multiary trees. The complexity of online learning problems stems from the sequential and adaptive nature of the adversary, which we can capture with multiary trees. Formally, for a general space $\\boldsymbol{\\mathcal{A}}$ and a finite set $\\kappa$ , an $\\boldsymbol{\\mathcal{A}}$ -valued $\\kappa$ -ary tree $\\mathbf{v}$ of depth $d$ is a sequence of mappings $\\mathbf{\\dot{v}}_{t}\\,:\\,\\mathcal{K}^{t-1}\\,\\rightarrow\\,\\mathcal{A}$ for $t~\\in~[d]$ . A path in a depth- $d$ tree is a sequence $\\pmb{\\varepsilon}\\,=\\,\\stackrel{\\cdot}{\\left(\\varepsilon_{1},\\dots,\\varepsilon_{d}\\right)}\\,\\stackrel{\\cdot\\,\\cdot}{\\in}\\,\\dot{\\mathcal{K}^{d}}$ . We use the notation $\\mathbf{v}_{t}(\\varepsilon)$ to denote $\\mathbf{v}_{t}(\\varepsilon_{1},\\dots,\\varepsilon_{t-1})$ for $t\\in[d]$ and the boldface notation $\\mathbf{v}(\\varepsilon)$ to denote $(\\mathbf{v}_{1}(\\varepsilon),\\dots,\\mathbf{v}_{d}(\\varepsilon))\\in\\mathcal{A}^{d}$ . Throughout we will only consider $\\boldsymbol{\\wp}$ -ary trees valued in either $\\mathcal{X}$ or $\\Delta(y)$ , where the paths are denoted by the boldface y. We refer to $\\mathcal{X}$ -valued trees as context trees and $\\Delta(\\mathcal{Y})$ -valued trees as probabilistic trees. ", "page_idx": 3}, {"type": "text", "text": "Time-varying context sets. So far we consider the context set $\\mathcal{X}$ to be constant over time. But all of our results can be extended easily to allow for time-varying context spaces. Details of this generalization can be found in Appendix C. ", "page_idx": 3}, {"type": "text", "text": "2.1 Prior work: the Shtarkov sum in context-free and fixed designs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Before introducing our complexity measure that characterizes $\\mathcal{R}_{T}(\\mathcal{F})$ , we review some prior settings where the minimax regret can be characterized by the well-studied Shtarkov sum. First we introduce the notion of likelihood of a hypothesis $f$ with respect to a context and label sequence, which plays a key role in defining complexity measures and optimal algorithms. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (Likelihood) For $f:(\\mathcal{X}\\times\\mathcal{Y})^{*}\\times\\mathcal{X}\\to\\Delta(\\mathcal{Y})$ and length $-d$ sequences $x_{1:d}\\in$ $\\mathcal{X}^{d},y_{1:d}\\in\\mathcal{Y}^{d}$ , the likelihood $P_{f}(y_{1:d}|x_{1:d})$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nP_{f}(y_{1:d}|x_{1:d})=\\prod_{t=1}^{d}f\\big(x_{1:t},y_{1:t-1}\\big)(y_{t}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we use the compact notation $f(x_{1:t},y_{1:t-1})$ for $f(x_{1},y_{1},\\ldots,x_{t-1},y_{t-1},x_{t}).$ ", "page_idx": 3}, {"type": "text", "text": "In the classical context-free setting where $\\mathcal{X}$ can be thought of as a singleton, any sequential expert $f$ degenerates to a joint distribution over label sequences. Indeed, given any label sequence $y_{1:t-1}$ , $f\\bar{(y_{1:t-1})}\\in\\Delta(\\mathcal{Y})$ can be interpreted as the conditional distribution $f$ assigns to the next label $y_{t}$ . We use $\\begin{array}{r}{P_{f}(y_{1:d})=\\prod_{t=1}^{d}f(y_{1:t-1})(y_{t})}\\end{array}$ to denote this distribution. Similarly, the learner\u2019s strategy is also specified by  a joint distribution that is decomposed to a sequence of conditional distributions $\\hat{p}_{t}=\\hat{p}_{t}\\bar{\\left(}\\cdot|y_{1:t-1}\\right)\\stackrel{\\cdot}{\\in}\\bar{\\Delta}(\\mathcal{Y})$ . In this setup the minimax regret $\\mathcal{R}_{T}(\\mathcal{F})$ is characterized by the Shtarkov sum [Sht87]. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2.3 ([Sht87]) In the context-free setting, for any hypothesis class $\\mathcal{F}$ and horizon $T$ , the Shtarkov sum $S_{T}({\\mathcal{F}})$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nS_{T}({\\mathcal{F}})=\\sum_{y_{1:T}\\in{\\mathcal{y}}^{T}}\\operatorname*{sup}_{f\\in{\\mathcal{F}}}P_{f}(y_{1:T}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Moreover, the minimax regret is given by $\\mathcal{R}_{T}(\\mathcal{F})\\,=\\,\\log\\,S_{T}(\\mathcal{F}),$ , and the unique minimax optimal strategy is the normalized maximum likelihood (NML) distribution given by ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{n m l}(y_{1:T})=\\frac{\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(y_{1:T})}{\\sum_{y_{1:T}^{\\prime}\\in\\mathcal{Y}^{T}}\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(y_{1:T}^{\\prime})},\\qquad\\forall y_{1:T}\\in\\mathcal{Y}^{T}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To go beyond this classical context-agnostic setting and incorporate contextual information, prior work (e.g. [JSS21]) also considered an easier problem than the aforementioned sequential probability assignment, by forcing nature to reveal the context sequence $x_{1:T}$ to the learner at the start of the game. This is known as the fixed design setting or transductive online learning [WHGS23], where the goal is to characterize the so-called fixed design maximal minimax regret ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}^{\\mathrm{FD}}(\\mathcal{F}):=\\operatorname*{sup}_{x_{1:T}\\in\\mathcal{X}^{T}}\\operatorname*{inf}_{\\hat{p}_{1}}{\\operatorname*{sup}_{\\mathcal{Y}_{1}}}\\cdot\\cdot\\operatorname*{inf}_{\\hat{p}_{T}}\\operatorname*{sup}_{\\mathcal{R}T}\\mathcal{R}_{T}\\big(\\mathcal{F};\\hat{p}_{1:T},x_{1:T},y_{1:T}\\big).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It is straightforward to see that after projecting on $x_{1:T}$ , the hypothesis class $\\mathcal{F}$ again collapses to a set of joint distributions over $y^{T}$ specified by the likelihood function in Definition 2.2. Moreover, this set of distributions can be accessed by the learner from the start, so the fixed design setting can ", "page_idx": 3}, {"type": "text", "text": "be essentially reduced to the context-free setting. To be more specific, for any $f\\in\\mathcal F$ , it induces an expert in the context-free setting after being projected on $x_{1:T}$ , which is denoted by $f\\vert_{x_{1:T}}$ and ", "page_idx": 4}, {"type": "equation", "text": "$$\nf|_{x_{1:T}}(y_{1:t-1}):=f(x_{1:t},y_{1:t-1})\\in\\Delta(\\mathcal{Y}),\\forall t\\in[T],y_{1:t-1}\\in\\mathcal{Y}^{t-1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and let $\\mathcal{F}|_{x_{1:T}}:=\\{f|_{x_{1:T}}:f\\in\\mathcal{F}\\}$ . Then given any predetermined $x_{1:T}$ , the learner is equivalently competing with $\\mathcal{F}|_{x_{1:T}}$ in the context-free setting. With the following natural variant of the Shtarkov sum, we can easily characterize $\\mathcal{R}_{T}^{\\mathrm{FD}}(\\mathcal{F})$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 2.4 (Conditional Shtarkov sum) Given a context sequence $x_{1:T}\\,\\in\\,\\chi^{T}$ , the Shtarkov sum of $\\mathcal{F}$ conditioned on $x_{1:T}$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\nS_{T}({\\mathcal{F}}|x_{1:T}):=\\sum_{y_{1:T}\\in y^{T}}\\operatorname*{sup}_{f\\in{\\mathcal{F}}}P_{f}\\big(y_{1:T}|x_{1:T}\\big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In fact, $S_{T}({\\mathcal{F}}|x_{1:T})$ is just the Shtarkov sum of the projected class $\\mathcal{F}|_{x_{1:T}}$ in the context-free setting. The following result characterizes the fixed-design setting: ", "page_idx": 4}, {"type": "text", "text": "Proposition 2.5 (Minimax regret, fixed design [JSS21]) In the fixed design setting, for any hypothesis class $\\mathcal{F}$ and horizon $T$ , the fixed design maximal minimax regret is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}^{\\mathrm{FD}}(\\mathcal{F})=\\operatorname*{sup}_{x_{1:T}\\in\\mathcal{X}^{T}}\\log S_{T}(\\mathcal{F}|x_{1:T}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and, given any context sequence $x_{1:T}$ , the minimax optimal response is NML with respect to $\\mathcal{F}|_{x_{1:T}}$ ", "page_idx": 4}, {"type": "text", "text": "3 Minimax regret via contextual Shtarkov sum ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Now we state one of our main results about the characterization of the minimax regret of sequential probability assignment. First we introduce the key concept of contextual Shtarkov sum, which is a natural generalization of Shtarkov sum in the context-free setting. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.1 (Contextual Shtarkov sum) The contextual Shtarkov sum $S_{T}(\\mathcal{F}|\\mathbf{x})$ of a hypothesis class $\\mathcal{F}$ on a given context tree $\\mathbf{x}$ of depth $T$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\nS_{T}(\\mathcal{F}|\\mathbf{x}):=\\sum_{\\mathbf{y}\\in\\mathcal{Y}^{T}}\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Just like the conditional Shtarkov sum, the contextual Shtarkov sum $S_{T}(\\mathcal{F}|\\mathbf{x})$ can be interpreted as the Shtarkov sum of the projected class $\\mathcal{F}|_{\\mathbf{x}}:=\\{f|_{\\mathbf{x}}:f\\in\\mathcal{F}\\}$ where $f|_{\\mathbf{x}}$ is the induced context-free expert specified by ", "page_idx": 4}, {"type": "equation", "text": "$$\nf|_{\\mathbf{x}}(y_{1:t-1}):=f(\\mathbf{x}(y_{1:t-1}),y_{1:t-1})\\in\\Delta(\\mathcal{Y}),\\forall t\\in[T],y_{1:t-1}\\in\\mathcal{Y}^{t-1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we have slightly abused the notation to use $\\mathbf{x}(y_{1:t-1})$ to denote the length- $\\cdot t$ context sequence obtained by tracing tree $\\mathbf{x}$ through the (partial) path $y_{1:t-1}$ . Next we show that the minimax regret $\\mathcal{R}_{T}(\\mathcal{F})$ is characterized by the worst-case contextual Shtarkov sum: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2 (Main result: minimax regret) For any hypothesis class $\\mathcal{F}\\subseteq\\{(\\mathcal{X}\\times\\mathcal{Y})^{*}\\times\\mathcal{X}\\rightarrow$ $\\Delta(\\mathcal{V})\\}$ and horizon $T$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F})=\\operatorname*{sup}_{\\mathbf{x}}\\log S_{T}(\\mathcal{F}|\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the supremum is taken over all $\\mathcal{X}$ -valued context trees x of depth $T$ . ", "page_idx": 4}, {"type": "text", "text": "Since any context sequence $x_{1:T}$ can be thought as a special context tree $\\mathbf{x}$ that is constant in each level $t\\ \\in\\ [T]$ (i.e., $\\mathbf{x}_{t}(\\mathbf{y})\\;=\\;x_{t},\\forall\\mathbf{y})$ , we can find that the supremum over context trees in Theorem 3.2 strictly subsumes the supremum over context sequences in Proposition 2.5. Thus we can see the separation between $\\mathcal{R}_{T}(\\mathcal{F})$ and $\\mathcal{R}_{T}^{\\mathrm{FD}}(\\mathcal{F})$ is clearly exhibited. ", "page_idx": 4}, {"type": "text", "text": "The full proof of Theorem 3.2 as well as an overview are provided in Appendix A ", "page_idx": 4}, {"type": "text", "text": "3.1 Applications: an improved regret upper bound in terms of sequential entropy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To illustrate the utility of our characterization in Theorem 3.2, we walk through some examples where we are able to recover and sharpen existing regret upper bounds with relatively short proofs via contextual Shtarkov sum. As a start, we provide a short proof in Appendix A.6 of the classical regret bound for a finite hypothesis class. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.3 (Finite classes) For any $\\mathcal{F}\\subseteq[0,1]^{\\mathcal{X}}$ and horizon $T$ , $\\mathcal{R}_{T}(\\mathcal{F})\\leq\\log|\\mathcal{F}|$ . ", "page_idx": 5}, {"type": "text", "text": "Let us go back to the binary label setting with non-sequential experts, that is, ${\\mathcal{D}}\\;=\\;\\{0,1\\}$ and $\\mathcal{F}\\subseteq[0,1]^{\\chi}$ , and $f(x)\\in[0,\\bar{1}]$ is interpreted as the probability assigned to label 1 by this expert $f$ . We will show a regret bound that outperforms the state-of-the-art ones in [BFR20; WHGS23] with a surprisingly simple proof. To proceed, we need the following notation. Given a context tree $\\mathbf{x}$ of depth $T$ , let $\\mathcal{F}\\circ\\mathbf{x}=\\left\\{f\\circ\\mathbf{x}:f\\in\\mathcal{F}\\right\\}$ , where $f\\circ\\mathbf{x}$ is the $[0,1]$ -valued tree such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(f\\circ\\mathbf{x})_{t}(\\mathbf{y})=f(\\mathbf{x}_{t}(\\mathbf{y})),\\forall\\mathbf{y}\\in\\mathcal{y}^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Next we introduce the definitions of sequential $\\ell_{\\infty}$ covers and entropy. ", "page_idx": 5}, {"type": "text", "text": "Definition 3.4 (Sequential $\\ell_{\\infty}$ cover and entropy) Given a hypothesis class $\\mathcal{F}\\subseteq[0,1]^{\\mathcal{X}}$ and a context tree $\\mathbf{x}$ of depth $T$ , we say a collection of $\\mathbb{R}$ -valued trees $V_{\\mathbf{x},\\alpha}$ is a sequential cover of ${\\mathcal{F}}\\circ\\mathbf{x}$ at scale $\\alpha>0$ if for any $f\\in\\mathcal{F},\\mathbf{y}\\in\\mathcal{Y}^{T}$ , there exists some $v\\in V_{\\mathbf{x},\\alpha}$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n|f(\\mathbf{x}_{t}(\\mathbf{y}))-v_{t}(\\mathbf{y})|\\leq\\alpha,\\forall t\\in[T].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Let the sequential $\\ell_{\\infty}$ covering number $\\mathcal{N}_{\\infty}(\\mathcal{F}\\circ{\\bf x},\\alpha,T)$ be the size of the smallest such cover. The sequential $\\ell_{\\infty}$ entropy of $\\mathcal{F}$ at scale $\\alpha$ and depth $T$ is defined as the logarithm of the worst-case sequential covering number: $\\begin{array}{r}{\\mathcal{H}_{\\infty}(\\mathcal{F},\\alpha,T):=\\operatorname*{lup}_{\\mathbf{x}}\\log\\mathcal{N}_{\\infty}(\\mathcal{F}\\circ\\mathbf{x},\\alpha,T),}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 3.5 (Global sequential $\\ell_{\\infty}$ cover and entropy) Given a hypothesis class $\\mathcal{F}\\subseteq[0,1]^{\\mathcal{X}}$ , we say a collection of mappings $\\mathcal{G}_{\\alpha}\\subseteq[0,1]^{\\chi_{*}}$ is a global sequential cover of $\\mathcal{F}$ at scale $\\alpha>0$ and depth $T$ if for any $f\\in\\mathcal{F},x_{1:T}\\in\\mathcal{X}^{T}$ , there exists some $g\\in\\mathcal G_{\\alpha}$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n|f(x_{t})-g(x_{1:t})|\\leq\\alpha,\\forall t\\in[T].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Let the global sequential $\\ell_{\\infty}$ covering number $\\textstyle{N_{G}(\\mathcal{F},\\alpha,T)}$ be the size of the smallest such cover. The global sequential $\\ell_{\\infty}$ entropy of $\\mathcal{F}$ at scale $\\alpha$ and depth $T$ is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{H}_{G}(\\mathcal{F},\\alpha,T):=\\log\\mathcal{N}_{G}(\\mathcal{F},\\alpha,T).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proposition 3.6 ([BFR20; WHGS23]) For any $\\mathcal{F}\\subseteq[0,1]^{\\mathcal{X}}$ and horizon $T$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F})\\leq\\operatorname*{min}\\bigg\\{\\underbrace{\\operatorname*{inf}_{\\alpha>0}\\bigl\\{4T\\alpha+c\\mathcal{H}_{\\infty}(\\mathcal{F},\\alpha,T)\\bigr\\}}_{[8\\mathrm{FR}20]},\\underbrace{\\operatorname*{inf}_{\\alpha>0}\\bigl\\{T\\log(1+2\\alpha)+\\mathcal{H}_{G}(\\mathcal{F},\\alpha,T)\\bigr\\}}_{[\\mathrm{WHGS}23]}\\bigg\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "It is easy to show that $\\begin{array}{r}{\\mathcal{H}_{\\infty}(\\mathcal{F},\\alpha,T)\\,\\leq\\,\\mathcal{H}_{G}(\\mathcal{F},\\alpha,T)}\\end{array}$ , but, in general, the two bounds in Proposition 3.6 are incomparable due to constants and different dependence on $\\alpha$ (more discussions on these bounds are deferred to Appendix C). Starting from the contextual Shtarkov sum, we are able to derive a bound that combines the best of these two bounds: ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.7 (Main result: sequential entropy bound) For any $\\mathcal{F}\\subseteq[0,1]^{\\mathcal{X}}$ and horizon $T$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F})\\leq\\operatorname*{inf}_{\\alpha>0}\\Bigl\\{T\\log(1+2\\alpha)+\\mathcal{H}_{\\infty}(\\mathcal{F},\\alpha,T)\\Bigr\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof For any scale $\\alpha\\,>\\,0$ and depth- $\\boldsymbol{\\mathcal{T}}$ context tree $\\mathbf{x}$ , let $V_{\\mathbf{x},\\alpha}$ be a sequential cover of $\\mathcal{F}\\circ\\mathbf{x}$ at scale $\\alpha$ with size $\\mathcal{N}_{\\infty}(\\mathcal{F}\\circ{\\bf x},\\alpha,T)$ . We can always assume $V_{\\mathbf{x},\\alpha}$ to be $[0,1]$ -valued without loss of generality because otherwise we can just truncate it without violating its coverage guarantee. Define the smoothed covering set $\\begin{array}{r}{\\tilde{V}_{\\mathbf{x},\\alpha}=\\left\\{\\tilde{v}:\\forall t\\in[T],\\tilde{v}_{t}(\\cdot)=\\frac{v_{t}(\\cdot)+\\alpha}{1+2\\alpha},v\\in V_{\\mathbf{x},\\alpha}\\right\\}}\\end{array}$ vt1(+\u00b7)2+\u03b1\u03b1 , v \u2208Vx,\u03b1 , inspired by [BFR23; WHGS23]. Then for any $f\\in\\mathcal{F},\\mathbf{y}\\in\\mathcal{Y}^{T}$ , there exists some $v\\in V_{\\mathbf{x},\\alpha}$ such that $|f(\\mathbf{x}_{t}(\\mathbf{y}))\\!-\\!v_{t}(\\mathbf{y})|\\leq$ $\\alpha,\\forall t\\in[T]$ and hence $\\tilde{v}$ satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{f(\\mathbf{x}_{t}(\\mathbf{y}))}{\\tilde{v}_{t}(\\mathbf{y})}\\leq1+2\\alpha,\\quad\\frac{1-f(\\mathbf{x}_{t}(\\mathbf{y}))}{1-\\tilde{v}_{t}(\\mathbf{y})}\\leq1+2\\alpha.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Hence ", "page_idx": 6}, {"type": "equation", "text": "$$\nP_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))=\\prod_{t=1}^{T}f(\\mathbf{x}_{t}(\\mathbf{y}))^{y_{t}}(1-f(\\mathbf{x}_{t}(\\mathbf{y})))^{1-y_{t}}\\leq(1+2\\alpha)^{T}\\prod_{t=1}^{T}\\tilde{v}_{t}(\\mathbf{y})^{y_{t}}(1-\\tilde{v}_{t}(\\mathbf{y}))^{1-y_{t}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\mathbf{y}}\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))\\leq(1+2\\alpha)^{T}\\displaystyle\\sum_{\\mathbf{y}}\\operatorname*{sup}_{\\tilde{v}\\in\\tilde{V}_{\\mathbf{x},\\alpha}}\\prod_{t=1}^{T}\\tilde{v}_{t}(\\mathbf{y})^{y_{t}}(1-\\tilde{v}_{t}(\\mathbf{y}))^{1-y_{t}}}\\\\ &{\\displaystyle\\leq(1+2\\alpha)^{T}\\displaystyle\\sum_{\\tilde{v}\\in\\tilde{V}_{\\mathbf{x},\\alpha}}\\sum_{\\mathbf{y}}\\prod_{t=1}^{T}\\tilde{v}_{t}(\\mathbf{y})^{y_{t}}(1-\\tilde{v}_{t}(\\mathbf{y}))^{1-y_{t}}=(1+2\\alpha)^{T}|\\tilde{V}_{\\mathbf{x},\\alpha}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the last equality follows from Lemma D.1, treating $\\tilde{v}$ as sequential experts. Finally, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{T}(\\mathcal{F})=\\underset{\\mathbf{x}}{\\operatorname*{sup}}\\log\\left(\\displaystyle\\sum_{\\mathbf{y}}\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\,P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))\\right)}\\\\ &{\\qquad\\quad\\leq\\underset{\\mathbf{x}}{\\operatorname*{sup}}\\log\\left((1+2\\alpha)^{T}|\\tilde{V}_{\\mathbf{x},\\alpha}|\\right)=T\\log(1+2\\alpha)+\\mathcal{H}_{\\infty}(\\mathcal{F},\\alpha,T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Since our choice of $\\alpha$ is arbitrary, the result follows. ", "page_idx": 6}, {"type": "text", "text": "3.2 The inadequacy of sequential $\\ell_{\\infty}$ covering number ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conclude this section with a discussion on the suboptimality of regret bounds based on sequential covering numbers as in Proposition 3.6 and Theorem 3.7. Let us consider the binary label setting and the following hypothesis classes over the unit Hilbert ball $\\mathcal{X}=\\mathbb{B}_{2}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{F}^{\\mathrm{Lin}}:=\\left\\{x\\mapsto\\frac{\\langle w,x\\rangle+1}2:w\\in\\mathbb{B}_{2}\\right\\},\\quad\\mathcal{F}^{\\mathrm{AbsLin}}:=\\left\\{x\\mapsto|\\langle w,x\\rangle|:w\\in\\mathbb{B}_{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We can see that the sequential $\\ell_{\\infty}$ covering numbers of ${\\mathcal{F}}^{\\mathrm{Lin}}$ and $\\mathcal{F}^{\\mathrm{AbsLin}}$ are of the same order for all scales, thus the aforementione\u221ad results will yield the same regret bound for these two classes. However, we have $\\mathcal{R}_{T}(\\mathcal{F}^{\\mathrm{Lin}})=\\tilde{O}(\\sqrt{T})$ while $\\mathcal{R}_{T}(\\mathcal{F}^{\\mathrm{AbsLin}})=\\tilde{\\Theta}(T^{2/3})$ [RS15; WHGS23], which implies that the sequential $\\ell_{\\infty}$ covering number, in its current form within the regret bound, cannot characterize the minimax regret. ", "page_idx": 6}, {"type": "text", "text": "It is worth mentioning that an $\\tilde{\\Omega}(\\sqrt{T})$ lower bound on $\\mathcal{R}_{T}(\\mathcal{F}^{\\mathrm{Lin}})$ is achievable, via an $\\tilde{\\Omega}(1/\\alpha^{2})$ lower bound on the sequential fat-shattering dimension $\\mathrm{sfat}_{\\alpha}(\\mathcal{F}^{\\mathrm{Lin}})$ combined with Proposition 2 in [WHGS23]. The same lower bound al\u221aso holds in the finite-dimensional case, where $\\mathbb{B}_{2}$ is a unit $d$ -dimensional Euclidean ball with $d\\geq\\sqrt{T}$ [WHGS23, Footnote 6]. Our proof (Appendix A.7) of the next result works in both the infinite and finite dimensional (with $d\\geq T$ ) cases. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.8 $(\\Omega({\\sqrt{T}})$ lower bound for the linear class ${\\mathcal{F}}^{\\mathrm{Lin}}$ ) For ${\\mathcal{F}}^{\\mathrm{Lin}}$ defined as in Eq. (1) with $\\mathbb{B}_{2}$ being the unit Hilbert ball or the unit $d$ -dimensional Euclidean ball with $d\\geq T$ , then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F}^{\\mathrm{Lin}})\\geq\\mathcal{R}_{T}^{\\mathrm{FD}}(\\mathcal{F}^{\\mathrm{Lin}})\\geq\\sqrt{T}/4.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proof of Lemma 3.8 is based on lower bounding the conditional Shtark\u221aov sums (and hence the contextual Shtarkov sums) of ${\\mathcal{F}}^{\\mathrm{Lin}}$ . From Theorem 3.2 we know that the $\\tilde{O}(\\sqrt{T})$ upper bound holds for the log of contextual Shtarkov sums as well but we do not have a direct proof of this fact so far. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 Contextual Normalized Maximum Likelihood (cNML) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Input: Hypothesis class $\\mathcal{F}$ , horizon $T$   \nFor $t=1,2,...,T$ do 1. Observe context $x_{t}\\in\\mathscr{X}$ 2. If $\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(y_{1:t-1}|x_{1:t-1})>0$ , predict $\\hat{p}_{t}\\in\\Delta(\\mathcal{N})$ with $\\hat{p}_{t}(y)=\\frac{\\operatorname*{sup}_{\\mathbf{x}}S_{T}^{x_{1:t},(y_{1:t-1},y)}(\\mathcal{F}|\\mathbf{x})}{\\sum_{y^{\\prime}\\in\\mathcal{Y}}\\operatorname*{sup}_{\\mathbf{x}}S_{T}^{x_{1:t},(y_{1:t-1},y^{\\prime})}(\\mathcal{F}|\\mathbf{x})},\\forall y\\in\\mathcal{Y},$ and otherwise set $\\hat{p}_{t}$ to be an arbitrary member of $\\Delta^{+}(\\mathcal{Y})$ 3. Receive label $y_{t}\\in\\mathcal{V}$ ", "page_idx": 7}, {"type": "text", "text": "4 Contextual NML, the minimax optimal algorithm ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "So far we have settled the minimax regret of sequential probability assignment in a nonconstructive way. Now we switch to the algorithmic lens to study the optimal strategy that achieves the minimax regret. Remarkably, we show that the minimax optimal algorithm can be described by a data-dependent variant of the contextual Shtarkov sum, which is named contextual Shtarkov sum with prefix. ", "page_idx": 7}, {"type": "text", "text": "Definition 4.1 (Contextual Shtarkov sum with prefix) Given sequences $x_{1:t}\\in\\mathcal{X}^{t},y_{1:t}\\in\\mathcal{Y}^{t},t\\in$ $[T]$ and a context tree $\\mathbf{x}$ of depth $T-t$ , the contextual Shtarkov sum $S_{T}^{x_{1:t},y_{1:t}}(\\mathcal{F}|\\mathbf{x})$ of $\\mathcal{F}$ on $\\mathbf{x}$ with prefix $x_{1:t},y_{1:t}$ is defined as ", "page_idx": 7}, {"type": "equation", "text": "$$\nS_{T}^{x_{1:t},y_{1:t}}(\\mathcal{F}|\\mathbf{x})=\\sum_{\\mathbf{y}\\in\\mathcal{Y}^{T-t}}\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(y_{1:t},\\mathbf{y}|x_{1:t},\\mathbf{x}(\\mathbf{y})).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Now we present our prediction strategy, contextual normalized maximum likelihood (cNML), which is summarized in Algorithm 1. In each round $t$ , with $x_{1:t},y_{1:t-1}$ as past observations, the learner first checks whether $\\operatorname*{sup}_{f\\in{\\mathcal{F}}}P_{f}\\big(y_{1:t-1}|x_{1:t-1}\\big)\\,>\\,0$ since if that is not the case and $\\begin{array}{r}{\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(y_{1:t-1}\\vert x_{1:t-1})=0}\\end{array}$ , the cumulative losses of all experts in $\\mathcal{F}$ have already blown up to $+\\infty$ and the learner only needs to predict any $\\hat{p}\\,\\in\\,\\Delta^{+}(\\mathcal{N})$ in all remaining rounds. On the other hand, if $\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(y_{1:t-1}\\vert x_{1:t-1})>0$ , then ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{y\\in\\mathcal{Y}}\\operatorname*{sup}_{\\mathbf{x}}S_{T}^{x_{1:t},(y_{1:t-1},y)}(\\mathcal{F}|\\mathbf{x})>0\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and the $\\hat{p}_{t}$ given by Eq. (2) is indeed a valid member of $\\Delta(\\mathcal{Y})$ (shown in Appendix B) and is used as the learner\u2019s prediction. The following theorem shows that cNML is the minimax optimal algorithm, with proof deferred to Appendix B. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.2 (Main result: optimal algorithm) The contextual normalized maximum likelihood strategy (Algorithm 1) is minimax optimal. ", "page_idx": 7}, {"type": "text", "text": "To see that cNML is reduced to NML in the context-free setting, it suffices to consider the case where $\\operatorname*{sup}_{f\\in{\\mathcal{F}}}P_{f}\\big(y_{1:T}\\big)\\;>\\;0$ since otherwise NML will simply assign 0 probability on this sequence $y_{1:T}$ and during the actual round-wise implementation of NML, it also predicts an arbitrary element from $\\Delta^{+}(\\mathcal{Y})$ in those rounds $t$ where $\\operatorname*{sup}_{f}P_{f}(y_{1:t-1})\\,=\\,0$ . Now for any $y_{1:T}$ such that $\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}\\big(y_{1:T}\\big)>0$ , the prediction by cNML in each round $t$ is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{p}_{t}(y)=\\frac{\\sum_{\\mathbf{y}\\in\\mathcal{Y}^{T-t}}\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(y_{1:t-1},y,\\mathbf{y})}{\\sum_{\\mathbf{y^{\\prime}}\\in\\mathcal{Y}^{T-t+1}}\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(y_{1:t-1},\\mathbf{y}^{\\prime})},\\forall y\\in\\mathcal{Y}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "which can be summarized into a joint density over $y_{1:T}$ by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{p}(y_{1:T})=\\frac{\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(y_{1:T})}{\\sum_{y_{1:T}^{\\prime}\\in\\mathcal{Y}^{T}}\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(y_{1:T}^{\\prime})}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Recall that this is exactly the NML prediction $p_{n m l}(y_{1:T})$ . ", "page_idx": 7}, {"type": "text", "text": "Remark 4.3 (Relaxations and efficient algorithms) One may wonder if more efficient algorithms are available when it is not easy to compute contextual Shtarkov sums with prefix. One solution is to apply the framework of admissible relaxation in [RSS12], which provides a systematic way of constructing efficient algorithms at the cost of worse regret guarantees. Notice that the worst-case log contextual Shtarkov sums with prefix constitute a trivially \u201cadmissible relaxation\u201d since they are the exact conditional game values. ", "page_idx": 8}, {"type": "text", "text": "5 Perspectives on contextual Shtarkov sums ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we provide further insight into contextual Shtarkov sums, defined in Sections 3 and 4. ", "page_idx": 8}, {"type": "text", "text": "5.1 Contextual Shtarkov sums through martingales ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We can relate our characterization of the minimax regret to the more extensively studied sequential Rademacher complexity, which arises in online learning problems with hypothesis class $\\mathcal{F}\\subseteq[0,1]^{\\mathcal{X}}$ and bounded convex losses like absolute loss. Specifically, the (conditional) sequential Rademacher complexity [RST15] is defined by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\Re_{T}(\\mathcal{F};\\mathbf{x}):=\\mathbb{E}_{\\varepsilon}\\Big[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\varepsilon_{t}f\\big(\\mathbf{x}_{t}(\\varepsilon)\\big)\\Big],\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathbf{x}$ is a depth- $\\mathbf{\\nabla}\\cdot\\mathbf{\\mathcal{T}}$ binary context tree and $\\pmb{\\varepsilon}\\,=\\,(\\varepsilon_{1},\\dots,\\varepsilon_{T})\\,\\in\\,\\{\\pm1\\}^{T}$ is a sequence of i.i.d. Rademacher random variables. A notable feature of $\\Re_{T}(\\mathcal{F};\\mathbf{x})$ is that it is the expected supremum of the sum of a martingale differences, i.e., for any $f,\\mathbb{E}[\\varepsilon_{t}f(\\mathbf{\\dot{x}}_{t}(\\varepsilon))|\\varepsilon_{1},\\dots,\\varepsilon_{t-1}]=0$ . Likewise, $S_{T}(\\mathcal{F}|\\mathbf{x})$ also admits a martingale interpretation. To see this, let $\\mathcal{F}\\subseteq\\{(\\mathcal{X}\\times\\mathcal{Y})^{*}\\times\\mathcal{X}\\to\\Delta(\\mathcal{Y})\\}$ and rewrite $S_{T}(\\mathcal{F}|\\mathbf{x})$ for any context tree $\\mathbf{x}$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\nS_{T}(\\mathcal{F}|\\mathbf{x})=\\sum_{\\mathbf{y}\\in\\mathcal{Y}^{T}}\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))=\\mathbb{E}_{\\mathbf{y}}\\Big[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\prod_{t=1}^{T}\\Bigl(|\\mathcal{Y}|\\cdot f(\\mathbf{x}_{1:t}(\\mathbf{y}),y_{1:t-1})(y_{t})\\Bigr)\\Bigr],\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathbf{y}=(y_{1},\\dots,y_{T})$ is a sequence of i.i.d. variables following the uniform distribution over $\\boldsymbol{\\wp}$ . It is easy to check that $\\mathbb{E}[|\\mathcal{V}|\\cdot f(\\mathbf{x}_{1:t}(\\mathbf{y}),y_{1:t-1})(y_{t})|y_{1},\\dots,y_{t-1}]=1$ , and thus ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\Big\\{\\prod_{s=1}^{t}\\Big(|\\mathcal{V}|\\cdot f(\\mathbf{x}_{1:s}(\\mathbf{y}),y_{1:s-1})(y_{s})\\Big)\\Big\\}_{t\\in[T]}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "is a martingale with respect to filtration $\\mathcal{F}_{t}\\,=\\,\\sigma(y_{1},\\dots,y_{t}),t\\,\\in\\,[T]$ . It would be of independent interest to study the contextual Shtarkov sums more quantitatively by developing new tools for such product-type martingales. ", "page_idx": 8}, {"type": "text", "text": "5.2 General Shtarkov sums ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We can also interpret contextual Shtarkov sums as an instance of general Shtarkov sums, which are defined over sub-probability measures. ", "page_idx": 8}, {"type": "text", "text": "Definition 5.1 (Sub-probability measure) A set $\\mathcal{P}=\\{p:\\mathcal{K}\\rightarrow[0,1]\\}$ is a class of sub-probability measures over a finite set $\\kappa$ if ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sum_{k\\in K}p(k)\\leq1,\\forall p\\in\\mathcal{P}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Due to Lemma D.1, it is easy to see that for any hypothesis class $\\mathcal{F}\\subseteq\\{(\\mathcal{X}\\times\\mathcal{Y})^{*}\\times\\mathcal{X}\\to\\Delta(\\mathcal{Y})\\}$ and depth- $\\mathbf{\\nabla}\\cdot\\mathbf{\\mathcal{T}}$ context tree $\\mathbf{x}$ , they induce a class ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{P}_{\\mathcal{F}|\\mathbf{x}}:=\\{P_{f}(\\cdot|\\mathbf{x}(\\cdot)):f\\in\\mathcal{F}\\}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "that is a class of sub-probability measures over $y^{T}$ . Moreover, for any $\\mathcal{F}$ , depth- $(T-t)$ context tree $\\mathbf{x}$ and sequences $x_{1:t}\\in\\mathcal{X}^{t},y_{1:t}\\in\\mathcal{Y}^{t}$ , the induced ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{P}_{\\mathcal{F}^{x_{1:t},y_{1:t}}|\\mathbf{x}}:=\\{P_{f}(y_{1:t},\\cdot|x_{1:t},\\mathbf{x}(\\cdot)):f\\in\\mathcal{F}\\}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "is a class of sub-probability measure over $y^{T-t}$ since ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{y}\\in\\mathcal{Y}^{T-t}}P_{f}\\big(y_{1:t},\\mathbf{y}\\big|x_{1:t},\\mathbf{x}(\\mathbf{y})\\big)=P_{f}\\big(y_{1:t}\\big|x_{1:t}\\big)\\leq1.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Next we introduce the notion of general Shtarkov sum over classes of sub-probability measures. ", "page_idx": 9}, {"type": "text", "text": "Definition 5.2 (General Shtarkov sum) Given any class $\\mathcal{P}$ of sub-probability measures over $\\kappa$ , the general Shtarkov sum of $\\mathcal{P}$ is defined as ", "page_idx": 9}, {"type": "equation", "text": "$$\nS(\\mathcal P)=\\sum_{k\\in K}\\operatorname*{sup}_{p\\in\\mathcal P}p(k).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "With the notion of general Shtarkov sum, it is not hard to verify that the contextual Shtarkov sums with $\\&$ without prefix can be interpreted as instances of general Shtarkov sums: ", "page_idx": 9}, {"type": "text", "text": "Proposition 5.3 For any horizon $T,t\\in[T],$ , data sequence $x_{1:t}\\in\\mathcal{X}^{t},y_{1:t}\\in\\mathcal{Y}^{t}$ , and context trees $\\mathbf{x},\\mathbf{x}^{\\prime}$ of depth $T,T-t$ respectively, we have ", "page_idx": 9}, {"type": "equation", "text": "$$\nS_{T}(\\mathcal{F}|\\mathbf{x})=S(\\mathcal{P}_{\\mathcal{F}|\\mathbf{x}}),\\quad S_{T}^{x_{1:t},y_{1:t}}(\\mathcal{F}|\\mathbf{x}^{\\prime})=S(\\mathcal{P}_{\\mathcal{F}^{x_{1:t},y_{1:t}}|\\mathbf{x}^{\\prime}}).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "It would be interesting to find out other instances of general Shtarkov sums that capture the complexities of other online learning problems with log loss. ", "page_idx": 9}, {"type": "text", "text": "6 Discussions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we characterize the minimax regret and the optimal prediction strategy for sequential probability assignment, generalizing the classical results in the context-free setting. Moreover, our results are general enough to subsume the setting of multiary labels and sequential hypothesis classes, which has not been sufficiently explored before. Remarkably, our characterization holds for arbitrary hypothesis classes that may not admit the regularity assumptions implicitly required by prior works (e.g. [RST15; BFR20]). ", "page_idx": 9}, {"type": "text", "text": "For future works, it would be interesting to study the minimax regret of specific classes more quantitatively using our contextual Shtarkov sums. It is also intriguing to consider the setting of infinite labels. Although most of our arguments would go through under sufficient regularity conditions, a more systematic study is needed. On the practical side, it is important to develop algorithms that are more computationally efficient than cNML and with provable guarantees. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We are grateful to Changlong Wu for telling us about Proposition C.2 and to Zeyu Jia for insights leading to Lemma 3.8. We would also like to thank Blair Bilodeau and Sasha Voitovych for helpful discussions and comments on earlier drafts of this work. ZL is supported by the Vector Research Grant at the Vector Institute. IA is supported by the Vatat Scholarship from the Israeli Council for Higher Education. DMR is supported by an NSERC Discovery Grant and funding through his Canada CIFAR AI Chair at the Vector Institute. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[ABDMNY21] N. Alon, O. Ben-Eliezer, Y. Dagan, S. Moran, M. Naor, and E. Yogev. \u201cAdversarial laws of large numbers and optimal regret in online classification\u201d. In: Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing. 2021, pp. 447\u2013455 (cit. on p. 3).   \n[AHKKV23] I. Attias, S. Hanneke, A. Kalavasis, A. Karbasi, and G. Velegkas. \u201cOptimal Learners for Realizable Regression: PAC Learning and Online Learning\u201d. In: Thirtyseventh Conference on Neural Information Processing Systems. 2023 (cit. on p. 3).   \n[BRY98] A. Barron, J. Rissanen, and B. Yu. \u201cThe minimum description length principle in coding and modeling\u201d. In: IEEE transactions on information theory 44.6 (1998), pp. 2743\u20132760 (cit. on p. 2).   \n[BPS09] S. Ben-David, D. P\u00e1l, and S. Shalev-Shwartz. \u201cAgnostic Online Learning.\u201d In: COLT. Vol. 3. 2009, p. 1 (cit. on p. 3).   \n[BHS23] A. Bhatt, N. Haghtalab, and A. Shetty. \u201cSmoothed Analysis of Sequential Probability Assignment\u201d. In: Thirty-seventh Conference on Neural Information Processing Systems. 2023 (cit. on p. 3).   \n[BFR20] B. Bilodeau, D. Foster, and D. Roy. \u201cTight bounds on minimax regret under logarithmic loss via self-concordance\u201d. In: International Conference on Machine Learning. PMLR. 2020, pp. 919\u2013929 (cit. on pp. 2, 3, 6, 10, 14, 15, 25).   \n[BFR23] B. Bilodeau, D. J. Foster, and D. M. Roy. \u201cMinimax rates for conditional density estimation via empirical entropy\u201d. In: The Annals of Statistics 51.2 (2023), pp. 762\u2013790 (cit. on pp. 7, 15).   \n[CL06] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge university press, 2006 (cit. on pp. 1, 2).   \n[CL99] N. Cesa-Bianchi and G. Lugosi. \u201cMinimax regret under log loss for general classes of experts\u201d. In: Proceedings of the Twelfth annual conference on computational learning theory. 1999, pp. 12\u201318 (cit. on p. 3).   \n[Cov74] T. M. Cover. \u201cUniversal gambling schemes and the complexity measures of Kolmogorov and Chaitin\u201d. In: Technical Report, no. 12 (1974) (cit. on pp. 1, 2).   \n[Cov91] T. M. Cover. \u201cUniversal portfolios\u201d. In: Mathematical finance 1.1 (1991), pp. 1\u2013 29 (cit. on pp. 1, 2).   \n[CO96] T. M. Cover and E. Ordentlich. \u201cUniversal portfolios with side information\u201d. In: IEEE Transactions on Information Theory 42.2 (1996), pp. 348\u2013363 (cit. on pp. 1, 2).   \n[Dav73] L. D. Davisson. \u201cUniversal noiseless coding\u201d. In: IEEE Trans. Inf. Theory 19 (1973), pp. 783\u2013795 (cit. on p. 2).   \n[DS04] M. Drmota and W. Szpankowski. \u201cPrecise minimax redundancy and regret\u201d. In: IEEE Transactions on Information Theory 50.11 (2004), pp. 2686\u20132707 (cit. on p. 2).   \n[Fed91] M. Feder. \u201cGambling using a finite state machine\u201d. In: IEEE Transactions on Information Theory 37.5 (1991), pp. 1459\u20131465 (cit. on pp. 1, 2).   \n[FMG92] M. Feder, N. Merhav, and M. Gutman. \u201cUniversal prediction of individual sequences\u201d. In: IEEE transactions on Information Theory 38.4 (1992), pp. 1258\u2013 1270 (cit. on p. 2).   \n[Fit66] B. Fitingof. \u201cOptimal encoding with unknown and variable message statistics\u201d. In: Probl. Inform. Transm. 2 (1966), pp. 3\u201311 (cit. on p. 2).   \n[FKLMS18] D. J. Foster, S. Kale, H. Luo, M. Mohri, and K. Sridharan. \u201cLogistic regression: The importance of being improper\u201d. In: Conference on learning theory. PMLR. 2018, pp. 167\u2013208 (cit. on p. 1).   \n[Fre96] Y. Freund. \u201cPredicting a binary sequence almost as well as the optimal biased coin\u201d. In: Proceedings of the ninth annual conference on Computational learning theory. 1996, pp. 89\u201398 (cit. on p. 2).   \n[Gr\u00fc05] P. Gr\u00fcnwald. \u201cMinimum description length tutorial\u201d. In: (2005) (cit. on p. 2).   \n[HY01] M. H. Hansen and B. Yu. \u201cModel selection and the principle of minimum description length\u201d. In: Journal of the American Statistical Association 96.454 (2001), pp. 746\u2013774 (cit. on p. 2).   \n[JSS21] P. Jacquet, G. Shamir, and W. Szpankowski. \u201cPrecise minimax regret for logistic regression with categorical feature values\u201d. In: Algorithmic Learning Theory. PMLR. 2021, pp. 755\u2013771 (cit. on pp. 2, 4, 5).   \n[Kel56] J. L. Kelly. \u201cA new interpretation of information rate\u201d. In: the bell system technical journal 35.4 (1956), pp. 917\u2013926 (cit. on pp. 1, 2).   \n[Kol65] A. N. Kolmogorov. \u201cThree approaches to the definition of the concept \u201cquantity of information\u201d\u201d. In: Problemy peredachi informatsii 1.1 (1965), pp. 3\u201311 (cit. on p. 2).   \n[Lit88] N. Littlestone. \u201cLearning quickly when irrelevant attributes abound: A new linearthreshold algorithm\u201d. In: Machine learning 2 (1988), pp. 285\u2013318 (cit. on p. 3). N. Merhav and M. Feder. \u201cUniversal prediction\u201d. In: IEEE Transactions on Information Theory 44.6 (1998), pp. 2124\u20132147 (cit. on pp. 1, 2).   \nJ. Mourtada and S. Ga\u00efffas. \u201cAn improper estimator with optimal excess risk in misspecified density estimation and logistic regression\u201d. In: Journal of Machine Learning Research 23.31 (2022), pp. 1\u201349 (cit. on p. 22).   \nM. Opper and D. Haussler. \u201cWorst case prediction over sequences under log loss\u201d. In: The Mathematics of Information Coding, Extraction and Distribution. Springer, 1999, pp. 81\u201390 (cit. on p. 3).   \nA. Orlitsky and N. P. Santhanam. \u201cSpeaking of infinity [iid strings]\u201d. In: IEEE Transactions on Information Theory 50.10 (2004), pp. 2215\u20132230 (cit. on p. 2). A. Rakhlin and K. Sridharan. \u201cOnline non-parametric regression\u201d. In: Conference on Learning Theory. PMLR. 2014, pp. 1232\u20131264 (cit. on pp. 2, 3).   \nA. Rakhlin and K. Sridharan. \u201cStatistical Learning and Sequential Prediction\u201d. In: 2014 (cit. on p. 3).   \nA. Rakhlin and K. Sridharan. \u201cSequential probability assignment with binary alphabets and large classes of experts\u201d. In: arXiv preprint arXiv:1501.07340 (2015) (cit. on pp. 2, 3, 7, 14, 25).   \nA. Rakhlin, K. Sridharan, and A. Tewari. \u201cOnline learning: Random averages, combinatorial parameters, and learnability\u201d. In: Advances in Neural Information Processing Systems 23 (2010) (cit. on pp. 2, 3).   \nA. Rakhlin, K. Sridharan, and A. Tewari. \u201cSequential complexities and uniform martingale laws of large numbers\u201d. In: Probability theory and related fields 161 (2015), pp. 111\u2013153 (cit. on pp. 1\u20133, 9, 10, 14).   \nS. Rakhlin, O. Shamir, and K. Sridharan. \u201cRelax and randomize: From value to algorithms\u201d. In: Advances in Neural Information Processing Systems 25 (2012) (cit. on p. 9).   \nJ. Rissanen. \u201cModeling by shortest data description\u201d. In: Automatica 14.5 (1978), pp. 465\u2013471 (cit. on p. 2).   \nJ. Rissanen. \u201cUniversal coding, information, prediction, and estimation\u201d. In: IEEE Transactions on Information theory 30.4 (1984), pp. 629\u2013636 (cit. on pp. 1, 2). J. Rissanen. \u201cComplexity of strings in the class of Markov sources\u201d. In: IEEE Transactions on Information Theory 32.4 (1986), pp. 526\u2013532 (cit. on p. 2). J. Rissanen. \u201cStochastic complexity\u201d. In: Journal of the Royal Statistical Society: Series B (Methodological) 49.3 (1987), pp. 223\u2013239 (cit. on p. 2).   \nJ. Rissanen and G. Langdon. \u201cUniversal modeling and coding\u201d. In: IEEE Transactions on Information Theory 27.1 (1981), pp. 12\u201323 (cit. on p. 2).   \nJ. J. Rissanen. \u201cGeneralized Kraft inequality and arithmetic coding\u201d. In: IBM Journal of research and development 20.3 (1976), pp. 198\u2013203 (cit. on p. 2). J. J. Rissanen. \u201cFisher information and stochastic complexity\u201d. In: IEEE transactions on information theory 42.1 (1996), pp. 40\u201347 (cit. on p. 2).   \nG. I. Shamir. \u201cOn the MDL principle for iid sources with large alphabets\u201d. In: IEEE transactions on information theory 52.5 (2006), pp. 1939\u20131955 (cit. on p. 2).   \nG. I. Shamir. \u201cLogistic regression regret: What\u2019s the catch?\u201d In: Conference on Learning Theory. PMLR. 2020, pp. 3296\u20133319 (cit. on p. 1).   \nY. M. Shtarkov. \u201cUniversal Sequential Coding of Single Messages\u201d. In: Problems of Information Transmission 23 (3 1987), pp. 3\u201317 (cit. on pp. 1, 2, 4).   \nM. Sion. \u201cOn general minimax theorems\u201d. In: Pacific Journal of Mathematics 8 (1958), pp. 171\u2013176 (cit. on p. 15).   \nR. Solmonoff. \u201cA formal theory of inductive inference. I\u201d. In: II Information and Control 7 (1964), pp. 224\u2013254 (cit. on p. 2).   \nW. Szpankowski. \u201cOn asymptotics of certain recurrences arising in universal coding\u201d. In: PROBLEMS OF INFORMATION TRANSMISSION C/C OF PROBLEMY PEREDACHI INFORMATSII 34 (1998), pp. 142\u2013146 (cit. on p. 2). ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[Szp98] ", "page_idx": 11}, {"type": "text", "text": "V. G. Vovk. \u201cA game of prediction with expert advice\u201d. In: Proceedings of the eighth annual conference on Computational learning theory. 1995, pp. 51\u201360 (cit. on p. 1).   \n[WHGS23] C. Wu, M. Heidari, A. Grama, and W. Szpankowski. \u201cRegret Bounds for Log-loss via Bayesian Algorithms\u201d. In: IEEE Transactions on Information Theory (2023) (cit. on pp. 2\u20134, 6, 7, 15, 25, 26).   \n[XB97] Q. Xie and A. R. Barron. \u201cMinimax redundancy for the class of memoryless sources\u201d. In: IEEE Transactions on Information Theory 43.2 (1997), pp. 646\u2013 657 (cit. on p. 2).   \n[XB00] Q. Xie and A. R. Barron. \u201cAsymptotic minimax regret for data compression, gambling, and prediction\u201d. In: IEEE Transactions on Information Theory 46.2 (2000), pp. 431\u2013445 (cit. on p. 1).   \n[ZL77] J. Ziv and A. Lempel. \u201cA universal algorithm for sequential data compression\u201d. In: IEEE Transactions on information theory 23.3 (1977), pp. 337\u2013343 (cit. on p. 2).   \n[ZL78] J. Ziv and A. Lempel. \u201cCompression of individual sequences via variable-rate coding\u201d. In: IEEE transactions on Information Theory 24.5 (1978), pp. 530\u2013536 (cit. on p. 2). ", "page_idx": 12}, {"type": "text", "text": "A Proofs for Section 3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Notations. When the context and label sequences $x_{1:T},y_{1:T}$ are clear from the context, we may use $f_{t}$ to denote the probability vector $f(x_{1:t},y_{1:t-1})\\in\\Delta(\\mathcal{X})$ produced by hypothesis $f$ at time $t$ for notational convenience. We also adopt the notation for repeated operators in [RST15; BFR20], denoting $\\mathrm{Opt}_{1}\\cdot\\cdot\\cdot\\mathrm{Opt}_{T}[\\cdot\\cdot\\cdot]$ by $\\langle\\mathrm{(Opt}_{t}\\rangle\\!\\!\\rangle_{t=1}^{T}\\left[\\,\\cdots\\right]$ . For any discrete distribution $P$ and discrete random variables $X,Y$ , let $H(P)$ be the entropy of $P$ and $H(X|Y)$ be the conditional entropy of $X$ given $Y$ . ", "page_idx": 13}, {"type": "text", "text": "A.1 Proof overview of Theorem 3.2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Before presenting the proof of Theorem 3.2 in full details in Appendices A.2 to A.4, we give a high-level overview here. ", "page_idx": 13}, {"type": "text", "text": "The proof starts from swapping the pairs of inf and sup (after randomizing the labels revealed by the nature) in the extensive formulation of $\\mathcal{R}_{T}(\\mathcal{F})$ to move to the dual game, where the learner predicts after seeing the action of the nature. Trivially the value of this swapped game is a lower bound for $\\dot{\\mathcal{R}}_{T}(\\mathcal{F})$ , and after rearranging we get that ", "page_idx": 13}, {"type": "equation", "text": "$$\ng a m e=\\operatorname*{sup}_{\\mathbf{x},\\mathbf{p}}\\mathbb{E}_{\\mathbf{y}\\sim\\mathbf{p}}[\\mathcal{R}_{T}(\\mathcal{F};\\mathbf{p}(\\mathbf{y}),\\mathbf{x}(\\mathbf{y}),\\mathbf{y})]\\leq\\mathcal{R}_{T}(\\mathcal{F}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the supremum is taken over all context trees $\\mathbf{x}$ and probabilistic trees $\\mathbf{p}$ , of depth $T$ . Also $\\mathbb{E}_{\\mathbf{y}\\sim\\mathbf{p}}$ means the nested conditional expectations $\\mathbb{E}_{y_{1}\\sim\\mathbf{p}_{1}(\\mathbf{y})}\\mathbb{E}_{y_{2}\\sim\\mathbf{p}_{2}(\\mathbf{y})}\\cdot\\cdot\\cdot\\mathbb{E}_{y_{T}\\sim\\mathbf{p}_{T}(\\mathbf{y})}$ . ", "page_idx": 13}, {"type": "text", "text": "Similar to the proof of Lemma 6 in [BFR20] for the binary label setting, we apply the minimax theorem with a tweak that we devise to handle multiary labels to derive that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F})=\\operatorname*{sup}_{\\mathbf{x},\\mathbf{p}}\\mathbb{E}_{\\mathbf{y}\\sim\\mathbf{p}}[\\mathcal{R}_{T}(\\mathcal{F};\\mathbf{p}(\\mathbf{y}),\\mathbf{x}(\\mathbf{y}),\\mathbf{y})]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "under some mild regularity condition for $\\mathcal{F}$ . A key observation is that the supremum over depth- $\\boldsymbol{\\cdot}\\boldsymbol{T}$ probabilistic trees $\\mathbf{p}$ is equivalent to the supremum over joint distributions $P$ over $\\upgamma^{T}$ . Based on this observation and some algebra, for a fixed context tree $\\mathbf{x}$ , the supremum over $p$ in Eq. (3) is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{P\\in\\Delta(y^{T})}H(P)+\\mathbb{E}_{\\mathbf{y}\\sim P}\\big[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\log P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))\\big].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The value of this maximization problem can be easily computed to be ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\log\\left(\\sum_{\\mathbf{y}}\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))\\right)=\\log S_{T}(\\mathcal{F}|\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{T}(\\mathcal{F})=\\underset{\\mathbf{x},\\mathbf{p}}{\\operatorname*{sup}}\\mathbb{E}_{\\mathbf{y}\\sim\\mathbf{p}}[\\mathcal{R}_{T}(\\mathcal{F};\\mathbf{p}(\\mathbf{y}),\\mathbf{x}(\\mathbf{y}),\\mathbf{y})]}\\\\ &{\\qquad\\qquad=\\underset{\\mathbf{x}}{\\operatorname*{sup}}\\underset{P\\in\\Delta(\\mathcal{Y}^{T})}{\\operatorname*{sup}}H(P)+\\mathbb{E}_{\\mathbf{y}\\sim P}\\big[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\log P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))\\big]=\\underset{\\mathbf{x}}{\\operatorname*{sup}}\\log S_{T}(\\mathcal{F}|\\mathbf{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "However, Eq. (3) is not guaranteed when there is no assumed regularity condition for $\\mathcal{F}$ . To get away from this, prior works have assumed a particular hypothesis is included in $\\mathcal{F}$ such that the enlarged class allows for the minimax swap [RS15; BFR20]. Nevertheless, even adding a mere hypothesis may lead to suboptimal analysis for some classes $\\mathcal{F}$ , say when $\\mathcal{R}_{T}(\\mathcal{F})$ is of constant order. To completely get rid of any regularity assumption and obtain a unified characterization of the minimax regret for arbitrary class $\\mathcal{F}$ , we provide a novel argument as follows. For an arbitrary class $\\mathcal{F}$ , we study a smooth truncated version of it, denoted by $\\bar{\\mathcal F}^{\\delta}$ for any level $\\delta\\in(0,1/2)$ , such that $\\mathcal{F}^{\\delta}$ always validates the use of the minimax theorem and hence $\\begin{array}{r}{\\mathcal{R}_{T}(\\mathcal{F}^{\\delta})=\\operatorname*{sup}_{\\mathbf{x}}\\log S_{T}(\\mathcal{F}^{\\delta}|\\mathbf{x})}\\end{array}$ . Then we give a series of refined analysis comparing the minimax regrets and contextual Shtarkov sums of $\\mathcal{F}$ and $\\mathcal{F}^{\\delta}$ that yields ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{T}(\\mathcal{F})\\leq\\mathcal{R}_{T}(\\mathcal{F}^{\\delta})+T\\log(1+|\\mathcal{V}|\\delta)=\\underset{\\mathbf{x}}{\\operatorname*{sup}}\\log S_{T}(\\mathcal{F}^{\\delta}|\\mathbf{x})+T\\log(1+|\\mathcal{V}|\\delta)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\log\\biggl(\\underset{\\mathbf{x}}{\\operatorname*{sup}}S_{T}(\\mathcal{F}|\\mathbf{x})+\\delta\\cdot C(T,|\\mathcal{V}|)\\biggr)+T\\log(1+|\\mathcal{V}|\\delta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $C(T,|\\mathcal{V}|)\\,<\\,\\infty$ is a positive constant that only depends on $T$ and $|\\mathcal{V}|$ . Sending $\\delta\\,\\rightarrow\\,0^{+}$ will conclude that $\\begin{array}{r}{\\mathcal{R}_{T}(\\mathcal{F})\\leq\\operatorname*{sup}_{\\mathbf{x}}\\log S_{T}(\\mathcal{F}|\\mathbf{x})}\\end{array}$ , which finishes the whole proof as we already have $\\begin{array}{r}{\\mathcal{R}_{T}(\\mathcal{F})\\geq\\operatorname*{sup}_{\\mathbf{x}}\\log S_{T}(\\mathcal{F}|\\mathbf{x})}\\end{array}$ from the start. ", "page_idx": 13}, {"type": "text", "text": "A.2 Minimax swap ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As standard in online learning literature, we will first move to a dual game after applying a minimax swap at each round of the game. Under mild assumptions, the value of the original game coincides with the that of the swapped game. More specifically, we have: ", "page_idx": 14}, {"type": "text", "text": "Lemma A.1 Whenever $\\mathcal{F}$ satisfies that for every sequence $x_{1:T}\\in\\mathcal{X}^{T},y_{1:T}\\in\\mathcal{Y}^{T},$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f\\in{\\mathcal{F}}}\\sum_{t=1}^{T}\\ell(f(x_{1:t},y_{1:t-1}),y_{t})<\\infty,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F})=\\underset{\\mathbf{x},\\mathbf{p}}{\\operatorname*{sup}}\\,\\mathbb{E}_{\\mathbf{y}\\sim\\mathbf{p}}[\\mathcal{R}_{T}(\\mathcal{F};\\mathbf{p}(\\mathbf{y}),\\mathbf{x}(\\mathbf{y}),\\mathbf{y})],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the supremum is taken over all $\\mathcal{X}$ -valued $\\boldsymbol{\\wp}$ -ary trees x and $\\Delta(\\mathcal{Y})$ -valued $\\boldsymbol{\\wp}$ -ary trees p, of depth T. Also $\\mathbb{E}_{\\mathbf{y}\\sim\\mathbf{p}}$ means the nested conditional expectations $\\mathbb{E}_{y_{1}\\sim\\mathbf{p}_{1}(\\mathbf{y})}\\mathbb{E}_{y_{2}\\sim\\mathbf{p}_{2}(\\mathbf{y})}\\cdot\\cdot\\cdot\\mathbb{E}_{y_{T}\\sim\\mathbf{p}_{T}(\\mathbf{y})}$ . ", "page_idx": 14}, {"type": "text", "text": "To deal with the unboundedness of log loss in the proof, we introduce the following truncation method inspired by [BFR23; WHGS23], generalizing the one in [BFR20] which was specific to binary labels. ", "page_idx": 14}, {"type": "text", "text": "Definition A.2 (Smooth truncation) The general smooth truncation map $\\tau_{\\delta}:\\Delta(\\mathcal{V})\\rightarrow\\Delta(\\mathcal{V})$ is defined such that for all $p\\in\\Delta(\\mathcal{V})$ and $y\\in\\mathcal{V}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tau_{\\delta}(p)(y)=\\frac{p(y)+\\delta}{1+|\\mathcal{Y}|\\delta},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "given threshold $\\delta\\in(0,1/2)$ . ", "page_idx": 14}, {"type": "text", "text": "It is easy to check that $\\tau_{\\delta}(p)$ is indeed a valid member in $\\Delta(\\mathcal{Y})$ and $\\tau_{\\delta}(p)(y)\\in[\\delta/(1+|\\mathcal{V}|\\delta),(1+$ $\\delta)/(1+|\\mathcal{V}|\\delta)]$ . Moreover, it is not hard to verify that $\\tau_{\\delta}(\\Delta(\\mathcal{Y}))=\\{p\\in\\Delta(\\mathcal{Y}):p(y)\\in[\\delta/(1+$ $|\\mathcal{V}|\\delta),(1+\\delta)/(1+|\\mathcal{V}|\\delta)],\\forall y\\in\\mathcal{V}\\}$ . We will use $\\Delta^{\\delta}(\\mathcal{Y})$ to denote this image set $\\tau_{\\delta}(\\Delta(\\mathcal{Y}))$ . ", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma A.1 Fix $\\delta\\in(0,1/2)$ . By restricting the learner\u2019s prediction $\\hat{p}_{t}$ to $\\Delta^{\\delta}(\\mathcal{Y})$ , we get an upper bound on $\\mathcal{R}_{T}(\\mathcal{F})$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{T}(\\mathcal{F})\\leq\\left\\langle\\!\\!\\left\\langle\\operatorname*{sup}_{x_{t}}\\operatorname*{inf}_{\\hat{p}_{t}\\in\\Delta^{\\delta}(y)}\\operatorname*{sup}_{y_{t}}\\!\\right\\rangle\\!\\right\\rangle_{t=1}^{T}\\left[\\displaystyle\\sum_{t=1}^{T}\\ell(\\hat{p}_{t},y_{t})-\\operatorname*{inf}_{f\\in\\mathcal{F}}\\displaystyle\\sum_{t=1}^{T}\\ell(f_{t},y_{t})\\right]}\\\\ &{\\qquad\\qquad=\\left\\langle\\!\\!\\left\\langle\\operatorname*{sup}_{x_{t}}\\operatorname*{inf}_{\\hat{p}_{t}\\in\\Delta^{\\delta}(y)}\\operatorname*{sup}_{y_{t}}\\!\\right\\rangle\\!\\!\\right\\rangle_{t=1}^{T-1}\\operatorname*{sup}_{\\mathcal{F}T\\in\\Delta^{\\delta}(y)}\\operatorname*{sup}_{p_{T}}\\!\\!\\mathbb{E}_{y_{T}\\sim p_{T}}\\left[\\displaystyle\\sum_{t=1}^{T}\\ell(\\hat{p}_{t},y_{t})-\\operatorname*{inf}_{f\\in\\mathcal{F}}\\displaystyle\\sum_{t=1}^{T}\\ell(f_{t},y_{t})\\right]\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now we can apply Sion\u2019s minimax theorem [Sio58] to the function ", "page_idx": 14}, {"type": "equation", "text": "$$\nA(\\hat{p}_{T},p_{T})=\\mathbb{E}_{y_{T}\\sim p_{T}}\\Big[\\sum_{t=1}^{T}\\ell(\\hat{p}_{t},y_{t})-\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\ell(f_{t},y_{t})\\Big]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "to derive that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{p}_{T}\\in\\Delta^{\\delta}(\\mathcal{Y})}\\operatorname*{sup}_{p_{T}\\in\\Delta(\\mathcal{Y})}A\\bigl(\\hat{p}_{T},p_{T}\\bigr)=\\operatorname*{sup}_{p_{T}\\in\\Delta(\\mathcal{Y})}\\operatorname*{inf}_{\\hat{p}_{T}\\in\\Delta^{\\delta}(\\mathcal{Y})}A\\bigl(\\hat{p}_{T},p_{T}\\bigr).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This is because: ", "page_idx": 14}, {"type": "text", "text": "1. $A(\\hat{p}_{T},p_{T})$ is convex and continuous in $\\hat{p}_{T}$ over the compact $\\Delta^{\\delta}(\\mathcal{Y})$ and ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "2. $A(\\hat{p}_{T},p_{T})$ is concave and continuous in $p_{T}$ over the compact $\\Delta(\\mathcal{Y})$ , which is further due to that $A(\\hat{p}_{T},p_{T})$ is linear in $p_{T}$ and is bounded given Eq. (4). ", "page_idx": 14}, {"type": "text", "text": "Hence ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{T}(\\mathcal{F})\\leq\\Bigg\\langle\\!\\Bigg\\langle\\!\\!\\operatorname*{sup}_{x_{t}}\\operatorname*{inf}_{\\hat{p}_{t}\\in\\Delta^{\\delta}(y)}\\operatorname*{sup}_{y_{t}}\\!\\Bigg\\rangle\\!\\Bigg\\rangle_{t=1}^{T-1}\\operatorname*{sup}_{y_{T}}\\operatorname*{inf}_{\\hat{p}_{T}\\in\\Delta^{\\delta}(y)}\\mathbb{E}_{y_{T}\\sim p_{T}}\\bigg[\\displaystyle\\sum_{t=1}^{T}\\ell(\\hat{p}_{t},y_{t})-\\operatorname*{inf}_{f\\in\\mathcal{F}}\\!\\sum_{t=1}^{T}\\ell(f_{t},y_{t})\\bigg]}\\\\ &{\\quad\\quad\\quad=\\Bigg\\langle\\!\\Bigg\\langle\\!\\operatorname*{sup}_{x_{t}}\\operatorname*{inf}_{\\hat{p}_{t}\\in\\Delta^{\\delta}(y)}\\!\\Bigg\\rangle\\!\\Bigg\\rangle_{t=1}^{T-2}\\operatorname*{sup}_{x_{T-1}}\\operatorname*{inf}_{\\hat{p}_{T-1}\\in\\Delta^{\\delta}(y)}\\operatorname*{sup}_{p_{T-1}}\\!\\!\\!\\mathrm{sup}_{x_{T-1}\\sim p_{T-1}}}\\\\ &{\\bigg[\\displaystyle\\sum_{t=1}^{T-1}\\ell(\\hat{p}_{t},y_{t})+\\operatorname*{sup}_{x_{T}}\\operatorname*{sup}_{\\hat{p}_{T}\\in\\Delta^{\\delta}(y)}\\mathbb{E}_{y_{T}\\sim p_{T}}\\ell(\\hat{p}_{T},y_{T})-\\mathbb{E}_{y_{T}\\sim p_{T}}\\operatorname*{inf}_{f\\in\\mathcal{F}}\\!\\sum_{t=1}^{T}\\ell(f_{t},y_{t})\\bigg]\\Bigg\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Again the order of $\\operatorname*{inf}_{\\hat{p}_{T-1}\\in\\Delta^{\\delta}(\\mathcal{Y})}$ and $\\ensuremath{\\mathrm{sup}}_{p_{T-1}\\in\\Delta(\\mathcal{Y})}$ with respect to ", "page_idx": 15}, {"type": "equation", "text": "$$\n3(\\hat{p}_{T-1},p_{T-1})=\\mathbb{E}_{y_{T-1}\\sim p_{T-1}}\\bigg[\\sum_{t=1}^{T-1}\\ell(\\hat{p}_{t},y_{t})+\\operatorname*{sup}_{x_{T}}\\operatorname*{sup}_{p_{T}}\\Big[\\operatorname*{inf}_{\\hat{p}_{T}\\in\\Delta^{\\delta}(y)}\\mathbb{E}_{y_{T}\\sim p_{T}}\\ell(\\hat{p}_{T},y_{T})-\\mathbb{E}_{y_{T}\\sim p_{T}}\\operatorname*{inf}_{f\\in\\mathcal{F}(\\hat{p}_{t},y_{T})\\in\\mathcal{F}(\\hat{p}_{T})}\\mathbb{E}_{y_{T}\\sim p_{T}}\\ell(\\hat{p}_{T},y_{T})\\Big]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "can be swapped due to the same reason as above, leading to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{T}(\\mathcal{F})\\leq\\Bigg\\{\\underset{x_{T}}{\\underbrace{\\operatorname*{inf}}}\\ \\underset{\\hat{r}_{t}\\in\\Delta^{\\xi}(\\mathcal{R})}{\\underbrace{\\operatorname*{inf}}}\\ \\underset{y_{t+1}}{\\underbrace{\\operatorname*{sup}}}\\ \\Bigg\\}_{t=1}^{T-2}\\ \\underset{x_{T-1}}{\\underbrace{\\operatorname*{sup}}}\\ \\underset{\\hat{p}_{T-1}\\in\\Delta^{\\xi}(\\mathcal{R})}{\\underbrace{\\operatorname*{inf}}}\\ \\underset{\\hat{r}_{t}\\in T^{-1}\\sim\\mathcal{P}^{T}-1}}\\\\ &{\\ \\ \\ \\ \\Big\\Big\\ \\Big\\ \\Big\\ \\Big\\ \\Big\\{\\sum_{t=1}^{T-1}\\ell(\\hat{\\rho}_{t},y_{t})+\\underset{x_{T}}{\\operatorname*{sup}}\\Big[\\underset{\\hat{r}_{T-1}}{\\underbrace{\\operatorname*{inf}}}\\ \\underset{\\hat{p}_{T-r}\\in\\Delta^{\\xi}(\\mathcal{P})}{\\underbrace{\\operatorname*{inf}}}\\mathbb{E}_{y_{T-\\sim p_{T}}\\ell(\\hat{\\rho}_{T},y_{T})}-\\mathbb{E}_{y_{T}\\sim\\rho_{T-1}}\\underset{\\hat{r}_{t}\\in\\mathcal{P}_{t-1}}{\\underbrace{\\operatorname*{inf}}}\\mathbb{E}_{\\hat{\\rho}_{t}}^{T}\\Big]\\Big\\}}\\\\ &{\\ \\ \\ \\ \\ =\\Bigg\\{\\underset{x_{t}}{\\underbrace{\\left\\{\\operatorname*{sup}}\\underbrace{\\operatorname*{inf}}}{\\underbrace{\\operatorname*{inf}}}}\\ \\underset{\\ell=1}{\\underbrace{\\operatorname*{sup}}}\\ \\underset{\\hat{r}_{t-2}\\in\\Delta^{\\xi}(\\mathcal{R})}{\\underbrace{\\operatorname*{inf}}}\\ \\underset{y_{T-2}}{\\underbrace{\\operatorname*{sup}}}\\ \\underset{\\ell=1}{\\underbrace{\\operatorname*{sup}}}\\ \\mathbb{E}_{p_{T-2}\\sim\\mathcal{P}^{T}-2}}\\\\ &{\\ \\ \\ \\ \\Big\\{\\sum_{t=1}^{T-2}\\ell(\\hat{\\rho}_{t},y_{t})+\\underset{x_{T-1}}{\\operatorname*{sup}}\\ \\underset{\\hat{r}_{T-1}}{\\operatorname*{sup}}\\Big[\\underset{\\hat{p}_{T-1}\\in\\Delta^{\\xi}(\\mathcal{R})}{\\underbrace{\\operatorname*{inf}}}\\ \\mathbb{E}_{y_{T-1}\\sim\\rho_\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Repeating this procedure through all $T$ rounds yields ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F})\\leq\\left\\langle\\left\\langle\\operatorname*{sup}_{x_{t}}\\operatorname*{sup}_{p_{t}\\sim p_{t}}\\right\\rangle\\right\\rangle_{t=1}^{T}\\operatorname*{sup}_{f\\in\\mathcal{F}}\\left[\\sum_{t=1}^{T}\\operatorname*{inf}_{\\hat{p}_{t}\\in\\Delta^{\\delta}(\\mathcal{F})}\\mathbb{E}_{y_{t}\\sim p_{t}}[\\ell(\\hat{p}_{t},y_{t})]-\\ell(f_{t},y_{t})\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By Lemma A.7, we know that we do not lose too much by restricting learner\u2019s prediction to $\\Delta^{\\delta}(\\mathcal{Y})$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F})\\leq\\left\\langle\\!\\left\\langle\\operatorname*{sup}_{x_{t}}\\operatorname*{sup}_{p_{t}\\sim p_{t}}\\right\\rangle\\!\\right\\rangle_{t=1}^{T}\\operatorname*{sup}_{f\\in\\mathcal{F}}\\biggr[\\sum_{t=1}^{T}\\operatorname*{inf}_{\\hat{p}_{t}\\sim p_{t}}[\\ell(\\hat{p}_{t},y_{t})]-\\ell(f_{t},y_{t})\\biggr]+|\\mathcal{Y}|\\delta T.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Sending $\\delta\\rightarrow0^{+}$ on the RHS of the above inequality, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F})\\leq\\left\\langle\\!\\left\\langle\\operatorname*{sup}_{x_{t}}\\operatorname*{sup}_{p_{t}}\\mathbb{E}_{y_{t}\\sim p_{t}}\\right\\rangle\\!\\right\\rangle_{t=1}^{T}\\operatorname*{sup}_{f\\in\\mathcal{F}}\\biggr[\\sum_{t=1}^{T}\\operatorname*{inf}_{\\hat{p}_{t}\\sim p_{t}}[\\ell(\\hat{p}_{t},y_{t})]-\\ell(f_{t},y_{t})\\biggr].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It is easy to see that on the RHS of the above inequality, the inner infimum over $\\hat{p}_{t}\\;\\in\\;\\Delta(\\mathcal{Y})$ is achieved at $\\hat{p}_{t}=p_{t}$ due to the nature of log loss. So ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{T}(\\mathcal{F})\\leq\\Big\\langle\\!\\!\\left\\langle\\underset{x_{t}}{\\operatorname*{sup}}\\operatorname*{sup}_{p_{t}\\sim p_{t}}\\!\\right\\rangle\\!\\!\\right\\rangle_{t=1}^{T}\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\Big[\\underset{t=1}{\\overset{T}{\\sum}}\\mathbb{E}_{y_{t}\\sim p_{t}}[\\ell(p_{t},y_{t})]-\\ell(f_{t},y_{t})\\Big]}\\\\ &{\\quad\\quad\\quad=\\underset{\\mathbf{x},\\mathbf{p}}{\\operatorname*{sup}}\\mathbb{E}_{\\mathbf{y}\\sim\\mathbf{p}}[\\mathcal{R}_{T}(\\mathcal{F};\\mathbf{p}(\\mathbf{y}),\\mathbf{x}(\\mathbf{y}),\\mathbf{y})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where in the last equality we use the compact notation of trees to further simplify our expression and this concludes the proof. \u25a0 ", "page_idx": 15}, {"type": "text", "text": "Lemma A.3 For any hypothesis class $\\mathcal{F}$ and horizon $T$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{x},\\mathbf{p}}\\mathbb{E}_{\\mathbf{y}\\sim\\mathbf{p}}[\\mathcal{R}_{T}(\\mathcal{F};\\mathbf{p}(\\mathbf{y}),\\mathbf{x}(\\mathbf{y}),\\mathbf{y})]=\\operatorname*{sup}_{\\mathbf{x}}\\log S_{T}(\\mathcal{F}|\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It is implied that whenever $\\mathcal{F}$ satisfies Eq. (5), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F})=\\operatorname*{sup}_{\\mathbf{x}}\\log{S_{T}(\\mathcal{F}|\\mathbf{x})}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma A.3 First we can see that the outcome sequence $y_{1:T}$ generated under any tree $\\mathbf{p}$ is the same thing as $y_{1:T}$ generated by its associated joint distribution over $y^{T}$ , and vice versa. So we can replace the supremum over trees $\\mathbf{p}$ in the LHS of Eq. (6) by the supremum over joint distributions $P$ over $y^{T}$ . Hence, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{\\mathbf{x},\\mathbf{p}}\\mathbb{E}_{\\mathbf{y}\\sim\\mathbf{p}}[\\mathcal{R}_{T}(\\mathcal{F};\\mathbf{p}(\\mathbf{y}),\\mathbf{x}(\\mathbf{y}),\\mathbf{y})]=\\displaystyle\\operatorname*{sup}_{\\mathbf{x},P}\\mathbb{E}_{\\mathbf{y}\\sim P}[\\mathcal{R}_{T}(\\mathcal{F};\\mathbf{p}(\\mathbf{y}),\\mathbf{x}(\\mathbf{y}),\\mathbf{y})]}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ &{\\displaystyle=\\operatorname*{sup}_{\\mathbf{x},P}\\mathbb{E}_{\\mathbf{y}\\sim P}\\Big[\\displaystyle\\sum_{t=1}^{T}\\ell(P_{t},y_{t})-\\displaystyle\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\ell(f_{t},y_{t})\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $P_{t}$ denotes the conditional distribution $P_{t}(\\cdot|y_{1:t-1})\\in\\Delta(\\mathcal{Y})$ of $y_{t}$ under $P$ given $y_{1:t-1}$ . ", "page_idx": 16}, {"type": "text", "text": "Now fix the context tree $\\mathbf{x}$ and distribution $P$ . Then we can see that $\\begin{array}{r l}{\\mathbb{E}_{\\mathbf{y}\\sim P}[\\ell(P_{t},y_{t})]}&{{}=}\\end{array}$ $H(y_{t}|y_{1:t-1})$ . So $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{y}\\sim P}[\\sum_{t=1}^{T}\\ell(P_{t},y_{t})]=\\sum_{t=1}^{T}H(y_{t}|y_{1:t-1})=H(P)}\\end{array}$ . Further notice that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\ell(f_{t},y_{t})=\\operatorname*{inf}_{f\\in\\mathcal{F}}(-\\log P_{f}(y_{1:T}|x_{1:T}))=-\\operatorname*{sup}_{f\\in\\mathcal{F}}\\log P_{f}(y_{1:T}|x_{1:T}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So naturally we define the map $F_{\\mathbf{x}}:\\mathcal{V}^{T}\\rightarrow\\mathbb{R}\\cup\\{-\\infty\\}$ by ", "page_idx": 16}, {"type": "equation", "text": "$$\nF_{\\mathbf{x}}(\\mathbf{y})=\\operatorname*{sup}_{f\\in\\mathcal{F}}\\log P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y})),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and then we see that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{y}\\sim P}\\Big[\\sum_{t=1}^{T}\\ell(P_{t},y_{t})-\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\ell((f(x_{t}(\\mathbf{y})),y_{t})\\Big]=H(P)+\\mathbb{E}_{\\mathbf{y}\\sim P}[F_{\\mathbf{x}}(\\mathbf{y})].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For any given tree $\\mathbf{x}$ , notice that the optimization problem ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{P\\in\\Delta(\\mathcal{Y}^{T})}H(P)+\\mathbb{E}_{\\mathbf{y}\\sim P}[F_{\\mathbf{x}}(\\mathbf{y})]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "is actually a maximization problem in the form of $\\mathrm{max}_{P\\in\\Delta(y^{T})}\\,H(P)+\\langle P,v\\rangle$ , where $v$ is some $|\\mathcal{V}|^{T}-$ dimensional vector. According to the conjugacy between negative entropy function and logsum-exp function, the optimal $P^{*}$ is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\nP^{*}(\\mathbf{y})=\\frac{\\exp(F_{\\mathbf{x}}(\\mathbf{y}))}{\\sum_{\\mathbf{y^{\\prime}}}\\exp(F_{\\mathbf{x}}(\\mathbf{y^{\\prime}}))}=\\frac{\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))}{\\sum_{\\mathbf{y^{\\prime}}}\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(\\mathbf{y^{\\prime}}|\\mathbf{x}(\\mathbf{y^{\\prime}}))},\\forall\\mathbf{y}\\in\\mathcal{Y}^{T}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that the above formula for $P^{*}$ is also valid when $F_{\\mathbf{x}}(\\mathbf{y})=-\\infty$ for some $\\mathbf{y}$ , since $P^{*}$ should be supported on $\\{\\mathbf{y}\\in\\mathcal{Y}^{T}:F_{\\mathbf{x}}(\\mathbf{y})>-\\infty\\}$ , and $F_{\\mathbf{x}}(\\mathbf{y})$ cannot be $-\\infty$ for all $\\mathbf{y}$ due to Lemma D.1. The associated value of this maximization problem is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\log\\left(\\sum_{\\mathbf{y}}\\exp(F_{\\mathbf{x}}(\\mathbf{y}))\\right)=\\log\\left(\\sum_{\\mathbf{y}}\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))\\right)\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{\\mathbf{x},\\mathbf{p}}\\mathbb{E}_{\\mathbf{y}\\sim\\mathbf{p}}[\\mathcal{R}_{T}(\\mathcal{F};\\mathbf{p}(\\mathbf{y}),\\mathbf{x}(\\mathbf{y}),\\mathbf{y})]=\\operatorname*{sup}_{\\mathbf{x},P}\\mathbb{E}_{\\mathbf{y}\\sim P}\\Big[\\displaystyle\\sum_{t=1}^{T}\\ell(P_{t},y_{t})-\\displaystyle\\operatorname*{inf}_{f\\in\\mathcal{F}}\\displaystyle\\sum_{t=1}^{T}\\ell(f_{t},y_{t})\\Big]}\\\\ &{\\displaystyle=\\operatorname*{sup}_{\\mathbf{x}}\\operatorname*{sup}\\Big\\{H(P)+\\mathbb{E}_{\\mathbf{y}\\sim P}[F_{\\mathbf{x}}(\\mathbf{y})]\\Big\\}}\\\\ &{\\displaystyle=\\operatorname*{sup}_{\\mathbf{x}}\\log\\left(\\displaystyle\\sum_{\\mathbf{y}}\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))\\right)}\\\\ &{\\displaystyle=\\operatorname*{sup}_{\\mathbf{x}}\\log S_{T}(\\mathcal{F}|\\mathbf{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In the proof of Lemma A.1, if we do not restrict the learner\u2019s prediction and simply swap the order of inf and sup to produce an inequality at each time $t$ , we will reach the following folklore result. ", "page_idx": 17}, {"type": "text", "text": "Lemma A.4 For any hypothesis class $\\mathcal{F}$ and horizon $T$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F})\\geq\\operatorname*{sup}_{\\mathbf{x},\\mathbf{p}}\\mathbb{E}_{\\mathbf{y}\\sim\\mathbf{p}}[\\mathcal{R}_{T}(\\mathcal{F};\\mathbf{p}(\\mathbf{y}),\\mathbf{x}(\\mathbf{y}),\\mathbf{y})].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma A.4 To get Eq. (7), we simply need to reverse the order of sup and inf at each time in the extensive formulation of minimax regret and produce an inequality: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{T}(\\mathcal{F})=\\underset{x_{1}}{\\operatorname*{sup}}\\frac{\\operatorname*{inf}\\operatorname*{sup}}{\\rho_{1}}\\cdots\\underset{x_{T}}{\\operatorname*{sup}}\\frac{\\operatorname*{inf}}{\\rho_{T}}\\underset{y_{T}}{\\operatorname*{sup}}\\mathcal{R}_{T}(\\mathcal{F};\\hat{p}_{1:T},x_{1:T},y_{1:T})}\\\\ &{\\qquad=\\left\\lbrace\\underset{x_{t}}{\\operatorname*{sup}}\\frac{\\operatorname*{inf}}{\\rho_{t}}\\right\\rbrace\\underset{y_{t}}{\\operatorname*{sup}}\\right\\rbrace_{t=1}^{T}\\left[\\underset{c=1}{\\overset{T}{\\sum}}\\ell(\\hat{p}_{t},y_{t})-\\underset{f\\in\\mathcal{F}_{t-1}}{\\operatorname*{inf}}\\frac{T}{\\sum}\\ell(f(x_{1:t},y_{1:t-1}),y_{t})\\right]}\\\\ &{\\qquad=\\left\\lbrace\\underset{x_{t}}{\\operatorname*{sup}}\\frac{\\operatorname*{inf}}{\\rho_{t}}\\underset{y_{t}}{\\operatorname*{sup}}\\right\\rbrace_{t=1}^{T-1}\\underset{x_{T}}{\\operatorname*{sup}}\\frac{\\operatorname*{inf}}{\\rho_{T}}\\underset{y_{T}}{\\operatorname*{sup}}\\underset{x_{t}>r>r}{\\left[\\underset{t=1}{\\overset{T}{\\sum}}\\ell(\\hat{p}_{t},y_{t})-\\underset{f\\in\\mathcal{F}_{t-1}}{\\operatorname*{inf}}\\frac{T}{\\int}\\ell(f_{t},y_{t})\\right]}}\\\\ &{\\qquad\\geq\\left\\lbrace\\underset{x_{t}}{\\operatorname*{sup}}\\frac{\\operatorname*{inf}}{\\rho_{t}}\\underset{y_{t}}{\\operatorname*{sup}}\\right\\rbrace_{t=1}^{T-1}\\underset{x_{T}}{\\operatorname*{sup}}\\underset{y_{T}}{\\operatorname*{sup}}\\frac{\\operatorname*{inf}}{\\rho_{T}}\\mathbb{E}_{y\\sim\\mathcal{P}_{T}}\\left[\\underset{t=1}{\\overset{T}{\\sum}}\\ell(\\hat{p}_{t},y_{t})-\\underset{f\\in\\mathcal{F}_{t-1}}{\\operatorname*{inf}}\\frac{T}{\\int}\\ell(f_{t},y_{t})\\right]}\\\\ &{\\qquad=\\left\\lbrace\\underset{x_{t}}{\\operatorname*{sup}}\\frac{\\operatorname*{inf}}{\\rho_{t}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Iterating the argument and rearranging terms as above, we will get that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{T}(\\mathcal{F})\\geq\\left\\langle\\!\\!\\left\\langle\\underset{x_{t}}{\\operatorname*{sup}}\\operatorname*{sup}_{\\mathcal{P}_{t}\\sim p_{t}}\\!\\right\\rangle\\!\\right\\rangle_{t=1}^{T}\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\Big[\\displaystyle\\sum_{t=1}^{T}\\underset{\\hat{p}_{t}}{\\operatorname*{inf}}\\,\\mathbb{E}_{y_{t}\\sim p_{t}}[\\ell(\\hat{p}_{t},y_{t})]-\\ell(f_{t},y_{t})\\Big]}\\\\ &{\\quad\\quad=\\left\\langle\\!\\left\\langle\\underset{x_{t}}{\\operatorname*{sup}}\\operatorname*{sup}_{\\mathcal{P}_{t}\\sim p_{t}}\\!\\right\\rangle\\!\\right\\rangle_{t=1}^{T}\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\Big[\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{y_{t}\\sim p_{t}}[\\ell(p_{t},y_{t})]-\\ell(f_{t},y_{t})\\Big]}\\\\ &{\\quad\\quad=\\underset{\\mathbf{x},\\mathbf{p}}{\\operatorname*{sup}}\\mathbb{E}_{\\mathbf{y}\\sim\\mathbf{p}}[\\mathcal{R}_{T}(\\mathcal{F};\\mathbf{p}(\\mathbf{y}),\\mathbf{x}(\\mathbf{y}),\\mathbf{y})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.3 Smooth truncated hypothesis class ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To remove the reliance on Eq. (4), we introduce a smooth truncated version of $\\mathcal{F}$ that always satisfies Eq. (4) and study its minimax regret as well as contextual Shtarkov sums, compared to those of the untruncated class $\\mathcal{F}$ . To be more specific, we will apply the smooth truncation map to hypotheses: for any $\\delta\\,\\in\\,(0,1/2)$ and $f:(\\mathcal{X}\\stackrel{\\cdot}{\\times}\\mathcal{Y})^{*}\\times\\mathcal{X}\\rightarrow\\bar{\\Delta(\\mathcal{Y})}$ , we use $f^{\\delta}$ to denote its smooth truncated counterpart $\\tau_{\\delta}\\circ f$ ; for any hypothesis class $\\mathcal{F}$ , we use $\\mathcal{F}^{\\delta}$ to denote the corresponding smooth truncated class $\\tau_{\\delta}\\circ\\mathcal{F}=\\{\\tau_{\\delta}\\circ f:f\\in\\mathcal{F}\\}$ . It is easy to verify that any smooth truncated class $\\mathcal{F}^{\\delta}$ satisfies Eq. (4) and hence ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F}^{\\delta})=\\operatorname*{sup}_{\\mathbf{x}}\\log S_{T}(\\mathcal{F}^{\\delta}|\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next we control the effect of truncation on the minimax regret. ", "page_idx": 17}, {"type": "text", "text": "Lemma A.5 For any $\\mathcal{F},T$ and $\\delta\\in(0,1/2)$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F})\\leq\\mathcal{R}_{T}(\\mathcal{F}^{\\delta})+T\\cdot\\log(1+|\\mathcal{Y}|\\delta).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma A.5 Fix threshold $\\delta\\in(0,1/2)$ and hypothesis $f$ . By Lemma A.7, for any given sequences $x_{1:T},y_{1:T}$ , there is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\ell(f^{\\delta}(x_{1:t},y_{1:t-1}),y_{t})-\\sum_{t=1}^{T}\\ell(f(x_{1:t},y_{1:t-1}),y_{t})\\leq T\\cdot\\log(1+|\\mathcal{Y}|\\delta).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, for any sequence of predictions $\\hat{p}_{1:T}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathcal{R}_{T}(\\mathcal{F};\\hat{p}_{1:T},x_{1:T},y_{1:T})=\\sum_{t=1}^{T}\\ell(\\hat{p}_{t},y_{t})-\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\ell(f_{t},y_{t})}}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{T}\\ell(\\hat{p}_{t},y_{t})-\\operatorname*{inf}_{f^{\\delta}\\in\\mathcal{F}^{\\delta}}\\sum_{t=1}^{T}\\ell(f_{t}^{\\delta},y_{t})+T\\cdot\\log(1+|\\mathcal{V}|\\delta)}\\\\ &{=\\mathcal{R}_{T}(\\mathcal{F}^{\\delta};\\hat{p}_{1:T},x_{1:T},y_{1:T})+T\\cdot\\log(1+|\\mathcal{V}|\\delta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 18}, {"type": "text", "text": "Lemma A.6 There exists a constant $M(T)<\\infty$ that only depends on $T$ such that for any $f,x_{1:T}\\in$ $\\mathcal{X}^{T},y_{1:T}\\in\\mathcal{Y}^{T}$ and $\\delta\\in(0,1/2)$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nP_{f^{\\delta}}(y_{1:T}|x_{1:T})\\leq P_{f}(y_{1:T}|x_{1:T})+\\delta\\cdot M(T).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma A.6 Fix threshold $\\delta\\in(0,1/2)$ , hypothesis $f$ and sequences $x_{1:T},y_{1:T}$ . Then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{P_{f^{\\delta}}\\big(y_{1:T}|x_{1:T}\\big)=\\displaystyle\\prod_{t=1}^{T}f_{t}^{\\delta}(y_{t})=\\displaystyle\\prod_{t}\\Biggl(\\frac{f_{t}(y_{t})+\\delta}{1+|y|\\delta}\\Biggr)}}\\\\ &{\\leq\\displaystyle\\prod_{t=1}^{t}(f_{t}(y_{t})+\\delta)}\\\\ &{=\\displaystyle\\prod_{t}f_{t}(y_{t})+\\delta\\cdot\\sum_{t}\\displaystyle\\prod_{t^{\\prime}\\neq t}f_{t^{\\prime}}(y_{t^{\\prime}})+\\dots+\\delta^{T}}\\\\ &{\\leq\\displaystyle\\prod_{t}f_{t}(y_{t})+\\delta\\cdot M(T)}\\\\ &{=P_{\\varepsilon}(\\eta_{t},\\vert\\eta_{t},x\\vert)+\\delta\\cdot M(T)}\\end{array}\n$$$=P_{f}(y_{1:T}|y_{1:T})+\\delta\\cdot M(T),$ ", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we can set $\\begin{array}{r}{M(T)=T+\\binom{T}{2}+\\binom{T}{3}+\\cdot\\cdot\\cdot+\\binom{T}{T}}\\end{array}$ since $f_{t}(y_{t})$ \u2019s are bounded by 1. ", "page_idx": 18}, {"type": "text", "text": "A.4 Putting together ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Now we are fully prepared to finish the proof of Theorem 3.2, our main result in Section 3. Proof of Theorem 3.2 By Lemma A.6, we have that for any context tree $\\mathbf{x}$ of depth $T$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{y}\\in\\mathcal{Y}^{T}}\\operatorname*{sup}_{f^{\\delta}\\in\\mathcal{F}^{\\delta}}P_{f^{\\delta}}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))\\leq\\sum_{\\mathbf{y}\\in\\mathcal{Y}^{T}}\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))+\\delta\\cdot M(T)\\cdot|\\mathcal{Y}|^{T}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{T}(\\mathcal{F}^{\\delta})=\\underset{\\mathbf{x}}{\\operatorname*{sup}}\\log S_{T}(\\mathcal{F}^{\\delta}|\\mathbf{x})}\\\\ &{\\qquad\\qquad=\\underset{\\mathbf{x}}{\\operatorname*{sup}}\\log\\left(\\underset{\\mathbf{y}\\in\\mathcal{F}^{T}}{\\sum}\\underset{f^{\\delta}\\in\\mathcal{F}^{\\delta}}{\\operatorname*{sup}}P_{f^{\\delta}}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))\\right)}\\\\ &{\\qquad\\qquad\\leq\\underset{\\mathbf{x}}{\\operatorname*{sup}}\\log\\left(\\underset{\\mathbf{y}\\in\\mathcal{F}^{T}}{\\sum}\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))+\\delta\\cdot M(T)\\cdot|\\mathcal{F}|^{T}\\right)}\\\\ &{\\qquad\\qquad=\\log\\left(\\underset{\\mathbf{x}}{\\operatorname*{sup}}\\underset{\\mathbf{y}\\in\\mathcal{F}^{T}}{\\sum}\\underset{f^{\\delta}\\in\\mathcal{F}}{\\operatorname*{sup}}P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))+\\delta\\cdot M(T)\\cdot|\\mathcal{F}|^{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Together with Lemma A.5, we get that for any $\\delta\\in(0,1/2)$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F})\\leq\\log\\left(\\operatorname*{sup}_{{\\bf x}}\\sum_{{\\bf y}\\in\\mathcal{Y}^{T}}\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}({\\bf y}|{\\bf x}({\\bf y}))+\\delta\\cdot M(T)\\cdot|\\mathcal{Y}|^{T}\\right)+T\\cdot\\log(1+|\\mathcal{Y}|\\delta).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "After sending $\\delta\\rightarrow0^{+}$ on the RHS of Eq. (9), ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F})\\leq\\log\\left(\\operatorname*{sup}_{\\mathbf{x}}\\sum_{\\mathbf{y}\\in\\mathcal{Y}^{T}}\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))\\right)=\\operatorname*{sup}_{\\mathbf{x}}\\log S_{T}(\\mathcal{F}|\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recall that we have $\\begin{array}{r}{\\mathcal{R}_{T}(\\mathcal{F})\\geq\\operatorname*{sup}_{\\mathbf{x}}\\log S_{T}(\\mathcal{F}|\\mathbf{x})}\\end{array}$ from Lemma A.4 and Lemma A.3. So finally, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F})=\\operatorname*{sup}_{\\mathbf{x}}\\log{S_{T}(\\mathcal{F}|\\mathbf{x})}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A.5 Additional proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma A.7 For any $p\\in\\Delta(\\mathcal{V})$ and $\\delta\\in(0,1/2)$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\ell(\\tau_{\\delta}(p),y)\\leq\\ell(p,y)+\\log(1+|y|\\delta)\\leq\\ell(p,y)+|y|\\delta,\\forall y\\in\\mathcal{y}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma A.7 By direct computation, for any $y\\in\\mathcal{V}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell(\\tau_{\\delta}(p),y)-\\ell(p,y)=\\log\\displaystyle\\Big(\\frac{p(y)}{p(y)+\\delta}\\cdot(1+|y|\\delta)\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\log\\displaystyle(1+|y|\\delta)}\\\\ &{\\qquad\\qquad\\qquad\\leq|y|\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A.6 Proof of Proposition 3.3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Starting from Theorem 3.2 that $\\begin{array}{r}{\\mathcal{R}_{T}(\\mathcal{F})=\\operatorname*{sup}_{\\mathbf{x}}\\log S_{T}(\\mathcal{F}|\\mathbf{x})}\\end{array}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{T}(\\mathcal{F})=\\underset{\\mathbf{x}}{\\operatorname*{sup}}\\log\\left(\\displaystyle\\sum_{\\mathbf{y}}\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\,P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))\\right)}\\\\ &{\\qquad\\quad\\leq\\underset{\\mathbf{x}}{\\operatorname*{sup}}\\log\\left(\\displaystyle\\sum_{\\mathbf{y}}\\sum_{f\\in\\mathcal{F}}P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))\\right)}\\\\ &{\\qquad=\\underset{\\mathbf{x}}{\\operatorname*{sup}}\\log\\left(\\displaystyle\\sum_{f\\in\\mathcal{F}}\\sum_{\\mathbf{y}}P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))\\right)=\\log|\\mathcal{F}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last equality is due to Lemma D.1. ", "page_idx": 19}, {"type": "text", "text": "A.7 Proof of Lemma 3.8 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "It suffices to show that $\\mathcal{R}_{T}^{\\mathrm{FD}}(\\mathcal{F}^{\\mathrm{Lin}})\\;\\geq\\;\\sqrt{T}/4$ . In particular, we only need to find some context sequence $x_{1:T}$ such that $\\log S_{T}(\\mathcal{F}^{\\mathrm{Lin}}|x_{1:T})\\,\\geq\\,\\sqrt{T}/4$ due to Proposition 2.5. Here we pick $x_{1:T}$ such that $x_{t}$ are unit vectors and $\\boldsymbol{x}_{t}\\,\\perp\\,\\boldsymbol{x}_{t^{\\prime}}$ whenever $t\\,\\ne\\,t^{\\prime}$ . Such sequence exists because the ", "page_idx": 19}, {"type": "text", "text": "dimension of $\\mathbb{B}_{2}$ is no smaller than $T$ . In this way, for each possible label sequence $y_{1:T}\\in\\{0,1\\}^{T}$ , we can see that the $f_{w}\\in\\mathcal{F}^{\\mathrm{Lin}}$ that is indexed by ", "page_idx": 20}, {"type": "equation", "text": "$$\nw=\\sum_{t=1}^{T}{\\frac{2y_{t}-1}{\\sqrt{T}}}x_{t}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "achieves likelihood $\\begin{array}{r}{P_{f_{w}}(y_{1:T}|x_{1:T})=(\\frac{1+1/\\sqrt{T}}{2})^{T}}\\end{array}$ . Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{S}_{T}(\\mathcal{F}^{\\mathrm{Lin}}|x_{1:T})=\\sum_{y_{1:T}\\in\\{0,1\\}^{T}}\\operatorname*{sup}_{f\\in\\mathcal{F}^{\\mathrm{Lin}}}P_{f}(y_{1:T}|x_{1:T})\\ge\\sum_{y_{1:T}\\in\\{0,1\\}^{T}}\\left(\\frac{1+1/\\sqrt{T}}{2}\\right)^{T}=\\left(1+1/\\sqrt{T}\\right)^{T}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which implies that $\\mathcal{R}_{T}^{\\mathrm{FD}}\\big(\\mathcal{F}^{\\mathrm{Lin}}\\big)=\\log S_{T}\\big(\\mathcal{F}^{\\mathrm{Lin}}|x_{1:T}\\big)\\geq T\\log(1+1/\\sqrt{T})\\geq\\sqrt{T}/4$ for all $T\\geq1$ . ", "page_idx": 20}, {"type": "text", "text": "B Proofs for Section 4 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Notations. Again we may use $f_{t}$ to denote the probability vector $f(x_{1:t},y_{1:t-1})\\in\\Delta(\\mathcal{Y})$ produced by hypothesis $f$ at time $t$ when the context and label sequences $x_{1:T},y_{1:T}$ are clear from the context. For a context tree $\\mathbf{x}$ of depth $T\\!-\\!t$ and a path $\\mathbf{y}\\in\\mathcal{Y}^{T-i}$ , we re-index $\\mathbf{x}(\\mathbf{y})$ as $(\\mathbf{x}_{t+1}(\\mathbf{y}),\\dots,\\mathbf{x}_{T}(\\mathbf{y}))$ whenever it takes the last $T-t$ entries of the entire context sequence. And we do the same for the probabilistic tree $\\mathbf{p}$ as well. That is, whenever $\\mathbf{y}\\,=\\,(y_{t+1},\\dots,\\dot{y}_{T})\\,\\in\\,\\mathcal{Y}^{T-t}$ takes the last $T-t$ entries of the whole label sequence and $\\mathbf y\\sim\\mathbf p$ , then we will denote this label generating process by $y_{t+1}\\sim\\mathbf{p}_{t+1}(\\mathbf{y}),\\dots,y_{T}\\sim\\mathbf{\\bar{p}}_{T}(\\mathbf{y})$ . ", "page_idx": 20}, {"type": "text", "text": "B.1 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Recall that the minimax regret is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F})=\\left\\langle\\!\\!\\left\\langle\\operatorname*{sup}_{x_{t}}\\operatorname*{inf}_{\\hat{p}_{t}}\\operatorname*{sup}_{y_{t}}\\!\\right\\rangle\\!\\right\\rangle_{t=1}^{T}\\Big[\\sum_{t=1}^{T}\\ell(\\hat{p}_{t},y_{t})-\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\ell(f(x_{1:t},y_{1:t-1}),y_{t})\\Big].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Through this extensive form of the minimax regret, we know that given $x_{1:t},y_{1:t-1}$ , the minimax prediction $\\hat{p}_{t}^{*}$ at round $t$ is the one that minimizes the following expression over all $\\hat{p}_{t}\\in\\Delta(\\mathcal{N})$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{y_{t}}\\left\\langle\\!\\left\\langle\\operatorname*{sup}_{x_{s}}\\operatorname*{inf}_{\\hat{p}_{s}}\\operatorname*{sup}_{y_{s}}\\right\\rangle\\!\\right\\rangle_{s=t+1}^{T}\\Big[\\sum_{s=t}^{T}\\ell(\\hat{p}_{s},y_{s})-\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{s=1}^{T}\\ell(f(x_{1:s},y_{1:s-1}),y_{s})\\Big]\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Define ", "page_idx": 20}, {"type": "equation", "text": "$$\nG(\\mathcal F,x_{1:t},y_{1:t})=\\left\\langle\\!\\left\\langle\\operatorname*{sup}_{x_{s}}\\operatorname*{inf}_{\\hat{p}_{s}}\\operatorname*{sup}_{y_{s}}\\!\\right\\rangle\\!\\right\\rangle_{s=t+1}^{T}\\Big[\\sum_{s=t+1}^{T}\\ell(\\hat{p}_{s},y_{s})-\\operatorname*{inf}_{f\\in\\mathcal F}\\sum_{s=1}^{T}\\ell(f(x_{1:s},y_{1:s-1}),y_{s})\\Big]\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and now ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{p}_{t}^{*}=\\underset{\\hat{p}_{t}\\in\\Delta(y)}{\\operatorname{argmin}}\\operatorname*{sup}\\Bigl\\{\\ell\\bigl(\\hat{p}_{t},y_{t}\\bigr)+G\\bigl(\\mathcal{F},x_{1:t},y_{1:t}\\bigr)\\Bigr\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The crux of the proof is to show the following: ", "page_idx": 20}, {"type": "text", "text": "Lemma B.1 For any hypothesis class $\\mathcal{F}$ and sequences $x_{1:t}\\in\\mathcal{X}^{t},y_{1:t}\\in\\mathcal{Y}^{t},$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\nG(\\mathcal{F},x_{1:t},y_{1:t})=\\operatorname*{sup}_{\\mathbf{x}}\\log{S_{T}^{x_{1:t},y_{1:t}}(\\mathcal{F}|\\mathbf{x})}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The proof of Lemma B.1 is done by essentially following the same strategy in Appendix A since $G(\\mathcal{F},x_{1:t},y_{1:t})$ admits a similar extensive form with the minimax regret $\\mathcal{R}_{T}(\\mathcal{F})$ . For completeness we provide its proof in Appendix B.2. Given Lemma B.1, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{p}_{t}^{*}=\\underset{\\hat{p}_{t}\\in\\Delta(\\mathcal{Y})}{\\operatorname{argmin}}\\operatorname*{sup}\\Bigl\\{\\ell\\bigl(\\hat{p}_{t},y_{t}\\bigr)+\\operatorname*{sup}\\log S_{T}^{x_{1:t},y_{1:t}}(\\mathcal{F}|\\mathbf{x})\\Bigr\\}}\\\\ &{\\qquad=\\underset{\\hat{p}_{t}\\in\\Delta(\\mathcal{Y})}{\\operatorname{argmin}}\\operatorname*{sup}\\log\\Bigl(\\frac{\\operatorname*{sup}_{\\mathbf{x}}S_{T}^{x_{1:t},y_{1:t}}(\\mathcal{F}|\\mathbf{x})}{\\hat{p}_{t}(y_{t})}\\Bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We apply the following result to solve the above program: ", "page_idx": 20}, {"type": "text", "text": "Lemma B.2 [MG22, Lemma 15] Let $g\\ :\\ \\mathcal{V}\\ \\rightarrow\\ [0,+\\infty]$ be a measurable function such that $\\begin{array}{r}{\\int_{\\mathcal{V}}g(y)d\\mu\\in(0,+\\infty)}\\end{array}$ . Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{p}\\operatorname*{sup}_{y\\in\\mathcal{Y}}\\log\\frac{g(y)}{p(y)}=\\log\\Big(\\int_{\\mathcal{Y}}g(y)\\mu(d y)\\Big),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the infimum in Eq. (11) spans over all probability densities $p:\\mathcal{V}\\to[0,+\\infty)$ with respect to $\\mu,$ , and the infimum is reached at ", "page_idx": 21}, {"type": "equation", "text": "$$\np^{*}=\\frac{g}{\\int_{\\mathcal{V}}g(y)d\\mu}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Letting $\\begin{array}{r}{g(y)=\\operatorname*{sup}_{\\mathbf{x}}S_{T}^{x_{1:t},(y_{1:t-1},y)}(\\mathcal{F}|\\mathbf{x})\\in[0,1]}\\end{array}$ and $\\mu$ be the counting measure on the finite space $\\boldsymbol{\\wp}$ , we can apply Lemma B.2 whenever not all $g(y)$ \u2019s are 0. In this case, we solve that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{p}_{t}^{*}(y)=\\frac{g(y)}{\\sum_{y^{\\prime}\\in\\mathcal{Y}}g(y^{\\prime})}=\\frac{\\operatorname*{sup}_{\\mathbf{x}}S_{T}^{x_{1:t},(y_{1:t-1},y)}(\\mathcal{F}|\\mathbf{x})}{\\sum_{y^{\\prime}\\in\\mathcal{Y}}\\operatorname*{sup}_{\\mathbf{x}}S_{T}^{x_{1:t},(y_{1:t-1},y^{\\prime})}(\\mathcal{F}|\\mathbf{x})},\\forall y\\in\\mathcal{Y}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "On the other hand, if $g(y)=0,\\forall y\\in\\mathcal{y}$ , then any $\\hat{p}_{t}$ such that $\\hat{p}_{t}(y)>0,\\forall y\\in\\mathcal{y}$ , is an minimax optimal prediction. Moreover, it implies that $P_{f}(y_{1:t-1}|x_{1:t-1})=0,\\forall f\\in\\mathcal{F}$ . This is because for arbitrary context tree x, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle0=\\sum_{y_{t}}\\sum_{\\mathbf{y}\\in\\mathcal{Y}^{T-t}}P_{f}(y_{1:t},\\mathbf{y}|x_{1:t},\\mathbf{x}(\\mathbf{y}))}\\\\ {\\displaystyle=\\sum_{y_{t}}P_{f}(y_{1:t}|x_{1:t})}\\\\ {\\displaystyle=P_{f}(y_{1:t-1}|x_{1:t-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "So the cumulative loss for each expert $f$ up to round $t\\,-\\,1$ already blows up to $+\\infty$ and the learner only needs to predict an arbitrary $\\hat{p}\\ \\in\\ \\Delta^{+}(\\mathcal{N})$ in all remaining rounds to achieve $\\mathcal{R}_{T}(\\mathcal{F};\\hat{p}_{1:T},x_{1:T},y_{1:T})=\\overset{\\cdot}{-\\infty}$ . ", "page_idx": 21}, {"type": "text", "text": "Overall, we can see that the minimax optimal prediction $\\hat{p}_{t}^{*}\\in\\Delta(\\mathcal{N})$ at round $t$ given $x_{1:t},y_{1:t-1}$ is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{p}_{t}^{*}(y)=\\frac{\\operatorname*{sup}_{\\mathbf{x}}S_{T}^{x_{1:t},(y_{1:t-1},y)}(\\mathcal{F}|\\mathbf{x})}{\\sum_{y^{\\prime}\\in\\mathcal{Y}}\\operatorname*{sup}_{\\mathbf{x}}S_{T}^{x_{1:t},(y_{1:t-1},y^{\\prime})}(\\mathcal{F}|\\mathbf{x})},\\forall y\\in\\mathcal{Y},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "if there exists y \u2208Y such that supx STx1:t,(y1:t\u22121,y)(F|x) > 0. Otherwise, select p\u02c6t\u2217 to be an arbitrary element in $\\Delta^{+}(\\mathcal{Y})$ (and so do all remaining rounds). ", "page_idx": 21}, {"type": "text", "text": "B.2 Auxiliary lemmas ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Recall that for any hypothesis class $\\mathcal{F}$ and sequences $x_{1:t}\\in\\mathcal{X}^{t},y_{1:t}\\in\\mathcal{Y}^{t}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{x}(\\mathcal{F},x_{1:t},y_{1:t})=\\bigg\\langle\\!\\bigg\\langle\\!\\operatorname*{sup}_{s_{s}}\\operatorname*{inf}_{s_{s}}\\!\\bigg\\rangle\\!\\bigg\\rangle_{s=t+1}^{T}\\Big[\\displaystyle\\sum_{s=t+1}^{T}\\ell(\\hat{p}_{s},y_{s})-\\displaystyle\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{s=1}^{T}\\ell(f(x_{1:s},y_{1:s-1}),y_{s})\\!\\Big]}\\\\ &{\\qquad\\qquad=\\bigg\\langle\\!\\bigg\\langle\\!\\operatorname*{sup}_{s_{s}}\\operatorname*{inf}_{s_{s}}\\operatorname*{sup}_{s}\\!\\bigg\\rVert_{s=t+1}^{T}\\bigg[\\displaystyle\\sum_{s=t+1}^{T}\\ell(\\hat{p}_{s},y_{s})-\\displaystyle\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{s=1}^{T}\\ell(f(x_{1:s},y_{1:s-1}),y_{s})\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To prove Lemma B.1, we need the following lemmas. ", "page_idx": 21}, {"type": "text", "text": "Lemma B.3 For any hypothesis class $\\mathcal{F}$ and sequences $x_{1:t}\\in\\mathcal{X}^{t},y_{1:t}\\in\\mathcal{Y}^{t},$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nG(\\mathcal{F},x_{1:t},y_{1:t})\\geq\\operatorname*{sup}_{{\\bf x},{\\bf p}}\\mathbb{E}_{{\\bf y}\\sim{\\bf p}}\\Big[\\sum_{s=t+1}^{T}\\mathbb{E}_{y_{s}\\sim{\\bf p}_{s}({\\bf y})}[\\ell({\\bf p}_{s}({\\bf y}),y_{s})]-\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{s=1}^{T}\\ell(f_{s},y_{s})\\Big].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "And whenever for every $x_{t+1:T}\\in\\mathcal{X}^{T-t},y_{t+1:T}\\in\\mathcal{Y}^{T-t}$ , it holds ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f\\in{\\mathscr{F}}}\\sum_{s=1}^{T}\\ell(f(x_{1:s},y_{1:s-1}),y_{s})<\\infty,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then ", "page_idx": 22}, {"type": "equation", "text": "$$\nG(\\mathcal{F},x_{1:t},y_{1:t})=\\operatorname*{sup}_{\\mathbf{x},\\mathbf{p}}\\mathbb{E}_{\\mathbf{y}\\sim\\mathbf{p}}\\Big[\\sum_{s=t+1}^{T}\\mathbb{E}_{y_{s}\\sim\\mathbf{p}_{s}(\\mathbf{y})}[\\ell(\\mathbf{p}_{s}(\\mathbf{y}),y_{s})]-\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{s=1}^{T}\\ell(f_{s},y_{s})\\Big].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma B.3 First we see that similar to the proof of Lemma A.4, we can reverse every pair of sup over $p_{s}$ and inf over $\\hat{p}_{s}$ in the extensive formulation of $G(\\mathcal{F},x_{1:t},y_{1:t})$ and rearrange terms to obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\nG(\\mathcal F,x_{1:t},y_{1:t})\\geq\\left\\langle\\!\\left\\langle\\operatorname*{sup}_{x_{s}}\\operatorname*{sup}_{p_{s}\\sim p_{s}}\\right\\rangle\\!\\right\\rangle_{s=t+1}^{T}\\Big[\\sum_{s=t+1}^{T}\\operatorname*{inf}_{\\hat{p}_{s}\\sim p_{s}}[\\ell(\\hat{p}_{s},y_{s})]-\\operatorname*{inf}_{f\\in\\mathcal F}\\sum_{s=1}^{T}\\ell(f_{s},y_{s})\\Big],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and again due to the nature of log loss, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G(\\mathcal{F},x_{1:t},y_{1:t})\\geq\\displaystyle\\left\\langle\\!\\left\\langle\\operatorname*{sup}_{x_{s}}\\operatorname*{sup}_{\\mathcal{F}_{s}\\sim p_{s}}\\right\\rangle\\!\\right\\rangle_{s=t+1}^{T}\\Big[\\displaystyle\\sum_{s=t+1}^{T}\\mathbb{E}_{y_{s}\\sim p_{s}}[\\ell(p_{s},y_{s})]-\\displaystyle\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{s=1}^{T}\\ell(f_{s},y_{s})\\Big]}\\\\ &{\\qquad\\qquad=\\displaystyle\\operatorname*{sup}_{\\mathbf{x},\\mathbf{p}}\\mathbb{E}_{\\mathbf{y}\\sim\\mathbf{p}}\\Big[\\displaystyle\\sum_{s=t+1}^{T}\\mathbb{E}_{y_{s}\\sim\\mathbf{p}_{s}(\\mathbf{y})}[\\ell(\\mathbf{p}_{s}(\\mathbf{y}),y_{s})]-\\displaystyle\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{s=1}^{T}\\ell(f_{s},y_{s})\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where in the last step we compress the expression using trees (of depth $T\\!-\\!t)$ and Eq. (12) is proved. ", "page_idx": 22}, {"type": "text", "text": "To show that the minimax swap is valid under Eq. (13), we follow the same strategy as in the proof of Lemma A.1 by restricting the learner\u2019s prediction $\\hat{p}_{s}$ to $\\Delta^{\\delta}(\\mathcal{Y})$ for any threshold $\\delta\\,\\in\\,(0,1/2)$ which yields ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{x}(\\mathcal{F},x_{1:t},y_{1:t})\\le\\left\\langle\\bigg\\langle\\operatorname*{sup}_{x_{s}}\\operatorname*{sup}_{\\mathbb{B}_{y_{s}\\sim p_{s}}}\\bigg\\rangle\\right\\rangle_{s=t+1}^{T}\\Big[\\displaystyle\\sum_{s=t+1}^{T}\\operatorname*{inf}_{\\hat{p}_{s}\\in\\Delta^{\\delta}(y)}\\mathbb{E}_{y_{s}\\sim p_{s}}[\\ell(\\hat{p}_{s},y_{s})]-\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{s=1}^{T}\\ell(f_{s},y_{s})\\Big]}\\\\ &{\\qquad\\qquad\\le\\left\\langle\\bigg\\langle\\operatorname*{sup}_{x_{s}}\\operatorname*{sup}_{\\bar{p}_{s}\\sim p_{s}}\\bigg\\rangle\\right\\rangle_{s=t+1}^{T}\\Big[\\displaystyle\\sum_{s=t+1}^{T}\\displaystyle\\operatorname*{inf}_{\\hat{p}_{s}\\sim p_{s}}[\\ell(\\hat{p}_{s},y_{s})]-\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{s=1}^{T}\\ell(f_{s},y_{s})\\Big]+|\\mathcal{Y}|\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "So Eq. (14) is proved by sending $\\delta\\,\\rightarrow\\,0^{+}$ on the RHS of the last inequality and the established Eq. (12). ", "page_idx": 22}, {"type": "text", "text": "Lemma B.4 For any hypothesis class $\\mathcal{F}$ and sequences $x_{1:t}\\in\\mathcal{X}^{t},y_{1:t}\\in\\mathcal{Y}^{t}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{x},\\mathbf{p}}\\mathbb{E}_{\\mathbf{y}\\sim\\mathbf{p}}\\Big[\\sum_{s=t+1}^{T}\\mathbb{E}_{y_{s}\\sim\\mathbf{p}_{s}(\\mathbf{y})}[\\ell(\\mathbf{p}_{s}(\\mathbf{y}),y_{s})]-\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{s=1}^{T}\\ell(f_{s},y_{s})\\Big]=\\operatorname*{sup}_{\\mathbf{x}}\\log S_{T}^{x_{1:t},y_{1:t}}(\\mathcal{F}|\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma B.4 The proof follows that of Lemma A.3. By replacing the probabilistic tree $\\mathbf{p}$ by the joint distribution $P\\in\\Delta(\\mathcal{Y}^{T-t})$ , we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{\\mathbf{x},\\mathbf{p}}\\mathbb{E}_{{\\mathbf{y}}\\sim{\\mathbf{p}}}\\Big[\\displaystyle\\sum_{s=t+1}^{T}\\mathbb{E}_{y_{s}\\sim{\\mathbf{p}}_{s}(\\mathbf{y})}[\\ell({\\mathbf{p}}_{s}(\\mathbf{y}),y_{s})]-\\displaystyle\\operatorname*{inf}_{f\\in{\\mathcal{F}}}\\sum_{s=1}^{T}\\ell(f_{s},y_{s})\\Big]}\\\\ &{\\displaystyle=\\operatorname*{sup}_{\\mathbf{x},{\\mathbf{P}}}\\mathbb{E}_{{\\mathbf{y}}\\sim P}\\Big[\\displaystyle\\sum_{s=t+1}^{T}\\ell(P_{s},y_{s})-\\displaystyle\\operatorname*{inf}_{f\\in{\\mathcal{F}}}\\sum_{s=1}^{T}\\ell(f_{s},y_{s})\\Big]}\\\\ &{\\displaystyle=\\operatorname*{sup}_{\\mathbf{x}}\\quad\\operatorname*{sup}_{P\\in\\Delta({\\mathcal{Y}}^{T}-t)}H(P)+\\mathbb{E}_{{\\mathbf{y}}\\sim P}\\Big[\\displaystyle\\operatorname*{sup}_{f\\in{\\mathcal{F}}}\\log P_{f}(y_{1:t},{\\mathbf{y}}|x_{1:t},{\\mathbf{x}}(\\mathbf{y}))\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly, for any fixed $\\mathbf{x}$ , define the map $F_{\\mathbf{x}}^{x_{1:t},y_{1:t}}:\\mathcal{Y}^{T-t}\\rightarrow\\mathbb{R}\\cup\\{-\\infty\\}$ by ", "page_idx": 22}, {"type": "equation", "text": "$$\nF_{\\mathbf{x}}^{x_{1:t},y_{1:t}}(\\mathbf{y})=\\operatorname*{sup}_{f\\in\\mathcal{F}}\\log P_{f}(y_{1:t},\\mathbf{y}|x_{1:t},\\mathbf{x}(\\mathbf{y})),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and now we solve ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{P\\in\\Delta(\\mathcal{Y}^{T-t})}H(P)+\\mathbb{E}_{\\mathbf{y}\\sim P}[F_{\\mathbf{x}}^{x_{1:t},y_{1:t}}(\\mathbf{y})].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "If there exists some $\\mathbf{y}\\in\\b{\\mathcal{V}}^{T-t}$ such that $F_{\\mathbf{x}}^{x_{1:t},y_{1:t}}(\\mathbf{y})>-\\infty$ , then the optimal $P^{*}$ is given by ", "page_idx": 23}, {"type": "equation", "text": "$$\nP^{*}(\\mathbf{y})=\\frac{\\exp(F_{\\mathbf{x}}^{x_{1:t},y_{1:t}}(\\mathbf{y}))}{\\sum_{\\mathbf{y^{\\prime}}}\\exp(F_{\\mathbf{x}}^{x_{1:t},y_{1:t}}(\\mathbf{y^{\\prime}}))}=\\frac{\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(y_{1:t},\\mathbf{y}|x_{1:t},\\mathbf{x}(\\mathbf{y}))}{\\sum_{\\mathbf{y^{\\prime}}}\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(y_{1:t},\\mathbf{y^{\\prime}}|x_{1:t},\\mathbf{x}(\\mathbf{y^{\\prime}}))},\\forall\\mathbf{y}\\in\\mathcal{Y}^{T-t},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{\\mathbf{x},\\mathbf{p}}\\mathbb{E}_{\\mathbf{y}\\sim\\mathbf{p}}\\Big[\\displaystyle\\sum_{s=t+1}^{T}\\mathbb{E}_{\\boldsymbol{y}_{s}\\sim\\mathbf{p}_{s}(\\mathbf{y})}[\\ell(\\mathbf{p}_{s}(\\mathbf{y}),\\boldsymbol{y}_{s})]-\\displaystyle\\operatorname*{inf}_{f\\in\\mathcal{F}}\\sum_{s=1}^{T}\\ell(f_{s},\\boldsymbol{y}_{s})\\Big]}\\\\ &{\\displaystyle=\\operatorname*{sup}_{\\mathbf{x}}\\operatorname*{sup}_{P\\in\\Delta(\\mathcal{Y}^{T-t})}H(P)+\\mathbb{E}_{\\mathbf{y}\\sim P}[F_{\\mathbf{x}}^{x_{1:t},y_{1:t}}(\\mathbf{y})]}\\\\ &{\\displaystyle=\\operatorname*{sup}_{\\mathbf{x}}\\log\\Big(\\displaystyle\\sum_{\\mathbf{y}}\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(y_{1:t},\\mathbf{y}|x_{1:t},\\mathbf{x}(\\mathbf{y}))\\Big)}\\\\ &{\\displaystyle=\\operatorname*{sup}_{\\mathbf{x}}\\log S_{T}^{x_{1:t},y_{1:t}}(\\mathcal{F}|\\mathbf{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "However, if $F_{\\mathbf{x}}^{x_{1:t},y_{1:t}}(\\mathbf{y})\\,=\\,-\\infty$ for all $\\mathbf{y}$ , then it implies that for any context tree $\\mathbf{x}$ , path $\\mathbf{y}$ , and $f\\in\\mathcal F$ , $P_{f}(y_{1:t},\\mathbf{y}|x_{1:t},\\mathbf{x}(\\mathbf{y}))=0$ and hence, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{{\\mathbf{x}},{\\mathbf{p}}}{\\operatorname*{sup}}\\mathbb{E}_{{\\mathbf{y}}\\sim{\\mathbf{p}}}\\Big[\\displaystyle\\sum_{s=t+1}^{T}\\mathbb{E}_{y_{s}\\sim{\\mathbf{p}}_{s}(\\mathbf{y})}[\\ell({\\mathbf{p}}_{s}(\\mathbf{y}),y_{s})]-\\operatorname*{inf}_{f\\in{\\mathcal{F}}}\\displaystyle\\sum_{s=1}^{T}\\ell(f_{s},y_{s})\\Big]}\\\\ &{=\\underset{{\\mathbf{x}}}{\\operatorname*{sup}}\\quad\\underset{P\\in\\Delta({\\mathcal{Y}}^{T-t})}{\\operatorname*{sup}}H(P)+\\mathbb{E}_{{\\mathbf{y}}\\sim P}[F_{{\\mathbf{x}}^{1+t}}^{x_{1},y_{1+t}}(\\mathbf{y})]}\\\\ &{=-\\infty}\\\\ &{=\\underset{{\\mathbf{x}}}{\\operatorname*{sup}}\\log\\Big(\\displaystyle\\sum_{{\\mathbf{y}}}\\underset{f\\in{\\mathcal{F}}}{\\operatorname*{sup}}P_{f}(y_{1:t},\\mathbf{y}|x_{1:t},\\mathbf{x}(\\mathbf{y}))\\Big)}\\\\ &{=\\underset{{\\mathbf{x}}}{\\operatorname*{sup}}\\log S_{T}^{x_{1},t_{1},y_{1+t}}({\\mathcal{F}}|\\mathbf{x}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which finishes our proof. ", "page_idx": 23}, {"type": "text", "text": "Now we are able to prove the key result Lemma B.1. ", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma B.1 Fix any hypothesis class $\\mathcal{F}$ and sequences $x_{1:t}\\,\\in\\,\\mathcal{X}^{t},y_{1:t}\\,\\in\\,\\mathcal{Y}^{t}$ . First we know ", "page_idx": 23}, {"type": "equation", "text": "$$\nG(\\mathcal{F},x_{1:t},y_{1:t})\\ge\\operatorname*{sup}_{\\mathbf{x}}\\log\\log S_{T}^{x_{1:t},y_{1:t}}(\\mathcal{F}|\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "due to Eq. (12) and Lemma B.4. For the other direction, let us fix any threshold value $\\delta\\in(0,1/2)$ and then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma(\\mathcal{F},x_{1:t},y_{1:t})\\leq G(\\mathcal{F}^{\\delta},x_{1:t},y_{1:t})+T\\cdot\\log(1+|\\mathcal{Y}|\\delta)}\\\\ &{\\qquad\\qquad\\qquad=\\operatorname*{sup}_{x}S_{T}^{x_{1:t},y_{1:t}}(\\mathcal{F}^{\\delta}|\\mathbf{x})+T\\cdot\\log(1+|\\mathcal{Y}|\\delta)}\\\\ &{\\qquad\\qquad\\qquad=\\operatorname*{sup}_{x}\\log\\biggl(\\sum_{y\\in\\mathcal{Y}^{T}-t}\\operatorname*{sup}_{y}P_{f^{\\delta}}(y_{1:t},\\mathbf{y}|x_{1:t},\\mathbf{x}(\\mathbf{y}))\\biggr)+T\\cdot\\log(1+|\\mathcal{Y}|\\delta)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\operatorname*{sup}_{x}\\log\\biggl(\\underbrace{\\sum_{y\\in\\mathcal{Y}^{T}-t}f^{\\delta}\\in\\mathcal{F}^{\\delta}}_{\\mathbf{y}\\in\\mathcal{Y}^{T}-t}f(y_{1:t},\\mathbf{y}|x_{1:t},\\mathbf{x}(\\mathbf{y}))+\\delta\\cdot M(T)\\cdot|\\mathcal{Y}|^{T}\\biggr)+T\\cdot\\log(1+|\\mathcal{Y}|\\delta)}\\\\ &{\\qquad\\qquad\\qquad=\\log\\biggl(\\operatorname*{sup}_{x\\in\\mathcal{Y}^{T}-t}\\operatorname*{sup}_{y}P_{f}(y_{1:t},\\mathbf{y}|x_{1:t},\\mathbf{x}(\\mathbf{y}))+\\delta\\cdot M(T)\\cdot|\\mathcal{Y}|^{T}\\biggr)+T\\cdot\\log(1+|\\mathcal{Y}|\\delta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we have applied Lemma A.7, Lemma B.3, Lemma B.4, and Lemma A.6 accordingly. Similarly, we send $\\delta\\rightarrow0^{+}$ on the RHS of the last inequality and get ", "page_idx": 23}, {"type": "equation", "text": "$$\nG(\\mathcal{F},x_{1:t},y_{1:t})\\leq\\operatorname*{sup}_{\\mathbf{x}}\\log\\Bigl(\\sum_{\\mathbf{y}\\in\\mathcal{Y}^{T-t}}\\operatorname*{sup}_{f\\in\\mathcal{F}}P_{f}(y_{1:t},\\mathbf{y}|x_{1:t},\\mathbf{x}(\\mathbf{y}))\\Bigr)=\\operatorname*{sup}_{\\mathbf{x}}\\log S_{T}^{x_{1:t},y_{1:t}}(\\mathcal{F}|\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 23}, {"type": "text", "text": "C Additional discussions ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "C.1 On the time-variant context space ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section we generalize our analysis to the setting where the context space can evolve over time. We model time-varying context sets by a sequence of maps $\\mathcal{X}_{t}:\\mathcal{X}^{t-1}\\times\\dot{\\mathcal{Y}}^{t-1}\\rightarrow2^{\\mathcal{X}},t\\in[T]$ as in [RS15; BFR20]. In each round $t$ , instead of picking any context from $\\mathcal{X}$ , the nature is now required to only choose $x_{t}$ from $\\mathcal{X}_{t}(x_{1:t-1},y_{1:t-1})\\subseteq\\mathcal{X}$ . Then the minimax regret with respect to $(\\mathcal{X}_{t})_{t\\in[T]}$ is rewritten as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F})=\\bigg\\langle\\!\\!\\bigg\\langle\\operatorname*{sup}_{x_{t}\\in\\mathcal{X}_{t}(x_{1:t-1},y_{1:t-1})}\\operatorname*{inf}_{\\hat{p}_{t}}\\operatorname*{sup}_{y_{t}}\\!\\bigg\\rangle\\!\\bigg\\rangle_{t=1}^{T}\\mathcal{R}_{T}(\\mathcal{F};\\hat{p}_{1:T},x_{1:T},y_{1:T}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "A context tree $\\mathbf{x}$ is consistent with respect to $({\\mathcal{X}}_{t})_{t\\in[T]}$ if for all $t\\,\\in\\,[T]$ and $\\mathbf{y}\\,\\in\\,\\mathcal{Y}^{T},\\,\\mathbf{x}_{t}(\\mathbf{y})\\,\\in$ $\\mathcal{X}_{t}(x_{1:t-1},y_{1:t-1})$ . Then our results in Section 3 and Section 4 can be generalized simply by replacing the supremum over all context trees (of depth $T$ ) by the supremum over all consistent context trees. For example, we will have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}(\\mathcal{F})=\\operatorname*{sup}_{\\substack{\\mathbf{x}:\\mathbf{x}\\,\\mathrm{is\\,consistent}}}\\log S_{T}(\\mathcal{F}|\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "C.2 On the global and non-global sequential cover ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Now we go back to consider the usual setting of binary label and constant experts, i.e., ${\\mathcal{D}}=\\{0,1\\}$ and $\\mathcal{F}\\subseteq[0,1]^{\\mathcal{X}}$ . As mentioned in Section 3, previous works [BFR20; WHGS23] provided regret upper bounds based on $\\ell_{\\infty}$ sequential entropy. More specifically, both of their bounds are in the form of $O(\\operatorname*{inf}_{\\alpha>0}\\{\\alpha T+\\mathcal{H}(\\mathcal{F},\\alpha,T)\\})$ , with $\\mathcal{H}(\\mathcal{F},\\alpha,\\bar{T})$ being either the non-global entropy $\\mathcal{H}_{\\infty}(\\mathcal{F},\\alpha,T)$ or the global entropy $\\mathcal{H}_{G}(\\mathcal{F},\\alpha,T)$ . It is then natural to ask which one of these two bounds is tighter. It is straightforward to prove that $\\mathcal{H}_{\\infty}(\\mathcal{F},\\alpha,T)$ is no larger than $\\mathcal{H}_{G}(\\mathcal{F},\\alpha,T)$ . In fact, the gap between them is at most a polylog factor, as we state and prove below.1 The proof of $\\mathcal{H}_{\\infty}(\\mathcal{F},\\bar{\\alpha},\\bar{T})\\leq\\mathcal{H}_{G}(\\mathcal{F},\\alpha,T)$ is also included for completeness. Before stating the results, we introduce the definition of sequential fat-shattering dimension. ", "page_idx": 24}, {"type": "text", "text": "Definition C.1 We say an $\\mathcal{X}$ -valued binary tree $\\mathbf{x}$ of depth $d$ is $\\alpha$ -shattered by a class $\\mathcal{F}\\subseteq[0,1]^{\\mathcal{X}}$ for some $\\alpha>0$ , if there exists a $[0,1]$ -valued binary tree s of depth $d$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\forall\\mathbf{y}\\in\\{0,1\\}^{d},\\exists f\\in\\mathcal{F},\\mathrm{~s.t.~}(2y_{t}-1)\\cdot\\big(f(\\mathbf{x}_{t}(\\mathbf{y}))-\\mathbf{s}_{t}(\\mathbf{y})\\big)\\geq\\frac{\\alpha}{2},\\forall t\\in[d].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In this case, s is called the witness of the shattering. The sequential fat-shattering dimension of $\\mathcal{F}$ at scale $\\alpha$ , denoted by $\\mathrm{sfat}_{\\alpha}(\\mathcal{F})$ , is the largest $d$ such that some depth- $d$ context tree is $\\alpha$ -shattered by $\\mathcal{F}$ . ", "page_idx": 24}, {"type": "text", "text": "Proposition C.2 For any scale $\\alpha>0$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{H}_{\\infty}(\\mathcal{F},\\alpha,T)\\geq\\operatorname*{min}\\{T,\\,\\operatorname*{sup}_{\\alpha^{\\prime}>\\alpha}\\mathrm{sfat}_{2\\alpha^{\\prime}}(\\mathcal{F})\\}\\cdot\\log(2).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, together with $\\begin{array}{r l r}{\\mathcal{H}_{\\infty}(\\mathcal{F},\\alpha,T)}&{\\leq}&{\\mathcal{H}_{G}(\\mathcal{F},\\alpha,T)}\\end{array}$ and the folklore $\\begin{array}{r l}{\\mathcal{H}_{G}(\\mathcal{F},\\alpha,T)}&{{}\\leq}\\end{array}$ $O(\\mathrm{sfat}_{\\alpha}(\\mathcal{F})\\log(T/\\alpha))$ , we conclude that the regret upper bounds $O(\\operatorname{inf}_{\\alpha>0}\\{\\alpha T\\ \\ +$ $\\mathcal{H}(\\mathcal{F},\\alpha,T)\\}),\\mathcal{H}\\in\\{\\mathcal{H}_{\\infty},\\mathcal{H}_{G}\\}$ , differ by at most a polylog factor. ", "page_idx": 24}, {"type": "text", "text": "Proof of Proposition C.2 Fix any $\\alpha^{\\prime}>\\alpha>0$ and let $d_{\\alpha^{\\prime}}$ denote $\\operatorname*{min}\\{T,\\mathrm{sfat}_{2\\alpha^{\\prime}}(\\mathcal{F})\\}$ . Then there exists a context tree $\\mathbf{x}$ and a witness tree s, both of depth $d_{\\alpha^{\\prime}}$ , such that for any path $\\dot{\\mathbf{y}}\\in\\lbrace0,1\\rbrace^{d_{\\alpha^{\\prime}}}$ , there exists an $f\\in\\mathcal F$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\forall t\\in[d_{\\alpha^{\\prime}}],(2y_{t}-1)\\cdot(f(x_{t}(\\mathbf{y}))-s_{t}(\\mathbf{y}))\\geq\\alpha^{\\prime}>\\alpha.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $V_{\\mathbf{x},\\alpha}$ be an arbitrary sequential $\\ell_{\\infty}$ covering of $\\mathcal{F}$ on $\\mathbf{x}$ . Now we select a path $\\mathbf{y}$ and a sequence of subsets $V_{\\mathbf{x},\\alpha}^{(t)}\\ \\subseteq\\ V_{\\mathbf{x},\\alpha},t\\ \\in\\ [d_{\\alpha^{\\prime}}]$ in the following recursive way. Define $V_{\\mathbf{x},\\alpha}^{(0)}\\ =\\ V_{\\mathbf{x},\\alpha}$ . For each $t~\\in~[d_{\\alpha^{\\prime}}]$ , choose $y_{t}~\\in~\\{0,1\\}$ such that $2y_{t}\\bar{\\mathbf{\\alpha}}-1\\;\\in\\;\\{-1,\\dot{+}1\\}$ is the minority among all $\\mathrm{sgn}(v_{t}(y_{1:t-1})\\,-\\,s_{t}(y_{1:t-1})),v\\,\\in\\,V_{{\\bf x},\\alpha}^{(t-1)}$ (ignoring those of 0\u2019s). Finally update $V_{\\mathbf{x},\\alpha}^{(t)}\\,=\\,\\{v\\,\\in$ V x(,t\u03b1\u22121): sgn(vt(y1:t\u22121) \u2212st(y1:t\u22121)) = 2yt \u22121}. ", "page_idx": 25}, {"type": "text", "text": "First we argue that, if there is any time $t^{\\prime}\\,\\in\\,[d_{\\alpha^{\\prime}}]$ such that $V_{\\mathbf{x},\\alpha}^{(t^{\\prime}-1)}\\,\\neq\\,\\emptyset,V_{\\mathbf{x},\\alpha}^{(t^{\\prime})}\\,=\\,\\emptyset$ , then $V_{\\mathbf{x},\\alpha}$ is not a valid cover of $\\mathcal{F}$ on $\\mathbf{x}$ . Otherwise, recall we have selected $y_{1},\\ldots,y_{t^{\\prime}-1}$ . Now pick an arbitrary $y_{t^{\\prime}}\\in\\{0,1\\}$ . By Eq. (15) we can find some $f\\in\\mathcal F$ such that $\\left(2y_{t}-1\\right)\\cdot\\left(f(x_{t}(y_{1:t-1})\\right)-$ $s_{t}(y_{1:t-1}))>\\partial$ , $\\forall t\\in[t^{\\prime}]$ . Since $V_{\\mathbf{x},\\alpha}$ is a covering at scale $\\alpha$ , there is $v\\in V_{\\mathbf{x},\\alpha}$ such that $\\lvert v_{t}(\\mathbf{y})-$ $f(x_{t}(\\mathbf{y}))|\\;\\leq\\;\\alpha,\\forall t\\;\\in\\;[t^{\\prime}]$ . This implies that $\\mathrm{sgn}(f(x_{t}(\\mathbf{y}))\\,-\\,s_{t}(\\mathbf{y}))\\,=\\,\\mathrm{sgn}(v_{t}(\\mathbf{y})\\,-\\,s_{t}(\\mathbf{y}))\\,=$ $2y_{t}-1,\\forall t\\in[t^{\\prime}]$ . So we can always find some member of $V_{\\mathbf{x},\\alpha}^{(t^{\\prime}-1)}$ to match the minority sign of $v_{t^{\\prime}}\\bigl(y_{1:t^{\\prime}-1}\\bigr)-s_{t^{\\prime}}\\bigl(y_{1:t^{\\prime}-1}\\bigr),v\\in V_{\\mathbf{x},\\alpha}^{(t^{\\prime}-1)}$ , which means that $V_{\\mathbf{x},\\alpha}^{(t^{\\prime})}\\neq\\emptyset$ and yields a contradiction. ", "page_idx": 25}, {"type": "text", "text": "Now we know that $|V_{\\mathbf{x},\\alpha}^{(t)}|\\;\\ge\\;1,\\forall t\\;\\in\\;[d_{\\alpha^{\\prime}}]$ . By design $|V_{\\mathbf{x},\\alpha}^{(t)}|\\ \\leq\\ |V_{\\mathbf{x},\\alpha}^{(t-1)}|/2,\\forall t\\ \\in\\ [d_{\\alpha^{\\prime}}]$ , so we must have $|V_{\\mathbf{x},\\alpha}|\\,=\\,|V_{\\mathbf{x},\\alpha}^{(0)}|\\,\\geq\\,2^{d_{\\alpha^{\\prime}}}$ . As the choice of covering is arbitrary, the covering number $\\mathcal{N}_{\\infty}(\\mathcal{F}\\circ{\\bf x},\\alpha,d_{\\alpha^{\\prime}})$ is also lower bounded by $2^{d_{\\alpha^{\\prime}}}$ and hence $\\mathcal{H}_{\\infty}(\\mathcal{F},\\alpha,d_{\\alpha^{\\prime}})\\,\\geq\\,d_{\\alpha^{\\prime}}\\cdot\\log(2)$ . If $\\operatorname*{sup}_{\\alpha^{\\prime}>\\alpha}\\operatorname{sfat}_{2\\alpha^{\\prime}}(\\mathcal{F})\\leq T$ , then we get that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{H}_{\\infty}(\\mathcal{F},\\alpha,T)\\geq\\operatorname*{sup}_{\\alpha^{\\prime}>\\alpha}\\mathcal{H}_{\\infty}(\\mathcal{F},\\alpha,\\mathrm{sfat}_{2\\alpha^{\\prime}}(\\mathcal{F}))\\geq\\operatorname*{sup}_{\\alpha^{\\prime}>\\alpha}\\mathrm{sfat}_{2\\alpha^{\\prime}}(\\mathcal{F})\\cdot\\log(2).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "If there is some $\\alpha^{\\prime}>\\alpha$ such that $\\mathrm{sfat}_{2\\alpha^{\\prime}}({\\mathcal{F}})\\geq T$ , then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{H}_{\\infty}(\\mathcal{F},\\alpha,T)=\\mathcal{H}_{\\infty}(\\mathcal{F},\\alpha,d_{\\alpha^{\\prime}})\\geq T\\cdot\\log(2).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining these two cases together, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{H}_{\\infty}(\\mathcal{F},\\alpha,T)\\geq\\operatorname*{min}\\{T,\\,\\operatorname*{sup}_{\\alpha^{\\prime}>\\alpha}\\mathrm{sfat}_{2\\alpha^{\\prime}}(\\mathcal{F})\\}\\cdot\\log(2).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proposition C.3 Let $\\mathcal{G}_{\\alpha}$ be a global sequential $\\alpha$ -covering of $\\mathcal{F}$ as defined in [WHGS23]. Then for any context tree x, there exists a sequential cover $V_{\\mathbf{x},\\alpha}$ of $\\mathcal{F}\\circ\\mathbf{x}$ at scale $\\alpha$ with $|V_{\\mathbf{x},\\alpha}|\\leq|\\mathcal{G}_{\\alpha}|$ . This implies that $\\begin{array}{r}{\\mathcal{H}_{\\infty}(\\mathcal{F},\\alpha,T)\\leq\\log|\\mathcal{G}_{\\alpha}|}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "Proof of Proposition C.3 Fix arbitrary context tree $\\mathbf{x}$ . For any $g\\in\\mathcal G_{\\alpha}$ , define the $[0,1]$ -valued tree $v^{g}$ by $v_{t}^{g}(\\mathbf{y})\\stackrel{-}{=}g(x_{1:t}(\\mathbf{y})),\\forall t\\in[T],\\mathbf{y}\\stackrel{\\cdot}{\\in}y^{T}$ . Now let $V_{\\mathbf{x},\\alpha}=\\{v^{g}:g\\in\\mathcal{G}_{\\alpha}\\}$ and we will show that $V_{\\mathbf{x},\\alpha}$ is indeed a sequential cover of $\\mathcal{F}\\circ\\mathbf{x}$ at scale $\\alpha$ . ", "page_idx": 25}, {"type": "text", "text": "For any $f\\in\\mathcal{F}$ and $\\textbf{y}\\in\\b{\\mathscr{V}}^{T}$ , tree $\\mathbf{x}$ yields a length\u2212 $T$ sequence $x_{1:T}(\\mathbf{y})$ and by definition of the global sequential covering, there exists $g\\in\\mathcal G_{\\alpha}$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n|f(x_{t}(\\mathbf{y}))-g(x_{1:t}(\\mathbf{y}))|\\leq\\alpha,\\forall t\\in[T].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "So by our construction of $V_{\\mathbf{x},\\alpha}$ , $v^{g}\\in V_{\\mathbf{x},\\alpha}$ holds ", "page_idx": 25}, {"type": "equation", "text": "$$\n|f(x_{t}(\\mathbf{y}))-v_{t}^{g}(\\mathbf{y})|=|f(x_{t}(\\mathbf{y}))-g(x_{1:t}(\\mathbf{y}))|\\leq\\alpha,\\forall t\\in[T],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which yields our claim after observing $|V_{\\mathbf{x},\\alpha}|\\leq|\\mathcal{G}_{\\alpha}|$ . ", "page_idx": 25}, {"type": "text", "text": "D Additional proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Lemma D.1 For any $\\mathcal{X}$ -valued $\\boldsymbol{\\wp}$ -ary context tree x of depth $T$ , and $f:(\\mathcal{X}\\times\\mathcal{Y})^{*}\\times\\mathcal{X}\\to\\Delta(\\mathcal{Y})$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{y}\\in\\mathcal{Y}^{T}}P_{f}(\\mathbf{y}|\\mathbf{x}(\\mathbf{y}))=1,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we recall that $\\mathbf{x}(\\mathbf{y})$ denotes the context sequence $(\\mathbf{x}_{1}(\\mathbf{y}),\\dots,\\mathbf{x}_{T}(\\mathbf{y}))$ . ", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma D.1 This is done by induction on the depth $T$ . The key observation is that for any label sequence y, $\\mathbf{x}_{t}(\\mathbf{y})=\\mathbf{x}_{t}(y_{1},\\ldots,y_{t-1})$ only depends on the first $t-1$ labels. For $T=1$ , any context tree $\\mathbf{x}$ is represented by its root node $\\mathbf{x}_{1}(\\cdot)=\\mathbf{x}_{1}\\in\\mathcal{X}$ and hence ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{y_{1}}P_{f}(y_{1}|\\mathbf{x}_{1})=\\sum_{y_{1}}f(\\mathbf{x}_{1})(y_{1})=1.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Suppose Eq. (16) holds for all context trees $\\mathbf{x}$ of depth $T\\leq d$ and all sequential functions $f$ . Now given any context tree $\\mathbf{x}\\,=\\,(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{d+1})$ of depth $T\\,=\\,d+1$ , we denote its depth $d$ subtree $(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{d})$ by $\\mathbf{x}_{[d]}$ . Then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{y\\in\\mathcal{Y}^{d+1}}P_{f}(y|\\mathbf{x}(\\mathbf{y}))=\\displaystyle\\sum_{y_{1:d}}P_{f}(y_{1:d+1}|\\mathbf{x}_{1},\\mathbf{x}_{2}(y_{1}),\\ldots,\\mathbf{x}_{d+1}(y_{1:d}))}&{}\\\\ {=\\displaystyle\\sum_{y_{1:d}}P_{f}(y_{1:d}|\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{d}(y_{1:d-1}))\\cdot f(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{d+1}(y_{1:d}),y_{1:d})(y_{d+1})}&{}\\\\ {=\\displaystyle\\sum_{y_{1:d}}P_{f}(y_{1:d}|\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{d}(y_{1:d-1}))\\displaystyle\\sum_{y_{d+1}}f(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{d+1}(y_{1:d}),y_{1:d})(y_{d+1})}&{}\\\\ {=\\displaystyle\\sum_{y_{1:d}}P_{f}(y_{1:d}|\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{d}(y_{1:d-1}))}&{}\\\\ {=\\displaystyle\\sum_{y_{1:d}}P_{f}(y_{1:d}|\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{d}(y_{1:d-1}))}&{}\\\\ {=\\displaystyle\\sum_{x\\in\\mathcal{Y}^{d}}P_{f}(\\mathbf{y}|\\mathbf{x}_{1:d}|\\mathbf{y})=1,}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last step is due to induction. We are done. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 27}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 27}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA]   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 27}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 27}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 27}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 27}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Abstract summarizes theorems we have proven. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We discuss limitations in the Discussion section. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We don\u2019t see how to justify this without machine checkable proofs, which we have not provided. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: There are no experiments. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: There is no data or code. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 29}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: There are no experiments. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: There are no experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: There are no experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have read the code and do not see any violation. Our work relates to the mathematical foundations of a basic task in ML. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper presents a mathematical characterization of the limits of probabilistic forecasting, and a (meta)algorithm that achieves these limits. For any class of interest, there remains significant work to realize that algorithm in an efficient way. As such, our impact is most directly on the theoretical community, who might then have direct societal impact by producing an minimax optimal algorithm. As such, our societal impact may be great, but it will always be quite indirect. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We are not releasing models or data. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We use no such assets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: No new assets are introduced. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. ", "page_idx": 32}, {"type": "text", "text": "\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: No such experiments were performed. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]