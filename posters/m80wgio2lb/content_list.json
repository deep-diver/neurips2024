[{"type": "text", "text": "Scaling Sign Language Translation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Biao Zhang Garrett Tanzer Orhan Firat Google DeepMind {biaojiaxing,gtanzer,orhanf}@google.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sign language translation (SLT) addresses the problem of translating information from a sign language in video to a spoken language in text. Existing studies, while showing progress, are often limited to narrow domains and/or few sign languages and struggle with open-domain tasks. In this paper, we push forward the frontier of SLT by scaling pretraining data, model size, and number of translation directions. We perform large-scale SLT pretraining on different data including 1) noisy multilingual YouTube SLT data, 2) parallel text corpora, and 3) SLT data augmented by translating video captions to other languages with off-the-shelf machine translation models. We unify different pretraining tasks with task-specific prompts under the encoder-decoder architecture, and initialize the SLT model with pretrained (m/By)T5 models across model sizes. SLT pretraining results on How2Sign and FLEURS-ASL#0 (ASL to 42 spoken languages) demonstrate the significance of data/model scaling and cross-lingual cross-modal transfer, as well as the feasibility of zero-shot SLT. We finetune the pretrained SLT models on 5 downstream open-domain SLT benchmarks covering 5 sign languages. Experiments show substantial quality improvements over the vanilla baselines, surpassing the previous state-of-the-art (SOTA) by wide margins. ", "page_idx": 0}, {"type": "image", "img_path": "M80WgiO2Lb/tmp/5602a605c8d3b2dd2885a1d2c20eed4b281b5a5d63b5217fc899d3e61865a063.jpg", "img_caption": ["Figure 1: BLEU scores on different benchmarks: our model sets new SOTA results across benchmarks and sign languages. Note we didn\u2019t show BLEURT because not all previous studies report BLEURT. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Scalable neural networks trained on large amount of unlabeled and/or weakly-labeled data from multiple modalities and multiple tasks have resulted in performance significantly exceeding that of single-task models trained on particular domains [30, 11, 33, 18]. Sign language translation (SLT), (b) Clip overview. Top: a sequence of skeletons for a sign language video where the used keypoints are annotated in red; Bottom: We pretrain SLT models on randomly sampled clips of $N$ seconds from the video. Each segment in the plot represents a caption, and $[\\mathsf{C a p}_{i},\\ldots,\\mathsf{C a p}_{j}]$ (i.e., green segments) denotes captions fully covered by the clip. \u201c $\"S_{i}/{E_{i}}\"$ : the start/end time stamp for caption $\\mathbf{Cap}_{i}$ . ", "page_idx": 0}, {"type": "image", "img_path": "M80WgiO2Lb/tmp/1dca2fd32e29287a1e2a1eac065707ea79bfa0e4556fcf29d63b43bd30b23134.jpg", "img_caption": ["(a) Encoder-decoder based SLT model and different SLT pretraining tasks. We use red, green, and blue colors to indicate the input prompt, sign frames, and target output, respectively. \u201csign lang\u201d: sign language name; \u201csrc lang/tgt lang\u201d: source/target spoken language name; $\"<\\ast>\"$ : task-specific control tokens; \u201csource/target text\u201d: source/target text for MT; \u201cclip frames (clip text)\u201d: concatenation of sign frames (caption texts) corresponding to a video clip; \u201ctranslated clip text\u201d: augmented data by off-theshelf MT models; \u201cclip text with timestamps\u201d: concatenation of caption texts and their start and end timestamps. "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "M80WgiO2Lb/tmp/6f23e47df5acd5cf54671991e96015d36f7a31de8c7575c87f261b4ad4110241.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Figure 2: Illustration of model architecture and pretraining task for SLT. We perform large-scale pretraining and adopt multi-task learning at clip level (multiple captions) to better leverage the supervised knowledge. ", "page_idx": 1}, {"type": "text", "text": "as a video-to-text translation $\\mathrm{{task}^{1}}$ , features significant cross-modality challenges in video understanding and text generation. While extra forms of supervision such as glosses have been helpful in bridging the modality gap [4], they are nonstandardized/incomplete systems available only for small datasets [12]. Researchers have instead turned to more scalable approaches such as adapting pretrained vision and text models [7, 37, 50] and jointly modeling with machine translation [MT, 58]. Despite encouraging progress, these studies were performed at small scale with success on narrowed domains and on few sign languages. In open-domain SLT settings, unfortunately, they have shown limited effectiveness [28]. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we aim to improve open-domain SLT for multiple sign languages by means of largescale SLT pretraining with more data, larger models and more languages. Inspired by the finding that jointly training SLT models with MT data enables positive knowledge transfer to SLT [58], we explore the following pretraining tasks and data: web-crawled multilingual SLT, multilingual MT, and augmented SLT. Although high-quality SLT training data are scarce, weakly-labeled SLT data covering diverse topics and signers are readily available from platforms like YouTube. Prior studies have demonstrated the feasibility of collecting massive YouTube SLT data and its effectiveness on improving SLT [46, 44, 43], and we follow this effort in a multilingual setup. Different from SLT, text-based MT datasets are massive and resource-rich across hundreds of spoken languages [3, 10]. We explore a subset of MADLAD-400 [23] including up to 41 spoken languages for the pretraining. In addition, we construct synthetic multiway SLT data by translating video captions with an off-theshelf MT model, which allows us to strengthen direct SLT across more translation directions. We investigate different ways of mixing these data to exploit weakly supervised SLT knowledge as well as cross-task cross-modality transfer at scale. ", "page_idx": 1}, {"type": "text", "text": "As in Figure 2, we extend the unified encoder-decoder SLT framework from [46, 42, 44, 43] with extra tasks and modalities similar to [58], across different pretrained model families (T5, mT5 and ByT5) and different model sizes. We distinguish different tasks by carefully designed input prompts that contain task-specific control tokens. This affords us high flexibility in choosing what tasks and languages to incorporate into the pretraining, easing ablations and the scaling. We then finetune the pretrained SLT models on downstream SLT benchmarks to refine the learned SLT knowledge. ", "page_idx": 2}, {"type": "text", "text": "We evaluate the effect of scaling on 6 open-domain SLT benchmarks across 5 sign languages. FLEURS-ASL#0 [42], built on FLORES-200 [10], gives us a testbed to analyze multiway American Sign Language (ASL)-to-X SLT (we examine English and 41 other target languages), while the other benchmarks are for a single language pair. While pretraining results show the acquired general SLT capability, we also report finetuning results following [46]. Our main findings are below: ", "page_idx": 2}, {"type": "text", "text": "\u2022 Adding more pretraining data, either machine translation or sign language translation data, is a promising way to improve SLT, yielding quality gains of varying degrees.   \n\u2022 Zero-shot ASL-to-X translation for language pairs not seen during pretraining is achievable by jointly training on ASL-to-En SLT data and En-to-X MT data.   \n\u2022 Augmenting SLT data by translating target captions to other languages with off-the-shelf MT models substantially improves the translation.   \n\u2022 Using larger models is not always helpful: ByT5 Base (582M) often outperforms XL (3.74B), but model scaling does benefit SLT when modeling capacity becomes a bottleneck (e.g., when more languages and data are used).   \n\u2022 Learned metrics (e.g., BLEURT) show higher correlation between pretrained and finetuned SLT scores than classical metrics (e.g., BLEU or ChrF). ", "page_idx": 2}, {"type": "text", "text": "Putting everything together, our model achieves new state-of-the-art results across the benchmarks as shown in Figure 1, demonstrating the significance of scaling SLT. ", "page_idx": 2}, {"type": "text", "text": "2 Sign Language Translation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Modeling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We build on a line of work using T5 model families for SLT [46, 42, 44, 43], which build upon earlier SLT work [4, 61, 58] using the encoder-decoder architecture [41, 47]. Figure 2a shows the overall structure. The encoder takes as input the concatenation of a prompt instructing the task and a sequence of sign language video frames; the decoder predicts the text output in a target spoken language one token at a time. We adopt the family of pretrained $(\\mathrm{m}/\\mathrm{By})\\mathrm{T}5$ models [35, 53] as the backbone and adapt them to SLT via large-scale SLT pretraining followed by downstream finetuning, i.e. $(\\mathbf{m}/\\mathbf{B}\\mathbf{y})\\mathbf{T}\\bar{\\mathbf{s}}$ initialization $\\rightarrow\\mathbf{S}\\mathbf{L}\\mathbf{T}$ pretraining $\\rightarrow\\mathbf{S}\\mathbf{L}\\mathbf{T}$ finetuning. ", "page_idx": 2}, {"type": "text", "text": "We rely on web-crawled YouTube SLT data for SLT pretraining, which provide high coverage on domains and signers albeit at lower quality. Although recent debates value data quality over data quantity in pretraining [21, 24, 29], we argue that they were established on the availability of massive high-quality training data, which doesn\u2019t hold for SLT yet. We expect that the pretraining could capture the (weakly) supervised SLT knowledge from the crawled data as in previous studies [46]. ", "page_idx": 2}, {"type": "text", "text": "As shown in Figure 2b, we adopt the clip-level training following [42] that randomly samples a clip of $N$ seconds from the sign video and then predicts various types of in-clip information (such as caption texts and their start and end timestamps) based on the frames of the entire clip. Detailed tasks are listed in Figure 2a, which are all formulated as sequence-to-sequence tasks. They are distinguished by prompts with different control tokens and are trained with the standard maximum likelihood objective. For the baseline, we consider the following two tasks: SLT and alignment, and train it by mixing these two tasks with a pre-specified mix ratio. ", "page_idx": 2}, {"type": "text", "text": "SLT This is the core task that directly models the translation from clip frames to the clip text in a target language. It is indispensable for the model to acquire the translation capability. ", "page_idx": 2}, {"type": "text", "text": "Alignment It is an auxiliary task for SLT, learning to align the input clip with its captions. We train the model to infer the start and end time stamp for each in-clip caption. Apart from regularization, this task could improve the model\u2019s understanding of sign language [42]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Scaling Model Size, Number of Languages and Pretraining Data Size ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Model Scaling Scaling model size increases modeling capacity, which has been widely proven effective in improving the task performance [30, 18, 19]. We study whether and how increasing model size affects the SLT performance and compare (By/m)T5 models for SLT at different scales. ", "page_idx": 3}, {"type": "text", "text": "Language Scaling While most SLT works focus on a few sign and spoken languages, we expand our study to massive languages, covering up to 80 sign/spoken languages during pretraining, and 5 sign language and 42 spoken languages at evaluation. We are interested in whether a single SLT model could support multiple sign/spoken languages with non-trivial performance, and whether knowledge transfer could improve SLT on low-resource languages [57, 44]. ", "page_idx": 3}, {"type": "text", "text": "Data Scaling Data scarcity is the main bottleneck hindering the development of SLT. To address this issue, we investigate the following three types of data for the pretraining: ", "page_idx": 3}, {"type": "text", "text": "SLT We crawl multilingual YouTube SLT data following the recipe [44] except that we didn\u2019t perform human annotation and filtering. This allows us to significantly scale up the SLT data by $3{\\sim}6$ times, reaching ${\\sim}6\\mathrm{,}600$ hours in total, albeit at much lower quality. ", "page_idx": 3}, {"type": "text", "text": "Machine Translation Unlike SLT, MT is a text-to-text translation task with rich parallel resources, particularly for high-resource languages [2]. We explore adding multilingual MT data into the pretraining and mark this task with control token \u201c<mt>\u201d [58]. ", "page_idx": 3}, {"type": "text", "text": "Augmented SLT SLT data are often one-to-one translation data, where each sign language only has translation in one spoken language. This makes the translation of a sign language to other spoken languages difficult. We thus augment SLT data to one-to-many by translating the target text to other spoken languages via off-the-shelf MT models. As in Figure 2a, we use \u201c<aug>\u201d to separate genuine SLT data from the augmented one [6]. ", "page_idx": 3}, {"type": "text", "text": "3 Setup", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "MT Pretraining Data We use the parallel sentence-level portion of MADLAD-400 [23] as the MT pretraining data. We extract a subset of MADLAD-400 for experiments, including 41 languages (apart from English (En)) covering diverse language families and scripts, and explore the impact of $\\mathrm{En\\mathrm{{\\rightarrow}X\\mathrm{{x}}}}$ and $\\mathrm{Xx\\mathrm{\\rightarrow}E n}$ MT data on SLT in experiments. We create two settings for the pretraining: ", "page_idx": 3}, {"type": "text", "text": "\u2022 MT-Small: A high/medium-resource subset including 11 languages es, de, fr, it, cs, pl, ru, zh, ar, ja, hi.   \n\u2022 MT-Large: This set includes all 41 languages. Apart from MT-Small, it has nl, pt, sv, hu, da, fi, el, sk, no, bg, lt, lv, sl, et, ko, hr, is, sr, tr, vi, id, he, th, ms, uk, ca, ta, fil, ne, cy. ", "page_idx": 3}, {"type": "text", "text": "Table 7 shows the statistics for each language. Unless otherwise specified, we balance the MT data distribution over languages during training by temperature sampling with a rate of 5 [2]. ", "page_idx": 3}, {"type": "text", "text": "SLT Pretraining Data We experiment with noisy captioned sign language videos from YouTube. This is the full set of videos pre-manual filtering in [44]. Estimated statistics for each sign language are summarized in Table 7. We also have two settings for this data: ", "page_idx": 3}, {"type": "text", "text": "\u2022 YT-ASL: ${\\sim}2\\mathrm{,}800$ hours of noisy captioned ASL videos; a superset of YouTube-ASL [46] (modulo video churn) and the same dataset used by [43].   \n\u2022 YT-Full: ${\\sim}6\\mathrm{,}600$ hours of noisy captioned multilingual sign language videos; a superset of [44]. ", "page_idx": 3}, {"type": "text", "text": "During training, we mix the SLT data for all languages in proportion to their duration. We further augment these data with other spoken languages via MADLAD-MT-3B [23]. For ASL SLT data, we translate the English captions to 41 spoken languages listed in MT-Large, which makes YT-ASL 43-way multilingual SLT, namely Aug-YT-ASL; for other SLT data, we translate the target text into English, resulting in 3-way multilingual SLT.2We refer to the augmented SLT data for all sign languages as Aug-YT-Full. Similar to MT-Small and MT-Large, we reorganize the augmented data to Aug-YT-ASL-Small/Aug-YT-Full-Small and Aug-YT-ASL-Large/Aug-YT-Full-Large. ", "page_idx": 3}, {"type": "table", "img_path": "M80WgiO2Lb/tmp/1a13da0fb259c0ba73e3864d5da26549e869edf20543f14c2515dc4bcffd4b2f.jpg", "table_caption": [], "table_footnote": ["Table 1: Summary of downstream SLT benchmarks. \u201c#Train/#Dev/#Test\u201d: the number of examples in the train, dev and test split. Note the sign language video and the target text in these benchmarks are often pre-segmented and aligned at sentence level. \u201cDGS/ASL/GSS\u201d: German/American/Greek Sign Language; \u201cEn/De/Fr/It\u201d: English/German/French/Italian; \u201cLIS-CH\u201d: Italian Sign Language of Switzerland; \u201cLSF-CH\u201d: French Sign Language of Switzerland; \u201cDSGS\u201d: Swiss German Sign Language. "], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "SLT Pretraining Mixture We ablate across several SLT pretraining mixtures. ", "page_idx": 4}, {"type": "text", "text": "\u2022 Baseline: Caption alignment and SLT tasks. We use the task weights from [42], including $4\\%$ for alignment.   \n\u2022 Baseline $\\mathbf{\\Gamma}+\\mathbf{M}\\mathbf{T}$ : We mix MT data into Baseline with a sampling probability of $p_{M T}$ .   \n\u2022 Baseline $^+$ Augmented SLT: We replace the Baseline SLT data with the augmented SLT data and uniformly sample the target language for each example at each step.   \n\u2022 Baseline $+\\mathbf{M}\\mathbf{T}+$ Augmented SLT: Baseline $+\\,M T$ but with augmented target languages, as above. ", "page_idx": 4}, {"type": "text", "text": "Downstream Benchmarks, Evaluation and Model Setting We thoroughly evaluate the translation performance on a range of open-domain SLT benchmarks, including How2Sign [14], Elementary23 $[48]^{3}$ , WMT23 [28] and FLEURS-ASL#0 (signer id #0) [42]. Detailed information for each benchmark is given in Table 1. Overall, the evaluation covers 5 source sign languages and 42 target spoken languages.4 ", "page_idx": 4}, {"type": "text", "text": "We report translation results for Pretraining and Finetuning. During inference, we use beam search with a beam size of 5. We evaluate translation with detokenized BLEU [31] and ChrF [32], as well as neural metric, BLEURT [34]. We use BLEURT as the main metric [17]. We initialize our SLT model with three T5 model families: T5 [35], mT5 [52] and ByT5 [53], at three different sizes: Base, Large and XL. We optimize models with Adafactor [39], and set the maximum text input, landmark input, and text output length to 512. More setup details are given in Appendix A.1. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 SLT Pretraining Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Model scaling doesn\u2019t improve SLT consistently: Base often outperforms Large/XL. Table 2 also shows that scaling up model size rarely results in consistent quality improvements. Different from findings on text-only tasks [35, 53], Base surpasses Large and XL in most cases, where Large often converges the slowest and performs the worst. Model scaling alone doesn\u2019t significantly reduce the video-text modality gap, although better optimization and checkpoint selection could help. XL performs relatively comparable to Base. When MT data is mixed in and modeling capacity becomes the bottleneck, the value of model scaling by XL emerges as shown in Figure 8 and Table 5. ", "page_idx": 4}, {"type": "table", "img_path": "M80WgiO2Lb/tmp/2fb093f42fcaec7e626241cfc2328331c3bd4d7b5a9855152ddc6c3a3f1b2eed.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "M80WgiO2Lb/tmp/5f010dbb333db8909f468604ad0c975399b32f90c59ea29942c052093781ec76.jpg", "img_caption": ["Table 2: Pretraining performance (BLEURT \u2191) for different sized $({\\bf B}{\\bf y}/{\\bf m}){\\bf T}5$ models when pretrained on YTASL and YT-Full. Results are reported on the test set of How2Sign and FLEURS-ASL#0 $\\scriptstyle\\left(\\right.\\rightarrow\\mathrm{En}$ , i.e. English as the target). Best results for each model family are highlighted in bold. ", "(a) BLEURT scores for FLEURS-ASL#0 $(\\rightarrow\\mathrm{En})$ ). (b) Zero-shot BLEURT for FLEURS-ASL#0 $\\langle{\\rightarrow}\\mathrm{X})$ . ", "Figure 3: Pretraining performance for Baseline $+~M T$ when varying MT languages. We show BLEURT\u2191 results on FLEURS-ASL#0, and set $p_{m t}\\,=\\,0.5$ . Note MT languages are added separately instead of jointly. Results are for ByT5 Base. $^{\\ast}\\mathrm{X}\\mathrm{\\rightarrow}\\mathrm{En}^{\\mathrm{>}}$ : MT data for translation into English; $\\scriptstyle{\\mathrm{{\\cdots}}}\\mathrm{{X}}\\leftrightarrow{\\mathrm{{En}}}^{*}$ : MT data for both translation directions; \u201cAvg\u201d: average performance over languages. MT languages are arranged in descending order from left to right based on their training data quantity. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Backbone affects SLT substantially; ByT5 generally performs the best. While several previous studies selected T5 [46, 28] or mT5 [44] as the SLT backbone, we observe in Table 2 that ByT5- based SLT outperforms its T5 counterpart in most settings, confirming the results of [43] at scale. Given that larger models do not consistently perform better, it seems less likely that ByT5\u2019s superiority comes from its encoder-heavy parameter allocation, and more likely that it is due to its spelling capabilities and reduced input length gap between byte text sequences and video frame sequences. Unless otherwise stated, we use ByT5 Base for the following experiments. ", "page_idx": 5}, {"type": "text", "text": "Scaling SLT data generally improves quality significantly. Adding more SLT data, i.e. from YT-ASL to YT-Full, largely improves the translation quality in most settings. For ByT5-based SLT particularly, the gain reaches ${\\sim}7$ BLEURT on How2Sign and ${\\sim}11$ BLEURT on FLEURSASL#0 (En) for Base and XL, respectively. We conjecture that adding more (multilingual) SLT data helps reduce the modality gap (especially with skeletons, which lack pretrained representations) and enable cross-lingual knowledge transfer [2, 57, 55, 44]. ", "page_idx": 5}, {"type": "text", "text": "Mixing MT and SLT data yields positive knowledge transfer to SLT. We next explore whether and how the addition of MT data benefits SLT, starting with YT-ASL and bilingual MT data with $p_{m t}=0.5$ . Figures 3a and 5 show that adding bilingual translation data improves SLT performance generally, confirming the findings of SLTUNet [58]\u2014that jointly training with MT enables positive knowledge transfer\u2014at scale. The quality gains vary greatly across languages, which show little correlation with language family or training data scale. For example, adding a large amount of $\\mathrm{Fr\\mathrm{{\\rightarrow}}E n}$ data ${\\sim}243\\mathrm{M}$ sentence pairs) helps little (or even hurts) on FLEURS-ASL#0 (En), while adding a small amount of $\\scriptstyle\\mathrm{Ja}\\to\\mathrm{En}$ data ( $.5\\mathrm{M}$ sentence pairs) gives a gain of at least 3 BLEURT on How2Sign and FLEURS-ASL#0 (En). ", "page_idx": 5}, {"type": "text", "text": "Translation direction of MT data affects transfer to SLT. There are three ways to leverage MT data for SLT: 1) $\\scriptstyle\\mathrm{X}\\to\\mathrm{En}$ , 2) $\\mathrm{En}{\\rightarrow}\\mathrm{X}$ , and 3) both. We compare 1) and 3) in Figures 3a and 5 for ASLto-En SLT. The translation direction of MT data influences SLT performance greatly and varies across languages. On average, $\\scriptstyle\\mathrm{X}\\to\\mathrm{En}$ benefits ASL-to-En SLT more than $\\scriptstyle\\mathrm{X}\\leftrightarrow\\mathrm{En}$ : $+0.08$ and $+0.9$ BLEURT on How2Sign and FLEURS-ASL#0 (EN), respectively. We speculate that including translation into X uses model capacity, which, while enabling zero-shot ASL-to-X SLT as discussed below, results in slightly worse ASL-to-En performance. This suggests that MT data with the same target language as SLT is most effective for transfer. Table 3 shows further support where $\\mathrm{En}{\\rightarrow}\\mathrm{X}$ surpasses $\\scriptstyle\\mathrm{X}\\to\\mathrm{En}$ on multilingual SLT. ", "page_idx": 5}, {"type": "image", "img_path": "M80WgiO2Lb/tmp/795f1dafa9ad4f2c43f640ce2e1e58a67e8262b4c1824cadbd15694696cb304f.jpg", "img_caption": ["Figure 4: Pretraining performance for Baseline $+~M T$ when changing the mixing ratio of MT data $p_{m t}$ on FLEURS-ASL#0 (En and De) test set. We show BLEURT $\\uparrow$ results as we vary $p_{m t}$ from 0.3 to 0.9. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "We can achieve zero-shot bilingual ASL-to-X SLT via ASL-to-En $\\mathbf{S}\\mathbf{L}\\mathbf{T}+\\mathbf{E}\\mathbf{n}{\\leftrightarrow}\\mathbf{X}$ MT, albeit at poor quality. If knowledge can be transferred from MT to SLT, one straightforward question is whether we can achieve zeroshot SLT by jointly training with MT. We do so by training on ASL-to-En $\\mathrm{SLT}+\\mathrm{En}{\\leftrightarrow}\\mathrm{X}$ MT data and examining zero-shot ASL-to-X SLT on FLEURS-ASL#0 (X). Figure 3b shows that this works effectively. On Pl and It, we observe quality gains over 12 BLEURT; on average, adding MT data improves zero-shot SLT by ${\\sim}6$ BLEURT. Nevertheless, the overall zero-shot SLT performance is middling, and the gains are unstable across languages, e.g. performance degrades for ASL-to-Hi SLT with joint MT training. Similar findings were also observed in multilingual MT and speech translation [57, 13]. Deeper analysis in Appendix A.2 reveals that zero-shot SLT also suffers from the off-target translation problem [57], i.e. translating into a wrong target language; adding MT data can alleviate it, mostly for high-resource languages. ", "page_idx": 6}, {"type": "text", "text": "Using a higher sampling ratio for the MT data, i.e. larger $p_{m t}$ , often improves SLT. We start with $p_{m t}\\,=$ 0.5, i.e., sampling equal amount of SLT and MT data, in the above experiments following intuition. However, the proportion of different types of data often has nonnegligible influence in multilingual modeling [2, 9]. We next explore its impact on SLT and use MT En-De for illustration. Figure 4 and 6 shows that $p_{m t}~=~0.5$ is sub-optimal and sampling more MT data improves SLT in ", "page_idx": 6}, {"type": "table", "img_path": "M80WgiO2Lb/tmp/9c71cd62c18c0d9cdad8a8949f8ee126e431c05ba098eb7b48a4ff273d4d507a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 3: Pretraining performance for Baseline $+\\ M T$ with $p_{m t}~~=~0.9$ when scaling up languages and data. We show averaged BLEURT\u2191 results on FLEURSASL#0. Results are for ByT5 Base. \u201cDir\u201d: translation direction of MT data; \u201cSmall/Large\u201d: average results over the target languages included in MT-Small/MT-Large on FLEURS-ASL#0. ", "page_idx": 6}, {"type": "text", "text": "most settings, regardless of using ByT5 Base or XL, YT-ASL or YT-Full, MT De\u2192En or $\\mathbf{De}{\\leftrightarrow}$ En, and How2Sign or FLEURS-ASL#0 $(\\mathrm{En}/\\mathrm{De})$ . In addition, increasing the proportion of MT data also improves zero-shot ASL-to-De SLT. Note another benefit of using more MT data is to accelerate training, as loading SLT data is much slower than loading text-only MT data. We use $p_{m t}=0.9$ by default in the following experiments. ", "page_idx": 6}, {"type": "text", "text": "Multilingual MT improves multilingual (zero-shot) SLT. The above experiments mainly analyze SLT with bilingual MT. We next investigate how multilingual MT affects multilingual (zero-shot) SLT, particularly the use of MT-Small and MT-Large. We report results for ASL-to-Small and ASLto-Large SLT on FLEURS-ASL#0 where Small and Large denote the target languages covered by ", "page_idx": 6}, {"type": "text", "text": "MT-Small and MT-Large, respectively. Note all SLT directions are zero-shot except the translation to English. ", "page_idx": 7}, {"type": "text", "text": "Table 3 summarizes the average performance. Using multilingual $\\scriptstyle\\mathrm{X}\\to\\mathrm{En}$ MT data results in unstable ASL-to-X SLT performance, which even hurts SLT on YT-Full. In contrast, multilingual $\\mathrm{En}{\\rightarrow}\\mathrm{X}$ and $\\mathrm{En}{\\leftrightarrow}\\mathrm{X}$ MT data are both very helpful to SLT, where the former often outperforms the latter. By default, we still use $\\mathrm{En}{\\leftrightarrow}X$ MT data in the following experiments so as to fully leverage the knowledge in MT data during pretraining. ", "page_idx": 7}, {"type": "text", "text": "Figures $_{7\\mathrm{a}}$ and 7b further show the language breakdown results. Adding multilingual MT significantly improves ASL-to-En SLT when using YT-ASL alone, while the gain almost disappears when using larger-scale SLT data, YT-Full. Again, we note that the overall zero-shot translation quality is poor \u2013 the best average BLEURT on Small and Large is 29.52 and 30.69, respectively. Achieving significant ASL-to-X SLT requires techniques beyond naive SLT and MT data mixing. ", "page_idx": 7}, {"type": "text", "text": "Data augmentation and large-capacity modeling are promising methods for multilingual SLT. In MT, a common solution to improve zero-shot quality is to construct pseudo translation data for zeroshot directions [2, 15, 56, 16]. We examine this practice for SLT. We adopt publicly pretrained MT models to generate data for more target languages for the YouTube SLT data (i.e., Augmented SLT). Results in Table 4 demonstrate the effectiveness of Augmented SLT, which significantly improves the best performance for ByT5 Base-based SLT to 36.01 and 39.85 average BLEURT on Small and Large with a gain of 6.49 $29.52{\\rightarrow}36.01$ ) and 9.16 $(30.69{\\rightarrow}39.85)$ ), respectively. Note there are 42 languages in Large. ByT5 Base may be insufficient in accommodating translation for such amount of languages. Increasing the modeling capacity to XL yields another gain of 9.11 $(36.01{\\rightarrow}45.12\\$ ) and 8.2 $39.85{\\rightarrow}48.05$ ) average BLEURT on Small and Large, respectively. On YT-ASL, Augmented SLT and ByT5 XL also lead to substantial quality improvements by 13.84 ( $24.69{\\rightarrow}38.53{\\rangle}/15.96$ $(26.60{\\rightarrow}42.56)$ average BLEURT on Small/Large. The final performance even surpasses the cascading baseline, i.e. ASL-to-En SLT chained with En-to-X MT, under both YT-ASL and YT-Full. Figures 8a ", "page_idx": 7}, {"type": "table", "img_path": "M80WgiO2Lb/tmp/ff8e16941d59ddf1e666ecc70103038fb66538947f924e8f6ef2cddd38148d3e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 4: Pretraining performance (averaged BLEURT\u2191) for Baseline $^+$ Augmented $S L T+{\\mathit{M T}}$ with $p_{m t}=0.9$ on FLEURS-ASL#0 test set. MT data are multilingual in both directions. Baseline is for ByT5 Base; \u201cMT-3B\u201d: MADLAD-MT-3B, the model used for SLT augmentation; \u201cCascading\u201d: translating FLEURS-ASL#0 to English and then performing MT to other target languages. ", "page_idx": 7}, {"type": "text", "text": "and 8b also show the quality improvements across languages resulted from data augmentation and ByT5 XL. ", "page_idx": 7}, {"type": "text", "text": "4.2 SLT Finetuning Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We report the finetuning performance measured by BLEURT in Table 5. We also include the BLEU and ChrF results as well as the corresponding pretraining results in Appendix (Tables 8 and 9). ", "page_idx": 7}, {"type": "text", "text": "Finetuning on downstream benchmarks substantially improves SLT performance. Table 5 shows that finetuning the pretrained SLT models yields substantial quality gains across benchmarks and settings. This is because the potential of pretrained models is not fully elicited by direct evaluation due to video recording, domain and (clip-based) pretraining vs. (segment-based) inference mismatches, and finetuning largely mitigates these gaps. For example, pretraining with external augmented SLT and MT data results in even worse pretraining performance $(6){\\rightarrow}(7))$ ) in Table 9. After finetuning, nevertheless, model (7) significantly surpasses model (6) by 5.12 BLEURT on average. ", "page_idx": 7}, {"type": "text", "text": "Adding multilingual SLT data (YT-Full) into the pretraining greatly improves the performance from 14.26 (model (5)) to 32.48 BLEURT (model (8)) in Table 9. However, the quality gain after finetuning for YT-ASL based models is often higher than their YT-Full counterparts, where the largest gain reaches ${\\sim}28$ BLEURT for model (5). We argue that pretraining on YT-ASL mainly teaches understanding of ASL, so pretrained performance on other sign languages is poor, but finetuning can quickly adapt the learned representations to other sign languages. ", "page_idx": 7}, {"type": "table", "img_path": "M80WgiO2Lb/tmp/9158aabb93b73ae866733dae06cc36d1408a2e633002c4c9614081d38058d10e.jpg", "table_caption": [], "table_footnote": ["Table 5: Finetuning performance (BLEURT\u2191) on downstream SLT benchmarks. \u201cH2S/E23\u201d: How2Sign/Elementary23. \u201cSRF/SS\u201d: WMT23 DSGS SRF/SS test split. \u201cAvg\u201d: averaged performance over all benchmarks. MT data are added in both translation directions. Previous SOTA: How2Sign [43], Elementary23 [48] and WMT23 SRF [28], WMT23 LIS-CH, LSF-CH, SS [44]. All models are finetuned on each SLT benchmark separately except (9). "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Note we also finetuned the vanilla ByT5 model without SLT pretraining for reference, which achieves 7.68 and 3.10 BLEU on Elementary23 and WMT23 DSGS SS, respectively. Despite their inferiority, these results already surpass the previous SOTA, further showing the potential of ByT5. ", "page_idx": 8}, {"type": "text", "text": "A model\u2019s pretraining performance may be misleading when estimating its downstream finetuning performance, depending on the evaluation metric. Intuitively, a model with better pretrained results should result in better finetuned results. The Spearman\u2019s correlation results in Table 6 confirm this intuition, where the correlation scores are positive across metrics. However, BLEU and ChrF have a correlation score of 0.347 and 0.186, respectively, which are very moderate. The correlation for ChrF is even not significant, which may be caused by the use of BLEU as the model selection metric. In contrast, the correlation of BLEURT reaches 0.578 and is significant at $p<0.01$ . ", "page_idx": 8}, {"type": "text", "text": "Model, data and language scaling together leads to new state-of-the-art results. Diving deeper into Table 5, we see clear improvements brought by scaling model size, data, and/or languages for SLT. Adding YT-ASL SLT data into the pretraining yields ${\\sim}10$ average BLEURT improvement $(1){\\rightarrow}(2)$ ). Jointly training SLT with MT data produces another gain of ${\\sim}6$ BLEURT $((2){\\rightarrow}(3))$ ). Data augmentation adds an improvement of ${\\sim}3$ BLEURT $((3){\\rightarrow}(4))$ ), which matches the quality achieved by adding ", "page_idx": 8}, {"type": "table", "img_path": "M80WgiO2Lb/tmp/25ef34dcb3fc06595b86176d754a5cfeca9babb00b3147b0862ea85b7c8056f4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 6: Spearman correlation between direct (i.e. pretraining) and finetuning SLT results under different metrics based on Tables 5 and 9. $^{\\dagger}/^{\\ddag}$ : significant at $p<0.05/0.01$ . ", "page_idx": 8}, {"type": "text", "text": "large amount of extra multilingual SLT data to the baseline, i.e. (4) 40.91 vs. (6) 40.28. By further increasing the amount of MT and augmented SLT data as well as the ByT5 model size, we reach an average BLEU, ChrF and BLEURT of 16.90, 39.49, and 49.60, respectively (model (8)). These results also outperform previous best results, establishing the new SOTA. ", "page_idx": 8}, {"type": "text", "text": "Multilingual finetuning improves multilingual SLT with encouraging performance, although it still underperforms bilingual finetuning on average. We next study multilingual finetuning on the direct mix of different SLT benchmarks. Table 5 ( $(8){\\rightarrow}(9)$ shows that multilingual SLT outperforms previous SOTA on almost all benchmarks, but underperforms its bilingual counterpart by 1.22 BLEURT on average. How to balance modeling capacity among different languages in a joint model and avoid cross-lingual/modality interference is a well known issue in multilingual modeling [2, 57, 49], and multilingual SLT also suffers [55], which we leave to future. Still, multilingual SLT facilitates transfer to LIS-CH, leading to a substantial gain of 2.6 BLEURT $(8){\\rightarrow}(9)$ ). ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The main bottleneck of SLT is data scarcity. Early studies address this issue by developing more data efficient neural architectures and/or training algorithms. Camgoz et al. [4] pioneered the study with encoder-decoder based recurrent models for SLT, which was quickly replaced by Transformer and multi-task learning with CTC regularization [5]. Zhou et al. [61] developed spatial-temporal architecture to model the collaboration of different visual cues. Another way is to transfer the knowledge from pretrained models, augmentations, and other tasks. Chen et al. [7, 8] proposed to leverage pretrained visual encoders and MT models to improve SLT, while Zhang et al. [58] explored transferring translation knowledge from MT data directly. Zhou et al. [60] employed back-translation to generate pseudo SLT training data. Ye et al. [54] augmented the training data by the mix-up algorithm. Yet another way to address data scarcity is to make data less scarce. Shi et al. [40], Uthus et al. [46], and Tanzer and Zhang [44] collected large-scale SLT data from YouTube and improved data quality via manual filtering; Albanie et al. [1] developed a British Sign Language translation corpus based on BBC broadcasts instead. Tanzer [43] scaled up ASL data by eschewing manual filtering and tolerating misaligned or irrelvant data. We follow and scale to noisy multilingual sign language data, MT data, and augmented paralel data. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Despite the aforementioned advancements, many studies still heavily depend on sign glosses. As a bridge between sign video and target text, sign glosses ease learning, but are expensive to annotate, not always available, nonstandardized, and cannot cope with sign language grammar in generality [12]. Recent research therefore turns to gloss-free SLT, which often underperforms gloss-based counterparts [25, 59, 50] and performs poorly in open-domain settings [38, 51, 28]. We substantially improve gloss-free SLT performance across benchmarks through scaling. In this regard, our work is closely related to SSVP-SLT [37] but with different focuses. SSVP-SLT improves SLT by pretraining a neural sign encoder through large-scale self-supervised learning. By contrast, we adopt static landmarks to represent sign frames and improve the translation by transferring knowledge from other languages and tasks. The methods used in our study are orthogonal to SSVP-SLT. In addition, our work also falls into the category of improving multilingual SLT [55, 20]. We didn\u2019t evaluate our models on these multilingual benchmarks though as they are either unavailable at the time of paper writing or unusable due to licensing issues. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion, Limitations, and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We presented a systematic study of data, model and language scaling for SLT via large-scale SLT pretraining. In general, scaling substantially improves SLT. We observe positive knowledge transfer from other sign language data and from machine translation data. By joint SLT and MT training, we show the feasibility of achieving zero-shot SLT. Data augmentation expanding SLT data to more spoken languages via off-the-shelf MT models significantly improves multilingual SLT. Putting everything together, finetuning our pretrained SLT models leads to new state-of-the-art SLT results across 5 benchmarks covering 5 sign languages (but still far from usable quality). ", "page_idx": 9}, {"type": "text", "text": "Although our models have nominally been pretrained on a massive number of sign languages (up to 80), we lack comprehensive and reliable multilingual benchmarks to fully understand their abilities and limitations. In addition, our models are limited to encoder-decoder based (m/By)T5 models, and SLT pretraining requires many computational resources, increasing the difficulty of reproduction. ", "page_idx": 9}, {"type": "text", "text": "In the future, we expect that continuing to scale sign language data, number of sign languages, vision pretraining/multimodality, etc. will reap further gains. As suggested by [42], it will be important to evaluate these growing capabilities on multilingual, standardized, open-domain SLT benchmarks. ", "page_idx": 9}, {"type": "text", "text": "Ethics Statement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We preprocess all sign videos with simplified landmarks as a form of anonymization and privacy protection. While the pretraining SLT data is larger scale than prior work, it may still suffer from demographic biases. Even if demographics were represented in proportion to the real world, and even with simplified landmarks, the resulting SLT models may not perform equally across groups and should be evaluated for fairness before real-world deployment. Our study mainly aims to understand the impact of scaling on SLT, and while we significantly improve translation quality, it is still far from usable for real-world applications. For many such applications, the other half of sign language translation\u2014sign language generation\u2014is also essential, whereas we focus only on sign language understanding in this work. Advancing both of these is critical to ensure that Deaf/Hard of Hearing signers get equal access to technology and the information that comes through it. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank the reviewers for their insightful comments. We thank Ankush Garg for valuable feedback on this work, Chris Dyer for constructive comments that greatly improve the quality of this paper, Sam Sepah and Google Translate team for supporting this research. We also thank the T5X team [36] for infrastructure support. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Samuel Albanie, G\u00fcl Varol, Liliane Momeni, Hannah Bull, Triantafyllos Afouras, Himel Chowdhury, Neil Fox, Bencie Woll, Rob Cooper, Andrew McParland, et al. Bbc-oxford british sign language dataset. arXiv preprint arXiv:2111.03635, 2021.   \n[2] Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, Wolfgang Macherey, Zhifeng Chen, and Yonghui Wu. Massively multilingual neural machine translation in the wild: Findings and challenges, 2019. [3] Ankur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, et al. Building machine translation systems for the next thousand languages. arXiv preprint arXiv:2205.03983, 2022.   \n[4] Necati Cihan Camgoz, Simon Hadfield, Oscar Koller, Hermann Ney, and Richard Bowden. Neural sign language translation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7784\u20137793, 2018.   \n[5] Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, and Richard Bowden. Sign language transformers: Joint end-to-end sign language recognition and translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10023\u201310033, 2020. [6] Isaac Caswell, Ciprian Chelba, and David Grangier. Tagged back-translation. In Ond\u02c7rej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Andr\u00e9 Martins, Christof Monz, Matteo Negri, Aur\u00e9lie N\u00e9v\u00e9ol, Mariana Neves, Matt Post, Marco Turchi, and Karin Verspoor, editors, Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pages 53\u201363, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-5206. URL https://aclanthology.org/W19-5206.   \n[7] Yutong Chen, Fangyun Wei, Xiao Sun, Zhirong Wu, and Stephen Lin. A simple multi-modality transfer learning baseline for sign language translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5120\u20135130, 2022. [8] Yutong Chen, Ronglai Zuo, Fangyun Wei, Yu Wu, Shujie Liu, and Brian Mak. Two-stream network for sign language recognition and translation. Advances in Neural Information Processing Systems, 35:17043\u201317056, 2022.   \n[9] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116, 2019.   \n[10] Marta R Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022.   \n[11] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 7480\u20137512. PMLR, 2023. ", "page_idx": 10}, {"type": "text", "text": "[12] Aashaka Desai, Maartje De Meulder, Julie A. Hochgesang, Annemarie Kocab, and Alex X. Lu. Systemic biases in sign language ai research: A deaf-led call to reevaluate research agendas, 2024. ", "page_idx": 11}, {"type": "text", "text": "[13] Tu Anh Dinh. Zero-shot speech translation. arXiv preprint arXiv:2107.06010, 2021. ", "page_idx": 11}, {"type": "text", "text": "[14] Amanda Duarte, Shruti Palaskar, Lucas Ventura, Deepti Ghadiyaram, Kenneth DeHaan, Florian Metze, Jordi Torres, and Xavier Giro-i Nieto. How2Sign: A Large-scale Multimodal Dataset for Continuous American Sign Language. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021. ", "page_idx": 11}, {"type": "text", "text": "[15] Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. Beyond englishcentric multilingual machine translation. Journal of Machine Learning Research, 22(107): 1\u201348, 2021. ", "page_idx": 11}, {"type": "text", "text": "[16] Markus Freitag and Orhan Firat. Complete multilingual neural machine translation. arXiv preprint arXiv:2010.10239, 2020. ", "page_idx": 11}, {"type": "text", "text": "[17] Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and Andr\u00e9 F. T. Martins. Results of WMT22 metrics shared task: Stop using BLEU \u2013 neural metrics are better and more robust. In Philipp Koehn, Lo\u00efc Barrault, Ond\u02c7rej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-juss\u00e0, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Tom Kocmi, Andr\u00e9 Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, Matteo Negri, Aur\u00e9lie N\u00e9v\u00e9ol, Mariana Neves, Martin Popel, Marco Turchi, and Marcos Zampieri, editors, Proceedings of the Seventh Conference on Machine Translation (WMT), pages 46\u201368, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.2. ", "page_idx": 11}, {"type": "text", "text": "[18] Team Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. ", "page_idx": 11}, {"type": "text", "text": "[19] Team Gemini, Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. ", "page_idx": 11}, {"type": "text", "text": "[20] Shester Gueuwou, Sophie Siake, Colin Leong, and Mathias M\u00fcller. JWSign: A highly multilingual corpus of Bible translations for more diversity in sign language processing. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9907\u20139927, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.664. URL https://aclanthology.org/2023.findings-emnlp.664. ", "page_idx": 11}, {"type": "text", "text": "[21] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. ", "page_idx": 11}, {"type": "text", "text": "[22] Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation. In Loic Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussa, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Tom Kocmi, Andre Martins, Makoto Morishita, and Christof Monz, editors, Proceedings of the Sixth Conference on Machine Translation, pages 478\u2013494, Online, November 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.wmt-1.57. ", "page_idx": 11}, {"type": "text", "text": "[23] Sneha Kudugunta, Isaac Rayburn Caswell, Biao Zhang, Xavier Garcia, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat. MADLAD-400: A multilingual and document-level large audited dataset. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview. net/forum?id $\\cdot$ Y45ZCxslFx.   \n[24] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8424\u20138445, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.577. URL https://aclanthology.org/2022. acl-long.577.   \n[25] Kezhou Lin, Xiaohan Wang, Linchao Zhu, Ke Sun, Bang Zhang, and Yi Yang. Gloss-free end-to-end sign language translation. arXiv preprint arXiv:2305.12876, 2023.   \n[26] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-Ling Chang, Ming Guang Yong, Juhyun Lee, et al. Mediapipe: A framework for building perception pipelines. arXiv preprint arXiv:1906.08172, 2019.   \n[27] Qingsong Ma, Johnny Wei, Ond\u00c5TMej Bojar, and Yvette Graham. Results of the wmt19 metrics shared task: Segment-level and strong mt systems pose big challenges. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 62\u201390, Florence, Italy, August 2019. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/W19-5302.   \n[28] Mathias M\u00fcller, Malihe Alikhani, Eleftherios Avramidis, Richard Bowden, Annelies Braffort, Necati Cihan Camg\u00f6z, Sarah Ebling, Cristina Espa\u00f1a-Bonet, Anne G\u00f6hring, Roman Grundkiewicz, Mert Inan, Zifan Jiang, Oscar Koller, Amit Moryossef, Annette Rios, Dimitar Shterionov, Sandra Sidler-Miserez, Katja Tissi, and Davy Van Landuyt. Findings of the second WMT shared task on sign language translation (WMT-SLT23). In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the Eighth Conference on Machine Translation, pages 68\u201394, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.wmt-1.4. URL https://aclanthology.org/2023.wmt-1.4.   \n[29] Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig Schmidt. Quality not quantity: On the interaction between dataset design and robustness of clip. Advances in Neural Information Processing Systems, 35:21455\u201321469, 2022.   \n[30] Team OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[31] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL $\\overline{{\\,^{\\prime}02\\,}}$ , page 311\u2013318, USA, 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://doi.org/10.3115/ 1073083.1073135.   \n[32] Maja Popovic\u00b4. chrF: character n-gram F-score for automatic MT evaluation. In Ond\u02c7rej Bojar, Rajan Chatterjee, Christian Federmann, Barry Haddow, Chris Hokamp, Matthias Huck, Varvara Logacheva, and Pavel Pecina, editors, Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392\u2013395, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/W15-3049. URL https://aclanthology. org/W15-3049.   \n[33] Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, et al. Scaling speech technology to $1{,}000+$ languages. arXiv preprint arXiv:2305.13516, 2023.   \n[34] Amy Pu, Hyung Won Chung, Ankur P Parikh, Sebastian Gehrmann, and Thibault Sellam. Learning compact metrics for mt. In Proceedings of EMNLP, 2021.   \n[35] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.   \n[36] Adam Roberts, Hyung Won Chung, Gaurav Mishra, Anselm Levskaya, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, et al. Scaling up models and data with t5x and seqio. Journal of Machine Learning Research, 24(377): 1\u20138, 2023.   \n[37] Phillip Rust, Bowen Shi, Skyler Wang, Necati Cihan Camg\u00f6z, and Jean Maillard. Towards privacy-aware sign language translation at scale. arXiv preprint arXiv:2402.09611, 2024.   \n[38] Marcelo Sandoval-Castaneda, Yanhong Li, Bowen Shi, Diane Brentari, Karen Livescu, and Gregory Shakhnarovich. TTIC\u2019s submission to WMT-SLT 23. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the Eighth Conference on Machine Translation, pages 344\u2013350, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.wmt-1.35. URL https://aclanthology.org/ 2023.wmt-1.35.   \n[39] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596\u20134604. PMLR, 2018.   \n[40] Bowen Shi, Diane Brentari, Gregory Shakhnarovich, and Karen Livescu. Open-domain sign language translation learned from online video. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6365\u20136379, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.427. URL https://aclanthology.org/2022.emnlp-main.427.   \n[41] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper_files/paper/ 2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf.   \n[42] Garrett Tanzer. Fleurs-asl: Including american sign language in massively multilingual multitask evaluation, 2024. URL https://arxiv.org/abs/2408.13585.   \n[43] Garrett Tanzer. Fingerspelling within sign language translation, 2024. URL https://arxiv. org/abs/2408.07065.   \n[44] Garrett Tanzer and Biao Zhang. Youtube-sl-25: A large-scale, open-domain multilingual sign language parallel corpus, 2024. URL https://arxiv.org/abs/2407.11144.   \n[45] Laia Tarr\u00e9s, Gerard I. G\u00e1llego, Amanda Duarte, Jordi Torres, and Xavier Gir\u00f3 i Nieto. Sign language translation from instructional videos, 2023.   \n[46] Dave Uthus, Garrett Tanzer, and Manfred Georg. Youtube-asl: A large-scale, open-domain american sign language-english parallel corpus. Advances in Neural Information Processing Systems, 36, 2024.   \n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[48] Andreas Voskou, Konstantinos P Panousis, Harris Partaourides, Kyriakos Tolias, and Sotirios Chatzis. A new dataset for end-to-end sign language translation: The greek elementary school dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1966\u20131975, 2023.   \n[49] Zirui Wang, Zihang Dai, Barnabas Poczos, and Jaime Carbonell. Characterizing and avoiding negative transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.   \n[50] Ryan Wong, Necati Cihan Camgoz, and Richard Bowden. Sign2GPT: Leveraging large language models for gloss-free sign language translation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=LqaEEs3UxU.   \n[51] Baixuan Xu, Haochen Shi, Tianshi Zheng, Qing Zong, Weiqi Wang, Zhaowei Wang, and Yangqiu Song. KnowComp submission for WMT23 sign language translation task. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the Eighth Conference on Machine Translation, pages 351\u2013358, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.wmt-1.36. URL https: //aclanthology.org/2023.wmt-1.36.   \n[52] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934, 2020.   \n[53] Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. ByT5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguistics, 10:291\u2013306, 2022. doi: 10.1162/tacl_a_00461. URL https://aclanthology.org/2022.tacl-1.17.   \n[54] Jinhui Ye, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, and Hui Xiong. Cross-modality data augmentation for end-to-end sign language translation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 13558\u201313571, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.904. URL https://aclanthology.org/ 2023.findings-emnlp.904.   \n[55] Aoxiong Yin, Zhou Zhao, Weike Jin, Meng Zhang, Xingshan Zeng, and Xiaofei He. Mlslt: Towards multilingual sign language translation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5099\u20135109, 2022. doi: 10.1109/CVPR52688. 2022.00505.   \n[56] Biao Zhang and Rico Sennrich. Edinburgh\u2019s end-to-end multilingual speech translation system for IWSLT 2021. In Marcello Federico, Alex Waibel, Marta R. Costa-juss\u00e0, Jan Niehues, Sebastian Stuker, and Elizabeth Salesky, editors, Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021), pages 160\u2013168, Bangkok, Thailand (online), August 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.iwslt-1.19. URL https://aclanthology.org/2021.iwslt-1.19.   \n[57] Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. Improving massively multilingual neural machine translation and zero-shot translation. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1628\u20131639, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.148. URL https: //aclanthology.org/2020.acl-main.148.   \n[58] Biao Zhang, Mathias M\u00fcller, and Rico Sennrich. SLTUNET: A simple unified model for sign language translation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ EBS4C77p_5S.   \n[59] Benjia Zhou, Zhigang Chen, Albert Clap\u00e9s, Jun Wan, Yanyan Liang, Sergio Escalera, Zhen Lei, and Du Zhang. Gloss-free sign language translation: Improving from visual-language pretraining. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20871\u201320881, 2023.   \n[60] Hao Zhou, Wengang Zhou, Weizhen Qi, Junfu Pu, and Houqiang Li. Improving sign language translation with monolingual data by sign back-translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1316\u20131325, 2021.   \n[61] Hao Zhou, Wengang Zhou, Yun Zhou, and Houqiang Li. Spatial-temporal multi-cue network for sign language recognition and translation. IEEE Transactions on Multimedia, 24:768\u2013779, 2021. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 7: Data statistics for YouTube sign language and MADLAD spoken language. We list ISO 639-3 code, language name, and the number of hours/clips/videos for sign language; for spoken language, we list BCP-47 code, language name and the number of parallel examples. \u201cK\u201d: thousand, \u201cM\u201d: million. Note that like Tanzer and Zhang [44] pre-filtering and Tanzer [43], these language labels are heuristically estimated based on public video metadata, such as caption language and text in the video title, description, etc. ", "page_idx": 15}, {"type": "table", "img_path": "M80WgiO2Lb/tmp/0046dcdc23da913039be50f67ec3f0ea8433d43f9afccefc1c38047136c23ffd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "M80WgiO2Lb/tmp/7ef5e2020b9d83784f9936a991a6ce83ec5073d1b63408b9762228d3a6f6e156.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "M80WgiO2Lb/tmp/9d12373590a33e1fe8092c0468c88a1b7e30f9e7baace2e917ca5c04039e7118.jpg", "img_caption": ["Figure 5: Pretraining performance for Baseline $+~M T$ when varying MT languages on How2Sign test set. We show BLEURT\u2191results and set $p_{m t}=0.5$ . Note only YT-ASL and bilingual MT data are used, i.e. MT languages are added separately instead of jointly. Results are for ByT5 Base. $^{\\ast}\\mathrm{X}\\mathrm{\\rightarrow}\\mathrm{En}^{\\mathrm{,}\\ast}$ : MT data for translation into English; $\\mathbf{\\omega}^{\\ast}\\mathbf{X}\\mathbf{\\leftrightarrow}\\mathbf{En}^{\\circ};$ : MT data for both translation directions; \u201cAvg\u201d: average performance over languages. MT languages are arranged in descending order from left to right based on the quantity of translation data available for each language. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.1 Setup", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Sign Video Preprocessing Our landmark preprocessing is identical to [46], and we use the same random 34-second video clipping as [42]. We preprocess sign language video with its default frame rate but discard every second frame for computational efficiency. We convert each video frame to a 255-dimensional normalized vector using MediaPipe Holistic landmarks [26], which also facilitates video anonymization. The input video is eventually transformed into a vector sequence and then mapped to the encoder via a linear projection layer. ", "page_idx": 16}, {"type": "text", "text": "Downstream Benchmarks Note the official SLT track in WMT23 for LIS-CH and LSF-CH is for sign language generation rather than translation. We reversed it as a SLT dataset. FLEURS-ASL#0 is the subset of FLEURS-ASL [42] recorded by signer $\\#0$ , i.e. 353 sentences from FLORES [10] translated into ASL by a Certified Deaf Interpreter. We report only signer $\\#0$ because the rest of the benchmark was not complete when these experiments were run. ", "page_idx": 16}, {"type": "image", "img_path": "M80WgiO2Lb/tmp/316803e13d3960dab32f1dd470f60ec4b5c4d80f654c649ef8fa5a8fba6695fa.jpg", "img_caption": ["Figure 6: Pretraining performance for Baseline $+~M T$ when changing the mixing ratio of MT data $p_{m t}$ on How2Sign test set. We show BLEURT $\\uparrow$ results and vary $p_{m t}$ from 0.3 to 0.9. Note only bilingual MT En-De data are explored. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "For these benchmarks, we only use the sign language video and target text without glosses. All SLT models in this study are gloss-free. ", "page_idx": 17}, {"type": "text", "text": "Model Setting For Pretraining, we use a batch size of 256 and a constant learning rate of 0.001. We pretrain models up to 1M steps using 64/64/128 TPU-v3 chips for Base/Large/XL, taking 7- 20 days. We select the best checkpoint for downstream application based on the How2Sign dev performance measured by BLEU5. ", "page_idx": 17}, {"type": "text", "text": "For Finetuning, we use a batch size of 32 and a constant learning rate of 0.0005. By default, we perform finetuning on each downstream benchmark separately. We only consider the SLT task at finetuning, and directly finetune the model on well aligned (sign video segment, target translation) pairs, which is provided in all downstream benchmarks. We tune models up to 50K steps using 16/32 TPU-v3 chips for Base/XL, taking $2{\\sim}5$ days. We select the best checkpoint for final evaluation based on the dev-set BLEU. ", "page_idx": 17}, {"type": "text", "text": "A.2 More Results and Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Zero-shot SLT benefits from MT data partially because it reduces off-target translation. A key factor affecting zero-shot MT is the off-target problem, where the model translates into a wrong target language [57]. We examine this problem for zero-shot SLT according to experiments in Figure 3b and show the results in Table 10. ", "page_idx": 17}, {"type": "text", "text": "We noticed that zero-shot SLT also suffers from off-target translation, particularly for those languages distant from English. For example, Baseline only has a language accuracy of 1.7, 45.0 and 42.2 for Cs, Zh and Hi, respectively. Adding MT data generally improves the translation language accuracy, such as 1.7/78.5 to 61.2/97.7 for Cs/Pl. But there are also exceptions, like Zh and Hi, where the accuracy reduces from $45.0/42.2$ to $15.0/4.8\\$ . A deeper inspection shows that jointly training with MT leads to more empty outputs for these languages: the empty rate increases from 2.8/1.4 to $17.3/43.3$ for Zh/Hi. We argue that this may be because 1) these languages have significantly less parallel MT data, e.g. Hi only has 1.2M examples, and 2) the parallel corpus from MADLAD can also be quite noisy. ", "page_idx": 17}, {"type": "text", "text": "Different evaluation metrics may disagree. There is a hot debate in MT community regarding which metric we should use for translation evaluation [22]. While BLEU has been widely adopted, it often shows poor correlation with human evaluation, particularly when the translation models are strong [27]. Instead, neural metrics are recommended [17]. We follow this trend and adopt ", "page_idx": 17}, {"type": "image", "img_path": "M80WgiO2Lb/tmp/2895581cfb17f29e094ffd70b6452aed4f8a326cce9a403e6fb880c0a1eac784.jpg", "img_caption": ["(a) Results for training with MT-Small. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "M80WgiO2Lb/tmp/4a30abc818e3a7908f10ee36b68649a3050c02f6cbb88c5ed7789e9afa7d38ca.jpg", "img_caption": ["(b) Results for training with MT-Large. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 7: Per-language pretraining performance for Baseline $+\\,M T$ with $p_{m t}=0.9$ when scaling up languages and data. We show BLEURT\u2191results on FLEURS-ASL#0. We add multilingual MT data into SLT pretraining and compare MT-Small with MT-Large. Results are for ByT5 Base. ", "page_idx": 18}, {"type": "text", "text": "BLEURT as the main metric. To be compatible with past studies and also ease future comparison, we also add BLEU and ChrF. Table 8 shows some disagreements between BLEURT and BLEU/ChrF. For example, model (4) performs better than (comparable to) model (5) on average based on ChrF (BLEU), while BLEURT scores show a clear superiority of model (5) over (4). Evaluation metric selection should be more careful due to these disagreements. In this study, we rely more on BLEURT for the analysis as it correlates better with human evaluation [17]. ", "page_idx": 18}, {"type": "text", "text": "Qualitative examples of translation quality. See Table 11 for qualitative examples of finetuned ASL to English translation on How2Sign from our best model, compared to the prior state of the art set by Tanzer [43]. Our model resolves some small issues in (2) and (3) analyzed in Appendix C of Tanzer [43]; the remaining discrepancies in these particular examples seem to be issues with the dataset quality. ", "page_idx": 18}, {"type": "image", "img_path": "M80WgiO2Lb/tmp/62dc673a5b3b6f948de41963aa2ef1823f2fdb9ccd5612d82b4c71afac56cd7e.jpg", "img_caption": ["(a) Results for training with MT-Small and Aug-YT-ASL/Full-Small. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "M80WgiO2Lb/tmp/e3d9d9e9bd5936e45fb9094b4a2f9dc7f39fd170188e493e15bf8038d14fe719.jpg", "img_caption": ["(b) Results for training with MT-Large and Aug-YT-ASL/Full-Large. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 8: Per-language pretraining performance for SLT with augmented SLT data. We show BLEURT\u2191 results for Baseline $^+$ Augmented $S L T+M T$ with $p_{m t}=0.9$ on FLEURS test set. MT data are multilingual in both directions. Data augmentation substantially improves SLT performance across languages. ", "page_idx": 19}, {"type": "table", "img_path": "M80WgiO2Lb/tmp/51ac1b100c42b761d2f66faf95c86eeaf96faaaabd2a2919b8259443b759274a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "M80WgiO2Lb/tmp/4624a062e661674837209bd28be0d75b28727ba95fa2fa368965955704247ed3.jpg", "table_caption": ["(a) BLEU\u2191scores. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "M80WgiO2Lb/tmp/a2b07d1e784cf5868236b862a51c7831e9f34b85ec73b3a0b3dd09f0041cb675.jpg", "table_caption": ["(b) ChrF\u2191scores. ", "(c) BLEURT\u2191scores. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 8: Finetuning performance on downstream SLT benchmarks. \u201cH2S/E23\u201d: How2Sign/Elementary23. \u201cSRF/SS\u201d: WMT23 DSGS SRF/SS test split. \u201cAvg\u201d: averaged performance over all benchmarks. MT data are added in both translation directions. Previous SOTA: How2Sign [43], Elementary23 [48] and WMT23 SRF [28], WMT23 LIS-CH, LSF-CH, SS [44]. Scaling SLT reaches new SOTA across benchmarks. All models are finetuned on each SLT benchmark separately except (9). ", "page_idx": 20}, {"type": "table", "img_path": "M80WgiO2Lb/tmp/129dc0fcae2b2bc073e93fc06c68c3a2c5551d65867354d9badc61ec0389b2ba.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "M80WgiO2Lb/tmp/5d262d3f74a914736fced77e7fd8c3b7301db2a441f7a3caf4a0f8abc9051ce1.jpg", "table_caption": ["(a) BLEU\u2191scores. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "M80WgiO2Lb/tmp/6cd0725a4f0247cbea064e96ad5b4da8d2dc7da0d741d9dbd1bea81a73839493.jpg", "table_caption": ["(b) ChrF\u2191scores. "], "table_footnote": ["(c) BLEURT\u2191scores. "], "page_idx": 21}, {"type": "table", "img_path": "M80WgiO2Lb/tmp/49dac843070927f359881689b05e35c793203007923a8da84f471e443f44faf5.jpg", "table_caption": ["Table 9: Pretraining performance on downstream SLT benchmarks. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "M80WgiO2Lb/tmp/b4bd4a86728754d4156bb16b5b1f3950afd22be5632b198074d22c0b1b2409cb.jpg", "table_caption": ["(a) Language Accuracy: the accuracy of translations in the correct target language. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 10: Analysis for zero-shot SLT in Figure 3b. Higher language accuracy indicates less off-target translation, thus better quality; lower empty rate is better. ", "page_idx": 21}, {"type": "table", "img_path": "M80WgiO2Lb/tmp/997a75333e1dcc21f4deb2d5106655313ca850208c871ded26b11d9d2e45f59d.jpg", "table_caption": ["Table 11: Qualitative examples of finetuned sentence-level ASL to English translation results on How2Sign, instances originally selected by Tarr\u00e9s et al. [45]. We compare the reference, the prior SOTA Tanzer [43], and our best model. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We summarize our contributions in the introduction, which are supported by empirical results in the Section 4. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We discussed the limitations in Section 6 and the Ethics Statement. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our study builds on top of empirical experiments without theoretical results. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We elaborate the pretraining and finetuning algorithm in Section 2 with optimization and evaluation details in Section 3. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: ", "page_idx": 25}, {"type": "text", "text": "In terms of data: MADLAD-400 is publicly available, but our noisy YouTube dataset is not. However, we provide information about the crawling method, and the covered sign languages and data statistics. The results could also be reproduced directionally with YouTube-SL-25, which is a smaller but cleaner dataset. ", "page_idx": 25}, {"type": "text", "text": "In terms of code: The used $(\\mathrm{m}/\\mathrm{By})\\mathrm{T}5$ model checkpoints are publicly available, but the framework we used to finetune them with multimodal inputs has not been open sourced, so we are unable to release our code. However, the methodology is simple and we provide the details to replicate it. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide training and test details in Section A.1 ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: Due to high computational cost, we run SLT pretraining and finetuning once for each setting. To make sure the evaluation is reliable, we compare different methods with several metrics, including BLEU, ChrF and BLEURT. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide details for the computational resources in Section 3. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our work follows the ethics guidelines of NeurIPS 2024. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We discussed potential societal impacts in Section 6 ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We didn\u2019t release data or models from this study. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The used data and models are properly cited in this paper. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We didn\u2019t release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our study doesn\u2019t include crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our study doesn\u2019t include crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]