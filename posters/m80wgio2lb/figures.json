[{"figure_path": "M80WgiO2Lb/figures/figures_0_1.jpg", "caption": "Figure 1: BLEU scores on different benchmarks: our model sets new SOTA results across benchmarks and sign languages. Note we didn't show BLEURT because not all previous studies report BLEURT.", "description": "This figure compares the BLEU scores achieved by the proposed model and the previous state-of-the-art (SOTA) models on various sign language translation benchmarks. The benchmarks include How2Sign ASL, Elementary23 GSS, WMT23 LIS-CH, WMT23 LSF-CH, and WMT23 DSGS (both SRF and SS splits).  The radar chart visually represents the performance of both the proposed model and the SOTA on each benchmark. The significantly higher scores of the proposed model in the blue line compared to the previous SOTA in red show substantial improvements in translation quality across all benchmarks.  The absence of BLEURT scores is noted, as not all prior studies reported them.", "section": "1 Introduction"}, {"figure_path": "M80WgiO2Lb/figures/figures_1_1.jpg", "caption": "Figure 2: Illustration of model architecture and pretraining task for SLT. We perform large-scale pretraining and adopt multi-task learning at clip level (multiple captions) to better leverage the supervised knowledge.", "description": "This figure illustrates the encoder-decoder architecture used for sign language translation (SLT).  It highlights the different input components for various tasks during pretraining: SLT (sign language translation), alignment (aligning video clips with captions), machine translation (MT), and augmented SLT (data augmentation via off-the-shelf machine translation). Each task utilizes task-specific prompts, sign frames, and produces corresponding target outputs. The multi-task learning approach at the clip level (multiple captions) enhances the model's learning by leveraging supervised knowledge from various sources. The figure shows how the model takes as input the sign frames and a prompt specifying the task, and outputs the translated text.  The diagram shows the different data types used in the pretraining phase, which include sign language video frames, captions, and machine-translated text from other languages.", "section": "2 Sign Language Translation"}, {"figure_path": "M80WgiO2Lb/figures/figures_1_2.jpg", "caption": "Figure 2: Illustration of model architecture and pretraining task for SLT. We perform large-scale pretraining and adopt multi-task learning at clip level (multiple captions) to better leverage the supervised knowledge.", "description": "This figure illustrates the architecture of the encoder-decoder based sign language translation (SLT) model and the different pretraining tasks used.  The encoder takes as input sign frames and a prompt specifying the task (SLT, Machine Translation (MT), Alignment, or Augmented SLT). The decoder outputs the target text.  The figure also shows how video clips are segmented for training, focusing on using multiple captions per clip and their timestamps for more effective training.", "section": "2 Sign Language Translation"}, {"figure_path": "M80WgiO2Lb/figures/figures_5_1.jpg", "caption": "Figure 1: BLEU scores on different benchmarks: our model sets new SOTA results across benchmarks and sign languages. Note we didn't show BLEURT because not all previous studies report BLEURT.", "description": "This figure compares the BLEU scores achieved by the proposed model and the previous state-of-the-art (SOTA) models on several sign language translation benchmarks.  The benchmarks cover different sign languages (LIS-CH, LSF-CH, DSGS, ASL) and target languages. The results demonstrate that the proposed model significantly outperforms the SOTA on all benchmarks.", "section": "1 Introduction"}, {"figure_path": "M80WgiO2Lb/figures/figures_6_1.jpg", "caption": "Figure 1: BLEU scores on different benchmarks: our model sets new SOTA results across benchmarks and sign languages. Note we didn't show BLEURT because not all previous studies report BLEURT.", "description": "This figure presents a bar chart comparing the BLEU scores achieved by the proposed model against the previous state-of-the-art (SOTA) across various sign language translation benchmarks (WMT23 LIS-CH, Elementary23 GSS, WMT23 LSF-CH, How2Sign ASL, WMT23 DSGS (SRF), WMT23 DSGS (SS)). The chart visually demonstrates that the proposed model significantly outperforms the SOTA on all benchmarks, showcasing substantial improvements in sign language translation quality.  The absence of BLEURT scores is noted due to inconsistent reporting in prior research.", "section": "1 Introduction"}, {"figure_path": "M80WgiO2Lb/figures/figures_16_1.jpg", "caption": "Figure 1: BLEU scores on different benchmarks: our model sets new SOTA results across benchmarks and sign languages. Note we didn't show BLEURT because not all previous studies report BLEURT.", "description": "This figure presents the BLEU scores achieved by the proposed model and the previous state-of-the-art (SOTA) models on several sign language translation (SLT) benchmarks.  Each bar represents a different benchmark dataset, and the height of the bar shows the BLEU score.  The results demonstrate that the new model significantly outperforms the SOTA models on all benchmarks across different sign languages. BLEURT scores were not included because not all previous studies reported them.", "section": "1 Introduction"}, {"figure_path": "M80WgiO2Lb/figures/figures_17_1.jpg", "caption": "Figure 1: BLEU scores on different benchmarks: our model sets new SOTA results across benchmarks and sign languages. Note we didn't show BLEURT because not all previous studies report BLEURT.", "description": "This figure compares the BLEU scores achieved by the proposed model against the previous state-of-the-art (SOTA) models on several benchmark datasets for sign language translation.  Each bar represents a different benchmark dataset, and the height of the bar indicates the BLEU score.  The model significantly outperforms SOTA across all benchmarks. The figure highlights the substantial improvements achieved by the proposed approach, particularly in achieving new SOTA results on several benchmark datasets.", "section": "1 Introduction"}, {"figure_path": "M80WgiO2Lb/figures/figures_18_1.jpg", "caption": "Figure 3: Pretraining performance for Baseline + MT when varying MT languages. We show BLEURT\u2191 results on FLEURS-ASL#0, and set pmt = 0.5. Note MT languages are added separately instead of jointly. Results are for ByT5 Base. \"X\u2192En\": MT data for translation into English; \"X\u2194En\": MT data for both translation directions; \u201cAvg\u201d: average performance over languages. MT languages are arranged in descending order from left to right based on their training data quantity.", "description": "This figure displays the results of experiments that investigate the impact of adding different Machine Translation (MT) languages to the Baseline model during the pretraining phase. The experiments utilize the BLEURT metric to measure the performance of the model on the FLEURS-ASL#0 benchmark. The parameter pmt is set to 0.5, indicating that the SLT and MT tasks are given equal weight. Two scenarios are explored:\n\n1.  Adding MT data that translates only into English (X\u2192En).\n2.  Adding MT data that translates both into and from English (X\u2194En).\n\nThe results are presented for each MT language individually, alongside the average across languages. The order of MT languages shown on the x-axis is determined by the amount of training data used. The figure clearly demonstrates how different MT languages impact the performance of the SLT model, and how the directionality of MT data (X\u2192En versus X\u2194En) influences the results.", "section": "4.1 SLT Pretraining Results"}, {"figure_path": "M80WgiO2Lb/figures/figures_18_2.jpg", "caption": "Figure 3: Pretraining performance for Baseline + MT when varying MT languages. We show BLEURT\u2191 results on FLEURS-ASL#0, and set pmt = 0.5. Note MT languages are added separately instead of jointly. Results are for ByT5 Base. \"X\u2192En\": MT data for translation into English; \"X\u2194En\": MT data for both translation directions; \u201cAvg\u201d: average performance over languages. MT languages are arranged in descending order from left to right based on their training data quantity.", "description": "This figure displays the impact of adding various machine translation (MT) languages on the performance of a sign language translation (SLT) model. Two experiments are shown: one translating to English only and the other performing zero-shot translation to multiple languages. The y-axis represents the BLEURT score (higher is better), while the x-axis lists the MT languages included, ordered by the amount of training data.  The results demonstrate the effects of including multilingual MT data on SLT performance, highlighting that even adding a small amount of data from certain languages (e.g. Ja\u2192En) results in significant quality improvements.", "section": "4.1 SLT Pretraining Results"}, {"figure_path": "M80WgiO2Lb/figures/figures_19_1.jpg", "caption": "Figure 3: Pretraining performance for Baseline + MT when varying MT languages. We show BLEURT\u2191 results on FLEURS-ASL#0, and set pmt = 0.5. Note MT languages are added separately instead of jointly. Results are for ByT5 Base. \"X\u2192En\": MT data for translation into English; \"X\u2194En\": MT data for both translation directions; \u201cAvg\u201d: average performance over languages. MT languages are arranged in descending order from left to right based on their training data quantity.", "description": "This figure displays the results of experiments evaluating the impact of incorporating machine translation (MT) data from various languages into a sign language translation (SLT) pretraining process.  The results are measured using the BLEURT metric (higher is better).  Two main scenarios are considered: MT data for translation into English only (X\u2192En) and MT data for translation in both directions (X\u2194En). The figure shows that adding more MT languages generally improves the performance of the SLT model, although the effect varies by language.  The average performance across all added MT languages is also shown.", "section": "4.1 SLT Pretraining Results"}, {"figure_path": "M80WgiO2Lb/figures/figures_19_2.jpg", "caption": "Figure 3: Pretraining performance for Baseline + MT when varying MT languages. We show BLEURT\u2191 results on FLEURS-ASL#0, and set pmt = 0.5. Note MT languages are added separately instead of jointly. Results are for ByT5 Base. \"X\u2192En\": MT data for translation into English; \"X\u2194En\": MT data for both translation directions; \u201cAvg\u201d: average performance over languages. MT languages are arranged in descending order from left to right based on their training data quantity.", "description": "This figure displays the impact of adding Machine Translation (MT) data from various languages on the quality of sign language translation (SLT).  It shows BLEURT scores (a metric assessing translation quality) for two scenarios: adding MT data only translating to English (X\u2192En) and adding MT data for both translations to and from English (X\u2194En).  The chart illustrates that adding more MT data generally improves SLT performance, although the effect varies across languages.  The languages on the x-axis are ordered by the amount of training data available for them.", "section": "4.1 SLT Pretraining Results"}]