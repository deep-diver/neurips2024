[{"figure_path": "M80WgiO2Lb/tables/tables_4_1.jpg", "caption": "Table 1: Summary of downstream SLT benchmarks. \"#Train/#Dev/#Test\": the number of examples in the train, dev and test split. Note the sign language video and the target text in these benchmarks are often pre-segmented and aligned at sentence level. \"DGS/ASL/GSS\": German/American/Greek Sign Language; \"En/De/Fr/It\": English/German/French/Italian; \u201cLIS-CH\": Italian Sign Language of Switzerland; \"LSF-CH\": French Sign Language of Switzerland; \"DSGS\": Swiss German Sign Language.", "description": "This table presents a summary of six open-domain sign language translation (SLT) benchmarks used to evaluate the model's performance.  Each benchmark is described by the sign language, the target spoken language, and the number of training, development, and testing examples.  It highlights the datasets' characteristics: pre-segmented and aligned data at the sentence level, and the specific sign languages involved (ASL, GSS, LIS-CH, LSF-CH, DSGS).", "section": "Downstream Benchmarks, Evaluation and Model Setting"}, {"figure_path": "M80WgiO2Lb/tables/tables_5_1.jpg", "caption": "Table 2: Pretraining performance (BLEURT \u2191) for different sized (By/m)T5 models when pretrained on YT-ASL and YT-Full. Results are reported on the test set of How2Sign and FLEURS-ASL#0 (\u2192En, i.e. English as the target). Best results for each model family are highlighted in bold.", "description": "This table presents the BLEURT scores achieved by different sized T5 model families (T5, mT5, and ByT5) after pretraining on two different datasets: YT-ASL and YT-Full.  The evaluation is performed on two benchmarks: How2Sign and FLEURS-ASL#0 (with English as the target language).  The best performing model for each family is highlighted in bold, showcasing the impact of model size and training data on performance.", "section": "4.1 SLT Pretraining Results"}, {"figure_path": "M80WgiO2Lb/tables/tables_6_1.jpg", "caption": "Table 2: Pretraining performance (BLEURT \u2191) for different sized (By/m)T5 models when pretrained on YT-ASL and YT-Full. Results are reported on the test set of How2Sign and FLEURS-ASL#0 (\u2192En, i.e. English as the target). Best results for each model family are highlighted in bold.", "description": "This table presents the BLEURT scores achieved by different sized ByT5 and mT5 models after pretraining on two datasets: YT-ASL and YT-Full.  The evaluation was performed on the How2Sign and FLEURS-ASL#0 benchmark datasets, with English as the target language. The best performing model within each family (ByT5 and mT5) is highlighted in bold.  It demonstrates the impact of model size and data quantity on the pretraining performance.", "section": "4.1 SLT Pretraining Results"}, {"figure_path": "M80WgiO2Lb/tables/tables_7_1.jpg", "caption": "Table 2: Pretraining performance (BLEURT \u2191) for different sized (By/m)T5 models when pretrained on YT-ASL and YT-Full. Results are reported on the test set of How2Sign and FLEURS-ASL#0 (\u2192En, i.e. English as the target). Best results for each model family are highlighted in bold.", "description": "This table shows the BLEURT scores achieved by different sized models (T5, mT5, ByT5) pretrained on two different datasets (YT-ASL and YT-Full). The evaluation is performed on the How2Sign and FLEURS-ASL#0 test sets, with English as the target language. The best results for each model family are highlighted in bold, indicating the optimal model size for each dataset and model type.", "section": "4.1 SLT Pretraining Results"}, {"figure_path": "M80WgiO2Lb/tables/tables_8_1.jpg", "caption": "Table 5: Finetuning performance (BLEURT\u2191) on downstream SLT benchmarks. \"H2S/E23\": How2Sign/Elementary23. \"SRF/SS\": WMT23 DSGS SRF/SS test split. \"Avg\": averaged performance over all benchmarks. MT data are added in both translation directions. Previous SOTA: How2Sign [43], Elementary23 [48] and WMT23 SRF [28], WMT23 LIS-CH, LSF-CH, SS [44]. All models are finetuned on each SLT benchmark separately except (9).", "description": "This table presents the BLEURT scores achieved after fine-tuning various SLT models on six different downstream benchmarks.  It compares the performance of different model configurations (ByT5 base model, with YT-ASL data, with MT data, etc.) against the previous state-of-the-art (SOTA).  The benchmarks cover five different sign languages and include several different test set splits for evaluation. The final row shows that multilingual finetuning was also performed.", "section": "4.2 SLT Finetuning Results"}, {"figure_path": "M80WgiO2Lb/tables/tables_8_2.jpg", "caption": "Table 6: Spearman correlation between direct (i.e., pretraining) and finetuned SLT results under different metrics based on Tables 5 and 9. */* : significant at p < 0.05/0.01.", "description": "This table presents the Spearman's rank correlation coefficients between the pretraining and finetuning results for Sign Language Translation (SLT) using three different evaluation metrics: BLEU, ChrF, and BLEURT.  The correlation coefficients indicate the strength of the relationship between the performance of the models in the pretraining phase and their performance after finetuning.  The significance levels (p-values) are also provided, showing the statistical significance of the correlations.  A high correlation suggests that the pretraining results are a good indicator of finetuned performance.  The table helps to evaluate the effectiveness of the pretraining strategy for SLT and assess the predictive power of different evaluation metrics.", "section": "4 Experiments"}, {"figure_path": "M80WgiO2Lb/tables/tables_15_1.jpg", "caption": "Table 1: Summary of downstream SLT benchmarks. \"#Train/#Dev/#Test\": the number of examples in the train, dev and test split. Note the sign language video and the target text in these benchmarks are often pre-segmented and aligned at sentence level. \"DGS/ASL/GSS\": German/American/Greek Sign Language; \"En/De/Fr/It\": English/German/French/Italian; \u201cLIS-CH\": Italian Sign Language of Switzerland; \"LSF-CH\": French Sign Language of Switzerland; \"DSGS\": Swiss German Sign Language.", "description": "This table summarizes the six open-domain sign language translation (SLT) benchmarks used in the paper's experiments.  It provides details on the number of training, development, and test examples for each benchmark, indicating the sign language and target spoken language involved.  Abbreviations for sign languages and their corresponding spoken languages are also included for clarity.", "section": "Downstream Benchmarks, Evaluation and Model Setting"}, {"figure_path": "M80WgiO2Lb/tables/tables_16_1.jpg", "caption": "Table 1: Summary of downstream SLT benchmarks. \"#Train/#Dev/#Test\": the number of examples in the train, dev and test split. Note the sign language video and the target text in these benchmarks are often pre-segmented and aligned at sentence level. \"DGS/ASL/GSS\": German/American/Greek Sign Language; \"En/De/Fr/It\": English/German/French/Italian; \u201cLIS-CH\": Italian Sign Language of Switzerland; \"LSF-CH\": French Sign Language of Switzerland; \"DSGS\": Swiss German Sign Language.", "description": "This table summarizes six open-domain sign language translation (SLT) benchmarks used in the paper to evaluate the performance of their proposed SLT model.  It lists the sign language and target spoken language for each benchmark, along with the number of training, development, and test examples. The table also provides abbreviations for the sign languages (DGS, ASL, GSS) and spoken languages (En, De, Fr, It).", "section": "Downstream Benchmarks, Evaluation and Model Setting"}, {"figure_path": "M80WgiO2Lb/tables/tables_20_1.jpg", "caption": "Table 5: Finetuning performance (BLEURT\u2191) on downstream SLT benchmarks. \"H2S/E23\": How2Sign/Elementary23. \"SRF/SS\": WMT23 DSGS SRF/SS test split. \"Avg\": averaged performance over all benchmarks. MT data are added in both translation directions. Previous SOTA: How2Sign [43], Elementary23 [48] and WMT23 SRF [28], WMT23 LIS-CH, LSF-CH, SS [44]. All models are finetuned on each SLT benchmark separately except (9).", "description": "This table presents the BLEURT scores achieved after fine-tuning various SLT models on six different benchmarks (How2Sign, Elementary23, and four WMT23 tasks).  It compares the performance of the proposed model against previous state-of-the-art (SOTA) results.  The table showcases the impact of different pretraining techniques (adding YT-ASL, MT data, Augmented SLT data, and using larger ByT5 models) on the final finetuned model's performance. The final row demonstrates the impact of multilingual SLT tuning on the model's performance across multiple sign and spoken languages. ", "section": "4.2 SLT Finetuning Results"}, {"figure_path": "M80WgiO2Lb/tables/tables_20_2.jpg", "caption": "Table 5: Finetuning performance (BLEURT\u2191) on downstream SLT benchmarks. \"H2S/E23\": How2Sign/Elementary23. \"SRF/SS\": WMT23 DSGS SRF/SS test split. \"Avg\": averaged performance over all benchmarks. MT data are added in both translation directions. Previous SOTA: How2Sign [43], Elementary23 [48] and WMT23 SRF [28], WMT23 LIS-CH, LSF-CH, SS [44]. All models are finetuned on each SLT benchmark separately except (9).", "description": "This table presents the BLEURT scores achieved after finetuning various SLT models on six different downstream benchmarks.  It shows the performance improvements gained by incorporating different pretraining techniques (e.g., adding YouTube data, multilingual machine translation data, augmented SLT data) and using larger models.  The table also includes a comparison to the previous state-of-the-art (SOTA) results, highlighting the substantial improvements achieved by the proposed method.  Note that the last row shows the performance when multilingual tuning is applied.", "section": "4.2 SLT Finetuning Results"}, {"figure_path": "M80WgiO2Lb/tables/tables_20_3.jpg", "caption": "Table 5: Finetuning performance (BLEURT\u2191) on downstream SLT benchmarks. \"H2S/E23\": How2Sign/Elementary23. \"SRF/SS\": WMT23 DSGS SRF/SS test split. \"Avg\": averaged performance over all benchmarks. MT data are added in both translation directions. Previous SOTA: How2Sign [43], Elementary23 [48] and WMT23 SRF [28], WMT23 LIS-CH, LSF-CH, SS [44]. All models are finetuned on each SLT benchmark separately except (9).", "description": "This table presents the BLEURT scores achieved after fine-tuning various SLT models on six different benchmarks.  It shows a comparison of performance improvements obtained by incorporating different pretraining data (YT-ASL, YT-Full, MT-Small, MT-Large, augmented SLT data), model sizes (ByT5 Base, ByT5 XL), and multilingual SLT tuning. The table highlights the significant improvement in performance achieved by the proposed model compared to previous state-of-the-art (SOTA) results.", "section": "4.2 SLT Finetuning Results"}, {"figure_path": "M80WgiO2Lb/tables/tables_21_1.jpg", "caption": "Table 9: Pretraining performance on downstream SLT benchmarks.", "description": "This table presents the results of the pretraining phase of the Sign Language Translation (SLT) experiments.  It shows the BLEU, ChrF, and BLEURT scores for various models across different benchmarks, namely How2Sign (H2S), Elementary23 (E23), and WMT23. The models evaluated include a ByT5 Base model and several variations with added data (YT-ASL, YT-Full, MT-Small), data augmentation (Aug-YT-ASL-Small), and larger model sizes (ByT5 XL).  The table allows a comparison of performance across different model configurations and datasets to determine the impact of these factors on SLT pretraining.", "section": "4.1 SLT Pretraining Results"}, {"figure_path": "M80WgiO2Lb/tables/tables_21_2.jpg", "caption": "Table 8: Finetuning performance on downstream SLT benchmarks. \"H2S/E23\": How2Sign/Elementary23. \"SRF/SS\": WMT23 DSGS SRF/SS test split. \"Avg\": averaged performance over all benchmarks. MT data are added in both translation directions. Previous SOTA: How2Sign [43], Elementary23 [48] and WMT23 SRF [28], WMT23 LIS-CH, LSF-CH, SS [44]. Scaling SLT reaches new SOTA across benchmarks. All models are finetuned on each SLT benchmark separately except (9).", "description": "This table presents the BLEU, chrF, and BLEURT scores achieved by different models on several downstream SLT benchmarks. The models are trained using varying configurations of pretraining data, model sizes, and language scaling techniques.  The results show substantial improvements over previous state-of-the-art (SOTA) models, particularly after finetuning. Note that the model achieves new SOTA after multilingual SLT tuning.", "section": "4.2 SLT Finetuning Results"}, {"figure_path": "M80WgiO2Lb/tables/tables_21_3.jpg", "caption": "Table 2: Pretraining performance (BLEURT \u2191) for different sized (By/m)T5 models when pretrained on YT-ASL and YT-Full. Results are reported on the test set of How2Sign and FLEURS-ASL#0 (\u2192En, i.e. English as the target). Best results for each model family are highlighted in bold.", "description": "This table presents the BLEURT scores achieved by different sized T5, mT5, and ByT5 models after pretraining on two different datasets: YT-ASL and YT-Full.  The evaluation is performed on the How2Sign and FLEURS-ASL#0 benchmarks, with English as the target language. The best performing model for each family is highlighted.", "section": "4.1 SLT Pretraining Results"}, {"figure_path": "M80WgiO2Lb/tables/tables_21_4.jpg", "caption": "Table 2: Pretraining performance (BLEURT \u2191) for different sized (By/m)T5 models when pretrained on YT-ASL and YT-Full. Results are reported on the test set of How2Sign and FLEURS-ASL#0 (\u2192En, i.e. English as the target). Best results for each model family are highlighted in bold.", "description": "This table presents the BLEURT scores achieved by different sized ByT5 and mT5 models after pretraining on two different datasets: YT-ASL and YT-Full. The evaluation is performed on the How2Sign and FLEURS-ASL#0 test sets, with English as the target language.  The bold values indicate the best performance within each model family.", "section": "4.1 SLT Pretraining Results"}, {"figure_path": "M80WgiO2Lb/tables/tables_21_5.jpg", "caption": "Table 10: Analysis for zero-shot SLT in Figure 3b. Higher language accuracy indicates less off-target translation, thus better quality; lower empty rate is better.", "description": "This table shows the language accuracy and empty rate for zero-shot SLT (Sign Language Translation) with and without multilingual machine translation (MT) data.  The results are broken down by target language and show that adding MT data generally improves the translation language accuracy, particularly for languages geographically close to English. However, there are also exceptions, particularly for languages with limited parallel MT data, such as Hindi and Chinese.", "section": "4.1 SLT Pretraining Results"}, {"figure_path": "M80WgiO2Lb/tables/tables_22_1.jpg", "caption": "Table 2: Pretraining performance (BLEURT \u2191) for different sized (By/m)T5 models when pretrained on YT-ASL and YT-Full. Results are reported on the test set of How2Sign and FLEURS-ASL#0 (\u2192En, i.e. English as the target). Best results for each model family are highlighted in bold.", "description": "This table presents the BLEURT scores achieved by different sized T5, mT5, and ByT5 models after being pretrained on two different datasets, YT-ASL and YT-Full.  The evaluation is performed on the How2Sign and FLEURS-ASL#0 benchmarks, with English as the target language.  Bold values indicate the best performance for each model family.", "section": "4.1 SLT Pretraining Results"}]