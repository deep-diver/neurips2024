{"references": [{"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-12-01", "reason": "This paper introduces denoising diffusion probabilistic models, a foundational model for many image generation and inpainting tasks, directly influencing the core methodology of the PrefPaint model."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces the CLIP model, which is used in PrefPaint as the backbone for the reward model, enabling the alignment of image inpainting results with human aesthetic preferences."}, {"fullname_first_author": "Yasin Abbasi-Yadkori", "paper_title": "Improved algorithms for linear stochastic bandits", "publication_date": "2011-01-01", "reason": "This paper provides the theoretical foundation for bounding the reward model error, a crucial element in the reinforcement learning framework used by PrefPaint to ensure the reliable alignment with human preferences."}, {"fullname_first_author": "John Schulman", "paper_title": "Trust region policy optimization", "publication_date": "2015-07-01", "reason": "This paper introduces the trust region policy optimization algorithm, which inspired the reward trustiness-aware alignment process in PrefPaint, improving efficiency and robustness of the reinforcement learning process."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-07-01", "reason": "This work details the Proximal Policy Optimization algorithm, a crucial component of the reinforcement learning strategy in PrefPaint, enhancing the stability and efficiency of the training process for better alignment with human preferences."}]}