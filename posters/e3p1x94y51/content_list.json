[{"type": "text", "text": "SemFlow: Binding Semantic Segmentation and Image Synthesis via Rectified Flow ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chaoyang Wang1 Xiangtai $\\mathbf{Li}^{1}$ Lu Qi2 Henghui Ding3 Yunhai Tong1 Ming-Hsuan Yang2 ", "page_idx": 0}, {"type": "text", "text": "1School of Intelligence Science and Technology, Peking University 2UC, Merced 3Institute of Big Data, Fudan University   \nProject page: https://wang-chaoyang.github.io/project/semflow   \ncywang $@$ stu.pku.edu.cn, qqlu1992 $@$ gmail.com, xiangtai94 $@$ gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Semantic segmentation and semantic image synthesis are two representative tasks in visual perception and generation. While existing methods consider them as two distinct tasks, we propose a unified framework (SemFlow) and model them as a pair of reverse problems. Specifically, motivated by rectified flow theory, we train an ordinary differential equation (ODE) model to transport between the distributions of real images and semantic masks. As the training object is symmetric, samples belonging to the two distributions, images and semantic masks, can be effortlessly transferred reversibly. For semantic segmentation, our approach solves the contradiction between the randomness of diffusion outputs and the uniqueness of segmentation results. For image synthesis, we propose a finite perturbation approach to enhance the diversity of generated results without changing the semantic categories. Experiments show that our SemFlow achieves competitive results on semantic segmentation and semantic image synthesis tasks. We hope this simple framework will motivate people to rethink the unification of low-level and high-level vision. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Understanding semantic content and creating images from semantic conditions are fundamental research topics in computer vision. Semantic segmentation [45; 7; 59; 85; 11; 10] and image synthesis [8; 32; 28; 72; 90] are two representative dense prediction tasks and inspires various downstream applications, including autonomous driving [5] and medical image analysis [89]. The former aims to assign a category label to each pixel in the image, while the latter aims to generate realistic images given semantic layouts. ", "page_idx": 0}, {"type": "text", "text": "Although semantic segmentation and image synthesis constitute a pair of reverse problems, existing works typically solve them using two distinct methodologies. On the one hand, segmentation models mostly follow the spirit of discriminative models. Specifically, a pre-trained backbone is employed to extract multi-scale features, and then task-specific decoders are used for dense prediction. On the other hand, semantic image synthesis frameworks are mainly built upon generative adversarial networks (GAN) [19; 52; 57; 62] or diffusion models (DM) [63; 64; 25]. GAN-based methods [90; 72; 28] typically take semantic layouts as inputs and adopt a discriminator for adversarial training. Meanwhile, DM-based methods [36] generate from noise with semantic layouts functioning as control signals. ", "page_idx": 0}, {"type": "text", "text": "An intuitive solution is to represent the segmentation mask as a colormap and model it as a conditional image generation task [56]. However, there are several implementation challenges. Overall, most of the discriminative segmentation models do not apply to this problem due to their irreversibility. For generative adversarial networks, their generators are typically unidirectional. Jointly training multiple unidirectional models to achieve bidirectional generation [90] is not the concern of this paper. Latent diffusion models (LDMs) have recently demonstrated great potential in generative tasks. Beyond generative tasks, several works [56; 69] attempt to apply diffusion models for segmentation, with images functioning as conditions, but their applicable tasks are limited to be class-agnostic. There are three main problems for existing LDM-based segmentation frameworks: 1) The contradiction between the randomness of generation outputs and the certainty of segmentation labels. 2) The huge cost brought by multiple inference steps. 3) The irreversibility between semantic masks and images. ", "page_idx": 0}, {"type": "image", "img_path": "E3P1X94Y51/tmp/a77a810f9200050b68fd1ca153bc67c984294c86de860c24e579500c1cff3f48.jpg", "img_caption": ["Figure 1: Rectified flow bridges semantic segmentation (SS) and semantic image synthesis (SIS). SS and SIS are modeled as a pair of transportation problems between the distributions of images and masks. They share the same ODE and only differ in the direction of the velocity field. We propose a finite perturbation operation on the mask to enable multi-modal generation without changing the semantic labels. Grey dots represent data samples. Colored dots represent semantic centroids, also known as anchors in Eq. 7. Colored bubbles represent the scale of perturbation. ", "Reverse Flow ODE (Semantic Image Synthesis) "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we use rectified flow [40; 42; 17] to enable LDM as a unified framework for semantic segmentation and semantic image synthesis. Our key idea is summarized in Fig. 1. Starting from an LDM [58] framework, we solve the above problems with three methodologies. First, we redefine the mapping function to address the randomness. Previous works [56] aim to learn the mapping from the joint distribution of Gaussian noise and images to the segmentation masks. We argue that Gaussian noise in this mapping function is redundant and negatively affects the determinism of semantic segmentation results. Instead, our model directly learns the mapping from images to masks. Secondly, we make the mapping reversible via rectified flow. Rectified flow is an ordinary differential equation (ODE) framework with a time-symmetric training object. This feature allows the model to be trained once to obtain bi-directional transmission capabilities and can be solved numerically using simple ODE solvers such as Euler. Finally, we propose a finite perturbation method to enable multi-modal image synthesis, as the mapping is one-to-one in semantic segmentation but one-to-many in semantic synthesis. We add amplitude-limited noise, enabling masks to be sampled from a collection of semantic-invariant distributions rather than a fixed value, and further improving the quality of synthesized results. Moreover, our model needs fewer inference steps than traditional diffusion models because the transport trajectory is straight, significantly reducing the gap between LDM and traditional discriminative models in segmentation. ", "page_idx": 1}, {"type": "text", "text": "The main contributions are as follows: 1) We introduce SemFlow, a new unified framework that binds semantic segmentation and image synthesis with rectified flow, effectively leveraging the length of the generative models. 2) We propose specialized designs of SemFlow, including pseudo mask modeling, bi-directional training of segmentation and generation, and a finite perturbation strategy. 3) We validate SemFlow on several popular benchmarks. For semantic segmentation, SemFlow dramatically narrows the gap between diffusion models and discriminative models in terms of accuracy and inference speed with a more elegant framework. Meanwhile, SemFlow also performs decently in semantic image synthesis tasks. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Diffusion Models and Rectified Flow. Diffusion models [25; 63; 64] have shown impressive results in the field of generation, such as image generation [58; 55; 83; 27; 13; 54], video generation [80], image editing [4; 46; 60; 51; 23], image super resolution [61; 26] and point cloud [47; 50; 88; 82; 48]. Most of these methods are based on stochastic differential equations (SDEs) and need multiple steps for generation. Recently, some works [40; 39; 38; 1; 22] propose to model with probability flow ordinary differential equations (ODEs) to reduce the inference steps. Specifically, rectified flow [40; 42] defines the forward process as straight paths and uses reflow to minimize the sampling steps to one. Although rectified flow has demonstrated decent results on image generation and image transfer tasks, they are limited to low-level vision and lack an exploration of the unification of segmentation and generation tasks. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Semantic Segmentation is one of the core tasks in visual perception, aiming to assign each pixel of the given image a semantic category. Previous semantic segmentation approaches are typically built upon discriminative modeling, consisting of a strong backbone [21; 16; 43; 44; 33] for feature extraction and a task-specific decoder head [7; 59; 85; 11; 10; 87; 65; 81; 15; 14] for mask prediction. Recently, some works [34; 86; 18; 77; 75; 3; 70; 9; 20; 2; 29; 76; 71] exploit diffusion models for segmentation. They typically follow the spirit of discriminative models and employ the diffusion model as a feature extractor. Although UniGS [56] and LDMSeg [69] attempt to use a plain Stable Diffusion framework for segmentation, their applicable tasks are limited to be class-agnostic. In practice, reconciling the stochastic outputs of diffusion models with the deterministic results of semantic segmentation is difficult. We rethink this problem and re-model it with rectified flow. ", "page_idx": 2}, {"type": "text", "text": "Semantic Image Synthesis is the reverse problem of semantic segmentation, aiming to generate realistic images given semantic layouts [8; 53; 41; 74; 91; 66; 67; 79; 32; 35; 49]. Several studies have delved into semantic synthesis through two methodologies. One methodology [90; 28; 72; 6; 68] is based on GAN and trained with adversarial loss and reconstruction loss. However, some of these methods can only generate unimodal outputs. Although some methods [67; 92] have been developed to address the diversity issues, GAN-based frameworks typically suffer from unstable training and need careful parameter tuning. Another methodology [73] employs diffusion models and regards it as a conditional image generation task, where semantic masks function as control signals. Some works further add a greater variety of control signals to enhance the consistency of synthesized results, like textual prompts [78] and bounding box [36]. Despite these methods achieves good synthetic results, their architecture is usually asymmetric, and the generator is usually unidirectional. This hinders the exploration of the unification of semantic segmentation and image synthesis. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first review diffusion models and the differential equations of diffusion-based segmentation models (DSMs) and then analyze the disadvantages of existing approaches. Afterward, we propose our SemFlow, which is inspired by rectified flow theory. It solves the randomness problem in the existing DSM and unifies semantic segmentation and image synthesis with one model. We will elaborate on each phase of our method. ", "page_idx": 2}, {"type": "text", "text": "3.1 Diffusion Model and Segmentation Modeling. ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models are a class of likelihood-based models that define a Markov chain of forward and backward processes. In the forward process, the noise is gradually added to the real data $x_{0}$ to form the noisy data $x_{t}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nq(x_{t}|x_{0})=\\mathcal{N}(x_{t};\\sqrt{\\alpha_{t}}x_{0},(1-\\alpha_{t})I),\\quad t\\in[0..T],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\alpha_{t}$ is functions of $t$ . During training, the model $\\epsilon_{\\theta}$ learns to estimate the noise by minimizing the objective with L2 loss: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathbb{E}_{{x_{0}}\\sim q(x_{0}),\\epsilon\\sim\\mathcal{N}(0,I),t}\\left[\\left|\\left|\\epsilon_{\\theta}(\\sqrt{\\alpha_{t}}x_{0}+\\sqrt{1-\\alpha_{t}}\\epsilon_{t})-\\epsilon_{t}\\right|\\right|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In the backward process, the model starts from Gaussian noise and gradually denoises to generate realistic data [63] via, ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{t-1}=\\sqrt{\\alpha_{t-1}}\\left(\\frac{x_{t}-\\sqrt{1-\\alpha_{t}}\\epsilon_{\\theta}(x_{t},t)}{\\sqrt{\\alpha_{t}}}\\right)+\\sqrt{1-\\alpha_{t-1}-\\sigma_{t}^{2}}\\epsilon_{\\theta}(x_{t},t)+\\sigma_{t}\\epsilon_{t}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Luckily, Eq. 3 provides a unified solution for DDPM and DDIM, where the former belongs to an SDE modeling and the latter is an ODE modeling. We parameterized it as ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{0}=f(x_{T},\\sigma),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\sigma$ controls the way of modeling: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sigma_{t}={\\left\\{\\begin{array}{l l}{{\\sqrt{(1-\\alpha_{t-1})(1-\\alpha_{t})}}{\\sqrt{1-\\alpha_{t}/\\alpha_{t-1}}}}&{{\\mathrm{DDPM}}}\\\\ {0}&{{\\mathrm{DDIM}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the image generation task, $x_{0}$ represents the real images, and $x_{T}$ represents the Gaussian noise. We extend the boundaries of Eq. 4 and apply it to semantic segmentation problems. An intuitive way is to encode the segmentation mask into colormaps and model segmentation as a conditional image generation task. Given image $I$ , we rewrite the projection $f$ into a formulation of conditional generation, ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{0}=f(x_{T},\\sigma,I).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Eq. 6 formulates the existing diffusion-based segmentation models with generative modeling. However, this modeling approach suffers from the following problems: 1) The randomness of initial noise and the determinism of the segmentation mask are contradictory. 2) From a transmission point of view, the transfer from noise to the segmentation mask does not conform to the paradigm of semantic segmentation task, where noise is redundant. 3) The images in this approach only serve as the condition, which is non-causal in reverse transfer. Thus, the reversed transfer is a separate problem. ", "page_idx": 3}, {"type": "text", "text": "3.2 Unify Segmentation and Synthesis with Rectified Flow ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Task-agnostic Framework. We employ the standard Stable Diffusion [58] (SD) framework for our task without any task-specific decoder head or well-designed text prompts. To this end, we design the network architecture using the following three steps. First, we convert the semantic segmentation masks to 3-channel pseudo mask $M=(m_{0},m_{1},m_{2})$ to align with the images $I$ . Assume that the valid region [0, 255] (for real images) is divided into $k$ parts with a spacing of $s$ , the pseudo mask corresponding to category index $c$ is formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nm_{0}^{\\prime}=[c/k^{2}],\\ m_{1}^{\\prime}=\\lfloor(c-m_{0}^{\\prime}*k^{2})/k\\rfloor,\\ m_{2}^{\\prime}=c-m_{0}^{\\prime}*k^{2}-m_{1}^{\\prime}*k,\\ M=s*(m_{0}^{\\prime},m_{1}^{\\prime},m_{2}^{\\prime}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $s*(k-1)<255$ and $\\lfloor\\cdot\\rfloor$ means the floor operator. The $(m_{0}^{i},m_{1}^{i},m_{2}^{i})$ are called anchors. ", "page_idx": 3}, {"type": "text", "text": "After the transformation, we adopt a VAE encoder $\\mathcal{E}$ to compress the images and pseudo masks into the latent space. The corresponding VAE decoder $\\mathcal{D}$ restores the latent variables to the pixel space. ", "page_idx": 3}, {"type": "equation", "text": "$$\nz_{0}=\\mathcal{E}(I),\\quad z_{1}=\\mathcal{E}(M),\\quad\\tilde{I}=\\mathcal{D}(\\tilde{z_{0}}),\\quad\\tilde{M}=\\mathcal{D}(\\tilde{z_{1}}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tilde{z}$ means the output of the UNet. ", "page_idx": 3}, {"type": "text", "text": "Discussion. Although previous works [69] argue that segmentation masks are lower in entropy and specifically re-train a lighter network, we still adopt the off-the-shelf VAE from SD. The reasons are as follows: 1) The number of parameters in VAE is negligible compared to the UNet (84M vs. 860M). 2) The VAE specifically trained for segmentation masks can not be applied to images, thus destroying the bi-directional transportation. 3) Specific training creates spatial priors, such as clustering, which hinders our exploration of LDM\u2019s segmentation capability itself. ", "page_idx": 3}, {"type": "text", "text": "Note that we do not use image captions or image features as prompts. As our model unifies semantic segmentation and image synthesis with one model, the usage of captions contradicts the definition of semantic segmentation task while the features are non-causal for image synthesis. Thus, in this work, we set the prompt as empty. ", "page_idx": 3}, {"type": "text", "text": "Bi-directional Training and Inference. Contrary to the conventional approach, we propose modeling the segmentation task with rectified flow. It is an ODE framework that aims to learn the mapping between two distributions through straight trajectories. ", "page_idx": 3}, {"type": "text", "text": "Denote $\\pi_{0}$ and $\\pi_{1}$ represent the distribution of the latent variables of images and masks, respectively. Denote $z_{0}\\sim\\pi_{0}$ and $z_{1}\\sim\\pi_{1}$ , the trajectory from $z_{0}$ to $z_{1}$ can be formulated as $z_{t}=\\varphi_{t}(z_{0},z_{1})$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}z_{t}}{\\mathrm{d}t}=\\frac{\\partial\\varphi_{t}(z_{0},z_{1})}{\\partial t}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "When $\\varphi$ is the trajectory of rectified flow [40], $z_{t}$ can be reformulated as the linear interpolation process between $z_{0}$ and $z_{1}$ as $z_{t}=(1-t)z_{0}+t z_{1}$ . We aim to learn the velocity field using neural ", "page_idx": 3}, {"type": "text", "text": "networks $v_{\\theta}(z_{t},t)$ and solve it with optimization methods, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\mathcal{L}}=\\int_{0}^{1}\\mathbb{E}_{(z_{0},z_{1})\\sim\\gamma}\\left[\\left|\\left|v_{\\theta}(z_{t},t)-\\frac{\\partial\\varphi_{t}(z_{0},z_{1})}{\\partial t}\\right|\\right|^{2}\\right]\\mathrm{d}t,}\\\\ {\\displaystyle=\\int_{0}^{1}\\mathbb{E}_{(z_{0},z_{1})\\sim\\gamma}\\left[||v_{\\theta}(z_{t},t)-(z_{1}-z_{0})||^{2}\\right]\\mathrm{d}t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\gamma$ indicates the coupling of images and their corresponding pseudo masks. ", "page_idx": 4}, {"type": "text", "text": "Eq. 10 is our training loss. Upon training completed, the transfer from $z_{0}$ to $z_{1}$ can be described via an ODE: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}z_{t}}{\\mathrm{d}t}=v_{\\theta}(z_{t},t),\\quad t\\in[0,1].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "So far, we have constructed a mapping from the distribution of images to that of masks. Compared with DSMs in Eq. 6, our approach avoids the interference of randomness, enabling the application of diffusion models to semantic segmentation tasks. ", "page_idx": 4}, {"type": "text", "text": "Moreover, Eq. 10 has a time-symmetric form, which results in an equivalent problem by exchanging $z_{0}$ and $z_{1}$ and filpping the sign of $v_{\\theta}$ . Interestingly, the transportation problem from $\\pi_{1}$ to $\\pi_{0}$ indicates the semantic image synthesis task. This means that semantic segmentation and semantic image synthesis essentially become a pair of mutually reverse problems that share the same ODE (Eq. 11) and have solutions with opposite signs. ", "page_idx": 4}, {"type": "text", "text": "In the inference stage, we can obtain the approximate results by numerical solvers like the forward Euler method, which can be formulated as follows, ", "page_idx": 4}, {"type": "text", "text": "Semantic segmentation is regarded as the transportation from $z_{0}$ to $z_{1}$ , which we call forward flow. The ODE is Eq. 11 and the numerical solution is, ", "page_idx": 4}, {"type": "equation", "text": "$$\nz_{t+\\frac{1}{N}}=z_{t}+\\frac{1}{N}v_{\\theta}(z_{t},t),\\quad t\\in\\{0,1,...,N-1\\}/N.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "After $\\tilde{M}$ is restored with Eq. 8, we calculate the L2 distance between $\\tilde{M}$ and anchors and obtain the segmentation mask. ", "page_idx": 4}, {"type": "text", "text": "Correspondingly, the semantic image synthesis task is considered to transfer in a reverse direction, which we call the reverse flow. Specifically, this transfer can be described as $\\begin{array}{r}{\\frac{\\mathrm{d}z_{t}}{\\mathrm{d}t}=-v_{\\theta}(z_{t},t)}\\end{array}$ , and the solution is, ", "page_idx": 4}, {"type": "equation", "text": "$$\nz_{t-\\frac{1}{N}}=z_{t}-\\frac{1}{N}v_{\\theta}(z_{t},t),\\quad t\\in\\{N,N-1,...,1\\}/N.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.3 Finite Perturbation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In Eq. 10, the model is configured to learn the one-to-one mapping between images and masks. However, assigning a fixed mask to each image brings about several problems. First, we find that constant $z_{1}$ in Eq. 13 hinders the multi-modal generation for the image synthesis task. Moreover, the pseudo masks are low in entropy. We hypothesize that the low-entropy distribution of masks hinders the training process and may finally spoil the quality of synthesis results. ", "page_idx": 4}, {"type": "text", "text": "To this end, we propose to add finite perturbation on the pseudo masks. Specifically, given pseudo masks $M$ which has a spacing of $s$ , we add a noise with a limited amplitude on $M$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\nM^{\\prime}=M+\\epsilon,\\quad\\epsilon\\sim\\mathrm{U}(-\\beta,\\beta),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathrm{U}$ is a uniform distribution. We set $0<\\beta<s/2$ to ensure that the semantic label of each pixel does not change. Therefore, we propose to replace $z_{1}$ with $z_{1}^{\\prime}=\\mathcal{E}(M^{\\prime})$ in Eq. 10 and Eq. 13. We show the effectiveness of this design in Sec. 4.3. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Experimental Settings ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Dataset and Metrics. We study SemFlow using three popular datasets: COCO-Stuff [37], CelebAMask-HQ [30], and Cityscapes [12]. They contain 171, 19, and 19 categories, respectively. For semantic segmentation, we evaluate with mean intersection over union (mIoU). For semantic image synthesis, we assess with the Fr\u00e9chet inception distance (FID) [24] and learned perceptual image patch similarity (LPIPS) [84]. ", "page_idx": 4}, {"type": "table", "img_path": "E3P1X94Y51/tmp/a0ae5441ba62ef1cf6655f048c09d54a173327fb322801a77bd36f49a1d7bb2d.jpg", "table_caption": ["Table 1: Semantic segmentation results on COCO-Stuff dataset. SS and SIS represents semantic segmentation and semantic image synthesis, respectively. Sampler-N means the usage of a specific sampler with N inference steps. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Implementation Details. The SemFlow model is built upon Stable Diffusion UNet and initialized with weights from the pre-trained SD 1.5. Images and semantic masks in COCO-Stuff, CelebAMaskHQ, and Cityscapes are resized and cropped into $512\\times512$ , $512\\times512$ , and $512\\times1024$ , respectively. We use the off-the-shelf VAE of the corresponding Stable Diffusion model. The spacing $s$ is set as 50, and the division coefficient $k$ is 6. The amplitude of perturbation is 6. Note that there is no need to train two models for segmentation and synthesis separately since the optimization target in Eq. 10 is time-symmetric. ", "page_idx": 5}, {"type": "text", "text": "Baselines. We seek to unify semantic segmentation (SS) and semantic image synthesis (SIS) into a pair of reverse problems and train one model for a solution. As few previous works achieve this goal, we thus take different baselines for SS and SIS, respectively. For SS, we design two baseline models, DSMs [58], which follow the principle of diffusion-based conditional generation modeling in Sec. 3.1. The network structure and hyperparameters of DSM follow SemFlow, except that the inputs of UNet are eight channels. Specifically, the noise and images are concatenated in the channel dimension. DSM-DDIM and DSM-DDPM differ in differential equation modeling (ODE vs. SDE). For SIS, we take pix2pixHD [72], SPADE [53], SC-GAN [74], BBDM [31] and CycleGAN [90] as the baselines. ", "page_idx": 5}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Semantic Segmentation. Fig. 2 shows the qualitative comparison between SemFlow and DSMs. Our SemFlow demonstrates satisfactory performance in a range of scenarios and exhibits high accuracy in classifying the semantic labels of targets. In contrast, DSM fails in semantic segmentation tasks, regardless of SDE or ODE modeling. First, DSM is inferior to SemFlow in discriminating different semantic categories. Moreover, the outputs of DSM-DDIM and DSM-DDPM change dramatically with different random seeds. As discussed in Sec. 3.1, DSM is susceptible to the randomness inherent in diffusion models, making it unable to produce deterministic results. ", "page_idx": 5}, {"type": "text", "text": "Tab. 1 shows the quantitative results on the COCO-Stuff dataset. We compare SemFlow with two variants of DSM and MaskFormer, which is a classical discriminative segmentation model. Regarding accuracy, our SemFlow achieves 38.6 mIoU and outperforms DSMs by a remarkable margin. Moreover, SemFlow only uses a simple sampler, forward Euler method, and fewer inference steps than DSMs. Further reducing the inference steps brings about faster generation but witnesses a slight drop in performance, which is analyzed in Sec. 4.3. ", "page_idx": 5}, {"type": "text", "text": "Semantic Image Synthesis. We compare our approach with other specialist models on semantic image synthesis tasks in Tab. 1. On CelebAMask-HQ, SemFlow achieves a performance of 32.6 FID and 0.393 LPIPS. ", "page_idx": 5}, {"type": "text", "text": "Beyond specialist models, we also qualitatively compare our approach with CycleGAN [90] in Fig. 3 to demonstrate the overall performance on SS and SIS tasks. All results of SemFlow are generated with one model. In other words, the ODE modeling of SS and SIS share the same velocity field and only differ in the sign. The first and third rows show the image synthesis results given semantic layouts, while the second and fourth row shows the segmentation results. Our synthesis and segmentation results are inspiring and significantly outperform CycleGAN. Note that CycleGAN essentially trains two unidirectional generators while we employ only one model for the two tasks. ", "page_idx": 5}, {"type": "image", "img_path": "E3P1X94Y51/tmp/365c89c788331dcbf7ffe5c7b1f17b28d5c4fc992781d7813b279ef717dbfec5.jpg", "img_caption": ["Figure 2: Semantic segmentation results on COCO-Stuff dataset. For the ground truth, each color reflects the value of anchors (Eq. 7), which corresponds to one semantic category, and the color white indicates the ignored regions. The predictions of DSM vary considerably under different random seeds. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "E3P1X94Y51/tmp/68563262766bca60ef662ff972d05c1e78f0fdad210cf36fd9dbe3cf039589bd.jpg", "img_caption": ["Figure 3: Semantic segmentation and semantic image synthesis results on Cityscapes dataset. The color black in the ground truth indicates the ignored region. The segmentation results of SemFlow are colored following [12]. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "E3P1X94Y51/tmp/00c6ad4700e86853b39f2dd60d89dcfa0b1ea328008ab37af1a274924bcac872.jpg", "img_caption": ["Figure 4: Semantic image synthesis results on CelebAMask-HQ dataset. Semantic masks are colored to show different semantic components. SemFlow w/ Perturbation indicates the finite perturbation operation in Eq. 14. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Discussion. Although the performance gap exists compared with specialist models, our approach solves the two major issues of diffusion-based segmentation approaches: 1) Randomness of outputs and 2) Large inference steps. We believe it significantly reduces the gap between discriminative models and diffusion models. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Perturbation. Fig. 4 shows the synthesized results of SemFlow on the CelebAMask-HQ dataset and demonstrates the importance of perturbation operation. First, perturbation enables our approach to generate multi-modal results from one semantic layout using different noises. The synthesized results show diverse appearances, such as skin, hair color, and the background, and exhibit good consistency with the masks. Moreover, we also show the synthesis results of the model without perturbation in Fig. 4. It is witnessed that the model without perturbation is only able to generate uni-modal results, and the synthesized images exhibit a loss of detail and suffer from severe over-smoothing problems. ", "page_idx": 7}, {"type": "text", "text": "Straight Trajectory. Our SemFlow aims to establish linear trajectories between the distribution of images and the masks, which enables it to sample with fewer steps. ", "page_idx": 7}, {"type": "text", "text": "Tab. 2 compares the performance of DSM and SemFlow on segmentation tasks under different inference steps, where DSM sampled with DDPM and DDIM represent SDE and ODE modeling, respectively. Overall, SemFlow outperforms DSM by a remarkable margin regardless of the inference steps. Specifically, the DSMs\u2019 performance declines markedly as the number of steps decreases. With 5 steps, the DSM-DDPM achieves a mIoU of 12.3, while the DSM-DDIM achieves 9.5 mIoU. On the contrary, our SemFlow achieves 28.3 even with one step. ", "page_idx": 7}, {"type": "text", "text": "Fig. 5 provides semantic image synthesis results with different inference steps. Although fewer steps will result in some loss of detail and make the images appear smoother, the results of one-step synthesis are still competitive. This indicates that the trajectory between the two distributions is straightened. Fig. 6 and Fig. 7 visualizes the latent variable sampled from the trajectory. The transition between $z_{0}$ and $z_{1}$ is smooth. ", "page_idx": 7}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this work, we present SemFlow, a framework that employs a diffusion model to integrate semantic segmentation and semantic image synthesis as a pair of reverse problems. To overcome the contradiction between the randomness of diffusion models and the certainty of semantic segmentation results, we propose to model semantic segmentation as a transport problem between image and mask distributions. We then employ rectified flow to learn the transfer function, which brings the beneftis of reversible transportation. We propose a finite perturbation method to enable multi-modal generation, which also greatly improves the quality of synthesized results. With straight trajectory modeling, our model can sample with much fewer steps. Experimental results show that even with a weak sampler, our model still achieves comparable or even better results than specialist models. We hope our research can inspire the findings on unified generative model design for the community. ", "page_idx": 7}, {"type": "table", "img_path": "E3P1X94Y51/tmp/c8b4ffe49245dfaa6ccd0645b2389326cc02e3ed42d8bdc8e89e67deb049b5e5.jpg", "table_caption": ["Table 2: Semantic segmentation results with different inference steps on COCO-Stuff dataset. mIoU is used as the metric. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "E3P1X94Y51/tmp/255e9fbcf54f721f3b15d157dfbd0ecd0fa88269bcb68799387419b4388ceae2.jpg", "img_caption": ["Figure 5: Image synthesis results with different inference steps. We use the forward Euler method to get numerical solutions. Our approach obtains competitive results even with only one inference step. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "E3P1X94Y51/tmp/b6beb174926cfc6e73b45a163d3d2356414cebabf16cb70a8951fc57794ef1a3.jpg", "img_caption": ["Figure 6: Visualization of latent variables on the trajectory from $z_{1}$ to $z_{0}$ (Semantic image synthesis). Top row: COCO-Stuff. Bottom row: Cityscapes. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "E3P1X94Y51/tmp/f55be6c74acbd04fbe741d616ff16e6acdae79b3dfbfa8dc35836e8e7e0bafc3.jpg", "img_caption": ["Figure 7: Visualization of latent variables on the trajectory from $z_{0}$ to $z_{1}$ (Semantic segmentation). Top row: COCO-Stuff. Bottom row: Cityscapes. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Acknowledgement. This work was supported by the National Key Research and Development Program of China (No. 2023YFC3807600). ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Michael S Albergo, Nicholas M Boff,i and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. 3   \n[2] Tomer Amit, Tal Shaharbany, Eliya Nachmani, and Lior Wolf. Segdiff: Image segmentation with diffusion probabilistic models. arXiv preprint arXiv:2112.00390, 2021. 3   \n[3] Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and Artem Babenko. Labelefficient semantic segmentation with diffusion models. In ICLR, 2022. 3   \n[4] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. 3   \n[5] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In CVPR, 2020. 1   \n[6] JungWoo Chae, Hyunin Cho, Sooyeon Go, Kyungmook Choi, and Youngjung Uh. Semantic image synthesis with unconditional generator. In NeurIPS, 2023. 3   \n[7] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. TPAMI, 2017. 1, 3   \n[8] Qifeng Chen and Vladlen Koltun. Photographic image synthesis with cascaded refinement networks. In ICCV, 2017. 1, 3   \n[9] Ting Chen, Lala Li, Saurabh Saxena, Geoffrey Hinton, and David J Fleet. A generalist framework for panoptic segmentation of images and videos. In ICCV, 2023. 3   \n[10] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Maskedattention mask transformer for universal image segmentation. In CVPR, 2022. 1, 3   \n[11] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for semantic segmentation. In NeurIPS, 2021. 1, 3, 6   \n[12] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 5, 7   \n[13] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. 2   \n[14] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. MeViS: A large-scale benchmark for video segmentation with motion expressions. In ICCV, 2023. 3   \n[15] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip HS Torr, and Song Bai. MOSE: A new dataset for video object segmentation in complex scenes. In ICCV, 2023. 3   \n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 3   \n[17] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. 2   \n[18] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Han Hu, Dong Chen, et al. Instructdiffusion: A generalist modeling interface for vision tasks. In CVPR, 2024. 3   \n[19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 2020. 1   \n[20] Zhangxuan Gu, Haoxing Chen, Zhuoer Xu, Jun Lan, Changhua Meng, and Weiqiang Wang. Diffusioninst: Diffusion model for instance segmentation. In ICASSP, 2024. 3   \n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 3   \n[22] Eric Heitz, Laurent Belcour, and Thomas Chambon. Iterative $\\alpha$ -(de) blending: A minimalist deterministic diffusion model. In SIGGRAPH, 2023. 3   \n[23] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-toprompt image editing with cross attention control. In ICLR, 2023. 3   \n[24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 2017. 6   \n[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 1, 2   \n[26] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. JMLR, 2022. 3   \n[27] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshops, 2021. 2   \n[28] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017. 1, 3   \n[29] Minh-Quan Le, Tam V Nguyen, Trung-Nghia Le, Thanh-Toan Do, Minh N Do, and Minh-Triet Tran. Maskdiff: Modeling mask distribution with diffusion probabilistic model for few-shot instance segmentation. In AAAI, 2024. 3   \n[30] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive facial image manipulation. In CVPR, 2020. 5   \n[31] Bo Li, Kaitao Xue, Bin Liu, and Yu-Kun Lai. Bbdm: Image-to-image translation with brownian bridge diffusion models. In CVPR, 2023. 6   \n[32] Ke Li, Tianhao Zhang, and Jitendra Malik. Diverse image synthesis from semantic layouts via conditional imle. In ICCV, 2019. 1, 3   \n[33] Xiangtai Li, Zhao Houlong, Han Lei, Tong Yunhai, and Yang Kuiyuan. Gff: Gated fully fusion for semantic segmentation. In AAAI, 2020. 3   \n[34] Xinghui Li, Jingyi Lu, Kai Han, and Victor Prisacariu. Sd4match: Learning to prompt stable diffusion model for semantic matching. arXiv preprint arXiv:2310.17569, 2023. 3   \n[35] Yuheng Li, Yijun Li, Jingwan Lu, Eli Shechtman, Yong Jae Lee, and Krishna Kumar Singh. Collaging class-specific gans for semantic image synthesis. In ICCV, 2021. 3   \n[36] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In CVPR, 2023. 1, 3   \n[37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 5   \n[38] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In ICLR, 2023. 3   \n[39] Qiang Liu. Rectified flow: A marginal preserving approach to optimal transport. arXiv preprint arXiv:2209.14577, 2022. 3   \n[40] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. 2, 3, 4   \n[41] Xihui Liu, Guojun Yin, Jing Shao, Xiaogang Wang, et al. Learning to predict layout-to-image conditional convolutions for semantic image synthesis. In NeurIPS, 2019. 3   \n[42] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. In ICLR, 2024. 2, 3   \n[43] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. 3   \n[44] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In CVPR, 2022. 3   \n[45] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. 1   \n[46] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In CVPR, 2022. 3   \n[47] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In CVPR, 2021. 3   \n[48] Shitong Luo and Wei Hu. Score-based point cloud denoising. In ICCV, 2021. 3   \n[49] Zhengyao Lv, Xiaoming Li, Zhenxing Niu, Bing Cao, and Wangmeng Zuo. Semantic-shape adaptive feature modulation for semantic image synthesis. In CVPR, 2022. 3   \n[50] Zhaoyang Lyu, Zhifeng Kong, Xudong Xu, Liang Pan, and Dahua Lin. A conditional point diffusionrefinement paradigm for 3d point cloud completion. In ICLR, 2022. 3   \n[51] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022. 3   \n[52] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014. 1   \n[53] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatiallyadaptive normalization. In CVPR, 2019. 3, 6   \n[54] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 2   \n[55] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. 2   \n[56] Lu Qi, Lehan Yang, Weidong Guo, Yu Xu, Bo Du, Varun Jampani, and Ming-Hsuan Yang. Unigs: Unified representation for image generation and segmentation. In CVPR, 2024. 1, 2, 3   \n[57] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016. 1   \n[58] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 4, 6   \n[59] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. 1, 3   \n[60] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In SIGGRAPH, 2022. 3   \n[61] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. TPAMI, 2022. 3   \n[62] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NeurIPS, 2016. 1   \n[63] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 1, 2, 3   \n[64] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. 1, 2   \n[65] Yiran Song, Qianyu Zhou, Xiangtai Li, Deng-Ping Fan, Xuequan Lu, and Lizhuang Ma. Ba-sam: Scalable bias-mode attention mask for segment anything model. arXiv preprint arXiv:2401.02317, 2024. 3   \n[66] Vadim Sushko, Edgar Sch\u00f6nfeld, Dan Zhang, Juergen Gall, Bernt Schiele, and Anna Khoreva. You only need adversarial supervision for semantic image synthesis. In ICLR, 2021. 3   \n[67] Zhentao Tan, Menglei Chai, Dongdong Chen, Jing Liao, Qi Chu, Bin Liu, Gang Hua, and Nenghai Yu. Diverse semantic image synthesis via probability distribution modeling. In CVPR, 2021. 3   \n[68] Hao Tang, Song Bai, and Nicu Sebe. Dual attention gans for semantic image synthesis. In ACM MM, 2020. 3   \n[69] Wouter Van Gansbeke and Bert De Brabandere. A simple latent diffusion approach for panoptic segmentation and mask inpainting. arXiv preprint arXiv:2401.10227, 2024. 2, 3, 4   \n[70] Qiang Wan, Zilong Huang, Bingyi Kang, Jiashi Feng, and Li Zhang. Harnessing diffusion models for visual perception with meta prompts. arXiv preprint arXiv:2312.14733, 2023. 3   \n[71] Chaoyang Wang, Xiangtai Li, Henghui Ding, Lu Qi, Jiangning Zhang, Yunhai Tong, Chen Change Loy, and Shuicheng Yan. Explore in-context segmentation via latent diffusion models. arXiv preprint arXiv:2403.09616, 2024. 3   \n[72] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Highresolution image synthesis and semantic manipulation with conditional gans. In CVPR, 2018. 1, 3, 6   \n[73] Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, and Houqiang Li. Semantic image synthesis via diffusion models. arXiv preprint arXiv:2207.00050, 2022. 3   \n[74] Yi Wang, Lu Qi, Ying-Cong Chen, Xiangyu Zhang, and Jiaya Jia. Image synthesis via semantic composition. In ICCV, 2021. 3, 6   \n[75] Jiahao Xie, Wei Li, Xiangtai Li, Ziwei Liu, Yew Soon Ong, and Chen Change Loy. Mosaicfusion: Diffusion models as data augmenters for large vocabulary instance segmentation. arXiv preprint arXiv:2309.13042, 2023. 3   \n[76] Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen. Diffusion models trained with large data are transferable visual models. arXiv preprint arXiv:2403.06090, 2024. 3   \n[77] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Openvocabulary panoptic segmentation with text-to-image diffusion models. In CVPR, 2023. 3   \n[78] Han Xue, Zhiwu Huang, Qianru Sun, Li Song, and Wenjun Zhang. Freestyle layout-to-image synthesis. In CVPR, 2023. 3   \n[79] Dingdong Yang, Seunghoon Hong, Yunseok Jang, Tianchen Zhao, and Honglak Lee. Diversity-sensitive conditional generative adversarial networks. In ICLR, 2019. 3   \n[80] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. Diffusion probabilistic modeling for video generation. Entropy, 2023. 2   \n[81] Haobo Yuan, Xiangtai Li, Chong Zhou, Yining Li, Kai Chen, and Chen Change Loy. Open-vocabulary sam: Segment and recognize twenty-thousand classes interactively. arXiv preprint, 2024. 3   \n[82] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent point diffusion models for 3d shape generation. In NeurIPS, 2022. 3   \n[83] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. 2   \n[84] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018. 6   \n[85] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, 2017. 1, 3   \n[86] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing text-to-image diffusion models for visual perception. In ICCV, 2023. 3   \n[87] Chong Zhou, Xiangtai Li, Chen Change Loy, and Bo Dai. Edgesam: Prompt-in-the-loop distillation for on-device deployment of sam. arXiv preprint arXiv:2312.06660, 2023. 3   \n[88] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation and completion through point-voxel diffusion. In ICCV, 2021. 3   \n[89] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang. Unet $^{++}$ : A nested u-net architecture for medical image segmentation. In MICCAI, 2018. 1   \n[90] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, 2017. 1, 2, 3, 6   \n[91] Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. Sean: Image synthesis with semantic regionadaptive normalization. In CVPR, 2020. 3   \n[92] Zhen Zhu, Zhiliang Xu, Ansheng You, and Xiang Bai. Semantically multi-modal image synthesis. In CVPR, 2020. 3 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Overview. In this supplementary, we present more results and details: ", "page_idx": 12}, {"type": "text", "text": "\u2022 A. More details on our method.   \n\u2022 B. More empirical studies and visual results.   \n\u2022 C. Discussion of the limitations, broader impacts and safeguards. ", "page_idx": 12}, {"type": "text", "text": "A More Details on Method ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Amplitude of Perturbation. In the main paper, the perturbation follows the uniform distribution $\\epsilon\\sim\\mathrm{U}(-\\beta,\\beta)$ . We set $0<\\beta<s/2$ to prevent the semantic labels from changing. Here is the proof. Let $M=(m_{0},m_{1},m_{2})$ be the anchor for category $c_{1}$ , $M^{\\prime}=M+\\epsilon$ be the anchor with perturbation, and $P=(p_{0},p_{1},p_{2})$ be another anchor for category $c_{2},c_{1}\\neq c_{2}$ . First, it is obvious that $E(M^{\\prime})=$ $E(M)\\,=\\,M$ , which means that adding perturbation will not change the estimation of original anchors. Then we only need to prove $||\\bar{\\boldsymbol{M^{\\prime}}}-\\boldsymbol{M}||_{2}<||\\boldsymbol{M^{\\prime}}-\\boldsymbol{P}||_{2}$ , which is equivalent to $\\sum_{i\\in[3]}\\bar{\\epsilon}_{i}^{2}<$ $\\begin{array}{r}{\\sum_{i\\in[3]}(\\epsilon_{i}+m_{i}-p_{i})^{2}}\\end{array}$ , and can be further written as $\\begin{array}{r}{0<\\sum_{i\\in[3]}(m_{i}-p_{i})(m_{i}-p_{i}+2\\epsilon_{i})}\\end{array}$ . Since $-s<2\\epsilon_{i}<s$ and $m_{i}-p_{i}$ can only take the values of $\\{...,-2s,-s,0,s,2s,...\\}$ , the two terms must have the same sign or one of them equals 0. The proof is completed. ", "page_idx": 12}, {"type": "text", "text": "Implementation Details. We train CelebAMask-HQ and Cityscapes with a batch size of 256 with AdamW optimizer for 80K and 8K steps, respectively. The initial learning rate is set as $2\\times10^{-5}$ and $5\\times10^{-5}$ . Linear learning rate scheduler is adopted. For COCO-Stuff dataset, we use a constant learning rate of $1\\times10^{-5}$ with a batch size of 128 for 320K steps. The COCO-Stuff indicates the version of COCO-Stuff-164k. ", "page_idx": 12}, {"type": "text", "text": "B More Empirical Studies and Visual Results ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Sampling from Other Distributions. Fig. 8 shows the synthesized images which are sampled with different perturbations $\\epsilon\\sim\\mathrm{U}(-\\beta^{\\prime},\\beta^{\\prime})$ . It can be observed that the model can obtain high-quality synthetic results only when the perturbations in the inference stage and the training stage follow the same distribution. Specifically, the synthesized images are dim when $\\beta^{\\prime}<\\beta$ , and overexposed when $\\beta^{\\prime}>\\beta$ . ", "page_idx": 12}, {"type": "image", "img_path": "E3P1X94Y51/tmp/226e2a2dcff404f0ac452b22d65379e9773df53264fc544ef8c03543471b89c3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Figure 8: Image synthesis results sampled from other distributions. We sample with different perturbation $\\epsilon\\sim\\mathrm{U}(-\\beta^{\\prime},\\beta^{\\prime})$ . In the training stage, the perturbation follows $\\mathrm{U}(-\\bar{\\beta},\\beta)$ . ", "page_idx": 12}, {"type": "text", "text": "More Visual Results on One-step Generation. Fig. 9 shows that our model can generate competitive images with only one inference step. ", "page_idx": 12}, {"type": "text", "text": "Selection of ODE solvers. Fig. 10 provides visualization results on different ODE solvers. A stronger solver has the potential to bring about better results. ", "page_idx": 12}, {"type": "image", "img_path": "E3P1X94Y51/tmp/a4efc405a631e410feb1388cb8c48b3811c35ea6c07be855750fd486cf0a447d.jpg", "img_caption": ["Figure 9: Synthesized images with one inference step. "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "E3P1X94Y51/tmp/8b6a1e6dc7051ec04a503aac0be2ce2ba27c7b576d38c5afcd7378783796d92f.jpg", "img_caption": ["Figure 10: The influences of ODE solvers. (a) Euler indicates sampling with euler-25 solver. (b) RK45 indicates the Runge-Kutta method of order 5(4). "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "More Visual Results on Cityscapes. Fig. 11 shows the results of multi-modal image synthesis. The appearance of the synthesized scene varies according to different random seeds. Moreover, our model can learn the laws of shadows and light angles (the third column), demonstrating its great potential. ", "page_idx": 13}, {"type": "text", "text": "Fig. 12 shows visual results of semantic segmentation. Our model accurately segments the objects and assigns the correct semantic labels. ", "page_idx": 13}, {"type": "text", "text": "C Discussion ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Limitations. Some previous work, such as Stable Diffusion, SDXL show that scaling up the training data is essential to exploit the potential of diffusion models fully. Due to the limited computational resources and the number of image-mask pairs in the existing dataset, we leave it to future work. In addition, although this paper does not focus on the network structure and the design of ODE solvers, we believe there is room for exploration to further improve performance. ", "page_idx": 13}, {"type": "text", "text": "Broader Impacts. Our SemFlow extends the diffusion model from the generative domain to the segmentation task and narrows the gap with traditional discriminative models. It also functions as a unified framework to bind semantic segmentation and image synthesis. We hope this will motivate people to explore the unification of low-level and high-level vision. In terms of social impact, it will encourage artistic content creation, but at the same time may face the problem of fake content. ", "page_idx": 13}, {"type": "text", "text": "Safeguards. The content produced by a generative model is highly correlated with its training data.   \nEnsuring fair and clean training data can effectively prevent models from generating harmful content. ", "page_idx": 13}, {"type": "image", "img_path": "E3P1X94Y51/tmp/adca9f90be0769421d2431b4c3cecb6948c4867303ca8e462f913b29e16e0a1c.jpg", "img_caption": ["Figure 11: Image synthesis results on Cityscapes. We show the results under three random seeds for each semantic mask. The first row: semantic layouts. The second to the fourth row: synthesized results. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "E3P1X94Y51/tmp/66ed8a2305a07e7a576af47c1e1b273c1f2db0be7606bf4b7462db7e64e5e70d.jpg", "img_caption": ["Figure 12: Semantic segmentation results on Cityscapes. The color black in the ground truth indicates the ignored regions. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We include the main claims in the abstract and introduction. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: We discuss the limitations in Appendix C. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 15}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: Assumptions and proof are in Section 3 and Appendix A. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We provide the implementation details in Section 4. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [No] ", "page_idx": 17}, {"type": "text", "text": "Justification: We are not able to provide the code at submission time. We will definitely release the code in the future. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Experimental setting and details are in Section 4. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [No] ", "page_idx": 17}, {"type": "text", "text": "Justification: We did not conduct experiments with error bars. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We provide the implementation details in Section 4. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics . ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We discuss broader impacts in Appendix C. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We describe the safeguards in Appendix C. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We use existing assets with properly credited. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 19}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: No new assets. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Not involve crowdsourcing nor research with human subjects. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Not involve crowdsourcing nor research with human subjects. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}]