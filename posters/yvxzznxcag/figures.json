[{"figure_path": "YVXzZNxcag/figures/figures_1_1.jpg", "caption": "Figure 1: Knowledge circuit obtained from \"The official language of France is French\" in GPT2-Medium. Left: a simplified circuit and the whole circuit is in Figure 9 in Appendix. We use --> to skip some complex connections between nodes. Here, L15H0 means the first attention head in the 15th layer and MLP12 means the multi-perception layer in the 13th layer. Right: the behavior of several special heads. The matrix on the left is the attention pattern of each attention head and the right heapmap demonstrates the output logits of the head by mapping to the vocabulary space.", "description": "This figure illustrates the knowledge circuit for the sentence \"The official language of France is French\" in the GPT-2 Medium model.  The left side shows a simplified version of the circuit, highlighting key components like attention heads (e.g., L15H0) and Multilayer Perceptrons (MLPs, e.g., MLP12) and how information flows between them.  The right side shows the attention patterns and output logits for several specific attention heads, visualizing how these components contribute to the final prediction of the word \"French\".  It demonstrates the collaborative encoding of knowledge within the language model.", "section": "3 Knowledge Circuits Discovery in Transformers"}, {"figure_path": "YVXzZNxcag/figures/figures_5_1.jpg", "caption": "Figure 2: The activated circuit component distributions in Layers in GPT2-Medium.", "description": "This figure shows the distribution of activated components (attention heads and MLPs) across different layers in the GPT2-Medium language model for four different knowledge types: Linguistic, Commonsense, Bias, and Factual.  Each bar represents a layer, and the height of the bar indicates the average activation frequency of components within that layer for the specified knowledge type.  The figure visually demonstrates the distribution of knowledge across different layers of the model, revealing that some types of knowledge might be more prevalent in lower or higher layers of the network.", "section": "3 Knowledge Circuits Discovery in Transformers"}, {"figure_path": "YVXzZNxcag/figures/figures_6_1.jpg", "caption": "Figure 3: The rank and probability of the target entity o at both the last subject token and the last token position when unembedding the intermediate layer's output for the fact \"The official language of France is French\".", "description": "This figure shows how the probability and rank of the target entity (\"French\") change across different layers of the transformer model when processing the input sentence \"The official language of France is\". The x-axis represents the layer number and the y-axis shows both the rank (on a log scale) and probability of the target entity.  Two lines show the rank of the target entity, one at the subject position (France) and one at the final token position (last token). Two additional lines show the corresponding probabilities for the target entity at those positions. The figure illustrates how the model processes and integrates information across layers, eventually culminating in the high probability of the correct answer at the final token position.", "section": "5 Knowledge Circuits Elucidate Internal Mechanisms for Knowledge Editing"}, {"figure_path": "YVXzZNxcag/figures/figures_7_1.jpg", "caption": "Figure 4: Different behaviors when we edit the language model. In the original model, we can see the mover head L15H3 actually move the original token \u201cController\u201d and other information, while for ROME, we observe the mover head select the correct information \u201cIntel\u201d, which means ROME successfully added the \u201cIntel\u201d to model. For the FT layer-0 editing, we can find this method directly write the edited knowledge into edited component. However, we find these two editing methods would affect other unrelated input \u201cWindows server is created by?\"", "description": "This figure compares the behavior of three different knowledge editing methods (original model, ROME, and FT-M) on the same language model. It shows how each method handles the insertion of new knowledge (in this case, changing the creator of \u201cPlatform Controller Hub\u201d) and the impact it has on the model\u2019s response to related and unrelated questions. The original model shows a lack of modification for the new fact, leading to incorrect answers when probing for new knowledge. ROME shows the edited information correctly integrated into the model's reasoning chain, which leads to the correct answer for related questions but produces a false positive for an unrelated fact. Lastly, FT-M illustrates the direct injection of new knowledge into the model, causing overfitting and resulting in correct answers for both related and unrelated questions.", "section": "Knowledge circuits elucidate internal mechanisms for knowledge editing"}, {"figure_path": "YVXzZNxcag/figures/figures_8_1.jpg", "caption": "Figure 5: Left: fact hallucination case \"The official currency of Malaysia is called\", we observe that, at layer 15, the Mover Head selects incorrect information. Right: In-context learning case, we notice that some new heads focusing on the demonstration appear in the knowledge circuit.", "description": "This figure shows two cases from the paper. The left panel shows a hallucination where the model incorrectly identifies the currency of Malaysia.  The knowledge circuit analysis reveals that at layer 15, a mover head selects incorrect information. The right panel shows in-context learning, where the model initially produces an incorrect answer but corrects it after seeing a demonstration.  The analysis of the knowledge circuit reveals several new attention heads appear in the computation graph when the demonstration is incorporated, focusing on its context.", "section": "Knowledge Circuits Facilitate Interpreting Language Model Behaviors"}, {"figure_path": "YVXzZNxcag/figures/figures_18_1.jpg", "caption": "Figure 6: The output of the model. Ablating the mover head would increase the probability of \"Italian\", \"English\" and \"Spanish\", which are not subject-related. While ablating the relation head would lead to the increase of some meaningless words \"a\", \"that\", which are not relation-related.", "description": "This figure shows the top 10 token output probabilities for three different scenarios: (a) original output, (b) ablating mover head, and (c) ablating relation head.  By comparing the probabilities across these scenarios, we can see how removing specific components of the language model (the mover head and the relation head) impacts its predictions and how these components affect information transfer and generation of meaningful vs. meaningless tokens.  Specifically, removing the mover head increases probabilities of words not closely related to the subject, and removing the relation head increases probabilities of unrelated or meaningless words.", "section": "A Running Example of Knowledge Circuit"}, {"figure_path": "YVXzZNxcag/figures/figures_18_2.jpg", "caption": "Figure 7: The average rank of the target entity o for the Dval in the vocabulary when mapping the output of each layer in the model to the embedding space. We can find that the in GPT2-Medium and GPT2-Large, the model would get the knowledge at middle-to-later layers. While for TinyLLAMA, the layer may be more later.", "description": "This figure shows the average rank of the target entity across different layers for three different language models: GPT2-Medium, GPT2-Large, and TinyLLAMA. The rank is calculated by mapping the model's output at each layer to the vocabulary space.  The results show that for GPT2-Medium and GPT2-Large, the target entity tends to achieve a higher rank in the middle and later layers. This suggests that the model progressively aggregates information related to the target entity as it processes through the layers.  In contrast, TinyLLAMA shows a more concentrated pattern where the target entity appears closer to the final layers. This difference in behavior might be attributed to the varying architectural differences and knowledge capacity among the models.", "section": "4 Knowledge Circuits Unveil Implicit Neural Knowledge Representations"}, {"figure_path": "YVXzZNxcag/figures/figures_19_1.jpg", "caption": "Figure 1: Knowledge circuit obtained from \"The official language of France is French\" in GPT2-Medium. Left: a simplified circuit and the whole circuit is in Figure 9 in Appendix. We use --> to skip some complex connections between nodes. Here, L15H0 means the first attention head in the 15th layer and MLP12 means the multi-perception layer in the 13th layer. Right: the behavior of several special heads. The matrix on the left is the attention pattern of each attention head and the right heapmap demonstrates the output logits of the head by mapping to the vocabulary space.", "description": "This figure shows a knowledge circuit extracted from a GPT2-medium language model when processing the sentence \"The official language of France is French.\"\n\nThe left panel displays a simplified version of the circuit, showing the connections between key components (attention heads, Multilayer Perceptrons, and embeddings).  The full circuit is available in Figure 9 of the Appendix.  Arrows indicate the flow of information, highlighting how different components collaborate to process and represent the knowledge.\n\nThe right panel shows the detailed behavior of specific components within the circuit. The matrices represent the attention patterns of different attention heads, illustrating where each head focuses its attention within the input sentence. The heatmaps visualize the output logits of each head, showing its contribution to the final prediction of each word (after mapping the logits to the vocabulary).", "section": "Knowledge Circuits in Pretrained Transformers"}, {"figure_path": "YVXzZNxcag/figures/figures_20_1.jpg", "caption": "Figure 9: The knowledge circuit from the \"The official language of France is French\" in GPT2-Medium.", "description": "This figure shows a detailed knowledge circuit derived from the sentence \"The official language of France is French\" within the GPT2-Medium model.  The circuit visually represents the network of interactions between various components of the transformer architecture, such as attention heads and MLP layers, that collectively contribute to the model's ability to generate the correct response. The nodes represent different components in the network, and the edges represent the connections between these components. By analyzing the structure of the circuit, researchers can gain insights into the model's internal workings, enabling a better understanding of how knowledge is represented and used by the model. The figure helps illustrate the flow of information through the model as it processes and generates the target output.", "section": "3 Knowledge Circuits Discovery in Transformers"}, {"figure_path": "YVXzZNxcag/figures/figures_21_1.jpg", "caption": "Figure 10: a specific case in Multi-hop reasoning. When we removed the context of the first hop question, we found the model also directly gave us the answer. The phenomenon appears in both GPT2 and TinyLLAMA.", "description": "This figure demonstrates a case where a language model successfully answers a multi-hop question even when the context of the first hop is removed.  The model's ability to answer correctly without the first hop suggests an alternative reasoning mechanism beyond simply combining information from sequential hops.  The phenomenon is observed in both GPT-2 and TinyLLAMA models.", "section": "C.1 Multi-hop Factual Knowledge Editing"}, {"figure_path": "YVXzZNxcag/figures/figures_23_1.jpg", "caption": "Figure 1: Knowledge circuit obtained from \"The official language of France is French\" in GPT2-Medium. Left: a simplified circuit and the whole circuit is in Figure 9 in Appendix. We use --> to skip some complex connections between nodes. Here, L15H0 means the first attention head in the 15th layer and MLP12 means the multi-perception layer in the 13th layer. Right: the behavior of several special heads. The matrix on the left is the attention pattern of each attention head and the right heapmap demonstrates the output logits of the head by mapping to the vocabulary space.", "description": "This figure shows a knowledge circuit extracted from the GPT2-Medium model for the sentence \"The official language of France is French\". The left panel displays a simplified version of the circuit (the full circuit is shown in Figure 9 in the Appendix), which visualizes the connections between different components of the model such as attention heads and MLP layers.  Arrows (-->) represent connections between nodes where some steps might be skipped for simplification.  The right panel displays the behavior of specific components including attention patterns and output logits for several attention heads.  The attention pattern matrices show how much attention each head pays to each word in the input sentence, while the heatmaps illustrate the model's output logits (probabilities) for different words in the vocabulary after each head's processing.", "section": "1 Introduction"}, {"figure_path": "YVXzZNxcag/figures/figures_23_2.jpg", "caption": "Figure 1: Knowledge circuit obtained from \"The official language of France is French\" in GPT2-Medium. Left: a simplified circuit and the whole circuit is in Figure 9 in Appendix. We use --> to skip some complex connections between nodes. Here, L15H0 means the first attention head in the 15th layer and MLP12 means the multi-perception layer in the 13th layer. Right: the behavior of several special heads. The matrix on the left is the attention pattern of each attention head and the right heapmap demonstrates the output logits of the head by mapping to the vocabulary space.", "description": "This figure shows an example of a knowledge circuit extracted from the GPT-2 Medium language model for the sentence \"The official language of France is French.\"\n\nThe left panel displays a simplified version of the circuit, showing the flow of information through various components of the transformer architecture (attention heads and MLP layers).  The arrows illustrate the connections and information flow.  The full circuit is available in Figure 9 of the Appendix.\n\nThe right panel displays the behavior of some key attention heads. The leftmost heatmap represents the attention weights assigned by each head, showing which parts of the input sentence each head focuses on. The rightmost heatmap illustrates the output logits of the attention head, mapping these values to words in the model's vocabulary.", "section": "Knowledge Circuits Discovery in Transformers"}, {"figure_path": "YVXzZNxcag/figures/figures_24_1.jpg", "caption": "Figure 12: FT-M Rank Change Across Different Layers", "description": "This figure visualizes the changes in the ranking of the target new token's probability across different layers when editing the model using FT-M for different knowledge types.  It shows the effect of applying the FT-M method for knowledge editing at different layers of the model. The vertical lines indicate that FT-M directly embeds the editing information into the model's information flow, while the peaks a few layers after the edited layer illustrate ROME's more gradual effect. ", "section": "Edit Experiments"}]