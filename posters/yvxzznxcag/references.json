{"references": [{"fullname_first_author": "Kevin Meng", "paper_title": "Locating and editing factual associations in GPT", "publication_date": "2022-12-01", "reason": "This paper is foundational for knowledge editing techniques and is directly referenced and compared against in the current paper's experiments."}, {"fullname_first_author": "Mor Geva", "paper_title": "Transformer feed-forward layers are key-value memories", "publication_date": "2021-11-01", "reason": "This paper introduced the key-value memory concept in transformer networks, a core concept underlying the current paper's analysis of knowledge circuits."}, {"fullname_first_author": "Nelson Elhage", "paper_title": "A mathematical framework for transformer circuits", "publication_date": "2021-12-01", "reason": "This paper establishes the theoretical framework of circuits in neural networks, providing the foundation for the current paper's circuit-based approach."}, {"fullname_first_author": "Arthur Conmy", "paper_title": "Towards automated circuit discovery for mechanistic interpretability", "publication_date": "2023-12-01", "reason": "This paper presents a method for automated circuit discovery, a crucial technique used by the current paper to identify knowledge circuits."}, {"fullname_first_author": "Damai Dai", "paper_title": "Knowledge neurons in pretrained transformers", "publication_date": "2022-05-22", "reason": "This paper introduces the concept of knowledge neurons, which the current paper builds upon to investigate the interaction of multiple components in articulating knowledge."}]}