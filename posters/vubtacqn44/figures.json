[{"figure_path": "VUBtAcQN44/figures/figures_0_1.jpg", "caption": "Figure 3: Visual results with our model and other sota methods. The scenarios are chosen from RVSOD/actioncliptest00256 and RVSOD/actioncliptest00707 respectively.", "description": "This figure compares the visual results of the proposed method with other state-of-the-art methods for video salient object ranking.  It shows that previous methods (Liu's and Lin's) tend to focus on objects with prominent static saliency cues, while the proposed method emphasizes instance-wise temporal correlations, resulting in more accurate results, especially for objects with significant motion.", "section": "4.2 Compared to State-of-the-art Methods"}, {"figure_path": "VUBtAcQN44/figures/figures_3_1.jpg", "caption": "Figure 2: The main architecture of our model. Firstly, we obtain instances of each frame through an object detector and extract features of each instance using an attention mechanism and position embedding. Then, we utilize a spatial-temporal graph reasoning network to fuse spatial-temporal saliency cues, ultimately obtaining saliency scores and final results.", "description": "This figure shows the overall architecture of the proposed VSOR model. It consists of three main stages: 1) Instance feature extraction, where an object detector identifies instances in each frame, and attention and position embeddings enhance instance features. 2) Spatial-temporal graph reasoning, where a graph neural network fuses spatial and temporal saliency cues from multiple scales. This stage includes spatial correlation modeling (considering interactions between instances, local and global contrasts), and temporal correlation modeling (capturing instance interactions and motion-aware contrast across adjacent frames). 3) Saliency score prediction, where a fully connected neural network predicts final saliency scores which are combined with instance segmentation results to produce a saliency ranking map. The figure illustrates the data flow and the interactions within each stage, highlighting the key components and their relationships.", "section": "3 Method"}, {"figure_path": "VUBtAcQN44/figures/figures_6_1.jpg", "caption": "Figure 3: Visual results with our model and other sota methods. The scenarios are chosen from RVSOD/actioncliptest00256 and RVSOD/actioncliptest00707 respectively.", "description": "This figure presents a qualitative comparison of the proposed method against other state-of-the-art (SOTA) video salient object ranking methods.  It shows the results for four different video clips, each displayed with the input image, ground truth saliency masks, and the saliency masks produced by the different methods (Liu's, Lin's, and the proposed 'Ours'). The color-coding in the 'Salient Instance' column indicates different salient objects, with the same color consistently used for the same object across different frames in a sequence. This visualization helps to demonstrate how the proposed method, unlike the others, handles the temporal dynamics of saliency more accurately.", "section": "4.2 Compared to State-of-the-art Methods"}, {"figure_path": "VUBtAcQN44/figures/figures_7_1.jpg", "caption": "Figure 3: Visual results with our model and other sota methods. The scenarios are chosen from RVSOD/actioncliptest00256 and RVSOD/actioncliptest00707 respectively.", "description": "This figure compares the visual results of the proposed method against three other state-of-the-art (SOTA) methods. Two example video clips are shown, and the results of each method are presented alongside the ground truth (GT).  The figure visually demonstrates the improvements achieved by the authors' model, particularly in accurately identifying and ranking salient objects that exhibit significant motion, such as a person walking. This contrasts with the other methods, which tend to overemphasize static objects.", "section": "4.2 Compared to State-of-the-art Methods"}, {"figure_path": "VUBtAcQN44/figures/figures_8_1.jpg", "caption": "Figure 5: Our method for video retargeting. We generate instance-level saliency information including bounding box, mask and ranks based on our VSOR model. Thus, we get the cropping center according to instance-level saliency information.", "description": "This figure illustrates the video retargeting method proposed in the paper.  It shows a step-by-step process: 1. Input videos are processed; 2. The VSOR model generates instance-level saliency information (bounding boxes, masks, and ranking); 3. A saliency center is calculated based on the instance-level saliency; 4. A cropping region is determined using the saliency center; 5. Smoothing is applied for temporal consistency. The result is a retargeted video suitable for different aspect ratios.", "section": "4.4 Application to Video Retargeting"}, {"figure_path": "VUBtAcQN44/figures/figures_9_1.jpg", "caption": "Figure 6: Comparison of retargeting, (a) is the method of seam carving [12], (b) is smartVideoCrop [13], (c) is our methods. The yellow box in (a) shows the artifacts after retargeting.", "description": "This figure compares the results of three different video retargeting methods: seam carving, smartVideoCrop, and the proposed method from the paper.  The goal is to show how each method handles resizing a video while maintaining the visual quality.  The seam carving method (a) produces noticeable artifacts, especially shown in the yellow bounding box. The smartVideoCrop (b) method improves the results.  (c) shows the results of the proposed method, aiming for a better result in terms of artifact reduction and preserving visual quality.", "section": "4.4 Application to Video Retargeting"}, {"figure_path": "VUBtAcQN44/figures/figures_13_1.jpg", "caption": "Figure 7: Flowchat for dataset collection and annotation.", "description": "This flowchart illustrates the process of creating the dataset used in the paper.  It starts with data collection, then proceeds to mask generation for each instance within the collected videos. Instance ranking is performed based on fixation maps, leading to the final separation of videos into two categories: those with single instances and those with multiple instances.", "section": "A.1 Details of Dataset Generation"}, {"figure_path": "VUBtAcQN44/figures/figures_14_1.jpg", "caption": "Figure 3: Visual results with our model and other sota methods. The scenarios are chosen from RVSOD/actioncliptest00256 and RVSOD/actioncliptest00707 respectively.", "description": "This figure shows a comparison of the proposed method with other state-of-the-art methods for video salient object ranking.  It visually demonstrates the advantages of the proposed method in accurately identifying and ranking salient objects, especially those with rich motion cues but less static saliency. The comparison highlights the limitations of previous methods that mainly prioritize objects with prominent static cues, neglecting the importance of dynamic saliency.", "section": "4.2 Compared to State-of-the-art Methods"}, {"figure_path": "VUBtAcQN44/figures/figures_14_2.jpg", "caption": "Figure 9: The results of the cropping based methods. The upper part is the smart-video-crop [13], and the lower part is our model. The red boxes in the images represent the crop area.", "description": "This figure compares video retargeting results using two different methods: smart-video-crop [13] and the proposed method.  The smart-video-crop method uses image-level saliency for cropping, which can lead to inaccurate results. The proposed method, on the other hand, uses instance-level saliency from the VSOR model, resulting in more accurate and visually appealing results.  The red boxes highlight the cropped regions.", "section": "4.4 Application to Video Retargeting"}]