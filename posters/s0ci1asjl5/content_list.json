[{"type": "text", "text": "Gaussian Approximation and Multiplier Bootstrap for Polyak-Ruppert Averaged Linear Stochastic Approximation with Applications to TD Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Eric Moulines Ecole Polytechnique, MBUZAI ", "page_idx": 0}, {"type": "text", "text": "Department of Statistics and Data Science, Shenzhen International Center of Mathematics, Southern University of Science and Technology ", "page_idx": 0}, {"type": "text", "text": "Zhuo-Song Zhang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Statistics and Data Science, Shenzhen International Center of Mathematics, Southern University of Science and Technology ", "page_idx": 0}, {"type": "text", "text": "AlexeyNaumov HSE University,   \nSteklov Mathematical Institute   \nof Russian Academy of Sciences ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we obtain the Berry-Esseen bound for multivariate normal approximation for the Polyak-Ruppert averaged iterates of the linear stochastic approximation (LSA) algorithm with decreasing step size. Moreover, we prove the non-asymptotic validity of the confidence intervals for parameter estimation with LSA based on multiplier bootstrap. This procedure updates the LSA estimate together with a set of randomly perturbed LSA estimates upon the arrival of subsequent observations. We illustrate our findings in the setting of temporal difference learning with linear function approximation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stochastic approximation (SA) methods are a central component for solving various optimization problems that arise in machine learning [32, 26], empirical risk minimization [72] and reinforcement learning [42, 67]. There is a vast number of contributions in the literature, which cover both asymptotic [48, 53] and non-asymptotic [45, 15, 36] properties of the SA estimates. The primarily important property among the asymptotic ones of the SA estimates is their asymptotic normality [53], which is important due to its role in constructing (asymptotic) confidence intervals and hypothesis testing [71]. However, a natural question of the rate of convergence in the appropriate central limit theorems (CLT) is not well addressed in literature even in the relatively simple setting of the linear stochastic approximation (LSA) [21], [34], [9]. ", "page_idx": 0}, {"type": "text", "text": "Alternatively, confidence sets for SA algorithms can be constructed in a non-asymptotic manner based on concentration inequalities [4]. These bounds are often regarded as loose [60], yielding suboptimal performance of the statistical procedures based on the latter estimates [28]. In contrast, for statistical inference procedures based on independent and identically distributed (i.i.d.) observations, such as $M$ -estimators [71], there is a machinery of non-parametric methods for constructing confidence sets with the bootstrap [19, 58]. This approach is accompanied with theoretical guarantees, showing the non-asymptotic validity of the bootstrap-based confidence intervals for parameters in linear regression [64] and statistical tests [12]. Extending theoretical guarantees to a non-classical situation with online learning algorithms encounters serious problems, essentially related to the problem of obtaining rate of convergence in the corresponding CLTs. At the same time, many phenomena arising in the analysis of nonlinear SA algorithms already appear in the analysis of LSA problems. ", "page_idx": 0}, {"type": "text", "text": "The LSA procedure aims to find an approximate solution for the linear system $\\bar{\\mathbf{A}}\\theta^{\\star}=\\bar{\\mathbf{b}}$ with a unique solution $\\theta^{\\star}$ based on a sequence of observations $\\{(\\mathbf{A}(Z_{k}),\\mathbf{b}(Z_{k}))\\}_{k\\in\\mathbb{N}}$ . Here $\\mathbf{A}:Z\\to\\mathbb{R}^{d\\times d}$ and $\\mathbf{b}:Z\\to\\mathbb{R}^{d}$ are measurable functions and $(Z_{k})_{k\\in\\mathbb{N}}$ is a sequence of noise variables taking values in some measurable space $(Z,{\\mathcal{Z}})$ with a distribution $\\pi$ satisfying $\\mathbb{E}[\\mathbf{A}(Z_{k})]=\\bar{\\mathbf{A}}$ and $\\mathbb{E}[{\\bf b}(\\breve{Z_{k}})]=\\bar{\\bf b}$ We focus on the setting of independent and identically distributed (i.i.d.) observations $\\{Z_{k}\\}_{k\\in\\mathbb{N}}$ With a sequence of decreasing step sizes $(\\alpha_{k})_{k\\in\\mathbb{N}}$ and the starting point $\\theta_{0}\\in\\mathbb{R}^{d}$ , we consider the estimates $\\{\\bar{\\theta}_{n}\\}_{n\\in\\mathbb{N}}$ given by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\theta_{k}=\\theta_{k-1}-\\alpha_{k}\\{\\mathbf{A}(Z_{k})\\theta_{k-1}-\\mathbf{b}(Z_{k})\\}~,~~k\\geq1,~~~\\bar{\\theta}_{n}=n^{-1}\\sum_{k=n}^{2n-1}\\theta_{k}~,~~n\\geq1~.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here, we have fixed the size of the burn-in period (see, e.g., [16, 44]) to $n_{0}\\,=\\,n$ . Provided that $n$ is large enough, the burn-in size affects only a constant factor in the subsequent bounds. The sequence $\\{\\theta_{k}\\}_{k\\in\\mathbb{N}}$ corresponds to the standard LSA iterates, while $\\{\\bar{\\theta}_{n}\\}_{n\\in\\mathbb{N}}$ corresponds to the Polyak-Ruppert (PR) averaged iterates [59, 53]. It is known that $\\bar{\\theta}_{n}$ is asymptotically normal with a minimax-optimal covariance matrix (see [53] and [23] for discussion). Specifically, under appropriate technical conditions on the step sizes $\\{\\alpha_{k}\\}$ and noisy observations $\\{\\mathbf{A}\\bar{(Z_{k})}\\}$ , it holds that ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\sqrt{n}(\\bar{\\theta}_{n}-\\theta^{\\star})\\stackrel{d}{\\rightarrow}\\mathcal{N}(0,\\Sigma_{\\infty})\\;,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\Sigma_{\\infty}$ is the asymptotic covariance matrix defined later in Section 3.1. There is a long list of contributions to the non-asymptotic analysis of ${\\bar{\\theta}}_{n}$ , particularly [43] and [16], which study moment and Bernstein-type concentration bounds for $\\sqrt{n}(\\bar{\\theta}_{n}-\\theta^{\\star})$ . Unfortunately, such bounds do not imply Berry-Esseen type inequalities for ${\\sqrt{n}}({\\bar{\\theta}}_{n}-{\\dot{\\theta}}^{\\star})$ , that is, they do not allow us to control the quantity ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\rho_{n}^{\\mathrm{Conv}}=\\operatorname*{sup}_{B\\in\\mathrm{Conv}(\\mathbb{R}^{d})}\\left|\\mathbb{P}\\big(\\sqrt{n}(\\bar{\\theta}_{n}-\\theta^{\\star})\\in B\\big)-\\mathbb{P}(\\Sigma_{\\infty}^{1/2}\\eta\\in B)\\right|\\ ,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathrm{Conv}(\\mathbb{R}^{d})$ refers to the set of convex sets in $\\mathbb{R}^{d}$ . While the Berry-Esseen bounds are a popular subject of study in probability theory, starting from the classical work [20], most results are obtained for sums of random variables or martingale difference sequences [52, 8]. We can only mention a few results for SA algorithms, see Section 2 for more details. This paper aims to provide the latter bounds for the specific setting of the LSA procedure. Our primary contribution is twofold: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We establish a BerryEsseen bound for accuracy of normal approximation of the distribution of Polyak-Ruppert averaged LSA iterates with a polynomially decreasing step size. Our results suggest that the best rate of normal approximation, in the sense of (2), is of order $n^{-1/4}$ up to logarithmic factors in $n$ , where $n$ denotes the number of samples. Interestingly, this rate is achieved with an aggressive step size, $\\alpha_{k}=c_{0}/\\sqrt{k}$ . Our proof technique follows the Berry-Esseen bounds for nonlinear statistics provided in [63].   \n\u00b7 We provide non-asymptotic confidence bounds for the distribution of the PR-averaged statistic ${\\sqrt{n}}({\\bar{\\theta}}_{n}-{\\bar{\\theta^{\\star}}})$ using the multiplier bootstrap procedure. In particular, our bounds imply that the quantiles of the exact distribution of $\\bar{\\sqrt{n}}(\\bar{\\theta}_{n}-\\theta^{\\star})$ can be approximated at a rate of $n^{-1/4}$ , where $n$ is the number of samples used in the procedure, provided that $n$ .is sufficiently large (see A4 for exact conditions). To the best of our knowledge, this is the first non-asymptotic bound on the accuracy of bootstrap approximation in SA algorithms. We apply the proposed methodology to the temporal difference learning (TD) algorithm for policy evaluation in reinforcement learning. ", "page_idx": 1}, {"type": "text", "text": "The rest of the paper is organized as follows. In Section 2, we provide a literature review on the non-asymptotic analysis of the LSA algorithm and bootstrap methods. Next, in Section 3, we analyze the convergence rate of Polyak-Ruppert averaged LSA iterates to the normal distribution. In Section 4, we discuss the multiplier bootstrap approach for LSA and establish bounds on the accuracy of approximating the quantiles of the true distribution. Finally, we apply our findings to TD learning and present numerical illustrations in Section 5. ", "page_idx": 1}, {"type": "text", "text": "Notations. For matrix $A\\,\\in\\,\\mathbb{R}^{d\\times d}$ we denote by $\\|A\\|$ its operator norm. For symmetric matrix $Q=Q^{\\top}\\succ0$ \uff0c $Q\\in\\mathbb{R}^{d\\times d}$ and $x\\in\\mathbb{R}^{d}$ we define the corresponding norm $\\|x\\|_{Q}=\\sqrt{x^{\\top}Q x}$ and define the respective matrix $Q$ -norm of the matrix $B\\in\\mathbb{R}^{d\\times d}$ by $\\|B\\|_{Q}=\\operatorname*{sup}_{x\\neq0}\\|B x\\|_{Q}/\\|x\\|_{Q}$ For sequences $a_{n}$ and $b_{n}$ , we write $a_{n}\\lesssim b_{n}$ if there exist a constant $c>0$ such that $a_{n}\\leq c b_{n}$ for $c>0$ . For simplicity we state the main results of the paper up to constant factors. ", "page_idx": 1}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Among contributions to the analysis of the LSA algorithm, we should mention the papers [53, 34, 9, 6]. These works investigate the asymptotic properties of the LSA estimates (such as asymptotic normality and almost sure convergence) under i.i.d. and Markov noise. Non-asymptotic results for the LSA and PR-averaged LSA estimates were obtained in [55, 47, 7, 35, 44], where MSE bounds were established, and in [43, 17, 16], which provided high-probability error bounds. The latter results enable the construction of Bernstein-type confidence intervals for the error $\\bar{\\theta}_{n}-\\theta^{\\star}$ . Unfortunately, the corresponding bounds typically depend on unknown problem properties of (1), related to the design matrix $\\bar{\\mathbf{A}}$ and the noise variables $\\mathbf{\\dot{A}}(Z_{k})$ \uff0c ${\\bf b}(Z_{k})$ . For this reason, applying these error bounds in practice is complicated. Furthermore, concentration bounds for the LSA error [43, 17, 16] do not imply convergence rates of the rescaled error ${\\sqrt{n}}({\\bar{\\theta}}_{n}-\\theta^{\\star})$ to the normal distribution in Wasserstein or Kolmogorov distance. Non-asymptotic convergence rates were previously studied in [2] using the Stein method, but the resulting rate corresponds to a smoothed Wasserstein distance. Recent work [65] investigates convergence rates to the normal distribution in Wasserstein distance for LSA with Markovian observations. Both papers yield bounds that are less tight with respect to their dependence on trajectory length $n$ than those presented in the present work, see a detailed comparison after Theorem 2. ", "page_idx": 2}, {"type": "text", "text": "A popular method for constructing confidence intervals in the context of parametric estimation is based on the bootstrap approach ([19]). Its analysis has attracted many contributions, in particular a series of papers [12] and [13] that validate a bootstrap procedure for a test based on the maximum of a large number of statistics. Their study shows a close relationship between bootstrap validity results, Gaussian comparison and anticoncentration bounds for rectangular sets. The papers [64] and [27] investigate the applicability of likelihood-based statistics for finite samples and large parameter dimensions under possible model misspecification. The important step in proving bootstrap validity is again based on Gaussian comparison and anticoncentration bounds, but now for spherical sets. The bootstrap procedure for spectral projectors of covariance matrices is discussed in [46] and [31]. The authors follow the same steps to prove the validity of the bootstrap. ", "page_idx": 2}, {"type": "text", "text": "Extending the classical bootstrap approach to online learning algorithms is a challenge. For example, the iterates $\\{\\theta_{k}\\}_{k\\in\\mathbb{N}}$ determined by (1) are not necessarily stored in memory, which makes the classical bootstrap inapplicable. This problem can be solved by performing randomly perturbed updates of the online procedure, as proposed in [22] for the iterates of the Stochastic Gradient Descent (SGD) algorithm. The authors in [56] used the same procedure for the case of Markov noise and policy evaluation algorithms in reinforcement learning, but in both papers the authors only consider the asymptotic validity. In our paper we use the same multiplier bootstrap approach (see Section 4), but we provide an explicit error bound for the bootstrap approximation of the distribution of the statistics $\\sqrt{n}(\\bar{\\theta}_{n}-\\theta^{\\star})$ ", "page_idx": 2}, {"type": "text", "text": "In addition to the bootstrap approach, one can also use the pivotal statistics [37, 40, 41] or various estimates of the asymptotic covariance matrix [73] to construct the confidence intervals for $\\theta^{\\star}$ The latter approach can be based on the plug-in estimators [39], batch mean estimators [11] or in combination with the multiplier bootstrap approach [74]. However, the theoretical guarantees for mentioned methods remain purely asymptotic. ", "page_idx": 2}, {"type": "text", "text": "3  Accuracy of normal approximation for LSA ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first study the rate of normal approximation for the tail-averaged LSA procedure. When there is no risk of ambiguity, we shall use simply the notations ${\\bf A}_{n}={\\bf A}(Z_{n})$ and $\\mathbf{b}_{n}=\\mathbf{b}(Z_{n})$ .Starting from the definition (1), we get with elementary transformations that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\theta_{n}-\\theta^{\\star}=(\\mathrm{I}-\\alpha_{n}\\mathbf{A}_{n})(\\theta_{n-1}-\\theta^{\\star})-\\alpha_{n}\\varepsilon_{n}\\;,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where we have set $\\varepsilon_{n}=\\varepsilon(Z_{n})$ with ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\varepsilon(z)=\\tilde{\\bf A}(z)\\theta^{\\star}-\\tilde{\\bf b}(z)\\;,\\quad\\tilde{\\bf A}(z)={\\bf A}(z)-\\bar{\\bf A}\\;,\\quad\\tilde{\\bf b}(z)={\\bf b}(z)-\\bar{\\bf b}\\;\\;.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here the random variable $\\varepsilon(Z_{n})$ can be viewed as a noise, measured at the optimal point $\\theta^{\\star}$ Wenow assume the following technical conditions: ", "page_idx": 2}, {"type": "text", "text": "A1.Sequence $\\{Z_{n}\\}_{n\\in\\mathbb{N}}$ is a sequence of i.i.d. random variables defined on a probability space $\\textstyle(\\Omega,{\\mathcal{F}},\\mathbb{P})$ withdistribution $\\pi$ ", "page_idx": 2}, {"type": "text", "text": "A2. $\\begin{array}{r}{\\int_{Z}\\mathbf{A}(z)\\mathrm{d}\\pi(z)=\\bar{\\mathbf{A}}}\\end{array}$ and $\\begin{array}{r}{\\int_{Z}\\mathbf{b}(z)\\mathrm{d}\\pi(z)=\\bar{\\mathbf{b}}}\\end{array}$ with the matrix $-\\bar{\\mathbf{A}}$ being Hurwitz. Moreover, $\\begin{array}{r}{\\|\\varepsilon\\|_{\\infty}=\\operatorname*{sup}_{z\\in{Z}}\\|\\varepsilon(z)\\|<+\\infty,}\\end{array}$ and the mapping $z\\rightarrow\\mathbf{A}(z)$ is bounded, that is, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{C}_{\\mathbf{A}}=\\operatorname*{sup}_{z\\in\\mathbf{Z}}\\|\\mathbf{A}(z)\\|\\vee\\operatorname*{sup}_{z\\in\\mathbf{Z}}\\|\\tilde{\\mathbf{A}}(z)\\|<\\infty\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Moreover, for thenoise covariance matrix ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma_{\\varepsilon}=\\int_{Z}\\varepsilon(z)\\varepsilon(z)^{\\top}\\mathrm{d}\\pi(z)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "it holds that its smallest eigenvalue is bounded away from O, that is, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}:=\\lambda_{\\operatorname*{min}}(\\Sigma_{\\varepsilon})>0\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It is possible to change (4) to the moment-type bound as it was previously considered in [43] and [16], see the detailed discussion after Theorem 2. The fact that the matrix $-\\mathbf{\\bar{A}}$ is Hurwitz implies that the linear system $\\bar{\\mathbf{A}}\\theta=\\bar{\\mathbf{b}}$ has a unique solution $\\theta^{\\star}$ . Moreover, this fact is sufficient to show that the matrix $\\mathbf{I}-\\dot{\\alpha}\\bar{\\mathbf{A}}$ is a contraction in an appropriate matrix $Q$ -norm for small enough $\\alpha>0$ . Precisely, the following result holds: ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Let $-\\bar{\\mathbf{A}}$ be a Hurwitz matrix. Then for any $P=P^{\\top}\\succ\\mathrm{I},$ there exists a unique matrix $Q\\doteq Q^{\\top}\\succ\\mathrm{I},$ satisfying the Lyapunov equation $\\bar{\\mathbf{A}}^{\\top}Q+Q\\bar{\\mathbf{A}}=P.$ Moreover, setting ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{a=\\frac{\\lambda_{\\mathrm{min}}(P)}{2\\|Q\\|}\\;,\\quad a n d\\quad\\alpha_{\\infty}=\\frac{\\lambda_{\\mathrm{min}}(P)}{2\\kappa_{Q}\\|\\hat{\\mathbf{A}}\\|_{Q}^{2}}\\wedge\\frac{\\|Q\\|}{\\lambda_{\\mathrm{min}}(P)}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\kappa_{Q}=\\lambda_{\\mathrm{max}}(Q)/\\lambda_{\\mathrm{min}}(Q)$ , it holds for any $\\alpha\\in[0,\\alpha_{\\infty}]$ that $\\alpha a\\leq1/2$ and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\mathbf{I}-\\alpha\\bar{\\mathbf{A}}\\|_{Q}^{2}\\leq1-\\alpha a~.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The proof of Proposition 1 is provided in Appendix D.1. Note that it is possible to set $P=\\mathrm{I}$ as in [18], yet we will observe that other choices of $P$ could be more beneficial. Now consider an assumption on the step sizes $\\alpha_{k}$ and number of observations $n$ ", "page_idx": 3}, {"type": "text", "text": "A3. The step sizes $\\{\\alpha_{k}\\}_{k\\in\\mathbb{N}}$ has a form $\\alpha_{k}=c_{0}/k^{\\gamma}$ where $\\gamma\\in[1/2;1)$ and $c_{0}\\in(0;\\alpha_{\\infty}]$ .Moreover, we assume that $n\\geq d$ and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\frac{\\sqrt{n}}{(1+\\log n)\\log n}\\ge\\frac{c_{0}\\kappa_{Q}\\,\\mathrm{C}_{\\bf A}^{2}}{a(1-\\sqrt{2}/2)}\\lor\\frac{4}{a c_{0}(1-\\sqrt{2}/2)}\\ ,\\ i f\\gamma=1/2\\ ,\\right.\\qquad}\\\\ {\\left.\\frac{n^{1-\\gamma}}{\\log n}\\ge\\frac{2c_{0}\\kappa_{Q}\\,\\mathrm{C}_{\\bf A}^{2}}{a(2\\gamma-1)(1-(1/2)^{1-\\gamma})}\\lor\\frac{8\\gamma(1-\\gamma)}{a c_{0}(1-(1/2)^{1-\\gamma}}\\ ,\\ i f1/2<\\gamma<1\\ .}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The main aim of lower bounding $n$ is to ensure that the number of observations is large enough in order that the LSA error related to the choice of initial condition $\\theta_{0}-\\theta^{\\star}$ becomes small. ", "page_idx": 3}, {"type": "text", "text": "3.1  Central limit theorem for Polyak-Ruppert averaged LSA iterates. ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "It is known that the assumptions A1-A3 guarantee that the CLT applies to the iterates of $\\bar{\\theta}_{n}$ ,namely, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sqrt{n}(\\bar{\\theta}_{n}-\\theta^{\\star})\\stackrel{d}{\\rightarrow}\\mathcal{N}(0,\\Sigma_{\\infty})\\;,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the asymptotic covariance matrix $\\Sigma_{\\infty}$ has a form ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma_{\\infty}=\\bar{\\mathbf{A}}^{-1}\\Sigma_{\\varepsilon}\\bar{\\mathbf{A}}^{-\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $\\Sigma_{\\varepsilon}$ is defined in (5). This result can be found for example in [53] and [23]. We are interested in the Berry-Esseen type bound for the rateof convergence in (10), that is, we aim to bound $\\rho_{n}^{\\mathrm{Conv}}$ defined in (2) w.r.t. the available sample size $n$ We control $\\rho_{n}^{\\mathrm{Conv}}$ using a method from [63] based on randomized multivariate concentration inequality. Below we briefly state its setting and required definitions. Let $X_{1},\\ldots,X_{n}$ be independent random variables taking values on $\\mathcal{X}$ and $T\\,{\\dot{=}}\\,T(X_{1},\\cdot\\cdot\\cdot,X_{n})$ be a general $d$ -dimensional statistics such that $T=W+D$ , where ", "page_idx": 3}, {"type": "equation", "text": "$$\nW=\\sum_{\\ell=1}^{n}\\xi_{\\ell},\\quad D:=D(X_{1},\\ldots,X_{n})=T-W,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\xi_{\\ell}=h_{\\ell}(X_{\\ell})$ and $h_{\\ell}:\\mathcal{X}\\to\\mathbb{R}^{d}$ is a Borel measurable function. Here the statistics $D$ can be non-linear and is treated as an error term, which is \"small\" compared to $W$ in an appropriate sense. Assume that $\\mathbb{E}[\\xi_{\\ell}]=0$ and $\\begin{array}{r}{\\sum_{\\ell=1}^{n}\\mathbb{E}[\\xi_{\\ell}\\xi_{\\ell}^{\\top}]=\\mathrm{I}_{d}}\\end{array}$ .Let $\\begin{array}{r}{\\Upsilon=\\Upsilon_{n}=\\'\\sum_{\\ell=1}^{n}\\mathbb{E}[\\|\\xi_{\\ell}\\|^{3}]}\\end{array}$ . Then, with $\\eta\\sim\\mathcal{N}(0,\\mathrm{I}_{d})$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{B\\in\\mathrm{Conv}(\\mathbb{R}^{d})}|\\mathbb{P}(T\\in A)-\\mathbb{P}(\\eta\\in A)|\\le259d^{1/2}\\Upsilon+2\\mathbb{E}[\\|W\\|\\|D\\|]+2\\sum_{\\ell=1}^{n}\\mathbb{E}[\\|\\xi_{\\ell}\\|\\|D-D^{(\\ell)}\\|],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $D^{(\\ell)}=D(X_{1},\\ldots,X_{\\ell-1},X_{\\ell}^{\\prime},X_{\\ell+1},\\ldots,X_{n})$ and $X_{\\ell}^{\\prime}$ is an independent copy of $X_{\\ell}$ . This result is due to [63, Theorem 2.i]. One can modify the bound (13) for the setting when $\\begin{array}{r}{\\sum_{\\ell=1}^{n}\\mathbb{E}[\\xi_{\\ell}\\xi_{\\ell}^{\\top}]~=~\\Sigma~\\succ~0}\\end{array}$ .This result due to [63, Corollary 2.3]. Following the construction (12), we set $T=\\sqrt{n}\\bar{\\mathbf{A}}(\\bar{\\theta}_{n}-\\theta^{\\star})$ and consider it as a nonlinear statistic of i.i.d. random variables $Z_{1},\\ldots,Z_{2n}$ , which drive the LSA dynamics (1). We can exactly represent $T$ as a sum of linear $(W)$ and non-linear parts $(D)$ , where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W=-\\displaystyle\\frac{1}{\\sqrt{n}}\\sum_{k=n}^{2n-1}\\varepsilon_{k+1},\\quad D=\\displaystyle\\frac{1}{\\sqrt{n}}\\frac{\\theta_{n}-\\theta^{\\star}}{\\alpha_{n}}-\\displaystyle\\frac{1}{\\sqrt{n}}\\frac{\\theta_{2n}-\\theta^{\\star}}{\\alpha_{2n}}-\\displaystyle\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}\\bigl({\\bf A}_{k}-\\bar{\\bf A}\\bigr)\\bigl(\\theta_{k-1}-\\theta^{\\star}\\bigr)}\\\\ {+\\displaystyle\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}\\bigl(\\theta_{k-1}-\\theta^{\\star}\\bigr)\\left(\\displaystyle\\frac{1}{\\alpha_{k}}-\\displaystyle\\frac{1}{\\alpha_{k-1}}\\right).\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof of this result can be bound in Proposition 3. To obtain a bound for the approximation accuracy in (2) using the bound (13), we need to upper bound $\\mathbb{E}^{1/2}[\\|D(Z_{1},\\ldots,Z_{2n})\\|^{2}]$ and $\\mathbb{E}[\\|D-$ $D^{(i)}\\|]$ . The first result below provides a second moment bound on $D$ ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Assume A1, A2, and A3. Then we obtain the following error bound: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{1/2}\\left[\\|D(Z_{1},\\ldots,Z_{n})\\|^{2}\\right]\\lesssim\\frac{\\sqrt{\\kappa_{Q}}\\|\\varepsilon\\|_{\\infty}}{\\sqrt{a}}\\left(\\frac{1}{n^{(1-\\gamma)/2}}+\\frac{\\mathrm{C}_{\\mathbf{A}}}{n^{\\gamma/2}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left(\\frac{\\sqrt{\\kappa_{Q}}\\,\\mathrm{C}_{\\mathbf{A}}}{n^{(1-\\gamma)/2}\\sqrt{a}}+\\frac{\\sqrt{\\kappa_{Q}}\\,n^{2\\gamma-1}}{a}\\right)\\exp\\biggl\\{-\\frac{c_{0}a n^{1-\\gamma}}{2(1-\\gamma)}\\biggr\\}\\|\\theta_{0}-\\theta^{\\star}\\|~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lesssim$ stands for inequality up to an absolute constant. ", "page_idx": 4}, {"type": "text", "text": "The proof of Theorem 1 is provided in Appendix A.3. Now it remains to upper bound the term $\\mathbb{E}[\\|\\bar{D}-D^{(i)}\\|]$ , which is done in Appendix B.1 using the synchronous coupling methods [10]. Combining these bounds, we obtain the following theorem: ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Assume A1, A2, and A3. Then the following bound holds: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\rho_{n}^{\\mathrm{Conv}}\\lesssim\\frac{d^{1/2}\\|\\varepsilon\\|_{\\infty}^{3}}{\\lambda_{\\operatorname*{min}}^{3/2}\\sqrt{n}}+\\frac{1}{\\lambda_{\\operatorname*{min}}}\\left(\\frac{C_{1}}{n^{(1-\\gamma)/2}}+\\frac{C_{2}}{n^{\\gamma/2}}\\right)+\\frac{\\Delta_{1}}{\\lambda_{\\operatorname*{min}}}\\exp\\biggl\\{-\\frac{c_{0}a n^{1-\\gamma}}{2(1-\\gamma)}\\biggr\\}\\|\\theta_{0}-\\theta^{\\star}\\|~,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Delta_{1}=\\Delta_{1}(n,a,\\mathrm{C}_{\\bf A},\\mathrm{Tr}\\,\\Sigma_{\\varepsilon})$ is a polynomial function defined in (31), and constants $\\mathsf{C}_{1},\\mathsf{C}_{2}$ depending upon $a,\\mathrm{C}_{\\mathbf{A}},\\kappa_{Q},\\mathrm{Tr}\\,\\Sigma_{\\varepsilon},$ . are defined in (32). ", "page_idx": 4}, {"type": "text", "text": "The proof of Theorem 2 is provided in Appendix B. Note that the assumption A2 requires that $\\varepsilon(Z_{1}\\bar{)}$ is almost sure bounded. It is a strong assumption, but can be partially relaxed. Following the stability of matrix products technique, used in [17, Proposition 3], it is possible to consider the setting when the random variable $\\|\\tilde{\\mathbf{A}}(Z_{1})\\|$ has only finite number of moments. In particular, a finite third moment of $\\|\\tilde{\\mathbf{A}}(Z_{1})\\|$ implies that $\\Vert\\varepsilon(Z_{1})\\Vert$ also has a finite third moment, which is sufficient to obtain a counterpart to Theorem 1. However, this generalization requires non-trivial technical work on generalizing the stability of matrix products result (see Corollary 4 in Appendix D ). ", "page_idx": 4}, {"type": "text", "text": "Note that the bound of Theorem 2 predicts the optimal error of normal approximation for PolyakRuppert averaged estimates of order $n^{-1/4}$ , which is achieved with the aggressive step size $\\alpha_{k}=$ $c_{0}/{\\sqrt{k}}$ , that is, when setting $\\gamma=1/2$ in (14). In this case we obtain the optimized bound ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\rho_{n}^{\\mathrm{Conv}}\\lesssim\\frac{\\mathsf{C}_{3}}{\\lambda_{\\mathrm{min}}n^{1/4}}+\\frac{d^{1/2}\\|\\varepsilon\\|_{\\infty}^{3}}{\\lambda_{\\mathrm{min}}^{3/2}\\sqrt{n}}+\\frac{\\Delta_{1}\\exp\\bigl\\{-c_{0}a\\sqrt{n}\\bigr\\}}{\\lambda_{\\mathrm{min}}}\\|\\theta_{0}-\\theta^{\\star}\\|\\ ,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ${\\mathsf C}_{3}={\\mathsf C}_{3}(a,\\mathrm{C}_{\\mathbf A},\\kappa_{Q},\\mathrm{Tr}\\,\\Sigma_{\\varepsilon},\\|\\varepsilon\\|_{\\infty})$ is provided in (32). ", "page_idx": 4}, {"type": "text", "text": "Discussion. Our proof technique of Theorem 2 reveals an interesting feature: fastest rate of convergence in the convex distance $\\rho_{n}^{\\mathrm{Conv}}$ corresponds to the learning rate schedule that admits the fastest decay of the second-order term in the MSE bound for remainder statistics $D$ (see Theorem 1). Results similar to the one of Theorem 2 have been recently obtained in the literature in [65] and [2]. The author in [65] considers the LSA problem specified to the temporal-difference learning (see Section 5) with Markov noise and obtains convergence rate in Wasserstein distance of order $\\bar{n}^{-1/4}$ \uff0c which corresponds to the \"optimal\" step size schedule $\\alpha_{k}=c_{0}/k^{3/4}$ . Using the bound of [49, eq. (3)] (see also section 2 in [57]), this result yield a suboptimal bound of order $n^{-1/8}$ for the convex distance $\\rho_{n}^{\\mathrm{Conv}}$ .Such an upperbound may be loose for some classes of distributions, but it is not clear if in particular setting of LSA the bound of [65] could imply scaling of order $n^{-1/4}$ for $\\rho_{n}^{\\mathrm{Conv}}$ . At the same time, in case of $X_{1},\\ldots,X_{n}$ forming a Markov chain in (12) there is no available counterpart of the bound (13). Generalizing (13) is an interesting research direction that would allow to obtain a counterpart of Theorem 2 in case of Markovian dynamics. Similarly, the result of [2] holds for much stronger metrics, which controls the convergence of moments of twice differentiable functions. We provide additional details about connections between this metric and $\\rho_{n}^{\\mathrm{Conv}}$ in Appendix B.2. At the same time, the authors in [2] cover the non-linear setting of PR-averaged iterates of stochastic gradient descent algorithm under strong convexity. ", "page_idx": 5}, {"type": "text", "text": "Remark 1. The leading (with respect to $n$ )termsof theboundfromTheorem1have animplicit dependenceontheproblemdimension $d$ due to the presence of $\\lambda_{\\operatorname*{min}}$ Yet theresult ofTheorem $^{\\,I}$ can be improved in a sense of dependence in dimension if one is interested not in the rates of convergence for $\\hat{\\sqrt{n}}(\\bar{\\theta}_{n}-\\theta^{\\star})$ but in tpjtediteatd ${\\sqrt{n}}\\dot{\\Pi}^{\\top}({\\bar{\\theta}}_{n}-\\theta^{\\star})$ for some $\\Pi\\in\\mathbb{R}^{d\\times m}$ $m\\leq d.$ If this is the case, one may apply (13) for the class $\\mathrm{Conv}_{m}=\\mathrm{Conv}(\\mathbb{R}^{m})$ of convex sets in $\\mathbb{R}^{m}$ and obtain, setting step size $\\alpha_{k}=c_{0}/\\sqrt{k}$ and $\\Sigma_{\\varepsilon}^{\\left(\\Pi\\right)}=\\Pi\\Sigma_{\\varepsilon}\\Pi^{\\top}$ , that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\rho_{n}^{\\mathrm{Conv}}\\lesssim\\frac{\\mathsf{C}_{4}}{\\lambda_{\\mathrm{min}}n^{1/4}}+\\frac{m^{1/2}\\|\\varepsilon\\|_{\\infty}^{3}}{\\lambda_{\\mathrm{min}}^{3/2}\\sqrt{n}}+\\frac{\\Delta_{1}\\mathrm{e}^{-c_{0}a\\sqrt{n}}}{\\lambda_{\\mathrm{min}}}\\|\\theta_{0}-\\theta^{\\star}\\|~,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and the constant $\\mathsf{C}_{4}=\\mathsf{C}_{4}(a,\\mathrm{C}_{\\mathbf{A}},\\kappa_{Q},\\mathrm{Tr}\\,\\Sigma_{\\varepsilon}^{\\mathrm{(II)}},\\|\\varepsilon\\|_{\\infty})$ is provided in (32). ", "page_idx": 5}, {"type": "text", "text": "Remark 2. Results similar to Theorem $^{\\,l}$ can be obtained not only for the Polyak-Ruppert averaged estimator $\\bar{\\theta}_{n}$ ,but alsofor the last iterate $\\theta_{n}$ . In particular, it is known (see e.g. [23]), that the last iterateerror $\\theta_{n}-\\theta^{\\star}$ is also asymptotically normal: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\theta_{n}-\\theta^{*}}{\\sqrt{\\alpha_{n}}}\\to\\mathcal{N}(0,\\Sigma_{\\mathrm{last}})\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the covariance matrix $\\Sigma_{\\mathrm{last}}$ is differentfrom $\\Sigma_{\\infty}$ .In such a case $\\Sigma_{\\mathrm{last}}$ can be found as a solution to appropriate Lyapunov equation, see [23]. Then, we can use the perturbation-expansion techniquefrom $I I J,$ and, applying similar technique of randomized concentration inequalities $I63J$ (see (13)), we can obtain the Berry-Esseen bound ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{sup}_{B\\in\\mathrm{Conv}(\\mathbb{R}^{d})}\\left|\\mathbb{P}\\big(\\frac{\\theta_{n}-\\theta^{*}}{\\sqrt{\\alpha_{n}}}\\in B\\big)-\\mathbb{P}\\big(\\Sigma_{\\mathrm{last}}^{1/2}\\eta\\in B\\big)\\right|\\lesssim\\sqrt{\\alpha_{n}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4  Multiplier bootstrap for LSA ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In order to perform statistical inference with the Polyak-Ruppert estimator $\\bar{\\theta}_{n}$ , we propose an online bootstrap resampling procedure, which recursively updates the LSA estimate as well as a large number of randomly perturbed LSA estimates, upon the arrival of each data point. The suggested procedure follows the one outlined in [22]. It has the following advantages: it does not rely on the asymptotic distribution of the error $\\bar{\\sqrt{n}}(\\bar{\\theta}_{n}-\\theta^{\\star})$ , does not require to know the moments of $\\sqrt{n}(\\bar{\\theta}_{n}^{\\cdot}-\\bar{\\theta}^{\\star})$ or its asymptotic covariance matrix $\\Sigma_{\\infty}$ , and does not involve any data splitting. ", "page_idx": 5}, {"type": "text", "text": "We state the suggested procedure as follows. Let $\\mathcal{W}^{2n}\\,=\\,\\{W_{\\ell}\\}_{1\\leq\\ell\\leq2n}$ be a set of i.d. random variables, independent of $\\mathcal{Z}^{2n}\\,=\\,\\{Z_{\\ell}\\}_{1\\leq\\ell\\leq2n}$ with $\\mathbb{E}[W_{1}]\\,=\\,1$ and $\\mathrm{Var}[W_{1}]\\,=\\,1\\$ .We write, respectively, $\\mathbb{P}^{\\flat}\\,=\\,\\mathbb{P}(\\cdot|\\mathcal{Z}^{2n})$ and $\\mathbb{E}^{\\flat}\\,=\\,\\mathbb{E}(\\cdot|\\mathcal{Z}^{2n})$ for the corresponding conditional probability and expectation. In parallel with procedure (i) that generates $\\{\\theta_{k}\\}_{1\\leq k\\leq2n}$ and $\\bar{\\theta}_{n}$ , we generate $M$ independent samples $(w_{n}^{\\ell},\\dots,w_{2n}^{\\ell})$ \uff0ch $1\\leq\\ell\\leq M$ distribted as $\\mathcal{W}^{2n}$ , and recursivly update $M$ randomly perturbed LSA estimates, that is, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{k}^{\\mathbf{b},\\ell}=\\theta_{k-1}^{\\mathbf{b},\\ell}-\\alpha_{k}w_{k}^{\\ell}\\{\\mathbf{A}(Z_{k})\\theta_{k-1}^{\\mathbf{b},\\ell}-\\mathbf{b}(Z_{k})\\}\\;,\\;\\;k\\geq n+1\\;,\\;\\;\\theta_{n}^{\\mathbf{b},\\ell}=\\theta_{n}\\;,}\\\\ &{\\bar{\\theta}_{n}^{\\mathbf{b},\\ell}=n^{-1}\\sum_{k=n}^{2n-1}\\theta_{k}^{\\mathbf{b},\\ell}\\;,\\;\\;n\\geq1\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We use a short notation ${\\bar{\\theta}}_{n}^{\\mathsf{b}}$ for $\\bar{\\theta}_{n}^{\\mathsf{b},1}$ . The key idea of the procedure (15) is that the \"Bootstrap-world\" distribution (that is, the one conditional on $\\mathcal{Z}^{2n}$ ) of the perturbed samples ${\\sqrt{n}}({\\bar{\\theta}}_{n}^{\\mathsf{\\bar{b}}}-{\\bar{\\theta}}_{n})$ is close to the distribution of the quantity of interest, that is, ${\\sqrt{n}}({\\dot{\\bar{\\theta}}}_{n}-\\theta^{\\star})$ . Precisely, the main result of this section will show that the quantity ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{B\\in\\mathrm{Conv}(\\mathbb{R}^{d})}|\\mathbb{P}^{\\mathrm{b}}(\\sqrt{n}(\\bar{\\theta}_{n}^{\\mathrm{b}}-\\bar{\\theta}_{n})\\in B)-\\mathbb{P}(\\sqrt{n}(\\bar{\\theta}_{n}-\\theta^{\\star})\\in B)|\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "is small. Although an analytic expression for $\\mathbb{P}^{\\mathtt{b}}(\\sqrt{n}(\\bar{\\theta}_{n}^{\\mathtt{b}}-\\bar{\\theta}_{n})\\,\\in\\,B)$ is not available, one can approximate it from numerical simulations according to (15) by generating sufficiently large number $M$ of perturbed trajectories. Standard arguments, see e.g. [62, Section 5.1] suggest that the accuracy of Monte-Carlo approximation is of order $M^{-1/2}$ . To analyze the suggested procedure, we shall impose an additional assumption on the trajectory length $n$ ", "page_idx": 6}, {"type": "text", "text": "A4. Assumption $A3$ holds with $\\gamma=1/2$ and $c_{0}\\leq1/(\\mathrm{C}_{\\mathbf{A}}^{2}\\,\\kappa_{Q}\\mathrm{e})$ Moreover, setting ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h(n)=\\left\\lceil\\left(\\frac{4\\,\\mathrm{C}_{\\mathbf{A}}\\,\\kappa_{Q}^{1/2}}{(\\sqrt{2}-1)a}\\right)^{2}(1+2\\log\\left(2n^{4}\\right))^{2}\\right\\rceil\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "it holds that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\sqrt{n}}{h(n)}\\geq\\frac{2}{a(\\sqrt{2}-1)}\\lor\\frac{c_{0}}{\\alpha_{\\infty}}\\ ,\\ a n d\\ \\frac{\\sqrt{n}}{\\log^{2}n}\\geq\\frac{c_{0}(1\\lor\\mathbf{C}_{\\mathbf{A}}^{2})}{a}\\lor c_{0}a\\,\\mathrm{C}_{\\mathbf{A}}^{2}\\lor\\frac{4}{a c_{0}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Moreover, we assume that for $\\lambda_{\\mathrm{min}}$ defined in (6) it holds that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\lambda_{\\mathrm{min}}\\geq8\\|\\varepsilon\\|_{\\infty}\\sqrt{\\frac{\\|\\Sigma_{\\varepsilon}\\|\\log n}{n}}+\\frac{8(\\|\\Sigma_{\\varepsilon}\\|+\\|\\varepsilon\\|_{\\infty}^{2})\\log n}{n}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that the new bound (18) simply states that ${\\sqrt{n}}/\\log^{2}(n)$ is sufficiently large, since $h(n)$ scales as $\\log^{2}n$ . We discuss the assumption A4 in more details in the proof scheme. Now we formulate the main result of this section. We analyze only the setting of polynomially decaying step size with $\\gamma=1/2$ , since decay rate of (16) essentially depends on the approximation rate of Theorem 2, with the fastest rate achieved when $\\gamma=1/2$ . For other learning rates the decay rate of right-hand side in Theorem 3 will be slower. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Assume A1, A2, A3 with $\\gamma=1/2$ and $A4.$ Then with $\\mathbb{P}$ - probability at least $1-6/n$ holdsthat ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{^{8}\\in\\mathrm{Conv}(\\mathbb{R}^{d})}{\\operatorname*{sup}}\\vert\\mathbb{P}^{\\mathbb{b}}(\\sqrt{n}(\\bar{\\theta}_{n}^{\\mathbb{b}}-\\bar{\\theta}_{n})\\in B)-\\mathbb{P}(\\sqrt{n}(\\bar{\\theta}_{n}-\\theta^{\\star})\\in B)\\vert\\lesssim\\frac{\\kappa_{Q}^{2}(\\mathrm{CA}_{\\mathbf{A}}^{4}\\vee1)(1+\\Vert\\varepsilon\\Vert_{\\infty}^{2})\\log n}{a^{5/2}\\lambda_{\\operatorname*{min}}n^{1/4}}}\\\\ &{\\qquad+\\,\\frac{\\sqrt{d}}{\\sqrt{n}}\\left(\\frac{\\Vert\\varepsilon\\Vert_{\\infty}^{3}}{\\lambda_{\\operatorname*{min}}^{3/2}}+\\kappa_{Q}\\Vert\\varepsilon\\Vert_{\\infty}\\frac{\\sqrt{\\log n}}{\\sqrt{\\lambda_{\\operatorname*{min}}}}+\\frac{\\kappa_{Q}(1+\\Vert\\varepsilon\\Vert_{\\infty}^{2}/\\lambda_{\\operatorname*{min}})\\log n}{\\sqrt{n}}\\right)+\\frac{\\Delta_{2}\\mathrm{e}^{-(c_{0}/2)a\\sqrt{n}}}{\\lambda_{\\operatorname*{min}}}\\Vert\\theta_{0}-\\theta^{\\star}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Delta_{2}=\\Delta_{2}(n,a,\\mathrm{C}_{\\bf A},\\|\\varepsilon\\|_{\\infty})$ is a polynomial function defined in (42). ", "page_idx": 6}, {"type": "text", "text": "The proof of Theorem 3 is based on the Gaussian approximation performed both in the \"real\" world and bootstrap world together with an appropriate Gaussian comparison inequality. The main steps of the proof are illustrated by the following scheme: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~~~\\sqrt{n}\\bar{\\mathbf{A}}(\\bar{\\theta}_{n}-\\theta^{\\star})\\;\\longleftarrow\\;\\xrightarrow[]{\\mathrm{Gaussian\\,\\,approx\\,in\\,\\,antion\\,,\\,Th.\\,2}}\\;\\xi\\sim\\mathcal{N}(0,\\Sigma_{\\varepsilon})}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\Big\\uparrow\\mathrm{Gaussian\\,\\,comparison,\\,Th}}\\\\ &{~~~~~~~~~~~~~\\sqrt{n}\\bar{\\mathbf{A}}(\\bar{\\theta}_{n}^{\\mathrm{b}}-\\bar{\\theta}_{n})\\;\\stackrel{\\mathrm{Gaussian\\,\\,approx.\\,\\,in\\,\\,Bootstrap\\,\\,world,\\,Th.\\,\\,4}}{\\longleftarrow\\,}\\xi^{\\mathrm{b}}\\sim\\mathcal{N}(0,\\Sigma_{\\varepsilon}^{\\mathrm{b}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In the above scheme we have denoted by \u2265b = n-1 2m-1 E en, Eee the sample covariance matrix approximating $\\Sigma_{\\varepsilon}$ . Gaussian approximation for the true distribution of ${\\sqrt{n}}{\\bar{\\mathbf{A}}}({\\bar{\\theta}}_{n}-\\theta^{\\star})$ follows from Theorem 2. Proof of Gaussian approximation in the Bootstrap world Theorem 4 is also based on the inequality (13), but is more complicated and involves the expansion analysis of the LSA error from [1]. This technique allows to separate the LSA error into different scales with respect to the step sizes $\\{\\alpha_{k}\\}$ , see Appendix C.4 for details. However, this technique requires to impose additional assumption A4 - eq. (18). Proof of the Gaussian comparison part of Theorem 5 is based on Pinsker's inequality and matrix Bernstein inequality. The latter result requires that $n$ is large enough to ensure that minimal eigenvalue of $\\Sigma_{\\varepsilon}^{\\mathsf{b}}$ is closeto $\\lambda_{\\operatorname*{min}}$ , justifying the assumption A4 - eq. (19). Detailed proof if provided in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Discussion. We emphasize that the Gaussian approximation result of Theorem 2 (with Bootstrap world generalization in Theorem 4) is a key result to prove the above bootstrap validity. This argument was missing in the earlier works studying confidence intervals for stochastic optimization algorithms [11, 73, 74], where the authors considered procedures to estimate $\\Sigma_{\\infty}$ in (11). They combine nonasymptotic bounds on the accuracy of recovering $\\Sigma_{\\infty}$ with only asymptotic validity of the resulting confidence intervals. We expect that our proof technique for Theorem 2 can be used to provide similar non-asymptotic validity results for outlined approaches for constructing confidence intervals based on the estimation of the asymptotic covariance matrix. ", "page_idx": 7}, {"type": "text", "text": "Corollary 1. (Set of Euclidean balls or ellipsoids) Suppose that we are interested in estimating quantile of a given order $\\alpha\\in(0,1)$ and somematrix $\\bar{B}\\in\\mathbb{R}^{d\\times d}$ that is,thequantity ", "page_idx": 7}, {"type": "equation", "text": "$$\nt_{\\alpha}=\\operatorname*{inf}\\{t>0:\\mathbb{P}(\\sqrt{n}\\|B(\\bar{\\theta}_{n}-\\theta^{\\star})\\|\\geq t)\\leq\\alpha\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We define its counterpart in the Bootstrap world, $t_{\\alpha}^{\\flat}$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\nt_{\\alpha}^{\\mathsf{b}}=\\operatorname*{inf}\\{t>0:\\mathbb{P}^{\\mathsf{b}}(\\sqrt{n}\\|B(\\bar{\\theta}_{n}^{\\mathsf{b}}-\\bar{\\theta}_{n})\\|\\geq t)\\leq\\alpha\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Notethat $t_{\\alpha}^{\\flat}$ is defines with respect to the bootstrap measure, therefore, it depends onthe data $\\mathcal{Z}^{2n}$ This bootstrap critical value $t_{\\alpha}^{\\flat}$ is applied in the Bootstrap world to build the confidence set ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}(\\alpha)=\\{\\theta\\in\\mathbb{R}^{d}:\\sqrt{n}\\|B(\\theta-\\bar{\\theta}_{n})\\|\\leq t_{\\alpha}^{\\mathsf{b}}\\}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Theorem 3 justifies this construction and evaluate the coverage probability of the true value $\\theta^{\\star}$ by thisset.Itstatesthat ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\theta^{\\star}\\notin\\mathcal{E}(\\alpha))=\\mathbb{P}(\\sqrt{n}\\|B(\\bar{\\theta}_{n}-\\theta^{\\star})\\|>t_{\\alpha}^{\\ b})\\approx\\alpha\\;,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with the error of order $n^{-1/4}$ in the right-hand side. Although an analytic expression for $t_{\\alpha}^{\\flat}$ isnot available, one can approximate it by generating a large number $M$ of independent samplesof $\\mathcal{W}_{n}$ and computing from them the empirical distribution function of ${\\sqrt{n}}\\|{\\dot{B}}({\\bar{\\theta}}_{n}^{\\mathsf{b}}-{\\bar{\\theta}}_{n})\\|$ ,following (15). ", "page_idx": 7}, {"type": "text", "text": "Remark 3. A natural question that arises after Theorem 3 is whether it is possible to prove similar bounds for the iterates of first-order stochastic optimization algorithms. There are several MSE bounds for corresponding algorithms with explicit dependence on the step size $\\alpha_{k}$ ;see,forexample, [45, 5]. Therefore, we expect that it is possible to obtain a counterpart to Theorem 2. At the same time,for general first-order stochastic optimization algorithms, unlike LSA,there are no counterparts to the precise error expansions of [1]. Thus, proving the counterpart of Theorem 3 in this setting is more challenging. Similarly, we emphasize that generalizations of the procedure (15) to cases where $\\{Z_{k}\\}_{k\\in\\mathbb{N}}$ are dependent, for example, form a Markov chain, are complicated. The approach of [63] is not directly applicable in this setting, and appropriate generalization of (13) is a separate and challengingresearch direction. ", "page_idx": 7}, {"type": "text", "text": "5   Applications to the TD learning and numerical results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We illustrate our findings for the setting of temporal difference (TD) learning algorithm [66, 67] for policy evaluation in RL. Non-asymptotic error bounds for this algorithm attracted lot of contributions [43, 16, 30, 51, 38]. At the same time, confidence intervals for TD were studied in [22, 56] only in terms of their asymptotic validity. In the TD algorithm we consider a discounted MDP (Markov Decision Process) given by a tuple $(S,\\mathcal{A},\\mathrm{P},r,\\bar{\\gamma})$ . Here $\\boldsymbol{S}$ and $\\boldsymbol{\\mathcal{A}}$ stand for state and action spaces, and $\\gamma\\,\\in\\,(0,1)$ is a discount factor.Assume that $\\boldsymbol{S}$ is a complete metric space with metric ${\\mathsf{d}}_{S}$ and Borel $\\sigma$ -algebra $B(S)$ $\\mathrm{P}$ stands for the transition kernel $\\mathrm{P}(B|s,a)$ , which determines the probability of moving from state $s$ to a set $B\\in B(S)$ when action $a$ is performed. Reward function $\\bar{\\boldsymbol{r}}\\colon\\boldsymbol{S}\\times\\dot{\\boldsymbol{A}}\\rightarrow[0,1]$ is assumed to be deterministic. Policy $\\pi(\\cdot|s)$ is the distribution over action space $\\boldsymbol{\\mathcal{A}}$ corresponding to agent's action preferences in state $s\\in S$ . We aim to estimate value function ", "page_idx": 7}, {"type": "equation", "text": "$$\nV^{\\pi}(s)=\\mathbb{E}\\bigl[\\sum_{k=0}^{\\infty}\\gamma^{k}r(s_{k},a_{k})|s_{0}=s\\bigr]\\;,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $a_{k}\\sim\\pi(\\cdot|s_{k})$ , and $s_{k+1}\\sim\\mathrm{P}(\\cdot|s_{k},a_{k})$ for any $k\\in\\mathbb{N}$ . Define the transition kernel under $\\pi$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{P}_{\\pi}(B|s)=\\int_{\\cal A}\\mathrm{P}(B|s,a)\\pi(\\mathrm{d}a|s)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "which corresponds to the 1-step transition probability from state $s$ to a set $B\\in B(S)$ . The state space $\\boldsymbol{S}$ here can be arbitrary. It is a common option to consider the linear function approximation for $V^{\\pi}(s)$ , defined for $s\\in S$ $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ , and a feature mapping $\\varphi\\colon S\\to\\mathbb{R}^{d}$ as $V_{\\theta}^{\\pi}(s)\\stackrel{\\bullet}{=}\\varphi^{\\top}(s)\\theta$ . Here $d$ is the dimension of feature space. Our goal is to find a parameter $\\theta^{\\star}$ which is defined as a unique solution to the projected Bellman equation, see [70], which defines the best linear approximation of $V^{\\pi}$ . We denote by $\\mu$ the invariant distribution over the state space $\\boldsymbol{S}$ induced by $\\mathrm{P}^{\\pi}\\bar{(\\cdot|_{\\it s})}$ in (20). We define the design matrix $\\Sigma_{\\varphi}$ as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\Sigma_{\\varphi}=\\mathbb{E}_{\\mu}[\\varphi(s)\\varphi(s)^{\\top}]\\in\\mathbb{R}^{d\\times d}\\;.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Consider the following assumptions on the generative mechanism and on the feature mapping $\\varphi(\\cdot)$ ", "page_idx": 8}, {"type": "text", "text": "TD 1. Tuples $(s,a,s^{\\prime})$ are generated i.i.d.with $s\\sim\\mu_{\\l}$ $,\\,a\\sim\\pi(\\cdot|s),\\,s^{\\prime}\\sim\\mathrm{P}(\\cdot|s,a)$ ", "page_idx": 8}, {"type": "text", "text": "TD 2. Matrix $\\Sigma_{\\varphi}$ is non-degenerate with the minimal eigenvalue $\\lambda_{\\operatorname*{min}}(\\Sigma_{\\varphi})>0$ Moreover, the feature mapping $\\varphi(\\cdot)$ satisfies $\\operatorname*{sup}_{s\\in{\\cal S}}\\|\\varphi(s)\\|\\leq1$ ", "page_idx": 8}, {"type": "text", "text": "In the setting of linear function approximation the estimation of $V^{\\pi}(s)$ reduces to estimating $\\theta^{\\star}\\in\\mathbb{R}^{d}$ which can be done via the LSA procedure. Here, the $k$ -th step randomness is given by the tuple $Z_{k}=(s_{k},a_{k},s_{k}^{\\prime})$ . Then, the corresponding LSA update can be written as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\theta_{k}=\\theta_{k-1}-\\alpha_{k}(\\mathbf{A}_{k}\\theta_{k-1}-\\mathbf{b}_{k})\\mathrm{~,~}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where ${\\bf A}_{k}$ and $\\mathbf{b}_{k}$ are given, respectively, by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{A}_{k}=\\varphi(s_{k})\\{\\varphi(s_{k})-\\gamma\\varphi(s_{k}^{\\prime})\\}^{\\top}\\;,\\quad\\mathbf{b}_{k}=\\varphi(s_{k})r(s_{k},a_{k})\\;.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We provide the expressions for the corresponding system matrix $\\bar{\\mathbf{A}}=\\mathbb{E}[\\mathbf{A}_{k}]$ and the right-hand side $\\mathbf{\\bar{b}}$ in Appendix E. We verify that assumption A2 holds and, furthermore, we provide a tighter counterpart to the result of Proposition 1. This result closely follows [51] and [61]. ", "page_idx": 8}, {"type": "text", "text": "Proposition 2. Let $\\{\\theta\\}_{k\\in\\mathbb{N}}$ be a sequence of $T D$ updates generated by (22) under TD 1 and ${\\mathbf{\\nabla}T\\mathbf{D}\\,2}$ Then this update scheme satisfies assumption A2 with ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{C}_{\\bf A}=2(1+\\gamma)\\;,\\quad\\|\\varepsilon\\|_{\\infty}=2(1+\\gamma)(\\|\\theta^{\\star}\\|+1)\\;,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "moreover, one can check that $\\|\\mathbf{I}-\\alpha{\\bar{A}}\\|^{2}\\leq1-\\alpha a$ with ", "page_idx": 8}, {"type": "equation", "text": "$$\na=(1-\\gamma)\\lambda_{\\mathrm{min}}(\\Sigma_{\\varphi})\\;,\\quad\\alpha_{\\infty}=(1-\\gamma)/(1+\\gamma)^{2}\\;,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "that is,Proposition $^{\\,l}$ holdswith $Q=\\mathrm{I},$ ", "page_idx": 8}, {"type": "text", "text": "Proof of Proposition 2 is provided in Appendix E. Since all the assumptions in A2 are fulfilled, we can verify tightness of the bound Theorem 2 for different learning rate schedules $\\alpha_{k}$ in (22). ", "page_idx": 8}, {"type": "text", "text": "Numerical results. Efficiency of the multiplier bootstrap approach (15) to the problems of constructing confidence sets in online algorithms has been demonstrated in the works [22] and [56]. We aim to illustrate the tightness of our bounds for normal approximation outlined in Theorem 2 in the setting of TD learning with linear function approximation. To this end, we consider the classical Garnet problem [3], in the simplified version proposed by [25]. This problem is characterized by the number of states $N_{s}$ , number of actions $a$ , and branching factor $b$ (i.e. the number of neighbors of each state in the MDP). We set these values to $N_{s}=10$ $a=2$ and $b=3$ , and aim to evaluate the value function of the randomly generated policy $\\pi(\\cdot|s)$ . Details on the way the policy $\\pi$ is set can be found in Appendix F. We consider the problem of policy evaluation in this MDP using the TD learning algorithm with identity feature mapping, that is, $\\phi(s)=e_{s}$ (that is, $s$ -th coordinate vector) for $s\\in\\{1,\\ldots,N_{s}\\}$ . We run the procedure (22) with the learning rates $\\alpha_{k}=c_{0}/k^{\\gamma}$ and different powers $\\gamma\\in\\{0.5,0.7,0.75\\}$ . For each of the experiments we aim to estimate the supremum ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{n}:=\\operatorname*{sup}_{x\\in\\mathbb{R}}\\mathopen{}\\mathclose\\bgroup\\left|\\mathbb{P}(\\sqrt{n}\\|\\bar{\\theta}_{n}-\\theta^{\\star}\\|\\leq x)-\\mathbb{P}(\\|\\Sigma_{\\infty}^{1/2}\\eta\\|\\leq x)\\aftergroup\\egroup\\right|~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "$\\eta\\sim\\mathcal{N}(0,\\mathrm{I}_{N_{s}})$ , and show that this supremum scales as $n^{-1/4}$ when $\\gamma=1/2$ and admits slower decay for other powers of $\\gamma$ We approximate true probability $\\mathbb{P}(\\|\\Sigma_{\\infty}^{1/2}\\eta\\|\\leq x)$ by the corresponding empirical probabilities based on sample of size $M\\gg n$ . Second, for $n\\in\\{1600,\\ldots,1638400\\}$ where next sample size is twice larger than the previous one, we generate $N=1638400$ trajectories of TD algorithm and approximate the distribution of ${\\sqrt{n}}\\|{\\bar{\\theta}}_{n}-\\theta^{\\star}\\|$ based on the corresponding empirical distribution. We report our results in Figure 1, showing that the smallest values of $\\Delta_{n}$ correspond to the step size schedule $\\gamma=1/2$ , moreover, the decay rate $n^{-1/4}$ seems to be tight, otherwise one should expect further decay of $\\Delta_{n}n^{1/4}$ . Additional simulations are provided in Appendix F. ", "page_idx": 8}, {"type": "image", "img_path": "S0Ci1AsJL5/tmp/f1fe963b2bb3e592fdefd2595e2c5cbe22846c43b601c32ce50c02d710ba363f.jpg", "img_caption": ["Figure 1: Subfigure (a): Rescaled error ${\\sqrt{n}}\\|{\\bar{\\theta}}_{n}-\\theta^{\\star}\\|$ , averaged over $N$ independent TD trajectories for different trajectory lengths $n$ . Subfigure (b): approximate quantity $\\Delta_{n}$ from (23) for different powers $\\gamma$ and $n$ . Subfigure (c): $\\Delta_{n}$ , rescaled by a factor $n^{1/4}$ , predicted by Theorem 2. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have established, to the best of our knowledge, the first fully non-asymptotic confidence bounds for parameter estimation in the LSA algorithm using the multiplier bootstrap. This result is based on a novel Berry-Esseen bound for the Polyak-Ruppert averaged LSA iterates, which is of independent interest. Our paper suggests several interesting directions for further research. First, our Berry-Esseen bounds are obtained using the randomized concentration inequality [63], and it would be valuable to generalize this approach to the setting of Markov chains. Second, it is natural to extend our results to the first-order gradient methods, both for stochastic optimization and variational inequalities. Third, it becomes possible to prove the fully non-asymptotic validity of confidence intervals obtained with plug-in techniques or other estimators of the asymptotic covariance matrix of $\\bar{\\theta}_{n}$ . These could then be compared with the multiplier bootstrap confidence intervals in terms of their dependence on problem dimension $d$ and other instance-dependent quantities. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work of S. Samsonov and A. Naumov was prepared within the framework of the HSE University Basic Research Program. The work of E. Moulines has been partly funded by the European Union (ERC-2022-SYG-OCEAN-101071601). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. The work of Q.-M. Shao is partially supported by National Nature Science Foundation of China NSFC 12031005 and Shenzhen Outstanding Talents Training Fund, China. The work of Z.-S. Zhang is partially supported by National Nature Science Foundation of China NSFC 12301183 and National Nature Science Found for Excellent Young Scientists Fund. This research was supported in part through computational resources of HPC facilities at HSE University [33]. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Rafk Aguech, Eric Moulines, and Pierre Priouret. On a perturbation approach for the analysis of stochastic tracking algorithms. SIAM Journal on Control and Optimization, 39(3):872-899, 2000.   \n[2] Andreas Anastasiou, Krishnakumar Balasubramanian, and Murat A. Erdogdu. Normal approximation for stochastic gradient descent via non-asymptotic rates of martingale CLT. In Alina Beygelzimer and Daniel Hsu, editors, Proceedings of the Thirty-Second Conference on Learning Theory, volume 99 of Proceedings of Machine Learning Research, pages 115-137. PMLR, 25-28 Jun 2019.   \n[3] TW Archibald, KIM McKinnon, and LC Thomas. On the generation of markov decision processes. Journal of the Operational Research Society, 46(3):354-361, 1995.   \n[4] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47:235-256, 2002.   \n[5] F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate $\\mathrm{o}(1/\\mathrm{n})$ . In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013.   \n[6]  A. Benveniste, M. Metivier, and P. Priouret. Adaptive algorithms and stochastic approximations, volume 22. Springer Science & Business Media, 2012.   \n[7] J. Bhandari, D. Russo, and R. Singal. A finite time analysis of temporal diffrence learning with linear function approximation. In Conference On Learning Theory, pages 1691-1692, 2018.   \n[8]  E. Bolthausen. Exact Convergence Rates in Some Martingale Central Limit Theorems. The Annals of Probability, 10(3):672 - 688, 1982.   \n[9]  Vivek S Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint. Cambridge University Press, 2008.   \n[10] Nicolas Brosse, Alain Durmus, and Eric Moulines. The promises and pitfalls of stochastic gradient langevin dynamics. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. CesaBianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[11] Xi Chen, Jason D. Lee, Xin T. Tong, and Yichen Zhang. Statistical inference for model parameters in stochastic gradient descent. The Annals of Statistics, 48(1):251 - 273, 2020.   \n[12]  Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Gaussian approximations and multiplier bootstrap for maxima of sums of high-dimensional random vectors. Ann. Statist., 41(6):2786-2819, 2013.   \n[13]  Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Central limit theorems and bootstrap in high dimensions. Ann. Probab., 45(4):2309-2352, 2017.   \n[14] G. Dalal, Balazs Szorenyi, and G. Thoppe. A tale of two-timescale reinforcement learning with the tightest finite-time bound. arXiv preprint arXiv: 1911.09157, 2019.   \n[15]  John C Duchi, Alekh Agarwal, Mikael Johansson, and Michael I Jordan. Ergodic mirror descent. SIAM Journal on Optimization,22(4):1549-1578, 2012.   \n[16] Alain Durmus, Eric Moulines, Alexey Naumov, and Sergey Samsonov. Finite-time highprobability bounds for Polyak-Ruppert averaged iterates of linear stochastic approximation. Mathematics of Operations Research, 2024.   \n[17]  Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, Kevin Scaman, and Hoi-To Wai. Tight high probability bounds for linear stochastic approximation with fixed stepsize. In M. Ranzato, A. Beygelzimer, K. Nguyen, P. S. Liang, J. W. Vaughan, and Y. Dauphin, editors, Advances in Neural Information Processing Systems, volume 34, pages 30063-30074. Curran Associates, Inc., 2021.   \n[18] Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, and Hoi-To Wai. On the stability of random matrix product with markovian noise: Application to linear stochastic approximation and td learning. In Mikhail Belkin and Samory Kpotufe, editors, Proceedings of Thirty FourthConference onLearning Theory,volume134ofProceedingsof MachineLearning Research, pages i711-1752. PMLR, 15-19 Aug 2021.   \n[19]Bradley Efron. Bootstrap methods: another look at the jackknife. In Breakthroughs in statistics: Methodology and distribution, pages 569-593. Springer, 1992.   \n[20]  Carl-Gustav Esseen. Fourier analysis of distribution functions. A mathematical study of the Laplace-Gaussian law. Acta Mathematica, 77(none):1 - 125, 1945.   \n[21]  E. Eweda and O. Macchi. Quadratic mean and almost-sure convergence of unbounded stochastic approximation algorithms with correlated observations. Ann. Inst. H. Poincare Sect. B (N.S.), 19(3):235-255, 1983.   \n[22]  Yixin Fang, Jinfeng Xu, and Lei Yang. Online bootstrap confidence intervals for the stochastic gradient descent estimator. Journal of Machine Learning Research, 19(78):1-21, 2018.   \n[23]  G. Fort. Central limit theorems for stochastic approximation with controlled Markov chain dynamics. ESAIM: PS, 19:60-80, 2015.   \n[24]  Robert E Gaunt and Siqi Li. Bounding Kolmogorov distances through Wasserstein and related integral probability metrics. Journal of Mathematical Analysis and Applications, 522(1):126985, 2023.   \n[25] Matthieu Geist, Bruno Scherrer, et al. Off-policy learning with eligibility traces: a survey. J. Mach. Learn. Res., 15(1):289-333, 2014.   \n[26] Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Pres Cambridge, MA, USA, 2016. http://www.deeplearningbook.org.   \n[27] Friedrich Gotze, Alexey Naumov, Vladimir Spokoiny, and Vladimir Ulyanov. Large ball probabilities, Gaussian comparison and anti-concentration. Bernoulli, 25(4A):2538-2563, 2019.   \n[28] Botao Hao, Yasin Abbasi Yadkori, Zheng Wen, and Guang Cheng. Bootstrapping upper confidence bound. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[29] De Huang, Jonathan Niles-Weed, Joel A Tropp, and Rachel Ward. Matrix concentration for products. Foundations of Computational Mathematics, pages 1-33, 2021.   \n[30] Dongyan Huo, Yudong Chen, and Qiaomin Xie. Bias and extrapolation in markovian linear stochastic approximation with constant stepsizes. In Abstract Proceedings of the 2023 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, pages 81-82, 2023.   \n[31]  Moritz Jirak and Martin Wahl. Quantitative limit theorems and bootstrap approximations for empirical spectral projectors. Probability Theory and Related Fields, 190(1):119-177, 2024.   \n[32] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[33] PS Kostenetskiy, RA Chulkevich, and VI Kozyrev. Hpc resources of the higher school of economics. In Journal of Physics: Conference Series, volume 1740, page 012050. IOP Publishing, 2021.   \n[34] Harold Kushner and G George Yin. Stochastic approximation and recursive algorithms and applications, volume 35. Springer Science & Business Media, 2003.   \n[35]  C. Lakshminarayanan and C. Szepesvari. Linear stochastic approximation: How far does constant step-size and iterate averaging go? In International Conference on Artificial Intelligence and Statistics, pages 1347-1355, 2018.   \n[36]  Guanghui Lan. An optimal method for stochastic composite optimization.  Mathematical Programming, 133(1-2):365-397, 2012.   \n[37] Sokbae Lee, Yuan Liao, Myung Hwan Seo, and Youngki Shin. Fast inference for quantile regression with tens of milions of observations. Journal of Econometrics, page 105673, 2024.   \n[38] Gen Li, Weichen Wu, Yuejie Chi, Cong Ma, Alessandro Rinaldo, and Yuting Wei. Highprobability sample complexities for policy evaluation with linear function approximation. IEEE Transactions on Information Theory, 70(8):5969-5999, 2024.   \n[39] Xiang Li, Jiadong Liang, Xiangyu Chang, and Zhihua Zhang. Statistical estimation and online inference via local sgd. In Po-Ling Loh and Maxim Raginsky, editors, Proceedings of Thirty FifhConference onLeaning Theory,volume178ofProceedings of MachineLeaning Research, pages 1613-1661. PMLR, 02-05 Jul 2022.   \n[40]  Xiang Li, Jiadong Liang, and Zhihua Zhang. Online statistical inference for nonlinear stochastic approximation with Markovian data. arXiv preprint arXiv:2302.07690, 2023.   \n[41] Xiang Li, Wenhao Yang, Jiadong Liang, Zhihua Zhang, and Michael I Jordan. A statistical analysis of Polyak-Ruppert averaged Q-learning. In International Conference on Artificial Intelligence and Statistics, pages 2207-2261. PMLR, 2023.   \n[42] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra Shane Leg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.   \n[43] Wenlong Mou, Chris Junchi Li, Martin J Wainwright, Peter L Bartlett, and Michael I Jordan. On linear stochastic approximation: Fine-grained Polyak-Ruppert and non-asymptotic concentration. In Conference on Learning Theory, pages 2947-2997. PMLR, 2020.   \n[44]  Wenlong Mou, Ashwin Pananjady, Martin J Wainwright, and Peter L Bartlett. Optimal and instance-dependent guarantees for markovian linear stochastic approximation. Mathematical Statistics and Learning, 7(1):41-153, 2024.   \n[45] Eric Moulines and Francis Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. Advances in neural information processing systems, 24:451-459, 2011.   \n[46]  Alexey Naumov, Vladimir Spokoiny, and Vladimir Ulyanov. Bootstrap confidence sets for spectral projectors of sample covariance. Probab. Theory Related Fields, 174(3-4):1091-1132, 2019.   \n[47]  Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574-1609, 2009.   \n[48]  Arkadij Semenovi Nemirovskij and David Borisovich Yudin. Problem complexity and method efficiency in optimization. 1983.   \n[49] Ivan Nourdin, Giovanni Peccati, and Xiaochuan Yang. Multivariate normal approximation on the wiener space: new bounds in the convex distance. Journal of Theoretical Probability, 35(3):2020-2037, 2022.   \n[50] A. Osekowski. Sharp Martingale and Semimartingale Inequalities. Monografie Matematyczne 72. Birkhauser Basel, 1 edition, 2012.   \n[51] Gandharv Patil, A Prashanth, Dheraj Nagaraj, and Doina Precup. Finite time analysis of tmporal difference learning with linear function approximation: Tail averaging and regularisation. In International Conference on Artificial Intelligence and Statistics, pages 5438-5448. PMLR, 2023.   \n[52]  V. Petrov. Sums of Independent Random Variables. Ergebnisse der Mathematik und ihrer Grenzgebiete. 2. Folge. Springer Berlin Heidelberg, 1975.   \n[53]Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838-855, 1992.   \n[54]  A. S. Poznyak. Advanced Mathematical Tools for Automatic Control Engineers: Deterministic Techniques. Elsevier, Oxford, 2008.   \n[55]  Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pages 1571-1578, 2012.   \n[56] Pratik Ramprasad, Yuantong Li, Zhuoran Yang, Zhaoran Wang, Will Wei Sun, and Guang Cheng. Online bootstrap inference for policy evaluation in reinforcement learning. J. Amer. Statist. Ass0c., 118(544):2901-2914, 2023.   \n[57] Nathan Ross. Fundamentals of Stein's method. Probability Surveys, 8(none):210 - 293, 2011.   \n[58] Donald B Rubin. The bayesian bootstrap. The annals of statistics, pages 130-134, 1981.   \n[59] David Ruppert. Efficient estimations from a slowly convergent robbins-monro proce. Technical report, Cornell University Operations Research and Industrial Engineering, 1988.   \n[60] Daniel Russo and Benjamin Van Roy Learning to optimize via posterior sampling. Mathematics of Operations Research, 39(4):1221-1243, 2014.   \n[61] Sergey Samsonov, Danil Tiapkin, Alexey Naumov, and Eric Moulines. Improved HighProbability Bounds for the Temporal Difference Learning Algorithm via Exponential Stability. In Shipra Agrawal and Aaron Roth, editors, Proceedings of Thirty Seventh Conference on Learning Theory, volume 247 of Proceedings of Machine Learning Research, pages 4511-4547. PMLR, 30 Jun-03 Jul 2024.   \n[62] Jun Shao. Mathematical statistics. Springer Science & Business Media, 2003.   \n[63]  Qi-Man Shao and Zhuo-Song Zhang. Berry-Esseen bounds for multivariate nonlinear statistics with applications to M-estimators and stochastic gradient descent algorithms. Bernoulli, 28(3):1548-1576, 2022.   \n[64]  Vladimir Spokoiny and Mayya Zhilova. Bootstrap confidence sets under model misspecification. The Annals of Statistics, 43(6):2653 - 2675, 2015.   \n[65] R Srikant. Rates of convergence in the central limit theorem for markov chains, with an application to TD learning. arXiv preprint arXiv:2401.15719, 2024.   \n[66] R. S Suton. Learning to predict by the methods of temporal differences. Machine learning, 3(1):9-44, 1988.   \n[67]  R. S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018.   \n[68] Joel A. Tropp. Freedman's inequality for matrix martingales. Electron. Commun. Probab., 16:262-270, 2011.   \n[69] Joel A Tropp et al. An introduction to matrix concentration inequalities. Foundations and Trends@ in Machine Learning, 8(1-2):1-230, 2015.   \n[70] J. N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674-690, May 1997.   \n[71] A.W. Van Der Vaart and J. A. Wellner. Weak convergence and empirical processes. Springer Series in Statistics, 1996.   \n[72]  Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.   \n[73] Xi Chen Wanrong Zhu and Wei Biao Wu. Online Covariance Matrix Estimation in Stochastic Gradient Descent. Journal of the American Statistical Association, 118(541):393-404, 2023.   \n[74] Yanjie Zhong, Todd Kuffner, and Soumendra Lahiri. Online Bootstrap Inference with Nonconvex Stochastic Gradient Descent Estimator. arXiv preprint arXiv:2306.02205, 2023.   \n[75]  Vladimir Mikhailovich Zolotarev. Probability metrics. Theory of Probability & Its Applications, 28(2):278-302,1984. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Proofs for accuracy of normal approximation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1  Expansion of the LSA-PR error ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proposition 3. The following expansion holds: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{n}\\bar{\\mathbf{A}}(\\bar{\\theta}_{n}-\\theta^{\\star})=-\\underbrace{\\frac{1}{\\sqrt{n}}\\displaystyle\\sum_{k=n+1}^{2n}\\varepsilon_{k}}_{W}+\\underbrace{\\frac{1}{\\sqrt{n}}\\displaystyle\\frac{\\theta_{n}-\\theta^{\\star}}{\\alpha_{n}}}_{D_{1}}-\\underbrace{\\frac{1}{\\sqrt{n}}\\displaystyle\\frac{\\theta_{2n}-\\theta^{\\star}}{\\alpha_{2n}}}_{D_{2}}}\\\\ &{\\phantom{\\underbrace{-\\frac{1}{\\sqrt{n}}\\displaystyle\\sum_{k=n+1}^{2n}(\\mathbf{A}_{k}-\\bar{\\mathbf{A}})(\\theta_{k-1}-\\theta^{\\star})}}_{D_{3}}+\\underbrace{\\frac{1}{\\sqrt{n}}\\displaystyle\\sum_{k=n+1}^{2n}\\left(\\theta_{k-1}-\\theta^{\\star}\\right)\\left(\\displaystyle\\frac{1}{\\alpha_{k}}-\\displaystyle\\frac{1}{\\alpha_{k-1}}\\right)}_{D_{4}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. We use expansion (3) and rewrite it as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\theta_{k}-\\theta^{\\star}=(\\mathrm{I}-\\alpha_{k}\\bar{\\mathbf{A}})(\\theta_{k-1}-\\theta^{\\star})-\\alpha_{k}(\\mathbf{A}_{k}-\\bar{\\mathbf{A}})(\\theta_{k-1}-\\theta^{\\star})-\\alpha_{k}\\varepsilon_{k}\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The previous equation implies, after algebraic manipulation and division by $\\alpha_{k}$ , that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bar{\\bf A}(\\theta_{k-1}-\\theta^{\\star})=\\frac{\\theta_{k-1}-\\theta^{\\star}}{\\alpha_{k}}-\\frac{\\theta_{k}-\\theta^{\\star}}{\\alpha_{k}}-({\\bf A}_{k}-\\bar{\\bf A})(\\theta_{k-1}-\\theta^{\\star})-\\varepsilon_{k}\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Taking average for $k$ from $n+1$ to $2n$ and multiplying by $\\sqrt{n}$ we obtain the statement. ", "page_idx": 15}, {"type": "text", "text": "A.2  Bounding the error of LSA's algorithm last iterate ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We begin with of technical lemma on the behaviour of the last iterate of the LSA procedure $\\theta_{n}$ given in (1). This result is classical and can be traced back to [53], with the modern exposition re-appearing in [7, 14, 17]. We provide the proof here for completeness. In the proof we use conventions for the product of random matrices $\\Gamma_{1:k}$ outlined in (67). ", "page_idx": 15}, {"type": "text", "text": "Proposition 4, Assume A1, A2, and $A3$ .Then for any $k\\geq n$ where $n$ satisfies (9), it holds for $2\\ {\\stackrel{\\cdot}{\\geq}}\\ p\\leq\\log n^{2}$ that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}^{1/p}[\\|\\theta_{k}-\\theta^{\\star}\\|^{p}]\\leq\\sqrt{\\kappa_{Q}}\\mathrm{e}\\exp\\bigl\\{-(a/2)\\sum_{\\ell=1}^{k}\\alpha_{\\ell}\\bigr\\}\\|\\theta_{0}-\\theta^{\\star}\\|+\\frac{4\\mathrm{e}\\sqrt{\\kappa_{Q}}\\|\\varepsilon\\|_{\\infty}p}{\\sqrt{a}}\\sqrt{\\alpha_{k}}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Expanding the decomposition (66), we obtain that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}^{1/p}[\\Vert\\theta_{k}-\\theta^{\\star}\\Vert^{p}]\\leq\\mathbb{E}^{1/p}[\\Vert\\Gamma_{1:k}\\{\\theta_{0}-\\theta^{\\star}\\}\\Vert^{p}]+\\mathbb{E}^{1/p}[\\Vert\\sum_{j=1}^{k}\\alpha_{j}\\Gamma_{j+1:k}\\varepsilon_{j}\\Vert^{p}]\\;,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and we bound the both term separately. Since the sample size $n$ satisfies (9), we get applying Corollary 4 (see (69), that for $2\\leq p\\leq\\log n$ it holds ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}^{1/p}[\\Vert\\Gamma_{1:k}\\{\\theta_{0}-\\theta^{\\star}\\}\\Vert^{p}]\\leq\\sqrt{\\kappa_{Q}}\\mathrm{e}\\exp\\bigl\\{-(a/2)\\sum_{\\ell=1}^{k}\\alpha_{\\ell}\\bigr\\}\\Vert\\theta_{0}-\\theta^{\\star}\\Vert\\ .\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now we proceed with the second term in (25). Applying Burholder's inequality [50], we obtain that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\mathbb{E}^{1/p}[\\|\\sum_{j=1}^{k}\\alpha_{j}\\Gamma_{j+1:k\\ell_{j}}\\|^{p}]\\leq p\\left(\\mathbb{E}^{2/p}\\left[\\left(\\sum_{j=1}^{k}\\alpha_{j}^{2}\\|\\Gamma_{j+1:k\\ell_{j}}\\|^{2}\\right)^{p/2}\\right]\\right)^{1/2}}&{}\\\\ {\\leq p\\left(\\sum_{j=1}^{k}\\alpha_{j}^{2}\\mathbb{E}^{2/p}[\\|\\Gamma_{j+1:k\\ell_{j}}\\|^{p}]\\right)^{1/2}}&{}\\\\ {\\leq p\\sqrt{\\kappa}q\\mathrm{e}\\|\\varepsilon\\|_{\\infty}\\Bigg(\\sum_{j=1}^{k}\\alpha_{j}^{2}\\displaystyle\\prod_{\\ell=j+1}^{k}\\big(1-\\frac{a\\alpha_{\\ell}}{4}\\big)\\Bigg)^{1/2}}&{}\\\\ {\\leq\\frac{4\\mathrm{e}\\sqrt{\\kappa}q\\|\\varepsilon\\|_{\\infty}p}{\\sqrt{a}}\\sqrt{\\alpha_{k}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Corollary 2. Under assumptions of Proposition 4, it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\forall k\\in[n,2n-1]:\\|\\theta_{k}-\\theta^{\\star}\\|\\geq g(k,\\|\\theta_{0}-\\theta^{\\star}\\|,n)\\right)\\leq\\frac{1}{n}\\;,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we have defined ", "page_idx": 16}, {"type": "equation", "text": "$$\ng(k,\\|\\theta_{0}-\\theta^{\\star}\\|,n)=\\sqrt{\\kappa_{Q}}\\mathrm{e}^{2}\\exp\\bigl\\{-(a/2)\\sum_{\\ell=1}^{k}\\alpha_{\\ell}\\bigr\\}\\|\\theta_{0}-\\theta^{\\star}\\|+\\frac{8\\mathrm{e}^{2}\\sqrt{\\kappa_{Q}}\\|\\varepsilon\\|_{\\infty}\\log n}{\\sqrt{a}}\\sqrt{\\alpha_{k}}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We first note that Lemma 1 implies, setting $\\delta=1/n^{2}$ , that for every fixed $k\\in[n;2n-1]$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\|\\theta_{k}-\\theta^{\\star}\\|\\geq\\sqrt{\\kappa_{Q}}\\mathrm{e}^{2}\\exp\\bigl\\{-(a/2)\\sum_{\\ell=1}^{k}\\alpha_{\\ell}\\bigr\\}\\|\\theta_{0}-\\theta^{\\star}\\|+\\frac{8\\mathrm{e}^{2}\\sqrt{\\kappa_{Q}}\\|\\varepsilon\\|_{\\infty}\\log n}{\\sqrt{a}}\\sqrt{\\alpha_{k}}\\right)\\leq\\frac{1}{n^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Application of the union bound concludes the proof. ", "page_idx": 16}, {"type": "text", "text": "We conclude this part with a simple consequence of Markov's inequality. ", "page_idx": 16}, {"type": "text", "text": "Lemma 1. Fi $\\textstyle{x\\,\\delta\\in(0,1)}$ andlet $Y$ be a positive random variable, such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}^{1/p}[Y^{p}]\\leq C_{1}+C_{2}p\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for any $2\\leq p\\leq\\log\\left(1/\\delta\\right)$ .Then it holds with probability at least $1-\\delta$ that ", "page_idx": 16}, {"type": "equation", "text": "$$\nY\\leq\\mathrm{e}C_{1}+\\mathrm{e}C_{2}\\log\\left(1/\\delta\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Applying Markov's inequality, for any $t\\geq0$ we get that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}(Y\\geq t)\\leq{\\frac{\\mathbb{E}[Y^{p}]}{t^{p}}}\\leq{\\frac{(C_{1}+C_{2}p)^{p}}{t^{p}}}\\ .\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we set $p=\\log{(1/\\delta)},t=\\mathrm{e}C_{1}+\\mathrm{e}C_{2}\\log{(1/\\delta)},$ and aim to check that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{(C_{1}+C_{2}\\log{(1/\\delta)})^{\\log{(1/\\delta)}}}{(\\mathrm{e}C_{1}+\\mathrm{e}C_{2}\\log{(1/\\delta)})^{\\log{(1/\\delta)}}}\\leq\\delta\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Taking logarithms from both sides, the latter inequality is equivalent to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\log\\left(1/\\delta\\right)\\log\\frac{C_{1}+C_{2}\\log\\left(1/\\delta\\right)}{\\mathrm{e}(C_{1}+C_{2}\\log\\left(1/\\delta\\right))}\\leq\\log\\delta\\ ,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which turns into exact equality ", "page_idx": 16}, {"type": "text", "text": "A.3Proof of Theorem 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Since both terms in the right-hand side of the error bound of Proposition 4 scales linearly with $\\sqrt{\\kappa_{Q}}$ , for simplicity we do not trace it in the subsequent bounds (i.e. assume $\\kappa_{Q}=1.$ ), and then keep the required scaling with $\\kappa_{Q}$ only in the final bounds. The decomposition (24) is a key element of our proof and allows to treat different error sources $D_{1}-D_{4}$ separately. For the last iterate we have, using Proposition 4, that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{E}^{1/2}\\left[\\|\\theta_{n}-\\theta^{\\star}\\|^{2}\\right]\\lesssim\\frac{\\|\\varepsilon\\|_{\\infty}}{\\sqrt{a}}\\sqrt{\\alpha_{n}}+\\exp\\biggl\\{-(a/2)\\displaystyle\\sum_{\\ell=1}^{n}\\alpha_{\\ell}\\biggr\\}\\|\\theta_{0}-\\theta^{\\star}\\|}}\\\\ {{\\displaystyle\\mathbb{E}^{1/2}\\left[\\|\\theta_{2n}-\\theta^{\\star}\\|^{2}\\right]\\lesssim\\frac{\\|\\varepsilon\\|_{\\infty}}{\\sqrt{a}}\\sqrt{\\alpha_{2n}}+\\exp\\biggl\\{-(a/2)\\displaystyle\\sum_{\\ell=1}^{2n}\\alpha_{\\ell}\\biggr\\}\\|\\theta_{0}-\\theta^{\\star}\\|~.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, using that $\\begin{array}{r}{\\sum_{k=1}^{n}\\alpha_{k}\\ge\\frac{c_{0}n^{1-\\gamma}}{1-\\gamma}}\\end{array}$ we obtain that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{E}^{1/2}\\big[\\|D_{1}\\|^{2}\\big]\\lesssim\\frac{\\|\\varepsilon\\|_{\\infty}}{\\sqrt{a}n^{(1-\\gamma)/2}}+\\exp\\biggl\\{-\\frac{c_{0}a n^{1-\\gamma}}{2(1-\\gamma)}\\biggr\\}\\|\\theta_{0}-\\theta^{\\star}\\|~,}}\\\\ {{\\displaystyle\\mathbb{E}^{1/2}\\big[\\|D_{2}\\|^{2}\\big]\\lesssim\\frac{\\|\\varepsilon\\|_{\\infty}}{\\sqrt{a}n^{(1-\\gamma)/2}}+\\exp\\biggl\\{-\\frac{c_{0}a(2n)^{1-\\gamma}}{1-\\gamma}\\biggr\\}\\|\\theta_{0}-\\theta^{\\star}\\|~.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we proceed with $D_{3}$ . Since it is a sum of a martingale-difference sequence w.r.t. $\\mathcal{F}_{k}\\;=\\;$ $\\sigma(Z_{\\ell},\\ell\\le\\ k)$ , we get using Proposition 4, that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}[\\|D_{3}\\|^{2}]\\lesssim\\frac{C_{\\mathtt{A}}^{2}}{n}\\sum_{k=n}^{2^{n}}\\mathbb{E}[\\|\\theta_{k-1}-\\theta^{*}\\|^{2}]}\\\\ &{\\lesssim\\frac{C_{\\mathtt{A}}^{2}}{n}\\sum_{k=n+1}^{2^{n}}\\frac{\\|\\varepsilon\\|_{\\infty}^{2}\\alpha_{k}}{a}+\\frac{C_{\\mathtt{A}}^{2}}{n}\\sum_{k=n+1}^{2^{n}}\\exp\\biggl\\{-a\\sum_{\\ell=1}^{k}\\alpha_{\\ell}\\biggr\\}\\|\\theta_{0}-\\theta^{*}\\|^{2}}\\\\ &{\\lesssim\\frac{C_{\\mathtt{A}}^{2}}{n}\\sum_{k=n+1}^{2^{n}}\\frac{\\|\\varepsilon\\|_{\\infty}^{2}\\alpha_{k}}{a}+\\frac{C_{\\mathtt{A}}^{2}}{n\\alpha_{2n}}\\exp\\biggl\\{-a\\sum_{\\ell=1}^{n}\\alpha_{\\ell}\\biggr\\}\\underbrace{\\sum_{k=n+1}^{2n}\\alpha_{k}\\exp\\biggl\\{-a\\sum_{\\ell=n+1}^{k}\\alpha_{\\ell}\\biggr\\}}_{S_{1}}\\|\\theta_{0}-\\theta^{*}\\|}\\\\ &{\\lesssim\\frac{C_{\\mathtt{A}}^{2}}{a n\\gamma}\\|\\varepsilon\\|_{\\infty}^{2}+\\frac{C_{\\mathtt{A}}^{2}}{n^{1-\\gamma}a}\\exp\\biggl\\{-\\frac{c_{0}a n^{1-\\gamma}}{1-\\gamma}\\biggr\\}\\|\\theta_{0}-\\theta^{*}\\|^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we additionally used that $S_{1}\\lesssim1/a$ due to Lemma 3. Now it remains to bound the term $D_{4}$ from the representation (24). Using Minkowski's inequality and Proposition 4, we get that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}^{1/2}\\big[\\|D_{4}\\|^{2}\\big]\\lesssim\\displaystyle\\frac{2n}{\\sqrt{n}}\\sum_{k=n+1}^{2n}\\mathbb{E}^{1/2}\\big[\\|\\theta_{k}-\\theta^{*}\\|^{2}\\big]\\left(\\frac{1}{\\alpha_{k}}-\\frac{1}{\\alpha_{k-1}}\\right)}&{}\\\\ &{\\lesssim\\displaystyle\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}\\frac{\\|\\varepsilon\\|_{\\infty}(k^{\\gamma}-(k-1)^{\\gamma})}{\\sqrt{a}}\\sqrt{\\alpha_{k}}}\\\\ &{\\qquad\\qquad\\qquad+\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}(k^{\\gamma}-(k-1)^{\\gamma})\\exp\\Biggl\\{-(a/2)\\sum_{\\ell=1}^{k}\\alpha_{\\ell}\\Biggr\\}\\|\\theta_{0}-\\theta^{*}\\|}\\\\ &{\\lesssim\\frac{\\|\\varepsilon\\|_{\\infty}}{\\sqrt{a}\\sqrt{n}}\\sum_{k=n+1}^{2n}\\frac{1}{k^{1-\\gamma/2}}+\\frac{n^{2\\gamma-1}}{a}\\exp\\Biggl\\{-\\frac{c_{0}a n^{1-\\gamma}}{2(1-\\gamma)}\\Biggr\\}\\|\\theta_{0}-\\theta^{*}\\|}\\\\ &{\\lesssim\\frac{\\|\\varepsilon\\|_{\\infty}}{\\sqrt{a}\\sqrt{a}\\sqrt{(1-\\gamma)/2}}+\\frac{n^{2\\gamma-1}}{a}\\exp\\Biggl\\{-\\frac{c_{0}a n^{1-\\gamma}}{2(1-\\gamma)}\\Biggr\\}\\|\\theta_{0}-\\theta^{*}\\|\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining the estimates above yields the result of Theorem 1. ", "page_idx": 17}, {"type": "text", "text": "We conclude this section with some technical lemmas. ", "page_idx": 17}, {"type": "text", "text": "Lemma 2 (Lemma 24 in [18]). Let $b>0$ and $(\\alpha_{k})_{k\\geq0}$ be a non-increasing sequence such that $\\alpha_{0}\\leq1/b$ .Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{n+1}\\alpha_{j}\\prod_{l=j+1}^{n+1}(1-\\alpha_{l}b)=\\frac{1}{b}\\left\\{1-\\prod_{l=1}^{n+1}(1-\\alpha_{l}b)\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. The proof of this statement is given in [18], we provide it here for completeness. Let us denote $\\begin{array}{r}{\\overline{{u_{j:n+1}}}=\\prod_{l=j}^{n+1}(1-\\alpha_{l}b)}\\end{array}$ .Then, for $j\\in\\{1,\\ldots,n\\!+\\!1\\}$ $u_{j+1:n+1}-u_{j:n+1}=b\\alpha_{j}u_{j+1:n+1}$ Hence, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{n+1}\\alpha_{j}\\prod_{l=j+1}^{n+1}(1-\\alpha_{l}b)=\\frac{1}{b}\\sum_{j=1}^{n+1}(u_{j+1:n+1}-u_{j:n+1})=b^{-1}(1-u_{1:n+1})\\;,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and the statement follows. ", "page_idx": 17}, {"type": "text", "text": "Lemma 3 (Modified Lemma 25 in [18]). Let $b>0$ andlet $\\alpha_{\\ell}=c_{0}/\\ell^{\\gamma}$ \uff0c $\\gamma\\in\\left[1/2;1\\right)$ ,such that $c_{0}\\leq1/b$ Then for any n satisfying ", "page_idx": 17}, {"type": "equation", "text": "$$\nn\\geq2+2\\mathopen{}\\mathclose\\bgroup\\left(\\frac{2\\gamma}{c_{0}b}\\aftergroup\\egroup\\right)^{1/(1-\\gamma)}~,\\quad a n d\\quad\\frac{n^{1-\\gamma}}{1+\\log(n)}\\geq\\frac{2\\gamma(1-\\gamma)}{c_{0}b(1-(1/2)^{1-\\gamma}}~.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "any $k\\geq n$ it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{k+1}\\alpha_{j}^{2}\\prod_{\\ell=j+1}^{k+1}(1-\\alpha_{\\ell}b)\\le(4/b)\\alpha_{k+1}\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. From elementary algebra, we obtain that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\alpha_{\\ell}-\\alpha_{\\ell+1}=\\frac{c_{0}}{\\ell^{\\gamma}}-\\frac{c_{0}}{(\\ell+1)^{\\gamma}}=\\frac{c_{0}((1+1/\\ell)^{\\gamma}-1)}{(\\ell+1)^{\\gamma}}\\le\\frac{c_{0}}{(\\ell+1)^{\\gamma}}\\frac{\\gamma}{\\ell}\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we used the fact that $(1+x)^{\\gamma}\\leq1+\\gamma x$ for $\\gamma\\in[1/2;1)$ and $x\\in[0,1]$ . Hence, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\alpha\\ell}{\\alpha_{\\ell+1}}\\leq1+\\frac{\\gamma}{\\ell}\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus we obtain that, since $k\\geq n$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=1}^{k+1}\\!\\!\\!\\!\\!\\alpha_{j}^{\\!\\ k+1}\\prod_{\\ell=j+1}^{k+1}(1-\\alpha_{\\ell}b)=\\alpha_{k+1}\\!\\!\\sum_{j=1}^{k+1}\\!\\!\\!\\!\\alpha_{j}\\prod_{\\ell=j+1}^{k+1}\\left(\\frac{\\alpha_{\\ell-1}}{\\alpha_{\\ell}}\\right)\\!(1-\\alpha_{\\ell}b)}\\\\ &{\\qquad\\le\\alpha_{k+1}\\sum_{j=1}^{k+1}\\!\\!\\!\\alpha_{j}\\prod_{\\ell=j+1}^{k+1}\\left(1+\\frac{\\gamma}{\\ell-1}\\right)(1-\\alpha_{\\ell}b)}\\\\ &{\\qquad\\le\\alpha_{k+1}\\displaystyle\\sum_{j=1}^{k+1}\\alpha_{j}\\exp\\left\\{\\sum_{\\ell=j+1}^{n}\\frac{\\gamma}{\\ell-1}\\right\\}\\exp\\left\\{-\\sum_{\\ell=j+1}^{n}\\alpha_{\\ell}b\\right\\}\\exp\\left\\{\\sum_{\\ell=n+1}^{k+1}\\frac{\\gamma}{\\ell-1}\\right\\}\\exp\\left\\{-\\sum_{\\ell=n+1}^{k+1}\\!\\!\\!\\!\\!\\alpha_{\\ell}b\\right\\}\\exp\\left\\{\\displaystyle{\\sum_{\\ell=n+1}^{k+1}\\frac{\\gamma}{\\ell-1}}\\right\\}\\exp\\left\\{\\displaystyle{\\sum_{\\ell=n+1}^{k+1}\\frac{\\gamma}{\\ell-1}}\\right\\}}\\\\ &{\\qquad\\le\\alpha_{k+1}\\displaystyle\\sum_{j=1}^{k+1}\\alpha_{j}\\exp\\left\\{\\sum_{\\ell=j+1}^{n}\\frac{\\gamma}{\\ell-1}\\right\\}\\exp\\left\\{-\\sum_{\\ell=j+1}^{n}\\alpha_{\\ell}b\\right\\}\\exp\\left\\{-\\frac{b}{2}\\sum_{\\ell=n+1}^{k+1}\\alpha_{\\ell}b\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we have used in the last identity that, since $n$ satisfies (26), that for $\\ell\\geq n/2$ it holds ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\gamma}{\\ell-1}\\leq\\alpha\\ell b/2\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We will now prove that for $j\\leq n-1$ , it holds ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{\\ell=j+1}^{n}{\\frac{\\gamma}{\\ell-1}}\\leq(b/2)\\sum_{\\ell=j+1}^{n}\\alpha_{\\ell}\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For $j\\ge n/2$ , the bound (29) directly follows from (28). We now turn to the proof of (29)for $j\\le n/2$ Note first that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{\\ell=j+1}^{n}{\\frac{\\gamma}{\\ell-1}}\\leq\\sum_{k=2}^{n}{\\frac{\\gamma}{\\ell-1}}\\leq\\gamma(\\log(n)+1)\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "On the other hand, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{\\ell=j+1}^{n}{\\frac{1}{\\ell^{\\gamma}}}\\geq\\int_{j+1}^{n+1}{\\frac{\\mathrm{d}x}{x^{\\gamma}}}={\\frac{{\\big(}(n+1)^{1-\\gamma}-(j+1)^{1-\\gamma}{\\big)}}{1-\\gamma}}\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Comparing the above bounds, to ensure that (29) holds, it is enough to check that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\gamma(1+\\log{(n)})\\leq\\frac{c_{0}b}{2(1-\\gamma)}\\big(n^{1-\\gamma}-(n/2)^{1-\\gamma}\\big)\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that (30) is guaranteed by (26). Thus we obtain that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{j=1}^{k+1}\\alpha_{j}^{2}\\prod_{\\ell=j+1}^{k+1}\\left(1-\\alpha_{\\ell}b\\right)\\leq\\alpha_{k+1}\\sum_{j=1}^{k+1}\\alpha_{j}\\exp\\left\\{-(b/2)\\sum_{\\ell=j+1}^{k+1}\\alpha_{\\ell}\\right\\}}}\\\\ {{\\displaystyle\\leq\\alpha_{k+1}\\sum_{j=1}^{k+1}\\alpha_{j}\\prod_{\\ell=j+1}^{k+1}\\left(1-(b/4)\\alpha_{\\ell}\\right)}}\\\\ {{\\displaystyle\\leq\\left(4/b\\right)\\alpha_{k+1}~,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality follows from Lemma 2. ", "page_idx": 18}, {"type": "text", "text": "B Proof of Theorem 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We first define explicitly the remainder terms outlined in the statement of Theorem 2: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{1}(n,a,\\mathrm{C}_{\\bf A},\\mathrm{Tr}\\,\\Sigma_{\\varepsilon})=\\kappa_{Q}\\left(\\frac{\\mathrm{C}_{\\bf A}\\,\\sqrt{\\mathrm{Tr}\\,\\Sigma_{\\varepsilon}}}{n^{(1-\\gamma)/2}\\sqrt{a}}+\\frac{\\sqrt{\\mathrm{Tr}\\,\\Sigma_{\\varepsilon}}n^{2\\gamma-1}}{a}+\\frac{\\sqrt{\\mathrm{Tr}\\,\\Sigma_{\\varepsilon}}}{a}+\\frac{\\mathrm{C}_{\\bf A}\\,\\sqrt{\\mathrm{Tr}\\,\\Sigma_{\\varepsilon}}n^{\\gamma-1/2}}{a^{2}}\\right)\\;,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and constants $\\mathsf C_{1},\\mathsf C_{2}$ are given, respectively, by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{1}=\\frac{\\kappa_{Q}\\left\\|\\varepsilon\\right\\|_{\\infty}\\sqrt{\\mathrm{Tr}\\sum_{\\varepsilon}}}{\\sqrt{a}}+\\frac{\\kappa_{Q}\\left(\\mathrm{Tr}\\sum_{\\varepsilon}+\\mathrm{C}_{\\mathbf{A}}\\sqrt{\\mathrm{Tr}\\sum_{\\varepsilon}}\\right)}{a}\\ ,}\\\\ &{C_{2}=\\frac{\\kappa_{Q}\\,\\mathrm{Ca}\\,\\sqrt{\\mathrm{Tr}\\sum_{\\varepsilon}}}{\\sqrt{a}}\\left(\\|\\varepsilon\\|_{\\infty}+\\sqrt{\\mathrm{Tr}\\sum_{\\varepsilon}}+\\mathrm{C}_{\\mathbf{A}}\\right)\\ ,}\\\\ &{C_{3}=\\frac{\\kappa_{Q}\\left(\\mathrm{C}_{\\mathbf{A}}\\,\\sqrt{1}\\right)\\sqrt{\\mathrm{Tr}\\sum_{\\varepsilon}}\\left(\\|\\varepsilon\\|_{\\infty}+\\sqrt{\\mathrm{Tr}\\sum_{\\varepsilon}}+\\mathrm{C}_{\\mathbf{A}}\\right)}{\\sqrt{a}}\\ ,}\\\\ &{C_{4}=\\frac{\\kappa_{Q}\\left(\\mathrm{C}_{\\mathbf{A}}\\,\\sqrt{1}\\right)\\sqrt{\\mathrm{Tr}\\sum_{\\varepsilon}^{(1)}\\left(\\|\\varepsilon\\|_{\\infty}+\\sqrt{\\mathrm{Tr}\\sum_{\\varepsilon}^{(1)}}+\\mathrm{C}_{\\mathbf{A}}\\right)}}{\\sqrt{a}}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To complete the proof we only need to combine (13) with the bounds of Theorem 1. Applying the Cauchy-Schwartz inequality, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|D\\|\\|W\\|]\\le\\mathbb{E}^{1/2}[\\|D\\|^{2}]\\mathbb{E}^{1/2}[\\|W\\|^{2}]\\lesssim\\frac{\\sqrt{\\kappa_{Q}}\\|\\varepsilon\\|_{\\infty}\\sqrt{\\operatorname{Tr}\\Sigma_{\\varepsilon}}}{\\sqrt{a}}\\left(\\frac{1}{n^{(1-\\gamma)/2}}+\\frac{\\mathbf{C}_{\\mathbf{A}}}{n^{\\gamma/2}}\\right)+}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\sqrt{\\kappa_{Q}}\\left(\\frac{\\mathbf{C}_{\\mathbf{A}}\\sqrt{\\operatorname{Tr}\\Sigma_{\\varepsilon}}}{n^{(1-\\gamma)/2}\\sqrt{a}}+\\frac{\\sqrt{\\operatorname{Tr}\\Sigma_{\\varepsilon}}n^{2\\gamma-1}}{a}\\right)\\exp\\Biggl\\{-\\frac{c_{0}a n^{1-\\gamma}}{2(1-\\gamma)}\\Biggr\\}\\|\\theta_{0}-\\theta^{\\star}\\|\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now it remains to bound the last term in (13). Using the Cauchy-Schwartz inequality and Lemma 4, weobtainthat ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\displaystyle\\sum_{i=n}^{2n-1}\\|\\varepsilon_{i}\\|\\|D-D^{(i)}\\|]\\leq\\mathbb{E}^{1/2}[\\|\\varepsilon_{1}\\|^{2}]\\sum_{i=n}^{2n-1}\\mathbb{E}^{1/2}[\\|D-D^{(i)}\\|^{2}]}\\\\ &{\\quad\\lesssim\\frac{\\kappa_{Q}\\left(\\mathrm{Tr}\\,\\Sigma_{\\varepsilon}+\\mathrm{C}_{\\mathbf{A}}\\,\\sqrt{\\mathrm{Tr}\\,\\Sigma_{\\varepsilon}}\\right)}{a}n^{\\gamma-1/2}+\\frac{\\kappa_{Q}\\,\\mathrm{C}_{\\mathbf{A}}\\,\\sqrt{\\mathrm{Tr}\\,\\Sigma_{\\varepsilon}}}{\\sqrt{a}}\\left(\\|\\varepsilon\\|_{\\infty}+\\sqrt{\\mathrm{Tr}\\,\\Sigma_{\\varepsilon}}+\\mathrm{C}_{\\mathbf{A}}\\right)n^{1/2-\\gamma/2}}\\\\ &{\\qquad\\quad+\\,\\kappa_{Q}\\left(\\frac{\\sqrt{\\mathrm{Tr}\\,\\Sigma_{\\varepsilon}}}{a}+\\frac{\\mathrm{C}_{\\mathbf{A}}\\,\\sqrt{\\mathrm{Tr}\\,\\Sigma_{\\varepsilon}}n^{\\gamma-1/2}}{a^{2}}\\right)\\exp\\biggl\\{-\\frac{c_{0}a n^{1-\\gamma}}{2(1-\\gamma)}\\biggr\\}\\|\\theta_{0}-\\theta^{\\star}\\|\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and the statement follows. ", "page_idx": 19}, {"type": "text", "text": "B.1  Proof of auxiliary lemmas for Theorem 2. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our proof of Theorem 2 is based on the key lemma below, which allows us to bound $\\mathbb{E}^{1/2}[\\Vert D-$ $D^{(i)}\\|^{2}]$ for $i\\in\\{n+1,\\ldots,2n\\}$ ", "page_idx": 19}, {"type": "text", "text": "Lemma 4. Assume A1,A2,and A3. Then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\sum_{=n+1}^{2n}\\mathbb{E}^{1/2}[\\|D-D^{(i)}\\|^{2}]\\lesssim\\frac{\\kappa_{Q}(\\sqrt{\\mathrm{Tr}\\sum_{\\varepsilon}}+\\mathrm{C}_{\\mathbf{A}})}{a}n^{\\gamma-1/2}+\\frac{\\kappa_{Q}\\,\\mathrm{C}_{\\mathbf{A}}}{\\sqrt{a}}\\left(\\|\\varepsilon\\|_{\\infty}+\\sqrt{\\mathrm{Tr}\\sum_{\\varepsilon}}+\\mathrm{C}_{\\mathbf{A}}\\right)n^{1/2-\\frac{\\gamma}{2}}}\\\\ {+\\kappa_{Q}\\left(\\frac{1}{a}+\\frac{\\mathrm{C}_{\\mathbf{A}}\\,n^{\\gamma-1/2}}{a^{2}}\\right)\\exp\\Biggl\\{-\\frac{c_{0}a n^{1-\\gamma}}{2(1-\\gamma)}\\Biggr\\}\\|\\theta_{0}-\\theta^{\\star}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Since both terms in the right-hand side of the error bound of Proposition 4 scales linearly with $\\sqrt{\\kappa_{Q}}$ , for simplicity we do not trace it in the subsequent bounds (i.e. assume $\\kappa_{Q}=1.$ ), and then keep the required scaling with $\\kappa_{Q}$ only in the final bounds. Consider the sequences of noise variables ", "page_idx": 19}, {"type": "text", "text": "which differs only in position $i$ $,n+1\\leq i\\leq2n$ ,Wwith $Z_{i}^{\\prime}$ being an independent copy of $Z_{i}$ .Consider the associatedSAprocesses ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{k}=\\theta_{k-1}-\\alpha_{k}\\{\\mathbf{A}(Z_{k})\\theta_{k-1}-\\mathbf{b}(Z_{k})\\}\\;,\\quad k\\geq1,\\quad\\theta_{0}=\\theta_{0}\\in\\mathbb{R}^{d}}\\\\ &{\\theta_{k}^{(i)}=\\theta_{k-1}^{(i)}-\\alpha_{k}\\{\\mathbf{A}(Y_{k})\\theta_{k-1}^{(i)}-\\mathbf{b}(Y_{k})\\}\\;,\\quad k\\geq1\\;,\\quad\\theta_{0}^{(i)}=\\theta_{0}\\in\\mathbb{R}^{d}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $Y_{k}\\,=\\,Z_{k}$ for $k\\ne i$ and $Y_{i}\\,=\\,Z_{i}^{\\prime}$ . From the above representations we easily observe that $\\theta_{k}=\\theta_{k}^{(i)}$ for $k<i$ moreover, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{i}-\\theta_{i}^{(i)}=\\alpha_{i}\\big\\{({\\bf A}(Z_{i}^{\\prime})-{\\bf A}(Z_{i}))\\theta_{i-1}-{\\bf b}(Z_{i}^{\\prime})+{\\bf b}(Z_{i})\\big\\}}\\\\ &{\\quad\\quad\\quad=\\alpha_{i}({\\bf A}(Z_{i}^{\\prime})-{\\bf A}(Z_{i}))(\\theta_{i-1}-\\theta^{\\star})-\\alpha_{i}(\\varepsilon_{i}-\\varepsilon_{i}^{\\prime})\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\varepsilon_{i}=\\varepsilon(Z_{i})$ and $\\varepsilon_{i}^{\\prime}=\\varepsilon(Z_{i}^{\\prime})$ . Representation (34) implies, together with Proposition 4, that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{1/2}[\\|\\theta_{i}-\\theta_{i}^{(i)}\\|^{2}]\\lesssim\\alpha_{i}\\sqrt{\\operatorname{Tr}\\Sigma_{\\varepsilon}}+\\frac{{\\displaystyle\\mathrm{C}}_{\\mathbf{A}}\\|\\varepsilon\\|_{\\infty}\\alpha_{i}^{3/2}}{\\sqrt{a}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\alpha_{i}\\exp\\{-(a/2)\\displaystyle\\sum_{j=1}^{i-1}\\alpha_{j}\\}\\|\\theta_{0}-\\theta^{\\star}\\|\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Moreover, for any $j>i$ one observes, expanding (33), that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\theta_{j}-\\theta_{j}^{(i)}=\\bigg\\{\\prod_{k=i+1}^{j}(\\mathrm{I}-\\alpha_{k}\\mathbf{A}(Z_{k}))\\bigg\\}\\big(\\theta_{i}-\\theta_{i}^{(i)}\\big)\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now we note that, using Minkowski's inequality, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}^{1/2}[\\|D-D^{(i)}\\|^{2}]\\le\\sum_{j=1}^{4}\\mathbb{E}^{1/2}[\\|D_{j}-D_{j}^{(i)}\\|^{2}]\\;,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and bound the respective differences separately. Recall that here $D_{1}-D_{4}$ are defined in (24), and $D_{1}^{(i)}-D_{4}^{(i)}$ are theirspctvcoutrpats wth $Z_{i}$ substuted with $Z_{i}^{\\prime}$ First we note that the term $D_{1}=D_{1}^{(i)}$ for any $n+1\\leq i\\leq2n$ .Next, using (36)and (35), we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{E}^{1/2}[\\|D_{2}-D_{2}^{(i)}\\|^{2}]=\\frac{1}{\\sqrt{n}\\alpha_{2n}}\\mathbb{E}^{1/2}[\\|{\\pmb\\theta}_{2n}-{\\pmb\\theta}_{2n}^{(i)}\\|^{2}]}}\\\\ {{\\displaystyle\\leq\\frac{1}{\\sqrt{n}\\alpha_{2n}}\\mathbb{E}^{1/2}[\\|{\\displaystyle\\prod_{k=i+1}^{2n}}\\ ({\\bf I}-\\alpha_{k}{\\bf A}_{k})\\|^{2}]\\mathbb{E}^{1/2}[\\|{\\pmb\\theta}_{i}-{\\pmb\\theta}_{i}^{(i)}\\|^{2}]}}\\\\ {{\\displaystyle\\stackrel{(a)}{\\lesssim}\\frac{\\alpha_{i}\\big(\\sqrt{\\mathrm{Tr}\\,\\Sigma_{\\varepsilon}}+\\mathrm{C}_{\\bf A}\\,\\|{\\pmb\\varepsilon}\\|_{\\infty}\\big)}{\\sqrt{n}\\alpha_{2n}}\\exp\\{-\\frac{a}{2}\\sum_{k=i+1}^{2n}\\alpha_{k}\\}}}\\\\ {{\\displaystyle\\quad\\quad\\quad\\quad+\\frac{\\alpha_{i}}{\\sqrt{n}\\alpha_{2n}}\\exp\\{-\\frac{a}{2}\\sum_{k=1}^{2n}\\alpha_{k}\\}\\|{\\pmb\\theta}_{0}-{\\pmb\\theta}^{\\star}\\|\\;.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In the inequality (a) above we additionally used the stability of matrix product introduced from Corollary 4. Summing the above inequality for $i=n+1$ to $2n$ and applying Lemma 2, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=n+1}^{2n}\\mathbb{E}^{1/2}[\\|D_{2}-D_{2}^{(i)}\\|^{2}]\\lesssim\\frac{\\sqrt{\\mathrm{Tr}\\sum_{\\varepsilon}}+\\mathrm{C_A}\\,\\|\\varepsilon\\|_{\\infty}}{a\\sqrt{n}\\alpha_{2n}}}&{{}\\quad{\\scriptstyle(38)}}\\\\ {\\displaystyle}&{\\qquad\\qquad\\qquad\\qquad+\\,\\frac{1}{a\\sqrt{n}\\alpha_{2n}}\\exp\\bigl\\{-\\frac{a}{2}\\sum_{k=1}^{n}\\alpha_{k}\\bigr\\}\\|\\theta_{0}-\\theta^{\\star}\\|}\\\\ {\\displaystyle}&{\\qquad\\qquad\\lesssim\\frac{\\sqrt{\\mathrm{Tr}\\sum_{\\varepsilon}}+\\mathrm{C_A}\\,\\|\\varepsilon\\|_{\\infty}}{a}n^{\\gamma-1/2}+\\frac{n^{\\gamma-1/2}}{a}\\exp\\biggl\\{-\\frac{c_{0}a n^{1-\\gamma}}{2(1-\\gamma)}\\biggr\\}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now we proceed with the difference $D_{3}-D_{3}^{(i)}$ . Using (24), we get ", "page_idx": 21}, {"type": "equation", "text": "$$\nD_{3}-D_{3}^{(i)}=\\frac{1}{\\sqrt{n}}(\\mathbf{A}_{i}-\\mathbf{A}_{i}^{\\prime})(\\theta_{i-1}-\\theta^{\\star})+\\frac{1}{\\sqrt{n}}\\sum_{k=i+1}^{2n}(\\mathbf{A}_{k}-\\bar{\\mathbf{A}})(\\theta_{k-1}-\\theta_{k-1}^{(i)})~.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The expression above is a sum of martingale-difference terms w.r.t. filtration $\\mathcal{F}_{k}^{\\prime}=\\sigma(Z_{i}^{\\prime},Z_{\\ell},\\ell\\leq k)$ Hence, we get, using (36), that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|D_{3}-D_{3}^{(i)}\\|^{2}]\\lesssim\\displaystyle\\frac{\\mathrm{C}_{\\mathrm{A}}^{2}}{n}\\mathbb{E}[\\|\\theta_{i-1}-\\theta^{\\star}\\|^{2}]+\\displaystyle\\frac{\\mathrm{C}_{\\mathrm{A}}^{2}}{n}\\sum_{k=i+1}^{2n}\\mathbb{E}[\\|\\theta_{k-1}-\\theta_{k-1}^{(i)}\\|^{2}]}\\\\ &{\\phantom{\\quad\\quad}\\lesssim\\displaystyle\\frac{\\mathrm{C}_{\\mathrm{A}}^{2}\\,\\|\\varepsilon\\|_{\\infty}^{2}\\alpha_{i}}{n a}+\\frac{\\mathrm{C}_{\\mathrm{A}}^{2}\\,\\|\\theta_{0}-\\theta^{\\star}\\|^{2}}{n}\\exp\\bigl\\{-a\\displaystyle\\sum_{j=1}^{i-1}\\alpha_{j}\\bigr\\}}\\\\ &{\\phantom{\\quad\\quad\\quad}\\qquad\\qquad+\\,\\displaystyle\\frac{\\mathrm{C}_{\\mathrm{A}}^{2}}{n}\\mathbb{E}[\\|\\theta_{i}-\\theta_{i}^{(i)}\\|^{2}]\\sum_{k=i+1}^{2n}\\exp\\bigl\\{-a\\displaystyle\\sum_{j=i+1}^{k-1}\\alpha_{j}\\bigr\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using now the bound (35), we obtain that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|\\theta_{i}-\\theta_{i}^{(i)}\\|^{2}]\\sum_{k=i+1}^{2n}\\exp\\{-a\\frac{k-1}{\\sum_{j=i+1}^{n}\\alpha_{j}}\\}}\\\\ &{\\quad\\lesssim\\bigg(\\alpha_{i}^{2}\\,\\mathrm{Tr}\\Sigma_{\\varepsilon}+\\frac{C_{\\star}^{2}\\,\\alpha_{i}^{3}}{a}\\bigg)\\sum_{k=i+1}^{2n}\\exp\\{-a\\frac{k-1}{\\sum_{j=i+1}^{n}\\alpha_{j}}\\}+\\alpha_{i}^{2}\\big\\|\\theta_{0}-\\theta^{*}\\big\\|^{2}\\sum_{k=i+1}^{2n}\\exp\\{-a\\frac{k-1}{\\sum_{j=1}^{n}\\alpha_{i}}\\}}\\\\ &{\\quad\\lesssim\\bigg(\\frac{\\alpha_{i}^{2}\\,\\mathrm{Tr}\\Sigma_{\\varepsilon}}{\\alpha_{2n}}+\\frac{C_{\\star}^{2}\\,\\alpha_{i}^{3}}{a\\alpha_{2n}}\\bigg)\\sum_{k=i+1}^{2n}\\alpha_{k}\\exp\\{-a\\frac{k-1}{\\sum_{j=i+1}^{n}\\alpha_{j}}\\}+\\frac{\\alpha_{i}^{2}}{\\alpha_{2n}}\\|\\theta_{0}-\\theta^{*}\\|_{k=i+1}^{2}\\sum_{k=i+1}^{2n}\\alpha_{k}\\exp\\{-a\\frac{k}{\\sum_{j=i}^{n}\\alpha_{j}}\\}}\\\\ &{\\quad\\stackrel{(a)}{\\lesssim}\\frac{\\alpha_{i}^{2}\\,\\mathrm{Tr}\\Sigma_{\\varepsilon}}{a\\alpha_{2n}}+\\frac{C_{\\star}^{2}\\,\\alpha_{i}^{3}}{a^{2}\\alpha_{2n}}+\\frac{\\alpha_{i}^{2}}{a\\alpha_{2n}}\\|\\theta_{0}-\\theta^{*}\\|^{2}\\exp\\{-a\\frac{k-1}{\\sum_{j=1}^{n}\\alpha_{j}}\\}}\\\\ &{\\quad\\stackrel{(b)}{\\lesssim}\\frac{\\alpha_{i}^{2}\\,\\mathrm{Tr}\\Sigma_{\\varepsilon}}{a\\alpha_{2n}}+\\frac{C_{\\star}^{2}}{a\\alpha_{2n}}+\\frac{\\alpha_{i}^{2}}{a\\alpha_{2n}}\\|\\theta_{0}-\\theta^{*}\\|^{2}\\exp\\{-a\\frac{k-1}{\\sum_{j=i}^{n}\\alpha \n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In the above formula in (a) we additionally used that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{k=i+1}^{2n}\\alpha_{k}\\exp\\bigl\\{-a\\sum_{j=i+1}^{k-1}\\alpha_{j}\\bigr\\}\\leq\\int_{0}^{+\\infty}\\exp\\{-a x\\}\\,d x=\\frac{1}{a}\\;,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and in (b) we used that $\\alpha_{i}\\leq a$ . Hence, combining everything in (39), and using additionally that $\\alpha_{i}\\leq a,\\kappa_{Q}\\geq1$ ,weget ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{E}^{1/2}[\\|D_{3}-D_{3}^{(i)}\\|^{2}]\\lesssim\\frac{\\mathrm{C}_{\\mathbf{A}}}{\\sqrt{n a}}\\left(\\|\\varepsilon\\|_{\\infty}\\sqrt{\\alpha_{i}}+\\frac{\\alpha_{i}\\big(\\sqrt{\\mathrm{Tr}\\,\\Sigma_{\\varepsilon}^{-}}+\\mathrm{C}_{\\mathbf{A}}\\big)}{\\sqrt{\\alpha_{2n}}}\\right)+}}\\\\ {{\\displaystyle\\frac{\\mathrm{C}_{\\mathbf{A}}}{\\sqrt{n}}\\left(1+\\frac{\\alpha_{i}}{\\sqrt{a\\alpha_{2n}}}\\right)\\exp\\bigl\\{-\\frac{a}{2}\\sum_{j=1}^{i-1}\\alpha_{i}\\bigr\\}\\|\\theta_{0}-\\theta^{\\star}\\|~.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Summing the above inequality for $i=n+1$ to $2n$ , and using that $\\alpha_{k}=c_{0}/k^{\\gamma}$ , we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{i=n+1}^{2n}\\sqrt{\\alpha_{i}}\\stackrel{<}{\\sim}n^{1-\\gamma/2}\\;,\\quad\\sum_{i=n+1}^{2n}\\frac{\\alpha_{i}}{\\sqrt{\\alpha_{2n}}}\\stackrel{<}{\\sim}n^{1-\\gamma/2}\\;,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and, hence, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{i=n+1}^{2n}\\mathbb{E}^{1/2}[\\|D_{3}-D_{3}^{(i)}\\|^{2}]}}\\\\ &{\\lesssim\\frac{C_{\\mathsf{A}}}{\\sqrt{a}}\\left(\\|\\varepsilon\\|_{\\infty}+\\sqrt{\\mathrm{Tr}\\Sigma_{\\varepsilon}}+\\mathrm{C}_{\\mathsf{A}}\\right)n^{1/2-\\gamma/2}+\\frac{\\mathrm{C}_{\\mathsf{A}}}{\\sqrt{n}\\alpha_{2n}}\\|\\theta_{0}-\\theta^{\\star}\\|\\underbrace{2^{n}}_{i=n+1}\\alpha_{i}\\exp\\left\\{-\\frac{a}{2}\\sum_{j=1}^{i-1}\\alpha_{j}\\right\\}}\\\\ &{\\stackrel{(b)}{\\lesssim}\\frac{\\mathrm{C}_{\\mathsf{A}}}{\\sqrt{a}}\\left(\\|\\varepsilon\\|_{\\infty}+\\sqrt{\\mathrm{Tr}\\Sigma_{\\varepsilon}}+\\mathrm{C}_{\\mathsf{A}}\\right)n^{1/2-\\gamma/2}+\\frac{\\mathrm{C}_{\\mathsf{A}}\\,n^{\\gamma-1/2}}{a}\\|\\theta_{0}-\\theta^{\\star}\\|\\exp\\left\\{-\\frac{a}{2}\\displaystyle\\sum_{j=1}^{n}\\alpha_{j}\\right\\}}\\\\ &{\\lesssim\\frac{\\mathrm{C}_{\\mathsf{A}}}{\\sqrt{a}}\\left(\\|\\varepsilon\\|_{\\infty}+\\sqrt{\\mathrm{Tr}\\Sigma_{\\varepsilon}}+\\mathrm{C}_{\\mathsf{A}}\\right)n^{1/2-\\gamma/2}+\\frac{\\mathrm{C}_{\\mathsf{A}}\\,n^{\\gamma-1/2}}{a^{2}}\\|\\theta_{0}-\\theta^{\\star}\\|\\exp\\left\\{-\\frac{c_{0}a^{1-\\gamma}}{2(1-\\gamma)}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where for the last identity we used the fact that $\\alpha_{k}=c_{0}/k^{\\gamma}$ , and for the bound (b) we used (40). It remains to upper bound the difference $D_{4}-D_{4}^{(i)}$ . Note first that, proceeding as in (27) we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\alpha_{k-1}-\\alpha_{k}\\le\\frac{\\gamma}{k-1}\\frac{1}{\\alpha_{k-1}}\\stackrel{<}{\\sim}\\frac{1}{(k-1)^{1-\\gamma}}\\;.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using now the definition of $D_{4}$ in (24), we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{E}^{1/2}[\\|D_{4}-D_{4}^{(i)}\\|^{2}]=\\frac{1}{\\sqrt{n}}\\mathbb{E}^{1/2}[\\|\\sum_{k=i+1}^{2n}\\left(\\theta_{k-1}-\\theta_{k-1}^{(i)}\\right)\\left(\\frac{1}{\\alpha_{k}}-\\frac{1}{\\alpha_{k-1}}\\right)\\|^{2}]}}\\\\ {{\\displaystyle\\leq\\frac{1}{\\sqrt{n}}\\mathbb{E}^{1/2}[\\|\\theta_{i}-\\theta_{i}^{(i)}\\|^{2}]\\sum_{k=i+1}^{2n}\\left(\\frac{1}{\\alpha_{k}}-\\frac{1}{\\alpha_{k-1}}\\right)\\exp\\{-\\frac{a}{2}\\sum_{j=i+1}^{k-1}\\alpha_{j}\\}\\ .}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, using the bound (35) and taking sum for $i=n+1$ to $2n$ \uff0c ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\sum_{i=n+1}^{2n}{\\mathbb{E}}^{1/2}[\\|D_{4}-D_{4}^{(i)}\\|^{2}]\\lesssim\\left(\\frac{\\sqrt{\\mathrm{Tr}\\sum_{\\varepsilon}}+\\mathrm{C}_{\\mathrm{A}}}{\\sqrt{n}}\\right)\\sum_{i=n+1}^{2n}\\alpha_{i}\\sum_{k=i+1}^{2n}\\left(\\frac{1}{\\alpha_{k}}-\\frac{1}{\\alpha_{k-1}}\\right)\\exp\\{-a\\sum_{j=i+1}^{k-1}\\alpha_{j}\\}}}\\\\ {{\\displaystyle\\frac{\\|\\theta_{0}-\\theta^{\\star}\\|}{\\sqrt{n}}\\sum_{i=n+1}^{2n}\\alpha_{i}\\sum_{k=i+1}^{2n}\\left(\\frac{1}{\\alpha_{k}}-\\frac{1}{\\alpha_{k-1}}\\right)\\exp\\{-\\frac{a}{2}\\sum_{j=1}^{k-1}\\alpha_{j}\\}~.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Changing now the summation order, we obtain that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{i=n+1}^{2n}\\alpha_{i}\\,\\sum_{k=i+1}^{2n}\\left(\\frac{1}{\\alpha_{k}}-\\frac{1}{\\alpha_{k-1}}\\right)\\exp\\{-a\\sum_{j=i+1}^{k-1}\\alpha_{j}\\}\\lesssim\\frac{1}{a}\\sum_{k=n+2}^{2n}\\left(\\frac{1}{\\alpha_{k}}-\\frac{1}{\\alpha_{k-1}}\\right)}}\\\\ &{}&{=\\frac{1}{a}\\left(\\frac{1}{\\alpha_{n+1}}-\\frac{1}{\\alpha_{2n}}\\right)\\lesssim\\frac{1}{a\\alpha_{2n}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, combining the above bounds, we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=n+1}^{2n}\\mathbb{E}^{1/2}[\\|D_{4}-D_{4}^{(i)}\\|^{2}]\\lesssim\\frac{\\sqrt{\\mathrm{Tr}\\,\\Sigma_{\\varepsilon}}+\\mathrm{C}_{\\mathbf{A}}}{\\sqrt{n}a\\alpha_{2n}}+\\displaystyle\\sum_{i=n+1}^{2n}\\mathbb{E}^{1/2}[\\|D_{4}-D_{4}^{(i)}\\|^{2}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\lesssim\\frac{\\sqrt{\\mathrm{Tr}\\,\\Sigma_{\\varepsilon}}+\\mathrm{C}_{\\mathbf{A}}}{a}n^{\\gamma-1/2}+\\displaystyle\\frac{\\|\\theta_{0}-\\theta^{\\star}\\|}{\\sqrt{n}a}\\exp\\left\\{-\\frac{c_{0}a n^{1-\\gamma}}{2(1-\\gamma)}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It remains now to combine (38), (39), and (41) in (37) ", "page_idx": 22}, {"type": "text", "text": "B.2  Relations between $\\rho_{n}^{\\mathrm{Conv}}$ and integral probability metrics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section we closely follow the exposition outlined in [24]. Consider two $\\mathbb{R}^{d}$ valuedrandom variables $X$ and $Y$ . Then the integral probability metric [75], associated with the class of functions $\\mathcal{H}=\\{h:\\mathbb{R}^{d}\\rightarrow\\mathbb{R},\\mathbb{E}[|h(X)|]<\\mathrm{\\dot{\\infty}},\\bar{\\mathbb{E}}[|h(Y)|]<\\infty\\}$ is defined as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathsf{d}_{\\mathcal{H}}(X,Y)=\\operatorname*{sup}_{h\\in\\mathcal{H}}\\left|\\mathbb{E}[h(X)]-\\mathbb{E}[h(Y)]\\right|\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Different choices of $\\mathcal{H}$ induce different metrics, in particular, we mention the following: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{H}_{K}=\\{\\mathbf{1}_{x\\leq u},\\quad u=(u_{1},\\dotsc,u_{d})\\in\\mathbb{R}^{d}\\}}&{}\\\\ {\\mathcal{H}_{C o n v}=\\{\\mathbf{1}_{x\\in B},\\quad B\\in\\mathrm{Conv}(\\mathbb{R}^{d})\\}}&{}\\\\ {\\mathcal{H}_{W}=\\{h:\\mathbb{R}^{d}\\rightarrow\\mathbb{R},\\quad\\|h\\|_{\\mathrm{Lip}}\\leq1\\}}&{}\\\\ {\\mathcal{H}_{[m]}=\\{h:\\mathbb{R}^{d}\\rightarrow\\mathbb{R},\\quad h^{m-1}\\mathrm{~is~Lipschitz~with}\\;|h|_{j}\\leq1\\;,\\quad1\\leq j\\leq m\\}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\operatorname{Conv}(\\mathbb{R}^{d})$ refrsto the set of conex  in $\\mathbb{R}^{d}$ \uff0c $\\begin{array}{r}{\\|h\\|_{\\mathrm{Lip}}\\,=\\,\\operatorname*{sup}_{x\\neq y}\\frac{\\|h(x)-h(y)\\|}{\\|x-y\\|}}\\end{array}$ and the quantity $|h|_{j}$ is defined as ", "page_idx": 23}, {"type": "equation", "text": "$$\n|h|_{j}=\\operatorname*{max}_{\\substack{i_{1},\\ldots,i_{j}\\in\\{1,...,d\\}}}\\|\\frac{\\partial^{j}h(u)}{\\partial u_{i_{1}}\\ldots\\partial u_{i_{j}}}\\|_{\\infty}\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In other words, for $m\\in\\mathbb{N}$ the class $\\mathcal{H}_{[m]}$ corresponds to the functions with bounded derivatives up to the $(m-1)$ -th order. The class $\\mathcal{H}_{K}$ induces the Kolmogorov distance between distributions [75], class $\\mathcal{H}_{C o n v}$ induces the metric $\\rho_{n}^{\\mathrm{Conv}}$ defined in 2), which is the main object of studies in the current paper. Class $\\mathcal{H}_{W}$ induces the celebrated Wasserstein distance, and classes $\\mathcal{H}_{[m]}$ induce smoothed Wasserstei distances. We will denote th respectiv metrics by ${\\mathsf{d}}_{K},\\rho_{n}^{\\mathrm{Conv}},{\\mathsf{d}}_{W}$ , and ${\\mathsf d}_{[m]}$ respectively. Then, obviously, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathsf{d}_{K}(X,Y)\\leq\\rho_{n}^{\\mathrm{Conv}}(X,Y)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for any random vectors $X$ and $Y$ . Other relations are more involved. When $Y$ is a multivariate normal vector, it is known (see e.g. [49]) that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\rho_{n}^{\\mathrm{Conv}}(X,Y)\\leq C\\sqrt{\\mathsf{d}_{W}(X,Y)}\\;,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "wheretheconstant $C$ in the above inequality depends on the covariance matrix of vector $Y$ .This inequality justifies comparison of our bounds of Theorem 2 with the result of [65]. The authors in [2] considered integral probability metric ${\\mathsf{d}}_{[2]}$ and obtained rate of convergence ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\bf d}_{[2]}(\\sqrt{n}(\\bar{\\theta}_{n}-\\theta^{\\star}),Y)\\leq\\frac{C_{1}}{\\sqrt{n}}\\;,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\boldsymbol{Y}\\,\\sim\\,\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{\\Sigma}_{\\infty})$ :and $C_{1}$ in the above inequality stands for a constant depending upon problem dimension $d$ and other instance-dependent parameters from A2. Applying the result of [24, Proposition 2.6] yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathsf{d}_{K}\\bigl(\\sqrt{n}\\bigl(\\bar{\\theta}_{n}-\\theta^{\\star}\\bigr),Y\\bigr)\\lesssim\\bigl(\\mathsf{d}_{[2]}\\bigl(\\sqrt{n}(\\bar{\\theta}_{n}-\\theta^{\\star}),Y)\\bigr)^{1/3}\\lesssim\\frac{1}{n^{1/6}}\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, the result of [2] implies rate of convergence of $\\sqrt{n}(\\bar{\\theta}_{n}-\\theta^{\\star})$ to normal law $\\mathcal{N}(0,\\Sigma_{\\infty})$ of order $n^{-1/6}$ in a sense of Kolmogorov distance ${\\sf d}_{K}$ Our result of Theorem 2 implies the respective rate of order $n^{-1/4}$ Atth a s $\\rho_{n}^{\\mathrm{Conv}}$ can be directlyrelated to ${\\mathsf{d}}_{[2]}$ ", "page_idx": 23}, {"type": "text", "text": "C Bootstrap validity proof ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "C.1  Proof of Theorem 3 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We first define explicitly the remainder term $\\Delta_{2}$ outlined in the statement of Theorem 3, that is, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Delta_{2}(n,a,\\mathrm{C}_{\\bf A},\\|\\varepsilon\\|_{\\infty})=\\frac{\\kappa_{Q}^{3/2}(\\mathrm{C}_{\\bf A}^{3}\\vee1)\\|\\varepsilon\\|_{\\infty}n^{1/4}\\sqrt{\\log n}}{a^{3/2}}\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We now define the following sets, with the convention $\\alpha_{\\ell}=c_{0}/\\sqrt{\\ell}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{I}_{1}=\\left\\lbrace\\nabla t\\in[n,2n-1]:|\\{\\theta_{1}-\\theta^{*}\\}|\\geq\\sqrt{n}\\varphi^{n}\\mathrm{erf}\\left(-\\frac{a_{1}}{2}\\frac{\\Delta}{n}\\right)(\\theta_{1}-\\varphi^{*})\\right\\rbrace\\qquad(4)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\frac{8\\pi^{2}\\sqrt{n}\\|c\\|_{\\infty}\\|_{\\infty}\\log\\varphi^{*}\\sin\\varphi^{*}\\sin\\varphi^{*}\\sqrt{n}\\varphi^{*}\\sin\\varphi^{*}\\sqrt{n}\\varphi^{*}\\sin\\varphi^{*}\\sqrt{n}\\varphi^{*}\\sin\\varphi^{*}\\sqrt{n}\\varphi^{*}\\sin\\varphi^{*}\\sqrt{n}\\varphi^{*}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\frac{1}{\\sqrt{n}}=\\frac{3}{\\sqrt{n}}\\sum_{i=1}^{n}\\frac{\\sqrt{n}}{n}\\sum_{i=1}^{n}(1-\\frac{a_{i}\\Delta^{2}}{n})\\Bigg\\}\\;,}\\\\ &{\\mathbb{I}_{3}=\\left\\lbrace\\frac{\\sum_{i=1}^{n-1}2\\sqrt{n}\\frac{\\Delta}{n}\\mathrm{e}^{-1}(2\\pi)\\mathrm{e}^{-1}\\left(\\mathbb{I}_{1}\\right)\\sqrt{n}\\frac{\\sqrt{n}\\varphi^{*}}{\\sqrt{n}}+\\frac{4(1+\\left\\lfloor1\\frac{\\sqrt{n}}{2}\\right\\rfloor-\\frac{a_{i}\\Delta^{2}}{n})^{2}\\mathrm{in}\\varphi^{*}}{n}\\right\\rbrace\\;,}\\\\ &{\\mathbb{I}_{4}=\\left\\lbrace\\nabla t\\in[n,2n-1]:\\;\\Big\\|\\begin{array}{l l}{\\sum_{i=1}^{n}\\left(\\mathbb{A}_{4}-\\mathbb{A}\\right)\\Gamma_{i+1-1}\\right\\rbrace\\leq\\frac{\\mathbb{S}\\mathbf{C}\\mathbf{A}_{4}\\sqrt{n}\\sqrt{n}\\varphi^{*}\\sqrt{n}\\varphi^{*}\\sin\\varphi}{\\sqrt{n}\\overline{{n}}\\varphi^{*}\\sin\\varphi}\\;,}\\\\ {\\sum_{i=1}^{n}\\Big\\langle\\mathbb{A}_{4}\\Big\\rangle\\sin\\theta_{4}\\quad\\Big\\langle\\frac{\\sum_{i=1}^{n}\\Delta}{n}\\Big,\\Big\\|\\frac{\\sum_{i=1}^{n}\\Delta}{n}\\Big,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, due to Corollary 2, we have that $\\begin{array}{r}{\\mathbb{P}(\\Omega_{1})\\geq1-\\frac{1}{n}}\\end{array}$ . Similarly, due to Corollary 5, $\\begin{array}{r}{\\mathbb{P}(\\Omega_{2})\\geq1-\\frac{1}{n}}\\end{array}$ The bounds on $\\mathbb{P}(\\Omega_{3})$ and $\\mathbb{P}(\\Omega_{4})$ follows from Lemma 5 and Lemma 6, respectively. Similarly, Proposition 7 implies that $\\mathbb{P}(\\Omega_{5})\\geq1-\\frac{1}{n}$ . Hence, based on the sets above, we can construct ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Omega_{0}=\\Omega_{1}\\cap\\Omega_{2}\\cap\\Omega_{3}\\cap\\Omega_{4}\\cap\\Omega_{5}\\cap\\Omega_{6}\\mathrm{~,~}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "such that $\\begin{array}{r}{\\mathbb{P}(\\Omega_{0})\\geq1-\\frac{6}{n}}\\end{array}$ . All further on, we restrict ourselves to the event $\\Omega_{0}$ . Restricting to this event, we obtain that, with Minkowski's inequality, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{B\\in\\mathrm{Conv}(\\mathbb{R}^{d})}{\\operatorname*{sup}}\\,|\\mathbb{P}^{\\mathrm{b}}(\\sqrt{n}(\\bar{\\theta}_{n}^{\\mathrm{b}}-\\bar{\\theta}_{n})\\in B)-\\mathbb{P}(\\sqrt{n}(\\bar{\\theta}_{n}-\\theta^{\\star})\\in B)|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\underset{B\\in\\mathrm{Conv}(\\mathbb{R}^{d})}{\\operatorname*{sup}}\\,\\big|\\mathbb{P}^{\\mathrm{b}}(\\sqrt{n}(\\bar{\\theta}_{n}^{\\mathrm{b}}-\\bar{\\theta}_{n})\\in B)-\\mathbb{P}^{\\mathrm{b}}(\\xi^{\\mathrm{b}}\\in B)\\big|}\\\\ &{\\quad\\quad\\quad\\quad+\\underset{B\\in\\mathrm{Conv}(\\mathbb{R}^{d})}{\\operatorname*{sup}}\\,\\big|\\mathbb{P}(\\xi\\in B)-\\mathbb{P}^{\\mathrm{b}}(\\xi^{\\mathrm{b}}\\in B)\\big|}\\\\ &{\\quad\\quad\\quad\\quad+\\underset{B\\in\\mathrm{Conv}(\\mathbb{R}^{d})}{\\operatorname*{sup}}\\,\\big|\\mathbb{P}\\big(\\sqrt{n}(\\bar{\\theta}_{n}-\\theta^{\\star})\\in B\\big)-\\mathbb{P}(\\xi\\in B)\\big|\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we set $\\boldsymbol{\\xi}^{\\flat}\\sim\\mathcal{N}(0,\\bar{\\mathbf{A}}^{-1}\\Sigma_{\\varepsilon}^{\\flat}\\bar{\\mathbf{A}}^{-\\top})$ $\\begin{array}{r}{\\Sigma_{\\varepsilon}^{\\mathsf{b}}=n^{-1}\\sum_{\\ell=1}^{n}\\varepsilon_{\\ell}\\varepsilon_{\\ell}^{\\top}}\\end{array}$ , and $\\boldsymbol{\\xi}\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{\\Sigma}_{\\infty})$ , where $\\Sigma_{\\infty}=$ $\\bar{\\mathbf{A}}^{-1}\\Sigma_{\\varepsilon}\\bar{\\mathbf{A}}^{-\\top}$ . Now we control the frst supremum using Theorem 4, second one using Theorem 5, and third with Theorem 2. ", "page_idx": 24}, {"type": "text", "text": "Lemma 5. Assume A1, $A2$ A3with $\\gamma=1/2$ ,and A4. Then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\Omega_{3})\\geq1-1/n.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. The proof follows directly from the matrix Bernstein inequality, e.g. [69]. We note that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\sum_{\\varepsilon}^{-1/2}\\varepsilon_{\\ell}\\varepsilon_{\\ell}^{\\top}\\sum_{\\varepsilon}^{-1/2}-\\mathrm{I}\\right\\|\\leq1+\\left\\|\\Sigma_{\\varepsilon}^{-1/2}\\varepsilon\\right\\|_{\\infty}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\sum_{k=n+1}^{2n}\\mathbb{E}[(\\Sigma_{\\varepsilon}^{-1/2}\\varepsilon_{\\ell}\\varepsilon_{\\ell}^{\\top}\\Sigma_{\\varepsilon}^{-1/2}-\\mathrm{I})^{2}]\\|\\le n\\mathbb{E}\\|\\Sigma_{\\varepsilon}^{-1/2}\\varepsilon\\|_{\\infty}^{2}\\;.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma 6. Assume A1, A2, A3 with $\\gamma=1/2$ ,and A4.Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\Omega_{4}\\cap\\Omega_{2}\\right)\\geq1-\\frac{1}{n}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Denote ", "page_idx": 25}, {"type": "equation", "text": "$$\nX_{k}=(\\mathbf{A}_{k}-\\bar{\\mathbf{A}})\\Gamma_{\\ell+1:k-1}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and let $\\mathcal{F}_{k,l+1}\\,=\\,\\sigma\\{Z_{j},\\ell+1\\,\\le\\,j\\,\\le\\,k\\}$ \uff0c\u201d $\\ell+1\\,\\leq\\,k\\,\\leq\\,2n$ Then $\\mathbb{E}[X_{k}|\\mathcal{F}_{k-1,\\ell+1}]\\,=\\,0$ . Let $\\begin{array}{r}{S_{\\ell}=\\sum_{k=\\ell+1}^{n}X_{k}}\\end{array}$ Note that on $\\Omega_{2}$ quadratic variation of $S_{\\ell}$ can be controlled as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}^{2}:=\\operatorname*{max}(\\|\\displaystyle\\sum_{k=\\ell+1}^{2n}\\mathbb{E}[X_{k}X_{k}^{\\top}|\\mathcal{F}_{k-1,l+1}]\\|,\\|\\displaystyle\\sum_{k=\\ell+1}^{2n}\\mathbb{E}[X_{k}^{\\top}X_{k}|\\mathcal{F}_{k-1,l+1}]\\|)}\\\\ &{\\quad\\le\\kappa_{Q}\\mathrm{e}^{4}\\,\\mathrm{C}_{\\mathbf{A}}^{2n}\\displaystyle\\sum_{k=\\ell+1}^{2n}\\displaystyle\\prod_{j=\\ell+1}^{k-1}(1-a\\alpha_{j}/4)^{2}\\le\\frac{4\\kappa_{Q}\\mathrm{e}^{4}\\,\\mathrm{C}_{\\mathbf{A}}^{2}}{a\\alpha_{\\ell}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Furthermore,on $\\Omega_{2}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|X_{k}\\|\\leq\\sqrt{\\kappa_{Q}}\\mathrm{e}^{2}\\,\\mathrm{C}_{\\mathbf{A}}\\prod_{j=\\ell+1}^{k-1}(1-a\\alpha_{j}/4)\\leq\\sqrt{\\kappa_{Q}}\\mathrm{e}^{2}\\,\\mathrm{C}_{\\mathbf{A}}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It remains to apply the Freedman inequality for matrix-values martingales [68] and use the union bound over $\\ell\\in[n,2n-1]$ \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Lemma 7. Assume A1, A2,A3 with $\\gamma=1/2$ andA4.Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\Omega_{5})\\geq1-1/n.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. We first fix $h\\in[1;n]$ \uff0c $m\\in[n,2n-h]$ , and consider the random variable ", "page_idx": 25}, {"type": "equation", "text": "$$\nT_{n}=\\|\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}(\\mathbf{A}_{\\ell}-\\bar{\\mathbf{A}})\\|\\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then we control its variance as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname{nax}(\\|\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}^{2}\\mathbb{E}[(\\mathbf{A}_{\\ell}-\\bar{\\mathbf{A}})(\\mathbf{A}_{\\ell}-\\bar{\\mathbf{A}})^{\\top}]\\|,\\|\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}^{2}\\mathbb{E}[(\\mathbf{A}_{\\ell}-\\bar{\\mathbf{A}})^{\\top}(\\mathbf{A}_{\\ell}-\\bar{\\mathbf{A}})]\\|)\\leq\\mathrm{C}_{\\mathbf{A}}^{2}\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "moreover, $\\begin{array}{r}{\\|(\\mathbf{A}_{\\ell}-\\bar{\\mathbf{A}})(\\mathbf{A}_{\\ell}-\\bar{\\mathbf{A}})^{\\top}\\|\\le\\mathrm{C}_{\\mathbf{A}}^{2}}\\end{array}$ Applying now the matrix Bernstein inequality [69], we obtain that with probability at least $1-1\\bar{/n}^{3}$ ,wehave ", "page_idx": 25}, {"type": "equation", "text": "$$\nT_{n}\\leq\\mathrm{C}_{\\mathbf{A}}\\,\\sqrt{2\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}^{2}}\\sqrt{\\log\\left(2n^{3}d\\right)}+\\frac{\\alpha_{m+1}\\,\\mathrm{C}_{\\mathbf{A}}}{3}\\log\\left(2n^{3}d\\right)\\leq2\\,\\mathrm{C}_{\\mathbf{A}}\\,\\sqrt{\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}^{2}}\\log\\left(2n^{4}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In the last line here we used that $d\\leq n$ . Rest of the proof follows by taking union bound over $h$ and $m$ together with $\\|B\\|_{Q}^{2}\\leq\\kappa_{Q}\\|B\\|^{2}$ valid for any matrix $B\\in\\mathbb{R}^{d\\times d}$ \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Lemma 8. Assume A1,A2,A3 with $\\gamma=1/2$ andA4.Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\Omega_{6})\\geq1-1/n.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. It is easy to check that $\\|\\varepsilon_{\\ell}\\varepsilon_{\\ell}^{\\top}-\\Sigma_{\\varepsilon}\\|\\leq\\|\\Sigma_{\\varepsilon}\\|+\\|\\varepsilon\\|_{\\infty}^{2}.$ Moreover, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{Var}^{2}:=\\|\\sum_{\\ell=1}^{n}(\\varepsilon_{\\ell}\\varepsilon_{\\ell}^{\\top}-\\Sigma_{\\varepsilon})^{2}\\|\\leq n\\|\\varepsilon\\|_{\\infty}^{2}\\|\\Sigma_{\\varepsilon}\\|.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It remains to apply the matrix Bernstein inequality together with the bound $n\\geq d$ from A3. ", "page_idx": 25}, {"type": "text", "text": "C.2 Rate of Gaussian approximation in the bootstrap world ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The main result of this section is the following theorem. ", "page_idx": 26}, {"type": "text", "text": "Theorem 4. Assume A1, A2, A3 with $\\gamma=1/2$ , and A4. Then, conditionally on the event $\\Omega_{0}$ the following error bound holds: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{B\\in\\mathrm{Conv}(\\mathbb{R}^{d})}\\bigg|\\mathbb{P}^{\\mathrm{b}}\\big(\\sqrt{n}(\\bar{\\theta}_{n}^{\\mathrm{b}}-\\bar{\\theta}_{n})\\in B\\big)-\\mathbb{P}^{\\mathrm{b}}(\\xi^{\\mathrm{b}}\\in B)\\bigg|\\lesssim\\frac{d^{1/2}\\|\\varepsilon\\|_{\\infty}^{3}}{\\lambda_{\\operatorname*{min}}^{3/2}\\sqrt{n}}+\\frac{\\kappa_{Q}^{3/2}(\\mathrm{C}_{\\mathbf{A}}^{3}\\vee1)\\|\\varepsilon\\|_{\\infty}^{2}\\log n}{a^{3/2}\\lambda_{\\operatorname*{min}}n^{1/4}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\frac{\\kappa_{Q}^{3/2}(\\mathrm{C}_{\\mathbf{A}}^{3}\\vee1)\\|\\varepsilon\\|_{\\infty}n^{1/4}\\sqrt{\\log n}}{a^{3/2}\\lambda_{\\operatorname*{min}}}\\exp\\{-\\frac{a}{4}\\displaystyle\\sum_{j=1}^{n}\\alpha_{j}\\}\\|\\theta_{0}-\\theta^{\\star}\\|~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\boldsymbol{\\xi}^{\\flat}\\sim\\mathcal{N}(0,\\bar{\\mathbf{A}}^{-1}\\Sigma_{\\varepsilon}^{\\flat}\\bar{\\mathbf{A}}^{-\\top})$ and $\\begin{array}{r}{\\Sigma_{\\varepsilon}^{\\mathsf{b}}=n^{-1}\\sum_{\\ell=1}^{n}\\varepsilon_{\\ell}\\varepsilon_{\\ell}^{\\top}}\\end{array}$ ", "page_idx": 26}, {"type": "text", "text": "Proof. Since both terms in the right-hand side of the error bound of Proposition 4 scales linearly with $\\sqrt{\\kappa_{Q}}$ , for simplicity we do not trace it in the subsequent bounds (i.e. assume $\\kappa_{Q}=1$ ), and then keep the required scaling with $\\kappa_{Q}$ only in the final bounds. Recall first that the quantities $\\theta_{k}^{\\mathsf{b}}$ and $\\theta_{k}$ are defined in (15). We start from the following decomposition: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{\\dot{\\beta}}_{k}^{\\mathrm{b}}-\\theta_{k}=(\\mathbf{I}-\\alpha_{k}{\\bar{\\mathbf{A}}})(\\theta_{k-1}^{\\mathrm{b}}-\\theta_{k-1})-\\alpha_{k}(w_{k}-1)\\varepsilon_{k}-\\alpha_{k}(\\mathbf{A}_{k}-{\\bar{\\mathbf{A}}})(\\theta_{k-1}^{\\mathrm{b}}-\\theta_{k-1})-\\alpha_{k}(w_{k}-1)\\mathbf{A}_{k}(\\theta_{k-1}^{\\mathrm{b}}-\\theta_{k-1})+\\alpha_{k}(w_{k}-1)\\varepsilon_{k}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Taking average for $k$ from $n+1$ to $2n$ , we get after multiplying by $\\sqrt{n}$ that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{n}\\bar{\\mathbf{A}}(\\bar{\\theta}_{n}^{\\mathrm{b}}-\\bar{\\theta}_{n})=-\\underbrace{\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}(w_{k}-1)\\varepsilon_{k}}_{W^{\\mathrm{b}}}+\\underbrace{\\frac{1}{\\sqrt{n}}\\frac{\\theta_{n}^{\\mathrm{b}}-\\theta_{n}}{\\alpha_{n}}}_{D_{1}^{\\mathrm{b}}}-\\underbrace{\\frac{1}{\\sqrt{n}}\\frac{\\theta_{n}^{\\mathrm{b}}-\\theta_{2n}}{\\alpha_{2n}}}_{D_{2}^{\\mathrm{b}}}}\\\\ &{\\quad-\\underbrace{\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}(w_{k}-1)\\mathbf{A}_{k}(\\theta_{k-1}^{\\mathrm{b}}-\\theta^{\\mathrm{b}})}_{D_{3}^{\\mathrm{b}}}+\\underbrace{\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}\\left(\\theta_{k-1}^{\\mathrm{b}}-\\theta_{k-1}\\right)\\left(\\frac{1}{\\alpha_{k}}-\\frac{1}{\\alpha_{k-1}}\\right)}_{D_{4}^{\\mathrm{b}}}}\\\\ &{\\quad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-\\underbrace{\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}(\\mathbf{A}_{k}-\\bar{\\mathbf{A}})(\\theta_{k-1}^{\\mathrm{b}}-\\theta_{k-1})}_{D_{5}^{\\mathrm{b}}}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The formula (44) resembles the key representation $T^{\\flat}:=\\sqrt{n}\\bar{\\mathbf{A}}(\\bar{\\theta}_{n}^{\\flat}-\\bar{\\theta}_{n})=W^{\\flat}+D^{\\flat}$ where ", "page_idx": 26}, {"type": "equation", "text": "$$\nD^{\\mathsf{b}}=D_{1}^{\\mathsf{b}}+\\ldots+D_{5}^{\\mathsf{b}}\\;,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and $D_{1}^{\\mathsf{b}}-D_{5}^{\\mathsf{b}}$ are defined in (44). Now we aim to apply the result of [63]: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{B\\in\\mathrm{Conv}(\\mathbb{R}^{d})}{\\operatorname*{sup}}\\vert\\mathbb{P}^{\\mathrm{b}}(T^{\\mathrm{b}}\\in B)-\\mathbb{P}^{\\mathrm{b}}(\\xi^{\\mathrm{b}}\\in B)\\vert\\le259d^{1/2}\\Upsilon+2\\mathbb{E}^{\\mathrm{b}}[\\Vert W^{\\mathrm{b}}\\Vert\\Vert D^{\\mathrm{b}}\\Vert]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\operatorname*{2}\\sum_{\\ell=n+1}^{2n}\\mathbb{E}^{\\mathrm{b}}[\\Vert\\xi_{\\ell}\\Vert\\Vert D^{\\mathrm{b}}-D^{(\\mathrm{b},\\ell)}\\Vert]~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\begin{array}{r}{\\xi_{\\ell}=\\frac{1}{\\sqrt{n}}(w_{\\ell}-1)\\varepsilon_{\\ell}}\\end{array}$ We finish the proof by the application of the formula (46). In order to bound the quantities $\\mathbb{E}^{\\boldsymbol{\\ b}}\\|\\boldsymbol{D}^{\\boldsymbol{\\ b}}\\|^{2}$ and $\\mathbb{E}^{\\mathsf{b}}\\|D^{(\\mathsf{b},i)}\\|^{2}$ , we apply the respective results of Proposition 5 and Proposition 6, respectively. Namely, applying the Cauchy-Schwartz inequality together with Proposition 5, we get that on the event $\\Omega_{0}$ it holds ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\hat{E}}[\\|D^{\\mathbb{b}}\\|\\|W^{\\mathbb{b}}\\|]\\leq\\left\\{\\mathbb{E}^{\\mathbb{b}}[\\|D^{\\mathbb{b}}\\|^{2}]\\right\\}^{1/2}\\{\\mathbb{E}}^{\\mathbb{b}}[\\|W^{\\mathbb{b}}\\|^{2}]\\right\\}^{1/2}\\lesssim\\frac{\\kappa_{Q}^{2}(\\mathbb{C}_{\\mathbf{A}}^{4}\\vee1)\\|\\varepsilon\\|_{\\infty}\\sqrt{\\mathrm{Tr}\\,\\Sigma_{\\varepsilon}^{\\mathbb{b}}}\\log n}{n^{1/4}a^{5/2}}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\kappa_{Q}^{3/2}\\|\\varepsilon\\|_{\\infty}\\bigg(\\frac{(\\mathbb{C}_{\\mathbf{A}}^{3}\\vee1)n^{1/4}}{\\sqrt{a}}+\\frac{(\\mathbb{C}_{\\mathbf{A}}^{5}\\vee1)\\sqrt{\\log n}}{\\sqrt{n}a}\\bigg)\\exp\\bigg\\{-\\frac{c_{0}a\\sqrt{n}}{2}\\bigg\\}\\|\\theta_{0}-\\theta^{\\star}\\|~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Similarly, applying Minkowski's inequality and Proposition 6, we obtain that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\xi^{\\tt b}[\\sum_{i=n}^{2n-1}\\|\\xi_{i}\\|\\|D^{\\tt b}-D^{(\\tt b},i)\\|]\\le\\left\\{\\mathbb{E}^{\\tt b}[\\|\\xi_{1}\\|^{2}]\\right\\}^{1/2}\\sum_{i=n}^{2n-1}\\left\\{\\mathbb{E}^{\\tt b}[\\|D^{\\tt b}-D^{(\\tt b},i)\\|^{2}]\\right\\}^{1/2}}\\\\ {\\displaystyle\\lesssim\\frac{\\kappa_{Q}^{3/2}({\\cal C}_{\\bf A}^{3}\\vee1)\\|\\varepsilon\\|_{\\infty}^{2}\\log n}{a^{3/2}n^{1/4}}+\\frac{\\kappa_{Q}\\|\\varepsilon\\|_{\\infty}^{2}\\,{\\cal C}_{\\bf A}^{2}}{a^{2}\\sqrt{n}}+\\frac{\\kappa_{Q}^{3/2}({\\cal C}_{\\bf A}^{3}\\vee1)\\|\\varepsilon\\|_{\\infty}n^{1/4}\\sqrt{\\log n}}{a^{3/2}}\\exp\\{-\\frac{a}{4}\\sum_{j=1}^{n}\\alpha_{j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now it remains to combine the bounds above in (46). ", "page_idx": 27}, {"type": "text", "text": "Proposition 5. Assume Al, A2, A3 with $\\gamma=1/2$ , and A4. Then, conditionally on the event $\\Omega_{0}$ the following error bound holds: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\mathbb{E}^{\\mathbf{b}}\\left[\\|D^{\\mathbf{b}}(w_{1},\\ldots,w_{2n},Z_{1},\\ldots,Z_{2n})\\|^{2}\\right]\\right\\}^{1/2}\\lesssim\\frac{\\kappa_{Q}^{2}(\\mathbf{C}_{\\mathbf{A}}^{4}\\vee1)\\|\\varepsilon\\|_{\\infty}\\log n}{n^{1/4}a^{5/2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\kappa_{Q}^{3/2}\\bigg(\\frac{(\\mathbf{C}_{\\mathbf{A}}^{3}\\vee1)n^{1/4}}{\\sqrt{a}}+\\frac{(\\mathbf{C}_{\\mathbf{A}}^{5}\\vee1)\\sqrt{\\log n}}{\\sqrt{n}a}\\bigg)\\exp\\bigg\\{\\!-\\!\\frac{c_{0}a\\sqrt{n}}{2}\\!\\bigg\\}\\|\\theta_{0}-\\theta^{\\star}\\|~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\lesssim$ stands for inequality up to an absolute constant. ", "page_idx": 27}, {"type": "text", "text": "Proof of Proposition 5 is provided below in Appendix C.5. The lemma below is a direct counterpart of Lemma 4. ", "page_idx": 27}, {"type": "text", "text": "Proposition 6. Assume A1, A2, A3 with $\\gamma=1/2$ .and A4. Then, conditionally on the event $\\Omega_{0}$ the following error bound holds: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\ }&{\\displaystyle\\sum_{i=n+1}^{2n}\\mathbb{E}^{\\mathbb{b}}\\big[\\|D^{\\mathrm{b}}-D^{(\\mathrm{b},i)}\\|^{2}\\big]^{1/2}\\lesssim\\frac{(\\mathrm{C}_{\\mathrm{A}}^{3}\\vee1)\\|\\varepsilon\\|_{\\infty}}{a^{3/2}}n^{1/4}\\log n+\\frac{\\|\\varepsilon\\|_{\\infty}\\,\\mathrm{C}_{\\mathrm{A}}^{2}}{a^{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\,\\frac{(\\mathrm{C}_{\\mathrm{A}}^{3}\\vee1)n^{3/4}\\sqrt{\\log n}}{a^{3/2}}\\exp\\big\\{-\\frac{a}{4}\\sum_{j=1}^{n}\\alpha_{j}\\big\\}\\|\\theta_{0}-\\theta^{\\star}\\|~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Proposition 6 is provided below in Appendix C.6. ", "page_idx": 27}, {"type": "text", "text": "Lemma 9. For any $k\\geq n$ on the set $\\Omega_{0}$ the following inequality holds: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathbf{b}}[\\|\\theta_{k}^{\\mathbf{b}}-\\theta_{k}\\|^{2}]\\lesssim\\frac{\\alpha_{k}\\|\\varepsilon\\|_{\\infty}^{2}\\,\\mathrm{C}_{\\mathbf{A}}^{2}}{a^{3}}+\\mathrm{C}_{\\mathbf{A}}^{2}\\,k\\prod_{j=1}^{k}(1-a\\alpha_{j}/4)^{2}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. A direct application of Lemma 11 with $L=0$ yields that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathtt{b}}[\\|\\theta_{k}^{\\mathtt{b}}-\\theta_{k}\\|^{2}]\\lesssim\\frac{\\alpha_{k}\\|\\varepsilon\\|_{\\infty}^{2}}{a}\\left(1+\\frac{\\mathrm{C}_{\\mathbf{A}}^{2}}{a^{2}}\\right)+\\mathrm{C}_{\\mathbf{A}}^{2}\\,k\\prod_{j=1}^{k}(1-a\\alpha_{j}/4)^{2}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}\\;.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now to complete the proof it remains to notice that $\\mathrm{C}_{\\mathbf{A}}\\geq a$ ", "page_idx": 27}, {"type": "text", "text": "Lemma 10. For any matrix-valued sequences $(U_{n})_{n\\in\\mathbb{N}}$ $(V_{n})_{n\\in\\mathbb{N}}$ and for any $M\\in\\mathbb{N}$ it holds that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\prod_{k=1}^{M}U_{k}-\\prod_{k=1}^{M}V_{k}=\\sum_{k=1}^{M}\\{\\prod_{j=k+1}^{M}V_{j}\\}(U_{k}-V_{k})\\{\\prod_{j=1}^{k-1}U_{j}\\}\\;.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "C.3  Gaussian comparison inequality ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Theorem 5. Assume A1 and $A2$ .Then on the set $\\Omega_{3}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{B\\in\\mathrm{Conv}(\\ensuremath{\\mathbb{R}}^{d})}|\\mathbb{P}(\\xi\\in B)-\\mathbb{P}^{\\ b}(\\xi^{\\ b}\\in B)|\\le4\\|\\Sigma_{\\varepsilon}^{-1/2}\\varepsilon\\|_{\\infty}\\sqrt{\\frac{d\\log n}{n}}+\\frac{4\\sqrt{d}(1+\\|\\Sigma_{\\varepsilon}^{-1/2}\\varepsilon\\|_{\\infty}^{2})\\log n}{n}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. We will use the following inequality ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Vert\\mathcal{N}(0,\\Sigma_{1})-\\mathcal{N}(0,\\Sigma_{2})\\Vert\\mathfrak{r v}\\leq\\frac{1}{2}\\Vert\\Sigma_{1}^{-1/2}\\Sigma_{2}\\Sigma^{-1/2}-\\mathrm{I}\\Vert\\mathfrak{r r}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Applying (47) we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{B\\in\\mathrm{Conv}(\\mathbb{R}^{d})}|\\mathbb{P}(\\xi\\in B)-\\mathbb{P}^{\\mathtt{b}}(\\xi^{\\mathtt{b}}\\in B)||\\le\\frac{\\sqrt{d}}{2}\\|\\Sigma_{\\varepsilon}^{-1/2}\\Sigma_{\\varepsilon}^{\\mathtt{b}}\\Sigma_{\\varepsilon}^{-1/2}-\\mathrm{I}\\|.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "It remains to apply definition of $\\Omega_{3}$ ", "page_idx": 28}, {"type": "text", "text": "C.4  Auxiliary technical results. ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For the analysis of the difference term $\\theta_{k}^{\\mathsf{b}}-\\theta_{k}$ we use the perturbation expansion technique introduced in [1], see also [16]. Within this approach, we represent the fuctuation component of the error $\\tilde{\\theta}_{n}^{(\\mathsf{f l})}$ defined in (66) as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tilde{\\theta}_{n}^{(\\mathsf{f l})}=J_{n}^{(0)}+H_{n}^{(0)}\\ ,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the latter terms are defined by the following pair of recursions ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{n}^{(0)}=\\left(\\mathbf{I}-\\alpha_{n}\\bar{\\mathbf{A}}\\right)J_{n-1}^{(0)}-\\alpha_{n}\\varepsilon(Z_{n})\\;,}\\\\ &{H_{n}^{(0)}=\\left(\\mathbf{I}-\\alpha_{n}\\mathbf{A}(Z_{n})\\right)H_{n-1}^{(0)}-\\alpha_{n}\\tilde{\\mathbf{A}}(Z_{n})J_{n-1}^{(0)}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{J_{0}^{(0)}=0\\;,}}\\\\ {{H_{0}^{(0)}=0\\;.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Moreover, it is known that for $L\\geq1$ the term $H_{n}^{(0)}$ can be further decomposed as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\nH_{n}^{(0)}=\\sum_{\\ell=1}^{L}J_{n}^{(\\ell)}+H_{n}^{(L)}\\;.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here the terms $J_{n}^{(\\ell)}$ and $H_{n}^{(\\ell)}$ are givenbyth following reures: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{n}^{(\\ell)}=\\left(\\mathrm{I}-\\alpha_{n}\\bar{\\mathbf{A}}\\right)J_{n-1}^{(\\ell)}-\\alpha_{n}\\tilde{\\mathbf{A}}(Z_{n})J_{n-1}^{(\\ell-1)}\\;,\\qquad J_{0}^{(\\ell)}=0\\;,}\\\\ &{H_{n}^{(\\ell)}=\\left(\\mathrm{I}-\\alpha_{n}\\mathbf{A}(Z_{n})\\right)H_{n-1}^{(\\ell)}-\\alpha_{n}\\tilde{\\mathbf{A}}(Z_{n})J_{n-1}^{(\\ell)}\\;,\\quad H_{0}^{(\\ell)}=0\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The expansion depth $L$ here controls the desired approximation accuracy. Informally, one can showthat $\\mathbb{E}^{1/p}[\\|J_{n}^{(\\ell)}\\|^{p}]\\lesssim\\alpha_{n}^{(\\ell+1)/2}$ and silarly $\\overline{{\\mathbb{E}^{1/p}}}[\\|H_{n}^{(\\ell)}\\|^{p}]\\lesssim\\alpha_{n}^{(\\ell+\\bar{1})/2}$ \u2264 \u03b1(+i)/2. Using the outlined expansion, we prove the following lemma: ", "page_idx": 28}, {"type": "text", "text": "Lemma 11. Assume A1, A2, A3 with $\\gamma=1/2$ , and A4. Then for any $k\\geq n$ and $L\\in\\mathbb N$ the following decomposition holds: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\theta_{k}^{\\mathsf{b}}-\\theta_{k}=J_{k}^{\\mathsf{b},0}+\\sum_{j=1}^{L}J_{k}^{\\mathsf{b},j}+H_{k}^{\\mathsf{b},L},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal J}_{k}^{{\\bf b},0}=-\\sum_{\\ell=n+1}^{k}\\alpha_{\\ell}(w_{\\ell}-1)\\Gamma_{\\ell+1:k}\\tilde{\\varepsilon}_{\\ell},}}\\\\ {{\\displaystyle{\\cal J}_{k}^{{\\bf b},j}=-\\sum_{\\ell=n+1}^{k}\\alpha_{\\ell}(w_{\\ell}-1)\\Gamma_{\\ell+1:k}A_{\\ell}{\\cal J}_{\\ell-1}^{{\\bf b},j-1},\\quad j\\in[1,L]}}\\\\ {{\\displaystyle{\\cal H}_{k}^{{\\bf b},L}=-\\sum_{\\ell=n+1}^{k}\\alpha_{\\ell}(w_{\\ell}-1)\\Gamma_{\\ell+1:k}^{{\\bf b}}A_{\\ell}{\\cal J}_{\\ell-1}^{{\\bf b},L}\\;,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and the quantities $\\tilde{\\varepsilon}_{\\ell}$ are defined as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tilde{\\varepsilon}_{\\ell}=\\mathbf{A}_{\\ell}\\big(\\theta_{\\ell-1}-\\theta^{\\star}\\big)+\\varepsilon_{\\ell}\\;.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Moreover, on the event $\\Omega_{0}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathbb{b}}[\\|J_{k}^{\\mathfrak{b},j}\\|^{2}]\\lesssim\\frac{\\alpha_{k}^{j+1}\\|\\varepsilon\\|_{\\infty}^{2}\\,\\mathbb{C}_{\\mathbf{A}}^{2j}}{a^{j+1}}+\\mathbb{C}_{\\mathbf{A}}^{2j+2}\\prod_{j=1}^{k}(1-a\\alpha_{j}/4)^{2}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}\\;,\\quad j\\in[0,L]\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathtt{b}}[\\|H_{k}^{\\mathtt{b},L}\\|^{2}]\\lesssim\\frac{\\alpha_{k}^{L+1}\\,\\mathrm{C}_{\\mathtt{A}}^{2(L+1)}\\,\\|\\varepsilon\\|_{\\infty}^{2}}{a^{L+3}}+\\mathrm{C}_{\\mathbf{A}}^{2(L+1)}\\,k\\prod_{j=1}^{k}(1-a\\alpha_{j}/4)^{2}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. We start from the decomposition ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\theta_{k}^{\\mathrm{b}}-\\theta_{k}=(\\mathrm{I}-\\alpha_{k}w_{k}\\mathbf{A}_{k})(\\theta_{k-1}^{\\mathrm{b}}-\\theta_{k-1})-\\alpha_{k}(w_{k}-1)\\tilde{\\varepsilon}_{k}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Expanding the recurrence above till $k\\,=\\,n$ , and using the fact that $\\theta_{n}^{\\mathsf{b}}=\\theta_{n}$ , we get running the recurrence (54), that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\theta_{k}^{\\mathsf{b}}-\\theta_{k}=-\\sum_{\\ell=n+1}^{k}\\alpha_{\\ell}(w_{\\ell}-1)\\Gamma_{\\ell+1:k}^{\\mathsf{b}}\\tilde{\\varepsilon}_{\\ell}\\;.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence, proceeding as in (48), we obtain the representation ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{J_{k}^{(\\mathbf{b},0)}=\\left(\\mathrm{I}-\\alpha_{k}\\mathbf{A}_{k}\\right)J_{k-1}^{(\\mathbf{b},0)}-\\alpha_{k}(w_{k}-1)\\tilde{\\varepsilon}_{k}\\;,}&&{\\;\\;\\;J_{0}^{(\\mathbf{b},0)}=0\\;,}\\\\ &{H_{k}^{(\\mathbf{b},0)}=\\left(\\mathrm{I}-\\alpha_{k}w_{k}\\mathbf{A}_{k}\\right)H_{k-1}^{(\\mathbf{b},0)}-\\alpha_{k}(w_{k}-1)\\mathbf{A}_{k}J_{k-1}^{(\\mathbf{b},0)}\\;,}&&{\\;\\;\\;\\;H_{0}^{(\\mathbf{b},0)}=0\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Itis easyto check that $J_{k}^{(\\mathsf{b},0)}+H_{k}^{(\\mathsf{b},0)}=\\theta_{k}^{\\mathsf{b}}-\\theta_{k}$ $H_{k}^{({\\mathsf{b}},0)}$ along the lines of (49), we arrive at the decomposition (50). Since $w_{k}$ for $k=n+1,\\ldots,2n$ are i.i.d., we get using the definition of the events $\\Omega_{1}$ and $\\Omega_{2}$ , that on the event $\\Omega_{0}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\tilde{\\varepsilon}_{\\ell}\\|^{2}\\lesssim\\|\\varepsilon\\|_{\\infty}^{2}+\\mathbf{C}_{\\mathbf{A}}^{2}\\exp\\bigl\\{-a\\displaystyle\\sum_{j=1}^{\\ell-1}\\alpha_{j}\\bigr\\}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}+\\frac{\\alpha_{\\ell}\\|\\varepsilon\\|_{\\infty}^{2}\\log^{2}n}{a}}\\\\ &{\\qquad\\lesssim\\|\\varepsilon\\|_{\\infty}^{2}+\\mathbf{C}_{\\mathbf{A}}^{2}\\displaystyle\\prod_{j=1}^{\\ell-1}(1-a\\alpha_{j}/2)^{2}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}+\\frac{\\alpha_{\\ell}\\|\\varepsilon\\|_{\\infty}^{2}\\log^{2}n}{a}}\\\\ &{\\qquad\\lesssim\\|\\varepsilon\\|_{\\infty}^{2}+\\mathbf{C}_{\\mathbf{A}}^{2}\\displaystyle\\prod_{j=1}^{\\ell-1}(1-a\\alpha_{j}/2)^{2}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where for the last bound we have additionally used that $\\alpha_{\\ell}\\log^{2}n/a\\leq1$ for $\\ell\\geq n$ . The latter bound is guarantdbyHe, using thboud5gethewith tedeninf $J_{k}^{\\mathsf{b},0}$ we obtain that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}^{\\mathbb{B}}[\\|J_{k}^{\\mathbb{B},\\mathbb{B}}\\|^{2}]=\\displaystyle\\sum_{\\ell=m+1}^{k}\\alpha_{\\ell}^{2}\\|\\Gamma_{\\ell+1;k}\\xi_{\\ell}\\|^{2}=\\displaystyle\\sum_{\\ell=m+1}^{k}\\alpha_{\\ell}^{2}\\|\\Gamma_{\\ell+1;k}\\big(\\mathbf{A}_{\\ell}(\\theta_{\\ell-1}-\\theta^{*})+\\varepsilon_{\\ell}\\big)\\|^{2}}\\\\ {\\lesssim\\|\\varepsilon\\|_{\\infty}^{2}\\displaystyle\\sum_{\\ell=m+1}^{k}\\alpha_{\\ell}^{2}\\displaystyle\\prod_{j=\\ell+1}^{k}(1-a\\alpha_{j}/4)^{2}+\\displaystyle C_{\\lambda}^{2}\\displaystyle\\sum_{\\ell=m+1}^{k}\\alpha_{\\ell}^{2}\\displaystyle\\prod_{j=1}^{k}(1-a\\alpha_{j}/4)^{2}\\|\\theta_{0}-\\theta^{*}\\|^{2}}\\\\ {+\\displaystyle\\frac{\\|\\varepsilon\\|_{\\infty}^{2}\\big(\\mathbb{C}_{\\lambda}^{2}\\log^{2}n\\big)}{a}\\displaystyle\\sum_{\\ell=m+1}^{k}\\alpha_{\\ell}^{3}\\displaystyle\\prod_{j=\\ell+1}^{k}(1-a\\alpha_{j}/4)^{2}}\\\\ {\\lesssim\\frac{\\|\\varepsilon\\|_{\\infty}^{2}\\alpha_{k}}{a}+\\mathbb{C}_{\\lambda}^{2}\\log\\left(\\frac{k}{n}\\right)\\displaystyle\\prod_{j=1}^{k}(1-a\\alpha_{j}/4)^{2}\\|\\theta_{0}-\\theta^{*}\\|^{2}+\\frac{\\alpha_{\\ell}^{2}\\|\\varepsilon\\|_{\\infty}^{2}\\big(\\mathbb{C}_{\\lambda}^{2}\\log^{2}n\\big)}{a^{2}}}\\\\ {\\lesssim\\frac{\\|\\varepsilon\\|_{\\infty}^{2}\\alpha_{k}}{a}+\\mathbb{C}_{\\lambda}^{2}\\displaystyle\\prod_{j=1}^{k}(1-a\\alpha_{j}/4)^{2}\\|\\theta_{0}-\\theta^{*}\\|^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we additionally used the fact that $k\\in[n;2n]$ and $n$ satisfies A4. Assume now that the bound onJk Jb,i-1 has a form ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathtt{b}}[\\|J_{k}^{\\mathtt{b},j-1}\\|^{2}]\\lesssim\\frac{\\|\\varepsilon\\|_{\\infty}^{2}\\alpha_{k}^{j}}{a^{j}}+\\mathrm{C}_{\\mathtt{A}}^{2j}\\prod_{\\ell=1}^{k}(1-a\\alpha_{\\ell}/4)^{2}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}\\;.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, using the martingale property of $J_{k}^{\\mathsf{b},j}$ , we write that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathfrak{L}^{\\mathtt{b}}[\\|J_{k}^{\\mathtt{b},j}\\|^{2}]=\\sum_{\\ell=n+1}^{k}\\alpha_{\\ell}^{2}\\mathbb{E}^{\\mathtt{b}}[\\|\\Gamma_{\\ell+1:k}A_{\\ell}J_{\\ell-1}^{\\mathtt{b},j-1}\\|^{2}]}\\\\ {\\displaystyle\\lesssim\\sum_{\\ell=n+1}^{k}\\frac{\\alpha_{\\ell}^{j+2}\\|\\varepsilon\\|_{\\infty}^{2}\\,\\ C_{\\mathtt{A}}^{2j}}{a^{j}}\\prod_{j=\\ell+1}^{k}\\big(1-a\\alpha_{j}/4\\big)^{2}+\\big(\\displaystyle\\sum_{k}^{2j+2}\\sum_{\\ell=n+1}^{k}\\alpha_{\\ell}^{2}\\prod_{j=1}^{k}(1-a\\alpha_{j}/4)^{2}\\big\\|\\theta_{0}-\\ell\\big)}\\\\ {\\displaystyle\\lesssim\\frac{\\alpha_{k}^{j+1}\\|\\varepsilon\\|_{\\infty}^{2}\\,\\ C_{\\mathtt{A}}^{2j}}{a^{j+1}}+\\mathrm{C}_{\\mathtt{A}}^{2j+2}\\prod_{j=1}^{k}(1-a\\alpha_{j}/4)^{2}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and thus the bound (52) is proved. Moreover, using (51) and Minkowski's inequality, we obtain that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathbb{E}^{\\mathbb{b}}[\\|H_{k}^{\\mathbb{b},L}\\|^{2}])^{1/2}\\le\\mathrm{C}_{\\mathbf{A}}\\displaystyle\\sum_{\\ell=n+1}^{k}\\alpha_{\\ell}(\\mathbb{E}^{\\mathbb{b}}[\\|\\Gamma_{\\ell+1:k}^{\\mathbb{b}}\\|^{2}])^{1/2}(\\mathbb{E}^{\\mathbb{b}}[\\|J_{\\ell-1}^{\\mathbb{b},L}\\|^{2}])^{1/2}}\\\\ &{\\qquad\\qquad\\lesssim\\mathrm{C}_{\\mathbf{A}}\\displaystyle\\sum_{\\ell=n+1}^{k}\\frac{\\alpha_{\\ell}^{(L+3)/2}\\|\\ell\\|_{\\infty}\\,\\mathrm{C}_{\\mathbf{A}}^{L}}{a^{(L+1)/2}}\\displaystyle\\prod_{j=\\ell+1}^{k}\\,(1-a\\alpha_{j}/4)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\mathrm{C}_{\\mathbf{A}}^{L+1}\\displaystyle\\sum_{\\ell=n+1}^{k}\\alpha_{\\ell}\\displaystyle\\prod_{j=1}^{k}(1-a\\alpha_{j}/4)^{2}\\|\\theta_{0}-\\theta^{*}\\|}\\\\ &{\\qquad\\qquad\\lesssim\\frac{\\alpha_{k}^{(L+1)/2}\\,\\mathrm{C}_{\\mathbf{A}}^{L+1}\\,\\|\\xi\\|_{\\infty}}{a^{(L+3)/2}}+\\mathrm{C}_{\\mathbf{A}}^{L+1}\\sqrt{k}\\displaystyle\\prod_{j=1}^{k}(1-a\\alpha_{j}/4)^{2}\\|\\theta_{0}-\\theta^{*}\\|\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and (53) follows. ", "page_idx": 30}, {"type": "text", "text": "C.5 Proof of Proposition 5 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Recall that the quantity $D^{\\mathsf{b}}$ is defined in (45). Since $\\theta_{n}^{\\mathsf{b}}=\\theta_{n}$ ,weconcludethat $D_{1}^{\\mathsf{b}}=0$ Toestimate other terms we will use the main error decomposition outlined in Lemma 11, that is, the expansion ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\theta_{k}^{\\mathsf{b}}-\\theta_{k}=\\sum_{\\ell=0}^{L}J_{k}^{\\mathsf{b},\\ell}+H_{k}^{\\mathsf{b},L},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "applied with different $L\\geq0$ . To bound ${D}_{2}^{\\mathsf{b}}$ we take $L=0$ and obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{E}[\\|D_{2}^{\\mathtt{b}}\\|^{2}]\\lesssim\\mathbb{E}^{\\mathtt{b}}[\\|J_{2n}^{\\mathtt{b},j}\\|^{2}]+\\mathbb{E}^{\\mathtt{b}}[\\|H_{2n}^{\\mathtt{b},L}\\|^{2}]\\lesssim\\frac{\\|\\varepsilon\\|_{\\infty}^{2}}{n\\alpha_{2n}a}\\left(1+\\frac{\\mathbf{C}_{\\mathtt{A}}^{2}}{a^{2}}\\right)+\\frac{\\mathbf{C}_{\\mathtt{A}}^{2}}{\\alpha_{2n}^{2}}\\prod_{j=1}^{n}(1-a\\alpha_{j}/4)^{2}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}}\\\\ &{\\quad\\quad\\quad\\lesssim\\frac{\\|\\varepsilon\\|_{\\infty}^{2}}{a\\sqrt{n}}\\left(1+\\frac{\\mathbf{C}_{\\mathtt{A}}^{2}}{a^{2}}\\right)+n\\,\\mathrm{C}_{\\mathtt{A}}^{2}\\prod_{j=1}^{2n}(1-a\\alpha_{j}/4)^{2}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "To estimate ${\\cal D}_{3}^{\\flat}$ we note that ", "page_idx": 30}, {"type": "equation", "text": "$$\nD_{3}^{\\mathsf{b}}=\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}(w_{k}-1)\\mathbf{A}_{k}(\\theta_{k-1}^{\\mathsf{b}}-\\theta^{\\star})=D_{3,1}^{\\mathsf{b}}+D_{3,2}^{\\mathsf{b}}\\;,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we have set, respectively, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{D_{3,1}^{\\mathbf{b}}=\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}(w_{k}-1)\\mathbf{A}_{k}\\big(\\theta_{k-1}^{\\mathbf{b}}-\\theta_{k-1}\\big)},}}\\\\ {{\\displaystyle{D_{3,2}^{\\mathbf{b}}=\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}\\big(w_{k}-1\\big)\\mathbf{A}_{k}\\big(\\theta_{k-1}-\\theta^{\\star}\\big)}~.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "It follows from Lemma 9 that on the event $\\Omega_{0}$ it holds ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}^{\\mathbb{b}}[\\|D_{3,1}^{\\mathbb{b}}\\|^{2}]\\leq\\displaystyle\\frac{\\mathbf{C}_{\\mathbf{A}}^{2}}{n}\\sum_{k=n+1}^{2n}\\mathbb{E}^{\\mathbb{b}}[\\|\\theta_{k-1}^{\\mathbb{b}}-\\theta_{k-1}\\|^{2}]}\\\\ {\\lesssim\\displaystyle\\frac{\\mathbf{C}_{\\mathbf{A}}^{2}}{n}\\sum_{k=n+1}^{2n}\\frac{\\alpha_{k}\\|\\varepsilon\\|_{\\infty}^{2}}{a}\\left(1+\\frac{\\mathbf{C}_{\\mathbf{A}}^{2}}{a^{2}}\\right)+\\mathrm{C_{A}^{4}}\\sum_{k=n+1}^{2n}\\prod_{j=1}^{k}(1-a\\alpha_{j}/4)^{2}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}}\\\\ {\\lesssim\\frac{\\mathbf{C}_{\\mathbf{A}}^{2}\\|\\varepsilon\\|_{\\infty}^{2}}{a\\sqrt{n}}\\left(1+\\frac{\\mathbf{C}_{\\mathbf{A}}^{2}}{a^{2}}\\right)+\\frac{\\mathbf{C}_{\\mathbf{A}}^{4}\\sqrt{n}}{a}\\exp\\biggl\\{-c_{0}a\\sqrt{n}\\biggr\\}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Moreover, on the set $\\Omega_{0}$ it holds (since $\\Omega_{0}\\subseteq\\Omega_{1}$ 0, that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}^{\\mathbb{P}}[\\|D_{3,2}^{\\mathbb{b}}\\|^{2}]=\\displaystyle\\frac{1}{n}\\sum_{k=n+1}^{2n}\\|\\mathbf{A}_{k}(\\theta_{k-1}-\\theta^{*})\\|^{2}}\\\\ {\\lesssim\\frac{\\mathbb{C}_{\\alpha}^{2}}{n}\\sum_{k=n+1}^{2n}\\left(\\exp\\{-a\\displaystyle\\sum_{\\ell=1}^{k}\\alpha_{\\ell}\\}\\|\\theta_{0}-\\theta^{*}\\|^{2}+\\frac{\\alpha_{k}\\|\\varepsilon\\|_{\\infty}^{2}\\log^{2}n}{a}\\right)}\\\\ {\\lesssim\\displaystyle\\frac{\\mathbb{C}_{\\alpha}^{2}}{n a\\alpha_{2n}}\\exp\\{-a\\displaystyle\\sum_{\\ell=1}^{n}\\alpha_{\\ell}\\}\\|\\theta_{0}-\\theta^{*}\\|^{2}+\\frac{\\mathbb{C}_{\\alpha}^{2}\\|\\varepsilon\\|_{\\infty}^{2}\\log^{2}n}{n a}\\sum_{k=n+1}^{2n}\\alpha_{k}}\\\\ {\\lesssim\\frac{\\mathbb{C}_{\\alpha}^{2}}{a\\sqrt{n}}\\exp\\Biggl\\{-c_{0}a\\sqrt{n}\\Biggr\\}\\|\\theta_{0}-\\theta^{*}\\|^{2}+\\frac{\\mathbb{C}_{\\alpha}^{2}\\|\\varepsilon\\|_{\\infty}^{2}\\log^{2}n}{a\\sqrt{n}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Combining the above bounds, we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\mathbf{b}}[\\|D_{3}^{\\mathbf{b}}\\|^{2}]\\lesssim\\mathbb{E}^{\\mathbf{b}}[\\|D_{3,1}^{\\mathbf{b}}\\|^{2}]+\\mathbb{E}^{\\mathbf{b}}[\\|D_{3,2}^{\\mathbf{b}}\\|^{2}]}\\\\ &{\\qquad\\qquad\\lesssim\\frac{\\mathbf{C}_{\\mathbf{A}}^{2}\\,\\|\\varepsilon\\|_{\\infty}^{2}\\log^{2}n}{a\\sqrt{n}}\\left(1+\\frac{\\mathbf{C}_{\\mathbf{A}}^{2}}{a^{2}}\\right)+\\frac{\\mathbf{C}_{\\mathbf{A}}^{4}\\,\\sqrt{n}}{a}\\exp\\biggl\\{-c_{0}a\\sqrt{n}\\biggr\\}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now we proceed with the term ${D}_{4}^{\\mathsf{b}}$ . Applying Minkowski's inequality, we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}^{\\mathbb{b}}[\\|D_{4}^{\\mathbb{b}}\\|^{2}]\\}^{1/2}\\leq\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}\\bigg(\\frac{1}{\\alpha_{k}}-\\frac{1}{\\alpha_{k-1}}\\bigg)\\,\\big\\{\\mathbb{E}^{\\mathbb{b}}[\\|\\theta_{k-1}^{\\mathbb{b}}-\\theta_{k-1}\\|^{2}]\\big\\}^{1/2}}\\\\ &{\\lesssim\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}\\frac{1}{\\sqrt{k}}\\left(\\frac{\\sqrt{\\alpha_{k}}\\|\\varepsilon\\|_{\\infty}}{\\sqrt{4}}\\left(1+\\frac{\\mathbf{C}_{\\mathbf{A}}}{a}\\right)+\\mathbf{C}_{\\mathbf{A}}\\,\\sqrt{k}\\prod_{j=1}^{k}(1-a\\alpha_{j}/4)\\|\\theta_{0}-\\theta^{*}\\|\\right)}\\\\ &{\\lesssim\\frac{\\|\\varepsilon\\|_{\\infty}}{n^{1/4}\\sqrt{a}}\\,\\bigg(1+\\frac{\\mathbf{C}_{\\mathbf{A}}}{a}\\bigg)+\\frac{\\mathbf{C}_{\\mathbf{A}}}{\\sqrt{n}}\\sum_{k=n+1}^{2n}\\prod_{j=1}^{k}(1-a\\alpha_{j}/4)\\|\\theta_{0}-\\theta^{*}\\|}\\\\ &{\\lesssim\\frac{\\|\\varepsilon\\|_{\\infty}}{n^{1/4}\\sqrt{a}}\\,\\bigg(1+\\frac{\\mathbf{C}_{\\mathbf{A}}}{a}\\bigg)+\\mathbf{C}_{\\mathbf{A}}\\,\\exp\\bigg\\{-\\frac{c_{0}a\\sqrt{n}}{2}\\bigg\\}\\|\\theta_{0}-\\theta^{*}\\|\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "It remains to upper bound the term $D_{5}^{\\mathsf{b}}$ . Using the decomposition, suggested by Lemma 11 with $L=2$ , we get that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{5}^{\\mathrm{b}}=\\displaystyle\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}\\big(\\mathbf{A}_{k}-\\bar{\\mathbf{A}}\\big)\\big(\\theta_{k-1}^{\\mathrm{b}}-\\theta_{k-1}\\big)=\\underbrace{\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}\\big(\\mathbf{A}_{k}-\\bar{\\mathbf{A}}\\big)J_{k-1}^{\\mathrm{b},0}}_{D_{5,1}^{\\mathrm{b}}}}\\\\ &{\\qquad+\\underbrace{\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}\\big(\\mathbf{A}_{k}-\\bar{\\mathbf{A}}\\big)J_{k-1}^{\\mathrm{b},1}}_{D_{5,2}^{\\mathrm{b}}}+\\underbrace{\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}\\big(\\mathbf{A}_{k}-\\bar{\\mathbf{A}}\\big)J_{k-1}^{\\mathrm{b},2}}_{D_{5,3}^{\\mathrm{b}}}+\\underbrace{\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}\\big(\\mathbf{A}_{k}-\\bar{\\mathbf{A}}\\big)H_{k-1}^{\\mathrm{b},2}}_{D_{5,4}^{\\mathrm{b}}}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Here we have to consider expansion until $H^{\\mathsf{b,2}}$ , since dealing with the latter term (outlined as $D_{5,4}^{\\mathsf{b}}$ in the above expansion) is possible only with Minkowski's inequality. Now we consider the summands $D_{5,1}^{\\mathsf{b}}-D_{5,4}^{\\mathsf{b}}$ separately. Consider first the term $D_{5,1}^{\\mathsf{b}}$ . Changing the summation order, we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal D}_{5,1}^{\\mathrm{b}}=-\\frac{1}{\\sqrt{n}}\\sum_{k=n+1}^{2n}({\\bf A}_{k}-\\bar{\\bf A})\\sum_{\\ell=n+1}^{k-1}\\alpha_{\\ell}(w_{\\ell}-1)\\Gamma_{\\ell+1:k-1}\\tilde{\\varepsilon}_{\\ell}}}\\\\ {{\\displaystyle~~~=-\\frac{1}{\\sqrt{n}}\\sum_{\\ell=n+1}^{2n-1}\\alpha_{\\ell}(w_{\\ell}-1)\\biggr(\\sum_{k=\\ell+1}^{2n}({\\bf A}_{k}-\\bar{\\bf A})\\Gamma_{\\ell+1:k-1}\\biggr)\\tilde{\\varepsilon}_{\\ell}\\;.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then on the event $\\Omega_{0}$ we get, since $\\Omega_{0}\\subseteq\\Omega_{4}$ , and using that $n$ satisfies A4, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\sum_{k=\\ell+1}^{2n}(\\mathbf{A}_{k}-\\bar{\\mathbf{A}})\\Gamma_{\\ell+1:k-1}\\|^{2}\\lesssim\\frac{\\log n}{a\\alpha_{\\ell}}\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Combining the above bound together with the one provided by (55), we obtain that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathbb{b}}[\\|D_{5,1}^{\\mathbb{b}}\\|^{2}]\\lesssim\\frac{\\|\\varepsilon\\|_{\\infty}^{2}}{n}\\sum_{\\ell=n+1}^{2n-1}\\frac{\\alpha_{\\ell}\\log n}{a}+\\frac{\\mathbf{C}_{\\mathbf{A}}^{2}}{n}\\sum_{\\ell=n+1}^{2n-1}\\frac{\\alpha_{\\ell}\\log n}{a}\\prod_{j=1}^{\\ell-1}(1-a\\alpha_{j}/2)^{2}\\|\\theta_{0}-\\theta^{*}\\|^{2}}\\\\ {\\lesssim\\frac{\\|\\varepsilon\\|_{\\infty}^{2}\\log n}{\\sqrt{n}a}+\\frac{\\mathbf{C}_{\\mathbf{A}}^{2}\\log n}{a^{2}n}\\exp\\bigl\\{-a\\sum_{j=1}^{n}\\alpha_{j}\\bigr\\}\\|\\theta_{0}-\\theta^{*}\\|^{2}\\;.\\qquad\\qquad\\qquad\\qquad\\qquad}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Similarly, for the term $D_{5,2}^{\\flat}$ we get, changing the order of summation,that ", "page_idx": 32}, {"type": "equation", "text": "$$\nD_{5,2}^{\\mathbf{b}}=\\frac{1}{\\sqrt{n}}\\sum_{\\ell=n+1}^{2n-1}\\alpha_{\\ell}(w_{\\ell}-1)\\bigg(\\sum_{k=\\ell+1}^{2n}(\\mathbf{A}_{k}-\\bar{\\mathbf{A}})\\Gamma_{\\ell+1:k-1}\\bigg)A_{\\ell}J_{\\ell-1}^{\\mathbf{b},0}\\ .\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Hence, using the bound (56) together with (52), we get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\mathbb{P}}[\\|D_{5,2}^{\\mathbb{b}}\\|^{2}]\\lesssim\\displaystyle\\frac{1}{n}\\sum_{\\ell=n+1}^{2n-1}\\frac{\\alpha_{\\ell}\\log n}{a}\\,\\mathrm{C}_{\\mathrm{A}}^{2}\\Bigg(\\frac{\\alpha_{\\ell}\\|\\varepsilon\\|_{\\infty}^{2}}{a}+\\mathrm{C}_{\\mathrm{A}}^{2}\\displaystyle\\prod_{j=1}^{\\ell-1}(1-a\\alpha_{j}/4)^{2}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}\\Bigg)}\\\\ &{\\qquad\\qquad\\lesssim\\frac{\\mathrm{C}_{\\mathrm{A}}^{2}\\,\\|\\varepsilon\\|_{\\infty}^{2}\\log n}{n a^{2}}\\sum_{\\ell=n+1}^{2n-1}\\alpha_{\\ell}^{2}+\\frac{\\mathrm{C}_{\\mathrm{A}}^{4}\\log n}{n a}\\sum_{\\ell=n+1}^{2n-1}\\alpha_{\\ell}\\displaystyle\\prod_{j=1}^{\\ell-1}(1-a\\alpha_{j}/4)^{2}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}}\\\\ &{\\qquad\\lesssim\\frac{\\mathrm{C}_{\\mathrm{A}}^{2}\\,\\|\\varepsilon\\|_{\\infty}^{2}\\log n}{n a^{2}}+\\frac{\\mathrm{C}_{\\mathrm{A}}^{4}\\log n}{n a}\\exp\\{-(a/2)\\displaystyle\\sum_{j=1}^{n}\\alpha_{j}\\}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We proceed with $D_{5,3}^{\\flat}$ .We change the summation order and proced exactlyas with $D_{5,2}^{\\flat}$ . Indeed, ", "page_idx": 32}, {"type": "equation", "text": "$$\nD_{5,3}^{\\mathbf{b}}=\\frac{1}{\\sqrt{n}}\\sum_{\\ell=n+1}^{2n-1}\\alpha_{\\ell}(w_{\\ell}-1)\\bigg(\\sum_{k=\\ell+1}^{2n}({\\bf A}_{k}-\\bar{\\bf A})\\Gamma_{\\ell+1:k-1}\\bigg)A_{\\ell}J_{\\ell-1}^{\\mathbf{b},1}\\ ,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathbb{b}}[\\|D_{5,3}^{\\mathbb{b}}\\|^{2}]\\lesssim\\frac{1}{n}\\sum_{\\ell=n+1}^{2n-1}\\frac{\\alpha_{\\ell}\\log n}{a}\\,\\mathrm{C}_{\\mathbf{A}}^{2}\\bigg(\\frac{\\alpha_{\\ell}^{2}\\|\\varepsilon\\|_{\\infty}^{2}\\,\\mathrm{C}_{\\mathbf{A}}^{2}}{a^{2}}+\\mathrm{C}_{\\mathbf{A}}^{4}\\prod_{j=1}^{\\ell-1}(1-a\\alpha_{j}/4)^{2}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}\\bigg)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "It remains to upper bound $D_{5,4}^{\\flat}$ . Proceeding as above, we change the summation order, and obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\nD_{5,4}^{\\mathrm{b}}=\\frac{1}{\\sqrt{n}}\\sum_{\\ell=n+1}^{2n-1}\\alpha_{\\ell}(w_{\\ell}-1)\\bigg(\\sum_{k=\\ell+1}^{2n}(\\mathbf{A}_{k}-\\bar{\\mathbf{A}})\\Gamma_{\\ell+1:k-1}^{\\mathrm{b}}\\bigg)A_{\\ell}J_{\\ell-1}^{\\mathrm{b},2}\\ .\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Applying Minkowski's inequality, we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(\\mathbb{E}^{\\mathbb{B}}[\\|D_{s,\\pm}^{\\mathbb{B}}]\\|^{2})^{1/2}\\lesssim\\frac{C_{\\hbar}^{2}}{\\sqrt{n}}\\sum_{m=+1}^{2n-1}\\alpha_{\\ell}\\sum_{r=1}^{2n}(\\mathbb{E}^{\\mathbb{B}}[\\|\\Gamma_{t+1,k-1}^{\\mathbb{B}}\\|^{2}])^{1/2}(\\mathbb{E}^{\\mathbb{B}}[\\|J_{t-1}^{\\mathbb{B},2}]^{2})^{1/2}}\\\\ &{\\lesssim\\frac{C_{\\hbar}^{2}}{\\sqrt{n}}\\frac{2n-1}{\\ell-m+1}\\,\\,\\frac{k\\varepsilon^{2}}{k\\ell}\\sum_{k=\\ell+1}^{2n}\\exp\\{-\\frac{\\alpha}{4}\\sum_{j=\\ell+1}^{k-1}\\alpha_{j}\\}\\frac{\\|\\varepsilon\\|_{\\infty}\\,C_{\\hbar}^{2}}{\\alpha^{3/2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\frac{C_{\\hbar}^{2}}{\\sqrt{n}}\\sum_{\\ell=m+1}^{2n-1}\\alpha_{\\ell}\\sum_{r=\\ell+1}^{2n-1}\\exp\\{-\\frac{\\alpha}{4}\\sum_{j=1}^{k-1}\\alpha_{j}\\}\\|\\theta_{0}-\\theta^{*}\\|}\\\\ &{\\lesssim\\frac{C_{\\hbar}^{4}}{\\sqrt{n}\\ell^{3}/2}\\frac{2n-1}{\\ell-m+1}\\alpha_{\\ell}^{3/2}+\\frac{C_{\\hbar}^{3}}{\\sqrt{n}\\alpha}\\exp\\{-\\frac{\\alpha}{4}\\sum_{j=1}^{m}\\alpha_{j}\\}\\|\\theta_{0}-\\theta^{*}\\|}\\\\ &{\\lesssim\\frac{C_{\\hbar}^{4}}{n^{1/4}\\alpha^{5/2}}\\frac{\\alpha_{\\ell}^{5}}{\\sqrt{n}\\alpha^{3}}\\frac{\\alpha_{\\ell}^{5}}{\\sqrt{n}\\alpha^{9}}\\{-\\frac{\\alpha}{4}\\sum_{j=1}^{n}\\alpha_{j}\\}\\|\\theta_{0}-\\theta^{*}\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now the result follows from the representation (45) and combinations of the above bounds for $D_{1}^{\\mathsf{b}}-$ ${D}_{5}^{\\mathsf{b}}$ ", "page_idx": 33}, {"type": "text", "text": "C.6  Proof of Proposition 6 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Consider the sequences of weights ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left(w_{1},\\ldots,w_{i-1},w_{i},w_{i+1},\\ldots,w_{2n}\\right)\\,\\mathrm{and}\\,\\left(w_{1},\\ldots,w_{i-1},w_{i}^{\\prime},w_{i+1},\\ldots,w_{2n}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which differs only in position $i$ $n+1\\leq i\\leq2n$ , with $w_{i}^{\\prime}$ being an independent copy of $w_{i}$ . Consider the associated SA processes ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{k}^{\\mathsf{b}}=\\theta_{k-1}^{\\mathsf{b}}-\\alpha_{k}w_{k}\\{\\mathbf{A}(Z_{k})\\theta_{k-1}-\\mathbf{b}(Z_{k})\\}\\;,\\quad k\\geq n+1,\\quad\\theta_{n}^{\\mathsf{b}}=\\theta_{n}\\in\\mathbb{R}^{d}}\\\\ &{\\theta_{k}^{(\\mathsf{b},i)}=\\theta_{k-1}^{(\\mathsf{b},i)}-\\alpha_{k}w_{k}^{(i)}\\{\\mathbf{A}(Z_{k})\\theta_{k-1}^{(\\mathsf{b},i)}-\\mathbf{b}(Z_{k})\\}\\;,\\quad k\\geq n+1\\;,\\quad\\theta_{n}^{(\\mathsf{b},i)}=\\theta_{n}\\in\\mathbb{R}^{d}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $w_{k}^{(i)}=w_{k}$ for $k\\neq i$ and $w_{i}^{(i)}=w_{i}^{\\prime}$ Respectiverandom variables $D^{\\mathsf{b}}$ and $D^{(\\mathsf{b},i)}$ arebased on the first and second sequences from (57), respectively, and are constructed according to the equation (44). From the above representations we eaily observe that $\\theta_{k}^{\\mathsf{b}}=\\theta_{k}^{(\\mathsf{b},i)}$ for $k<i$ moreover, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{i}^{\\mathbf{b}}-\\theta_{i}^{(\\mathbf{b},i)}=-\\alpha_{i}(w_{i}-w_{i}^{\\prime})\\bigl\\{\\mathbf{A}(Z_{i})\\bigr)\\theta_{i-1}^{\\mathbf{b}}-\\mathbf{b}(Z_{i})\\bigr\\}}\\\\ &{\\qquad\\qquad=-\\alpha_{i}(w_{i}-w_{i}^{\\prime})\\bigl\\{\\mathbf{A}(Z_{i})\\bigr)(\\theta_{i-1}^{\\mathbf{b}}-\\theta_{i-1})-\\mathbf{b}(Z_{i})\\bigr\\}}\\\\ &{\\qquad\\qquad=-\\alpha_{i}(w_{i}-w_{i}^{\\prime})\\bigl\\{\\mathbf{A}(Z_{i})\\bigr)(\\theta_{i-1}^{\\mathbf{b}}-\\theta_{i-1})+\\tilde{\\varepsilon}_{i}\\bigr\\}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\varepsilon_{i}=\\varepsilon(Z_{i})$ and $\\varepsilon_{i}^{\\prime}=\\varepsilon(Z_{i}^{\\prime})$ . From the above representation we get, applying Lemma 9 and (55), that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\mathbb{b}}[\\|\\theta_{i}^{\\tt b}-\\theta_{i}^{(\\tt b}_{i}^{(\\tt b})\\|^{2}]]^{1/2}\\lesssim\\alpha_{i}\\,\\mathrm{C}_{\\bf A}\\{\\mathbb{E}^{\\mathbb{b}}[\\|\\theta_{i}^{\\tt b}-\\theta_{i}\\|^{2}]\\}^{1/2}+\\alpha_{i}\\,\\mathrm{C}_{\\bf A}\\,\\|\\tilde{\\varepsilon}_{i}\\|}\\\\ &{\\lesssim\\alpha_{i}\\,\\mathrm{C}_{\\bf A}\\,\\|\\varepsilon\\|_{\\infty}+\\frac{\\alpha_{i}^{3/2}\\|\\varepsilon\\|_{\\infty}\\,\\mathrm{C}_{\\bf A}^{2}}{a^{3/2}}+(\\mathrm{C}_{\\bf A}^{2}\\,\\vee\\!1)\\sqrt{i}\\displaystyle\\prod_{j=1}^{i}(1-a\\alpha_{j}/4)\\|\\theta_{0}-\\theta^{\\star}\\|}\\\\ &{\\lesssim\\frac{\\alpha_{i}\\|\\varepsilon\\|_{\\infty}\\,\\mathrm{C}_{\\bf A}^{2}}{a}+(\\mathrm{C}_{\\bf A}^{2}\\,\\vee\\!1)\\sqrt{i}\\displaystyle\\prod_{j=1}^{i}(1-a\\alpha_{j}/4)\\|\\theta_{0}-\\theta^{\\star}\\|~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where for the last line we have additionally assumed that $\\alpha_{i}\\lesssim a$ for $i\\geq n$ . Moreover, for any $j>i$ one observes, expanding (58), that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\theta_{j}^{\\mathbf{b}}-\\theta_{j}^{\\left(\\mathbf{b},i\\right)}=\\left\\{\\prod_{k=i+1}^{j}(\\mathrm{I}-\\alpha_{k}w_{k}\\mathbf{A}(Z_{k}))\\right\\}(\\theta_{i}^{\\mathbf{b}}-\\theta_{i}^{\\left(\\mathbf{b},i\\right)})=\\Gamma_{i+1:j}^{\\mathbf{b}}\\left(\\theta_{i}^{\\mathbf{b}}-\\theta_{i}^{\\left(\\mathbf{b},i\\right)}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, similarly to (37), we obtain that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\{{\\mathbb E}^{{\\bf b}}[\\|D^{{\\bf b}}-D^{({\\bf b},i)}\\|^{2}]\\}^{1/2}\\le\\sum_{j=1}^{5}\\{{\\mathbb E}^{{\\bf b}}[\\|D_{j}^{{\\bf b}}-D_{j}^{({\\bf b},i)}\\|^{2}]\\}^{1/2}\\;,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and bound the respective differences separately. By the construction of the process above, we note that $D_{1}^{\\mathsf{b}}=D_{1}^{({\\mathsf{b}},i)}$ . Proceeding futher, and using the equation (44), we obtain that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\mathbb{b}}[\\|D_{2}^{\\mathbb{b}}-D_{2}^{(\\mathbb{b},i)}\\|^{2}]\\}^{1/2}=\\displaystyle\\frac{1}{\\sqrt{n}\\alpha_{2n}}\\Big\\{\\mathbb{E}^{\\mathbb{b}}[\\|\\theta_{2n}^{\\mathbb{b}}-\\theta_{2n}^{(\\mathbb{b},i)}\\|^{2}]\\Big\\}^{1/2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{1}{\\sqrt{n}\\alpha_{2n}}\\Big\\{\\mathbb{E}^{\\mathbb{b}}[\\|\\Gamma_{i+1:2n}^{\\mathbb{b}}\\|^{2}]\\Big\\}^{1/2}\\big\\{\\mathbb{E}^{\\mathbb{b}}[\\|\\theta_{i}^{\\mathbb{b}}-\\theta_{i}^{(\\mathbb{b},i)}\\|^{2}]\\big\\}^{1/2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\lesssim\\displaystyle\\frac{\\alpha_{i}\\|\\varepsilon\\|_{\\infty}\\,C_{\\mathbf{A}}^{2}}{\\sqrt{n}\\alpha_{2n}a}\\exp\\big\\{-\\displaystyle\\frac{a}{4}\\sum_{j=i+1}^{2n}\\alpha_{j}\\big\\}+\\displaystyle\\frac{(C_{\\mathbf{A}}^{2}\\,\\vee1)\\sqrt{i}}{\\sqrt{n}\\alpha_{2n}}\\exp\\big\\{-\\displaystyle\\frac{a}{4}\\sum_{j=1}^{2n}\\alpha_{j}\\big\\}\\|\\theta_{0}-\\frac{1}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, taking sum for $i$ from $n+1$ to $2n$ , and applying Lemma 2, we get that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{i=n+1}^{2n}\\{\\mathbb{E}^{\\|[D_{2}^{k}-D_{2}^{(k)}]\\|^{2}}\\}^{1/2}\\lesssim\\sum_{i=n+1}^{2n}\\frac{\\alpha_{i}\\|e\\|_{\\infty}\\mathsf{C}_{\\alpha}^{2}}{\\sqrt{\\pi}\\alpha_{2n}\\alpha}\\exp\\{-\\frac{a}{4}\\sum_{j=1}^{2n}\\alpha_{j}\\}}}\\\\ &{}&{+\\sum_{i=n+1}^{2n}\\frac{(\\mathbb{C}_{\\alpha}^{2}\\mathsf{V})\\sqrt{1}}{\\sqrt{\\pi}\\alpha_{2n}}\\exp\\{-\\frac{a}{4}\\sum_{j=1}^{2n}\\alpha_{j}\\}\\|\\theta_{0}-\\theta^{*}\\|}\\\\ &{}&{\\lesssim\\frac{\\|e\\|_{\\infty}\\mathsf{C}_{\\alpha}^{2}}{\\sqrt{\\pi}\\alpha_{2n}\\alpha^{2}}+\\frac{(\\mathbb{C}_{\\alpha}^{2}\\mathsf{V})\\ln\\eta}{\\alpha_{2n}}\\exp\\{-\\frac{a}{4}\\sum_{j=1}^{2n}\\alpha_{j}\\}\\|\\theta_{0}-\\theta^{*}\\|}\\\\ &{}&{\\lesssim\\frac{\\|e\\|_{\\infty}\\mathsf{C}_{\\alpha}^{2}}{\\alpha^{2}}+(\\mathbb{C}_{\\alpha}^{2}\\mathsf{V})n^{3/2}\\exp\\{-\\frac{a}{4}\\sum_{j=1}^{2n}\\alpha_{j}\\}\\|\\theta_{0}-\\theta^{*}\\|}\\\\ &{}&{\\lesssim\\frac{\\|e\\|_{\\infty}\\mathsf{C}_{\\alpha}^{2}}{\\alpha^{2}}+(\\mathbb{C}_{\\alpha}^{2}\\mathsf{V})n^{3/4}\\exp\\{-\\frac{a}{4}\\sum_{j=1}^{2n}\\alpha_{j}\\}\\|\\theta_{0}-\\theta^{*}\\|\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Here in the last line above we used a particular form $\\alpha_{k}=c_{0}/\\sqrt{k}$ , and relied on the bound ", "page_idx": 34}, {"type": "equation", "text": "$$\nn^{3/4}\\exp\\{-\\frac{a}{4}\\sum_{j=n+1}^{2n}\\alpha_{j}\\}\\leq1\\;,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which is guaranteed by the lower bound on the trajectory length $n$ of the form ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{\\sqrt{n}}{\\log n}\\geq\\frac{3}{2(\\sqrt{2}-1)a c_{0}}\\;.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The latter condition is guaranteed by A4. Now we proceed with $D_{3}^{\\mathsf{b}}-D_{3}^{({\\mathsf{b}},i)}$ . Using is definition in (44), we get ", "page_idx": 34}, {"type": "equation", "text": "$$\nD_{3}^{\\mathbf{b}}-D_{3}^{(\\mathbf{b},i)}=\\frac{1}{\\sqrt{n}}(w_{i}-w_{i}^{\\prime})\\mathbf{A}_{i}(\\theta_{i-1}^{\\mathbf{b}}-\\theta^{\\star})+\\frac{1}{\\sqrt{n}}\\sum_{k=i+1}^{2n}(w_{k}-1)\\mathbf{A}_{k}(\\theta_{k-1}^{\\mathbf{b}}-\\theta_{k-1}^{(\\mathbf{b},i)})\\;.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Since the latter term is a martingale-difference, we obtain that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|D_{3}^{\\mathbf{b}}-D_{3}^{(\\mathbf{b},i)}\\|^{2}]\\lesssim\\displaystyle\\frac{\\mathrm{C}_{\\mathbf{A}}^{2}}{n}\\mathbb{E}[\\|\\theta_{i-1}^{\\mathbf{b}}-\\theta^{\\star}\\|^{2}]+\\displaystyle\\frac{\\mathrm{C}_{\\mathbf{A}}^{2}}{n}\\sum_{k=i+1}^{2n}\\mathbb{E}^{\\mathbf{b}}[\\|\\theta_{k-1}^{\\mathbf{b}}-\\theta_{k-1}^{(\\mathbf{b},i)}\\|^{2}]}\\\\ &{\\qquad\\qquad\\lesssim\\displaystyle\\frac{\\mathrm{C}_{\\mathbf{A}}^{2}}{n}\\mathbb{E}[\\|\\theta_{i-1}^{\\mathbf{b}}-\\theta_{i-1}\\|^{2}]+\\displaystyle\\frac{\\mathrm{C}_{\\mathbf{A}}^{2}}{n}\\|\\theta_{i-1}-\\theta^{\\star}\\|^{2}+\\displaystyle\\frac{\\mathrm{C}_{\\mathbf{A}}^{2}}{n}\\sum_{k=i+1}^{2n}\\mathbb{E}^{\\mathbf{b}}[\\|\\theta_{k-1}^{\\mathbf{b}}-\\theta_{k-1}^{(\\mathbf{b},i)}\\|^{2}]}\\\\ &{\\qquad\\qquad\\lesssim\\displaystyle\\frac{\\mathrm{C}_{\\mathbf{A}}^{2}}{n}\\mathbb{E}[\\|\\theta_{i-1}^{\\mathbf{b}}-\\theta_{i-1}\\|^{2}]+\\displaystyle\\frac{\\mathrm{C}_{\\mathbf{A}}^{2}}{n}\\|\\theta_{i-1}-\\theta^{\\star}\\|^{2}+\\displaystyle\\frac{\\mathrm{C}_{\\mathbf{A}}^{2}}{n}\\sum_{k=i+1}^{2n}\\mathbb{E}^{\\mathbf{b}}[\\|\\Gamma_{i+1:k-1}^{\\mathbf{b}}\\|^{2}\\|\\theta_{i}^{\\mathbf{b}}\\|^{2}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Hence we obtain, using (59) together with the definition of $\\Omega_{1}$ ,that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathfrak{L}[\\|D_{3}^{\\mathrm{b}}-D_{3}^{(\\mathsf{b},i)}\\|^{2}]\\lesssim\\frac{\\alpha_{i-1}\\|\\varepsilon\\|_{\\infty}^{2}\\,\\mathsf{C}_{\\mathbf{A}}^{4}}{n a^{3}}+\\frac{\\alpha_{i-1}\\|\\varepsilon\\|_{\\infty}^{2}\\,\\mathsf{C}_{\\mathbf{A}}^{2}\\log^{2}n}{a n}+\\frac{\\mathsf{C}_{\\mathbf{A}}^{2}}{n}\\exp\\left\\{-a\\displaystyle\\sum_{\\ell=1}^{i}\\alpha_{\\ell}\\right\\}\\|\\theta_{0}-\\theta^{*}\\|^{2}}\\\\ &{}&{\\quad+\\,\\displaystyle{\\frac{\\mathrm{C}_{\\mathbf{A}}^{2}}{n}}\\left(\\frac{\\alpha_{i}^{2}\\|\\varepsilon\\|_{\\infty}^{2}\\,\\mathsf{C}_{\\mathbf{A}}^{4}}{a^{2}}+(\\mathrm{C}_{\\mathbf{A}}^{4}\\,\\vee1)i\\displaystyle\\prod_{j=1}^{i}(1-a\\alpha_{j}/4)^{2}\\|\\theta_{0}-\\theta^{*}\\|^{2}\\right)\\displaystyle\\sum_{k=i+1}^{2n}\\exp\\left\\{-\\frac{a}{2}\\sum_{j=i+1}^{k-1}\\alpha_{j}\\right\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Considering the latter term in the sum, we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{1}\\lesssim\\frac{\\alpha_{i}^{2}\\|\\varepsilon\\|_{\\infty}^{2}\\,\\mathsf{C}_{\\mathbf{A}}^{6}}{n\\alpha_{2n}a^{2}}\\sum_{k=i+1}^{2n}\\alpha_{k}\\exp\\bigl\\{-\\frac{a}{2}\\sum_{j=i+1}^{k-1}\\alpha_{j}\\bigr\\}+\\frac{(\\mathsf{C}_{\\mathbf{A}}^{6}\\,\\mathsf{V}1)i}{n}\\sum_{k=i+1}^{2n}\\exp\\bigl\\{-\\frac{a}{2}\\sum_{j=1}^{k-1}\\alpha_{j}\\bigr\\}\\|\\theta_{0}-\\theta^{*}\\|^{2}}\\\\ &{\\quad\\lesssim\\frac{\\alpha_{i}\\|\\varepsilon\\|_{\\infty}^{2}\\,\\mathsf{C}_{\\mathbf{A}}^{6}}{n a^{3}}+\\frac{(\\mathsf{C}_{\\mathbf{A}}^{6}\\,\\mathsf{V}1)i}{n\\alpha_{2n}}\\exp\\bigl\\{-\\frac{a}{2}\\sum_{j=1}^{i}\\alpha_{j}\\bigr\\}\\|\\theta_{0}-\\theta^{*}\\|^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus, summing the equations (61) for $i$ from $n+1$ to $2n$ , we obtain that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{n=1}^{2n}\\{\\mathbb{E}^{\\mathrm{b}}[\\|D_{3}^{\\mathrm{b}}-D_{3}^{(\\mathrm{b},i)}\\|^{2}]\\}^{1/2}\\lesssim\\sum_{i=n+1}^{2n}\\left(\\frac{\\sqrt{\\alpha_{i-1}}\\|\\varepsilon\\|_{\\infty}\\,{\\mathrm C}_{\\mathrm{A}}^{2}}{\\sqrt{n}a^{3/2}}+\\frac{\\sqrt{\\alpha_{i-1}}\\|\\varepsilon\\|_{\\infty}\\,{\\mathrm C}_{\\mathrm{A}}\\log n}{\\sqrt{a n}}+\\frac{\\sqrt{\\alpha_{i}}\\|\\varepsilon\\|_{\\infty}\\,{\\mathrm C}_{\\mathrm{B}}}{\\sqrt{n}a^{3/2}}\\right.}}\\\\ &{}&{+\\left.\\frac{{\\mathrm{C}}_{\\mathrm{A}}^{3}\\,\\nabla1}{\\sqrt{n}}\\sum_{i=n+1}^{2n}\\exp\\bigl\\{-\\frac{a}{2}\\displaystyle\\sum_{\\ell=1}^{i}\\alpha_{\\ell}\\bigr\\}\\|\\theta_{0}-\\theta^{\\star}\\|\\right.}\\\\ &{}&{\\left.\\lesssim\\frac{({\\mathrm C}_{\\mathrm{A}}^{3}\\,\\nabla1)\\|\\varepsilon\\|_{\\infty}}{a^{3/2}}n^{1/4}\\log n+\\frac{{\\mathrm C}_{\\mathrm{A}}^{3}\\,\\nabla1}{a}\\exp\\bigl\\{-\\frac{a}{2}\\displaystyle\\sum_{\\ell=1}^{n}\\alpha_{\\ell}\\bigr\\}\\|\\theta_{0}-\\theta^{\\star}\\|\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Using now the definition of ${D}_{4}^{\\mathsf{b}}$ in (44) and Minkowski's inequality, we write ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\mathbb{B}}[\\|D_{4}^{\\bf b}-D_{4}^{(\\bf b,\\it i)}\\|^{2}]^{1/2}\\le\\displaystyle\\frac{1}{\\sqrt{n}}\\{\\mathbb{E}^{\\mathbb{B}}[\\|\\theta_{i}^{\\bf b}-\\theta_{i}^{(\\bf b,\\it i)}\\|^{2}]\\}^{1/2}\\displaystyle\\sum_{k=i+1}^{2n}\\left(\\frac{1}{\\alpha_{k}}-\\frac{1}{\\alpha_{k-1}}\\right)\\exp\\{-\\frac{a}{4}\\sum_{j=i+1}^{k-1}\\alpha_{j}\\}}\\\\ &{\\lesssim\\frac{\\alpha_{i}\\|\\varepsilon\\|_{\\infty}}{\\sqrt{n}}\\,{\\mathbb{C}_{\\bf A}^{2}}\\displaystyle\\sum_{k=i+1}^{2n}\\alpha_{k}\\exp\\{-\\frac{a}{4}\\sum_{j=i+1}^{k-1}\\alpha_{j}\\}}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\frac{({\\mathbb{C}_{\\bf A}^{2}}\\vee{\\mathbb{I}})\\sqrt{i}}{\\sqrt{n}}\\,\\displaystyle\\sum_{k=i+1}^{2n}\\alpha_{k}\\exp\\{-\\frac{a}{4}\\sum_{j=1}^{k-1}\\alpha_{j}\\}\\|\\theta_{0}-\\theta^{*}\\|}\\\\ &{\\lesssim\\frac{\\alpha_{i}\\|\\varepsilon\\|_{\\infty}\\,{\\mathbb{C}_{\\bf A}^{2}}}{\\sqrt{n}a^{2}}+\\frac{({\\mathbb{C}_{\\bf A}^{2}}\\vee{\\mathbb{I}})\\sqrt{i}}{\\sqrt{n}a}\\exp\\{-\\frac{a}{4}\\sum_{j=1}^{i}\\alpha_{j}\\}\\|\\theta_{0}-\\theta^{*}\\|\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus, taking sum for $i$ from $n+1$ to $2n$ , we get ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{n+1}^{2n}\\{\\mathbb{E}^{\\mathrm{b}}[\\|D_{4}^{\\mathrm{b}}-D_{4}^{(\\mathrm{b},i)}\\|^{2}]\\}^{1/2}\\lesssim\\sum_{i=n+1}^{2n}\\frac{\\alpha_{i}\\|\\varepsilon\\|_{\\infty}\\,C_{\\mathbf{A}}^{2}}{\\sqrt{n}a^{2}}+\\sum_{i=n+1}^{2n}\\frac{(C_{\\mathbf{A}}^{2}\\vee1)\\sqrt{i}}{\\sqrt{n}a}\\exp\\{-\\frac{a}{4}\\sum_{j=1}^{i}\\alpha_{j}\\}\\|\\theta_{0}-\\theta_{j}\\|_{\\infty}^{2n}}}\\\\ &{}&{\\lesssim\\frac{\\|\\varepsilon\\|_{\\infty}\\,C_{\\mathbf{A}}^{2}}{a^{2}}+\\frac{(C_{\\mathbf{A}}^{2}\\vee1)\\sqrt{n}}{a^{2}}\\exp\\{-\\frac{a}{4}\\sum_{j=1}^{n}\\alpha_{i}\\}\\|\\theta_{0}-\\theta^{\\star}\\|\\ .\\ \\ \\ (63)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Similarly, with the definition of $D_{5}^{\\mathsf{b}}$ in (44), we write ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\bar{y}}_{5}^{\\mathsf{b}}-D_{5}^{(\\mathsf{b},i)}=\\displaystyle\\frac{1}{\\sqrt{n}}\\sum_{k=i+1}^{2n}\\left(\\mathbf{A}_{k}-\\bar{\\mathbf{A}}\\right)\\!\\left(\\theta_{k-1}^{\\mathsf{b}}-\\theta_{k-1}^{(\\mathsf{b},i)}\\right)=\\displaystyle\\frac{1}{\\sqrt{n}}\\left\\{\\sum_{k=i+1}^{2n}(\\mathbf{A}_{k}-\\bar{\\mathbf{A}})\\Gamma_{i+1;k-1}^{\\mathsf{b}}\\right\\}\\!\\left(\\theta_{i}^{\\mathsf{b}}-\\theta_{i}^{(\\mathsf{b},i)}\\right)}\\\\ &{\\qquad\\qquad=\\underbrace{\\frac{1}{\\sqrt{n}}\\left\\{\\displaystyle\\sum_{k=i+1}^{2n}\\left(\\mathbf{A}_{k}-\\bar{\\mathbf{A}}\\right)\\Gamma_{i+1;k-1}\\right\\}\\!\\left(\\theta_{i}^{\\mathsf{b}}-\\theta_{i}^{(\\mathsf{b},i)}\\right)}_{T_{2}}\\quad}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\frac{1}{\\sqrt{n}}\\left\\{\\sum_{k=i+1}^{2n}\\left(\\mathbf{A}_{k}-\\bar{\\mathbf{A}}\\right)\\!\\left(\\Gamma_{i+1;k-1}^{\\mathsf{b}}-\\Gamma_{i+1;k-1}\\right)\\right\\}\\!\\left(\\theta_{i}^{\\mathsf{b}}-\\theta_{i}^{(\\mathsf{b},i)}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Now we bound the terms $T_{2}$ and $T_{3}$ separately. Indeed, for the term $T_{2}$ we get, applying the definition of the set $\\Omega_{4}$ , that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\mathbf{b}}[\\|T_{2}\\|^{2}]\\lesssim\\frac{1}{n}\\left(\\frac{\\mathrm{C}_{\\mathbf{A}}^{2}\\log n}{a\\alpha_{i}}+\\mathrm{C}_{\\mathbf{A}}^{2}\\log^{2}n\\right)\\mathbb{E}^{\\mathbf{b}}[\\|\\theta_{i}^{\\mathbf{b}}-\\theta_{i}^{(\\mathbf{b},i)}\\|^{2}]}\\\\ &{\\qquad\\qquad\\lesssim\\frac{\\mathrm{C}_{\\mathbf{A}}^{2}\\log n}{n a\\alpha_{i}}\\mathbb{E}^{\\mathbf{b}}[\\|\\theta_{i}^{\\mathbf{b}}-\\theta_{i}^{(\\mathbf{b},i)}\\|^{2}]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "In the above bounds we have used that \u03b1e \u2264 alogn. . For the term $T_{3}$ we get, applying Lemma 10, that for any vector $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ \uff0c ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{i=i+1}^{2n}\\mathbf{\\Sigma}_{(\\mathbf{A}_{k}-\\bar{\\mathbf{A}})(\\Gamma_{i+1:k-1}^{\\mathbf{b}}-\\Gamma_{i+1:k-1})v}=\\sum_{k=i+1}^{2n}\\sum_{\\ell=i+1}^{k-1}(\\mathbf{A}_{k}-\\bar{\\mathbf{A}})\\Gamma_{\\ell+1:k-1}\\alpha_{\\ell}(w_{\\ell}-1)\\mathbf{A}_{\\ell}\\Gamma_{i+1:\\ell-1}^{\\mathbf{b}}\\,v}\\\\ {=\\sum_{\\ell=i+1}^{2n-1}\\alpha_{\\ell}(w_{\\ell}-1)\\Bigg\\{\\sum_{k=\\ell+1}^{2n}(\\mathbf{A}_{k}-\\bar{\\mathbf{A}})\\Gamma_{\\ell+1:k-1}\\Bigg\\}\\mathbf{A}_{\\ell}\\Gamma_{i+1:\\ell-1}^{\\mathbf{b}}\\,.}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "From the above representation we obtain, using the definition of the set $\\Omega_{4}$ , that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\mathbb{b}}[\\|T_{3}\\|^{2}]\\lesssim\\frac{\\mathbf{C}_{\\mathbf{A}}^{2}}{n}\\displaystyle\\sum_{\\ell=i+1}^{2n-1}\\alpha_{\\ell}^{2}\\left(\\frac{\\mathbf{C}_{\\mathbf{A}}^{2}\\log n}{a\\alpha_{\\ell}}+\\mathbf{C}_{\\mathbf{A}}^{2}\\log^{2}n\\right)\\exp\\Biggl\\{-\\frac{a}{4}\\displaystyle\\sum_{j=i+1}^{\\ell-1}\\alpha_{j}\\Biggr\\}\\mathbb{E}^{\\mathbb{b}}[\\|\\theta_{i}^{\\mathbf{b}}-\\theta_{i}^{(\\mathbf{b},i)}\\|^{2}]}\\\\ &{\\quad\\quad\\lesssim\\frac{\\mathbf{C}_{\\mathbf{A}}^{4}}{n}\\displaystyle\\sum_{\\ell=i+1}^{2n-1}\\frac{\\alpha_{\\ell}\\log n}{a}\\exp\\Biggl\\{-\\frac{a}{4}\\displaystyle\\sum_{j=i+1}^{\\ell-1}\\alpha_{j}\\Biggr\\}\\mathbb{E}^{\\mathbb{b}}[\\|\\theta_{i}^{\\mathbf{b}}-\\theta_{i}^{(\\mathbf{b},i)}\\|^{2}]}\\\\ &{\\quad\\quad\\lesssim\\frac{\\mathbf{C}_{\\mathbf{A}}^{4}\\log n}{n a^{2}}\\,\\mathbb{E}^{\\mathbb{b}}[\\|\\theta_{i}^{\\mathbf{b}}-\\theta_{i}^{(\\mathbf{b},i)}\\|^{2}]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Combining the above bounds, we obtain that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\mathbf{b}}[\\|D_{5}^{\\mathbf{b}}-D_{5}^{(\\mathbf{b},i)}\\|^{2}]\\lesssim\\frac{\\mathbb{C}_{\\mathbf{A}}^{2}\\log n}{n a}\\left(\\frac{1}{\\alpha_{i}}+\\frac{\\mathbf{C}_{\\mathbf{A}}^{2}}{a}\\right)\\mathbb{E}^{\\mathbf{b}}[\\|\\theta_{i}^{\\mathbf{b}}-\\theta_{i}^{(\\mathbf{b},i)}\\|^{2}]}\\\\ &{\\qquad\\qquad\\qquad\\lesssim\\frac{\\mathbb{C}_{\\mathbf{A}}^{2}\\log n}{n a\\alpha_{i}}\\mathbb{E}^{\\mathbf{b}}[\\|\\theta_{i}^{\\mathbf{b}}-\\theta_{i}^{(\\mathbf{b},i)}\\|^{2}]\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where we have additionally used that $\\alpha_{i}\\leq a/\\,\\mathrm{C}_{\\mathbf{A}}^{2}$ . Thus, using the upper bound (59), we obtain that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{i=n+1}^{2n}\\frac{\\{\\mathbb{E}^{\\|\\{\\cal J}_{0}^{\\bf b}\\|}\\|_{\\mathcal{S}}^{\\bf b}-D_{\\mathrm{t}}^{(\\bf b,i)}\\|^{2}\\}^{1/2}}{\\alpha^{1}}}\\quad}&{}\\\\ &{\\lesssim\\sum_{i=n+1}^{2n}\\frac{\\sum_{\\mathbf{A}}\\sqrt{\\log n}}{\\sqrt{\\alpha_{i}\\alpha n}}\\frac{\\alpha_{i}\\|\\varepsilon\\|_{\\infty}\\zeta_{\\bf A}^{2}}{a}+\\sum_{i=n+1}^{2n}\\frac{\\sum_{\\mathbf{A}}\\sqrt{\\log n}}{\\sqrt{\\alpha_{i}\\alpha n}}(C_{\\mathbf{A}}^{2}\\mathbb{V}^{1})\\sqrt{i}\\prod_{j=1}^{i}(1-a\\alpha_{j}/4)\\|\\theta_{0}-\\theta^{*}\\|_{\\infty}}\\\\ &{\\lesssim\\frac{\\mathbb{C}_{\\mathbf{A}}^{\\|\\cal J_{0}\\|}\\|_{\\infty}}{a^{3/2}}n^{1/4}\\sqrt{\\log n}}\\\\ &{\\quad\\quad\\quad\\quad+\\frac{(\\mathbb{C}_{\\mathbf{A}}^{3}\\mathbb{V})\\sqrt{\\log n}}{a^{1/2}a_{2}^{3/2}}\\exp\\{-\\frac{a}{4}\\sum_{j=1}^{n}\\alpha_{j}\\}\\left\\{\\sum_{i=n+1}^{2n}\\alpha_{i}\\prod_{j=n+1}^{i}(1-a\\alpha_{j}/4)\\right\\}\\|\\theta_{0}-\\theta^{*}\\|_{\\infty}}\\\\ &{\\lesssim\\frac{\\mathbb{C}_{\\mathbf{A}}^{3}\\|\\varepsilon\\|_{\\infty}}{a^{3/2}}n^{1/4}\\sqrt{\\log n}+\\frac{(\\mathbb{C}_{\\mathbf{A}}^{3})\\log n}{a^{3/2}}\\exp\\{-\\frac{a}{4}\\sum_{j=1}^{n}\\alpha_{j}\\}\\|\\theta_{0}-\\theta^{*}\\|\\ .\\quad\\quad\\quad(64)}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Now it remains to combine the bounds outlined above in (60), (62), (63), and (64), and the statement follows. ", "page_idx": 37}, {"type": "text", "text": "D  Proof of stability of random matrix product ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "D.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "The fact that there exists a unique matrix $Q$ , such that the following Lyapunov equation holds: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{A}}^{\\top}Q+Q\\bar{\\mathbf{A}}=P\\ ,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "follows directly from [54, Lemma 9.1, p. 140]. In order to show the second part of the statement, we note that for any non-zerovector $\\boldsymbol{x}\\in\\bar{\\mathbb{R}}^{d}$ ,wehave ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{x^{\\top}(\\mathrm{I}-\\alpha\\bar{\\mathbf{A}})^{\\top}Q(\\mathrm{I}-\\alpha\\bar{\\mathbf{A}})x}{x^{\\top}Q x}=1-\\alpha\\frac{x^{\\top}(\\bar{\\mathbf{A}}^{\\top}Q+Q\\bar{\\mathbf{A}})x}{x^{\\top}Q x}+\\alpha^{2}\\frac{x^{\\top}\\bar{\\mathbf{A}}^{\\top}Q\\bar{\\mathbf{A}}x}{x^{\\top}Q x}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x}=1-\\alpha\\frac{x^{\\top}P x}{x^{\\top}Q x}+\\alpha^{2}\\frac{x^{\\top}\\bar{\\mathbf{A}}^{\\top}Q\\bar{\\mathbf{A}}x}{x^{\\top}Q x}}\\\\ &{\\phantom{x x x x x x x}{x}1-\\alpha\\frac{\\lambda_{\\mathrm{min}}(P)}{\\|Q\\|}+\\alpha^{2}\\frac{\\|\\bar{\\mathbf{A}}\\|_{Q}^{2}}{\\lambda_{\\mathrm{min}}(Q)}}\\\\ &{\\phantom{x x x x x}{x}1-\\alpha a\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where we set ", "page_idx": 37}, {"type": "equation", "text": "$$\na={\\frac{1}{2}}{\\frac{\\lambda_{\\operatorname*{min}}(P)}{\\lambda_{\\operatorname*{max}}(Q)}}\\;,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and used the fact that $\\alpha\\leq\\alpha_{\\infty}$ , where $\\alpha_{\\infty}$ is defined in (7). ", "page_idx": 37}, {"type": "text", "text": "D.2  Proofs for auxiliary results on products of random matrix ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Our analysis of the bootstrap procedure and the last iterate error of LSA procedure is based on the error expansion technique from [1], see also [16]. Namely, to perform the expansion, we decompose theLSA iterates $\\theta_{n}$ defined in (1) into a transient and fuctuation terms: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\theta_{k}-\\theta^{\\star}=\\tilde{\\theta}_{n}^{(\\mathsf{t r})}+\\tilde{\\theta}_{n}^{(\\mathsf{f l})}\\ ,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where we have defined the quantities ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\tilde{\\theta}_{k}^{(\\mathrm{tr})}=\\Gamma_{1:k}\\{\\theta_{0}-\\theta^{\\star}\\}\\;,\\quad\\tilde{\\theta}_{k}^{(\\mathrm{fl})}=-\\sum_{j=1}^{k}\\alpha_{j}\\Gamma_{j+1:k}\\varepsilon_{j}\\;,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "setting ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\Gamma_{m:k}=\\prod_{i=m}^{k}\\left(\\operatorname{I}-\\alpha_{i}\\mathbf{A}\\big(Z_{i}\\big)\\right)\\,,\\quad m,k\\in\\mathbb{N},m\\leq k\\,\\,,\\,\\mathrm{with\\,\\,the\\,\\,convention\\,\\,}\\Gamma_{m:k}=\\mathbf{I}\\,,m>k\\,\\,.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The dependence of $\\Gamma_{m;k}$ upon the stepsizes $\\left(\\alpha_{j}\\right)$ is implicit in (67). Here the quantity $\\tilde{\\theta}_{k}^{(\\mathrm{tr})}$ is the transient component of the error, which determines the rate at which the initial error $\\ddot{\\theta_{0}}-\\theta^{\\star}$ is forgotten. The term $\\tilde{\\theta}_{k}^{(\\mathsf{f l})}$ corresponds to the fuctuation component of the error and is determined by the oscillations of the last iterate $\\theta_{k}$ around $\\theta^{\\star}$ ", "page_idx": 38}, {"type": "text", "text": "In order to bound the moment $\\mathbb{E}[\\|\\theta_{k}-\\theta^{\\star}\\|^{p}]$ , we first prove a stability results on the products of random matrices $\\Gamma_{m;k}$ arising in the LSA recursion. Towards this aim we first introduce some notations and defnitions. For a matrix $B\\,\\in\\,\\mathbb{R}^{d\\times d}$ we denote by $(\\sigma_{\\ell}(B))_{\\ell=1}^{d}$ its singular values. For $q\\geq1$ , the Shatten $q$ -norm of $B$ is denoted by $\\begin{array}{r}{\\|B\\|_{q}=\\{\\sum_{\\ell=1}^{d}\\sigma_{\\ell}^{q}(B)\\}^{1/q}}\\end{array}$ . For $q,p\\geq1$ and a random matrix $\\mathbf{X}$ we write $\\|\\mathbf{X}\\|_{q,p}=\\{\\mathbb{E}[\\|\\mathbf{X}\\|_{q}^{p}]\\}^{1/p}$ . Our proof technique is based on the stability results arising in [29], see also [16]. ", "page_idx": 38}, {"type": "text", "text": "Lemma 12 (Proposition 15 in [16]). Let $\\{\\mathbf{Y}_{\\ell}\\}_{\\ell\\in\\mathbb{N}}$ be an independent sequence and $P$ be a positive definite matrix. Assume that for each $\\ell\\in\\mathbb{N}$ there exist $m_{\\ell}\\in(0,1)$ and $\\sigma_{\\ell}>0$ such that $\\|\\mathbb{E}[\\mathbf{\\dot{Y}}_{\\ell}]\\|_{P}^{2}\\leq$ $1-m_{\\ell}$ and $\\|\\mathbf{Y}_{\\ell}-\\mathbb{E}[\\mathbf{Y}_{\\ell}]\\|_{P}\\leq\\sigma_{\\ell}$ almost surely. Define $\\begin{array}{r}{{\\bf Z}_{k}=\\prod_{\\ell=0}^{k}{\\bf Y}_{\\ell}={\\bf Y}_{k}{\\bf Z}_{k-1},}\\end{array}$ for $k\\geq1$ and starting from $\\mathbf{Z}_{0}$ . Then, for any $2\\le q\\le p$ and $k\\geq1$ \uff0c ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|\\mathbf{Z}_{k}\\|_{p,q}^{2}\\leq\\kappa_{P}\\prod_{\\ell=1}^{k}(1-m_{\\ell}+(p-1)\\sigma_{\\ell}^{2})\\|P^{1/2}\\mathbf{Z}_{0}P^{-1/2}\\|_{p,q}^{2}\\;,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where we recall that $\\kappa_{P}=\\lambda_{\\mathrm{min}}^{-1}(P)\\lambda_{\\mathrm{max}}(P)$ ", "page_idx": 38}, {"type": "text", "text": "Now we aim to bound $\\Gamma_{m;k}$ defined in (67) using Lemma 12. We identify the latter with $\\prod_{\\ell=m}^{k}\\mathbf{Y}_{\\ell}$ where $\\mathbf{Y}_{\\ell}\\;=\\;\\mathrm{I}\\,-\\,\\alpha_{\\ell}\\mathbf{A}_{\\ell},\\ell\\;\\geq\\;1$ ,and $\\mathbf{Y}_{0}~=\\mathrm{~I~}$ Applying the bound (8), we get $\\|\\mathbb{E}[\\mathbf{Y}_{\\ell}]\\|_{Q}^{2}\\;=$ $\\|\\mathbf{I}-\\alpha_{\\ell}\\bar{\\mathbf{A}}\\|_{Q}^{2}\\leq1-a\\alpha_{\\ell}$ . Further, assumption A2 implies that almost surely, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{Y}_{\\ell}-\\mathbb{E}[\\mathbf{Y}_{\\ell}]\\|_{Q}=\\alpha_{\\ell}\\|\\mathbf{A}_{\\ell}-\\bar{\\mathbf{A}}\\|_{Q}\\le\\alpha_{\\ell}\\sqrt{\\kappa_{Q}}\\,\\mathbf{C}_{\\mathbf{A}}=b_{Q}\\alpha_{\\ell}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore, (68) holds with $m_{\\ell}=a\\alpha_{\\ell}$ and $\\sigma_{\\ell}=b_{Q}\\alpha_{\\ell}$ .As $\\|\\boldsymbol{\\mathrm{I}}\\|_{p}=d^{1/p}$ , we obtain the following corollary. ", "page_idx": 38}, {"type": "text", "text": "Corollary 3. Assume A1 and A2. Then, for any $\\alpha_{\\ell}\\in[0,\\alpha_{\\infty}]$ \uff0c $2\\leq q\\leq p,$ and $1\\leq m\\leq k,$ it holds ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}^{1/q}\\left[\\Vert\\Gamma_{m:k}\\Vert^{q}\\right]\\leq\\Vert\\Gamma_{m:k}\\Vert_{p,q}\\leq\\sqrt{\\kappa_{Q}}d^{1/p}\\prod_{\\ell=m}^{k}(1-a\\alpha_{\\ell}+(p-1)b_{Q}^{2}\\alpha_{\\ell}^{2})~,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\alpha_{\\infty}$ was defined in (7), and $b_{Q}=\\sqrt{\\kappa_{Q}}\\,\\mathrm{C}_{\\mathbf{A}}$ ", "page_idx": 38}, {"type": "text", "text": "Corollary 4. Assume A1, A2, and A3. Then for any $2\\leq q\\leq\\log n$ andany $k\\geq n$ $1\\leq m\\leq k$ it holdsthat ", "page_idx": 38}, {"type": "equation", "text": "$$\n{\\mathbb E}^{1/q}\\left[\\|\\Gamma_{m:k}\\|^{q}\\right]\\leq\\sqrt{\\kappa_{Q}}\\mathrm{e}\\exp\\left\\{-(a/2)\\sum_{\\ell=m}^{k}\\alpha_{\\ell}\\right\\}\\,,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\alpha_{\\infty}$ is defined in (7). Moreover, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}^{1/q}\\left[\\|\\Gamma_{m:k}\\|^{q}\\right]\\leq\\sqrt{\\kappa_{Q}}\\mathrm{e}\\prod_{\\ell=m}^{k}\\big(1-\\frac{a\\alpha_{\\ell}}{4}\\big)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. We first apply the result of Corollary 3. Indeed, for $k\\geq n$ , and any $2\\,\\le\\,q\\,\\le\\,p$ , it holds, setting $b_{Q}=\\sqrt{\\kappa_{Q}}\\,\\mathrm{C}_{\\mathbf{A}}$ , that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{E}^{1/q}\\left[\\|\\Gamma_{m:k}\\|^{q}\\right]\\leq\\sqrt{\\kappa_{Q}}d^{1/p}\\prod_{\\ell=m}^{k}(1-a\\alpha_{\\ell}+(p-1)b_{Q}^{2}\\alpha_{\\ell}^{2})}}\\\\ {{\\displaystyle\\leq\\sqrt{\\kappa_{Q}}d^{1/p}\\exp\\left\\{-a\\sum_{\\ell=m}^{k}\\alpha_{\\ell}+(p-1)b_{Q}^{2}\\sum_{\\ell=m}^{k}\\alpha_{\\ell}^{2}\\right\\}\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Note that, setting $p=\\log n$ , and provided that $n$ satisfies (9), we easily obtain that, for $\\ell\\geq n/2$ \uff0c ", "page_idx": 38}, {"type": "equation", "text": "$$\n(\\log n)b_{Q}^{2}\\alpha_{\\ell}^{2}\\leq a\\alpha_{\\ell}/2\\ .\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Hence, for $m\\geq n/2$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n{\\mathbb E}^{1/q}\\left[\\|\\Gamma_{m:k}\\|^{q}\\right]\\leq\\sqrt{\\kappa_{Q}}\\mathrm{e}\\exp\\bigl\\{-\\frac{a}{2}\\sum_{\\ell=m}^{k}\\alpha_{\\ell}\\bigr\\}\\,,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and the statement follows. Suppose now that $m<n/2$ . In such a case we have, applying (71), that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}^{1/q}\\left[\\|\\Gamma_{m:k}\\|^{q}\\right]\\leq\\sqrt{\\kappa_{Q}}\\mathrm{e}\\exp\\left\\{-\\displaystyle a\\sum_{\\ell=m}^{k}\\alpha_{\\ell}+(\\log n)b_{Q}^{2}\\sum_{\\ell=m}^{k}\\alpha_{\\ell}^{2}\\right\\}}\\\\ {\\displaystyle\\leq\\sqrt{\\kappa_{Q}}\\mathrm{e}\\exp\\left\\{-\\displaystyle a\\sum_{\\ell=m}^{n}\\alpha_{\\ell}+(\\log n)b_{Q}^{2}\\sum_{\\ell=m}^{n}\\alpha_{\\ell}^{2}\\right\\}\\exp\\left\\{-(a/2)\\sum_{\\ell=n+1}^{k}\\alpha_{\\ell}\\right\\}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and we need to bound the first term in the product. We first consider $\\alpha_{\\ell}=c_{0}\\ell^{-1/2}$ , and use the inequalities ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{\\ell=m}^{n}{\\frac{1}{\\ell}}\\leq\\left(1+\\int_{m}^{n}{\\frac{d x}{x}}\\right)\\wedge\\left(\\int_{m-1}^{n}{\\frac{d x}{x}}\\right)=\\left(1+\\log{\\frac{n}{m}}\\right)\\wedge\\left(\\log{\\frac{n}{m-1}}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{\\ell=m}^{n}{\\frac{1}{\\sqrt{\\ell}}}\\geq\\int_{m}^{n}{\\frac{d x}{\\sqrt{x}}}=2({\\sqrt{n}}-{\\sqrt{m}})\\;.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus, it is enough to satisfy the constraint ", "page_idx": 39}, {"type": "equation", "text": "$$\n(\\log n)b_{Q}^{2}c_{0}^{2}(1+\\log n-\\log m)\\leq a c_{0}(\\sqrt{n}-\\sqrt{m})\\;.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Since $m<n/2$ , it is enough to ensure that ", "page_idx": 39}, {"type": "equation", "text": "$$\n(1+\\log n)(\\log n)b_{Q}^{2}c_{0}^{2}\\leq a c_{0}(\\sqrt{n}-\\sqrt{n/2})\\;,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "or, equivalently, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{\\sqrt{n}}{(1+\\log n)\\log n}\\geq\\frac{c_{0}b_{Q}^{2}}{a(1-1/\\sqrt{2})}\\;,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which is granted by A3. Combining the above bounds in (72), we obtain that the lemma's statement (69) holds for the step size $\\alpha_{\\ell}=c_{0}/\\ell^{1/2}$ .Similarly, for $\\alpha_{\\ell}=c_{0}/\\ell^{\\gamma}$ With $\\gamma\\in(1/2;1)$ ,weget for $m\\geq n/2$ that ", "page_idx": 39}, {"type": "equation", "text": "$$\n{\\mathbb E}^{1/q}\\left[\\|\\Gamma_{m:k}\\|^{q}\\right]\\leq\\sqrt{\\kappa_{Q}}\\mathrm{e}\\exp\\bigl\\{-\\frac{a}{2}\\sum_{\\ell=m}^{k}\\alpha_{\\ell}\\bigr\\}\\,,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "since the relation (71) holds. Similarly, for $m<n/2$ , the desired upper bound would follow from the inequality ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{\\ell=m}^{n}{\\frac{1}{\\ell^{2\\gamma}}}\\leq\\int_{m-1}^{n}{\\frac{d x}{x^{2\\gamma}}}={\\frac{(m-1)^{1-2\\gamma}-n^{1-2\\gamma}}{2\\gamma-1}}\\leq{\\frac{1}{2\\gamma-1}}\\;,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "together with an inequality ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{(\\log n)b_{Q}^{2}c_{0}^{2}}{2\\gamma-1}\\leq(a/2)c_{0}(n^{1-\\gamma}-(n/2)^{1-\\gamma})\\;.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The latter inequality can be re-written as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{n^{1-\\gamma}}{\\log n}\\geq\\frac{2c_{0}b_{Q}^{2}}{a(2\\gamma-1)(1-(1/2)^{1-\\gamma}}\\;,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which is also granted by A3. Combining the above inequalities implies that (69) holds for $\\alpha_{\\ell}=c_{0}/\\ell^{\\gamma}$ The bound (70) can be immediately obtained from (69) using the fact that $\\mathrm{e}^{-x}\\,\\leq\\,1\\,-\\,x/2$ for $x\\in[0;1]$ \u53e3 ", "page_idx": 39}, {"type": "text", "text": "Corollary 5. Under conditions of Corollary 4 it holds with $\\mathbb{P}$ -probability at least $1-1/n^{2}$ that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left\\|\\Gamma_{m:k}\\right\\|\\leq\\sqrt{\\kappa_{Q}}\\mathrm{e}^{2}\\exp\\left\\{-(a/2)\\sum_{\\ell=m}^{k}\\alpha_{\\ell}\\right\\}\\,,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\|\\Gamma_{m:k}\\|\\leq\\sqrt{\\kappa_{Q}}\\mathrm{e}^{2}\\prod_{\\ell=m}^{k}\\big(1-\\frac{a\\alpha_{\\ell}}{4}\\big)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. It is sufficient to choose $q=2\\log n$ and use Markov's inequality together with the union bound. \u53e3 ", "page_idx": 40}, {"type": "text", "text": "Proposition 7. Assume A1, A2, A3 with $\\gamma=1/2$ . and A4. Then on the set $\\Omega_{5}$ defined in (43), it holds for any $n\\leq m\\leq k\\leq2n,$ that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left\\{\\mathbb{E}^{\\mathsf{b}}[\\|\\Gamma_{m+1:k}^{\\mathsf{b}}\\|^{2}]\\right\\}^{1/2}\\leq\\kappa_{Q}^{3/2}\\mathrm{e}^{9/8}\\exp\\left\\{-\\frac{a}{4}\\sum_{\\ell=m+1}^{k}\\alpha_{\\ell}\\right\\}\\;.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. Our proof relies on the auxiliary result of Lemma 13 below together with the blocking technique. Indeed, let us represent ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\displaystyle k-m=N h+r\\;,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $r<h$ and $h=h(n)$ is a block size defined in (17). Then we obtain, using the independence of bootstrap weights $w_{m+1},\\ldots,w_{k}$ , that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}^{\\mathbb{b}}[\\|\\Gamma_{m+1:k}^{\\mathbb{b}}\\|^{2}]\\hat{\\}^{1/2}\\leq\\sqrt{\\kappa_{Q}}\\{\\mathbb{E}^{\\mathbb{b}}[\\|\\Gamma_{m+1:k}^{\\mathbb{b}}\\|_{Q}^{2}]\\}^{1/2}}\\\\ {=\\sqrt{\\kappa_{Q}}\\displaystyle\\prod_{j=1}^{N}\\big\\{\\mathbb{E}^{\\mathbb{b}}[\\|\\Gamma_{m+1+(j-1)h:m+j h}^{\\mathbb{b}}\\|_{Q}^{2}]\\big\\}^{1/2}\\big\\{\\mathbb{E}^{\\mathbb{b}}[\\|\\Gamma_{m+1+N h:k}^{\\mathbb{b}}\\|_{Q}^{2}]\\big\\}^{1/2}}\\\\ {\\leq\\sqrt{\\kappa_{Q}}\\exp\\left\\{-\\frac{a}{4}\\sum_{\\ell=m+1}^{k}\\alpha_{\\ell}\\right\\}\\big\\{\\mathbb{E}^{\\mathbb{b}}[\\|\\Gamma_{m+1+N h:k}^{\\mathbb{b}}\\|_{Q}^{2}]\\big\\}^{1/2}\\exp\\left\\{\\displaystyle\\frac{a}{4}\\sum_{\\ell=m+1+N h:k}^{k}a\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "In the last inequality we applied Lemma 13 to each of the blocks of length $h$ in the first bound. It remains to upper bound the residual terms. Since the remainder block has length less then $h$ ,wehave due to (79) (which holds according to A4), that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\exp\\left\\{\\frac{a}{4}\\sum_{\\ell=m+1+N h:k}^{k}\\alpha_{\\ell}\\right\\}\\le\\exp\\left\\{\\frac{\\alpha_{\\infty}a}{4}\\right\\}\\le\\mathrm{e}^{1/8}\\;,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the last inequality is due to Proposition 1. Next, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big\\{\\mathbb{E}^{\\mathrm{b}}[\\|\\Gamma_{m+1+N h;k}^{\\mathbf{b}}\\|_{Q}^{2}]\\big\\}^{1/2}\\leq\\kappa_{Q}\\displaystyle\\prod_{\\ell=m+1+N h;k}^{k}\\{\\mathbb{E}^{\\mathrm{b}}[\\|(\\mathbf{I}-\\alpha_{\\ell}w_{\\ell}\\mathbf{A}_{\\ell})\\|^{2}]\\}^{1/2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\kappa_{Q}\\displaystyle\\prod_{\\ell=m+1+N h;k}^{k}\\{\\mathbb{E}^{\\mathrm{b}}[(1+\\alpha_{\\ell}|w_{\\ell}|\\,\\mathrm{C}_{\\mathbf{A}})^{2}]\\}^{1/2}}\\\\ &{\\qquad\\qquad\\leq\\kappa_{Q}\\displaystyle\\prod_{\\ell=m+1+N h;k}^{k}\\{\\mathbb{E}^{\\mathrm{b}}[1+2\\alpha_{\\ell}|w_{\\ell}|\\,\\mathrm{C}_{\\mathbf{A}}+\\alpha_{\\ell}^{2}w_{\\ell}^{2}\\,\\mathrm{C}_{\\mathbf{A}}^{2}]\\}^{1/2}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Since ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[|w_{\\ell}|]\\leq\\sqrt{\\mathbb{E}[w_{\\ell^{2}}]}\\leq\\sqrt{(\\mathbb{E}[w_{\\ell}])^{2}+\\operatorname{Var}w_{\\ell}}=\\sqrt{2}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "we get from previous bound ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big\\{\\mathbb{E}^{\\mathbf{b}}[\\|\\Gamma_{m+1+N h:k}^{\\mathbf{b}}\\|_{Q}^{2}]\\big\\}^{1/2}\\leq\\kappa_{Q}\\displaystyle\\prod_{\\ell=m+1+N h:k}^{k}(1+2\\sqrt{2}\\alpha_{\\ell}\\,\\mathbf{C_{A}}+2\\alpha_{\\ell}^{2}\\,\\mathbf{C_{A}^{2}})^{1/2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\kappa_{Q}\\exp\\Bigg\\{\\sqrt{2}\\,\\mathbf{C_{A}}\\displaystyle\\sum_{\\ell=m+1+N h:k}^{k}\\alpha_{\\ell}\\Bigg\\}\\leq\\kappa_{Q}\\mathrm{e}^{\\sqrt{2}\\,\\mathbf{C_{A}}\\,c_{0}h/\\sqrt{n}}\\leq\\kappa_{Q}\\mathrm{e}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where in the last line we additionally used (18). ", "page_idx": 40}, {"type": "text", "text": "Lemma 13. Assume A1, A2, A3 with $\\gamma=1/2$ andA4.Ontheset $\\Omega_{5}$ defined in(43),it holdsfor $h=h(n)$ definedin(17)andany $m\\in[n;2n-h]$ that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\big\\{\\mathbb E^{\\flat}[\\|\\Gamma_{m+1:m+h}^{\\flat}\\|_{Q}^{2}]\\big\\}^{1/2}\\leq\\exp\\left\\{-\\frac{a}{4}\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. Recallthat we use the notation $\\mathbb{E}^{\\flat}[\\cdot]=\\mathbb{E}[\\cdot|\\mathcal{Z}^{2n}]$ ,where $\\mathcal{Z}^{2n}=(Z_{1},...,Z_{2n})$ are the random variables used in the construction of the iterates $\\{\\theta_{k}\\}_{1\\leq k\\leq n}$ in (1). ", "page_idx": 41}, {"type": "text", "text": "Let $h\\in\\mathbb N$ be a block length, which value will be determined later, and consider a product ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\Gamma_{m+1:m+h}^{\\flat}=\\prod_{\\ell=m+1}^{m+h}(\\mathrm{I}-\\alpha_{\\ell}w_{\\ell}\\mathbf{A}_{\\ell})\\;.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Expanding the product of matrices (75), we obtain ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\Gamma_{m:m+h}^{\\mathrm{b}}=\\mathrm{I}-\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}\\mathbf{A}_{\\ell}-\\mathbf{S}+\\mathbf{R}=\\mathrm{I}-\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}\\bar{\\mathbf{A}}-\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}\\big(\\mathbf{A}_{\\ell}-\\bar{\\mathbf{A}}\\big)-\\mathbf{S}+\\mathbf{R}\\,,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where S = m+h $\\begin{array}{r}{{\\bf S}=\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}(w_{\\ell}-1){\\bf A}_{\\ell}}\\end{array}$ $\\{w_{\\ell}\\}_{\\ell=m+1}^{m+h}$ and theremainder $\\mathbf{R}$ collects the higher-order terms in the products ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbf{R}=\\sum_{r=2}^{h}(-1)^{r}\\sum_{(i_{1},\\dots,i_{r})\\in\\mathbb{I}_{r}^{\\ell}}\\prod_{u=1}^{r}\\alpha_{i_{u}}w_{i_{u}}\\mathbf{A}_{i_{u}}\\;.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "with $|_{r}^{\\ell}=\\{(i_{1},\\ldots,i_{r})\\in\\{m+1,\\ldots,m+h\\}^{r}\\,:\\,i_{1}<\\cdots<i_{r}\\}$ . We first consider the contracting part in matrix $Q$ -norm. Indeed, applying (8), we obtain that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\|\\mathbf{I}-\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}\\bar{\\mathbf{A}}\\|_{Q}^{2}\\leq1-a\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}\\;,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "provided that $h$ is set in such a manner that \u2265=m+1 @e \u2264 \u03b1, where \u03b1 is defined in (7). Hence, we get from the above inequality that for any $u\\in\\mathbb{R}^{d}$ , it holds that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\|\\mathbf{I}-\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}\\bar{\\mathbf{A}}\\|_{Q}\\leq1-(a/2)\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}\\;.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Now we need to estimate the remainders in the representation (76). On the set $\\Omega_{5}$ , it holds that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\|\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}(\\mathbf{A}_{\\ell}-\\bar{\\mathbf{A}})\\|_{Q}\\leq2\\,\\mathrm{C}_{\\mathbf{A}}\\,\\sqrt{\\kappa_{Q}}\\sqrt{\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}^{2}}\\log(2n^{4})\\;.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Moreover, it is straightforward to check that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\boldsymbol{\\ b}}[\\|\\mathbf{S}\\|_{Q}^{2}]\\leq\\mathrm{C}_{\\mathbf{A}}^{2}\\,\\kappa_{Q}\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}^{2}\\ .\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "In order to bound the remainder term $\\mathbf{R}$ ,wenotethat ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\mathbb{b}}[\\|\\mathbf{R}\\|_{Q}]\\leq\\displaystyle\\sum_{r=2}^{h}\\binom{h}{r}\\alpha_{m+1}^{r}(2\\,\\mathrm{C}_{\\mathbf{A}})^{r}\\kappa_{Q}^{r/2}\\leq\\alpha_{m+1}^{2}(2\\,\\mathrm{C}_{\\mathbf{A}})^{2}\\kappa_{Q}\\displaystyle\\sum_{r=0}^{h-2}\\binom{h}{r+2}\\alpha_{m+1}^{r}(2\\,\\mathrm{C}_{\\mathbf{A}})^{r}\\kappa_{Q}^{r/2}}\\\\ &{\\qquad\\qquad\\leq\\frac{\\alpha_{m+1}^{2}h^{2}(2\\,\\mathrm{C}_{\\mathbf{A}})^{2}\\kappa_{Q}}{2}\\exp\\bigl\\{2\\alpha_{m+1}\\,\\mathrm{C}_{\\mathbf{A}}\\,\\kappa_{Q}^{1/2}\\bigr\\}}\\\\ &{\\qquad\\qquad\\leq\\frac{\\alpha_{m+1}^{2}h^{2}(2\\,\\mathrm{C}_{\\mathbf{A}})^{2}\\kappa_{Q}\\mathrm{e}}{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "To complete the proof it remains to set the parameter $h$ in such a way that we can guarantee ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathrm{Ca}\\,\\sqrt{\\kappa_{Q}}\\sqrt{\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}^{2}}\\,\\bigg(1+2\\log(2n^{4})\\bigg)+\\frac{\\alpha_{m+1}^{2}h^{2}\\,\\mathrm{C}_{\\bf A}^{2}\\,\\kappa_{Q}\\mathrm{e}}{2}\\leq\\frac{a}{4}\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}\\;,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "keeping at the same time the constraint ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}\\leq\\alpha_{\\infty}\\;.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Recall that $\\alpha_{\\ell}=c_{0}/\\sqrt{\\ell}$ . Thus, using the bounds (73) and (74), we obtain that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{a}{4}\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}\\geq\\frac{a c_{0}}{2}(\\sqrt{m+h}-\\sqrt{m+1})\\geq\\frac{a c_{0}}{2}(\\sqrt{m+h}-\\sqrt{m})\\;,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}^{2}=\\sum_{\\ell=m+1}^{m+h}\\frac{c_{0}^{2}}{\\ell}\\leq c_{0}^{2}(\\log\\left(m+h\\right)-\\log m)\\;.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Hence, taking into accut (79 and (8), ad $\\textstyle{\\frac{1}{m+1}}\\leq{\\frac{1}{m}}$ the inequality 77) wouldfllowfrom the bound ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathrm{C_{A}}\\,\\sqrt{\\kappa_{Q}}\\sqrt{\\log(m+h)-\\log(m)}\\bigg(1+2\\log(2n^{4})\\bigg)+\\frac{c_{0}h^{2}\\,\\mathrm{C_{A}^{2}}\\,\\kappa_{Q}\\mathrm{e}}{2m}\\leq\\frac{a}{2}(\\sqrt{m+h}-\\sqrt{m})\\,.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Since $\\log\\left(1+x\\right)\\leq x$ for $x\\geq0$ and $c_{0}\\,\\mathrm{C}_{\\mathbf{A}}^{2}\\,\\kappa_{Q}\\mathrm{e}\\leq1$ , the latter inequality is satisfied if ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathrm{Ca}\\;\\sqrt{\\kappa_{Q}}\\frac{\\sqrt{h}}{\\sqrt{m}}\\Bigg(1+2\\log(2n^{4})\\Bigg)+\\frac{h^{2}}{2m}\\leq\\frac{a}{2}(\\sqrt{m+h}-\\sqrt{m})\\;.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Now we use one more lower bound ", "page_idx": 42}, {"type": "equation", "text": "$$\n{\\sqrt{m+h}}-{\\sqrt{m}}={\\sqrt{m}}({\\sqrt{1+h/m}}-1)\\geq{\\frac{{\\sqrt{m}}({\\sqrt{2}}-1)h}{m}}={\\frac{({\\sqrt{2}}-1)h}{\\sqrt{m}}}\\;,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which follows from an elementary inequality $\\sqrt{1+x}\\geq1+(\\sqrt{2}-1)x$ , valid for $0\\leq x\\leq1$ .Hence, (81) would from the inequality ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathrm{Ca}\\,\\sqrt{\\kappa_{Q}}\\frac{\\sqrt{h}}{\\sqrt{m}}\\Bigg(1+2\\log(2n^{4})\\Bigg)+\\frac{h^{2}}{2m}\\leq\\frac{a(\\sqrt{2}-1)h}{2\\sqrt{m}}\\ .\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Setting $h$ is such a manner that ", "page_idx": 42}, {"type": "equation", "text": "$$\n{\\frac{h}{\\sqrt{m}}}\\leq{\\frac{a(\\sqrt{2}-1)}{2}}\\;,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "inequality (82) would follow from ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathrm{Ca}\\,\\sqrt{\\kappa_{Q}}\\frac{\\sqrt{h}}{\\sqrt{m}}\\biggl(1+2\\log(2n^{4})\\biggr)\\leq\\frac{a(\\sqrt{2}-1)h}{4\\sqrt{m}}\\ .\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The latter inequality is satisfied, if the block size $h$ satisfies ", "page_idx": 42}, {"type": "equation", "text": "$$\nh\\geq\\bigg(\\frac{4\\,\\mathrm{C}_{\\bf A}\\,\\kappa_{Q}^{1/2}}{(\\sqrt{2}-1)a}\\bigg)^{2}(1+2\\log\\,(2n^{4}))^{2}\\;.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Thus, setting $h(n)$ as in (17), all previous inequalities will be fulfilled, provided that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\frac{h(n)}{\\sqrt{n}}\\right.\\ \\ \\leq\\frac{a(\\sqrt{2}-1)}{2}}\\\\ {\\frac{c_{0}h(n)}{\\sqrt{n}}\\ \\ \\leq\\alpha_{\\infty}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Here last inequality follows from (78) and the following simple bounds, where we use that $m\\geq n$ and $\\sqrt{1+x}\\ \\overset{\\,.}{\\leq}1+x/2$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}\\leq\\sum_{\\ell=n+1}^{n+h}\\alpha_{\\ell}=c_{0}\\sum_{\\ell=n+1}^{n+h}\\frac{1}{\\sqrt{\\ell}}\\leq c_{0}\\int_{n}^{n+h}\\frac{d x}{\\sqrt{x}}=2c_{0}(\\sqrt{n+h}-\\sqrt{n})\\leq\\frac{c_{0}h}{\\sqrt{n}}\\,.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Now (76) implies that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\big\\{\\mathbb{E}^{\\flat}[\\|\\Gamma_{m+1:m+h}^{\\flat}\\|_{Q}^{2}]\\big\\}^{1/2}\\leq1-(a/4)\\sum_{\\ell=m+1}^{m+h}\\alpha_{\\ell}\\;,\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and the statement follows from an elementary inequality $1+x\\leq\\mathrm{e}^{x}$ ", "page_idx": 43}, {"type": "text", "text": "E Applications to the TD learning ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Recall that the temporal difference learning algorithm in the LSA's setting can be written as ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\theta_{k}=\\theta_{k-1}-\\alpha_{k}(\\mathbf{A}_{k}\\theta_{k-1}-\\mathbf{b}_{k})\\mathrm{~,~}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where ${\\bf A}_{k}$ and $\\mathbf{b}_{k}$ are given by ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\bf A}_{k}=\\varphi(s_{k})\\{\\varphi(s_{k})-\\gamma\\varphi(s_{k}^{\\prime})\\}^{\\top}\\;,}\\\\ {{\\bf b}_{k}=\\varphi(s_{k})r(s_{k},a_{k})\\;.\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Recall that our aim is to estimate the agent's value function ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V^{\\pi}(s)=\\mathbb{E}[\\sum_{k=0}^{\\infty}\\gamma^{k}r(s_{k},a_{k})|s_{0}=s]\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $a_{k}\\sim\\pi(\\cdot|s_{k})$ , and $s_{k+1}\\sim\\mathrm{P}(\\cdot|s_{k},a_{k})$ , for any $k\\in\\mathbb{N}$ We define the transition kernel under policy $\\pi$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{P}_{\\pi}(B|s)=\\int_{\\cal A}\\mathrm{P}(B|s,a)\\pi(\\mathrm{d}a|s)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "which corresponds to the 1-step transition probability from state $s$ to a set $B\\in B(S)$ : We denote by $\\mu$ the invariant distribution over the state space $\\boldsymbol{S}$ induced by the transition kernel $\\operatorname{P}_{\\pi}(\\cdot|s)$ in (85). In this case the TD learning updates (83) correspond to the approximate solution of the deterministic system $\\bar{\\mathbf{A}}\\theta^{\\star}=\\bar{\\mathbf{b}}$ , where we have set, respectively, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\bar{\\mathbf{A}}=\\mathbb{E}_{s\\sim\\mu,s^{\\prime}\\sim\\mathrm{P}_{\\pi}(\\cdot|s)}[\\varphi(s)\\{\\varphi(s)-\\gamma\\varphi(s^{\\prime})\\}^{\\top}]}\\\\ {\\bar{\\mathbf{b}}=\\mathbb{E}_{s\\sim\\mu,a\\sim\\pi(\\cdot|s)}[\\varphi(s)r(s,a)]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "E.1Proof of Proposition 2 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We first need to check that the matrix $\\bar{\\mathbf{A}}+\\bar{\\mathbf{A}}^{\\top}$ ,where $\\bar{\\mathbf{A}}$ is defined in (86), is positive-definite. In order to show this fact we closely follow the exposition of [61, Lemma 18] and [51, Lemma 5]. Define a random matrix $\\mathbf{A}$ as an independent copy of ${\\bf A}_{k}$ from (84), that is, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbf{A}=\\varphi(s)\\{\\varphi(s)-\\gamma\\varphi(s^{\\prime})\\}^{\\top}\\ ,\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $s\\sim\\mu$ , and $s^{\\prime}\\sim\\mathrm{P}_{\\pi}(\\cdot|s)$ . With the definition of $\\mathbf{A}$ , we get that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{A}+\\mathbf{A}^{\\top}=\\varphi(s)\\{\\varphi(s)-\\gamma\\varphi(s^{\\prime})\\}^{\\top}+\\{\\varphi(s)-\\gamma\\varphi(s^{\\prime})\\}\\varphi(s)^{\\top}}\\\\ &{\\qquad\\qquad=2\\varphi(s)\\varphi(s)^{\\top}-\\gamma\\{\\varphi(s)\\varphi(s^{\\prime})^{\\top}+\\varphi(s^{\\prime})\\varphi(s)^{\\top}\\}}\\\\ &{\\qquad\\qquad\\succeq(2-\\gamma)\\varphi(s)\\varphi(s)^{\\top}-\\gamma\\varphi(s^{\\prime})\\varphi(s^{\\prime})^{\\top}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where we used an elementary inequality $u v^{\\top}+v u^{\\top}\\preceq(u u^{\\top}+v v^{\\top})$ valid for any $u,v\\in\\mathbb{R}^{d}$ . Hence, with the definition of $\\Sigma_{\\varphi}$ in (21), we get ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{A}}+\\bar{\\mathbf{A}}^{\\top}=\\mathbb{E}[\\mathbf{A}+\\mathbf{A}^{\\top}]\\succeq2(1-\\gamma)\\Sigma_{\\varphi}\\ .\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Hence, $\\bar{\\mathbf{A}}+\\bar{\\mathbf{A}}^{\\top}$ is positive-definite, and we can set $P=\\bar{\\mathbf{A}}+\\bar{\\mathbf{A}}^{\\top}$ in the right-hand side of the Lyapunov equation (65). Obviously, $Q=\\mathrm{I}$ is a solution to the corresponding Lyapunov equation ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{A}}^{\\top}Q+Q\\bar{\\mathbf{A}}=\\bar{\\mathbf{A}}+\\bar{\\mathbf{A}}^{\\top}\\ .\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Moreover, applying [61, Lemma 18], we obtain ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{A}}^{\\top}\\bar{\\mathbf{A}}\\preceq\\mathbb{E}[\\mathbf{A}^{\\top}\\mathbf{A}]\\preceq(1+\\gamma)^{2}\\Sigma_{\\varphi}\\ .\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Hence, we get for $\\alpha\\leq(1-\\gamma)/(1+\\gamma)^{2}$ , and applying (87) and (88), that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathrm{I}-\\alpha\\bar{\\mathbf{A}})^{\\top}(\\mathrm{I}-\\alpha\\bar{\\mathbf{A}})=\\mathrm{I}-\\alpha(\\bar{\\mathbf{A}}^{\\top}+\\bar{\\mathbf{A}})+\\alpha^{2}\\bar{\\mathbf{A}}^{\\top}\\bar{\\mathbf{A}}}\\\\ &{\\qquad\\qquad\\qquad\\preceq\\mathrm{I}-2\\alpha(1-\\gamma)\\Sigma_{\\varphi}+\\alpha^{2}(1+\\gamma)^{2}\\Sigma_{\\varphi}}\\\\ &{\\qquad\\qquad\\qquad\\preceq\\mathrm{I}-\\alpha(1-\\gamma)\\Sigma_{\\varphi}}\\\\ &{\\qquad\\qquad\\qquad\\preceq(1-\\alpha(1-\\gamma)\\lambda_{\\operatorname*{min}}(\\Sigma_{\\varphi}))\\mathrm{I}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Hence, the bound (8) holds with $a=(1-\\gamma)\\lambda_{\\operatorname*{min}}(\\Sigma_{\\varphi})$ and $\\alpha_{\\infty}=(1-\\gamma)/(1+\\gamma)^{2}$ ", "page_idx": 44}, {"type": "text", "text": "F Experimental details for the TD learning ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Here we provide some details on numerical experiments. Code to run experiments is provided in https: //github.com/svsamsonov/BootstrapLSA. For the considered Garnet problem we choosethepolicy $\\pi$ in the following way. For any $a\\in A$ ,weset ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\pi(a|s)=\\frac{U_{a}^{(s)}}{\\sum_{i=1}^{|A|}U_{i}^{(s)}}\\;,\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the $U_{i}^{(s)}$ are independent random variables following uniform distribution $\\mathcal{U}[0,1]$ .Here we assume that each action $a\\in A$ can be selected at any state $\\bar{s}\\in\\{1,\\ldots,N_{s}\\}$ . We generate an instance of Garnet problem with mentioned parameters, and find analytically the true parameter $\\theta^{\\star}$ . In order to estimate the supremum ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{n}:=\\operatorname*{sup}_{x\\in\\mathbb{R}}\\mathopen{}\\mathclose\\bgroup\\left|\\mathbb{P}(\\sqrt{n}\\|\\bar{\\theta}_{n}-\\theta^{\\star}\\|\\leq x)-\\mathbb{P}(\\|\\Sigma_{\\infty}^{1/2}\\eta\\|\\leq x)\\aftergroup\\egroup\\right|~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "$\\eta\\sim\\mathcal{N}(0,\\mathrm{I}_{N_{s}})$ , and show that this supremum scales as $n^{-1/4}$ when $\\gamma=1/2$ and admits slower decay for other powers of $\\gamma$ We first approximate true probability $\\mathbb{P}(\\|\\Sigma_{\\infty}^{1/2}\\eta\\|\\leq x)$ by the corresponding empirical probabilities based on sample of size $M\\gg n$ . We fix $M=2\\cdot10^{7}$ . We choose trajectory lengths ", "page_idx": 44}, {"type": "equation", "text": "$$\nn\\in\\{1600,3200,6400,12800,25600,51200,102400,204800,409600,819200,1638400\\}\\;.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "fix the length of burn-in period $n_{0}=102400$ , and generate $N=1638400$ independent trajectories starting in the fixed point $\\boldsymbol{\\theta}_{0}\\,\\in\\,\\mathbb{R}^{N_{s}}$ .We set the learning rate schedule as $\\alpha_{k}\\,=\\,c_{0}/k^{\\gamma}$ and try different values $\\gamma\\in\\{0.5,0.65,0.7,0.75\\}$ . In Figure 2 we provide respective plots for the rescaled RMSE ${\\sqrt{n}}\\|{\\bar{\\theta}}_{n}-{\\theta}^{\\star}\\|$ and its non-rescaled version $\\lVert\\bar{{\\boldsymbol{\\theta}}}_{n}-{\\boldsymbol{\\theta}}^{\\star}\\rVert$ , averaged over $N$ trajectories. Then in Figure 3 we provide the expressions for $\\Delta_{n}$ and $\\Delta_{n}n^{1/4}$ , both in log-scale for y axis and without log-scaling. Unfortunately, even the chosen order of trajectory length $n$ seems to be insufficient in order to significantly distinguish, for example, between $\\gamma=0.5$ and $\\gamma=0.65$ . However, learning rate schedule with faster decay performs consistently worse in terms of $\\Delta_{n}$ . The order of lines in Figure 3- (b) is remarkable. However, note that the current experiment is already rather computationally intense for artificial problem and takes about 12 hours of compute on a Core i9 - 10920x processor with 12 cores with $3.7\\:\\mathrm{GHz}$ (for a bunch of $N=1638400$ trajectories of length $n=1638400_{c}$ ", "page_idx": 44}, {"type": "image", "img_path": "S0Ci1AsJL5/tmp/5378d887b301ae005186f94882875d06db85a29f8d857653dde22261912a13ef.jpg", "img_caption": [], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "Figure 2: Subfigure (a): Rescaled error ${\\sqrt{n}}\\|{\\bar{\\theta}}_{n}-\\theta^{\\star}\\|$ , averaged over $N$ independent TD trajectories for different trajectory lengths $n$ . Subfigure (b): same for $\\|\\tilde{\\theta_{n}}-\\theta^{\\star}\\|$ ", "page_idx": 45}, {"type": "image", "img_path": "S0Ci1AsJL5/tmp/1d87d06c3868a88ed79b9abc2723964c1b6ee26855f8ff9fa2b991d7fa4ac02f.jpg", "img_caption": ["Figure 3: Subfigure (a): Rescaled error $\\Delta_{n}n^{1/4}$ . Subfigure (b): same with logarithmic scale on $y$ axis. "], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Justification: Main results are, respectively, the ones of Theorem 2 and Theorem 3, their statements are complete and supported by the proofs in the Appendix section. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 46}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Justification: We discuss the limitations of our setting related to the LSA problem, and not more general non-linear stochastic optimisation problems. We highlight the potential generalizations of Theorem 2 to the non-linear setting and discuss why generalizing Theorem 3 might be more challenging. We also discuss the limitation related to i.i.d. observations. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 46}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: All our theoretical results are provided with references to assumptions, that are stated in Section 3 and Section 4. All results are given with proofs, that are correctly referenced for each theorem and corollary. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 47}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: Numerical results are stated with a complete description of the environments that are used, as well as the precise sets of hyperparameters that we used. The code (in Python) is provided as supplementary with the paper, making it easy for one to reproduce our numerical experiments. At the same time, tracing the second-order terms in the normal approximation is computationally involved and can take sufficiently large amount of time. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct thedataset). ", "page_idx": 47}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 48}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: All code is open source, link to a github repository is included. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 48}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: The algorithm used in the numerical experiments are exactly the algorithms described in the paper. The Garnet environements are given with the parameters used for generation, and with reference to the original problem. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 48}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 48}, {"type": "text", "text": "Answer: [No] ", "page_idx": 48}, {"type": "text", "text": "Justification: Unfortunately, error bars for computing the second-order terms in normal approximation are quite computationally intense, moreover, tracing the terms of order $n^{1/4}$ requires quick increase of trajectory length $n$ ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 49}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: All necessary information to reproduce experiments is provided in Appendix F. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 49}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: This paper is of purely theoretical nature, and the proposed methods do not deal with sensitive attributes that could induce unfairness or privacy issues. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 49}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: This paper is of purely theoretical nature. We do not foresee any societal harm from the proof of non-asymptotic bootstrap validity and normal approximation bounds in Kolmogorov distance. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 50}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Justification: Not applicable. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 50}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: Not applicable: no existing assets are used. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: \u00b7 The answer NA means that the paper does not use existing assets. ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 51}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: Not applicable: paper does not release new assets. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 51}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: Not applicable: paper does not involve crowdsourcing nor research on human subjects. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 51}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 51}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)wereobtained? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: Not applicable: paper does not involve crowdsourcing nor research on human subjects. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 52}]