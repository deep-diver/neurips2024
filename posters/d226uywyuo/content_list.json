[{"type": "text", "text": "Knowledge Graph Completion by Intermediate Variables Regularization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Changyi Xiao, Yixin Cao\u2217 School of Computer Science, Fudan University changyi_xiao@fudan.edu.cn, caoyixin2011@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Knowledge graph completion (KGC) can be framed as a 3-order binary tensor completion task. Tensor decomposition-based (TDB) models have demonstrated strong performance in KGC. In this paper, we provide a summary of existing TDB models and derive a general form for them, serving as a foundation for further exploration of TDB models. Despite the expressiveness of TDB models, they are prone to overftiting. Existing regularization methods merely minimize the norms of embeddings to regularize the model, leading to suboptimal performance. Therefore, we propose a novel regularization method for TDB models that addresses this limitation. The regularization is applicable to most TDB models and ensures tractable computation. Our method minimizes the norms of intermediate variables involved in the different ways of computing the predicted tensor. To support our regularization method, we provide a theoretical analysis that proves its effect in promoting low trace norm of the predicted tensor to reduce overfitting. Finally, we conduct experiments to verify the effectiveness of our regularization technique as well as the reliability of our theoretical analysis. The code is available at https://github.com/changyi7231/IVR. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A knowledge graph (KG) can be represented as a 3rd-order binary tensor, in which each entry corresponds to a triplet of the form (head entity, relation, tail entity). A value of 1 denotes a known true triplet, while 0 denotes a false triplet. Despite containing a large number of known triplets, KGs are often incomplete, with many triplets missing. Consequently, the 3rd-order tensors representing the KGs are incomplete. The objective of knowledge graph completion (KGC) is to infer the true or false values of the missing triplets based on the known ones, i.e., to predict which of the missing entries in the tensor are 1 or 0. ", "page_idx": 0}, {"type": "text", "text": "A number of models have been proposed for KGC, which can be classified into translation-based models, tensor decomposition-based (TDB) models and neural networks models [Zhang et al., 2021]. We only focus on TDB models in this paper due to their wide applicability and great performance [Lacroix et al., 2018, Zhang et al., 2020]. TDB models can be broadly categorized into two groups: CANDECOMP/PARAFAC (CP) decomposition-based models, including CP [Lacroix et al., 2018], DistMult [Yang et al., 2014] and ComplEx [Trouillon et al., 2017], and Tucker decomposition-based models, including SimplE [Kazemi and Poole, 2018], ANALOGY [Liu et al., 2017], QuatE [Zhang et al., 2019] and TuckER [Bala\u017eevi\u00b4c et al., 2019]. To provide a thorough understanding of TDB models, we present a summary of existing models and derive a general form that unifies them, which provides a fundamental basis for further exploration of TDB models. ", "page_idx": 0}, {"type": "text", "text": "TDB models have been proven to be theoretically fully expressive [Trouillon et al., 2017, Kazemi and Poole, 2018, Bala\u017eevi\u00b4c et al., 2019], implying they can represent any real-valued tensor. However, in practice, TDB models frequently fall prey to severe overfitting. To counteract this issue, various regularization techniques have been employed in KGC. One commonly used technique is the squared Frobenius norm regularization [Nickel et al., 2011, Yang et al., 2014, Trouillon et al., 2017]. Lacroix et al. [2018] proposed another regularization, N3 norm regularization, based on the tensor nuclear $p$ -norm, which outperforms the squared Frobenius norm in terms of performance. Additionally, Zhang et al. [2020] introduced DURA, a regularization technique based on the duality of TDB models and distance-based models that results in significant improvements on benchmark datasets. Nevertheless, N3 and DURA rely on CP decomposition, limiting their applicability to CP [Lacroix et al., 2018] and ComplEx [Trouillon et al., 2017]. As a result, there is a pressing need for a regularization technique that is widely applicable and can effectively alleviate the overfitting issue. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce a novel regularization method for KGC to improve the performance of TDB models. Our regularization focuses on preventing overfitting while maintaining the expressiveness of TDB models as much as possible. It is applicable to most TDB models, while also ensuring tractable computation. Existing regularization methods for KGC rely on minimizing the norms of embeddings to regularize the model [Yang et al., 2014, Lacroix et al., 2018], leading to suboptimal performance. To achieve superior performance, we present an intermediate variables regularization (IVR) approach that minimizes the norms of intermediate variables involved in the processes of computing the predicted tensor of TDB models. Additionally, our approach fully considers the computing ways because different ways of computing the predicted tensor may generate different intermediate variables. ", "page_idx": 1}, {"type": "text", "text": "To support the efficacy of our regularization approach, we further provide a theoretical analysis. We prove that our regularization is an upper bound of the overlapped trace norm [Tomioka et al., 2011]. The overlapped trace norm is the sum of the trace norms of the unfolding matrices along each mode of a tensor [Kolda and Bader, 2009], which can be considered as a surrogate measure of the rank of a tensor. Thus, the overlapped trace norm reflects the correlation among entities and relations, which can pose a constraint to jointly entities and relations embeddings learning. In specific, entities and relations in KGs are usually highly correlated. For example, some relations are mutual inverse relations, or a relation may be a composition of another two relations [Zhang et al., 2021]. Through minimizing the upper bound of the overlapped trace norm, we encourage a high correlation among entities and relations, which brings strong regularization and alleviates the overfitting problem. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of this paper are listed below: ", "page_idx": 1}, {"type": "text", "text": "1. We present a detailed overview of a wide range of TDB models and establish a general form to serve as a foundation for further TDB model analysis.   \n2. We introduce a new regularization approach for TDB models based on the general form to mitigate the overfitting issue, which is notable for its generality and effectiveness.   \n3. We provide a theoretical proof of the efficacy of our regularization and validate its practical utility through experiments. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Tensor Decomposition Based Models Research in KGC has been vast, with TDB models garnering attention due to their superior performance. The two primary TDB approaches that have been extensively studied are CP decomposition [Hitchcock, 1927] and Tucker decomposition [Tucker, 1966]. CP decomposition represents a tensor as a sum of $n$ rank-one tensors, while Tucker decomposition decomposes a tensor into a core tensor and a set of matrices. Kolda and Bader [2009] shows more details about CP decomposition and Tucker decomposition. ", "page_idx": 1}, {"type": "text", "text": "Several techniques have been developed for applying CP decomposition and Tucker decomposition in KGC. For instance, Lacroix et al. [2018] employed the original CP decomposition, whereas DistMult [Yang et al., 2014], a variant of CP decomposition, made the embedding matrices of head entities and tail entities identical to simplify the model. SimplE [Kazemi and Poole, 2018] tackled the problem of independence among the embeddings of head and tail entities within CP decomposition. ComplEx [Trouillon et al., 2017] extended DistMult to the complex space to handle asymmetric relations. QuatE [Zhang et al., 2019] explored hypercomplex space to further enhance KGC. Other techniques include HolE [Nickel et al., 2016] proposed by Nickel et al. [2016], which operates on circular correlation, and ANALOGY [Liu et al., 2017], which explicitly utilizes the analogical structures of KGs. Notably, Liu et al. [2017] confirmed that HolE is equivalent to ComplEx. Additionally, Bala\u017eevi\u00b4c et al. [2019] introduced TuckER, which is based on the Tucker decomposition and has achieved state-of-the-art performance across various benchmark datasets for KGC. Nickel et al. [2011] proposed a three-way decomposition RESCAL over each relational slice of the tensor. You can refer to [Zhang et al., 2021] or [Ji et al., 2021] for more detailed discussion about KGC models or TDB models. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Regularization Although TDB models are highly expressive [Trouillon et al., 2017, Kazemi and Poole, 2018, Bala\u017eevi\u00b4c et al., 2019], they can suffer severely from overfitting in practice. Consequently, several regularization approaches have been proposed. A common regularization approach is to apply the squared Frobenius norm to the model parameters [Nickel et al., 2011, Yang et al., 2014, Trouillon et al., 2017]. However, this approach does not correspond to a proper tensor norm, as shown by Lacroix et al. [2018]. Therefore, they proposed a novel regularization method, N3, based on the tensor nuclear 3-norm, which is an upper bound of the tensor nuclear norm. Likewise, Zhang et al. [2020] introduced DURA, a regularization method that exploits the duality of TDB models and distance-based models, and serves as an upper bound of the tensor nuclear 2-norm. However, both N3 and DURA are derived from the CP decomposition, and thus are only applicable to CP and ComplEx models. ", "page_idx": 2}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In Section 3.1, we begin by providing an overview of existing TDB models and derive a general form for them. Thereafter, we present our intermediate variables regularization (IVR) approach in Section 3.2. Finally, in Section 3.3, we provide theoretical analysis to support the efficacy of our proposed regularization technique. ", "page_idx": 2}, {"type": "text", "text": "3.1 General Form ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To facilitate the subsequent theoretical analysis, we initially provide a summary of existing TDB models and derive a general form for them. Given a set of entities $\\mathcal{E}$ and a set of relations $\\mathcal{R}$ , a KG contains a set of triplets $S=\\{(i,j,k)\\}\\subset\\mathcal{E}\\times\\mathcal{R}\\times\\mathcal{E}.$ . Let $X\\in\\{0,1\\}^{|\\mathcal{E}|\\times|\\mathcal{R}|\\times|\\mathcal{E}|}$ represent the KG tensor, with $X_{i j k}=1$ iff $(i,j,k)\\in\\mathcal{S}$ , where $|\\mathcal{E}|$ and $|{\\mathcal{R}}|$ denote the number of entities and relations, respectively. Let $H\\in\\mathbb{R}^{|\\mathcal{E}|\\times D},R\\in\\mathbb{R}^{|\\mathcal{R}|\\times D}$ and $\\pmb{T}\\in\\mathbb{R}^{|\\mathcal{E}|\\times D}$ be the embedding matrices of head entities, relations and tail entities, respectively, where $D$ is the embedding dimension. ", "page_idx": 2}, {"type": "text", "text": "Various TDB models can be attained by partitioning the embedding matrices into $P$ parts. We reshape $H\\in\\mathbb{R}^{|\\mathcal{E}|\\times D}$ , $R\\in\\mathbb{R}^{|\\mathcal{R}|\\times D}$ and $\\pmb{T}\\in\\bar{\\mathbb{R}}^{|\\mathcal{E}|\\times D}$ into $H\\in\\mathbb{R}^{|\\mathcal{E}|\\times(\\check{D}/P)\\times P}$ , $R\\in\\mathbb{R}^{|\\dot{\\mathcal{R}}|\\times(D/P)\\times P}$ and $\\pmb{T}\\in\\mathbb{R}^{|\\mathcal{E}|\\times(D/P)\\times P}$ , respectively, where $P$ is the number of parts we partition. For different $P$ , we can get different TDB models. ", "page_idx": 2}, {"type": "text", "text": "CP/DistMult Let $P=1$ , CP [Lacroix et al., 2018] can be represented as ", "page_idx": 2}, {"type": "equation", "text": "$$\nX_{i j k}=\\langle H_{i:1},R_{j:1},T_{k:1}\\rangle:=\\sum_{d=1}^{D/P}H_{i d1}R_{j d1}T_{k d1}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\langle\\cdot,\\cdot,\\cdot\\rangle$ is the dot product of three vectors. DistMult [Yang et al., 2014], a particular case of CP, which shares the embedding matrices of head entities and tail entities, i.e., $H=T$ . ", "page_idx": 2}, {"type": "text", "text": "ComplEx/HolE Let $P=2$ , ComplEx [Trouillon et al., 2017] can be represented as ", "page_idx": 2}, {"type": "equation", "text": "$$\nX_{i j k}=\\langle H_{i:1},R_{j:1},T_{k:1}\\rangle+\\langle H_{i:2},R_{j:1},T_{k:2}\\rangle+\\langle H_{i:1},R_{j:2},T_{k:2}\\rangle-\\langle H_{i:2},R_{j:2},T_{k:1}\\rangle\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Liu et al. [2017] proved that HolE [Nickel et al., 2011] is equivalent to ComplEx. ", "page_idx": 2}, {"type": "text", "text": "SimplE Let $P=2$ , SimplE [Kazemi and Poole, 2018] can be represented as ", "page_idx": 2}, {"type": "equation", "text": "$$\nX_{i j k}=\\langle H_{i:1},R_{j:1},T_{k:2}\\rangle+\\langle H_{i:2},R_{j:2},T_{k:1}\\rangle\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "ANALOGY Let $P=4$ , ANALOGY [Liu et al., 2017] can be represented as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{X_{i j k}=\\langle H_{i:1},R_{j:1},T_{k:1}\\rangle+\\langle H_{i:2},R_{j:2},T_{k:2}\\rangle+\\langle H_{i:3},R_{j:3},T_{k:3}\\rangle+\\langle H_{i:3},R_{j:4},T_{k:4}\\rangle}\\\\ &{\\qquad+\\left\\langle H_{i:4},R_{j:3},T_{k:4}\\right\\rangle-\\langle H_{i:4},R_{j:4},T_{k:3}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "QuatE Let $P=4$ , QuatE [Zhang et al., 2019] can be represented as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{X_{i j k}=\\langle H_{i:1},R_{j:1},T_{k:1}\\rangle-\\langle H_{i:2},R_{j:2},T_{k:1}\\rangle-\\langle H_{i:3},R_{j:3},T_{k:1}\\rangle-\\langle H_{i:4},R_{j:4},T_{k:1}\\rangle}\\\\ {+\\left\\langle H_{i:1},R_{j:2},T_{k:2}\\right\\rangle+\\langle H_{i:2},R_{j:1},T_{k:2}\\rangle+\\langle H_{i:3},R_{j:4},T_{k:2}\\rangle-\\langle H_{i:4},R_{j:3},T_{k:2}\\rangle}\\\\ {+\\left\\langle H_{i:1},R_{j:3},T_{k:3}\\right\\rangle-\\langle H_{i:2},R_{j:4},T_{k:3}\\rangle+\\langle H_{i:3},R_{j:1},T_{k:3}\\rangle+\\langle H_{i:4},R_{j:2},T_{k:3}\\rangle}\\\\ {+\\left\\langle H_{i:1},R_{j:4},T_{k:4}\\right\\rangle+\\langle H_{i:2},R_{j:3},T_{k:4}\\rangle-\\langle H_{i:3},R_{j:2},T_{k:4}\\rangle+\\langle H_{i:4},R_{j:1},T_{k:4}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "TuckER Let $P=D$ , TuckER [Bala\u017eevic\u00b4 et al., 2019] can be represented as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{X}_{i j k}=\\sum_{l=1}^{P}\\sum_{m=1}^{P}\\sum_{n=1}^{P}W_{l m n}\\pmb{H}_{i1l}\\pmb{R}_{j1m}\\pmb{T}_{k1n}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\boldsymbol{W}\\in\\mathbb{R}^{P\\times P\\times P}$ is the core tensor. ", "page_idx": 3}, {"type": "text", "text": "General Form Through our analysis, we observe that all TDB models can be expressed as a linear combination of several dot product. The key distinguishing factors among these models are the choice of the number of parts $P$ and the core tensor $W$ . The number of parts $P$ determines the dimensions of the dot products of the embeddings, while the core tensor $W$ determines the strength of the dot products. It is important to note that TuckER uses a parameter tensor as its core tensor, whereas the core tensors of other models are predetermined constant tensors. Therefore, we can derive a general form of these models as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle X_{i j k}=\\sum_{l=1}^{P}\\sum_{m=1}^{P}\\sum_{n=1}^{P}W_{l m n}\\langle H_{i\\lambda},R_{j;m},T_{k;n}\\rangle=\\sum_{l=1}^{P}\\sum_{m=1}^{P}\\sum_{n=1}^{P}W_{l m n}(\\sum_{d=1}^{D/P}H_{i d l}R_{j d m}T_{k d n})}}\\\\ {{\\displaystyle=\\sum_{d=1}^{D/P}(\\sum_{l=1}^{P}\\sum_{m=1}^{P}W_{l m n}H_{i d l}R_{j d m}T_{k d n})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "or ", "page_idx": 3}, {"type": "equation", "text": "$$\nX=\\sum_{d=1}^{D/P}W\\times_{1}H_{:d:}\\times_{2}R_{:d:}\\times_{3}T_{:d:}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\times_{n}$ is the mode- $^{\\cdot n}$ product [Kolda and Bader, 2009], and $\\boldsymbol{W}\\in\\mathbb{R}^{P\\times P\\times P}$ is the core tensor, which can be a parameter tensor or a predetermined constant tensor. The general form Eq.(2) can also be considered as a sum of ${\\cal D}/{\\cal P}$ TuckER decompositions, which is also called block-term decomposition [De Lathauwer, 2008]. Eq.(2) is a block-term decomposition with a shared core tensor $W$ . This general form is easy to understand, facilitates better understanding of TDB models and paves the way for further exploration of TDB models. The general form presents a unified view of TDB models and helps the researchers understand the relationship between different TDB models. Moreover, the general form motivates the researchers to propose new methods and establish unified theoretical frameworks that are applicable to most TDB models. Our proposed regularization in Section 3.2 and the theoretical analysis Section 3.3 are examples of such contributions. ", "page_idx": 3}, {"type": "text", "text": "The Number of Parameters and Computational Complexity The parameters of Eq.(2) come from two parts, the core tensor $W$ and the embedding matrices $H,R$ and $\\textbf{\\emph{T}}$ . The number of parameters of the core tensor $W$ is equal to $P^{3}$ if $W$ is a parameter tensor and otherwise equal to 0. The number of parameters of the embedding matrices is equal to $|\\mathcal{E}|D+|\\mathcal{R}|D$ if $H=T$ and otherwise equal to $\\bar{2}|\\mathcal{E}|D+|\\mathcal{R}|D$ . The computational complexity of Eq.(2) is equal to ${\\mathcal{O}}(D P^{2}|{\\mathcal{E}}|^{2}|{\\mathcal{R}}|)$ . The larger the number of parts $P$ , the more expressive the model and the more the computation. Therefore, the choice of $P$ is a trade-off between expressiveness and computation. ", "page_idx": 3}, {"type": "image", "img_path": "d226uyWYUo/tmp/2345346bcdd2868ecea4fc38d00f799f94722575903596edba3fe0195b75452b.jpg", "img_caption": ["Figure 1: Left shows a 3rd order tensor. Middle describes the corresponding mode- $^{\\,i}$ fibers of the tensor. Fibers are the higher-order analogue of matrix rows and columns. A fiber is defined by fixing every index but one. Right describes the corresponding mode- $^{i}$ unfolding of the tensor. The mode- $i$ unfolding of a tensor arranges the mode- $^{\\,.\\,i}$ fibers to be the columns of the resulting matrix. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "TuckER and Eq.(2) TuckER [Bala\u017eevic\u00b4 et al., 2019] also demonstrated that TDB models can be represented as a Tucker decomposition by setting specific core tensors $W$ . Nevertheless, we must stress that TuckER does not explicitly consider the number of parts $P$ and the core tensor $W$ , which are pertinent to the number of parameters and computational complexity of TDB models. Moreover, in Appendix A, we demonstrate that the conditions for a TDB model to learn logical rules are also dependent on $P$ and $W$ . By selecting appropriate $P$ and $W$ , TDB models can be able to learn symmetry rules, antisymmetry rules, and inverse rules. ", "page_idx": 4}, {"type": "text", "text": "3.2 Intermediate Variables Regularization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "TDB models are theoretically fully expressive [Trouillon et al., 2017, Kazemi and Poole, 2018, Bala\u017eevic\u00b4 et al., 2019], which can represent any real-valued tensor. However, TDB models suffer from the overfitting problem in practice [Lacroix et al., 2018]. Therefore, several regularization methods have been proposed, such as squared Frobenius norm method [Yang et al., 2014] and nuclear 3-norm method [Lacroix et al., 2018], which minimize the norms of the embeddings $\\{H,R,T\\}$ to regularize the model. Nonetheless, merely minimizing the embeddings tends to have suboptimal impacts on the model performance. To enhance the model performance, we introduce a new regularization method that minimizes the norms of the intermediate variables involved in the processes of computing $\\mathbf{\\deltaX}$ . To ensure the broad applicability of our method, our regularization is rooted in the general form of TDB models Eq.(2). ", "page_idx": 4}, {"type": "text", "text": "To compute $\\mathbf{\\deltaX}$ in Eq.(2), we can first compute the intermediate variable $W\\times_{1}H_{:d:}\\times_{2}R_{:d:}$ , and then combine ${\\cal T}_{:d}$ : to compute $\\mathbf{\\deltaX}$ . Thus, in addition to minimizing the norm of ${\\cal T}_{:d:}$ , we also need to minimize the norm of $W\\times_{1}H_{:d:}\\times_{2}R_{:d:}$ . Since different ways of computing $\\mathbf{\\deltaX}$ can result in different intermediate variables, we fully consider the computing ways of $\\mathbf{\\deltaX}$ . Eq.(2) can also be written as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle{\\vphantom{\\int}}}\\\\ {\\displaystyle{\\vphantom{\\int}}}\\\\ {\\displaystyle{\\vphantom{\\int}}}\\\\ {\\displaystyle{\\vphantom{\\int}}}\\\\ {\\displaystyle{\\vphantom{\\int}}}\\end{array}\\left({\\pmb{W}}\\times_{2}{\\pmb{R}}_{:d:}\\times_{3}{\\pmb{T}}_{:d:}\\right)\\times_{1}{\\pmb{H}}_{:d:},}\\\\ {\\displaystyle{\\vphantom{\\int}}}\\\\ {\\displaystyle{\\pmb{X}}=\\sum_{d=1}^{D/P}\\left({\\pmb{W}}\\times_{3}{\\pmb{T}}_{:d:}\\times_{1}{\\pmb{H}}_{:d:}\\right)\\times_{2}{\\pmb{R}}_{:d:}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Thus, we also need to minimize the norms of intermediate variables $\\{W\\times_{2}R_{:d:}\\times_{3}T_{:d:},W\\times_{3}$ $\\pi_{:d:}\\times_{1}H_{:d:}\\}$ and $\\{H_{:d:},R_{:d:}\\}$ . In summary, we should minimize the (power of Frobenius) norms $\\{||H_{:d:}||_{F}^{\\alpha},||R_{:d:}||_{F}^{\\alpha},||T_{:d:}||_{F}^{\\alpha}\\}$ and $\\{||W\\times_{1}\\dot{H}_{:d:}\\times_{2}R_{:d:}||_{F}^{\\alpha},||W\\times_{2}R_{:d:}\\times_{3}T_{:d:}||_{F}^{\\alpha},||W\\times_{3}T_{:d:}\\times_{1}$ $H_{:d:}||\\mathcal{\\alpha}\\}$ , where $\\alpha$ is the power of the norms. ", "page_idx": 4}, {"type": "text", "text": "Since computing $\\mathbf{\\deltaX}$ is equivalent to computing $X_{(1)}$ or $X_{(2)}$ or $\\boldsymbol{X}_{(3)}$ , we can also minimize the norms of intermediate variables involved in the processes of computing $\\mathbf{\\boldsymbol{X}}_{(1)},\\mathbf{\\boldsymbol{X}}_{(2)}$ and $X_{(3)}$ , where $X_{(n)}$ is the mode- ${\\cdot n}$ unfolding of a tensor $\\mathbf{\\deltaX}$ [Kolda and Bader, 2009]. See Figure 1 for an example of the notation $X_{(n)}$ . We can represent $X_{(1)},X_{(2)}$ and $\\boldsymbol{X}_{(3)}$ as [Kolda and Bader, 2009]: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle{\\pmb X_{(1)}=\\sum_{d=1}^{D/P}(\\pmb W\\times_{1}\\pmb H;_{d:})_{(1)}(\\pmb T;d\\pmb\\mathscr{B}\\pmb R_{:d:})^{T},}}\\\\ {\\displaystyle{\\pmb X_{(2)}=\\sum_{d=1}^{D/P}(\\pmb W\\times_{2}\\pmb R_{:d:})_{(2)}(\\pmb T;d\\pmb\\mathscr{B}\\pmb H_{:d:})^{T},}}\\\\ {\\displaystyle{\\pmb X_{(3)}=\\sum_{d=1}^{D/P}(\\pmb W\\times_{3}\\pmb T;_{d:})_{(3)}(\\pmb R;_{d:}\\otimes\\pmb H_{:d:})^{T}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\otimes$ is the Kronecker product. Thus, the intermediate variables include $\\{W\\times_{1}H_{:d:},W\\times_{2}$ $R_{:d:},W\\times_{3}T_{:d:}\\right\\}$ and $\\{T_{:d:}\\otimes{\\cal R}_{:d:},T_{:d:}\\otimes{\\cal H}_{:d:},{\\cal R}_{:d:}\\otimes{\\cal H}_{:d:}\\}$ . Therefore, we should minimize the (power of Frobenius) norm $\\textstyle\\{\\|\\boldsymbol{W}\\!\\times\\!_{1}\\boldsymbol{H}_{:d:}\\|_{F}^{\\alpha},\\|\\boldsymbol{W}\\!\\times\\!_{2}\\boldsymbol{R}_{:d:}\\|_{F}^{\\alpha},\\|$ $\\|\\mathbf{\\hat{W}}\\!\\times\\!_{3}\\mathbf{Z}_{:d:}\\|_{F}^{\\alpha}\\right\\}$ and $\\{\\|\\pmb{T}_{:d:}\\otimes\\pmb{R}_{:d:}\\|_{F}^{\\alpha}=$ $\\|T_{:d:}\\|_{F}^{\\alpha}\\|R_{:d:}\\|_{F}^{\\alpha},\\|T_{:d:}\\otimes H_{:d:}\\|_{F}^{\\alpha}=\\|T_{:d:}\\|_{F}^{\\alpha}\\|H_{:d:}\\|_{F}^{\\alpha},\\|R_{:d:}\\otimes H_{:d:}\\|_{F}^{\\alpha}=\\|R_{:d:}\\|_{F}^{\\alpha}\\|H_{:d:}\\|_{F}^{\\alpha}\\}.$ ", "page_idx": 5}, {"type": "text", "text": "Our Intermediate Variables Regularization (IVR) is defined as a combination of all these norms: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{reg}(\\boldsymbol{X})=\\displaystyle\\sum_{d=1}^{D/P}\\lambda_{1}(\\|\\boldsymbol{H};\\boldsymbol{d};\\|_{F}^{\\alpha}+\\|\\boldsymbol{R}_{:d:}\\|_{F}^{\\alpha}+\\|\\boldsymbol{T}_{:d:}\\|_{F}^{\\alpha})}\\\\ &{\\,\\,+\\lambda_{2}(\\|\\boldsymbol{T}_{:d:}\\|_{F}^{\\alpha}\\|\\boldsymbol{R}_{:d:}\\|_{F}^{\\alpha}+\\|\\boldsymbol{T}_{:d:}\\|_{F}^{\\alpha}\\|\\boldsymbol{H}_{:d:}\\|_{F}^{\\alpha}+\\|\\boldsymbol{R}_{:d:}\\|_{F}^{\\alpha}\\|\\boldsymbol{H}_{:d:}\\|_{F}^{\\alpha})}\\\\ &{\\,\\,+\\lambda_{3}(\\|\\boldsymbol{W}\\times\\boldsymbol{\\mathrm{x}}_{1}\\boldsymbol{H}_{:d:}\\|_{F}^{\\alpha}+\\|\\boldsymbol{W}\\times\\boldsymbol{\\mathrm{z}}_{2}\\boldsymbol{R}_{:d:}\\|_{F}^{\\alpha}+\\|\\boldsymbol{W}\\times\\boldsymbol{\\mathrm{3}}\\boldsymbol{T}_{:d:}\\|_{F}^{\\alpha})}\\\\ &{\\,\\,+\\lambda_{4}(\\|\\boldsymbol{W}\\times\\boldsymbol{\\mathrm{2}}\\boldsymbol{R}_{:d:}\\times\\boldsymbol{\\mathrm{3}}\\boldsymbol{T}_{:d:}\\|_{F}^{\\alpha}+\\|\\boldsymbol{W}\\times\\boldsymbol{\\mathrm{3}}\\boldsymbol{T}_{:d:}\\times_{1}\\boldsymbol{H}_{:d:}\\|_{F}^{\\alpha}+\\|\\boldsymbol{W}\\times\\boldsymbol{\\mathrm{1}}\\boldsymbol{H}_{:d:}\\times_{2}\\boldsymbol{R}_{:d:}\\|_{F}^{\\alpha})}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\{\\lambda_{i}>0|i=1,2,3,4\\}$ are the regularization coefficients. ", "page_idx": 5}, {"type": "text", "text": "In conclusion, our proposed regularization term is the sum of the norms of variables involved in the different ways of computing the tensor $\\mathbf{\\deltaX}$ . ", "page_idx": 5}, {"type": "text", "text": "We can easily get the weighted version of Eq.(3), in which the regularization term corresponding to the sampled training triplets only [Lacroix et al., 2018, Zhang et al., 2020]. For a training triplet $(i,j,k)$ , the weighted version of Eq.(3) is as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathrm{reg}(X_{i j k})=\\sum_{d=1}^{D/P}\\lambda_{1}(\\|H_{i d:}\\|_{F}^{\\alpha}+\\|R_{j d:}\\|_{F}^{\\alpha}+\\|T_{k d:}\\|_{F}^{\\alpha})}\\\\ &{\\displaystyle+\\lambda_{2}(\\|T_{k d:}\\|_{F}^{\\alpha}\\|R_{j d:}\\|_{F}^{\\alpha}+\\|T_{k d:}\\|_{F}^{\\alpha}\\|H_{i d:}\\|_{F}^{\\alpha}+\\|R_{j d:}\\|_{F}^{\\alpha}\\|H_{i d:}\\|_{F}^{\\alpha})}\\\\ &{\\displaystyle+\\lambda_{3}(\\|W\\times_{1}H_{i d:}\\|_{F}^{\\alpha}+\\|W\\times_{2}H_{j d:}\\|_{F}^{\\alpha}+\\|W\\times_{3}T_{k d:}\\|_{F}^{\\alpha})}\\\\ &{\\displaystyle+\\lambda_{4}(\\|W\\times_{2}R_{j d:}\\times_{3}T_{k d:}\\|_{F}^{\\alpha}+\\|W\\times_{3}T_{k d:}\\times_{1}H_{i d:}\\|_{F}^{\\alpha}+\\|W\\times_{1}H_{i d:}\\times_{2}R_{j d:}\\|_{F}^{\\alpha})}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The computational complexity of Eq.(4) is the same as that of Eq.(1), i.e., $\\mathcal{O}(D P^{2})$ , which ensures that our regularization is computationally tractable. ", "page_idx": 5}, {"type": "text", "text": "The hyper-parameters $\\lambda_{i}$ make IVR scalable. We can easily reduce the number of hyper-parameters by setting some of them zero or equal. The hyper-parameters make us able to achieve a balance between performance and efficiency as shown in Section 4.3. We set $\\lambda_{1}=\\lambda_{3}$ and $\\lambda_{2}=\\lambda_{4}$ for all models to reduce the number of hyper-parameters. You can refer to Appendix $\\mathbf{C}$ for more details about the setting of hyper-parameters. ", "page_idx": 5}, {"type": "text", "text": "We use the same loss function, multiclass log-loss function, as in [Lacroix et al., 2018]. For a training triplet $(i,j,k)$ , our loss function is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\ell(\\pmb{X}_{i j k})=-\\pmb{X}_{i j k}+\\log(\\sum_{k^{'}=1}^{|\\mathcal{E}|}\\exp(\\pmb{X}_{i j k^{'}}))+\\mathrm{reg}(\\pmb{X}_{i j k})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "At test time, we use $X_{i,j,}$ : to rank tail entities for a query $(i,j,?)$ . ", "page_idx": 5}, {"type": "text", "text": "3.3 Theoretical Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To support the effectiveness of our regularization IVR, we provide a deeper theoretical analysis of its properties. The establishment of the theoretical framework of IVR is inspired by Lemma 1 in ", "page_idx": 5}, {"type": "text", "text": "Appendix B, which relates the Frobenius norm and the trace norm of a matrix. Lemma 1 shows that the trace norm of a matrix is an upper bound of a function of several Frobenius norms of intermediate variables, which prompts us to establish the relationship between IVR and trace norm. Based on Lemma 1, we prove that IVR serves as an upper bound for the overlapped trace norm of the predicted tensor, which promotes the low nuclear norm of the predicted tensor to regularize the model. ", "page_idx": 6}, {"type": "text", "text": "The overlapped trace norm [Kolda and Bader, 2009] for a 3rd-order tensor is defined as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nL(\\boldsymbol{X};\\alpha):=\\|\\boldsymbol{X}_{(1)}\\|_{*}^{\\alpha/2}+\\|\\boldsymbol{X}_{(2)}\\|_{*}^{\\alpha/2}+\\|\\boldsymbol{X}_{(3)}\\|_{*}^{\\alpha/2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\alpha$ is the power coefficient in Eq.(3). $\\|X_{(1)}\\|_{*},\\|X_{(2)}\\|_{*}$ and $\\left\\Vert X_{(3)}\\right\\Vert_{*}$ are the matrix trace norms of $\\underline{{X}}_{(1)},X_{(2)}$ and $\\boldsymbol{X}_{(3)}$ , respectively, which are the sums of singular values of the respective matrices. The matrix trace norm is widely used as a convex surrogate for matrix rank due to the non-differentiability of matrix rank [Goldfarb and Qin, 2014, Lu et al., 2016, Mu et al., 2014]. Thus, $L(X;\\alpha)$ serves as a surrogate for $\\mathrm{rank}(X_{(1)})^{\\alpha/2}+\\mathrm{rank}(X_{(2)})^{\\alpha/2}+\\mathrm{rank}(X_{(3)})^{\\alpha/2}$ , where $\\operatorname{rank}(X_{(1)})$ , $\\operatorname{rank}(X_{(2)})$ and $\\operatorname{rank}(X_{(3)})$ are the matrix ranks of $X_{(1)},X_{(2)}$ and $\\boldsymbol{X}_{(3)}$ , respectively. In KGs, each head entity, each relation and each tail entity uniquely corresponds to a row of $X_{(1)},X_{(2)}$ and $\\boldsymbol{X}_{(3)}$ , respectively. Therefore, $\\operatorname{rank}(X_{(1)}),\\operatorname{rank}(X_{(2)})$ and $\\operatorname{rank}(X_{(3)})$ measure the correlation among the head entities, relations and tail entities, respectively. Entities or relations in KGs are highly correlated. For instance, some relations are mutual inverse relations or one relation may be a composition of another two relations [Zhang et al., 2021]. Thus, the overlapped trace norm $L(X;\\alpha)$ can pose a constraint to the embeddings of entities and relations. Minimizing $L(X;\\alpha)$ encourage a high correlation among entities and relations, which brings strong regularization and reduces overfitting. We next establish the relationship between our regularization term Eq.(3) and $L(X;\\alpha)$ by Proposition 1 and Proposition 2. We will prove that Eq.(3) is an upper bound of $L(X;\\alpha)$ . ", "page_idx": 6}, {"type": "text", "text": "Proposition 1. For any $\\mathbf{\\deltaX}$ , and for any decomposition of $\\mathbf{\\deltaX}$ , $\\begin{array}{r}{\\pmb{X}=\\sum_{d=1}^{D/P}\\pmb{W}\\times_{1}\\pmb{H}_{:d:}\\times_{2}\\pmb{R}_{:d:}\\times_{3}\\pmb{T}_{:d:}}\\end{array}$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\sqrt{\\lambda_{1}\\lambda_{4}}L(X;\\alpha)\\le\\displaystyle\\sum_{d=1}^{D/P}\\lambda_{1}(\\|H_{:d:}\\|_{F}^{\\alpha}+\\|R_{:d:}\\|_{F}^{\\alpha}+\\|T_{:d:}\\|_{F}^{\\alpha})}\\\\ &{\\ +\\lambda_{4}(\\|W\\times_{2}R_{:d:}\\times_{3}T_{:d:}\\|_{F}^{\\alpha}+\\|W\\times_{3}T_{:d:}\\times_{1}H_{:d:}\\|_{F}^{\\alpha}+\\|W\\times_{1}H_{:d:}\\times_{2}R_{:d:}\\|_{F}^{\\alpha})}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "If $\\mathbf{\\Psi}^{\\cdot}X_{(1)}=U_{1}\\Sigma_{1}V_{1}^{T}$ , $X_{(2)}=U_{2}\\Sigma_{2}V_{2}^{T}$ , $X_{(3)}=U_{3}\\Sigma_{3}V_{3}^{T}$ are compact singular value decompositions of $X_{(1)},X_{(2)},X_{(3)}$ [Bai et al., 2000], respectively, then there exists a decomposition of $\\mathbf{\\deltaX}$ , $\\begin{array}{r}{\\pmb{X}=\\sum_{d=1}^{D/P}\\pmb{W}\\times_{1}\\pmb{H}_{:d:}\\times_{2}\\pmb{R}_{:d:}\\times_{3}\\pmb{T}_{:d:}}\\end{array}$ , such that the two sides of Eq.(5) equal. ", "page_idx": 6}, {"type": "text", "text": "Proposition 2. For any $\\mathbf{\\deltaX}$ , and for any decomposition of $\\mathbf{\\deltaX}$ , $\\begin{array}{r}{\\pmb{X}=\\sum_{d=1}^{D/P}\\pmb{W}\\times_{1}\\pmb{H}_{:d:}\\times_{2}\\pmb{R}_{:d:}\\times_{3}\\pmb{T}_{:d:}}\\end{array}$ we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\sqrt{\\lambda_{2}\\lambda_{3}}L(X)\\leq\\displaystyle\\sum_{d=1}^{D/P}\\lambda_{2}(\\|T_{:d:}\\|_{F}^{\\alpha}\\|R_{:d:}\\|_{F}^{\\alpha}+\\|T_{:d:}\\|_{F}^{\\alpha}\\|H_{:d:}\\|_{F}^{\\alpha}+\\|R_{:d:}\\|_{F}^{\\alpha}\\|H_{:d:}\\|_{F}^{\\alpha})}\\\\ &{\\,+\\,\\lambda_{3}(\\|W\\times_{1}H_{:d:}\\|_{F}^{\\alpha}+\\|W\\times_{2}R_{:d:}\\|_{F}^{\\alpha}+\\|W\\times_{3}T_{:d:}\\|_{F}^{\\alpha})}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "And there exists some $X^{'}$ , and for any decomposition of $X^{'}$ , such that the two sides of Eq.(6) can not achieve equality. ", "page_idx": 6}, {"type": "text", "text": "Please refer to Appendix \u221aB for the proofs. Proposition 1 establishes that the r.h.s. of Eq.(5) provides a tight upper bound f\u221aor $2\\sqrt{\\lambda_{1}\\lambda_{4}}L(\\mathbf{\\bar{X}};\\alpha)$ , while Proposition 2 demonstrates that the r.h.s. of Eq.(4) is an upper bound of $2\\sqrt{\\lambda_{2}\\lambda_{3}}L(X;\\alpha)$ , but this bound is not always tight. Our proposed regularization term, Eq.(3), combines these two upper limits by adding the r.h.s. of Eq.(5) and the r.h.s. of Eq.(6). As a result, minimizing Eq.(3) can effectively minimize $L(X;\\alpha)$ to regularize the model. ", "page_idx": 6}, {"type": "text", "text": "The two sides of Eq.(5) can achieve equality if $P=D$ , meaning that the TDB model is TuckER model [Bala\u017eevi\u00b4c et al., 2019]. Although $L(X;\\alpha)$ may not always serve as a tight lower bound of the r.h.s. of Eq.(5) for TDB models other than TuckER model, it remains a common lower bound for all TDB models. To obtain a more tight lower bound, the exact values of $P$ and $W$ are required. For example, in the case of CP model [Lacroix et al., 2018] ( $P=1$ and $W=1$ ), the nuclear 2-norm $\\|X\\|_{*}$ is a more tight lower bound. The nuclear 2-norm is defined as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|X\\|_{*}:=\\operatorname*{min}\\{\\sum_{d=1}^{D}\\|H_{:d1}\\|_{F}\\|R_{:d1}\\|_{F}\\|T_{:d1}\\|_{F}|X=\\sum_{d=1}^{D}W\\times_{1}H_{:d1}\\times_{2}R_{:d1}\\times_{3}T_{:d1}\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "table", "img_path": "d226uyWYUo/tmp/a1ebab337a7c4031d947e9bf83921fd860a2011847f284e8f042162d4eef158a.jpg", "table_caption": ["Table 1: Knowledge graph completion results on WN18RR, FB15k-237 and YGAO3-10 datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "where $W=1$ . The following proposition establishes the relationship between $\\|X\\|_{*}$ and $L(X;2)$ : Proposition 3. For any $\\mathbf{\\deltaX}$ , and for any decomposition of $\\mathbf{\\deltaX}$ , $\\begin{array}{r}{\\pmb{X}=\\sum_{d=1}^{D}W\\times_{1}\\pmb{H}_{:d1}\\times_{2}\\pmb{R}_{:d1}\\times_{3}\\pmb{T}_{:d1},}\\end{array}$ , and $W=1$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\sqrt{\\lambda_{1}\\lambda_{4}}L({\\pmb X};2)\\leq6\\sqrt{\\lambda_{1}\\lambda_{4}}\\|{\\pmb X}\\|_{*}\\leq\\displaystyle\\sum_{d=1}^{D}\\lambda_{1}(\\|{\\pmb H}_{:d1}\\|_{F}^{2}+\\|{\\pmb R}_{:d1}\\|_{F}^{2}+\\|{\\pmb T}_{:d1}\\|_{F}^{2a})}\\\\ &{+\\lambda_{4}(\\|{\\pmb W}\\times_{2}{\\pmb R}_{:d1}\\times_{3}{\\pmb T}_{:d1}\\|_{F}^{2}+\\|{\\pmb W}\\times_{3}{\\pmb T}_{:d1}\\times_{1}{\\pmb H}_{:d1}\\|_{F}^{2}+\\|{\\pmb W}\\times_{1}{\\pmb H}_{:d1}\\times_{2}{\\pmb R}_{:d1}\\|_{F}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Although the r.h.s. of Eq.(6) is not always a tight upper bound for $2\\sqrt{\\lambda_{2}\\lambda_{3}}L(X;\\alpha)$ like the r.h.s. of Eq.(5), we observe that minimizing the combination of these two bounds, Eq.(3), can lead to better performance. The reason behind this is that the r.h.s. of Eq.(5) is neither an upper bound nor a lower bound of the r.h.s. of Eq.(6) for all $\\mathbf{\\deltaX}$ . We present Proposition 4 in Appendix $\\mathbf{B}$ to prove this claim. ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We first introduce the experimental settings in Section 4.1 and show the results in Section 4.2. We next conduct ablation studies in Section 4.3. Finally, we verify the reliability of our proposed upper bounds in Section 4.4. Please refer to Appendix C for more experimental details. ", "page_idx": 7}, {"type": "text", "text": "4.1 Experimental Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets We evaluate the models on three KGC datasets, WN18RR [Dettmers et al., 2018], FB15k237 [Toutanova et al., 2015] and YAGO3-10 [Dettmers et al., 2018]. ", "page_idx": 7}, {"type": "text", "text": "Models We use CP, ComplEx, SimplE, ANALOGY, QuatE and TuckER as baselines. We denote CP with squared Frobenius norm method [Yang et al., 2014] as CP-F2, CP with N3 method [Lacroix ", "page_idx": 7}, {"type": "table", "img_path": "d226uyWYUo/tmp/01c1e221a2369496a0bb8cefc17f368b7742fa354ba5eb93231c39f7510ed7cf.jpg", "table_caption": ["Table 2: The results on WN18RR and FB15k-237 datasets with different upper bounds. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "d226uyWYUo/tmp/9c377539b3c20dd46b78a78244ad93c2ad07101a2e65e7bf8193a6be59fa59d7.jpg", "table_caption": ["Table 3: The results on Kinship dataset with different upper bounds. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "et al., 2018] as CP-N3, CP with DURA method [Zhang et al., 2020] as CP-DURA and CP with IVR method as CP-IVR. The notations for other models are similar to the notations for CP. ", "page_idx": 8}, {"type": "text", "text": "Evaluation Metrics We use the filtered MRR and Hits $\\mathbb{\\@N}$ $(\\mathrm{H@N})$ [Bordes et al., 2013] as evaluation metrics and choose the hyper-parameters with the best flitered MRR on the validation set. We run each model three times with different random seeds and report the mean results. ", "page_idx": 8}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "See Table 1 for the results. For CP and ComplEx, the models that N3 and DURA are suitable, the results show that N3 enhances the models more than F2, and DURA outperforms both F2 and N3, leading to substantial improvements. IVR achieves better performance than DURA on WN18RR dataset and achieves similar performance to DURA on FB15k-237 and YAGO3-10 dataset. For SimplE, ANALOGY, QuatE, and TuckER, the improvement offered by F2 is minimal, while IVR significantly boosts model performance. In summary, these results demonstrate the effectiveness and generality of IVR. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct ablation studies to examine the effectiveness of the upper bounds. Our notations for the models are as follows: the model with upper bound Eq.(5) is denoted as IVR-1, model with upper bound Eq.(6) as IVR-2, and model with upper bound Eq.(3) as IVR. ", "page_idx": 8}, {"type": "text", "text": "See Table 2 for the results. We use TuckER as the baseline. IVR with only 1 regularization coefficient, IVR-1, achieves comparable results with vanilla IVR, which shows that IVR can still perform well with fewer hyper-parameters. IVR-1 outperforms IVR-2 due to the tightness of Eq.(5). ", "page_idx": 8}, {"type": "text", "text": "4.4 Upper Bounds ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We verify that minimizing the upper bounds can effectively minimize $L(X;\\alpha)$ . $L(X;\\alpha)$ can measure the correlation of $\\mathbf{\\deltaX}$ . Lower values of $L(X;\\alpha)$ encourage higher correlations among entities and relations, and thus bring a strong constraint for regularization. Upon training the models, we compute $L(X;\\alpha)$ . As computing $L(X;\\alpha)$ for large KGs is impractical, we conduct experiments on a small KG dataset, Kinship [Kok and Domingos, 2007], which consists of 104 entities and 25 relations. We use TuckER as the baseline and compare it against IVR-1 (Eq.(5)), IVR-2 (Eq.(6)), and IVR (Eq.(3)). ", "page_idx": 8}, {"type": "text", "text": "See Table 3 for the results. Our results demonstrate that the upper bounds are effective in minimizing $L(X;\\alpha)$ . All three upper bounds can lead to a decrease of $L(X;\\alpha)$ , achieving better performance (Table 2) by more effective regularization. The $L(X;\\alpha)$ of IVR-1 is smaller than that of IVR-2 because the upper bound in Eq.(5) is tight. IVR, which combines IVR-1 and IVR-2, produces the most reduction of $L(X;\\alpha)$ . This finding suggests that combining the two upper bounds can be more effective. Overall, our experimental results confirm the reliability of our theoretical analysis. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we undertake an analysis of TDB models in KGC. We first offer a summary of TDB models and derive a general form that facilitates further analysis. TDB models often suffer from overftiting, and thus, we propose a regularization based on our derived general form. It is applicable to most TDB models and improve the model performance. We further propose a theoretical analysis to support our regularization and experimentally validate our theoretical analysis. Our regularization is limited to TDB models, hoping that more regularization will be proposed in other types of models, such as translation-based models and neural networks models. We also intend to explore how to apply our regularization to other fields, such as tensor completion [Song et al., 2019]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the National Key Research and Development Program of China (2020AAA0106000), the National Natural Science Foundation of China (U19A2079), and the CCCD Key Lab of Ministry of Culture and Tourism. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Zhaojun Bai, James Demmel, Jack Dongarra, Axel Ruhe, and Henk van der Vorst. Templates for the solution of algebraic eigenvalue problems: a practical guide. SIAM, 2000. ", "page_idx": 9}, {"type": "text", "text": "Ivana Bala\u017eevic\u00b4, Carl Allen, and Timothy Hospedales. Tucker: Tensor factorization for knowledge graph completion. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5185\u20135194, 2019.   \nJames Bergstra, R\u00e9mi Bardenet, Yoshua Bengio, and Bal\u00e1zs K\u00e9gl. Algorithms for hyper-parameter optimization. Advances in neural information processing systems, 24, 2011.   \nAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. Advances in neural information processing systems, 26, 2013.   \nCarlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Reexamining low rank matrix factorization for trace norm regularization. arXiv preprint arXiv:1706.08934, 2017.   \nLieven De Lathauwer. Decompositions of a higher-order tensor in block terms\u2014part i: Lemmas for partitioned matrices. SIAM Journal on Matrix Analysis and Applications, 30(3):1022\u20131032, 2008.   \nTim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge graph embeddings. In Thirty-second AAAI conference on artificial intelligence, 2018.   \nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011.   \nDonald Goldfarb and Zhiwei Qin. Robust low-rank tensor recovery: Models and algorithms. SIAM Journal on Matrix Analysis and Applications, 35(1):225\u2013253, 2014.   \nFrank L Hitchcock. The expression of a tensor or a polyadic as a sum of products. Journal of Mathematics and Physics, 6(1-4):164\u2013189, 1927.   \nShaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE transactions on neural networks and learning systems, 33(2):494\u2013514, 2021.   \nSeyed Mehran Kazemi and David Poole. Simple embedding for link prediction in knowledge graphs. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 4289\u20134300, 2018.   \nStanley Kok and Pedro Domingos. Statistical predicate invention. In Proceedings of the 24th international conference on Machine learning, pages 433\u2013440, 2007.   \nTamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3): 455\u2013500, 2009.   \nTimoth\u00e9e Lacroix, Nicolas Usunier, and Guillaume Obozinski. Canonical tensor decomposition for knowledge base completion. In International Conference on Machine Learning, pages 2863\u20132872. PMLR, 2018.   \nHanxiao Liu, Yuexin Wu, and Yiming Yang. Analogical inference for multi-relational embeddings. In International conference on machine learning, pages 2168\u20132178. PMLR, 2017.   \nCanyi Lu, Jiashi Feng, Yudong Chen, Wei Liu, Zhouchen Lin, and Shuicheng Yan. Tensor robust principal component analysis: Exact recovery of corrupted low-rank tensors via convex optimization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5249\u20135257, 2016.   \nCun Mu, Bo Huang, John Wright, and Donald Goldfarb. Square deal: Lower bounds and improved relaxations for tensor recovery. In International conference on machine learning, pages 73\u201381. PMLR, 2014.   \nMaximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning on multi-relational data. In Proceedings of the 28th International Conference on International Conference on Machine Learning, pages 809\u2013816, 2011.   \nMaximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. Holographic embeddings of knowledge graphs. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.   \nQingquan Song, Hancheng Ge, James Caverlee, and Xia Hu. Tensor completion algorithms in big data analytics. ACM Transactions on Knowledge Discovery from Data (TKDD), 13(1):1\u201348, 2019.   \nRyota Tomioka, Taiji Suzuki, Kohei Hayashi, and Hisashi Kashima. Statistical performance of convex tensor decomposition. Advances in neural information processing systems, 24, 2011.   \nKristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and Michael Gamon. Representing text for joint embedding of text and knowledge bases. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 1499\u20131509, 2015.   \nTh\u00e9o Trouillon, Christopher R Dance, \u00c9ric Gaussier, Johannes Welbl, Sebastian Riedel, and Guillaume Bouchard. Knowledge graph completion via complex tensor factorization. Journal of Machine Learning Research, 18:1\u201338, 2017.   \nLedyard R Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3): 279\u2013311, 1966.   \nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. arXiv e-prints, pages arXiv\u20131412, 2014.   \nJing Zhang, Bo Chen, Lingxi Zhang, Xirui Ke, and Haipeng Ding. Neural, symbolic and neuralsymbolic reasoning on knowledge graphs. AI Open, 2:14\u201335, 2021.   \nShuai Zhang, Yi Tay, Lina Yao, and Qi Liu. Quaternion knowledge graph embeddings. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 2735\u20132745, 2019.   \nZhanqiu Zhang, Jianyu Cai, and Jie Wang. Duality-induced regularizer for tensor factorization based knowledge graph completion. Advances in Neural Information Processing Systems, 33, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Logical Rules ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "KGs often involve some logical rules to capture inductive capacity [Zhang et al., 2021]. Thus, we analyze how to design models such that the models can learn the symmetry, antisymmetry and inverse rules. We have derived a general form for TDB models, Eq.(1). Next, we study how to enable Eq.(1) to learn logical rules. We first define the symmetry rules, antisymmetry rules and inverse rules. We denote $\\bar{\\mathbf{X}_{i j k}}=f(H_{i::},R_{j::},T_{k::})$ as $f(\\boldsymbol{h},\\boldsymbol{r},t)$ for simplicity. We assume ${\\pmb H}={\\pmb T}$ , which mainly aims to reduce overfitting Yang et al. [2014]. Existing TDB models for KGC except CP [Lacroix et al., 2018] all forces $H=T$ . ", "page_idx": 11}, {"type": "text", "text": "A relation $r$ is symmetric if $\\forall h,t,(h,r,t)\\,\\in\\,S\\,\\rightarrow\\,(t,r,h)\\,\\in\\,\\mathcal{S}$ . A model is able to learn the symmetry rules if ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\exists\\boldsymbol{r}\\in\\mathbb{R}^{D}\\land\\boldsymbol{r}\\neq0,\\forall\\boldsymbol{h},t\\in\\mathbb{R}^{D},f(\\boldsymbol{h},\\boldsymbol{r},t)=f(t,\\boldsymbol{r},\\boldsymbol{h})\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "A relation $r$ is antisymmetric if $\\forall h,t,(h,r,t)\\in\\mathcal{S}\\to(t,r,h)\\notin\\mathcal{S}$ . A model is able to learn the antisymmetry rules if ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\exists\\boldsymbol{r}\\in\\mathbb{R}^{D}\\land\\boldsymbol{r}\\neq0,\\forall\\boldsymbol{h},t\\in\\mathbb{R}^{D},f(\\boldsymbol{h},\\boldsymbol{r},t)=-f(t,\\boldsymbol{r},\\boldsymbol{h})\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "A relation $r_{1}$ is inverse to a relation $r_{2}$ if $\\forall h,t,(h,r_{1},t)\\in\\mathcal{S}\\to(t,r_{2},h)\\in\\mathcal{S}.$ . A model is able to learn the inverse rules if ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\forall\\pmb{r}_{1}\\in\\mathbb{R}^{D},\\exists\\pmb{r}_{2}\\in\\mathbb{R}^{D},\\forall\\pmb{h},t\\in\\mathbb{R}^{D},f(\\pmb{h},\\pmb{r}_{1},t)=f(\\pmb{t},\\pmb{r}_{2},\\pmb{h})\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "We restrict $r\\neq0$ because $\\pmb{r}=0$ will result in $f$ equal to an identically zero function. By choosing different $P$ and $W$ , we can define different TDB models as discussed in Section 3.1. Next, we give a theoretical analysis to establish the relationship between logical rules and TDB models. ", "page_idx": 11}, {"type": "text", "text": "Theorem 1. Assume a model can be represented as the form of $E q.(\\boldsymbol{l})$ , then a model is able to learn the symmetry rules iff $\\operatorname{rank}(W_{(2)}^{T}\\stackrel{\\cdot}{-}S W_{(2)}^{T})<P$ . A model is able to learn the antisymmetry rules iff $\\operatorname{rank}(W_{(2)}^{T}+S W_{(2)}^{T})<P$ . A model is able to learn the inverse rules iff $\\mathrm{rank}(W_{(2)}^{T})=$ $\\operatorname{rank}([W_{(2)}^{T},S W_{(2)}^{T}])$ , where $\\pmb{S}\\in\\mathbb{R}^{P^{2}\\times P^{2}}$ is a permutation matrix with $S_{(i-1)P+j,(j-1)P+i}\\,=$ $1(i,j\\,=\\,1,2,...\\,,P)$ and otherwise $\\boldsymbol{O}$ , $[W_{(2)}^{T},S W_{(2)}^{T}]$ is the concatenation of matrix $W_{(2)}^{T}$ and matrix $S W_{(2)}^{T}$ . ", "page_idx": 11}, {"type": "text", "text": "Proof. According to the symmetry rules, for any triplet $(i,j,k)$ , we have that $f(i,j,k)=f(k,j,i)$ , i.e., $f(\\mathbf{H}_{i:},\\mathbf{R}_{j:},\\mathbf{T}_{k:})\\,=\\,f(\\mathbf{H}_{k:},\\mathbf{R}_{j:},\\mathbf{T}_{i:})\\,=\\,f(\\mathbf{T}_{k:},\\mathbf{R}_{j:},\\mathbf{H}_{i:})$ (we use $\\mathbf{H}=\\mathbf{T}$ here). We replace $\\mathbf{H}_{i}$ : by $\\mathbf{h}$ , replace ${\\bf R}_{j}$ : by $\\mathbf{r}$ , replace ${\\bf T}_{k}$ : by $\\mathbf{t}$ , then we have $f({\\bf h},{\\bf r},{\\bf t})=f({\\bf t},{\\bf r},{\\bf h})$ . ", "page_idx": 11}, {"type": "text", "text": "Then we have that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{l=1}^{P}\\displaystyle\\sum_{m=1}^{P}\\sum_{n=1}^{P}W_{l m n}\\langle h_{l},r_{m},t_{n}\\rangle-\\displaystyle\\sum_{l=1}^{P}\\displaystyle\\sum_{m=1}^{P}W_{l m n}\\langle t_{l},r_{m},h_{n}\\rangle}\\\\ &{\\displaystyle=\\sum_{l=1}^{P}\\displaystyle\\sum_{m=1}^{P}\\sum_{n=1}^{P}W_{l m n}\\langle h_{l},r_{m},t_{n}\\rangle-\\displaystyle\\sum_{l=1}^{P}\\displaystyle\\sum_{m=1}^{P}\\sum_{n=1}^{P}W_{m n l}\\langle h_{l},r_{m},t_{n}\\rangle}\\\\ &{\\displaystyle=\\sum_{l=1}^{P}\\displaystyle\\sum_{m=1}^{P}\\sum_{n=1}^{P}(W_{l m n}-W_{m n l})\\langle h_{l},r_{m},t_{n}\\rangle}\\\\ &{\\displaystyle=\\sum_{l=1}^{P}\\displaystyle\\sum_{m=1}^{P}\\sum_{n=1}^{P}(W_{l m n}-W_{m n l})r_{m}^{T}(h_{l}*t_{n})}\\\\ &{\\displaystyle=\\sum_{l=1}^{P}\\displaystyle\\sum_{m=1}^{P}\\left(W_{l m n}-W_{m n l}\\right)r_{m}^{T}(h_{l}*t_{n})}\\\\ &{\\displaystyle=\\sum_{l=1}^{P}\\displaystyle\\sum_{m=1}^{P}\\sum_{n=1}^{P}(W_{l m n}-W_{m n l})r_{m}^{T}(h_{l}*t_{n})=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where $^*$ is the Hadamard product. Since the above equation holds for any $\\mathbf{h},\\pmb{t}\\in\\mathbb{R}^{D}$ , we can get ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\sum_{m=1}^{P}(W_{l m n}-W_{n m l})r_{:m}^{T}={\\bf0}(l,n=1,2,\\ldots,P)\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Therefore, a model is able to learn the symmetry rules iff ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\exists\\pmb{r}\\in\\mathbb{R}^{D}\\land\\pmb{r}\\neq0,\\sum_{m=1}^{P}(\\pmb{W}_{l m n}-\\pmb{W}_{n m l})\\pmb{r}_{:m}^{T}=\\mathbf{0}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Therefore, the symmetry rule is transformed into a system of linear equations. This system of linear equations have non-zero solution iff $\\mathrm{rank}(W_{(2)}^{T}-\\dot{S}W_{(2)}^{T})<P$ . Thus, a model is able to learn the symmetry rules iff ran $\\mathrm{1k}(W_{(2)}^{T}-S W_{(2)}^{T})<P$ . ", "page_idx": 12}, {"type": "text", "text": "Similarly, a model is able to learn the anti-symmetry rules iff $\\operatorname{rank}(W_{(2)}^{T}+S W_{(2)}^{T})<P,$ . For the inverse rule: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\forall\\pmb{r}_{1}\\in\\mathbb{R}^{D},\\exists\\pmb{r}_{2}\\in\\mathbb{R}^{D},\\forall\\pmb{h},t\\in\\mathbb{R}^{D},f(\\pmb{h},\\pmb{r}_{1},t)=f(\\pmb{t},\\pmb{r}_{2},\\pmb{h})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "a model is able to learn the symmetry rules iff the following equation ", "page_idx": 12}, {"type": "equation", "text": "$$\nW_{(2)}^{T}\\boldsymbol{r}_{1}=S W_{(2)}^{T}\\boldsymbol{r}_{2}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "for any $r_{1}\\in\\mathbb{R}^{D}$ , there exists $\\pmb{r}_{2}\\in\\mathbb{R}^{D}$ such that the equation holds. Thus, the column vectors of $W_{(2)}^{T}$ can be expressed linearly by the column vectors of $S W_{(2)}^{T}$ . Since $\\boldsymbol{S}$ is a permutation matrix and $S=S^{T}$ , we have that $S^{2}=I$ , thus ", "page_idx": 12}, {"type": "equation", "text": "$$\nS W_{(2)}^{T}r_{1}=S^{2}W_{(2)}^{T}r_{2}=W_{(2)}^{T}r_{2}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "For any $r_{1}\\in\\mathbb{R}^{D}$ , there exists $\\pmb{r}_{2}\\in\\mathbb{R}^{D}$ such that the above equation holds. Thus, the column vectors of $S W_{(2)}^{T}$ can be expressed linearly by the column vectors of $W_{(2)}^{T}$ . Therefore, the column space of $W_{(2)}^{T}$ is equivalent to the column space of $S W_{(2)}^{T}$ , thus we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname{rank}(W_{(2)}^{T})=\\operatorname{rank}(S W_{(2)}^{T})=\\operatorname{rank}([W_{(2)}^{T},S W_{(2)}^{T}])\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Meanwhile, if $\\operatorname{rank}(W_{(2)}^{T})=\\operatorname{rank}([W_{(2)}^{T},S W_{(2)}^{T}])$ , then the columns of (T2) can be expressed linearly by the columns of $S W_{(2)}^{T}$ , thus ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\forall\\pmb{r}_{1}\\in\\mathbb{R}^{D},\\exists\\pmb{r}_{2}\\in\\mathbb{R}^{D},W_{(2)}^{T}\\pmb{r}_{1}=\\pmb{S}\\pmb{W}_{(2)}^{T}\\pmb{r}_{2}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, a model is able to learn the inverse rules iff $\\operatorname{rank}(W_{(2)}^{T})=\\operatorname{rank}([W_{(2)}^{T},S W_{(2)}^{T}]).$ ", "page_idx": 12}, {"type": "text", "text": "By this theoerm, we only need to judge the relationship between $P$ and the matrix rank about $W_{(2)}$ . ComplEx, SimplE, ANALOGYY and QuatE design specific core tensors to make the models enable to learn the logical rules. We can easily verify that these models satisfy the conditions in this theorem. For example, for ComplEx, we have that $P=2$ and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{(2)}^{T}=\\left(\\begin{array}{l l}{1}&{0}\\\\ {0}&{-1}\\\\ {0}&{-1}\\\\ {1}&{0}\\end{array}\\right),S W_{(2)}^{T}=\\left(\\begin{array}{l l}{1}&{0}\\\\ {0}&{-1}\\\\ {0}&{1}\\end{array}\\right),[W_{(2)},S W_{(2)}^{T}]=\\left(\\begin{array}{l l l l}{1}&{0}&{1}&{0}\\\\ {0}&{1}&{0}&{-1}\\\\ {0}&{-1}&{0}&{1}\\\\ {1}&{0}&{1}&{0}\\end{array}\\right)}\\\\ &{\\mathrm{rank}(W_{(2)}^{T}-S W_{(2)}^{T})=\\mathrm{rank}\\Big(\\left(\\begin{array}{l l}{0}&{0}\\\\ {0}&{-2}\\\\ {0}&{0}\\end{array}\\right)=1<P=2}\\\\ &{\\mathrm{rank}(W_{(2)}^{T}+S W_{(2)}^{T})=\\mathrm{rank}\\Big(\\left(\\begin{array}{l l}{2}&{0}\\\\ {0}&{0}\\\\ {0}&{0}\\end{array}\\right)=1<P=2}\\\\ &{\\mathrm{rank}(W_{(2)}^{T})=\\mathrm{rank}([W_{(2)},S W_{(2)}^{T}])=2}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, ComplEx is able to learn the symmetry rules, antisymmetry rules and inverse rules. ", "page_idx": 12}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To prove the Proposition 1 and Proposition 2, we first prove the following lemma. ", "page_idx": 13}, {"type": "text", "text": "Lemma 1. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|Z\\|_{*}^{\\alpha}=\\operatorname*{min}_{Z=U V^{T}}\\frac{1}{2}(\\lambda\\|U\\|_{F}^{2\\alpha}+\\frac{1}{\\lambda}\\|V\\|_{F}^{2\\alpha})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\alpha\\;>\\;0,\\;\\lambda\\;>\\;0$ and $\\{Z,U,V\\}$ are real matrices. If ${Z}\\,=\\,\\hat{U}\\Sigma\\hat{V}^{T}$ is a singular value d\u221aecomposition of $Z$ , then equality holds for the choice $U=\\lambda^{\\frac{-1}{2\\alpha}}\\hat{U}\\sqrt{\\Sigma}$ and ${\\cal V}=\\lambda^{\\frac{1}{2\\alpha}}\\hat{\\cal V}\\sqrt{\\Sigma}$ , where $\\sqrt{\\Sigma}$ is the element-wise square root of $\\Sigma$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. The proof of Lemma 1 is based on the proof of Lemma 9 in [Ciliberto et al., 2017]. Let the singular value decomposition of $\\textbf{Z}\\in\\ \\mathbb{R}^{m\\times n}$ be ${z}\\:=\\:\\hat{U}\\Sigma\\hat{V}^{T}$ , where $\\hat{U}\\ \\in\\ \\mathbb{R}^{m\\times r},\\Sigma\\ \\in$ $\\mathbb{R}^{r\\times r},\\bar{\\hat{V}}\\,\\in\\mathbb{R}^{n\\times r},r\\,=\\,\\mathrm{rank}(Z),\\hat{U}^{T}\\hat{U}\\,=\\,I_{r\\times r}$ and $\\hat{V}^{T}\\hat{V}\\,=\\,I_{r\\times r}$ . We choose any $U,V$ such that $Z=U V^{T}$ , then we have $\\pmb{\\Sigma}=\\hat{U}^{T}U V^{T}\\hat{V}$ . Moreover, since $\\hat{U},\\hat{V}$ have orthogonal columns, $\\|\\hat{U}^{T}U\\|_{F}\\leq\\|U\\|_{F},\\|\\hat{V}^{T}V\\|_{F}\\leq\\|V\\|_{F}$ . Then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|Z\\|_{*}^{\\alpha}=\\mathrm{Tr}(\\Sigma)^{\\alpha}=\\mathrm{Tr}(\\hat{U}^{T}U V^{T}\\hat{V})^{\\alpha}\\leq\\|\\hat{U}^{T}U\\|_{F}^{\\alpha}\\|V\\hat{V}^{T}\\|_{F}^{\\alpha}}\\\\ &{\\leq(\\sqrt{\\lambda}\\|U\\|_{F}^{\\alpha})(\\frac{1}{\\sqrt{\\lambda}}\\|V\\|_{F}^{\\alpha})\\leq\\frac{1}{2}(\\lambda\\|U\\|_{F}^{2\\alpha}+\\frac{1}{\\lambda}\\|V\\|_{F}^{2\\alpha})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the first upper bound is Cauchy-Schwarz inequality and the third upper bound is AM-GM inequality. ", "page_idx": 13}, {"type": "text", "text": "Let $U=\\lambda^{\\frac{-1}{2\\alpha}}\\hat{U}\\sqrt{\\Sigma}$ and ${\\cal V}=\\lambda^{\\frac{1}{2\\alpha}}\\hat{\\cal V}\\sqrt{\\Sigma}$ , we have that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{2}(\\lambda\\|U\\|_{F}^{2\\alpha}+\\frac{1}{\\lambda}\\|V\\|_{F}^{2\\alpha})=\\frac{1}{2}(\\|\\hat{U}\\sqrt{\\Sigma}\\|_{F}^{2\\alpha}+\\|\\hat{V}\\sqrt{\\Sigma}\\|_{F}^{2\\alpha})=\\frac{1}{2}(\\|\\sqrt{\\Sigma}\\|_{F}^{2\\alpha}+\\|\\sqrt{\\Sigma}\\|_{F}^{2\\alpha})=\\|Z\\|_{*}^{\\alpha}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In summary, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|Z\\|_{*}^{\\alpha}=\\operatorname*{min}_{z=U V^{T}}\\frac{1}{2}(\\lambda\\|U\\|_{F}^{2\\alpha}+\\frac{1}{\\lambda}\\|V\\|_{F}^{2\\alpha})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proposition 1. For any $\\mathbf{\\deltaX}$ , and for any decomposition of $\\mathbf{\\deltaX}$ , $\\begin{array}{r}{\\pmb{X}=\\sum_{d=1}^{D/P}\\pmb{W}\\times_{1}\\pmb{H}_{:d:}\\times_{2}\\pmb{R}_{:d:}\\times_{3}\\pmb{T}_{:d:}}\\end{array}$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\sqrt{\\lambda_{1}\\lambda_{4}}L(X;\\alpha)\\le\\displaystyle\\sum_{d=1}^{D/P}\\lambda_{1}(\\|H_{:d:}\\|_{F}^{\\alpha}+\\|R_{:d:}\\|_{F}^{\\alpha}+\\|T_{:d:}\\|_{F}^{\\alpha})}\\\\ &{\\ +\\lambda_{4}(\\|W\\times_{2}R_{:d:}\\times_{3}T_{:d:}\\|_{F}^{\\alpha}+\\|W\\times_{3}T_{:d:}\\times_{1}H_{:d:}\\|_{F}^{\\alpha}+\\|W\\times_{1}H_{:d:}\\times_{2}R_{:d:}\\|_{F}^{\\alpha})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "If $X_{(1)}~=~U_{1}\\Sigma_{1}V_{1}^{T},X_{(2)}~=~U_{2}\\Sigma_{2}V_{2}^{T},X_{(3)}~=~U_{3}\\Sigma_{3}V_{3}^{T}$ are compact singular value decompositions of $X_{(1)},X_{(2)},X_{(3)}$ , respectively, then there exists a decomposition of $\\mathbf{\\deltaX}$ , $\\textbf{\\em X}=$ dD=/1P W \u00d71 H:d: \u00d72 R:d: \u00d73 T:d:, such that the two sides of Eq.(5) equal, where P = D, $\\begin{array}{r l}{{}}&{{{}^{-\\lambda}=\\;\\sqrt{\\lambda_{1}/\\lambda_{4}}\\,{}^{-\\frac{1}{\\alpha}}U_{1}\\sqrt{\\Sigma_{1}},R_{1:1}\\;=\\;\\sqrt{\\lambda_{1}/\\lambda_{4}}{}^{\\frac{-1}{\\alpha}}U_{2}\\sqrt{\\Sigma_{2}},T_{:1:}\\;=\\;\\sqrt{\\lambda_{1}/\\lambda_{4}}{}^{\\frac{\\cdot}{\\alpha}}U_{3}\\sqrt{\\Sigma_{3}},W\\;=\\;\\sqrt{\\lambda_{1}/\\lambda_{4}}{}^{\\frac{\\cdot}{\\alpha}}T_{:1}\\sqrt{\\Sigma_{3}},W\\;=\\;\\sqrt{\\lambda_{1}/\\lambda_{4}}{}^{\\frac{\\cdot}{\\alpha}}T_{:1}\\sqrt{\\Sigma_{3}},W\\;=\\;\\sqrt{\\lambda_{1}/\\lambda_{4}}{}^{\\frac{\\cdot}{\\alpha}}T_{:1}\\sqrt{\\Sigma_{3}},W\\;=\\;\\frac{1}{\\lambda_{2}},}}\\\\ {{}}&{{{}^{-\\lambda}/\\lambda_{4}}{}^{\\frac{\\cdot}{\\alpha}}X\\times_{1}\\sqrt{\\Sigma_{1}^{-1}}U_{1}^{T}\\times_{2}\\sqrt{\\Sigma_{2}^{-1}}U_{2}^{T}\\times_{3}\\sqrt{\\Sigma_{3}^{-1}}U_{3}^{T}.}}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "Proof. Let the $n$ -rank [Kolda and Bader, 2009] of $\\pmb{X}\\,\\in\\,\\mathbb{R}^{n_{1}\\times n_{2}\\times n_{3}}$ be $(r_{1},r_{2},r_{3})$ , then $U_{1}~\\in$ $\\mathbb{R}^{n_{1}\\times r_{1}},U_{2}\\;\\;\\in\\;\\;\\mathbb{R}^{n_{2}\\times r_{2}},U_{3}\\;\\;\\in\\;\\;\\mathbb{R}^{n_{3}\\times r_{3}},\\Sigma_{1}\\;\\;\\in\\;\\;\\mathbb{R}^{r_{1}\\times r_{1}},\\Sigma_{2}\\;\\;\\in\\;\\;\\mathbb{R}^{r_{2}\\times r_{2}},\\Sigma_{3}\\in\\;\\;\\mathbb{R}^{r_{3}\\times r_{3}}.$ 3 \u2208 Rr3\u00d7r3, W \u2208 $\\mathbb{R}^{r_{1}\\times r_{2}\\times r_{3}}$ . ", "page_idx": 13}, {"type": "text", "text": "If $X=\\mathbf{0}$ , the above proposition is obviously true, we define ${\\bf0}^{-1}:={\\bf0}$ here. ", "page_idx": 13}, {"type": "text", "text": "For any $X\\neq\\mathbf{0}$ , since $\\begin{array}{r}{X_{(1)}=\\sum_{d=1}^{D/P}H_{:d:}(W_{(1)}({\\cal T}_{:d:}\\otimes{\\cal R}_{:d:})^{T}),X_{(2)}=\\sum_{d=1}^{D/P}R_{:d:}(W_{(2)}({\\cal T}_{:d:}\\otimes{\\cal R}_{:d:})^{T}).}\\end{array}$ $H_{:d:})^{T}$ ), $\\begin{array}{r}{\\pmb{X}_{(3)}=\\sum_{d=1}^{D/P}\\pmb{T}_{:d:}(\\pmb{W}_{(3)}(\\pmb{R}_{:d:}\\otimes\\pmb{H}_{:d:})^{T})}\\end{array}$ , by applying Lemma 1 to $X_{(1)},X_{(2)},X_{(3)}$ , we ", "page_idx": 13}, {"type": "text", "text": "have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2L(X;\\alpha)}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{D/P}\\|\\mathbf{H}_{\\mathcal{A}}\\big(\\mathbf{W}_{(1)}(\\mathbf{T}_{\\mathcal{A},i}\\otimes\\mathbf{R}_{\\mathcal{A},i})^{T}\\big)\\|_{*}^{\\alpha/2}+\\big\\|\\mathbf{R}_{\\mathcal{A}}\\big(\\mathbf{W}_{(2)}(T_{\\mathcal{A},i}\\otimes\\mathbf{H}_{\\mathcal{A}})^{T}\\big)\\big\\|_{*}^{\\alpha/2}+\\big\\|\\mathbf{T}_{\\mathcal{A}}\\big(\\mathbf{W}_{(3)}(\\mathbf{R}_{\\mathcal{A}}\\otimes\\mathbf{R}_{\\mathcal{A},i})^{T}\\big)\\big\\|_{*}^{\\alpha/2}}\\\\ &{~~\\displaystyle\\sum_{i=1}^{D/P}\\lambda\\big(\\|\\mathbf{H}_{\\mathcal{A}}\\|_{\\mathcal{F}}^{\\alpha}+\\|\\mathbf{R}_{\\mathcal{A}}\\|_{\\mathcal{F}}^{\\alpha}+\\|\\mathbf{T}_{\\mathcal{A}}\\|_{\\mathcal{F}}^{\\alpha}\\big)}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{D/P}\\lambda\\big(\\|\\mathbf{W}_{(1)}(\\mathbf{T}_{\\mathcal{A},i}\\otimes\\mathbf{R}_{\\mathcal{A}})^{T}\\|_{\\mathcal{F}}^{\\alpha}+\\|\\mathbf{W}_{(2)}(T_{\\mathcal{A},\\mathcal{B}}\\otimes\\mathbf{H}_{\\mathcal{A}})^{T}\\|_{\\mathcal{F}}^{\\alpha}+\\|\\mathbf{W}_{(3)}(\\mathbf{R}_{\\mathcal{A}}\\otimes\\mathbf{H}_{\\mathcal{A}})^{T}\\|_{\\mathcal{F}}^{\\alpha}\\big)}\\\\ &{~~\\displaystyle+\\frac{1}{\\lambda}\\big(\\|\\mathbf{W}_{(1)}(T_{\\mathcal{A},i}\\otimes\\mathbf{R}_{\\mathcal{A}})^{T}\\|_{\\mathcal{F}}^{\\alpha}+\\|\\mathbf{W}_{(2)}(T_{\\mathcal{A},\\mathcal{B}}\\otimes\\mathbf{H}_{\\mathcal{A}})^{T}\\|_{\\mathcal{F}}^{\\alpha}+\\|\\mathbf{W}_{(3)}(\\mathbf{R}_{\\mathcal{A}}\\otimes\\mathbf{H}_{\\mathcal{A}})^{T}\\|_{\\mathcal{F}}^{\\alpha}\\big)}\\\\ &{~~\\displaystyle=\\sum_{i=1}^{D/P}\\lambda\\big(\\|\\mathbf{H}_{\\mathcal{A},i} \n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $\\lambda=\\sqrt{\\lambda_{1}/\\lambda_{4}}$ , we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\sqrt{\\lambda_{1}\\lambda_{4}}L(X;\\alpha)\\le\\displaystyle\\sum_{d=1}^{D/P}\\lambda_{1}(\\|H_{:d:}\\|_{F}^{\\alpha}+\\|R_{:d:}\\|_{F}^{\\alpha}+\\|T_{:d:}\\|_{F}^{\\alpha})}\\\\ &{\\ +\\lambda_{4}(\\|W\\times_{2}R_{:d:}\\times_{3}T_{:d:}\\|_{F}^{\\alpha}+\\|W\\times_{3}T_{:d:}\\times_{1}H_{:d:}\\|_{F}^{\\alpha}+\\|W\\times_{1}H_{:d:}\\times_{2}R_{:d:}\\|_{F}^{\\alpha})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $\\begin{array}{l l l l l l l}{U_{1}^{T}U_{1}}&{=}&{I_{r_{1}\\times r_{1}},U_{2}^{T}U_{2}}&{=}&{I_{r_{2}\\times r_{2}},U_{3}^{T}U_{3}}&{=}&{I_{r_{3}\\times r_{3}}}\\end{array}$ , thus we have $\\begin{array}{r l}{X_{(1)}}&{{}=}\\end{array}$ $U_{1}U_{1}^{T}X_{(1)},X_{(2)}=U_{2}U_{2}^{T}X_{(2)},X_{(3)}=U_{3}U_{3}^{T}X_{(3)},\\mathrm{th}$ en ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{X=X\\times_{1}(U_{1}U_{1}^{T})\\times_{2}(U_{2}U_{2}^{T})\\times_{3}(U_{3}U_{3}^{T})}\\\\ &{\\quad=X\\times_{1}(U_{1}\\sqrt{\\Sigma_{1}}\\sqrt{\\Sigma_{1}^{-1}}U_{1}^{T})\\times_{2}(U_{2}\\sqrt{\\Sigma_{2}}\\sqrt{\\Sigma_{2}^{-1}}U_{2}^{T})\\times_{3}(U_{3}\\sqrt{\\Sigma_{3}}\\sqrt{\\Sigma_{3}^{-1}}U_{3}^{T})}\\\\ &{\\quad=X\\times_{1}\\sqrt{\\Sigma_{1}^{-1}}U_{1}^{T}\\times_{2}\\sqrt{\\Sigma_{2}^{-1}}U_{2}^{T}\\times_{3}\\sqrt{\\Sigma_{3}^{-1}}U_{3}^{T}\\times_{1}U_{1}\\sqrt{\\Sigma_{1}}\\times_{2}U_{2}\\sqrt{\\Sigma_{2}}\\times_{3}U_{3}\\sqrt{\\Sigma_{3}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $\\mathrm{~\\textit~{~P~}~}=\\mathrm{~\\textit~{~D~}~}$ and $\\begin{array}{r c c c c l}{{{\\cal H}_{:1:}}}&{{=}}&{{\\sqrt{\\lambda_{1}/\\lambda_{4}}^{-\\frac{1}{\\alpha}}U_{1}\\sqrt{\\Sigma_{1}},R_{:1:}}}&{{=}}&{{\\sqrt{\\lambda_{1}/\\lambda_{4}}^{-\\frac{1}{\\alpha}}U_{2}\\sqrt{\\Sigma_{2}},T_{:1:}}}&{{=}}&{{\\frac{\\lambda_{1}/\\lambda_{4}}{\\alpha}^{-\\frac{1}{\\alpha}}U_{2}\\sqrt{\\Sigma_{2}},T_{:1:}}}\\end{array}$ $\\sqrt{\\lambda_{1}/\\lambda_{4}}^{-\\frac{1}{\\alpha}}U_{3}\\sqrt{\\pmb{\\Sigma}_{3}},W=\\sqrt{\\lambda_{1}/\\lambda_{4}}^{\\frac{3}{\\alpha}}X\\times_{1}\\sqrt{\\pmb{\\Sigma}_{1}^{-1}}U_{1}^{T}\\times_{2}\\sqrt{\\pmb{\\Sigma}_{2}^{-1}}U_{2}^{T}\\times_{3}\\sqrt{\\pmb{\\Sigma}_{3}^{-1}}U_{3}^{T}$ , then $\\mathbf{X}=$ $W\\times_{1}H_{:1:}\\times_{2}R_{:1:}\\times_{3}T_{:1:}$ : is a decomposition of $\\mathbf{\\deltaX}$ . Since ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{X_{(1)}=\\sqrt{\\lambda_{1}/\\lambda_{4}}^{-1}U_{1}\\sqrt{\\Sigma_{1}}W_{(1)}(T_{1:}\\otimes R_{1:1:})^{T}=U_{1}\\sqrt{\\Sigma_{1}}\\sqrt{\\Sigma_{1}}V_{1}^{T}}\\\\ &{X_{(2)}=\\sqrt{\\lambda_{1}/\\lambda_{4}}^{-1}U_{2}\\sqrt{\\Sigma_{2}}W_{(2)}(T_{1:}\\otimes H_{:1:})^{T}=U_{2}\\sqrt{\\Sigma_{2}}\\sqrt{\\Sigma_{2}}V_{2}^{T}}\\\\ &{X_{(3)}=\\sqrt{\\lambda_{1}/\\lambda_{4}}^{-1}U_{3}\\sqrt{\\Sigma_{3}}W_{(3)}(R_{:1:}\\otimes H_{:1:})^{T}=U_{3}\\sqrt{\\Sigma_{3}}\\sqrt{\\Sigma_{3}}V_{3}^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "thus ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{(1)}({\\mathbf{T}}_{:1:}\\otimes{\\mathbf{R}}_{:1:})^{T}=\\sqrt{\\lambda_{1}/\\lambda_{4}}^{\\frac{1}{\\alpha}}\\sqrt{\\Sigma_{1}}V_{1}^{T}}\\\\ &{W_{(2)}({\\mathbf{T}}_{:1:}\\otimes{\\mathbf{H}}_{:1:})^{T}=\\sqrt{\\lambda_{1}/\\lambda_{4}}^{\\frac{1}{\\alpha}}\\sqrt{\\Sigma_{2}}V_{2}^{T}}\\\\ &{W_{(3)}({\\mathbf{R}}_{:1:}\\otimes{\\mathbf{H}}_{:1:})^{T}=\\sqrt{\\lambda_{1}/\\lambda_{4}}^{\\frac{1}{\\alpha}}\\sqrt{\\Sigma_{3}}V_{3}^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If $P=D$ , we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\frac{D/\\rho}{\\Delta t}\\left\\langle\\mathbf{1}\\left\\vert\\mathbf{H}_{\\cdot}\\right\\vert\\right\\vert\\phi^{*}+\\left\\vert\\mathbf{R}_{\\cdot}\\right\\vert\\right\\vert\\phi^{*}+\\left\\vert\\left\\vert\\Gamma_{\\mathcal{A}}\\right\\vert\\right\\vert_{F}^{\\infty}}\\\\ &{~+\\lambda_{1}\\left(\\left\\vert\\mathbf{V}_{\\cdot}\\right\\vert\\left\\vert\\mathbf{\\phi}_{L}+\\left\\vert\\mathbf{R}_{\\cdot}\\right\\vert\\right\\vert\\phi^{*}+\\left\\vert\\mathbf{1}W_{\\cdot}\\right\\vert\\left\\vert\\phi_{L}\\right\\vert\\right)}\\\\ &{+\\lambda_{1}\\left(\\left\\vert\\mathbf{H}_{\\cdot}\\right\\vert\\left\\vert\\mathbf{\\phi}_{L}^{*}+\\left\\vert\\mathbf{R}_{\\cdot}\\right\\vert\\right\\vert\\phi_{R}^{*}+\\left\\vert\\mathbf{H}_{\\cdot}\\right\\vert\\left\\vert\\mathbf{\\phi}_{L}^{*}\\right\\vert\\right)}\\\\ &{-\\lambda_{1}\\left(\\left\\vert\\mathbf{H}_{\\cdot}\\right\\vert\\right\\vert\\phi^{*}+\\left\\vert\\mathbf{H}_{\\cdot}\\right\\vert\\right\\vert\\phi_{R}^{*}+\\left\\vert\\mathbf{I}_{\\cdot}\\right\\vert\\frac{1}{\\rho}\\right)}\\\\ &{+\\lambda_{1}\\left(\\left\\vert\\mathbf{V}_{\\cdot}\\right\\vert\\mathbf{\\phi}_{R}^{*}+\\left\\vert\\mathbf{1}_{\\cdot}\\right\\vert\\frac{1}{\\rho}\\right\\vert+\\left\\vert\\mathbf{V}_{\\times}\\right\\vert\\mathbf{\\phi}_{L}^{*}\\right\\vert\\left\\vert\\frac{1}{\\rho}+\\left\\vert\\mathbf{W}_{\\cdot1}\\right\\vert\\frac{1}{\\rho}+\\left\\vert\\mathbf{H}_{\\cdot}\\right\\vert\\mathbf{H}_{\\cdot}\\right\\vert\\frac{1}{\\rho}\\right)}\\\\ &{-\\lambda_{1}\\left(\\left\\vert\\mathbf{H}_{\\cdot}\\right\\vert\\left\\vert\\mathbf{\\phi}_{R}^{*}+\\left\\vert\\mathbf{H}_{\\cdot1}\\right\\vert\\phi_{R}^{*}\\right\\vert+\\left\\vert\\mathbf{I}_{\\cdot1}\\right\\vert\\frac{1}{\\rho}\\right)}\\\\ &{+\\lambda_{1}(\\left\\vert\\mathbf{W}_{\\cdot1}\\otimes\\mathbf{R}_{1})^{\\top}\\left\\vert\\mathbf{\\phi}_{R}^{*}+\\left\\vert\\mathbf{W}_{\\cdot2}\\right\\vert\\left(\\Gamma_{\\cdot}\\right\\vert\\otimes\\mathbf{H}_{\\cdot1}\\otimes\\mathbf{H}_{\\cdot1}\\right)^{T}\\left\\vert\\mathbf{F}_{\\cdot}^{*}+\\left\\vert\\mathbf{W}_{\\cdot3}\\right\\vert\\left(\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proposition 2. For any $\\mathbf{\\deltaX}$ , and for any decomposition of $\\mathbf{\\deltaX}$ , X, $\\begin{array}{r}{\\pmb{X}=\\sum_{d=1}^{D/P}\\pmb{W}\\times_{1}\\pmb{H}_{:d:}\\times_{2}}\\end{array}$ $R_{:d:}\\times_{3}T_{:d:}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\sqrt{\\lambda_{2}\\lambda_{3}}L(X)\\leq\\displaystyle\\sum_{d=1}^{D/P}\\lambda_{2}(\\|T_{:d:}\\|_{F}^{\\alpha}\\|R_{:d:}\\|_{F}^{\\alpha}+\\|T_{:d:}\\|_{F}^{\\alpha}\\|H_{:d:}\\|_{F}^{\\alpha}+\\|R_{:d:}\\|_{F}^{\\alpha}\\|H_{:d:}\\|_{F}^{\\alpha})}\\\\ &{\\,+\\,\\lambda_{3}(\\|W\\times_{1}H_{:d:}\\|_{F}^{\\alpha}+\\|W\\times_{2}R_{:d:}\\|_{F}^{\\alpha}+\\|W\\times_{3}T_{:d:}\\|_{F}^{\\alpha})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "And there exists some $X^{'}$ , and for any decomposition of $X^{'}$ , such that the two sides of Eq.(6) can not achieve equality. ", "page_idx": 15}, {"type": "equation", "text": "$\\begin{array}{r c l c r c l}{X_{(1)}\\!}&{=}&{\\sum_{d=1}^{D/P}(H_{:d:}W_{(1)})(T_{:d:}\\otimes\\,R_{:d:})^{T},X_{(2)}}&{=}&{\\sum_{d=1}^{D/P}(R_{:d:}W_{(2)})(T_{:d:}\\otimes\\,R_{:d:}\\otimes\\,R_{:d:}\\otimes\\,R_{:d:}\\otimes\\,R_{:d:}\\otimes\\,R_{:d:}\\otimes\\,)}\\end{array}$ ", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{2L(\\boldsymbol{X};\\boldsymbol{\\alpha})}}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{D/P}\\|(\\boldsymbol{H}_{\\mathcal{A}}\\boldsymbol{W}_{(1)})(T_{\\mathcal{A};\\boldsymbol{\\beta}}\\otimes\\boldsymbol{R}_{\\mathcal{A};\\boldsymbol{\\beta}})^{T}\\|_{*}^{\\boldsymbol{\\alpha}/2}+\\|(\\boldsymbol{R}_{\\mathcal{A}}\\boldsymbol{W}_{(2)})(T_{\\mathcal{A}}\\otimes\\boldsymbol{H}_{\\mathcal{A};\\boldsymbol{\\beta}})^{T}\\|_{*}^{\\boldsymbol{\\alpha}/2}+\\|(T_{\\mathcal{A}}\\boldsymbol{W}_{(3)})(\\boldsymbol{R}_{\\mathcal{A}}\\otimes\\boldsymbol{H}_{\\mathcal{A};\\boldsymbol{\\beta}})^{T}\\|_{*}^{\\boldsymbol{\\alpha}/2}}\\\\ &{\\le\\displaystyle\\sum_{d=1}^{D/P}\\lambda(\\|T_{\\mathcal{A}}\\otimes\\boldsymbol{R}_{\\mathcal{A}}\\|_{F}^{\\alpha}+\\|T_{\\mathcal{A}}\\otimes\\boldsymbol{H}_{\\mathcal{A};\\boldsymbol{\\beta}}\\|_{F}^{\\alpha}+\\|\\boldsymbol{R}_{\\mathcal{A}}\\otimes\\boldsymbol{H}_{\\mathcal{A};\\boldsymbol{\\beta}}\\|_{F}^{\\alpha})}\\\\ &{+\\displaystyle\\frac{1}{\\lambda}(\\|\\boldsymbol{H}_{\\mathcal{A}}\\boldsymbol{W}_{(1)}\\|_{F}^{\\alpha}+\\|\\boldsymbol{R}_{\\mathcal{A}}\\boldsymbol{W}_{(2)}\\|_{F}^{\\alpha}+\\|T_{\\mathcal{A}}\\boldsymbol{W}_{(3)}\\|_{F}^{\\alpha})}\\\\ &{=\\displaystyle\\sum_{d=1}^{D/P}\\lambda(\\|T_{\\mathcal{A}}\\|_{F}^{\\alpha}\\|R_{\\mathcal{A}}\\|_{F}^{\\alpha}+\\|T_{\\mathcal{A}}\\|_{F}^{\\alpha}\\|H_{\\mathcal{A}}^{\\alpha}\\|_{F}^{\\alpha}+\\|R_{\\mathcal{A}}\\|_{F}^{\\alpha}\\|H_{\\mathcal{A}}\\|_{F}^{\\alpha})}\\\\ &{\\quad+\\lambda(\\|\\boldsymbol{W}_{\\mathcal{A}}\\|_{H_{\\mathcal{A}}^{\\alpha}}\\|_{F}^{\\alpha}+\\|\\boldsymbol{W}_{\\mathcal{A}}\\|_{F}^{\\alpha}+\\|\\boldsymbol{W}_{\\mathcal{A}}\\|_{F}^{\\alpha})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\lambda=\\sqrt{\\lambda_{2}/\\lambda_{3}}$ , we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\sqrt{\\lambda_{2}\\lambda_{3}}L(X)\\leq\\displaystyle\\sum_{d=1}^{D/P}\\lambda_{2}(\\|\\pmb{T}_{:d:}\\|_{F}^{\\alpha}\\|\\pmb{R}_{:d:}\\|_{F}^{\\alpha}+\\|\\pmb{T}_{:d:}\\|_{F}^{\\alpha}\\|\\pmb{H}_{:d:}\\|_{F}^{\\alpha}+\\|\\pmb{R}_{:d:}\\|_{F}^{\\alpha}\\|\\pmb{H}_{:d:}\\|_{F}^{\\alpha})}\\\\ &{+\\lambda_{3}(\\|\\pmb{W}\\times\\pmb{1}_{H:}\\|_{F}^{\\alpha}+\\|\\pmb{W}\\times_{2}\\pmb{R}_{:d:}\\|_{F}^{\\alpha}+\\|\\pmb{W}\\times_{3}\\pmb{T}_{:d:}\\|_{F}^{\\alpha})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\pmb{X}^{'}\\in\\mathbb{R}^{2\\times2\\times2}$ , $X_{1,1,1}^{'}=X_{2,2,2}^{'}=1$ and $X_{i j k}^{'}=0$ otherwise. Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{X}_{(1)}^{'}=\\pmb{X}_{(2)}^{'}=\\pmb{X}_{(3)}^{'}=\\left(\\begin{array}{c c c c}{1}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{1}\\end{array}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus $\\operatorname{rank}(\\mathbf{X}_{(1)}^{'})=\\operatorname{rank}(\\mathbf{X}_{(2)}^{'})=\\operatorname{rank}(\\mathbf{X}_{(3)}^{'})=2$ . In Lemma 1, a necessary condition of the equality holds is that $\\operatorname{rank}(\\pmb{U})=\\operatorname{rank}(\\pmb{V})=\\operatorname{rank}(\\pmb{Z})$ . Thus, the equality holds only if $P=D$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{rank}(X_{(1)}^{'})=\\mathrm{rank}(\\pmb{T}_{1:}\\otimes\\pmb{R}_{:1:})=2,\\mathrm{rank}(X_{(2)}^{'})=\\mathrm{rank}(\\pmb{T}_{:1:}\\otimes\\pmb{H}_{:1:})=2}\\\\ &{\\mathrm{rank}(X_{(3)}^{'})=\\mathrm{rank}(\\pmb{R}_{:1:}\\otimes\\pmb{H}_{:1:})=2}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since for any matrix $A,B,\\operatorname{rank}(A\\otimes B)=\\operatorname{rank}(A)\\operatorname{rank}(B)$ , we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{rank}(T_{:1:})\\,\\mathrm{rank}(R_{:1:})=2,\\mathrm{rank}(T_{:1:})\\,\\mathrm{rank}(H_{:1:})=2,\\mathrm{rank}(R_{:1:})\\,\\mathrm{rank}(H_{:1:})=2\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The above equations have no non-negative integer solution, thus there is no decomposition of $X^{'}$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\sqrt{\\lambda_{2}\\lambda_{3}}L(X)=\\displaystyle\\sum_{d=1}^{D/P}\\lambda_{2}(\\|\\pmb{T}_{:d:}\\|_{F}^{\\alpha}\\|\\pmb{R}_{:d:}\\|_{F}^{\\alpha}+\\|\\pmb{T}_{:d:}\\|_{F}^{\\alpha}\\|\\pmb{H}_{:d:}\\|_{F}^{\\alpha}+\\|\\pmb{R}_{:d:}\\|_{F}^{\\alpha}\\|\\pmb{H}_{:d:}\\|_{F}^{\\alpha})}\\\\ &{+\\lambda_{3}(\\|\\pmb{W}\\times\\pmb{1}_{H:}\\|_{F}^{\\alpha}+\\|\\pmb{W}\\times\\pmb{2}_{H:}\\|_{F}^{\\alpha}+\\|\\pmb{W}\\times\\pmb{3}_{T:d:}\\|_{F}^{\\alpha})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proposition 3. For any $\\mathbf{\\deltaX}$ , and for any decomposition of $\\mathbf{\\deltaX}$ , $\\begin{array}{r}{\\pmb{X}=\\sum_{d=1}^{D}W\\times_{1}\\pmb{H}_{:d1}\\times_{2}\\pmb{R}_{:d1}\\times_{3}\\pmb{T}_{:d1}}\\end{array}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\sqrt{\\lambda_{1}\\lambda_{4}}L({\\pmb X};2)\\le6\\sqrt{\\lambda_{1}\\lambda_{4}}\\|{\\pmb X}\\|_{*}\\leq\\displaystyle\\sum_{d=1}^{D}\\lambda_{1}(\\|{\\pmb H}_{:d1}\\|_{F}^{2}+\\|{\\pmb R}_{:d1}\\|_{F}^{2}+\\|{\\pmb T}_{:d1}\\|_{F}^{2})}\\\\ &{+\\lambda_{4}(\\|{\\pmb W}\\times_{2}{\\pmb R}_{:d1}\\times_{3}{\\pmb T}_{:d1}\\|_{F}^{2}+\\|{\\pmb W}\\times_{3}{\\pmb T}_{:d1}\\times_{1}{\\pmb H}_{:d1}\\|_{F}^{2}+\\|{\\pmb W}\\times_{1}{\\pmb H}_{:d1}\\times_{2}{\\pmb R}_{:d1}\\|_{F}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "where $W=1$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. $\\forall X$ , let $\\begin{array}{r l r}{S_{1}}&{=}&{\\{(W,H,R,T)|X\\ =\\ \\sum_{d=1}^{D}W\\ \\times_{1}\\ H_{:d:}\\ \\times_{2}\\ R_{:d:}\\ \\times_{3}\\ T_{:d:}\\},S_{2}\\ =}\\end{array}$ $\\begin{array}{r}{\\{(W,H,R,T)|X=\\sum_{d=1}^{D}W\\times_{1}H_{:d1}\\times_{2}R_{:d1}\\times_{3}T_{:d1},W=1\\}.}\\end{array}$ We have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle2\\sqrt{\\lambda_{1}\\lambda_{4}}(\\sum_{d=1}^{D}\\|{\\pmb H}_{:d1}\\|_{F}\\|{\\pmb R}_{:d1}\\|_{F}\\|{\\pmb T}_{:d1}\\|_{F})}\\\\ {\\displaystyle\\leq2(\\sum_{d=1}^{D}\\lambda_{4}\\|{\\pmb R}_{:d1}\\|_{F}^{2}\\|{\\pmb T}_{:d1}\\|_{F}^{2})^{1/2}(\\sum_{d=1}^{D}\\lambda_{1}\\|{\\pmb H}_{:d1}\\|_{F}^{2})^{1/2}}\\\\ {\\displaystyle=\\sum_{d=1}^{D}\\lambda_{1}\\|{\\pmb H}_{:d1}\\|_{F}^{2}+\\lambda_{4}\\|{\\pmb W}\\times_{2}{\\pmb R}_{:d:\\times}{\\pmb\\lambda}_{3}{\\bf\\lambda T}_{:d:}\\|_{F}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $W\\ =\\ 1$ . The equality holds if and only if $\\lambda_{4}\\|R_{:d1}\\|_{F}^{2}\\|T_{:d1}\\|_{F}^{2}~=~\\lambda_{1}\\|H_{:d1}\\|_{F}^{2}$ , i.e., $\\sqrt{\\lambda_{4}/\\lambda_{1}}\\|\\pmb{R}_{:d1}\\|_{2}\\|\\pmb{T}_{:d1}\\|_{2}=\\|\\pmb{H}_{:d1}\\|_{2}$ . For a CP decomposition of $\\mathbf{\\deltaX}$ , $\\begin{array}{r}{\\pmb{X}=\\sum_{d=1}^{D}\\pmb{W}\\times_{1}\\pmb{H}_{:d1}\\times_{2}}\\end{array}$ R:d1 \u00d73 T:d1, we let H:d1 = $\\begin{array}{r l r}{\\pmb{H}_{:d1}^{\\prime}}&{=}&{\\sqrt{\\frac{\\|\\pmb{R}_{:d1}\\|_{2}\\|\\pmb{T}_{:d1}\\|_{2}}{\\|\\pmb{H}_{:d1}\\|_{2}}}\\pmb{H}_{:i},\\pmb{R}_{:i}^{\\prime}}&{=\\ \\sqrt{\\frac{\\|\\pmb{H}_{:d1}\\|_{2}}{\\|\\pmb{R}_{:d1}\\|_{2}\\|\\pmb{T}_{:d1}\\|_{2}}}\\pmb{R}_{:i},\\pmb{T}_{:i}^{\\prime}}&{=\\ T_{:i}}\\end{array}$ if $\\|\\pmb{H}_{:d1}\\|_{2}\\,\\neq\\,0,\\|\\pmb{R}_{:d1}\\|_{2}\\,\\neq\\,0,\\|\\pmb{T}_{:d1}\\|_{2}\\,\\neq\\,0$ and otherwise ${\\pmb H}_{:d}^{'}\\,=\\,{\\bf0},{\\pmb R}_{:d}^{'}\\,=\\,{\\bf0},{\\pmb T}_{:d}^{'}\\,=\\,{\\bf0}$ . Then $\\begin{array}{r}{\\pmb{X}=\\sum_{d=1}^{D}\\pmb{W}\\times_{1}\\pmb{H}_{:d1}^{'}\\times_{2}\\pmb{R}_{:d1}^{'}\\times_{3}\\pmb{T}_{:d1}^{'}}\\end{array}$ is another CP decomposition of $\\mathbf{\\deltaX}$ and $\\|\\boldsymbol{R}_{:d1}^{'}\\|_{2}\\|\\boldsymbol{T}_{:d1}^{'}\\|_{2}=$ $\\|\\pmb{H}_{:d1}^{\\prime}\\|_{2}$ . Thus ", "page_idx": 16}, {"type": "equation", "text": "$$\n2\\sqrt{\\lambda_{1}\\lambda_{4}}\\|X\\|_{*}=\\operatorname*{min}_{S_{2}}\\sum_{d=1}^{D}\\lambda_{1}\\|H_{:d1}\\|_{F}^{2}+\\lambda_{4}\\|W\\times_{2}R_{:d:}\\times_{3}T_{:d:}\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, we can get ", "page_idx": 16}, {"type": "equation", "text": "$$\n2\\sqrt{\\lambda_{1}\\lambda_{4}}\\|X\\|_{*}=\\operatorname*{min}_{S_{2}}\\sum_{d=1}^{D}\\lambda_{1}\\|R_{:d1}\\|_{F}^{2}+\\lambda_{4}\\|W\\times_{2}T_{:d:}\\times_{3}H_{:d:}\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n2\\sqrt{\\lambda_{1}\\lambda_{4}}\\|\\pmb{X}\\|_{*}=\\operatorname*{min}_{S_{2}}\\sum_{d=1}^{D}\\lambda_{1}\\|\\pmb{T}_{:d1}\\|_{F}^{2}+\\lambda_{4}\\|\\pmb{W}\\times_{2}\\pmb{H}_{:d:}\\times_{3}\\pmb{R}_{:d:}\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Propostion 1, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n2\\sqrt{\\lambda_{1}\\lambda_{4}}\\|\\pmb{X}_{(1)}\\|_{*}=\\sum_{d=1}^{D}\\lambda_{1}\\|\\pmb{H}_{:d1}\\|_{F}^{2}+\\lambda_{4}\\|\\pmb{W}\\times_{2}\\pmb{R}_{:d:}\\times_{3}\\pmb{T}_{:d:}\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $S_{2}$ is a subset of $S_{1}$ , we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|X_{(1)}\\|_{*}\\leq\\|X\\|_{*}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, we can prove that $\\|X_{(2)}\\|_{*}\\leq\\|X\\|_{*}$ and $\\|X_{(3)}\\|_{*}\\leq\\|X\\|_{*}$ , thus ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\sqrt{\\lambda_{1}\\lambda_{4}}L(X;2)\\le6\\sqrt{\\lambda_{1}\\lambda_{4}}\\|X\\|_{*}\\leq\\displaystyle\\sum_{d=1}^{D}\\lambda_{1}(\\|H_{:d1}\\|_{F}^{2}+\\|R_{:d1}\\|_{F}^{2}+\\|T_{:d1}\\|_{F}^{2})}\\\\ &{\\,+\\,\\lambda_{4}(\\|W\\times_{2}R_{:d1}\\times_{3}T_{:d1}\\|_{F}^{2}+\\|W\\times_{3}T_{:d1}\\times_{1}H_{:d1}\\|_{F}^{2}+\\|W\\times_{1}H_{:d1}\\times_{2}R_{:d1}\\|_{F}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proposition 4. For any $\\mathbf{\\deltaX}$ , there exists a decomposition of $\\begin{array}{r}{{\\pmb X}=\\sum_{d=1}^{D/P}{\\hat{\\pmb W}}\\times_{1}{\\hat{\\pmb H}}_{:d:}\\times_{2}{\\hat{\\pmb R}}_{:d:}\\times_{3}{\\hat{\\pmb T}}_{:d:}}\\end{array}$ , such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{d=1}^{D/P}\\lambda_{1}(\\|\\hat{H}_{:d:}\\|_{F}^{\\alpha}+\\|\\hat{R}_{:d:}\\|_{F}^{\\alpha}+\\|\\hat{T}_{:d:}\\|_{F}^{\\alpha})}\\\\ &{+\\lambda_{4}(\\|\\hat{W}\\times_{2}\\hat{R}_{:d:}\\times_{3}\\hat{T}_{:d:}\\|_{F}^{\\alpha}+\\|\\hat{W}\\times_{3}\\hat{T}_{:d:}\\times_{1}\\hat{H}_{:d:}\\|_{F}^{\\alpha}+\\|\\hat{W}\\times_{1}\\hat{H}_{:d:}\\times_{2}\\hat{R}_{:d:}\\|_{F}^{\\alpha})}\\\\ &{<\\!\\sqrt{\\lambda_{1}\\lambda_{4}}/\\sqrt{\\lambda_{2}\\lambda_{3}}\\displaystyle\\sum_{d=1}^{D/P}\\lambda_{2}(\\|\\hat{T}_{:d:}\\|_{F}^{\\alpha}\\|_{\\mathcal{F}}+\\|\\hat{T}_{:d:}\\|_{F}^{\\alpha}\\|_{\\mathcal{F}}^{\\alpha}\\!\\!-\\!\\|_{F:d:}\\|_{F}^{\\alpha}\\!+\\!\\|\\hat{R}_{:d:}\\|_{F}^{\\alpha}\\|_{\\mathcal{F}}^{\\alpha}\\!\\!-\\!\\|_{F}^{\\alpha}}\\\\ &{+\\lambda_{3}(\\|\\hat{W}\\times_{1}\\hat{H}_{:d:}\\|_{F}^{\\alpha}+\\|\\hat{W}\\times_{2}\\hat{R}_{:d:}\\|_{F}^{\\alpha}+\\|\\hat{W}\\times_{3}\\hat{T}_{:d:}\\|_{F}^{\\alpha})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Furthermore, for some X, there exists a decomposition of X, X = dD=/1P $\\begin{array}{r}{{\\cal X}=\\sum_{d=1}^{D/P}\\hat{W}\\!\\times\\!_{1}\\hat{H}_{:d:}\\!\\times\\!_{2}\\hat{R}_{:d:}\\!\\times\\!_{3}\\hat{T}_{:d:}}\\end{array}$ , such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{d=1}^{D/P}\\lambda_{1}(\\|\\tilde{H}_{:d:}\\|_{F}^{\\alpha}+\\|\\tilde{R}_{:d:}\\|_{F}^{\\alpha}+\\|\\tilde{T}_{:d:}\\|_{F}^{\\alpha})}\\\\ &{+\\lambda_{4}(\\|\\tilde{W}\\times_{2}\\tilde{R}_{:d:}\\times_{3}\\tilde{T}_{:d:}\\|_{F}^{\\alpha}+\\|\\tilde{W}\\times_{3}\\tilde{T}_{:d:}\\times_{1}\\tilde{H}_{:d:}\\|_{F}^{\\alpha}+\\|\\tilde{W}\\times_{1}\\tilde{H}_{:d:}\\times_{2}\\tilde{R}_{:d:}\\|_{F}^{\\alpha})}\\\\ &{>\\sqrt{\\lambda_{1}\\lambda_{4}}/\\sqrt{\\lambda_{2}\\lambda_{3}}\\displaystyle\\sum_{d=1}^{D/P}\\lambda_{2}(\\|\\tilde{T}_{:d:}\\|_{F}^{\\alpha}\\|_{F}^{\\alpha}+\\|\\tilde{T}_{:d:}\\|_{F}^{\\alpha}\\|_{F}^{\\alpha}+\\|\\tilde{R}_{:d:}\\|_{F}^{\\alpha}\\|_{F}^{\\alpha}+\\|\\tilde{R}_{:d:}\\|_{F}^{\\alpha}\\|_{F}^{\\alpha})}\\\\ &{+\\lambda_{3}(\\|\\tilde{W}\\times_{1}\\tilde{H}_{:d:}\\|_{F}^{\\alpha}+\\|\\tilde{W}\\times_{2}\\tilde{R}_{:d:}\\|_{F}^{\\alpha}+\\|\\tilde{W}\\times_{3}\\tilde{T}_{:d:}\\|_{F}^{\\alpha})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. To simplify the notations, we denote $\\hat{H}_{:d:}$ as $\\hat{H},\\,\\hat{R}_{:d:}$ as $\\hat{R}$ and $\\hat{\\pmb{T}}_{:d:}$ as $\\hat{\\pmb T}$ . To prove the inequality, we only need to prove ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\sqrt{\\lambda_{1}/\\lambda_{4}}(\\|\\hat{\\pmb{H}}\\|_{F}^{\\alpha}+\\|\\hat{\\pmb{H}}\\|_{F}^{\\alpha}+\\|\\hat{\\pmb{T}}\\|_{F}^{\\alpha})}\\\\ &{+\\sqrt{\\lambda_{4}/\\lambda_{1}}(\\|\\hat{\\pmb{W}}\\times_{2}\\hat{\\pmb{R}}\\times_{3}\\hat{\\pmb{T}}\\|_{F}^{\\alpha}+\\|\\hat{\\pmb{W}}\\times_{3}\\hat{\\pmb{T}}\\times_{1}\\hat{\\pmb{H}}\\|_{F}^{\\alpha}+\\|\\hat{\\pmb{W}}\\times_{1}\\hat{\\pmb{H}}\\times_{2}\\hat{\\pmb{R}}\\|_{F}^{\\alpha})}\\\\ &{>\\sqrt{\\lambda_{2}/\\lambda_{3}}(\\|\\hat{\\pmb{T}}\\|_{F}^{\\alpha}\\|\\hat{\\pmb{H}}\\|_{F}^{\\alpha}+\\|\\hat{\\pmb{T}}\\|_{F}^{\\alpha}\\|\\hat{\\pmb{H}}\\|_{F}^{\\alpha}+\\|\\hat{\\pmb{R}}\\|_{F}^{\\alpha}\\|\\hat{\\pmb{H}}\\|_{F}^{\\alpha})}\\\\ &{+\\sqrt{\\lambda_{3}/\\lambda_{2}}(\\|\\hat{\\pmb{W}}\\times_{1}\\hat{\\pmb{H}}\\|_{F}^{\\alpha}+\\|\\hat{\\pmb{W}}\\times_{2}\\hat{\\pmb{R}}\\|_{F}^{\\alpha}+\\|\\hat{\\pmb{W}}\\times_{3}\\hat{\\pmb{T}}\\|_{F}^{\\alpha})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "aL edt $c_{1}=\\sqrt{\\lambda_{1}\\lambda_{3}}/\\sqrt{\\lambda_{1}\\lambda_{3}}$ , ufso rw aen oy $\\mathbf{\\deltaX}$ , $\\begin{array}{r}{\\pmb{X}=\\sum_{d=1}^{D/P}(c_{1}^{-3}\\hat{W})\\times_{1}\\left(c_{1}\\hat{H}_{:d:}\\right)\\times_{2}\\left(c_{1}\\hat{R}_{:d:}\\right)\\times_{3}\\left(c_{1}\\hat{T}_{:d:}\\right)}\\end{array}$ is $\\mathbf{\\deltaX}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle c_{2}(\\|\\hat{\\boldsymbol H}\\|_{F}^{\\alpha}+\\|\\hat{\\boldsymbol R}\\|_{F}^{\\alpha}+\\|\\hat{\\boldsymbol T}\\|_{F}^{\\alpha})+\\frac{1}{c_{2}}(\\|\\hat{\\boldsymbol W}\\times_{2}\\hat{\\boldsymbol R}\\times_{3}\\hat{\\boldsymbol T}\\|_{F}^{\\alpha}+\\|\\hat{\\boldsymbol W}\\times_{3}\\hat{\\boldsymbol T}\\times_{1}\\hat{\\boldsymbol H}\\|_{F}^{\\alpha}+\\|\\hat{\\boldsymbol W}\\times_{1}\\hat{\\boldsymbol H}\\times_{2}\\hat{\\boldsymbol R}\\|_{F}^{\\alpha})}\\\\ {\\displaystyle\\succ c_{2}(\\|\\hat{\\boldsymbol T}\\|_{F}^{\\alpha}\\|\\hat{\\boldsymbol R}\\|_{F}^{\\alpha}+\\|\\hat{\\boldsymbol T}\\|_{F}^{\\alpha}\\|\\hat{\\boldsymbol H}\\|_{F}^{\\alpha}+\\|\\hat{\\boldsymbol R}\\|_{F}^{\\alpha}\\|\\hat{\\boldsymbol H}\\|_{F}^{\\alpha})+\\frac{1}{c_{2}}(\\|\\hat{\\boldsymbol W}\\times_{1}\\hat{\\boldsymbol H}\\|_{F}^{\\alpha}+\\|\\hat{\\boldsymbol W}\\times_{2}\\hat{\\boldsymbol R}\\|_{F}^{\\alpha}+\\|\\hat{\\boldsymbol W}\\times_{3}\\hat{\\boldsymbol T}\\|_{F}^{\\alpha})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $c_{2}=\\sqrt{\\lambda_{1}^{2}\\lambda_{3}}/\\sqrt{\\lambda_{2}\\lambda_{4}^{2}}$ . ", "page_idx": 18}, {"type": "text", "text": "If $X\\in\\mathbb{R}^{1\\times1\\times1}$ , let $\\begin{array}{r}{c=X^{2}+100,\\hat{H}=\\hat{R}=\\hat{T}=c,\\hat{W}=\\frac{X}{c^{3}}}\\end{array}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle c_{2}(\\|\\hat{\\boldsymbol{H}}\\|_{F}^{\\alpha}+\\|\\hat{\\boldsymbol{H}}\\|_{F}^{\\alpha}+\\|\\hat{\\boldsymbol{T}}\\|_{F}^{\\alpha})+\\frac{1}{c_{2}}(\\|\\hat{\\boldsymbol{W}}\\times_{2}\\hat{\\boldsymbol{R}}\\times_{3}\\hat{\\boldsymbol{T}}\\|_{F}^{\\alpha}+\\|\\hat{\\boldsymbol{W}}\\times_{3}\\hat{\\boldsymbol{T}}\\times_{1}\\hat{\\boldsymbol{H}}\\|_{F}^{\\alpha}+\\|\\hat{\\boldsymbol{W}}\\times_{1}\\hat{\\boldsymbol{H}}\\times_{2}\\hat{\\boldsymbol{R}}\\|_{F}^{\\alpha})}\\\\ {\\displaystyle c_{2}(\\boldsymbol{X}^{2}+100)^{2}+\\frac{1}{c_{2}}\\frac{X^{2}}{(X^{2}+100)^{2}}}\\\\ {\\displaystyle\\zeta c_{2}(\\|\\hat{\\boldsymbol{T}}\\|_{F}^{\\alpha}\\|\\hat{\\boldsymbol{R}}\\|_{F}^{\\alpha}+\\|\\hat{\\boldsymbol{T}}\\|_{F}^{\\alpha}\\|\\hat{\\boldsymbol{H}}\\|_{F}^{\\alpha}+\\|\\hat{\\boldsymbol{R}}\\|_{F}^{\\alpha}\\|\\hat{\\boldsymbol{H}}\\|_{F}^{\\alpha})+\\frac{1}{c_{2}}(\\|\\hat{\\boldsymbol{W}}\\times_{1}\\hat{\\boldsymbol{H}}\\|_{F}^{\\alpha}+\\|\\hat{\\boldsymbol{W}}\\times_{2}\\hat{\\boldsymbol{R}}\\|_{F}^{\\alpha}+\\|\\hat{\\boldsymbol{W}}\\times_{3}\\hat{\\boldsymbol{T}}\\|_{F}^{\\alpha})}\\\\ {\\displaystyle c_{2}(\\boldsymbol{X}^{2}+100)^{4}+\\frac{1}{c_{2}}\\frac{X^{2}}{(X^{2}+100)^{4}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For any $\\pmb{X}\\in\\mathbb{R}^{n_{1}\\times n_{2}\\times n_{3}}$ , $\\operatorname*{max}\\{n_{1},n_{2},n_{3}\\}>1$ , let $\\hat{W}$ be a diagonal tensor, i.e., $\\hat{W}_{i,j,k}=1$ if $i=$ $j=k$ and otherwise 0 and let $\\hat{W}\\,\\in\\,\\mathbb{R}^{n_{1}n_{2}n_{3}\\times n_{1}n_{2}n_{3}},\\hat{H}\\,\\in\\,\\mathbb{R}^{n_{1}\\times n_{1}n_{2}n_{3}},\\hat{R}\\,\\in\\,\\mathbb{R}^{n_{2}\\times n_{1}n_{2}n_{3}},\\hat{T}\\,\\in$ $\\mathbb{R}^{n_{3}\\times n_{1}n_{2}n_{3}}$ , $\\hat{H}_{i,m}=X_{i j k},\\hat{R}_{j,m}=1,\\hat{T}_{k,m}=1$ , $m=(i-1)n_{2}n_{3}+(j-1)n_{3}+k$ and otherwise 0. An example of $\\pmb{X}\\in\\mathbb{R}^{2\\times2\\times2}$ is as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\pmb{H}}=\\left(\\begin{array}{c c c c c c}{X_{1,1,1}}&{X_{1,1,2}}&{X_{1,2,1}}&{X_{1,2,2}}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}&{X_{2,1,1}}&{X_{2,1,2}}&{X_{2,2,1}}&{X_{2,2,2}}\\end{array}\\right)}\\\\ &{\\hat{\\pmb{R}}=\\left(\\begin{array}{c c c c c c}{1}&{1}&{0}&{0}&{1}&{1}&{0}&{0}\\\\ {0}&{0}&{1}&{1}&{0}&{0}&{1}&{1}\\end{array}\\right)\\hat{\\pmb{T}}=\\left(\\begin{array}{c c c c c c}{1}&{0}&{1}&{0}&{1}&{0}&{1}&{0}\\\\ {0}&{1}&{0}&{1}&{0}&{1}&{0}&{1}\\end{array}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus $X=\\hat{W}\\times_{1}\\hat{H}\\times_{2}\\hat{R}\\times_{3}\\hat{T}$ is a decomposition of $\\mathbf{\\deltaX}$ , for $X=(c_{1}^{-3}\\hat{W})\\times_{1}(c_{1}\\hat{H})\\times_{2}(c_{1}\\hat{R})\\times_{3}$ $(c_{1}\\hat{T})$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle c_{2}(\\|{\\hat{\\cal H}}\\|_{F}^{\\alpha}+\\|{\\hat{\\cal H}}\\|_{F}^{\\alpha}+\\|{\\hat{\\cal T}}\\|_{F}^{\\alpha})+\\displaystyle\\frac{1}{c_{2}}(\\|{\\hat{\\cal W}}\\times_{3}{\\hat{\\cal H}}\\|_{F}^{\\alpha}+\\|{\\hat{\\cal W}}\\times_{3}{\\hat{\\cal T}}\\|_{F}^{\\alpha}+\\|{\\hat{\\cal W}}\\times_{1}{\\hat{\\cal H}}\\|_{F}^{\\alpha}+\\|{\\hat{\\cal W}}\\times_{1}{\\hat{\\cal H}}\\times_{2}{\\hat{\\cal H}}}\\\\ {\\displaystyle-c_{2}(\\|{\\hat{\\cal H}}\\|_{F}^{\\alpha}+\\|{\\hat{\\cal H}}\\|_{F}^{\\alpha}+\\|{\\hat{\\cal T}}\\|_{F}^{\\alpha})+\\displaystyle\\frac{1}{c_{2}}(\\|{\\hat{\\cal W}}_{(1)}({\\hat{\\cal T}}\\otimes{\\hat{\\cal R}})^{T}\\|_{F}^{\\alpha}+\\|{\\hat{\\cal W}}_{(2)}({\\hat{\\cal T}}\\otimes{\\hat{\\cal H}})^{T}\\|_{F}^{\\alpha}+\\|{\\hat{\\cal W}}_{(3)}({\\hat{\\cal R}}\\otimes{\\hat{\\cal H}})^{T}\\|_{F}^{\\alpha}+\\|{\\hat{\\cal W}}\\|_{F}^{\\alpha}}\\\\ {\\displaystyle\\lesssim\\!\\frac{1}{c_{2}}(\\|({\\hat{\\cal F}}\\otimes{\\hat{\\cal R}})^{T}\\|_{F}^{\\alpha}+\\|({\\hat{\\cal T}}\\otimes{\\hat{\\cal H}})^{T}\\|_{F}^{\\alpha}+\\|({\\hat{\\cal R}}\\otimes{\\hat{\\cal H}})^{T}\\|_{F}^{\\alpha})+c_{2}(\\|{\\hat{\\cal H}}\\|_{F}^{\\alpha}+\\|{\\hat{\\cal H}}\\|_{F}^{\\alpha}+\\|{\\hat{\\cal F}}\\|_{F}^{\\alpha})}\\\\ {\\displaystyle=\\!c_{2}(\\|{\\hat{\\cal T}}\\|_{F}^{\\alpha}\\|{\\hat{\\cal R}}\\|_{F}^{\\alpha}+\\|{\\hat{\\cal T}}\\|_{F}^{\\alpha}\\|{\\hat{\\cal H}}\\|_{F}^{\\alpha}+\\|{\\hat{\\cal R}}\\|_{F}^{\\alpha}\\|_{F}^{\\alpha} \n$$\u2225F\u03b1 ) ", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For any $\\pmb{X}\\in\\mathbb{R}^{n_{1}\\times n_{2}\\times n_{3}}$ , let $\\tilde{W}=X/2\\sqrt{2},\\tilde{H}=\\sqrt{2}I_{n_{1}\\times n_{1}},\\tilde{R}=\\sqrt{2}I_{n_{2}\\times n_{2}},\\tilde{T}=\\sqrt{2}I_{n_{3}\\times n_{3}},$ thus ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle c_{2}(\\|\\tilde{H}\\|_{F}^{\\alpha}+\\|\\tilde{R}\\|_{F}^{\\alpha}+\\|\\tilde{T}\\|_{F}^{\\alpha})+\\frac{1}{c_{2}}(\\|\\tilde{W}\\times_{2}\\tilde{R}\\times_{3}\\tilde{T}\\|_{F}^{\\alpha}+\\|\\tilde{W}\\times_{3}\\tilde{T}\\times_{1}\\tilde{H}\\|_{F}^{\\alpha}+\\|\\tilde{W}\\times_{1}\\tilde{H}\\times_{2}\\tilde{H}\\|_{F}^{\\alpha})}\\\\ {\\displaystyle c_{2}(2n_{1}+2n_{2}+2n_{3})+\\frac{1}{c_{2}}(\\|{\\cal X}_{(1)}\\|_{F}^{2}/2+\\|{\\cal X}_{(2)}\\|_{F}^{2}/2+\\|{\\cal X}_{(3)}\\|_{F}^{2}/2)}\\\\ {\\displaystyle c_{2}(\\|\\tilde{T}\\|_{F}^{\\alpha}\\|\\tilde{R}\\|_{F}^{\\alpha}+\\|\\tilde{T}\\|_{F}^{\\alpha}\\|\\tilde{H}\\|_{F}^{\\alpha}+\\|\\tilde{R}\\|_{F}^{\\alpha}\\|\\tilde{H}\\|_{F}^{\\alpha})+\\frac{1}{c_{2}}(\\|\\tilde{W}\\times_{1}\\tilde{H}\\|_{F}^{\\alpha}+\\|\\tilde{W}\\times_{2}\\tilde{R}\\|_{F}^{\\alpha}+\\|\\tilde{W}\\times_{3}\\tilde{T}\\|_{F}^{\\alpha})}\\\\ {\\displaystyle c_{2}(4n_{3}n_{2}+4n_{3}n_{1}+4n_{2}n_{1})+\\frac{1}{c_{2}}(\\|{\\cal X}_{(1)}\\|_{F}^{2}/4+\\|{\\cal X}_{(2)}\\|_{F}^{2}/4+\\|{\\cal X}_{(3)}\\|_{F}^{2}/4)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus if $\\|{\\pmb X}_{(1)}\\|_{F}^{2}>16c_{2}^{2}(n_{3}n_{2}+n_{3}n_{1}+n_{2}n_{1})$ , we have ", "page_idx": 18}, {"type": "text", "text": "$\\begin{array}{l}{\\displaystyle c_{2}(\\|\\tilde{\\boldsymbol{H}}\\|_{F}^{\\alpha}+\\|\\tilde{\\boldsymbol{R}}\\|_{F}^{\\alpha}+\\|\\tilde{\\boldsymbol{T}}\\|_{F}^{\\alpha})+\\frac{1}{c_{2}}(\\|\\tilde{\\boldsymbol{W}}\\times_{2}\\tilde{\\boldsymbol{R}}\\times_{3}\\tilde{\\boldsymbol{T}}\\|_{F}^{\\alpha}+\\|\\tilde{\\boldsymbol{W}}\\times_{3}\\tilde{\\boldsymbol{T}}\\times_{1}\\tilde{\\boldsymbol{H}}\\|_{F}^{\\alpha}+\\|\\tilde{\\boldsymbol{W}}\\times_{1}\\tilde{\\boldsymbol{H}}\\times_{2}\\tilde{\\boldsymbol{R}}\\|_{F}^{\\alpha})}\\\\ {\\displaystyle\\sim c_{2}(\\|\\tilde{\\boldsymbol{T}}\\|_{F}^{\\alpha}\\|\\tilde{\\boldsymbol{R}}\\|_{F}^{\\alpha}+\\|\\tilde{\\boldsymbol{T}}\\|_{F}^{\\alpha}\\|\\tilde{\\boldsymbol{H}}\\|_{F}^{\\alpha}+\\|\\tilde{\\boldsymbol{R}}\\|_{F}^{\\alpha}\\|\\tilde{\\boldsymbol{H}}\\|_{F}^{\\alpha})+\\frac{1}{c_{2}}(\\|\\tilde{\\boldsymbol{W}}\\times_{1}\\tilde{\\boldsymbol{H}}\\|_{F}^{\\alpha}+\\|\\tilde{\\boldsymbol{W}}\\times_{2}\\tilde{\\boldsymbol{R}}\\|_{F}^{\\alpha}+\\|\\tilde{\\boldsymbol{W}}\\times_{3}\\tilde{\\boldsymbol{T}}\\|_{F}^{\\alpha})}\\end{array}$ \u2225F\u03b1 ) > \u2225F\u03b1 ) end the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "C Experimental details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Datasets The statistics of the datasets, WN18RR, FB15k-237, YAGO3-10 and Kinship, are shown in Table 4. ", "page_idx": 19}, {"type": "table", "img_path": "d226uyWYUo/tmp/46103c9554b9de607cf066389080e8cfba3b9eb7e46097bb16e2dd13c251eb35.jpg", "table_caption": ["Table 4: The statistics of the datasets. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 5: The settings for total embedding dimension $D$ and number of parts $P$ . ", "page_idx": 19}, {"type": "table", "img_path": "d226uyWYUo/tmp/05bc53447fa5b90bbc435f415e3311d7c66e68b70df96b41369edb9fa44aec9f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "d226uyWYUo/tmp/77cfa5bb2b9326b0a43a0d6b79b328d0729376bda601e930ca8ba8a7c202b172.jpg", "table_caption": ["Table 6: The settings for power $\\alpha$ and regularization coefficients $\\lambda_{i}$ "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Evaluation Metrics $\\begin{array}{r}{\\mathrm{MR}{=}\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{rank}_{i}}\\end{array}$ , where $\\mathrm{rank}_{i}$ is the rank of ith triplet in the test set and $N$ is the number of the triplets. Lower MR indicates better performance. ", "page_idx": 19}, {"type": "text", "text": "$\\begin{array}{r}{\\mathrm{MRR}{=}\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{\\mathrm{rank}_{i}}}\\end{array}$ . Higher MRR indicates better performance. ", "page_idx": 19}, {"type": "text", "text": "$\\begin{array}{r}{\\mathrm{{Hits}}@\\mathbf{N}=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{I}(\\mathrm{rank}_{i}\\leq N)}\\end{array}$ , where $\\mathbb{I}(\\cdot)$ is the indicator function. Hits $\\mathbb{\\mathrm{(}}\\mathbb{\\alpha}\\mathrm{\\mathbf{N}}$ is the ratio of the ranks that no more than $N$ , Higher Hits $\\mathbb{\\mathrm{(}}\\mathbb{\\alpha}\\mathrm{\\mathbf{N}}$ indicates better performance. ", "page_idx": 19}, {"type": "text", "text": "Hyper-parameters We use a heuristic approach to choose the hyper-parameters and reduce the computation cost with the help of Hyperopt, a hyper-parameter optimization framework based on TPE [Bergstra et al., 2011]. ", "page_idx": 19}, {"type": "text", "text": "For the hyper-parameter $\\alpha$ , we search for the best $\\alpha$ in $\\{2.0,2.25,2.5,2.75,3.0,3.25,3.5\\}$ with fixed hyper-parameters $\\lambda_{i}=0.0001$ . ", "page_idx": 19}, {"type": "text", "text": "For the hyper-parameter $\\lambda_{i}$ , we set $\\lambda_{1}=\\lambda_{3}$ and $\\lambda_{2}=\\lambda_{4}$ for all models to reduce the number of hyper-parameters because we notice that the first row of Eq.(4) $\\|\\pmb{H}_{i d:}\\|_{F}^{\\alpha}+\\|\\pmb{R}_{j d:}\\|_{F}^{\\alpha}+\\|\\pmb{T}_{k d:}\\|_{F}^{\\alpha}$ is equal to the third row of Eq.(4) $\\|\\pmb{W}\\times_{1}\\pmb{H}_{i d:}\\|_{F}^{\\alpha}+\\|\\pmb{W}\\times_{2}\\pmb{R}_{j d:}\\|_{F}^{\\alpha}+\\|\\pmb{W}\\times_{3}\\pmb{T}_{k d:}\\|_{F}^{\\alpha}$ and the second row of Eq.(4) $\\|T_{k d:}\\|_{F}^{\\alpha}\\|R_{j d:}\\|_{F}^{\\alpha}\\,+\\|T_{k d:}\\|_{F}^{\\alpha}\\|H_{i d:}\\|_{F}^{\\alpha}\\,+\\|R_{j d:}\\|_{F}^{\\alpha}\\|H_{i d:}\\|_{F}^{\\alpha}$ is equal to the fourth row of Eq.(4) $\\|W\\,\\times_{2}\\,R_{j d};\\,\\times_{3}\\,T_{k d:}\\|_{F}^{\\alpha}\\,+\\,\\|W\\,\\times_{3}\\,T_{k d:}\\,\\times_{1}\\,H_{i d:}\\|_{F}^{\\alpha}\\,+\\,\\|W\\,\\times_{1}\\,H_{i d:}\\,\\times_{2}\\,H_{i d:}\\|_{F}^{\\alpha}\\,.$ $R_{j d:}||\\boldsymbol{\\alpha}|$ for CP and ComplEx. We then search the regularization coefficients $\\lambda_{1}$ and $\\lambda_{2}$ in $\\{0.001,0.003,0.005,0.007,0.01,0.03,0.05,0.07\\}$ for WN18RR dataset and FB15k-237 dataset, and search $\\lambda_{1}$ and $\\lambda_{2}$ in $\\{0.0001,0.0003,0.0005,0.0007,0.001,0.003,0.05,0.007\\}$ for YAGO3-10 dataset. Thus, we need 64 runs for each model to find the best $\\lambda_{i}$ . To further reduce the number of runs, we use Hyperopt, a hyper-parameter optimization framework based on TPE [Bergstra et al., 2011], to tune hyper-parameters. In our experiments, we only need 20 runs to find the best $\\lambda_{i}$ . ", "page_idx": 19}, {"type": "table", "img_path": "d226uyWYUo/tmp/2f6acfaee55f531b6169bae749a201f9f7343626b0e1d2cb64d36c08ea7fda51.jpg", "table_caption": ["Table 7: The results on WN18RR dataset with different $\\alpha$ . "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "We use Adagrad [Duchi et al., 2011] with learning rate 0.1 as the optimizer. We set the batch size to 100 for WN18RR dataset and FB15k-237 dataset and 1000 for YAGO3-10 dataset. We train the models for 200 epochs. The settings for total embedding dimension $D$ and number of parts $P$ are shown in Table 5. The settings for power $\\alpha$ and regularization coefficients $\\lambda_{i}$ are shown in Table 6. ", "page_idx": 20}, {"type": "text", "text": "Random Initialization We run each model three times with different random seeds and report the mean results. We do not report the error bars because our model has very small errors with respect to random initialization. The standard deviations of the results are very small. For example, the standard deviations of MRR, $\\operatorname{H}\\!\\circledcirc1$ and $\\mathrm{H}@10$ of CP model with our regularization are 0.00037784, 0.00084755 and 0.00058739 on WN18RR dataset, respectively. This indicates that our model is not sensitive to the random initialization. ", "page_idx": 20}, {"type": "text", "text": "The hyper-parameter $\\alpha$ We analyze the impact of the hyper-parameter, the power of the Frobenius norm $\\alpha$ . We run experiments on WN18RR dataset with ComplEx model. We set $\\alpha$ to $\\{2.0,2.25,2.5,2.75,3.0,3.25,3.5\\}$ . See Table 7 for the results. ", "page_idx": 20}, {"type": "text", "text": "The results show that the performance generally increases as $\\alpha$ increases and then decreases as $\\alpha$ increases. The best $\\alpha$ for WN18RR dataset is 3.0. Therefore, we should set a more appropriate $\\alpha$ value to obtain better performance. ", "page_idx": 20}, {"type": "text", "text": "The hyper-parameter $\\lambda_{i}$ We analyze the impact of the hyper-parameter, the regularization coefficient $\\lambda_{i}$ . We run experiments on WN18RR dataset with ComplEx model. We set $\\lambda_{i}$ to $\\{0.001,0.003,0.005,0.007,0.01,0.03,0.05,0.07,0.1\\}$ . See Table 8 and Table 9 for the results. ", "page_idx": 20}, {"type": "text", "text": "The experimental results show that the model performance first increases and then decreases with the increase of $\\lambda_{i}$ , without any oscillation. Thus, we can choose suitable regularization coefficients to prevent overfitting while maintaining the expressiveness of TDB models as much as possible. ", "page_idx": 20}, {"type": "text", "text": "The Number of Parts $P$ In Section 3.1, we show that the number of parts $P$ can affect the expressiveness and computation. Thus, we study the impact of $P$ on the model performance. We evaluate the model on WN18RR dataset. We set the total embedding dimension $D$ to 256, and set the parts $P$ to $\\{1,2,4,8,16,32,64,128,256\\}$ . See Table 10 for the results. The time is the AMD Ryzen 7 4800U CPU running time on the test set. ", "page_idx": 20}, {"type": "text", "text": "The results show that the model performance generally improves and the running time generally increases as $P$ increases. Thus, the larger the part $P$ , the more expressive the model and the more the computation. ", "page_idx": 20}, {"type": "text", "text": "A Pseudocode for IVR We present a pseudocode of our method in Alg.(1). ", "page_idx": 20}, {"type": "text", "text": "Table 8: The performance of ComplEx on WN18RR dataset with different $\\lambda_{1}$ . ", "page_idx": 21}, {"type": "table", "img_path": "d226uyWYUo/tmp/81b1f400cf7d03987301edf680b8e77da745c9e8d87f62d5cc461482b365f0d5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 9: The performance of ComplEx on WN18RR dataset with different $\\lambda_{2}$ . ", "page_idx": 21}, {"type": "table", "img_path": "d226uyWYUo/tmp/0110b9c4ce58fa4dd358dcf135a2c70e16d109b279f7d6574e2194915233e06c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "d226uyWYUo/tmp/fe36cef06c72035344c8d3c568f34558f84831becaf5bb685f145f1c6ccaf950.jpg", "table_caption": ["Table 10: The results on WN18RR dataset with different $P$ . "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Algorithm 1 A pseudocode for IVR ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Input: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Core tensor: $W$ , embeddings: $(H,R,T)$ , triplet: $(i,j,k)$ . Regularization coefficients: $\\lambda_{l}(l=1,2,3,4)$ , the number of parts $P$ . power coefficients: $\\alpha$ Output: Output of model Eq.(1): $X_{i j k}$ , IVR regularization Eq.(6): $\\mathrm{reg}(X_{i j k})$ 2: 1: $\\begin{array}{r l}&{\\mathrm{Initialization:~reg:=0,}x_{1d}:=H_{i d:},x_{2d}:=R_{j d:},x_{3d}:=T_{k d:}}\\\\ &{\\mathrm{reg:=reg+}\\sum_{d=1}^{D/P}\\lambda_{1}(\\|x_{1d}\\|_{F}^{\\alpha}+\\|x_{2d}\\|_{F}^{\\alpha}+\\|x_{3d}\\|_{F}^{\\alpha})}\\\\ &{\\mathrm{reg:=reg+}\\sum_{d=1}^{D/P}\\lambda_{2}(\\|x_{1d}\\|_{F}^{\\alpha}\\|x_{2d}\\|_{F}^{\\alpha}+\\|x_{1d}\\|_{F}^{\\alpha}\\|x_{3d}\\|_{F}^{\\alpha}+\\|x_{2d}\\|_{F}^{\\alpha}\\|x_{3d}\\|_{F}^{\\alpha})}\\\\ &{x_{1d:}:=W\\times_{1}H_{i d:},x_{2d}:=W\\times_{2}R_{j d:},x_{3d}:=W\\times_{3}T_{k d:}}\\\\ &{\\mathrm{reg:=reg+}\\sum_{d=1}^{D/P}\\lambda_{3}(\\|x_{1d}\\|_{F}^{\\alpha}+\\|x_{2d}\\|_{F}^{\\alpha}+\\|x_{3d}\\|_{F}^{\\alpha})}\\\\ &{x_{1d}:=x_{1d}\\times_{2}R_{j d:},x_{2d:}=x_{2}\\times_{3}T_{k d:},x_{3d}:=x_{3d}\\times_{1}H_{i d:}}\\\\ &{\\mathrm{reg:=reg+}\\sum_{d=1}^{D/P}\\lambda_{4}(\\|x_{1d}\\|_{F}^{\\alpha}+\\|x_{2d}\\|_{F}^{\\alpha}+\\|x_{3d}\\|_{F}^{\\alpha})}\\\\ &{\\mathrm{re.~}\\sum_{\\substack{r=1}}^{D/P}\\lambda_{1}(\\|x_{1d}\\|_{F}^{\\alpha}+\\|x_{2d}\\|_{F}^{\\alpha}+\\|x_{3d}\\|_{F}^{\\alpha})}\\end{array}$ 3: 4: 56:: 7: 8: $\\begin{array}{r}{\\pmb{x}:=\\sum_{d=1}^{D/P}\\pmb{x}_{1d}\\times_{3}\\pmb{T}_{k d}}\\end{array}$ : 9: return: x, reg ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our claims reflect well the effect of our proposed regularization on knowledge graph completion. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We shortly discuss the limitations in the Conclusion Section ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have provided the detailed assumptions and proofs in the Appendix B. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have provided the experimental details in Appendix C. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have provided the code in supplemental material. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] (See Appendix C Paragraph \"Random Initialization\") ", "page_idx": 24}, {"type": "text", "text": "Justification: We have provided the experimental details in Appendix C. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have provided the information about the statistical significance of the experiments in Appendix C Paragraph \"Random Initialization\". ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have provided the related compute resources in Appendix C. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our research complies in all respects with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our paper does not use existing assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]