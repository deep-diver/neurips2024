[{"figure_path": "J42SwBemEA/figures/figures_2_1.jpg", "caption": "Figure 1: Overall architecture of SCR.", "description": "The figure illustrates the architecture of the State Chrono Representation (SCR) method.  It shows two key components: a state representation encoder that generates a representation for individual states and a chronological embedding that captures the relationship between current and future states within a trajectory.  A novel behavioral metric is learned for both state representations and temporal state pairs.  The output state representations are used as input to the policy and Q networks of a deep reinforcement learning agent.", "section": "3 State Chrono Representation"}, {"figure_path": "J42SwBemEA/figures/figures_4_1.jpg", "caption": "Figure 1: Overall architecture of SCR.", "description": "The figure illustrates the overall architecture of the State Chrono Representation (SCR) approach.  It shows two key components: a state representation, \u03c6(x), and a chronological embedding, \u03c8(x\u1d62, x\u2c7c). The state representation \u03c6(x) is generated using a behavioral metric (d) that measures the distance between states based on rewards and dynamic divergence. The chronological embedding \u03c8(x\u1d62, x\u2c7c) combines the state representations of a current state (x\u1d62) and its future state (x\u2c7c) to capture temporal relationships between them. A novel chronological behavioral metric (d\u03c8) is used to calculate the distance between these chronological embeddings. Finally, a temporal measurement (m) quantifies the sum of rewards between the current and future states, providing an additional piece of information for state representation learning. This whole process involves two state encoders and a policy network.", "section": "3 State Chrono Representation"}, {"figure_path": "J42SwBemEA/figures/figures_6_1.jpg", "caption": "Figure 3: Examples of DM_Control with (a) default setting and (b) distraction setting.", "description": "This figure shows example images from the DeepMind Control Suite environment.  Panel (a) displays images from the default setting, showing clean, simple backgrounds and consistent camera angles. Panel (b) shows images from a distraction setting, which introduces significant visual noise including a changing background video, altered robot colors, and variable camera angles. These distractions simulate real-world complexities and make the control tasks significantly more challenging.", "section": "4.1 Configurations"}, {"figure_path": "J42SwBemEA/figures/figures_7_1.jpg", "caption": "Figure 4: Aggregate metrics on distract setting.", "description": "This figure shows a comparison of different reinforcement learning methods across four metrics: Median, IQM, Mean, and Optimality Gap.  The comparison is made using a distracting setting, meaning the environment introduces elements that make the learning task more challenging. Each method is represented by a colored box, indicating its performance. The x-axis shows the normalized scores, and the y-axis implicitly represents the different reinforcement learning algorithms.  The figure visually summarizes the relative performance of various algorithms under challenging conditions.", "section": "4.2 Main Results"}, {"figure_path": "J42SwBemEA/figures/figures_8_1.jpg", "caption": "Figure 5: Training curves of SCR and baseline methods in the distraction setting of DM_Control. Mean scores on 10 runs with std (shadow shape). Training curves of all tasks are shown in Appendix B.3.", "description": "This figure presents the training curves for several deep reinforcement learning methods (SCR, SAC, DrQ, DBC, MICO, SimSR) across four different DeepMind Control tasks in a distracting setting.  Each curve represents the average reward obtained over 10 independent training runs, with the shaded area indicating the standard deviation.  The distracting setting makes the tasks more challenging by adding background video, changing object colors, or randomizing camera positions.  The figure demonstrates the superior performance and generalization of the SCR method compared to the baseline methods, especially for the tasks with sparse rewards, like cartpole_swingup_sparse.", "section": "4.2 Main Results"}, {"figure_path": "J42SwBemEA/figures/figures_8_2.jpg", "caption": "Figure 6: Ablation study on cheetah-run (left) and walker-walk (right) in the distraction setting. Mean scores on 10 runs with std (shadow shape).", "description": "This ablation study in the distraction setting of DeepMind Control Suite shows the training curves for cheetah-run and walker-walk tasks.  It compares the full SCR model against versions where components have been removed (chronological embedding, temporal measurement) or replaced (distance function). The shadow shapes represent the standard deviation over 10 runs. This figure demonstrates the relative contribution and importance of each component in SCR's overall performance.", "section": "4.3 Ablation Study"}, {"figure_path": "J42SwBemEA/figures/figures_8_3.jpg", "caption": "Figure 7: Training curves with varying sampling steps. Left: cheetah-run. Right: walker-walk. Mean scores on 10 runs with std (shadow shape).", "description": "This figure shows the impact of varying the number of sampling steps on the performance of the SCR model in two DeepMind Control Suite tasks: cheetah-run and walker-walk.  The x-axis represents training steps (environment steps), and the y-axis represents the average return (cumulative reward) over 10 runs.  Each line represents a different sampling range [1,10], [1,50], [1,100], [1,150] as well as fixed step counts of 50 and 100. The shaded area around each line represents the standard deviation, showing the variability of the results across different runs. The aim of the experiment is to determine the optimal range for sampling steps that balances efficiency and stability during the learning process. ", "section": "4.2 Main Results"}, {"figure_path": "J42SwBemEA/figures/figures_9_1.jpg", "caption": "Figure 8: Training curves in Meta-World. Mean success rates on 5 runs with std (shadow shape). Training curves of all tasks are shown in Appendix B.5.", "description": "This figure shows the training performance of various reinforcement learning methods (SAC, DrQ, DBC, MICO, SimSR, and SCR) across six different tasks in the Meta-World environment. Each line represents the average success rate over five runs, with shaded areas indicating standard deviation. The results demonstrate that SCR consistently outperforms the baselines in all tasks, showcasing its superior generalization and efficiency.", "section": "4.4 Experiments on Meta-World"}, {"figure_path": "J42SwBemEA/figures/figures_20_1.jpg", "caption": "Figure 3: Examples of DM_Control with (a) default setting and (b) distraction setting.", "description": "This figure shows example images from the DeepMind Control suite.  (a) shows examples of the default setting, demonstrating the simplicity of the backgrounds and the clarity of the robot.  (b) shows examples from the distraction setting, illustrating the added noise and complexity, including a cluttered background video, altered robot colors, and a different camera angle. This highlights the challenges in generalization that the State Chrono Representation (SCR) method aims to address.", "section": "4.1 Configurations"}, {"figure_path": "J42SwBemEA/figures/figures_21_1.jpg", "caption": "Figure 3: Examples of DM_Control with (a) default setting and (b) distraction setting.", "description": "This figure shows examples of the DeepMind Control Suite environments used in the paper's experiments.  The left image (a) depicts the default setting of the environment, while the right image (b) demonstrates the distraction setting. The distraction setting makes the control tasks significantly more challenging due to the added background video, changes in robot color, and randomized camera position.", "section": "4.1 Configurations"}, {"figure_path": "J42SwBemEA/figures/figures_21_2.jpg", "caption": "Figure 11: Training curves on MiniGrid-FourRooms [9]. Results are average over 5 seeds.", "description": "This figure shows the training curves of five different methods on MiniGrid-FourRooms environment. The methods are PPO, PPO+DBC, PPO+MICO, PPO+SimSR, and PPO+SCR. The y-axis represents the average success rate, and the x-axis represents the number of environment steps. The shaded area represents the standard deviation over 5 runs. This figure visually demonstrates the performance comparison of different methods, especially highlighting the effectiveness of PPO+SCR in achieving high success rates.", "section": "B.6 Additional Experiments on MiniGrid"}]