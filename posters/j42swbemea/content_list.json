[{"type": "text", "text": "State Chrono Representation for Enhancing Generalization in Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jianda Chen1 Wen Zheng Terence $\\mathbf{Ng}^{1,2}$ Zichen Chen3 Sinno Jialin Pan4 Tianwei Zhang1 ", "page_idx": 0}, {"type": "text", "text": "1Nanyang Technological University 2Continental Automotive Singapore 3University of California, Santa Barbara 4The Chinese University of Hong Kong {jianda001, ngwe0099}@ntu.edu.sg zichen_chen@ucsb.edu sinnopan@cuhk.edu.hk tianwei.zhang@ntu.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In reinforcement learning with image-based inputs, it is crucial to establish a robust and generalizable state representation. Recent advancements in metric learning, such as deep bisimulation metric approaches, have shown promising results in learning structured low-dimensional representation space from pixel observations, where the distance between states is measured based on task-relevant features. However, these approaches face challenges in demanding generalization tasks and scenarios with non-informative rewards. This is because they fail to capture sufficient long-term information in the learned representations. To address these challenges, we propose a novel State Chrono Representation (SCR) approach. SCR augments state metric-based representations by incorporating extensive temporal information into the update step of bisimulation metric learning. It learns state distances within a temporal framework that considers both future dynamics and cumulative rewards over current and long-term future states. Our learning strategy effectively incorporates future behavioral information into the representation space without introducing a significant number of additional parameters for modeling dynamics. Extensive experiments conducted in DeepMind Control and Meta-World environments demonstrate that SCR achieves better performance comparing to other recent metric-based methods in demanding generalization tasks. The codes of SCR are available in https://github.com/jianda-chen/SCR. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In deep reinforcement learning (Deep RL), one of the key challenges is to derive an optimal policy from highly dimensional environmental observations, particularly images [5, 12, 33]. The stream of images received by an RL agent contains temporal relationships and significant spatial redundancy. This redundancy, along with potentially distracting visual inputs, poses difficulties for the agent in learning optimal policies. Numerous studies have highlighted the importance of building state representations that can effectively distinguish task-relevant information from task-irrelevant surroundings [45, 46, 8]. Such representations have the potential to facilitate the RL process and improve the generalizability of learned policies. As a result, representation learning has emerged as a fundamental aspect in the advancement of Deep RL algorithms, gaining increased attention within the RL community [21]. ", "page_idx": 0}, {"type": "text", "text": "Related Works. The main objective of representation learning in RL is to develop a mapping function that transforms high-dimensional observations into low-dimensional embeddings. This process helps reduce the influence of redundant signals and simplify the policy learning process. Previous research has utilized autoencoder-like reconstruction losses [43, 18], which have shown impressive results in various visual RL tasks. However, due to the reconstruction of all visual input including noise, these approaches often lead to overftiting on irrelevant environmental signals. Data augmentation methods [42, 22, 23] have demonstrated promise in tasks involving noisy observations. These methods primarily enhance perception models without directly impacting the policy within the Markov Decision Process (MDP) framework. Other approaches involve learning auxiliary tasks [33], where the objective is to predict additional tasks related to the environment using the learned representation as input. However, these auxiliary tasks are often designed independently of the primary RL objective, which can potentially limit their effectiveness in improving the overall RL performance. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In recent advancements, behavioral metrics, such as the bisimulation metric [11, 10, 6, 4, 5, 47] and the MICo distance [7], have been developed to measure the dissimilarity between two states by considering differences in immediate reward signals and the divergence of next-state distributions. Behavioral metrics-based methods establish approximate metrics within the representation space, preserving the behavioral similarities among states. This means that state representations are constrained within a structured metric space, wherein each state is positioned or clustered relative to others based on their behavioral distances. Moreover, behavioral metrics have been proven to set an upper bound on state-value discrepancies between corresponding states. By learning behavioral metrics within representations, these methods selectively retain task-relevant features essential for achieving the optimal policy, which involves maximizing the value function and shaping agent behaviors. At the same time, they filter out noise that is unrelated to state values and behavioral metrics, thereby improving robustness in noisy environments. ", "page_idx": 1}, {"type": "text", "text": "However, behavioral metrics face challenges when handling demanding generalizable RL tasks and scenarios with sparse rewards [20]. While they can capture long-term behavioral metrics to some extent through the temporal-difference update mechanism, their reliance on one-step transition data limits their learning efficiency, especially in the case of sparse rewards. This limitation may result in representations that encode non-informative signals [20] and can restrict their effectiveness in capturing long-term reward information. Some model-based approaches attempt to mitigate these issues by learning transition models [14, 16, 27]. However, learning a large transition model with long trajectories requires a large number of parameters and significant computational resources. Alternatively, $N$ -step reinforcement learning methods can be used to address the problem [48]. Nevertheless, these methods introduce higher variance in value estimation compared to one-step methods, which may lead to unstable training. ", "page_idx": 1}, {"type": "text", "text": "Contributions. To overcome the above challenges, we introduce the State Chrono Representation (SCR) learning framework. This metric-based approach enables the learning of long-term behavioral representations and accumulated rewards that span from present to future states. Within the SCR framework, we propose the training of two distinct state encoders. One encoder focuses on crafting a state representation for individual states, while the other specializes in generating a Chronological Embedding that captures the relationship between a state and its future states. In addition to learning the conventional behavioral metric for state representations, we introduce a novel behavioral metric specifically designed for temporal state pairs. This new metric is approximated within the chronological embedding space. To efficiently approximate this behavioral metric in a lower-dimensional vector space, we propose an alternative distance metric that differs from the typical $L_{p}$ norm. To incorporate long-term rewards information into these representations, we introduce a \u201cmeasurement\u201d that quantifies the sum of rewards between the current and future states. Instead of directly regressing this measurement, we impose two constraints to restrict its range and value. Note that SCR is a robust representation learning methodology that can be integrated into any existing RL algorithm. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are threefold: 1) We introduce a new framework, SCR, for representation learning with a focus on behavioral metrics involving temporal state pairs. We also provide a practical method for approximating these metrics; 2) We develop a novel measurement specifically adapted for temporal state pairs and propose learning algorithms that incorporate this measurement while enforcing two constraints; 3) Through experiments conducted on the Distracting DeepMind Control Suite [34], we demonstrate that our proposed representation exhibits enhanced generalization and efficiency in challenging generalization tasks. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Markov Decision Process: A Markov Decision Process (MDP) is a mathematical framework defined by the tuple $\\mathcal{M}=(\\mathcal{S},\\mathcal{A},P,r,\\gamma)$ . Here $\\boldsymbol{S}$ represents the state space, which encompasses all possible states. $\\boldsymbol{\\mathcal{A}}$ denotes the action space, comprising all possible actions that an agent can take in each state. The state transition probability function, $P$ , defines the probability of transitioning from the current state $s_{t}\\,\\in\\,S$ to any subsequent state $s_{t+1}\\in\\mathcal S$ at time $t$ . Given the action $a_{t}\\,\\in\\,A$ taken, the probability is denoted as $P(s_{t+1}|s_{t},a_{t})$ . The reward function, $r:S\\times A\\to\\mathbb{R}$ , assigns an immediate reward $r(s_{t},a_{t})$ to the agent for taking action $a_{t}$ in state $s_{t}$ . The discount factor, $\\gamma\\in[0,1]$ , determines the present value of future rewards. ", "page_idx": 2}, {"type": "text", "text": "A policy, $\\pi:S\\times A$ , is a strategy that specifies the probability $\\pi(a|s)$ of taking action $a$ in state $s$ The objective in an MDP is to find the optimal policy $\\pi^{*}$ that maximizes the expected discounted cumulative reward, $\\begin{array}{r}{\\pi^{*}=\\arg\\operatorname*{max}_{\\pi}\\mathbb{E}_{a\\sim\\pi}[\\sum_{t}\\gamma^{t}\\bar{r}(s_{t},\\bar{a}_{t})]}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Behavioral Metric: In DBC [47], the bisimulation metric defines a pseudometric $d:S\\times S\\rightarrow\\mathbb{R}$ to measure the distance between two states. Additionally, a variant of bisimulation metric, known as $\\pi$ -bisimulation metric, is defined with respect to a specific policy $\\pi$ . ", "page_idx": 2}, {"type": "text", "text": "Theorem 2.1 ( $\\pi$ -bisimulation metric). The $\\pi$ -bisimulation metric update operator $\\mathcal{F}_{b i s i m}:\\mathbb{M}\\to\\mathbb{M}$ is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}_{b i s i m}d(\\mathbf{x},\\mathbf{y}):=|r_{\\mathbf{x}}^{\\pi}-r_{\\mathbf{y}}^{\\pi}|+\\gamma\\mathcal{W}(d)(P_{\\mathbf{x}}^{\\pi},P_{\\mathbf{y}}^{\\pi}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where M is the space of d, $\\begin{array}{r}{r_{\\mathbf{x}}^{\\pi}=\\sum_{a\\in\\mathcal{A}}\\pi(a|\\mathbf{x})r_{\\mathbf{x}}^{a},\\,P_{\\mathbf{x}}^{\\pi}=\\sum_{a\\in\\mathcal{A}}\\pi(a|\\mathbf{x})P_{\\mathbf{x}}^{a}}\\end{array}$ , $\\mathcal{W}$ is the Wasserstein distance, and $r_{\\mathbf{x}}^{a}$ is $r(\\mathbf{x},a)$ for sho rt. ${\\mathcal{F}}_{b i s i m}$ has a unique l east fixed point $d_{b i s i m}^{\\pi}$ . ", "page_idx": 2}, {"type": "text", "text": "MICo [7] defines an alternative metric that samples the next states instead of directly measuring the intractable Wasserstein distance. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2.2 (MICo distance). The MICo distance update operator $\\mathcal{F}_{M I C o}:\\mathbb{M}\\rightarrow\\mathbb{M}$ is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{F}_{M I C o}d(\\mathbf{x},\\mathbf{y}):=|r_{\\mathbf{x}}^{\\pi}-r_{\\mathbf{y}}^{\\pi}|+\\gamma\\mathbb{E}_{\\mathbf{x}^{\\prime}\\sim P_{\\mathbf{x}}^{\\pi},\\mathbf{y}^{\\prime}\\sim P_{\\mathbf{y}}^{\\pi}}d(\\mathbf{x}^{\\prime},\\mathbf{y}^{\\prime}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "${\\mathcal{F}}_{M I C o}$ has a fixed point $d_{M I C o}^{\\pi}$ ", "page_idx": 2}, {"type": "text", "text": "3 State Chrono Representation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Although the bisimulation metric [47] and MICo [7] have their strengths, they do not adequately capture future information. This limitation can degrade the effectiveness of state representations in policy learning. To address this issue and incorporate future details, we propose a novel approach, State Chrono Representation (SCR). The detailed architecture of SCR is illustrated in Figure 1. ", "page_idx": 2}, {"type": "image", "img_path": "J42SwBemEA/tmp/01ff67476bf6f16107222c2dbe85a6a9b0e79af9a659fe3df5a6f38d39c8b85d.jpg", "img_caption": ["Figure 1: Overall architecture of SCR. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "SCR consists of two key components: a state representation $\\phi(\\mathbf{x})\\in\\mathbb{R}^{n}$ for a given state $\\mathbf{x}$ , and a chronological embedding $\\psi(\\mathbf{x}_{i},\\mathbf{x}_{j})\\in\\mathbb{R}^{n}$ that captures the relationship between a current state $\\mathbf{x}_{i}$ and its future state $\\mathbf{x}_{j}$ within the same trajectory. The state representation, $\\phi(\\mathbf{x})$ , is developed using a behavioral metric $d$ that quantifies the reward and dynamic divergence between two states. On the other hand, the chronological embedding, $\\psi(\\mathbf{x}_{i},\\mathbf{x}_{j})$ , fuses these two state representations using deep learning, with a focus on capturing the long-term behavioral correlation between the current state $\\mathbf{x}_{i}$ and the future state $\\mathbf{x}_{j}$ . To compute the distance between any two chronological embeddings, we propose a \u201cchronological\u201d behavioral metric, which is further improved through a Bellman operator-like MSE loss [36]. Moreover, $\\phi(\\mathbf{x})$ incorporates a novel temporal measurement $m$ to evaluate the transition from the current state to a future state, effectively capturing sequential reward data from the optimal behavior. Due to the complexity of the regression target, this temporal measurement operates within the defined lower and upper constraints, providing a stable prediction target for both the measurement and state representation during the learning process. ", "page_idx": 2}, {"type": "text", "text": "3.1 Metric Learning for State Representation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The state representation encoder $\\phi$ is trained by approximating a behavioral metric, such as the MICo distance. In our model, we utilize a MICo-based metric transformation operator. Instead of using sampling-based prediction in MICo, we employ latent dynamics-based modeling to assess the divergence between two subsequent state distributions. This approach draws inspiration from the methodology in SimSR [45]. The metric update operator for latent dynamics, denoted by $\\mathcal{F}$ , is defined as follows. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1. Let $\\hat{d}\\ :\\ \\mathbb{R}^{n}\\ \\times\\ \\mathbb{R}^{n}\\ \\to\\ \\mathbb{R}$ be a metric in the latent state representation space, $d_{\\phi}(\\mathbf{x}_{i},\\mathbf{y}_{i^{\\prime}})\\,:=\\,\\hat{d}(\\phi(\\mathbf{x}_{i}),\\phi(\\mathbf{y}_{i^{\\prime}}))$ be a metric in the state domain. The metric update operator $\\mathcal{F}$ is defined as, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}d_{\\phi}(\\mathbf{x}_{i},\\mathbf{y}_{i^{\\prime}})=|r_{\\mathbf{x}_{i}}-r_{\\mathbf{y}_{i^{\\prime}}}|+\\gamma\\mathbb{E}_{\\mathbf{\\Phi}_{\\phi(\\mathbf{x}_{i+1})\\sim\\hat{P}(\\cdot|\\phi(\\mathbf{x}_{i}),a_{\\mathbf{x}_{i}})}}\\mathbf{\\Phi}\\hat{d}(\\phi(\\mathbf{x}_{i+1}),\\phi(\\mathbf{y}_{i^{\\prime}+1})),}\\\\ {\\phi(\\mathbf{y}_{i^{\\prime}+1})\\!\\sim\\!\\hat{P}(\\cdot|\\phi(\\mathbf{y}_{i^{\\prime}}),a_{\\mathbf{y}_{i^{\\prime}}})}\\\\ {a_{\\mathbf{x}_{i}},a_{\\mathbf{y}_{i^{\\prime}}}\\!\\sim\\!\\pi}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $a_{\\mathbf{x}_{i}}$ and $\\boldsymbol{a}_{\\mathbf{y}_{i^{\\prime}}}$ are the actions in states $\\mathbf{x}_{i}$ and $\\mathbf{y}_{i^{\\prime}}$ , respectively, and $\\hat{P}$ is the learned latent dynamics model. $\\scriptstyle{\\dot{\\mathcal{F}}}$ has a fixed point $d_{\\phi}^{\\pi}$ . ", "page_idx": 3}, {"type": "text", "text": "Proof. Refer to Appendix A.3.1 for the detailed proof. ", "page_idx": 3}, {"type": "text", "text": "To learn the approximation for $d_{\\phi}^{\\pi}$ in the embedding space, one needs to specify the form of distance $\\hat{d}$ for latent vectors. Castro et al. [7] showed that a behavioral metric with a sample-based next state distribution divergence is a diffuse metric, as the divergence of the next state distribution corresponds to the \u0141ukaszyk-Karmowski distance [49] that measures the expected distance between two samples drawn from two distributions, respectively (detailed definition is in Appendix A.4.1). ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2 (Diffuse metric [7]). A function $d:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ based on a vector space $\\mathcal{X}$ is a diffuse metric if the following axioms hold: 1) $d(\\mathbf{a},\\mathbf{b})\\geq0$ for any $\\mathbf{a},\\mathbf{b}\\in\\mathcal{X};2)$ ) $d(\\mathbf{a},\\bar{\\mathbf{b}})=d(\\mathbf{b},\\mathbf{a})$ for any $\\mathbf{a},\\mathbf{b}\\in\\mathcal{X};3)$ $d(\\mathbf{a},\\mathbf{b})+d(\\mathbf{b},\\mathbf{c})\\geq d(\\mathbf{a},\\mathbf{c})$ for any $\\mathbf{a},\\mathbf{b},\\mathbf{c}\\in\\mathcal{X}$ . ", "page_idx": 3}, {"type": "text", "text": "MICo provides an approximation of the behavioral metric using an angular distance: $\\hat{d}_{M I C o}({\\bf a},{\\bf b})=$ \u2225a\u222522+2\u2225b\u222522+ \u03b2\u03b8(a, b), where a, b \u2208Rn, \u03b8(a, b) represents the angle between vectors a and b, and $\\beta=\\mathbf{\\bar{\\boldsymbol{0}}}.1$ is a hyperparameter. This distance calculation includes a non-zero self-distance, which makes it compatible with expressing the \u0141ukaszyk-Karmowski distance [49]. However, the angle function $\\theta(\\mathbf{a},\\mathbf{b})$ only considers the angle between a and b, which requires computations involving cosine similarity and the arccos function. This can sometimes lead to numerical discrepancies. DBC [47] recommends using the $L_{1}$ norm with zero self-distance, which is only suitable for the Wasserstein distance. On the other hand, SimSR [45] utilizes the cosine distance, derived from cosine similarity, but it does not satisfy the triangle inequality and does not have a non-zero self-distance. ", "page_idx": 3}, {"type": "text", "text": "To address the challenges mentioned above, we propose a revised distance, $\\hat{d}({\\mathbf{a}},{\\mathbf{b}})$ , in the embedding space. It is characterized as a diffuse metric and formulated as follows: ", "page_idx": 3}, {"type": "text", "text": "Definition 3.3. We define $\\hat{d}~:~\\mathbb{R}^{n}\\,\\times\\,\\mathbb{R}^{n}\\ \\to\\ \\mathbb{R}$ as a distance function, where $\\hat{d}({\\bf a},{\\bf b})\\;\\;=\\;\\;$ $\\sqrt{\\|\\mathbf{a}\\|_{2}^{2}+\\|\\mathbf{b}\\|_{2}^{2}-\\mathbf{a}^{\\top}\\mathbf{b}}$ , for any $\\mathbf{a},\\mathbf{b}\\in\\mathbb{R}^{n}$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.4. $\\hat{d}$ is a diffuse metric. ", "page_idx": 3}, {"type": "text", "text": "Proof. Refer to Appendix A.3.2 for the detailed proof. ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.5 (Non-zero self-distance). The self-distance of $\\hat{d}$ is not strict to zero, i.e., $\\hat{d}({\\bf a},{\\bf a})\\;=\\;$ $\\|\\mathbf{a}\\|_{2}\\geq0$ . It becomes zero if and only if every element in vector a is zero. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.4 validates that $\\hat{d}$ is a diffuse metric satisfying triangle inequality, and Lemma 3.5 shows $\\hat{d}$ has non-zero self-distance capable of approximating dynamic divergence, which is a \u0141ukaszykKarmowski distance. Moreover, the structure of $\\hat{d}$ closely resembles the $L_{2}$ norm, with the only difference being that the factor of $\\mathbf{a}^{\\top}\\mathbf{b}$ is $-1$ instead of $-2$ . This construction, which involves only vector inner product and square root computations without divisions or trigonometric functions, helps avoid numerical computational issues and simplify the implementation. ", "page_idx": 3}, {"type": "text", "text": "To learn the representation function $\\phi$ , a common approach is to minimize MSE loss between the two ends of (1). The loss, which incorporates $\\hat{d}$ and is dependent on $\\phi$ , can be expressed as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\phi}(\\phi)=\\mathbb{E}_{\\mathbf{\\Phi}\\mathbf{x}_{i},\\mathbf{y}_{i^{\\prime}},\\mathbf{r}_{\\mathbf{x}_{i}},\\mathbf{r}_{\\mathbf{y}_{i^{\\prime}}\\sim\\mathcal{D}}}\\left|\\hat{d}(\\phi(\\mathbf{x}_{i}),\\phi(\\mathbf{y}_{i^{\\prime}}))\\mathbf{\\Phi}-\\middle|r_{\\mathbf{x}_{i}}-r_{\\mathbf{y}_{i^{\\prime}}}\\right|-\\gamma\\hat{d}(\\phi(\\mathbf{x}_{i+1}),\\phi(\\mathbf{y}_{i^{\\prime}+1}))\\right|^{2},}\\\\ {\\phi(\\mathbf{x}_{i+1}),\\phi(\\mathbf{y}_{i^{\\prime}+1})\\sim\\hat{P}\\,\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{D}$ represents the replay buffer or the sampled rollouts used in the RL learning process. ", "page_idx": 4}, {"type": "text", "text": "3.1.1 Long-term Temporal Information ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The loss $\\mathcal{L}_{\\phi}(\\phi)$ in Eqn. (2) heavily relies on the temporaldifference update mechanism, utilizing only one-step transition information. However, this approach fails to effectively capture and encode long-term information from trajectories. Incorporating temporal information into the representation while maintaining the structure and adhering to behavioral metrics presents a complex challenge. To address this challenge, we propose two distinct methods: Chronological Embedding (Section 3.2) and Temporal Measurement (Section 3.3). Each technique is designed ", "page_idx": 4}, {"type": "image", "img_path": "J42SwBemEA/tmp/4c7256322ac550d2f2c1fff1dd0dbe81b88b5522a469bb9805f81fe0fc37683f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "to capture the temporal essence of a rollout denoted as Figure 2: An example with two rollouts.   \n$\\tau(\\mathbf{x}_{i};\\mathbf{x}_{j})$ , which represents a sequence starting from state $\\mathbf{x}_{i}$ and reaching a future state $\\mathbf{x}_{j}$ . ", "page_idx": 4}, {"type": "text", "text": "As illustrated in Figure 2, the goal of the chronological embedding is to create a novel paired state embedding, denoted as $\\psi(\\mathbf{x}_{i},\\mathbf{x}_{j})$ , capturing the temporal representation of the rollout $\\tau(\\mathbf{x}_{i};\\mathbf{x}_{j})$ . This is achieved by transforming a pair of state representations $\\phi(\\mathbf{x}_{i})$ and $\\phi(\\mathbf{x}_{j})$ to a vector: $\\psi(\\mathbf{x}_{i},\\mathbf{x}_{j}){\\mathrel{:}}{=}\\psi(\\phi(\\mathbf{x}_{i}),\\phi(\\mathbf{x}_{j}))\\,\\in\\,\\mathbb{R}^{n}$ . The objective of learning $\\psi$ is to develop a \u201cchronological\u201d behavioral metric $d_{\\psi}$ that quantifies the distance between rollouts $\\tau(\\mathbf{x}_{i};\\mathbf{x}_{j})$ and $\\tau(\\mathbf{y}_{i^{\\prime}};\\mathbf{y}_{j^{\\prime}})$ . On the other hand, temporal measurement aims to compute a specific \u201cdistance\u201d $m$ between states $\\mathbf{x}_{i}$ and $\\mathbf{x}_{j}$ , which provides insights into the cumulative rewards accumulated during the rollout $\\tau(\\mathbf{x}_{i};\\mathbf{x}_{j})$ . ", "page_idx": 4}, {"type": "text", "text": "Note that learning the state representation solely based on either $d_{\\psi}$ or $m$ poses challenges. While $d_{\\psi}$ improves the representation expressiveness, capturing both rewards and dynamics may be less efficient in the learning process. On the other hand, learning $m$ is efficient but may not adequately capture the dynamics. Therefore, we propose a unified approach that leverages the strengths of both $d_{\\psi}$ and $m$ to overcome the challenges and improve the overall state representation. Empirical results in Section 4.3 support the effectiveness of our proposal. ", "page_idx": 4}, {"type": "text", "text": "3.2 Chronological Embedding ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The chronological embedding, denoted by $\\psi(\\mathbf{x}_{i},\\mathbf{x}_{j})\\in\\mathbb{R}^{n}$ , is designed to capture the relationship between a given state $\\mathbf{x}_{i}$ and any of its future states $\\mathbf{x}_{j}$ over the same trajectory. To capture the comprehensive behavioral knowledge, we introduce a distance function $d_{\\psi}:(S\\times\\bar{S})\\times(S\\times\\bar{S})\\rightarrow\\mathbb{R}$ , which is intended to reflect the behavioral metric and enable the encoder $\\psi$ to incorporate behavioral information. Building upon the MICo distance in Theorem 2.2, we specify the metric update operator ${\\mathcal{F}}_{C h r o n o}$ for $d_{\\psi}$ in the following theorem. Proof is detailed in Appendix A.3.3. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.6. Let $\\mathbb{M}_{\\psi}$ be the space of $d_{\\psi}$ . The metric update operator $\\mathcal{F}_{C h r o n o}:\\mathbb{M}_{\\psi}\\to\\mathbb{M}_{\\psi}$ is defined as follows, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{F}_{C h r o n o d_{\\psi}}(\\mathbf{x}_{i},\\mathbf{x}_{j},\\mathbf{y}_{i^{\\prime}},\\mathbf{y}_{j^{\\prime}})=|r_{\\mathbf{x}_{i}}-r_{\\mathbf{y}_{i^{\\prime}}}|+\\gamma\\mathbb{E}_{\\mathbf{x}_{i+1}\\sim P_{\\mathbf{x}}^{\\pi},\\mathbf{y}_{i^{\\prime}+1}\\sim P_{\\mathbf{y}}^{\\pi}}d_{\\psi}(\\mathbf{x}_{i+1},\\mathbf{x}_{j},\\mathbf{y}_{i^{\\prime}+1},\\mathbf{y}_{j^{\\prime}}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where time step $i<j$ and $i^{\\prime}<j^{\\prime}$ . ${\\mathcal{F}}_{C h r o n o}$ has a fixed point $d_{\\psi}^{\\pi}$ . ", "page_idx": 4}, {"type": "text", "text": "Here $d_{\\psi}^{\\pi}$ represents the \u201cchronological\u201d behavioral metric. Our objective is to closely approximate $d_{\\psi}^{\\pi}$ , which measures the distance between two sets of states $(\\mathbf{x}_{i},\\mathbf{x}_{j})$ and $(\\mathbf{y}_{i^{\\prime}},\\mathbf{y}_{j^{\\prime}})$ , taking into account the difference in immediate rewards and dynamics divergence. To co-learn the encoder $\\psi$ with $d_{\\psi}^{\\pi}$ , we represent $d_{\\psi}^{\\pi}$ as $d_{\\psi}^{\\pi}({\\bf x}_{i},{\\bf x}_{j},{\\bf y}_{i^{\\prime}},{\\bf y}_{j^{\\prime}})\\,:=\\,\\hat{d}(\\psi(({\\bf x}_{i},{\\bf x}_{j})),\\psi({\\bf y}_{i^{\\prime}},{\\bf y}_{j^{\\prime}}))$ , where $\\hat{d}$ is defined in Definition 3.3. Similar to Eqn. (1), we construct $d_{\\psi}^{\\pi}$ to compute the distance in the embedding space. ", "page_idx": 4}, {"type": "equation", "text": "$$\nd_{\\psi}^{\\pi}(\\mathbf{x}_{i},\\mathbf{x}_{j},\\mathbf{y}_{i^{\\prime}},\\mathbf{y}_{j^{\\prime}})=|r_{\\mathbf{x}_{i}}-r_{\\mathbf{y}_{i^{\\prime}}}|+\\gamma\\mathbb{E}_{\\mathbf{x}_{i+1}\\sim P_{\\mathbf{x}}^{\\pi},\\mathbf{y}_{i^{\\prime}+1}\\sim P_{\\mathbf{y}}^{\\pi}}\\hat{d}(\\psi(\\mathbf{x}_{i+1},\\mathbf{x}_{j}),\\psi(\\mathbf{y}_{i^{\\prime}+1},\\mathbf{y}_{j^{\\prime}})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To ensure computational efficiency, the parameters of the encoders $\\phi$ and $\\psi$ are shared. The encoder $\\psi$ extracts outputs from $\\phi$ , and the distance measure is appropriately adapted as $d_{\\psi}^{\\pi}({\\bf x}_{i},{\\bf x}_{j},{\\bf y}_{i^{\\prime}},{\\bf y}_{j^{\\prime}}):=$ $\\hat{d}(\\psi(\\phi(\\mathbf{x}_{i}),\\phi(\\mathbf{x}_{j})),\\psi(\\phi(\\mathbf{y}_{i^{\\prime}}),\\phi(\\mathbf{y}_{j^{\\prime}})))$ . The objective of learning the chronological embedding is to minimize the MSE loss between both sides of Eqn. (4) w.r.t. $\\phi$ and $\\psi$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\psi}(\\psi,\\phi)=\\mathbb{E}_{\\mathbf{x}_{i},\\mathbf{x}_{j},\\mathbf{y}_{i^{\\prime}},\\mathbf{y}_{j^{\\prime}},r_{\\mathbf{x}_{i}}}\\,\\Big\\vert\\hat{d}(\\psi(\\phi(\\mathbf{x}_{i}),\\phi(\\mathbf{x}_{j})),\\psi(\\phi(\\mathbf{y}_{i^{\\prime}}),\\phi(\\mathbf{y}_{j^{\\prime}})))-\\vert r_{\\mathbf{x}_{i}}-r_{\\mathbf{y}_{i^{\\prime}}}\\vert}\\\\ {r_{\\mathbf{y}_{i^{\\prime}}},\\mathbf{x}_{i+1},\\mathbf{y}_{i^{\\prime}+1}\\!\\sim\\!D}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Minimizing Eqn. (5) is to encourage the embeddings of similar state sequences to become closer in the embedded space, thereby strengthening the classification of similar behaviors. ", "page_idx": 5}, {"type": "text", "text": "3.3 Temporal Measurement ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To enhance the ability of SCR to gain future insights, we introduce the concept of temporal measurement, which quantifies the disparities between a current state $\\mathbf{x}_{i}$ and a future state $\\mathbf{x}_{j}$ . This measurement, denoted by $m(\\mathbf{x}_{i},\\mathbf{x}_{j})$ , aims to measure the differences in state values or the cumulative rewards obtained between the current and future states. We construct an approximate measurement based on the state representation $\\phi$ , denoted by $\\hat{m}_{\\phi}(\\mathbf{x}_{i},\\mathbf{x}_{j}):=\\hat{m}(\\phi(\\mathbf{x}_{i}),\\phi(\\bar{\\mathbf{x}}_{j}))$ . Here, $\\hat{m}:\\mathbb{R}^{n}\\times\\mathbb{R}^{n}\\to\\mathbb{R}$ is a non-parametric asymmetric metric function (details are presented at the end of this section). This approach structures the representation space of $\\phi(\\mathbf{x})$ around the \u201cdistance\u201d $m$ , ensuring that $\\phi(\\mathbf{x})$ contains sufficient information for future state planning. ", "page_idx": 5}, {"type": "text", "text": "We propose using $m$ to represent the expected discounted accumulative rewards obtained by an optimal policy $\\pi^{*}$ from state $\\mathbf{x}_{i}$ to state $\\mathbf{x}_{j}$ : $\\begin{array}{r}{m(\\mathbf{x}_{i},\\mathbf{x}_{j})=\\mathbb{E}_{\\pi^{*}}\\left[\\sum_{t=0}^{j-i}\\gamma^{t}r_{\\mathbf{s}_{t}}\\Big\\vert\\mathbf{s}_{0}=\\mathbf{x}_{i},\\mathbf{s}_{j-i}=\\mathbf{\\bar{x}}_{j}\\right]}\\end{array}$ . However, obtaining the exact value of $m(\\mathbf{x}_{i},\\mathbf{x}_{j})$ is challenging because the optimal policy $\\pi^{*}$ is unknown and is, in fact, the primary goal of the RL task. Instead of directly approximating $m(\\mathbf{x}_{i},\\mathbf{x}_{j})$ , we learn the approximation $\\hat{m}(\\mathbf{x}_{i},\\mathbf{x}_{j})$ in an alternative manner that ensures it falls within a feasible range covering the true $m(\\mathbf{x}_{i},\\mathbf{x}_{j})$ . ", "page_idx": 5}, {"type": "text", "text": "To construct this range, we introduce two constraints. The first constraint, serving as a lower boundary, states that the expected discounted cumulative reward obtained by any policy $\\pi$ , whether optimal or not, cannot exceed $m$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=0}^{j-i}\\gamma^{t}r_{\\mathbf{s}_{t}}\\bigg\\vert\\mathbf{s}_{0}=\\mathbf{x}_{i},\\mathbf{s}_{j-i}=\\mathbf{x}_{j}\\right]\\leq m(\\mathbf{x}_{i},\\mathbf{x}_{j}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This constraint is based on a fact that any sub-optimal policy is inferior to the optimal policy. Based on this constraint, we propose the following objective to learn the approximation $\\hat{m}_{\\phi}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{l o w}(\\phi)=\\mathbb{E}_{\\tau(\\mathbf{x}_{i};\\mathbf{x}_{j})\\sim\\pi}\\left|\\mathrm{ReLU}\\left(\\sum_{t=0}^{j-i}\\gamma^{t}r_{\\mathbf{x}_{t}}-\\hat{m}(\\phi(\\mathbf{x}_{i}),\\phi(\\mathbf{x}_{j}))\\right)\\right|^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\operatorname{ReLU}(x)\\,=\\,\\operatorname*{max}(0,x)$ . This objective is non-zero when the constraint in Eqn. (6) is not satisfied. Hence, it increases the value of $m(\\phi(\\mathbf{x}_{i}),\\phi(\\mathbf{x}_{j}))$ until it exceeds the sampled reward sum. ", "page_idx": 5}, {"type": "text", "text": "The second constraint serving as the upper boundary is proposed based on Fig. 2. To ensure that the absolute value $|m(\\mathbf{x}_{i},\\mathbf{x}_{j})|$ remains within certain limits, we propose the following inequality: ", "page_idx": 5}, {"type": "equation", "text": "$$\n|\\hat{m}(\\mathbf{x}_{i},\\mathbf{x}_{j})|\\leq d(\\mathbf{x}_{i},\\mathbf{y}_{i^{\\prime}})+|\\hat{m}(\\mathbf{y}_{i^{\\prime}},\\mathbf{y}_{j^{\\prime}})|+d(\\mathbf{x}_{j},\\mathbf{y}_{j^{\\prime}}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $d$ is the behavioral metric introduced in Section 3.1. The right-hand side of the inequality represents the longer path from $\\mathbf{x}_{i}$ to $\\mathbf{x}_{j}$ . This inequality demonstrates that the absolute temporal measurement $|m(\\mathbf{x}_{i},\\mathbf{x}_{j})|$ should not exceed the sum of the behavioral metrics at the initial states $\\mathbf{\\Pi}(\\mathbf{x}_{i}$ and $\\mathbf{y}_{i^{\\prime}}$ ), i.e., $d(\\mathbf{x}_{i},\\mathbf{y}_{i^{\\prime}})$ , the final states pair ${\\bf{x}}_{j}$ and $\\mathbf{y}_{j^{\\prime}}$ ), i.e. $d(\\mathbf{x}_{j},\\mathbf{y}_{j^{\\prime}})$ , and the measurement $|m({\\bf y}_{i},{\\bf y}_{j})|$ . This constraint leads to the following objective for training $\\hat{m}_{\\phi}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{\\gamma}_{u p}(\\phi)\\!=\\!\\Bigl\\lvert\\mathrm{ReLU}\\Bigl(\\!\\left\\lvert\\hat{m}(\\phi(\\mathbf{x}_{i}),\\phi(\\mathbf{x}_{j}))\\right\\rvert\\!-\\!\\mathrm{sg}\\Bigl(\\!\\hat{d}(\\phi(\\mathbf{x}_{i}),\\phi(\\mathbf{y}_{i^{\\prime}}))\\!+\\!\\hat{d}(\\phi(\\mathbf{x}_{j}),\\phi(\\mathbf{y}_{j^{\\prime}}))\\!+\\!\\hat{m}(\\phi(\\mathbf{y}_{i^{\\prime}}),\\phi(\\mathbf{y}_{j^{\\prime}}))\\!\\Bigr)\\!\\Bigr)\\!\\Bigr\\rvert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where sg means \u201cstop gradient\u201d. This objective aims to decrease the value of $\\hat{m}_{\\phi}$ when the constraint in Eqn. (8) is not satisfied. By simultaneously optimizing both constraints in a unified manner, ", "page_idx": 5}, {"type": "text", "text": "we guarantee that the approximated temporal measurement, $m$ , is confined within a specific range, defined by the lower and upper constraints. The overall objective for $\\hat{m}$ is formulated as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\hat{m}}(\\phi)=\\mathcal{L}_{l o w}(\\phi)+\\mathcal{L}_{u p}(\\phi).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Asymmetric Metric Function for $\\hat{m}$ . The measurement $\\hat{m}(\\mathbf{x}_{i},\\mathbf{x}_{j})$ , designed to measure the distance based on the rewards, is intended to be asymmetric with respect to $\\mathbf{x}_{i}$ and $\\mathbf{x}_{j}$ . This is because we assume that state $\\mathbf{x}_{i}$ comes before $\\mathbf{x}_{j}$ , resulting in a distinct relationship compared to the progression from $\\mathbf{x}_{j}$ to $\\mathbf{x}_{i}$ . Recent research has focused on studying quasimetrics in deep learning [28, 39, 38] and developed various methodologies to compute asymmetric distances. In our method, we choose to utilize Interval Quasimetric Embedding (IQE) [38] to implement $\\hat{m}$ (details are in Appendix A.5.1). ", "page_idx": 6}, {"type": "text", "text": "3.4 Overall Objective ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As shown in Figure 1, the encoders are designed to predict state representations $\\phi(\\mathbf{x})$ for individual states and chrono embedding $\\psi(\\mathbf{x}_{i},\\mathbf{x}_{j})$ to capture the relationship between states $\\mathbf{x}_{i}$ and $\\mathbf{x}_{j}$ . The measurement $\\hat{m}$ is then computed based on $\\phi(\\mathbf{x}_{i})$ and $\\phi(\\mathbf{x}_{j})$ to incorporate the accumulated rewards between these states. The components $\\psi$ and $\\hat{m}$ work together to enhance the state representations $\\phi$ and capture temporal information, thereby improving their predictive capabilities for future insights. The necessity of $\\psi$ and $\\hat{m}$ is demonstrated in the ablation study in Section 4.3. Therefore, a comprehensive objective to minimize is formulated in a unified manner as follows, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\phi,\\psi)=\\mathcal{L}_{\\phi}(\\phi)+\\mathcal{L}_{\\psi}(\\psi,\\phi)+\\mathcal{L}_{\\hat{m}}(\\phi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Our method, SCR, can be seamlessly integrated with a wide range of deep RL algorithms, which can effectively utilize the representation $\\phi(\\mathbf{x})$ as a crucial input component. In our implementation, we specifically employ Soft Actor-Critic (SAC) [13] as our foundation RL algorithm. The state representation serves as the input state for both the policy network and Q-value network in SAC. Other implementation details can be found in Appendix A.5. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Configurations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "DeepMind Control Suite. The primary objective of our proposed SCR is to develop a robust and generalizable representation for deep RL when dealing with high-dimensional observations. To evaluate its effectiveness, we conduct experiments using the DeepMind Control Suite (DM_Control) environment, which involves rendered pixel observations [37] and a distraction setting called Distracting Control Suite [34]. This environment utilizes the MuJoCo engine, offering pixel observations for continuous control tasks. The DM_Control environment provides diverse testing scenarios, including background and camera pose distractions, simulating real-world complexities of camera inputs. (1) Default setting evaluates the effectiveness of each RL approach. Frames are rendered at a resolution of $84\\times84$ RGB pixels as shown in Figure 3(a). We stack three frames as states for the RL agents. (2) Distraction setting evaluates both the generalization and effectiveness of each RL approach. The distraction [34] consists of 3 components, as shown in Figure 3(b): 1) background video distraction, where the clean and simple background is replaced with a natural video; 2) object color distraction, which involves slight changes in the color of the robots; and 3) camera pose distraction, where the camera\u2019s position and angle are randomized during rendering from the simulator. We observe that tasks become significantly more challenging when the camera pose distraction is applied. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We compare our method against several prominent algorithms in the field, including: 1) SAC [13], a baseline deep RL method for continuous control; 2) DrQ [42], a data augmentation method using random crop; 3) DBC [47], representation learning with the bisimulation metric; 4) MICo [7], representation learning with MICo distance; and 5) SimSR [45], representation learning with the behavioral metric approximated by the cosine distance. ", "page_idx": 6}, {"type": "image", "img_path": "J42SwBemEA/tmp/8a56e79fac5b57d730f873e03623aaef2d29061e67474e042398d9561a802e5f.jpg", "img_caption": ["Figure 3: Examples of DM_Control with (a) default setting and (b) distraction setting. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "J42SwBemEA/tmp/fd64ea2e2bda16891ebbf2e0d43fe1192d40fa34c39c777233b183b5394ba0cf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "J42SwBemEA/tmp/e42bb09440f493bc550aae2f02ceb2daae754914b0dbd942ad1b04a0e703748a.jpg", "table_caption": ["Table 1: Results (mean\u00b1std) on DM_Control with the default setting at 500K environment steps. ", "Table 2: Results (mean\u00b1std) on DM_Control with distraction setting at 500K environment step. Distraction includes background, robot body color, and camera pose. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "J42SwBemEA/tmp/31d6f35590970f1c5ab25bf6966e4839377391900c43c63502052ef5761d1731.jpg", "img_caption": ["Figure 4: Aggregate metrics on distract setting. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Default Setting. To verify the sample efficiency of our method, we compare it with others on eight tasks in DM_Control and report the experimental results in Table 1. We trained each method on each task for 500K environment steps. All results are averaged over 10 runs. We observe that SCR achieves comparable results to the augmentation method DrQ and the state-of-the-art behavioral metric approach SimSR. It is worth noting that for a DM_Control task, the maximum achievable scores are 1000, and a policy that collects scores around 900 is considered nearly optimal. These findings highlight the effectiveness of our SCR in mastering standard RL control tasks. ", "page_idx": 7}, {"type": "text", "text": "Distraction Setting. To further evaluate the generalization ability of our method, we perform comparison experiments on DM_Control with Distraction using the same training configuration as in the default setting. We evaluate 10 different seeds, and conduct 100 episodes per run. The scores from each run are summarized in Table 2. The std is computed across the mean scores of these 10 runs. The results show that our method outperforms the others in all eight tasks, notably in the sparse reward task cartpole-swing_up_sparse, where other methods achieve nearly zero scores. The camera-pose distraction poses a significant challenge for metric-based methods like DBC, MICo, and SimSR, as it leads to substantial distortion of the robot\u2019s shape and position in the image state. DrQ outperforms other behavioral metric methods, possibly due to its random cropping technique, which facilitates better alignment of the robot\u2019s position. The aggregate metrics [2] are shown in Figure 4, where scores are normalized by dividing by 1000. Figure 5 presents the training curves, while additional curves can be found in Appendix B.3. These results demonstrate the superior performance of our method in handling distractions and highlight our strong generalization capabilities. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Impact of Different Components. To evaluate the impact of each component in SCR, we perform an ablation study where certain components are selectively removed or substituted. Figure 6 shows the training curves on cheetah-run and walker-walk in the distraction setting. SCR denotes the full model. SCR w/o $\\psi$ removes the chronological embedding $\\psi$ . SCR w/o $\\hat{m}$ refers to the exclusion of the approximation $\\hat{m}$ . SCR w/ $\\phi$ only removes losses $\\mathcal{L}_{\\psi}(\\phi,\\psi)$ and ${\\mathcal{L}}_{\\hat{m}}(\\phi)$ but keep $\\mathcal{L}_{\\phi}(\\phi)$ . SCR w/ cos replaces the distance function $\\hat{d}$ for computing metrics on the representation space with cosine distance, akin to SimSR does. SCR w/ MICo replaces $\\hat{d}$ with MICo\u2019s angular distance. SCR w/ L1 replaces $\\hat{d}$ with the $L_{1}$ distance as adopted by DBC. The results demonstrate the superior performance of the full model and the importance of $\\psi$ and $\\hat{m}$ . The absence of these components can lead to worse performance and unstable training. The results also demonstrate that the proposed $\\hat{d}$ surpasses existing distance functions. ", "page_idx": 7}, {"type": "image", "img_path": "J42SwBemEA/tmp/bed79255cef77785e828f8e440e1b21cbf1ea641ab3d398826ff70fa3641b798.jpg", "img_caption": ["Figure 5: Training curves of SCR and baseline methods in the distraction setting of DM_Control. Mean scores on 10 runs with std (shadow shape). Training curves of all tasks are shown in Appendix B.3. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "J42SwBemEA/tmp/e07ea5d1e9fd7db1242f1e00c15d985a1595135897de7b1c8b0a605cd3393819.jpg", "img_caption": ["Figure 6: Ablation study on cheetah-run (left) and walker-walk (right) in the distraction setting. Mean scores on 10 runs with std (shadow shape). "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "J42SwBemEA/tmp/b70be3d38ce4fb5dd431c7125cf962498508f12022b4fae98b98874da7ecf566.jpg", "img_caption": ["Figure 7: Training curves with varying sampling steps. Left: cheetah-run. Right: walker-walk. Mean scores on 10 runs with std (shadow shape). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Impact of Sampling Steps. In our previous experiments, we uniformly sample the number of steps between $i$ and $j$ in the range [1, 100]. In this experiment, we investigate the impact of this hyper-parameter on SCR. We conduct experiments on the cheetah-run and walker-walk tasks in the distraction setting with various step sampling ranges: [1, 10], [1, 50], [1, 100], and [1, 150]. We also make comparisons with fixed step counts: 50 and 100. The results presented in Figure 7 show that sampling step counts in the range [1, 100] yields the optimal results and remains stable across tasks. Therefore, we set this hyper-parameter to uniformly sample in the range [1, 100] for all experiments. ", "page_idx": 8}, {"type": "text", "text": "4.4 Experiments on Meta-World ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present experimental investigations conducted in Meta-World [44], a comprehensive simulated benchmark that includes distinct robotic manipulation tasks. Our focus is on six specific tasks: basketball-v2, coffee-button-v2, door-open-v2, drawer-open- $_{\\cdot\\nu2}$ , pick-place-v2, and window-open- $\\nu2$ , chosen for their diverse objects in environments. The observations are rendered as $84\\!\\times\\!84$ RGB pixels with a frame stack of 3, following the DM_Control format. Table 3 displays the average success rates over all tasks and five seeds, while Figure 8 shows the training curves for a subset of these tasks. Additional training curves for all tasks can be found in Appendix B.5. Our proposed SCR consistently achieves optimal performance in all tasks, while other baseline methods, except for $\\mathrm{DrQ}$ , fail to converge to such high levels of performance. Even though DrQ demonstrates proficiency in achieving optimal performance, it shows less sampling efficiency than SCR, highlighting the effectiveness of SCR in the applied setting. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our proposed State Chrono Representation learning framework (SCR) effectively captures information from complex, high-dimensional, and noisy inputs in deep RL. SCR improves upon previous metricbased approaches by incorporating a long-term perspective and quantifying state distances across ", "page_idx": 8}, {"type": "table", "img_path": "J42SwBemEA/tmp/aef49ef27a0b636b169d07cf23e8c7759d8766540321d536643b8d91060f5183.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "J42SwBemEA/tmp/6cf467b02c820c8529d795a137e24946a7e2dd410bd6d7111c7b039a4eabf533.jpg", "img_caption": ["Table 3: Average success rates for six tasks in Meta-World. ", "Figure 8: Training curves in Meta-World. Mean success rates on 5 runs with std (shadow shape). Training curves of all tasks are shown in Appendix B.5. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "temporal trajectories. Our extensive experiments demonstrate its effectiveness compared with several metric-based baselines in complex environments with distractions, making a promising avenue for future research on generalization in representation learning. ", "page_idx": 9}, {"type": "text", "text": "Limitations. This work does not address truly Partially Observable MDPs (POMDPs), which are common in real-world applications. In future work, we plan to integrate SCR with POMDP methods to solve real-world problems, or to design unified solutions that better accommodate these complexities. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank the anonymous reviewers for the constructive feedback. This study is supported under RIE2020 Industry Alignment Fund \u2013 Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). Sinno J. Pan thanks the support of the Hong Kong Jockey Club Charities Trust to the JC STEM Lab of Integration of Machine Learning and Symbolic Reasoning. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] The successor representation in human reinforcement learning. Nature human behaviour, 1(9): 680\u2013692, 2017. ", "page_idx": 9}, {"type": "text", "text": "[2] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances in Neural Information Processing Systems, 2021. ", "page_idx": 9}, {"type": "text", "text": "[3] Andr\u00e9 Barreto, Will Dabney, R\u00e9mi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David Silver. Successor features for transfer in reinforcement learning. Advances in neural information processing systems, 30, 2017. ", "page_idx": 9}, {"type": "text", "text": "[4] Pablo S. Castro and Doina Precup. Using bisimulation for policy transfer in mdps. In Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Volume 1 - Volume 1, AAMAS $\\cdot\\,10$ , Richland, SC, 2010. International Foundation for Autonomous Agents and Multiagent Systems. ISBN 9780982657119. ", "page_idx": 9}, {"type": "text", "text": "[5] Pablo Samuel Castro. Scalable methods for computing state similarity in deterministic markov decision processes. Proceedings of the AAAI Conference on Artificial Intelligence, 34(06): 10069\u201310076, Apr. 2020. doi: 10.1609/aaai.v34i06.6564. URL https://ojs.aaai.org/ index.php/AAAI/article/view/6564. ", "page_idx": 9}, {"type": "text", "text": "[6] Pablo Samuel Castro, Prakash Panangaden, and Doina Precup. Equivalence relations in fully and partially observable markov decision processes. In Proceedings of the 21st International Joint Conference on Artificial Intelligence, IJCAI\u201909, San Francisco, CA, USA, 2009. Morgan Kaufmann Publishers Inc. ", "page_idx": 9}, {"type": "text", "text": "[7] Pablo Samuel Castro, Tyler Kastner, Prakash Panangaden, and Mark Rowland. MICo: Improved representations via sampling-based state similarity for markov decision processes. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id $\\equiv$ wFp6kmQELgu. [8] Jianda Chen and Sinno Pan. Learning representations via a robust behavioral metric for deep reinforcement learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 36654\u201336666. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/ paper/2022/file/eda9523faa5e7191aee1c2eaff669716-Paper-Conference.pdf.   \n[9] Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo Perez-Vicente, Lucas Willems, Salem Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks. In Advances in Neural Information Processing Systems 36, New Orleans, LA, USA, December 2023.   \n[10] Norm Ferns and Doina Precup. Bisimulation metrics are optimal value functions. UAI\u201914, pages 210\u2013219, Arlington, Virginia, USA, 2014. AUAI Press. ISBN 9780974903910.   \n[11] Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite markov decision processes. UAI \u201904, pages 162\u2013169, Arlington, Virginia, USA, 2004. AUAI Press. ISBN 0974903906.   \n[12] Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G. Bellemare. DeepMDP: Learning continuous latent space models for representation learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2170\u20132179. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr.press/v97/gelada19a.html.   \n[13] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1861\u20131870. PMLR, 10\u201315 Jul 2018. URL https://proceedings.mlr.press/v80/haarnoja18b.html.   \n[14] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2019.   \n[15] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International Conference on Machine Learning, pages 2555\u20132565, 2019.   \n[16] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023.   \n[17] Philippe Hansen-Estruch, Amy Zhang, Ashvin Nair, Patrick Yin, and Sergey Levine. Bisimulation makes analogies in goal-conditioned reinforcement learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 8407\u20138426. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/hansen-estruch22a.html.   \n[18] Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander Lerchner. DARLA: Improving zero-shot transfer in reinforcement learning. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1480\u20131490. PMLR, 06\u201311 Aug 2017. URL https://proceedings. mlr.press/v70/higgins17a.html.   \n[19] Michael Janner, Igor Mordatch, and Sergey Levine. Gamma-models: Generative temporal difference learning for infinite-horizon prediction. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1724\u20131735. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper_files/paper/2020/file/12ffb0968f2f56e51a59a6beb37b2859-Paper.pdf.   \n[20] Mete Kemertas and Tristan Ty Aumentado-Armstrong. Towards robust bisimulation metric learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum? id=ySFGlFjgIfN.   \n[21] Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rockt\u00e4schel. A survey of zero-shot generalisation in deep reinforcement learning. J. Artif. Int. Res., 76, feb 2023. ISSN 1076-9757. doi: 10.1613/jair.1.14174. URL https://doi.org/10.1613/jair.1.14174.   \n[22] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: Contrastive unsupervised representations for reinforcement learning. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 5639\u20135650. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/laskin20a.html.   \n[23] Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 19884\u201319895. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper_files/paper/2020/file/e615c82aba461681ade82da2da38004a-Paper.pdf.   \n[24] Alex X. Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 741\u2013752. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 08058bf500242562c0d031ff830ad094-Paper.pdf.   \n[25] Kuang-Huei Lee, Ian Fischer, Anthony Liu, Yijie Guo, Honglak Lee, John Canny, and Sergio Guadarrama. Predictive information accelerates learning in rl. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 11890\u201311901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 89b9e0a6f6d1505fe13dea0f18a2dcfa-Paper.pdf.   \n[26] Qiyuan Liu, Qi Zhou, Rui Yang, and Jie Wang. Robust representation learning by clustering with bisimulation metrics for visual reinforcement learning with distractions. In Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence, AAAI\u201923/IAAI\u201923/EAAI\u201923. AAAI Press, 2023. ISBN 978-1-57735-880-0. doi: 10.1609/aaai.v37i7.26063. URL https://doi.org/10.1609/ aaai.v37i7.26063.   \n[27] Tung D Nguyen, Rui Shu, Tuan Pham, Hung Bui, and Stefano Ermon. Temporal predictive coding for model-based planning in latent space. In International Conference on Machine Learning, pages 8130\u20138139. PMLR, 2021.   \n[28] Silviu Pitis, Harris Chan, Kiarash Jamali, and Jimmy Ba. An inductive bias for distances: Neural nets that respect the triangle inequality. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HJeiDpVFPr.   \n[29] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel\u00e1ez, Alexander Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv:1704.00675, 2017.   \n[30] Chris Reinke and Xavier Alameda-Pineda. Successor feature representations. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/ forum?id=MTFf1rDDEI.   \n[31] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[32] Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman. Data-efficient reinforcement learning with self-predictive representations. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id $\\equiv$ uCQfPZwRaUu.   \n[33] Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and Pieter Abbeel. Masked world models for visual control. In 6th Annual Conference on Robot Learning, 2022. URL https://openreview.net/forum?id $\\equiv$ Bf6on28H0Jv.   \n[34] Austin Stone, Oscar Ramirez, Kurt Konolige, and Rico Jonschkowski. The distracting control suite \u2013 a challenging benchmark for reinforcement learning from pixels. arXiv preprint arXiv:2101.02722, 2021.   \n[35] Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning from reinforcement learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 9870\u20139879. PMLR, 18\u201324 Jul 2021. URL https://proceedings. mlr.press/v139/stooke21a.html.   \n[36] Richard S Sutton. Reinforcement learning: An introduction. A Bradford Book, 2018.   \n[37] Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa. dm_control: Software and tasks for continuous control. Software Impacts, 6:100022, 2020. ISSN 2665-9638. doi: https:// doi.org/10.1016/j.simpa.2020.100022. URL https://www.sciencedirect.com/science/ article/pii/S2665963820300099.   \n[38] Tongzhou Wang and Phillip Isola. Improved representation of asymmetrical distances with interval quasimetric embeddings. In NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations, 2022. URL https://openreview.net/forum?id=KRiST_rzkGl.   \n[39] Tongzhou Wang and Phillip Isola. On the learning and learnability of quasimetrics. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=y0VvIg25yk.   \n[40] Tongzhou Wang, Antonio Torralba, Phillip Isola, and Amy Zhang. Optimal goal-reaching reinforcement learning via quasimetric learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 36411\u201336430. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/wang23al.html.   \n[41] Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Reinforcement learning with prototypical representations. 2021.   \n[42] Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id $\\equiv$ GY6-6sTvGaf.   \n[43] Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improving sample efficiency in model-free reinforcement learning from images. Proceedings of the AAAI Conference on Artificial Intelligence, 35(12):10674\u201310681, May 2021. doi: 10.1609/aaai.v35i12.17276. URL https://ojs.aaai.org/index.php/AAAI/article/ view/17276.   \n[44] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning (CoRL), 2019. URL https://arxiv.org/abs/ 1910.10897.   \n[45] Hongyu Zang, Xin Li, and Mingzhong Wang. Simsr: Simple distance-based state representations for deep reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence, 36(8):8997\u20139005, Jun. 2022. doi: 10.1609/aaai.v36i8.20883. URL https://ojs.aaai.org/ index.php/AAAI/article/view/20883.   \n[46] Xianghua Zeng, Hao Peng, Angsheng Li, Chunyang Liu, Lifang He, and Philip S Yu. Hierarchical state abstraction based on structural information principles. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pages 4549\u20134557, 2023.   \n[47] Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning invariant representations for reinforcement learning without reconstruction. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id $=$ -2FCwDKRREu.   \n[48] Junmin Zhong, Ruofan Wu, and Jennie Si. A long n-step surrogate stage reward for deep reinforcement learning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 12733\u201312745. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/ paper/2023/file/29ef811e72b2b97cf18dd5d866b0f472-Paper-Conference.pdf.   \n[49] Szymon \u0141ukaszyk. A new concept of probability metric and its applications in approximation of scattered data sets. Computational mechanics, 33:299\u2013304, 2004. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Broader Impact ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This paper is a research on improving the representation of deep reinforcement learning. We think that there are no potential societal consequences that need to be highlighted. ", "page_idx": 14}, {"type": "text", "text": "A.2 Additional Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Recent studies have extensively explored representation learning in reinforcement learning (RL). Some previous works [18, 24, 43] employ autoencoders to encode image states into low-dimensional latent embeddings, resulting in improved visual perception and accelerated policy learning. Other approaches [42, 22, 23, 41, 35] utilize data augmentation techniques such as random crop or noise injection, combined with contrastive loss, to learn more generalizable state representations. Another set of approaches [25, 33, 15] focus on learning representation by predicting auxiliary tasks to extract more information from the environment. Successor representations [19, 30] tackles the state occupancy distribution to learn a more informative state representation, showing generalization capabilities between tasks. ", "page_idx": 14}, {"type": "text", "text": "In recent research, metric learning methods have been employed in RL to measure distances between state representations. Some approaches aim to approximate bisimulation metrics [47, 20] while others focus on learning sample-based distances [45, 7]. Bisimulation metrics have also been utilized in goal-based RL to enhance state representation [17]. A more recent work introduces quasimetrics learning as a novel RL objective for cost MDPs [40] but it is not for general MDPs. ", "page_idx": 14}, {"type": "text", "text": "Self-predictive representation (SPR) [32] learn representations regarding a future state but it focuses on predicting future states, enforcing the representation of predicted future states to be close to the true future states. SPR\u2019s representation focuses on dynamics learning, without directly considering the reward or value. In contrast, SCR focuses on predicting a metric/distance between current state and future state and the metrics are related to rewards or values regarding policy learning. ", "page_idx": 14}, {"type": "text", "text": "Successor Representations (SRs) [1, 3, 30] are a class of methods designed to learn representations that facilitate generalization. SRs achieve this by focusing on the occupancy of states, enabling generalization across various tasks and reward functions. In contrast, SCR measures the distance between states specifically to handle observable distractions. ", "page_idx": 14}, {"type": "text", "text": "A.3 Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.3.1 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem 3. Let $\\hat{d}:\\mathbb{R}^{n}\\times\\mathbb{R}^{n}\\to\\mathbb{R}$ be a metric in the latent state representation space, $d_{\\phi}(\\mathbf{x}_{i},\\mathbf{y}_{i^{\\prime}}):=$ $\\hat{d}(\\phi(\\mathbf{x}_{i}),\\phi(\\mathbf{y}_{i^{\\prime}}))$ be a metric in the state domain. The metric update operator $\\mathcal{F}$ is defined as, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}d_{\\phi}(\\mathbf{x}_{i},\\mathbf{y}_{i^{\\prime}})=|r_{\\mathbf{x}_{i}}-r_{\\mathbf{y}_{i^{\\prime}}}|+\\gamma\\mathbb{E}_{\\mathbf{\\Phi}_{\\phi(\\mathbf{x}_{i+1})\\sim\\hat{P}(\\cdot|\\phi(\\mathbf{x}_{i}),a_{\\mathbf{x}_{i}})}}\\ \\hat{d}(\\phi(\\mathbf{x}_{i+1}),\\phi(\\mathbf{y}_{i^{\\prime}+1})),}\\\\ {\\phi(\\mathbf{y}_{i^{\\prime}+1})\\sim\\hat{P}(\\cdot|\\phi(\\mathbf{y}_{i^{\\prime}}),a_{\\mathbf{y}_{i^{\\prime}}})~~~~~~~~~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\hat{\\mathbb M}$ is the space of $d$ , with $a_{\\mathbf{x}_{i}}$ and $\\boldsymbol{a}_{\\mathbf{y}_{i^{\\prime}}}$ being the actions at states $\\mathbf{x}_{i}$ and $\\mathbf{y}_{i^{\\prime}}$ , respectively, and P\u02c6 is the learned latent dynamics model. F has a fixed point d\u03c0\u03d5. ", "page_idx": 14}, {"type": "text", "text": "Proof. We follow the proof techniques from [7] and [45]. By substituting $\\hat{d}(\\phi(\\mathbf{x}_{i+1}),\\phi(\\mathbf{y}_{i^{\\prime}+1}))$ with $d_{\\phi}(\\mathbf{x}_{i+1},\\mathbf{y}_{i^{\\prime}+1})$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}d_{\\phi}(\\mathbf{x}_{i},\\mathbf{y}_{i^{\\prime}})=|r_{\\mathbf{x}_{i}}-r_{\\mathbf{y}_{i^{\\prime}}}|+\\gamma\\mathbb{E}_{\\mathbf{\\Phi}_{\\phi}(\\mathbf{x}_{i+1})\\sim\\hat{P}(\\cdot|\\phi(\\mathbf{x}_{i}),a_{\\mathbf{x}_{i}})}\\,\\,d_{\\phi}\\big(\\mathbf{x}_{i+1},\\mathbf{y}_{i^{\\prime}+1}\\big).}\\\\ {\\phi(\\mathbf{y}_{i^{\\prime}+1})\\!\\sim\\!\\hat{P}(\\cdot|\\phi(\\mathbf{y}_{i^{\\prime}}),a_{\\mathbf{y}_{i^{\\prime}}})\\,\\,\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The operator $\\mathcal{F}d_{\\phi}$ is a contraction mapping with respect to the $L_{\\infty}$ norm because, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathcal{F}d_{\\phi}(\\mathbf{x},\\mathbf{y})-\\mathcal{F}d_{\\phi^{\\prime}}(\\mathbf{x},\\mathbf{y})|=\\Biggl|\\gamma\\mathbb{E}_{\\phi(\\mathbf{x}_{i+1})\\sim\\hat{P}(\\cdot|\\phi(\\mathbf{x}_{i}),a_{\\mathbf{x}_{i}})}\\,\\,(d_{\\phi}-d_{\\phi^{\\prime}})(\\mathbf{x}_{i+1},\\mathbf{y}_{i^{\\prime}+1})\\Biggr|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\phi(\\mathbf{y}_{i^{\\prime}+1})\\sim\\hat{P}(\\cdot|\\phi(\\mathbf{y}_{i^{\\prime}}),a_{\\mathbf{y}_{i^{\\prime}}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\gamma\\|(d_{\\phi}-d_{\\phi^{\\prime}})\\big(\\mathbf{x}_{i+1},\\mathbf{y}_{i^{\\prime}+1}\\big)\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By Banach\u2019s fixed point theorem, operator $\\mathcal{F}$ has a fixed point $d_{\\phi}^{\\pi}$ . ", "page_idx": 14}, {"type": "text", "text": "A.3.2 Proof of Theorem 3.4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To prove the distance $\\hat{d}$ is a diffuse metric, we need to prove that $\\hat{d}$ satisfies all of three axioms in the Definition 3.2 (definition of diffuse metric). ", "page_idx": 15}, {"type": "text", "text": "Lemma A.1. $\\|\\mathbf{a}\\|_{2}^{2}+\\|\\mathbf{b}\\|_{2}^{2}-\\mathbf{a}^{\\top}\\mathbf{b}\\geq0$ for any a, b \u2208R. ", "page_idx": 15}, {"type": "text", "text": "Proof. Because $\\mathbf{a}^{\\top}\\mathbf{b}\\leq\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|$ , we have, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\mathbf{a}\\|^{2}+\\|\\mathbf{b}\\|^{2}-\\mathbf{a}^{\\top}\\mathbf{b}}\\\\ &{\\geq\\|\\mathbf{a}\\|^{2}+\\|\\mathbf{b}\\|^{2}-\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|}\\\\ &{\\geq\\|\\mathbf{a}\\|^{2}+\\|\\mathbf{b}\\|^{2}-2\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|}\\\\ &{=(\\|\\mathbf{a}\\|-\\|\\mathbf{b}\\|)^{2}}\\\\ &{>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This lemma indicates that the term under the square root of $\\hat{d}$ is always non-negative. $\\hat{d}$ is able to measure any two vectors a, $\\mathbf{b}\\in\\mathbb{R}^{n}$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma A.2 (Non-negative). $\\hat{d}(\\mathbf{a},\\mathbf{b})\\geq0$ for any a, $\\mathbf{b}\\in\\mathbb{R}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. By definition, the square root is non-negative. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.3 (Symmetric). $\\hat{d}(\\mathbf{a},\\mathbf{b})=\\hat{d}(\\mathbf{b},\\mathbf{a})$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\hat{d}}(\\mathbf{a},\\mathbf{b})={\\sqrt{\\|\\mathbf{a}\\|_{2}^{2}+\\|\\mathbf{b}\\|_{2}^{2}-\\mathbf{a}^{\\top}\\mathbf{b}}}={\\sqrt{\\|\\mathbf{b}\\|_{2}^{2}+\\|\\mathbf{a}\\|_{2}^{2}-\\mathbf{b}^{\\top}\\mathbf{a}}}={\\hat{d}}(\\mathbf{b},\\mathbf{a})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma A.4 (Triangle inequality). $\\hat{d}(\\mathbf{a},\\mathbf{b})+\\hat{d}(\\mathbf{b},\\mathbf{c})\\geq\\hat{d}(\\mathbf{a},\\mathbf{c}),f o r\\,a n y\\,\\mathbf{a},\\mathbf{b},\\mathbf{c}\\in\\mathbb{R}.$ ", "page_idx": 15}, {"type": "text", "text": "Proof. To prove this lemma, it is equivalent to prove the following inequality by definition of $\\hat{d}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sqrt{\\|\\mathbf{a}\\|^{2}+\\|\\mathbf{b}\\|^{2}-\\mathbf{a}^{\\top}\\mathbf{b}}+\\sqrt{\\|\\mathbf{b}\\|^{2}+\\|\\mathbf{c}\\|^{2}-\\mathbf{b}^{\\top}\\mathbf{c}}\\geq\\sqrt{\\|\\mathbf{a}\\|^{2}+\\|\\mathbf{c}\\|^{2}-\\mathbf{a}^{\\top}\\mathbf{c}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Because $-\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|\\leq\\mathbf{x}^{\\top}\\mathbf{y}\\leq\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|,\\forall\\mathbf{x},\\mathbf{y}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\sqrt{\\|\\mathbf{a}\\|^{2}+\\|\\mathbf{b}\\|^{2}-\\mathbf{a}^{\\top}\\mathbf{b}}+\\sqrt{\\|\\mathbf{b}\\|^{2}+\\|\\mathbf{c}\\|^{2}-\\mathbf{b}^{\\top}\\mathbf{c}}}\\\\ &{\\geq\\!\\sqrt{\\|\\mathbf{a}\\|^{2}+\\|\\mathbf{b}\\|^{2}-\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|}+\\sqrt{\\|\\mathbf{b}\\|^{2}+\\|\\mathbf{c}\\|^{2}-\\|\\mathbf{b}\\|\\|\\mathbf{c}\\|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sqrt{\\|\\mathbf{a}\\|^{2}+\\|\\mathbf{c}\\|^{2}+\\|\\mathbf{a}\\|\\|\\mathbf{c}\\|}\\geq\\sqrt{\\|\\mathbf{a}\\|^{2}+\\|\\mathbf{c}\\|^{2}-\\mathbf{a}^{\\top}\\mathbf{c}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sqrt{\\|\\mathbf{a}\\|^{2}+\\|\\mathbf{b}\\|^{2}-\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|}+\\sqrt{\\|\\mathbf{b}\\|^{2}+\\|\\mathbf{c}\\|^{2}-\\|\\mathbf{b}\\|\\|\\mathbf{c}\\|}\\geq\\sqrt{\\|\\mathbf{a}\\|^{2}+\\|\\mathbf{c}\\|^{2}+\\|\\mathbf{a}\\|\\|\\mathbf{c}\\|}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "is true, then inequality (15) is true and Lemma A.4 is proven. To prove inequality (18), we can take squares on both sides without sign changing because both sides are non-negative. Then we have, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left({\\sqrt{\\|\\mathbf{a}\\|^{2}+\\|\\mathbf{b}\\|^{2}-\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|}}+{\\sqrt{\\|\\mathbf{b}\\|^{2}+\\|\\mathbf{c}\\|^{2}-\\|\\mathbf{b}\\|\\|\\mathbf{c}\\|}}\\right)^{2}\\geq\\|\\mathbf{a}\\|^{2}+\\|\\mathbf{c}\\|^{2}+\\|\\mathbf{a}\\|\\|\\mathbf{c}\\|.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To prove inequality (18), it is equivalent to prove inequality (19). Expand and simplify inequality (19), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\therefore{\\sqrt{\\|\\mathbf{a}\\|^{2}+\\|\\mathbf{b}\\|^{2}-\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|}}\\,{\\sqrt{\\|\\mathbf{b}\\|^{2}+\\|\\mathbf{c}\\|^{2}-\\|\\mathbf{b}\\|\\|\\mathbf{c}\\|}}\\geq-2\\|\\mathbf{b}\\|^{2}+\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|+\\|\\mathbf{b}\\|\\,\\|\\mathbf{c}\\|+\\|\\mathbf{a}\\|\\,\\|\\mathbf{c}\\|.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The left-hand side of inequality (20) is non-negative. ", "page_idx": 15}, {"type": "text", "text": "1) if right hand side $-2\\|\\mathbf{b}\\|^{2}+\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|+\\|\\mathbf{b}\\|\\|\\mathbf{c}\\|+\\|\\mathbf{a}\\|\\|\\mathbf{c}\\|<0$ , then inequality (20) is proven and backtrace to Lemma A.4 is proven. ", "page_idx": 15}, {"type": "text", "text": "2) if right hand side $-2\\|\\mathbf{b}\\|^{2}+\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|+\\|\\mathbf{b}\\|\\|\\mathbf{c}\\|+\\|\\mathbf{a}\\|\\|\\mathbf{c}\\|\\ge0$ , we take square on both sides of inequality (20) and have, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To prove inequality (21), we let left-hand side subtract right-hand side, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{4(\\|\\mathbf{a}\\|^{2}+\\|\\mathbf{b}\\|^{2}-\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|)(\\|\\mathbf{b}\\|^{2}+\\|\\mathbf{c}\\|^{2}-\\|\\mathbf{b}\\|\\|\\mathbf{c}\\|)-(-2\\|\\mathbf{b}\\|^{2}+\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|+\\|\\mathbf{b}\\|\\|\\mathbf{c}\\|+\\|\\mathbf{a}\\|\\|\\mathbf{c}\\|)}\\\\ &{=3\\|\\mathbf{a}\\|^{2}\\|\\mathbf{b}\\|^{2}+3\\|\\mathbf{a}\\|^{2}\\|\\mathbf{c}\\|^{2}+3\\|\\mathbf{b}\\|^{2}\\|\\mathbf{c}\\|^{2}-6\\|\\mathbf{a}\\|^{2}\\|\\mathbf{b}\\|\\|\\mathbf{c}\\|-6\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|\\|\\mathbf{c}\\|^{2}+6\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|^{2}\\|\\mathbf{c}\\|}\\\\ &{=3(\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|+\\|\\mathbf{b}\\|\\|\\mathbf{c}\\|-\\|\\mathbf{a}\\|\\|\\mathbf{c}\\|)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, inequality (21) is proven. Consequently, inequality (20) in the case of $-2\\|\\mathbf{b}\\|^{2}+\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|+$ $\\|\\mathbf{b}\\|\\|\\mathbf{c}\\|+\\|\\mathbf{a}\\|\\|\\mathbf{c}\\|\\geq0$ is proven. Summarize with the case of $-2\\|\\mathbf{b}\\|^{2}+\\|\\mathbf{a}\\|\\|\\mathbf{b}\\|+\\|\\mathbf{b}\\|\\|\\mathbf{c}\\|+$ $\\|\\mathbf{a}\\|\\|\\mathbf{c}\\|<0$ , inequality (20) is proven and Lemma A.4 is proven. ", "page_idx": 16}, {"type": "text", "text": "Theorem A.5. $\\hat{d}$ is a diffuse metric. ", "page_idx": 16}, {"type": "text", "text": "Proof. By summarizing Lemma A.2, A.3, and A.4, function $\\hat{d}$ holds the three axioms of diffuse metric. Therefore, function d\u02c6 is a diffuse metric. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.3.3 Proof of Theorem 3.6 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Theorem 5. Let $\\mathbb{M}_{\\psi}$ be the space of $d_{\\psi}$ . The metric update operator $\\mathcal{F}_{C h r o n o}\\,:\\,\\mathbb{M}_{\\psi}\\,\\to\\,\\mathbb{M}_{\\psi}$ is defined as, ", "page_idx": 16}, {"type": "text", "text": "$\\mathcal{F}_{C h r o n o d_{\\psi}}(\\mathbf{x}_{i},\\mathbf{x}_{j},\\mathbf{y}_{i^{\\prime}},\\mathbf{y}_{j^{\\prime}})=|r_{\\mathbf{x}_{i}}-r_{\\mathbf{y}_{i^{\\prime}}}|+\\gamma\\mathbb{E}_{\\mathbf{x}_{i+1}\\sim P_{\\mathbf{x}}^{\\pi},\\mathbf{y}_{i^{\\prime}+1}\\sim P_{\\mathbf{y}}^{\\pi}}d_{\\psi}(\\mathbf{x}_{i+1},\\mathbf{x}_{j},\\mathbf{y}_{i^{\\prime}+1},\\mathbf{y}_{j^{\\prime}}).$ (23) ${\\mathcal{F}}_{C h r o n o}$ has a fixed point. ", "page_idx": 16}, {"type": "text", "text": "Proof. In practice, we have assumed that the sampled $\\mathbf{x}_{j}$ and $\\mathbf{y}_{j^{\\prime}}$ are future states of $\\mathbf{x}_{i}$ and $\\mathbf{y}_{i^{\\prime}}$ , respectively. To be precise, we extend the definition of ${\\mathcal{F}}_{C h r o n o}$ to cover the cases that $i>j$ or $i^{\\prime}\\bar{>}\\,j^{\\prime}$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{C h r o n o d\\psi}(\\mathbf{x}_{i},\\mathbf{x}_{j},\\mathbf{y}_{i^{\\prime}},\\mathbf{y}_{j^{\\prime}})=}\\\\ &{\\int(0,\\ }\\\\ &{\\left\\{|r_{\\mathbf{x}_{i}}-r_{\\mathbf{y}_{i^{\\prime}}}|+\\gamma|\\mathbb{E}_{\\mathbf{x}_{i+1}\\sim P_{\\mathbf{x}}^{\\pi},\\mathbf{y}_{i^{\\prime}+1}\\sim P_{\\mathbf{y}}^{\\pi}}d_{\\psi}(\\mathbf{x}_{i+1},\\mathbf{x}_{j},\\mathbf{y}_{i^{\\prime}+1},\\mathbf{y}_{j^{\\prime}}),\\right.\\ \\mathrm{~otherwise}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We follow the proof in Section A.3.1. ${\\mathcal{F}}_{C h r o n o}$ is contraction mapping because. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ |\\mathcal{F}_{C h r o n o d_{\\psi}}(\\mathbf{x}_{i},\\mathbf{x}_{j},\\mathbf{y}_{i^{\\prime}},\\mathbf{y}_{j^{\\prime}})-\\mathcal{F}_{C h r o n o d_{\\psi^{\\prime}}}(\\mathbf{x}_{i},\\mathbf{x}_{j},\\mathbf{y}_{i^{\\prime}},\\mathbf{y}_{j^{\\prime}})|}\\\\ &{=\\left|\\gamma\\mathbb{E}_{\\mathbf{x}_{i+1}\\sim P_{\\mathbf{x}}^{\\pi},\\mathbf{y}_{i^{\\prime}+1}\\sim P_{\\mathbf{y}}^{\\pi}}(d_{\\psi}-d_{\\psi^{\\prime}})(\\mathbf{x}_{i+1},\\mathbf{x}_{j},\\mathbf{y}_{i^{\\prime}+1},\\mathbf{y}_{j^{\\prime}})\\right|}\\\\ &{\\ \\ \\ \\ge\\!\\gamma\\|(d_{\\psi}-d_{\\psi^{\\prime}})(\\mathbf{x}_{i+1},\\mathbf{x}_{j},\\mathbf{y}_{i^{\\prime}+1},\\mathbf{y}_{j^{\\prime}})\\|_{\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Banach\u2019s fixed point theorem, operator ${\\mathcal{F}}_{C h r o n o}$ has a fixed point $d_{\\psi}^{\\pi}$ . ", "page_idx": 16}, {"type": "text", "text": "A.4 Theoretical Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.4.1 \u0141ukaszyk-Karmowski Distance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Definition A.6 (\u0141ukaszyk-Karmowski Distance). Given a metric space $(X,d)$ where $d$ is a metric defined on $X\\times X$ , the \u0141ukaszyk-Karmowski distance [49] is a distance between two probability measures $\\mu$ and $\\nu$ on $X$ using metric $d$ , and is defined as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nd_{L K}(d)(\\mu,\\nu)=\\mathbb{E}_{x\\sim\\mu,x^{\\prime}\\sim\\nu}d(x,x^{\\prime})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Intuitively, $d_{L K}$ measures the expected distance between two samples drawn from $\\mu$ and $\\nu$ , respectively. If we measure the $d_{L K}(d)$ distance between two identical distributions, the distance is not restricted to zero, i.e., $d_{L K}(d)(\\mu,\\mu)\\geq0$ . ", "page_idx": 16}, {"type": "text", "text": "A.5 Implementation Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A.5.1 Implementation of $\\hat{m}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We adopt IQE [38] to implement $\\hat{m}$ . Given two vectors $\\mathbf{a},\\mathbf{b}\\,\\in\\,\\mathbb{R}^{n}$ , reshaping to $\\mathbb{R}^{k\\times l}$ where $k\\times l=n$ , IQE first computes the union of the interval for each component: ", "page_idx": 17}, {"type": "equation", "text": "$$\nd_{i}(\\mathbf{a},\\mathbf{b})=|\\bigcup_{j=1}^{l}[\\mathbf{a}_{i j},\\operatorname*{max}(\\mathbf{a}_{i j},\\mathbf{b}_{i j})]|,\\forall i=1,2,...,k,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $[\\cdot,\\cdot]$ is the interval on the real line. Then it computes the distance among all components $d_{i}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\nd_{I Q E}(\\mathbf{a},\\mathbf{b})=\\alpha\\cdot\\operatorname*{max}(d_{1}(\\mathbf{a},\\mathbf{b}),..d_{k}(\\mathbf{a},\\mathbf{b}))+(1-\\alpha)\\cdot\\operatorname*{mean}(d_{1}(\\mathbf{a},\\mathbf{b}),..d_{k}(\\mathbf{a},\\mathbf{b})),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\alpha\\in\\mathbb{R}$ is an adaptive weight to balance the \u201cmax\u201d and \u201cmean\u201d terms. In the scope of our method, we adopt $d_{I Q E}(\\mathbf{a},\\mathbf{b})$ to implement $\\hat{m}$ . ", "page_idx": 17}, {"type": "text", "text": "A.5.2 Network Architecture ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The encoder $\\phi$ takes an input of the states and consists of 4 convolutional layers followed by 1 fullyconnected layer. The output dimension of $\\phi$ is 256. The encoder $\\psi$ takes an input of 512-dimensional vector (concatenated with $\\phi(\\mathbf{x}_{i})$ and $\\phi(\\mathbf{x}_{j}))$ , feed it into two layer MLPs with 512 hidden units, and output a 256 dim embedding. Q network and policy network are 3-layer MLPs with 1024 hidden units. ", "page_idx": 17}, {"type": "table", "img_path": "J42SwBemEA/tmp/9db088441f7a41a9ce7a80160bfbedd5df04f4cdf7718259d5a08244f2a441ea.jpg", "table_caption": ["A.5.3 Hyperparameters "], "table_footnote": ["Table 4: Hyperparameters "], "page_idx": 17}, {"type": "text", "text": "Algorithm 1 A learning step in jointly learning SCR and SAC. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Require: Replay Buffer $\\mathcal{D}$ , $\\mathrm{^Q}$ network $Q$ , policy network $\\pi$ , target Q network $\\bar{Q}$ , state encoder $\\phi$ , target state encoder $\\bar{\\phi}$ , chronological encoder $\\psi$ .   \nSample a batch of trajectories with size $B$ : $\\{\\tau_{k}\\}_{k=1}^{B}\\sim\\mathcal{D}$ Sample state $\\mathbf{x}_{i}$ , transition at $\\mathbf{x}_{i}$ and its future state $\\mathbf{x}_{j}$ from each trajectory $\\tau_{k}$ : {(xi, xi+1, ri, ai, xj)k \u223c\u03c4k}kB=1 Compute loss $\\mathcal{L}_{\\phi}(\\phi)$ in (2) Compute loss $\\mathcal{L}_{\\psi}(\\psi,\\phi)$ in (5) Compute loss ${\\mathcal{L}}_{\\hat{m}}(\\phi)$ in (9) Compute loss $\\mathcal{L}(\\phi,\\dot{\\psi})=\\mathcal{L}_{\\phi}(\\phi)+\\mathcal{L}_{\\psi}(\\psi,\\phi)+\\mathcal{L}_{\\hat{m}}(\\phi)$ in (10) Update $\\phi$ and $\\psi$ by minimizing loss ${\\mathcal{L}}(\\phi,\\psi)$ Compute RL loss $\\mathcal{L}_{R L}$ according to SAC objectives Update $\\phi$ , $Q$ and $\\pi$ by minimizing loss $\\mathcal{L}_{R L}$ Soft update target $\\mathrm{Q}$ network: $\\bar{Q}\\,\\overline{{{=}}}\\,\\alpha_{Q}Q+(1-\\alpha_{Q})\\bar{Q}$ Soft update target state encoder: $\\bar{\\phi}=\\alpha_{\\phi}\\phi+(1-\\alpha_{\\phi})\\bar{\\phi}$   \ndef $\\textsf{d}(\\textsf{x},\\mathrm{~\\mathtt~{~y~}~})$ : a = (x.pow (2.).sum( ${\\tt d i m}\\!=\\!-1$ , keepdim $=$ True) + y.pow (2.).sum( ${\\tt d i m}\\!=\\!-1$ , keepdim $=$ True)) # $\\mathrm{~\\boldmath~x~}\\hat{\\mathrm{~\\boldmath~2~}}+\\mathrm{~\\boldmath~y~}\\hat{\\mathrm{~\\boldmath~2~}}$ b = (x \\* y).sum( $\\mathtt{d i m}\\!=\\!-1$ , keepdim $=$ True) # xy return (a - b).sqrt () # sqrt(x^2 + y^2 - xy)   \ndef encoder_loss(self , s_i , s_i_1 , s_j , s_j_1 , r_i , r_j , agg_rew_i_j): phi_i $=$ self.encoder(s_i) phi_i_1 $=$ self.encoder(s_i_1) phi_j $=$ self.encoder(s_j) phi_j_1 $=$ self.encoder(s_j_1) # concatenate i and j for compute loss (2) where j is not required phi_t $=$ torch.cat([phi_i , phi_j ]) phi_t1 $=$ torch.cat([ phi_i_1 , phi_j_1 ]) r_t $=$ torch.cat([r_i , r_j]) # compute loss (2) # permute batch to create y perm $=$ torch.randperm(phi_t.shape [0]) d_phi_t $=$ d(phi_t , phi_t[perm ]) d_phi_t1 $=$ d(phi_t1 , phi_t1[perm ]) # loss (2) loss_phi $=$ F.mse_loss(d_phi_t , r_t $^+$ self.discount $^*$ d_phi_t1. detach ()) # compute loss (5) psi $=$ self.psi_mlp(phi_i , phi_j) psi_1 $=$ self.psi_mlp(phi_i_1 , phi_j) # permute batch to create y perm $=$ torch.randperm(psi.shape [0]) d_psi $=$ d(psi , psi[perm ]) d_psi_1 $=$ d(psi_1 , psi_1[perm ]) # loss (5) loss_psi $=$ F.mes_loss(d_psi , r_i $^+$ self.discount $^*$ d_psi_1.detach ()) # compute m m $=$ iqe(phi_i , phi_j) # compute loss (8) low_bound $=$ agg_rew_i_j loss_low $=$ (F.mse_loss(m, low_bound , reduction $=~\\cdot$ \u2019none \u2019) $^*$ (m < low_bound).detach ()).mean () # compute loss (10)   \n39 up_bound $=$ d(phi_i , phi_i[perm ]) $^+$ d(phi_j , phi_j[perm ]) + agg_rew_i_j[perm]   \n40 up_bound $=$ up_bound.detach ()   \n41 loss_up $=$ (F.mse_loss(m.abs(), up_bound , reduction $=\\cdot$ \u2019none \u2019) $^\\ast$ (m. abs() $>$ up_bound).detach ()).mean ()   \n42   \n43 return loss_phi $^+$ loss_psi $^+$ loss_low $^+$ loss_up   \n44 ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Listing 1: Pseudo code of learning of representation ", "page_idx": 19}, {"type": "text", "text": "1 A.6 Computational Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "2 The experiment is done on servers with an 128-cores CPU, 512 GB RAM and NVIDIA A100 GPUs.   \n3 Two instances are running in one GPU at the same time. ", "page_idx": 19}, {"type": "text", "text": "4 B Experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "5 B.1 Additional Information for Distraction Settings in DeepMind Control Suite ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "6 For the distraction setting in Section 4.2, we utilized the Distracting Control Suite [34] with the   \n7 setting \"difficulty=easy\". This involves mixing the background with videos from the DAVIS2017 [29] dataset. Specifically, the training environment samples videos from the DAVIS2017 train set, while the evaluation environment uses videos from the validation set. Each episode reset triggers the   \n10 sampling of a new video. Additionally, it introduces variability in each episode by applying a   \n1 uniformly sampled RGB color shift to the robot\u2019s body color and randomly selecting the camera   \n12 pose. The specifics of the RGB color shift range and camera pose variations are in line with the   \n13 Distracting Control Suite paper [34]. Different random seeds are used for the training and evaluating   \n14 environments at the start of training to ensure diverse environments. ", "page_idx": 19}, {"type": "text", "text": "15 B.2 Name of Tasks in DeepMind Control Suite ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "16 Due to limited space in main text, we use short name for tasks in DeepMind Control Suite. The full name of tasks are listed in below Table 5: ", "page_idx": 19}, {"type": "table", "img_path": "J42SwBemEA/tmp/a767d683c89975959e287a47e2a440fd3600f452d313be0206b3ac51fc6b3d2d.jpg", "table_caption": [], "table_footnote": ["Table 5: Full name of tasks in DeepMind Control Suite "], "page_idx": 19}, {"type": "text", "text": "18 B.3 Additional Results of Distracting Control Suite in Section 4.2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "19 We extend the experiments in Section 4.2 with compared with RAP [8], showing results in Table 6.   \n20 Figure 9 shows the training curves of SCR and baseline methods in distraction setting of DM_Control. ", "page_idx": 19}, {"type": "table", "img_path": "J42SwBemEA/tmp/7f66ce85aa932ef73d3bcfad32ff21767fe58bce462f9d3b6de7f7f110d58ebd.jpg", "table_caption": [], "table_footnote": ["Table 6: Results (mean\u00b1std) on DM_Control with distraction setting at 500K environment step. Distraction includes background, robot body color, and camera pose. "], "page_idx": 20}, {"type": "image", "img_path": "J42SwBemEA/tmp/4e7ea9ac4230f4e6c6805d7c5c6cceffb7315529531d8e2e9159d8e907b17d46.jpg", "img_caption": ["Figure 9: Training curves of SCR and baseline methods in distraction setting of DM_Control. Curves are evaluation scores average on 10 runs and shadow shapes are std. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "21 B.4 Additional Comparison Experiments with Data Augmentation Methods ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "22 Data augmentation is a category of methods efficient on reinforcement learning, toward sample   \n23 efficiency and noise invariance. Additional baseline CBM [26] is bisimulation metric method combing   \n24 with data augmentation method $\\mathrm{DrQ-v}2$ , and it achieves significant performance in distracting   \n25 DM_Control. To compare with CBM, we embed data augmentation method $\\mathrm{DrQ}$ into our proposed   \n26 method SCR. Table 7 indicates that the performance of SCR combined with DrQ surpass baseline   \n27 CBM. ", "page_idx": 20}, {"type": "table", "img_path": "J42SwBemEA/tmp/e9ce0332a2dba5179304e5018e15837c86b7db35c5759fc71bf62a9e4528600b.jpg", "table_caption": [], "table_footnote": ["Table 7: Results (mean\u00b1std) on DM_Control with distraction setting at 500K environment step. Distraction includes background, robot body color, and camera pose. "], "page_idx": 20}, {"type": "text", "text": "28 B.5 Training Curves Meta-World ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "29 Figure 10 shows the training curves of SCR and baseline methods in Meta-World in Section 4.4. ", "page_idx": 20}, {"type": "image", "img_path": "J42SwBemEA/tmp/055cea784c2497492622b09a404a89618d3f0460d1cf734726148712f03086d2.jpg", "img_caption": ["Figure 10: Training Curves of Meta-World. Mean success rates on 5 runs with std (shadow shape). "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "30 B.6 Additional Experiments on MiniGrid ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "31 We provide additional experiments on MiniGrid-FourRooms [9] environment. MiniGrid is under a   \n32 sparse reward setting that the agent receives a reward only when it successfully reaches a specific   \n33 goal, without rewards at previous steps. This experiment verifies the motivation of SCR that is able   \n34 to handle non-informative rewards. We compare various metric-based representation methods that   \n35 combine with backbone PPO [31] algorithms. We train each run for 5M environment steps. Table   \nshows the results with PPO, PPO+DBC, PPO+MICo, $\\scriptstyle\\mathrm{PPO+SimSR}$ , and $\\mathrm{PPO}\\mathrm{+}\\mathrm{SCR}$ . ", "page_idx": 21}, {"type": "table", "img_path": "J42SwBemEA/tmp/e460c845d1c21fb28682b6986a2755b14c7952391ff508a54f25c19ca1b2d0ea.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 8: Results (mean $\\pm$ std) on MiniGrid-FourRooms [9] at 5M environment steps. Results are average over 5 seeds. ", "page_idx": 21}, {"type": "image", "img_path": "J42SwBemEA/tmp/8a2f791a002460207895ca06689ccf3cd3c5337e73f198a153a6fb273b636f45.jpg", "img_caption": ["Figure 11: Training curves on MiniGrid-FourRooms [9]. Results are average over 5 seeds. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "37 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "38 1. Claims   \n39 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n40 paper\u2019s contributions and scope?   \n41 Answer: [Yes]   \n42 Justification: We have included the claims of contributions and scope.   \n43 Guidelines:   \n44 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n45 made in the paper.   \n46 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n47 contributions made in the paper and important assumptions and limitations. A No or   \n48 NA answer to this question will not be perceived well by the reviewers.   \n49 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n50 much the results can be expected to generalize to other settings.   \n51 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n52 are not attained by the paper.   \n53 2. Limitations   \n54 Question: Does the paper discuss the limitations of the work performed by the authors?   \n55 Answer: [Yes]   \n56 Justification: We discuss in the Conclusion Section.   \n57 Guidelines:   \n58 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n59 the paper has limitations, but those are not discussed in the paper.   \n60 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n61 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n62 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n63 model well-specification, asymptotic approximations only holding locally). The authors   \n64 should reflect on how these assumptions might be violated in practice and what the   \n65 implications would be.   \n66 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n67 only tested on a few datasets or with a few runs. In general, empirical results often   \n68 depend on implicit assumptions, which should be articulated.   \n69 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n70 For example, a facial recognition algorithm may perform poorly when image resolution   \n71 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n72 used reliably to provide closed captions for online lectures because it fails to handle   \n73 technical jargon.   \n74 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n75 and how they scale with dataset size.   \n76 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n77 address problems of privacy and fairness.   \n78 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n79 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n80 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n81 judgment and recognize that individual actions in favor of transparency play an impor  \n82 tant role in developing norms that preserve the integrity of the community. Reviewers   \n83 will be specifically instructed to not penalize honesty concerning limitations.   \n84 3. Theory Assumptions and Proofs   \n85 Question: For each theoretical result, does the paper provide the full set of assumptions and ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "90 \u2022 The answer NA means that the paper does not include theoretical results.   \n91 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n92 referenced.   \n93 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n94 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n95 they appear in the supplemental material, the authors are encouraged to provide a short   \n96 proof sketch to provide intuition.   \n97 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n98 by formal proofs provided in appendix or supplemental material.   \n99 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "100 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "101 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n102 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n103 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "05 Justification: We provide implementation details in Appendix.   \n06 Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "139 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n140 tions to faithfully reproduce the main experimental results, as described in supplemental   \n141 material?   \n142 Answer: [No]   \n143 Justification: We provide pseudo codes in Appendix. We will release source code after   \n144 published.   \n145 Guidelines:   \n146 \u2022 The answer NA means that paper does not include experiments requiring code.   \n147 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n148 public/guides/CodeSubmissionPolicy) for more details.   \n149 \u2022 While we encourage the release of code and data, we understand that this might not be   \n150 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n151 including code, unless this is central to the contribution (e.g., for a new open-source   \n152 benchmark).   \n153 \u2022 The instructions should contain the exact command and environment needed to run to   \n154 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n155 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n156 \u2022 The authors should provide instructions on data access and preparation, including how   \n157 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n158 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n159 proposed method and baselines. If only a subset of experiments are reproducible, they   \n160 should state which ones are omitted from the script and why.   \n161 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n162 versions (if applicable).   \n163 \u2022 Providing as much information as possible in supplemental material (appended to the   \n164 paper) is recommended, but including URLs to data and code is permitted.   \n165 6. Experimental Setting/Details   \n166 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n167 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n168 results?   \n169 Answer: [Yes]   \n170 Justification: We provide details in Appendix.   \n171 Guidelines:   \n172 \u2022 The answer NA means that the paper does not include experiments.   \n173 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n174 that is necessary to appreciate the results and make sense of them.   \n175 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n176 material.   \n177 7. Experiment Statistical Significance   \n178 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n179 information about the statistical significance of the experiments?   \n180 Answer: [Yes]   \n181 Justification: We report the standard deviation of the experimental results.   \n182 Guidelines:   \n183 \u2022 The answer NA means that the paper does not include experiments.   \n184 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n185 dence intervals, or statistical significance tests, at least for the experiments that support   \n186 the main claims of the paper.   \n187 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n188 example, train/test split, initialization, random drawing of some parameter, or overall   \n189 run with given experimental conditions).   \n190 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n191 call to a library function, bootstrap, etc.)   \n192 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n193 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n194 of the mean.   \n195 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n196 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n197 of Normality of errors is not verified.   \n198 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n199 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n200 error rates).   \n201 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n202 they were calculated and reference the corresponding figures or tables in the text.   \n203 8. Experiments Compute Resources   \n204 Question: For each experiment, does the paper provide sufficient information on the com  \n205 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n206 the experiments?   \n207 Answer: [Yes]   \n208 Justification: We provide details in Appendix.   \n209 Guidelines:   \n210 \u2022 The answer NA means that the paper does not include experiments.   \n211 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n212 or cloud provider, including relevant memory and storage.   \n213 \u2022 The paper should provide the amount of compute required for each of the individual   \n214 experimental runs as well as estimate the total compute.   \n215 \u2022 The paper should disclose whether the full research project required more compute   \n216 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n217 didn\u2019t make it into the paper).   \n218 9. Code Of Ethics   \n219 Question: Does the research conducted in the paper conform, in every respect, with the   \n220 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n221 Answer: [Yes]   \n222 Justification: We follow NeurIPS Code of Ethics.   \n223 Guidelines:   \n224 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n225 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n226 deviation from the Code of Ethics.   \n227 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n228 eration due to laws or regulations in their jurisdiction).   \n229 10. Broader Impacts   \n230 Question: Does the paper discuss both potential positive societal impacts and negative   \n231 societal impacts of the work performed?   \n232 Answer: [Yes]   \n233 Justification: This paper is a research on improving the representation of deep reinforce  \n234 ment learning. We think that there are no potential societal consequences that need to be   \n235 highlighted.   \n236 Guidelines:   \n237 \u2022 The answer NA means that there is no societal impact of the work performed.   \n238 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n239 impact or why the paper does not address societal impact.   \n240 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n241 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n242 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n243 groups), privacy considerations, and security considerations.   \n244 \u2022 The conference expects that many papers will be foundational research and not tied   \n245 to particular applications, let alone deployments. However, if there is a direct path to   \n246 any negative applications, the authors should point it out. For example, it is legitimate   \n247 to point out that an improvement in the quality of generative models could be used to   \n248 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n249 that a generic algorithm for optimizing neural networks could enable people to train   \n250 models that generate Deepfakes faster.   \n251 \u2022 The authors should consider possible harms that could arise when the technology is   \n252 being used as intended and functioning correctly, harms that could arise when the   \n253 technology is being used as intended but gives incorrect results, and harms following   \n254 from (intentional or unintentional) misuse of the technology.   \n255 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n256 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n257 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n258 feedback over time, improving the efficiency and accessibility of ML).   \n259 11. Safeguards   \n260 Question: Does the paper describe safeguards that have been put in place for responsible   \n261 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n262 image generators, or scraped datasets)?   \n263 Answer: [NA]   \n264 Justification: Our paper poses no such risks.   \n265 Guidelines:   \n266 \u2022 The answer NA means that the paper poses no such risks.   \n267 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n268 necessary safeguards to allow for controlled use of the model, for example by requiring   \n269 that users adhere to usage guidelines or restrictions to access the model or implementing   \n270 safety filters.   \n271 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n272 should describe how they avoided releasing unsafe images.   \n273 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n274 not require this, but we encourage authors to take this into account and make a best   \n275 faith effort.   \n276 12. Licenses for existing assets   \n277 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n278 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n279 properly respected?   \n280 Answer: [Yes]   \n281 Justification: We have cited their original paper.   \n282 Guidelines:   \n283 \u2022 The answer NA means that the paper does not use existing assets.   \n284 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n285 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n286 URL.   \n287 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n288 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n289 service of that source should be provided.   \n290 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n291 package should be provided. For popular datasets, paperswithcode.com/datasets   \n292 has curated licenses for some datasets. Their licensing guide can help determine the   \n293 license of a dataset.   \n294 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n295 the derived asset (if it has changed) should be provided.   \n296 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n297 the asset\u2019s creators.   \n298 13. New Assets   \n299 Question: Are new assets introduced in the paper well documented and is the documentation   \n300 provided alongside the assets?   \n301 Answer: [NA]   \n302 Justification: We do not release new assets.   \n303 Guidelines:   \n304 \u2022 The answer NA means that the paper does not release new assets.   \n305 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n306 submissions via structured templates. This includes details about training, license,   \n307 limitations, etc.   \n308 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n309 asset is used.   \n310 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n311 create an anonymized URL or include an anonymized zip file.   \n312 14. Crowdsourcing and Research with Human Subjects   \n313 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n314 include the full text of instructions given to participants and screenshots, if applicable, as   \n315 well as details about compensation (if any)?   \n316 Answer: [NA]   \n317 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n318 Guidelines:   \n319 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n320 human subjects.   \n321 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n322 tion of the paper involves human subjects, then as much detail as possible should be   \n323 included in the main paper.   \n324 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n325 or other labor should be paid at least the minimum wage in the country of the data   \n326 collector.   \n327 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n328 Subjects   \n329 Question: Does the paper describe potential risks incurred by study participants, whether   \n330 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n331 approvals (or an equivalent approval/review based on the requirements of your country or   \n332 institution) were obtained?   \n333 Answer: [NA]   \n334 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n335 Guidelines:   \n336 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n337 human subjects.   \n338 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n339 may be required for any human subjects research. If you obtained IRB approval, you   \n340 should clearly state this in the paper.   \n341 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n342 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n343 guidelines for their institution.   \n344 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n345 applicable), such as the institution conducting the review. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}]