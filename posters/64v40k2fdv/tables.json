[{"figure_path": "64V40K2fDv/tables/tables_2_1.jpg", "caption": "Table 1: The different scale of Uni-Mol dataset and Uni-Mol2 dataset", "description": "This table shows the different scales of the Uni-Mol and Uni-Mol2 datasets, including the number of SMILES, scaffolds, and data sources used in each dataset.  Uni-Mol2 is significantly larger than Uni-Mol, which is important for the pretraining model.", "section": "3 Pretraining"}, {"figure_path": "64V40K2fDv/tables/tables_5_1.jpg", "caption": "Table 2: Architecture of Uni-Mol2 at different scale", "description": "This table details the architecture of the Uni-Mol2 model at different scales, showing the number of parameters, layers, embedding dimensions, attention heads, pair embedding and hidden dimensions, FFN embedding dimensions, learning rate, and batch size for each model variant.", "section": "3.3 Architecture"}, {"figure_path": "64V40K2fDv/tables/tables_7_1.jpg", "caption": "Table 3: Metrics about Scaling Law for Uni-Mol2", "description": "This table presents the evaluation metrics for the scaling law of the Uni-Mol2 model.  It shows the Relative Mean Absolute Error (RMAE), Mean Squared Error (MSE), R-squared, and Pearson Correlation Coefficient for the 570M and 1.1B parameter versions of the model. These metrics assess how well the model's predicted validation loss aligns with the actual validation loss across different model sizes.", "section": "4 Scaling Laws"}, {"figure_path": "64V40K2fDv/tables/tables_7_2.jpg", "caption": "Table 4: Mean absolute error(MAE, \u2193) results on QM9 Dataset", "description": "This table presents the mean absolute error (MAE) achieved by various models on the QM9 dataset.  The QM9 dataset contains various quantum mechanical properties of molecules, and each property is treated as a separate task for evaluation.  The table shows the performance of several baseline models (GROVERbase, GROVERlarge, GEM, Uni-Mol) and different sizes of the Uni-Mol2 model (84M, 164M, 310M, 570M, 1.1B parameters). The results are reported for the HOMO, LUMO, HOMO-LUMO gap, alpha, Cv, mu, R2, and ZPVE properties, illustrating how the accuracy improves as the model size increases.", "section": "5.1 QM9 Dataset"}, {"figure_path": "64V40K2fDv/tables/tables_8_1.jpg", "caption": "Table 5: Mean absolute error(MAE, \u2193) results on COMPAS-1D Dataset.", "description": "This table presents the mean absolute error (MAE) results for four different properties (aEA, aIP, dispersion, Dipmom Debye) predicted by various models on the COMPAS-1D dataset.  The models include Uni-Mol, Uni-Mol2 with 84M parameters, Uni-Mol2 with 1.1B parameters, and Uni-Mol2 variants using atom and bond features. Lower MAE values indicate better performance.", "section": "5 Downstream Experiment"}, {"figure_path": "64V40K2fDv/tables/tables_8_2.jpg", "caption": "Table 6: Mean absolute error(MAE, \u2193) about HOMO-LUMO GAP on QM9 Dataset", "description": "This table presents the mean absolute error (MAE) for predicting the HOMO-LUMO gap on the QM9 dataset.  It shows the MAE for Uni-Mol2 models of different sizes (84M, 164M, 310M, 570M, and 1.1B parameters) trained on three different subsets of the QM9 training data (train50, train100, and train200). The results indicate how the model's performance changes with increasing model size and training data.", "section": "5.3 The Performance on Limited QM9 Dataset"}, {"figure_path": "64V40K2fDv/tables/tables_13_1.jpg", "caption": "Table 2: Architecture of Uni-Mol2 at different scale", "description": "This table details the architecture of the Uni-Mol2 model at different scales, showing the number of parameters, layers, embedding dimensions, attention heads, pair embedding dimensions, pair hidden dimensions, FFN embedding dimensions, learning rate, and batch size for each model size.", "section": "3.3 Hyperparameter and Training Details"}, {"figure_path": "64V40K2fDv/tables/tables_13_2.jpg", "caption": "Table 2: Architecture of Uni-Mol2 at different scale", "description": "This table details the architecture of the Uni-Mol2 model at different scales, ranging from 42M to 1.1B parameters.  It shows the number of parameters, layers, embedding dimensions, attention heads, pair embedding and hidden dimensions, feed-forward network (FFN) embedding dimension, learning rate, and batch size for each model variant.  This allows for a comparison of model complexity across different scales.", "section": "3.3 Hyperparameter and Training Details"}, {"figure_path": "64V40K2fDv/tables/tables_13_3.jpg", "caption": "Table 2: Architecture of Uni-Mol2 at different scale", "description": "This table details the architecture of the Uni-Mol2 model at different scales, ranging from 42M to 1.1B parameters.  For each scale, it lists the number of parameters, the number of layers, embedding dimensions, attention heads, pair embedding and hidden dimensions, FFN embedding dimension, learning rate, and batch size.  These specifications provide a comprehensive overview of the model's configuration at different sizes, enabling a comparative analysis of the scaling characteristics of the model.", "section": "3.3 Hyperparameter and Training Details"}, {"figure_path": "64V40K2fDv/tables/tables_14_1.jpg", "caption": "Table 4: Mean absolute error(MAE, \u2193) results on QM9 Dataset", "description": "This table presents the mean absolute error (MAE) achieved by various models on the QM9 dataset.  The MAE is a measure of the average absolute difference between predicted and actual values for different properties of molecules in the QM9 dataset. Lower MAE values indicate better performance.  The table compares Uni-Mol2 at different scales (84M, 164M, 310M, 570M, and 1.1B parameters) against other baseline models, including GROVERbase, GROVERlarge, GEM, and Uni-Mol.  Results are shown for multiple properties including HOMO, LUMO, GAP, alpha, Cv, mu, R2, ZPVE.", "section": "5.1 QM9 Dataset"}, {"figure_path": "64V40K2fDv/tables/tables_14_2.jpg", "caption": "Table 11: Mean absolute error(MAE, \u2193) results on Biogen ADME Dataset", "description": "This table presents the mean absolute error (MAE) of the Uni-Mol, Uni-Mol2 84M, and Uni-Mol2 1.1B models on three different ADME properties: HCLint-1, PERM-1, and SOLU-1. Lower MAE values indicate better predictive performance.  The numbers in parentheses represent the standard deviation.  The table shows that the 1.1B parameter Uni-Mol2 model generally achieves the lowest MAE across these three properties, suggesting improved performance with increased model scale.", "section": "5.2 Biogen ADME Dataset"}, {"figure_path": "64V40K2fDv/tables/tables_15_1.jpg", "caption": "Table 12: Training Time of Uni-Mol2 at different scale", "description": "This table shows the training time (in GPU hours) and the number of GPUs used for training the Uni-Mol2 model at different scales (84M, 164M, 310M, 570M, and 1.1B parameters).  It demonstrates the increasing computational cost associated with larger model sizes.", "section": "D Pretrain Time Complexity and GPU Resources"}]