[{"figure_path": "ecPIg6o84Z/figures/figures_1_1.jpg", "caption": "Figure 1: Vision-language based evaluation of generated reports. Our proposed evaluation metric for a reference report (GT) and a candidate report takes into account the associated image, thereby overcoming several drawbacks of common metrics that only measure the distance between two texts. Consequently, when comparing two semantically very similar reports that suit an image (a), our metric suggests a high degree of similarity, whereas other metrics are unaware of these similarities and therefore give low scores due to textual differences. In contrast, when comparing two reports that are textually similar but differ by a single word that impacts the location of a finding (b), common metrics still provide a very high score, while our metric penalizes for that error.", "description": "This figure showcases a comparison between the proposed VLScore metric and existing metrics (B-4, BERTScore, CheXbert, RadGraph) in evaluating the similarity of generated radiology reports.  Two scenarios are presented: (a) Reports with the same findings but different wording, and (b) Reports with very similar wording but a critical difference in location.  The figure demonstrates VLScore's ability to account for image context, leading to more accurate similarity assessments than text-only based metrics.", "section": "1 Introduction"}, {"figure_path": "ecPIg6o84Z/figures/figures_7_1.jpg", "caption": "Figure 2: Scores of equivalent normal reports. When comparing two reports with no findings, the score is expected to be high, as both convey the same clinical findings with differences only in the writing of the report. Compared to BLEU-4 (a) and RadGraph F1 (b), our metric yields high scores (right-side), while the other metrics yield low scores (bottom-side). BERTScore (c) provides mid-range scores instead of high scores. CheXpert (d) provides higher scores than the others, yet lower scores than our metric, although it was expected to yield scores close to 1 as both reports in each pair contain no findings.", "description": "This figure compares the performance of the proposed VLScore metric against existing metrics (BLEU-4, RadGraph F1, BERTScore, and CheXpert) on a set of radiology reports with no findings.  The scatter plots illustrate the correlation between VLScore and each of the other metrics.  Ideally, all metrics would produce high scores for reports describing the same clinical absence of findings, but differences in how they handle textual variations lead to varying results. The VLScore consistently demonstrates higher scores than other methods, indicating its ability to account for semantic similarity beyond superficial textual matches.", "section": "4.3 Ablation study"}]