{"references": [{"fullname_first_author": "L\u00e9na\u00efc Chizat", "paper_title": "On the global convergence of gradient descent for over-parameterized models using optimal transport", "publication_date": "2018-12-03", "reason": "This paper provides a foundational theoretical framework for analyzing the convergence of gradient descent in over-parameterized models, which is highly relevant to the study of Transformers."}, {"fullname_first_author": "Yiping Lu", "paper_title": "A mean field analysis of deep ResNet and beyond: Towards provably optimization via overparameterization from depth", "publication_date": "2020-07-13", "reason": "This paper extends the mean-field analysis to deep residual networks, offering valuable insights and techniques applicable to the analysis of Transformers' optimization landscape."}, {"fullname_first_author": "Luigi Ambrosio", "paper_title": "Gradient Flows in Metric Spaces and in the Space of Probability Measures", "publication_date": "2005-01-01", "reason": "This book provides the mathematical foundation for Wasserstein gradient flows, a crucial concept in the paper's analysis of Transformer optimization."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-03", "reason": "As the seminal paper introducing the Transformer architecture, it serves as the basis for the model studied in this paper."}, {"fullname_first_author": "Zeyuan Allen-Zhu", "paper_title": "A convergence theory for deep learning via over-parameterization", "publication_date": "2019-06-09", "reason": "This paper provides a significant convergence theory for deep learning models, which serves as a theoretical foundation for the analysis of over-parameterized Transformers."}]}