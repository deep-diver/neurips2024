[{"figure_path": "V6hrg4O9gg/tables/tables_7_1.jpg", "caption": "Table 1: Summary of C++ to CUDA translation results across various code metrics and compilation accuracy. Second-best results are underlined.", "description": "This table presents the quantitative results of the C++ to CUDA translation task performed by CODEROSETTA and several other models (GPT4, Gemini-Ultra, Gemini-Pro, DeepSeekCoder, StarCoder, TransCoder, and BabelTower).  The metrics used to evaluate the translation quality are BLEU, CodeBLEU, ChrF, and ROUGE-L.  Compilation accuracy, representing the percentage of successfully compiled translated code, is also included.  Underlining highlights the second-best performance for each metric, demonstrating CODEROSETTA's improved performance compared to the state-of-the-art baselines.", "section": "5.1 C++ to CUDA"}, {"figure_path": "V6hrg4O9gg/tables/tables_8_1.jpg", "caption": "Table 3: Ablation Study for C++ to CUDA.", "description": "This ablation study analyzes the impact of each training objective (Masked Language Modeling, Abstract Syntax Tree Entity Recognition, Denoising Autoencoding with adaptive noise injection, and back translation) on the code translation results for C++ to CUDA.  It shows the BLEU and CodeBLEU scores for each experiment where one of these training components was removed. The baseline results are also provided for comparison.", "section": "5.3 Ablation Study"}, {"figure_path": "V6hrg4O9gg/tables/tables_8_2.jpg", "caption": "Table 2: Types of compilation errors (28 codes with compilation error out of a total 180 codes).", "description": "This table shows the frequency of different types of compilation errors encountered in the 28 out of 180 code samples that failed to compile after translation.  The most frequent errors were undefined generic types (48%), missing variable initialization (26%), and missing closing braces (9%).  Other less frequent errors include wrong function calls and non-trivial errors.", "section": "5.1.1 Post-processing: Compilation Error Analysis"}, {"figure_path": "V6hrg4O9gg/tables/tables_9_1.jpg", "caption": "Table 4: Fortran to C++ translation results.", "description": "This table presents the CodeBLEU scores achieved by various models on the Fortran to C++ translation task.  The models compared include several large language models (LLMs) like GPT-4, Gemini-Ultra, and Gemini-Pro, as well as open-source models such as DeepSeekCoder and StarCoder.  A fine-tuned version of StarCoder is also included.  The results highlight the performance of CODEROSETTA, which significantly outperforms the other models on this challenging translation task.", "section": "5.4 Fortran to C++"}, {"figure_path": "V6hrg4O9gg/tables/tables_14_1.jpg", "caption": "Table 5: CUDA to C++ translation results across different models. We use a similar prompt as the one in Figure 15 with small adjustments.", "description": "This table presents the results of translating CUDA code back to C++ using different models, including GPT4, Gemini-Pro, and the proposed CODEROSETTA model.  The models were evaluated using the BLEU and CodeBLEU metrics.  The prompt used for evaluation was similar to that in Figure 15 but with slight modifications.", "section": "A CUDA to C++ Translation Results"}, {"figure_path": "V6hrg4O9gg/tables/tables_16_1.jpg", "caption": "Table 1: Summary of C++ to CUDA translation results across various code metrics and compilation accuracy. Second-best results are underlined.", "description": "This table presents the quantitative results of the C++ to CUDA translation task, comparing CODEROSETTA against several state-of-the-art baselines and large language models.  Metrics include BLEU, CodeBLEU, ChrF, and ROUGE-L, reflecting different aspects of code translation quality. Compilation accuracy is also provided as a measure of the practical utility of the generated code.  The table highlights CODEROSETTA's superior performance, particularly when compared to closed-source LLMs.", "section": "5.1 C++ to CUDA"}, {"figure_path": "V6hrg4O9gg/tables/tables_18_1.jpg", "caption": "Table 1: Summary of C++ to CUDA translation results across various code metrics and compilation accuracy. Second-best results are underlined.", "description": "This table presents a comparison of different models' performance on the task of translating C++ code to CUDA code.  The models are evaluated using several metrics, including BLEU, CodeBLEU, ChrF, and ROGUE-L, which measure different aspects of translation quality.  Compilation accuracy is also reported, indicating the percentage of successfully compiled translations.  The table highlights CODEROSETTA's superior performance compared to other models, especially in terms of BLEU and CodeBLEU scores and compilation accuracy.", "section": "5.1 C++ to CUDA"}, {"figure_path": "V6hrg4O9gg/tables/tables_19_1.jpg", "caption": "Table 8: Dataset statistics for C++, CUDA, and Fortran programming languages.", "description": "This table presents the sizes of the training, validation, and test datasets used in the experiments.  The datasets are categorized by programming language pair (C++ \u2194 CUDA and C++ \u2194 Fortran).  Note that the C++ \u2194 CUDA dataset includes both unpaired and paired data, while the C++ \u2194 Fortran dataset includes unpaired and paired data. The sizes are provided in terms of the number of files and their total size in MB or KB.", "section": "4.1 Datasets"}, {"figure_path": "V6hrg4O9gg/tables/tables_19_2.jpg", "caption": "Table 9: Effect of different beam sizes on C++ to CUDA translation.", "description": "This table presents the results of an ablation study that shows the impact of varying beam sizes on the performance of the CODEROSETTA model for C++ to CUDA code translation.  The metrics used to evaluate the model's performance are BLEU and CodeBLEU. The table shows that a beam size of 5 yields the best results in terms of both BLEU and CodeBLEU scores.  Larger beam sizes do not significantly improve the results, indicating that a beam size of 5 provides a good balance between exploration and exploitation.", "section": "5.1 C++ to CUDA"}, {"figure_path": "V6hrg4O9gg/tables/tables_19_3.jpg", "caption": "Table 10: C++ to Fortran translation results in terms of CodeBLEU.", "description": "This table presents the CodeBLEU scores achieved by different large language models (LLMs) on the task of translating C++ code to Fortran code.  The models compared include GPT4, Gemini-Ultra, Gemini-Pro, and the authors' proposed model, CODEROSETTA.  The results highlight CODEROSETTA's superior performance in this complex translation task.", "section": "5.4 Fortran to C++"}]