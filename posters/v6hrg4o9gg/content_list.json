[{"type": "text", "text": "CodeRosetta: Pushing the Boundaries of Unsupervised Code Translation for Parallel Programming ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ali TehraniJamsaz, Arijit Bhattacharjee, Le Chen, Nesreen K. Ahmed\u2662 Amir Yazdanbakhsh\u2660, Ali Jannesari ", "page_idx": 0}, {"type": "text", "text": "Iowa State University, Ames, Iowa, USA {tehrani, arbhatt9, lechen, jannesari}@iastate.edu \u2662Cisco Outshift, San Jose, CA, USA nesahmed@cisco.com \u2660Google DeepMind, Mountain View, CA, USA ayazdan@google.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advancements in Large Language Models (LLMs) have renewed interest in automatic programming language translation. Encoder-decoder transformer models, in particular, have shown promise in translating between different programming languages. However, translating between a language and its high-performance computing (HPC) extensions remains underexplored due to challenges such as complex parallel semantics. In this paper, we introduce CodeRosetta, an encoder-decoder transformer model designed specifically for translating between programming languages and their HPC extensions. CodeRosetta is evaluated on ${\\mathrm{C}}++\\leftrightarrow{\\mathrm{CUDA}}$ and Fortran $\\leftrightarrow C++$ translation tasks. It uses a customized learning framework with tailored pretraining and training objectives to effectively capture both code semantics and parallel structural nuances, enabling bidirectional translation. Our results show that CodeRosetta outperforms state-of-the-art baselines in $C++$ to CUDA translation by 2.9 BLEU and 1.72 CodeBLEU points while improving compilation accuracy by $6.05\\%$ . Compared to general closed-source LLMs, our method improves $C++$ to CUDA translation by 22.08 BLEU and 14.39 CodeBLEU, with $2.75\\%$ higher compilation accuracy. Finally, CodeRosetta exhibits proficiency in Fortran to parallel $C++$ translation, marking it, to our knowledge, as the first encoder-decoder model for this complex task, improving CodeBLEU by at least 4.63 points compared to closed-source and open-code LLMs.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Automatic code translation between programming languages offers numerous benefits, such as modernizing legacy systems, enabling cross-platform development, and refactoring sequential code into parallel high-performance versions. However, this task poses significant challenges, primarily due to the scarcity of parallel corpora\u2014paired datasets of the same applications written in different languages (e.g., ${\\mathrm{C}}\\mathrm{+}+\\leftrightarrow{\\mathrm{CUDA}}$ or Fortran $\\leftrightarrow C++$ ). This lack of data limits the effectiveness of supervised learning approaches. While recent advances in code LLMs have shown promise in general code translation, translating code that involves parallel programming paradigms (e.g., $C++$ to CUDA) remains largely unexplored. That is primarily due to the inherent complexities in capturing and correctly replicating parallel code semantics [28]. ", "page_idx": 0}, {"type": "text", "text": "TransCoder [36] and its follow-up works [37, 39] have demonstrated the potential of unsupervised learning for code translation. However, these methods often struggle with the complexities of translating between a language and its specialized extensions, such as $C++$ to CUDA. To address this, BabelTower [46] proposes a CUDA-specific metric and ranking model. Yet, its reliance on languageor library-specific metrics limits its scope, restricting it to unidirectional code translation $\\mathbb{C}\\mathbf{+}+\\mathbf{\\beta}\\rightarrow$ CUDA). Moreover, extending BabelTower to other programming paradigms requires redefining syntax-specific metrics, a process that is both time-consuming and dependent on domain expertise. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address these limitations, we introduce CodeRosetta, an encoder-decoder transformer model specifically designed for unsupervised translation between programming languages and their highperformance computing (HPC) parallel extensions. Unlike prior methods that rely on language-specific metrics, CodeRosetta employs new pre-training and training objectives\u2014including Abstract Syntax Tree (AST) Entity Recognition and customized noise injection strategies for Denoising AutoEncoding\u2014to learn the inherent features and semantics of code in an unsupervised manner, without relying on language-specific metrics. In summary, this paper makes the following contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Unsupervised code translation for parallel programming. We present CodeRosetta, an encoder-decoder transformer model tailored for translating between programming languages and their parallel programming extension, specifically targeting $C++$ to CUDA and Fortran to $C++$ . ", "page_idx": 1}, {"type": "text", "text": "\u2022 Customized pre-training and training objectives for code translation to parallel programs. We introduce two new learning objectives for learning parallel programming syntax and nuances: (1) Abstract Syntax Tree (AST) entity recognition, enabling the model to reason about code structure by identifying and categorizing different syntactic elements, and (2) tailored denoising auto-encoding, incorporating weighted token dropping and insertion, along with an adaptive corruption rate, to help the model discern subtle differences between language constructs and their extensions. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Bidirectional translation without language-specific metrics. Unlike prior works that rely on program-specific metrics for parallel code translation, which narrow the scope of code translation, CodeRosetta learns bidirectionally (e.g., $\\mathrm{C}++\\leftrightarrow$ CUDA and CUDA $\\leftrightarrow C++$ ) in an unsupervised manner, broadening its scope to different translation tasks. ", "page_idx": 1}, {"type": "text", "text": "Our results show that for $C++$ to CUDA translation, CodeRosetta achieves a 2.9 BLEU and 1.72 CodeBLUE improvement over existing methods while also increasing compilation accuracy by $6.\\,05\\%$ . Compared to closed-source LLMs, CodeRosetta\u2019s bidirectional approach exhibits even higher gains, with a 19.84 BLEU and 14.39 CodeBLEU improvement, and $2.75\\%$ higher compilation accuracy. To the best of our knowledge, CodeRosetta is the first model to demonstrate proficiency in the task of Fortran to $C++$ translation, surpassing the performance of existing closed-source LLMs and open-code LLMs on standard metrics, with up to 4.63-point improvement in CodeBLEU. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Automatic parallelization. Translating from C to CUDA poses a major challenge. Early efforts in this area primarily involved semi-automatic tools that required significant developer intervention. Noaje et al. [30] implemented an OpenMP C [11] to CUDA translation using the OMPi compiler. Other tools, such as CUDAfy.NET and GPUcc [48], provided annotations to assist the translation process. DawnCC [27] automatically annotates $\\mathbf{C}$ and $C++$ code for parallelism, utilizing static analysis to identify opportunities for optimizing execution on multicore and GPU architectures with OpenMP/OpenACC directives. However, much of the responsibility for identifying parallelizable sections and optimizing memory usage remained with the developer. Efforts to translate between $C/C++$ and Fortran have been more limited. FABLE [15] is one of the few frameworks designed for this, facilitating automatic translation of Fortran to $C++$ while preserving the original code\u2019s semantics through advanced analysis and transformation techniques. ", "page_idx": 1}, {"type": "text", "text": "Neural machine translation. Tournavitis et al. [42] proposed a framework that combines static analysis with machine learning to identify parallelizable code regions and determine the optimal parallelization scheme. This adaptive approach aims to reduce the overhead of manual parallelization while accommodating different architectures. TransCoder [36] pioneered the use of unsupervised learning techniques to translate code across various high-level languages, including Java, $C++$ , and Python, without the need for parallel corpora. Building on TransCoder\u2019s architecture, BabelTower [46] extends its capabilities to perform parallel semantic conversion between C and CUDA. ", "page_idx": 1}, {"type": "text", "text": "Denoising Auto-Encoding (DAE) has become a popular technique for training encoder-decoder models, as seen in methods like CodeT5 [45] and PLBART [2]. These models typically use common noising strategies such as masking and token dropping. One of the key differences in the noising strategies used by CodeRosetta lies in its language-specific characteristics. Rather than random token dropping, CodeRosetta employs weighted random dropping, prioritizing language-specific reserved keywords to enhance the model\u2019s understanding of the target language\u2019s semantics. Another unique strategy is token insertion, which encourages the model to differentiate between valid and invalid tokens. These objectives enable CodeRosetta to better distinguish between different extensions of the same programming language. In summary, CodeRosetta is a sequence-to-sequence transformer model that learns in an unsupervised manner to translate between programming languages and parallel programming APIs. Additional related work is presented in Appendix J. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 CodeRosetta: Unsupervised Code Translation for Parallel Programming ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section presents the design and training methodology of CodeRosetta, our proposed encoderdecoder transformer model for unsupervised code translation. We begin by outlining the overall architecture, followed by a detailed discussion of the pre-training and training objectives that enable CodeRosetta to effectively capture the nuances of both general-purpose programming languages and their parallel extensions. We focus on the $\\mathrm{C}++\\leftrightarrow$ CUDA and $\\mathrm{C}\\!+\\!+\\!\\leftrightarrow$ Fortran translation tasks. ", "page_idx": 2}, {"type": "text", "text": "3.1 Cross Language Masked Language Modeling ", "text_level": 1, "page_idx": 2}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/928d53e2a492e63e9dcaf94efe7aed5c0a3a60e9ca0e944b90cda65fa54c4569.jpg", "img_caption": ["Figure 1: Masked Language Modeling (MLM) pretraining steps in CodeRosetta. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Pre-training plays a crucial role in enabling transformer models to develop a foundational understanding of programming languages. We use Masked Language Modeling (MLM) [47], a widely adopted pre-training objective, to achieve this, as outlined in Figure 1. In MLM, the model receives input code with a portion of tokens randomly masked. The objective is to predict the masked tokens based on the surrounding context, thereby encouraging the model to learn both local syntactic patterns and broader semantic relationships within code. ", "page_idx": 2}, {"type": "text", "text": "To further challenge the model and better reflect code structure, we mask entire words rather than individual tokens. For instance, in the input code snippet \u201c int index\u201d, the entire word \u201c index\u201d would be masked, requiring the model to predict the missing identifier based on its type (\u201c int\u201d) and its usage in the surrounding code. This approach mirrors how code comprehension often relies on understanding the roles of variables and functions within their scope. ", "page_idx": 2}, {"type": "text", "text": "Additionally, while MLM is typically applied to monolingual datasets, we extend it to a cross-lingual setting by training on a combined dataset of both $C++$ and the target language (CUDA or Fortran). This cross-lingual exposure enables CodeRosetta to learn shared programming concepts and syntactic structures across languages, such as control flow statements (if, else, while) and variable declarations. By recognizing these commonalities, the model can transfer knowledge across languages, improving its ability to translate even unseen code patterns. ", "page_idx": 2}, {"type": "text", "text": "3.2 Abstract Syntax Tree Entity Recognition ", "text_level": 1, "page_idx": 2}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/3694356e3825082beeaac4f76b923f0acb38acc8b28670bcafc2ade4768e8b01.jpg", "img_caption": ["Figure 2: Abstract Syntax Tree Entity Recognition pretraining steps in CodeRosetta. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Following cross-lingual MLM pre-training, we introduce a new pre-training objective called Abstract Syntax Tree (AST) Entity Recognition (AER) to further improve CodeRosetta\u2019s understanding of code structure. This approach draws inspiration from Named Entity Recognition (NER) in natural language processing [20], where models learn to classify words or phrases into predefined categories (e.g., person, location, or organization). In AER, CodeRosetta learns to recognize and categorize various syntactic components in code. ", "page_idx": 3}, {"type": "text", "text": "The process, illustrated in Figure 2, starts by using Tree-sitter2, a multi-language parsing library, to generate the Abstract Syntax Tree (AST) of a source code snippet. The AST representation provides a hierarchical, tree-structured view of the code, with each node corresponding to constructs such as function definitions, variable declarations, or arithmetic expressions. From this AST, we extract a set of entities and their corresponding categories. Examples of categories used in our implementation include function, variable, constant, pointer, and literal. ", "page_idx": 3}, {"type": "text", "text": "During AER pre-training, CodeRosetta tokenizes the input code and predicts the syntactic category of each token based on its role in the AST. Tokens that do not correspond to any specific category are labeled as \u201cO\u201d (Outside). This training enables CodeRosetta to develop an understanding of the syntactic relationships between code elements, an essential step in accurately translating and generating code across different languages and extensions. ", "page_idx": 3}, {"type": "text", "text": "A key strength of AER is its flexibility\u2014the set of entity categories can be easily adapted for different languages or programming paradigms. For instance, when focusing on CUDA code, we can introduce specialized categories for parallel constructs such as threadIdx, blockIdx, and gridDim, enabling CodeRosetta to learn the language-specific semantics of parallel programming. ", "page_idx": 3}, {"type": "text", "text": "Furthermore, AER is highly adaptable. Even in cases where AST parsing is only partially available, CodeRosetta can still leverage this pre-training, showcasing its applicability to diverse code translation tasks. The complete list of tags used in our implementation is provided in Appendix D.2. ", "page_idx": 3}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/adc021a8a31da07b4adbe5e1dc47c473a7525c66661c4896f54627369718e46e.jpg", "img_caption": ["Figure 3: Denoising Auto Encoding. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "While cross-lingual MLM and AST Entity Recognition effectively pre-train CodeRosetta\u2019s encoder to generate meaningful representations of source code, the decoder remains untrained at this stage. Consequently, attempting direct code translation would result in suboptimal performance due to the decoder\u2019s lack of exposure to the target language\u2019s syntax and semantics. To bridge this gap, we employ a Denoising Auto-Encoding (DAE) training strategy specifically tailored for code translation with adaptive noise injection mechanisms. In essence, DAE training involves corrupting the input source code with various types of noise and then training the model to reconstruct the original, noise-free code. This process compels the decoder to learn both the underlying syntactic rules of the target language and the ability to recover meaningful code from perturbed inputs, simulating the challenges of translating real-world code with potential variations and inconsistencies. ", "page_idx": 3}, {"type": "text", "text": "To initiate the DAE training phase, we first initialize the decoder using the pre-trained encoder\u2019s weights, providing it with a starting point for language understanding. Next, we apply a combination of common noise injection techniques, such as random token masking and shuflifng, alongside our new noise strategies designed to emphasize the distinctions between programming languages and their extensions. Figure 3 illustrates the overall process of DAE training in CodeRosetta. We now delve into the specifics of our customized noise injection methods, which distinguish CodeRosetta from conventional DAE-based code translation models. These strategies are crucial for enabling the model to discern the subtle but significant differences between languages like $C++$ and their high-performance counterparts like CUDA. ", "page_idx": 3}, {"type": "text", "text": "Weighted token dropping. To encourage the model to learn the distinctive features of each language and its extensions, we introduce a weighted token dropping strategy during the noise injection phase. Unlike uniform random token removal, this approach assigns higher removal probabilities to language-specific keywords, encouraging the model to focus on critical syntactic elements. ", "page_idx": 4}, {"type": "text", "text": "For each programming language or extension, CodeRosetta maintains a list of reserved keywords. During token dropping, these keywords are prioritized, making them more likely to be removed than other tokens. For example, when training on CUDA code, keywords like blockIdx, threadIdx, blockDim, __global__, and atomicSub are more frequently targeted for removal. ", "page_idx": 4}, {"type": "text", "text": "This weighted sampling creates a more challenging reconstruction task for the model, compelling the decoder to develop a deeper understanding of the language-specific semantics and parallel programming constructs. While the reserved keywords are given higher priority, the weighted random sampling still ensures that other tokens are occasionally dropped, preserving the overall balance of the noise injection process. ", "page_idx": 4}, {"type": "text", "text": "Language-specific token insertion. In addition to weighted token dropping, we implement a language-specific token insertion strategy to improve CodeRosetta\u2019s ability to discern between languages and their extensions during code generation. This method strengthens the model\u2019s robustness against out-of-vocabulary tokens, preventing it from inadvertently blending elements from different languages. ", "page_idx": 4}, {"type": "text", "text": "During DAE training, CodeRosetta must distinguish between valid and invalid tokens within the target language. To facilitate this, we construct a vocabulary of unique tokens for each programming language in our training dataset, tracking their frequency of occurrence. Tokens from the vocabulary of other languages are then randomly inserted into the input code based on their probability from the frequency distribution. For example, in the $C++$ to CUDA translation task, we insert CUDA-specific tokens into $C++$ code inputs during DAE training. CodeRosetta is then trained to recognize and disregard these foreign tokens while reconstructing the original $C++$ code. This process enables the model to develop an understanding of language boundaries, ensuring it generates syntactically and semantically valid code during translation. ", "page_idx": 4}, {"type": "text", "text": "Adaptive noise ratios Additionally, we introduce an adaptive noise strategy. Instead of applying a fixed noise ratio, such as $10\\%$ for token dropping, we begin with an initial noise ratio and progressively increase it throughout the training process. This approach allows the model to gradually adapt to more challenging conditions as it learns to reconstruct the corrupted input sequences. As the training progresses, the input sequences become increasingly corrupted, making the reconstruction task more dififcult and forcing the model to learn more robust representations. ", "page_idx": 4}, {"type": "text", "text": "There is a maximum corruption rate that, once reached, halts further increases in noise. This prevents over-corrupting the inputs, ensuring that the model can still derive meaningful patterns. The impact of adaptive noise ratios, along with the new noise strategies, is examined in our ablation study (Section 5.3). ", "page_idx": 4}, {"type": "text", "text": "To further support accurate code generation in the target language, we prepend a special <LANG> token to each input sequence. During DAE, this token indicates the language of the corrupted input, prompting the decoder to reconstruct the code in the same language. This mechanism ensures that the model remains focused on generating code within the correct language context. ", "page_idx": 4}, {"type": "text", "text": "3.4 Back Translation for Unsupervised Refinement ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/bc8549525dc5ad87f12b1d52be8efef82fc3c5c0de5eceaf077de7fce409f31e.jpg", "img_caption": ["Figure 4: Back Translation. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "To further improve CodeRosetta\u2019s translation quality and its ability to capture complex code semantics, we employ back translation during the training process [36]. As illustrated in Figure 4, this technique leverages the model\u2019s bidirectional capability, enabling both source-to-target and target-to-source translations, forming a weakly supervised learning loop. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "In back translation, the model is trained on a source-to-target task (e.g., $C++$ to CUDA) while simultaneously performing the reverse translation (target-to-source, CUDA to $C++$ ). For each batch of source code, CodeRosetta first translates it into the target language. The generated target code is then used as input for a reverse translation, where the model attempts to reconstruct the original source code. ", "page_idx": 5}, {"type": "text", "text": "This forward and backward translation cycle provides continuous feedback, allowing CodeRosetta to compare the reconstructed source code with the original input, thereby learning to detect and correct errors in both translation directions. Through this iterative refinement, the model gradually improves its comprehension of nuanced language differences and complex code structures, resulting in more accurate and semantically consistent translations. ", "page_idx": 5}, {"type": "text", "text": "Crucially, we alternate between batches of different language pairs during back translation. This ensures that the model receives balanced exposure to both directions, preventing bias towards a specific language and encouraging the development of robust, generalized translation capabilities. ", "page_idx": 5}, {"type": "text", "text": "3.5 Finetuning with Synthetic Data from Language Models (Optional Step) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "While CodeRosetta demonstrates promising results through unsupervised training, we explore the potential of further enhancements by leveraging the capabilities of large language models (LLMs) such as GPT-4 [1] and Gemini Ultra [41]. These LLMs, trained on extensive text and code datasets, have exhibited impressive code generation abilities. However, directly employing such large models for code translation can be computationally expensive and impractical for many real-world applications. ", "page_idx": 5}, {"type": "text", "text": "To address this, we adopt a knowledge distillation approach [18], where these LLMs serve as teacher models to generate synthetic data for fine-tuning CodeRosetta, a smaller student model. This method allows us to capture the expertise of the larger models while maintaining computational efifciency. ", "page_idx": 5}, {"type": "text", "text": "Specifically, we prompt GPT-4 and Gemini to translate $C++$ code into CUDA where feasible. After filtering out empty or invalid translations, natural text, and non-relevant data (i.e., instances lacking CUDA-specific keywords), we are left with approximately 5,000 high-quality translations from an initial set of 100,000. This significant reduction highlights the inherent challenges in $C++$ to CUDA translation. ", "page_idx": 5}, {"type": "text", "text": "The resulting synthetic dataset of ${\\mathrm{C}}{+}{+}{\\leftrightarrow}{\\mathrm{CUDA}}$ pairs is then used to fine-tune CodeRosetta. This process allows CodeRosetta to incorporate the valuable knowledge embedded in the larger LLMs without incurring their high computational costs. It is important to note that this fine-tuning step is optional and can be omitted if access to powerful LLMs for synthetic data generation is not feasible. ", "page_idx": 5}, {"type": "text", "text": "4 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Training hyperparameters. We implement CodeRosetta using the HuggingFace Transformers library v4.40.1 [47]. The model is a 12-layer encoder-decoder transformer, with each layer having 12 attention heads and a hidden dimension of 1,536. We initialized the tokenizer with a pre-trained Byte Pair Encoding (BPE) tokenizer from UniXcoder [17], which was further trained on our specific training datasets. The training was conducted using the AdamW optimizer [24] and a batch size of 16, using gradient accumulation over two steps. The experiments were run on a single node with four Nvidia A100 SXM4 GPUs, each with 80GB of memory. To speed up the training process, mixed-precision training was enabled. The final model consists of ${\\sim}0.8$ billion parameters. ", "page_idx": 5}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate CodeRosetta on two code translation tasks: $C++$ to CUDA and Fortran to $C++$ . Table 8 provides an overview of the datasets used. For the $C++$ to CUDA translation task, we use the dataset from BabelTower [46], which consists of: ", "page_idx": 5}, {"type": "text", "text": "\u2022 Unpaired training set: A collection of $243{,}008\\,\\,\\mathrm{C}{+}+$ and CUDA source code flies, meeaning there is no direct correspondance between the files in each language. To avoid any language bias, we ensure an equal number of $C++$ and CUDA files during training. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Paired validation and test sets: The validation set consists of 184 pairs, and the test set has 180 pairs of $C++$ and CUDA source code flies. Each pair represents the same program implemented in both languages, providing a benchmark for evaluating translation accuracy. ", "page_idx": 6}, {"type": "text", "text": "For Fortran to $C++$ , no dedicated parallel corpus exists for this specific translation. Thus, we construct our training dataset as follows: ", "page_idx": 6}, {"type": "text", "text": "\u2022 Unpaired training set: We extract the $C++$ and Fortran subsets from the Stack V2 dataset [25], which includes over 3 billion source code flies across more than 600 programming languages. We ensure an equal number of files from each language to prevent bias during training. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Paired fine-tuning set: For fine-tuning, we use the small paired $C++.$ -Fortran dataset introduced by Bin et al. [19]. This set is also used for validation. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Test set: To evaluate the final model performance, we use a test set of 33 paired $C++$ and Fortran programs. ", "page_idx": 6}, {"type": "text", "text": "4.2 Data Preprocessing ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To ensure the quality and consistency of training data, we applied task-specific preprocessing steps for each translation task. $\\mathbf{C++}$ to CUDA. Although the BabelTower dataset [46] was reportedly cleaned, we found noisy data within the CUDA files. To address this, we curated a list of CUDA-specific reserved keywords and flitered the dataset, retaining only those CUDA flies that contained at least one such keyword. This step significantly reduced noise and resulted in a final training set of $243{,}008\\,\\,\\mathrm{C}{+}+$ flies, matched by an equal number of CUDA flies. The validation and test sets remained unchanged, comprising 184 and 180 paired examples, respectively. ", "page_idx": 6}, {"type": "text", "text": "$\\mathbf{C}+\\mathbf{+}$ to Fortran. Preprocessing the Stack V2 dataset for $C++$ to Fortran translation involved managing the large imbalance between $C++$ and Fortran flies, as well as flitering out low-quality or uninformative code snippets. We implemented the following steps: ", "page_idx": 6}, {"type": "text", "text": "\u2022 Educational value flitering: Inspired by the phi-1 model data flitering approach [16], We randomly sampled $100{,}000\\,\\mathrm{C}{+}+$ flies from Stack V2 and employed GPT-3.5 to assess their \u201ceducational value\u201d for learning $C++$ coding concepts. We prompted GPT-3.5 (see Figure 5 to classify each snippet as either \u201cYes\u201d or \u201cNo\u201d based on its educational value. These labels were then used to fine-tune a binary classifier built on the CodeSage model [49], which we applied to the remaining $C++$ flies in Stack V2. Only files deemed educationally valuable were retained. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Balancing language representation: From the filtered $C++$ files, we randomly selected a subset equal in size to the number of Fortran files to create a balanced training set. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Length-based filtering: To ensure training stability and avoid biases toward very short or long code snippets, we filtered out files containing fewer than ten tokens or more than 1,000 tokens in both languages. ", "page_idx": 6}, {"type": "text", "text": "After these steps, the final training set for $C++$ to Fortran translation consisted of 474,856 files. For fine-tuning and validation, we used the small paired $C++$ -Fortran dataset from Bin et al. [19], which contains 282 samples. The model was then evaluated on a test set of 33 paired samples. ", "page_idx": 6}, {"type": "table", "img_path": "V6hrg4O9gg/tmp/8492a5a2a5f81ad6153d242d46067cc4878affb7c843dd437fcbbc9f7ac972cb.jpg", "table_caption": ["Table 1: Summary of $C++$ to CUDA translation results across various code metrics and compilation accuracy. Second-best results are underlined. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "CUDA, we compare (a) \u201cBabelTower [46]\u201d,3 a state-of-the-art unsupervised code translation model specifically designed for $C++$ to CUDA translation, and (b) \u201cTranscoder [36]\u201d, a general unsupervised code translation model that has demonstrated strong performance on various language pairs. Since a single evaluation metric may capture only one aspect of translation quality [14], we supplement BLEU and CodeBLEU with ROUGE-L [22] and ChrF [33], as recommended by [14]. However, because generated translations from TransCoder and BabelTower were unavailable, ROUGE-L and ChrF scores are only provided for GPT-4, Gemini-Ultra, and Gemini-Pro. We further compare CodeRosetta with two popular open-source code LLMs: StarCoder (starcoder2-7b) [21] and DeepSeekCoder (DeepSeek-Coder-V2-Lite-Base) [12]. ", "page_idx": 7}, {"type": "text", "text": "For the Fortran to $C++$ task, we evaluate CodeRosetta against StarCoder [21], an LLM model (15.5B parameters) featuring a decoder-only transformer architecture, fine-tuned on a comprehensive corpus of Fortran code and DeepSeekCoder (DeepSeek-Coder-V2-Lite-Base) [12]. Additionally, we evaluate CodeRosetta alongside several prominent closed-source LLMs, including GPT-4 [1] and Gemini [41], by prompting them to perform code translation using carefully crafted prompts (Appendix I). By evaluating against a broad spectrum of both specialized code translation models and general-purpose LLMs, we effectively gauge CodeRosetta\u2019s stranghts and limitations across diverse translation tasks and programming paradigms. ", "page_idx": 7}, {"type": "text", "text": "5 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1 $\\mathbf{C}++$ to CUDA ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 1 presents the results of CodeRosetta for ${\\mathrm{C}}{+}{+}{\\rightarrow}{\\mathrm{CUDA}}$ translation. For BabelTower and TransCoder, the results are directly quoted from BabelTower [46], as their models and implementations are not publicly available. Comparing the performance of CodeRosetta to other models, it demonstrates superior translation capabilities for $C++$ to CUDA. Specifically, CodeRosetta outperforms BabelTower by 2.9 BLEU points. Additionally, it achieves a CodeBLEU score of 78.84, which is 1.72 points higher than BabelTower. Although GPT4 and Gemini were not specifically trained on this dataset, they still reached CodeBLEU scores of 64.45 and 64.20, respectively. Evtikhiev et.al [14] indicate that ChrF and ROGUE-L metrics are better suited for code generation tasks than BLEU and CodeBLEU. Notably, CodeRosetta also surpasses these models in both ChrF and ROUGE-L metrics. ", "page_idx": 7}, {"type": "text", "text": "CodeRosetta effectively learns the necessary semantics to generate CUDA code without relying on specific metrics for training, a departure from previous approaches. The compilation accuracy of CodeRosetta is $98\\cdot85\\%$ after post-processing. For examples of the CUDA code generated by our model compared to other baselines, please refer to Appendix B. Furthermore, CodeRosetta is bidirectional, allowing it to translate both $C++$ to CUDA and vice versa. Please refer to Appendix A for CUDA to $C++$ results. ", "page_idx": 7}, {"type": "table", "img_path": "V6hrg4O9gg/tmp/a7b43074d5ca4d4b7974600eb6c392da2ae0e3cd6f64a29c7194f4471b835937.jpg", "table_caption": ["Table 3: Ablation Study for $C++$ to CUDA. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.1.1 Post-processing: Compilation Error Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our test set, consisting of 180 samples, provided diverse input scenarios to evaluate our model\u2019s performance. We observed that 23 samples generated compilation errors when processed through the NVCC compiler with the required flags.4 Upon manual investigation, we found that most errors were trivial and could be easily fixed with minor edits. ", "page_idx": 8}, {"type": "text", "text": "Specifically, $48\\%$ of the errors were attributed to the use of an undefined generic type T. Another $9\\%$ resulted from missing closing braces, while $26\\%$ were due to a single missing variable initialization. Additionally, $9\\%$ of the errors were caused by incorrect function calls. Only $8\\%$ of the files contained no trivial errors. By applying quick fixes for the undefined generic type T, missing variable initializations, and missing closing braces, the overall compilation accuracy significantly improved, with $98\\cdot85\\%$ of all generated code becoming compilable. This indicates that most errors were simple and could be easily resolved by incorporating compiler feedback, which will be a focus of our future work. Subsection F.1 and Figure 13 in the Appendix presents examples of our findings. ", "page_idx": 8}, {"type": "table", "img_path": "V6hrg4O9gg/tmp/3c345e067d9848d65cd3b46f981acf5644c131dfb252abb3570a4735403ecefd.jpg", "table_caption": ["Table 2: Types of compilation errors (28 codes with compilation error out of a total 180 codes). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2 Runtime Evaluation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Although CodeRosetta demonstrates more accurate translations based on the aforementioned metrics compared to the reference code, these metrics are derived from static evaluations, leaving runtime performance uncertain. To address this, we randomly selected 30 translated CUDA kernels from the test set and created unique template programs to execute them. We ran the translated CUDA kernels using NVCC and found that the functional correctness of the generated code was preserved in the majority of samples (approximately $93\\%$ ). For further details, see Appendix Section B. ", "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct an ablation study to evaluate the impact of each training objective on the code translation results of CodeRosetta. Specifically, we remove individual training objectives (e.g., AER) while keeping the other components intact and retraining the model. Table 3 presents the results of the ablation study for $C++$ to CUDA translation. As observed, removing any of the pertaining or training objectives negatively impacts translation results, with Masked Language Modeling having the most significant effect when omitted. This is expected, as Masked Language Modeling is the primary pretraining objective that enables the model to understand source code. ", "page_idx": 8}, {"type": "text", "text": "AER training task. CodeRosetta employs two pre-training tasks for training its encoder: Mask Language Modeling (MLM) and Abstract Syntax Tree Entity Recognition (AER). In this phase, we maintain consistent training setups except for the removal of the AER component. ", "page_idx": 8}, {"type": "table", "img_path": "V6hrg4O9gg/tmp/ed90a89bb9bc913b0ea2e3b53ae5c0c5ef14c1f8a5453d6c4a1fee1061159484.jpg", "table_caption": ["Table 4: Fortran to $C++$ translation results. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Denoising Auto Encoding. We also investigate the effectiveness of various noise types and the adaptive corruption rate during Denoising Auto Encoding. For this ablation study, we train the model without weighted token dropping, insertion, and adaptive corruption rate. ", "page_idx": 9}, {"type": "text", "text": "Fine-tuning Data extraction from larger models is a common practice. In this phase of the ablation study, we evaluate CodeRosetta\u2019s performance without fine-tuning it on the synthetic dataset. From Table 3, we observe that the removal of each proposed learning objective negatively impacts the model\u2019s ability to deliver improved code translation. ", "page_idx": 9}, {"type": "text", "text": "5.4 Fortran to $\\mathbf{C++}$ ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We train and apply CodeRosetta for translation between $C++$ and Fortran. Fortran has had a long-standing presence in the scientific computing community; however, its integration with modern HPC systems [38] can pose significant challenges for developers. Due to the complexities involved in translating Fortran to $C++$ , there has been limited effort to address this issue. Bin et al. [19] were the first to make significant strides in this area, curating a small paired dataset specifically for this translation task and fine-tuning several open-code LLMs. ", "page_idx": 9}, {"type": "text", "text": "They found StarCoder (15B), when fine-tuned, benefited the most from their paired dataset. We compare CodeRosetta with the fine-tuned StarCoder (15B), as well as with other general LLMs. The results are shown in Table 4. Fine-tuning CodeRosetta on the dataset from Bin et al. [19] further enhances its performance, achieving a CodeBLEU score of 65.93. Notably, CodeRosetta outperforms StarCoder, even though StarCoder is nearly 20 times larger, highlighting the efifciency of our model. It also surpasses state-of-the-art models like GPT-4 and Gemini by a substantial margin, achieving an improvement of at least 4.63 points in CodeBLEU. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduced CodeRosetta, an encoder-decoder transformer model designed for translating between programming languages and their high-performance computing (HPC) extensions. We proposed two novel learning objectives: Abstract Syntax Tree (AST) Entity Recognition (AER) and customized Denoising Auto-Encoding, which incorporates weighted token dropping and insertion. These contributions enable CodeRosetta to capture both the general syntactic structure of code and the specific nuances of parallel programming constructs, without relying on language-specific metrics. Our experiments show that CodeRosetta significantly outperforms state-of-the-art baselines on $C++$ to CUDA translation, achieving improvements up to 2.9 BLEU, 1.72 in CodeBLEU, and $6.05\\%$ in compilation accuracy. Furthermore, CodeRosetta is, to the best of our knowledge, the first model to demonstrate proficiency in translating Fortran to its parallel counterpart in $C++$ , highlighting its potential in handling diverse programming paradigms. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank NSF for their generous support in funding this project (#2211982). In addition, we extend our gratitude to Intel Labs for supporting this project. We also would like to extend our gratitude towards Pengcheng Yin and Chandu Thekkath for their feedback on the early draft of this work. We also appreciate the support from the extended team at Google DeepMind. We thank the Research IT team5 of Iowa State University for providing access to HPC clusters for conducting the experiments of this research project. We also thank National Center for Supercomputing Applications for providing Delta GPUs through allocation CIS230375 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program [7]. Lastly, we would like to express our sincere appreciation to the anonymous reviewers, area chairs, and program chairs of NeurIPS 2024 for their valuable feedback and insights, which significantly contributed to the improvement of this work. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023.   \n[2] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Unified Pre-training for Program Understanding and Generation. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, NAACL, 2021.   \n[3] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighof,f Mayank Mishra, Alex Gu, Manan Dey, et al. SantaCoder: Don\u2019t Reach for the Stars! arXiv preprint arXiv:2301.03988, 2023.   \n[4] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M\u00e9rouane Debbah, \u00c9tienne Gofifnet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The Falcon Series of Open Language Models. arXiv preprint arXiv:2311.16867, 2023.   \n[5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen Technical Report. arXiv preprint arXiv:2309.16609, 2023.   \n[6] Mohamed-Walid Benabderrahmane, Louis-No\u00ebl Pouchet, Albert Cohen, and C\u00e9dric Bastoul. The Polyhedral Model Is More Widely Applicable Than You Think. In Proceedings of the 19th Joint European Conference on Theory and Practice of Software, International Conference on Compiler Construction. Springer-Verlag, 2010.   \n[7] Timothy J. Boerner, Stephen Deems, Thomas R. Furlani, Shelley L. Knuth, and John Towns. ACCESS: Advancing Innovation: NSF\u2019s Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support. In PEARC, 2023.   \n[8] Uday Bondhugula, Albert Hartono, J. Ramanujam, and P. Sadayappan. A Practical Automatic Polyhedral Parallelizer and Locality Optimizer. In PLDI, 2008.   \n[9] Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al. MultiPLE: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation. IEEE Transactions on Software Engineering, 2023.   \n[10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374, 2021.   \n[11] L. Dagum and R. Menon. OpenMP: An Industry Standard API for Shared-memory Programming. IEEE Computational Science and Engineering, 1998.   \n[12] DeepSeek-AI, Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y. Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding Zeng, Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao, Zhibin Gou, Zhenda Xie, Zhewen Hao, Bingxuan Wang, Junxiao Song, Deli Chen, Xin Xie, Kang Guan, Yuxiang You, Aixin Liu, Qiushi Du, Wenjun Gao, Xuan Lu, Qinyu Chen, Yaohui Wang, Chengqi Deng, Jiashi Li, Chenggang Zhao, Chong Ruan, Fuli Luo, and Wenfeng Liang. DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence. arXiv preprint arXiv:2406.11931, 2024.   \n[13] Xianzhong Ding, Le Chen, Murali Emani, Chunhua Liao, Pei-Hung Lin, Tristan Vanderbruggen, Zhen Xie, Alberto Cerpa, and Wan Du. HPC-GPT: Integrating Large Language Model for High-Performance Computing. In Proceedings of the SC \u201923 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis, 2023.   \n[14] Mikhail Evtikhiev, Egor Bogomolov, Yaroslav Sokolov, and Timofey Bryksin. Out of the BLEU: How Should We Assess Quality of the Code Generation Models? Journal of Systems and Software, 2023.   \n[15] Ralf W. Grosse-Kunstleve, Thomas C. Terwilliger, Nicholas K. Sauter, and Paul D. Adams. Automatic Fortran to $C++$ Conversion with FABLE. Source Code for Biology and Medicine, 2012.   \n[16] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00e9bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks Are All You Need. arXiv preprint arXiv:2306.11644, 2023.   \n[17] Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. UniXcoder: Unified Cross-Modal Pre-training for Code Representation. In ACL, 2022.   \n[18] Geoffrey Hinton, Oriol Vinyals, and Jef fDean. Distilling the Knowledge in a Neural Network. arXiv preprint arXiv:1503.02531, 2015.   \n[19] Bin Lei, Caiwen Ding, Le Chen, Pei-Hung Lin, and Chunhua Liao. Creating a Dataset for High-Performance Computing Code Translation using LLMs: A Bridge Between OpenMP Fortran and $C++$ . In HPEC, 2023.   \n[20] Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li. A Survey on Deep Learning for Named Entity Recognition. ICDE, 2020.   \n[21] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighof,f Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. StarCoder: May the Source be With You! Transactions on Machine Learning Research, 2023.   \n[22] Chin-Yew Lin. ROUGE: A Package for Automatic Evaluation of Summaries. In Text summarization branches out, 2004.   \n[23] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[24] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In ICLR, 2017.   \n[25] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krau\u00df, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighof,f Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. StarCoder 2 and The Stack v2: The Next Generation. arXiv preprint arXiv:2402.19173, 2024.   \n[26] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering Code Large Language Models with Evol-Instruct. arXiv preprint arXiv:2306.08568, 2023.   \n[27] Gleison Mendon\u00e7a, Breno Guimar\u00e3es, P\u00e9ricles Alves, M\u00e1rcio Pereira, Guido Ara\u00fajo, and Fernando Magno Quint\u00e3o Pereira. DawnCC: Automatic Annotation for Data Parallelism and Oflfoading. ACM TACO, 2017.   \n[28] Daniel Nichols, Joshua H Davis, Zhaojun Xie, Arjun Rajaram, and Abhinav Bhatele. Can Large Language Models Write Parallel Code? In HPDC, 2024.   \n[29] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. arXiv preprint arXiv:2203.13474, 2022.   \n[30] Gabriel Noaje, Christophe Jaillet, and Micha\u00ebl Krajecki. Source-to-Source Code Translator: OpenMP C to CUDA. In HPCC, 2011.   \n[31] Rangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lambert Pouguem Wassi, Michele Merler, Boris Sobolev, Raju Pavuluri, Saurabh Sinha, and Reyhaneh Jabbarvand. Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, pages 1\u201313, 2024.   \n[32] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: A Method for Automatic Evaluation of Machine Translation. In ACL, 2002.   \n[33] Maja Popovi\u0107. chrF: Character n-gram F-score for Automatic MT Evaluation. In Proceedings of the tenth workshop on statistical machine translation, 2015.   \n[34] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. CodeBLEU: a Method for Automatic Evaluation of Code Synthesis. arXiv preprint arXiv:2009.10297, 2020.   \n[35] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. Code Llama: Open Foundation Models for Code. arXiv preprint arXiv:2308.12950, 2023.   \n[36] Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. Unsupervised Translation of Programming Languages. NeurIPS, 2020.   \n[37] Baptiste Roziere, Jie M. Zhang, Francois Charton, Mark Harman, Gabriel Synnaeve, and Guillaume Lample. Leveraging Automated Unit Tests for Unsupervised Code Translation. In ICLR, 2022.   \n[38] Thomas Sterling, Maciej Brodowicz, and Matthew Anderson. High performance Computing: Modern Systems and Practices. Morgan Kaufmann, 2017.   \n[39] Marc Szafraniec, Baptiste Roziere, Hugh Leather, Francois Charton, Patrick Labatut, and Gabriel Synnaeve. Code Translation with Compiler Representations. In ICLR, 2023.   \n[40] CodeGemma Team. CodeGemma: Open Code Models Based on Gemma. arXiv preprint arXiv:2406.11409, 2024.   \n[41] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: A Family of Highly Capable Multimodal Models, 2024.   \n[42] Georgios Tournavitis, Zheng Wang, Bj\u00f6rn Franke, and Michael F.P. O\u2019Boyle. Towards a Holistic Approach to Auto-Parallelization: Integrating Proflie-Driven Parallelism Detection and Machine-Learning Based Mapping. In PLDI, 2009.   \n[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and Efifcient Foundation Language Models. arXiv preprint arXiv:2302.13971, 2023.   \n[44] Sven Verdoolaege, Juan Carlos Juega, Albert Cohen, Jos\u00e9 Ignacio G\u00f3mez, Christian Tenllado, and Francky Catthoor. Polyhedral Parallel Code Generation for CUDA. ACM TACO, 2013.   \n[45] Yue Wang, Weishi Wang, Shafiq Joty, and Steven C. H. Hoi. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. In EMNLP, 2021.   \n[46] Yuanbo Wen, Qi Guo, Qiang Fu, Xiaqing Li, Jianxing Xu, Yanlin Tang, Yongwei Zhao, Xing Hu, Zidong Du, Ling Li, et al. BabelTower: Learning to Auto-parallelized Program Translation. In ICML, 2022.   \n[47] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-Art Natural Language Processing. In EMNLP, 2020.   \n[48] Jingyue Wu, Artem Belevich, Eli Bendersky, Mark Heffernan, Chris Leary, Jacques Pienaar, Bjarke Roune, Rob Springer, Xuetian Weng, and Robert Hundt. gpucc- An Open-Source GPGPU Compiler. In CGO, 2016.   \n[49] Dejiao Zhang, Wasi Ahmad, Ming Tan, Hantian Ding, Ramesh Nallapati, Dan Roth, Xiaofei Ma, and Bing Xiang. Code Representation Learning At Scale. In ICLR, 2024.   \n[50] Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. Opencodeinterpreter: Integrating Code Generation with Execution and Refinement. arXiv preprint arXiv:2402.14658, 2024.   \n[51] Shuyan Zhou, Uri Alon, Sumit Agarwal, and Graham Neubig. CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code. In EMNLP, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A CUDA to $\\mathbf{C++}$ Translation Results 15 ", "page_idx": 14}, {"type": "text", "text": "B Functional Correctness Analysis 16 ", "page_idx": 14}, {"type": "text", "text": "C Decontamination Analysis 16 ", "page_idx": 14}, {"type": "text", "text": "D Unsupervised Training Parameters 18 ", "page_idx": 14}, {"type": "text", "text": "D.1 Training Parameters . . 18   \nD.2 AST Entity Recognition Tags . . 19   \nD.3 Dataset Statistics . . 19   \nE Impact of Beam Size 19   \nF Analysis of Generated Code from CodeRosetta and Closed-Source LLMs 19   \nF.1 Common Issues and Post-processing in CodeRosetta-Generated Code 20   \nG Discussion on Unsupervised Training 20   \nG.1 Fine-tuning for Code Translation 20   \nG.2 Back Translation . . 21 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "H Translation Pitfalls: Invalid Tokens in Target Language 21 ", "page_idx": 14}, {"type": "text", "text": "I Prompt Template and LLMs 21 ", "page_idx": 14}, {"type": "text", "text": "J Additional Related Work 22 ", "page_idx": 14}, {"type": "text", "text": "K Limitations 22 ", "page_idx": 14}, {"type": "text", "text": "A CUDA to $\\mathbf{C++}$ Translation Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "CodeRosetta is capable of bidirectional translation between languages. Once trained for $C++$ to CUDA translation, it can also translate CUDA back to $C++$ , unlike previous approaches such as BabelTower [46]. In this section, we compare CodeRosetta with GPT4 and Gemini on the task of translating CUDA back to $C++$ . Table 5 summarizes the results. As shown, CodeRosetta demonstrates higher accuracy in translating CUDA to $C++$ . Moreover, we observed that Gemini struggles to clearly distinguish between CUDA and $C++$ , frequently generating $C++$ translations that are nearly identical to the original CUDA input. ", "page_idx": 14}, {"type": "text", "text": "Table 5: CUDA to $C++$ translation results across different models. We use a similar prompt as the one in Figure 15 with small adjustments. ", "page_idx": 14}, {"type": "table", "img_path": "V6hrg4O9gg/tmp/1596220d02b2089acbfce68a0bce00c0d61ff116891f2843a4f81973354894b2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Functional Correctness Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The metrics and results shown in Table 1 may have limitations in capturing functional equivalence, as discussed by Evtikhiev et al. [14]. To address this, we evaluated the functional correctness of the translated code by compiling and executing the generated programs. For the ${\\mathrm{C}}\\mathbf{+}\\mathbf{+}\\to{\\mathrm{CUDA}}$ translation task, we randomly selected 30 generated CUDA kernels and developed a template program for their execution. We then compared the runtime results of the translated CUDA code against the reference implementations. Our findings indicate that $93\\%$ of the translated CUDA code produced results consistent with the reference. ", "page_idx": 15}, {"type": "text", "text": "We analyzed three representative cases of CUDA translation in detail. In the first case, shown in Figure 6, the kernel is designed to be launched with a grid of thread blocks. Each thread calculates its global index i, and if i is within the array\u2019s bounds $\\bf(i\\mathrm{~<~N)~}$ , it assigns the value ALPHA to the element at index i \\* INCX in the array X. CodeRosetta successfully identified the optimal 2D grid structure with (blockIdx.x $^+$ blockIdx.y \\* gridDim.x) \\* blockDim.x $^+$ threadIdx.x, whereas other models defaulted to a less efifcient 1D structure using blockIdx.x \\* blockDim.x $^+$ threadIdx.x. This choice of grid structure significantly impacts CUDA performance, and CodeRosetta\u2019s selection mirrors that of the baseline implementation. Furthermore, CodeRosetta employed the correct grid structure in four additional instances where other models did not. ", "page_idx": 15}, {"type": "text", "text": "The second case, illustrated in Figure 7, involves a kernel designed to initialize an array of offsets for sorting purposes. Each offset corresponds to the starting position of a column in a flattened 2D grid. This is often useful for parallel sorting algorithms or other operations requiring column-wise processing. The expression int tid $=$ threadIdx.x $^+$ blockIdx.x \\* blockDim.x; assigns each thread a unique index across the entire grid of blocks, enabling access to distinct elements in a global array. In contrast, the expression int tid $=$ threadIdx.x; provides an index that is only unique within a single block. Without proper offset calculations, threads across different blocks could access the same data, potentially leading to race conditions and negating the kernel\u2019s intended behavior. This issue was observed in several examples where Gemini-Ultra produced incorrect results due to this oversight. ", "page_idx": 15}, {"type": "text", "text": "The third case, depicted in Figure 8, processes 3D arrays in parallel. Each thread calculates its 3D position, checks bounds, and updates specific elements of the array vec based on values from vec1. The kernel averages and scales values from vec1, storing the results in vec while ensuring safe memory access within the array\u2019s limits. CodeRosetta correctly handled large block and grid dimensions by using unsigned long, whereas both GPT-4 and Gemini-Ultra failed due to the use of int, leading to index overflow. ", "page_idx": 15}, {"type": "text", "text": "We also analyzed Fortran to $C++$ translations, shown in Figure 9. The translated code snippets maintained functional equivalence, specifically in the synchronization of shared variables between threads. OpenMP, used in the Fortran code, relies on directives such as #pragma omp critical, #pragma omp flush, and #pragma omp atomic to ensure synchronization and memory visibility. In the $C++$ translation, equivalent functionality was achieved using std::mutex, std::atomic, and std::atomic_thread_fence. Both approaches ensure that ${\\tt x}$ is updated and visible to the second thread before it prints its value, effectively synchronizing the thread actions. CodeRosetta, Gemini-Pro, and Gemini-Ultra correctly recognized the use of OpenMP in the original code, while GPT-4 did not and opted for a different approach. This highlights the limitations of metrics such as BLEU, which focus on syntax rather than functionality. Despite functional equivalence, GPT-4\u2019s translation would score lower due to its syntactic divergence. This underscores the necessity of human evaluation to ensure code correctness, as no single automated metric can fully capture functional behavior. ", "page_idx": 15}, {"type": "text", "text": "C Decontamination Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The $C++$ to CUDA dataset was obtained from BabelTower [46], which has gone through deduplication and cleaning. Notably, there is no paired trained data available within the dataset, meaning the model does not encounter $C++$ code alongside the CUDA equivalent during training. As such, the model must rely solely on self-supervised training objectives to learn to embed source code from different ", "page_idx": 15}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/1f4886fee94917dab33d8093c86a4a74aeb59dda25904c15b4266db821b788ba.jpg", "img_caption": ["(a) Reference CUDA kernel. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/9910c066667e8fb6a30526e0fd4430f9841d97c5c8c611fa1d4e1e58552fc24a.jpg", "img_caption": ["(b) CodeRosetta generated CUDA. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/f5491091d68899d266fbaea01cf0260efd87aa72b3b9c6d8a3403f4b2e81e3fd.jpg", "img_caption": ["(c) GPT4 generated CUDA. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/6f009b522a61d1f3206963e46947e64e56346c4fc3ea462a9ae0c07efa971f68.jpg", "img_caption": ["(d) Gemini Ultra generated CUDA. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 6: CUDA kernel function to fill an array $\\mathtt{X}$ with a constant value ALPHA, parallelized across threads. ", "page_idx": 16}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/8d519b6fdabbc3fc9a1a1ae9e4c560283f4589ba6426f77e0447c261605c25f6.jpg", "img_caption": ["(a) Reference CUDA kernel. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/21dcc727137c6f5df21a711b0f5e3585fb0049dbc6b3bdbdb5b564473bee4a96.jpg", "img_caption": ["(b) CodeRosetta generated CUDA "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/359ce771936757f84677fde9ee2cfcb6ab42d24b2c37b409c9b3386273b1c530.jpg", "img_caption": ["(c) GPT4 generated CUDA. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/abe167ed08607c779388a15491b1498e0b2800f5c455288c78db611d4bfe9bbe.jpg", "img_caption": ["(d) Gemini Ultra generated CUDA. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 7: CUDA kernel to set sorting offsets based on row and column indices, where each thread computes an offset value for a given column index. ", "page_idx": 16}, {"type": "text", "text": "languages into a shared embedding space. Paired data is available only in the test set, which we used for evaluating the model\u2019s performance. ", "page_idx": 16}, {"type": "text", "text": "To assess the potential overlap between the test and the training data from BabelTower, we used CodeBERTScore [51] to measure similarity. ", "page_idx": 16}, {"type": "table", "img_path": "V6hrg4O9gg/tmp/467f5db8683a372ca097018e4ca98e7c1a7203e7a341971c01e46668a3e72737.jpg", "table_caption": ["Table 6: $\\mathrm{C}\\!+\\!+\\!\\mapsto\\!\\!$ CUDA Decontamination Analysis. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 6 presents the distribution of CodeBERT scores and the corresponding amount of data in each range. For example, $48\\cdot61\\%$ of training data achieved a CodeBERTScore between 0.7 and 0.8 when compared against test data. Ranges with no data are omitted. A score below 0.8 indicates low or moderate similarity. As shown, the majority of the training samples exhibit a CodeBERTScore below 0.8, reflecting minimal similarity to the test set. A similar trend was observed when we applied this analysis to the synthetic dataset. ", "page_idx": 16}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/d3ffabe6dd6ee0407c1f2c26133808b7588387880d06fd29a0a62546483d639a.jpg", "img_caption": ["(a) Referece CUDA kernel. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/884e4f2cf2e564068cc1bf636c7ffe74242a11e7c45ac54c0e3c59270a46da7a.jpg", "img_caption": ["(b) CodeRosetta generated CUDA. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/f8edd0ca4bd731b8ea65adef4e82c3b732819fbd3ccc1a101655533a449ab837.jpg", "img_caption": ["Figure 8: CUDA kernel opL23, averaging 3D grid elements from vec1 into vec, with boundary checks. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/beaa049bb7ccf7f4981d7dd32d53e8ce96766b6d8ae73e371668e91a8e0f2e3f.jpg", "img_caption": ["(d) Gemini Ultra generated CUDA. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "D D Unsupervised Training Parameters ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Training Parameters ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For Masked Language Modeling (MLM) training, we use a learning rate of $8\\times10^{-5}$ and train for 100 epochs with $15\\%$ masking. After each epoch, we measure the perplexity on the validation set and save the model if the perplexity is the lowest. For Abstract Syntax Tree (AST) entity recognition, we use a learning rate of $5\\times\\dot{1}0^{-6}$ and train for ten epochs. We then create the encoder-decoder model by transferring the encoder\u2019s weights to initialize the decoder, so the decoder begins with some foundational knowledge. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "For Denoising Auto-Encoding and Back Translation, we use a learning rate of $5\\times10^{-5}$ and train for 20 epochs. For Denoising Auto-Encoding, we set the masking to $15\\%$ , token dropping to $25\\%$ , and token insertion to $15\\%$ , with a denoising ratio increasing by $2.5\\%$ per epoch. Finally, for fine-tuning, we use a learning rate of $5\\times10^{-5}$ for ten epochs. At each training iteration, we save the model with the lowest validation loss. All the parameter values are determined empirically through detailed hyperparameter tuning. ", "page_idx": 18}, {"type": "text", "text": "D.2 AST Entity Recognition Tags ", "text_level": 1, "page_idx": 18}, {"type": "table", "img_path": "V6hrg4O9gg/tmp/40e50b9501fc96d413fc07f4519895bb84426ca588b7e43ffc7c23280891a85e.jpg", "table_caption": ["The AER tags used in pretraining are shown in Table 7. ", "Table 7: AER Tags. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.3 Dataset Statistics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A detailed overview of the dataset is shown in Table 8. ", "page_idx": 18}, {"type": "text", "text": "E Impact of Beam Size ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We conducted beam search decoding with varying beam sizes, returning the top candidate in each case. The results, shown in Table 9, indicate that CodeRosetta consistently produces the same output, regardless of the beam size. ", "page_idx": 18}, {"type": "text", "text": "F Analysis of Generated Code from CodeRosetta and Closed-Source LLMs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "$\\mathbf{C++}\\rightarrow\\mathbf{CUDA};$ : In this part, we compare the code generated by CodeRosetta, GPT4, and GeminiUltra. As the BabelTower model and its code are not publicly available, we were unable to access them. However, the BabelTower paper highlights a kernel where the model failed to generate CUDA code due to a syntax error when defining keyCharPtr, as shown in Figure 10. In contrast, CodeRosetta successfully generates the correct CUDA code. It is interesting to note that CodeRosetta also recognized the if condition and improved the readability of the code by inverting the if statement, similar to the approach taken by Gemini-Ultra and GPT4. Additionally, CodeRosetta adheres to the preferred practice of declaring a variable or pointer before assigning a value, which is why first keyCharPtr is defined out of the if statement. ", "page_idx": 18}, {"type": "text", "text": "We demonstrate another example in Figure 11, where CodeRosetta accurately reproduces the reference CUDA kernel without adding unnecessary lines of code, such as a host or main function, which is often seen in other models. ", "page_idx": 18}, {"type": "text", "text": "Fortran $\\to\\mathbf{C}\\mathbf{+}\\mathbf{+};$ : Figures 9, 12 show examples of $C++$ code generated by CodeRosetta in comparison with other LLMs. Despite CodeRosetta\u2019s smaller size, it effectively translates Fortran code into correct $C++$ code. ", "page_idx": 18}, {"type": "text", "text": "Moreover, we also evaluated our model in terms of $\\mathrm{C}++\\rightarrow$ Fortran translation 10. The results indicate the capability of CodeRosetta in translating to and from Fortran code. ", "page_idx": 18}, {"type": "table", "img_path": "V6hrg4O9gg/tmp/b55785c813aa50815e48957960a7165e682c8da4b6cc29b6aba22cf920ae40ad.jpg", "table_caption": ["Table 8: Dataset statistics for $C++$ , CUDA, and Fortran programming languages. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "V6hrg4O9gg/tmp/89dd3add876c1aef59bd917055a145125597f7e0dc2b055ce900b880c59f0b6f.jpg", "table_caption": ["Table 9: Effect of different beam sizes on $C++$ to CUDA translation. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "V6hrg4O9gg/tmp/3aad4f5ee148eaa087979e55c12645ff3a6f452f4a3fa5a3d6e96805a58b4daf.jpg", "table_caption": ["Table 10: $C++$ to Fortran translation results in terms of CodeBLEU. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "F.1 Common Issues and Post-processing in CodeRosetta-Generated Code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Code translated by large language models like GPT-4 often includes additional caller functions that extend beyond the scope of the original function. In contrast, code translated by CodeRosetta may occasionally fail to compile despite being syntactically correct. We identified two common issues in the code generated by CodeRosetta and applied a simple post-processing method to ensure a fair comparison across models. ", "page_idx": 19}, {"type": "text", "text": "The first issue involves the use of generic types, which can enhance code efifciency but require explicit type definitions at compile time. Figure 13a shows the use of a generic type, although the necessary definition is missing. Adding the type definition, as shown in Figure 13b, resolves the compilation issue. The second issue relates to misses variable initialization in the function definition, as shown in Figure 13c. By initializing the required variable, as demonstrated in Figure 13d, the compilation problem is resolved. Lastly, for longer code snippets, CodeRosetta occasionally omits the closing curly bracket. ", "page_idx": 19}, {"type": "text", "text": "G Discussion on Unsupervised Training ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "G.1 Fine-tuning for Code Translation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In the context of code translation, paired data is scarce. However, our model benefits from a strong foundational understanding of code translation acquired through unsupervised and self-supervised pre-training on 243K training examples for $\\mathrm{C}++\\leftrightarrow$ CUDA. We demonstrate that fine-tuning, even with a small amount of synthetic data\u2014without verifying the one-to-one mapping between the generated samples and the input code in a supervised manner\u2014can further improve the model\u2019s performance. Specifically, fine-tuning with merely 5K paired samples (less than $2\\%$ of total data) generated by larger models still led to significant performance gains. While synthetic data may introduce some errors (as large models can make translation mistakes), the combination of this foundational pre-training and fine-tuning with a small synthetic dataset yields further improvements. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "G.2 Back Translation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Back Translation (BT) has been extensively used in unsupervised translation tasks for both natural language and code. We integrate this technique with the denoising auto-encoding (DAE) objective, ensuring that the model is not trained exclusively on a single objective. During training, the model alternates between DAE and BT for each batch of data. This prevents the model from relying solely on BT and \u2019cheating\u2019 by outputting the input source code as an intermediate translation. To better understand this behavior, we analyzed the intermediate outputs during back translation. ", "page_idx": 20}, {"type": "text", "text": "For instance, Figure 14 shows a $C++$ input and its corresponding intermediate CUDA translation. As shown, while the model attempts to translate the code to CUDA, the output contains errors, such as the undefined variable j. In the back translation process, this noisy CUDA code output is fed back into the model, which then attempts to reconstruct the original $C++$ input. Since the model alternates between languages during back translation, it occasionally generates noisy CUDA or $C++$ code. This approach improves the model\u2019s robustness when handling noisy inputs in translation tasks. ", "page_idx": 20}, {"type": "text", "text": "H Translation Pitfalls: Invalid Tokens in Target Language ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "During translation between programming languages (e.g., from $C++$ to CUDA), certain entities, libraries, and syntaxes present in the source language may not be valid or supported in the target language. For example, $C++$ Standard Template Libraries (STL) such as std::unique_ptr are not compatible with CUDA\u2019s device code and must be excluded from translations. The pre-training process in CodeRosetta equips the model with semantic knowledge of both source and target languages, reducing the frequency of invalid tokens during translation. Nonetheless, there are still instances where the model may fail to correctly map common source language entities to valid target language counterparts. ", "page_idx": 20}, {"type": "text", "text": "While our test set contained no occurrences of std::unique_ptr, we deliberately included this construct in a separate $C++$ code example to evaluate CodeRosetta\u2019s handling of STL-specific constructs. Figure 16 demonstrates this case, where the model successfully generates CUDA code by omitting the unsupported std::unique_ptr in the device kernel. Instead, the use of std::unique_ptr is correctly retained in the host kernel, specifically in the main function, which runs on the CPU. Since CodeRosetta is trained to focus on device function generation, the translation is accurate in this instance. ", "page_idx": 20}, {"type": "text", "text": "On the other hand, Figure 17 illustrates a case of incorrect translation, where CodeRosetta, along with other large closed-source models like GPT-4, Gemini-Ultra, and Gemini-Pro, failed to generate valid CUDA code. The translated code includes the line $\\mathbf{\\ddot{\\nabla}r h o}\\ =\\ \\boldsymbol{0}\\,;$ , which initializes the rho variable to zero. In a multi-threaded GPU environment, executing this kernel across multiple threads and blocks simultaneously can lead to a race condition, as multiple threads would attempt to write to the same memory location concurrently. Without synchronization mechanisms like atomic operations or reduction techniques, this results in unpredictable and incorrect behavior. The correct approach would be to initialize rho in the host code and use atomicAdd to accumulate values in the device code safely. ", "page_idx": 20}, {"type": "text", "text": "I Prompt Template and LLMs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we describe the prompt template used to translate between different programming languages and libraries. The template, shown in Figure 15, served as the basis for all translation tasks, with language-specific adjustments made by updating the source and target languages as required. For this study, we use OpenAI API\u2019s GPT-4 API, using a fixed temperature of zero to ensure deterministic outputs across all models, including CodeRosetta. All queries were executed on May 18th, 2024, ensuring consistency in results throughout the experiments. ", "page_idx": 20}, {"type": "text", "text": "J Additional Related Work ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Automatic parallelization. Early efforts in auto-parallelization were primarily focused on identifying independent loops that could be executed in parallel. Renowned compilers like the Portland Group (PGI) and Intel\u2019s $C++$ Compiler (ICC) have embedded auto-parallelization capabilities, offering pragma-based hints to guide the parallelization process. These compilers analyze loop dependencies, data flow, and potential side effects to generate parallel code, often targeting OpenMP or MPI for multi-threading and distributed computing, respectively. The advent of Polyhedral model-based tools marked a significant advancement in auto-parallelization techniques. The Polyhedral model [6] offers a powerful algebraic representation for optimizing loop nests with afifne bounds and access patterns. Pluto [8] is an auto-parallelization tool that utilizes the Polyhedral model to perform loop transformations, tiling, and fusion for effective parallel execution while considering data locality optimization. PPCG (Polyhedral Parallel Code Generation) [44] is another tool that exploits the polyhedral model to automatically optimize and generate parallel code from high-level abstractions, targeting multicore CPUs and GPUs. ", "page_idx": 21}, {"type": "text", "text": "Neural machine translation. TransCoder-ST [37] extends the original work [36] by adding automated unit testing. TransCoder-IR [39] extends it even further by exploiting LLVM IR for program translation. HPC-GPT [13] uses GPT4 to create an instruction-answer dataset for two tasks (AI models and datasets for HPC and data race detection), then Llama model [43] is supervised tuned on this dataset. Pan et al. [31] provided one of the first studies on the types of errors that are often produced in code translation. ", "page_idx": 21}, {"type": "text", "text": "There is a growing number of large language models (LLMs) for code generation [5, 35, 40, 50, 26, 4, 29, 3]. Most of these works focus mainly on natural language to code generation. Although these Code LLMs can generate code in various programming languages, Python, in particular, has received more attention compared to others. This could be due to the number of available benchmarks that assess Python coding capabilities [10, 23], though other programming languages have been gaining more attention recently as well [9]. Despite the growing number of Code LLMs, these models are typically not specifically trained for code translation, even though they can perform code translation to some extent, as shown by Pan et al. [31]. ", "page_idx": 21}, {"type": "text", "text": "K Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "While CodeRosetta demonstrates promising results in code translation, several avenues for future work exist. Currently, CodeRosetta targets $\\mathrm{C}++\\rightarrow$ CUDA and Fortran $\\rightarrow\\mathbf{C}++$ translation. Extending its capabilities to encompass a wider range of HPC languages and parallel programming models would further broaden its scope. In addition, we plan to improve the set of entity categories used in AER to capture a better representation of code semantics. This will involve incorporating additional tags for constructs like data types, control flow mechanisms, and parallel programming-specific primitives. ", "page_idx": 21}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/42a431cf7793259169b70d16f9f29935aa155f41a408e599fca2dfe396c5a5e5.jpg", "img_caption": ["(c) GPT4 generated $C++$ . ", "(d) Gemini Ultra generated $C++$ "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 9: A $C++$ OpenMP example with thread sync using atomic operations and critical sections. ", "page_idx": 22}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/d4ac56322ebcbb104c532d31e63c82ac42657031a51c1ca4195118b812c100d8.jpg", "img_caption": ["(a) $C++$ code. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/203ed8d89569aa344e7ab225cfec9b54e95f4c9002198bd152c7d0f0f9a350eb.jpg", "img_caption": ["(c) BabelTower. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/8e626d6bb66c84662a6cdd459a2284afe9290b2a731132f2321d7c4aaedc17a7.jpg", "img_caption": ["(e) Gemini Ultra. ", "Figure 10: Comparison of the generated kernelXor CUDA kernel. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/217fcc446191a89b7d6bf81dfe1f9fabea72660dd936730f7717a8ec8356859e.jpg", "img_caption": ["(b) CUDA reference. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/a94187b684f8e298b9d855ef0bf5f850ef66025bb29243867220b83dc2f6e5ce.jpg", "img_caption": ["(d) CodeRosetta. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/d625f3307dd543a88ffcb9be3035e97e3231c47794dc75c9beec22c63820a55e.jpg", "img_caption": ["(f) GPT4. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/451f1d44f5138f19cd60b2b40a4f8925038dac9ff62579d8ca48d041b88e4627.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "(e) Gemini Ultra. ", "page_idx": 24}, {"type": "text", "text": "Figure 11: Power of elements CUDA kernel. ", "page_idx": 24}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/bbc8e368223ffad3beeeecb02013523a323df062a66b3ccf543935f115511eea.jpg", "img_caption": ["(e) Gemini Ultra. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 12: Element-wise addition of two multi-dimensional arrays. ", "page_idx": 25}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/5b700b672e43a84df4ea454a3304cdc3a97d5d1ddd23bc737225a6ed15ccb2fe.jpg", "img_caption": ["(a) Generated CUDA code. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/1e5d930942e7b436508b87683bf786ed48e4a9940eea9c90a2fa7ea2cd154a88.jpg", "img_caption": ["(c) Generated CUDA code "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/cda8c1dc850ebab00841b5f5178682ce28ae508e703197ef34986a6f76696be0.jpg", "img_caption": ["(b) Corrected CUDA code. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/3b6517427a9c8eaa042eccbc2ac447b026d9f8b1551dc058e34664d4c822ef17.jpg", "img_caption": ["(d) Corrected CUDA code "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 13: Post Compilation fixes on CUDA kernel. ", "page_idx": 26}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/6ebae9dc25c46b598763d85a8d934a35292810954ca6711de94cc80a22735d4b.jpg", "img_caption": ["(a) Input $C++$ code. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/5fa5c075158d26c2456ca4ab3954a7d233618e675ddbcfd76c61fb2bfb3f57b7.jpg", "img_caption": ["Figure 14: Back translation intermediate results. ", "(b) Intermediate CUDA generated code. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/b6f1f18bf0b08296a9fd585c01a61fb6f4b7dc324fe5473384be2429477d5163.jpg", "img_caption": ["Figure 15: Prompt for translating $C++$ to CUDA. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/6cb971cbbd27f106e2ebf168a7dddfdb170ca1bb210b15ca300a3535240ed5e1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "(a) Example of $C++$ code with std::unique_ptr ", "page_idx": 27}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/4dc4ab9bb316437071d5b9998c4123524a4299c4e5a3801bd674832ceb3876c1.jpg", "img_caption": ["(b) Translated CUDA code. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 16: Example of translation of a $C++$ code with std::unique_ptr ", "page_idx": 27}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/cc886ee9f92744fe115dbd4ef59bd3b691bc3044295d631055355a3d06c765f5.jpg", "img_caption": ["(a) $C++$ Code "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "V6hrg4O9gg/tmp/16d54897eff2d81a7c4b8aa0c5058ee0291e8769fb7a637a9538316ad1a6eba0.jpg", "img_caption": ["(b) Wrong translated CUDA code. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 17: Example of a failed $C++$ to CUDA translation. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide a bidirectional encoder-decoder transformer model that is capable of learning to translate programming languages as well as their HPC extensions. Results indicated ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The limitations of our study are addressed in Appendix(Section K) . First, our research does not extend CodeRosetta to a broader range of applications. Second, we have not incorporated several potentially beneficial AER tags. Despite these limitations, our methodology has demonstrated significant effectiveness in two critical tasks in the HPC community: translating $C++$ to Fortran and $C++$ to CUDA. We plan to address these limitations in future research efforts. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efifciency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not include theoretical results. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We submitted the code as supplemental material and also created an anonymous repository ( https://anonymous.4open.science/r/neurips_coderosetta-CAB2/) to include everything needed to reproduce the experiments. Please refer to the README file in the repository for detailed steps. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might sufifce, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 29}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufifcient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We submitted the code as supplemental material and also created an anonymous repository ( https://anonymous.4open.science/r/neurips_coderosetta-CAB2/) to include both data and code. Please refer to the README flie in the repository for details. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Section 4 discussed experimental settings/details such as data preprocessing, data splits, hyper-parameters, etc. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our work does not include statistical experiments due to limited computational resources.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufifcient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?[Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We discussed the information on the computer resources at the beginning of Section 4. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The research conducted in our paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efifciency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper poses no such risks Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The creators of the data and models used in this work have been properly credited. The license and terms of use are explicitly mentioned and properly respected. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our work does not release new assets. We use existing, publicly available resources for our analysis. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]