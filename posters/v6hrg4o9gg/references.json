{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-03-08", "reason": "This paper is a technical report describing GPT-4, a large language model that serves as a strong baseline and comparison point for the capabilities of CODEROSETTA in code translation tasks."}, {"fullname_first_author": "Wasi Ahmad", "paper_title": "Unified Pre-training for Program Understanding and Generation", "publication_date": "2021-00-00", "reason": "This paper introduces a unified pre-training approach for program understanding and generation, which directly relates to CODEROSETTA's pre-training and training objectives for unsupervised code translation."}, {"fullname_first_author": "Ebtesam Almazrouei", "paper_title": "The Falcon Series of Open Language Models", "publication_date": "2023-11-16", "reason": "This paper presents the Falcon series of open-source language models which directly compare with closed source models like GPT4 in terms of performance and hence serves as a key comparison point for evaluating CODEROSETTA"}, {"fullname_first_author": "Jinze Bai", "paper_title": "Qwen Technical Report", "publication_date": "2023-09-16", "reason": "This paper introduces Qwen, another large language model, providing another key comparison point for evaluating CODEROSETTA against state-of-the-art models."}, {"fullname_first_author": "Mohamed-Walid Benabderrahmane", "paper_title": "The Polyhedral Model Is More Widely Applicable Than You Think", "publication_date": "2010-00-00", "reason": "This paper discusses the polyhedral model, a technique relevant to CODEROSETTA's approach to handling parallel programming syntax and nuances in code translation."}]}