[{"type": "text", "text": "Computation-Aware Gaussian Processes: Model Selection And Linear-Time Inference ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jonathan Wenger1 Kaiwen Wu2 Philipp Hennig3 Jacob R. Gardner2 Geoff Pleiss4 John P. Cunningham1 ", "page_idx": 0}, {"type": "text", "text": "1 Columbia University 2 University of Pennsylvania   \n3 University of Tu\u00a8bingen, Tu\u00a8bingen AI Center   \n4 University of British Columbia, Vector Institute ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Model selection in Gaussian processes scales prohibitively with the size of the training dataset, both in time and memory. While many approximations exist, all incur inevitable approximation error. Recent work accounts for this error in the form of computational uncertainty, which enables\u2014at the cost of quadratic complexity\u2014an explicit tradeoff between computation and precision. Here we extend this development to model selection, which requires significant enhancements to the existing approach, including linear-time scaling in the size of the dataset. We propose a novel training loss for hyperparameter optimization and demonstrate empirically that the resulting method can outperform SGPR, CGGP and SVGP, state-of-the-art methods for GP model selection, on medium to largescale datasets. Our experiments show that model selection for computation-aware GPs trained on 1.8 million data points can be done within a few hours on a single GPU. As a result of this work, Gaussian processes can be trained on large-scale datasets without significantly compromising their ability to quantify uncertainty\u2014 a fundamental prerequisite for optimal decision-making. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gaussian Processes (GPs) remain a popular probabilistic model class, despite the challenges in scaling them to large datasets. Since both computational and memory resources are limited in practice, approximations are necessary for both inference and model selection. Among the many approximation methods, perhaps the most common approach is to map the data to a lower-dimensional representation. The resulting posterior approximations typically have a functional form similar to the exact GP posterior, except where posterior mean and covariance feature low-rank updates. This strategy can be explicit\u2014by either defining feature functions (e.g. Nystro\u00a8m [1], RFF [2])\u2014or a lower-dimensional latent inducing point space (e.g. SoR, DTC, FITC [3], SGPR [4], SVGP [5]), or implicit\u2014by using an iterative numerical method (e.g. CGGP [6\u201310]). All of these methods then compute coefficients for this lower-dimensional representation from the full set of observations by direct projection (e.g. CGGP) or via an optimization objective (e.g. SGPR, SVGP). ", "page_idx": 0}, {"type": "text", "text": "While effective and widely used in practice, the inevitable approximation error adversely impacts predictions, uncertainty quantification, and ultimately downstream decision-making. Many proposed methods come with theoretical error bounds [e.g. 2, 11\u201314], offering insights into the scaling and asymptotic properties of each method. However, theoretical bounds often require too many assumptions about the data-generating process to offer \u201creal-world\u201d guarantees [15], and in practice, the fidelity of the approximation is ultimately determined by the available computational resources. ", "page_idx": 0}, {"type": "image", "img_path": "tDvFa5OJyS/tmp/70d333004b5d522275a65346d8434aee5d2c183fcea77a56e2347f904b9bc636.jpg", "img_caption": ["GP mean GP uncertainty (latent $^+$ noise) Training data Posterior for Data-Generating Hyperparameters "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Comparison of an exact GP posterior (CholeskyGP) and three scalable approximations: SVGP, CaGP-CG and CaGP-Opt (ours). Hyperparameters for each model were optimized using model selection strategies specific to each approximation. The posterior predictive given the datagenerating hyperparameters is denoted by gray lines and for each method the posterior (dark-shaded) and the posterior predictive are shown (light-shaded). While all methods, including the exact GP, do not recover the data-generating process, CaGP-CG and CaGP-Opt are much closer than SVGP. SVGP expresses almost no posterior variance near the inducing point in the data-sparse region and thus almost all deviation from the posterior mean is considered to be observational noise. In contrast, CaGP-CG and CaGP-Opt express significant posterior variance in regions with no data. ", "page_idx": 1}, {"type": "text", "text": "One central pathology is overconfidence, which has been shown to be detrimental in key applications of GPs such as Bayesian optimization [e.g. variance starvation of RFF, 16], and manifests itself even in state-of-the-art variational methods like SVGP. SVGP, because it treats inducing variables as \u201cvirtual observations\u201d, can be overconfident at the locations of the inducing points if they are not in close proximity to training data, which becomes increasingly likely in higher dimensions. This phenomenon can be seen in a toy example in Figure 1, where SVGP has near zero posterior variance at the inducing point away from the data. See also Section S5.1 for a more detailed analysis. ", "page_idx": 1}, {"type": "text", "text": "These approximation errors are a central issue in inference, but they are exacerbated in model selection, where errors compound and result in biased selections of hyperparameters [12, 17, 18]. Continuing the example, SVGP has been observed to overestimate the observation noise [18], which can lead to oversmoothing. This issue can also be seen in Figure 1, where the SVGP model produces a smoother posterior mean than the exact (Cholesky)GP and attributes most variation from the posterior mean to observational noise (see also Figure S3(b)). There have been efforts to understand these biases [18] and to mitigate the impact of approximation error on model selection for certain approximations [e.g. CGGP, 12], but overcoming these issues for SVGP remains a challenge. ", "page_idx": 1}, {"type": "text", "text": "Recently, Wenger et al. [19] introduced computation-aware Gaussian processes (CaGP), a class of GP approximation methods which\u2014for a fixed set of hyperparameters\u2014provably does not suffer from overconfidence. Like SVGP and the other approximations mentioned above, CaGP also relies on low-rank posterior updates. Unlike these other methods, however, CaGP\u2019s posterior updates are constructed to guarantee that its posterior variance is always larger than the exact GP variance. This conservative estimate can be interpreted as additional uncertainty quantifying the approximation error due to limited computation; i.e. computational uncertainty. However, so far CaGP has fallen short in demonstrating wallclock time improvements for posterior inference over variational methods and model selection has so far remained an open problem. ", "page_idx": 1}, {"type": "text", "text": "Contributions In this work, we extend computation-aware Gaussian processes by demonstrating how to perform inference in linear time in the number of training data, while maintaining its theoretical guarantees. Second, we propose a novel objective that allows model selection without a significant bias that would arise from naively conducting model selection on the projected GP. In detail, we enforce a sparsity constraint on the \u201cactions\u201d of the method, which unlocks linear-time inference, in a way that is amenable to hardware acceleration. We optimize these actions end-to-end alongside the hyperparameters with respect to a custom training loss, to optimally retain as much information from the data as possible given a limited computational budget. The resulting hyperparameters are less prone to oversmoothing and attributing variation to observational noise, as can be seen in Figure 1, when compared to SVGP. We demonstrate that our approach is strongly competitive on large-scale data with state-of-the-art variational methods, such as SVGP, without inheriting their pathologies. As a consequence of our work, one can train GPs on up to 1.8 million data points in a few hours on a single GPU without adversely impacting uncertainty quantification. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We aim to learn a latent function mapping from $\\mathbb{X}\\subseteq\\mathbb{R}^{d}$ to $\\mathbb{Y}\\subseteq\\mathbb{R}$ given a training dataset $\\pmb{X}=$ $(\\pmb{x}_{1},\\dots,\\pmb{x}_{n})\\in\\mathbb{R}^{n\\times d}$ of $n$ inputs $\\mathbf{\\boldsymbol{x}}_{j}\\in\\mathbb{R}^{d}$ and corresponding targets $\\pmb{y}=\\left(y_{1},\\ldots,y_{n}\\right)^{\\mathsf{T}}\\in\\mathbb{R}^{n}$ . ", "page_idx": 2}, {"type": "text", "text": "Gaussian Processes A Gaussian process $f\\sim\\mathcal{G P}(\\mu,K_{\\theta})$ is a stochastic process with mean function $\\mu$ and kernel $K_{\\theta}$ such that ${\\pmb f}=f({\\pmb X})=(f({\\pmb x}_{1}),\\dots,f({\\pmb x}_{n}))^{\\top}\\sim{\\mathcal{N}}({\\pmb\\mu},{\\pmb K}_{\\pmb\\theta})$ is jointly Gaussian with mean ${\\pmb{\\mu}}_{i}=\\mu({\\pmb{x}}_{i})$ and covariance $K_{i j}=K_{\\theta}(x_{i},{\\pmb x}_{j})$ . The kernel $K_{\\theta}$ depends on hyperparameters $\\pmb\\theta\\in\\mathbb{R}^{p}$ , which we omit in our notation. Assuming ${\\pmb y}\\mid f({\\pmb X})\\sim{\\mathcal{N}}{\\big(}f({\\pmb X}),\\sigma^{2}{\\pmb I}{\\big)}$ , the posterior is a Gaussian process $\\mathcal{G P}(\\mu_{\\star},K_{\\star})$ where the mean and covariance functions evaluated at a test input $\\mathbf{\\boldsymbol{x}}_{\\diamond}\\in\\mathbb{R}^{d}$ are given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mu_{\\star}(f(x_{\\diamond}))=\\mu(x_{\\diamond})+K(x_{\\diamond},X)v_{\\star},}}\\\\ {{K_{\\star}(f(x_{\\diamond}),f(x_{\\diamond}))=K(x_{\\diamond},x_{\\diamond})-K(x_{\\diamond},X)\\hat{K}^{-1}K(X,x_{\\diamond}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\hat{\\pmb{K}}=\\pmb{K}+\\sigma^{2}\\pmb{I}$ and the representer weights are defined as $\\pmb{v}_{\\star}=\\hat{\\pmb{K}}^{-1}(\\pmb{y}-\\pmb{\\mu})$ . ", "page_idx": 2}, {"type": "text", "text": "In model selection, the computational bottleneck when optimizing kernel hyperparameters $\\pmb{\\theta}$ is the repeated evaluation of the negative log-marginal likelihood ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\ell^{\\mathrm{NLL}}(\\pmb\\theta)=-\\log p(\\pmb y\\mid\\pmb\\theta)=\\frac{1}{2}\\big(\\underbrace{(y-\\pmb\\mu)^{\\top}\\hat{K}^{-1}(y-\\pmb\\mu)}_{\\mathrm{quadratic~loss:~promotes~data~fit}}+\\underbrace{\\log\\operatorname*{det}(\\hat{K})}_{\\mathrm{model~complexity}}+n\\log(2\\pi)\\big)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and its gradient. Computing (2) and its gradient via a Cholesky decomposition has time complexity ${\\mathcal{O}}(n^{3})$ and requires $\\bar{O}(n^{2})$ memory, which is prohibitive for large $n$ . ", "page_idx": 2}, {"type": "text", "text": "Sparse Gaussian Process Regression (SGPR) [4] Given a set of $m~\\ll~n$ inducing points $\\bar{Z}\\,=\\,(z_{1},\\ldots,z_{m})^{\\mathsf{T}}$ and defining $\\pmb{u}\\;:=\\;f(\\pmb{Z})\\;=\\;(f(z_{1}),\\ldots,f(z_{m}))^{\\top}$ , SGPR defines a variational approximation to the posterior through the factorization $\\begin{array}{r}{p_{\\star}(\\dot{f}(\\cdot)\\ensuremath{\\;|\\;}\\ensuremath{\\dot{\\boldsymbol{y}}})\\approx q(f(\\cdot))=\\int p(f(\\cdot)\\ensuremath{\\;|\\;}}\\end{array}$ $\\pmb{u})$ $q(\\boldsymbol{u})d\\boldsymbol{u}$ , where $q(\\boldsymbol{u})$ is an $m$ -dimensional multivariate Gaussian. The mean and covariance of $q(u)$ (denoted as $m:=\\mathbb{E}_{q}(\\pmb{u})$ , $\\Sigma:=\\operatorname{Cov}_{q}(\\mathbf{\\boldsymbol{u}}))$ are jointly optimized alongside the kernel hyperparameters $\\pmb{\\theta}$ using the evidence lower bound (ELBO) as an objective: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m,\\boldsymbol{\\Sigma},\\boldsymbol{\\theta},\\boldsymbol{Z}=\\arg\\operatorname*{min}_{m,\\boldsymbol{\\Sigma},\\boldsymbol{\\theta},\\boldsymbol{Z}}\\ell^{\\mathrm{ELBO}},}\\\\ {\\ell^{\\mathrm{ELBO}}(m,\\boldsymbol{\\Sigma},\\boldsymbol{\\theta},\\boldsymbol{Z}):=\\ell^{\\mathrm{NLL}}(\\boldsymbol{\\theta})+\\mathrm{KL}(q(f)\\parallel p_{*}(f\\mid\\boldsymbol{y},\\boldsymbol{\\theta}))}\\\\ {=-\\,\\mathbb{E}_{q(f)}(\\log p(\\boldsymbol{y}\\mid f))+\\mathrm{KL}(q(\\boldsymbol{u})\\parallel p(\\boldsymbol{u}))\\ge-\\log p(\\boldsymbol{y}\\mid\\boldsymbol{\\theta}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The inducing point locations $Z$ can be either optimized as additional parameters during training or chosen a-priori, typically in a data-dependent way [see e.g. Sec. 7.2 of 20]. Following Titsias [4], ELBO optimization and posterior inference both require $\\mathcal{O}\\bar{(}n m^{2})$ computation and ${\\mathcal{O}}(n m)$ memory, a significant improvement over the costs of exact GPs. ", "page_idx": 2}, {"type": "text", "text": "Stochastic Variational Gaussian Processes (SVGP) [5] SVGP extends SGPR to reduce complexity further to $\\mathcal{O}(m^{3})$ computation and $\\mathcal{O}(m^{2})$ memory. It accomplishes this reduction by replacing the first term in Equation (4) with an unbiased approximation ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{q(f)}(\\log p(\\pmb{y}\\mid f))=\\mathbb{E}_{q(f)}(\\sum_{i=1}^{n}\\log p(y_{i}\\mid f(\\pmb{x}_{i})))\\approx n\\,\\mathbb{E}_{q(f(\\pmb{x}_{i}))}(\\log p(y_{i}\\mid f(\\pmb{x}_{i})))\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Following Hensman et al. [5], we optimize $m,\\Sigma$ alongside $\\theta,Z$ through joint gradient updates. Because the asymptotic complexities no longer depend on $n$ , SVGP can scale to extremely large datasets that would not be able to fit into computer/GPU memory. ", "page_idx": 2}, {"type": "text", "text": "Computation-aware Gaussian Process Inference $(\\mathbf{CaGP})$ [19] $\\mathrm{CaGP^{1}}$ maps the data $\\textit{\\textbf{y}}$ into a lower dimensional subspace defined by its actions $S_{i}\\in\\mathbb{R}^{n\\times i}$ on the data, which defines an approximate posterior ${\\mathcal{G P}}(\\mu_{i},K_{i})$ with ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mu_{i}({\\pmb x}_{\\diamond})=\\mu({\\pmb x}_{\\diamond})+K({\\pmb x}_{\\diamond},{\\pmb X})v_{i}}}\\\\ {{K_{i}({\\pmb x}_{\\diamond},{\\pmb x}_{\\diamond})=K({\\pmb x}_{\\diamond},{\\pmb x}_{\\diamond})-K({\\pmb x}_{\\diamond},{\\pmb X})C_{i}K({\\pmb X},{\\pmb x}_{\\diamond}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $C_{i}\\,=\\,S_{i}(\\pmb{S}_{i}^{\\top}\\hat{\\pmb{K}}\\pmb{S}_{i})^{-1}\\pmb{S}_{i}^{\\top}\\approx\\pmb{K}^{-1}$ is a low-rank approximation of the precision matrix and $\\mathbf{\\boldsymbol{v}}_{i}=C_{i}(\\mathbf{\\boldsymbol{y}}-\\mathbf{\\boldsymbol{\\mu}})\\approx\\mathbf{\\boldsymbol{v}}_{\\star}$ approximates the representer weights. Both converge to the corresponding exact quantity as the number of iterations, equivalently the downdate rank, $i\\,\\rightarrow\\,n$ . Note that the CaGP posterior only depends on the space spanned by the columns of $S_{i}$ , not the actual matrix (Lemma S4). Finally, the CaGP posterior can be computed in $O(n^{2}i)$ time and $\\mathcal{O}(n i)$ memory. ", "page_idx": 3}, {"type": "text", "text": "CaGP captures the approximation error incurred by limited computational resources as additional uncertainty about the latent function. More precisely, for any data-generating function $y\\in\\mathbb{H}_{K^{\\sigma}}$ in the RKHS $\\mathbb{H}_{K^{\\sigma}}$ defined by the kernel, the worst-case squared error of the corresponding approximate posterior mean $\\mu_{i}^{y}$ is equal to the approximate predictive variance (see Theorem S2): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{y\\in\\mathbb{H}_{K^{\\sigma}},\\|y\\|_{\\mathbb{H}_{K^{\\sigma}}}\\leq1}\\left(y(x_{\\diamond})-\\mu_{i}^{y}(x_{\\diamond})\\right)^{2}=K_{i}(\\pmb{x}_{\\diamond},\\pmb{x}_{\\diamond})+\\sigma^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This guarantee is identical to one for the exact GP posterior mean and variance (see Theorem S1), except with the approximate quantities instead.2 Additionally, it holds that CaGP\u2019s marginal (predictive) variance is always larger or equal to the (predictive) variance of the exact GP and monotonically decreasing, i.e. $K_{i}(\\pmb{x}_{\\diamond},\\pmb{x}_{\\diamond})\\geq K_{j}(\\pmb{x}_{\\diamond},\\pmb{x}_{\\diamond})\\geq\\overline{{K}}_{n}(\\pmb{x}_{\\diamond},\\pmb{x}_{\\diamond})=K_{\\star}(\\pmb{x}_{\\diamond},\\pmb{x}_{\\diamond})$ for $i\\le j\\le n$ (see Proposition S1). Therefore, given fixed hyperparameters, CaGP is guaranteed to never be overconfident and as the computational budget increases, the precision of its estimate increases. Here we call such a posterior computation-aware, and we will extend the use of this object to model selection. ", "page_idx": 3}, {"type": "text", "text": "3 Model Selection in Computation-Aware Gaussian Processes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Model selection for GPs most commonly entails maximizing the evidence $\\log p(\\boldsymbol{y}\\mid\\boldsymbol{\\theta})$ as a function of the kernel hyperparameters $\\pmb\\theta\\in\\mathbb{R}^{p}$ . As with posterior inference, evaluating this objective and its gradient is computationally prohibitive in the large-scale setting. Therefore, our central goal will be to perform model selection for computation-aware Gaussian processes in order to scale to a large number of training data while avoiding the introduction of pathologies via approximation. ", "page_idx": 3}, {"type": "text", "text": "We begin by viewing the computation-aware posterior as exact inference assuming we can only observe $i$ linear projections $\\tilde{\\pmb{y}}$ of the data defined by (linearly independent) actions $\\bar{S_{i}}\\in\\mathbb{R}^{n\\times i}$ , i.e. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widetilde{\\pmb{y}}:=\\pmb{S}_{i}^{\\prime\\top}\\pmb{y}\\in\\mathbb{R}^{i},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{i}^{\\prime}=S_{i}\\operatorname{chol}({S_{i}}^{\\top}S_{i})^{-\\top}\\in\\mathbb{R}^{n\\times i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "is the action matrix with orthonormalized columns. The corresponding likelihood is given by ", "page_idx": 3}, {"type": "text", "text": "As we show in Lemma S1, the resulting Bayesian posterior is then given by Equation (5). Recall that the CaGP posterior only depends on the column space of the actions $S_{i}$ (see Lemma S4), which is why Equation (5) can be written in terms of $S_{i}$ directly rather than using $S_{i}^{\\prime}$ .3 ", "page_idx": 3}, {"type": "text", "text": "Projected-Data Log Marginal Likelihood The reinterpretation of the computation-aware posterior as exact Bayesian inference immediately suggests using evidence maximization for the projected data $\\tilde{\\pmb{y}}=\\pmb{S}_{i}^{\\prime}\\overset{\\top}{\\pmb{y}}\\in\\mathbb{R}^{i}$ as the model selection criterion, leading to the following loss (see Lemma S2): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{\\mathrm{proj}}^{\\mathrm{NL}}(\\boldsymbol{\\theta})=-\\log p(\\boldsymbol{\\tilde{y}}\\mid\\boldsymbol{\\theta})=-\\log\\mathcal{N}\\Big(\\Tilde{y};S_{i}^{\\prime\\top}\\boldsymbol{\\mu},S_{i}^{\\prime\\top}\\hat{\\boldsymbol{K}}S_{i}^{\\prime}\\Big)\\qquad\\underset{\\mathrm{project}}{\\mathrm{penalize}}\\mathrm{s}_{\\mathrm{near-colinear}}\\mathrm{actions}}\\\\ &{=\\frac{1}{2}\\big(\\underbrace{(\\boldsymbol{y}-\\boldsymbol{\\mu})^{\\mathrm{T}}S_{i}(S_{i}^{\\top}\\hat{\\boldsymbol{K}}S_{i})^{-1}S_{i}^{\\top}(\\boldsymbol{y}-\\boldsymbol{\\mu})}_{\\mathrm{quadraitic~loss:~promotes}\\;\\mathrm{fiting}\\;\\mathrm{projected}\\;\\mathrm{dat}\\;\\hat{\\boldsymbol{y}}}\\bigg)\\underbrace{\\mathrm{log}\\;\\mathrm{det}(S_{i}^{\\top}\\hat{\\boldsymbol{K}}S_{i})-\\log\\operatorname*{det}(S_{i}^{\\top}S_{i})}_{\\mathrm{projected}\\;\\mathrm{model~complexity}}+i\\log(2\\pi)\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Equation (9) involves a Gaussian random variable of dimension $i\\ll n$ , and so all previously intractable quantities in Equation (2) (i.e. the inverse and determinant) are now cheap to compute. Analogous to the $\\mathrm{CaGP}$ posterior, we can express the projected-data log marginal likelihood fully in terms of the actions $S_{i}$ without having to orthonormalize, which results in an additional term penalizing colinearity. Unfortunately, this training loss does not lead to good generalization performance, as there is only training signal in the $i$ -dimensional space spanned by the actions $S_{i}$ the data are projected onto. Specifically, the quadratic loss term only promotes fitting the projected data $\\tilde{\\pmb{y}}$ , not all observations $\\textit{\\textbf{y}}$ . See Figures S1 and S2 for experimental validation of this claim. ", "page_idx": 3}, {"type": "text", "text": "ELBO Training Loss Motivated by this observation, we desire a tractable training loss that encourages maximizing the evidence for the entire set of targets $\\textit{\\textbf{y}}$ . Importantly though, we need to accomplish this evidence maximization without incurring prohibitive $\\bar{\\mathcal{O}}(n^{3})$ computational cost. We define a variational objective, using the computation-aware posterior $q_{i}(f\\mid y,\\theta)$ in Equation (5) to define a variational family $\\begin{array}{r}{\\mathcal{Q}:=\\big\\{q_{i}(\\pmb{f})=\\mathcal{N}(\\pmb{f};\\mu_{i}(\\pmb{X}),K_{i}(\\pmb{X},\\pmb{X}))\\mid\\pmb{S}\\in\\mathbb{R}^{n\\times i}\\big\\}}\\end{array}$ parametrized by the action matrix $\\boldsymbol{S}$ . We can then specify a (negative) evidence lower bound (ELBO) as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{CaGP}}^{\\mathrm{ELBO}}(\\pmb\\theta)=\\underset{\\mathrm{balances~data~fit~and~model~complexity}}{\\underbrace{\\ell^{\\mathrm{NLL}}(\\pmb\\theta)}}+\\underset{\\mathrm{regularizes~s.t.}\\;q_{i}\\;\\approx\\;p_{\\star}}{\\underbrace{\\mathrm{KL}(q_{i}\\parallel p_{\\star})}}\\ge-\\log p(\\pmb\\eta\\mid\\pmb\\theta).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This loss promotes learning the same hyperparameters as if we were to maximize the computationally intractable evidence $\\log p(\\boldsymbol{y}\\mid\\boldsymbol{\\theta})$ while minimizing the error due to posterior approximation $q_{i}(f)\\overset{\\cdot}{\\approx}p_{\\star}(f\\mid y,\\pmb{\\theta})$ . In the computation-aware setting, this translates to minimizing computational uncertainty, which captures this inevitable error. ", "page_idx": 4}, {"type": "text", "text": "Although both the evidence and $\\mathrm{KL}$ terms of the ELBO involve computationally intractable terms, these problematic terms cancel out when combined. This results in an objective that costs the same as evaluating CaGP\u2019s predictive distribution, i.e. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\ell_{\\mathrm{CaGP}}^{\\mathrm{ELBO}}(\\pmb{\\theta})=\\displaystyle\\frac{1}{2}\\bigg(\\frac{1}{\\sigma^{2}}\\Big(\\|\\pmb{y}-\\mu_{i}(\\pmb{X})\\|_{2}^{2}+\\displaystyle\\sum_{j=1}^{n}K_{i}(\\pmb{x}_{j},\\pmb{x}_{j})\\Big)+(n-i)\\log(\\sigma^{2})+n\\log(2\\pi)}\\\\ {\\pmb{\\theta}_{i}^{\\top}S^{\\top}K S\\pmb{\\tilde{v}}_{i}-\\mathrm{tr}\\big((S^{\\top}\\pmb{\\hat{K}}S)^{-1}S^{\\top}\\pmb{K}S\\big)+\\log\\operatorname*{det}(S^{\\top}\\pmb{\\hat{K}}S)-\\log\\operatorname*{det}(S^{\\top}S)\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\widetilde{\\pmb{v}}_{i}=(\\pmb{S}^{\\top}\\hat{K}\\pmb{S})^{-1}\\pmb{S}^{\\top}(\\pmb{y}_{\\bot}\\bot)$ . For a derivation of this expression of the loss see Lemma S3. If we compare the training loss $\\ell_{\\mathrm{CaGP}}^{\\mathrm{ELBO}}$ in Equation (10) with the projected-data NLL in Equation (9), there is an explicit squared loss penalty on the entire data $\\textit{\\textbf{y}}$ , rather than just for the projected data $\\tilde{\\pmb{y}}$ , resulting in better generalization as Figures S1 and S2 show on synthetic data. In our experiments, this objective was critical to achieving state-of-the-art performance. ", "page_idx": 4}, {"type": "text", "text": "4 Choice of Actions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "So far we have not yet specified the actions $\\boldsymbol{S}\\,\\in\\,\\mathbb{R}^{n\\times i}$ mapping the data to a lower-dimensional space. Ideally, we would want to optimally compress the data both for inference and model selection. ", "page_idx": 4}, {"type": "text", "text": "Posterior Entropy Minimization We can interpret choosing actions $\\boldsymbol{S}$ as a form of active learning, where instead of just observing individual datapoints, we allow ourselves to observe linear combinations of the data $\\textit{\\textbf{y}}$ . Taking an information-theoretic perspective [21], we would then aim to choose actions such that uncertainty about the latent function is minimized. In fact, we show in Lemma S5 that given a prior $f\\sim{\\mathcal{G P}}(\\mu,K)$ for the latent function and a budget of $i$ actions $\\boldsymbol{S}$ , the actions that minimize the entropy of the posterior at the training data ", "page_idx": 4}, {"type": "equation", "text": "$$\n(s_{1},\\ldots,s_{i})=\\underset{S\\in\\mathbb{R}^{n\\times i}}{\\arg\\operatorname*{min}}\\,\\mathrm{H}_{p(f(\\pmb{X})|S^{\\top}\\pmb{y})}(f(\\pmb{X}))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "are the top- $^{\\,.\\,i}$ eigenvectors $s_{1},\\ldots,s_{i}$ of $\\hat{K}$ in descending order of the eigenvalue magnitude (see also Zhu et al. [22]). Unfortunately, computing these actions is just as prohibitive computationally as computing the intractable GP posterior. ", "page_idx": 4}, {"type": "text", "text": "(Conjugate) Gradient / Residual Actions Due to the intractability of choosing actions to minimize posterior entropy, we could try to do so approximately. The Lanczos algorithm [23] is an iterative method to approximate the eigenvalues and eigenvectors of symmetric positive-definite matrices. Given an appropriate seed vector, the space spanned by its approximate eigenvectors is equivalent to the span of the gradients / residuals $\\dot{\\mathbf{\\omega}_{i}}=(\\dot{\\mathbf{\\omega}_{y}}-\\mu)-\\tilde{K}v_{i-1}$ of the method of Conjugate Gradients (CG) [24] when used to iteratively compute an approximation $\\pmb{v}_{i}\\approx\\pmb{v}_{\\star}=\\hat{\\pmb{K}}^{-1}(\\pmb{\\breve{y}}-\\pmb{\\mu})$ to the representer weights $\\pmb{v_{\\star}}$ . We show in Lemma S4 that the $\\mathrm{CaGP}$ posterior only depends on the span of its actions. Therefore choosing approximate eigenvectors computed via the Lanczos process as actions is equivalent to using CG residuals. This allows us to reinterpret CaGP-CG, as introduced by Wenger et al. [19], as approximately minimizing posterior entropy.4 See Section S3.1 for details. ", "page_idx": 4}, {"type": "image", "img_path": "tDvFa5OJyS/tmp/b94945acb8fde280d3b5b52b6772203fd8ea8d22b796eb3f51b7947c6ea4d2e8.jpg", "img_caption": ["Figure 2: Visualization of action vectors defining the data projection. We perform model selection using two CaGP variants, with CG and learned sparse actions\u2014denoted as CaGP-CG, and CaGPOpt\u2014on a toy 2-dimensional dataset. Left: For each $\\pmb{x}_{j}\\,\\in\\,\\{\\pmb{x}_{1},\\ldots,\\pmb{x}_{n}\\}$ , we plot the magnitude of the entries of the top-5 eigenvectors of $\\hat{K}$ and of the first five action vectors. Yellow denotes larger magnitudes; blue denotes smaller magnitudes. Right: We compare the span of the actions $\\boldsymbol{S}$ against the top- $^{\\,.\\,i}$ eigenspace throughout training by measuring the Grassman distance between the two subspaces (see also Section S5.2). CaGP-CG actions are closer to the kernel eigenvectors than the CaGP-Opt actions, both of which are more closely aligned than randomly chosen actions. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "As Figure 2 illustrates, CG actions are similar to the top- $i$ eigenspace all throughout hyperparameter optimization. However, this choice of actions focuses exclusively on posterior inference and incurs quadratic time complexity $O(n^{2}i)$ . ", "page_idx": 5}, {"type": "text", "text": "Learned Sparse Actions So far in our action choices we have entirely ignored model selection and tried to choose optimal actions assuming fixed kernel hyperparameters. The second contribution of this work, aside from demonstrating how to perform model selection, is recognizing that the actions should be informed by the outer optimization loop for the hyperparameters. We thus optimize the actions alongside the hyperparameters end-to-end, meaning the training loss for model selection defines what data projections are informative. This way the actions are adaptive to the hyperparameters without spending unnecessary budget on computing approximately optimal actions for the current choice of hyperparameters. Specifically, the actions are chosen by optimizing $\\ell_{\\mathrm{CaGP}}^{\\mathrm{ELBO}}$ as a function of the hyperparameters $\\pmb{\\theta}$ and the actions $\\boldsymbol{S}$ , s.t. ", "page_idx": 5}, {"type": "equation", "text": "$$\n(\\pmb{\\theta}_{\\star},\\pmb{S}_{i})=\\arg\\operatorname*{min}_{(\\pmb{\\theta},S)}\\ell_{\\mathrm{CaGP}}^{\\mathrm{ELBO}}(\\pmb{\\theta},S).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Naively this approach introduces an $n\\times i$ dimensional optimization problem, which in general is computationally prohibitive. To keep the computational cost low and to optimally leverage hardware acceleration via GPUs, we impose a sparse block structure on the actions (see Eq. 14) where each block is a column vector $\\pmb{s}_{j}^{\\prime}\\ \\in\\ \\mathbb{R}^{k\\times1}$ with $k~=~n/i$ entries such that the total number of trainable action parameters, i.e. non-zero entries $\\mathrm{nnz}(S)\\,=\\,k\\cdot i\\,=\\,n$ , equals the number of training data. Due to the sparsity, these actions cannot perfectly match the maxi", "page_idx": 5}, {"type": "equation", "text": "$$\nS=\\left[\\begin{array}{l l l l}{s_{1}^{\\prime}}&{0}&{\\cdots}&{0^{-}}\\\\ {0}&{s_{2}^{\\prime}}&{}&{0}\\\\ {\\vdots}&{}&{\\ddots}&{\\vdots}\\\\ {0}&{\\cdots}&{0}&{s_{i-}^{\\prime}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "mum eigenvector actions. Nevertheless, we see in Figure 2 that optimizing these sparse actions in conjunction with hyperparameter optimization produces a nontrivial alignment with optimal action choice minimizing posterior entropy. Importantly, the sparsity constraint not only reduces the dimensionality of the optimization problem, but crucially also reduces the time complexity of posterior inference and model selection to linear in the number of training data points. ", "page_idx": 5}, {"type": "text", "text": "4.1 Algorithms and Computational Complexity ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We give algorithms both for iteratively constructed dense actions (Algorithm S1), as used in CaGPCG, and for sparse batch actions (Algorithm S2), as used for CaGP-Opt, in the supplementary material.5 The time complexity is $\\mathcal{O}(n^{2}i)$ for dense actions and $O(n i\\operatorname*{max}(i,k))$ for sparse actions, where $k$ is the maximum number of non-zero entries per column of $S_{i}$ . Both have the same linear memory requirement: $\\mathcal{O}(n i)$ . Since the training loss $\\ell_{\\mathrm{CaGP}}^{\\mathrm{ELBO}}$ only involves terms that are also present in the posterior predictive, both model selection and predictions incur the same complexity. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.2 Related Work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Computational Uncertainty and Probabilistic Numerics All CaGP variants discussed in this paper fall into the category of probabilistic numerical methods [25\u201327], which aim to quantify approximation error arising from limited computational resources via additional uncertainty about the quantity of interest [e.g. 28\u201331]. Specifically, the iterative formulation of CaGP (i.e. Algorithm S1) originally proposed by Wenger et al. [19] employs a probabilistic linear solver [32\u201335]. ", "page_idx": 6}, {"type": "text", "text": "Scalable GPs with Lower-Bounded Log Marginal Likelihoods Numerous scalable GP approximations beyond those in Sections 1 and 2 exist; see Liu et al. [36] for a comprehensive review. Many GP models [e.g., 4, 5, 37\u201339] learn hyperparameters through maximizing variational lower bounds in the same spirit as SGPR, SVGP and our method. Similar to our work, interdomain inducing point methods [40\u201342] learn a variational posterior approximation on a small set of linear functionals applied to the latent GP. However, unlike our method, their resulting approximate posterior is usually prone to underestimating uncertainty in the same manner as SGPR and SVGP. Finally, similar to our proposed training loss for CaGP-CG, Artemev et al. [43] demonstrate how one can use the method of conjugate gradients to obtain a tighter lower bound on the log marginal likelihood. ", "page_idx": 6}, {"type": "text", "text": "GP Approximation Biases and Computational Uncertainty Scalable GP methods inevitably introduce approximation error and thus yield biased hyperparameters and predictive distributions, with an exception of Potapczynski et al. [12] which trade bias for increased variance. Numerous works have studied pathologies associated with optimizing variational lower bounds, especially in the context of SVGP [12, 16\u201318], and various remedies have been proposed. In order to mitigate biases from approximation, several works alternatively propose replacing variational lower bounds with alternative model selection objectives, including leave-one-out cross-validation [44] and losses that directly target predictive performance [45, 46]. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We benchmark the generalization of computation-aware GPs with two different action choices, CaGP-Opt (ours) and CaGP-CG [19], using our proposed training objective in Equation (10) on a range of UCI datasets for regression [53]. We compare against SVGP [5], often considered to be state-of-the-art for large-scale GP regression. Per recommendations by Ober et al. [15], we also include SGPR [4] as a strong baseline for all datasets where this is computationally feasible. We also train Conjugate Gradient-based GPs (CGGP) [e.g. 7, 9, 10] using the training procedure proposed by Wenger et al. [10]. Note that CaGP-CG recovers CGGP in its posterior mean and produces nearly identical predictive error at half the computational cost for inference [Sec. 2.1 & 4 of 19], which is why the main difference between CaGP-CG and CGGP in our experiments is the training objective. Finally, we also train an exact CholeskyGP on the smallest datasets, where this is still feasible. ", "page_idx": 6}, {"type": "text", "text": "Experimental Details All datasets were randomly partitioned into train and test sets using a (0.9, 0.1) split for five random seeds. We used a zero-mean GP prior and a $\\mathrm{Matern(3/2)}$ kernel with an outputscale $o^{2}$ and one lengthscale per input dimension $l_{j}^{2}$ , as well as a scalar observation noise for the likelihood $\\sigma^{2}$ , s.t. $\\pmb{\\theta}\\stackrel{=}{=}(o,l_{1},\\stackrel{\\cdot}{\\dots},l_{d}^{\\stackrel{\\cdot}{,}}\\sigma)\\in\\mathbb{R}^{d+2}$ . We used the existing implementations of SGPR, SVGP and CGGP in GPyTorch [7] and also implemented $\\mathrm{CaGP}$ in this framework (see Section S4.2 for our open-source implementation). For SGPR and SVGP we used $m=1024$ inducing points and for CGGP, CaGP-CG and CaGP-Opt we chose $i=512$ . We optimized the hyperparameters $\\pmb{\\theta}$ either with Adam [54] for a maximum of 1000 epochs in float32 or with LBFGS [55] for 100 epochs in float64, depending on the method and problem scale. On the largest dataset \u201cPower\u201d, we used 400 epochs for SVGP and 200 for CaGP-Opt due to resource constraints. For SVGP we used a batch size of 1024 throughout. We scheduled the learning rate via PyTorch\u2019s [56] LinearLR(end factor ${\\it\\Psi}=0$ .1) scheduler for all methods and performed a hyperparameter sweep for the (initial) learning rate. All experiments were run on an NVIDIA Tesla V100-PCIE-32GB GPU, except for \u201cPower\u201d, where we used an NVIDIA A100 80GB PCIe GPU to have sufficient memory for CaGP-Opt with $i=512$ . Our exact experiment configuration can be found in Table S1. ", "page_idx": 6}, {"type": "table", "img_path": "tDvFa5OJyS/tmp/d7086a68173afd4afea369843ff63cd50110ea48939ec994be6ab3fd5e81b369.jpg", "table_caption": ["Table 1: Generalization error (NLL, RMSE, and wall-clock time) on UCI benchmark datasets. The table shows the best results for all methods across learning rate sweeps, averaged across five random seeds. We report the epoch where each method obtained the lowest average test NLL, and all performance metrics (NLL, RMSE, and wall-clock runtime) are reported for this epoch. Highlighted in bold and color are the best approximate methods per metric (difference $>1$ standard deviation). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Evaluation Metrics We evaluate the generalization performance once per epoch on the test set by computing the (average) negative log-likelihood (NLL) and the root mean squared error (RMSE), as well as recording the wallclock runtime. Runtime is measured at the epoch with the best average performance across random seeds. ", "page_idx": 7}, {"type": "text", "text": "CaGP-Opt Matches or Outperforms SVGP Table 1 and Figure 3 show generalization performance of all methods for the best choice of learning rate. In terms of both NLL and RMSE, CaGPOpt outperforms or matches the variational baselines SGPR and SVGP at comparable runtime (except on \u201cBike\u201d). SGPR remains scompetitive for smaller datasets; however, it does not scale to the largest datasets. There are some datasets and metrics in which specific methods dominate, for example on \u201cBike\u201d SGPR outperforms all other approximations, while on \u201cProtein\u201d methods based on CG, i.e. CGGP and CaGP-CG, perform the best. However, CaGP-Opt consistently performs either best or second best and scales to over a million datapoints. These results are quite remarkable for numerous reasons. First, CaGP is comparable in runtime to SVGP on large datasets despite the fact that it incurs a linear-time computational complexity while SVGP is constant time.6 Second, while all of the methods we compare approximate the GP posterior with low-rank updates, CaGP-Opt (with $i=512$ ) here uses half the rank of SGPR/SVGP $m=1024$ . Nevertheless, CaGP-Opt is able to substantially outperform SVGP even on spatial datasets like 3DRoad where low-rank posterior updates are often poor [57]. These results suggest that CaGP-Opt can be a more efficient approximation than inducing point methods, and that low-rank GP approximations may be more applicable than previously thought [58, 59]. Figure 3 shows the NLL and RMSE learning curves for the best choice of learning rate per method. CaGP-Opt often shows a \u201cramp-up\u201d phase, compared to SVGP, but then improves or matches its generalization performance. This gap is particularly large on \u201cRoad\u201d, where CaGP-Opt is initially worse than SVGP but dominates in the second half of training. ", "page_idx": 7}, {"type": "image", "img_path": "tDvFa5OJyS/tmp/92517576d1e9837d58529775ea07e2f77657f82bcb7e5d0d22efb4b982f05013.jpg", "img_caption": ["Figure 3: Learning curves of GP approximation methods on UCI benchmark datasets. Rows show train and test loss as a function of wall-clock time for the best choice of learning rate per method. CaGP-Opt generally displays a \u201cramp-up\u201d phase early in training where performance is worse than that of SVGP. As training progresses, CaGP-Opt matches or surpasses SVGP performance. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "SVGP Overestimates Observation Noise and (Often) Lengthscale In Figure S5 we show that SVGP typically learns larger observation noise than other methods as suggested by previous work [18, 45] and hinted at by observations on synthetic data in Figure 1 and Figure S3(b). Additionally on larger datasets SVGP also often learns large lengthscales, which in combination with a large observation noise can lead to an oversmoothing effect. In contrast, CaGP-Opt generally learns lower observational noise than SVGP. Of course, learning a small observation noise, in particular, is important for achieving low RMSE and thus also NLL, and points to why we should expect CaGPOpt to outperform SVGP. These hyperparameter results suggest that CaGP-Opt interprets more of the data as signal, while SVGP interprets more of the data as noise. ", "page_idx": 8}, {"type": "image", "img_path": "tDvFa5OJyS/tmp/64ba64c4be9dab45ebd76089f54967d3154d67332459d499620bc1e326449688.jpg", "img_caption": ["Figure 4: Uncertainty quantification for CaGP-Opt and SVGP. Difference between the desired coverage $(95\\%)$ and the empirical coverage of the GP $95\\%$ credible interval on the \u201cPower\u201d dataset. After training, CaGP-Opt has better empirical coverage than SVGP. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "CaGP Improves Uncertainty Quantification Over SVGP A key advantage of CaGP-Opt and CaGP-CG is that their posterior uncertainty estimates capture both the uncertainty due to limited data and due to limited computation. To that end, we assess the frequentist coverage of CaGP-Opt\u2019s uncertainty estimates. We report the absolute difference between a desired coverage percentage $\\alpha$ and the fraction of data that fall into the $\\alpha$ credible interval of the CaGP-Opt posterior; i.e. $\\varepsilon_{\\mathrm{coverage}}^{\\alpha}=$ $\\begin{array}{r}{|\\alpha-\\frac{1}{n_{\\mathrm{test}}}\\sum_{i=1}^{n_{\\mathrm{test}}}1(y_{i}\\,\\in\\,I_{q({\\pmb x}_{i})}^{\\alpha})|}\\end{array}$ . Figure 4 compares the $95\\%$ coverage error for both CaGP-Opt and SVGP on the largest dataset (\u201cPower\u201d). From this plot, we see that the CaGP credible intervals are more aligned with the desired coverage. We hypothesize that these results reflect the different uncertainty properties of the methods: CaGP-Opt overestimates posterior uncertainty while SVGP is prone towards overconfidence. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduced strategies for model selection and posterior inference for computationaware Gaussian processes, which scale linearly with the number of training data rather than quadratically. The key technical innovations being a sparse projection of the data, which balances minimizing posterior entropy and computational cost, and a scalable way to optimize kernel hyperparameters, both of which are amenable to GPU acceleration. All together, these advances enable competitive or improved performance over previous approximate inference methods on large-scale datasets, in terms of generalization and uncertainty quantification. Remarkably, our method outperforms SVGP\u2014often considered the de-facto GP approximation standard\u2014 even when compressing the data into a space of half the dimension of the variational parameters. Finally, we also demonstrate that computation-aware GPs avoid many of the pathologies often observed in inducing point methods, such as overconfidence and oversmoothing. ", "page_idx": 9}, {"type": "text", "text": "Limitations While CaGP-Opt obtains the same linear time and memory costs as SGPR, it is not amenable to stochastic minibatching and thus cannot achieve the constant time/memory capabilities of SVGP. In practice, this asymptotic difference does not result in substantially different wall clock times, as SVGP requires many more optimizer steps than CaGP-Opt due to batching. (Indeed, on many datasets we find that CaGP-Opt is faster.) CaGP-Opt nevertheless requires larger GPUs than SVGP on datasets with more than a million data points. Moreover, tuning CaGP-Opt requires choosing the appropriate number of actions (i.e. the rank of the approximate posterior update), though we note that most scalable GP approximations have a similar tunable parameter (e.g. number of inducing points). Perhaps the most obvious limitation is that CaGP, unlike SVGP, is limited to GP regression with a conjugate observational noise model. We leave extensions to classification and other non-conjugate likelihoods as future work. ", "page_idx": 9}, {"type": "text", "text": "Outlook and Future Work An immediate consequence of this work is the ability to apply computation-aware Gaussian processes to \u201creal-world\u201d problems, as our approach solves CaGP\u2019s open problems of model selection and scalability. Looking forward, an exciting future vision is a general framework for problems involving a Gaussian process model with a downstream task where the actions are chosen optimally, given resource constraints, to solve said task. Future work will pursue this direction beyond Gaussian likelihoods to non-conjugate models and downstream tasks such as Bayesian optimization. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "JW and JPC are supported by the Gatsby Charitable Foundation (GAT3708), the Simons Foundation (542963), the NSF AI Institute for Artificial and Natural Intelligence (ARNI: NSF DBI 2229929) and the Kavli Foundation. JG and KW are supported by the NSF (IIS-2145644, DBI-2400135). PH gratefully acknowledges co-funding by the European Union (ERC, ANUBIS, 101123955). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them. PH is a member of the Machine Learning Cluster of Excellence, funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany\u2019s Excellence Strategy \u2013 EXC number 2064/1 \u2013 Project number 390727645; he also gratefully acknowledges the German Federal Ministry of Education and Research (BMBF) through the Tu\u00a8bingen AI Center (FKZ: 01IS18039A); and funds from the Ministry of Science, Research and Arts of the State of Baden-Wu\u00a8rttemberg. GP acknowledges support from NSERC and the Canada CIFAR AI Chair program. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] C. Williams and M. Seeger. \u201cUsing the Nystro\u00a8m method to speed up kernel machines\u201d. In: Advances in Neural Information Processing Systems (NeurIPS). 2001 (cit. on p. 1). [2] A. Rahimi and B. Recht. \u201cRandom Features for Large-Scale Kernel Machines\u201d. In: Advances in Neural Information Processing Systems (NeurIPS). 2007 (cit. on p. 1). [3] J. Quin\u02dconero-Candela and C. E. Rasmussen. \u201cA unifying view of sparse approximate Gaussian process regression\u201d. In: Journal of Machine Learning Research 6 (2005) (cit. on p. 1). [4] M. Titsias. \u201cVariational learning of inducing variables in sparse Gaussian processes\u201d. In: International Conference on Artificial Intelligence and Statistics (AISTATS). PMLR. 2009, pp. 567\u2013574 (cit. on pp. 1, 3, 7). [5] J. Hensman, N. Fusi, and N. D. Lawrence. \u201cGaussian processes for big data\u201d. In: Conference on Uncertainty in Artificial Intelligence (UAI). 2013, pp. 282\u2013290 (cit. on pp. 1, 3, 7). [6] M. Gibbs. \u201cBayesian Gaussian processes for classification and regression\u201d. PhD thesis. 1997 (cit. on p. 1). [7] J. R. Gardner, G. Pleiss, D. Bindel, K. Q. Weinberger, and A. G. Wilson. \u201cGPyTorch: Blackbox matrix-matrix Gaussian process inference with GPU acceleration\u201d. In: Advances in Neural Information Processing Systems (NeurIPS). 2018, pp. 7576\u20137586 (cit. on pp. 1, 5, 7). [8] G. Pleiss, J. Gardner, K. Weinberger, and A. G. Wilson. \u201cConstant-time predictive distributions for Gaussian processes\u201d. In: International Conference on Machine Learning (ICML). 2018, pp. 4114\u20134123 (cit. on p. 1). [9] K. A. Wang, G. Pleiss, J. R. Gardner, S. Tyree, K. Q. Weinberger, and A. G. Wilson. \u201cExact Gaussian processes on a million data points\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) 32 (2019) (cit. on pp. 1, 5, 7).   \n[10] J. Wenger, G. Pleiss, P. Hennig, J. P. Cunningham, and J. R. Gardner. \u201cPreconditioning for Scalable Gaussian Process Hyperparameter Optimization\u201d. In: International Conference on Machine Learning (ICML). 2022 (cit. on pp. 1, 5, 7).   \n[11] F. Bach. \u201cOn the equivalence between kernel quadrature rules and random feature expansions\u201d. In: Journal of Machine Learning Research 18.21 (2017), pp. 1\u201338 (cit. on p. 1).   \n[12] A. Potapczynski, L. Wu, D. Biderman, G. Pleiss, and J. P. Cunningham. \u201cBias-Free Scalable Gaussian Processes via Randomized Truncations\u201d. In: International Conference on Machine Learning (ICML). 2021 (cit. on pp. 1, 2, 7).   \n[13] D. R. Burt, C. E. Rasmussen, and M. van der Wilk. \u201cRates of Convergence for Sparse Variational Gaussian Process Regression\u201d. In: International Conference on Machine Learning (ICML). 2019. URL: http://arxiv.org/abs/1903.03571 (cit. on p. 1).   \n[14] M. Kang, F. Scha\u00a8fer, J. Guinness, and M. Katzfuss. Asymptotic properties of Vecchia approximation for Gaussian processes. 2024. DOI: 10.48550/arXiv.2401.15813 (cit. on p. 1).   \n[15] S. W. Ober, D. R. Burt, A. Artemev, and M. van der Wilk. \u201cRecommendations for Baselines and Benchmarking Approximate Gaussian Processes\u201d. In: NeurIPS Workshop on Gaussian Processes, Spatiotemporal Modeling, and Decision-making Systems. 2022. URL: https:// gp-seminar-series.github.io/assets/camera_ready/62.pdf (cit. on pp. 1, 7).   \n[16] Z. Wang, C. Gehring, P. Kohli, and S. Jegelka. \u201cBatched Large-scale Bayesian Optimization in High-dimensional Spaces\u201d. In: International Conference on Artificial Intelligence and Statistics (AISTATS). 2018. DOI: 10.48550/arXiv.1706.01445. URL: http://arxiv. org/abs/1706.01445 (cit. on pp. 2, 7).   \n[17] R. E. Turner and M. Sahani. \u201cTwo problems with variational expectation maximisation for time series models\u201d. In: Bayesian Time Series Models. Cambridge University Press, 2011, pp. 104\u2013124. DOI: 10.1017/CBO9780511984679.006 (cit. on pp. 2, 7).   \n[18] M. Bauer, M. van der Wilk, and C. E. Rasmussen. \u201cUnderstanding probabilistic sparse Gaussian process approximations\u201d. In: Advances in Neural Information Processing Systems (NeurIPS). Vol. 29. 2016 (cit. on pp. 2, 7, 9).   \n[19] J. Wenger, G. Pleiss, M. Pfo\u00a8rtner, P. Hennig, and J. P. Cunningham. \u201cPosterior and Computational Uncertainty in Gaussian processes\u201d. In: Advances in Neural Information Processing Systems (NeurIPS). 2022 (cit. on pp. 2, 3, 5, 7, 16, 17, 23).   \n[20] D. R. Burt, C. E. Rasmussen, and M. v. d. Wilk. \u201cConvergence of Sparse Variational Inference in Gaussian Processes Regression\u201d. In: Journal of Machine Learning Research (Aug. 2020). DOI: 10.48550/arXiv.2008.00323 (cit. on p. 3).   \n[21] N. Houlsby, F. Husza\u00b4r, Z. Ghahramani, and M. Lengyel. \u201cBayesian active learning for classification and preference learning\u201d. In: arXiv (2011). URL: https://arxiv.org/abs/1112. 5745 (cit. on pp. 5, 21).   \n[22] H. Zhu, C. K. Williams, R. Rohwer, and M. Morciniec. \u201cGaussian regression and optimal finite dimensional linear models\u201d. In: Neural Networks and Machine Learning. 1997 (cit. on p. 5).   \n[23] C. Lanczos. An iteration method for the solution of the eigenvalue problem of linear differential and integral operators. United States Government Press Office Los Angeles, CA, 1950 (cit. on pp. 5, 21).   \n[24] M. R. Hestenes and E. Stiefel. \u201cMethods of conjugate gradients for solving linear systems\u201d. In: Journal of Research of the National Bureau of Standards 49 (1952) (cit. on pp. 5, 21).   \n[25] P. Hennig, M. A. Osborne, and M. Girolami. \u201cProbabilistic numerics and uncertainty in computations\u201d. In: Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences 471.2179 (2015) (cit. on p. 7).   \n[26] J. Cockayne, C. Oates, T. Sullivan, and M. Girolami. \u201cBayesian probabilistic numerical methods\u201d. In: SIAM Review 61.4 (2019), pp. 756\u2013789 (cit. on p. 7).   \n[27] P. Hennig, M. A. Osborne, and H. P. Kersting. Probabilistic Numerics: Computation as Machine Learning. Cambridge University Press, 2022. ISBN: 978-1-316-68141-1. DOI: 10 . 1017/9781316681411 (cit. on p. 7).   \n[28] M. Pf\u00a8ortner, I. Steinwart, P. Hennig, and J. Wenger. Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers. 2023. DOI: 10.48550/arXiv.2212.12474 (cit. on p. 7).   \n[29] L. Tatzel, J. Wenger, F. Schneider, and P. Hennig. Accelerating Generalized Linear Models by Trading off Computation for Uncertainty. 2024. DOI: 10.48550/arXiv.2310.20285 (cit. on p. 7).   \n[30] M. Pfo\u00a8rtner, J. Wenger, J. Cockayne, and P. Hennig. Computation-Aware Kalman Filtering and Smoothing. 2024. DOI: 10.48550/arxiv.2405.08971 (cit. on p. 7).   \n[31] D. Hegde, M. Adil, and J. Cockayne. Calibrated Computation-Aware Gaussian Processes. 2024. DOI: 10.48550/arXiv.2410.08796 (cit. on p. 7).   \n[32] P. Hennig. \u201cProbabilistic Interpretation of Linear Solvers\u201d. In: SIAM Journal on Optimization 25.1 (2015), pp. 234\u2013260 (cit. on p. 7).   \n[33] J. Cockayne, C. Oates, I. C. Ipsen, and M. Girolami. \u201cA Bayesian Conjugate Gradient Method\u201d. In: Bayesian Analysis 14.3 (2019), pp. 937\u20131012 (cit. on p. 7).   \n[34] S. Bartels, J. Cockayne, I. C. Ipsen, and P. Hennig. \u201cProbabilistic linear solvers: A unifying view\u201d. In: Statistics and Computing 29.6 (2019), pp. 1249\u20131263 (cit. on p. 7).   \n[35] J. Wenger and P. Hennig. \u201cProbabilistic Linear Solvers for Machine Learning\u201d. In: Advances in Neural Information Processing Systems (NeurIPS). 2020. DOI: 10.48550/arXiv.2010. 09691. URL: http://arxiv.org/abs/2010.09691 (cit. on p. 7).   \n[36] H. Liu, Y.-S. Ong, X. Shen, and J. Cai. \u201cWhen Gaussian process meets big data: A review of scalable GPs\u201d. In: Transactions on Neural Networks and Learning Systems 31.11 (2020), pp. 4405\u20134423 (cit. on p. 7).   \n[37] J. Hensman, A. Matthews, and Z. Ghahramani. \u201cScalable variational Gaussian process classification\u201d. In: International Conference on Artificial Intelligence and Statistics (AISTATS). PMLR, 2015, pp. 351\u2013360 (cit. on p. 7).   \n[38] H. Salimbeni, C.-A. Cheng, B. Boots, and M. Deisenroth. \u201cOrthogonally Decoupled Variational Gaussian Processes\u201d. In: Advances in Neural Information Processing Systems (NeurIPS). Vol. 31. 2018 (cit. on p. 7).   \n[39] L. Wu, G. Pleiss, and J. P. Cunningham. \u201cVariational nearest neighbor Gaussian process\u201d. In: International Conference on Machine Learning (ICML). PMLR, 2022, pp. 24114\u201324130 (cit. on p. 7).   \n[40] J. Hensman, N. Durrande, and A. Solin. \u201cVariational Fourier Features for Gaussian Processes\u201d. In: Journal of Machine Learning Research 18.151 (2018), pp. 1\u201352 (cit. on p. 7).   \n[41] V. Dutordoir, N. Durrande, and J. Hensman. \u201cSparse Gaussian Processes with Spherical Harmonic Features\u201d. In: International Conference on Machine Learning (ICML). Vol. 119. 2020, pp. 2793\u20132802 (cit. on p. 7).   \n[42] M. Van der Wilk, V. Dutordoir, S. John, A. Artemev, V. Adam, and J. Hensman. A framework for interdomain and multioutput Gaussian processes. 2020. DOI: 10.48550/arXiv.2003. 01115 (cit. on p. 7).   \n[43] A. Artemev, D. R. Burt, and M. van der Wilk. \u201cTighter Bounds on the Log Marginal Likelihood of Gaussian Process Regression Using Conjugate Gradients\u201d. In: International Conference on Machine Learning (ICML). 2021 (cit. on p. 7).   \n[44] M. Jankowiak and G. Pleiss. Scalable Cross Validation Losses for Gaussian Process Models. 2022. DOI: 10.48550/arXiv.2105.11535 (cit. on p. 7).   \n[45] M. Jankowiak, G. Pleiss, and J. Gardner. \u201cParametric Gaussian Process Regressors\u201d. In: International Conference on Machine Learning (ICML). Vol. 119. 2020, pp. 4702\u20134712 (cit. on pp. 7, 9).   \n[46] Y. Wei, R. Sheth, and R. Khardon. \u201cDirect Loss Minimization for Sparse Gaussian Processes\u201d. In: International Conference on Artificial Intelligence and Statistics (AISTATS). Vol. 130. 2021, pp. 2566\u20132574 (cit. on p. 7).   \n[47] A. Tsanas and M. Little. Parkinsons Telemonitoring. UCI Machine Learning Repository. 2009. DOI: 10.24432/C5ZS3N (cit. on p. 8).   \n[48] H. Fanaee-T and J. Gama. Bike Sharing. UCI Machine Learning Repository. 2013. DOI: 10. 24432/C5W894 (cit. on p. 8).   \n[49] P. Rana. Physicochemical Properties of Protein Tertiary Structure. UCI Machine Learning Repository. 2013. DOI: 10.24432/C5QW3H (cit. on p. 8).   \n[50] M. Naeem and S. Asghar. KEGG Metabolic Reaction Network (Undirected). UCI Machine Learning Repository. 2011. DOI: 10.24432/C5G609 (cit. on p. 8).   \n[51] M. Kaul. 3D Road Network (North Jutland, Denmark). UCI Machine Learning Repository. 2013. DOI: 10.24432/C5GP51 (cit. on p. 8).   \n[52] G. Hebrail and A. Berard. Individual Household Electric Power Consumption. UCI Machine Learning Repository. 2006. DOI: 10.24432/C58K54 (cit. on p. 8).   \n[53] M. Kelly, R. Longjohn, and K. Nottingham. The UCI Machine Learning Repository. 2017. URL: https://archive.ics.uci.edu (cit. on p. 7).   \n[54] D. P. Kingma and J. Ba. \u201cAdam: A method for stochastic optimization\u201d. In: International Conference on Learning Representations (ICLR) (2015) (cit. on p. 7).   \n[55] J. Nocedal. \u201cUpdating quasi-Newton matrices with limited storage\u201d. In: Mathematics of Computation 35.151 (1980), pp. 773\u2013782 (cit. on p. 7).   \n[56] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Ko\u00a8pf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. \u201cPyTorch: An Imperative Style, High-Performance Deep Learning Library\u201d. In: Advances in Neural Information Processing Systems (NeurIPS). 2019. DOI: 10.48550/arXiv.1912.01703. URL: http: //arxiv.org/abs/1912.01703 (cit. on p. 7).   \n[57] G. Pleiss, M. Jankowiak, D. Eriksson, A. Damle, and J. Gardner. \u201cFast matrix square roots with applications to Gaussian processes and Bayesian optimization\u201d. In: Advances in Neural Information Processing Systems (NeurIPS). 2020, pp. 22268\u201322281 (cit. on p. 9).   \n[58] A. Datta, S. Banerjee, A. O. Finley, and A. E. Gelfand. \u201cHierarchical nearest-neighbor Gaussian process models for large geostatistical datasets\u201d. In: Journal of the American Statistical Association 111.514 (2016), pp. 800\u2013812 (cit. on p. 9).   \n[59] M. Katzfuss and J. Guinness. \u201cA General Framework for Vecchia Approximations of Gaussian Processes\u201d. In: Statistical Science 36.1 (2021). DOI: 10.1214/19-sts755 (cit. on p. 9).   \n[60] M. Kanagawa, P. Hennig, D. Sejdinovic, and B. K. Sriperumbudur. Gaussian Processes and Kernel Methods: A Review on Connections and Equivalences. arXiv:1807.02582 [cs, stat]. 2018. DOI: 10.48550/arXiv.1807.02582. URL: http://arxiv.org/abs/1807.02582 (cit. on p. 16).   \n[61] V. Wild, M. Kanagawa, and D. Sejdinovic. \u201cConnections and Equivalences between the Nystro\u00a8m Method and Sparse Variational Gaussian Processes\u201d. In: arXiv (2021). URL: http: //arxiv.org/abs/2106.01121 (cit. on pp. 16, 17).   \n[62] G. H. Golub and C. F. Van Loan. Matrix computations. John Hopkins University Press, 2012 (cit. on pp. 21, 25).   \n[63] G. Meurant and Z. Strakos\u02c7. \u201cThe Lanczos and conjugate gradient algorithms in finite precision arithmetic\u201d. en. In: Acta Numerica 15 (May 2006), pp. 471\u2013542. DOI: 10.1017/ S096249290626001X (cit. on p. 21). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This supplementary material contains additional results and in particular proofs for all theoretical statements. References referring to sections, equations or theorem-type environments within this document are prefixed with $\\mathbf{\\nabla}^{\\bullet}\\mathbf{S}^{\\bullet}$ , while references to, or results from, the main paper are stated as is. ", "page_idx": 14}, {"type": "text", "text": "S1 Theoretical Results 15 ", "page_idx": 14}, {"type": "text", "text": "S1.1 Alternative Derivation of CaGP Posterior 15   \nS1.2 Worst Case Error Interpretations of the Variance of Exact GPs, CaGPs and SVGPs . 16   \nS1.3 CaGP\u2019s Variance Decreases Monotonically as the Number of Iterations Increases . . 17 ", "page_idx": 14}, {"type": "text", "text": "S2 Training Losses 17 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "S2.1 Projected-Data Log-Marginal Likelihood 17   \nS2.2 Evidence Lower Bound (ELBO) . . . 18   \nS2.3 Comparison of Training Losses 20 ", "page_idx": 14}, {"type": "text", "text": "S3 Choice of Actions 20 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "S3.1 (Conjugate) Gradient / Residual Policy . 21   \nS3.2 Information-theoretic Policy 21 ", "page_idx": 14}, {"type": "text", "text": "S4 Algorithms 23 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "S4.1 Iterative and Batch Versions of CaGP 23   \nS4.2 Implementation . 23 ", "page_idx": 14}, {"type": "text", "text": "S5 Additional Experimental Results and Details 24 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "S5.1 Inducing Points Placement and Uncertainty Quantification of SVGP 24   \nS5.2 Grassman Distance Between Subspaces . 25   \nS5.3 Generalization Experiment . . 25   \nS5.3.1 Impact of Learning Rate on Generalization . . . . . . 25   \nS5.3.2 Evolution Of Hyperparameters During Training \uff1a\uff1a 27 ", "page_idx": 14}, {"type": "text", "text": "S1 Theoretical Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "S1.1 Alternative Derivation of CaGP Posterior ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma S1 ( $\\mathbf{CaGP}$ Inference as Exact Inference Given a Modified Observation Model) Given a Gaussian process prior $f\\sim{\\mathcal{G P}}(\\mu,K)$ and training data $(X,y)$ the computation-aware $G P$ posterior $\\mathcal G\\mathcal P(\\mu_{i},K_{i})$ (see Equation (5)) with linearly independent and fixed actions $\\boldsymbol{S}$ is equivalent to an exact batch GP posterior $(f\\,\\lrcorner\\,\\Tilde{\\boldsymbol{y}})$ given data $\\tilde{\\b{y}}^{\\prime}=\\b{S}^{\\prime}^{\\top}\\b{y}$ observed according to the likelihood $\\tilde{\\pmb{y}}\\mid f(\\pmb{X})\\sim\\mathcal{N}\\Big(\\dot{\\pmb{S^{\\prime}}}^{\\top}f(\\pmb{X}),\\dot{\\sigma^{2}}\\pmb{I}\\Big)$ , where $S^{\\prime}=S\\operatorname{chol}(S^{\\mathsf{T}}S)^{-\\mathsf{T}}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. First note that by definition $S^{\\prime}$ has orthonormal columns, since ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{S}^{\\prime}^{\\top}\\boldsymbol{S}^{\\prime}=(\\boldsymbol{S}\\operatorname{chol}(\\boldsymbol{S}^{\\top}\\boldsymbol{S})^{-\\top})^{\\top}\\boldsymbol{S}\\operatorname{chol}(\\boldsymbol{S}^{\\top}\\boldsymbol{S})^{-\\top}}\\\\ &{\\quad\\quad\\quad=\\operatorname{chol}(\\boldsymbol{S}^{\\top}\\boldsymbol{S})^{-1}\\boldsymbol{S}^{\\top}\\boldsymbol{S}\\operatorname{chol}(\\boldsymbol{S}^{\\top}\\boldsymbol{S})^{-\\top}}\\\\ &{\\quad\\quad\\quad=L^{-1}L L^{\\top}L^{-\\top}}\\\\ &{\\quad\\quad\\quad=I.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now by basic properties of Gaussian distributions, we have for arbitrary $X_{\\diamond}\\in\\mathbb{R}^{n_{\\diamond}\\times d}$ that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\biggl({f(\\stackrel{\\tilde{y}}{X}_{\\diamond})}\\biggr)\\sim\\mathcal{N}\\biggl(\\biggl({S^{\\prime}}^{\\top}\\mu(X)\\biggr)\\,,\\,\\biggl({S^{\\prime}}^{\\top}K(X,X)S^{\\prime}+\\sigma^{2}{S^{\\prime}}^{\\top}S^{\\prime}\\quad{S^{\\prime}}^{\\top}K(X,X_{\\diamond})\\biggr)\\biggr)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "is jointly Gaussian, where we used that ${\\pmb I}={\\pmb S}^{\\prime}{}^{\\top}{\\pmb S}^{\\prime}$ . ", "page_idx": 14}, {"type": "text", "text": "Therefore we have that $(f(\\mathbf{X}_{\\diamond})\\mid\\tilde{\\pmb{y}})\\sim\\mathcal{N}(\\mu_{i}(\\mathbf{X}_{\\diamond}),K_{i}(\\mathbf{X}_{\\diamond},\\mathbf{X}_{\\diamond}))$ with ", "page_idx": 14}, {"type": "text", "text": "$\\begin{array}{r l}&{\\mu_{i}(X_{\\diamond})=\\mu(X_{\\diamond})+K(X_{\\diamond},X){S^{\\prime}(S^{\\prime}}^{\\top}(K(X,X)+\\sigma^{2}I)S^{\\prime})^{-1}(\\tilde{y}-{S^{\\prime}}^{\\top}\\mu),}\\\\ &{\\qquad\\qquad=\\mu(X_{\\diamond})+K(X_{\\diamond},X)S(S^{\\top}(K(X,X)+\\sigma^{2}I)S)^{-1}S^{\\top}(y-\\mu)}\\end{array}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K_{i}(X_{\\diamond},X_{\\diamond})=K(X_{\\diamond},X_{\\diamond})-K(X_{\\diamond},X){S^{\\prime}(S^{\\prime}}^{\\mathrm{T}}(K(X,X)+\\sigma^{2}I){S^{\\prime}})^{-1}{S^{\\prime}}^{\\mathrm{T}}K(X,X_{\\diamond})}\\\\ &{\\qquad\\qquad\\qquad=K(X_{\\diamond},X_{\\diamond})-K(X_{\\diamond},X){S(S^{\\intercal}(K(X,X)+\\sigma^{2}I)S)^{-1}}{S^{\\intercal}}K(X,X_{\\diamond})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is equivalent to the definition of the CaGP posterior in Equation (5). This proves the claim. ", "page_idx": 15}, {"type": "text", "text": "S1.2 Worst Case Error Interpretations of the Variance of Exact GPs, CaGPs and SVGPs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In order to understand the impact of approximation on the uncertainty quantification of both CaGP and SVGP, it is instructive to compare the theoretical guarantees they admit, when the latent function is assumed to be in the RKHS of the kernel. In the context of model selection, this corresponds to the ideal case where the optimization has converged to the ground truth hyperparameters. ", "page_idx": 15}, {"type": "text", "text": "Let $f\\,\\sim\\mathcal{G P}(0,K)$ be a Gaussian process with kernel $K$ and define the observed process $y(\\cdot)=$ $f(\\cdot)+\\sigma\\varepsilon(\\cdot)$ where $\\varepsilon\\sim\\mathcal{G P}(0,\\delta)$ is white noise with noise level $\\sigma^{2}>0$ , i.e. $\\delta({\\pmb x},{\\pmb x}^{\\prime})=1({\\pmb x}={\\pmb x}^{\\prime})$ . Consequently, the covariance kernel of the data-generating process $y(\\cdot)$ is given by $K^{\\sigma}({\\bf{x}},{\\bf{x}}^{\\prime}):=$ $K({\\pmb x},{\\pmb\\bar{\\pmb x}}^{\\prime})+\\stackrel{\\cdot}{\\sigma}^{2}\\delta({\\pmb x},{\\pmb x}^{\\prime})$ and we denote the corresponding RKHS as $\\mathbb{H}_{K^{\\sigma}}$ . ", "page_idx": 15}, {"type": "text", "text": "For exact Gaussian process inference, the pointwise (relative) worst-case squared error of the posterior mean is precisely given by the posterior predictive variance. ", "page_idx": 15}, {"type": "text", "text": "Theorem S1 (Worst Case Error Interpretation of GP Variance [60]) ", "page_idx": 15}, {"type": "text", "text": "Given a set of training inputs $x_{1},\\ldots,x_{n}\\ \\in\\ \\mathbb{X}_{}$ , the $G P$ posterior $\\mathcal{G P}(\\mu_{\\star},K_{\\star})$ satisfies for any $\\pmb{x}\\neq\\pmb{x}_{j}$ that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{y\\in\\mathbb{H}_{K^{\\sigma}},\\|y\\|_{\\mathbb{H}_{K^{\\sigma}}}\\leq1}\\!\\underbrace{\\big(y(\\pmb{x}_{\\diamond})-\\mu_{\\star}^{y}(\\pmb{x}_{\\diamond})\\big)^{2}}_{\\mathrm{error~of\\,posterior\\,mean}}=\\underbrace{K_{\\star}(\\pmb{x}_{\\diamond},\\pmb{x}_{\\diamond})+\\sigma^{2}}_{\\mathrm{predictive\\,variance}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If $\\sigma^{2}=0$ , then the above also holds for $\\pmb{x}\\in\\pmb{x}_{j}$ ", "page_idx": 15}, {"type": "text", "text": "Proof. See Proposition 3.8 of Kanagawa et al. [60]. ", "page_idx": 15}, {"type": "text", "text": "CaGP admits precisely the same guarantee just with the approximate posterior mean and covariance function. The fact that the impact of the approximation on the posterior mean is exactly captured by the approximate predictive variance function is what is meant by the method being computationaware. ", "page_idx": 15}, {"type": "text", "text": "Theorem S2 (Worst Case Error Interpretation of CaGP Variance [19]) ", "page_idx": 15}, {"type": "text", "text": "Given a set of training inputs ${\\pmb x}_{1},\\ldots,{\\pmb x}_{n}\\,\\in\\,\\mathbb{X}$ , the CaGP posterior $\\mathcal G\\mathcal P(\\mu_{i},K_{i})$ satisfies for any $\\pmb{x}\\neq\\pmb{x}_{j}$ that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{y\\in\\mathbb{H}_{K^{\\sigma}},\\|y\\|_{\\mathbb{H}_{K^{\\sigma}}}\\leq1}\\underset{\\mathrm{error~of~approximate~posterior~mean}}{\\underbrace{(y(x_{\\diamond})-\\mu_{i}^{y}(x_{\\diamond}))^{2}}}=\\underset{\\mathrm{approximate~predictive~variar}}{\\underbrace{K_{i}(x_{\\diamond},x_{\\diamond})+\\sigma^{2}}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$I f\\,\\sigma^{2}=0,$ , then the above also holds for $\\mathbf{\\boldsymbol{x}}\\in\\mathbf{\\boldsymbol{x}}_{j}$ ", "page_idx": 15}, {"type": "text", "text": "Proof. See Theorem 2 of Wenger et al. [19]. ", "page_idx": 15}, {"type": "text", "text": "While SVGP also admits a decomposition of its approximate predictive variance into two (relative) worst-case errors, neither of these is the error we care about, namely the difference between the data-generating function $y\\in\\mathbb{H}_{K^{\\sigma}}$ and the approximate posterior mean $\\mu_{\\mathrm{SVGP}}(x_{\\diamond})$ . It only involves a worst-case error term over the unit ball in the RKHS of the approximate kernel $Q^{\\sigma}\\approx K^{\\sigma}$ . ", "page_idx": 15}, {"type": "text", "text": "Theorem S3 (Worst Case Error Interpretation of SVGP Variance [61]) ", "page_idx": 15}, {"type": "text", "text": "Given a set of training inputs ${\\pmb x}_{1},\\ldots,{\\pmb x}_{n}\\in\\mathbb{X}$ and (fixed) inducing points $\\boldsymbol{Z}\\in\\mathbb{R}^{m\\times d}$ , the optimal variational posterior $\\mathcal{G P}(\\mu_{\\mathrm{SVGP}}^{\\star},K_{\\mathrm{SVGP}}^{\\star})$ of SVGP is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{S\\mathrm{SUP}}^{\\star,y}(x_{\\diamond})=K(x_{\\diamond},Z)(\\sigma^{2}K(Z,Z)+K(Z,X)K(X,Z))^{-1}K(Z,X)y(X)\\qquad\\qquad\\qquad}\\\\ {\\Upsilon_{\\mathrm{SVGP}}^{\\star}(x_{\\diamond},x_{\\diamond}^{\\prime})=K(x_{\\diamond},x_{\\diamond}^{\\prime})-Q(x_{\\diamond},x_{\\diamond}^{\\prime})+K(x_{\\diamond},Z)(K(Z,Z)+\\sigma^{-2}K(Z,X)K(X,Z))^{-1}K(Z,X)\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $Q(\\pmb{x},\\pmb{x}^{\\prime})=K(\\pmb{x},\\pmb{Z})K(\\pmb{Z},\\pmb{Z})^{-1}K(\\pmb{Z},\\pmb{x}^{\\prime})$ is the Nystr\u00a8om approximation of the covariance function $K(x,x^{\\prime})$ (see Eqns. (25) and (26) of Wild et al. $I6I J$ . The optimized SVGP posterior ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{sup}_{y\\in\\mathbb{H}_{Q^{\\sigma}},\\|h\\|_{\\mathbb{H}_{Q^{\\sigma}}}\\leq1}}}\\\\ &{}&{+\\operatorname*{sup}_{\\substack{f\\in\\mathbb{H}_{K},\\|f\\|_{\\mathbb{H}_{K}}\\leq1\\,\\underbar{\\operatorname{sup}}\\mathrm{~of~exact~posterior~mean~assuming~}y(\\cdot)\\,\\mathrm{is~in~the~RKHS~of~the~approxin}}}\\\\ &{}&{+\\operatorname*{sup}_{\\substack{f\\in\\mathbb{H}_{K},\\|f\\|_{\\mathbb{H}_{K}}\\leq1\\,\\underbar{\\operatorname{tr}}_{\\mathrm{errof~exact~posterior~mean~given~noise-free~obsersations~at~inducing}}}}\\\\ &{}&{=\\,\\underbar{K_{\\mathrm{SVGP}}^{\\star}}(x_{\\diamond},\\mathbf{x}_{\\diamond})+\\sigma^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If $\\sigma^{2}=0$ , then the above also holds for $\\mathbf{\\boldsymbol{x}}\\in\\mathbf{\\boldsymbol{x}}_{j}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. See Theorem 6 of Wild et al. [61]. ", "page_idx": 16}, {"type": "text", "text": "S1.3 CaGP\u2019s Variance Decreases Monotonically as the Number of Iterations Increases ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proposition S1 (CaGP\u2019s Variance Decreases Monotonically with the Number of Iterations) Given a training dataset of size $n$ , let $\\mathcal G\\mathcal P(\\mu_{i},K_{i})$ be the corresponding CaGP posterior defined in Equation (5) where $i\\leq n$ denotes the downdate rank / number of iterations and assume the CaGP actions $S_{i}\\,\\in\\,\\mathbb{R}^{n\\times i}$ are linearly independent. Then it holds for arbitrary $\\mathbf{\\boldsymbol{x}}_{\\odot}\\in\\mathbb{X}$ and $i\\leq j\\leq n,$ , that ", "page_idx": 16}, {"type": "equation", "text": "$$\nK_{i}(\\pmb{x}_{\\diamond},\\pmb{x}_{\\diamond})\\geq K_{j}(\\pmb{x}_{\\diamond},\\pmb{x}_{\\diamond})\\geq K_{n}(\\pmb{x}_{\\diamond},\\pmb{x}_{\\diamond})=K_{\\star}(\\pmb{x}_{\\diamond},\\pmb{x}_{\\diamond})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $K_{\\star}(x_{\\diamond},\\pmb{x}_{\\diamond})$ is the variance of the exact $G P$ posterior in Equation (1). ", "page_idx": 16}, {"type": "text", "text": "Proof. Wenger et al. [19] originally defined the approximate precision matrix $\\mathbf{{C}}_{i}=$ $\\begin{array}{r}{\\sum_{\\ell=1}^{i}\\frac{1}{\\eta_{\\ell}}\\pmb{d}_{\\ell}\\bar{\\pmb{d}}_{\\ell}^{\\top}=\\sum_{\\ell=1}^{i}\\tilde{\\pmb{d}}_{\\ell}\\tilde{\\pmb{d}}_{\\ell}^{\\top}}\\end{array}$ as a sum of rank-1 matrices and show that this definition is equivalent to the batch form $C_{i}=S_{i}(S_{i}^{\\top}\\hat{K}S_{i})^{-1}S_{i}^{\\top}$ we use in this work [see Lemma S1, Eqn. (S37) in 19]. Therefore we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K_{i}(x_{\\circ},x_{\\circ})=K(x_{\\circ},x_{\\circ})-K(x_{\\circ},X)C_{i}K(X,x_{\\circ})}\\\\ &{\\phantom{=}=K(x_{\\circ},x_{\\circ})\\phantom{-\\sum_{i}}K(x_{\\circ},X)\\tilde{d}_{i}\\tilde{d}_{i}^{\\intercal}K(X,x_{\\circ})}\\\\ &{\\phantom{=}\\,}\\\\ &{=K(x_{\\circ},x_{\\circ})\\phantom{-\\sum_{i}}\\sum_{\\ell=1}^{i}(K(x_{\\circ},X)\\tilde{d}_{\\ell})^{2}}\\\\ &{\\phantom{=}\\kappa(x_{\\circ},x_{\\circ})\\phantom{-\\sum_{i}}\\sum_{\\ell=1}^{i}(K(x_{\\circ},X)\\tilde{d}_{\\ell})^{2}}\\\\ &{\\geq K(x_{\\circ},x_{\\circ})\\phantom{-\\sum_{i}}\\sum_{\\ell=1}^{i}(K(x_{\\circ},X)\\tilde{d}_{\\ell})^{2}\\phantom{-\\sum_{i}}\\mathrm{\\quad~since~}i\\leq j}\\\\ &{\\geq K(x_{\\circ},x_{\\circ})\\phantom{-\\sum_{i}}\\sum_{\\ell=1}^{n}(K(x_{\\circ},X)\\tilde{d}_{\\ell})^{2}}\\\\ &{=K(x_{\\circ},x_{\\circ})\\phantom{-\\sum_{i}}K(x_{\\circ},X)C_{n}K(X,x_{\\circ})}\\\\ &{=K(x_{\\circ},x_{\\circ})\\phantom{-\\sum_{i}}K(x_{\\circ},X)C_{n}\\,K(X,x_{\\circ})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last equality follows from the fact that $S_{n}\\in\\mathbb{R}^{n\\times n}$ has rank $n$ and therefore ", "page_idx": 16}, {"type": "equation", "text": "$$\nC_{n}=S_{n}(S_{n}^{\\top}\\hat{K}S_{n})^{-1}S_{n}^{\\top}=S_{n}S_{n}^{-1}\\hat{K}^{-1}(S_{n}S_{n}^{-1})^{\\top}=\\hat{K}^{-1}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "S2 Training Losses ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "S2.1 Projected-Data Log-Marginal Likelihood ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma S2 (Projected-Data Log-Marginal Likelihood) Under the assumptions of Lemma $_{S I}$ , the projected-data log-marginal likelihood is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{proj}}^{\\mathrm{NLL}}(\\pmb{\\theta})=-\\log p(\\tilde{\\pmb{y}}\\mid\\pmb{\\theta})=-\\log\\mathcal{N}\\Big(\\tilde{\\pmb{y}};S_{i}^{\\prime\\top}\\pmb{\\mu},S_{i}^{\\prime\\top}\\hat{K}S_{i}^{\\prime}\\Big)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n=\\frac{1}{2}\\big((\\boldsymbol{y}-\\boldsymbol{\\mu})^{\\top}\\boldsymbol{S}_{i}(S_{i}^{\\top}\\hat{\\boldsymbol{K}}\\boldsymbol{S}_{i})^{-1}S_{i}^{\\top}(\\boldsymbol{y}-\\boldsymbol{\\mu})+\\log\\operatorname*{det}(S_{i}^{\\top}\\hat{\\boldsymbol{K}}\\boldsymbol{S}_{i})-\\log\\operatorname*{det}(S_{i}^{\\top}S_{i})+i\\log(2\\pi)\\big)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. By the same argument as in Lemma S1 we obtain that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{\\mathrm{proj}}^{\\mathrm{NLL}}(\\pmb\\theta)=-\\log p(\\tilde{y}\\mid\\pmb\\theta)=-\\log N\\Big(\\tilde{y};S_{i}^{\\prime^{\\top}}\\pmb\\mu,S_{i}^{\\prime^{\\top}}\\hat{K}S_{i}^{\\prime^{\\prime}}\\Big)}\\\\ &{\\qquad\\qquad=\\frac12\\big((S_{i}^{\\prime^{\\top}}\\pmb y-S_{i}^{\\prime^{\\top}}\\pmb\\mu)^{\\top}(S_{i}^{\\prime^{\\top}}\\hat{K}S_{i}^{\\prime})^{-1}(S_{i}^{\\prime^{\\top}}\\pmb y-S_{i}^{\\prime^{\\top}}\\pmb\\mu)+\\log\\operatorname*{det}(S_{i}^{\\prime^{\\top}}\\hat{K}S_{i}^{\\prime})+i\\log(2\\pi)\\big)}\\\\ &{\\qquad\\qquad=\\frac12\\big((\\pmb y-\\pmb\\mu)^{\\top}S_{i}^{\\prime}(S_{i}^{\\prime^{\\top}}\\hat{K}S_{i}^{\\prime})^{-1}S_{i}^{\\prime^{\\top}}(\\pmb y-\\pmb\\mu)+\\log\\operatorname*{det}(S_{i}^{\\prime^{\\top}}\\hat{K}S_{i}^{\\prime})+i\\log(2\\pi)\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $S_{i}^{\\prime}=S_{i}L^{-\\top}$ , where $L^{-\\top}=\\operatorname{chol}(S_{i}^{\\top}S_{i})^{-\\top}$ is the orthonormalizing matrix, $L^{-\\top}$ cancels in the quadratic loss term, giving ", "page_idx": 17}, {"type": "equation", "text": "$$\n={\\frac{1}{2}}{\\big(}({\\boldsymbol{y}}-{\\boldsymbol{\\mu}})^{\\mathsf{T}}{\\boldsymbol{S}}_{i}({\\boldsymbol{S}}_{i}^{\\mathsf{T}}{\\hat{\\boldsymbol{K}}}{\\boldsymbol{S}}_{i})^{-1}{\\boldsymbol{S}}_{i}^{\\mathsf{T}}({\\boldsymbol{y}}-{\\boldsymbol{\\mu}})+\\log\\operatorname*{det}({\\boldsymbol{S}}_{i}^{\\prime}{\\mathsf{T}}{\\hat{\\boldsymbol{K}}}{\\boldsymbol{S}}_{i}^{\\prime})+i\\log(2\\pi){\\big)}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and finally we can decompose the log-determinant into a difference of log-determinants ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\cfrac{1}{2}\\big((\\boldsymbol{y}-\\boldsymbol{\\mu})^{\\top}\\boldsymbol{S}_{i}(\\boldsymbol{S}_{i}^{\\top}\\hat{\\boldsymbol{K}}\\boldsymbol{S}_{i})^{-1}\\boldsymbol{S}_{i}^{\\top}(\\boldsymbol{y}-\\boldsymbol{\\mu})+\\log\\operatorname*{det}(\\boldsymbol{S}_{i}^{\\top}\\hat{\\boldsymbol{K}}\\boldsymbol{S}_{i})-2\\log\\operatorname*{det}(\\boldsymbol{L})+i\\log(2\\pi)\\big)}\\\\ &{=\\cfrac{1}{2}\\big((\\boldsymbol{y}-\\boldsymbol{\\mu})^{\\top}\\boldsymbol{S}_{i}(\\boldsymbol{S}_{i}^{\\top}\\hat{\\boldsymbol{K}}\\boldsymbol{S}_{i})^{-1}\\boldsymbol{S}_{i}^{\\top}(\\boldsymbol{y}-\\boldsymbol{\\mu})+\\log\\operatorname*{det}(\\boldsymbol{S}_{i}^{\\top}\\hat{\\boldsymbol{K}}\\boldsymbol{S}_{i})-\\log\\operatorname*{det}(\\boldsymbol{L}\\boldsymbol{L}^{\\top})+i\\log(2\\pi)\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which using $L L^{\\top}=S_{i}^{\\top}S_{i}$ completes the proof. ", "page_idx": 17}, {"type": "text", "text": "S2.2 Evidence Lower Bound (ELBO) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma S3 (Evidence Lower Bound Training Loss) Define the variational family ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{Q}:=\\left\\{q(\\pmb{f})=\\mathcal{N}(\\pmb{f};\\mu_{i}(\\pmb{X}),K_{i}(\\pmb{X},\\pmb{X}))\\mid\\pmb{S}\\in\\mathbb{R}^{n\\times i}\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "then the evidence lower bound (ELBO) is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{\\mathrm{CaGP}}^{\\mathrm{ELBO}}(\\theta)=-\\log p(\\boldsymbol{y}\\mid\\theta)+\\mathrm{KL}(q(\\boldsymbol{f})\\mid\\mid p(\\boldsymbol{f}\\mid\\boldsymbol{y}))}\\\\ &{\\quad\\quad\\quad=-\\mathbb{E}_{q}(\\log p(\\boldsymbol{y}\\mid\\boldsymbol{f}))+\\mathrm{KL}(q(\\boldsymbol{f})\\mid\\mid p(\\boldsymbol{f}))}\\\\ &{\\quad\\quad\\quad\\quad=\\frac{1}{2}\\bigg(\\frac{1}{\\sigma^{2}}\\Big(\\|\\boldsymbol{y}-\\boldsymbol{\\mu}_{i}(\\boldsymbol{X})\\|_{2}^{2}+\\displaystyle\\sum_{j=1}^{n}K_{i}(\\boldsymbol{x}_{j},\\boldsymbol{x}_{j})\\Big)+(n-i)\\log(\\sigma^{2})+n\\log(2\\pi)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\tilde{v}_{i}^{\\top}S^{\\top}K S\\tilde{v}_{i}-\\mathrm{tr}\\big((S^{\\top}\\hat{K}S)^{-1}S^{\\top}K S\\big)+\\log\\operatorname*{det}(S^{\\top}\\hat{K}S)-\\log\\operatorname*{det}(S^{\\top}S)\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\tilde{\\pmb{v}}_{i}=(\\pmb{S}^{\\top}\\hat{K}\\pmb{S})^{-1}\\pmb{S}^{\\top}(\\pmb{y}-\\pmb{\\mu})$ are the \u201cprojected\u201d representer weights. ", "page_idx": 17}, {"type": "text", "text": "Proof. The ELBO is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n-\\ell_{\\mathrm{CaGP}}^{\\mathrm{ELBO}}(\\pmb\\theta)=\\mathbb{E}_{q}(\\log p(\\pmb{y}\\mid\\pmb{f}))-\\mathrm{KL}(q(\\pmb{f})\\mid\\mid p(\\pmb{f}))\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We first compute the expected log-likelihood term. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{q}(\\log p(\\boldsymbol{y}\\mid\\boldsymbol{f}))=\\mathbb{E}_{q}\\biggl(-\\frac{1}{2}\\left(\\frac{1}{\\sigma^{2}}(\\boldsymbol{y}-\\boldsymbol{f})^{\\top}(\\boldsymbol{y}-\\boldsymbol{f})+\\log\\operatorname*{det}(\\sigma^{2}I_{n\\times n})+n\\log(2\\pi)\\right)\\biggr)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=-\\frac{1}{2}\\left(\\frac{1}{\\sigma^{2}}\\mathbb{E}_{q}\\bigl((\\boldsymbol{y}-\\boldsymbol{f})^{\\top}(\\boldsymbol{y}-\\boldsymbol{f})\\bigr)+n\\log(\\sigma^{2})+n\\log(2\\pi)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now using $\\mathbb{E}\\!\\left(x^{\\top}A x\\right)=\\mathbb{E}\\!\\left(x\\right)^{\\top}A\\mathbb{E}\\!\\left(x\\right)+\\operatorname{tr}\\!\\left(A\\operatorname{Cov}\\!\\left(x\\right)\\right)$ , we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n=-\\frac{1}{2}\\left(\\frac{1}{\\sigma^{2}}\\bigg(\\|\\pmb{y}-\\mu_{i}(\\pmb{X})\\|_{2}^{2}+\\operatorname{tr}(K_{i}(\\pmb{X},\\pmb{X}))\\bigg)+n\\log(\\sigma^{2})+n\\log(2\\pi)\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n=-\\frac{1}{2}\\left(\\frac{1}{\\sigma^{2}}\\bigg(\\|\\pmb{y}-\\mu_{i}(\\pmb{X})\\|_{2}^{2}+\\sum_{j=1}^{n}K_{i}(\\pmb{x}_{j},\\pmb{x}_{j})\\bigg)+n\\log(\\sigma^{2})+n\\log(2\\pi)\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since both $q(f)$ and the prior $p(f)$ are Gaussian, the KL divergence term between them is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{d}_{\\mathbb{Z}}(q(f)\\parallel p(f))=\\frac{1}{2}\\Bigg((\\mu_{i}(X)-\\mu(X))^{\\top}K^{-1}(\\mu_{i}(X)-\\mu(X))+\\log\\bigg(\\frac{\\mathrm{det}(X)}{\\mathrm{det}(K_{i}(X,X))}\\bigg)\\Bigg)}&{}\\\\ {+\\mathrm{tr}(K^{-1}K_{i}(X,X))-n\\Bigg)}&{}\\\\ {=\\frac{1}{2}\\Bigg((K C_{i}(y-\\mu(X)))^{\\top}K^{-1}K C_{i}(y-\\mu(X))-\\log\\mathrm{det}(K^{-1}K_{i}(X,X))}\\\\ {+\\mathrm{tr}(I_{n\\setminus n}-C_{i}K)-n\\Bigg)}&{}\\\\ {=\\frac{1}{2}\\Bigg((y-\\mu(X))^{\\top}C_{i}K C_{i}(y-\\mu(X))-\\log\\mathrm{det}(I_{n\\setminus n}-C_{i}K)+\\mathrm{tr}(I_{n\\setminus n}-C_{i}K)}\\\\ {=\\frac{1}{2}\\Bigg(\\hat{v}_{i}^{\\top}S^{\\top}K S_{i}-\\log\\mathrm{det}(I_{n\\setminus n}-C_{i}K)+\\mathrm{tr}(I_{n\\setminus n}-C_{i}K)-n\\Bigg)}\\\\ {=\\frac{1}{2}\\Bigg(\\hat{v}_{i}^{\\top}S^{\\top}K S_{i}-\\log\\mathrm{det}(I_{n\\setminus n}-C_{i}K)-\\mathrm{tr}((S^{\\top}K)^{-1}S^{\\top}K S)\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, we use the matrix determinant lemma $\\operatorname*{det}(A+U V^{\\top})=\\operatorname*{det}(I_{m}+V^{\\top}A^{-1}U)\\operatorname*{det}(A);$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\frac{1}{2}\\Big(\\tilde{v}_{i}^{\\top}S^{\\top}K S\\tilde{v}_{i}-\\log\\operatorname*{det}(I_{i\\setminus i}-(S^{\\top}\\hat{K}S)^{-1}S^{\\top}K S)-\\mathrm{tr}((S^{\\top}\\hat{K}S)^{-1}S^{\\top}K S)\\Big)}\\\\ &{=\\frac{1}{2}\\bigg(\\tilde{v}_{i}^{\\top}S^{\\top}K S\\tilde{v}_{i}-\\log\\operatorname*{det}((S^{\\top}\\hat{K}S)^{-1}(S^{\\top}\\hat{K}S-S^{\\top}K S))-\\mathrm{tr}((S^{\\top}\\hat{K}S)^{-1}S^{\\top}K S)\\bigg)}\\\\ &{=\\frac{1}{2}\\bigg(\\tilde{v}_{i}^{\\top}S^{\\top}K S\\tilde{v}_{i}-\\log\\operatorname*{det}((S^{\\top}\\hat{K}S)^{-1}(\\sigma^{2}S^{\\top}S))-\\mathrm{tr}((S^{\\top}\\hat{K}S)^{-1}S^{\\top}K S)\\bigg)}\\\\ &{=\\frac{1}{2}\\bigg(\\tilde{v}_{i}^{\\top}S^{\\top}K S\\tilde{v}_{i}+\\log\\operatorname*{det}(S^{\\top}\\hat{K}S)-\\log\\operatorname*{det}(\\sigma^{2}S^{\\top}S)-\\mathrm{tr}((S^{\\top}\\hat{K}S)^{-1}S^{\\top}K S)\\bigg)}\\\\ &{=\\frac{1}{2}\\bigg(\\tilde{v}_{i}^{\\top}S^{\\top}K S\\tilde{v}_{i}+\\log\\operatorname*{det}(S^{\\top}\\hat{K}S)-\\log\\operatorname*{det}(\\sigma^{2}S^{\\top}S)-\\mathrm{tr}((S^{\\top}\\hat{K}S)^{-1}S^{\\top}K S)\\bigg)}\\\\ &{=\\frac{1}{2}\\bigg(\\tilde{v}_{i}^{\\top}S^{\\top}K S\\tilde{v}_{i}+\\log\\operatorname*{det}(S^{\\top}\\hat{K}S)-i\\log(\\sigma^{2})-\\log\\operatorname*{det}(S^{\\top}S)-\\mathrm{tr}((S^{\\top}\\hat{K}S)^{-1}S^{\\top}K S)\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "image", "img_path": "tDvFa5OJyS/tmp/a1c2315d2ecc0f8ca5f25a1bada10547555caeae8be8c31b874aeaed1e3cfc60.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure S1: Comparison of two different training losses for CaGP. The naive choice of the projecteddata log-marginal likelihood leads to increasingly worse generalization performance as measured by NLL. In comparison, the ELBO training loss leads to much better performance. ", "page_idx": 19}, {"type": "image", "img_path": "tDvFa5OJyS/tmp/950c4880e1b3b2dff4dd965aad0a7cb1410aef755ddb357f4396a37327ad4840.jpg", "img_caption": ["Figure S2: CaGP predictive distributions with hyperparameters optimized using different losses. When optimizing hyperparameters with respect to the projected-data log-marginal likelihood, CaGP-CG completely overestimates the noise scale, which leads to increasingly worse generalization performance. In comparison, the ELBO training loss leads to a much better overall fit. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "S3 Choice of Actions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We begin by proving that the CaGP posterior in Equation (5) is uniquely defined by the space spanned by the columns of the actions $\\mathrm{colsp}(S)$ , rather than the specific choice of the matrix $\\boldsymbol{S}$ . ", "page_idx": 19}, {"type": "text", "text": "Lemma S4 (The CaGP Posterior Is Uniquely Defined by the Column Space of the Actions) Let $\\boldsymbol{S}$ , $S^{\\prime}\\in\\stackrel{\\cdot}{\\mathbb{R}}^{n\\times i}$ be two action matrices, each of which consists of non-zero and linearly independent action vectors, such that their column spaces are identical, i.e. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{colsp}(S)=\\mathrm{colsp}(S^{\\prime}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "then the corresponding CaGP posteriors $\\mathcal G\\mathcal P(\\mu_{i},K_{i})$ and $\\mathcal{G P}(\\mu_{i}^{\\prime},K_{i}^{\\prime})$ are equivalent. ", "page_idx": 19}, {"type": "text", "text": "Proof. By assumption (S20) there exists $W\\,\\in\\,\\mathbb{R}^{i\\times i}$ such that ${\\cal S}^{'}\\,=\\,S W$ . Since action vectors are assumed to be linearly independent and non-zero, it holds that $i=\\mathrm{rank}({\\cal S}^{\\prime})=\\mathrm{rank}({\\cal S}W)=$ $\\mathrm{rank}(W)$ , where the last equality follows from standard properties of the matrix rank. Therefore $W$ is invertible, and we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\nC^{\\prime}=S^{^{\\prime}}({S^{^{\\prime}}}^{^{\\mathsf{T}}}\\hat{K}S^{^{\\prime}})^{-1}{S^{^{\\prime}}}^{^{\\mathsf{T}}}=S W(W^{\\mathsf{T}}S^{^{\\mathsf{T}}}\\hat{K}S W)^{-1}W^{\\mathsf{T}}S^{^{\\mathsf{T}}}=S(S^{\\mathsf{T}}\\hat{K}S)^{-1}S^{^{\\mathsf{T}}}=C.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since the CaGP posterior in Equation (5) is fully defined via the approximate precision matrix $_{C}$ , the desired result follows. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Corollary S1 (Action Order and Magnitude Does Not Change CaGP Posterior) The CaGP posterior in Equation (5) is invariant under permutation and rescaling of the actions. ", "page_idx": 20}, {"type": "text", "text": "Proof. This follows immediately by choosing a permutation or a diagonal matrix $W$ , respectively, such that $S^{\\prime}=S W$ in Lemma S4. ", "page_idx": 20}, {"type": "text", "text": "S3.1 (Conjugate) Gradient / Residual Policy ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Consider the following linear system ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{K}v_{\\star}=y-\\mu\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with symmetric positive definite kernel matrix $\\hat{K}=K\\!+\\!\\sigma^{2}I$ , observations $\\textit{\\textbf{y}}$ , prior mean evaluated at the data $\\pmb{\\mu}=\\mu(\\pmb{X})$ and representer weights $\\pmb{v_{\\star}}$ . ", "page_idx": 20}, {"type": "text", "text": "Lanczos process [23] The Lanczos process is an iterative method, which computes approximate eigenvalues $\\hat{\\mathbf{A}}=\\mathrm{diag}(\\hat{\\lambda}_{1},\\dots,\\hat{\\lambda}_{i})\\in\\bar{\\mathbb{R}}^{i\\times i}$ and approximate eigenvectors $\\hat{U}=(\\hat{\\pmb u}_{1}\\cdot\\cdot\\cdot\\hat{\\pmb u}_{i})\\in\\mathbb{R}^{n\\times i}$ for a symmetric positive definite matrix $\\hat{K}$ by repeated matrix-vector multiplication. Given an arbitrary starting vector $q_{1}\\in\\mathbb{R}^{n}$ , s.t. $\\|\\pmb q_{1}\\|_{2}=1$ , it returns $i$ orthonormal vectors $Q=(\\pmb{q}_{1}\\cdot\\cdot\\cdot\\pmb{q}_{i})\\in$ $\\mathbb{R}^{n\\times i}$ and a tridiagonal matrix $\\pmb{T}\\:=\\:\\pmb{Q}^{\\top}\\hat{\\pmb{K}}\\pmb{Q}\\:\\in\\:\\mathbb{R}^{i\\times i}$ . The eigenvalue approximations are given by an eigendecomposition of $\\pmb{T}=\\pmb{W}\\hat{\\pmb{\\Lambda}}\\pmb{W}^{\\top}$ , where $W\\in\\mathbb{R}^{i\\times i}$ orthonormal, and the eigenvector approximations are then given by $\\hat{U}=Q W\\in\\mathbb{R}^{n\\times i}$ [e.g. Sec. 10.1.4 of 62]. ", "page_idx": 20}, {"type": "text", "text": "Conjugate Gradient Method [24] The conjugate gradient method is an iterative method to solve linear systems with symmetric positive definite system matrix by repeated matrix-vector multiplication. When applied to Equation (S21), it produces a sequence of representer weights approximations $\\pmb{v}_{i}\\approx\\pmb{v}_{\\star}=\\hat{\\hat{\\cal K}}^{-1}(\\pmb{y}-\\pmb{\\mu})$ . Its residuals $\\pmb{r}_{i}=\\pmb{y}-\\pmb{\\mu}-\\hat{K}\\pmb{v}_{i}$ are proportional to the Lanczos vectors for a Lanczos process initialized at $\\begin{array}{r}{q_{1}\\,=\\,\\frac{r_{0}}{\\|r_{0}\\|_{2}}}\\end{array}$ , i.e. $Q=R D$ where $\\pmb{R}\\in\\mathbb{R}^{n\\times i}$ is the matrix of residuals and $D\\in\\mathbb{R}^{i\\times i}$ a diagonal matrix (e.g. [Alg. 11.3.2 in 62] or [Sec. 3 & Eqn. (3.4) of 63]). ", "page_idx": 20}, {"type": "text", "text": "Therefore choosing actions defined by the residuals of CG in CaGP-CG, i.e. $S=R$ , is equivalent to choosing actions ${\\cal S}^{\\prime}\\,=\\,\\hat{U}$ given by the eigenvector approximations computed by the Lanczos process initialized as above, since ", "page_idx": 20}, {"type": "text", "text": "$\\operatorname{colsp}(S)=\\operatorname{colsp}(R)=\\operatorname{colsp}(R D)=\\operatorname{colsp}(Q)=\\operatorname{colsp}(Q W)=\\operatorname{colsp}({\\hat{U}})=\\operatorname{colsp}(S^{\\prime})$ and by Lemma S4 it holds that the corresponding CaGP posteriors with actions $\\boldsymbol{S}$ and $S^{\\prime}$ are equivalent. ", "page_idx": 20}, {"type": "text", "text": "S3.2 Information-theoretic Policy ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In information-theoretic formulations of active learning, new data is selected to minimize uncertainty about a set of latent variables $_{\\textit{z}}$ . In other words, we would aim to minimize the entropy of the posterior $\\begin{array}{r}{\\mathrm{H}_{p(z\\mid X)}(z)=-\\int\\log p(z\\mid X)p(z\\mid X)\\,d z}\\end{array}$ as a function of the data $\\mathbf{\\deltaX}$ [21]. In analogy to active learning, in our setting we propose to perform computations $\\pmb{y}\\mapsto\\pmb{S}_{i}^{\\top}\\pmb{y}$ to maximally reduce uncertainty about the latent function $f(\\boldsymbol X)$ evaluated at the training data. ", "page_idx": 20}, {"type": "text", "text": "Lemma S5 (Information-theoretic Policy) ", "page_idx": 20}, {"type": "text", "text": "The actions $\\boldsymbol{S}$ minimizing the entropy of the computation-aware posterior $p(f(X)\\mid S^{\\top}y)$ at the training data, or equivalently the actions maximizing the mutual information between $f(X)$ and the projected data $S^{\\top}{\\boldsymbol{y}}$ , are given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big(s_{1},\\ldots,s_{i}\\big)=\\underset{S\\in\\mathbb{R}^{n\\times i}}{\\arg\\operatorname*{min}}\\,\\mathrm{H}_{p(f(\\mathbf{X})\\mid S^{\\top}\\!y)}\\big(f(\\mathbf{X})\\big)}\\\\ &{\\qquad\\qquad=\\underset{S\\in\\mathbb{R}^{n\\times i}}{\\arg\\operatorname*{max}}\\,\\mathrm{H}\\big(f(\\mathbf{X})\\big)-\\mathrm{H}\\big(f(\\mathbf{X})\\mid S^{\\top}\\!y\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Let $\\tilde{\\pmb{y}}:=\\pmb{S}^{\\top}\\pmb{y}$ and $f:=f(X)$ . By assumption, we have that $\\pmb{f}\\sim\\mathcal{N}(\\pmb{\\mu},\\pmb{K})$ . Recall that the entropy of a Gaussian random vector $\\pmb{f}\\,\\sim\\mathcal{N}(\\pmb{m},\\pmb{S})$ is given by $\\mathrm{H}(f)\\;=\\;{\\textstyle\\frac{1}{2}}\\bigl(\\log\\operatorname*{det}(S)+$ $n\\log(2\\pi e))$ . Now since the covariance function of the computation-aware posterior in Equation (5) does not depend on the targets $\\textit{\\textbf{y}}$ , neither does its entropy $\\mathrm{H}_{p(f\\mid S^{\\top}y)}(f)$ . ", "page_idx": 21}, {"type": "text", "text": "Therefore, by definition of the conditional entropy and using the law of the unconscious statistician, it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{H}(\\pmb{f}\\mid\\tilde{\\pmb{y}})=-\\int\\int\\log p(\\pmb{f}\\mid\\tilde{\\pmb{y}})p(\\pmb{f}\\mid\\tilde{\\pmb{y}})p(\\tilde{\\pmb{y}})\\,d\\pmb{f}\\,d\\tilde{\\pmb{y}}}}\\\\ &{=\\mathbb{E}_{p(\\tilde{\\pmb{y}})}\\big(\\mathrm{H}_{p(\\pmb{f}\\mid\\tilde{\\pmb{y}})}(\\pmb{f})\\big)}\\\\ &{=\\mathbb{E}_{p(\\pmb{y})}\\big(\\mathrm{H}_{p(\\pmb{f}\\mid\\pmb{S}^{\\intercal}\\pmb{y})}(\\pmb{f})\\big)}\\\\ &{=\\mathrm{H}_{p(\\pmb{f}\\mid\\pmb{S}^{\\intercal}\\pmb{y})}(\\pmb{f})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore we can rewrite the mutual information in terms of prior and posterior entropy, such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{H}(f)-\\mathrm{H}\\big(f\\mid S^{\\top}y\\big)=\\mathrm{H}(f)-\\mathrm{H}_{p(f\\mid S^{\\top}y)}(f)}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{2}\\big(\\log\\operatorname*{det}(K)+n\\log(2\\pi e)-\\log\\operatorname*{det}(K-K C_{i}K)-n\\log(2\\pi e)\\big)}\\\\ &{\\qquad\\qquad\\qquad=-\\frac{1}{2}\\log\\bigg(\\operatorname*{det}(K-K S(S^{\\top}\\hat{K}S)^{-1}S^{\\top}K)\\operatorname*{det}(K^{-1})\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Via the matrix determinant lemma $\\operatorname*{det}(A\\!+\\!U W V^{\\mathsf{T}})=\\operatorname*{det}(W^{-1}\\!+\\!V^{\\mathsf{T}}A^{-1}U)\\operatorname*{det}(W)\\operatorname*{det}(A),$ we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=-\\displaystyle\\frac{1}{2}\\log\\big(\\operatorname*{det}(-S^{\\boldsymbol{\\mathsf{T}}}\\boldsymbol{\\hat{K}}\\boldsymbol{S}+S K\\boldsymbol{K}^{-1}\\boldsymbol{K})\\operatorname*{det}(-(S^{\\boldsymbol{\\mathsf{T}}}\\boldsymbol{\\hat{K}}\\boldsymbol{S})^{-1})\\operatorname*{det}(\\boldsymbol{K})\\operatorname*{det}(\\boldsymbol{K}^{-1})\\Big)}\\\\ &{=-\\displaystyle\\frac{1}{2}\\log\\big(\\operatorname*{det}(S^{\\boldsymbol{\\mathsf{T}}}\\boldsymbol{K}\\boldsymbol{S}-S^{\\boldsymbol{\\mathsf{T}}}\\boldsymbol{\\hat{K}}\\boldsymbol{S})\\operatorname*{det}(-(S^{\\boldsymbol{\\mathsf{T}}}\\boldsymbol{\\hat{K}}\\boldsymbol{S})^{-1})\\big)}\\\\ &{=-\\displaystyle\\frac{1}{2}\\log\\operatorname*{det}(\\sigma^{2}S^{\\boldsymbol{\\mathsf{T}}}\\boldsymbol{S}(S^{\\boldsymbol{\\mathsf{T}}}\\boldsymbol{\\hat{K}}\\boldsymbol{S})^{-1})}\\\\ &{=\\displaystyle\\frac{1}{2}\\log\\operatorname*{det}(\\sigma^{-2}(S^{\\boldsymbol{\\mathsf{T}}}\\boldsymbol{S})^{-1}S^{\\boldsymbol{\\mathsf{T}}}\\boldsymbol{\\hat{K}}\\boldsymbol{S})}\\\\ &{=\\displaystyle\\frac{1}{2}\\big(\\log\\operatorname*{det}((S^{\\boldsymbol{\\mathsf{T}}}\\boldsymbol{S})^{-1}S^{\\boldsymbol{\\mathsf{T}}}\\boldsymbol{\\hat{K}}\\boldsymbol{S})-i\\log(\\sigma^{2})\\big)}\\\\ &{=\\displaystyle\\frac{1}{2}\\big(\\log\\operatorname*{det}(L^{-\\boldsymbol{\\mathsf{T}}}S^{\\boldsymbol{\\mathsf{T}}}\\boldsymbol{\\hat{K}}\\boldsymbol{S}L^{-1})-i\\log(\\sigma^{2})\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for $\\textbf{\\emph{L}}$ a square root of $S^{\\top}S$ . Now we can upper bound the above as follows ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{S\\in\\mathbb{R}^{n\\times i}}{\\operatorname*{max}}\\mathrm{H}(f)-\\mathrm{H}\\big(f\\mid S^{\\top}y\\big)\\leq\\underset{\\Tilde{S}\\in\\mathbb{R}^{n\\times i}}{\\operatorname*{max}}\\frac{1}{2}\\big(\\log\\operatorname*{det}(\\Tilde{S}^{\\top}\\hat{K}\\Tilde{S})-i\\log(\\sigma^{2})\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{1}{2}\\big(\\log\\operatorname*{det}(U\\hat{K}U^{\\top})-i\\log(\\sigma^{2})\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{1}{2}\\big(\\underset{j=1}{\\overset{i}{\\prod}}\\log(\\lambda_{j}(\\hat{K}))-i\\log(\\sigma^{2})\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $U$ are the orthonormal eigenvectors of K\u02c6 for the largest $i$ eigenvalues. Now choosing $S=U$ achieves the upper bound since $\\b{U}^{\\top}\\b{U}=\\b{I}$ and therefore $S_{i}=U$ is a solution to the optimization problem. ", "page_idx": 21}, {"type": "text", "text": "Finally using the argument above and since $\\operatorname{H}(f)$ does not depend on $\\boldsymbol{S}$ , we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,max}_{S\\in\\mathbb R^{n\\times i}}\\mathrm{H}(f)-\\mathrm{H}\\big(f\\mid S^{\\top}y\\big)=\\operatorname*{arg\\,max}_{S\\in\\mathbb R^{n\\times i}}\\mathrm{H}(f)-\\mathrm{H}_{p(f\\mid S^{\\top}y)}(f)=\\operatorname*{arg\\,min}_{S\\in\\mathbb R^{n\\times i}}\\mathrm{H}_{p(f\\mid S^{\\top}y)}(f)\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This proves the claim. ", "page_idx": 21}, {"type": "text", "text": "S4 Algorithms ", "text_level": 1, "page_idx": 22}, {"type": "table", "img_path": "tDvFa5OJyS/tmp/ebefe1f85f29373656af4f4fe753764416ebd7a32987b003e8ddfe755d3f92f3.jpg", "table_caption": ["S4.1 Iterative and Batch Versions of CaGP "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "tDvFa5OJyS/tmp/30b6f36dd2ee50cf830e3f3a75cc3fdb3728cbd2e1b35476475ab6ce65ef5853.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "S4.2 Implementation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We provide an open-source implementation of CaGP-Opt as part of GPyTorch. To install the package via pip, execute the following in the command line: ", "page_idx": 22}, {"type": "text", "text": "pip install git+https://github.com/cornellius-gp/linear_operator.git@sparsity pip install git+https://github.com/cornellius-gp/gpytorch.git@computation-aware-gps-v2 pip install pykeops ", "page_idx": 22}, {"type": "text", "text": "S5 Additional Experimental Results and Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "S5.1 Inducing Points Placement and Uncertainty Quantification of SVGP ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To better understand whether the overconfidence of SVGP at inducing points observed in the visualization in Figure 1 holds also in higher dimensions, we do the following experiment. For varying input dimension $d\\;\\in\\;\\{1,2,\\ldots,25\\}$ , we generate synthetic training data by sampling $n\\,=\\,500$ inputs $\\mathbf{\\deltaX}$ uniformly at random with corresponding targets sampled from a zero-mean Gaussian process $y\\sim{\\mathcal{G P}}(0,K^{\\sigma})$ , where $K^{\\sigma}(\\cdot,\\cdot)\\,=\\,\\dot{K}(\\cdot,\\cdot)\\,\\bar{+}\\,\\sigma^{\\bar{2}}\\delta(\\cdot,\\cdot)$ is given by the sum of a $\\mathrm{Matern(3/2)}$ and a white noise kernel with noise scale $\\sigma$ . We optimize the kernel hyperparameters, variational parameters and inducing points $m=64$ ) jointly for 300 epochs using Adam with a linear learning rate scheduler. At convergence we measure the average distance between inducing points and the nearest datapoint measured in lengthscale units, i.e. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\bar{d}_{l}(Z,X)=\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\operatorname*{min}_{j}\\lVert z_{i}-x_{j}\\rVert_{\\mathrm{diag}(l^{-2})}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $l\\in\\mathbb{R}^{d}$ is the vector of lengthscales (one per input dimension). We also compute the average ratio of the posterior variance to the predictive variance at the inducing points, i.e. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\bar{\\rho}(\\pmb{Z})=\\frac{1}{m}\\sum_{i=1}^{m}\\frac{K_{\\mathrm{posterior}}(z_{i},\\pmb{z}_{i})}{K_{\\mathrm{posterior}}(z_{i},\\pmb{z}_{i})+\\sigma^{2}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The results of our experiments are shown in Figure S3. We find that as expected the inducing points are optimized to lie closer to datapoints than points sampled uniformly at random. However, the inducing points lie increasingly far away from the training data as the dimension increases relative to the lengthscale that SVGP learns. Therefore this experiment suggests that the phenomenon observed in Figure 1, that SVGP can be overconfident at inducing points if they are far away from training datapoints, to be increasingly present as the input dimension increases. This is further substantiated by Figure S3(b) since the proportion of posterior variance to predictive variance at the inducing points is very small already in $d=4$ dimensions. This illustrates both SVGP\u2019s overconfidence at the inducing points (in particular in higher dimensions) and that its predictive variance is dominated by the learned observation noise, as we also saw in the illustrative Figure 1. ", "page_idx": 23}, {"type": "image", "img_path": "tDvFa5OJyS/tmp/a599fb73d31113c26df93df5fa4a757433daca78c60735e8fde551a281b5b227.jpg", "img_caption": ["(a) Average distance of inducing points to the nearest (b) Average ratio of posterior to predictive variance at datapoint measured in lengthscale units. SVGP\u2019s inducing point locations. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure S3: SVGP\u2019s inducing point placement and uncertainty in higher dimensions. (a) As the dimension increases, the inducing points SVGP learns lie increasingly far away from the data measured in lengthscale units given a fixed training data set size and number of inducing points. (b) SVGP\u2019s variance at the inducing points is dominated by the learned observational noise in higher dimensions, rather than by the posterior variance. The comparison to a CholeskyGP with the datagenerating hyperparameters shows that SVGP compensates for a lack of posterior variance at the inducing points by artificially inflating the observation noise. This illustrates both the overconfidence (in terms of posterior variance) of SVGP at the inducing points and its tendency to oversmooth. ", "page_idx": 23}, {"type": "text", "text": "S5.2 Grassman Distance Between Subspaces ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In Figure 2 we compute the distance between the subspaces spanned by random vectors, the actions $\\boldsymbol{S}$ of CaGP, and the space spanned by the top- $^{i}$ eigenvectors. The notion of subspace distance we use is the Grassman distance, i.e. for two subspaces spanned by the columns of matrices $A\\in\\mathbb{R}^{n\\times p}$ and $\\b{B}\\in\\mathbb{R}^{n\\times p}$ s.t. $p\\geq q$ the Grassman subspace distance is defined by ", "page_idx": 24}, {"type": "equation", "text": "$$\nd(A,B)=\\lVert\\pmb\\theta\\rVert_{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\pmb\\theta\\in\\mathbb{R}^{q}$ is the vector of principal angles between the two spaces, which can be computed via an SVD [e.g. Alg. 6.4.3 in 62]. ", "page_idx": 24}, {"type": "text", "text": "S5.3 Generalization Experiment ", "text_level": 1, "page_idx": 24}, {"type": "table", "img_path": "tDvFa5OJyS/tmp/a5684e354587ec75b303a5edb9fe2e17be5cfaa1d68d323f497a433afe53bcfc.jpg", "table_caption": ["Table S1: Detailed configuration of the generalization experiment in Section 5. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "S5.3.1 Impact of Learning Rate on Generalization ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To show the impact of different choices of learning rate on the GP approximations we consider, we show the test metrics for the learning rate sweeps in our main experiment in Figure S4. Note that not all choices of learning rate appear since a small minority of runs fail outright, for example if the learning rate is too large. ", "page_idx": 24}, {"type": "image", "img_path": "tDvFa5OJyS/tmp/49ca992ade2d7f41ae6afece3072f330172dd98a48e2aa280a87b153135138eb.jpg", "img_caption": ["Figure S4: Effects of (initial) learning rate when using either LBFGS with Wolfe line search (CholeskyGP, SGPR) or Adam (SVGP, CaGP-CG, CaGP-Opt) for hyperparameter optimization. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "S5.3.2 Evolution Of Hyperparameters During Training ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "To better understand how the kernel hyperparameters of each method evolve during training, we show their trajectories in Figure S5 for each dataset. Note that we only show the first three lengthscales per dataset (rather than up to $d=26$ ). ", "page_idx": 26}, {"type": "image", "img_path": "tDvFa5OJyS/tmp/2dcf475cacc30174566997ccf28ef529876e813d4895bc15052cd72681b9b34e.jpg", "img_caption": ["Figure S5: Learned hyperparameters for different GP approximations on UCI datasets. Showing only results for the best choice of learning rate per method. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We give both illustrative and theoretical justification for the claims about our method in Section 3 and provide extensive empirical results in Section 5. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We explicitly discuss the limitations of our method in the conclusion in a dedicated paragraph. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide proofs to all theoretical statements in the supplementary material or cite the appropriate reference for the result. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We describe the experiments including datasets, number of repeats, hyperparameters, method implementations and hardware in Section 5. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All data we use is publicly available in the UCI repository. We provide an open-source implementation of our method in Section S4.2. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We give a complete description of the choices we made for our benchmark experiments in Section 5. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We repeat all experiments multiple times and report bootstrapped confidence intervals. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: In Section 5, we describe the specific GPUs we use for each experiment and report wallclock time of all training runs. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Due to the methodological nature of this work, there are no potential harmful consequences of this work that we think need to be explicitly highlighted here. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This work is foundational methodological research and does not have a specific negative societal impact that we feel must be explicitly highlighted here. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not release data or models with a high risk for misuse. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We cite the appropriate source for all assets used in our work. All datasets for our experiments are licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) license. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: There are no new assets released with the paper. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not involve research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not involve research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]