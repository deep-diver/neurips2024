{"references": [{"fullname_first_author": "C. Williams", "paper_title": "Using the Nystr\u00f6m method to speed up kernel machines", "publication_date": "2001-01-01", "reason": "This paper introduces the Nystr\u00f6m method for approximating kernel matrices, a fundamental technique used in many scalable Gaussian process methods."}, {"fullname_first_author": "A. Rahimi", "paper_title": "Random Features for Large-Scale Kernel Machines", "publication_date": "2007-01-01", "reason": "This paper proposes random features for approximating kernel functions, which enables efficient large-scale kernel methods and is closely related to the approach used in the current work."}, {"fullname_first_author": "M. Titsias", "paper_title": "Variational learning of inducing variables in sparse Gaussian processes", "publication_date": "2009-01-01", "reason": "This is a foundational paper on sparse Gaussian process methods, introducing the use of inducing points for efficient inference."}, {"fullname_first_author": "J. Hensman", "paper_title": "Gaussian processes for big data", "publication_date": "2013-01-01", "reason": "This work is highly influential in the field of scalable Gaussian processes, proposing a variational inference method for large datasets."}, {"fullname_first_author": "J. Wenger", "paper_title": "Posterior and Computational Uncertainty in Gaussian processes", "publication_date": "2022-01-01", "reason": "This paper introduces computation-aware Gaussian processes, the foundation of the current work, which explicitly quantifies and incorporates approximation error into uncertainty estimates."}]}