[{"figure_path": "LyAFfdx8YF/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of Cross-Embodiment Unsupervised Reinforcement Learning (CEURL). The left subfigure illustrates the cross-embodiment setting with various possible embodiment changes. Directly training RL agents across embodiments under given tasks may result in task-aware rather than embodiment-aware knowledge. CEURL pre-trains agents in reward-free environments to extract embodiment-aware knowledge. The center subfigure shows the Pre-trained Embodiment-Aware Control (PEAC) algorithm, using our cross-embodiment intrinsic reward function RCE(T). The right subfigure demonstrates the fine-tuning phase, where pre-trained agents fast adapt to different downstream tasks, improving adaptation and generalization.", "description": "This figure illustrates the overall framework of Cross-Embodiment Unsupervised Reinforcement Learning (CEURL).  It shows three stages: 1) Cross-Embodiment setting depicting the challenge of training RL agents across different robot morphologies. 2) The PEAC algorithm's pre-training phase in reward-free environment to learn embodiment-aware knowledge using a novel intrinsic reward function.  3) The downstream task fine-tuning stage, where the pre-trained agent adapts quickly to new tasks.", "section": "1 Introduction"}, {"figure_path": "LyAFfdx8YF/figures/figures_5_1.jpg", "caption": "Figure 2: Benchmark environments, including DMC [56], Robosuite [73], Isaacgym [37].", "description": "This figure shows the benchmark environments used in the paper to evaluate the proposed PEAC algorithm.  The environments are diverse, including simulated environments from the DeepMind Control Suite (DMC), Robosuite, and Isaacgym.  The environments represent a variety of tasks and robotic platforms, demonstrating the versatility and generalizability of the PEAC approach across different embodiments and challenges.", "section": "4 Cross-Embodiment Exploration and Skill Discovery"}, {"figure_path": "LyAFfdx8YF/figures/figures_6_1.jpg", "caption": "Figure 3: Aggregate metrics [2] in state-based DMC. Each statistic for every algorithm has 120 runs (3 embodiment settings \u00d7 4 downstream tasks \u00d7 10 seeds).", "description": "This figure shows the performance comparison of different algorithms on state-based DeepMind Control Suite (DMC) tasks.  Four metrics are presented: Median, Interquartile Mean (IQM), Mean, and Optimality Gap.  Each algorithm was tested across three different embodiment settings and four downstream tasks, with ten random seeds used for each combination, resulting in 120 runs per algorithm. The figure visually represents the performance of PEAC against other state-of-the-art unsupervised reinforcement learning methods. The y-axis represents the algorithm, and the x-axis shows the performance scores.", "section": "5 Experiments"}, {"figure_path": "LyAFfdx8YF/figures/figures_6_2.jpg", "caption": "Figure 4: Aggregate metrics [2] in image-based DMC. Each statistic for every algorithm has 36 runs (3 embodiment settings \u00d7 4 downstream tasks \u00d7 3 seeds).", "description": "This figure shows the performance comparison of different algorithms on image-based DeepMind Control Suite (DMC) tasks.  Multiple algorithms (including PEAC and its variants) were evaluated across three different embodiment settings and four downstream tasks.  Each data point represents the average of 36 runs (10 seeds x 3 embodiment settings x 4 tasks).  The metrics used are Median, Interquartile Mean (IQM), Mean, and Optimality Gap, providing a comprehensive evaluation of the algorithms' performance in the cross-embodiment setting.", "section": "Experiments"}, {"figure_path": "LyAFfdx8YF/figures/figures_7_1.jpg", "caption": "Figure 1: Overview of Cross-Embodiment Unsupervised Reinforcement Learning (CEURL). The left subfigure illustrates the cross-embodiment setting with various possible embodiment changes. Directly training RL agents across embodiments under given tasks may result in task-aware rather than embodiment-aware knowledge. CEURL pre-trains agents in reward-free environments to extract embodiment-aware knowledge. The center subfigure shows the Pre-trained Embodiment-Aware Control (PEAC) algorithm, using our cross-embodiment intrinsic reward function RCE(T). The right subfigure demonstrates the fine-tuning phase, where pre-trained agents fast adapt to different downstream tasks, improving adaptation and generalization.", "description": "This figure illustrates the concept of Cross-Embodiment Unsupervised Reinforcement Learning (CEURL) and the proposed PEAC algorithm. The left panel shows different embodiments and how direct training on downstream tasks can lead to task-specific knowledge, while CEURL pre-trains agents in reward-free settings for embodiment-aware knowledge. The center panel details PEAC, highlighting its intrinsic reward function, and the right panel demonstrates the improved generalization and faster adaptation to downstream tasks.", "section": "1 Introduction"}, {"figure_path": "LyAFfdx8YF/figures/figures_8_1.jpg", "caption": "Figure 2: Benchmark environments, including DMC [56], Robosuite [73], Isaacgym [37].", "description": "This figure showcases the benchmark environments used in the paper's experiments to evaluate the proposed PEAC algorithm.  These include simulated environments from the DeepMind Control Suite (DMC), Robosuite, and Isaac Gym.  DMC provides various simulated robotic tasks with different state and image-based observation modalities. Robosuite offers a range of robotic manipulation tasks, and Isaac Gym is used for simulating legged robots and real-world scenarios. The diversity of these platforms allows for a comprehensive evaluation of PEAC's performance across different types of robots and tasks.", "section": "4 Cross-Embodiment Exploration and Skill Discovery"}, {"figure_path": "LyAFfdx8YF/figures/figures_8_2.jpg", "caption": "Figure 6: Ablation studies on pre-training timesteps.", "description": "This figure displays ablation studies on the impact of varying pre-training timesteps on the performance of different algorithms in image-based DeepMind Control Suite (DMC).  The x-axis represents the number of pre-training steps (100k, 500k, 1M, and 2M), and the y-axis represents the expert normalized score. The figure visualizes how the performance of different algorithms, including PEAC-LBS, PEAC-DIAYN, and various baselines (LBS, APT, Plan2Explore, RND, ICM, DIAYN, APS, LSD, CIC, Choreographer), changes with the amount of pre-training. This helps understand the impact of pre-training duration on cross-embodiment adaptation and skill discovery.", "section": "5.2 Evaluation of PEAC"}, {"figure_path": "LyAFfdx8YF/figures/figures_24_1.jpg", "caption": "Figure 8: Ablation study of pre-training steps in image-based DMC.", "description": "This figure presents an ablation study on the effect of different pre-training steps on the performance of various algorithms in image-based DeepMind Control Suite (DMC) environments.  It shows expert normalized scores for different algorithms (PEAC-DIAYN, LSD, DIAYN, CIC, PEAC-LBS, LBS, Choreographer, APT, Plan2Explore, RND, ICM) across three different embodiment settings (Walker-mass, Quadruped-mass, Quadruped-damping) and an overall average.  The results are shown separately for pre-training steps of 100k, 500k, 1M, and 2M steps, illustrating how the performance of each algorithm changes with varying pre-training durations.", "section": "B.8 More Ablation Studies"}, {"figure_path": "LyAFfdx8YF/figures/figures_25_1.jpg", "caption": "Figure 9: Visualization of the pre-trained model generalization to unseen embodiments.", "description": "This figure visualizes the generalization ability of pre-trained models to unseen embodiments using t-SNE dimensionality reduction.  The plots show the hidden states extracted from the models when sampling trajectories in different, previously unseen embodiments. Different colored points represent states from different embodiments.  The figure illustrates the ability of PEAC-LBS to better distinguish between the different unseen embodiments compared to other baselines.", "section": "B.9 Generalization results of pre-trained models"}, {"figure_path": "LyAFfdx8YF/figures/figures_27_1.jpg", "caption": "Figure 10: Benchmark environments of Walker-length and Cheetah-torsolength. In Walker-length, the length of the left foot sole of different robots is different. In Cheetah-torsolength, the length of the torso is different.", "description": "This figure shows four different benchmark environments used in the paper.  These are variations of the walker and cheetah robots, modified to have different leg lengths (Walker-length) and torso lengths (Cheetah-torsolength). These variations were used to test the generalizability of the PEAC algorithm to unseen embodiments with different morphologies.", "section": "B.11 More challenging tasks and varying embodiments"}, {"figure_path": "LyAFfdx8YF/figures/figures_28_1.jpg", "caption": "Figure 2: Benchmark environments, including DMC [56], Robosuite [73], Isaacgym [37].", "description": "This figure showcases the benchmark environments used to evaluate the PEAC algorithm.  It includes simulated environments from the DeepMind Control Suite (DMC), Robosuite, and Isaac Gym.  These environments represent a variety of robotic platforms and tasks, testing the algorithm's ability to generalize across different embodiments (robot designs and capabilities).  The image visually shows examples of the diverse robots and tasks, highlighting the scope of the experiments.", "section": "4 Cross-Embodiment Exploration and Skill Discovery"}]