[{"figure_path": "LyAFfdx8YF/tables/tables_18_1.jpg", "caption": "Table 3: Environment parameters used for state-based DMC and image-based DMC.", "description": "This table shows the parameter ranges for the mass (m) and damping (l) factors used in the state-based and image-based DeepMind Control Suite (DMC) experiments.  It details the values used for training and generalization across three different embodiment settings: Walker-mass, Quadruped-mass, and Quadruped-damping.", "section": "5.1 Experimental Setup"}, {"figure_path": "LyAFfdx8YF/tables/tables_20_1.jpg", "caption": "Table 3: Environment parameters used for state-based DMC and image-based DMC.", "description": "This table lists the parameters used for the state-based and image-based DeepMind Control Suite (DMC) experiments.  It shows the range of mass values (m) used for the Walker and Quadruped robots and the range of damping values (l) used for the Quadruped robot in both training and generalization phases of the experiments. These parameters define the different embodiments used in the cross-embodiment experiments.", "section": "5.1 Experimental Setup"}, {"figure_path": "LyAFfdx8YF/tables/tables_21_1.jpg", "caption": "Table 3: Environment parameters used for state-based DMC and image-based DMC.", "description": "This table shows the parameters used for generating different embodiments in both state-based and image-based DeepMind Control Suite (DMC) experiments.  For state-based DMC, three different embodiment distributions are created by varying the mass (Walker-mass, Quadruped-mass) or damping (Quadruped-damping) of the robots.  The table specifies the range of mass or damping values used during training and generalization phases for each embodiment setting.  For image-based DMC, the same three embodiment distributions and parameter ranges are used.", "section": "5.1 Experimental Setup"}, {"figure_path": "LyAFfdx8YF/tables/tables_21_2.jpg", "caption": "Table 1: Results of Robosuite and Isaacgym.", "description": "This table shows the performance of PEAC and baselines on Robosuite and Isaacgym benchmark environments.  It compares the average cumulative reward achieved by different algorithms across various tasks and robotic platforms (Panda, IIWA, Kinova3, Jaco, and Aliengo).  It highlights PEAC's superior performance in few-shot learning and generalization to unseen embodiments.  The results demonstrate the effectiveness of PEAC in both simulated and real-world environments.", "section": "5.2 Evaluation of PEAC"}, {"figure_path": "LyAFfdx8YF/tables/tables_22_1.jpg", "caption": "Table 7: Details of hyper-parameters used for Isaacgym.", "description": "This table lists the hyperparameters used for the Isaacgym experiments.  It includes parameters for the Proximal Policy Optimization (PPO) algorithm, such as the clip range, generalized advantage estimation (GAE) lambda, learning rate, reward discount factor, minimum policy standard deviation, number of environments, and batch sizes.  Additionally, it shows the hyperparameters specific to the Pre-trained Embodiment-Aware Control (PEAC) algorithm, including the type of historical information encoder (GRU), the length of the encoded historical information, and the architecture of the embodiment context model (MLP).", "section": "B.3 Hyper-parameters"}, {"figure_path": "LyAFfdx8YF/tables/tables_22_2.jpg", "caption": "Table 8: Aggregate metrics [2] in state-based DMC. For every algorithm, there are 3 embodiment settings, each trained with 10 seeds and fine-tuned under 4 downstream tasks, thus each statistic for every method has 120 runs.", "description": "This table presents the aggregate performance metrics (Median, IQM, Mean, and Optimality Gap) for various unsupervised reinforcement learning algorithms in the state-based DeepMind Control Suite (DMC) benchmark.  Each algorithm was evaluated across three different embodiment settings (different mass or damping), four downstream tasks, and ten random seeds.  The table summarizes the overall performance across all these conditions.  The results show the effectiveness of each algorithm, particularly compared against the proposed PEAC algorithm.", "section": "5.2 Evaluation of PEAC"}, {"figure_path": "LyAFfdx8YF/tables/tables_22_3.jpg", "caption": "Table 8: Aggregate metrics [2] in state-based DMC. For every algorithm, there are 3 embodiment settings, each trained with 10 seeds and fine-tuned under 4 downstream tasks, thus each statistic for every method has 120 runs.", "description": "This table presents the performance comparison of different reinforcement learning algorithms on state-based DeepMind Control Suite (DMC) tasks.  Four metrics (Median, Interquartile Mean (IQM), Mean, and Optimality Gap) are used to evaluate the performance across three different embodiment settings and four downstream tasks. Each algorithm was trained with 10 different random seeds, resulting in a total of 120 runs (3 embodiment settings * 4 tasks * 10 seeds) for each algorithm.  The table highlights the performance of the PEAC algorithm in comparison to several baselines.", "section": "5.2 Evaluation of PEAC"}, {"figure_path": "LyAFfdx8YF/tables/tables_23_1.jpg", "caption": "Table 10: Aggregate metrics [2] in image-based DMC. For every algorithm, there are 3 embodiment settings, each trained with 3 seeds and fine-tuned under 4 downstream tasks, thus each statistic for every method has 36 runs.", "description": "This table presents the aggregate performance metrics for various algorithms on image-based DeepMind Control Suite (DMC) tasks.  It shows the median, interquartile mean (IQM), mean, and optimality gap across different algorithms.  The experiments involved 3 embodiment settings, 3 random seeds for each setting, and 4 downstream tasks, resulting in 36 runs per algorithm.", "section": "5.2 Evaluation of PEAC"}, {"figure_path": "LyAFfdx8YF/tables/tables_23_2.jpg", "caption": "Table 16: Detailed results in state-based DMC in evaluation embodiments. Average cumulative reward (mean of 10 seeds) of the best policy trained by different algorithms.", "description": "This table presents the detailed results of the generalization ability of the fine-tuned models in state-based DeepMind Control Suite (DMC) using unseen embodiments. The average cumulative rewards of different algorithms are shown for different downstream tasks and embodiments (Walker-mass, Quadruped-mass, and Quadruped-damping).  This allows for assessment of the generalization performance of the pre-trained models across various unseen embodiments.", "section": "5.3 Generalization to Unseen Embodiments"}, {"figure_path": "LyAFfdx8YF/tables/tables_23_3.jpg", "caption": "Table 1: Results of Robosuite and Isaacgym.", "description": "This table presents the results of the Robosuite and Isaacgym experiments.  It shows the performance of the PEAC algorithm compared to baselines (ICM, RND, LBS) across various tasks (climb, leap, crawl, tilt) and embodiment settings (Al-disabled). The metrics used to evaluate the performance are not specified in the caption itself but are likely related to the success rate of completing each task or other performance metrics relevant to legged locomotion.  Higher numbers likely indicate better performance.", "section": "5.2 Evaluation of PEAC"}, {"figure_path": "LyAFfdx8YF/tables/tables_24_1.jpg", "caption": "Table 8: Aggregate metrics [2] in state-based DMC. For every algorithm, there are 3 embodiment settings, each trained with 10 seeds and fine-tuned under 4 downstream tasks, thus each statistic for every method has 120 runs.", "description": "This table presents the aggregate performance metrics for various reinforcement learning algorithms on state-based DeepMind Control Suite (DMC) tasks.  The metrics include median, interquartile mean (IQM), mean, and optimality gap (OG).  Each algorithm was evaluated across three embodiment settings, with ten seeds used for training and evaluation on four downstream tasks per embodiment, resulting in 120 total runs per algorithm.  The table allows comparison of the different algorithms' performance across these metrics, indicating their relative effectiveness and stability.", "section": "5.2 Evaluation of PEAC"}, {"figure_path": "LyAFfdx8YF/tables/tables_25_1.jpg", "caption": "Table 14: Ablation study for baselines w/ our embodiment discriminator in state-based DMC", "description": "This table presents the ablation study results in state-based DeepMind Control Suite (DMC) by adding the embodiment discriminator to the baselines. The results show that adding the embodiment discriminator to the baselines improves performance, and PEAC still outperforms the baselines.", "section": "B.8 More Ablation Studies"}, {"figure_path": "LyAFfdx8YF/tables/tables_25_2.jpg", "caption": "Table 16: Detailed results in state-based DMC in evaluation embodiments. Average cumulative reward (mean of 10 seeds) of the best policy trained by different algorithms.", "description": "This table presents the average cumulative reward achieved by different algorithms across various downstream tasks in state-based DeepMind Control Suite (DMC) environments. The experiments involve three distinct embodiment settings (Walker-mass, Quadruped-mass, Quadruped-damping) and four downstream tasks (stand, walk, run, flip). Each setting is tested with 10 random seeds, and the average cumulative reward is reported for each algorithm. This table helps to evaluate the generalization performance of each algorithm to unseen embodiments in state-based DMC.", "section": "5.3 Generalization to Unseen Embodiments"}, {"figure_path": "LyAFfdx8YF/tables/tables_26_1.jpg", "caption": "Table 8: Aggregate metrics [2] in state-based DMC. For every algorithm, there are 3 embodiment settings, each trained with 10 seeds and fine-tuned under 4 downstream tasks, thus each statistic for every method has 120 runs.", "description": "This table presents the aggregate performance metrics for various algorithms on state-based DeepMind Control Suite (DMC) tasks.  The metrics shown include median, interquartile mean (IQM), mean, and optimality gap (OG), providing a comprehensive performance summary.  Each algorithm was evaluated across three different embodiment settings, with 10 seeds per setting and four downstream tasks for each seed. This resulted in 120 runs (3 settings * 4 tasks * 10 seeds) per algorithm, ensuring robust statistical analysis.", "section": "5.2 Evaluation of PEAC"}, {"figure_path": "LyAFfdx8YF/tables/tables_26_2.jpg", "caption": "Table 16: Detailed results in state-based DMC in evaluation embodiments. Average cumulative reward (mean of 10 seeds) of the best policy trained by different algorithms.", "description": "This table presents the average cumulative reward achieved by different reinforcement learning algorithms across various tasks and embodiments in state-based DeepMind Control Suite (DMC).  The results are averaged over 10 seeds for each condition.  The \"evaluation embodiments\" refers to test environments that the pre-trained agents were not trained on, demonstrating generalization capacity.", "section": "5.2 Evaluation of PEAC"}, {"figure_path": "LyAFfdx8YF/tables/tables_26_3.jpg", "caption": "Table 18: Detailed results of Walker-mass-incline in image-based DMC.", "description": "This table presents the results of the image-based DMC experiments for the Walker-mass-incline setting.  The results show the average cumulative reward for each of four downstream tasks (stand, walk, run, and flip) for two algorithms: LBS and PEAC-LBS. PEAC-LBS consistently outperforms LBS in all tasks, suggesting its enhanced effectiveness in the more challenging incline terrain.", "section": "B.11 More challenging tasks and varying embodiments"}, {"figure_path": "LyAFfdx8YF/tables/tables_27_1.jpg", "caption": "Table 19: Detailed results of Walker-Cheetah in image-based DMC.", "description": "This table presents the quantitative results of applying different algorithms (DIAYN, PEAC-DIAYN, LBS, Choreographer, and PEAC-LBS) to the Walker-Cheetah environment in image-based DeepMind Control Suite (DMC).  The environment involves two different types of robots (Walker and Cheetah) with varying tasks (stand, run, flip). Each algorithm's performance is measured across four distinct task combinations, evaluating its effectiveness in handling the diverse embodiments.", "section": "B.11 More challenging tasks and varying embodiments"}, {"figure_path": "LyAFfdx8YF/tables/tables_27_2.jpg", "caption": "Table 20: Detailed results of Walker-Humanoid in image-based DMC.", "description": "This table presents the detailed results of the Walker-Humanoid experiment in image-based DeepMind Control Suite (DMC).  The experiment involves two different robots, a Walker and a Humanoid, each performing various locomotion tasks.  The results show the average cumulative reward (mean of 3 seeds) achieved by each algorithm for different tasks and robot combinations, providing a quantitative comparison of the performance of different algorithms under cross-embodiment conditions.", "section": "B.9 Generalization results of fine-tuned models"}, {"figure_path": "LyAFfdx8YF/tables/tables_27_3.jpg", "caption": "Table 21: Detailed results of Walker-length and Cheetah-torso_length in image-based DMC.", "description": "This table presents the results of the experiments conducted on two variations of the image-based DeepMind Control Suite (DMC) benchmark: Walker-length and Cheetah-torso_length.  These variations modify the morphology of the robots, specifically altering leg length (Walker-length) and torso length (Cheetah-torso_length).  The table shows the average cumulative reward achieved by different algorithms across various locomotion tasks (stand, walk, run, flip, run_backward, flip_backward) for each embodiment type.  The algorithms compared include DIAYN, PEAC-DIAYN, LBS, Choreographer, and PEAC-LBS, all of which are discussed extensively in the paper. The table highlights the performance differences of these algorithms across tasks and embodiments, illustrating their ability to generalize across varied morphologies.", "section": "5.3 Generalization to Unseen Embodiments"}]