[{"type": "text", "text": "PEAC: Unsupervised Pre-training for Cross-Embodiment Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chengyang Ying1 Zhongkai Hao1 Xinning Zhou1 Xuezhou $\\mathbf{X}\\mathbf{u}^{1}$ Hang $\\mathbf{S}\\mathbf{u}^{1,2*}$ Xingxing Zhang1 Jun Zhu1,2 ", "page_idx": 0}, {"type": "text", "text": "1Department of Computer Science & Technology, Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University 2Pazhou Lab (Huangpu), Guangzhou, China ycy21@mails.tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Designing generalizable agents capable of adapting to diverse embodiments has achieved significant attention in Reinforcement Learning (RL), which is critical for deploying RL agents in various real-world applications. Previous CrossEmbodiment RL approaches have focused on transferring knowledge across embodiments within specific tasks. These methods often result in knowledge tightly coupled with those tasks and fail to adequately capture the distinct characteristics of different embodiments. To address this limitation, we introduce the notion of Cross-Embodiment Unsupervised RL (CEURL), which leverages unsupervised learning to enable agents to acquire embodiment-aware and task-agnostic knowledge through online interactions within reward-free environments. We formulate CEURL as a novel Controlled Embodiment Markov Decision Process (CE-MDP) and systematically analyze CEURL\u2019s pre-training objectives under CE-MDP. Based on these analyses, we develop a novel algorithm Pre-trained Embodiment-Aware Control (PEAC) for handling CEURL, incorporating an intrinsic reward function specifically designed for cross-embodiment pre-training. PEAC not only provides an intuitive optimization strategy for cross-embodiment pre-training but also can integrate flexibly with existing unsupervised RL methods, facilitating cross-embodiment exploration and skill discovery. Extensive experiments in both simulated (e.g., DMC and Robosuite) and real-world environments (e.g., legged locomotion) demonstrate that PEAC significantly improves adaptation performance and cross-embodiment generalization, demonstrating its effectiveness in overcoming the unique challenges of CEURL. The project page and code are in https://yingchengyang.github.io/ceurl. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Cross-embodiment reinforcement learning (RL) involves designing algorithms that effectively function across various physical embodiments. The fundamental goal is to enable agents to apply skills and strategies learned from some embodiments to other embodiments, which may own different physical dynamics, action-effectors, shapes, and so on [28, 70, 58, 52, 12, 69, 60]. This capability significantly enhances the generalization of RL agents, reducing the necessity for embodiment-specific training. By adeptly adapting to new and shifting embodiment, cross-embodiment RL ensures that agents maintain reliable performance in unpredictable real-world scenarios, thereby benefiting the deployment process and reducing the need for extensive data collection for each new embodiment. ", "page_idx": 0}, {"type": "image", "img_path": "LyAFfdx8YF/tmp/87a5af077e9bc5632ec9b6cd559fc257462b1c20f55a00535132d3a33e2a7285.jpg", "img_caption": ["Figure 1: Overview of Cross-Embodiment Unsupervised Reinforcement Learning (CEURL). The left subfigure illustrates the cross-embodiment setting with various possible embodiment changes. Directly training RL agents across embodiments under given tasks may result in task-aware rather than embodiment-aware knowledge. CEURL pre-trains agents in reward-free environments to extract embodiment-aware knowledge. The center subfigure shows the Pre-trained Embodiment-Aware Control (PEAC) algorithm, using our cross-embodiment intrinsic reward function $\\mathcal{R}_{\\mathrm{CE}}(\\tau)$ . The right subfigure demonstrates the fine-tuning phase, where pre-trained agents fast adapt to different downstream tasks, improving adaptation and generalization. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "One of the primary challenges in this area is the transfer of knowledge across embodiments that have vastly different physical dynamics and environmental interactions. This requires the agent to abstract knowledge in a way that is not overly specialized to a single embodiment or some downstream tasks. However, directly training cross-embodiment agents under some given tasks will cause the learned knowledge highly related to these tasks rather than only to embodiments themselves. ", "page_idx": 1}, {"type": "text", "text": "Inspired by the transformative effects of unsupervised learning in natural language processing and computer vision [6, 21], which has demonstrated efficiency in extracting generalized knowledge independent of downstream tasks, we propose a natural question: Can we pre-train cross-embodiment agents in an unsupervised manner, i.e., online cross-embodiment pre-training in reward-free environments, to capture generalized knowledge only related to embodiments? Existing unsupervised RL techniques, including exploration [45, 38] and skill discovery [10, 26] ones, typically involve pre-training agents by engaging a single embodiment within a controlled Markov Decision Process (MDP) that lacks extrinsic reward signals. These pre-trained agents are then expected to quickly fine-tune to any downstream tasks characterized by extrinsic rewards using this specific embodiment. This approach of unsupervised RL fosters the development of policies that are not overly specialized to specific tasks or reward structures but are rather driven by intrinsic motivations of embodiments, which shows the potential for discovering more generalized knowledge across different embodiments. ", "page_idx": 1}, {"type": "text", "text": "In this work, we adapt the unsupervised RL paradigm to the cross-embodiment setting, introducing the concept of Cross-Embodiment Unsupervised RL (CEURL). This setting involves pre-training with a distribution of embodiments in reward-free environments, followed by fine-tuning to handle specific downstream tasks through these embodiments. These embodiments may own similar structures so that we can abstract generalized knowledge from them. To analyze CEURL and design corresponding algorithms, we formulate it as a Controlled Embodiment Markov Decision Process (CE-MDP), which comprises a distribution of controlled MDPs, each defined by its unique embodiment context. Compared to the traditional single-embodiment setting, the CE-MDP framework addresses the additional complexity caused by the inherent variability among embodiments. We then extend the information geometry analyses of the controlled MDP [11] to better explain the complexity of CEMDP. Our findings indicate that skill vertices within CE-MDP may no longer be simple deterministic policies and the behaviors across different embodiments can display substantial variability. ", "page_idx": 1}, {"type": "text", "text": "To address the complexities of CE-MDP, we undertake an in-depth analysis of the pre-training objective in CE-MDP. We aim to enable our pre-trained agent to quickly fine-tune for any downstream tasks denoted as $\\mathcal{R}_{\\mathrm{ext}}$ , especially under the worst-case reward scenarios. Thus, our pre-training objective involves minimizing across $\\mathcal{R}_{\\mathrm{ext}}$ while maximizing the fine-tuned policy $\\pi^{*}$ , leading to a complex min-max problem (Eq. 3). We further introduce a novel Pre-trained Embodiment-Aware Control (PEAC) algorithm to optimize this objective and handle CE-MDP, which improves the agent\u2019s robustness and adaptability across various embodiments by employing a cross-embodiment intrinsic reward $\\mathcal{R}_{\\mathrm{CE}}$ . This reward is complemented by an embodiment discriminator, which distinguishes between different embodiments. During fine-tuning, the pre-trained policy is further enhanced under the extrinsic reward, $\\mathcal{R}_{\\mathrm{ext}}$ with limited timesteps. Moreover, PEAC can integrate flexibly with existing single-embodiment unsupervised RL methods to achieve cross-embodiment exploration and skill discovery, resulting in two combination algorithm examples PEAC-LBS and PEAC-DIAYN. ", "page_idx": 1}, {"type": "text", "text": "To verify the versatility and effectiveness of our algorithm, we extensively evaluate PEAC in both simulated and real-world environments. In simulations, we choose state-based / image-based DeepMind Control Suite (DMC) environments extending Unsupervised RL Benchmark (URLB) [27] and different robotic arms in Robosuite [73]. Under these settings, PEAC demonstrates superior few-shot learning ability to downstream tasks, and remarkable generalization ability to unseen embodiments, surpassing existing state-of-the-art unsupervised RL models. Besides, we have evaluated PEAC in real-world Aliengo robots by considering practical joint failure settings based on Isaacgym [37], verifying PEAC\u2019s strong adaptability on different joint failures and various real-world terrains. ", "page_idx": 2}, {"type": "text", "text": "In summary, the main contributions are as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We propose a novel setting CEURL to enhance agents\u2019 adaptability and generalization across diverse embodiments, and then we introduce the Pre-trained Embodiment-Aware Control (PEAC) algorithm for handling CEURL. \u2022 We integrate PEAC with existing exploration and skill discovery techniques, designing practical methods and facilitating efficient cross-embodiment exploration and skill discovery. \u2022 Extensive experiments show that PEAC not only excels in fast fine-tuning but also effectively generalizes across new embodiments, outperforming current SOTA unsupervised RL models. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Cross-Embodiment RL. Designing generalizable agents simultaneously controlling diverse embodiments has achieved significant attention in RL. A common strategy involves using expert trajectories [70, 49, 5, 66, 60], internet-scale human videos [3, 58, 13], or offline datasets [29, 59, 8, 41] to train a generalist agent that can handle various tasks across different embodiments. However, these methods are often limited by the need for large-scale, costly datasets and the availability of expert trajectories. Additionally, the discrepancy between open-loop training and closed-loop testing may lead to distribution shifts [32], adversely affecting the final performance. An alternative line of research [28, 36, 64, 4, 66, 52, 12, 68] focuses on training general agents through online interaction across diverse environments. However, these methods treat the embodiment and task as a unified training environment and overlook the role of proprioception, i.e., the internal understanding of an agent\u2019s embodiment, which has recently proven to be beneficial for representation learning and optimization in RL [23, 15]. Thus these methods may not fully capture the intrinsic properties of different embodiments by linking knowledge to specific tasks. Emerging research suggests the potential of decoupling the training of embodiment characteristics from task execution, aiming to develop a unified cross-embodiment model. This involves unsupervised pre-training across a variety of embodiments, followed by task-aware fine-tuning, enabling a single agent to adeptly manage both roles effectively. ", "page_idx": 2}, {"type": "text", "text": "Unsupervised RL. Unsupervised RL leverages interactions with reward-free environments to extract useful knowledge, such as exploratory policies, diverse skills, or world models [17, 18]. These pre-trained models are utilized to fast adapt to downstream tasks within specific embodiments and environments. Unsupervised RL methods can be categorized into two main types: exploration and skill discovery. Exploration methods aim to maximize state coverage, typically through intrinsic rewards that encourage uncertainty [45, 7, 46, 51, 38, 40, 67] or state entropy [30, 47, 35, 34]. The resulting exploratory trajectories benefti pre-training actor-critic or world models, thereby enhancing fine-tuning efficiency [27]. Skill discovery methods focus on learning an array of distinguishable skills, often by maximizing the mutual information between states and acquired skills [10, 54, 20, 55, 25, 26, 72, 24, 61]. This approach benefits from theoretical insights into the information geometry of skill state distributions, emphasizing the importance of maximizing distances between different skills [11, 22, 42, 43, 44]. Recent efforts also explore incremental skill learning in dynamic environments [53, 31]. Unlike these methods generally focus on single embodiments, we aim to develop generalizable models capable of handling downstream tasks across a variety of embodiments. ", "page_idx": 2}, {"type": "text", "text": "3 Cross-Embodiment Unsupervised RL ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we analyze the cross-embodiment RL in an unsupervised manner, which is formulated as our Controlled Embodiment MDP. Then we propose a novel algorithm PEAC to optimize CE-MDP. ", "page_idx": 2}, {"type": "text", "text": "3.1 Controlled Embodiment Markov Decision Processes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Cross-embodiment RL can be formulated by contextual MDP [19] with a distribution of Markovian decision processes (MDPs) of $\\{\\mathcal{M}_{e}\\}$ . Cross-embodiment RL hopes to learn shared knowledge from this distribution of MDPs, which is crucial for enhancing the adaptability or generalization of agents across embodiments. However, directly optimizing agents by online interacting with $\\{\\mathcal{M}_{e}\\}$ or utilizing offline datasets sampled from $\\{\\mathcal{M}_{e}\\}$ may learn knowledge not only related to these embodiments but also highly related to these task reward functions in $\\{\\mathcal{M}_{e}\\}$ . This phenomenon may have negative impacts on learning the general knowledge across embodiments or improving the agent\u2019s generalization ability. For example, as the agent is required to handle $\\{\\mathcal{M}_{e}\\}$ , it will less explore the trajectories with low rewards. These trajectories, although not optimal for the embodiment in this task, might also include embodiment knowledge and be useful for other tasks. Without extrinsic task rewards, the agent is encouraged to learn embodiment-aware and task-agnostic knowledge, which can effectively adapt to any downstream task across embodiments. ", "page_idx": 3}, {"type": "text", "text": "In this paper, we propose to pre-train cross-embodiment agents in reward-free environments to ensure that the agent can learn knowledge only specialized in these embodiments themselves. In other words, we introduce unsupervised RL into cross-embodiment RL as a novel setting: cross-embodiment unsupervised RL (CEURL). As shown in Fig. 1, in CEURL, we first pre-train a general agent by interacting with the reward-free environment through varying embodiments sampled from an unknown embodiment distribution. Given any downstream task represented by the extrinsic reward $\\mathcal{R}_{\\mathrm{ext}}$ , the pre-trained agent is subsequently fine-tuned to control these embodiments, and other unseen embodiments from the distribution, to complete this task within limited steps (like one-tenth of the pre-training steps). Formally, we formulate CEURL as the following controlled embodiment MDP (CE-MDP): ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Controlled Embodiment MDP (CE-MDP)). A CE-MDP includes a distribution of controlled MDPs defined as $\\mathcal{M}_{e}^{c}=(S_{e},\\mathcal{A}_{e},\\mathcal{P}_{e},\\gamma)$ , where $e\\sim\\mathcal{E}$ and $\\mathcal{E}$ represents the embodiment distribution. Each embodiment may have different state spaces $\\ensuremath{\\mathcal{S}}_{e}$ and action spaces $\\mathcal{A}_{e}$ . $\\mathcal{P}_{e}$ : $S_{e}\\,\\times\\,A_{e}\\,\\rightarrow\\,\\Delta(S_{e})$ denoting the transition dynamics for embodiment e and $\\gamma$ is the discount factor. We define the state space $\\begin{array}{r}{\\mathcal{S}=\\cup_{e}\\mathcal{S}_{e}}\\end{array}$ and adopt a unified action embedding space $\\boldsymbol{\\mathcal{A}}$ with corresponding action projectors $\\phi_{e}:\\mathcal{A}\\rightarrow\\mathcal{A}_{e}$ , which can be fixed or learnable. ", "page_idx": 3}, {"type": "text", "text": "Thus we can establish a unified policy $\\pi:S\\to\\Delta(A)$ across all embodiments. For any embodiment $^e$ , we sample an action $\\textbf{\\em a}$ from $\\pi(\\cdot|s)$ for a state $s\\in S_{e}$ and execute the projected action $\\phi_{e}(\\pmb{a})$ . Without loss of generality, we assume $\\phi_{e}$ is fixed and focus our analysis on the policy $\\pi$ . To explain the complexities of CE-MDP with varying embodiment contexts, we extend the single-embodiment information geometry analyses [11] into our cross-embodiment setting. First, we consider the discount state distribution of $\\pi$ within $\\mathcal{M}_{e}^{c}$ at state $\\pmb{s}$ as $\\begin{array}{r}{d_{\\pi}^{e}(s)=(1-\\gamma)\\sum_{t=0}^{\\infty}\\left[\\gamma^{t}\\mathcal{P}_{e}(s_{t}=s)\\right]}\\end{array}$ . It is well known that the trajectory return of the state-based reward function can be computed as ", "page_idx": 3}, {"type": "equation", "text": "$$\nJ_{\\mathcal{M}_{e}^{c},\\mathcal{R}_{\\mathrm{ext}}}(\\pi)\\triangleq\\mathbb{E}_{\\tau\\sim\\mathcal{M}_{e}^{c},\\pi}\\left[\\mathcal{R}_{\\mathrm{ext}}(\\tau)\\right]=\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d_{\\pi}^{e}}\\left[\\mathcal{R}_{\\mathrm{ext}}(s)\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Thus, the properties of $d_{\\pi}^{e}$ are significant in determining useful initializations for downstream tasks. We consider the set $\\mathcal{D}^{e}\\overset{\\cdot\\cdot}{=}\\{d_{\\pi}^{e}\\in\\Delta(S)\\mid\\forall\\pi\\}$ , which includes all feasible $d_{\\pi}^{e}$ over the probability simplex. As shown in [11], for each $^e$ , $\\mathcal{D}^{e}$ is a convex set, and any useful policy, which can be optimal for certain downstream tasks under embodiment $^e$ , must be a vertex of $\\mathcal{D}^{e}$ , typically corresponding to deterministic policies. ", "page_idx": 3}, {"type": "text", "text": "However, in the context of CEURL, the unknown embodiment context $e$ introduces partial observability [14] and significant differences in the corresponding points of the same skill across different embodiments. In CE-MDP, we consider the entire embodiment space and define $d_{\\pi}^{\\mathcal{E}}(\\pmb{s})=\\mathbb{E}_{\\pmb{e}\\sim\\mathcal{E}}\\left[d_{\\pi}^{\\pmb{e}}(\\pmb{s})\\right]$ , with $\\mathcal{D}^{\\mathcal{E}}=\\{d_{\\pi}^{\\mathcal{E}}\\in\\Delta(S)\\mid\\forall\\pi\\}$ . The primary challenge lies in the high variability of embodiments, which complicates the process of learning a policy that generalizes well across different embodiments. We demonstrate that the vertices of $\\mathcal{D}^{\\mathcal{E}}$ may no longer correspond to deterministic policies, as they need to handle all embodiments in the distribution. This significantly heightens the challenge of the pre-training process in CE-MDP, making it more difficult to find useful cross-embodiment skills (proofs and discussion in Appendix A.1). ", "page_idx": 3}, {"type": "text", "text": "To solve CEURL under the paradigm of CE-MDP, the agent will collect reward-free trajectories $\\tau=(s_{0},a_{0},s_{1},...)$ with probability $\\begin{array}{r}{p_{\\mathcal{M}_{e}^{c},\\pi}(\\tau)=\\mathcal{P}_{e}(s_{0})\\prod_{t=0}\\pi(a_{t}|s_{t})\\mathcal{P}_{e}(s_{t+1}|s_{t},a_{t})}\\end{array}$ via some sampled embodiments $^e$ during the pre-training. These trajectories are then used in CEURL methods to design intrinsic rewards ${\\mathcal{R}}_{\\mathrm{int}}$ for pre-training agents. During fine-tuning, we will sample several embodiments $^e$ from $\\mathcal{E}$ and combine $\\mathcal{M}_{e}^{c}$ with a downstream task represented by extrinsic rewards $\\mathcal{R}_{\\mathrm{ext}}$ , and agents are required to maximize the task return over all embodiments, i.e., $\\mathbb{E}_{e\\sim\\mathcal{E}}\\left[J_{\\mathcal{M}_{e}^{c},\\mathcal{R}_{\\mathrm{ext}}}(\\pi)\\right]$ , within limited steps (like one-tenth or less of the pre-training steps). ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 Pre-trained Embodiment-Aware Control ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We primarily focus on the pre-training objective of CEURL, specifically determining the optimal pre-trained policy $\\pi$ for CEURL. In the fine-tuning stage, given any downstream task characterized by extrinsic reward $\\mathcal{R}_{\\mathrm{ext}}$ , the pre-trained policy $\\pi$ will be optimized into the fine-tuned policy $\\pi^{*}$ with limited steps to handle $\\mathcal{R}_{\\mathrm{ext}}$ via some RL algorithms like PPO [50]. Consequently, it is widely assumed that $\\pi^{*}$ will remain close to $\\pi$ during fine-tuning due to constraints on limited interactions with the environment [11]. Our cross-embodiment fine-tuning objective thus combines policy improvement under $\\mathcal{R}_{\\mathrm{ext}}$ and a policy constraint evaluated via KL divergence ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}(\\pi,\\pi^{*},\\mathcal{R}_{\\mathrm{ext}},e)\\triangleq\\mathbb{E}_{p_{M_{e}^{c},\\pi^{*}}(\\tau)}[\\mathcal{R}_{\\mathrm{ext}}(\\tau)]-\\mathbb{E}_{p_{M_{e}^{c},\\pi}(\\tau)}[\\mathcal{R}_{\\mathrm{ext}}(\\tau)]-\\beta D_{\\mathrm{KL}}(p_{M_{e}^{c},\\pi^{*}}(\\tau)||p_{\\bar{M},\\pi}(\\tau))_{\\mathrm{L}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\beta>0$ is the unknown trade-off parameter related to the fine-tuning steps (when fine-tuning steps tend towards infinity, $\\beta$ tends to 0 and this objective converges to the original RL objective), and $\\bar{\\mathcal{M}}$ represents the \u201caverage embodiment MDP\u201d satisfying that $p_{\\bar{\\mathcal{M}},\\pi}(\\tau)=\\mathbb{E}_{e\\sim\\mathcal{E}}\\left[p_{\\bar{\\mathcal{M}}_{e}^{c},\\pi}(\\tau)\\right]$ . During fine-tuning, we hope to optimize $\\pi^{*}$ by maximizing $\\mathcal{F}$ , i.e., the fine-tuned result is maxpMc,\u03c0\u2217(\u03c4) F(\u03c0, \u03c0\u2217, Rext, e). As the pre-trained policy \u03c0 needs to handle any downstream task, we consider the worst-case extrinsic reward function across the embodiment distribution, and our cross-embodiment pre-training objective can be formally represented as maximizing ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{U}(\\pi,\\mathcal{E})\\triangleq\\mathbb{E}_{e\\sim\\mathcal{E}}\\left[\\operatorname*{min}_{\\mathcal{R}_{\\mathrm{ext}}(\\tau)}\\operatorname*{max}_{p_{\\mathcal{A}_{e}^{c},\\pi^{*}}(\\tau)}\\mathcal{F}(\\pi,\\pi^{*},\\mathcal{R}_{\\mathrm{ext}},e)\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This objective is a min-max problem that is hard to optimize. Fortunately, we can simplify it as below Theorem 3.2 (Proof in Appendix A.2). The pre-training objective Eq. (3) of $(\\pi,\\mathcal{E})$ satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{U}(\\pi,\\mathcal{E})=\\mathbb{E}_{e\\sim\\mathcal{E}}\\left[-\\beta D_{\\mathrm{KL}}\\left(p_{\\mathcal{M}_{e}^{c},\\pi}(\\tau)\\|p_{\\bar{\\mathcal{M}},\\pi}(\\tau)\\right)\\right]=\\beta\\mathbb{E}_{e\\sim\\mathcal{E}}\\mathbb{E}_{\\tau\\sim p_{\\mathcal{M}_{e}^{c},\\pi}(\\tau)}\\left[\\log\\frac{p(e)}{p_{\\pi}(e|\\tau)}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here $p(e)$ and $p_{\\pi}(e|\\tau)$ are embodiment prior and posterior probabilities, respectively. This result simplifies our pre-trained objective as a form easy to calculate and optimize. Also, although $\\beta$ is an unknown parameter, the optimal pre-trained policy is independent of $\\beta$ . Based on these analyses, we propose a novel algorithm named Pre-trained Embodiment-Aware Control (PEAC). In PEAC, we first train an embodiment discriminator $q_{\\theta}(e|\\tau)$ to approximate $p_{\\pi}(e|\\tau)$ , which can learn the embodiment context via historical trajectories. For cross-embodiment pre-training, PEAC then utilizes our cross-embodiment intrinsic reward, which is defined following Eq. (4) as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathrm{CE}}(\\tau)\\triangleq\\log p(e)-\\log q_{\\theta}(e|\\tau).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Assuming the embodiment prior $p(e)$ is fixed, $\\mathcal{R}_{\\mathrm{CE}}$ encourages the agent to explore the region with low $\\log q_{\\theta}(e|\\tau)$ . In these trajectories, the embodiment discriminator is misled, where the agent may not have explored enough or different embodiment posteriors are similar. Thus, the embodiment discriminator can boost itself from these trajectories and learned embodiment-aware contexts that can effectively represent different embodiments, which benefit generalizing to unseen embodiments. ", "page_idx": 4}, {"type": "text", "text": "In practice, $\\mathcal{R}_{\\mathrm{CE}}$ needs to be calculated for each state $\\pmb{s}$ rather than the whole trajectory $\\tau$ , also, the embodiment discriminator needs to classify the embodiment context for every state. For RL backbones that encode historical information as the hidden state $^h$ like Dreamer [17, 18, 64], we directly train $q_{\\theta}(e|h,s)$ as the discriminator and further calculate $\\mathcal{R}_{\\mathrm{CE}}$ . For RL algorithms with Markovian policies like PPO [50], we encode a fixed length historical state-action pair to the hidden state $h$ and also train $q_{\\theta}(e|h,s)$ , following [28]. For a fair comparison, our policy still uses Markovian policy and does not utilize encoded historical messages. PEAC\u2019s pseudo-code is in Appendix C. ", "page_idx": 4}, {"type": "image", "img_path": "LyAFfdx8YF/tmp/b176aa1d37faedb71aec2d43c23b8e325901a0fe28eff7f6510f5e77eba23e92.jpg", "img_caption": ["Figure 2: Benchmark environments, including DMC [56], Robosuite [73], Isaacgym [37]. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Cross-Embodiment Exploration and Skill Discovery ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As shown above, PEAC pre-trains the agent for the optimal initialization to few-shot handle downstream tasks across embodiments. Besides, although PEAC does not directly explore or discover skills, it is flexible to combine with existing unsupervised RL methods, including exploration and skill discovery ones, to achieve cross-embodiment exploration and skill discovery. Below we will discuss in detail the specific combination between PEAC and these two classes respectively, exporting two practical combination algorithms, PEAC-LBS and PEAC-DIAYN, as examples. ", "page_idx": 5}, {"type": "text", "text": "Embodiment-Aware Exploration. Existing exploration methods mainly encourage the agent to explore unseen regions. As PEAC suggests the agent explores the region where the embodiment discriminator is wrong, it is natural to directly combine $\\mathcal{R}_{\\mathrm{CE}}$ and exploration intrinsic rewards to achieve cross-embodiment exploration, i.e., balancing embodiment representation learning and unseen state exploration. As an example, we take LBS [38], of which the intrinsic reward is the KL divergence between the latent prior and the approximation posterior, as the PEAC-LBS. As $\\mathcal{R}_{\\mathrm{CE}}$ and $\\mathcal{R}_{\\mathrm{LBS}}$ are both related to some KL divergence, we can directly add up these two intrinsic rewards with the same weight in PEAC-LBS, of which the detailed pseudo-code is in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "Embodiment-Aware Skill Discovery. Single-embodiment skill-discovery mainly maximizes the mutual information between trajectories $\\tau$ and skills $_{\\textit{z}}$ as $\\mathcal{Z}(\\tau;z)=D_{\\mathrm{KL}}(p(\\tau,z)\\|p(\\tau)p(z))$ [10], which has been shown as optimal initiation to some skill-based adaptation objective [11]. We combine it and our cross-embodiment fine-tuning objective Eq. (2) to propose a unified cross-embodiment skill-based adaptation objective as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{s}(\\pi,\\pi^{*},\\mathcal{R}_{\\mathrm{ext}},e)\\triangleq\\!\\mathbb{E}_{p_{\\mathcal{M}_{e}^{c},\\pi^{*}}(\\tau)}[\\mathcal{R}_{\\mathrm{ext}}(\\tau)]-\\underset{z^{*}}{\\operatorname*{max}}\\mathbb{E}_{p_{\\mathcal{M}_{e}^{c},\\pi}(\\tau|z^{*})}[\\mathcal{R}_{\\mathrm{ext}}(\\tau)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\beta D_{\\mathrm{KL}}\\big(p_{\\mathcal{M}_{e}^{c},\\pi^{*}}(\\tau)\\|p_{\\mathcal{M},\\pi}(\\tau)\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Similar to Theorem 3.2, we can define our pre-training objective and simplify it as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{U}_{s}(\\pi,\\mathcal{E})\\triangleq\\mathbb E_{e\\sim\\mathcal{E}}\\underset{\\mathcal{R}_{\\mathrm{cut}}(\\tau)}{\\operatorname*{min}}\\underset{p_{\\mathcal{M}_{\\epsilon}^{\\epsilon},\\pi^{*}}(\\tau)}{\\operatorname*{max}}\\mathcal{F}_{s}(\\pi,\\pi^{*},\\mathcal{R}_{\\mathrm{ext}},e)}\\\\ &{=-\\,\\beta\\mathbb E_{e}\\underset{p(z|{\\cal M}_{\\epsilon}^{\\epsilon})}{\\operatorname*{max}}\\left[\\mathbb{E}_{\\tau\\sim p_{{\\cal M}_{\\epsilon},\\pi}}\\log\\frac{p_{\\pi}(e|\\tau)}{p_{\\pi}(e)}+D_{\\mathrm{KL}}(p_{\\pi}(\\tau,z|{\\cal M}_{e}^{c})||p_{\\pi}(z|{\\cal M}_{e}^{c})p_{\\pi}(\\tau|{\\cal M}_{e}^{c}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof of Eq. (7) is in Appendix A.3, where we also show it is a general form of Theorem 3.2 and the single-embodiment skill-discovery result [11]. The result of Eq. (7) includes two terms for handling cross-embodiment and discovering skills respectively. In detail, the first term is the same as the objective in Eq. (4), thus we can directly optimize it via PEAC. As the second term is similar to the classical skill-discover objective $\\mathcal{T}(\\tau;z)$ but only embodiment-aware, we can extend existing skill-discovery methods into an embodiment-aware version for handling it. ", "page_idx": 5}, {"type": "text", "text": "We take DIAYN [10] as an example, resulting in PEAC-DIAYN. Overall, In the pre-training stage, given a random skill $_{\\textit{z}}$ and an embodiment $^e$ , we will sample trajectories with the policy $\\pi_{\\boldsymbol{\\theta}}(a|s,z,e)$ that is conditioned on $z$ and the predicted embodiment context. Then we will train a neural network $p(z,e|\\tau)$ to jointly predict the current skill and the embodiment. For training the policy, we combine $\\mathcal{R}_{\\mathrm{CE}}$ and $\\mathcal{R}_{\\mathrm{DAYN}}$ as the intrinsic reward. During fine-tuning, we utilize the embodiment discriminator, mapping observed trajectories to infer the embodiment context. We then train an embodiment-aware meta-controller $\\pi(z|e,\\tau)$ , which inputs the state and predicted context and then outputs the skill. It extends existing embodiment-agnostic meta-controller [39] and directly chooses from skill spaces rather than complicated action spaces. The pseudo-code of PEAC-DIAYN is in Appendix C. ", "page_idx": 5}, {"type": "image", "img_path": "LyAFfdx8YF/tmp/d5dca1d01ede9e02a38738c45af51dc27bd08e73d150641f531c1908630e4b4d.jpg", "img_caption": ["Figure 3: Aggregate metrics [2] in state-based DMC. Each statistic for every algorithm has 120 runs (3 embodiment settings $\\times\\,4$ downstream tasks $\\times\\,10$ seeds). "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "LyAFfdx8YF/tmp/25be5b438b46bbbdb05f7feb974ef15d67c996427e69734ff6d23d0f34eb0bb5.jpg", "img_caption": ["Figure 4: Aggregate metrics [2] in image-based DMC. Each statistic for every algorithm has 36 runs (3 embodiment settings $\\times\\,4$ downstream tasks $\\times\\,3$ seeds). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now present extensive empirical results to answer the following questions: ", "page_idx": 6}, {"type": "text", "text": "\u2022 Does PEAC enhance the cross-embodiment unsupervised pre-training for handling different downstream tasks? (Sec. 5.2)   \n\u2022 Can CEURL benefit cross-embodiment RL and effectively generalize to unseen embodiments? (Sec. 5.3)   \n\u2022 Does CEURL advantage to real-world cross-embodiment applications? (Sec. 5.4) ", "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To fully evaluate PEAC in CEURL, we choose extensive benchmarks (Fig. 2), including state-based / image-based Deepmind Control Suite (DMC) [56] in URLB [27], Robosuite [73, 69] for robotic manipulation, and Isaacgym [37] for simulation as well as real-world legged locomotion. Below we will introduce embodiments, tasks, and baselines for these settings, with more details in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "State-based DMC & Image-based DMC. These two benchmarks extend URLB [27], classical single-embodiment unsupervised RL settings. Based on basic embodiments, we change the mass or damping to conduct three distinct embodiment distributions: walker-mass, quadruped-mass, and quadruped-damping, following previous work with diverse embodiments [28, 65]. All downstream tasks follow URLB. These two settings take robot states and images as observations respectively. ", "page_idx": 6}, {"type": "text", "text": "In state-based DMC, we compare PEAC with 5 exploration and 5 skill-discovery methods: ICM [45], RND [7], Disagreement [46], ProtoRL [62], LBS [38], DIAYN [10], SMM [30], APS [34], CIC [26], and BeCL [61], which are standard and SOTA for this setting. For all methods, we take DDPG [33] as the RL backbone, which is widely used in this benchmark [27]. In image-based DMC, we take 5 exploration baselines: ICM, RND, Plan2Explore [51], APT [35], and LBS; as well as 4 skill-discovery baselines: DIAYN, APS, LSD [42], and CIC. Also, we choose a SOTA baseline Choreographer [39], which combines exploration and skill discovery. For all methods, we take DreamerV2 [18] as the backbone algorithm, which has currently shown leading performance in this benchmark [48]. ", "page_idx": 6}, {"type": "text", "text": "Robosuite. We further consider embodiment distribution with greater change: different robotic arms for manipulation tasks from Robosuite [73]. We pre-train our agents in robotic arms Panda, IIWA, and Kinova3. Besides, we take robotic arm Jaco for evaluating generalization. Following [69], we take DrQ [63] as the RL backbone and choose standard task settings: Door, Lift, and TwoArmPegInHole. ", "page_idx": 6}, {"type": "image", "img_path": "LyAFfdx8YF/tmp/544f98f8bfa3b0a82340b892a42d60f7b95d49897684d96887d24d3fd9d97726.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Isaacgym. To explore CEURL in realistic environments, we design embodiment distributions based on the Unitree A1 robot in Isaacgym simulation [37], which is widely used for the real-world legged robot control [1, 74]. As A1 owns 12 controllable joints, we design A1-disabled, a uniform distribution of 12 embodiments, each with a joint failure, respectively. It is realistic as robots may damage some joints when deploying in the real world, and they are still required to complete tasks to their best. We choose standard RL backbone PPO [50] and five downstream tasks: run, climb, leap, crawl, and tilt, following [74]. We take classical baselines for Robosuite and Isaacgym: ICM, RND, and LBS. Besides, we have deployed Aliengo robots with different failure joints to evaluate the effectiveness of PEAC in real-world applications. ", "page_idx": 7}, {"type": "text", "text": "5.2 Evaluation of PEAC ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "State-based DMC. We first report results in state-based DMC to show that PEAC can facilitate crossembodiment pre-training. All algorithms, repeated 10 random seeds, are pre-trained 2M timesteps in reward-free environments with different embodiments, followed by fine-tuned downstream tasks for all these embodiments with $100\\mathrm{k}$ timesteps. We train DDPG agents for each downstream task 2M steps to get the expert return and calculate the expert normalized score for each method. Following [2], in Fig. 3, we report mean, median, interquartile mean (IQM), and optimality gap (OG) metrics along with stratified bootstrap confidence intervals. Fig. 3 demonstrates that PEAC substantially outperforms other baselines on all metrics, indicating that our cross-embodiment intrinsic reward contributes positively to downstream tasks across different embodiments. Notably, compared with BeCL and CIC which get the second and third scores, PEAC not only has higher performance but also a smaller confidence interval, highlighting its stability. Appendix B.4 reports detailed results of these statistics (Table 8) and individual results for each downstream task (Table 9). ", "page_idx": 7}, {"type": "text", "text": "Image-based DMC. As described in Sec. 4, PEAC can flexibly combine with existing unsupervised RL methods. To verify it, we evaluate PEAC-LBS and PEAC-DIAYN in image-based DMC. The pre-training and fine-tuning steps are still 2M and 100k respectively. Also, we present four metrics: Median, IQM, Mean, and OG with stratified bootstrap confidence intervals in Fig. 4. Taking IQM as our primary metric, PEAC-LBS not only has a higher value but also a relatively smaller confidence interval, indicating its better stability. As mentioned in [39], pure skill-discovery methods like DIAYN struggle on this benchmark with a certain gap compared to exploratory method. The phenomenon seems more pronounced in cross-embodiment setting than single-embodiment setting, which might be because of the increased difficulty of finding consistent skills across embodiments. As PEAC-DIAYN discovers skills across-embodiment, it consistently leads in performance compared with all other pure skill discovery methods across all four statistics. In Appendix B.5, we report detailed results of these statistics in Table 10 and detailed results for all downstream tasks in Table 11. ", "page_idx": 7}, {"type": "image", "img_path": "LyAFfdx8YF/tmp/2e2152c4f1f0aa3744046b6048db43a406427145cafd712598af08ef4e1dbadd.jpg", "img_caption": ["Figure 7: Real-world results. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "LyAFfdx8YF/tmp/d32f910924b5e37c5a71e81e24b5af253519eb232238478ebcb781517ada26c3.jpg", "img_caption": ["Figure 6: Ablation studies on pre-training timesteps. ", "Robosuite. Besides, we validate PEAC in a more challenging setting Robosuite where different embodiments own different robotic arms (subfigures 3-6 in Fig. 2). As shown in Table 1, PEAC still significantly outperforms all baselines in both training and testing embodiments, demonstrating its powerful cross-embodiment ability and better generalization ability. The detailed results of each robotic arm are in Table 12 of Appendix B.6. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Ablation studies. We do several ablation studies in image-based DMC to clarify the contribution of PEAC better. First, we evaluate the effectiveness of pre-trained steps in fine-tuned performance. We pre-train agents for 100k, 500k, 1M, and 2M steps and then fine-tune them for 100k steps. As shown in Fig. 6, all algorithms improve with pre-training timesteps increasing, indicating that cross-embodiment pre-training effectively benefits fast handling downstream tasks. PEAC-LBS becomes the best-performing method from 1M steps on and PEAC-DIAYN significantly exceeds skill discovery methods. This suggests that PEAC excels at handling cross-embodiment tasks with increased pre-training steps. Additional results are in Appendix B.7. Besides pre-training steps, we also do more ablations studies of different components in PEAC to verify their effectiveness in Appendix B.8. For example, we evaluate the stability of PEAC-LBS in DMC-image under different $\\beta$ (we set it as 1.0 in all main experiments), which is the trade-off parameter for balancing the policy improvement term and the policy constraint term. Moreover, we also do an ablation study on our embodiment discriminator to verify the contribution of each component in our PEAC. More results and analyses are in Appendix B.8. ", "page_idx": 8}, {"type": "text", "text": "5.3 Generalization to Unseen Embodiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To answer the second question, we further assess the generalization ability of PEAC to unseen embodiments. First, we directly leverage pre-trained agents to zero-shot sample trajectories with different unseen embodiments and then visualize results through t-SNE [57] in Fig. 5, where different colored points represent states sampled via different embodiments. As shown in Fig. 5, PEAC-LBS can distinguish different embodiments\u2019 states more effectively compared to LBS, which is difficult to distinguish them (more results are in Appendix B.9). Furthermore, we evaluate the generalization ability of fine-tuned agents for all methods by zero-shot evaluating them with unseen embodiments and the same downstream task. In Table 2, we report the detailed generalization results of all 3 domains about state-based DMC and image-based DMC. The results demonstrate that the fine-tuned agents of PEAC can successfully handle the same downstream task with unseen embodiments, which illustrates that PEAC effectively learns cross-embodiment knowledge. Detailed results for each downstream task are in Appendix B.10 (Table 16-17). ", "page_idx": 8}, {"type": "text", "text": "5.4 Real-World Applications ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To validate CEURL in more realistic settings, we conduct results based on legged locomotion in Isaacgym, which is widely used for real-world applications. First, we present simulation results of A1-disabled in Table 1, with 100M pre-train timesteps and 10M fine-tune timesteps. As shown in Table 1, PEAC effectively establishes a good initialization model across embodiments with different joint failures and quickly adapts to downstream tasks, especially for challenging climb and leap tasks. ", "page_idx": 8}, {"type": "text", "text": "Besides, we have deployed PEAC fine-tuned agents in real-world Aliengo-disabled robots, i.e., Aliengo robots with different failure joints. As shown in Fig. 7, due to joint failure, the movement ability of the robot is limited compared to normal settings, but the robot still demonstrates strong adaptability on various terrains not seen in simulators. More images and videos of real-world applications are in Appendix B.12. ", "page_idx": 8}, {"type": "text", "text": "5.5 Limitations and Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In terms of limitations, we assume that different embodiments may own similar structures so that we can pre-train a unified agent for them. As a result, it might be challenging for PEAC to handle extremely different embodiments. Also, existing unsupervised RL methods still struggle to handle more challenging downstream tasks. In Appendix B.11, we take the first step to evaluate several more challenging downstream tasks and more different embodiment distributions, of which the results show that PEAC can still perform better than baselines. Designing more efficient cross-embodiment unsupervised algorithms for these more difficult and practical settings are interesting future directions. The Broader Impact os this work is discussed in Appendix D. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose to analyze cross-embodiment RL in an unsupervised RL perspective as CEURL, i.e., pre-training in an embodiment distribution. We formulate it as CE-MDP, with some more challenging properties than the single-embodiment setting. By analyzing the optimal cross-embodiment initialization, we propose PEAC with a principled intrinsic reward function and further show that PEAC can flexibly combine with existing unsupervised RL. Experimental results demonstrate that PEAC can effectively handle downstream tasks across embodiments for extensive settings, ranging from image-based observation, state-based observation, and real-world legged locomotion. We hope this work can encourage further research in developing RL agents for both task generalization and embodiment generalization, especially in real-world control. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by NSFC Projects (Nos. 92248303, 92370124, 62350080, 62276149, 62061136001), BNRist (BNR2022RC01006), Tsinghua Institute for Guo Qiang, and the High Performance Computing Center, Tsinghua University. J. Zhu was also supported by the XPlorer Prize. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Ananye Agarwal, Ashish Kumar, Jitendra Malik, and Deepak Pathak. Legged locomotion in challenging terrains using egocentric vision. In Conference on robot learning, pages 403\u2013415. PMLR, 2023.   \n[2] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34:29304\u201329320, 2021.   \n[3] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:24639\u201324654, 2022.   \n[4] Michael Beukman, Devon Jarvis, Richard Klein, Steven James, and Benjamin Rosman. Dynamics generalisation in reinforcement learning via adaptive context-aware policies. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023.   \n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[7] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. In International Conference on Learning Representations, 2018.   \n[8] Yevgen Chebotar, Quan Vuong, Karol Hausman, Fei Xia, Yao Lu, Alex Irpan, Aviral Kumar, Tianhe Yu, Alexander Herzog, Karl Pertsch, et al. Q-transformer: Scalable offline reinforcement learning via autoregressive q-functions. In Conference on Robot Learning, pages 3909\u20133928. PMLR, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[9] Thomas M Cover and Joy A Thomas. Elements of information theory. 1991.   \n[10] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. In International Conference on Learning Representations, 2018.   \n[11] Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. The information geometry of unsupervised reinforcement learning. In International Conference on Learning Representations, 2021.   \n[12] Gilbert Feng, Hongbo Zhang, Zhongyu Li, Xue Bin Peng, Bhuvan Basireddy, Linzhu Yue, Zhitao Song, Lizhi Yang, Yunhui Liu, Koushil Sreenath, et al. Genloco: Generalized locomotion controllers for quadrupedal robots. In Conference on Robot Learning, pages 1893\u20131903. PMLR, 2023.   \n[13] Dibya Ghosh, Chethan Anand Bhateja, and Sergey Levine. Reinforcement learning from passive data via latent intentions. In International Conference on Machine Learning, pages 11321\u201311339. PMLR, 2023.   \n[14] Dibya Ghosh, Jad Rahme, Aviral Kumar, Amy Zhang, Ryan P Adams, and Sergey Levine. Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability. Advances in Neural Information Processing Systems, 34:25502\u201325515, 2021.   \n[15] Kevin Gmelin, Shikhar Bahl, Russell Mendonca, and Deepak Pathak. Efficient rl via disentangled environment and agent representations. In International Conference on Machine Learning, pages 11525\u2013 11545. PMLR, 2023.   \n[16] Agrim Gupta, Silvio Savarese, Surya Ganguli, and Li Fei-Fei. Embodied intelligence via learning and evolution. Nature communications, 12(1):5721, 2021.   \n[17] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2019.   \n[18] Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In International Conference on Learning Representations, 2020.   \n[19] Assaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual markov decision processes. arXiv preprint arXiv:1502.02259, 2015.   \n[20] Steven Hansen, Will Dabney, Andre Barreto, David Warde-Farley, Tom Van de Wiele, and Volodymyr Mnih. Fast task inference with variational intrinsic successor features. In International Conference on Learning Representations, 2019.   \n[21] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000\u201316009, 2022.   \n[22] Shuncheng He, Yuhang Jiang, Hongchang Zhang, Jianzhun Shao, and Xiangyang Ji. Wasserstein unsupervised reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6884\u20136892, 2022.   \n[23] Edward S Hu, Kun Huang, Oleh Rybkin, and Dinesh Jayaraman. Know thyself: Transferable visual control policies through robot-awareness. In International Conference on Learning Representations, 2021.   \n[24] Zheyuan Jiang, Jingyue Gao, and Jianyu Chen. Unsupervised skill discovery via recurrent skill training. Advances in Neural Information Processing Systems, 35:39034\u201339046, 2022.   \n[25] Jaekyeom Kim, Seohong Park, and Gunhee Kim. Unsupervised skill discovery with bottleneck option learning. In International Conference on Machine Learning, pages 5572\u20135582. PMLR, 2021.   \n[26] Michael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats, Aravind Rajeswaran, and Pieter Abbeel. Unsupervised reinforcement learning with contrastive intrinsic control. Advances in Neural Information Processing Systems, 35:34478\u201334491, 2022.   \n[27] Michael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Lerrel Pinto, and Pieter Abbeel. Urlb: Unsupervised reinforcement learning benchmark. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.   \n[28] Kimin Lee, Younggyo Seo, Seunghyun Lee, Honglak Lee, and Jinwoo Shin. Context-aware dynamics model for generalization in model-based reinforcement learning. In International Conference on Machine Learning, pages 5757\u20135766. PMLR, 2020.   \n[29] Kuang-Huei Lee, Ofir Nachum, Mengjiao Sherry Yang, Lisa Lee, Daniel Freeman, Sergio Guadarrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk Michalewski, et al. Multi-game decision transformers. Advances in Neural Information Processing Systems, 35:27921\u201327936, 2022.   \n[30] Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and Ruslan Salakhutdinov. Efficient exploration via state marginal matching. arXiv preprint arXiv:1906.05274, 2019.   \n[31] Sang-Hyun Lee and Seung-Woo Seo. Unsupervised skill discovery for learning shared structures across changing environments. In International Conference on Machine Learning, pages 19185\u201319199. PMLR, 2023.   \n[32] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.   \n[33] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.   \n[34] Hao Liu and Pieter Abbeel. Aps: Active pretraining with successor features. In International Conference on Machine Learning, pages 6736\u20136747. PMLR, 2021.   \n[35] Hao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. Advances in Neural Information Processing Systems, 34:18459\u201318473, 2021.   \n[36] Xin Liu, Yaran Chen, Haoran Li, Boyu Li, and Dongbin Zhao. Cross-domain random pre-training with prototypes for reinforcement learning. arXiv preprint arXiv:2302.05614, 2023.   \n[37] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: High performance gpu based physics simulation for robot learning. In Thirty-ffith Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.   \n[38] Pietro Mazzaglia, Ozan Catal, Tim Verbelen, and Bart Dhoedt. Curiosity-driven exploration via latent bayesian surprise. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 7752\u20137760, 2022.   \n[39] Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Alexandre Lacoste, and Sai Rajeswar. Choreographer: Learning and adapting skills in imagination. In The Eleventh International Conference on Learning Representations, 2022.   \n[40] Mirco Mutti, Mattia Mancassola, and Marcello Restelli. Unsupervised reinforcement learning in multiple environments. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 7850\u20137858, 2022.   \n[41] Fei Ni, Jianye Hao, Yao Mu, Yifu Yuan, Yan Zheng, Bin Wang, and Zhixuan Liang. Metadiffuser: Diffusion model as conditional planner for offilne meta-rl. In International Conference on Machine Learning, pages 26087\u201326105. PMLR, 2023.   \n[42] Seohong Park, Jongwook Choi, Jaekyeom Kim, Honglak Lee, and Gunhee Kim. Lipschitz-constrained unsupervised skill discovery. arXiv preprint arXiv:2202.00914, 2022.   \n[43] Seohong Park, Kimin Lee, Youngwoon Lee, and Pieter Abbeel. Controllability-aware unsupervised skill discovery. arXiv preprint arXiv:2302.05103, 2023.   \n[44] Seohong Park, Oleh Rybkin, and Sergey Levine. Metra: Scalable unsupervised rl with metric-aware abstraction. arXiv preprint arXiv:2310.08887, 2023.   \n[45] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning, pages 2778\u20132787. PMLR, 2017.   \n[46] Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement. In International conference on machine learning, pages 5062\u20135071. PMLR, 2019.   \n[47] Vitchyr Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine. Skew-fit: State-covering self-supervised reinforcement learning. In International Conference on Machine Learning, pages 7783\u20137792. PMLR, 2020.   \n[48] Sai Rajeswar, Pietro Mazzaglia, Tim Verbelen, Alexandre Pich\u00e9, Bart Dhoedt, Aaron Courville, and Alexandre Lacoste. Mastering the unsupervised reinforcement learning benchmark from pixels. In International Conference on Machine Learning, pages 28598\u201328617. PMLR, 2023.   \n[49] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio G\u00f3mez Colmenarejo, Alexander Novikov, Gabriel Barth-maron, Mai Gim\u00e9nez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. Transactions on Machine Learning Research, 2022.   \n[50] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[51] Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. Planning to explore via self-supervised world models. In International Conference on Machine Learning, pages 8583\u20138592. PMLR, 2020.   \n[52] Milad Shafiee, Guillaume Bellegarda, and Auke Ijspeert. Manyquadrupeds: Learning a single locomotion policy for diverse quadruped robots. arXiv preprint arXiv:2310.10486, 2023.   \n[53] Nur Muhammad Mahi Shafiullah and Lerrel Pinto. One after another: Learning incremental skills for a changing world. In International Conference on Learning Representations, 2021.   \n[54] Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware unsupervised discovery of skills. In International Conference on Learning Representations, 2019.   \n[55] DJ Strouse, Kate Baumli, David Warde-Farley, Volodymyr Mnih, and Steven Stenberg Hansen. Learning more skills through optimistic exploration. In International Conference on Learning Representations, 2021.   \n[56] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.   \n[57] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.   \n[58] Mengda Xu, Zhenjia Xu, Cheng Chi, Manuela Veloso, and Shuran Song. Xskill: Cross embodiment skill discovery. In Conference on Robot Learning, pages 3536\u20133555. PMLR, 2023.   \n[59] Yifan Xu, Nicklas Hansen, Zirui Wang, Yung-Chieh Chan, Hao Su, and Zhuowen Tu. On the feasibility of cross-task transfer with model-based reinforcement learning. In The Eleventh International Conference on Learning Representations, 2022.   \n[60] Jonathan Yang, Catherine Glossop, Arjun Bhorkar, Dhruv Shah, Quan Vuong, Chelsea Finn, Dorsa Sadigh, and Sergey Levine. Pushing the limits of cross-embodiment learning for manipulation and navigation. arXiv preprint arXiv:2402.19432, 2024.   \n[61] Rushuai Yang, Chenjia Bai, Hongyi Guo, Siyuan Li, Bin Zhao, Zhen Wang, Peng Liu, and Xuelong Li. Behavior contrastive learning for unsupervised skill discovery. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 39183\u201339204. PMLR, 23\u201329 Jul 2023.   \n[62] Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Reinforcement learning with prototypical representations. In International Conference on Machine Learning, pages 11920\u201311931. PMLR, 2021.   \n[63] Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In International conference on learning representations, 2020.   \n[64] Chengyang Ying, Zhongkai Hao, Xinning Zhou, Hang Su, Songming Liu, Dong Yan, and Jun Zhu. Task aware dreamer for task generalization in reinforcement learning. arXiv preprint arXiv:2303.05092, 2023.   \n[65] Chengyang Ying, Xinning Zhou, Hang Su, Dong Yan, Ning Chen, and Jun Zhu. Towards safe reinforcement learning via constraining conditional value-at-risk. arXiv preprint arXiv:2206.04436, 2022.   \n[66] Chen Yu, Weinan Zhang, Hang Lai, Zheng Tian, Laurent Kneip, and Jun Wang. Multi-embodiment legged robot control as a sequence modeling problem. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 7250\u20137257. IEEE, 2023.   \n[67] Mingqi Yuan, Bo Li, Xin Jin, and Wenjun Zeng. Automatic intrinsic reward shaping for exploration in deep reinforcement learning. arXiv preprint arXiv:2301.10886, 2023.   \n[68] Zhecheng Yuan, Tianming Wei, Shuiqi Cheng, Gu Zhang, Yuanpei Chen, and Huazhe Xu. Learning to manipulate anywhere: A visual generalizable framework for reinforcement learning. arXiv preprint arXiv:2407.15815, 2024.   \n[69] Zhecheng Yuan, Sizhe Yang, Pu Hua, Can Chang, Kaizhe Hu, and Huazhe Xu. Rl-vigen: A reinforcement learning benchmark for visual generalization. Advances in Neural Information Processing Systems, 36, 2024.   \n[70] Kevin Zakka, Andy Zeng, Pete Florence, Jonathan Tompson, Jeannette Bohg, and Debidatta Dwibedi. Xirl: Cross-embodiment inverse reinforcement learning. In Conference on Robot Learning, pages 537\u2013546. PMLR, 2022.   \n[71] Amy Zhang, Shagun Sodhani, Khimya Khetarpal, and Joelle Pineau. Learning robust state abstractions for hidden-parameter block mdps. arXiv preprint arXiv:2007.07206, 2020.   \n[72] Andrew Zhao, Matthieu Lin, Yangguang Li, Yong-Jin Liu, and Gao Huang. A mixture of surprises for unsupervised reinforcement learning. Advances in Neural Information Processing Systems, 35:26078\u2013 26090, 2022.   \n[73] Yuke Zhu, Josiah Wong, Ajay Mandlekar, Roberto Mart\u00edn-Mart\u00edn, Abhishek Joshi, Soroush Nasiriany, and Yifeng Zhu. robosuite: A modular simulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293, 2020.   \n[74] Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christopher G Atkeson, S\u00f6ren Schwertfeger, Chelsea Finn, and Hang Zhao. Robot parkour learning. In 7th Annual Conference on Robot Learning, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Proof of Theorems ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we will provide detailed proof of theorems in the paper. ", "page_idx": 14}, {"type": "text", "text": "A.1 Properties and Challenges of $\\mathcal{D}^{\\mathcal{E}}$ ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We first construct an example to show that vertices of $\\mathcal{D}^{\\mathcal{E}}$ may no longer be deterministic policies. Considering a simple embodiment distribution with only 2 embodiments $e_{1},e_{2}$ with the embodiment probability $\\begin{array}{r}{p(e_{1})\\,{\\stackrel{\\,.}{=}}\\,p(e_{2})=\\frac12}\\end{array}$ . For each embodiment, there are two states $s_{1},s_{2}$ and two actions $\\mathbf{\\Delta}a_{1},a_{2}$ and the dynamic is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{e_{1}}(s_{1}|s_{1},a_{1})=1,p_{e_{1}}(s_{2}|s_{1},a_{1})=0,p_{e_{1}}(s_{1}|s_{1},a_{2})=0,p_{e_{1}}(s_{2}|s_{1},a_{2})=1}\\\\ &{p_{e_{1}}(s_{1}|s_{2},a_{1})=0,p_{e_{1}}(s_{2}|s_{2},a_{1})=1,p_{e_{1}}(s_{1}|s_{2},a_{2})=1,p_{e_{1}}(s_{2}|s_{2},a_{2})=0}\\\\ &{p_{e_{2}}(s_{1}|s_{1},a_{1})=0,p_{e_{2}}(s_{2}|s_{1},a_{1})=1,p_{e_{2}}(s_{1}|s_{1},a_{2})=1,p_{e_{2}}(s_{2}|s_{1},a_{2})=0}\\\\ &{p_{e_{2}}(s_{1}|s_{2},a_{1})=1,p_{e_{2}}(s_{2}|s_{2},a_{1})=0,p_{e_{2}}(s_{1}|s_{2},a_{2})=0,p_{e_{2}}(s_{2}|s_{2},a_{2})=1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\underbrace{\\binom{a_{1}}{\\begin{array}{c c c}{a_{1}}&{a_{2}}&{a_{1}}\\\\ {s_{1}}&{\\ldots}&{\\binom{a_{1}}{s_{2}}}\\\\ {a_{2}}&{\\ldots}&{a_{1}}\\end{array}}}_{e_{1}}\\underbrace{\\binom{a_{2}}{s_{1}}}_{e_{2}}\\underbrace{\\binom{a_{1}}{s_{2}}}_{a_{1}}\\underbrace{\\binom{a_{2}}{s_{3}}}_{e_{3}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In this setting, there are four deterministic policies: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{1}(s_{1})=a_{1},\\pi_{1}(s_{2})=a_{1},\\quad\\pi_{2}(s_{1})=a_{1},\\pi_{2}(s_{2})=a_{2},}\\\\ &{\\pi_{3}(s_{1})=a_{2},\\pi_{3}(s_{2})=a_{1},\\quad\\pi_{4}(s_{1})=a_{2},\\pi_{4}(s_{2})=a_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For any policy $\\mu$ , we denote that $\\rho_{1,\\mu},\\rho_{2,\\mu}$ are the state distribution of $\\mu$ under the environment $\\mathcal{P}_{e_{1}}$ or $\\mathcal{P}_{e_{2}}$ respectively. Then we can calculate that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\rho_{1,\\pi_{1}}=\\left(\\frac{1}{2},\\frac{1}{2}\\right),\\rho_{2,\\pi_{1}}=\\left(\\frac{1}{2},\\frac{1}{2}\\right);}\\\\ {\\displaystyle\\rho_{1,\\pi_{2}}=\\left(\\frac{1+\\gamma}{2},\\frac{1-\\gamma}{2}\\right),\\rho_{2,\\pi_{2}}=\\left(\\frac{1-\\gamma}{2},\\frac{1+\\gamma}{2}\\right);}\\\\ {\\displaystyle\\rho_{1,\\pi_{3}}=\\left(\\frac{1-\\gamma}{2},\\frac{1+\\gamma}{2}\\right),\\rho_{2,\\pi_{3}}=\\left(\\frac{1+\\gamma}{2},\\frac{1-\\gamma}{2}\\right);}\\\\ {\\displaystyle\\rho_{1,\\pi_{4}}=\\left(\\frac{1}{2},\\frac{1}{2}\\right),\\rho_{2,\\pi_{4}}=\\left(\\frac{1}{2},\\frac{1}{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As the embodiment probability is $\\begin{array}{r}{p(e_{1})\\,=\\,p(e_{2})\\,=\\,\\frac{1}{2}}\\end{array}$ , all these four policy share the same state distribution as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\rho_{\\pi_{1}}=\\rho_{\\pi_{2}}=\\rho_{\\pi_{3}}=\\rho_{\\pi_{4}}=\\left(\\frac{1}{2},\\frac{1}{2}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Furthermore, we consider a stochastic policy $\\pi$ satisfies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pi(\\pmb{a}_{1}|s_{1})=1,\\pi(\\pmb{a}_{2}|s_{1})=0,\\quad\\pi(\\pmb{a}_{1}|s_{2})=\\frac{1}{2},\\pi(\\pmb{a}_{2}|s_{2})=\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Next we will calculate $\\rho_{1,\\pi}$ and $\\rho_{2,\\pi}$ . For $\\rho_{1,\\pi}$ , at the timestep 0, we have the initial state distribution as $\\begin{array}{r}{p_{0}(\\pmb{s}_{1})=p_{0}(\\pmb{s}_{2})=\\frac{1}{2}}\\end{array}$ , assume that at timestep $t$ we have corresponding $p_{t}(\\pmb{s}_{1}),p_{t}(\\pmb{s}_{2})$ , we can naturally get the recurrence relation as ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{t+1}(s_{1})=p_{t}(s_{1})+{\\frac{1}{2}}p_{t}(s_{2}),\\quad p_{t+1}(s_{2})={\\frac{1}{2}}p_{t}(s_{2}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Naturally, we have $\\begin{array}{r}{p_{t}(s_{1})=1-\\frac{1}{2^{t}},p_{t}(s_{2})=\\frac{1}{2^{t}}}\\end{array}$ and thus the discount state distribution of $s_{2}$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\n(1-\\gamma)\\sum_{t=0}^{\\infty}{\\frac{\\gamma^{\\,t}}{2}}={\\frac{1-\\gamma}{2-\\gamma}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "And we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\rho_{1,\\pi}=\\left(\\frac{1}{2-\\gamma},\\frac{1-\\gamma}{2-\\gamma}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, we can calculate $\\rho_{2,\\pi}$ . At the timestep 0, we have the initial state distribution as $p_{0}(s_{1})=$ $\\begin{array}{r}{p_{0}\\big(\\pmb{s}_{2}\\big)=\\frac{1}{2}}\\end{array}$ , assume that at timestep $t$ we have corresponding $p_{t}(s_{1}),p_{t}(s_{2})$ , we can naturally get the recurrence relation as ", "page_idx": 15}, {"type": "equation", "text": "$$\np_{t+1}(s_{1})={\\frac{1}{2}}p_{t}(s_{2}),\\quad p_{t+1}(s_{2})=p_{t}(s_{1})+{\\frac{1}{2}}p_{t}(s_{2}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As $p_{t}(\\pmb{s}_{1})+p_{t}(\\pmb{s}_{2})=1$ , we can solve this recurrence relation via ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{p_{t+1}(s_{1})=\\displaystyle\\frac12p_{t}(s_{2})=\\displaystyle\\frac12-\\frac12p_{t}(s_{1}),}}\\\\ {{(-2)^{t+1}p_{t+1}(s_{1})=(-2)^{t}p_{t}(s_{1})-(-2)^{t}}}\\\\ {{=-\\displaystyle\\dots=(-2)^{0}p_{0}(s_{1})-\\left((-2)^{t}+(-2)^{t-1}+\\ldots+(-2)^{0}\\right)}}\\\\ {{=\\displaystyle\\frac12-\\frac1{3}-(-2)^{t+1}=\\displaystyle\\frac16+\\frac{(-2)^{t+1}}3,}}\\\\ {{p_{t+1}(s_{1})=\\displaystyle\\frac1{6\\times(-2)^{t+1}}+\\frac13}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus the discount state distribution of $s_{1}$ is ", "page_idx": 15}, {"type": "equation", "text": "$$\n(1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^{t}\\left(\\frac{1}{6\\times(-2)^{t}}+\\frac{1}{3}\\right)=\\frac{1}{3}+\\frac{1-\\gamma}{6}\\sum_{t=0}^{\\infty}\\left(-\\frac{\\gamma}{2}\\right)^{t}=\\frac{1}{3}+\\frac{1-\\gamma}{6}\\frac{1}{1+\\frac{\\gamma}{2}}=\\frac{1}{2+\\gamma}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "And we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\rho_{2,\\pi}=\\left(\\frac{1}{2+\\gamma},\\frac{1+\\gamma}{2+\\gamma}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As the embodiment probability is $\\begin{array}{r}{p(e_{1})=p(e_{2})=\\frac{1}{2}}\\end{array}$ , the state distribution of $\\pi$ is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\rho_{\\pi}=\\left(\\frac{2}{4-\\gamma^{2}},\\frac{2-\\gamma^{2}}{4-\\gamma^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Taking any $\\gamma\\in(0,1)$ , it is obvious that $\\rho_{\\pi}$ is not within the closure composed of $\\rho_{\\pi_{1}},\\rho_{\\pi_{2}},\\rho_{\\pi_{3}},\\rho_{\\pi_{4}}$ (actually the point $(1/2,1/2))$ . Thus we have explained that the vertices of $\\mathcal{D}^{\\mathcal{E}}$ might no longer be simple deterministic policies. ", "page_idx": 15}, {"type": "text", "text": "A.2 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Recall that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F(\\pi,\\pi^{*},\\mathcal{R}_{\\mathrm{ext}},\\epsilon)\\triangleq\\left[\\mathbb{E}_{p_{M_{\\epsilon}^{c},\\pi^{*}}(\\tau)}[\\mathcal{R}_{\\mathrm{ext}}(\\tau)]-\\mathbb{E}_{p_{M_{\\epsilon}^{c},\\pi}(\\tau)}[\\mathcal{R}_{\\mathrm{ext}}(\\tau)]-\\beta D_{\\mathrm{KL}}(p_{M_{\\epsilon}^{c},\\pi^{*}}(\\tau)||p_{\\tilde{M},\\pi}(\\tau))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We set a functional $f$ satisfying that ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(p(\\tau))=\\mathbb{E}_{p(\\tau)}[\\mathcal{R}_{\\mathrm{ext}}(\\tau)]-\\mathbb{E}_{p_{\\mathcal{M}_{e}^{c},\\pi}(\\tau)}[\\mathcal{R}_{\\mathrm{ext}}(\\tau)]-\\beta D_{\\mathrm{KL}}(p(\\tau)\\|p_{\\tilde{\\mathcal{M}},\\pi}(\\tau)).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using the calculus of variations, we can calculate its optimal value at the point $p^{*}$ satisfying that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\mathrm{ext}}(\\tau)=\\beta\\log\\frac{p^{*}(\\tau)}{p_{\\bar{\\mathcal{M}},\\pi}(\\tau)}+b\\beta,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "here $b$ is a constant not related to $p^{*}$ , and we have $p^{*}(\\tau)=p_{\\bar{\\mathcal{M}},\\pi}(\\tau)e^{\\frac{\\mathcal{R}_{\\mathrm{ext}}(\\tau)}{\\beta}-b}$ . As $\\begin{array}{r}{\\int p^{*}(\\tau)=1}\\end{array}$ , we can calculate that ", "page_idx": 15}, {"type": "equation", "text": "$$\nb=\\log\\int p_{\\bar{\\mathcal{M}},\\pi}(\\tau)e^{\\frac{\\mathcal{R}_{\\mathrm{ext}}(\\tau)}{\\beta}}d\\tau,\\quad p^{*}(\\tau)=\\frac{p_{\\bar{\\mathcal{M}},\\pi}(\\tau)e^{\\frac{\\mathcal{R}_{\\mathrm{ext}}(\\tau)}{\\beta}}}{\\int p_{\\bar{\\mathcal{M}},\\pi}(\\tau)e^{\\frac{\\mathcal{R}_{\\mathrm{ext}}(\\tau)}{\\beta}}d\\tau}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consequently, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{p_{M_{\\epsilon}^{\\epsilon},\\pi^{*}}(\\tau)}{\\mathrm{max}}\\dot{\\mathcal{F}}(\\pi,\\pi^{*},\\mathcal{R}_{\\mathrm{ext}},e)=\\mathbb{E}_{p^{*}(\\tau)}[\\mathcal{R}_{\\mathrm{ext}}(\\tau)]-\\mathbb{E}_{p_{M_{\\epsilon}^{\\epsilon},\\pi}(\\tau)}[\\mathcal{R}_{\\mathrm{ext}}(\\tau)]-\\beta D_{\\mathrm{KL}}(p^{*}(\\tau)\\|p_{\\bar{M},\\pi}(\\tau))}\\\\ &{=\\int p^{*}(\\tau)\\mathcal{R}_{\\mathrm{ext}}(\\tau)d\\tau-\\mathbb{E}_{p_{M_{\\epsilon}^{\\epsilon},\\pi}(\\tau)}[\\mathcal{R}_{\\mathrm{ext}}(\\tau)]-\\beta\\int p^{*}(\\tau)\\log\\frac{p^{*}(\\tau)}{p_{\\bar{M},\\pi}(\\tau)}d\\tau}\\\\ &{=\\int p^{*}(\\tau)\\mathcal{R}_{\\mathrm{ext}}(\\tau)d\\tau-\\mathbb{E}_{p_{M_{\\epsilon}^{\\epsilon},\\pi}(\\tau)}[\\mathcal{R}_{\\mathrm{ext}}(\\tau)]-\\beta\\int p^{*}(\\tau)\\frac{\\mathcal{R}_{\\mathrm{ext}}(\\tau)}{\\beta}d\\tau+\\beta\\log\\int p_{\\bar{M},\\pi}(\\tau)e^{\\frac{\\mathcal{R}_{\\mathrm{ext}}(\\tau)}{\\beta}}d\\tau}\\\\ &{=\\beta\\log\\int p_{\\bar{M},\\pi}(\\tau)e^{\\frac{\\mathcal{R}_{\\mathrm{ext}}(\\tau)}{\\beta}}d\\tau-\\mathbb{E}_{p_{M_{\\epsilon}^{\\epsilon},\\pi}(\\tau)}[\\mathcal{R}_{\\mathrm{ext}}(\\tau)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, we set a functional $g$ satisfying that ", "page_idx": 16}, {"type": "equation", "text": "$$\ng(r(\\tau))=\\beta\\log\\int p_{\\bar{\\mathcal{M}},\\pi}(\\tau)e^{\\frac{r(\\tau)}{\\beta}}d\\tau-\\mathbb{E}_{p_{\\mathcal{M}_{e}^{c},\\pi}(\\tau)}[r(\\tau)].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using the calculus of variations, we can calculate its optimal value at the point $r^{*}$ satisfying that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\frac{1}{\\beta}p_{\\bar{M},\\pi}(\\tau)e^{\\frac{r^{*}(\\tau)}{\\beta}}}{\\int p_{\\bar{M},\\pi}(\\tau)e^{\\frac{r^{*}(\\tau)}{\\beta}}d\\tau}=p_{\\mathcal{M}_{e}^{c},\\pi}(\\tau),\\quad\\frac{r^{*}(\\tau)}{\\beta}=\\log\\frac{p_{\\mathcal{M}_{e}^{c},\\pi}(\\tau)}{p_{\\bar{M},\\pi}(\\tau)}+\\log\\int p_{\\bar{M},\\pi}(\\tau)e^{\\frac{r^{*}(\\tau)}{\\beta}}d\\tau.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus we can calculate that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathcal{R}_{\\mathrm{cst}}(\\tau)}{\\mathrm{min}}\\underset{p_{\\mathcal{M}_{\\epsilon}^{\\epsilon},\\pi^{*}}(\\tau)}{\\mathrm{max}}\\mathcal{F}(\\pi,\\pi^{*},\\mathcal{R}_{\\mathrm{ext}},e)=\\beta\\log\\int p_{\\tilde{\\cal M},\\pi}(\\tau)e^{\\frac{r^{*}(\\tau)}{\\beta}}d\\tau-\\mathbb{E}_{p_{\\mathcal{M}_{\\epsilon}^{\\epsilon},\\pi}(\\tau)}[r^{*}(\\tau)]}\\\\ &{=\\!\\beta\\log\\int p_{\\tilde{\\cal M},\\pi}(\\tau)e^{\\frac{r^{*}(\\tau)}{\\beta}}d\\tau-\\beta\\mathbb{E}_{p_{\\mathcal{M}_{\\epsilon}^{\\epsilon},\\pi}(\\tau)}\\left[\\log\\frac{p_{{\\mathcal{M}_{\\epsilon}^{\\epsilon},\\pi}}(\\tau)}{p_{\\tilde{\\cal M},\\pi}(\\tau)}\\right]-\\beta\\log\\int p_{\\tilde{\\cal M},\\pi}(\\tau)e^{\\frac{r^{*}(\\tau)}{\\beta}}d\\tau}\\\\ &{=-\\beta D_{\\mathrm{KL}}\\left(p_{\\mathcal{M}_{\\epsilon}^{\\epsilon},\\pi}(\\tau)\\|p_{\\tilde{\\cal M},\\pi}(\\tau)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{e\\sim\\varepsilon}\\displaystyle\\operatorname*{min}_{\\mathcal{R}_{\\mathrm{c},\\pi}(\\tau)}\\operatorname*{max}_{\\pi_{\\mathrm{d},\\pi^{*}}(\\tau)}\\mathcal{F}(\\pi,\\pi^{*},\\mathcal{R}_{\\mathrm{ext}},e)=\\beta\\mathbb{E}_{e\\sim\\varepsilon}\\left[-D_{\\mathrm{KL}}\\left(p_{M_{e}^{c},\\pi}(\\tau)\\|p_{\\tilde{M},\\pi}(\\tau)\\right)\\right]}\\\\ {=\\beta\\mathbb{E}_{e\\sim\\varepsilon}\\mathbb{E}_{\\tau\\sim p_{M_{e}^{c},\\pi}(\\tau)}\\left[\\log\\frac{p_{\\pi}(\\tau)}{p_{\\pi}(\\tau|M_{e}^{c})}\\right]=\\beta\\mathbb{E}_{e\\sim\\varepsilon}\\mathbb{E}_{\\tau\\sim p_{M_{e}^{c},\\pi}(\\tau)}\\left[\\log\\frac{p_{\\pi}(\\tau)}{p_{\\pi}(e,\\tau)/p(e)}\\right]}\\\\ {=\\beta\\mathbb{E}_{e\\sim\\varepsilon}\\mathbb{E}_{\\tau\\sim p_{M_{e}^{c},\\pi}(\\tau)}\\left[\\log\\frac{p(e)}{p_{\\pi}(e,\\tau)/p_{\\pi}(\\tau)}\\right]=\\beta\\mathbb{E}_{e\\sim\\varepsilon}\\mathbb{E}_{\\tau\\sim p_{M_{e}^{c},\\pi}(\\tau)}\\left[\\log\\frac{p(e)}{p_{\\pi}(e|\\tau)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus we have proven this result. ", "page_idx": 16}, {"type": "text", "text": "A.3 Detailed Discussion and Proof about Embodiment-Aware Skill Discovery ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here we discuss our cross-embodiment skill-based adaptation objective. ", "page_idx": 16}, {"type": "text", "text": "We begin by proving Eq. (7). First, we show that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{e\\sim\\mathcal{E}}\\operatorname*{min}_{\\mathcal{R}_{\\mathrm{ext}}(\\tau)}\\operatorname*{max}_{p_{\\mathcal{M}_{e}^{c},\\pi^{*}}(\\tau)}\\mathcal{F}_{s}(\\pi,\\pi^{*},\\mathcal{R}_{\\mathrm{ext}},e)}\\\\ &{=-\\,\\mathbb{E}_{e}\\operatorname*{max}_{p(z|\\mathcal{M}_{e}^{c})}\\mathbb{E}_{z\\sim p(z|\\mathcal{M}_{e}^{c})}\\left[\\beta D_{\\mathrm{KL}}\\left(p_{\\mathcal{M}_{e}^{c},\\pi}(\\tau|z)\\|p_{\\bar{\\mathcal{M}},\\pi}(\\tau)\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Our proof is similar to the proof in Appendix A.2. Recall that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}_{s}(\\pi,\\pi^{*},\\mathcal{R}_{\\mathrm{ext}},e)\\triangleq\\Big[\\mathbb{E}_{p_{M_{e}^{c},\\pi^{*}}(\\tau)}[\\mathcal{R}_{\\mathrm{ext}}(\\tau)]-\\underset{z^{*}}{\\operatorname*{max}}\\mathbb{E}_{p_{M_{e}^{c},\\pi}(\\tau|z^{*})}[\\mathcal{R}_{\\mathrm{ext}}(\\tau)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\beta D_{\\mathrm{KL}}\\big(p_{{M_{e}^{c},\\pi^{*}}}(\\tau)\\big|\\|p_{\\bar{M},\\pi}(\\tau)\\big)\\Big]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similar to Eq. (22)-Eq. (25), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{p_{\\mathrm{A}_{\\mathrm{e}},\\pi^{*}}(\\tau)}{\\mathrm{max}}\\overset{\\cdot}{\\int}[\\mathbb{E}_{p_{\\mathrm{A}_{\\mathrm{e}},\\pi^{*}}(\\tau)}\\vert\\mathcal{R}_{\\mathrm{ext}}(\\tau)]-\\underset{z^{*}}{\\mathrm{max}}\\mathbb{E}_{p_{\\mathrm{A}_{\\mathrm{e}}^{\\varsigma},\\pi}(\\tau|z^{*})}\\vert\\mathcal{R}_{\\mathrm{ext}}(\\tau)\\vert-\\beta D_{\\mathrm{KL}}(p_{\\mathrm{A}_{\\mathrm{e}}^{\\varsigma},\\pi^{*}}(\\tau|z)\\vert\\vert p_{\\mathrm{A},\\pi}(\\tau))}\\\\ &{=\\beta\\log\\int p_{\\mathrm{\\bar{A}},\\pi}(\\tau)e^{\\frac{\\mathcal{R}_{\\mathrm{eat}}(\\tau)}{\\beta}}d\\tau-\\underset{z^{*}}{\\mathrm{max}}\\mathbb{E}_{p_{\\mathrm{A}_{\\mathrm{e}}^{\\varsigma},\\pi}(\\tau|z^{*})}\\vert\\mathcal{R}_{\\mathrm{ext}}(\\tau)\\vert}\\\\ &{=\\underset{z^{*}}{\\mathrm{min}}\\left[\\beta\\log\\int p_{\\mathrm{\\bar{A}},\\pi}(\\tau)e^{\\frac{\\mathcal{R}_{\\mathrm{eat}}(\\tau)}{\\beta}}d\\tau-\\mathbb{E}_{p_{\\mathrm{A}_{\\mathrm{e}}^{\\varsigma},\\pi}(\\tau|z^{*})}[\\mathcal{R}_{\\mathrm{ext}}(\\tau)]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Also, similar to Eq. (26)-Eq. (28), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\mathcal{R}_{\\mathrm{ext}}(\\tau)}{\\mathrm{min}}\\,\\operatorname*{min}_{z^{*}}\\left[\\beta\\log\\int p_{\\bar{\\mathcal{M}},\\pi}(\\tau)e^{\\frac{\\mathcal{R}_{\\mathrm{ext}}(\\tau)}{\\beta}}d\\tau-\\mathbb{E}_{p_{\\mathcal{M}_{e}^{c},\\pi}(\\tau|z^{*})}[\\mathcal{R}_{\\mathrm{ext}}(\\tau)]\\right]}\\\\ &{=\\underset{z^{*}}{\\mathrm{min}}\\,\\underset{\\mathcal{R}_{\\mathrm{ext}}(\\tau)}{\\mathrm{min}}\\,\\left[\\beta\\log\\int p_{\\bar{\\mathcal{M}},\\pi}(\\tau)e^{\\frac{\\mathcal{R}_{\\mathrm{ext}}(\\tau)}{\\beta}}d\\tau-\\mathbb{E}_{p_{\\mathcal{M}_{e}^{c},\\pi}(\\tau|z^{*})}[\\mathcal{R}_{\\mathrm{ext}}(\\tau)]\\right]}\\\\ &{=\\underset{z^{*}}{\\mathrm{min}}\\left[-\\beta D_{\\mathrm{KL}}\\left(p_{\\mathcal{M}_{e}^{c},\\pi}(\\tau|z^{*})\\|p_{\\bar{\\mathcal{M}},\\pi}(\\tau)\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\;\\;\\mathbb{E}_{e\\sim\\mathcal{E}}\\operatorname*{min}_{\\stackrel{\\mathrm{min}}{z\\sim}(\\tau)}\\operatorname*{max}_{s\\rightarrow(\\tau)}\\mathcal{F}_{s}(\\pi,\\pi^{*},\\mathcal{R}_{\\mathrm{ext}},e)}\\\\ &{=\\!\\mathbb{E}_{e}\\operatorname*{min}_{z^{*}}\\left[-\\beta D_{\\mathrm{KL}}\\left(p_{\\mathcal{M}_{e}^{c},\\pi}(\\tau|z^{*})\\|p_{\\bar{\\mathcal{M}},\\pi}(\\tau)\\right)\\right]}\\\\ &{=-\\,\\mathbb{E}_{e}\\operatorname*{max}_{z^{*}}\\left[\\beta D_{\\mathrm{KL}}\\left(p_{\\mathcal{M}_{e}^{c},\\pi}(\\tau|z^{*})\\|p_{\\bar{\\mathcal{M}},\\pi}(\\tau)\\right)\\right]}\\\\ &{=-\\,\\mathbb{E}_{e}\\operatorname*{max}_{p(z|\\mathcal{M}_{e}^{c})}\\mathbb{E}_{z\\sim p(z|\\mathcal{M}_{e}^{c})}\\left[\\beta D_{\\mathrm{KL}}\\left(p_{\\mathcal{M}_{e}^{c},\\pi}(\\tau|z)\\|p_{\\bar{\\mathcal{M}},\\pi}(\\tau)\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last equality holds from the fact that the maximum is achieved when putting all the probability weight on the input $_{\\textit{z}}$ maximizing $D_{\\mathrm{KL}}\\left(p_{\\mathcal{M}_{e}^{c},\\pi}(\\tau|z)\\|p_{\\bar{\\mathcal{M}},\\pi}(\\tau)\\right)$ . ", "page_idx": 17}, {"type": "text", "text": "Next, we will show that $D_{\\mathrm{KL}}\\left(p_{\\mathcal{M}_{e}^{c},\\pi}(\\tau|z)\\|p_{\\bar{\\mathcal{M}},\\pi}(\\tau)\\right)$ is a general form of our Theorem 3.2 and the results in the single-embodiment setting [11]. Naturally, when we ignore $_{\\textit{z}}$ , $\\mathcal{F}_{s}(\\pi,\\pi^{*},\\mathcal{R}_{\\mathrm{ext}},e)$ will degenerate into $\\mathcal{F}(\\pi,\\pi^{*},\\mathcal{R}_{\\mathrm{ext}},e)$ , and Eq. (7) will also degenerate into Eq. (4), i.e., the results in Theorem 3.2. On the other hand, if we change Eq. (7) into the single-embodiment setting, i.e., $\\mathcal{E}$ is a Dirac distribution with the probability $p(e)\\stackrel{}{=}1$ for some fixed $^e$ , then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\pi}{\\mathrm{max~}}\\underset{\\mathcal{R}_{\\mathrm{ext}}(\\tau)}{\\mathrm{min}}\\underset{p_{\\mathcal{M}_{\\mathrm{e}}^{c},\\pi^{*}}(\\tau)}{\\mathrm{max}}\\mathcal{F}_{s}(\\pi,\\pi^{*},\\mathcal{R}_{\\mathrm{ext}},e)}\\\\ &{=\\underset{\\pi}{\\mathrm{max}}\\left[-\\underset{p(z|\\mathcal{M}_{\\mathrm{e}}^{c})}{\\mathrm{max}}\\mathbb{E}_{z\\sim p(z|\\mathcal{M}_{\\mathrm{e}}^{c})}\\left[\\beta D_{\\mathrm{KL}}\\left(p_{\\mathcal{M}_{\\mathrm{e}}^{c},\\pi}(\\tau|z)\\|p_{\\mathcal{M}_{\\mathrm{e}}^{c},\\pi}(\\tau)\\right)\\right]\\right]}\\\\ &{=-\\underset{\\pi}{\\mathrm{min}}\\underset{p(z)}{\\mathrm{max}}\\mathbb{E}_{z\\sim p(z)}\\left[\\beta D_{\\mathrm{KL}}\\left(p_{\\mathcal{M}_{\\mathrm{e}}^{c},\\pi}(\\tau|z)\\|p_{\\mathcal{M}_{\\mathrm{e}}^{c},\\pi}(\\tau)\\right)\\right]}\\\\ &{\\approx-\\underset{\\rho}{\\mathrm{min}}\\underset{p(z)}{\\mathrm{max}}\\mathbb{E}_{z\\sim p(z)}\\left[\\beta D_{\\mathrm{KL}}\\left(p(\\tau|z)\\|\\rho(\\tau)\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "the last approximation simplifies the complex coupling relationship between $\\pi$ and $_{\\textit{z}}$ , following [11]. Furthermore, by Lemma 6.5 in [11] (proof in Theorem 13.1.1 from [9]), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\rho}\\operatorname*{max}_{p(z)}\\mathbb{E}_{z\\sim p(z)}\\left[D_{\\mathrm{KL}}\\left(p(\\tau|z)\\|\\rho(\\tau)\\right)\\right]=\\operatorname*{max}_{p(z)}\\mathcal{Z}(\\tau;z),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is the objective of existing single-embodiment skill-discovery methods. ", "page_idx": 17}, {"type": "text", "text": "Finally, we will Eq. (7), which further indicates that our cross-embodiment skill-based objective can be decomposed into two terms: one for handling cross-embodiment while the other aims at discovering skills. Actually, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\ \\mathbb{E}_{e\\sim\\ell}\\ \\underset{\\mathbb{R}_{\\alpha}\\in\\mathcal{T}_{\\ell}}{\\mathrm{min}}\\ \\underset{\\mathbb{R}_{\\alpha}\\in\\mathcal{F}_{\\ell}}{\\mathrm{min}}\\ \\mathcal{\\sum}_{s}(\\pi,\\pi^{*},\\mathcal{R}_{\\mathrm{eut}},e)}\\\\ &{=\\mathbb{E}_{z\\sim\\mathcal{P}(z|A_{\\mathcal{E}})}\\left[D_{\\mathrm{KL}}\\left(p_{M_{e},e}(\\tau|z)\\|p_{\\bar{M},\\pi}(\\tau)\\right)\\right]}\\\\ &{=\\int\\frac{p_{\\pi}(e,\\tau,z)}{p(e)}\\log\\frac{p_{\\pi}(\\tau|z,e)}{p_{\\pi}(\\tau)}d z d\\tau=\\int p_{\\pi}(\\tau,z|e)\\log\\frac{p_{\\pi}(z,e|\\tau)}{p_{\\pi}(e,z)}d z d\\tau}\\\\ &{=\\int p_{\\pi}(\\tau,z|e)\\log\\frac{p_{\\pi}(e|\\tau)p_{\\pi}(z|e,\\tau)}{p_{\\pi}(e)p_{\\pi}(z|e)}d z d\\tau}\\\\ &{=\\int p_{\\pi}(\\tau|e)\\log\\frac{p_{\\pi}(e|\\tau)}{p_{\\pi}(e|\\tau)}d\\tau+\\int p_{\\pi}(\\tau,z|e)\\log\\frac{p_{\\pi}(\\tau,z|e)}{p_{\\pi}(z|e)p_{\\pi}(\\tau|e)}d z d\\tau}\\\\ &{=\\mathbb{E}_{\\tau\\sim\\mathcal{P}\\times\\xi_{\\pi}}\\left[\\log\\frac{p_{\\pi}(e|\\tau)}{p_{\\pi}(e)}+D_{\\mathrm{KL}}(p_{\\pi}(\\tau,z|e)\\|p_{\\pi}(z|e)p_{\\pi}(\\tau|e))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B Experimental Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we will introduce more detailed information about our experiments. In Sec. B.1, we introduce the detailed environments and tasks used in our experiments. In Sec. B.2, we will illustrate all the baselines compared in experiments. Also, all hyper-parameters of experiments are in Sec. B.3. Moreover, we supplement more detailed experimental results about state-based DMC, image-based DMC, and Robosuite in Sec. B.4, Sec. B.5, and Sec. B.6, respectively. Then we conduct detailed generalization results of pre-trained models and fine-tuned models in Sec. B.9 and Sec. B.10, respectively. Finally, we report more detailed real-world experiments in Sec. B.12. ", "page_idx": 18}, {"type": "text", "text": "B.1 Embodiments and Tasks ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "State-based DMC. This benchmark is based on DMC [56] and URLB [27] with state-based observation. Each domain contains one robot and four downstream tasks. We extend it into the cross-embodiment settings: Walker-mass, Quadruped-mass, and Quadruped-damping. Walkermass extends the Walker robot in DMC, which is a two-leg robot, and designs a distribution with different mass $m$ , i.e., $m$ times the mass of a standard walker robot. Similarly, Quadruped-mass also considers quadruped robots with different mass $m$ . Quadruped-damping, on the other hand, changes the damping of the standard quadruped robot with $l$ times. The detailed parameters of training embodiments and generalization embodiments are in Table 3. ", "page_idx": 18}, {"type": "text", "text": "Image-based DMC. This benchmark is the same with state-based DMC but with image-based observation. Thus we consider similar three embodiment distributions: Walker-mass, Quadrupedmass, and Quadruped-damping. ", "page_idx": 18}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/af3601c83034a3f1d222067a7ff268d1d46927cf01b3e4baf6458150ce195a56.jpg", "table_caption": ["Table 3: Environment parameters used for state-based DMC and image-based DMC. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Robosuite. This benchmark utilizes the environment in [73] and follows the experimental setting in RL-Vigen [69], of which the cross-embodiment setting includes Panda, IIWA, and Kinova3. Here different embodiments may own different shapes (observations), and dynamics. Similarly, we pretrain cross-embodiments in all these three embodiments and fast fine-tune the pre-trained agents to downstream tasks. Besides these three embodiments, we also directly fine-tune our pre-trained models in one unseen embodiment: Jaco, to validate the cross-embodiment generalization ability of CEURL. For task sets, we consider three widely used tasks: Door, Lift, and TwoArmPegInHole. Noticing that although these three tasks can be finished by the same robots, their demand for robotic arms varies a lot. For example, TwoArmPegInHole needs two robotic arms but the other two tasks only need one. Consequently, we pre-train cross-embodiment agents for each single task, for all methods. ", "page_idx": 18}, {"type": "text", "text": "Isaacgym. We first design a setting in simulation based on Unitree A1 in Isaacgym, which is a challenging legged locomotion task and is widely used for real-world legged locomotion. The action space of A1 is a 12-dimension vector, representing 12 joint torque. Thus we consider our A1-disabled benchmark, including 12 embodiments, each of which owns a joint torque failure, i.e., the torque output of this joint is always 0 in this embodiment. This setting is practical as our robot may experience partial joint failure during use, and we still hope that it can complete the task as much as possible. ", "page_idx": 18}, {"type": "text", "text": "Moreover, we deploy PEAC into real-world Aliengo robots with failure joints. Similarly, we consider the embodiment distribution Aliengo-disabled, which owns 12 embodiments, each of which owns a joint torque failure respectively. We first pre-train a unified agent across these 12 embodiment in reward-free environments. During fine-tuning, for each embodiment, we utilize the same pre-trained agent to fine-tune the given moving task through this embodiment. Finally, we deploy the fine-tuned agent into the real-world setting to evaluate its movement ability under different kinds of terrains with joint failure. ", "page_idx": 18}, {"type": "text", "text": "B.2 Baselines and Implementations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "ICM [45]. Intrinsic Curiosity Module (ICM) designs intrinsic rewards as the divergence between the projected state representations in a feature space and the estimations made by a feature dynamics model. ", "page_idx": 19}, {"type": "text", "text": "RND [7]. Random Network Distillation (RND) utilizes a predictor network\u2019s error in imitating a randomly initialized target network to generate intrinsic rewards, enhancing exploration in learning environments. ", "page_idx": 19}, {"type": "text", "text": "Disagreement [46] / Plan2Explore [51]. The Disagreement algorithm leverages prediction variance across multiple models to estimate state uncertainty, guiding exploration towards less certain states. The Plan2Explore algorithm employs a self-supervised, world-model-based framework, using model disagreement to assess environmental uncertainty and incentivize exploration in sparse-reward scenarios. ", "page_idx": 19}, {"type": "text", "text": "ProtoRL [62]. Proto-RL combines representation learning and exploration through a selfsupervised learning framework, using prototype representations to pre-train task-independent representations in the environment, effectively improving policy learning in continuous control tasks. ", "page_idx": 19}, {"type": "text", "text": "APT [35]. Active Pre-training (APT) estimates entropy for a given state using a particle-based estimator based on the K nearest-neighbors algorithm. ", "page_idx": 19}, {"type": "text", "text": "LBS [38]. Latent Bayesian Surprise (LBS) applies Bayesian surprise within a latent space, efficiently facilitating exploration by measuring the disparity between an agent\u2019s prior and posterior beliefs about system dynamics. ", "page_idx": 19}, {"type": "text", "text": "Choreographer [39]. Choreographer is a model-based approach in unsupervised skill learning that employs a world model for skill acquisition and adaptation, distinguishing exploration from skill learning and leveraging a meta-controller for efficient skill adaptation in simulated scenarios, enhancing adaptability to downstream tasks and environmental exploration. ", "page_idx": 19}, {"type": "text", "text": "DIAYN [10]. Diversity is All You Need (DIAYN) autonomously learns a diverse set of skills by maximizing mutual information between states and latent skills, using a maximum entropy policy. ", "page_idx": 19}, {"type": "text", "text": "SMM [30]. State Marginal Matching (SMM) develops a task-agnostic exploration strategy by learning a policy to match the state distribution of an agent with a given target state distribution. ", "page_idx": 19}, {"type": "text", "text": "APS [34]. Active Pre-training with Successor Feature (APS) maximizes the mutual information between states and task variables by reinterpreting and combining variational successor features with nonparametric entropy maximization. ", "page_idx": 19}, {"type": "text", "text": "LSD [42]. Lipschitz-constrained Skill Discovery (LSD) adopts a Lipschitz-constrained state representation function, ensuring that maximizing this objective in the latent space leads to an increase in traveled distances or variations in the state space, thereby enabling the discovery of more diverse, dynamic, and far-reaching skills. ", "page_idx": 19}, {"type": "text", "text": "CIC [26]. Contrastive Intrinsic Control (CIC) is an unsupervised reinforcement learning algorithm that leverages contrastive learning to maximize the mutual information between state transitions and latent skill vectors, subsequently maximizing the entropy of these embeddings as intrinsic rewards to foster behavioral diversity. ", "page_idx": 19}, {"type": "text", "text": "BeCL [61]. Behavior Contrastive Learning (BeCL) utilizes contrastive learning for unsupervised skill discovery, defining its reward function based on the mutual information between states generated by the same skill. ", "page_idx": 19}, {"type": "text", "text": "For state-based DMC, almost all baselines (ICM, RND, Disagreement, ProtoRL, DIAYN, SMM, APS) combined with RL backbone DDPG are directly following the official implementation in urlb (https://github.com/rll-research/url_benchmark). For LBS, we refer the implementation in [48] (https://github.com/mazpie/mastering-urlb) and combine it with the code of urlb. For other more recent baselines, we also follow their official implementations, including CIC (https://github.com/rll-research/cic) and BeCL (https://github.com/ Rooshy-yang/BeCL). ", "page_idx": 20}, {"type": "text", "text": "For image-based DMC, almost all baselines (ICM, RND, Plan2Explore, APT, LBS, DIAYN, APS) combined with RL backbone DreamerV2 are directly following the official implementation in [48] (https://github.com/mazpie/mastering-urlb), which currently achieves the leading performance in image-based DMC of urlb. For CIC, we combine its official code (https://github. com/rll-research/cic), which mainly considers state-based DMC, and the DreamerV2 backbone in [48]. Similarly, for LSD, we refer to its official code (https://github.com/seohongpark/LSD) and combine it with the code of [48]. For Choreographer, of which the backbone is DreamerV2, we directly utilize its official code (https://github.com/mazpie/choreographer). ", "page_idx": 20}, {"type": "text", "text": "For Robosuite, our code is based on the code of RL-Vigen [69] (https://gemcollector.github. io/RL-ViGen), including the RL backbone DrQ. For Isaacgym, our code is based on the official code of [74] (https://github.com/ZiwenZhuang/parkour), which implements five downstream tasks (run, climb, leap, crawl, tilt). For these two settings (Robosuite and Isaacgym), as there are few works considering unsupervised RL in such a challenging setting, we implement classical baselines (ICM, RND, LBS) by referring their implementations in urlb (https://github.com/ rll-research/url_benchmark) and [48] (https://github.com/mazpie/mastering-urlb). ", "page_idx": 20}, {"type": "text", "text": "B.3 Hyper-parameters ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Baseline hyper-parameters are taken from their implementations (see Appendix B.2 above). Here we introduce PEAC\u2019s hyper-parameters. For all settings, hyper-parameters of RL backbones (DDPG, DreamerV2, PPO) follow standard settings. ", "page_idx": 20}, {"type": "text", "text": "First, for PEAC in state-based DMC with RL backbone DDPG, our code is based on urlb (https://github.com/mazpie/mastering-urlb) and inherits hyper-parameters of DDPG. For completeness, we list all hyper-parameters as ", "page_idx": 20}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/4b68f40e59fd382c1368c4dbc3e93319ea5fa122221c00b2d31b2139e304960d.jpg", "table_caption": [], "table_footnote": ["Table 4: Details of hyper-parameters used for state-based DMC. "], "page_idx": 20}, {"type": "text", "text": "Next, for PEAC-LBS and PEAC-DIAYN in image-based DMC with RL backbone DreamerV2, our code is based on [48] (https://github.com/mazpie/mastering-urlb). Hyper-parameters of ", "page_idx": 20}, {"type": "text", "text": "PEAC-LBS and PEAC-DIAYN inherit DreamerV2\u2019s hyper-parameters, as well as inherit hyperparameters of LBS and DIAYN, respectively. ", "page_idx": 21}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/d8e148ce57d5f4701ee0162001b4354f05bb3897b84cf68eaccd5b42d670e914.jpg", "table_caption": [], "table_footnote": ["Table 5: Details of hyper-parameters used for image-based DMC "], "page_idx": 21}, {"type": "text", "text": "Then, for PEAC in Robosuite, our code follows RL-Vigen [69] (https://gemcollector.github. io/RL-ViGen). PEAC\u2019s hyper-parameters, inheriting DrQ\u2019s hyperparameters, include ", "page_idx": 21}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/bc677f8e6cdea60d75753ef89e1c1739f5049178ae531bff20f22830a5c508db.jpg", "table_caption": [], "table_footnote": ["Table 6: Details of hyper-parameters used for Robosuite. "], "page_idx": 21}, {"type": "text", "text": "Finally, for PEAC in A1-disabled of Isaacgym with RL backbone PPO, our code follows [74] (https://github.com/ZiwenZhuang/parkour). PEAC\u2019s hyper-parameters, inheriting PPO\u2019s hyperparameters, include ", "page_idx": 21}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/7abadc62d69bd5769fde0b7acbceef1717ff116b4ad0751a0a7ccffc2c4e637c.jpg", "table_caption": [], "table_footnote": ["Table 7: Details of hyper-parameters used for Isaacgym. "], "page_idx": 22}, {"type": "text", "text": "B.4 Detailed results in state-based DMC ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In Table 8, we present detailed results in state-based DMC of all four statistics (medium, IQM, mean, OG) for baselines and our PEAC. The results indicate that PEAC performs the best in all these four metrics, while BeCL and CIC perform second and third respectively. Moreover, we report individual results for each downstream task of state-based DMC in Table 9. PEAC performs comparably to BeCL as well as CIC in the Walker-mass tasks and best on most Quadruped-mass and Quadruped-damping tasks. Especially, in the challenging Quadruped-damping setting, PEAC can complete cross-embodiment downstream tasks and significantly outperforms BeCL and CIC. ", "page_idx": 22}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/17e21382bbd5ecbaf78044747a9fa00fe47807238b15a61505732a3dc77fb718.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/6e382b4ff102064c9e6f04fa6f058bc027c0505f839947fcf81eb94567df8f9b.jpg", "table_caption": ["Table 8: Aggregate metrics [2] in state-based DMC. For every algorithm, there are 3 embodiment settings, each trained with 10 seeds and fine-tuned under 4 downstream tasks, thus each statistic for every method has 120 runs. ", "Table 9: Detailed results in state-based DMC. Average cumulative reward (mean of 10 seeds) of the best policy. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "B.5 Detailed results in image-based DMC ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In Table 10, we present detailed results in state-based DMC of all four statistics (medium, IQM, mean, OG) for baselines and our PEAC-LBS as well as PEAC-DIAYN. Besides these statistics, in Table 11, we further report the detailed results for the 12 downstream tasks, averaged across all embodiments and seeds. Overall, PEAC-LBS\u2019s performance is steadily on top, outperforming existing methods, especially in Walker-mass. Also, compared with other pure skill discovery methods, PEAC-DIAYN performs more consistently on all tasks and achieves higher average rewards. ", "page_idx": 23}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/2228f55c56e5d019ce99e41a58573c51acb9f9b84ffad61d093afc4e6f29ba50.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/2f633453d509780ea5a5372e60f300af8bc4e0d68c82b565fd55c8ecd2ae3456.jpg", "table_caption": ["Table 10: Aggregate metrics [2] in image-based DMC. For every algorithm, there are 3 embodiment settings, each trained with 3 seeds and fine-tuned under 4 downstream tasks, thus each statistic for every method has 36 runs. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 11: Detailed results in image-based DMC. Average cumulative reward (mean of 3 seeds) of the best policy trained by different algorithms. We bold the best performance of each task. The six baselines above are exploration-based methods (Choreographer utilizes both exploration and skill-discovery techniques), while the following four baselines are skill-discovery methods. ", "page_idx": 23}, {"type": "text", "text": "B.6 Detailed results in Robosuite ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In Table 12, we report detailed results in Robosuite with all tasks and robotic arms. Overall, PEAC performs better in more tasks and owns better generalization ability to unseen robot Jaco. ", "page_idx": 23}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/134e9d79fdb4008662aea84ffd10ccd7a057375d8f88215cbfb06511263a1d2e.jpg", "table_caption": ["Table 12: Detailed results in Robosuite. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "B.7 Ablation of timesteps in image-based DMC ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In Figure 8, we show additional results about the performance in three domains of image-based DMC for different algorithms and pre-training timesteps. Overall, PEAC-LBS outperforms all methods, while Choreographer and LBS are still competitive on the Quadruped-mass. Also, PEAC-DIAYN outperforms all other pure skill discovery methods. ", "page_idx": 24}, {"type": "image", "img_path": "LyAFfdx8YF/tmp/4595117608df6298b0e0b88add5a6b6c3285c5c9ab993d3d6b6f68bb7df0409e.jpg", "img_caption": ["Figure 8: Ablation study of pre-training steps in image-based DMC. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "B.8 More Ablation Studies ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this part, we conclude more ablation studies to better clarify the contribution of each component in PEAC. First, we supplement ablation studies of the hyperparameter $\\beta$ in Eq. 2 ( $\\beta$ is set to 1.0 in all our experiments). As discussed in the paper, $\\beta$ is a parameter that is negatively related to the fine-tuning timesteps and is for balancing the policy improvement term and the policy constraint term. When the fine-tuning timestep tends to the infinity, $\\beta$ tends to 0. Unfortunately, the relationship between $\\beta$ and the fine-tuning timesteps is complicated. Thus we evaluate PEAC-LBS under different $\\beta$ as below ", "page_idx": 24}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/4557bdcde55aab7e7d173a1a11d313f69992209396dbefa74b25593a00503005.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 13: Ablation for $\\beta$ of PEAC-LBS in image-based DMC. Average cumulative reward (mean of 3 seeds) of the best policy trained by different algorithms. ", "page_idx": 24}, {"type": "text", "text": "As shown in Table 13, when $\\beta$ is large, the performance of PEAC-LBS decreases more than $\\beta$ is small, but PEAC-LBS is overall stable with different $\\beta$ . ", "page_idx": 24}, {"type": "text", "text": "Moreover, to clarify the effectiveness of our embodiment discriminator, we supplement LBS-Context and DIAYN-Context, i.e., combining LBS and DIAYN with the embodiment discriminator in PEAC, which utilizes embodiment information during the pre-training stage. Our results in state-based DMC and Image-based DMC are in Table 14 and Table 15, respectively. ", "page_idx": 24}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/c2c88e8800241a93ff3231d5df4e686e0f2e09c09f1dea598db18b0c7a61b97c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/739357e5365ee7f2aed63e85c39f8b50243a9039c89e3c3cf6e0b68a96db404a.jpg", "table_caption": ["Table 14: Ablation study for baselines w/ our embodiment discriminator in state-based DMC. ", "Table 15: Ablation study for baselines w/ our embodiment discriminator in image-based DMC. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "As shown in these two tables, LBS-Context and DIAYN-Context own comparable or superior performance compared with LBS and DIAYN respectively, and PEAC still significantly outperforms them. Consequently, this ablation study highlights that both the embodiment discriminator and cross-embodiment intrinsic rewards $\\mathcal{R}_{\\mathrm{CE}}$ are effective for handling CEURL. ", "page_idx": 25}, {"type": "text", "text": "B.9 Generalization results of pre-trained models ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In Fig. 9, we evaluate the generalization ability of pre-trained models to unseen embodiments of all exploration methods in Walker-mass of image-based DMC. After pre-training on several embodiments, we zero-shot utilize these agents to sample trajectories via two different unseen embodiments. Given the trajectories, we reduce the dimension of the hidden states calculated by the world model via t-SNE [57], where points with different colors represent data generated by different embodiments. As shown in Fig. 9, all the baselines can not distinguish different embodiments, while our PEAC-LBS can roughly divide them into two regions, indicating the pre-trained model of PEAC-LBS own strong generalization ability to unseen embodiments. ", "page_idx": 25}, {"type": "image", "img_path": "LyAFfdx8YF/tmp/c07a55760e5c7abede7eede19189ffabd8960e6b8766c7b5efd5088a5b204851.jpg", "img_caption": ["Figure 9: Visualization of the pre-trained model generalization to unseen embodiments. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "B.10 Generalization results of fine-tuned models ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this part, we evaluate the generalization ability of the fine-tuned agents to unseen embodiments of state-based DMC and image-based DMC. In these two settings, we pre-train and fine-tune the agent with the sampled training embodiments (Train column in Table 3) and zero-shot evaluate the performance of the fine-tuned agents in the same task but with unseen in-distribution embodiments (Generalization column in Table 3). The detailed generalization results of all downstream tasks in state-based DMC and image-based DMC are in Table 16-17, respectively. As shown in Table 16, ", "page_idx": 25}, {"type": "text", "text": "PEAC still significantly outperforms all baselines in normalized average return and there is even a greater leading advantage than baselines, compared with the trained embodiments. This indicates that PEAC can effectively generalize to unseen embodiments and effectively handle downstream tasks. ", "page_idx": 26}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/8e9d2c14b7bdf0be18ff15d7a826ec8c377acceb5b8ed8969ef724e6741944ee.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Similarly, Table 17 shows that PEAC-LBS not only outperforms baselines but also owns a greater leading advantage than baselines, compared with the trained embodiments. Moreover, PEAC-DIAYN exceeds other pure-exploration methods and demonstrates strong generalization ability. ", "page_idx": 26}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/92fc6f7d447d158340392c76001190b5beb4ba4ff607ec79230366fd15e9a190.jpg", "table_caption": ["Table 16: Detailed results in state-based DMC in evaluation embodiments. Average cumulative reward (mean of 10 seeds) of the best policy trained by different algorithms. ", "Table 17: Detailed results in image-based DMC in evaluation embodiments. Average cumulative reward (mean of 3 seeds) of the best policy trained by different algorithms. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "B.11 More challenging tasks and varying embodiments ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we will consider CEURL in much more challenging tasks and more varying embodiment distributions, which are significant future directions for unsupervised cross-embodiment agents in more challenging real-world applications. ", "page_idx": 26}, {"type": "text", "text": "We first consider more complicated tasks including locomotion in complicated terrain. Following previous work [16], we design locomotion tasks in incline terrains and the results are below. ", "page_idx": 26}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/515265e3af2d6a8926ceb3595e23a7d6ad4ee20790db187454c6f2eba8e15177.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "As shown in Table 18, the performance of LBS and PEAC-LBS decreases when locomoting in the incline terrain due to its complexity. PEAC-LBS still significantly outperforms LBS, expressing that our method, especially the cross-embodiment intrinsic rewards, benefits cross-embodiment unsupervised pre-training for handling more complicated tasks. ", "page_idx": 26}, {"type": "text", "text": "Besides more complicated tasks, one possible future direction is to consider more different, or even exactly different embodiments. We take the first step by designing several settings with more varying and challenging embodiment distributions: ", "page_idx": 26}, {"type": "text", "text": "\u2022 Walker-Cheetah: includes two Walker robots with a mass of 0.4 and 1.6 times the normal mass, as well as two Cheetah robots with a mass of 0.4 and 1.6 times the normal mass.   \n\u2022 Walker-Humanoid: includes one Walker robot and one Humanoid robot. Their robot properties, robot shapes, and action spaces are all different.   \n\u2022 Walker-length and Cheetah-torsolength [71]: The former includes walker robots with different foot lengths while the second one includes cheetah robots with different torso lengths. Thus robots\u2019 properties and morphologies are different. The figures of these embodiments are in Fig. 10. ", "page_idx": 27}, {"type": "image", "img_path": "LyAFfdx8YF/tmp/178e3a97096c84238bde5d5a8f5ce8a733fb2c58c387621f004001a7ab636f18.jpg", "img_caption": ["Figure 10: Benchmark environments of Walker-length and Cheetah-torsolength. In Walker-length, the length of the left foot sole of different robots is different. In Cheetah-torsolength, the length of the torso is different. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "We mainly compare our PEAC-DIAYN and PEAC-LBS with DIAYN, LBS, and Choreographer in embodiment distributions: Walker-Cheetah, Walker-Humanoid, Walker-length, and Cheetahtorsolength. of which the results are in Table 19, Table 20, and Table 21 respectively. ", "page_idx": 27}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/f60a06d5aae418037799c33c0759708a992f0acefe4fc60e6d0c70d35f090c0e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/b2f8b731a585723d3abd91398ffcf9c2bf9b0b807fe16d0e998b5feb030a96ff.jpg", "table_caption": ["Table 19: Detailed results of Walker-Cheetah in image-based DMC. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "LyAFfdx8YF/tmp/370f8e7afa2b0b9de54ab4730bc10f0286d59c67eeb298db694740cb7189ad94.jpg", "table_caption": ["Table 20: Detailed results of Walker-Humanoid in image-based DMC "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 21: Detailed results of Walker-length and Cheetah-torso_length in image-based DMC. ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "As shown in these tables, PEAC can achieve much greater performance compared to baselines. These experiments indicate that PEAC has powerful abilities to handle various kinds of embodiment differences, including different morphologies. Unfortunately, when the embodiments vary a lot (like Walker-Humanoid), the performance of PEAC is still limited, thus designing more effective methods for handling complicated embodiments like Humanoid is a promising future direction for further considering cross-embodiment settings. ", "page_idx": 27}, {"type": "text", "text": "B.12 Real-World Applications ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "As a supplement of Sec. 5.4, we provide more detailed images of real-world robot deployments. As shown in Fig. 11, our method can fast fine-tune to different embodiments and handle different terrains, which are unseen in the simulation. A detailed video is provided on the paper homepage. ", "page_idx": 28}, {"type": "image", "img_path": "LyAFfdx8YF/tmp/469fbf8eedfd3020ca976dde233085ccb2b9eb851b7027e996f389353c927908.jpg", "img_caption": ["Figure 11: Real-world results for Aliengo robot with different joint failure in different terrains. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "B.13 Computing Resource ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In experiments, all the agents are trained by GeForce RTX 2080 Ti with Intel(R) Xeon(R) Silver 4210 CPU $\\textcircled{a}\\ 2.20\\mathrm{GHz}$ . In Image-based DMC / state-based DMC / Robosuite / Isaacgym, pre-training each algorithm (each seed, domain) takes around $2\\,/\\,0.5\\,/\\,1.5\\,/\\,2$ days respectively. ", "page_idx": 28}, {"type": "text", "text": "C Pseduo-codes of Algorithms ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Algorithm 1 Pre-trained Embodiment-Aware Control (PEAC)   \nRequire: $M$ training embodiments $\\lbrace e_{m}\\rbrace_{m=1}^{M}$ , $M$ replay buffers $\\{\\mathcal{D}_{m}\\}_{m=1}^{M}$ , $N$ testing embodiments $\\{e_{M+n}\\}_{n=1}^{N}$ , initialize neural network parameters of the policy 1: // Pre-Training 2: while is unsupervised phase do   \n3: // Data Collection 4: for $m=1,2,...,M$ do 5: Sample state-action pairs $\\{(s_{t}^{m},a_{t}^{m})\\}_{t}$ with the policy by controlling the embodiment $e_{m}$ and store them into $\\mathcal{D}_{m}$ . 6: end for 7: // Model Training 8: for update ste $\\mathbf{\\Delta}_{9}=1,2,...,U$ do   \n9: Sample state-action pairs form each replay buffer $\\{(s_{t}^{i},a_{t}^{i})_{t=1}^{T}\\}\\sim\\mathcal{D}_{i},i=1,2,...,M$   \n10: Update the embodiment discriminator via these data.   \n11: Compute the cross-embodiment intrinsic reward $\\mathcal{R}_{\\mathrm{CE}}$ for each state-action pair and concatenate them together.   \n12: Update the policy by RL backbones (like PPO, DDPG, DreamerV2, and so on) with these data and $\\mathcal{R}_{\\mathrm{CE}}$ .   \n13: end for   \n14: end while   \n15: // Fine-Tuning   \n16: while is supervised phase do   \n17: Sample state-action-reward pairs with extrinsic rewards $\\mathcal{R}_{\\mathrm{ext}}$ via embodiment $e_{m}$ and store them into $\\mathcal{D}_{m}$ .   \n18: Update the policy by jointly training data from different replay buffers via RL backbones.   \n19: end while   \n20: // Evaluation   \n21: Evaluate Nfine-tuned policy with downstream task $\\mathcal{R}_{\\mathrm{ext}}$ via $\\{e_{m}\\}_{m=1}^{M}$ and unseen embodiments {eM+n}n=1.   \nRequire: $M$ training embodiments $\\{e_{m}\\}_{m=1}^{M}$ , $M$ replay buffers $\\{\\mathcal{D}_{m}\\}_{m=1}^{M}$ , $N$ testing embodiments $\\{e_{M+n}\\}_{n=1}^{N}$ , initialize neural network parameters of the policy   \n1: // Pre-Training   \n2: while is unsupervised phase do   \n3: $//$ Data Collection (the same as PEAC)   \n4:   \n5: // Model Training   \n6: for update st $\\mathfrak{p}=1,2,...,U$ do   \n7: Sample state-action pairs form each replay buffer $\\{(s_{t}^{i},a_{t}^{i})_{t=1}^{T}\\}\\sim\\mathcal{D}_{i},i=1,2,...,M$   \n8: Update the embodiment discriminator via these data.   \n9: Update the components of LBS, including the Latent Prior model, the Latent Posterior model, and the Reconstruction model (In DreamerV2 backbone, we can directly utilize its prior model and posterior model).   \n10: Compute the intrinsic reward $\\mathcal{R}_{\\mathrm{CE}}+\\mathcal{R}_{\\mathrm{LBS}}$ for each state-action pair and concatenate them together.   \n11: Update the policy by RL backbones (like PPO, DDPG, DreamerV2, and so on) with these data and $\\mathcal{R}_{\\mathrm{CE}}+\\mathcal{R}_{\\mathrm{LBS}}$ .   \n12: end for   \n13: end while   \n14: // Fine-Tuning(the same as PEAC)   \n15: ...   \n16: // Evaluation   \n17: Evaluate fine-tuned policy with downstream task $\\mathcal{R}_{\\mathrm{ext}}$ via $\\{e_{m}\\}_{m=1}^{M}$ and unseen embodiments {eM+n}n=1. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "Algorithm 3 PEAC-DIAYN ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Require: $M$ training embodiments $\\{e_{m}\\}_{m=1}^{M}$ , $M$ replay buffers $\\{\\mathcal{D}_{m}\\}_{m=1}^{M}$ , $N$ testing embodiments $\\{e_{M+n}\\}_{n=1}^{N}$ , initialize neural network parameters of the behavior policy conditioned on skill and embodiment context $\\pi(\\cdot|s,z,e)$ , initialize neural network parameters of the embodiment-aware skill policy $\\pi(z|e,\\tau)$   \n1: // Pre-Training   \n2: while is unsupervised phase do   \n3: $//$ Data Collection (the same as PEAC)   \n4:   \n5: // Model Training   \n6: for update ste $\\mathbf{\\alpha})=1,2,...,U$ do   \n7: Sample state-action pairs form each replay buffer $\\{(s_{t}^{i},a_{t}^{i})_{t=1}^{T}\\}\\sim\\mathcal{D}_{i},i=1,2,...,M$   \n8: Update the embodiment discriminator via these data.   \n9: Update the skill discriminator of DIAYN via these data.   \n10: Compute the intrinsic reward $\\mathcal{R}_{\\mathrm{CE}}+\\mathcal{R}_{\\mathrm{DIAYN}}$ for each state-action pair and concatenate them together.   \n11: Update the behavior policy conditioned on skill and embodiment context by RL backbones (like PPO, DDPG, DreamerV2, and so on) with these data and $\\mathcal{R}_{\\mathrm{CE}}+\\mathcal{R}_{\\mathrm{DIAYN}}$ .   \n12: end for   \n13: end while   \n14: // Fine-Tuning   \n15: while is supervised phase do   \n16: Sample state-action-reward pairs with extrinsic rewards $\\mathcal{R}_{\\mathrm{ext}}$ via embodiment $e_{m}$ and store them into $\\mathcal{D}_{m}$ .   \n17: Update the embodiment-aware skill policy by jointly training data from different replay buffers via RL backbones.   \n18: end while   \n19: // Evaluation   \n20: Evaluate Nfine-tuned agents with downstream task $\\mathcal{R}_{\\mathrm{ext}}$ via $\\{e_{m}\\}_{m=1}^{M}$ and unseen embodiments {eM+n}n=1. ", "page_idx": 29}, {"type": "text", "text": "D Broader Impact ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Designing generalizable agents for varying tasks and embodiments is a major concern in reinforcement learning. This work focuses on cross-embodiment unsupervised reinforcement learning and proposes a novel algorithm PEAC, which leverages trajectories from different embodiments for pre-training, subsequently broadly enhancing performance on downstream tasks. Such advancements provide the potential for future real-world cross-embodiment control. One of the potential negative impacts is that algorithms using deep neural networks, which lack interoperability and face security and robustness issues. There are no serious ethical issues as this is basic research. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: As mentioned in the abstract and introduction, this work mainly integrates the unsupervised RL paradigm into cross-embodiment RL, as a novel concept CEURL (Sec. 3). Then we theoretically derive a novel algorithm PEAC (Sec. 3-4) and widely evaluate PEAC in a large number of environments (Sec. 5). All these contributions are described in detail in the paper. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Yes, we have discussed the limitations of this work in Sec. 5.5 ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Yes, we have included the detailed proofs of all results in the Appendix A, including properties of $\\mathcal{D}^{\\mathcal{E}}$ (Appendix A.1), proof of Theorem 3.2 (Appendix A.2), and proof about Embodiment-Aware Skill Discovery (Appendix A.3). ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have provided the code in the paper homepage. Also, we have provided detailed information on our experiments, including hyper-parameters, codebase, and pseudocode of our algorithms in Appendix B and Appendix C. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 32}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We have provided the source code in our paper homepage. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We have provided details for our experiments in Appendix B, including our environments, hyper-parameters, optimizers, and so on, as well as our source code in the paper homepage. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] , ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: To mitigate the effectiveness of the randomness, we repeat several random seeds for all experiments (10 for state-based DMC and 3 for the others, following previous works\u2019 settings). When reporting the results, besides reporting mean\u00b1std performance, we also report IQM and OG in state-based DMC and image-based DMC for better evaluating different algorithms. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Yes, we have reported the computing source in Appendix B.13, including the type of computing CPU/GPU, computing time, and so on. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our research does not involve human subjects or participants. And we have discussed the potential societal impact in Appendix. D. As this is basic research, there are no serious social issues. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Yes, we have discussed the Border Impact of this work in Appendix. D. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: There is no such risk in this paper. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have cited the original paper that produced the code/environment package and included the URL of these codebases in Appendix B.2. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Yes, we have provided documentation of our source code in the supplementary materials. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]