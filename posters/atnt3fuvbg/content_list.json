[{"type": "text", "text": "SureMap: Simultaneous mean estimation for single-task and multi-task disaggregated evaluation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mikhail Khodak\u2217 Lester Mackey, Alexandra Chouldechova, Miroslav Dud\u00edk ", "page_idx": 0}, {"type": "text", "text": "Princeton University Microsoft Research mkhodak@cs.cmu.edu {lmackey,alexandrac,mdudik}@microsoft.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Disaggregated evaluation\u2014estimation of performance of a machine learning model on different subpopulations\u2014is a core task when assessing performance and group-fairness of AI systems. A key challenge is that evaluation data is scarce, and subpopulations arising from intersections of attributes (e.g., race, sex, age) are often tiny. Today, it is common for multiple clients to procure the same AI model from a model developer, and the task of disaggregated evaluation is faced by each customer individually. This gives rise to what we call the multi-task disaggregated evaluation problem, wherein multiple clients seek to conduct a disaggregated evaluation of a given model in their own data setting (task). In this work we develop a disaggregated evaluation method called SureMap that has high estimation accuracy for both multi-task and single-task disaggregated evaluations of blackbox models. SureMap\u2019s efficiency gains come from (1) transforming the problem into structured simultaneous Gaussian mean estimation and (2) incorporating external data, e.g., from the AI system creator or from their other clients. Our method combines maximum a posteriori (MAP) estimation using a well-chosen prior together with cross-validation-free tuning via Stein\u2019s unbiased risk estimate (SURE). We evaluate SureMap on disaggregated evaluation tasks in multiple domains, observing significant accuracy improvements over several strong competitors. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Evaluation is a key challenge in modern AI, with much effort spent deciding what metrics to measure, with which methods, and on what data. This challenge is especially acute in fairness assessment, which requires not only high-quality data to run a model and score its outputs but also demographic information for defining groups. Due to the high cost of obtaining high-quality evaluation data, the issue of sample complexity\u2014sample size needed to get a good performance estimate\u2014remains salient, especially when we want to release not just one overall measure but instead to output a disaggregated evaluation that captures variation among demographic subpopulations of the data [Barocas et al., 2021]. For instance, we might want to assess group fairness by examining the variation in performance across groups of users defined by intersections of the demographic attributes age, race, and sex. The naive approach of independently evaluating each group\u2019s performance on its own data can fail because the sample sizes of intersectional groups rapidly decrease as we consider more attributes [Herlihy et al., 2024]. Recent work has shown how to improve upon naive methods by combining data from multiple subpopulations to inform their individual performance estimates [Miller et al., 2021, Herlihy et al., 2024]. ", "page_idx": 0}, {"type": "text", "text": "In today\u2019s technology landscape it is common for multiple clients to procure the same model (e.g., an automated speech recognition or language model) from an AI developer, with each client performing a disaggregated evaluation of the same model on their own data. We refer to this problem as the multi-task disaggregated evaluation. We formalize and study this problem, showing that one can improve the disaggregated evaluations of individual clients by using multi-task data in the form of summary statistics from other clients or from the model provider. Our approach uses the (out-ofdistribution) multi-task data to set the parameters of a multivariate normal prior and then performs maximum a posteriori (MAP) inference on the (in-distribution) client data. Formally, we model the problem as Gaussian mean estimation and design a simple-yet-expressive additive prior that can capture many different relationships between subpopulations. Drawing upon classical statistics, we fti prior parameters by minimizing Stein\u2019s unbiased risk estimator [SURE, Stein, 1981]. While motivated by multi-task considerations, we show that our method also performs well in the single-task setting. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "1. SureMap: We introduce a method that uses SURE to tune the parameters of a well-chosen Gaussian prior before applying MAP estimation. The prior is motivated by its attainment of a good efficiency\u2013expressivity tradeoff, requiring only a linear (in the number of subpopulations) number of parameters to recover several natural baselines for disaggregated evaluation. ", "page_idx": 1}, {"type": "text", "text": "2. Datasets: Disaggregated evaluation has few benchmarks [Herlihy et al., 2024], so we introduce new ones for both the single-task and multi-task settings, covering automated speech recognition (ASR) and also tabular domains (with linear models and also in-context LLMs).   \n3. Single-task: We find that SureMap is always competitive with strong baselines from prior work, while improving significantly in some settings with intersectional sensitive attributes. ", "page_idx": 1}, {"type": "text", "text": "4. Multi-task: Incorporating data from multiple clients into SureMap yields significant improvements across all evaluated settings. This multi-task approach is more accurate even with just one additional task and is the only method to consistently outperform the naive and pooling baselines. ", "page_idx": 1}, {"type": "text", "text": "1.2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Disaggregated evaluation is a core task in the fairness assessment of AI systems [Barocas et al., 2021]. Past work has sought to improve estimation accuracy by combining information across different groups, e.g., via Bayesian modeling [Miller et al., 2021], Gaussian process approximation of loss surfaces [Piratla et al., 2021], and structured regression [Herlihy et al., 2024]. The last work found that classical James\u2013Stein-type mean estimation [James and Stein, 1961, Bock, 1975] is often competitive, and so we adopt it as our first non-naive baseline. We also compare to structured regression itself, which turns out to have a tight mathematical connection to SureMap; indeed, apart from our use of Gaussian (ridge) rather than Laplace (lasso) priors (regularization)\u2014as well as our use of a more flexible tuning based on SURE rather than cross-validation\u2014the method of Herlihy et al. [2024] can be viewed as the discriminative counterpart to our generative approach (see $\\S E$ for details). ", "page_idx": 1}, {"type": "text", "text": "Within the disaggregated evaluation literature we are the first to formulate and study multi-task disaggregated evaluation. This is an important direction because (a) model providers often have their own data or data from multiple clients that can inform the evaluation and (b) transferring information across distributions is a key way to handle very low-sample regimes. We also contribute several datasets that we hope will spur further development in disaggregated evaluation. ", "page_idx": 1}, {"type": "text", "text": "SureMap relies on applying classical mean estimation tools to quantities modeled as Gaussian means. Notably, Miller et al. [2021] model scores via well-studied distributions\u2014e.g., Gaussians\u2014but since scores are related non-linearly to metrics it is unclear if this can lead to similarly simple estimators. To tune parameters, we use SURE, a popular statistical approach [Li, 1985, Donoho and Johnstone, 1995]. Specifically, in the empirical Bayes tradition, we use it to set the MAP estimator of a hierarchical model. Using SURE to tune the scale of an isotropic Gaussian prior was shown to be asymptotically (in the dimension) optimal in the case of heteroskedastic data distributions [Xie et al., 2012]. Since disaggregated evaluation data is highly heteroskedastic due to variation in group size, this is positive evidence for our approach, although our prior is non-isotropic and has many more variance parameters. ", "page_idx": 1}, {"type": "text", "text": "2 Setup", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We first describe the disaggregated evaluation problem $(\\S2.1)$ , recast it as a Gaussian mean estimation (\u00a72.2), and motivate a multi-task variant $(\\S2.3)$ , all while introducing several baselines estimators. ", "page_idx": 1}, {"type": "text", "text": "2.1 Setting and baselines ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We want to assess a predictive model $p:\\mathcal{X}\\to\\mathcal{Y}$ under some distribution $\\mathcal{D}$ over input space $\\mathcal{X}$ and output space $\\boldsymbol{\\wp}$ using error measure $\\ell:\\mathcal{V}\\times\\mathcal{V}\\rightarrow\\mathbb{R}$ . For example, in image classification, $\\mathcal{D}$ is a distribution over (image, label) pairs and $\\ell$ is the 0-1 error. To simplify notation, we mainly deal with the composite function $f(z)={\\bar{\\ell}}(y,p(x))$ acting on points $z=(x,y)$ in the product space $\\mathcal{Z}=\\mathcal{X}\\times\\mathcal{Y}$ . ", "page_idx": 1}, {"type": "text", "text": "In disaggregated evaluation, $\\mathcal{Z}$ is assumed to be a union of $d\\geq1$ disjoint subsets $\\mathcal{Z}_{g}$ , each associated to some subpopulation or group $g\\in[d]$ , where we use $[k]$ to denote the set $\\{1,\\ldots,k\\}$ . As a running example, suppose each point $z\\in{\\mathcal{Z}}$ is an individual whose sex $s$ and age $a$ are categorical variables with $d_{1}$ and $d_{2}$ possible values, respectively. Then $d=d_{1}d_{2}$ and each point has an associated index $g=(s-1)d_{2}+a$ denoting its intersection of sex $s\\in[d_{1}]$ and age $a\\in[d_{2}]$ . The task is then to use a set $S\\,\\sim\\,{\\mathcal{D}}^{n}$ of $n\\geq1$ i.i.d. samples from the distribution $\\mathcal{D}$ to estimate the vector of true subpopulation errors $\\pmb{\\mu}\\in\\mathbb{R}^{d}$ , with components $\\mu_{g}=\\mathbb{E}_{z\\sim\\mathcal{D}|\\mathcal{Z}_{g}}[f(z)]$ .1 We write $\\hat{\\pmb{\\mu}}(S)\\in\\mathbb{R}^{d}$ for an estimator of $\\pmb{\\mu}$ with components denoted as $\\hat{\\mu}_{g}(S)$ . ", "page_idx": 2}, {"type": "text", "text": "Empirically, we measure estimation accuracy, i.e., the performance of the performance estimate, via mean absolute error (MAE) $\\begin{array}{r}{L_{\\mu}^{\\mathrm{MAE}}(\\hat{\\pmb{\\mu}}(S))\\sp{\\prime}=\\frac{1}{d}\\|\\hat{\\pmb{\\mu}}(\\hat{S})-\\pmb{\\mu}\\|_{1}}\\end{array}$ , which is easy to interpret and less sensitive to outliers than mean squared error (MSE). Our method development, however, is based on a count-weighted version of MSE, where we denote group counts as $\\bar{n_{g}}=|S\\cap\\mathcal{Z}_{g}|$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nL_{\\mu}^{\\mathbf{n}}(\\hat{\\pmb{\\mu}}(S))=\\frac{1}{d}\\sum_{g=1}^{d}n_{g}(\\hat{\\mu}_{g}(S)-\\mu_{g})^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We conclude the setup with two baselines. The first is the naive estimator returning group means:2 ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{g}^{\\mathrm{naive}}(S)=\\frac{1}{n_{g}}\\sum_{z\\in S\\cap\\mathcal{Z}_{g}}f(z).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "While unbiased, $\\hat{\\mu}^{\\mathrm{naive}}$ can perform poorly on groups with few samples. The second baseline, the pooled estimator, returns an identical quantity\u2014the overall sample mean\u2014for all groups: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{g}^{\\mathrm{pooled}}(S)=\\frac{1}{n}\\sum_{z\\in S}f(z)=\\frac{1}{n}\\sum_{g=1}^{d}n_{g}\\hat{\\mu}_{g}^{\\mathrm{naive}}(S).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This estimator is generally biased (unless all group means are equal), but it can perform well in low-sample regimes thanks to a much lower variance. ", "page_idx": 2}, {"type": "text", "text": "2.2 A Gaussian model for disaggregated evaluation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We use a simple but natural model to aid in the design of disaggregated evaluation methods. Specifically, denoting the naive estimator by $\\mathbf{y}=\\hat{\\pmb{\\mu}}^{\\mathrm{naive}}(\\bar{\\boldsymbol{S}})\\in\\mathbb{R}^{d}$ , we model group $g$ \u2019s entry $y_{g}$ as being drawn from a Gaussian with (unknown) mean $\\mu_{g}$ and (known) variance $\\sigma^{2}/n_{g}$ , where $\\sigma^{2}$ is shared across groups. This reduces the problem of disaggregated evaluation, as defined in $\\S2.1$ , to that of estimating the mean of a multivariate Gaussian with known diagonal covariance $\\Sigma_{g,g}=\\sigma^{2}/n_{g}$ given a single sample $\\mathbf{y}\\sim{\\mathcal{N}}({\\pmb{\\mu}},{\\pmb{\\Sigma}})$ . Our model has many advantages in the disaggregated evaluation setting: ", "page_idx": 2}, {"type": "text", "text": "1. By the central limit theorem, $\\mathbf{y}$ is asymptotically normal with mean $\\pmb{\\mu}$ and diagonal covariance $\\Sigma$ for many distributions $\\mathcal{D}$ of interest, even when the underlying data is non-Gaussian. Furthermore, because the methods derived from our model only take y and $\\Sigma$ as input, they can be applied even when the evaluated statistic is not the pointwise average assumed by the setup in $\\S2.1$ , so long as $\\mathbf{y}\\sim{\\mathcal{N}}({\\pmb{\\mu}},{\\pmb{\\Sigma}})$ holds asymptotically. An example of this is when $y_{g}=\\hat{\\mu}_{q}^{\\mathrm{naive}}(S)$ corresponds to the area under the ROC curve (AUC) computed over group $g$ \u2019s data $S\\cap\\breve{\\mathscr{Z}}_{g}$ [Lehmann, 1951]; we demonstrate SureMap\u2019s applicability to AUC empirically in $\\S\\mathrm{G}$ (Figures 10 & 12).   \n2. While a shared variance is a strong assumption, it is perhaps the simplest way of incorporating the inductive bias that $\\Sigma_{g,g}$ will be highly correlated with the inverse of $\\mathit{\\Pi}^{n}g$ , the number of samples from group $g$ . In practice, we set $\\sigma^{2}$ to be the pooled estimate $\\begin{array}{r}{\\frac{1}{n-d}\\sum_{g=1}^{d}\\sum_{z\\in S\\cap\\mathcal{Z}_{g}}(f(z)-y_{g})^{2}}\\end{array}$   \n3. Gaussian mean estimation is one of the best-studied problem in statistics, with numerous well-tested baselines and approaches for developing new methods. In particular, we make significant use of the classic James\u2013Stein approach [James and Stein, 1961, Bock, 1975], SURE [Stein, 1981], and empirical Bayesian estimation methods [Robbins, 1964].   \n4. In the multi-task setting, clients are likely to be unwilling to share their actual data but possibly more willing to share group summary statistics. Thus methods developed for our Gaussian model\u2014which only require the group means $\\mathbf{y}$ , group counts $\\mathbf{n}$ , and an estimate of $\\sigma^{2}$ \u2014will be more broadly applicable than methods that act directly on the dataset $S\\subset{\\mathcal{Z}}$ . ", "page_idx": 2}, {"type": "text", "text": "This model can also be naturally extended\u2014using a non-diagonal covariance $\\Sigma_{}^{}.$ \u2014to disaggregated evaluation with non-disjoint groups, e.g., to simultaneously estimate performance both for all women and for only women in their forties. In the interest of brevity we focus on the disjoint group setting. ", "page_idx": 3}, {"type": "text", "text": "2.3 The multi-task setting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We can easily extend this model to study multi-task disaggregated evaluation, in which for each task $t=1,\\ldots,T$ (e.g., a client of the model provider) we observe a set $S_{t}\\subset\\mathcal{Z}$ of $n_{t}$ samples from the task distribution $\\mathcal{D}_{t}$ . The goal is then to output $T$ vectors $\\hat{\\pmb{\\mu}}_{t}$ that are close on-average to the tasks\u2019 subpopulation errors $\\bar{\\mu_{t;g}}=\\mathbb{E}_{z\\sim\\mathcal{D}_{t}}[f(z)|\\bar{z}\\in\\mathcal{Z}_{g}]$ . Converting to our Gaussian model, we observe $T$ vectors $\\mathbf{y}_{t}\\sim\\mathcal{N}(\\mu_{t},\\Sigma_{t})$ \u2014where we set $\\Sigma_{t;g,g}=\\sigma^{2}/n_{t;g}$ for some globally shared $\\sigma^{2}$ and task-specific group count vectors $\\mathbf{n}_{t}\\in\\mathbb{Z}_{\\geq0}^{d}$ \u2014 and must output $T$ mean estimates $\\hat{\\mu}_{t}(\\{\\mathbf{y}_{t}\\}_{t=1}^{T})\\in\\mathbb{R}^{d}$ . ", "page_idx": 3}, {"type": "text", "text": "We consider two natural multi-task baseline estimators. The first is the global naive estimator (or global estimator for short), which combines the data from all tasks, computes a single global vector of group averages, and uses it as the estimate for each task: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{t}^{\\mathrm{global}}(\\{S_{t}\\}_{t=1}^{T})=\\hat{\\mu}^{\\mathrm{naive}}\\left(\\bigcup_{t=1}^{T}S_{t}\\right)\\quad\\mathrm{or}\\quad\\hat{\\mu}_{t}^{\\mathrm{global}}(\\{\\mathbf{y}_{t}\\}_{t=1}^{T})=\\left(\\sum_{t=1}^{T}\\Sigma_{t}^{-1}\\right)^{-1}\\sum_{t=1}^{T}\\Sigma_{t}^{-1}\\mathbf{y}_{t}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "While low variance, the global estimator ignores variation across tasks. Our second baseline\u2014the multi-task offset estimator\u2014shifts the global estimate on each task to ensure that the task\u2019s pooled mean is preserved (thus accounting for the variation in pooled means across individual tasks): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{t}^{\\mathrm{offset}}(\\{\\mathbf{y}_{t}\\}_{t=1}^{T})=\\theta+\\hat{\\mu}^{\\mathrm{poled}}(\\mathbf{y}_{t}-\\theta),\\qquad\\mathrm{where}\\qquad\\theta=\\hat{\\mu}_{t}^{\\mathrm{global}}(\\{\\mathbf{y}_{t}\\}_{t=1}^{T}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the last section we reduced the problem of disaggregated evaluation to that of estimating a mean $\\pmb{\\mu}\\in\\mathbb{R}^{d}$ given a sample $\\mathbf{y}\\sim{\\mathcal{N}}({\\bar{\\mu^{,}}}\\,\\Sigma)$ , where $\\Sigma$ is known and diagonal. We now design a method, SureMap, for the latter problem. Our technical approach involves the following two steps: ", "page_idx": 3}, {"type": "text", "text": "1. Choosing a parameterized mean estimator. We use the MAP estimator under a multivariate normal prior that we design specifically for intersectional subpopulations. ", "page_idx": 3}, {"type": "text", "text": "2. Tuning the estimator\u2019s hyperparameters. We use SURE to estimate the quality of our estimator, which we then optimize over the choice of hyperparameters using the L-BFGS-B algorithm. ", "page_idx": 3}, {"type": "text", "text": "3.1 Designing a parameterized estimator ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As mean estimation is a vast area, we use three criteria for designing an estimator: it should (1) dominate baselines such as $\\hat{\\mu}^{\\mathrm{naive}}$ and $\\hat{\\pmb{\\mu}}^{\\mathrm{pooled}}$ ; (2) have relatively few hyperparameters; and (3) handle heteroskedasticity stemming from variation in group sizes. One natural source of candidates are James\u2013Stein-type shrinkage estimators: the original James\u2013Stein estimator famously dominates $\\hat{\\mu}^{\\mathrm{naive}}$ in MSE and has no hyperparameters to tune [James and Stein, 1961], satisfying our first two desiderata. Furthermore, while James and Stein [1961] assumed an isotropic $\\Sigma$ , subsequent estimators such as the following variant of an estimator due to Bock [1975] do handle heteroskedastic $\\Sigma$ :3 ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{\\theta}^{\\mathrm{Bock}}(\\mathbf{y})=\\theta+\\left(1-\\frac{d-2}{(\\mathbf{y}-\\theta)^{\\top}\\Sigma^{-1}(\\mathbf{y}-\\theta)}\\right)_{+}(\\mathbf{y}-\\theta),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $(\\cdot)_{+}=\\operatorname*{max}\\{\\cdot,0\\}$ , and $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ is a default estimate towards which $\\mathbf{y}$ is shrunk.4 ", "page_idx": 3}, {"type": "text", "text": "However, empirically we find that this often underperforms the pooled estimator in low-sample regimes; further, the form of $\\hat{\\mu}_{\\theta}^{\\mathrm{Bock}}$ shows that the amount of shrinkage towards $\\pmb{\\theta}$ is the same for each coordinate $g\\in[d]$ , despite intuition suggesting that we should shrink less for groups $g$ with more samples. Corrections to this tend to be involved and difficult to generalize [Efron and Morris, 1973]. ", "page_idx": 3}, {"type": "text", "text": "We thus turn to a different family of well-known Gaussian mean estimators: those that return the mode of the posterior distribution assuming $\\pmb{\\mu}$ is sampled from the conjugate prior $\\mathcal{N}(\\pmb{\\theta},\\pmb{\\Lambda})$ with mean $\\pmb{\\theta}\\in\\mathbb{R}^{\\bar{d}}$ and positive-definite covariance $\\mathbf{A}\\in\\mathbb{R}^{d\\times d}$ (e.g., Gelman et al. [2014, Equation 3.12]): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{\\boldsymbol{\\theta},\\Lambda}^{\\mathrm{MAP}}(\\mathbf{y})=(\\mathbf{\\Lambda}^{-1}+\\mathbf{\\Sigma}^{-1})^{-1}(\\mathbf{\\Lambda}^{-1}\\boldsymbol{\\theta}+\\mathbf{\\Sigma}^{-1}\\mathbf{y}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "aTNT3FuVBG/tmp/6ae8f20dbbcb551c09b753da744800ff7a9f6517011096afaa1542119cae3f2a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "aTNT3FuVBG/tmp/b26011b4eb8828889adf0bf1acca16450b6f2e40532462c1b219efe96f804334.jpg", "img_caption": ["Figure 1: Matrices $\\mathbf{C}_{A}\\in\\{0,1\\}^{d\\times d}$ , whose linear combination defines the covariance $\\Lambda(\\tau)$ of our additive intersectional effects prior (Eq. 9). In this example, there are two categories for sex $(\\mathtt{F},\\mathtt{M})$ and three for age (<25,25-64,>64), yielding $d=6$ groups. Shaded squares are 1s, unshaded are 0s. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "MAP naturally handles heteroskedasticity by weighting low-variance coordinates $g$ more heavily, satisfying our third criterion; however, its most general form has $O(d^{2})$ hyperparameters, violating the second. We next use problem structure to restrict the number of hyperparameters needed to define \u039b while still allowing \u00b5\u02c6\u03b8M,A\u039bP to express every baseline introduced in $\\S2.1$ , satisfying our first criterion. ", "page_idx": 4}, {"type": "text", "text": "3.1.1 An additive intersectional effects prior ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For brevity, we build up our estimator somewhat informally; a full description is in $\\S C.1$ . We return to our simple example where each group $g\\in[d]$ corresponds to an intersection $(s,a)\\in[d_{1}]\\times[d_{2}]$ of two attributes: a sex $s\\in[d_{1}]$ and an age $\\bar{a}\\in[d_{2}]$ . A simple prior that additively incorporates individual attribute effects into intersectional group means is the following: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mu_{g}=\\tau_{\\emptyset}\\zeta+\\tau_{\\mathrm{sex}}\\zeta_{a}^{\\mathrm{sex}}+\\tau_{\\mathrm{age}}\\zeta_{a}^{\\mathrm{age}}+\\tau_{\\mathrm{sex,age}}\\zeta_{g}+\\theta_{g}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ and $\\tau_{\\emptyset},\\tau_{\\mathrm{sex}},\\tau_{\\mathrm{age}},\\tau_{\\mathrm{sex,age}}\\in\\mathbb{R}_{\\ge0}$ are hyperparameters, $\\zeta\\sim\\mathcal{N}(0,1)$ is a scalar effect shared across all groups, the vector $\\zeta^{\\mathrm{sex}}\\in\\mathbb{R}^{d_{1}}$ has its $s$ th entry $\\zeta_{s}^{\\mathrm{sex}}\\sim\\mathcal{N}(0,1)$ shared by all groups $g$ whose sex is $s$ , the vector $\\zeta^{\\mathrm{age}}\\in\\mathbb{R}^{d_{2}}$ has its $a$ th entry $\\zeta_{a}^{\\mathrm{age}}\\sim\\mathcal{N}(0,1)$ shared by all groups $g$ whose age is $a$ , and the vector $\\zeta\\sim\\mathcal{N}(\\mathbf{0}_{d},\\mathbf{I}_{d})$ contains an independent noise term $\\zeta_{g}$ for each group $g$ . ", "page_idx": 4}, {"type": "text", "text": "The hyperparameter $\\tau_{\\varnothing}$ quantifies how much we expect all of the means to be shifted (by a shared positive or negative value) from the default $\\pmb{\\theta}$ . Hyperparameters $\\tau_{\\mathrm{sex}}$ and $\\tau_{\\mathrm{age}}$ express how large we expect contributions of sex and age alone to be towards the means. And finally, non-zero $\\tau_{\\mathrm{sex,age}}$ gives the prior flexibility to model heterogeneity across all intersectional groups. ", "page_idx": 4}, {"type": "text", "text": "Given the vector of hyperparameters $\\tau=(\\tau_{\\varnothing},\\dots,\\tau_{\\mathrm{sex,age}})\\in\\mathbb{R}_{\\ge0}^{4}$ , the prior can be written more compactly as $\\pmb{\\mu}\\sim\\mathcal{N}(\\bar{\\pmb{\\theta}},\\bar{\\mathbf{\\Lambda}}\\bar{(\\pmb{\\tau})})$ , where the covariance is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Lambda(\\tau)=\\sum_{A\\subseteq\\{\\mathrm{sex,age}\\}}\\tau_{A}^{2}\\mathbf{C}_{A}=\\tau_{\\varnothing}^{2}\\mathbf{C}_{\\varnothing}+\\tau_{\\mathrm{sex}}^{2}\\mathbf{C}_{\\mathrm{sex}}+\\tau_{\\mathrm{age}}^{2}\\mathbf{C}_{\\mathrm{age}}+\\tau_{\\mathrm{sex,age}}^{2}\\mathbf{C}_{\\mathrm{sex,age}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for matrices ${\\bf C}_{A}\\in\\{0,1\\}^{d\\times d}$ s.t. each entry $C_{A;g,h}$ is one iff groups $g$ and $h$ agree on the attributes included in $A$ . In particular, we have that $\\mathbf{C}_{\\varnothing}=\\mathbf{1}_{d\\times d}$ is the all-ones matrix, the entries $C_{\\mathrm{sex};g,h}$ of $\\mathbf{C}_{\\mathrm{sex}}\\in\\{0,1\\}^{d\\times d}$ are one iff groups $g$ and $h$ share the same sex attribute, the matrix $\\mathbf{C}_{\\mathrm{age}}$ is analogous, and $\\mathbf{C}_{\\mathrm{{sex},a g e}}=\\mathbf{I}_{d}$ is the $d\\times d$ identity. This structure is visualized in Figure 1. ", "page_idx": 4}, {"type": "text", "text": "3.1.2 Efficiency and expressivity ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As detailed in $\\mathrm{\\&C.1}$ , this prior can be naturally extended to any number of attributes $k$ using a covariance matrix $\\mathbf{A}(\\tau)\\,\\in\\,\\mathbb{R}^{d\\times d}$ specified by a vector $\\tau\\in\\mathbb{R}_{\\ge0}^{2^{k}}$ of $2^{k}$ hyperparameters. Since $k\\leq\\lfloor\\log_{2}d\\rfloor$ , the total number of hyperparameters (including $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ ) is $d+2^{k}={\\mathcal{O}}(d)$ , which is much smaller than the $O(d^{2})$ complexity of the general case. We can further reduce this by fixing the entries of $\\pmb{\\theta}$ , constraining them to be identical, or setting them using external (e.g., multi-task) data. ", "page_idx": 5}, {"type": "text", "text": "Despite this reduction in hyperparameters, we can show that for a suitable choice of $\\tau$ , the estimator $\\hat{\\mu}_{\\boldsymbol{\\theta},\\Lambda(\\tau)}^{\\mathrm{M}\\hat{\\mathrm{AP}}}$ recovers many estimators of interest, including the naive estimator and the (possibly offset) pooled estimator (see $\\S C.2)$ . This means that MAP with our structured prior should be able to outperform all four baselines from the previous section, if appropriately tuned. ", "page_idx": 5}, {"type": "text", "text": "3.2 Tuning by minimizing expected risk ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Having specified a parameterized estimator, there remains the question of setting its parameters $\\pmb{\\theta}$ and $\\tau$ . One might want to treat this as a hyperparameter tuning problem and use a data-splitting approach; however, the dimensionality of the problem makes standard techniques either expensive or noisy, and data splitting introduces additional randomness and design decisions into an already data-poor environment. We instead make continued use of our Gaussian assumption and turn to SURE, which given a differentiable estimator $\\hat{\\pmb{\\mu}}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ returns an unbiased estimate of its weighted MSE $L_{\\mu}^{\\mathbf{n}}$ using sample data $\\mathbf{y}\\sim{\\mathcal{N}}({\\pmb{\\mu}},{\\pmb{\\Sigma}})$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{R}_{\\mu}^{\\mathrm{n}}(\\mathbf{y})=\\frac{\\sigma^{2}}{d}\\Big(\\|\\hat{\\pmb{\\mu}}(\\mathbf{y})-\\mathbf{y}\\|_{\\mathbf{\\Sigma}^{-1}}^{2}-d+2\\mathbf{V_{y}}\\cdot\\hat{\\pmb{\\mu}}(\\mathbf{y})\\Big),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where given any $\\mathbf{W}\\succ\\mathbf{0}_{d\\times d}$ we denote $\\|\\mathbf{x}\\|_{\\mathbf{W}}^{2}=\\left\\langle\\mathbf{x},\\mathbf{W}\\mathbf{x}\\right\\rangle\\forall\\,\\mathbf{x}\\in\\mathbb{R}^{d}$ . Using SURE we can now tune the parameters of $\\hat{\\pmb{\\mu}}$ by minimizing $\\hat{R}_{\\mu}^{\\mathbf{n}}(\\mathbf{y})$ in a manner similar to empirical risk minimization. ", "page_idx": 5}, {"type": "text", "text": "3.2.1 Single-task SureMap ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the single-task setting, we fix $\\pmb{\\theta}\\:=\\:\\mathbf{0}_{d}$ and tune the variance parameters $\\tau\\,\\in\\,\\mathbb{R}_{\\ge0}^{2^{k}}$ . Letting ${\\bf A}(\\tau)=({\\bf A}^{-1}(\\tau)+\\Sigma^{-1})^{-1}{\\bf A}^{-1}(\\tau)$ , we define the single-task SureMap estimator as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\pmb{\\mu}}^{\\mathrm{SM}}(\\mathbf{y})=\\hat{\\pmb{\\mu}}_{\\mathbf{0}_{d},\\Lambda(\\hat{\\pmb{\\tau}})}^{\\mathrm{MAP}}(\\mathbf{y})=(\\mathbf{I}_{d}-\\mathbf{A}(\\hat{\\pmb{\\tau}}))\\mathbf{y}}\\\\ &{\\mathrm{for}\\,\\,\\,\\hat{\\pmb{\\tau}}=\\underset{\\pmb{\\tau}\\in\\mathbb{R}_{\\geq0}^{2^{k}}}{\\arg\\operatorname*{min}}\\,\\|\\mathbf{A}(\\pmb{\\tau})\\mathbf{y}\\|_{\\pmb{\\Sigma}^{-1}}^{2}-2\\,\\mathrm{Tr}(\\mathbf{A}(\\pmb{\\tau})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The optimization problem in the second line comes from substituting $\\hat{\\mu}_{\\mathbf{0}_{d},\\Lambda(\\hat{\\tau})}^{\\mathrm{MAP}}$ into SURE (Eq. 10). It is nonconvex, but we find that it can be quickly solved to sufficient accuracy with L-BFGS-B [Byrd et al., 1995], a standard method for bound-constrained optimization of differentiable functions. ", "page_idx": 5}, {"type": "text", "text": "3.2.2 Multi-task SureMap ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To generalize SureMap to the multi-task setting we propose to specify $\\hat{\\pmb\\theta}$ and $\\hat{\\tau}$ by minimizing SURE aggregated across tasks, i.e., $\\sum_{t=1}^{T}\\hat{R}_{\\mu_{t}}^{\\mathbf{n}_{t}}(\\mathbf{y}_{t})$ . While setting both parameters via direct optimization of this objective is the most straightforward approach, we find that it performs worse than single-task SureMap when there are only a few tasks $T\\ \\leq\\ 5)$ ) and rarely improves significantly above the multi-task global and offset estimators. This can be explained by observing the few-task limit\u2014i.e. $T=1$ \u2014in which case optimizing the aggregated SURE objective results in setting $\\hat{\\pmb{\\theta}}=\\mathbf{y}_{1}$ and thus makes the multi-task estimator equivalent to the naive estimator. ", "page_idx": 5}, {"type": "text", "text": "We find that a better approach is to treat the choice of $\\hat{\\pmb\\theta}$ as its own simultaneous mean estimation problem and apply the SureMap approach to it. In particular, our model $\\mathbf{y}_{t}\\sim\\mathcal{N}(\\mu_{t},\\Sigma_{t})$ and our prior $\\bar{\\pmb{\\mu}}_{t}\\sim\\mathcal{N}(\\pmb{\\theta},\\pmb{\\Lambda})$ imply that the samples $\\mathbf{y}_{t}\\sim\\mathcal{N}(\\theta,\\bar{\\Lambda}{+}\\Sigma_{t})$ have mean $\\pmb{\\theta}$ and known covariances (apart from tuning parameters). Therefore, the MAP estimator of $\\pmb{\\theta}$ itself given a hyperprior $\\pmb{\\theta}\\sim\\mathcal{N}(\\mathbf{0}_{d},\\mathbf{T})$ with covariance $\\mathbf{T}\\succ\\mathbf{0}$ will have the form $\\begin{array}{r}{\\hat{\\pmb{\\theta}}=\\left(\\mathbf{\\Gamma}^{-1}+\\sum_{t=1}^{T}(\\mathbf{\\Lambda}\\mathbf{A}+\\pmb{\\Sigma}_{t})^{-1}\\right)^{-1}\\sum_{t=1}^{T}(\\mathbf{\\Lambda}\\mathbf{A}+\\pmb{\\Sigma}_{t})^{-1}\\mathbf{y}_{t}}\\end{array}$ To reduce the number of tuning parameters, we use a prior of the same form as before by specifying ${\\bf T}={\\bf\\Lambda}\\Lambda(v)$ for $\\pmb{v}\\in\\mathbb{R}_{\\geq0}^{2^{k}}$ , i.e., the same structured covariance as described in $\\S3.1.1$ but with separately tuned parameters (see $\\S3.1.1$ and $\\S C.1$ ). Substituting the meta-level MAP estimator of $\\pmb{\\theta}$ into the ", "page_idx": 5}, {"type": "text", "text": "MAP estimator of $\\pmb{\\mu}_{t}$ and tuning the parameters $\\tau$ and $\\pmb{v}$ by optimizing the sum of SUREs (Eq. 10) ", "page_idx": 5}, {"type": "text", "text": "across tasks yields our multi-task SureMap estimator (see $\\mathrm{\\SC.}3$ for details): ", "page_idx": 6}, {"type": "equation", "text": "$\\hat{\\mu}_{t}^{\\mathrm{SM}}(\\{\\mathbf{y}_{t}\\}_{t=1}^{T})=\\hat{\\mu}_{\\hat{\\theta}(\\hat{\\tau},\\hat{v}),\\Lambda(\\hat{\\tau})}^{\\mathrm{MAP}}(\\mathbf{y}_{t})=\\mathbf{y}_{t}+\\mathbf{A}_{t}(\\hat{\\tau})(\\hat{\\theta}(\\hat{\\tau},\\hat{v})-\\mathbf{y}_{t})$ ", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{or}\\ \\hat{\\theta}(\\tau,v)=\\displaystyle\\sum_{t=1}^{T}\\mathbf{M}_{t}(\\tau,v)\\mathbf{y}_{t},\\qquad\\mathbf{A}_{t}(\\tau)=\\Big(\\mathbf{A}^{-1}(\\tau)+\\boldsymbol{\\Sigma}_{t}^{-1}\\Big)^{-1}\\mathbf{A}^{-1}(\\tau),}\\\\ &{\\quad\\mathbf{M}_{t}(\\tau,v)=\\bigg(\\mathbf{A}^{-1}(v)+\\displaystyle\\sum_{s=1}^{T}(\\mathbf{A}(\\tau)+\\boldsymbol{\\Sigma}_{s})^{-1}\\bigg)^{-1}(\\mathbf{A}(\\tau)+\\boldsymbol{\\Sigma}_{t})^{-1},}\\\\ &{\\quad\\hat{\\tau},\\hat{v}=\\arg\\operatorname*{min}_{t}\\displaystyle\\sum_{\\tau,v\\in\\mathbb{R}_{\\geq0}^{2k}}^{T}\\Big[\\big\\|\\mathbf{A}_{t}(\\tau)(\\hat{\\theta}(\\tau,v)-\\mathbf{y}_{t})\\big\\|_{\\Sigma_{t}^{-1}}^{2}+2\\,\\mathrm{Tr}\\big(\\mathbf{A}_{t}(\\tau)(\\mathbf{M}_{t}(\\tau,v)-\\mathbf{I}_{d})\\big)\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The optimization problem on the last line can again be approximately solved using L-BFGS-B. ", "page_idx": 6}, {"type": "text", "text": "3.3 Limitations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Various modeling assumptions impact performance of SureMap. For instance, the Gaussian error assumption is less appropriate when errors are heavy-tailed. In $\\S\\mathrm{G}$ , Figure 9, we consider MSE as an example of a target metric with heavy-tailed observation errors. We find that SureMap still performs well, but is no longer superior to previous approaches. One avenue for improvement would be to use a variance-stabilizing transformation (e.g., Hawkins and Wixley [1986]) prior to applying SureMap. ", "page_idx": 6}, {"type": "text", "text": "Note that SureMap achieves its improved accuracy by shrinking naive estimates towards a less granular estimator (e.g., a pooled mean). As a result the estimation is biased towards less disparity, which could lead to overly optimistic conclusions about fairness. For this reason it is extremely important to examine not just the point estimates, but also confidence intervals. These can be obtained, for example, by viewing SureMap as a regression approach (\u00a7E) and leveraging inference techniques for regression. ", "page_idx": 6}, {"type": "text", "text": "4 Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate our approach in several representative settings for disaggregated evaluation, including two tabular settings appearing in previous works [Miller et al., 2021, Herlihy et al., 2024, Liu et al., 2024], and three new settings: a multi-task tabular setting based on state-level U.S. census data and both a single-task and a multi-task ASR evaluation setting. ", "page_idx": 6}, {"type": "text", "text": "4.1 Tabular datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We consider three tabular datasets, two for the single-task and one for the multi-task setting, covering two important domains where fairness concerns can arise: healthcare records and demographic data. While we focus on the classification task and 0-1 error, in $\\S\\mathrm{G}$ we also report results for regression (Figures 8 & 9) and for classification with AUC as the target metric (Figures 10 & 12). ", "page_idx": 6}, {"type": "text", "text": "Diabetes. This is a tabular dataset of Strack et al. [2014], containing around 100K patient records with six race, two sex, and three age categories. We evaluate a logistic regression classifier trained to predict patient disposition after a hospital stay (discharged or otherwise). The target metric is the 0-1 error. ", "page_idx": 6}, {"type": "text", "text": "Adult. We use the classic Adult census dataset [Kohavi, 1996] to evaluate performance of an in-context LLM learner\u2014specifically llama-3-70b\u2014in predicting whether a person makes more or less than $\\mathbb{S}50\\mathrm{K}$ after being provided with eight examples via a modification of the prompt template of Liu et al. [2024]. The target metric is the 0-1 error, disaggregation is by race, sex, and age. ", "page_idx": 6}, {"type": "text", "text": "State-Level ACS (SLACS). This is a tabular dataset for multi-task setting derived from the census data for all U.S. states and Puerto Rico assembled by Ding et al. [2021]. Each datapoint corresponds to a person in one of nine race and two sex categories; we consider three age categories: below 25, 25\u201364, and over 64. The underlying task is to classify each person as earning either more or less than $\\mathbb{S}50\\mathrm{K}$ . We train a regularized logistic model on the data from California, and seek to evaluate its performance on the other 50 states/territories, which comprise the tasks. The target metric is the 0-1 error. ", "page_idx": 6}, {"type": "text", "text": "4.2 ASR datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We also introduce both single-task and multi-task speech recognition datasets, based on applying the popular Whisper ASR model [Radford et al., 2023]\u2014specifically whisper-tiny\u2014on the English part of the Common Voice (CV) dataset [Ardila et al., 2020], which contains utterances from individuals in one of nine age and three sex categories. ", "page_idx": 6}, {"type": "image", "img_path": "aTNT3FuVBG/tmp/c0dfa78bfb919ae5e456fae0f3eb1227739b5212e1b7376727905645c1bcfafd.jpg", "img_caption": ["Figure 2: Single-task evaluations on Diabetes (top, disaggregating by race, sex, and age), Adult (middle, disaggregating by race, sex, and age), and Common Voice (bottom, disaggregating by sex and age). The MAE is averaged across all groups (left), large groups (center), or small groups (right). Large and small groups are defined as above and below median group size. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Common Voice. This is a single-task dataset obtained by combining the validation and test partitions of the CV dataset. We calculate the word-error rate (WER) across all the utterances of each individual, which becomes the target metric to be predicted. ", "page_idx": 7}, {"type": "text", "text": "Common Voice Clusters (CVC). This is a multi-task ASR dataset. To construct it, we first cluster the utterances in the train partition of the CV dataset into 20 clusters by applying $k$ -means to the sums of GloVe word embeddings [Pennington et al., 2014] of their corresponding text strings. To model task relatedness, we then randomly reassign each utterance to a random cluster with probability $\\alpha\\in[0,1]$ . The resulting clusters are the tasks. The target metric is the word-error rate (WER) across all the utterances of each individual in a given cluster. In most experiments we use $\\alpha\\,=\\,{\\textstyle{\\frac{1}{2}}}$ , but we also investigate what happens when $\\alpha$ varies between zero, i.e., the original clusters, and one, corresponding to identically distributed tasks. ", "page_idx": 7}, {"type": "text", "text": "5 Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our main metric is MAE relative to a ground truth vector, which we take to be the mean of all available data for each subpopulation $g\\in[d]$ , except those with fewer than 40 samples. In our main results we subsample with replacement from the entire dataset at different rates and track performance as a function of the sizes of the resulting datasets. To obtain $95\\%$ confidence intervals we conduct 200 and 40 random trials at each subsampling rate in the single-task and multi-task settings, respectively. ", "page_idx": 7}, {"type": "text", "text": "5.1 Single-task ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare SureMap to the naive (Eq. 2) and pooled (Eq. 3) baselines, as well as to the Bock estimator with shrinkage towards the pooled estimator (Eq. 17) and the structured regression estimator of Herlihy et al. [2024]. On both Diabetes and Adult, SureMap significantly outperforms all competitors (Figure 2, top & middle), the greatest improvement is on subpopulations with limited data. In $\\S\\mathrm{G}$ , we consider a regression variant of Diabetes and observe similar results (Figure 8). On the Common Voice task, SureMap performs roughly similarly to Bock, while outperforming structured regression at some subsampling rates (Figure 2, bottom); here again the gains are driven by better performance on small groups. Pooling performs best when data is extremely limited, but it is not competitive with even modestly more data, and also underperforms on small groups. ", "page_idx": 7}, {"type": "image", "img_path": "aTNT3FuVBG/tmp/9fababe7e48db8a8212acb94f57ebd71ff120dc9b64f380a7e792c51c7b8958e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "", "img_caption": ["Figure 3: Multi-task evaluations on SLACS (top, disaggregating by race, sex, and age) and CVC (bottom, disaggregating by sex and age). Left: Performance across different subsampling rates. Right: Multiplicative improvement in MAE over naive estimator on individual tasks; subsampling rate $_{=0.1}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.2 Multi-task ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the multi-task setting, we use all the single-task methods as baselines while adding multi-task (MT) ones, including MT global (Eq. 4), MT offset (Eq. 5), and an MT extension of Bock (Eq. 6) in which $\\theta_{g}$ is set using the average across the group $g$ data on all other tasks. We first consider the SLACS task, for which Figure 3 (top left) shows that MT SureMap significantly outperforms other methods in the low-data regime while matching the best one (MT Bock) in the high-data regime. At subsampling rate 0.1, Figure 3 (top right) also shows that using multi-task data leads to improvement on all but two of the fifty tasks, that the reduction in MAE over the naive estimator on a typical task is $2\\mathbf{x}$ , and that this improvement only loosely correlates with the task\u2019s ground truth distance from the multi-task median. On the other hand, it also shows that while MT Bock\u2019s improvements are typically smaller, on SLACS it improves performance for every state (including the two where MT SureMap is worse). ", "page_idx": 8}, {"type": "text", "text": "On the CVC task, the MT offset baseline is the most competitive, except at the lowest subsampling rate where pooling is better and at higher subsampling rates where it stops improving with additional data (Figure 3, bottom left). SureMap outperforms it and all other methods across all subsampling rates and its advantage is greatest in low-data regimes, where it even outperforms pooling. In the task-level evaluation in Figure 3 (bottom right) we see that on every task, MT SureMap attains an improvement of $2{-}3.5\\mathrm{x}$ over the naive baseline and almost always outperforms MT Bock. Furthermore, the latter performs substantially worse on tasks whose ground truth vectors are far away from the multi-task center while MT SureMap is not affected. ", "page_idx": 8}, {"type": "text", "text": "5.3 Ablations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We next look at how the degree of included intersectional effects and multi-task structure affect performance. In Figure 4 (left), we evaluate utility of including higher-order interactions in the structured prior. We implement SureMap variants with up to $\\ell{\\mathrm{th}}$ -order interactions by setting $\\tau_{A}$ for $|A|\\in$ $\\{\\ell{+}1,\\ldots,k{-}1\\}$ to zero (but not for $|A|=k$ ). We observe that including zeroth-order effects $\\mathit{\\Pi}^{\\prime}\\mathit{\\Pi}^{\\ell}=0$ ) in single-task SureMap (i.e., shrinkage to pooling) improves upon the usual single-parameter Gaussian prior $\\mathcal{E}=-1$ ) in low-data regimes. Adding first-order effects ( $\\mathcal{\\ell}=1$ ) leads to substantial further im", "page_idx": 8}, {"type": "image", "img_path": "aTNT3FuVBG/tmp/da908a4a95b5c4c6484cd6c1a70bce4f27c407a90d574f73b7fa6f80b3a72f29.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 4: Left: Comparison of SureMap variants on SLACS. The SureMap (\u2113) variant sets to zero the entries of $\\tau$ corresponding to interactions of size $>\\ell$ (except for the highest-order interactions). Right: Evaluation of different methods as the interpolation coefficient that defines the CVC tasks is varied. ", "page_idx": 9}, {"type": "image", "img_path": "aTNT3FuVBG/tmp/157070102465f3bd6f4e67bfe2afaa907af7d907d73d929a4ccb71a0fb4f37a1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "", "img_caption": ["Figure 5: Performance as the number of tasks varies, evaluated on SLACS (left) and CVC (right). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "provement, but second-order effects $\\mathcal{E}=2$ ) make performance slightly worse. A similar effect can be observed among the multi-task variants, except there is no loss (but also no gain) to using the highestorder variant $\\mathcal{E}=2$ ). Overall, this study suggests that using the full-order SureMap is a reasonable default but that most of the method\u2019s effectiveness comes from zeroth- and first-order effects. ", "page_idx": 9}, {"type": "text", "text": "Figure 4 (right) tracks performance as the task-similarity parameter defining the CVC task is varied. MT SureMap outperforms all methods at all settings and is also not as strongly affected by the task similarity, at least as it is defined for the CVC data. This suggests that the structured prior we use may be useful even if the dataset means are quite different but the underlying evaluation problem (in this case estimating WER of ASR models) is the same. ", "page_idx": 9}, {"type": "text", "text": "Lastly, in Figure 5 we study how the number of tasks affects multi-task performance. On both SLACS and CVC, MT SureMap outperforms all single-task baselines (the best one being the single-task SureMap) at $T=2$ tasks, i.e., it can take advantage of even very little external information. In contrast, on CVC, the competitor multi-task methods (e.g., MT offset and MT Bock) do not even outperform single-task methods until $T\\geq5$ tasks. These results demonstrate that, unlike these comparators, multitask SureMap can be confidently used even when only one additional client\u2019s worth of data is available. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have introduced SureMap, a disaggregated evaluation approach, which combines MAP estimation under a structured Gaussian prior with hyperparameter tuning via SURE. SureMap achieves substantial empirical improvements over strong baselines in both single-task and multi-task settings. Valuable future directions include improving robustness to heavy-tailed data and developing multitask methods that can handle client privacy concerns. More broadly, we hope our work will have a positive impact by allowing model users to more accurately identify fairness-related harms, the first step towards mitigating them. However, using SureMap and any other disaggregated evaluation approach must be done with care, so as to not risk overconfidence in a model\u2019s fairness. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. Common Voice: A massivelymultilingual speech corpus. In Proceedings of the 12th Language Resources and Evaluation Conference, 2020.   \nSolon Barocas, Anhong Guo, Ece Kamar, Jacquelyn Krones, Meredith Ringel Morris, Jennifer Wortman Vaughan, W. Duncan Wadsworth, and Hanna Wallach. Designing disaggregated evaluations of AI systems: Choices, considerations, and tradeoffs. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, 2021.   \nMary Ellen Bock. Minimax estimators of the mean of a multivariate normal distribution. The Annals of Statistics, 3(1):209\u2013218, 1975.   \nJames R. Bunch, Christopher P. Nielsen, and Danny C. Sorensen. Rank-one modification of the symmetric eigenproblem. Numerische Mathematik, 31:31\u201348, 1978.   \nRichard H. Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing, 16(5):1190\u20131208, 1995.   \nCorinna Cortes and Mehryar Mohri. AUC optimization vs. error rate minimization. In Advances in Neural Information Processing Systems, 2003.   \nFrances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring Adult: New datasets for fair machine learning. In Advances in Neural Information Processing Systems, 2021.   \nDavid L. Donoho and Iain M. Johnstone. Adapting to unknown smoothness via wavelet shrinkage. Journal of the American Statistical Association, 90(432):1200\u20131224, 1995.   \nBradley Efron and Carl Morris. Stein\u2019s estimation rule and its competitors\u2013an empirical Bayes approach. Journal of the American Statistical Association, 68(341):117\u2013130, 1973.   \nSergey Feldman, Maya R. Gupta, and Bela A. Frigyik. Revisiting Stein\u2019s paradox: Multi-task averaging. Journal of Machine Learning Research, 15:3621\u20133662, 2014.   \nAndrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. Bayesian Data Analysis. Electronic Edition, 2014.   \nDouglas M. Hawkins and R. A. J. Wixley. A note on the transformation of chi-squared variables to normality. The American Statistician, 40(4):296\u2013298, 1986.   \nChristine Herlihy, Kimberly Truong, Alexandra Chouldechova, and Miroslav Dud\u00edk. A structured regression approach for evaluating model performance across intersectional subgroups. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, 2024.   \nRoger A. Horn and Charles R Johnson. Matrix Analysis. Cambridge University Press, second edition, 2013.   \nWillard James and Charles M. Stein. Estimation with quadratic loss. In Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, 1961.   \nRon Kohavi. Scaling up the accuracy of Naive-Bayes classifiers: A decision-tree hybrid. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, 1996.   \nS\u00f6ren Laue, Matthias Mitterreiter, and Joachim Giesen. Computing higher order derivatives of matrix and tensor expressions. In Advances in Neural Information Processing Systems, 2018.   \nErich Leo Lehmann. Consistency and unbiasedness of certain nonparametric tests. Annals of Mathematical Statistics, 22(1):165\u2013179, 1951.   \nKer-Chau Li. From Stein\u2019s unbiased risk estimates to the method of generalized cross validation. The Annals of Statistics, 13(4):1352\u20131377, 1985. ", "page_idx": 10}, {"type": "text", "text": "Jun S. Liu. Siegel\u2019s formula via Stein\u2019s identities. Statistics & Probability Letters, 21:247\u2013251, 1994. ", "page_idx": 11}, {"type": "text", "text": "Yanchen Liu, Srishti Gautam, Jiaqi Ma, and Himabindu Lakkaraju. Confronting LLMs with traditional ML: Rethinking the fairness of large language models in tabular classifications. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2024.   \nAndrew C. Miller, Leon A. Gatys, Joseph Futoma, and Emily Fox. Model-based metrics: Sampleefficient estimates of predictive model subpopulation performance. In Proceedings of the 6th Machine Learning for Healthcare Conference, 2021.   \nJeffrey Pennington, Richard Socher, and Christopher D. Manning. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 2014.   \nVihari Piratla, Soumen Chakrabarti, and Sunita Sarawagi. Active assessment of prediction services as accuracy surface over attribute combinations. In Advances in Neural Information Processing Systems, 2021.   \nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In Proceedings of the 40th International Conference on Machine Learning, 2023.   \nHerbert Robbins. The empirical Bayes approach to statistical decision problems. The Annals of Mathematical Statistics, 35(1):1\u201320, 1964.   \nSidney Siegel. Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill, 1956.   \nCharles M. Stein. Estimation of the mean of a multivariate normal distribution. The Annals of Statistics, 9(6):1135\u20131151, 1981.   \nBeata Strack, Jonathan P. DeShazo, Chris Gennings, Juan L. Olmo, Sebastian Ventura, Krzysztof J. Cios, , and John N. Clore. Impact of HbA1c measurement on hospital readmission rates: Analysis of 70,000 clinical database patient records. BioMed Research International, 2014, 2014.   \nQing Wang and Alexandria Guo. An efficient variance estimator of AUC and its applications to binary classification. Statistics in Medicine, 39:4107\u20134349, 2020.   \nHilde Weerts, Miroslav Dud\u00edk, Richard Edgar, Adrin Jalali, Roman Lutz, and Michael Madaio. Fairlearn: Assessing and improving fairness of AI systems. Journal of Machine Learning Research, 24:1\u20138, 2023.   \nXianchao Xie, S. C. Kou, and Lawrence D. Brown. SURE estimates for a heteroscedastic hierarchical model. Journal of the American Statistical Association, 107(500):1465\u20131479, 2012. ", "page_idx": 11}, {"type": "text", "text": "A Notation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "\u2022 For $p\\in[1,\\infty]$ we use $\\lVert\\cdot\\rVert_{p}$ to denote the $p$ -norm on $\\mathbb{R}^{d}$ .   \n\u2022 We use $\\left\\Vert\\cdot\\right\\Vert$ to denote the spectral norm on $\\mathbb{R}^{m\\times n}$ .   \n\u2022 For any positive semi-definite matrix $\\mathbf{A}\\succeq\\mathbf{0}_{d\\times d}$ we use the notation $\\|\\cdot\\|_{\\mathbf{A}}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}_{\\geq0}$ to denote the vector norm $\\|\\mathbf{x}\\|_{\\mathbf{A}}={\\sqrt{\\langle\\mathbf{x},\\mathbf{Ax}\\rangle}}=\\|{\\sqrt{\\mathbf{A}}}\\mathbf{x}\\|_{2}$ on $\\mathbf{x}\\in\\mathbb{R}^{d}$ .   \n\u2022 For any positive integer $k$ we use $[k]$ to denote the set $\\{1,\\ldots,k\\}$ .   \n\u2022 For any set $S$ we use $2^{S}$ to denote its powerset.   \n\u2022 For any vector $\\mathbf{a}\\in\\mathbb{R}^{d}$ we use $a_{i}$ to denote its $i$ th entry. If $\\mathbf{a}$ is only defined as an expression then we will abuse notation and use $(\\mathbf{a})_{i}$ to refer to its ith entry.   \n\u2022 For any vector $\\textbf{a}\\in\\mathbb{R}^{k}$ and any subset $\\textit{S}\\subset[k]$ with elements $s_{1}\\ <\\ \\cdot\\ \\cdot\\ <\\ s_{m}$ we define $\\mathbf{a}_{S}\\;=\\;\\left(a_{s_{1}}\\,\\mathrm{~\\boldmath~\\rho~}\\,\\cdot\\,\\mathrm{~\\boldmath~\\rho~}\\,a_{s_{m}}\\right)$ to be the vector of the entries of a whose indices correspond to the elements of $S$ sorted in ascending order.   \n\u2022 For any subset $S\\subset[k]$ we assume $\\times_{s\\in S}$ iterates over the elements in ascending order.   \n\u2022 For any matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$ we use $A_{i,j}$ to denote its $(i,j)$ th entry and ${\\bf A}_{i},$ : its $i$ th row. If A is only defined as an expression then we will abuse notation and use $(\\mathbf{A})_{i,j}$ to refer to its $(i,j)$ th entry.   \n\u2022 For any $k$ -tensor $\\mathbf{Z}\\in\\mathbb{R}^{\\times_{a=1}^{k}d_{a}}$ with dimensions $d_{1},\\dotsc,d_{k}\\in\\mathbb{Z}_{>0}$ and any vector $\\mathbf{c}\\in\\mathsf{X}_{a=1}^{k}[d_{a}]$ of indices we use $Z_{\\mathbf{c}}$ to refer the the $(c_{1},\\ldots,c_{k})\\mathrm{th}$ entry of $\\mathbf{Z}$ .   \n\u2022 We use $\\mathbf{\\Delta}\\mathbf{)}_{m},\\mathbf{1}_{m}\\,\\in\\,\\mathbb{R}^{m}$ and $\\mathbf{0}_{m\\times n},\\mathbf{1}_{m\\times n}\\in\\mathbb{R}^{m\\times n}$ to refer to all-zero and all-one vectors and matrices, and ${\\mathbf{I}}_{n}$ to refer the $n\\times n$ identity matrix. ", "page_idx": 12}, {"type": "text", "text": "B Heteroskedastic James\u2013Stein-type shrinkage estimation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "For reference we state a weighted version of Stein\u2019s unbiased risk estimate (SURE) [Stein, 1981]: ", "page_idx": 12}, {"type": "text", "text": "Lemma B.1. Suppose $\\mathbf{y}\\sim{\\mathcal{N}}({\\pmb{\\mu}},{\\pmb{\\Sigma}})$ for mean $\\pmb{\\mu}\\in\\mathbb{R}^{d}$ and diagonal p.s.d. covariance $\\Sigma\\in\\mathbb{R}^{d\\times d}$ and consider a function $\\hat{\\pmb{\\mu}}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}\\,s.t.$ . for every $i\\in[d]$ the function $\\hat{\\mu}_{i}$ is almost differentiable in $y_{i}$ and we have $\\mathbb{E}\\|\\nabla_{\\mathbf{y}}\\cdot\\hat{\\pmb{\\mu}}(\\mathbf{y})\\|_{1}\\,<\\,\\infty$ . Then for any diagonal matrix $\\textbf{W}\\in\\mathbb{R}^{d\\times d}$ we have $\\mathbb{E}\\|\\hat{\\mu}(\\mathbf{y})-\\mu\\|_{\\mathbf{W}}^{2}=\\mathbb{E}\\|\\hat{\\mu}(\\mathbf{y})-\\mathbf{y}\\|_{\\mathbf{W}}^{2}-\\operatorname{Tr}(\\mathbf{W}\\mathbf{\\Sigma})+2\\mathbb{E}[\\nabla_{\\mathbf{y}}\\cdot(\\mathbf{W}\\Sigma\\hat{\\mu}(\\mathbf{y}))]$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Expanding the squared norm and applying Stein\u2019s Lemma [Stein, 1981, Lemma 2] to the last term yields ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|\\hat{\\mu}(\\mathbf{y})-\\mu\\|_{\\mathbf{W}}^{2}=\\mathbb{E}\\|\\hat{\\mu}(\\mathbf{y})-\\mathbf{y}\\|_{\\mathbf{W}}^{2}+\\mathbb{E}\\|\\mathbf{y}-\\mu\\|_{\\mathbf{W}}^{2}+2\\mathbb{E}\\langle\\hat{\\mu}(\\mathbf{y})-\\mathbf{y},\\mathbf{W}(\\mathbf{y}-\\mu)\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}\\|\\hat{\\mu}(\\mathbf{y})-\\mathbf{y}\\|_{\\mathbf{W}}^{2}+\\mathrm{Tr}(\\mathbf{W}\\mathbf{\\Sigma})+2\\mathbb{E}[\\nabla_{\\mathbf{y}}\\cdot(\\mathbf{W}\\Sigma\\hat{\\mu}(\\mathbf{y}))-2\\,\\mathrm{Tr}(\\mathbf{W}\\mathbf{\\Sigma})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We now turn to James\u2013Stein-type estimators of the form \u00b5\u02c6(y) = y \u2212\u2225P(y\u2212cb)\u22252\u22121 for $c\\in\\mathbb R$ , $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ , and $\\mathbf{P}\\in\\mathbb{R}^{d\\times d}$ , roughly extending the one considered by Bock [1975] (in a more general form) to $\\mathbf{P}\\neq\\mathbf{I}_{d}$ and $\\mathbf{b}\\neq\\mathbf{0}_{d}$ . Setting $\\mathbf{x}=\\mathbf{\\bar{P}}(\\mathbf{y}-\\pmb{\\theta})$ , we apply Lemma B.1 to show that the expected $\\Sigma^{-1}$ -weighted MSE is ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|\\hat{\\mu}(\\mathbf{y})-\\pmb{\\mu}\\|_{\\mathbf{\\Sigma}^{-1}}^{2}=d+\\mathbb{E}\\left[\\frac{c^{2}-2c\\,\\mathrm{Tr}(\\mathbf{P})}{\\|\\mathbf{x}\\|_{\\mathbf{\\Sigma}^{-1}}^{2}}+\\frac{4c\\,\\mathrm{Tr}\\left(\\mathbf{x}\\mathbf{x}^{\\top}\\Sigma^{-1}\\mathbf{P}\\right)}{\\|\\mathbf{x}\\|_{\\mathbf{\\Sigma}^{-1}}^{4}}\\right]}\\\\ &{\\qquad\\qquad\\qquad=d+\\mathbb{E}\\left[\\frac{c^{2}-2c\\,\\mathrm{Tr}(\\mathbf{P})}{\\|\\mathbf{x}\\|_{\\mathbf{\\Sigma}^{-1}}^{2}}+\\frac{4c\\,\\mathrm{Tr}\\left(\\Sigma^{-\\frac{1}{2}}\\mathbf{x}\\mathbf{x}^{\\top}\\Sigma^{-\\frac{1}{2}}\\Sigma^{-\\frac{1}{2}}\\mathbf{P}\\Sigma^{\\frac{1}{2}}\\right)}{\\|\\mathbf{x}\\|_{\\Sigma^{-1}}^{4}}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq d+\\mathbb{E}\\left[\\frac{c^{2}\\|\\pmb{\\Sigma}\\|-2c\\,\\mathrm{Tr}\\left(\\Sigma\\mathbf{P}\\right)+4c\\|\\Sigma^{-\\frac{1}{2}}\\mathbf{P}\\Sigma^{\\frac{1}{2}}\\|}{\\|\\mathbf{x}\\|_{\\Sigma^{-1}}^{2}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where to compute the derivative in SURE we made use of the matrix calculus tool of Laue et al. [2018] and the last line follows by von Neumann\u2019s trace inequality. This upper bound is minimized at $c=\\mathrm{Tr}(\\mathbf{P})-2\\|\\Sigma^{-\\frac{1}{2}}\\mathbf{P}\\Sigma^{\\frac{1}{2}}\\|$ . Note that for $\\mathbf{P}=\\mathbf{I}_{d}$ this setting of $c$ exactly recovers the Bock estimator \u00b5\u02c6\u03b8B $\\hat{\\pmb{\\mu}}_{\\boldsymbol{\\theta}}^{\\mathrm{Bock}}$ in Equation 6, apart from the positive-part correction. ", "page_idx": 12}, {"type": "text", "text": "On the other hand, for shrinking to the pooled mean \u00b5\u02c6pooled(y) = 1dTr\u00d7(d\u03a3\u03a3\u2212\u221211)y we can apply a wellknown fact about the eigenvalues of symmetric rank-one updates (e.g. Bunch et al. [1978, Theorem 1]) to diagonal matrices to obtain \u2225\u03a3\u221221 P\u03a312 \u2225=   Id \u2212\u03a3\u22122T r1(d\u03a3\u00d7\u2212d1\u03a3)\u22122 , which yields the setting in Feldman et al. [2014] of $c=\\mathrm{Tr}(\\mathbf{P})-2\\|\\Sigma^{-\\frac{1}{2}}\\mathbf{P}\\Sigma^{\\frac{1}{2}}\\|=d-3$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{\\mu}^{\\mathrm{Bock}}(\\mathbf{y})=\\hat{\\mu}^{\\mathrm{poled}}(\\mathbf{y})+\\left(1-\\frac{d-3}{\\|\\mathbf{y}-\\hat{\\mu}^{\\mathrm{pooled}}(\\mathbf{y})\\|_{\\Sigma^{-1}}^{2}}\\right)_{+}(\\mathbf{y}-\\hat{\\mu}^{\\mathrm{pooled}}(\\mathbf{y}))\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lastly, we note that it is straightforward to extend SureMap to the case of non-diagonal covariance matrices $\\Sigma$ , i.e., when we want to simultaneously release performance estimates for groups with different numbers of intersections (e.g., just race, just age, and both race and age). In this case there will be nonzero covariance between elements of the vector y, e.g., those corresponding to a specific age bracket and an intersection of that bracket with a specific race. To handle this, one only needs to derive the SURE objective estimating the (non-diagonal) $\\Sigma^{-1}$ -weighted risk of the MAP estimator with (the same, non-diagonal) covariance $\\Sigma$ ; this can be done with the following generalization of Lemma B.1: ", "page_idx": 13}, {"type": "text", "text": "Lemma B.2. Suppose $\\mathbf{y}\\sim{\\mathcal{N}}({\\pmb{\\mu}},{\\pmb{\\Sigma}})$ for mean $\\pmb{\\mu}\\in\\mathbb{R}^{d}$ and p.s.d. covariance $\\Sigma\\,\\in\\,\\mathbb{R}^{d\\times d}$ , and consider a function $\\hat{\\mu}:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ s.t. for every $i\\in[d]$ the function $\\hat{\\mu}_{i}$ is a.e. differentiable in $y_{i}$ and $\\mathbb{E}\\|\\nabla_{\\mathbf{y}}\\cdot\\hat{\\boldsymbol{\\mu}}(\\mathbf{y})\\|_{1}<\\infty$ . Then for any p.s.d. $\\mathbf{W}\\in\\mathbb{R}^{d\\times d}\\mathbf{\\Sigma}_{W}$ e have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|\\hat{\\mu}(\\mathbf{y})-\\mu\\|_{\\mathbf{W}}^{2}=\\mathbb{E}\\|\\hat{\\mu}(\\mathbf{y})-\\mathbf{y}\\|_{\\mathbf{W}}^{2}+\\operatorname{Tr}(\\mathbf{W}\\mathbf{Z})+2\\mathbb{E}\\sum_{i=1}^{d}\\sum_{j=1}^{d}(\\Sigma\\odot(\\mathbf{W}\\nabla_{\\mathbf{y}}\\hat{\\mu}(\\mathbf{y})-\\mathbf{W}))_{i,j}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\nabla_{\\mathbf{y}}\\hat{\\pmb{\\mu}}(\\mathbf{y})$ is the Jacobian of $\\hat{\\pmb{\\mu}}$ with entries $\\nabla_{\\mathbf{y};i,j}\\hat{\\pmb{\\mu}}(\\mathbf{y})=\\partial_{y_{j}}\\hat{\\mu}_{i}(\\mathbf{y})$ ", "page_idx": 13}, {"type": "text", "text": "Proof. Note that since $\\mathbb{E}\\mathbf{y}\\,=\\,\\pmb{\\mu}$ we have by a multivariate version of Stein\u2019s lemma (e.g., Liu [1994, Lemma 1]) that the following identity holds for any a.e. continuous $f\\,:\\,\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ s.t. $\\mathbb{E}|\\partial_{y_{i}}f(\\mathbf{y})|<\\infty\\,\\forall\\,i\\in[d]$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[(\\mathbf{y}-\\pmb{\\mu})_{i}f(\\mathbf{y})]=\\operatorname{Cov}((\\mathbf{y}-\\pmb{\\mu})_{i},f(\\mathbf{y}))=\\langle\\pmb{\\Sigma}_{i,:},\\pmb{\\nabla}_{\\mathbf{y}}f(\\mathbf{y})\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using this equality with $f(\\mathbf{y})=(\\mathbf{W}({\\hat{\\mu}}-\\mathbf{y}))_{i}$ yields ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|\\hat{\\mu}(\\mathbf{y})-\\boldsymbol{\\mu}\\|_{\\mathbf{W}}^{2}}\\\\ &{\\quad=\\mathbb{E}\\|\\hat{\\mu}(\\mathbf{y})-\\mathbf{y}\\|_{\\mathbf{W}}^{2}+\\mathbb{E}\\|\\mathbf{y}-\\boldsymbol{\\mu}\\|_{\\mathbf{W}}^{2}+2\\mathbb{E}\\langle\\hat{\\mu}(\\mathbf{y})-\\mathbf{y},\\mathbf{W}(\\mathbf{y}-\\boldsymbol{\\mu})\\rangle}\\\\ &{\\quad=\\mathbb{E}\\|\\hat{\\mu}(\\mathbf{y})-\\mathbf{y}\\|_{\\mathbf{W}}^{2}+\\mathrm{Tr}(\\mathbf{W}\\mathbf{X})+2\\mathbb{E}\\displaystyle\\sum_{i=1}^{d}\\sum_{i=1}^{d}\\langle\\mathbf{X}_{i},\\nabla_{y}((\\mathbf{W}\\hat{\\mu}(\\mathbf{y}))_{i}-(\\mathbf{W}\\mathbf{y})_{i})\\rangle}\\\\ &{\\quad=\\mathbb{E}\\|\\hat{\\mu}(\\mathbf{y})-\\mathbf{y}\\|_{\\mathbf{W}}^{2}+\\mathrm{Tr}(\\mathbf{W}\\mathbf{X})+2\\mathbb{E}\\displaystyle\\sum_{i=1}^{d}\\sum_{j=1}^{d}\\sum_{i=j}^{d}\\sum_{i=j}^{d}\\sum_{i=i,i}\\hat{\\mu}_{j}(\\langle\\mathbf{W}_{i,:},\\hat{\\mu}(\\mathbf{y})\\rangle-\\mathbf{W}_{i,:}\\mathbf{y})}\\\\ &{\\quad=\\mathbb{E}\\|\\hat{\\mu}(\\mathbf{y})-\\mathbf{y}\\|_{\\mathbf{W}}^{2}+\\mathrm{Tr}(\\mathbf{W}\\mathbf{Z})+2\\mathbb{E}\\displaystyle\\sum_{i=1}^{d}\\sum_{j=1}^{d}\\sum_{i=1}^{d}\\sum_{j=1}^{d}\\hat{\\mu}_{j}(\\langle\\mathbf{W}_{i,:},\\nabla_{y},\\hat{\\mu}(\\mathbf{y})\\rangle-W_{i,j})}\\\\ &{\\quad=\\mathbb{E}\\|\\hat{\\mu}(\\mathbf{y})-\\mathbf{y}\\|_{\\mathbf{W}}^{2}+\\mathrm{Tr}(\\mathbf{W}\\mathbf{Z})+2\\mathbb{E}\\displaystyle\\sum_{i=1}^{d}\\sum_{j=1}^{d}(\\sum_{i=1}^{d}(\\mathbf{W}\\nabla_{y}\\hat{\\mu}(\\mathbf{y})-\\mathbf{W}))_{i,j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "C SureMap estimator ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section we describe the prior in full for any number of attributes, prove expressivity results reference in $\\S3.1.2$ , and describe how to compute the estimator using coordinate descent. ", "page_idx": 14}, {"type": "text", "text": "C.1 A linear prior ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Suppose each group $g\\ \\in\\ [d]$ corresponds to an intersection $\\textbf{g}\\in\\times_{a=1}^{k}[d_{a}]$ of $k$ attributes, with attribute $a$ having $d_{a}$ possible classes.5 For example, the first attribute could be one of $d_{1}$ different age brackets and the second could be one of $d_{2}$ different racial categories. For each subset $A\\subset[k]$ of attributes define a random tensor $\\mathbf{Z}_{A}\\;\\in\\;\\mathbb{R}^{\\times_{a}\\in A^{d_{a}}}$ with i.i.d. entries $Z_{A;\\mathbf{c}}\\,\\sim\\,{\\mathcal{N}}(0,1)$ , where $\\mathbf{c}\\in\\times_{a\\in A}[d_{a}]$ is a specific class combination of the attribute $A$ . Then the additive intersectional effects prior on the mean $\\mu_{g}$ of group $g$ is the weighted sum ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mu_{g}=\\theta_{g}+\\sum_{A\\in2^{[k]}}\\tau_{A}Z_{A;\\mathbf{g}_{A}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with coefficients $\\tau_{A}\\in\\mathbb{R}$ corresponding to each $A\\in2^{[k]}$ . These hyperparameters thus determine the strength of the effect of attribute intersection $A$ on the group means, with $\\tau_{A}Z_{A;\\mathbf{c}}$ being added to the means of all groups $g$ s.t. $\\mathbf{g}_{A}=\\mathbf{c}$ , i.e. whose attribute intersection results in the specific class combination c. ", "page_idx": 14}, {"type": "text", "text": "Letting $\\pmb{\\tau}=(\\tau_{\\varnothing},\\dots,\\tau_{[k]})\\in\\mathbb{R}_{\\ge0}^{2^{k}}$ be the vector of hyperparameters $\\tau_{A}$ and assuming $\\tau_{[k]}>0$ , this prior is equivalent to a Gaussian prior $\\pmb{\\mu}\\sim\\mathcal{N}(\\pmb{\\theta},\\pmb{\\Lambda}(\\pmb{\\tau}))$ with covariance ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{A}(\\tau)=\\sum_{A\\in2^{[k]}}\\tau_{A}^{2}\\mathbf{U}_{A}\\mathbf{U}_{A}^{\\top}=\\sum_{A\\in2^{[k]}}\\tau_{A}^{2}\\mathbf{C}_{A}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for matrices $\\mathbf{U}_{A}\\in\\{0,1\\}^{d\\times\\prod_{a\\in A}d_{a}}$ with orthogonal column vectors, each one corresponding to a different attribute intersection in $\\times_{a\\in A}[d_{a}]$ and its $g$ th entry indicating whether group $g$ is a subset of that intersection. Their outer product matrices $\\mathbf{C}_{A}=\\mathbf{U}_{A}\\mathbf{U}_{A}^{\\top}$ have entries $C_{A;g,h}$ that are one if $\\mathbf{g}_{A}=\\mathbf{h}_{A}$ and zero otherwise, i.e. they indicate if groups $g$ and $h$ have the same type (e.g. (senior, female)) of attribute intersection $A$ (e.g. (age, sex)). In the simplest case, if we are disaggregating across just one attribute, i.e. $k=1$ and $d_{1}=d$ , then $\\boldsymbol{\\mathbf{A}}(\\tau)=\\tau_{\\emptyset}^{\\hat{2}}\\mathbf{1}_{d\\times d}+\\tau_{[k]}^{2}\\mathbf{I}_{d}$ . Since $k\\leq\\lfloor\\log_{2}d\\rfloor$ , using this prior to define the covariance matrix reduces the number of hyperparameters to $d+2^{k}\\leq2d$ for $\\bar{\\hat{\\mu}}_{\\theta,\\Lambda(\\tau)}^{\\mathrm{MAP}}$ , compared to $O(d^{2})$ for a general covariance $\\pmb{\\Lambda}$ . ", "page_idx": 14}, {"type": "text", "text": "C.2 Expressivity of the linear prior ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem C.1. Consider a disaggregated setting with $k$ attributes with $d_{1},\\ldots,d_{k}$ possible categories, respectively, and total number of groups $\\begin{array}{r}{d=\\prod_{a=1}^{k}d_{a}}\\end{array}$ . If we use a diagonal covariance $\\Sigma\\in\\mathbb{R}^{d\\times d}$ satisfying $\\Sigma_{h,h}=\\sigma^{2}/n_{h}\\,\\forall\\,h$ then the following holds $\\forall\\,\\mathbf{y}\\in\\mathbb{R}^{d}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\tau_{\\varnothing}\\to\\infty}\\hat{\\pmb{\\mu}}_{\\mathbf{0}_{d},\\Lambda(\\pmb{\\tau})}^{\\mathrm{MAP}}(\\mathbf{y})=\\hat{\\pmb{\\mu}}^{\\mathrm{pooled}}(\\mathbf{y})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$\\operatorname*{lim}_{\\tau_{\\boldsymbol{\\theta}}\\to\\infty}\\hat{\\pmb{\\mu}}_{\\pmb{\\theta},\\mathbf{A}(\\tau)}^{\\mathrm{MAP}}(\\mathbf{y})=\\hat{\\pmb{\\mu}}^{\\mathrm{pooled}}(\\mathbf{y}-\\pmb{\\theta})+\\pmb{\\theta}$ ", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "as desired. ", "page_idx": 14}, {"type": "text", "text": "Proof. The first and third results can be easily shown using the fact that $\\Sigma$ is diagonal: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\tau_{[k]}\\to\\infty}\\hat{\\mu}_{\\mathbf{0}_{d},\\mathbf{A}(\\tau)}^{\\mathrm{MAP}}(\\mathbf{y})=\\operatorname*{lim}_{\\tau_{[k]}\\to\\infty}\\left(\\frac{\\mathbf{I}_{d}}{\\tau_{[k]}^{2}}+\\mathbf{\\Sigma}^{-1}\\right)^{-1}\\left(\\frac{\\pmb\\theta}{\\tau_{[k]}^{2}}+\\mathbf{\\Sigma}^{-1}\\mathbf{y}\\right)=\\mathbf{y}=\\hat{\\mu}^{\\mathrm{naive}}(\\mathbf{y})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\tau_{[k]}\\to0}\\hat{\\mu}_{\\pmb{\\theta},\\mathbf{A}(\\tau)}^{\\mathrm{MAP}}(\\mathbf{y})=\\operatorname*{lim}_{\\tau_{[k]}\\to0}\\left(\\frac{\\mathbf{I}_{d}}{\\tau_{[k]}^{2}}+\\mathbf{\\Sigma}^{-1}\\right)^{-1}\\left(\\frac{\\pmb{\\theta}}{\\tau_{[k]}^{2}}+\\mathbf{\\Sigma}^{-1}\\mathbf{y}\\right)=\\pmb{\\theta}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The second result follows from the last by substituting $\\pmb{\\theta}=\\mathbf{0}_{d}$ , so we just need to prove the latter one. First note that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mu}_{\\boldsymbol{\\theta},\\mathbf{A}(\\tau)}^{\\mathrm{MAP}}(\\mathbf{y})=\\left(\\mathbf{A}^{-1}(\\tau)+\\mathbf{\\Sigma}^{\\mathbf{\\Sigma}-1}\\right)^{-1}\\left(\\mathbf{\\Lambda}^{-1}(\\tau)\\boldsymbol{\\theta}+\\mathbf{\\Sigma}^{-1}\\mathbf{y}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\boldsymbol{\\theta}+\\left(\\mathbf{\\Lambda}\\Lambda^{-1}(\\tau)+\\mathbf{\\Sigma}^{-1}\\right)^{-1}\\Sigma^{-1}(\\mathbf{y}-\\boldsymbol{\\theta})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "so we just need to show that the second term approaches $\\hat{\\mu}^{\\mathrm{pooled}}(\\mathbf{y}\\!-\\!\\theta)$ . We use the Sherman-Morrison formula to compute ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Lambda^{-1}(\\tau)=(\\tau_{[k]}^{2}\\mathbf{I}_{d}+\\tau_{\\emptyset}^{2}\\mathbf{1}_{d}\\mathbf{1}_{d}^{\\top})^{-1}=\\frac{\\mathbf{I}_{d}}{\\tau_{[k]}^{2}}-\\frac{\\tau_{\\emptyset}^{2}\\mathbf{1}_{d}\\mathbf{1}_{d}^{\\top}}{\\tau_{[k]}^{4}+\\tau_{[k]}^{2}\\tau_{\\emptyset}^{2}d}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and apply it again to compute ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathbf{A}^{-1}(\\pmb{\\tau})+\\mathbf{\\Sigma}^{-1})^{-1}=\\left(\\frac{\\mathbf{I}_{d}}{\\tau_{[k]}^{2}}-\\frac{\\tau_{\\theta}^{2}\\mathbf{1}_{d}\\mathbf{1}_{d}^{\\top}}{\\tau_{[k]}^{2}+\\tau_{\\theta}^{2}d}+\\mathbf{\\Sigma}^{-1}\\right)^{-1}}\\\\ &{\\qquad\\qquad\\qquad=\\left(\\frac{\\mathbf{I}_{d}}{\\tau_{[k]}^{2}}+\\mathbf{\\Sigma}^{-1}\\right)^{-1}+\\frac{\\tau_{\\theta}^{2}\\left(\\frac{\\mathbf{I}_{d}}{\\tau_{[k]}^{2}}+\\mathbf{\\Sigma}\\mathbf{\\Sigma}^{-1}\\right)^{-1}\\mathbf{1}_{d}\\mathbf{1}_{d}^{\\top}\\left(\\frac{\\mathbf{I}_{d}}{\\tau_{[k]}^{2}}+\\mathbf{\\Sigma}^{-1}\\right)^{-1}}{\\tau_{[k]}^{4}+\\tau_{[k]}^{2}\\tau_{\\theta}^{2}d-\\tau_{\\theta}^{2}\\left.\\mathrm{Tr}\\left(\\left(\\frac{\\mathbf{I}_{d}}{\\tau_{[k]}^{2}}+\\mathbf{\\Sigma}^{-1}\\right)^{-1}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Defining $\\pmb{\\delta}=\\mathbf{y}-\\pmb{\\theta}$ , we have by L\u2019H\u00f4pital\u2019s rule that $\\forall\\,g\\in[d]$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{y_{(k)}\\sim0}{\\operatorname*{lims}}\\left(\\left(\\boldsymbol{\\Lambda}^{-1}\\right)\\left(\\boldsymbol{\\tau}+\\boldsymbol{\\Sigma}^{-1}\\right)\\cdots\\boldsymbol{\\Lambda}^{-1}\\right)\\beta_{y}}\\\\ &{=}\\\\ &{=\\frac{1}{r_{k+1}^{\\alpha+1}}\\frac{\\delta_{y}\\int_{\\mathbb{X}_{k}}^{\\mathbb{X}_{k}}{\\boldsymbol{\\mu}_{\\mathtt{p}}}}{r_{k+1}^{\\alpha+1}}+\\frac{1}{r_{k+1}^{\\alpha}}\\frac{\\delta_{y}\\int_{\\mathbb{X}_{k}}^{\\mathbb{T}_{k}}{\\mathrm{d}\\boldsymbol{\\tau}}}{r_{k+1}^{\\alpha+1}}}\\\\ &{\\overset{,,,}{\\quad\\quad\\sim}\\frac{1}{r_{k+1}^{\\alpha+1}}\\frac{\\delta_{y}\\int_{\\mathbb{X}_{k}}^{\\mathbb{T}_{k}}{\\boldsymbol{\\mu}_{\\mathtt{p}}}}{r_{k+1}^{\\alpha+1}}-\\frac{1}{r_{k+1}^{\\alpha}}\\frac{\\delta_{y}\\int_{\\mathbb{X}_{k}}^{\\mathbb{T}_{k}}{\\mathrm{d}\\boldsymbol{\\tau}}}{r_{k+1}^{\\alpha+1}-\\sum_{s=1}^{\\infty}\\delta_{y}}}\\\\ &{=\\underset{y_{(k)}\\sim\\alpha_{k}}{\\operatorname*{lims}}\\frac{\\delta_{y}\\int_{\\mathbb{X}_{k}}^{\\mathbb{T}_{k}}{\\boldsymbol{\\mu}_{\\mathtt{p}}}}{1+\\sum_{s=1}^{\\infty}\\delta_{y}}\\left(\\frac{\\delta_{y}}{\\delta_{x}+1}\\frac{\\delta_{y}\\int_{\\mathbb{X}_{k}}^{\\mathbb{T}_{k}}{\\boldsymbol{\\mu}_{\\mathtt{p}}}}{r_{k+1}^{\\alpha+1}}-\\frac{\\gamma_{k+1}^{\\alpha}\\gamma_{k+1}^{2}\\delta_{y}}{r_{k+1}^{\\alpha+1}}-\\frac{\\gamma_{k+1}^{\\alpha}\\gamma_{k,\\alpha}}{\\left(1+\\frac{r_{k+1}}{r_{k+1}^{\\alpha}}\\right)^{2}}\\frac{\\delta_{y}\\int_{\\mathbb{X}_{k}}^{\\mathbb{T}_{k}}{\\mathbb{T}_{k}}{\\mathbb{d}_{y}}}{\\left(1+\\frac \n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "as desired. ", "page_idx": 15}, {"type": "image", "img_path": "aTNT3FuVBG/tmp/306e3dde17ae4c7df9a1488ac50929e8f8780747afb34533cde742e6977d40d7.jpg", "img_caption": ["Figure 6: Performance as the number of tasks varies, evaluated on SLACS (left) and CVC (right). This plot is similar to Figure 5 but demonstrates the superiority of the multi-task version of SureMap that we choose (MetaMap) relative to alternatives. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.3 How to set the multi-task center ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In the single-task case we set the prior mean $\\pmb{\\theta}=\\mathbf{0}_{d}$ ; this can also be done in the multi-task setting. Alternatively, we can use multi-task data to construct estimator $\\hat{\\pmb\\theta}$ of some true underlying multi-task mean $\\pmb{\\theta}$ and substitute the former for $\\pmb{\\theta}$ ; for simplicity we will restrict to the ourselves to linear edsettiemrmationres $\\begin{array}{r}{\\hat{\\pmb\\theta}=\\sum_{t=1}^{T}\\mathbf M_{t}\\mathbf y_{t}}\\end{array}$ ,e  wwhilelr ea stshieg nm tahterimc ess $\\mathbf{M}_{1},\\ldots,\\mathbf{M}_{T}$ ca rfeo rinmd aenpde nsdeet ntth oosf ${\\bf y}_{1},\\ldots,{\\bf y}_{T}$ .s  Tboy minimizing the sum of the $\\Sigma_{t}^{-1}$ -weighted risks across tasks, as estimated by SURE: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{t}\\Vert\\hat{\\mu}_{\\hat{\\theta},\\Lambda}^{\\mathrm{MAP}}(\\mathbf{y}_{t})-\\mu_{t}\\Vert_{\\mathbf{S}_{t}^{-1}}^{2}=\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left(\\Vert\\hat{\\mu}_{\\hat{\\theta},\\Lambda}^{\\mathrm{MAP}}(\\mathbf{y}_{t})-\\mathbf{y}_{t}\\Vert_{\\mathbf{S}_{t}^{-1}}^{2}-d+2\\nabla_{\\mathbf{y}_{t}}\\cdot\\hat{\\mu}_{\\hat{\\theta},\\Lambda}^{\\mathrm{MAP}}(\\mathbf{y}_{t})\\right)}\\\\ &{\\displaystyle=d T+\\sum_{t=1}^{T}\\mathbb{E}_{t}\\Vert\\mathbf{A}_{t}(\\hat{\\theta}-\\mathbf{y}_{t})\\Vert_{\\mathbf{S}_{t}^{-1}}^{2}+2\\,\\mathrm{Tr}(\\mathbf{A}_{t}\\mathbf{M}_{t}-\\mathbf{A}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C.3.1 SureSolve ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This approach sets the prior mean by finding the $\\pmb{\\theta}$ that minimizes the above SURE objective, which is equivalent to solving an overconstrained linear system: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{J}}=d T+\\underset{\\theta\\in\\mathbb{R}^{d}}{\\operatorname{arg\\,min}}\\sum_{t=1}^{T}\\|\\mathbf{A}_{t}(\\theta-\\mathbf{y}_{t})\\|_{\\mathbf{X}_{t}^{-1}}^{2}-2\\operatorname{Tr}(\\mathbf{A}_{t})=\\left(\\sum_{t=1}^{T}\\mathbf{A}_{t}^{\\top}\\Sigma_{t}^{-1}\\mathbf{A}_{t}\\right)^{-1}\\sum_{t=1}^{T}\\mathbf{A}_{t}^{\\top}\\Sigma_{t}^{-1}\\mathbf{A}_{t}\\mathbf{y}_{t}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that this estimator can be expressed in the desired linear form $\\begin{array}{r}{\\widehat{\\pmb{\\theta}}\\,=\\,\\sum_{t=1}^{T}\\mathbf{M}_{t}\\mathbf{y}_{t}}\\end{array}$ , with the matrices $\\begin{array}{r}{\\mathbf{M}_{t}=\\left(\\sum_{s=1}^{T}\\mathbf{A}_{s}^{\\top}\\Sigma_{s}^{-1}\\mathbf{A}_{s}\\right)^{-1}\\mathbf{A}_{t}^{\\top}\\Sigma_{t}^{-1}\\mathbf{A}_{t}}\\end{array}$ depending on the parameters determining $\\pmb{\\Lambda}$ . ", "page_idx": 16}, {"type": "text", "text": "C.3.2 MetaMap ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Alternatively, if we assume the prior is correct for some $\\pmb{\\theta}$ and $\\Lambda$ , i.e. $\\mathbf{y}_{t}\\sim\\mathcal{N}(\\pmb{\\theta},\\pmb{\\Lambda}+\\pmb{\\Sigma}_{t})$ , then a natural estimator to use for $\\pmb{\\theta}$ given a fixed $\\pmb{\\Lambda}$ is a MAP estimator of $\\hat{\\theta}_{\\Gamma}$ with prior $\\mathcal{N}(\\mathbf{0}_{d},\\mathbf{T})$ for some covariance $\\mathbf{T}$ . Define $\\begin{array}{r}{\\Sigma_{\\Lambda}=(\\sum_{t=1}^{T}({\\bf{A}}+{\\Sigma}_{t})^{-1})^{-1}}\\end{array}$ and note that $\\begin{array}{r}{\\mathbf{y}_{\\Lambda}=\\pmb{\\Sigma}_{\\Lambda}\\sum_{t=1}^{T}(\\mathbf{\\Lambda}+\\pmb{\\Sigma}_{t})^{-1}\\mathbf{y}_{t}}\\end{array}$ is distributed as $\\mathcal{N}(\\pmb{\\theta},\\pmb{\\Sigma}_{\\pmb{\\Lambda}})$ . Then the MAP estimator of $\\pmb{\\theta}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{\\mathbf{f}}=\\left(\\mathbf{I}^{-1}+\\pmb{\\Sigma}_{\\mathbf{A}}^{-1}\\right)^{-1}\\pmb{\\Sigma}_{\\mathbf{A}}^{-1}\\mathbf{y}_{\\mathbf{A}}=\\sum_{t=1}^{T}\\left(\\mathbf{I}^{-1}+\\pmb{\\Sigma}_{\\mathbf{A}}^{-1}\\right)^{-1}(\\mathbf{A}+\\pmb{\\Sigma}_{t})^{-1}\\mathbf{y}_{t}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "has the necessary form $\\begin{array}{r}{\\hat{\\pmb\\theta}=\\sum_{t=1}^{T}\\mathbf M_{t}\\mathbf y_{t}}\\end{array}$ for $\\mathbf{M}_{t}=(\\Gamma^{-1}+\\Sigma_{\\Lambda}^{-1})^{-1}(\\Lambda+\\Sigma_{t})^{-1}$ . In this case the matrices depend on both $\\Lambda$ a nd $\\mathbf{T}$ , i.e. the covariance matrices of the prior and the meta-prior. ", "page_idx": 16}, {"type": "text", "text": "Input: target $f:\\mathcal{Z}\\to\\mathbb{R}$ , samples $S_{t}\\subset\\mathcal{Z}$ for each task $t=1,\\dots,T$ , partition $\\{\\mathcal{Z}_{g}\\}_{g=1}^{d}$ of $\\mathcal{Z}$ , each group $g$ an intersection of $k\\in\\mathbb{Z}_{>0}$ attributes ", "page_idx": 17}, {"type": "text", "text": "// compute naive group means   \nfor task $t\\in[T]$ do for group $\\bar{g}\\in[d]$ do $n_{t;g}\\gets|S_{t}\\cap\\mathcal{Z}_{g}|$ yt;g \u2190nt1; g z\u2208St\u2229Zg f(z)   \n// estimate group variances   \n$\\begin{array}{r l}&{n=\\sum_{t=1}^{T}|S_{t}|}\\\\ &{\\sigma^{2}\\gets\\frac{1}{n-d T}\\sum_{t=1}^{T}\\sum_{g=1}^{d}\\sum_{z\\in S_{t}\\cap Z_{g}}(f(z)-y_{t;g})^{2}}\\\\ &{\\mathbf{for}\\;t a s k\\;t\\in[T]\\;\\mathbf{do}}\\\\ &{\\;\\;\\;\\ L\\sum_{t}^{-1}\\gets\\mathrm{diag}(\\mathbf{n}_{t})/\\sigma^{2}}\\end{array}$   \n// compute auxiliary matrices (avoids inverting $\\pmb{\\Lambda}$ )   \nMethod $\\mathbf{A}_{t}\\left(\\tau\\right)$ : // compute prior covariance (matrices $\\mathbf{C}_{A}$ are defined in Appendix C.1) $\\textstyle\\mathbf{A}\\leftarrow\\sum_{A\\in2^{[k]}}\\tau_{A}^{2}\\mathbf{C}_{A}$ Output: $(\\bar{\\mathbf I}_{d}+\\mathbf{A}\\pmb{\\Sigma}_{t}^{-1})^{-1}$   \n// compute auxiliary matrices (avoids inverting $\\pmb{\\Sigma}_{t}^{-1}$ and $\\mathbf{T}$ )   \nMethod ${\\bf M}_{t}(\\tau,v)$ : // compute prior and meta-prior covariances $\\textstyle\\mathbf{A}\\leftarrow\\sum_{A\\in2^{[k]}}\\tau_{A}^{2}\\mathbf{C}_{A}$ $\\begin{array}{r}{\\mathbf{\\sigma}\\mathbf{\\sigma}\\mathbf{\\sigma}\\mathbf{\\sigma}\\mathbf{\\sigma}\\mathbf{\\sigma}\\mathbf{\\sigma}\\mathbf{\\sigma}\\mathbf{\\sigma}\\mathbf{}\\mathbf{\\sigma}\\mathbf{}\\mathbf{\\sigma}\\mathbf{}\\mathbf{\\sigma}\\mathbf{}\\mathbf{\\sigma}\\mathbf{}\\mathbf{}\\!}\\end{array}$ Output: $\\begin{array}{r l}&{\\left(\\mathbf{I}_{d}+\\mathbf{I}\\sum_{s=1}^{T}\\sum_{s}^{-1}(\\mathbf{A}\\mathbf{\\sum}_{s}^{-1}+\\mathbf{I}_{d})^{-1}\\right)^{-1}\\mathbf{\\Gamma}\\mathbf{\\sum}_{t}^{-1}(\\mathbf{A}\\mathbf{\\sum}_{t}^{-1}+\\mathbf{I}_{d})^{-1}}\\end{array}$   \n// estimates prior mean using MAP   \nMethod $\\hat{\\theta}(\\tau,v)$ :   \nOutput:  tT=1 Mt(\u03c4, \u03c5)yt   \n// optimize the sum of SUREs across tasks using L-BFGS-B   \n$\\begin{array}{r}{\\hat{\\tau},\\hat{v}=\\underset{\\tau,v\\in\\mathbb{R}_{\\geq0}^{2k}}{\\arg\\operatorname*{min}}\\sum_{t=1}^{T}\\|\\mathbf{A}_{t}(\\tau)(\\hat{\\theta}(\\tau,v)-\\mathbf{y}_{t})\\|_{\\mathbf{x}_{t}^{-1}}^{2}+2\\operatorname{Tr}\\big(\\mathbf{A}_{t}(\\tau)(\\mathbf{M}_{t}(\\tau,v)-\\mathbf{I}_{d})\\big)}\\end{array}$   \n$//$ return an estimate of the group means of each task $t$ using MAP   \nfor task $t\\in[T]$ do   \nOutput: $\\dot{\\mathbf{y}_{t}}+\\mathbf{A}_{t}(\\hat{\\pmb{\\tau}})(\\hat{\\pmb{\\theta}}(\\hat{\\tau},\\hat{\\mathbf{v}})-\\mathbf{y}_{t})$ ", "page_idx": 17}, {"type": "text", "text": "D Computation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here we note additional computational details. ", "page_idx": 17}, {"type": "text", "text": "D.1 Nonnegativity of SureMap ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In both the single and multi-task cases there is no guarantee that $\\pmb{\\theta}$ is in the convex hull of the values in $\\mathbf{y}$ or $\\{\\mathbf{y}_{t}\\}_{t=1}^{T}$ , and in fact it can be negative. Since the quantities we are estimating are usually nonnegative, in practice we do a post-hoc correction forcing $\\pmb{\\theta}$ to be nonnegative. ", "page_idx": 17}, {"type": "text", "text": "D.2 Optimization of SureMap ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Both the single-task and multi-task variants of SureMap require solving optimization problems, the former over $\\tau^{2}\\,=\\,(\\tau_{A}^{2})_{A\\subseteq[k]}$ (Eq. 12) and the latter also over $v^{2}=\\,\\rceil v_{A}^{2})_{A\\subseteq[k]}$ (Eq. 14), both of which are vectors in $\\mathbb{R}_{\\geq0}^{2^{k}}$ . We find that both problems can be quickly and efficiently optimized using L-BFGS-B over the entire domain $\\mathbb{R}_{\\geq0}^{2^{k}}$ and R2\u2265\u00b702k , respectively, using the default settings provided in its SciPy implementation.6 To initialize the algorithm we set all entries of $\\tau^{2}$ and $v^{2}$ to zero except those corresponding to the entire set $[k]$ , which we set to one.7In the remainder of this section, we describe how to compute gradients (passed to $\\mathrm{L}$ -BFGS-B) of the multi-task SureMap objective ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\nd T+\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left\\|\\mathbf{A}_{t}\\left(\\mathbf{y}_{t}-\\sum_{s=1}^{T}\\mathbf{M}_{s}\\mathbf{y}_{s}\\right)\\right\\|_{\\mathbf{\\pmb{\\Sigma}}_{t}^{-1}}^{2}+2\\operatorname{Tr}(\\mathbf{A}_{t}\\mathbf{M}_{t}-\\mathbf{A}_{t})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the matrices ${{\\bf{M}}_{t}}$ differ based on whether we are using the SureSolve or MetaMap variant. The gradients are taken w.r.t. the tuning parameters, which are the coefficients $\\tau_{1}^{2},\\dots,\\tau_{2^{k}}^{2}$ used to define the prior covariance $\\begin{array}{r}{\\mathbf{A}=\\sum_{i=1}^{2^{k}}\\tau_{i}^{2}\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}}\\end{array}$ and, in the second case, the coefficients $v_{1}^{2},\\ldots,v_{2^{k}}^{2}$ used to define the meta-prior covariance $\\begin{array}{r}{{\\bf{T}}=\\sum_{i=1}^{2^{k}}{v_{i}^{2}{\\bf{U}}_{i}{\\bf{U}}_{i}^{\\top}}}\\end{array}$ .8 Noting that $\\mathbf{A}_{t}=(\\mathbf{A}+\\pmb{\\Sigma}_{t}^{-1})^{-1}\\mathbf{A}^{-1}=(\\mathbf{I}_{d}+$ $\\Lambda\\Sigma_{t}^{-1})^{-1}$ and $(\\mathbf{A}+\\Sigma_{t})^{-1}=\\mathbf{A}^{-1}(\\mathbf{I}_{d}+\\Sigma_{t}\\mathbf{A}^{-1})^{-1}=\\Sigma_{t}^{-1}(\\mathbf{I}_{d}+\\Lambda\\Sigma_{t}^{-1})^{-1}=\\Sigma_{t}^{-1}\\mathbf{A}_{t}$ , we have the derivative $\\partial_{i}\\mathbf{A}_{t}=-\\mathbf{A}_{t}\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}\\{\\mathbf{\\Sigma}_{t}^{-1}\\mathbf{A}_{t}$ w.r.t. $\\tau_{i}^{2}$ , which yields $\\partial_{i}\\operatorname{Tr}(\\mathbf{A}_{t})=\\operatorname{Tr}(-\\mathbf{A}_{t}\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}\\Sigma_{t}^{-1}\\mathbf{A}_{t})$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{i}\\|\\mathbf{A}_{t}(\\pmb{\\theta}-\\mathbf{y}_{t})\\|_{\\mathbf{\\Sigma}_{t}^{-1}}^{2}=2(\\pmb{\\theta}-\\mathbf{y}_{t})^{\\top}\\mathbf{A}_{t}^{\\top}\\Sigma_{t}^{-1}\\partial_{i}\\mathbf{A}_{t}(\\pmb{\\theta}-\\mathbf{y}_{t})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=-2(\\pmb{\\theta}-\\mathbf{y}_{t})^{\\top}\\mathbf{A}_{t}^{\\top}\\Sigma_{t}^{-1}\\mathbf{A}_{t}\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}\\Sigma_{t}^{-1}\\mathbf{A}_{t}(\\pmb{\\theta}-\\mathbf{y}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "D.2.1 SureSolve ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "First note that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{i}[\\mathbf{A}_{t}^{\\top}\\Sigma_{t}^{-1}\\mathbf{A}_{t}]=\\partial_{i}[\\Sigma_{t}^{-1}\\mathbf{A}_{t}^{2}]=\\Sigma_{t}^{-1}(\\mathbf{A}_{t}\\partial_{i}\\mathbf{A}_{t}+\\partial_{i}\\mathbf{A}_{t}\\mathbf{A}_{t})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=-\\Sigma_{t}^{-1}(\\mathbf{A}_{t}^{2}+\\mathbf{A}_{t})\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}\\Sigma_{t}^{-1}(\\mathbf{A}_{t}^{2}+\\mathbf{A}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the first step follows by $\\Sigma_{t}^{-1}\\mathbf{A}_{t}=\\Sigma_{t}^{-1}(\\mathbf{I}_{d}+\\Lambda\\Sigma_{t}^{-1})^{-1}=(\\mathbf{I}_{d}+\\Sigma_{t}^{-1}\\Lambda)^{-1}\\Sigma_{t}^{-1}=\\mathbf{A}_{t}^{\\top}\\Sigma_{t}^{-1},$ . Then for $\\begin{array}{r}{\\mathbf{M}_{t}=\\left(\\sum_{s=1}^{T}\\mathbf{A}_{s}^{\\top}\\Sigma_{s}^{-1}\\mathbf{A}_{s}\\right)^{-1}\\mathbf{A}_{t}^{\\top}\\Sigma_{t}^{-1}\\mathbf{A}_{t}}\\end{array}$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{i}\\mathbf{M}_{t}=\\left(\\displaystyle\\sum_{s=1}^{T}\\mathbf{A}_{s}^{\\top}\\boldsymbol{\\Sigma}_{s}^{-1}\\mathbf{A}_{s}\\right)^{-1}\\partial_{i}[\\mathbf{A}_{t}^{\\top}\\boldsymbol{\\Sigma}_{t}^{-1}\\mathbf{A}_{t}]-\\left(\\displaystyle\\sum_{s=1}^{T}\\mathbf{A}_{s}^{\\top}\\boldsymbol{\\Sigma}_{s}^{-1}\\mathbf{A}_{s}\\right)^{-1}\\displaystyle\\sum_{s=1}^{T}\\partial_{i}[\\mathbf{A}_{s}^{\\top}\\boldsymbol{\\Sigma}_{s}^{-1}\\mathbf{A}_{s}]\\mathbf{M}_{t}}\\\\ &{\\phantom{\\sum{\\sum_{s=1}^{T}}}=-2\\left(\\displaystyle\\sum_{s=1}^{T}\\mathbf{A}_{s}^{\\top}\\boldsymbol{\\Sigma}_{s}^{-1}\\mathbf{A}_{s}\\right)^{-1}\\mathbf{\\Sigma}_{t}^{-1}(\\mathbf{A}_{t}^{2}+\\mathbf{A}_{t})\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}\\boldsymbol{\\Sigma}_{t}^{-1}(\\mathbf{A}_{t}^{2}+\\mathbf{A}_{t})}\\\\ &{\\phantom{\\sum{\\sum_{s=1}^{T}}}+2\\left(\\displaystyle\\sum_{s=1}^{T}\\mathbf{A}_{s}^{\\top}\\boldsymbol{\\Sigma}_{s}^{-1}\\mathbf{A}_{s}\\right)^{-1}\\displaystyle\\sum_{s=1}^{T}\\Sigma_{s}^{-1}(\\mathbf{A}_{s}^{2}+\\mathbf{A}_{s})\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}\\boldsymbol{\\Sigma}_{s}^{-1}(\\mathbf{A}_{s}^{2}+\\mathbf{A}_{s})\\mathbf{M}_{t}}\\\\ &{\\phantom{\\sum{\\sum_{s=1}^{T}}}=-2\\mathbf{B}_{i,t}+2\\displaystyle\\sum_{s=1}^{T}\\mathbf{B}_{i,s}\\mathbf{M}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbf{B}_{i,t}=\\left(\\sum_{s=1}^{T}\\mathbf{A}_{s}^{\\top}\\Sigma_{s}^{-1}\\mathbf{A}_{s}\\right)^{-1}\\Sigma_{t}^{-1}(\\mathbf{A}_{t}^{2}+\\mathbf{A}_{t})\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}\\Sigma_{t}^{-1}(\\mathbf{A}_{t}^{2}+\\mathbf{A}_{t}).}\\end{array}$ . We can then compute ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{i}\\operatorname{Tr}(\\mathbf{A}_{t}\\mathbf{M}_{t})=-2\\operatorname{Tr}\\left(\\mathbf{A}_{t}\\left(\\mathbf{B}_{i,t}-\\displaystyle\\sum_{s=1}^{T}\\mathbf{B}_{i,s}\\mathbf{M}_{t}\\right)\\right)-\\operatorname{Tr}(\\mathbf{A}_{t}\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}\\boldsymbol\\Sigma_{t}^{-1}\\mathbf{A}_{t}\\mathbf{M}_{t})}\\\\ &{\\phantom{\\partial_{i}\\operatorname{Tr}\\left(\\mathbf{A}_{t}\\left(\\mathbf{B}_{i,t}+\\left(\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}\\boldsymbol\\Sigma_{t}^{-1}\\mathbf{A}_{t}-\\displaystyle\\sum_{s=1}^{T}\\mathbf{B}_{i,s}\\right)\\mathbf{M}_{t}\\right)\\right)}=-2\\operatorname{Tr}\\left(\\mathbf{A}_{t}\\left(\\mathbf{B}_{i,t}+\\left(\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}\\boldsymbol\\Sigma_{t}^{-1}\\mathbf{A}_{t}-\\displaystyle\\sum_{s=1}^{T}\\mathbf{B}_{i,s}\\right)\\mathbf{M}_{t}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "6https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html   \n7This initialization corresponds to setting the prior covariance to be the $d\\times d$ identity.   \n8We use indices $i$ instead of subsets $A$ here for simplicity. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\partial_{t}\\bigg|\\mathbb{A}_{i}\\left(\\mathbb{R}_{i}-\\frac{T}{\\sum_{i=1}^{N}\\mathbf{A}_{i,N}}\\right)\\bigg|\\sum_{\\underline{{x}}_{1}^{i+1}}^{2}=\\hat{\\alpha}_{i}\\sum_{i=1}^{T}\\sum_{\\underline{{x}}_{1}^{i}}^{i}\\!\\!\\!(\\nu_{i}-\\ M_{S,1})^{\\top}\\mathbb{A}_{i}^{\\top}\\Sigma_{i}^{-1}\\mathbb{A}_{i}(y_{i}-M_{S,1})}\\\\ &{\\qquad=-\\sum_{i=1}^{T}(\\partial_{1}\\mathbf{M}_{S,1})^{\\top}\\mathbf{A}_{i}^{\\top}\\Sigma_{i}^{-1}\\mathbb{A}_{i}(y_{i}-\\hat{\\theta})}\\\\ &{\\qquad\\quad+2(\\nu_{i}-\\hat{\\theta})^{\\top}\\mathbb{A}_{i}^{\\top}\\Sigma_{i}^{-1}\\mathbb{A}_{i}(y_{i}-\\hat{\\theta})}\\\\ &{=4\\sum_{i=1}^{T}\\mathbb{S}_{i}^{\\top}\\mathbf{B}_{i}^{\\top}\\Lambda_{i}^{\\top}\\Sigma_{i}^{-1}\\mathbb{A}_{i}(y_{i}-\\hat{\\theta})}\\\\ &{\\qquad\\quad-\\hat{\\theta}\\overset{(a)}{\\underset{i=1}{\\overset{N}{\\sum}}}\\sum_{i=1}^{N}\\!\\!\\!\\sum_{i=1}^{N}\\!\\!\\!\\sum_{k=1}^{i}\\!\\!(\\nu_{i}-\\hat{\\theta})}\\\\ &{\\qquad-\\hat{\\theta}(y_{i}-\\hat{\\theta})^{\\top}\\Lambda_{i}^{\\top}\\Lambda_{i}^{\\top}\\Sigma_{i}^{-1}\\mathbb{A}_{i}(y_{i}-\\hat{\\theta})}\\\\ &{=4\\sum_{i=1}^{T}\\!\\sum_{k=1}^{N}\\!\\!\\!\\sum_{k=1}^{N}\\!\\!\\!\\sum_{i=1}^{N}\\!\\!\\!\\sum_{k=1}^{N}\\!\\!(y_{i}-\\hat{\\theta})}\\\\ &{\\qquad=-2(\\nu_{i}-\\hat{\\theta})^{\\top}\\Lambda_{i}^{\\top}\\Sigma_{i}^{-1}\\mathbb{A}_{i}\\mathbf{U}\\nabla\\overline{{\\Sigma}}_{i}^{-1}\\mathbb{A}_{i}(y_{i}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "D.3 MetaMap ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Note that $(\\boldsymbol{\\Lambda}+\\boldsymbol{\\Sigma}_{t})^{-1}=\\boldsymbol{\\Sigma}_{t}^{-1}(\\boldsymbol{\\Lambda}\\boldsymbol{\\Sigma}_{t}^{-1}+\\boldsymbol{\\mathrm{I}}_{d})=\\boldsymbol{\\Sigma}_{t}^{-1}\\boldsymbol{\\mathrm{A}}_{t}$ so we can write the matrices representing the estimator as $\\begin{array}{r}{\\mathbf{M}_{t}=(\\mathbf{I}^{-1}+\\pmb{\\Sigma}_{\\mathbf{A}}^{-1})^{-1}(\\mathbf{A}+\\pmb{\\Sigma}_{t})^{-1}=(\\mathbf{I}^{-1}+\\sum_{s=1}^{T}\\Sigma_{s}^{-1}\\mathbf{A}_{s})^{-1}\\Sigma_{t}^{-1}\\mathbf{A}_{t}}\\end{array}$ . Thus ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{i}\\mathbf{M}_{t}=(\\mathbf{T}^{-1}+\\boldsymbol{\\Sigma}_{\\Lambda}^{-1})^{-1}\\boldsymbol{\\Sigma}_{t}^{-1}\\partial_{i}\\mathbf{A}_{t}-(\\mathbf{T}^{-1}+\\boldsymbol{\\Sigma}_{\\Lambda}^{-1})^{-1}\\displaystyle{\\sum_{s=1}^{T}}\\boldsymbol{\\Sigma}_{s}^{-1}\\partial_{i}\\mathbf{A}_{s}\\mathbf{M}_{t}}\\\\ &{\\qquad=-\\mathbf{M}_{t}\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}\\boldsymbol{\\Sigma}_{t}^{-1}\\mathbf{A}_{t}+\\displaystyle{\\sum_{s=1}^{T}}\\mathbf{M}_{s}\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}\\boldsymbol{\\Sigma}_{s}^{-1}\\mathbf{A}_{s}\\mathbf{M}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We can then compute ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\partial_{i}\\operatorname{Tr}(\\mathbf{A}_{t}\\mathbf{M}_{t})=-\\operatorname{Tr}(\\mathbf{A}_{t}(\\mathbf{M}_{t}\\mathbf{A}_{t}+\\mathbf{A}_{t}\\mathbf{M}_{t})\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}\\Sigma_{t}^{-1})+\\sum_{s=1}^{T}\\operatorname{Tr}(\\mathbf{A}_{t}\\mathbf{M}_{s}\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}\\Sigma_{s}^{-1}\\mathbf{A}_{s}\\mathbf{M}_{t})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{\\lambda}\\Bigg\\Vert\\mathbb{A}_{t}\\Bigg(\\mathbb{S}_{t}-\\frac{T}{\\gamma=\\mathbf{M}}\\mathbf{M}_{*}\\mathbb{P}\\Bigg)\\Bigg\\Vert_{\\mathbf{X}_{t}^{*}}^{2}=\\underset{s=1}{\\overset{T}{\\sum}}\\underset{0}{\\overset{T}{\\sum}}(\\mathbb{M}_{s}\\mathbb{U}_{s}^{\\top}\\mathbb{\\sum}\\mathbb{X}_{t}^{-1}\\mathbb{A}_{t}(\\mathbb{P}_{t}-\\mathbb{M}_{*}\\mathbb{P}_{t})}\\\\ &{=-2\\underset{s=1}{\\overset{T}{\\sum}}(\\mathbb{\\lambda})\\mathbf{M}_{s}\\mathbb{P}_{t})\\mathbb{T}\\mathbb{A}_{t}^{\\top}\\mathbb{\\sum}\\mathbb{A}_{t}(\\mathbb{P}_{t}-\\hat{\\theta}_{T})}\\\\ &{\\quad+2\\mathbb{P}_{t}-\\hat{\\theta}_{T}\\mathbb{C}\\mathbb{\\lambda}\\mathbb{T}\\mathbb{A}_{t}^{\\top}\\mathbb{\\sum}\\mathbb{A}_{t}(\\mathbb{P}_{t}-\\hat{\\theta}_{T})}\\\\ &{=2\\underset{s=1}{\\overset{T}{\\sum}}(\\mathbf{M}_{s}\\mathbb{U}_{s}^{\\top}\\mathbb{C}\\mathbb{\\sum}\\mathbb{A}_{t}^{\\top}\\mathbb{M}_{s}^{\\top}\\mathbb{C}\\mathbb{\\hat{a}}_{t})}\\\\ &{\\quad-2\\underset{s=1}{\\overset{T}{\\sum}}\\underset{s=1}{\\overset{T}{\\sum}}(\\mathbf{M}_{s}\\mathbb{U}_{s}^{\\top}\\mathbb{C}\\mathbb{\\sum}^{-1}\\mathbb{A}_{t})\\mathbb{T}\\mathbb{A}_{s}\\mathbb{U}_{s}^{\\top}\\mathbb{A}_{t}(\\mathbb{P}_{t}-\\hat{\\theta}_{T})}\\\\ &{\\quad-2\\underset{s=1}{\\overset{T}{\\sum}}\\underset{s=1}{\\overset{T}{\\sum}}\\underset{s=1}{\\overset{T}{\\sum}}(\\mathbf{M}_{s}\\mathbb{U}_{s}^{\\top}\\mathbb{C}_{t}^{-1}\\mathbb{A}_{t}\\mathbb{M}_{s}^{\\top}\\mathbb{C}\\mathbb{\\lambda}_{t})\\mathbb{T}\\mathbb{A}_{s}\\mathbb{U}_{s}-\\hat{\\theta}_{T})}\\\\ &{\\quad-2\\mathbb{C}_{t}\\mathbb{C}_{t}\\Bigg\\Vert\\mathbb{C}\\mathbb{\\hat{a}} \n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lastly, note that $\\mathbf{M}_{t}\\,=\\,(\\mathbf{{T}}^{-1}+\\mathbf{{\\Sigma}}_{\\mathbf{{A}}}^{-1})^{-1}\\mathbf{{\\Sigma}}_{t}^{-1}\\mathbf{{A}}_{t}\\,=\\,(\\mathbf{I}_{d}+\\mathbf{{T}}\\mathbf{{\\Sigma}}_{\\mathbf{{A}}}^{-1})^{-1}\\mathbf{{T}}\\mathbf{{\\Sigma}}_{t}^{-1}\\mathbf{{A}}_{t}$ and redefine the derivative $\\partial_{i}$ to be w.r.t. $\\boldsymbol{v}_{i}^{2}$ , so that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\partial_{i}\\mathbf{M}_{t}=-(\\mathbf{I}_{d}+\\mathbf{I}\\pmb{\\Sigma}_{\\Lambda}^{-1})^{-1}\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}\\\\pmb{\\Sigma}_{\\Lambda}^{-1}\\mathbf{M}_{t}+(\\mathbf{I}_{d}+\\mathbf{I}\\pmb{\\Sigma}_{\\Lambda}^{-1})^{-1}\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}\\pmb{\\Sigma}_{t}^{-1}\\mathbf{A}_{t}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then we have $\\partial_{i}\\,\\mathrm{Tr}(\\mathbf{A}_{t}\\mathbf{M}_{t})=\\mathrm{Tr}(\\mathbf{A}_{t}(\\mathbf{I}_{d}+\\mathbf{I}\\boldsymbol{\\Sigma}_{\\Lambda}^{-1})^{-1}\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}(\\boldsymbol{\\Sigma}_{t}^{-1}\\mathbf{A}_{t}-\\boldsymbol{\\Sigma}_{\\Lambda}^{-1}\\mathbf{M}_{t}))$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{i}\\left\\|\\mathbf{A}_{t}\\left(\\mathbf{y}_{t}-\\hat{\\theta}_{\\mathbf{r}}\\right)\\right\\|_{\\mathbf{\\Sigma}_{t}^{-1}}^{2}}\\\\ &{\\quad=-2\\displaystyle\\sum_{s=1}^{T}(\\partial_{i}\\mathbf{M}_{s}\\mathbf{y}_{s})^{\\top}\\mathbf{A}_{t}^{\\top}\\mathbf{\\Sigma}_{t}^{-1}\\mathbf{A}_{t}\\big(\\mathbf{y}_{t}-\\hat{\\theta}_{\\mathbf{r}}\\big)}\\\\ &{\\quad=2\\left(\\displaystyle\\sum_{s=1}^{-1}\\hat{\\theta}_{\\mathbf{r}}-\\displaystyle\\sum_{s=1}^{T}\\Sigma_{s}^{-1}\\mathbf{A}_{s}\\mathbf{y}_{s}\\right)^{\\top}\\mathbf{U}_{i}\\mathbf{U}_{i}^{\\top}(\\mathbf{I}_{d}+\\Gamma\\Sigma_{\\Lambda}^{-1})^{-\\top}\\mathbf{A}_{t}^{\\top}\\Sigma_{t}^{-1}\\mathbf{A}_{t}\\big(\\mathbf{y}_{t}-\\hat{\\theta}_{\\Gamma}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "D.4 Handling groups with no data ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "It is frequently the case that a specific group $g$ may not have any examples, i.e., $n_{g}=0$ , and so we cannot define $\\Sigma_{g,g}=\\sigma^{2}/n_{g}$ . At the same time, we may need to handle the dimension associated with this group, either because we still need to report a value for it or because we are in the multi-task setting and other tasks do have examples of that group that may allow us to get an estimate. To handle this issue, in all computations we only use the precision matrix $\\mathbf{\\dot{Z}}^{-1}$ , which can be easily defined in the case of $n_{g}=0$ using $\\Sigma_{g,g}^{-1}=n_{g}/{\\sigma^{2}}^{\\prime}\\,{=0}$ . Note in particular that $(\\Lambda+\\Sigma)^{-1}=\\Sigma^{-1}(\\mathring{\\Lambda}\\Sigma^{-1}+\\ensuremath{\\mathbf{I}_{d}})^{-1}$ . ", "page_idx": 20}, {"type": "text", "text": "D.5 Handling near-singular covariances ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Since we would like to optimize over the entire domain $\\tau^{2}\\in\\mathbb{R}_{\\geq0}^{2^{k}}$ but $\\mathbf{A}(\\mathbf{0}_{2^{k}})=\\mathbf{0}_{d\\times d}$ is singular, we avoid inverting prior covariance matrices (i.e. $\\mathbf{\\nabla}\\Lambda,\\mathbf{T})$ in all computations. Note in particular that $\\mathbf{A}_{t}=(\\mathbf{A}^{-1}+\\mathbf{\\boldsymbol{\\Sigma}}^{-\\hat{1}})^{-1}\\mathbf{\\boldsymbol{\\Lambda}}^{-1}=(\\mathbf{I}_{d}+\\mathbf{\\boldsymbol{\\Lambda}}\\mathbf{\\boldsymbol{\\Sigma}}^{-1})^{-1}$ . ", "page_idx": 20}, {"type": "text", "text": "D.6 Handling group variances for AUC ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "While obtaining unbiased variance estimates for averages is straightforward, it is more involved for multi-point statistics such as AUC. For simplicity we just use $(n_{g}+1)/(12n_{g}n_{g}^{(0)}n_{g}^{(1)})$ , where $n_{g}^{(i)}$ is the number of members of group $g$ with label $i$ ; this estimate is derived from the variance estimate of the related Mann-Whitney -statistic [Siegel, 1956]. However, future work may consider improvements based on more complicated approaches [Cortes and Mohri, 2003, Wang and Guo, 2020], or using bootstrapping techniques as is done by Herlihy et al. [2024] for structured regression. ", "page_idx": 21}, {"type": "text", "text": "E Derivation of SureMap estimators via ridge regression ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this appendix we show that $\\hat{\\pmb{\\mu}}^{\\mathrm{SM}}$ and $\\hat{\\mu}_{t}^{\\mathrm{SM}}$ can be derived as ridge regression estimators. ", "page_idx": 21}, {"type": "text", "text": "Similar to the notation in Appendix C, we consider $k$ sensitive attributes (like sex, age, etc.) indexed by $a\\in[k]$ . The $a$ th sensitive attribute is denoted $g_{a}$ and takes values in $[d_{a}]$ . The joint sensitive attribute $\\mathbf{g}$ takes values in $\\mathcal{G}=[d_{1}]\\times[d_{2}]\\times\\cdot\\cdot\\cdot\\times[d_{k}]$ with the cardinality ${\\dot{d}}=|{\\boldsymbol{\\mathcal{G}}}|=d_{1}d_{2}\\cdot\\cdot\\cdot d_{k}$ . For $A\\subseteq[k]$ , write $\\mathbf{g}_{A}$ for the tuple $(g_{a})_{a\\in A}$ . ", "page_idx": 21}, {"type": "text", "text": "E.1 Single-task regression model ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Each joint attribute $\\mathbf{g}\\in\\mathcal{G}$ identifies an intersectional group (of order $k$ ). We seek to jointly fit means of all these groups, represented as a vector $\\pmb{\\mu}\\in\\mathbb{R}^{|\\mathcal{G}|}$ , based on the vector of empirical means $\\mathbf{y}\\in\\mathbb{R}^{|\\mathcal{G}|}$ . We posit a regression model ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mu=\\Phi\\beta\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\Phi\\in\\mathbb{R}^{|\\mathcal{G}|\\times|\\mathcal{I}|}$ is a feature matrix and $\\beta\\in\\mathbb{R}^{|\\mathcal{I}|}$ is a vector of regression coefficients. The columns of $\\Phi$ are referred to as features and indexed by $j\\in\\mathcal{I}$ , where $\\mathcal{I}$ is some index set. We assume a Gaussian prior on the parameter $\\beta\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{K})$ and a Gaussian distribution over observation errors, so $\\mathbf{y}\\sim\\mathcal{N}(\\bar{\\Phi}\\beta,\\Sigma)$ , where $\\mathbf{K}$ and $\\Sigma$ are, respectively, the prior covariance matrix and error covariance matrix. ", "page_idx": 21}, {"type": "text", "text": "The error covariance matrix $\\Sigma$ is assumed fixed and positive definite, the prior covariance matrix $\\mathbf{K}$ is viewed as a hyperparameter (with a specific structure to reduce its dimension); for simplicity, we assume that $\\mathbf{K}$ is positive definite (but that assumption is not necessary). Any generic forms of $\\Phi,\\mathbf{K}$ and $\\Sigma$ can be considered. Here we exhibit a specific form of $\\Phi$ and $\\mathbf{K}$ under which the MAP regression estimator is the same as $\\hat{\\pmb{\\mu}}^{\\mathrm{SM}}$ , when provided with the same error covariance matrix $\\Sigma$ . ", "page_idx": 21}, {"type": "text", "text": "We consider a structured form of matrix $\\Phi$ , with features being indicators of tuples of sensitive attribute values. The features are indexed by $j\\in\\mathcal{I}$ , where ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{I}=\\big\\{(A,\\mathbf{c}):\\:A\\subseteq[k],\\:\\mathbf{c}\\in\\prod_{a\\in A}[d_{a}]\\big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and defined as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Phi_{\\mathbf{g},(A,\\mathbf{c})}=1\\{\\mathbf{g}_{A}=\\mathbf{c}\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We will also consider subsets of features that focus on a specific subset of sensitive attributes $A\\subseteq[k]$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{J}_{A}=\\big\\{(A,\\mathbf{c}):\\;\\mathbf{c}\\in\\prod_{a\\in A}[d_{a}]\\big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and write $\\Phi_{A}\\in\\mathbb{R}^{|\\mathcal{G}|\\times|\\mathcal{I}_{A}|}$ for the submatrix composed of features indexed by $j\\in\\mathcal{I}_{A}$ ", "page_idx": 21}, {"type": "text", "text": "The prior matrix $\\mathbf{K}$ is diagonal, specified using a set of hyperparameters $\\tau\\,=\\,(\\tau_{A})_{A\\subseteq[k]}$ , with diagonal entries ", "page_idx": 21}, {"type": "equation", "text": "$$\nK_{(A,\\mathbf{c}),(A,\\mathbf{c})}=\\tau_{A}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The MAP solution under the model described above is obtained by solving ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\beta}=\\underset{\\beta}{\\arg\\operatorname*{min}}\\Big[(\\mathbf{y}-\\Phi\\boldsymbol{\\beta})^{\\top}\\Sigma^{-1}(\\mathbf{y}-\\Phi\\boldsymbol{\\beta})+\\boldsymbol{\\beta}^{\\top}\\mathbf{K}^{-1}\\boldsymbol{\\beta}\\Big],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is a weighted ridge regression problem. Setting the gradient to zero, the solution must satisfy ", "page_idx": 21}, {"type": "equation", "text": "$$\n-2\\Phi^{\\top}\\Sigma^{-1}(\\mathbf{y}-\\Phi\\beta)+2\\mathbf{K}^{-1}\\beta=\\mathbf{0}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence, ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\hat{\\boldsymbol{\\beta}}}=(\\Phi^{\\top}\\Sigma^{-1}\\Phi+\\mathbf{K}^{-1})^{-1}\\Phi^{\\top}\\Sigma^{-1}\\mathbf{y}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This yields a regression-based estimator ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\pmb{\\mu}}^{\\mathrm{reg}}=\\pmb{\\Phi}\\hat{\\beta}=\\pmb{\\Phi}(\\pmb{\\Phi}^{\\top}\\pmb{\\Sigma}^{-1}\\pmb{\\Phi}+\\mathbf{K}^{-1})^{-1}\\pmb{\\Phi}^{\\top}\\pmb{\\Sigma}^{-1}\\mathbf{y}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In contrast, by Eqs. (11) and (7), the $\\hat{\\pmb{\\mu}}^{\\mathrm{SM}}$ estimator is obtained as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\pmb{\\mu}}^{\\mathrm{SM}}=(\\mathbf{A}^{-1}+\\pmb{\\Sigma}^{-1})^{-1}\\pmb{\\Sigma}^{-1}\\mathbf{y},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the matrix $\\Lambda$ (following Eq. 22 in Appendix C.1) depends on the hyperparameter vector $\\tau$ as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{A}=\\sum_{A\\subseteq[k]}\\tau_{A}^{2}\\mathbf{U}_{A}\\mathbf{U}_{A}^{\\top},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathbf{U}_{A}$ is precisely the submatrix $\\Phi_{A}$ defined above. Thus, writing $\\phi_{A,\\mathbf{c}}\\in\\mathbb{R}^{|\\mathcal{G}|}$ for the column of $\\Phi$ indexed by $\\left(A,\\mathbf{c}\\right)$ , we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Lambda=\\sum_{A\\subseteq[k]}\\tau_{A}^{2}\\Phi_{A}\\Phi_{A}^{\\top}=\\sum_{A\\subseteq[k]}\\sum_{\\mathbf{c}\\in\\prod_{a\\in A}[d_{a}]}\\tau_{A}^{2}\\phi_{A,\\mathbf{c}}\\phi_{A,\\mathbf{c}}^{\\top}=\\Phi\\mathbf{K}\\Phi^{\\top}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, the $\\hat{\\mu}^{\\mathrm{SM}}$ estimator can be rewritten as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\pmb{\\mu}}^{\\mathrm{SM}}=\\left((\\Phi\\mathbf{K}\\Phi^{\\top})^{-1}+\\mathbf{\\Sigma}^{-1}\\right)^{-1}\\mathbf{\\Sigma}^{-1}\\mathbf{y}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Theorem E.1. With $\\Phi$ and $\\mathbf{K}$ defined as above (Eqs. 44 and 45), we have $\\hat{\\pmb{\\mu}}^{\\mathrm{reg}}=\\hat{\\pmb{\\mu}}^{\\mathrm{SM}}$ . ", "page_idx": 22}, {"type": "text", "text": "In the proof we use a variant of Sherman\u2013Morrison\u2013Woodbury formula [Horn and Johnson, 2013, Eq. 0.7.4.1], specialized to symmetric positive definite matrices: ", "page_idx": 22}, {"type": "text", "text": "Proposition E.1 (Symmetric SMW Formula). Let $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$ and $\\mathbf{R}\\in\\mathbb{R}^{m\\times m}$ be symmetric positive definite matrices and let $\\mathbf{X}\\in\\mathbb{R}^{n\\times m}$ . Then ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\mathbf{A}+\\mathbf{X}\\mathbf{R}\\mathbf{X}^{\\top})^{-1}=\\mathbf{A}^{-1}-\\mathbf{A}^{-1}\\mathbf{X}(\\mathbf{R}^{-1}+\\mathbf{X}^{\\top}\\mathbf{A}^{-1}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{A}^{-1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Theorem E.1. It suffices to show that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Phi(\\Phi^{\\top}\\Sigma^{-1}\\Phi+\\mathbf{K}^{-1})^{-1}\\Phi^{\\top}=\\big((\\Phi\\mathbf{K}\\Phi^{\\top})^{-1}+\\Sigma^{-1}\\big)^{-1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We do this by direct calculation, using the symmetric SMW formula: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi\\big(\\Phi^{\\top}\\Sigma^{-1}\\Phi+\\mathbf K^{-1}\\big)^{-1}\\Phi^{\\top}=\\Phi\\Big[\\mathbf K-\\mathbf K\\Phi^{\\top}\\big(\\Sigma+\\Phi\\mathbf K\\Phi^{\\top}\\big)^{-1}\\Phi\\mathbf K\\Big]\\Phi^{\\top}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\big(\\Phi\\mathbf K\\Phi^{\\top}\\big)-\\big(\\Phi\\mathbf K\\Phi^{\\top}\\big)\\Big(\\big(\\Sigma+\\big(\\Phi\\mathbf K\\Phi^{\\top}\\big)\\Big)^{-1}\\big(\\Phi\\mathbf K\\Phi^{\\top}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\big(\\big(\\Phi\\mathbf K\\Phi^{\\top}\\big)^{-1}+\\Sigma^{-1}\\big)^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The first equality follows by the SMW formula with $\\mathbf{A}=\\mathbf{K}^{-1}$ , $\\mathbf{R}=\\pmb{\\Sigma}^{-1}$ and $\\mathbf{X}=\\mathbf{\\Phi}^{\\top}$ , the second by multiplying out the terms, and the third by the SMW formula with $\\mathbf{A}^{-1}=\\Phi\\mathbf{K}\\boldsymbol{\\Phi}^{\\top}$ , $\\mathbf{R}^{-1}=\\pmb{\\Sigma}$ and $\\mathbf{X}$ equal to the $d\\times d$ identity matrix. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "E.2 Multi-task regression model ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Consider multi-task setting with tasks indexed by $t=1,\\dots,T$ . We write $\\mathcal{T}=[T]$ for the set of tasks. Multi-task setting can be reduced to single-task setting by viewing the task id as an additional sensitive attribute, and performing the same analysis as for the single-task setting, but with $\\mathcal{G}^{\\prime}=\\mathcal{T}\\times\\mathcal{G}$ , with the dimension $d^{\\prime}=|\\mathcal{G}^{\\prime}|=T d$ . ", "page_idx": 22}, {"type": "text", "text": "Formally, we seek to fit $\\pmb{\\mu}^{\\prime}\\in\\mathbb{R}^{|\\mathcal{G}^{\\prime}|}$ based on the vector of empirical means $\\mathbf{y}^{\\prime}\\in\\mathbb{R}^{|\\mathcal{G}^{\\prime}|}$ . The entries of $\\pmb{\\mu}^{\\prime}$ and $\\mathbf{y}^{\\prime}$ are denoted as $\\mu_{t,\\mathbf{g}}^{\\prime}$ and $y_{t,\\mathbf{g}}^{\\prime}$ for the task $t$ and the intersectional group g. We denote task-specific slices of these vectors as $\\tilde{\\mu_{t}^{\\prime}}=\\pmb{\\mu_{\\{t\\}\\times\\mathcal{G}}^{\\prime}}$ and $\\mathbf{y}_{t}^{\\prime}=\\mathbf{y}_{\\{t\\}\\times\\mathcal{G}}^{\\prime}$ . ", "page_idx": 22}, {"type": "text", "text": "Features are indexed by elements of $\\mathcal{J}^{\\prime}\\,=\\,\\mathcal{S}\\,\\times\\,\\mathcal{J}$ , where $S\\,=\\,\\mathcal{T}\\cup\\{{\\mathfrak{g}}{\\vert}0{\\mathfrak{b}}\\}$ , so there are taskspecific and global features. The feature matrix $\\Phi^{\\prime}\\in\\mathbb{R}^{|\\mathcal{G}^{\\prime}|\\times|\\mathcal{T}^{\\prime}|}$ uses the single-task feature matrix $\\bar{\\Phi}\\in\\mathbb{R}^{|\\mathcal{G}|\\times|\\bar{\\mathcal{T}}|}$ (defined in Eq. 44) as a building block. The matrix $\\Phi^{\\prime}$ has a block structure, with $|\\mathcal{T}|\\times|\\mathcal{S}|$ blocks of size $|\\mathcal G|\\stackrel{\\_}{\\times}|\\mathcal I|$ defined as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Phi_{t,s}^{\\prime}={\\left\\{\\begin{array}{l l}{\\Phi}&{{\\mathrm{if~}}s=t{\\mathrm{~or~}}s={\\mathfrak{g}}{\\mathrm{lob}},}\\\\ {\\mathbf{0}_{|{\\mathcal{G}}|\\times|{\\mathcal{I}}|}}&{{\\mathrm{otherwise}}.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We posit the regression model ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mu^{\\prime}=\\Phi^{\\prime}\\beta^{\\prime},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\beta^{\\prime}\\in\\mathbb{R}^{\\mathcal{I}^{\\prime}}$ . With the definition of $\\Phi^{\\prime}$ as above, this boils down to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mu_{t}^{\\prime}=\\Phi(\\beta_{t}^{\\prime}+\\beta_{\\mathfrak{g l o b}}^{\\prime})\\qquad{\\mathrm{for~all~}}t\\in\\mathcal{T}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "As before, we assume a Gaussian prior and Gaussian errors, $\\beta^{\\prime}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{K}^{\\prime}),\\mathbf{y}^{\\prime}\\sim\\mathcal{N}(\\Phi^{\\prime}\\beta^{\\prime},\\Sigma^{\\prime}).$ ", "page_idx": 23}, {"type": "text", "text": "The error covariance $\\Sigma^{\\prime}\\in\\mathbb{R}^{|\\mathcal{G}^{\\prime}|\\times|\\mathcal{G}^{\\prime}|}$ has a block-diagonal form with single-task error covariance matrices $\\Sigma_{t}\\in\\mathbb{R}^{|\\mathcal{G}|\\times|\\mathcal{G}|}$ , for $t\\in[T]$ , along the diagonal. ", "page_idx": 23}, {"type": "text", "text": "We consider a structured form of the prior covariance matrix $\\mathbf{K}^{\\prime}$ specified by two vectors of hyperparameters: $\\tau=(\\tau_{A}^{2})_{A\\subseteq[k]}$ and $\\pmb{v}=(v_{A})_{A\\subseteq[k]}$ . The matrix $\\mathbf{K}^{\\prime}$ is diagonal and positive definite, with entries ", "page_idx": 23}, {"type": "equation", "text": "$$\nK_{(s,A,\\mathbf{c}),(s,A,\\mathbf{c})}^{\\prime}=\\left\\{\\tau_{A}^{2}\\quad{\\mathrm{if~}}s\\in{\\mathcal{T}},\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It can also be viewed as a block-diagonal matrix with $|{\\cal S}|$ diagonal blocks of size $|{\\mathcal{I}}|\\times|{\\mathcal{I}}|$ defined as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{K}_{s,s}^{\\prime}=\\left\\{\\mathbf{K}\\quad{\\mathrm{if~}}s\\in{\\mathcal{T}},\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\mathbf{K}$ is the single-task prior matrix defined in Eq. (45), and $\\mathbf{V}\\in\\mathbb{R}^{|\\mathcal{I}|\\times|\\mathcal{I}|}$ is an analogous matrix based on the vector of hyperparameters $\\pmb{v}$ rather than $\\tau$ , with diagonal entries ", "page_idx": 23}, {"type": "equation", "text": "$$\nV_{(A,\\mathbf{c}),(A,\\mathbf{c})}=v_{A}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The multi-task regression-based estimator is obtained by solving the resulting MAP regression problem. Similarly to the single-task case, the estimator takes form ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{\\boldsymbol{\\mu}}^{\\prime^{\\mathrm{reg}}}=\\boldsymbol{\\Phi}^{\\prime}\\Big(\\boldsymbol{\\Phi}^{\\prime^{\\intercal}}\\boldsymbol{\\Sigma}^{\\prime^{-1}}\\boldsymbol{\\Phi}^{\\prime}+\\mathbf{K}^{\\prime^{-1}}\\Big)^{-1}\\boldsymbol{\\Phi}^{\\prime^{\\intercal}}\\boldsymbol{\\Sigma}^{\\prime^{-1}}\\mathbf{y}^{\\prime},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with the individual task estimates denoted $\\hat{\\mu}_{t}^{\\prime}{}^{\\mathrm{{reg}}}$ . ", "page_idx": 23}, {"type": "text", "text": "The multi-task SureMap estimator, introduced in $\\S3.2.2$ , takes form ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{\\pmb{\\mu}}_{t}^{\\mathrm{SM}}=\\mathbf{y}_{t}^{\\prime}+\\Big(\\mathbf{A}^{-1}+\\mathbf{\\Sigma}_{t}^{-1}\\Big)^{-1}\\mathbf{\\Lambda}^{-1}\\big(\\hat{\\pmb{\\theta}}-\\mathbf{y}_{t}^{\\prime}\\big),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{\\pmb{\\theta}}=\\left(\\mathbf{\\Gamma}^{-1}+\\sum_{t=1}^{T}(\\mathbf{A}+\\Sigma_{t})^{-1}\\right)^{-1}\\sum_{t=1}^{T}(\\mathbf{A}+\\Sigma_{t})^{-1}\\mathbf{y}_{t}^{\\prime},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{A}=\\Phi\\mathbf{K}\\Phi^{\\top}\\qquad\\mathrm{and}\\qquad\\mathbf{\\Gamma}\\mathbf{\\Gamma}=\\Phi\\mathbf{V}\\Phi^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Theorem E.2. With $\\Phi^{\\prime}$ , $\\mathbf{K}^{\\prime}$ and $\\Sigma^{\\prime}$ defined as above, we have $\\hat{\\mu}_{t}^{\\mathrm{\\tiny/Ieg}}=\\hat{\\mu}_{t}^{\\mathrm{\\tiny{SM}}}$ for all $t\\in[T]$ . ", "page_idx": 23}, {"type": "text", "text": "In the proof we use the following identities: ", "page_idx": 23}, {"type": "text", "text": "Proposition E.2. Let A, $\\mathbf{B}\\in\\mathbb{R}^{n\\times n}$ be symmetric positive definite matrices and let $\\mathbf{I}$ be the $n\\times n$ identity matrix. Then ", "page_idx": 23}, {"type": "equation", "text": "$$\n(\\mathbf{A}^{-1}+\\mathbf{B}^{-1})^{-1}\\mathbf{A}^{-1}=\\mathbf{I}-\\mathbf{A}(\\mathbf{A}+\\mathbf{B})^{-1}=\\mathbf{B}(\\mathbf{A}+\\mathbf{B})^{-1}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. We have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathbf{A}^{-1}+\\mathbf{B}^{-1})^{-1}\\mathbf{A}^{-1}=\\big[\\mathbf{A}-\\mathbf{A}(\\mathbf{A}+\\mathbf{B})^{-1}\\mathbf{A}\\big]\\mathbf{A}^{-1}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathbf{I}-\\mathbf{A}(\\mathbf{A}+\\mathbf{B})^{-1}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=(\\mathbf{A}+\\mathbf{B})(\\mathbf{A}+\\mathbf{B})^{-1}-\\mathbf{A}(\\mathbf{A}+\\mathbf{B})^{-1}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathbf{B}(\\mathbf{A}+\\mathbf{B})^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the first equality follows by the SMW formula (Proposition E.1), and the remaining equalities follow by simple algebraic manipulations. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem $E.2$ . Starting with Eq. (49), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\boldsymbol\\mu}^{\\mathrm{\\tiny/reg}}=\\Phi^{\\prime}\\Big(\\Phi^{\\prime\\,^{\\top}}\\Sigma^{\\prime\\,^{-1}}\\Phi^{\\prime}+\\mathbf K^{\\prime\\,^{-1}}\\Big)^{-1}\\Phi^{\\prime\\,^{\\top}}\\Sigma^{\\prime\\,^{-1}}\\mathbf y^{\\prime}}\\\\ &{\\qquad=\\Big(\\big(\\Phi^{\\prime}\\mathbf K^{\\prime}\\Phi^{\\prime\\,^{\\top}}\\big)^{-1}+\\Sigma^{\\prime\\,^{-1}}\\Big)^{-1}\\Sigma^{\\prime\\,^{-1}}\\mathbf y^{\\prime}}\\\\ &{\\qquad=\\Big(\\mathbf I-\\Sigma^{\\prime}\\big(\\Phi^{\\prime}\\mathbf K^{\\prime}\\Phi^{\\prime\\,^{\\top}}+\\Sigma^{\\prime}\\big)^{-1}\\Big)\\mathbf y^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the second equality follows by the same reasoning as in the proof of Theorem E.1, and the third equality follows by Proposition E.2. ", "page_idx": 24}, {"type": "text", "text": "We next evaluate $\\Phi^{\\prime}{\\bf K}^{\\prime}\\Phi^{\\prime}{}^{\\top}$ . Note that the matrices $\\Phi^{\\prime}$ , $\\mathbf{K}^{\\prime}$ and $\\Sigma^{\\prime}$ have the following block structure: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Phi^{\\prime}=\\left(\\begin{array}{c c c c}{\\Phi}&&&{\\Phi}\\\\ &{\\ddots}&&{\\vdots}\\\\ &&{\\Phi}&{\\Phi}\\end{array}\\right)\\,,\\quad\\mathbf{K}^{\\prime}=\\left(\\begin{array}{c c c c}{\\mathbf{K}}&{\\ddots}&&\\\\ &{\\ddots}&&\\\\ &&{\\mathbf{K}}&\\\\ &&&{\\mathbf{V}}\\end{array}\\right)\\,,\\quad\\mathbf{Z}^{\\prime}=\\left(\\begin{array}{c c c}{\\Sigma_{1}}&&&\\\\ &{\\ddots}&\\\\ &&{\\Sigma_{T}}\\end{array}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Phi^{\\prime}\\mathbf{K}^{\\prime}\\Phi^{\\prime\\intercal}=\\left(\\begin{array}{l l l}{\\Phi}&{\\ddots}&\\\\ &{\\ddots}&\\\\ &&{\\Phi}\\end{array}\\right)\\left(\\begin{array}{l l l}{\\mathbf{K}}&{\\ddots}&\\\\ &{\\ddots}&\\\\ &&{\\mathbf{K}}\\end{array}\\right)\\left(\\begin{array}{l l l}{\\Phi^{\\intercal}}&{\\ddots}&\\\\ &{\\ddots}&\\\\ &&{\\Phi^{\\intercal}}\\end{array}\\right)+\\left(\\begin{array}{l}{\\Phi}\\\\ {\\vdots}\\\\ {\\Phi}\\end{array}\\right)\\mathbf{V}\\left(\\Phi^{\\intercal}\\right.\\cdot\\cdot\\cdot\\Phi^{\\intercal}\\right)}\\\\ {=\\left(\\Phi\\mathbf{K}\\Phi^{\\intercal}\\right.}&{\\ddots}\\\\ &&{\\left.\\Phi\\mathbf{K}\\Phi^{\\intercal}\\right)+\\left(\\begin{array}{l}{\\mathbf{I}}\\\\ {\\vdots}\\\\ {\\mathbf{I}}\\end{array}\\right)\\Phi\\mathbf{V}\\Phi^{\\intercal}\\left(\\mathbf{I}\\,\\cdots\\,\\mathbf{I}\\right)}\\\\ {=\\left(\\begin{array}{l l l}{\\Lambda}&&\\\\ &{\\ddots}&\\\\ &&{\\Lambda}\\end{array}\\right)+\\left(\\begin{array}{l}{\\mathbf{I}}\\\\ {\\vdots}\\\\ {\\mathbf{I}}\\end{array}\\right)\\mathbf{I}\\left(\\mathbf{I}\\,\\cdots\\,\\mathbf{I}\\right)=\\mathbf{A}^{\\prime}+\\mathbf{X}\\mathbf{I}\\mathbf{X}^{\\intercal},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where in the last line we introduced the notation $\\Lambda^{\\prime}$ for the block-diagonal matrix with $T$ copies of matrix $\\Lambda$ along diagonal, and notation $\\mathbf{X}$ for the matrix obtained by stacking $T$ copies of the $|\\mathcal{G}|\\times|\\mathcal{G}|$ identity matrix I on top of each other. Plugging the last expression back into Eq. (52), we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\boldsymbol\\mu}^{\\mathrm{\\tiny/reg}}=\\Big[\\mathbf I-\\boldsymbol\\Sigma^{\\prime}\\Big(\\boldsymbol\\Lambda^{\\prime}+\\mathbf X\\mathbf I\\mathbf X^{\\top}+\\boldsymbol\\Sigma^{\\prime}\\Big)^{-1}\\Big]\\mathbf y^{\\prime}}\\\\ &{\\qquad=\\mathbf y^{\\prime}-\\boldsymbol\\Sigma^{\\prime}\\Big[(\\boldsymbol\\Lambda^{\\prime}+\\boldsymbol\\Sigma^{\\prime})^{-1}}\\\\ &{\\qquad\\qquad\\qquad-\\mathbf\\Gamma(\\boldsymbol\\Lambda^{\\prime}+\\boldsymbol\\Sigma^{\\prime})^{-1}\\mathbf X\\Big(\\mathbf T^{-1}+\\mathbf X^{\\top}(\\boldsymbol\\Lambda^{\\prime}+\\boldsymbol\\Sigma^{\\prime})^{-1}\\mathbf X\\Big)^{-1}\\mathbf X^{\\top}(\\boldsymbol\\Lambda^{\\prime}+\\boldsymbol\\Sigma^{\\prime})^{-1}\\Big]\\mathbf y^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the second equality follows by the SMW formula (Proposition E.1) with ${\\bf A}={\\bf A}^{\\prime}+{\\bf\\boldsymbol{\\Sigma}}^{\\prime}$ , $\\mathbf{R}=\\mathbf{T}$ , and $\\mathbf{X}=\\mathbf{X}$ . We next focus on simplifying the last term in the bracket in Eq. (53). ", "page_idx": 24}, {"type": "text", "text": "Since $\\Lambda^{\\prime}$ and $\\Sigma^{\\prime}$ are block-diagonal, the matrix $(\\mathbf{A}^{\\prime}+\\Sigma^{\\prime})^{-1}$ is also block-diagonal with blocks along the diagonal equal to $(\\mathbf{A}+\\mathbf{\\bar{\\Sigma}}\\mathbf{\\bar{\\Sigma}}_{t})^{-1}$ for $t=1,\\dots,T$ . Thus, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{X}^{\\top}(\\mathbf{A}^{\\prime}+\\mathbf{Z}^{\\prime})^{-1}\\mathbf{X}=(\\mathbf{I}\\,\\cdots\\,\\mathbf{I})\\left(^{(\\mathbf{A}+\\sum_{1})^{-1}}\\begin{array}{l l l}{\\ddots}&&\\\\ &{\\ddots}&\\\\ &&{(\\mathbf{A}+\\sum_{T})^{-1}}\\end{array}\\right)\\left(\\begin{array}{l}{\\!\\!\\!\\Gamma}\\\\ {\\!\\!\\!\\vdots\\!\\!}\\\\ {\\!\\!\\!\\Gamma}\\end{array}\\right)=\\sum_{t=1}^{T}(\\mathbf{A}+\\Sigma_{t})^{-1},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and similarly, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{X}^{\\top}(\\mathbf{A}^{\\prime}+\\Sigma^{\\prime})^{-1}\\mathbf{y}^{\\prime}=\\sum_{t=1}^{T}(\\mathbf{A}+\\Sigma_{t})^{-1}\\mathbf{y}_{t}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big(\\mathbf{\\Gamma}^{-1}+\\mathbf{X}^{\\top}(\\mathbf{\\boldsymbol{\\Lambda}}^{\\prime}+\\mathbf{\\boldsymbol{\\Sigma}}^{\\prime})^{-1}\\mathbf{X}\\Big)^{-1}\\mathbf{X}^{\\top}(\\mathbf{\\boldsymbol{\\Lambda}}^{\\prime}+\\mathbf{\\boldsymbol{\\Sigma}}^{\\prime})^{-1}\\mathbf{y}^{\\prime}}\\\\ &{\\qquad=\\Bigg(\\mathbf{\\Gamma}^{-1}+\\displaystyle\\sum_{t=1}^{T}(\\mathbf{\\boldsymbol{\\Lambda}}+\\mathbf{\\boldsymbol{\\Sigma}}_{t})^{-1}\\Bigg)^{-1}\\displaystyle\\sum_{t=1}^{T}(\\mathbf{\\boldsymbol{\\Lambda}}+\\mathbf{\\boldsymbol{\\Sigma}}_{t})^{-1}\\mathbf{y}_{t}^{\\prime}=\\hat{\\boldsymbol{\\theta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Plugging this back in Eq. (53), we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\hat{\\boldsymbol{\\mu}}}^{\\prime\\mathrm{reg}}=\\mathbf{y}^{\\prime}-\\mathbf{\\boldsymbol{\\Sigma}}^{\\prime}\\left[(\\mathbf{A}^{\\prime}+\\mathbf{\\boldsymbol{\\Sigma}}^{\\prime})^{-1}\\mathbf{y}^{\\prime}-(\\mathbf{A}^{\\prime}+\\mathbf{\\boldsymbol{\\Sigma}}^{\\prime})^{-1}\\mathbf{X}{\\hat{\\boldsymbol{\\theta}}}\\right]=\\mathbf{y}^{\\prime}-\\mathbf{\\boldsymbol{\\Sigma}}^{\\prime}(\\mathbf{\\boldsymbol{\\Lambda}}^{\\prime}+\\mathbf{\\boldsymbol{\\Sigma}}^{\\prime})^{-1}\\left[\\mathbf{y}^{\\prime}-\\left({\\overset{1}{\\vdots}}\\right){\\hat{\\boldsymbol{\\theta}}}\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using, again, the fact that matrices $\\Lambda^{\\prime}$ and $\\Sigma^{\\prime}$ are block-diagonal, the task-specific blocks of $\\hat{\\pmb{\\mu}}^{\\prime}{}^{\\mathrm{reg}}$ must be equal to ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\pmb{\\mu}}_{t}^{\\prime\\mathrm{reg}}=\\mathbf{y}_{t}^{\\prime}-\\Sigma_{t}(\\mathbf{A}+\\Sigma_{t})^{-1}(\\mathbf{y}_{t}^{\\prime}-\\hat{\\pmb{\\theta}})}\\\\ &{\\qquad=\\mathbf{y}_{t}^{\\prime}-\\left(\\mathbf{A}^{-1}+\\Sigma_{t}^{-1}\\right)^{-1}\\mathbf{A}^{-1}(\\mathbf{y}_{t}^{\\prime}-\\hat{\\pmb{\\theta}})}\\\\ &{\\qquad=\\mathbf{y}_{t}^{\\prime}+\\left(\\mathbf{A}^{-1}+\\Sigma_{t}^{-1}\\right)^{-1}\\mathbf{A}^{-1}(\\hat{\\pmb{\\theta}}-\\mathbf{y}_{t}^{\\prime})=\\hat{\\pmb{\\mu}}_{t}^{\\mathrm{SM}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the second equality follows by Proposition E.2. ", "page_idx": 25}, {"type": "text", "text": "F Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "F.1 Data and model resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We make use of data / models with the following sources / licenses: ", "page_idx": 25}, {"type": "text", "text": "1. Strack et al. [2014]: CC Attribution License.   \n2. Weerts et al. [2023]: MIT License.   \n3. Ardila et al. [2020]: CC0 License.   \n4. Radford et al. [2023]: Apache-2.0 License.   \n5. https://archive.ics.uci.edu/dataset/2/adult: CC BY 4.0 License   \n6. https://huggingface.co/JaaackXD/Llama-3-70B-GGUF: Meta Llama 3 License ", "page_idx": 25}, {"type": "text", "text": "We use the third and fourth resources to create a dataset of Whisper model evaluations on Common Voice utterances, which result in the Common Voice and CVC tasks described in $\\S4.2$ ; we also use the last two resources to create a dataset of in-context evaluations of Llama 3 on the Adult dataset, which results in the Adult task described in $\\S4.1$ . Both resources are released under a CC BY 4.0 License and are available at https://github.com/mkhodak/SureMap. ", "page_idx": 25}, {"type": "text", "text": "F.2 Computational ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "By far the most computation was required to generate the Common Voice, CVC, and Adult tasks, which was done on a machine with two RTX-8000 GPUs and took about a week. As described above, the corresponding datasets are made publicly available and easy to re-use without any GPU access. Given these dataset, the main experiments were run on a 40-core machine and take a couple hours, with the vast majority of this time spent running the structured regression approach of Herlihy et al. [2024]; see Figure 7 for a summary of the costs associated with each method evaluated in this paper. Code for both generating the task data and reproducing the method evaluations is available at https://github.com/mkhodak/SureMap. ", "page_idx": 25}, {"type": "image", "img_path": "aTNT3FuVBG/tmp/10a970f55e99e9efc2cc54d6e4e290e76cea821e237d7032ec97da18b57d25a0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 7: Cost of running the methods evaluated in this paper as a function of the number of tasks, with both axes scaled logarithmically. While they are 1-2 orders of magnitude more expensive than the baselines\u2014which all have closed form expressions\u2014SureMap and multi-task SureMap are also 1-2 orders of magnitude than structured regression [Herlihy et al., 2024]. Note that for the most part the runtime of all methods will usually be dwarfed by the cost of inference. ", "page_idx": 26}, {"type": "image", "img_path": "aTNT3FuVBG/tmp/9f1e08fe1deb9536419a6682fde6cda6d07c11a1509747a8c9ddb9482e7481c9.jpg", "img_caption": ["Figure 8: Evaluations on the regression variant of Diabetes, using MAE as the target metric, disaggregating by race, sex, and age. On the left the MAE is taken across all groups, while in the center it is only over large groups and on the left over small groups. Large and small are defined as the top and bottom half of all groups, respectively. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "aTNT3FuVBG/tmp/942d7674fa15e876148cbf36888d828e4f0bbf0b8b1c8f3faff49bba5b57ca6c.jpg", "img_caption": ["Figure 9: Evaluations on the regression variant of Diabetes, using MSE as the target metric, disaggregating by race, sex, and age. On the left the MAE is taken across all groups, while in the center it is only over large groups and on the left over small groups. Large and small are defined as the top and bottom half of all groups, respectively. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "G Additional evaluations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In Figures 8 & 9, we compare evaluation methods on the regression variant of the Diabetes task, where the goal is to predict a patient\u2019s length of stay using ridge regression. In Figures 10 & 12, we compare evaluation methods on Diabetes and SLACS with AUC as the target metric. In Figures 11 & 13, we compare evaluation methods on Common Voice and CVC with the character error rate (CER) as the target metric. And finally, in Figures 14 & 15, we compare evaluation methods on Diabetes, Adult, Common Voice, SLACS, and CVC according to RMSE instead of MAE. ", "page_idx": 26}, {"type": "image", "img_path": "aTNT3FuVBG/tmp/cc3de1b420ece8d34e06ed6c78f5a94f3f006a34c13dd6706b938bedfdb36c2a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 10: Single-task evaluations on the Diabetes classification setting (disaggregating by race, sex, and age) when using AUC as the target metric. In the left column the RMSE is taken across all groups, while in the center it is only over large groups and on the right over small groups. Large and small are defined as the top and bottom half of all groups, respectively. ", "page_idx": 27}, {"type": "image", "img_path": "aTNT3FuVBG/tmp/120ab90bd73d0991cedf6140a239017090fabc5cc826f95331c274ca560a2e1d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 11: Single-task evaluation on the Common Voice ASR setting (bottom, disaggregating by sex and age) when using CER as the target metric. In the left column the MAE is taken across all groups, while in the center it is only over large groups and on the right over small groups. Large and small are defined as the top and bottom half of all groups, respectively. ", "page_idx": 27}, {"type": "image", "img_path": "aTNT3FuVBG/tmp/3caf2cabe66be9eb26374d49effd34455099c8f1286f191432395e56a2065dfa.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 12: Multi-task evaluations on state-level ACS data (disaggregating by race, sex, and age) when using AUC as the target metric. On the left is the performance across different subsampling rates while on the right we show (multiplicative) performance improvement over the naive estimator on different tasks at subsampling rate 0.1. ", "page_idx": 27}, {"type": "image", "img_path": "aTNT3FuVBG/tmp/9ff0a67ce39bf09ee822a47ee5d01d64d5297a038063cad6bca0b76127061c1d.jpg", "img_caption": ["Figure 13: Multi-task evaluations on Common Voice clusters (disaggregating by sex and age) when using CER as the target metric. On the left is the performance across different subsampling rates while on the right we show (multiplicative) performance improvement over the naive estimator on different tasks at subsampling rate 0.1. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "aTNT3FuVBG/tmp/d739a54d14caa1a2492ef102d789c795fe4061ee3bc7cd3541a31e7c1e202d4c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "aTNT3FuVBG/tmp/a46e70eaf3c96515cb3608c8bea0e87a347a1ca1b255f9a07e5a96943d20a081.jpg", "img_caption": ["Figure 14: Single-task evaluations on the Diabetes classification setting (top, disaggregating by race, sex, and age), the Adult in-context classification setting (middle, disaggregating by race, sex, and age), and the Common Voice ASR setting (bottom, disaggregating by sex and age); these are the same evaluations as Figure 2 except with RMSE instead of MAE as the performance measure. In the left column the RMSE is taken across all groups, while in the center it is only over large groups and on the right over small groups. Large and small are defined as the top and bottom half of all groups, respectively. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "aTNT3FuVBG/tmp/602c9a8b86f09ffca6ed5f93d0c484a6783d89add72bc8caf62a5d9062615040.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "", "img_caption": ["Figure 15: Multi-task evaluations on state-level ACS data (top, disaggregating by race, sex, and age) and Common Voice clusters (bottom, disaggregating by sex and age); these plots visualize the same evaluations as Figure 3 except they use RMSE instead of MAE as the performance measure. On the left is the performance across different subsampling rates while on the right we show (multiplicative) performance improvement over the naive estimator on different tasks at subsampling rate 0.1. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: our abstract and introduction both describe the introduction of a new method for disaggregated evaluation, which we do. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: c.f. Sections 3.3 and 5.3. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: c.f. Appendix C.2. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: c.f. Appendix D Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: https://github.com/mkhodak/SureMap Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: c.f. Section 5, Appendix D, and the linked code. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: all figures except scatter plots have $95\\%$ confidence intervals. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: c.f. Appendix F.2. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: we have reviewed the Code of Ethics and our work conforms. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: c.f. Section 6. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 32}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The one asset released is just evaluations of an open-source model on opensource data and so unlikely to be misused. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: c.f. Appendix F.1. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: we release new datasets in conjunction with three of the tasks described in Section 4; see that section and Appendix F.1 for further details. This data is provided at the code link. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]