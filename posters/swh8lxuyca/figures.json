[{"figure_path": "Swh8LxuycA/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of contrastive goal-conditioned learning for text. Pictured is a prompt with a preferred and dispreferred response. Both source state tokens (ten) for the positive and negative trajectory are sampled from the preferred response. For illustrative purposes, the positve and negative source states are sampled as the same token, but in practice they can be different. The positive goal state is sampled as some future token (subtract) from the preferred response, and the negative goal state is sampled from any token (add) from the dispreferred response. The corresponding representations are retrieved from the last hidden state of the reward model. The training objective is then to maximize and minimize the similarity of the positive and negative representation pairs, respectively.", "description": "This figure illustrates the contrastive goal-conditioned learning approach used to train reward models. It shows how the model's hidden state representations are used to increase similarity between preferred future states and decrease similarity between dispreferred future states. The goal is to improve the reward model's ability to distinguish between good and bad responses.", "section": "3 Method"}, {"figure_path": "Swh8LxuycA/figures/figures_5_1.jpg", "caption": "Figure 2: AUROC scores comparing the baseline Codellama 7b Reward vs. our proposed method Q-Function 7b Reward on the rewards attributed to the base-model greedy generations across several math benchmarks.", "description": "This figure compares the Area Under the Receiver Operating Characteristic curve (AUROC) scores for two reward models: the baseline Codellama 7b Reward model and the proposed Q-Function 7b Reward model.  The AUROC scores are calculated for each model's ability to distinguish between correct and incorrect solutions generated by a base model across multiple mathematical reasoning benchmarks (GSM8k, MATH, algebra222, GSM-Hard, Asdiv, mawps, and svamp). The results show that the Q-Function 7b Reward model consistently outperforms the baseline Codellama 7b Reward model on all benchmarks, indicating that the proposed contrastive goal-conditioned training improves the reward model's ability to evaluate the quality of generated solutions.", "section": "4.1 Mathematical Reasoning with Code"}, {"figure_path": "Swh8LxuycA/figures/figures_6_1.jpg", "caption": "Figure 3: AUROC scores on the rewards attributed to partial base-model generations across 50 samples on GSM8k and MATH. The error bars depict the 95% confidence intervals (with sample size n = 50) at each percentile of generation considered. The Q-Function reward model has incremental increase in performance with more information, whereas, the traditional reward model's performance is a lot more varied in attributing intermediate rewards.", "description": "This figure compares the Area Under the Receiver Operating Characteristic curve (AUROC) scores for two different reward models (Codellama 7b Reward and Q-Function 7b Reward) when evaluating partial generations on the GSM8K and MATH datasets.  The x-axis represents the percentage of the generated sequence considered, while the y-axis shows the AUROC score. The error bars represent the 95% confidence interval, indicating the variability in the results.  The plot demonstrates that the Q-Function reward model exhibits a consistent and incremental improvement in AUROC as more of the generated sequence is considered, while the Codellama 7b Reward model shows more variability.  This suggests that the Q-Function model's learned representations better capture the likelihood of achieving a goal state at intermediate steps.", "section": "4.1.2 Reward Modeling Evaluations"}, {"figure_path": "Swh8LxuycA/figures/figures_7_1.jpg", "caption": "Figure 2: AUROC scores comparing the baseline Codellama 7b Reward vs. our proposed method Q-Function 7b Reward on the rewards attributed to the base-model greedy generations across several math benchmarks.", "description": "This figure compares the Area Under the Receiver Operating Characteristic curve (AUROC) scores for two reward models: the baseline Codellama 7b Reward and the proposed Q-Function 7b Reward. The AUROC scores are calculated based on the rewards assigned to greedy generations of a base model across multiple mathematical reasoning benchmarks.  The Q-Function model shows significant improvements in AUROC compared to the baseline model across various benchmarks, indicating better performance in distinguishing correct from incorrect solutions.", "section": "4.1 Mathematical Reasoning with Code"}, {"figure_path": "Swh8LxuycA/figures/figures_21_1.jpg", "caption": "Figure 3: AUROC scores on the rewards attributed to partial base-model generations across 50 samples on GSM8k and MATH. The error bars depict the 95% confidence intervals (with sample size n = 50) at each percentile of generation considered. The Q-Function reward model has incremental increase in performance with more information, whereas, the traditional reward model's performance is a lot more varied in attributing intermediate rewards.", "description": "The figure compares the Area Under the Receiver Operating Characteristic curve (AUROC) scores for two reward models (Q-Function 7b Reward and Codellama 7b Reward) when evaluating partial generations (i.e., only a certain percentage of the generated sequence is considered).  The x-axis represents the percentage of the generated sequence considered, and the y-axis represents the AUROC score.  The Q-Function model shows a consistent improvement in AUROC as more of the sequence is considered, while the performance of the Codellama model fluctuates more. Error bars represent 95% confidence intervals.", "section": "4.1.2 Reward Modeling Evaluations"}, {"figure_path": "Swh8LxuycA/figures/figures_23_1.jpg", "caption": "Figure 2: AUROC scores comparing the baseline Codellama 7b Reward vs. our proposed method Q-Function 7b Reward on the rewards attributed to the base-model greedy generations across several math benchmarks.", "description": "This figure compares the performance of two reward models (Codellama 7b Reward and Q-Function 7b Reward) on several math benchmarks. The AUROC (Area Under the Receiver Operating Characteristic curve) scores are shown for each benchmark.  The Q-Function 7b Reward model, which is the proposed model in the paper, consistently outperforms the baseline Codellama 7b Reward model across all benchmarks.", "section": "4.1 Mathematical Reasoning with Code"}, {"figure_path": "Swh8LxuycA/figures/figures_24_1.jpg", "caption": "Figure 2: AUROC scores comparing the baseline Codellama 7b Reward vs. our proposed method Q-Function 7b Reward on the rewards attributed to the base-model greedy generations across several math benchmarks.", "description": "The figure presents a bar chart comparing the Area Under the Receiver Operating Characteristic curve (AUROC) scores of two reward models: the baseline Codellama 7b Reward and the proposed Q-Function 7b Reward. The AUROC scores are shown for several math benchmarks, including GSM8K, MATH, algebra222, GSM-Hard, Asdiv, mawps, and svamp. The chart visually demonstrates the improvement in performance achieved by the proposed Q-Function 7b Reward model across these benchmarks.", "section": "4.1 Mathematical Reasoning with Code"}, {"figure_path": "Swh8LxuycA/figures/figures_24_2.jpg", "caption": "Figure 2: AUROC scores comparing the baseline Codellama 7b Reward vs. our proposed method Q-Function 7b Reward on the rewards attributed to the base-model greedy generations across several math benchmarks.", "description": "The figure compares the Area Under the Receiver Operating Characteristic curve (AUROC) scores for two reward models on several mathematical reasoning benchmarks.  The baseline model is Codellama 7b Reward, while the proposed model is Q-Function 7b Reward.  The AUROC scores represent how well each model can distinguish between correct and incorrect solutions generated by a base language model. The higher the AUROC score, the better the performance. The benchmarks shown include GSM8K, MATH, algebra222, GSM-Hard, Asdiv, mawps, and svamp, illustrating the performance across both in-distribution and out-of-distribution datasets. The results show consistent improvements across all benchmarks using the Q-Function 7b Reward method.", "section": "4.1 Mathematical Reasoning with Code"}, {"figure_path": "Swh8LxuycA/figures/figures_25_1.jpg", "caption": "Figure 2: AUROC scores comparing the baseline Codellama 7b Reward vs. our proposed method Q-Function 7b Reward on the rewards attributed to the base-model greedy generations across several math benchmarks.", "description": "This figure compares the Area Under the Receiver Operating Characteristic curve (AUROC) scores for two reward models: the baseline Codellama 7b Reward model and the proposed Q-Function 7b Reward model.  The AUROC scores are calculated on the rewards assigned by each model to the greedy generations from a base language model across multiple math benchmarks (GSM8k, MATH, algebra222, GSM-Hard, Asdiv, mawps, and svamp). The results show that the proposed Q-Function 7b Reward model consistently achieves higher AUROC scores than the baseline, indicating an improved ability to distinguish between correct and incorrect solutions.", "section": "4.1 Mathematical Reasoning with Code"}, {"figure_path": "Swh8LxuycA/figures/figures_25_2.jpg", "caption": "Figure 2: AUROC scores comparing the baseline Codellama 7b Reward vs. our proposed method Q-Function 7b Reward on the rewards attributed to the base-model greedy generations across several math benchmarks.", "description": "This figure compares the Area Under the Receiver Operating Characteristic curve (AUROC) scores of two reward models on various mathematical reasoning benchmarks.  The baseline model is Codellama 7b Reward, and the proposed model is Q-Function 7b Reward.  The AUROC scores reflect the models' ability to distinguish between correct and incorrect solutions generated by a base language model. The comparison demonstrates the improved performance of the proposed Q-Function 7b Reward model across several benchmarks, indicating its effectiveness in improving the quality of generated responses.", "section": "4.1 Mathematical Reasoning with Code"}, {"figure_path": "Swh8LxuycA/figures/figures_26_1.jpg", "caption": "Figure 2: AUROC scores comparing the baseline Codellama 7b Reward vs. our proposed method Q-Function 7b Reward on the rewards attributed to the base-model greedy generations across several math benchmarks.", "description": "This figure compares the Area Under the Receiver Operating Characteristic curve (AUROC) scores for two reward models: the baseline Codellama 7b Reward model and the proposed Q-Function 7b Reward model.  The AUROC scores are calculated based on the rewards assigned by each model to the greedy generations from a base model. The comparison is done across multiple mathematical reasoning benchmarks to evaluate the performance of the two models in distinguishing correct from incorrect solutions.", "section": "4.1 Mathematical Reasoning with Code"}, {"figure_path": "Swh8LxuycA/figures/figures_26_2.jpg", "caption": "Figure 2: AUROC scores comparing the baseline Codellama 7b Reward vs. our proposed method Q-Function 7b Reward on the rewards attributed to the base-model greedy generations across several math benchmarks.", "description": "This figure compares the Area Under the Receiver Operating Characteristic curve (AUROC) scores of two reward models on several mathematical reasoning benchmarks. The baseline model is Codellama 7b Reward, while the proposed model is Q-Function 7b Reward. The AUROC scores reflect the models' ability to distinguish between correct and incorrect solutions generated by a base language model.  The results show the improved performance of the Q-Function 7b Reward model across the benchmarks, demonstrating its effectiveness in identifying correct solutions.", "section": "4.1 Mathematical Reasoning with Code"}, {"figure_path": "Swh8LxuycA/figures/figures_27_1.jpg", "caption": "Figure 3: AUROC scores on the rewards attributed to partial base-model generations across 50 samples on GSM8k and MATH. The error bars depict the 95% confidence intervals (with sample size n = 50) at each percentile of generation considered. The Q-Function reward model has incremental increase in performance with more information, whereas, the traditional reward model's performance is a lot more varied in attributing intermediate rewards.", "description": "This figure compares the Area Under the Receiver Operating Characteristic curve (AUROC) scores for two different reward models when evaluating partial generations (i.e., only a portion of the generated sequence is considered).  The x-axis represents the percentage of the generated sequence considered for scoring, ranging from 0% to 100%. The y-axis shows the AUROC scores. Two reward models are compared: the baseline Codellama 7b Reward model and the proposed Q-Function 7b Reward model. Error bars indicate the 95% confidence intervals. The results demonstrate that the Q-Function model consistently achieves higher AUROC scores and shows a more stable and incremental improvement as more of the sequence is provided. In contrast, the baseline model exhibits greater variability in its performance and does not demonstrate as consistent an improvement.", "section": "4.1.2 Reward Modeling Evaluations"}, {"figure_path": "Swh8LxuycA/figures/figures_29_1.jpg", "caption": "Figure 2: AUROC scores comparing the baseline Codellama 7b Reward vs. our proposed method Q-Function 7b Reward on the rewards attributed to the base-model greedy generations across several math benchmarks.", "description": "The figure shows the Area Under the Receiver Operating Characteristic curve (AUROC) scores for two different reward models: the baseline Codellama 7b Reward and the proposed Q-Function 7b Reward model. The AUROC scores are calculated for several mathematical reasoning benchmarks, comparing the performance of both models in assigning rewards to the greedy generation from a base model.", "section": "4.1 Mathematical Reasoning with Code"}, {"figure_path": "Swh8LxuycA/figures/figures_30_1.jpg", "caption": "Figure 2: AUROC scores comparing the baseline Codellama 7b Reward vs. our proposed method Q-Function 7b Reward on the rewards attributed to the base-model greedy generations across several math benchmarks.", "description": "This figure compares the performance of two reward models, Codellama 7b Reward (baseline) and Q-Function 7b Reward (proposed), on several mathematical reasoning benchmarks. The AUROC (Area Under the Receiver Operating Characteristic curve) score is used to evaluate the models' ability to distinguish between correct and incorrect solutions generated by a base model.  The results indicate that the Q-Function 7b Reward model, which incorporates a contrastive goal-conditioned training approach, achieves higher AUROC scores compared to the baseline model across various benchmarks.", "section": "4.1 Mathematical Reasoning with Code"}]