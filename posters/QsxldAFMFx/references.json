{"references": [{"fullname_first_author": "Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-06-01", "reason": "This paper introduces the GPT-2 architecture used as a base model in the Rocket League experiments, showcasing its significance in the study."}, {"fullname_first_author": "McIlroy-Young", "paper_title": "Learning models of individual behavior in chess", "publication_date": "2022-07-01", "reason": "This paper is a direct extension of prior work on chess behavioral cloning and stylometry; the current work builds upon and expands its methodology."}, {"fullname_first_author": "Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2022-07-01", "reason": "This paper introduces the Low Rank Adapters (LoRA) method, crucial for the parameter-efficient fine-tuning employed in the study for scalability and efficiency."}, {"fullname_first_author": "Caccia", "paper_title": "Multi-head adapter routing for data-efficient fine-tuning", "publication_date": "2022-11-01", "reason": "This paper introduces the Multi-Head Adapter Routing (MHR) technique, a key component of the proposed methodology that enables soft parameter sharing across tasks (players)."}, {"fullname_first_author": "Ponti", "paper_title": "Modeling language variation and universals: A survey on typological linguistics for natural language processing", "publication_date": "2019-07-01", "reason": "This paper provides the theoretical background for the use of Polytropon, the parameter-efficient fine-tuning technique that forms the foundation of the multi-task learning approach."}]}