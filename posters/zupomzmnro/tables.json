[{"figure_path": "ZupoMzMNrO/tables/tables_6_1.jpg", "caption": "Table 1: Accelerating image generation on ImageNet for the DiT model family.", "description": "This table presents the results of accelerating image generation on the ImageNet dataset using three different DiT models (DiT-XL/2 256x256, DiT-XL/2 512x512, and DiT-L/2 256x256).  The table compares the performance of the proposed Learning-to-Cache (L2C) method against DDIM and shows metrics such as Number of Function Evaluations (NFE), Multiply-Accumulate operations (MACs), latency, speedup, Inception Score (IS), Fr\u00e9chet Inception Distance (FID),  and Recall. The results demonstrate that L2C significantly improves the efficiency of image generation compared to DDIM while maintaining or improving image quality.", "section": "4.2 Main Results"}, {"figure_path": "ZupoMzMNrO/tables/tables_6_2.jpg", "caption": "Table 2: Results with U-ViT-H/2 on ImageNet dataset. The resolution here is 256\u00d7256. We adopt the DPM-Solver-2, which has 2 function evaluations per step. The total NFE (instead of steps) is reported below. Guidance strength is set to 0.4.", "description": "This table shows the results of experiments using the U-ViT-H/2 model on the ImageNet dataset with a resolution of 256x256.  The DPM-Solver-2 sampling method was used, which performs two function evaluations per step.  The table compares different numbers of function evaluations (NFEs) for the baseline DPM-Solver and the proposed Learning-to-Cache (L2C) method, reporting metrics such as MACs (multiply-accumulate operations), latency, speedup, and FID (Fr\u00e9chet Inception Distance). Guidance strength was set to 0.4.", "section": "4 Experiments"}, {"figure_path": "ZupoMzMNrO/tables/tables_7_1.jpg", "caption": "Table 3: Comparison with other cache-based method on U-ViT with different steps.", "description": "This table compares the proposed Learning-to-Cache method with other cache-based methods (DeepCache and Faster Diffusion) on the U-ViT model.  It shows the number of function evaluations (NFE), latency, speedup relative to DPM-Solver, and FID (Fr\u00e9chet Inception Distance) for each method.  Lower FID indicates better image quality, and higher speedup indicates faster inference.", "section": "4 Experiments"}, {"figure_path": "ZupoMzMNrO/tables/tables_7_2.jpg", "caption": "Table 4: Maximum cacheable layers for DiT and U-ViT with different steps.", "description": "This table shows the maximum percentage of layers that can be cached in DiT-XL/2 and U-ViT-H/2 models without significantly impacting image quality, for different numbers of sampling steps (NFE).  It breaks down the cacheable layers into feed-forward (FFN) and multi-head self-attention (MHSA) layers separately. The high percentage of cacheable layers, especially in U-ViT-H/2, demonstrates the effectiveness of the proposed Learning-to-Cache (L2C) method in identifying redundant computations across timesteps.", "section": "4 Experiments"}, {"figure_path": "ZupoMzMNrO/tables/tables_8_1.jpg", "caption": "Table 7: Comparison with Layer Dropout", "description": "This table compares the performance of three different methods for accelerating diffusion transformers on the U-ViT model: Random Drop, Learning-to-Drop, and Learning-to-Cache.  The methods are evaluated based on the number of layers removed, latency, speedup factor, Inception Score (IS), Frechet Inception Distance (FID),  sFID, precision, and recall. The results demonstrate that the Learning-to-Cache method outperforms the other two methods across various metrics, showcasing its effectiveness in improving the speed and performance of diffusion transformer models without significant loss of image quality.", "section": "4.3 Analysis"}, {"figure_path": "ZupoMzMNrO/tables/tables_15_1.jpg", "caption": "Table 6: DPM-Solver with and without Shifted Cache Steps. Here we cache all the layers.", "description": "This table compares the performance of DPM-Solver with and without shifted cache steps.  The \"Cache\" row shows results when caching is applied, and the \"Cache - shifted\" row demonstrates the impact of shifting the cache steps to improve derivative accuracy.  Note that all layers are cached in both scenarios. The metrics evaluated are NFE (number of function evaluations), latency (in seconds), speedup (relative to the base DPM-Solver), Inception Score (IS), Frechet Inception Distance (FID),  sFID, Precision, and Recall.", "section": "B Additional Experiments"}, {"figure_path": "ZupoMzMNrO/tables/tables_16_1.jpg", "caption": "Table 7: Comparison with Layer Dropout", "description": "This table compares the performance of Learning-to-Cache with other layer dropout methods on the U-ViT model.  It shows the latency, speedup, Inception Score (IS), Frechet Inception Distance (FID), and other metrics for different methods with varying layer removal ratios.", "section": "4.3 Analysis"}, {"figure_path": "ZupoMzMNrO/tables/tables_16_2.jpg", "caption": "Table 8: \u03bb and \u03b8 for training the router", "description": "This table shows the hyperparameters \u03bb and \u03b8 used for training the router in different model configurations.  \u03bb is a regularization parameter controlling the sparsity of the router, and \u03b8 is a threshold used during inference to discretize the router's output.  The table specifies these values for various diffusion transformer models (DiT-XL/2, DiT-L/2, U-ViT-H/2) with varying numbers of function evaluations (NFEs) and resolutions.  The training cost in hours is also listed for each configuration.", "section": "4.1 Experimental Setup"}, {"figure_path": "ZupoMzMNrO/tables/tables_16_3.jpg", "caption": "Table 9: Performance with different \u03bb. Threshold \u03b8 is set to 0.1.", "description": "This table shows the impact of the hyperparameter \u03bb on the performance of the Learning-to-Cache method.  Different values of \u03bb lead to varying numbers of cached layers (Remove Ratio), resulting in different inference latencies, speedups, and image quality metrics (IS, FID, sFID, Precision, Recall).  The threshold \u03b8, which determines whether a layer is cached or not, is fixed at 0.1 for all experiments in this table.  Lower \u03bb values result in fewer layers being cached and slower inference but potentially better image quality.", "section": "4.3 Analysis"}]