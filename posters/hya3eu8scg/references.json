{"references": [{"fullname_first_author": "Julius Adebayo", "paper_title": "Sanity checks for saliency maps", "publication_date": "2018", "reason": "This paper is foundational for the work in this paper because it discusses sanity checks for saliency maps, a crucial concept that is further investigated in this paper."}, {"fullname_first_author": "Marco Ancona", "paper_title": "Explaining deep neural networks with a polynomial time algorithm for shapley value approximation", "publication_date": "2019", "reason": "This paper is important due to its discussion of explaining deep neural networks and its use of the Shapley value approximation, which is relevant to the methods investigated in this paper."}, {"fullname_first_author": "Christopher Anders", "paper_title": "Fairwashing explanations with off-manifold detergent", "publication_date": "2020", "reason": "This paper is important because it discusses the manipulation of explanations, a key concern that is addressed in this paper."}, {"fullname_first_author": "Sebastian Bach", "paper_title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "publication_date": "2015", "reason": "This paper is highly relevant to this paper due to its focus on pixel-wise explanations for non-linear classifier decisions, a central aspect of the current research."}, {"fullname_first_author": "Amirata Ghorbani", "paper_title": "Interpretation of neural networks is fragile", "publication_date": "2019", "reason": "This paper is important because it highlights the fragility of interpretations of neural networks, a concern that is directly addressed by the methods developed in this paper."}]}