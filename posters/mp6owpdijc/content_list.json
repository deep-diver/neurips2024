[{"type": "text", "text": "Autonomous Agents for Collaborative Task under Information Asymmetry ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wei Liu\u22c6\u2020 Chenxi Wang\u22c6\u2020 Yifei Wang\u22c6 Zihao Xie\u22c6 Rennai Qiu\u22c6 Yufan Dang\u22c6 Zhuoyun $\\mathbf{D}\\mathbf{u}^{\\star}$ Weize Chen\u22c6 Cheng Yang\u2663 Chen Qian\u22c6 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "\u22c6Tsinghua University \u2663Peng Cheng Laboratory, China thinkwee2767@gmail.com albertyang33@gmail.com qianc62@gmail.com ", "page_idx": 0}, {"type": "text", "text": "https://thinkwee.top/iagents/ ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Model Multi-Agent Systems (LLM-MAS) have greatly progressed in solving complex tasks. It communicates among agents within the system to collaboratively solve tasks, under the premise of shared information. However, when agents\u2019 collaborations are leveraged to perform multi-person tasks, a new challenge arises due to information asymmetry, since each agent can only access the information of its human user. Previous MAS struggle to complete tasks under this condition. To address this, we propose a new MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems. In iAgents, the human social network is mirrored in the agent network, where agents proactively exchange human information necessary for task resolution, thereby overcoming information asymmetry. iAgents employs a novel agent reasoning mechanism, InfoNav, to navigate agents\u2019 communication towards effective information exchange. Together with InfoNav, iAgents organizes human information in a mixed memory to provide agents with accurate and comprehensive information for exchange. Additionally, we introduce InformativeBench, the first benchmark tailored for evaluating LLM agents\u2019 task-solving ability under information asymmetry. Experimental results show that iAgents can collaborate within a social network of 140 individuals and 588 relationships, autonomously communicate over 30 turns, and retrieve information from nearly 70,000 messages to complete tasks within 3 minutes1. ", "page_idx": 0}, {"type": "text", "text": "\u201cA friend is someone with whom there is mutual understanding, emotional support, and shared experiences.\u201d ", "page_idx": 0}, {"type": "text", "text": "Joey\u2019s and Chandler\u2019s agents discuss the word \"friend\", after experiencing the whole Friends season one story. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "There has been notable progress in autonomous agents driven by the Large Language Model (LLM), especially in developing communicative agents for completing collaborative tasks [53, 45, 12, 38, 16, 43, 26, 63, 36], as shown in Figure 1a. In these multi-agent systems (MAS), multiple agents are created through role-play prompting [25] to imitate the ability of human experts and form a virtual entity (e.g., an agent company or hospital) to provide solutions derived from agents\u2019 communication. Agents share context in the virtual entity to facilitate collective decision-making. ", "page_idx": 0}, {"type": "text", "text": "Since autonomous communication among agents has achieved significant success in discussing, decomposing, and resolving various complex tasks, the natural idea is to upgrade the tasks for agents ", "page_idx": 0}, {"type": "image", "img_path": "mp6OWpDIJC/tmp/6f069f9fb08e01c2913e879c59ec6b8b43f467827f14a8411a1ab2f0cff22f71.jpg", "img_caption": ["a) Previous system forms one virtual entity to serve user. b) iAgents mirrors the social network. Agents work on behalf each user. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Comparison between previous MAS (left) and iAgents (right). The visibility range of information for each agent is highlighted with a colored background. On the left, all agents share all information (colored background of Virtual Company). On the right, each agent could only see information about its human user (separated colored backgrounds), and iAgents is designed to deal with such kind of information asymmetry. ", "page_idx": 1}, {"type": "text", "text": "from single-person to multi-person, where agents work on behalf of multiple human users and solve the collaboration task among these users. An intuitive solution is to assign each user an agent and perform autonomous collaborations among these agents. However, in such a setting, a new challenge arises. This challenge involves dealing with asymmetry [46, 44] in various types of information (environment, goals, and mind state) [66, 65, 34, 4, 54, 7] since each agent can only observe the information of its human user. Previous LLM-MAS are not suitable for handling this scenario, because 1) human information is sensitive and private, so the asymmetry can not be resolved by directly collecting all information into one place and sharing it as the context for MAS. 2) Human information is dynamic so it can not be easily memorized during pre-training and activated accurately through role-play prompting in MAS to avoid asymmetry. Essentially, agents\u2019 cooperation in previous MAS has adopted an introspective approach within the virtual entity (an agent hospital/town/software company), which struggles to deal with asymmetry in human information. ", "page_idx": 1}, {"type": "text", "text": "To bridge gaps in such asymmetry, agents need to retrieve information from humans and proactively exchange information, creating a new ecosystem combining the human and the agent network. Therefore, we propose the concept of iAgents (Informative Multi-Agent Systems) for achieving this kind of collaboration, as shown in Figure 1b. iAgents utilizes a new agent reasoning method (InfoNav) to model the agents\u2019 minds and navigate communication among agents toward proactive information exchange. Furthermore, a new memory mechanism is designed to provide agents with accurate and comprehensive information for exchange. Additionally, we introduced InformativeBench, the first benchmark evaluating agents\u2019 collaboration ability under information asymmetry. It includes both information-seeking tasks within large social networks and algorithm-like reasoning tasks over a small network. Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We raise the research problem of information asymmetry in the multi-agent system for enhancing human collaboration, which is the first to shift the research perspective in this area from a holistic system view to individuals within the system. It gives a new vision to the human-agent collaboration relationship. ", "page_idx": 1}, {"type": "text", "text": "2. We propose the iAgents framework to deal with the information asymmetry in a multi-agent system. Equipped with InfoNav and improved memory mechanism, iAgents could perform effective communication and collaboration within a social network (shown in Figure 6) of 140 individuals and 588 relationships, and across over 30 dialogues they searched nearly 70,000 messages and resolved the task within 3 minutes.   \n3. We introduce the first multi-agent information asymmetry benchmark, InformativeBench. Agents with some state-of-the-art LLM backends achieved an average accuracy of $50.48\\%$ on InformativeBench, with the most challenging task achieving only $22.8\\%$ accuracy, which reveals both potential promise and challenges in this direction. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Agents based on Large Language Models (LLMs) Originate from ancient Greek philosophy, where an \"agent\" denoted a being capable of intentional action, driven by mental states such as desires and beliefs [40]. As AI progresses, this concept integrates into the simulation and understanding of intelligent behavior [50]. Traditionally, agent research focused on some specific tasks [18, 23, 42], but the emergence of LLMs has shifted this focus. GPT-4, for instance, is recognized for achieving a form of general intelligence [4], prompting exploration into equipping LLMs with agency and intrinsic motivation [53]. Studies introduce frameworks for LLM-based agents, including memorisation [62], decision-making, perception, and action modules, leveraging the inherent autonomy, reactivity, proactiveness, and social ability of LLMs [48]. Notable applications include utilizing single-agent systems and multi-agent systems for task solving and simulation [38, 16, 43, 26, 63, 6, 31, 37]. However, most of this research has primarily focused on agent capabilities, often neglecting interaction and cooperation paradigms. This highlights a critical research gap [3], warranting further exploration in this area. ", "page_idx": 2}, {"type": "text", "text": "Human-Agent and Multi-Agent Cooperation Paradigms To ensure that agents align with human objectives [22, 32], human-agent cooperation is crucial. Two main paradigms of human-agent cooperation are the Equal-Partnership and the Instructor-Executor. The former emphasizes agents as communicators who understand human emotions [13], while the latter highlights the human\u2019s guiding role, with agents following instructions [10]. However, single-agent systems face limitations, such as the inability to collaborate, learn from social interactions, and function effectively in complex scenarios [41, 28, 53]. Research suggests that multi-agent systems with each agent holding specific functions, can stimulate stronger intelligence [30, 1, 39]. Collaborative multi-agent systems can efficiently handle complex tasks [29, 27, 9]. Recent studies focus on scenarios with information asymmetry among agents. For instance, [66] explores social intelligence among agents achieving private goals based on common scene information. Additionally, [65] develops an evaluation framework to simulate social interactions with LLMs. The study finds that learning from omniscient simulations enhances interaction naturalness but doesn\u2019t improve goal achievement in cooperative scenarios. Many real-world scenarios involve information asymmetry, posing challenges for multiagent systems. ", "page_idx": 2}, {"type": "text", "text": "Reasoning In previous research, agents are tasked with providing accurate information to human users in human-machine collaboration. Due to the nature of language models, the output of information relies on the user\u2019s input and the previously decoded content serving as rationale context. Therefore, some work on reasoning has explored how to improve the organization and expression of this rationale context [47, 58, 2, 8], to enhance the accuracy of output information. However, some research has also found that the reasoning process of LLMs differs from that of humans [56, 20, 5, 33]. For questions relying on internal knowledge within LLMs to answer, agents do not necessarily solve problems step by step like humans. Instead, compared to steps, having context with sufficient information content is more important. For scenarios discussed in this paper, we focus on machine-to-machine communication, relying on external knowledge of LLMs to collaboratively answer questions. This poses different requirements for LLM reasoning, especially in terms of how to promote information flow so sufficient information is included in the context provided to LLM. Agents need to actively [17] and accurately acquire, provide, and ask for information. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Without loss of generality, we formalize tasks in social networks that require information exchange for collaboration as a Question Answer (QA) task. The rationales $R$ necessary for answering the question $Q$ are distributed in different human information $(I_{1},I_{2})$ across the social network, which leads to information asymmetry. Consequently, agents $(A_{1},A_{2})$ of two individuals are required to collaborate, update the rationale set $(R_{1},R_{2})$ that they hold through communication $C$ , and by combining their rationales, they can reason and obtain the answer. The whole process can be formulated as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{A n s=R e a s o n i n g(Q,R)}}\\\\ {{R=R_{1}\\cup R_{2}}}\\\\ {{R_{1},R_{2}=C(I_{1},I_{2},A_{1},A_{2})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "mp6OWpDIJC/tmp/1f94db6b2f7e0c01c8c2d68090e604770a637526b0dfbbffadca620d5203abc3.jpg", "img_caption": ["Human Social Network Humans invoke agents Agents u6lize InfoNav to drive collabora6on Draw conclusion "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Overall architecture of iAgents. From left to right, 1) each individual in the social network is equipped with an agent, and 2) two human users invoke their agents to solve a task, each initially holding the information that is visible to its human user. Then 3) agents automatically raise communication and exchange necessary information on behalf of human users. Finally, 4) agents perform a consensus check on their planning completed by InfoNav to solve the task. ", "page_idx": 3}, {"type": "text", "text": "3.2 Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As shown in Figure 2, agents need to actively retrieve information from humans and exchange it with other agents. The communication can be represented as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nC_{n}=\\{U_{1},U_{2},...,U_{n}\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $U$ denotes an utterance in the communication $C$ , and $n$ is the maximum number of communication turns. Agents take turns making utterances to advance towards task resolution. Following the classical definition [59, 53], where agents observe the environment, think to make decisions, and then take action, we can organize agents\u2019 communication similarly. Each agent\u2019s behavior in one communication turn involves a pipeline of 1) observing the current communication progress $C$ and their held rationales $R,2$ ) thinking about how to update the rationale to $R^{n e w}$ and what query to make for retrieving information from humans, and 3) acting by retrieving information and making an utterance based on it. This pipeline can be formalized as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nU_{i}=\\left\\{\\begin{array}{l l}{{A c t_{A_{1}}(T h i n k_{A_{1}}(O b s_{A_{1}}^{i}))}}&{{i\\mathcal{V}_{0}2==1}}\\\\ {{}}&{{}}\\\\ {{A c t_{A_{2}}(T h i n k_{A_{2}}(O b s_{A_{2}}^{i}))}}&{{e l s e}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{O b s_{A}^{\\iota}=\\{R,C_{i-1}\\}}}\\\\ {{T h i n k_{A}(O b s_{A}^{i})=\\{q u e r y,R^{n e w}\\}}}\\\\ {{A c t_{A}(T h i n k_{A})=A(q u e r y(I))=U}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To ensure each generated utterance provides valuable information and eliminates asymmetry, how to exchange information and what information to exchange is crucial. To deal with these two questions, we use the InfoNav mechanism to guide communication towards effective information exchange. Furthermore, we introduce the Mixed Memory mechanism which organizes human information into Fuzzy and Clear Memory for accurate and comprehensive retrieval. Additionally, each agent can initiate new communication $C^{n e w}$ within their subnetwork, which means the communication $C$ may be recursive and can diffuse among the social network: ", "page_idx": 3}, {"type": "equation", "text": "$$\nC_{n}=\\{C_{1}^{n e w},C_{2}^{n e w},...,C_{m}^{n e w},U_{1},U_{2},...,U_{n}\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For example, if Alice\u2019s agent wants to collaborate with Bob\u2019s agent, Bob\u2019s agent might respond \"Hold on, I can ask Charlie\u2019s agent for help.\" ", "page_idx": 3}, {"type": "text", "text": "3.3 InfoNav ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As shown in Equation 6, the agent needs to be aware of its rationale set and ongoing communication to effectively advance the conversation. While the status of the rationale set can be implicitly inferred from utterances, this inference is often unreliable for LLM Agents. This unreliability can lead to incorrect states and then generate meaningless utterances, such as repetitive questioning or redundant thanking, which makes it harder to infer rationale and creates a vicious cycle. To address this, we propose the InfoNav mechanism. InfoNav plans and tracks the status of the agent\u2019s rationale set explicitly for better navigating the communication. Before each utterance, the agent reviews its plan to identify which unknown rationale to inquire about and then updates the plan based on the responses received. Figure 3 shows an example of InfoNav in action. Initially, we prompt the agent to generate a plan $P$ outlining the rationales needed to answer question $Q$ . Since the agent has no information at the beginning, all rationales in the plan are marked as unknown: ", "page_idx": 3}, {"type": "image", "img_path": "mp6OWpDIJC/tmp/ac81a395f151d77121f9e440075926716fb9025367d6767be15f00357fc8aea1.jpg", "img_caption": ["Figure 3: A case of the task asking two agents to find the longest activity among all schedules. InfoNav navigates the communication by providing a plan to the agent. It first 1) asks the agent to make a plan on what information is needed, then 2) fills the placeholder in this plan during communication. Finally it 3) performs a consensus check on the completed plan to 4) get the answer. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nP(r_{1}^{u},...,r_{m}^{u})=P r o m p t(Q)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $r^{u}$ denotes unknown rationales. During communication, if the agent gets the information of one rationale, it will update the status of this rationale from \u201cunknown\u201d to \u201cknown\u201d and fill this information into the rationale placeholder in the planning text. The plan is written in fluent natural language, making it explicit and effective for prompting the model. Therefore, using InfoNav, the rationale set $R$ in equation 6 is rewritten to plan $P$ , and the updated rationale $R^{n e w}$ is replaced to the plan with filled rationales $P(r^{k})$ , where $r^{k}$ represents known rationales: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{O b s_{A}=\\{P(r^{u}),C\\}}\\\\ &{P(r^{k})=T h i n k_{A}(O b s_{A})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "After multiple turns of communication, both sides finish the update of their plans. Agents then unify collected rationales and discard conflicting ones to reach an answer, denoted as \u201cConsensus Reasoning\u201d. Thus, equations 1 to 3 rewrite to: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{A n s=R e a s o n i n g(Q,R)}}\\\\ {{R=C o n s e n s u s(P_{1}(R_{1}),P_{2}(R_{2}))}}\\\\ {{P_{1}(R_{1}),P_{2}(R_{2})=C(I_{1},I_{2},A_{1},A_{2})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Previous reasoning methods[47, 58, 2, 8] focused on providing accurate plans. In contrast, InfoNav emphasizes navigating communication and information exchange with plans. The plan in InfoNav can be seen as a generalization of Dialogue Status Tracking (DST)[15, 51, 14] in conventional task-oriented dialogue systems. It also generalizes the concept of software in multi-agent software generation frameworks like ChatDev[38] or MetaGPT [16]. The plan maintains progress in tasksolving, guiding agents to share information during communication. ", "page_idx": 4}, {"type": "text", "text": "3.4 Mixed Memory ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In iAgents, agents are navigated by InfoNav to retrieve human information and share it with other agents for collaboration. Retrieval of human information is necessary since 1) human\u2019s lifelong information can not be stored in the \u201clong context\u201d (such as $128\\mathbf{k}$ tokens) of LLM, and 2) even though the information required for a single-turn conversation can fit into the context, the accumulation of information over multiple turns can lead to context explosion. It is also challenging to organize human information which is diverse in format and complex to understand. We propose organizing human information into two types of agent memories: Clear Memory and Fuzzy Memory. These memories facilitate reactive retrieval, ensuring accurate and comprehensive rationale extraction, as shown in Figure 2. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Clear Memory $(M e m_{C})$ stores information in a structured format to facilitate precise retrieval. Clear memory faithfully preserves the original information $(I)$ and supports accurate retrieval. Additionally, it enables information retrieval from multiple spans across different chat sessions (s), capturing evolving changes in rationales. ", "page_idx": 5}, {"type": "text", "text": "However, Clear Memory\u2019s strict exact-match requirements complicate the retrieval process. It also struggles to provide cohesive context. To address these issues, we introduce Fuzzy Memory $(M e m_{F})$ . Fuzzy memory stores summarized session texts $(I_{s})$ and uses embedding-based ANN retrieval [21]. Although both fuzzy memory and reflection [31] produce summary-like text, we emphasize objective summarization of information to facilitate session-level retrieval, rather than subjective generalizations to aid in planning. While this approach may lose some details, it offers a comprehensive context and enables robust, semantic-based retrieval. Therefore, the retrieval action in Equation 8 can be rewritten to involve both memory types: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{A c t_{A_{k}}(T h i n k_{A_{k}})=A_{k}(q u e r y_{k}(I_{k}))}&{{}}\\\\ {=A_{k}(S Q L(M e m_{C}),A N N(M e m_{F}))}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "What\u2019s more, the query of these two kinds of memories is decided by agents based on observations of previous executions, which means agents can reactively adjust their queries. Combining these two kinds of memory facilitates agents to cross-verify the retrieved information and provides InfoNav with comprehensive and accurate rationales. ", "page_idx": 5}, {"type": "text", "text": "4 InformativeBench ", "text_level": 1, "page_idx": 5}, {"type": "image", "img_path": "mp6OWpDIJC/tmp/444ad7600e9b14aef2a4d1830fb9fcd0d9e3da67272db56c07b6fbd82de37574.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 4: Two kinds of tasks in the InformativeBench. Each agent can only see the information (marked with different colors) of the human that it works on behalf of, which generates information asymmetry. Agents are 1) asked to find the needle information within the network or 2) reason to get an answer which is the output of an algorithm running on distributed information in the network. ", "page_idx": 5}, {"type": "text", "text": "To the best of our knowledge, there is no benchmark or dataset tailored for information asymmetry in the collaboration task among communicative agents. In this paper, we construct InformativeBench, the first benchmark to evaluate agent collaboration tasks featuring information asymmetry in social networks. It includes two categories with a total of five datasets. Details, including the scale, distribution, and metrics of the datasets, are provided in section C. What\u2019s more, recent studies have found that LLM continuously ingests internet data so static benchmarks can be easily memorized and overftited [64, 55, 61]. Hence, two pipelines for constructing InformativeBench are easy to realize and can be generalized to more domains for constant and dynamic evaluations. They are Needle-Oriented and Reasoning-Oriented pipelines, as shown in Figure 4. ", "page_idx": 5}, {"type": "text", "text": "Needle-Oriented Pipeline A \"needle\"[11] is inserted into the social network, and agents are tasked with finding this \"needle\" information. This evaluates their ability to share and locate information. The dataset can be created by splitting the needle and spreading it into the network, or by collecting pieces from the network and combining them. For the split method, the Needle in the Persona (NP) ", "page_idx": 5}, {"type": "text", "text": "dataset modifies the dialogue in the SPC dataset[19] by adding a common or opposite persona to two individuals\u2019 personas. Agents are asked to find this persona. For the combination method, the Friend$s T V$ dataset reconstructs the social network from the entire Season 1 script of Friends [49], involving 140 characters with 588 relationships, and combines two questions in the FriendsQA dataset [57, 24] as \"needle pieces\" to generate new question. This dataset, the largest in InformativeBench, features sarcasm, plot twists, and complex relationships for simulating real-world challenges. ", "page_idx": 6}, {"type": "text", "text": "Reasoning-Oriented Pipeline Humans are assigned different pieces of information, which serve as inputs for an algorithm (such as sorting or merging). Agents must reason to get the answer which is the algorithm\u2019s output. Therefore, the algorithm serves as an automatic verifier for information asymmetric reasoning. In InformativeBench, this is represented by the Schedule dataset, which develops a program for assigning different schedules to individuals. Agents are presented with algorithmic problems of varying difficulties, and the program automatically verifies the correctness of their solutions. The datasets include questions of three levels of difficulty: Easy) calculate the number of conflicting schedules between two people, Medium) find the longest activity among six people, and Hard) find the longest common free period among six people. ", "page_idx": 6}, {"type": "text", "text": "5 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We generically treat chat histories as human information. This approach simplifies modeling information asymmetry in social networks. Other types of information, such as knowledge bases, documents, or web content, can all be organized in mixed memory so iAgents is adaptable to all these kinds of information. We conduct all experiments with a maximum of 10 communication turns for agents. The experiments use gpt-4-0125-preview, gpt-3.5-turbo-16k, gemini-1.0-pro-latest, and claude-sonnet 2 as LLM backends. The temperature is set to 0.2. For Fuzzy Memory, we use gpt-4-0125-preview to summarize session text and OpenAI text-embedding-3-small to generate embeddings for ANN embedding search. We use precision as the metric for questions in the NP, ScheduleEasy, and FriendsTV datasets. For the ScheduleMedium and ScheduleHard datasets, we use F1 and IoU as the metrics, corresponding to the algorithm used. Details about the metrics are shown in Section C.3. For the Schedule and NP datasets, we do not activate mixed memory since the information scale is small and can be fully loaded in the LLM context. Additionally, for the Schedule dataset, we do not activate the agent\u2019s ability to initiate new communication due to the small scale of the social network. ", "page_idx": 6}, {"type": "text", "text": "6 Result ", "text_level": 1, "page_idx": 6}, {"type": "table", "img_path": "mp6OWpDIJC/tmp/95661099e96896da63e73e82340dbdaec2ee127d9918f54fb4c36c646ad9a2c3.jpg", "table_caption": ["6.1 InformativeBench Evaluations ", "Table 1: Evaluation results of iAgents on InformativeBench with different LLM backends. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "We first comprehensively assessed the performance of iAgents using some state-of-the-art LLMs on InformativeBench, as shown in Table 1. GPT-4 achieves over $50\\%$ accuracy across most datasets, indicating its potential to work on behalf of humans for cooperation. However, smaller-scale LLMs still face significant challenges in solving cooperation problems in information asymmetry. Most models could only achieve about $50\\%$ precision on the easiest NP task. For the Schedule dataset, as questions become harder, performance drops, with most models solving less than $20\\%$ of the hardest questions. The FriendsTV dataset introduces a large social network, requiring agents to use external memory to retrieve rationale from extensive human information. Most LLMs struggle to exceed $40\\%$ accuracy in this dataset. Thus, while previous studies show impressive performance when agents are omniscient, collaborating in information asymmetry remains challenging. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "mp6OWpDIJC/tmp/86a473c48e728ca27b9d0b080411417eb68477ec20a42fc5ba0648e00db34cbf.jpg", "table_caption": ["6.2 Ablation Study "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: Ablation study on iAgents. Dashes $(-)$ indicate: (1) iAgents on Reasoning-Oriented dataset does not equip other mechanisms, hence no ablation needed; (2) For NP dataset, iAgents does not utilize Mixed Memory hence there is no ablation. ", "page_idx": 7}, {"type": "text", "text": "We conducted ablation experiments on several key designs of the iAgents framework, as detailed in Table 2. Analyzing the FriendsTV dataset revealed that incorporation of the mixed memory mechanism led to a performance increase ranging from $2.38\\%$ to $6.34\\%$ , surpassing the impact of InfoNav, which resulted in only a $0.8\\%$ performance increase. This discrepancy underscores the greater significance of effective retrieval over reasoning during communication in large social networks with mass information. Notably, the ablation of both memory mechanisms emphasized the indispensability of mixed memory. The introduction of recursive communication exhibited the most significant performance gain $(12.7\\%)$ , primarily due to the challenges posed by the vast social network in the FriendsTV dataset. By actively introducing new communications within ongoing dialogues, agents could acquire and corroborate information, thus significantly enhancing performance. This highlights the imperative of scalability in our proposed framework for addressing real-world problems. ", "page_idx": 7}, {"type": "text", "text": "For the NP and Schedule datasets, the main challenge lies in facilitating effective multi-turn communication to exchange information for reasoning. Therefore, InfoNav emerged as pivotal in enhancing performance, resulting in performance increases ranging from $15\\%$ to $26\\%$ . When agents relied solely on initialized prompts to navigate multi-turn communication, they struggled to exchange information effectively to accomplish tasks. This deficiency was particularly evident in datasets like Schedule, which emphasize logical reasoning and computation. Across all difficulty levels, agents without the InfoNav mechanism failed to achieve accuracy exceeding $10\\%$ . ", "page_idx": 7}, {"type": "text", "text": "6.3 Analysis on Agents\u2019 Behaviour ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "InfoNav Behaviour We examined how agents utilize InfoNav for information exchange during multi-turn communication. Notably, we calculated the average number of unknown rationales solved each time InfoNav updated the plan and the proportion of rationales passed in consensus reasoning. Moreover, some rationales were solved in a \"Fake Solved\" hallucination, where agents filled in the rationale as \"solved, which is unknown\". We also documented the frequency of such occurrences. Table 3 shows that agents who propose fewer rationales to seek and achieve a higher solved ratio are more likely to accomplish the task. Interestingly, agents often fill multiple rationales concurrently rather than sequentially. Those agents with higher instances of synchronous completions suggest a deeper understanding of the task and greater confidence in filling rationales. Furthermore, the occurrence of Fake Solved instances is lower among agents who predict tasks correctly. The consensus ratio is also higher when agents successfully complete the task. It denotes that the information obtained by the two collaborating agents is relatively accurate and free of contradictions, thus increasing the likelihood of arriving at the correct conclusion through their final reasoning. Besides, we observed that agents not only propose rationales but also task states, such as the completion status of specific actions. The completion rates of these rationales and states are positively correlated with task success. In essence, the utilization of InfoNav by agents mirrors human intuition, emphasizing first careful planning, then proactive and accurate information exchange. ", "page_idx": 7}, {"type": "table", "img_path": "mp6OWpDIJC/tmp/3eafdad9e1ac2bdd8c9dda48007c57318fead01a36c3df81fc4374ae750aad2b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: Analysis InfoNav behaviour on the trajectory of iAgents using GPT4 as backend. When agents successfully complete the task, the static collected from their trajectory proves that they better utilize the InfoNav mechanism, since the rationale solved ratio, synchronous completions of rationales, and consensus ratio are higher, and present fewer fake solved hallucinations. ", "page_idx": 8}, {"type": "image", "img_path": "mp6OWpDIJC/tmp/ad91c28a9a5eceee93241c9e56a6852ed9d6e3938463d8c135927d5cc8c67060.jpg", "img_caption": ["Figure 5: The figure depicts the distribution of different behaviors of agents in adjusting memory retrieval based on the progress of communication. Agents predominantly tend to maintain parameters unchanged, but when changes occur, they tend to increase parameters to gain more information. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Memory Behaviour Similarly, we explored how agents adapt their memory retrieval strategies during communication. We examined three parameters in clear memory queries: the context window, which determines the breadth of contextual messages; the total message retrieval limit; and the size of the query keywords set. For fuzzy memory, we analyzed two parameters: the number of queried responses (topk) and the length of the query text. These findings are illustrated in Figure 5. Our analysis revealed several notable trends. The majority of agents do not change their behavior during communication. However, when agents decide to change their behavior, we observed that they tended to increase the amount of retrieved information over time. This augmentation trend was particularly pronounced on the overall message retrieval limit, where the frequency of \u201cincrease\u201d actions surpassed that of \u201cdecrease\u201d actions by nearly threefold. Furthermore, agents who completed tasks exhibited a more conservative approach, with a lower proportion of behavioral changes compared to agents unable to complete tasks. This phenomenon may be attributed to the difficulty of certain tasks, making agents continuously refine their strategies in pursuit of the required information. ", "page_idx": 8}, {"type": "text", "text": "6.4 Analysis on Real World Concern ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We studied two significant challenges in extending the iAgents to real-world applications. Firstly, we investigated whether the agent can effectively respond to human input without being overly influenced by factual knowledge obtained during pre-training [52, 35]. Secondly, we explored the agent\u2019s ability to engage in communication while upholding human privacy. Our experiments were conducted using the GPT3.5 model on the FriendsTV dataset. ", "page_idx": 8}, {"type": "text", "text": "Prior Distraction The FriendsTV contains information that could be memorized by LLM from the Internet, hence it is perfect for analyzing prior distractions. We anonymized the names of the primary characters in the dataset, for example, renaming \"Rachel\" to \"Alice\". The performance of the agents on this anonymized dataset decreased from $35.71\\%$ to $32.54\\%$ , suggesting that to some extent, agents can reason based on user-provided information rather than solely relying on knowledge memorized in pre-training. It may need further advancements, such as model unlearning [60], to fully address this issue. ", "page_idx": 8}, {"type": "text", "text": "Privacy Concern In investigating whether agents can communicate without compromising privacy, we conducted an experiment involving modifications to the agent\u2019s system prompt, emphasizing the importance of privacy preservation in utterances. The agent then utilized vague expressions such as \"somebody/somewhere\" and disclosed only relevant entity information. This adjustment led to a performance drop from $35.71\\%$ to $30.95\\%$ , indicating the ongoing challenge of achieving collaboration while ensuring privacy. It\u2019s important to note that we solely adjusted privacy settings on the output side, rather than restricting agent access to human information on the input side. This decision was made because setting access permissions might inadvertently reveal prior taskrelated information. Thus, the real challenge lies in appropriately regulating access to information based on task requirements, akin to teaching the agent to retrieve necessary information accurately. Additionally, absolute privacy protection is impractical, as absolute privacy protection amounts to forgoing problem-solving through collaboration. ", "page_idx": 9}, {"type": "text", "text": "7 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "While the iAgents framework introduces innovative multi-agent collaboration, it has several limitations and challenges. Privacy Issues: As discussed in Section 6.4, we examined the performance of agents communicating and collaborating under privacy constraints, highlighting the trade-off between privacy and collaboration. We define three privacy levels. L1: Users fully share personal information, allowing maximum efficiency for iAgents. L2: Users keep their personal information private. iAgents can handle this situation by deploying an edge-side small language model agent for information acquisition. L3: Users demand maximal privacy, with both personal and agent communication handled locally on private devices, which is still a challenge for small language model agents. Network Modeling: The current framework initiates communications based on user relevance but lacks nuanced modeling of human social networks and collaboration history. Enhancing network topology through added or removed nodes and incorporating past interactions could improve communication efficiency. Human-Agent Interaction: Although iAgents target full autonomy, human involvement for verification remains necessary in real-world scenarios. Processes must be designed to prompt user feedback and adjust agent strategies accordingly, ensuring alignment with user preferences. Cost: The high input token consumption (about 30,000 tokens per task) required for iAgents to handle human information is a challenge. However, advancements in long-context models, which extend input token length, present opportunities to reduce the cost of scaling iAgents. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper revisits the ecological role of agents within human society, where agents act on behalf of humans in communication to complete collaborative tasks. A primary focus lies in addressing the challenge of information asymmetry. We introduce a novel paradigm for designing multi-agent systems, termed iAgents, for addressing information asymmetry. Furthermore, we introduce a benchmark to evaluate the agents\u2019 collaboration ability under information asymmetry thoroughly. Going forward, we aim to confront several key challenges to successfully implement this system in the real world for augmenting human productivity, including deploying lightweight models at the edge to address privacy concerns and devising new Human-Computer Interaction paradigms for autonomous and controllable communication among agents, etc. iAgents does not role-play to replace human experts but consistently attributes the value of information to humans and we believe it can facilitate the productivity of human society within a secure and controllable framework. ", "page_idx": 9}, {"type": "text", "text": "9 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work was supported by the Postdoctoral Fellowship Program of CPSF under Grant Number GZB20230348 and the Tencent Rhino-Bird Focused Research Program. We wish to express our profound appreciation to Professor Zhiyuan Liu and Professor Maosong Sun from the Department of Computer Science at Tsinghua University for their detailed guidance and critical insights, which have been instrumental to the success of this work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] P. G. Balaji and D. Srinivasan. An Introduction to Multi-Agent Systems, pages 1\u201327. Springer Berlin Heidelberg, Berlin, Heidelberg, 2010.   \n[2] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17682\u201317690, 2024.   \n[3] Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research capabilities of large language models. arXiv preprint arXiv:2304.05332, 2023.   \n[4] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.   \n[5] Changyu Chen, Xiting Wang, Ting-En Lin, Ang Lv, Yuchuan Wu, Xin Gao, Ji-Rong Wen, Rui Yan, and Yongbin Li. Masked thought: Simply masking partial reasoning steps can improve mathematical reasoning learning of language models. arXiv preprint arXiv:2403.02178, 2024.   \n[6] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In The Twelfth International Conference on Learning Representations, 2024.   \n[7] Weize Chen, Ziming You, Ran Li, Yitong Guan, Chen Qian, Chenyang Zhao, Cheng Yang, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Internet of agents: Weaving a web of heterogeneous agents for collaborative intelligence. arXiv preprint arXiv:2407.07061, 2024.   \n[8] Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose triangle for thought generation. arXiv preprint arXiv:2311.04254, 2023.   \n[9] Zhuoyun Du, Chen Qian, Wei Liu, Zihao Xie, Yifei Wang, Yufan Dang, Weize Chen, and Cheng Yang. Multi-agent software development through cross-team collaboration. arXiv preprint arXiv:2406.08979, 2024.   \n[10] Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou. Assistgpt: A general multi-modal assistant that can plan, execute, inspect, and learn. CoRR, abs/2306.08640, 2023.   \n[11] Kamradt Greg. Llmtest_needleinahaystack. https://github.com/gkamradt/LLMTest_ NeedleInAHaystack, 2023. Needle In A Haystack - Pressure Testing LLMs.   \n[12] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: A survey of progress and challenges. arXiv preprint arXiv:2402.01680, 2024.   \n[13] Masum Hasan, Cengiz Ozel, Sammy Potter, and Ehsan Hoque. Sapien: Affective virtual agents powered by large language models\\*. In 2023 11th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW). IEEE, September 2023.   \n[14] Michael Heck, Carel van Niekerk, Nurul Lubis, Christian Geishauser, Hsien-Chin Lin, Marco Moresi, and Milica Ga\u0161ic. Trippy: A triple copy strategy for value independent neural dialog state tracking. In 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, page 35, 2020.   \n[15] Matthew Henderson, Blaise Thomson, and Steve Young. Word-based dialog state tracking with recurrent neural networks. In Proceedings of the 15th annual meeting of the special interest group on discourse and dialogue (SIGDIAL), pages 292\u2013299, 2014.   \n[16] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.   \n[17] Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan Luu, Junxian He, Pang Wei Koh, and Bryan Hooi. Uncertainty of thoughts: Uncertainty-aware planning enhances information seeking in large language models. arXiv preprint arXiv:2402.03271, 2024.   \n[18] Francois F. Ingrand, Michael P. Georgeff, and Anand S. Rao. An architecture for real-time reasoning and system control. IEEE Expert: Intelligent Systems and Their Applications, 7(6):34\u201344, dec 1992.   \n[19] Pegah Jandaghi, XiangHai Sheng, Xinyi Bai, Jay Pujara, and Hakim Sidahmed. Faithful persona-based conversational dataset generation with large language models. arXiv preprint arXiv:2312.10007, 2023.   \n[20] Mingyu Jin, Qinkai Yu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, et al. The impact of reasoning step length on large language models. arXiv preprint arXiv:2401.04925, 2024.   \n[21] Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535\u2013547, 2019.   \n[22] Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, and Geoffrey Irving. Alignment of language agents. CoRR, abs/2103.14659, 2021.   \n[23] Douglas B Lenat. Enabling agents to work together. Communications of the ACM, 37(7):126\u2013 142, 1994.   \n[24] Changmao Li and Jinho D Choi. Transformers to learn hierarchical contexts in multiparty dialogue for span-based question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5709\u20135714, 2020.   \n[25] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for\" mind\" exploration of large language model society. Advances in Neural Information Processing Systems, 36, 2024.   \n[26] Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, and Yang Liu. Agent hospital: A simulacrum of hospital with evolvable medical agents. arXiv preprint arXiv:2405.02957, 2024.   \n[27] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023.   \n[28] Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Diyi Yang, and Soroush Vosoughi. Training socially aligned language models on simulated social interactions. In The Twelfth International Conference on Learning Representations, 2023.   \n[29] Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large language models. arXiv preprint arXiv:2307.04738, 2023.   \n[30] Marvin Minsky. Society of mind. Simon and Schuster, 1988.   \n[31] Joon Sung Park, Joseph O\u2019Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1\u201322, 2023.   \n[32] Metty Paul, Leandros Maglaras, Mohamed Amine Ferrag, and Iman Almomani. Digitization of healthcare sector: A study on privacy and security concerns. ICT Express, 9(4):571\u2013588, 2023.   \n[33] Jacob Pfau, William Merrill, and Samuel R Bowman. Let\u2019s think dot by dot: Hidden computation in transformer language models. arXiv preprint arXiv:2404.15758, 2024.   \n[34] David Premack and Guy Woodruff. Does the chimpanzee have a theory of mind? Behavioral and brain sciences, 1(4):515\u2013526, 1978. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[35] Siya Qi, Yulan He, and Zheng Yuan. Can we catch the elephant? the evolvement of hallucination evaluation on natural language generation: A survey. arXiv preprint arXiv:2404.12041, 2024. ", "page_idx": 12}, {"type": "text", "text": "[36] Chen Qian, Yufan Dang, Jiahao Li, Wei Liu, Zihao Xie, YiFei Wang, Weize Chen, Cheng Yang, Xin Cong, Xiaoyin Che, Zhiyuan Liu, and Maosong Sun. Experiential co-learning of software-developing agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5628\u20135640, Bangkok, Thailand, August 2024. Association for Computational Linguistics.   \n[37] Chen Qian, Jiahao Li, Yufan Dang, Wei Liu, YiFei Wang, Zihao Xie, Weize Chen, Cheng Yang, Yingli Zhang, Zhiyuan Liu, et al. Iterative experience refinement of software-developing agents. arXiv preprint arXiv:2405.04219, 2024.   \n[38] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. In The 62nd Annual Meeting of the Association for Computational Linguistics, 2024.   \n[39] Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Scaling large-language-model-based multi-agent collaboration. arXiv preprint arXiv:2406.07155, 2024.   \n[40] Markus Schlosser. Agency. In Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Winter 2019 edition, 2019.   \n[41] Significant Gravitas. AutoGPT.   \n[42] Richard S Sutton, Andrew G Barto, et al. Reinforcement learning. Journal of Cognitive Neuroscience, 11(1):126\u2013134, 1999.   \n[43] Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. Medagents: Large language models as collaborators for zero-shot medical reasoning. arXiv preprint arXiv:2311.10537, 2023.   \n[44] Michael Tomasello. The cultural origins of human cognition. Harvard university press, 2009.   \n[45] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):1\u201326, 2024.   \n[46] Max Weber. Max Weber: Selections in Translation. Cambridge University Press, 1978.   \n[47] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 24824\u201324837. Curran Associates, Inc., 2022.   \n[48] Lilian Weng. Llm-powered autonomous agents. lilianweng.github.io, Jun 2023.   \n[49] Wikipedia contributors. Friends (tv series) \u2014 Wikipedia, the free encyclopedia, 2024.   \n[50] Michael Wooldridge and Nicholas R. Jennings. Intelligent agents: theory and practice. The Knowledge Engineering Review, 10(2):115\u2013152, 1995.   \n[51] Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming Xiong, Richard Socher, and Pascale Fung. Transferable multi-domain state generator for task-oriented dialogue systems. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 808\u2013819, 2019.   \n[52] Kevin Wu, Eric Wu, and James Zou. How faithful are rag models? quantifying the tug-of-war between rag and llms\u2019 internal prior. arXiv preprint arXiv:2404.10198, 2024.   \n[53] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. The rise and potential of large language model based agents: A survey. CoRR, abs/2309.07864, 2023.   \n[54] Hainiu Xu, Runcong Zhao, Lixing Zhu, Jinhua Du, and Yulan He. Opentom: A comprehensive benchmark for evaluating theory-of-mind reasoning capabilities of large language models. arXiv preprint arXiv:2402.06044, 2024.   \n[55] Ruijie Xu, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu. Benchmarking benchmark leakage in large language models. arXiv preprint arXiv:2404.18824, 2024.   \n[56] Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. Do large language models latently perform multi-hop reasoning? arXiv preprint arXiv:2402.16837, 2024.   \n[57] Zhengzhe Yang and Jinho D Choi. Friendsqa: Open-domain question answering on tv show transcripts. In Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue, pages 188\u2013197, 2019.   \n[58] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 11809\u201311822. Curran Associates, Inc., 2023.   \n[59] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023.   \n[60] Yuanshun Yao, Xiaojun Xu, and Yang Liu. Large language model unlearning. In Socially Responsible Language Modelling Research, 2023.   \n[61] Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, et al. A careful examination of large language model performance on grade school arithmetic. arXiv preprint arXiv:2405.00332, 2024.   \n[62] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A survey on the memory mechanism of large language model based agents. arXiv preprint arXiv:2404.13501, 2024.   \n[63] Zhiling Zheng, Oufan Zhang, Christian Borgs, Jennifer T Chayes, and Omar M Yaghi. Chatgpt chemistry assistant for text mining and the prediction of mof synthesis. Journal of the American Chemical Society, 145(32):18048\u201318062, 2023.   \n[64] Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. Don\u2019t make your llm an evaluation benchmark cheater. CoRR, abs/2311.01964, 2023.   \n[65] Xuhui Zhou, Zhe Su, Tiwalayo Eisape, Hyunwoo Kim, and Maarten Sap. Is this the real life? is this just fantasy? the misleading success of simulating social interactions with llms. arXiv preprint arXiv:2403.05020, 2024.   \n[66] Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, LouisPhilippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, and Maarten Sap. SOTOPIA: Interactive evaluation for social intelligence in language agents. In The Twelfth International Conference on Learning Representations, 2024. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "mp6OWpDIJC/tmp/f899b51bad7faf27f6c9490d6a8d52c16b818e47acf1f89a46e7f733e61ab1c3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 6: The visualization of social network in FriendsTV dataset. The connection of the six main characters is labeled with different colors. ", "page_idx": 14}, {"type": "text", "text": "Figure 6 illustrates the social network reconstructed from the plot of the entire first season of the Friends TV series in the FriendsTV dataset. Friends revolves around six main characters: Ross, Rachel, Monica, Joey, Phoebe, and Chandler. In the social network, these six protagonists are the important nodes with the most connections. Edges connecting them are displayed in different colors. The entire social network comprises 140 nodes and 588 edges, with an average node degree of 4.243, a network diameter of 6, and an average path length of 2.189. Similar to real-world social networks, this network is highly sparse. Many characters may appear in the same scene without interacting with each other, resulting in a network density of 0.061, which brings challenges for resolving information asymmetry. ", "page_idx": 14}, {"type": "text", "text": "B Notations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 4 presents a comprehensive list of all symbol notations employed in this paper, encompassing those utilized in the formalized description of the methodology as well as in the ablation and analysis experiments. ", "page_idx": 14}, {"type": "text", "text": "C InformativeBench Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Question Distribution ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Figure 7 presents the distribution of problem types across the three datasets in InformativeBench. The majority of the questions in InformativeBench are of the \"What\" and \"Who\" types, which have objective ground truth and lack ambiguity. In the Schedule dataset, questions are categorized into three difficulty levels, with each difficulty level corresponding to a different type of question: \"What\", \"How Many\", and \"How Long\". ", "page_idx": 14}, {"type": "table", "img_path": "mp6OWpDIJC/tmp/7bf5705c20cb5d00258902ab6851f9b64116da33c68dacae84d6d072bc835728.jpg", "table_caption": ["Table 4: Main notations used in this paper. "], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "mp6OWpDIJC/tmp/81bfc74efdbe591d548258e79ccacd1b646208c687c0d0c3f338194b29c0f067.jpg", "img_caption": ["Figure 7: The distribution of question types in the InformativeBench. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "C.2 Question Sample ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "mp6OWpDIJC/tmp/21b7ea905d5ebeaca93bea71d5594fe1eb03e53c8a6a0d959d9ca048b6c7e1e4.jpg", "table_caption": ["Table 5: Question sample in the InformativeBench. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "1. Needle in the Persona. In a segment of multi-party casual conversation among Alice, Bob, Charlie, and Dave, \"needle information\" related to a fantasy series is inserted. Questions are then posed to Bob and Dave\u2019s agents to identify this needle information.   \n2. Schedule. Each person is assigned a daily schedule. Questions of varying difficulty require agents to collaborate to discuss overlapping schedules for two human users, the longest schedule among multiple human users, and common free time for multiple human users.   \n3. FriendsTV. Based on questions from the FriendsQA data, new questions are synthesized. For instance, there is a scene in the third act of the seventh episode of the first season of Friends where a blackout occurs. Agents need to combine the rationales and answers to the questions \"Where did the blackout happen?\" and \"Who was worried about grandmother being affected by the blackout?\" to locate this scene in the script of the first season and find the relevant characters. ", "page_idx": 16}, {"type": "text", "text": "C.3 Metrics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we outline the evaluation metrics for all datasets and how to automate the evaluation process. ", "page_idx": 16}, {"type": "text", "text": "1. Needle in the Persona. We use accuracy to evaluate the agents\u2019 performance on the Needle in the Persona dataset, defined as the number of correctly answered questions divided by the total number of questions. All questions are in the form of \"what,\" \"where,\" and \"who,\" hence the ground truth is objectively unique. However, due to potential variations in the expression of names, locations, or other nouns, and the possibility that the agent\u2019s response may be a complete sentence containing reasoning or additional information, it is impractical to determine correctness through exact matches. We utilize GPT-4 to judge whether the agent\u2019s prediction aligns with the ground truth. It is important to note that, unlike other methods using LLMs for evaluation, we do not rely on GPT-4\u2019s own knowledge to determine the correctness of answers since we have an objective ground truth. GPT-4 is merely used to assess whether the agent\u2019s prediction and the ground truth refer to the same entity. We also manually verify GPT-4\u2019s judgments to ensure they align perfectly with human evaluations. The GPT-4 evaluation prompt is as follows: You are an experienced human labeler for reading comprehension tasks. Given a ground truth answer and a model prediction, you have to judge whether the model prediction is correct. The question is {question}. The ground truth answer is {ground_truth}. The model prediction is {prediction}. Return 1 if the model prediction is correct else 0. the model prediction may be a little different on the expression, as long as the meaning or key entity is correct, the answer can be regarded as correct. ", "page_idx": 16}, {"type": "text", "text": "2. Schedule. In the Schedule dataset, we define metrics based on specific algorithmic problems rather than simple correctness judgments, providing more continuous metrics to evaluate agents\u2019 abilities in finer granularity. ", "page_idx": 16}, {"type": "text", "text": "(a) ScheduleEasy. Under this difficulty level, agents are required to determine the minimum number of activities to delete to resolve scheduling confilcts between two human users\u2019 calendars, returning a numerical value. We also use accuracy for evaluation but normalize the agent\u2019s response through regularization and GPT-4 prompting to extract the specific numerical value. The agent\u2019s response is considered correct only when the numerical value matches exactly.   \n(b) ScheduleMedium. Agents are tasked with identifying the longest activities in each person\u2019s schedule, and in dataset configurations, there are often multiple longest activities. Agents need to return all possible names of the longest activities. We employ the F1 score for evaluation, as agents may miss some activities or erroneously recall others. Additionally, we use GPT-4 prompting to normalize the agent\u2019s response, mapping activity names mentioned in their response to a uniform representation present in the entire set of activities.   \n(c) ScheduleHard. Agents are required to identify the free time slots for all individuals and enumerate them. To compare multiple time slots in the ground truth and agents\u2019 predictions, we analogize to the Intersection-over-Union (IoU) metric used in the computer vision domain for object detection, generalizing it to one-dimensional time slots. We calculate the ratio of the intersection duration between the predicted time ", "page_idx": 16}, {"type": "text", "text": "slots and the ground truth time slots to the union duration. For example, if the predicted time slot is from 9 AM to 12 PM and the ground truth time slot is from 10 AM to 2 PM, then IoU is the duration of 2 hours divided by the duration of 5 hours, resulting in 0.4. ", "page_idx": 17}, {"type": "text", "text": "3. FriendsTV. The metrics design for the FriendsTV dataset is identical to Needle in the Persona, employing accuracy as the evaluation metric, as they share the same question types. ", "page_idx": 17}, {"type": "text", "text": "C.4 Benchmark Statistic ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "mp6OWpDIJC/tmp/a2b36e8562da9d4daafd05a04082811a132575590e6422d9a8f3a1e068b6e7fe.jpg", "table_caption": ["Table 6: Statistic of InformativeBench. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 6 presents detailed statistics of five datasets in InformativeBench, including the number of question-answer pairs and the scale of social networks. We utilize the FriendsTV dataset to simulate real-world challenges, providing a large-scale social network to test agents\u2019 writing abilities. The other datasets simulate smaller social networks, focusing on enabling agents to exchange information to solve complex reasoning tasks. The difficulty for agents in collaborating within human social networks lies not only in the scale of the social network (information acquisition) but also in effective communication (information exchange). Therefore, we designed datasets of varying scales and difficulties to comprehensively evaluate agents. As the social networks in datasets other than FriendsTV are relatively simple with limited information, we did not enable the MixedMemory mechanism in experiments with these datasets. ", "page_idx": 17}, {"type": "text", "text": "D InformativeBench Pipeline ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Needle-Oriented ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1.1 Needle in the Persona Pipeline ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The SPC dataset [19] is a dialogue dataset based on LLM. Researchers construct and sample the personas of both individuals in the dialogue, then prompt the LLM to generate coherent conversations that are faithful with these personas. Here, a persona refers to the background information of a individual, such as experiences, interests, occupations, demographic attributes, etc. We use SplitNeedle pipeline to split a new persona into two sentences then add it to the dialogues in SPC dataset to construct Needle in the Persona dataset. An example is shown in Figure 8 and 9. ", "page_idx": 17}, {"type": "text", "text": "Specifically, we randomly select two sets of dialogues, involving four individuals, to construct a small social network. Additionally, we randomly choose a new persona as the needle information to be injected into two individuals\u2019 profile within this social network, and modify the dialogues to reflect this persona injection. Finally, we have the agents verify each other\u2019s dialogue information to identify the needle information. The detailed pipeline is as follows: ", "page_idx": 17}, {"type": "text", "text": "1. Two samples are selected from the original SPC dataset, comprising two sets of dialogues (e.g., Alice-Bob, Charlie-Dave).   \n2. A new shared persona is added for Alice and Dave, for example: (a) They both recently developed an interest in fishing. (b) Alternatively, contrasting personas can be added, such as Alice enjoys eating vegetables, while Dave strongly dislikes them.   \n3. Modify the dialogues of Alice-Bob and Charlie-Dave to reflect the addition of the new persona mentioned above. ", "page_idx": 17}, {"type": "image", "img_path": "mp6OWpDIJC/tmp/dbfc8067928e96b33343bb9d97499cac561ef6455b9b7a72baaafbad56fadfe5.jpg", "img_caption": ["Figure 8: Example of Needle in the Persona dataset, part 1. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "mp6OWpDIJC/tmp/197305ad6f6116ca632e46870470af3a1ef9ccb0b207767beef3289ad7cf55a1.jpg", "img_caption": ["Figure 9: Example of Needle in the Persona dataset, part 2. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "4. Have Bob and Charlie summon their respective agents and initiate an automatic communication to identify this persona. ", "page_idx": 20}, {"type": "text", "text": "D.1.2 FriendsTV Pipeline ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Similar to the Needle in the Persona dataset, the FriendsTV dataset also focuses on querying Needle Information injected within social networks. Derived from the transcripts of the first season of the TV show \"Friends\", the FriendsTV dataset generate a dialogues dataset among characters in TV series. If two characters engage in dialogue during the first season, they are considered friends, thus automatically constructing a large-scale social network. ", "page_idx": 20}, {"type": "text", "text": "Unlike the Needle in the Persona dataset, we do not split needle information in the FriendsTV dataset. Instead, we synthesize a new Needle information question and answer based on two questions from the FriendsQA [57] dataset. The context, questions, and answers were manually annotated through crowdsourcing by the authors of the original FriendsQA paper. This was a remarkable project that spanned several years. We are very grateful for the contributions of the authors of FriendsQA. The original Friends script is publicly available online and can be accessed through multiple channels (GitHub, Kaggle). An example of our FriendsTV dataset is illustrated in Figure 10. Specifically, ", "page_idx": 20}, {"type": "text", "text": "1. Annotate Script. We divide each episode script into scenes then query GPT4 to annotate the listener of each utterance. We manually check with GPT assistance to make sure the correctness of listener labeling. At last, we explodes the utterance into 1v1 dialogues (e.g a utterance from A, B to C, D would be exploded into four utterances between AC, AD, BC, BD). ", "page_idx": 20}, {"type": "text", "text": "2. Normalization. We norm the abbr name back to the full name (\"mon\" $->$ \"monica\") and replace pronoun (\"her father\" $->$ \"Rachel\u2019s father\"). Some utterances have multiple listeners that be labeled as \"A/B\", \"A&B\", \"A and B\", \"A, B\". We explode all these utterance with multiple utterances having single listener. The labeled listener \"ALL\" is replaced to all characters in this scene except. We also manually check and remove some notes that accidentally transformed into the scripts (such as some introduction on the speaker/listener)   \n3. Generate Needle Information. Two questions with the same context and different participants in the FriendsQA dataset are chosen. We prompt GPT4 to generate a new QA based on these two questions. The new question needs the answers of two original questions to reason and since the participants are different for original questions, the information hence is in asymmetry. ", "page_idx": 20}, {"type": "text", "text": "D.2 Reasoning-Oriented ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The Reasoning-Oriented pipeline leverages algorithmic problems to assess the cooperative ability of agents. Given an algorithm, such as divide and conquer, dynamic programming, or segment tree, we distribute the algorithm\u2019s input as information to different individuals in the social network. Then, we pose questions regarding the algorithm, hoping that agents can collaborate to produce the algorithm\u2019s output. It is worth noting that such pipelines are not intended to evaluate whether agents can collaborate to solve algorithms, as collaboration through natural language is certainly less efficient than utilizing algorithms directly on collected information. The algorithm serves as a perfect verifier on questions with distributed inputs. What\u2019s more, it is easy to utilize algorithm for differentiating tasks of varying difficulty to comprehensively evaluate agent capabilities. Additionally, algorithmic problems usually can provide continuous metrics, rather than just binary classification metrics of correctness, which detail the performance of agents. In InformativeBench, the algorithmic input information consists of the schedules of different individuals, where schedules can be formalized as multiple time intervals. Then, we examine agents\u2019 performance on algorithmic problems such as overlapping intervals, longest common intervals, etc. Specifically, ", "page_idx": 20}, {"type": "text", "text": "1. Generate Schedule ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "(a) Activity Pool Setting We established a pool of single-person activities and a pool of multi-person activities. In the former, each activity requires only one participant, and the only attribute of the activity is its duration. In the latter, each activity requires multiple participants, and the attributes of the activity include its duration and the required number of participants. Additionally, we set up a routine activity pool, which ", "page_idx": 20}, {"type": "image", "img_path": "mp6OWpDIJC/tmp/d4cef1f5918c194c1742de289f0f3ec3ca98cdded41c75586a70532540c98e06.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 10: Example of FriendsTV dataset. We show related dialogue, but agents do not have direct access to it and they have to retrieve these dialogue as context from memories about the whole Friends season 1 stories. ", "page_idx": 21}, {"type": "image", "img_path": "mp6OWpDIJC/tmp/5e1ad90843b1e7621d6d338609458a5303a5575a4523cb651b49d01c7c63db3b.jpg", "img_caption": ["Figure 11: Example of ScheduleHard dataset, part 1. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Example of ScheduleHard Dataset, part 1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Emily: I noticed you didn\u2019t mention your afternoon plans. I\u2019ll be attending a cooking class with Alice from 20:00 to 22:00.   \nDave: Sounds fun! I have a conference call scheduled at 18:00, but it seems we won\u2019t overlap there. Later in the evening, I plan to spend some time listening to music at 21:30.   \nEmily: That\u2019s a nice way to unwind. I\u2019ll be wrapping up my cooking class by then and heading to bed at 22:00.   \nDave: And I\u2019ll be joining you in the land of dreams at the same time, after my music session. Looks like we\u2019ll both be ending our day with a good night\u2019s sleep.   \nEmily: Indeed. It\u2019s good to know when we\u2019ll be busy and when we can potentially catch up. Have a great day tomorrow!   \nDave: You too! Let\u2019s make the most of it.   \nDave: Good morning! I hope you had a good sleep. I\u2019ll be sleeping until 6:00 today. Franklin: Good morning! I actually plan to sleep a bit longer, until 6:30.   \nDave: Sounds good. I have yoga at noon. What\u2019s your plan around that time?   \nFranklin: I\u2019ll be having lunch at 11:00. So, I guess we\u2019ll both be busy around noon. Dave: Right. I\u2019ll be having my lunch at 13:00. Do you have any plans after your lunch? Franklin: No specific plans after lunch for me. How about you?   \nDave: I have a conference call scheduled at 18:00. It\u2019s going to be quite a discussion. Franklin: I see. I\u2019ll make sure to give you some quiet space for your call. I don\u2019t have anything planned for the evening.   \nDave: Thanks! Later in the evening, I\u2019m planning to spend some time listening to music around 21:30. What will you be doing then?   \nFranklin: I\u2019ll be heading to bed around that time, 22:30 to be exact. So, we\u2019ll have some quiet time in the house.   \nDave: Got it. I\u2019ll also be heading to bed at 22:00, right before you. Let\u2019s make sure to have a peaceful night.   \nFranklin: Sounds like a plan. Let\u2019s make the most of tomorrow! ", "page_idx": 23}, {"type": "text", "text": ": Let\u2019s work on behalf of alice and dave to find out when all our friends can join together today and list all free time spans? ", "page_idx": 23}, {"type": "text", "text": ": Ok! First, let\u2019s . ", "page_idx": 23}, {"type": "text", "text": "{autonomous communication between and : Conclusion: \"9:00-11:00\", \"13:30-16:30\", \"19:30-20:00\". ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "includes activities such as having breakfast. In the routine activity pool, each activity has attributes including its duration and a range of allowable start times. ", "page_idx": 24}, {"type": "text", "text": "(b) Individual Attribute Setting To distinguish the schedule of each individual, we randomly set an activity preference vector for each individual. This vector consists of 0s and 1s, corresponding to all activities in the activity pool, indicating whether the individual is willing to participate in the activity. Additionally, for each individual, we set a time vector, which consists of 0s and 1s, indicating whether the time slot is occupied. The time vector is based on half-hour units and is 48-dimensional. ", "page_idx": 24}, {"type": "text", "text": "(c) Schedule Generation Process For each group of individuals participating in the experiment, multi-person activities are allocated first. The number of multi-person activities, n, is set based on the number of individuals in each experimental batch. Next, n activities are randomly selected from the multi-person activity pool. Based on each individual\u2019s activity preference vector, participants are assigned to each multi-person activity as needed. The activity is then updated in the schedule of the participants, and the corresponding position of the time vector is set to 1. Subsequently, routine activities are generated for each individual, and the corresponding position of the time vector is also set to 1. Finally, for each individual, we use two pointers to traverse all the free time in their schedule and arrange single-person activities for them based on their needs and activity preference vector. ", "page_idx": 24}, {"type": "text", "text": "2. Generate Dialogue We generate pairwise dialogues within groups of individuals with symmetric information, enabling them to become aware of the schedules of all other individuals within the group. Specifically, we prompt GPT-4 in generating dialogues that exchange schedule information. This process includes the following key requirements: 1) For multi-person activities that person1 and person2 both participate in, the dialogue can mention the names of the other participants in that activity. 2) For multi-person activities that only person1 or person2 participates in, the dialogue should not mention the names of the other participants in that activity. 3) The generated dialogue needs to follow a certain format, for example, when person1 speaks to person2, the format should be \u201cperson1 to person2: \u201d. Finally, the dialogue returned by GPT-4 is split into individual messages, and the sender, receiver, and message text of each message are written into the database for subsequent memory retrieval. ", "page_idx": 24}, {"type": "text", "text": "3. Generate Question ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "(a) Easy Agents are asked to calculate how many intervals can be deleted at least so there are no overlapping time intervals between two individuals. It requires agents to share and examine the schedule and then reason to calculate the number of overlapping intervals. ", "page_idx": 24}, {"type": "text", "text": "(b) Medium Agents are asked to find the longest schedules of all individuals in the network. It requires agents to traverse all intervals from the human schedules and exchange to find the longest one. ", "page_idx": 24}, {"type": "text", "text": "(c) Hard Agents need to find out all intervals that everybody is free. It needs to collect all intervals and reasons to find the intervals with no activity interval covers. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: the paper elaborates the limitations in section 7. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: the paper does not include theoretical results. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: the paper describes the steps to generate all datasets in section 4 and section D. The paper describes methods in section 3. The paper describes the experimental setup in section 5. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: the paper provides open access to the data and code on https://github.com/ thinkwee/iAgents, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: the paper does not include training. The paper describes all the hyperparameters setup in section 5. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: error bars are not reported. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: the paper discussed the token cost in section 7. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: the paper discuss both potential positive societal impacts and negative societal impacts. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: the paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: the paper cites all the papers related to our dataset. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: new assets introduced in the paper are well documented and the documentation is provided alongside the assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}]