[{"type": "text", "text": "Inductive biases of multi-task learning and finetuning: multiple regimes of feature reuse ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Samuel Lippl\u2217 ", "page_idx": 0}, {"type": "text", "text": "Jack Lindsey\u2217,\u2217\u2217 ", "page_idx": 0}, {"type": "text", "text": "Center for Theoretical Neuroscience Columbia University New York, NY samuel.lippl@columbia.edu ", "page_idx": 0}, {"type": "text", "text": "Anthropic San Francisco, CA jackwlindsey@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural networks are often trained on multiple tasks, either simultaneously (multitask learning, MTL) or sequentially (pretraining and subsequent finetuning, $\\mathrm{PT+FT}$ ). In particular, it is common practice to pretrain neural networks on a large auxiliary task before finetuning on a downstream task with fewer samples. Despite the prevalence of this approach, the inductive biases that arise from learning multiple tasks are poorly characterized. In this work, we address this gap. We describe novel implicit regularization penalties associated with MTL and $\\mathrm{PT+FT}$ in diagonal linear networks and single-hidden-layer ReLU networks. These penalties indicate that MTL and $\\mathrm{PT+FT}$ induce the network to reuse features in different ways. 1) Both MTL and $\\mathrm{PT+FT}$ exhibit biases towards feature reuse between tasks, and towards sparsity in the set of learned features. We show a \u201cconservation law\u201d that implies a direct tradeoff between these two biases. 2) $\\mathrm{PT+FT}$ exhibits a novel \u201cnested feature selection\u201d regime, not described by either the \u201clazy\u201d or \u201crich\u201d regimes identified in prior work, which biases it to rely on a sparse subset of the features learned during pretraining. This regime is much narrower for MTL. 3) $\\mathrm{PT+FT}$ (but not MTL) in ReLU networks beneftis from features that are correlated between the auxiliary and main task. We confirm these findings empirically with teacher-student models, and introduce a technique \u2013 weight rescaling following pretraining \u2013 that can elicit the nested feature selection regime. Finally, we validate our theory in deep neural networks trained on image classification. We find that weight rescaling improves performance when it causes models to display signatures of nested feature selection. Our results suggest that nested feature selection may be an important inductive bias for finetuning neural networks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural networks are often trained on multiple tasks, either simultaneously (\u201cmulti-task learning,\u201d henceforth MTL, see [1, 2]) or sequentially (\u201cpretraining\u201d and subsequent \u201cfinetuning,\u201d henceforth $\\mathrm{PT+FT}$ , see [3, 4]). Empirically, models can transfer knowledge from auxiliary tasks to improve performance on tasks of interest. However, theoretical understanding of how auxiliary tasks influence learning and generalization is limited. ", "page_idx": 0}, {"type": "text", "text": "Auxiliary tasks are especially useful when there is less data available for the target task. Modern \u201cfoundation models,\u201d trained on data-rich general-purpose auxiliary tasks (like next-word prediction or image generation) before adaptation to downstream tasks, are a timely example of this use case [5]. ", "page_idx": 0}, {"type": "text", "text": "Auxiliary tasks are also commonly used in reinforcement learning, where performance feedback can be scarce [6]. Intuitively, auxiliary task learning biases the target task solution to use representations shaped by the auxiliary task. When the tasks share common structure, this influence may enable generalization from relatively few training samples for the task of interest. However, it can also have downsides, causing a model to inherit undesirable biases from auxiliary task learning [7, 8]. ", "page_idx": 1}, {"type": "text", "text": "An influential strategy in the literature on the theory of single-task learning has been to characterize the implicit regularization conferred by the combination of network architecture and optimization algorithm [9\u201313]. Alternatively, some others characterize the effects of explicit parameter regularization (e.g. an $\\ell_{2}$ -penalty on weights) on the inductive bias of networks towards learning certain functions [14\u201316]. Compared to the single-task case, the inductive bias of MTL and $\\mathrm{PT+FT}$ , whether obtained via explicit regularization or implicit regularization induced by optimization dynamics, is less well understood. Here we make progress on this question by studying inductive biases of MTL and $\\mathrm{PT+FT}$ in two network architectures that have been extensively theoretically studied: diagonal linear networks, and densely connected networks with one hidden layer and a ReLU nonlinearity. We then demonstrate that our insights transfer to practically relevant scenarios by studying deep neural networks trained on image classification tasks. ", "page_idx": 1}, {"type": "text", "text": "Our specific contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We characterize regularization penalties associated with MTL or $\\mathrm{PT+FT}$ in both diagonal linear networks and single hidden-layer ReLU networks (Section 3).   \n\u2022 We find that both MTL and $\\mathrm{PT+FT}$ are biased towards solutions that reuse features between tasks, and that rely on a sparse set of features (Section 4.2).   \n\u2022 We then find that under suitable scalings, $\\mathrm{PT+FT}$ exhibits a \u201cnested feature selection\u201d regime, distinct from previously characterized \u201crich\u201d and \u201clazy\u201d regimes, which biases finetuning to extract sparse subsets of the features learned during pretraining (Sections 4.3 and 4.4).   \n\u2022 We find that $\\mathrm{PT+FT}$ in ReLU networks can benefit from correlated (not just identical) features between the main and auxiliary task, but only when coefficients of features in the main task weights are of comparable magnitude (Section 4.5).   \n\u2022 Finally, we study deep neural networks trained on natural image data (CIFAR-100 and ImageNet) (Section 5). Remarkably, we find that rescaling weights before finetuning improves accuracy in ResNets. Our analysis of the network representations suggests that this weight rescaling also results in the network relying on a low-dimensional subspace of its pretrained representation (i.e. exhibiting nested feature selection behavior). Intriguingly, Vision Transformers already exhibit signatures of nested feature selection without weight rescaling, and do not benefit from weight rescaling, suggesting that the nested feature selection regime is beneficial for finetuning performance. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A variety of studies have characterized implicit regularization effects in deep learning. These include biases toward low-frequency functions [17], stable minima in the loss landscape [18], low-rank solutions [19], and lower-order moments of the data distribution [20]. Chizat & Bach [13] show that when using cross-entropy loss, shallow (single hidden layer) networks are biased to minimize the ${\\mathcal{F}}_{1}$ norm, an infinite-dimensional analogue of the $\\ell_{1}$ norm over the space of possible hidden-layer features [see also 12, 14]. Other work has shown that implicit regularization for mean squared error loss in nonlinear networks cannot be exactly characterized as norm minimization [21], though ${\\mathcal{F}}_{1}$ norm minimization is a precise description under certain assumptions on the inputs [22]. ", "page_idx": 1}, {"type": "text", "text": "Compared to the body of work on inductive biases of single-task learning, theoretical treatments of MTL and $\\mathrm{PT+FT}$ are more scarce. Some prior studies have characterized benefits of multi-task learning with a shared representational layer in terms of bounds on sample efficiency [23\u201325]. Others have characterized the learning dynamics of deep linear networks trained from nonrandom initializations, which can be applied to understand finetuning dynamics [26, 27]. Similarly, insights on the implicit regularization of gradient descent in linear models has been applied to better understand forgetting and generalization in a continual learning setup [28\u201331]. However, while these works demonstrate an effect of pretrained initializations on learned solutions, the linear models they study do not capture the notion of feature learning we are interested in. Finally, teacher-student setups have been used to study the impact of task similarity on continual learning in deep neural networks [32, 33]. This methodology could also be applied to our setup (i.e. to investigate generalization on a finetuning task), and, similarly, our tools could be applied to continual learning setups. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "A few empirical studies have compared the performance of MTL vs. $\\mathrm{PT+FT}$ in language tasks, with mixed results depending on the task studied [34, 35]. Others have observed that $\\mathrm{PT+FT}$ outperforms $\\mathrm{PT}+$ \u201clinear probing\u201d (training only the readout layer and keeping the previous layers frozen after pretraining), implying that finetuning beneftis from the ability to learn task-specific features [36, 37]. ", "page_idx": 2}, {"type": "text", "text": "Inductive biases of diagonal linear networks. The theoretical component of our study relies heavily on a line of work [38\u201342] that studies the inductive bias of a simplified \u201cdiagonal linear network\u201d model. Diagonal linear networks parameterize linear maps $f:\\mathbb{R}^{D^{\\star}}\\rightarrow\\mathbb{R}$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\nf_{\\vec{w}}(\\vec{x})=\\vec{\\beta}(\\vec{w})\\cdot\\vec{x},\\qquad\\beta_{d}(\\vec{w}):=w_{+,d}^{(2)}w_{+,d}^{(1)}-w_{-,d}^{(2)}w_{-,d}^{(1)}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\vec{\\beta}(\\vec{w})\\in\\mathbb R^{D}$ . These correspond to two-layer linear networks in which the first layer consists of one-to-one connections, with duplicate $^+$ and \u2212pathways to avoid saddle point dynamics around $w=0$ . Woodworth et al. [38] showed that overparameterized diagonal linear networks trained with gradient descent on mean squared error loss find the zero-training-error solution that minimizes $\\|{\\vec{\\beta}}\\|_{2}$ , when trained from large initial weight magnitude (the \u201clazy\u201d regime, equivalent to ridge regression). When trained from small initial weight magnitude, networks instead minimize $\\|\\vec{\\beta}\\|_{1}$ (the \u201crich\u201d regime). This bias is a linear analogue of feature learning/feature selection, as a model with an $\\ell_{1}$ penalty tends to learn solutions that depend on a sparse set of input dimensions. ", "page_idx": 2}, {"type": "text", "text": "Implicit vs. explicit regularization. Theoretical work on the inductive biases conferred by different architectures has studied both the implicit regularization induced by gradient descent and explicit $\\ell_{2}$ - regularization. Notably, in homogeneous networks trained with crossentropy loss, implicit and explicit regularization yield identical inductive biases in the limit of infinite training time and infinitesimal regularization [12, 43], but this does not hold in general [38]. While [38, 40] are able to characterize the implicit regularization of gradient descent for diagonal linear networks, it is technically much more challenging to derive a similar result for multi-output diagonal linear networks, or for ReLU networks of any kind. In contrast, the impact of explicit weight regularization has been characterized for both multi-output diagonal linear networks [16] and multi-output ReLU networks [44\u201346]. ", "page_idx": 2}, {"type": "text", "text": "Our main technical contributions to this theoretical landscape are (1) spelling out the implications of existing results on implicit regularization in diagonal linear networks and (2) providing a novel characterization of the inductive bias induced by applying explicit parameter regularization to the finetuning of ReLU networks from arbitrary initialization. Our choice to study explicit parameter regularization for ReLU networks is made primarily for theoretical tractability; we view these results as a proxy for understanding the more theoretically complex problem of implicit regularization. ", "page_idx": 2}, {"type": "text", "text": "3 Implicit and explicit regularization penalties for MTL and PT+FT ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Theoretical setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Architectures. First, we consider diagonal linear networks with hidden weights $\\vec{w}_{+},\\vec{w}_{-}\\in\\mathbb{R}^{D}$ and $O\\in\\{1,2\\}$ output weights $\\boldsymbol{v}_{+},\\boldsymbol{v}_{-}\\,\\in\\mathbb{R}^{O\\times D}$ . ( $\\boldsymbol{O}$ is 1 or 2 depending on the training paradigm, see below.) The resulting network function is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\nf_{w,v}(\\vec{x})=\\beta(w,v)\\vec{x},\\quad\\vec{\\beta}_{o}(w,v):=\\vec{v}_{+,o}\\circ\\vec{w}_{+}-\\vec{v}_{-,o}\\circ\\vec{w}_{-},\\quad\\beta(w,v)\\in\\mathbb{R}^{O\\times D}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Second, we consider ReLU networks with $H$ hidden neurons, hidden weights $w\\in\\mathbb{R}^{H\\times D}$ , and $O\\in\\{1,2\\}$ readout weights $\\boldsymbol{v}\\in\\mathbb{R}^{O\\times H}$ . The network function is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{w,v}(\\vec{x})=\\sum_{h=1}^{H}\\vec{v}_{h}(\\langle\\vec{w}_{h},\\vec{x}\\rangle)_{+},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $(\\cdot)_{+}$ is the ReLU nonlinearity. Importantly, the ReLU nonlinearity is homogeneous and, as a result, the network function is invariant to rescaling the hidden weights by $\\alpha>0$ and the readout weights by $\\frac{1}{\\alpha}$ . It will therefore be useful to consider a re-parameterization in terms of the ", "page_idx": 2}, {"type": "text", "text": "Under this re-parameterization, an equivalent definition of the network function is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{m,\\theta}(\\vec{x})=\\sum_{h=1}^{H}m_{h}\\big(\\langle\\vec{\\theta}_{h},\\vec{x}\\rangle\\big)_{+}=f_{w,v}(\\vec{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "UwvjJZWjPT/tmp/3f0e005234eae1dd53d1881799cdadcc0b1252479458862630c1a88f2a7899b2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: Theoretically derived regularization penalties. a, Explicit regularization penalty associated with multi-task learning. b, Implicit regularization penalty associated with finetuning in diagonal linear networks. c, Explicit regularization penalty associated with finetuning in ReLU networks. This penalty also depends on the changes in feature direction over finetuning (measured by the correlation between the unit-normalized feature directions pre vs. post finetuning). ", "page_idx": 3}, {"type": "text", "text": "Training paradigms. We consider two datasets: an auxiliary task $X^{a u x}\\,\\in\\,\\mathbb{R}^{n_{a u x}\\times D},\\vec{y}^{a u x}\\,\\in$ $\\mathbb{R}^{n_{a u x}}$ and a main task $X^{m a i n}\\in\\mathbb{R}^{n_{m a i n}\\times D}$ , $\\vec{y}^{m a i n}\\in\\mathbb{R}^{n_{m a i n}}$ . ", "page_idx": 3}, {"type": "text", "text": "First, we consider multi-task learning (MTL): simultaneous training on both the auxiliary and main task. In this case, we consider networks with $O=2$ outputs. The first output corresponds to the auxiliary task and the second output to the main task. Accordingly, we denote $\\beta^{a u x}\\,:=\\,\\beta_{1}$ and $\\beta^{m a i n}:=\\beta_{2}$ in the diagonal linear network and $\\vec{v}^{a u x}:=\\vec{v}_{1}$ and $\\vec{v}^{m a i n}:=\\vec{v}_{2}$ in the ReLU network. Second, we consider pretraining on the auxiliary task and subsequent finetuning on the main task $(\\mathsf{P T+F T})$ . In this case, we consider networks with a single output $(O=1)$ ), but re-initialize the readout weights before finetuning. Accordingly, we denote the parameters learned after pretraining by $w^{a u x}$ and $v^{a u x}$ , and the parameters learned after finetuning by $w^{m a i n}$ and $v^{m a i n}$ . We define $\\beta^{\\bar{m}a i\\bar{n}},\\beta^{a u x}$ (for the diagonal network) and $m^{m a i n},\\theta^{m a i n},m^{a u x},\\theta^{\\bar{a}u x}$ (for the ReLU network) analogously. ", "page_idx": 3}, {"type": "text", "text": "3.2 Explicit regularization penalties in multi-task learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To theoretically understand the inductive bias of diagonal linear and ReLU networks trained with MTL, we consider the effect of minimizing the $\\ell_{2}$ parameter norm as an approximation of the implicit bias of training with gradient descent from small initialization. We argue that this is a reasonable heuristic. First, the analogous result holds in the single-output case for infinitesimally small initialization and two layers (though not for deeper networks, see [38]). Second, for cross-entropy loss it has been shown that gradient flow on all positively homogeneous networks (including diagonal linear networks and ReLU networks) converges to a KKT point of a max-margin/min-parameter-norm objective [12]. Finally, explicit $\\ell_{2}$ parameter norm regularization (\u201cweight decay\u201d) is commonly used in practice, making its inductive bias important to understand in its own right as well. ", "page_idx": 3}, {"type": "text", "text": "We now derive the norms minimized by explicit regularization in MTL: ", "page_idx": 3}, {"type": "text", "text": "Corollary 1. For the multi-output diagonal linear network defined in Eq. 2, a solution $\\beta^{*}$ with minimal parameter norm $\\lVert\\boldsymbol{w}\\rVert_{2}^{2}+\\lVert\\boldsymbol{v}\\rVert_{2}^{2}$ subject to the constraint that it fits the training data $(X^{m a i n}\\vec{\\beta}^{m a i n}=\\vec{y}^{m a i n},X^{a u x}\\vec{\\beta}^{a u x}=\\vec{y}^{a u x})$ ) also minimizes the following: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{3}^{*}=\\underset{\\boldsymbol{\\beta}}{\\operatorname{arg\\,min}}\\left(2\\sum_{d=1}^{D}\\sqrt{(\\beta_{d}^{a u x})^{2}+(\\beta_{d}^{m a i n})^{2}}\\right)\\quad\\boldsymbol{s}.t.\\quad X^{m a i n}\\vec{\\beta}^{m a i n}=\\vec{y}^{m a i n},\\;X^{a u x}\\vec{\\beta}^{a u x}=\\vec{y}^{u u x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This norm is known as the group lasso [47] and denoted as $\\Vert\\cdot\\Vert_{1,2}$ . For ReLU networks, by an argument analogous to the one above, parameter norm minimization translates to minimizing $\\begin{array}{r}{\\sum_{h=1}^{H}\\sqrt{(m_{h}^{a u x})^{2}+(m_{h}^{m a i n})^{2}}}\\end{array}$ (see Appendix A.1 and [44, 46]). ", "page_idx": 3}, {"type": "text", "text": "The norm $\\|\\cdot\\|_{1,2}$ is plotted in Fig. 1a. We analyze its impact in Section 4. ", "page_idx": 3}, {"type": "text", "text": "3.3 Regularization penalties in finetuning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Diagonal linear networks. We now consider the behavior of $\\mathrm{PT+FT}$ in overparameterized diagonal linear networks trained to minimize mean-squared error using gradient flow. We assume that the network is initialized prior to pre-training with infinitesimal weights, and that during pretraining, network weights are optimized to convergence on the training dataset $(X^{a u x},\\vec{y}^{a u x})$ from the auxiliary task. After pretraining, the second-layer weights $\\boldsymbol{v}_{+}$ and $v_{-}$ ) are reinitialized with constant magnitude $\\gamma$ . Further, to ensure that the network output pre-finetuning is zero (as in [38]), we set the values of corresponding positive and negative pathway weights to be equal to their sum following pretraining1. The network weights are further optimized to convergence on the main task dataset $(\\bar{X^{m a i n}},\\vec{y}^{m a i n})$ . The dynamics of the pretraining and finetuning steps can be derived as a corollary of [38, 40]: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Corollary 2. If the gradient flow solution $\\vec{\\beta}^{a u x}$ for the diagonal linear model in Eq. 1 during pretraining fits the auxiliary task training data with zero error (i.e. $X^{a u x}\\vec{\\beta}^{a u x}\\,=\\,\\vec{y}^{a u x})$ , and following reinitialization of the second-layer weights and finetuning, the gradient flow solution $\\vec{\\beta}^{*}$ after finetuning fits the main task data with zero training error (i.e. $X^{m a i n}{\\vec{\\beta}}^{m a i n}={\\vec{y}}^{m a i n})$ , then ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\vec{\\beta}^{*}=\\arg\\operatorname*{min}||\\vec{\\beta}^{m a i n}||_{Q}\\quad s.t.\\quad X\\vec{\\beta}=\\vec{y},}\\\\ &{\\displaystyle||\\vec{\\beta}^{m a i n}||_{Q}:=\\sum_{d=1}^{D}\\left(|\\beta_{d}^{a u x}|+\\gamma^{2}\\right)q\\left(\\frac{2\\beta_{d}^{m a i n}}{|\\beta_{d}^{a u x}|+\\gamma^{2}}\\right),\\quad q(z)=2-\\sqrt{4+z^{2}}+z\\cdot\\mathrm{arcsinh}(z/2).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This corollary is proven in Appendix A.2. The norm is plotted in Fig. 1b. ", "page_idx": 4}, {"type": "text", "text": "ReLU networks. We assume that after pretraining, the readout layer is re-initialized with arbitrary new weights $\\vec{\\gamma}\\in\\mathbb{R}^{H}$ . We then characterize the solution to the finetuning task that minimizes the $\\ell_{2}$ norm of weight changes from this initialization. This is similar to the explicit weight regularization considered in the previous section, except we now penalize weight changes from a particular initialization rather than the origin. We chose to consider this regularization penalty for two reasons. First, it is sometimes studied in the continual learning setting [30, 48]. Second, infinitesimal explicit regularization is equivalent to the implicit regularization induced by gradient descent in the case of shallow linear models [10]. While this is not true more generally, it is a useful heuristic to motivate our theoretical analysis (which we then validate using our experiments in Section 4). ", "page_idx": 4}, {"type": "text", "text": "We show that this finetuning solution implicitly minimizes the following penalty: ", "page_idx": 4}, {"type": "text", "text": "Proposition 3. Consider a single-output ReLU network (see Eq. 3) with first-layer weights $\\vec{w}_{h}^{a u x}\\in$ $\\mathbb{R}^{D}$ after pretraining, and second-layer weights re-initialized to $\\vec{\\gamma}\\,\\in\\,\\mathbb{R}^{H}$ . The solution to the finetuning task that minimizes the $\\ell_{2}$ norm of changes in the weights, i.e. minimizes $\\sum_{h=1}^{H}\\|\\vec{w}_{h}-$ $\\vec{w}_{h}^{a u x}\\|_{2}^{2}+(v_{h}-\\gamma_{h})^{2}$ , is equivalent to the solution that minimizes ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\Im(\\theta^{m a i n},m^{m a i n}|\\theta^{a u x},m^{a u x}):=\\sum_{h=1}^{H}r(\\vec{\\theta}_{h}^{m a i n},m_{h}^{m a i n}|\\vec{\\theta}_{h}^{a u x},m_{h}^{a u x}),}}\\\\ {{\\displaystyle r(\\vec{\\theta}_{h}^{m a i n},m_{h}^{m a i n}|\\vec{\\theta}_{h}^{a u x},m_{h}^{a u x}):=(m_{h}^{m a i n}/u^{*}-\\gamma_{h})^{2}+(u^{*})^{2}+m_{h}^{a u x}-2u^{*}\\sqrt{m_{h}^{a u x}}\\langle\\vec{\\theta}_{h}^{m a i n},\\vec{\\theta}_{h}^{a u x}\\rangle,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\boldsymbol{u}^{*}$ is the unique positive real root of ", "page_idx": 4}, {"type": "equation", "text": "$$\n-(m_{h}^{m a i n})^{2}+\\gamma_{h}m_{h}^{m a i n}u-m_{h}^{a u x}\\langle\\vec{\\theta}_{h}^{m a i n},\\vec{\\theta}_{h}^{a u x}\\rangle u^{3}+u^{4}=0.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We prove the proposition in Appendix A.3. It implies that the regularization penalty associated with finetuning in the ReLU network only depends on the correlation $\\rho_{h}:=\\langle\\vec{\\theta}_{h}^{m a i n},\\vec{\\theta}_{h}^{a u x}\\rangle$ between the first-layer feature weights before and after finetuning, and the magnitudes of the weights of these features. We plot this penalty in Fig. 1c. ", "page_idx": 4}, {"type": "text", "text": "4 Implications of the theory: multiple regimes of feature reuse ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Sample efficiency in teacher-student models ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To validate these theoretical characterizations and illustrate their consequences, we now perform experiments in a teacher-student setup. In the diagonal linear network case, we consider a linear regression task defined by $\\vec{w}\\in\\mathbb{R}^{1000}$ with a sparse set of $k$ non-zero entries. We sample two such vectors, corresponding to \u201cauxiliary\u201d and \u201cmain\u201d tasks, varying the number of non-zero entries $k_{a u x}$ and $k_{m a i n}$ , and the number of shared features (overlapping non-zero entries). We then uniformly sample input vectors $\\vec{x}\\in\\mathbb{R}^{1000}$ from the unit sphere, using the ground-truth weights to generate the target. We train on 1024 auxiliary samples and vary the number of main task samples. ", "page_idx": 4}, {"type": "image", "img_path": "UwvjJZWjPT/tmp/be6c39bd34d03c50133a60040885d7c52b17a859c80ce4fbf811066491c1831b.jpg", "img_caption": ["Figure 2: $\\mathrm{PT+FT}$ and MTL benefit from feature sparsity and reuse. a,b, Generalization loss for a) diagonal linear networks and b) ReLU networks trained on a) a linear model with distinct active dimensions and b) a teacher network with distinct units between auxiliary and main task (STL: singletask learning). MTL and $\\mathrm{PT+FT}$ benefti from a sparser teacher on the main task. c,d, Generalization loss for c) diagonal linear networks and d) ReLU networks trained on a teacher model sharing all features between the auxiliary and main task. $\\mathrm{PT+FT}$ and MTL both generalize better than STL. e,f, Generalization loss for e) diagonal linear networks and f) ReLU networks trained on a teacher model with overlapping features. Networks benefit from feature sharing and can learn new features. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "In the ReLU network case, we consider a \u201cteacher\u201d ReLU network with a sparse number of units (i.e. a low-dimensional hidden layer) and different kinds of overlaps (e.g. shared, correlated, or orthogonal features) between the auxiliary and main task. We randomly sample input data $\\vec{x}\\in\\mathbb{R}^{15}$ from the unit sphere and use the teacher network to generate the target. We train on 1024 auxiliary samples and vary the number of main task samples. During finetuning, we randomly re-initialize the readout weights using a normal distribution with a variance of $10^{-3}\\sqrt{2/H}$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 PT+FT and MTL benefit from sparse and shared features ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Feature sparsity. To understand the impact of the derived penalties in the case of features either used or not used during pretraining, we consider their limit behavior for very large or very small pretrained features. First, we consider the limit $\\begin{array}{r}{\\frac{|\\beta_{d}|}{|\\beta_{d}^{a u x}|+\\gamma^{2}}\\to\\infty}\\end{array}$ (capturing the case of a feature not used during pretraining). In this limit, the MTL penalty converges to $2|\\beta_{d}|$ . Similarly, for finetuning in diagonal linear networks, the penalty converges to $c|\\beta_{d}|$ where $\\therefore\\sim\\mathcal{O}\\left(\\log\\left(1/(|\\beta_{d}^{a u x}|+\\gamma^{2})\\right)\\right)$ (per an analysis in [38]). For new features, both networks therefore have an $\\ell_{1}$ norm minimization bias, suggesting that they tend to learn a sparse set of new features (just like in the single-task case). ", "page_idx": 5}, {"type": "text", "text": "To test this insight, we consider a teacher-student setup without any shared features between the auxiliary and main task, and vary the number of main task features. Indeed, we found that both MTL and $\\mathrm{PT+FT}$ have a more rapidly decreasing generalization loss for fewer features (just like single-task learning (STL)) (Fig. 2a). We further confirmed that they learned a sparse set of weights (Fig. 7). ", "page_idx": 5}, {"type": "text", "text": "The same phenomenon holds true for the ReLU network penalties as well. We considered a teacherstudent network with six auxiliary task features and one to six uncorrelated main task features. Again, both MTL and $\\mathrm{PT+FT}$ have a lower generalization loss for fewer features (Fig. 2b). ", "page_idx": 5}, {"type": "text", "text": "Feature sharing. Next, we consider the opposite limit, i.e. large pretrained features: $\\begin{array}{r}{\\frac{\\rvert\\beta_{d}\\lvert}{\\lvert\\beta_{d}^{a u x}\\rvert+\\gamma^{2}}\\rightarrow0}\\end{array}$ In this limit, both the MTL and the $\\mathrm{PT+FT}$ penalty for diagonal linear networks converges to $\\frac{\\beta_{d}^{2}}{|\\beta_{d}^{a u x}|}$ , a weighted $\\ell_{2}$ bias. This implies that the networks preferentially use large pretrained features. To test this, we consider a teacher-student setup with fully overlapping dimensions. Indeed, both MTL and $\\mathrm{PT+FT}$ outperform STL in this case (Fig. 2c). Notably, they perform similarly to a network where we only finetune the second layer $\\mathrm{PT+FT}$ (Linear Probing, LP)), which exactly implements the weighted $\\ell_{2}$ bias. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "In the case of ReLU networks, we considered a teacher network with the same six features for the auxiliary and main task. Again, we found that MTL and $\\mathrm{PT+FT}$ outperformed STL, though training a linear readout from a model with fixed features (either by using the hidden layer $\\scriptstyle(\\mathrm{PT}+\\mathrm{FT}$ (LP)) or the neural tangent kernel $\\mathrm{PT+FT}$ (NTK))) generalized even better (Fig. 2d). ", "page_idx": 6}, {"type": "text", "text": "Simultaneous sparsity and feature sharing. Finally, our analysis above considered the limit behavior of each feature separately. This suggests that models should be able to 1) preferentially rely on pretrained features and 2) when necessary, learn a sparse set of new features. To test this insight, we consider partially overlapping teacher models. In diagonal linear networks, we consider teacher models with forty auxiliary and main task features, with twenty or thirty of those features overlapping (Fig. 2e). On the one hand, both $\\mathrm{PT+FT}$ and MTL outperformed STL, indicating that they were able to benefti from the pretrained features. On the other hand, they also performed better than the $\\mathrm{PT+FT}$ (LP) model which only finetuned the second layer (and therefore did not have a sparse inductive bias), indicating that they tended to learn a sparse set of new features. In ReLU networks, we consider teacher models with six auxiliary and main task features, varying their overlap. Again, we find that MTL and $\\mathrm{PT+FT}$ outperformed both STL and $\\mathrm{PT+FT}$ (LP), indicating that they benefitted from feature learning by implementing an inductive bias towards sparse and shared features. ", "page_idx": 6}, {"type": "text", "text": "Differences between the MTL and $\\mathbf{PT}\\mathbf{+}\\mathbf{FT}$ norms. So far, we have highlighted several similarities between the MTL and $\\mathrm{PT+FT}$ norms in diagonal linear networks: they tend towards the $\\ell_{1}$ norm for small auxiliary features and a weighted $\\ell_{2}$ norm for large auxiliary features. In the next section, we will highlight an important difference that arises in the intermediate regime. Here we briefly highlight a difference in the limit behavior for diagonal linear networks: the relative weights of the $\\ell_{1}$ - and weighted $\\ell_{2}$ -penalty are different between MTL and $\\mathrm{PT+FT}$ . In particular, in the $\\ell_{1}$ penalty limit, there is an extra factor of order $\\scriptscriptstyle\\mathcal{O}$ $\\left(\\log\\left(1/(|\\beta_{d}^{a u x}|+\\gamma^{2})\\right)\\right)$ in the $\\mathrm{PT+FT}$ penalty. Assuming small initializations, this factor tends to be larger than 2, the corresponding coefficient in the MTL penalty. Thus, $\\mathrm{PT+FT}$ is more strongly biased toward reusing features from the auxiliary task compared to MTL. We are careful to note, however, that in the case of ReLU networks this effect is complicated by a qualitatively different phenomenon with effects in the reverse direction (see Section 4.5). ", "page_idx": 6}, {"type": "text", "text": "4.3 A conservation law ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now turn our attention to the intermediate regime where the coefficients of a feature are of similar magnitude in the auxiliary and main tasks. We define two functions for a given penalty $P(\\beta_{d}^{m a i n},\\bar{\\beta}_{d}^{a u x})$ (where $\\beta_{d}^{m a i n}$ is the main task feature coefficient and $\\beta_{d}^{a u x}$ is the auxiliary task feature coefficient): 1) the \u201c $\\ell\\cdot$ -order,\u201d $\\frac{\\partial\\log P}{\\partial\\log\\beta_{d}^{m a i n}}$ , which measures, locally, how strongly $P$ changes with increasing $\\beta_{d}^{m a i n}$ and 2) the \u201cfeature dependence\u201d (FD), $\\frac{\\partial\\log P}{\\partial\\log\\beta_{d}^{a u x}}$ , which measures, locally, how much decreases for a larger auxiliary feature. In the previous section, we found that for $\\beta_{d}^{a u x}\\to0$ , $\\ell$ -order $\\rightarrow1$ and $\\mathrm{FD}\\rightarrow0$ , i.e. the penalty becomes $\\ell_{1}$ -like and does not depend on the magnitude of the auxiliary feature. In contrast, for $\\beta_{d}^{a u x}\\to1$ , $\\ell_{}$ -order $\\rightarrow2$ and $\\mathrm{FD}\\rightarrow-1$ , i.e. the penalty becomes $\\ell_{2}$ -like and depends inversely on the auxiliary task feature coefficient magnitude. How are $\\ell_{}$ -order and FD related for intermediate values of $\\beta_{d}^{a u x\\,.}$ ? Remarkably, we find an exact analytical relationship between these measures that holds for all penalties considered here: ", "page_idx": 6}, {"type": "text", "text": "Proposition 4. For the MTL and $P T{+}F T$ penalties derived in both the diagonal linear and ReLU cases, $\\ell-o r d e r+F D=1$ . ", "page_idx": 6}, {"type": "text", "text": "See Appendix A.4 for proof. ", "page_idx": 6}, {"type": "text", "text": "Thus, there is an exact tradeoff between the sparsity and feature dependence even for intermediate auxiliary task feature coefficient values: an increase in sparsity (i.e. a smaller $\\ell_{}$ -order) yields a corresponding decrease in feature dependence (i.e. FD becomes closer to zero). ", "page_idx": 6}, {"type": "image", "img_path": "UwvjJZWjPT/tmp/78920766daa80da528f2e1fc90529cd1843df8702df6a71d9bc423ca2f7d47f9.jpg", "img_caption": ["Figure 3: $\\mathrm{PT+FT}$ (much moreso than MTL) exhibits a nested feature selection regime. a-c, Diagonal linear networks. a, $\\ell_{}$ -order/feature dependence plotted for $\\beta_{d}^{m a i n}=1$ and varying the auxiliary task feature coefficient. b, Generalization loss for models trained on a teacher with 40 active units during the auxiliary task and a subset of those units active during the main task. c, Generalization loss for $\\mathrm{PT+FT}$ models whose weights are rescaled by the factor in the parentheses before finetuning. d-f, ReLU networks. d, $\\ell$ -order/feature dependence plotted for the explicit finetuning and MTL penalties, for $m=1$ and varying the auxiliary task feature coefficient. e, Generalization loss for models trained on a teacher network with six active units on the auxiliary task and a subset of those units on the main task. f, Generalization loss for $\\mathrm{PT+FT}$ models whose weights are rescaled before finetuning. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.4 The nested feature selection regime ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Proposition 4 predicts the existence of a novel \u201cnested feature selection\u201d regime: for intermediate magnitudes of the auxiliary features, the penalties should encourage both sparsity and feature dependence. To test this prediction in diagonal linear networks, we use a teacher-student setting in which all of the main task features are a subset of the auxiliary task features, i.e. $k_{m a i n}\\leq k_{a u x}$ , and the number of overlapping units is equal to $k_{m a i n}$ . Solving this task most efficiently involves \"nested feature selection,\" a bias towards feature reuse and towards sparsity among the reused features. We plot $\\ell$ -order and FD for $\\beta_{d}^{m a i n}=1$ and varying the auxiliary task feature coefficient (Fig. 3a) and find that for $\\beta_{d}^{a u x}\\approx1$ , both norms exhibit \u201clazy regime\u201d-like behavior ( $\\boldsymbol{\\ell}$ -order of around 2, and FD near 0). This predicts that neither MTL nor $\\mathrm{PT+FT}$ networks should be able to benefit from nested sparsity task structure in this regime, which we confirm empirically: as $k_{m a i n}$ decreases, the networks\u2019 sample efficiency does not become substantially better (Fig. 3b). ", "page_idx": 7}, {"type": "text", "text": "However, for features with auxiliary task coefficients that are moderately smaller than their main task coefficients, Fig. 3a suggests a broad regime where the $\\ell_{}$ -order is closer to 1 (incentivizing sparsity), but feature dependence remains high. We can produce this behavior in these tasks by rescaling the weights of the network following pretraining by a factor less than 1. In line with the prediction of the theory, performing this manipulation enables $\\mathrm{PT+FT}$ to leverage sparse structure within auxiliary task features (Fig. 3c), even while retaining their ability to privilege features learned during pretraining above others (see Fig. 9). By contrast, this regime is much narrower for MTL (Fig. 3a). ", "page_idx": 7}, {"type": "text", "text": "For ReLU networks, the MTL penalty is the same as in diagonal linear networks. We plot the regularization penalty derived for $\\mathrm{PT+FT}$ , conditioned on various correlations $\\langle\\theta_{h}^{m a i n},\\theta_{h}^{a u x}\\rangle$ between the post-pretraining and post-finetuning feature weights (Fig. 3d). For features that are fully aligned before and after pretraining, this penalty is again $\\ell_{2}$ -like for $m^{a u x}\\approx1$ . However, in most cases, these features change direction at least a little bit, and we find that in that case the penalty is more $\\ell_{1}$ -like while still remaining sufficiently feature-dependent. This suggests that a nested feature selection regime may arise in ReLU networks even when auxiliary task feature coefficients have comparable magnitude to main task feature coefficients. To test this insight, we considered a set of tasks in which the main task solution relies exclusively on a subset of auxiliary task features. We found that $\\mathrm{PT+FT}$ (even without any weight rescaling) was able to benefit from the nested sparsity structure, but MTL was not (Fig. 3e).2 Performing weight rescaling in either direction following pretraining uncovers the initialization-insensitive (FD near 0), sparsity-biased $\\ell_{\\mathrm{i}}$ -order near 1) rich / feature-learning regime and the initialization-biased (FD near $-1$ ), no-sparsity-bias ( $\\boldsymbol{\\ell}$ -order near 2) lazy learning regime (Fig. 3f). This suggests that for different architectures and different tasks, different rescaling values may be required to enter the nested feature selection regime. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.5 $\\mathbf{P}\\mathbf{T}\\mathbf{+}\\mathbf{F}\\mathbf{T}.$ , but not MTL, in ReLU networks benefits from correlated features ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The regularization associated with $\\mathrm{PT+FT}$ yields benefits even when main/auxiliary task directions are correlated but not identical (Fig. 1c). In contrast, MTL cannot softly share features as it encodes correlated features with entirely distinct units. To test this hypothesis, we conduct experiments in which the ground-truth auxiliary and main tasks rely on correlated but distinct features. Indeed, we find $\\mathrm{PT+FT}$ outperforms STL in this case, whereas MTL only does so in the low-sample setting (Fig. 4a). Notably, if features are identical, MTL outperforms $\\mathrm{PT+FT}$ in the low-sample setting (Fig. 2d). Thus, $\\mathrm{PT+FT}$ (compared to MTL) trades off the flexibility to \u201csoftly\u201d share features for reduced sampleefficiency when such flexibility is not needed. ", "page_idx": 8}, {"type": "text", "text": "The analysis in Fig. 3d would predict that ReLU networks can no longer benefit from correlated features if the magnitude of auxiliary task features is much higher than that of main task features. To test this, we varied the magnitude of the main task features and their correlation with ", "page_idx": 8}, {"type": "image", "img_path": "UwvjJZWjPT/tmp/fec8283c522d155515a531be91710408df79c53a7acd19750142cc99a398095c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: $\\mathrm{PT+FT}$ , but not MTL, in ReLU networks beneftis from correlated features. a, Generalization loss for main task features that are correlated (0.9 cosine similarity) with the auxiliary task features. $\\mathrm{PT+FT}$ outperforms both MTL and STL. b, Generalization loss for main task features with varying correlation and magnitude (mag.). $\\mathrm{PT+FT}$ only outperforms STL if the features are either identical in direction or identical in magnitude. ", "page_idx": 8}, {"type": "text", "text": "auxiliary task features (Fig. 4b). We found that for lower magnitudes, $\\mathrm{PT+FT}$ still improved performance for the case of identical but not correlated feature directions, confirming our prediction. ", "page_idx": 8}, {"type": "text", "text": "5 Weight rescaling in deep networks gives rise to nested feature selection ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our analysis has focused on shallow networks trained on synthetic tasks. To test the applicability of our insights, we conduct experiments with convolutional networks (ResNet-18, [49]) on a vision task (CIFAR-100, [50]), using classification of two image categories (randomly sampled for each training run) as the primary task and classification of the other 98 as the auxiliary task. As in our experiments above, MTL and $\\mathrm{PT+FT}$ improve sample efficiency compared to single-task learning (Fig. 5a). ", "page_idx": 8}, {"type": "text", "text": "Our findings in Section 4.4 indicate that the nested feature selection bias of $\\mathrm{PT+FT}$ can be exposed or masked by rescaling the network weights following pretraining. Such a bias may be beneficial when the main task depends on a small subset of features learned during pretraining, as may often be the case in practice. We experiment with rescaling in our CIFAR setup. We find that rescaling values less than 1 improve finetuning performance (Fig. 5b). These results suggest that rescaling network weights before finetuning may be practically useful. We corroborate this hypothesis with additional experiments using networks pre-trained on ImageNet [51] (see Fig. 10). ", "page_idx": 8}, {"type": "text", "text": "To facilitate comparison of the phenomenology in deep networks with our teacher-student experiments above, we propose a signature of nested feature selection that can be characterized without knowledge of the underlying feature space (since the correct feature basis to analyze is less clear in multi-hidden-layer entworks). Specifically, we propose to measure (1) the dimensionality of the network representation pre- and post-finetuning, and (2) the extent to which the representational structure post-finetuning is shared with / inherited from that of the network following pretraining prior to finetuning. We employ the commonly used participation ratio (PR, [52]) as a measure of dimensionality, and the effective number of shared dimensions (ENSD, [53]) as a soft measure of the number of aligned principal components between two representations. Intuitively, the PR and ENSD of network representations pre- and post-finetuning capture the key phenomena of the nested feature selection regime: we expect the dimensionality of network after finetuning to be lower than after pretraining $(P R({\\mathbf{X}}_{F T})\\,<\\,P R({\\mathbf{X}}_{P T}))$ , and for nearly all of the representational dimensions expressed by the network post-finetuning to be inherited from the network state after pretraining $(E\\\"{N}S D({\\bf X}_{P T},{\\bf X}_{F T})\\approx\\bar{P}R({\\bf X}_{F T}))$ . We validate that this description holds in our nonlinear teacher-student experiments with networks in the nested feature selection regime (Fig. 11). ", "page_idx": 8}, {"type": "image", "img_path": "UwvjJZWjPT/tmp/fcaa9c7c99abfe8c7fe02d44a857d99b89ba57e6927e69be1e8008d8ebabd114.jpg", "img_caption": ["Figure 5: Experiments in deep neural networks trained on CIFAR-100: a-c, ResNet-18, d-f, ViT. a,d, Accuracy for MTL, $\\mathrm{{PT}+\\mathrm{{FT}}}$ , and STL in a) ResNet-18 and d) ViT. b,e Accuracy for $\\mathrm{PT+FT}$ with weight rescaling in b) ResNet-18 and e) ViT. c,f The participation ration of c) ResNet-18\u2019s and f) ViT\u2019s layers before and after finetuning (PR Pre and PR Post) as well as their ENSD. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Remarkably, we find that the ResNet-18 exhibits the same phenomenology, but only for weights rescaled by a small value (Fig. 5c). This supports the hypothesis that the observed beneftis of rescaling indeed arise from pushing the network into the nested feature selection regime. ", "page_idx": 9}, {"type": "text", "text": "Finally, we conduct the same experiment for Vision Transformers (ViT) [54]. We confirm that $\\mathrm{PT+FT}$ improves performance over single-task learning, though MTL offers no similar benefit in this case (Fig. 5d). We further find that rescaling before finetuning (both by larger and smaller values) decreases generalization performance (Fig. 5e). Notably, our representational analysis reveals that rescaling by a larger value yields a higher-dimensional subspace both before and after finetuning, whereas rescaling by a smaller value yields a lower-dimensional subspace, but pushes the effective number of shared dimensions down substantially (Fig. 5f). This indicates that a rescaling value of 1 may already give rise to the nested feature selection regime and rescaling by a smaller value pushes the ViT towards the pure feature learning regime. Taken together, our results suggest finetuning performance is best when networks operate in the nested feature selection regime, and weight rescaling can push networks into this regime when it does not arise naturally. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work we have provided a detailed characterization of the inductive biases associated with two common training strategies, MTL and $\\mathrm{{PT}+\\mathrm{{FT}}}$ , in diagonal linear and ReLU networks. These biases incentivize both feature sharing and sparse task-specific feature learning. In the case of $\\mathrm{PT+FT}$ , we characterized a novel nested feature selection learning regime which encourages sparsity within features inherited from pretraining. This insight motivates a simple technique for improving $\\mathrm{PT+FT}$ performance by pushing networks into this regime, which shows promising empirical results. ", "page_idx": 9}, {"type": "text", "text": "There are several avenues for extending our theoretical work: for example, connecting our derived penalties for ReLU networks (which assumed explicit parameterization) to the implicit regularization induced by dynamics of gradient descent, and extending our theory to the case of cross-entropy loss. In addition, more work is needed to extend our theory to more complex tasks and larger models. For instance, we are interested in investigating how the weight magnitudes required to enter the nested feature regime depend on architecture and properties of the tasks \u2014 we observed a difference between the rescaling values required for ResNets and Vision Transformers, which our shallow network theory is unable to speak to. Better understanding the conditions needed for nested feature selection could also inspire more sophisticated interventions than our weight rescaling trick. Finally, we considered a particularly simple multi-task setup, with identical formats between auxiliary and main tasks \u2014 our theory could be extended to cases where the different tasks use different objectives. Nevertheless, our work already provides new and practical insights into multi-task learning and finetuning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We are grateful to the members of the Center for Theoretical Neuroscience for helpful comments and discussions. The work was supported by NSF 1707398 (Neuronex), Gatsby Charitable Foundation GAT3708, and the NSF AI Institute for Artificial and Natural Intelligence (ARNI). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "1. Vafaeikia, P., Namdar, K. & Khalvati, F. A Brief Review of Deep Multi-task Learning and Auxiliary Task Learning arXiv:2007.01126 [cs, stat]. July 2020. http://arxiv.org/abs/   \n2007.01126 (2023).   \n2. Zhang, Y. & Yang, Q. A Survey on Multi-Task Learning. IEEE Transactions on Knowledge and Data Engineering 34. Conference Name: IEEE Transactions on Knowledge and Data Engineering, 5586\u20135609. ISSN: 1558-2191 (Dec. 2022).   \n3. Du, Y., Liu, Z., Li, J. & Zhao, W. X. A Survey of Vision-Language Pre-Trained Models arXiv:2202.10936 [cs]. July 2022. http://arxiv.org/abs/2202.10936 (2023).   \n4. Zhou, C. et al. A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT arXiv:2302.09419 [cs]. May 2023. http://arxiv.org/abs/2302.09419 (2023).   \n5. Bommasani, R. et al. On the Opportunities and Risks of Foundation Models arXiv:2108.07258 [cs]. July 2022. http://arxiv.org/abs/2108.07258 (2023).   \n6. Jaderberg, M. et al. Reinforcement Learning with Unsupervised Auxiliary Tasks arXiv:1611.05397 [cs]. Nov. 2016. http://arxiv.org/abs/1611.05397 (2023).   \n7. Wang, A. & Russakovsky, O. Overwriting Pretrained Bias with Finetuning Data arXiv:2303.06167 [cs]. Aug. 2023. http://arxiv.org/abs/2303.06167 (2023).   \n8. Steed, R., Panda, S., Kobren, A. & Wick, M. Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (Association for Computational Linguistics, Dublin, Ireland, May 2022), 3524\u20133542. https://aclanthology.org/2022.acl-long.247 (2023).   \n9. Neyshabur, B. Implicit Regularization in Deep Learning arXiv:1709.01953 [cs]. Sept. 2017. http://arxiv.org/abs/1709.01953 (2023).   \n10. Gunasekar, S., Lee, J. D., Soudry, D. & Srebro, N. Implicit Bias of Gradient Descent on Linear Convolutional Networks in Advances in Neural Information Processing Systems 31 (Curran Associates, Inc., 2018). https://proceedings.neurips.cc/paper/2018/hash/   \n0e98aeeb54acf612b9eb4e48a269814c-Abstract.html (2023).   \n11. Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S. & Srebro, N. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research 19, 2822\u20132878. ISSN:   \n1532-4435 (Jan. 2018).   \n12. Lyu, K. & Li, J. Gradient Descent Maximizes the Margin of Homogeneous Neural Networks arXiv:1906.05890 [cs, stat]. Dec. 2020. http://arxiv.org/abs/1906.05890 (2023).   \n13. Chizat, L. & Bach, F. Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss en. in Proceedings of Thirty Third Conference on Learning Theory ISSN: 2640-3498 (PMLR, July 2020), 1305\u20131338. https://proceedings.mlr. press/v125/chizat20a.html (2023).   \n14. Savarese, P., Evron, I., Soudry, D. & Srebro, N. How do infinite width bounded norm networks look in function space?: 32nd Conference on Learning Theory, COLT 2019. Proceedings of Machine Learning Research 99, 2667\u20132690. http://www.scopus.com/inward/record. url?scp $\\lvert=$ 85132757852&partnerID $=$ 8YFLogxK (2023) (2019).   \n15. Ongie, G., Willett, R., Soudry, D. & Srebro, N. A function space view of bounded norm infinite width relu nets: The multivariate case. arXiv preprint arXiv:1910.01635 (2019).   \n16. Dai, Z., Karzand, M. & Srebro, N. Representation Costs of Linear Neural Networks: Analysis and Design in Advances in Neural Information Processing Systems 34 (Curran Associates, Inc.,   \n2021), 26884\u201326896. https://proceedings.neurips.cc/paper_files/paper/2021/ hash/e22cb9d6bbb4c290a94e4fff4d68a831-Abstract.html (2023).   \n17. Rahaman, N. et al. On the Spectral Bias of Neural Networks. en. https://openreview.net/ forum?id $\\cdot$ r1gR2sC9FX (2023) (Sept. 2018).   \n18. Mulayoff, R., Michaeli, T. & Soudry, D. The Implicit Bias of Minima Stability: A View from Function Space in Advances in Neural Information Processing Systems 34 (Curran Associates, Inc., 2021), 17749\u201317761. https://proceedings.neurips.cc/paper/2021/hash/ 944a5ae3483ed5c1e10bbccb7942a279-Abstract.html (2023).   \n19. Huh, M. et al. The Low-Rank Simplicity Bias in Deep Networks arXiv:2103.10427 [cs]. Mar. 2023. http://arxiv.org/abs/2103.10427 (2023).   \n20. Refinetti, M., Ingrosso, A. & Goldt, S. Neural networks trained with SGD learn distributions of increasing complexity arXiv:2211.11567 [cond-mat, stat]. May 2023. http://arxiv.org/ abs/2211.11567 (2023).   \n21. Razin, N. & Cohen, N. Implicit Regularization in Deep Learning May Not Be Explainable by Norms in Advances in Neural Information Processing Systems 33 (Curran Associates, Inc., 2020), 21174\u201321187. https://proceedings.neurips.cc/paper/2020/hash/ f21e255f89e0f258accbe4e984eef486-Abstract.html (2023).   \n22. Boursier, E., Pillaud-Vivien, L. & Flammarion, N. Gradient flow dynamics of shallow ReLU networks for square loss and orthogonal inputs en. in (May 2022). https://openreview. net/forum?id=L74c-iUxQ1I (2023).   \n23. Maurer, A., Pontil, M. & Romera-Paredes, B. The Benefti of Multitask Representation Learning. Journal of Machine Learning Research 17, 1\u201332. ISSN: 1533-7928. http://jmlr.org/ papers/v17/15-242.html (2023) (2016).   \n24. Wu, S., Zhang, H. R. & R\u00e9, C. Understanding and Improving Information Transfer in Multi-Task Learning arXiv:2005.00944 [cs]. May 2020. http://arxiv.org/abs/2005.00944 (2023).   \n25. Collins, L., Hassani, H., Soltanolkotabi, M., Mokhtari, A. & Shakkottai, S. Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks en. in (June 2024). https: //openreview.net/forum?id=M8UbECx485 (2024).   \n26. Braun, L., Domin\u00e9, C., Fitzgerald, J. & Saxe, A. Exact learning dynamics of deep linear networks with prior knowledge. en. Advances in Neural Information Processing Systems 35, 6615\u20136629. https://proceedings.neurips.cc/paper_files/paper/2022/ hash / 2b3bb2c95195130977a51b3bb251c40a - Abstract - Conference . html (2023) (Dec. 2022).   \n27. Shachaf, G., Brutzkus, A. & Globerson, A. A Theoretical Analysis of Fine-tuning with Linear Teachers in Advances in Neural Information Processing Systems 34 (Curran Associates, Inc., 2021), 15382\u201315394. https://proceedings.neurips.cc/paper/2021/hash/ 82039d16dce0aab3913b6a7ac73deff7-Abstract.html (2023).   \n28. Evron, I., Moroshko, E., Ward, R., Srebro, N. & Soudry, D. How catastrophic can catastrophic forgetting be in linear regression? in Conference on Learning Theory (PMLR, 2022), 4028\u2013 4079.   \n29. Lin, S., Ju, P., Liang, Y. & Shroff, N. Theory on Forgetting and Generalization of Continual Learning en. in Proceedings of the 40th International Conference on Machine Learning ISSN: 2640-3498 (PMLR, July 2023), 21078\u201321100. https://proceedings.mlr.press/v202/ lin23f.html (2024).   \n30. Evron, I. et al. Continual learning in linear classification on separable data in International Conference on Machine Learning (PMLR, 2023), 9440\u20139484.   \n31. Goldfarb, D., Evron, I., Weinberger, N., Soudry, D. & HAnd, P. The Joint Effect of Task Similarity and Overparameterization on Catastrophic Forgetting\u2014An Analytical Model in The Twelfth International Conference on Learning Representations (2024).   \n32. Lee, S., Goldt, S. & Saxe, A. Continual Learning in the Teacher-Student Setup: Impact of Task Similarity en. in Proceedings of the 38th International Conference on Machine Learning ISSN: 2640-3498 (PMLR, July 2021), 6109\u20136119. https://proceedings.mlr.press/v139/ lee21e.html (2024).   \n33. Lee, S., Mannelli, S. S., Clopath, C., Goldt, S. & Saxe, A. Maslow\u2019s Hammer in Catastrophic Forgetting: Node Re-Use vs. Node Activation en. in Proceedings of the 39th International Conference on Machine Learning ISSN: 2640-3498 (PMLR, June 2022), 12455\u201312477. https: //proceedings.mlr.press/v162/lee22g.html (2024).   \n34. Dery, L. M., Michel, P., Talwalkar, A. & Neubig, G. Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative en. in (Oct. 2021). https://openreview. net/forum?id $\\cdot$ 2bO2x8NAIMB (2023).   \n35. Weller, O., Seppi, K. & Gardner, M. When to Use Multi-Task Learning vs Intermediate FineTuning for Pre-Trained Encoder Transfer Learning in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) (Association for Computational Linguistics, Dublin, Ireland, May 2022), 272\u2013282. https://aclanthology. org/2022.acl-short.30 (2023).   \n36. Kumar, A., Raghunathan, A., Jones, R., Ma, T. & Liang, P. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution arXiv:2202.10054 [cs]. Feb. 2022. http: //arxiv.org/abs/2202.10054 (2023).   \n37. Kornblith, S., Shlens, J. & Le, Q. V. Do better ImageNet models transfer better? in (2019). https://arxiv.org/pdf/1805.08974.pdf (2023).   \n38. Woodworth, B. et al. Kernel and Rich Regimes in Overparametrized Models en. in Proceedings of Thirty Third Conference on Learning Theory ISSN: 2640-3498 (PMLR, July 2020), 3635\u2013 3673. https://proceedings.mlr.press/v125/woodworth20a.html (2023).   \n39. Pesme, S., Pillaud-Vivien, L. & Flammarion, N. Implicit Bias of SGD for Diagonal Linear Networks: a Provable Benefti of Stochasticity en. in (Nov. 2021). https://openreview.net/ forum?id=vvi7KqHQiA (2023).   \n40. Azulay, S. et al. On the Implicit Bias of Initialization Shape: Beyond Infinitesimal Mirror Descent en. in Proceedings of the 38th International Conference on Machine Learning (PMLR, July 2021), 468\u2013477. https://proceedings.mlr.press/v139/azulay21a.html (2023).   \n41. HaoChen, J. Z., Wei, C., Lee, J. & Ma, T. Shape Matters: Understanding the Implicit Bias of the Noise Covariance en. in Proceedings of Thirty Fourth Conference on Learning Theory (PMLR, July 2021), 2315\u20132357. https://proceedings.mlr.press/v134/haochen21a.html (2023).   \n42. Moroshko, E. et al. Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy in Advances in Neural Information Processing Systems 33 (Curran Associates, Inc., 2020), 22182\u201322193. https://proceedings.neurips.cc/paper/2020/hash/ fc2022c89b61c76bbef978f1370660bf-Abstract.html (2023).   \n43. Ji, Z., Dud\u00edk, M., Schapire, R. E. & Telgarsky, M. Gradient descent follows the regularization path for general losses in Conference on Learning Theory (PMLR, 2020), 2109\u20132136.   \n44. Yang, L. et al. A Better Way to Decay: Proximal Gradient Training Algorithms for Neural Nets en. in (Nov. 2022). https://openreview.net/forum?id $\\equiv$ 4y1xh8jClhC (2024).   \n45. Parhi, R. & Nowak, R. D. Near-Minimax Optimal Estimation With Shallow ReLU Neural Networks. IEEE Transactions on Information Theory 69. Conference Name: IEEE Transactions on Information Theory, 1125\u20131140. ISSN: 1557-9654. https://ieeexplore.ieee.org/ abstract/document/9899453 (2024) (Feb. 2023).   \n46. Shenouda, J., Parhi, R., Lee, K. & Nowak, R. D. Variation Spaces for Multi-Output Neural Networks: Insights on Multi-Task Learning and Network Compression. Journal of Machine Learning Research 25, 1\u201340. ISSN: 1533-7928. http://jmlr.org/papers/v25/23- 0677.html (2024) (2024).   \n47. Yuan, M. & Lin, Y. Model selection and estimation in regression with grouped variables. en. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2005.00532.x, 49\u201367. ISSN: 1467- 9868. https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005. 00532.x (2023) (2006).   \n48. Lubana, E. S., Trivedi, P., Koutra, D. & Dick, R. How do quadratic regularizers prevent catastrophic forgetting: The role of interpolation in Conference on Lifelong Learning Agents (PMLR, 2022), 819\u2013837.   \n49. He, K., Zhang, X., Ren, S. & Sun, J. Deep Residual Learning for Image Recognition in (2016), 770\u2013778. https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_ Residual_Learning_CVPR_2016_paper.html (2024).   \n50. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. Publisher: Toronto, ON, Canada (2009).   \n51. Deng, J. et al. ImageNet: A large-scale hierarchical image database in 2009 IEEE Conference on Computer Vision and Pattern Recognition (2009), 248\u2013255.   \n52. Gao, P. et al. A theory of multineuronal dimensionality, dynamics and measurement en. Nov.   \n2017. https://www.biorxiv.org/content/10.1101/214262v2 (2023).   \n53. Giaffar, H., Bux\u00f3, C. R. & Aoi, M. The effective number of shared dimensions: A simple method for revealing shared structure between datasets en. July 2023. https://www.biorxiv.org/ content/10.1101/2023.07.27.550815v1 (2023).   \n54. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).   \n55. Paszke, A. et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019).   \n56. Kornblith, S., Norouzi, M., Lee, H. & Hinton, G. Similarity of Neural Network Representations Revisited en. in Proceedings of the 36th International Conference on Machine Learning (PMLR, May 2019), 3519\u20133529. https://proceedings.mlr.press/v97/kornblith19a.html (2023). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Derivation of regularization penalties ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Multi-task learning in ReLU networks ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Multi-task ReLU networks with a shared feature layer and $O$ outputs can be written as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{w}(x)=\\displaystyle\\sum_{h=1}^{H}{\\vec{v}}_{h}(\\langle w_{h},{\\vec{x}}\\rangle)_{+}=\\displaystyle\\sum_{h=1}^{H^{\\rightarrow}}{\\vec{m}}_{h}(\\langle\\vec{\\theta}_{h},{\\vec{x}}\\rangle)_{+},}\\\\ {{\\vec{m_{h}}}={\\vec{v}}_{h}\\|{\\vec{w}}_{h}\\|_{2},\\displaystyle\\vec{\\theta_{h}}={\\vec{w}}_{h}/\\|{\\vec{w}}_{h}\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\vec{w}_{h},m_{h}\\in\\mathbb{R}^{O}$ . We are interested in how minimizing the parameter norm $\\begin{array}{r}{\\sum_{h=1}^{H}\\|\\vec{w}_{h}\\|_{2}^{2}\\!+\\!\\|\\vec{v}_{h}\\|_{2}^{2}}\\end{array}$ maps to minimizing a norm over the solution weights $\\vec{m}$ . For a given $\\vec{m}_{h}\\in\\mathbb{R}^{O}$ , the norm of the input weight $\\|\\vec{w}_{h}\\|_{2}$ is a free parameter $z_{h}>0$ , as we can set $\\vec{v}_{h}:=\\vec{m}_{h}/z$ . As any value for $z_{h}$ leads to the same function, we choose $z_{h}$ so as to minimize ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\vec{w}_{h}\\|_{2}^{2}+\\|\\vec{v}_{h}\\|_{2}^{2}=z_{h}^{2}+\\|\\vec{m}_{h}\\|_{2}^{2}/z_{h}^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Setting the derivative to zero implies ", "page_idx": 14}, {"type": "equation", "text": "$$\nz_{h}^{2}=\\|{\\boldsymbol{m}}{\\vec{h}}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As a result, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{h=1}^{H}\\|\\vec{w}_{h}\\|_{2}^{2}+\\|\\vec{v}_{h}\\|_{2}^{2}=2\\sum_{h=1}^{H}\\|\\vec{m}_{h}\\|_{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the right-hand side of the equation is the $\\ell_{1,2}$ norm. ", "page_idx": 14}, {"type": "text", "text": "A.2 Finetuning in diagonal linear networks ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Corollary 2. If the gradient flow solution $\\vec{\\beta}^{a u x}$ for the diagonal linear model in Eq. 1 during pretraining fits the auxiliary task training data with zero error (i.e. $X^{a u x}\\vec{\\beta}^{a u x}\\,=\\,\\vec{y}^{a u x})$ , and following reinitialization of the second-layer weights and finetuning, the gradient flow solution $\\vec{\\beta}^{*}$ after finetuning fits the main task data with zero training error (i.e. $X^{m a i n}{\\vec{\\beta}}^{m a i n}={\\vec{y}}^{m a i n})$ , then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\vec{\\beta}^{*}=\\arg\\operatorname*{min}||\\vec{\\beta}^{m a i n}||_{Q}\\quad s.t.\\quad X\\vec{\\beta}=\\vec{y},}\\\\ &{\\displaystyle||\\vec{\\beta}^{m a i n}||_{Q}:=\\sum_{d=1}^{D}\\left(|\\beta_{d}^{a u x}|+\\gamma^{2}\\right)q\\left(\\frac{2\\beta_{d}^{m a i n}}{|\\beta_{d}^{a u x}|+\\gamma^{2}}\\right),\\quad q(z)=2-\\sqrt{4+z^{2}}+z\\cdot\\mathrm{arcsinh}(z/2).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. We derive this result in two steps: first, we note that after pretraining (because of the infinitesimal initial weight magnitudes), one of the pathways remains at zero and one pa\u221athway encodes the effective linear predictor $\\beta^{a u x}$ . Thus, the first hidden layer has the weights $\\sqrt{\\beta^{a u x}}$ , which yields the new weight magnitude along the positive and negative pathway (due to the summing ope\u221aration we perform following finetuning, which ensures that the weights of both pathways are set to $\\sqrt{\\beta^{a u x}})$ . Having set the readout initialization to $\\gamma$ , we then apply Theorem 4.1 in [40]. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "A.3 Finetuning in ReLU networks ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proposition 3. Consider a single-output ReLU network (see Eq. 3) with first-layer weights $\\vec{w}_{h}^{a u x}\\in$ $\\mathbb{R}^{D}$ after pretraining, and second-layer weights re-initialized to $\\vec{\\gamma}\\,\\in\\,\\mathbb{R}^{H}$ . The solution to the finetuning task that minimizes the $\\ell_{2}$ norm of changes in the weights, i.e. minimizes $\\sum_{h=1}^{H}\\|\\vec{w}_{h}-$ $\\vec{w}_{h}^{a u x}\\|_{2}^{2}+(v_{h}-\\gamma_{h})^{2}$ , is equivalent to the solution that minimizes ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\Im(\\theta^{m a i n},m^{m a i n}|\\theta^{a u x},m^{a u x}):=\\sum_{h=1}^{H}r(\\vec{\\theta}_{h}^{m a i n},m_{h}^{m a i n}|\\vec{\\theta}_{h}^{a u x},m_{h}^{a u x}),}}\\\\ {{\\displaystyle r(\\vec{\\theta}_{h}^{m a i n},m_{h}^{m a i n}|\\vec{\\theta}_{h}^{a u x},m_{h}^{a u x}):=(m_{h}^{m a i n}/u^{*}-\\gamma_{h})^{2}+(u^{*})^{2}+m_{h}^{a u x}-2u^{*}\\sqrt{m_{h}^{a u x}}\\langle\\vec{\\theta}_{h}^{m a i n},\\vec{\\theta}_{h}^{a u x}\\rangle,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\boldsymbol{u}^{*}$ is the unique positive real root of ", "page_idx": 14}, {"type": "equation", "text": "$$\n-(m_{h}^{m a i n})^{2}+\\gamma_{h}m_{h}^{m a i n}u-m_{h}^{a u x}\\langle\\vec{\\theta}_{h}^{m a i n},\\vec{\\theta}_{h}^{a u x}\\rangle u^{3}+u^{4}=0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. We define the norm of the deviation from the parameters at initialization as ", "page_idx": 15}, {"type": "equation", "text": "$$\nR(w,v|w^{a u x},\\gamma):=\\sum_{h=1}^{H}(v_{h}-\\gamma)^{2}+\\|\\vec{w}_{h}-w_{h}^{a u x}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that $w$ and $v$ are parameterized in a redundant manner, as we can multiply $v_{h}$ by a constant $a>0$ and obtain the same function as long as we divide $\\vec{w}_{h}$ by the same constant. To parameterize a function in a unique fashion, we describe it in terms of the normalized hidden weight $\\vec{\\theta}_{h}:=\\vec{w}_{h}/\\lVert\\vec{w}_{h}\\rVert_{2}$ and the (signed) magnitude $m_{h}:=\\|\\vec{w}_{h}^{(1)}\\|_{2}v_{h}$ . We wish to compute the norm of the deviation from the initialization in terms of this parameterization. Specifically, we compute ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Tilde{R}(\\vec{\\theta},m|w^{a u x},\\gamma):=\\operatorname*{min}_{w,v}R(w,v|w^{a u x},\\gamma)\\mathrm{~s.t.~}\\forall_{h}\\vec{\\theta}_{h}=\\vec{w}_{h}/\\|\\vec{w}_{h}\\|_{2},m_{h}=\\|\\vec{w}_{h}\\|_{2}v_{h}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that Woodworth et al. [38] solve a version of this optimization problem for diagonal linear networks (though only for the case in which corresponding first and second layer weights are initialized with equal magnitude). ", "page_idx": 15}, {"type": "text", "text": "As this optimization problem decomposes over hidden units, we can solve it for each hidden unit individually: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\tilde{R}(\\vec{\\theta},m|w^{a u x},\\gamma)=\\sum_{h=1}^{H}\\tilde{r}(\\vec{\\theta}_{h},m_{h}|\\vec{w}_{h}^{a u x},\\gamma),}}\\\\ &{}&{\\tilde{r}(\\vec{\\theta}_{h},m_{h}|\\vec{w}_{h}^{a u x},\\gamma):=\\displaystyle\\operatorname*{min}_{\\vec{w}_{h},v_{h}}(v_{h}-\\gamma)^{2}+\\|\\vec{w}_{h}-\\vec{w}_{h}^{a u x}\\|_{2}^{2}}\\\\ &{}&{\\mathrm{s.t.}~\\vec{\\theta}_{h}=\\vec{w}_{h}/\\|\\vec{w}_{h}\\|_{2},m_{h}=\\|\\vec{w}_{h}\\|_{2}v_{h}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can express this as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{r}(\\vec{\\theta}_{h},m_{h}|w_{h}^{a u x},\\gamma)=\\displaystyle\\operatorname*{min}_{u>0}p(u),\\quad p(u):=(m_{h}/u-\\gamma)^{2}+\\left\\|u\\vec{\\theta}_{h}-\\vec{w}_{h}^{a u x}\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that this function is smooth and diverges to $\\infty$ both for $u\\rightarrow\\infty$ and $u\\to0$ . As long as we have a unique positive stationary point $\\boldsymbol{u}^{*}$ , this stationary point must therefore be a minimum. We simplify ", "page_idx": 15}, {"type": "equation", "text": "$$\np(u)=(m_{h}/u-\\gamma)^{2}+u^{2}+\\|\\vec{w}_{h}^{a u x}\\|_{2}^{2}-2u\\langle\\vec{\\theta}_{h},\\vec{w}_{h}^{a u x}\\rangle,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and, defining $m_{h}^{a u x}:=\\|\\vec{w}_{h}^{a u x}\\|_{2}$ , and the cosine similarity between corresponding pretraining and finetuning task weights $\\begin{array}{r}{\\rho_{h}:=\\frac{\\langle\\vec{w}_{h}^{(1)},\\vec{w}_{h}^{a u x}\\rangle}{\\|\\vec{w}_{h}^{(1)}\\|_{2}\\|\\vec{w}_{h}^{a u x}\\|_{2}}}\\end{array}$ , write this as ", "page_idx": 15}, {"type": "equation", "text": "$$\np(u)=(m_{h}/u-\\gamma)^{2}+u^{2}+(m_{h}^{a u x})^{2}-2u m_{h}^{a u x}\\rho_{h}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We then determine the stationary points by computing the derivative ", "page_idx": 15}, {"type": "equation", "text": "$$\np^{\\prime}(u)=-2(m_{h}/u-\\gamma)m_{h}/u^{2}+2u-2m_{h}^{a u x}\\rho_{h},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and setting it to zero. Simplifying this equation (and multiplying by $u^{3}/2$ ) yields ", "page_idx": 15}, {"type": "equation", "text": "$$\n-m_{h}^{2}+\\gamma m_{h}u-m_{h}^{a u x}\\rho_{h}u^{3}+u^{4}=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can compute the solution to this quartic equation, e.g. using np.roots and select the (unique, in all empirical cases we explored) positive real root, $u^{*}$ . We can then compute the original norm by plugging in $u^{*}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{r}(\\vec{\\theta_{h}},m_{h}|\\vec{w}_{h}^{a u x},\\gamma)=(m_{h}/u^{*}-\\gamma)^{2}+u^{*2}+(m_{h}^{a u x})^{2}-2u^{*}m_{h}^{a u x}\\rho_{h}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.4 Proof of Proposition 4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proposition 4. For the MTL and $P T{+}F T$ penalties derived in both the diagonal linear and ReLU cases, $\\ell-o r d e r+F D=1$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Below, for simplicity\u2019s sake, we always denote $\\beta^{m a i n}=\\beta$ , $m^{m a i n}=m$ , and $\\theta^{m a i n}=\\theta$ . ", "page_idx": 16}, {"type": "text", "text": "MTL penalty. We first consider the MTL penalty, which is identical for diagonal linear networks and ReLU networks. We assume (without loss of generalization) $m^{a u x}>0$ . In that case, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{m}{\\sqrt{(m^{a u x})^{2}+m^{2}}-m^{a u x}}\\frac{\\ell-\\mathrm{order}=\\frac{\\partial\\log\\left(\\sqrt{(m^{a u x})^{2}+m^{2}}-m^{a u x}\\right)}{\\partial\\log m}=}{m^{2}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{FD}=\\frac{\\partial\\log\\Big(\\sqrt{(m^{a u x})^{2}+m^{2}}-m^{a u x}\\Big)}{\\partial\\log m^{a u x}}=}\\\\ {\\frac{m^{a u x}}{\\sqrt{(m^{a u x})^{2}+m^{2}}-m^{a u x}}\\left(\\frac{m^{a u x}}{\\sqrt{(m^{a u x})^{2}+m^{2}}}-1\\right)=\\frac{(m^{a u x})^{2}-m^{a u x}\\sqrt{(m^{a u x})^{2}+m^{2}}}{(m^{a u x})^{2}+m^{2}-m^{a u x}\\sqrt{(m^{a u x})^{2}+m^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, $\\ell{\\mathrm{-order}}+{\\mathrm{FD}}=1.$ . ", "page_idx": 16}, {"type": "text", "text": "Finetuning in diagonal network. We assume (without loss of generalization) $\\beta_{d}^{a u x}>0$ and derive ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\ell\\mathrm{-order}=\\frac{\\partial\\log\\left((|\\beta_{d}^{a u x}|)\\,q\\left(\\frac{2\\beta_{d}}{\\beta_{d}^{a u x}}\\right)\\right)}{\\partial\\log\\beta_{d}}=\\frac{2\\beta_{d}}{(\\beta_{d}^{a u x})^{2}q\\left(\\frac{2\\beta_{d}}{|\\beta_{d}^{a u x}|}\\right)}q^{\\prime}\\left(\\frac{2\\beta_{d}}{|\\beta_{d}^{a u x}|}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{{\\mathrm{FD}}=\\frac{\\displaystyle\\partial\\log\\left(|\\beta_{d}^{a u x}|q\\left(\\frac{2\\beta_{d}}{|\\beta_{d}^{a u x}|}\\right)\\right)}{\\partial\\log|\\beta_{d}^{a u x}|}\\qquad\\qquad}\\\\ &{}&{\\frac{1}{{q}\\left(\\frac{2\\beta_{d}}{|\\beta_{d}^{a u x}|}\\right)}\\left(q\\left(\\frac{2\\beta_{d}}{|\\beta_{d}^{a u x}|}\\right)-q^{\\prime}\\left(\\frac{2\\beta_{d}}{|\\beta_{d}^{a u x}|}\\right)\\frac{2\\beta_{d}}{(\\beta_{d}^{a u x})^{2}}\\right)=1-\\frac{2\\beta_{d}}{(\\beta_{d}^{a u x})^{2}q\\left(\\frac{2\\beta_{d}}{|\\beta_{d}^{a u x}|}\\right)}q^{\\prime}\\left(\\frac{2\\beta_{d}}{|\\beta_{d}^{a u x}|}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, $\\ell{\\mathrm{-order}}+\\mathrm{FD}=1$ . ", "page_idx": 16}, {"type": "text", "text": "Finetuning in ReLU networks. We define ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{r}(\\vec{\\theta}_{h},m_{h},u|\\vec{\\theta}_{h}^{a u x},m_{h}^{a u x}):=(m_{h}/u-\\gamma)^{2}+u^{2}+m_{h}^{a u x}-2u\\sqrt{m_{h}^{a u x}}\\langle\\vec{\\theta}_{h},\\vec{\\theta}_{h}^{a u x}\\rangle,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where, without loss of generalization, we assume that $m_{h}^{a u x}>0$ and note that ", "page_idx": 16}, {"type": "equation", "text": "$$\nr(\\ensuremath{\\vec{\\theta}}_{h},m_{h}|\\ensuremath{\\vec{\\theta}}_{h}^{a u x},m_{h}^{a u x})=\\tilde{r}(\\ensuremath{\\vec{\\theta}}_{h},m_{h},u^{*}|\\ensuremath{\\vec{\\theta}}_{h}^{a u x},m_{h}^{a u x}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\boldsymbol{u}^{*}$ is the critical point, depending on $\\vec{\\theta_{h}},m_{h},\\vec{\\theta_{h}}^{a u x},m_{h}^{a u x}$ . By the chain rule, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial r(\\vec{\\theta}_{h},m_{h}|\\vec{\\theta}_{h}^{a u x},m_{h}^{a u x})}{\\partial m_{h}}=\\frac{\\partial\\tilde{r}(\\vec{\\theta}_{h},m_{h},u^{*}|\\vec{\\theta}_{h}^{a u x},m_{h}^{a u x})}{\\partial m_{h}}+\\frac{\\partial\\tilde{r}(\\vec{\\theta}_{h},m_{h},u^{*}|\\vec{\\theta}_{h}^{a u x},m_{h}^{a u x})}{\\partial u^{*}}\\frac{\\partial u^{*}}{m_{h}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the analogous statement is true for $m_{h}^{a u x}$ ", "page_idx": 16}, {"type": "text", "text": "However, by definition, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial\\tilde{r}(\\vec{\\theta_{h}},m_{h},u^{*}|\\vec{\\theta_{h}^{a u x}},m_{h}^{a u x})}{\\partial u^{*}}=0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell\\mathrm{-order}=\\frac{m_{h}}{r}\\frac{\\partial\\tilde{r}}{\\partial m_{h}}=\\frac{2m_{h}^{2}}{r(u^{*})^{2}},}\\\\ &{\\qquad\\mathrm{FD}=\\frac{m_{h}^{a u x}}{r}\\frac{\\partial\\tilde{r}}{\\partial m_{h}^{a u x}}=\\frac{m_{h}^{a u x}}{r}\\left(1-\\frac{\\rho_{h}u^{*}}{\\sqrt{m_{h}^{a u x}}}\\right)=\\frac{1}{r}\\left(m_{h}^{a u x}-\\rho_{h}\\sqrt{m_{h}^{a u x}}u^{*}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To prove the statement, we therefore must prove that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{2m_{h}^{2}}{(u^{*})^{2}}+m_{h}^{a u x}-\\rho\\sqrt{m_{h}^{a u x}}u^{*}=r=\\frac{m_{h}^{2}}{(u^{*})^{2}}+(u^{*})^{2}+(m_{h}^{a u x})-2u^{*}m_{h}^{a u x}\\rho_{h}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To do so, we leverage that (30) implies ", "page_idx": 17}, {"type": "equation", "text": "$$\n-\\frac{m_{h}^{2}}{(u^{*})^{2}}+(u^{*})^{2}-u^{*}\\sqrt{m_{h}^{a u x}}\\rho_{h}=0,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and therefore ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{m_{h}^{2}}{(u^{*})^{2}}=(u^{*})^{2}-u^{*}\\sqrt{m_{h}^{a u x}}\\rho_{h}=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Plugging this into (32) yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{2m_{h}^{2}}{(u^{*})^{2}}+m_{h}^{a u x}-\\rho\\sqrt{m_{h}^{a u x}}u^{*}=\\frac{m_{h}^{2}}{(u^{*})^{2}}+(u^{*})^{2}+m_{h}^{a u x}-2\\rho\\sqrt{m_{h}^{a u x}}u^{*}=r\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and completes the proof. ", "page_idx": 17}, {"type": "text", "text": "B Detailed methods ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We trained all networks with PyTorch [55]. ", "page_idx": 17}, {"type": "text", "text": "B.1 Diagonal linear networks ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We train the diagonal linear networks until they have reached a mean squared error below $10^{-10}$ , for pretraining, finetuning, multi-task learning and single-task learning. As a rough heuristic for determining the learning rate, we begin at a learning rate of $10^{6}$ and divide by ten whenever the loss exceeds 100 (indicating divergence). ", "page_idx": 17}, {"type": "text", "text": "B.2 ReLU networks ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We train ReLU networks with 1000 hidden units using the same learning rate heuristic and train until the mean squared error has reached a threshold of $10^{-8}$ . ", "page_idx": 17}, {"type": "text", "text": "B.3 ResNets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We consider a ResNet-18 in the standard PyTorch implementation and train it with gradient descent with a learning rate of $10^{-3}$ and momentum of 0.9. ", "page_idx": 17}, {"type": "text", "text": "B.4 Vision Transformers ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We consider a Vision Transformer with seven layers, eight heads, 384 hidden dimensions and an MLP with 1,536 hidden dimensions. We train the transformer using the Adam optimizer with a learning rate of $10^{-3}$ and weight decay 0.00005. ", "page_idx": 17}, {"type": "text", "text": "B.5 Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We provide a code database that enables reproducing all data used to create the main figures. We used one CPU for each diagonal network (total number of experiments: 5,544) and ReLU network experiment (total number of experiments: 6,740) and all of them took up to four hours. We used on GPU for the CIFAR and ViT experiments and all of them took up to twelve hours (total number of experiments: 5,000). As a rough (upper-bound) estimate, the experiments therefore required 50,000 hours of CPU time and 60,000 hours of GPU time. ", "page_idx": 17}, {"type": "image", "img_path": "UwvjJZWjPT/tmp/10635dee093313a2244b08e7123dd81311227123830d55daa2abe66e05b13d50.jpg", "img_caption": ["Figure 6: Larger-scale teacher-student experiments. a, Generalization loss of shallow ReLU networks trained on data from a ReLU teacher network. b, Generalization loss for different numbers of overlapping features (out of 40 total) between main and auxiliary tasks. NTK indicates the (lazy) tangent kernel solution. This is comparable to Fig. 2d, except with more teacher units and more data. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "C Robustness of main results to choice of number of auxiliary task samples and input dimension ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To increase confidence that our main results are robust to the number of data samples used (1024 auxiliary task samples and up to 1024 main task samples in most of our experiments), and the number of ground-truth units in the teacher network (6), we repeated the experiments of Fig. 2d with 8192 auxiliary task samples and 40 ground-truth features. Indeed, in this setting the rich regime also helps with generalization if and only if the teacher units are sparse (Fig. 6a). Further, MTL and $\\mathrm{PT+FT}$ tend to outperform STL if the features are overlapping and MTL tends to outperform $\\mathrm{PT+FT}$ (Fig. 6b). In particular, the finetuned networks still benefit from feature learning, especially if some features are novel. ", "page_idx": 18}, {"type": "text", "text": "D Analysis of learned solutions in linear and nonlinear networks ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our theory predicts inductive biases towards solutions that minimize norms, often either $\\ell_{1}$ -like (incentivizing sparsity) or $\\ell_{2}$ -like. Our experiments in the main text corroborate these description by analyzing how sample complexity depends on the feature sparsity of the ground-truth task solution, and how the sparse feature structures of the main and auxliary tasks relate. However, this evidence for sparsity biases (or lack thereof) is indirect; here we present more direct analyses of the learned solutions in linear and nonlinear networks that support the account we provide in the main text. ", "page_idx": 18}, {"type": "text", "text": "D.1 Diagonal linear networks ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To check whether the implicit regularization theory is a good explanation for these performance results, we directly measured the $\\ell_{1,2}$ and $Q$ norms of the solutions learned by networks, compared to the corresponding penalties of the ground truth weights. In Fig. 7a we see that as the amount of training data increases, the norms all converge to that of the ground truth solution, but in the low-sample regime, MTL and $\\mathrm{PT+FT}$ find solutions with lower values of their corresponding norm than the ground-truth function, consistent with the implicit regularization picture (by contrast, STL does not consistently find solutions with lower values of these norms than the ground truth). ", "page_idx": 18}, {"type": "text", "text": "Our theory predicts that weight rescaling by a factor less than 1.0 following pretraining reduces the propensity of the network to share features between auxiliary and main tasks during finetuning. We confirm that this is the case in Fig. 7b by analyzing the overlap between the auxiliary task features and the learned linear predictor for the main task. ", "page_idx": 18}, {"type": "text", "text": "In Fig. 7c we show that our measure of effective sparsity of learned solutions in diagonal linear networks effectively distinguishes between networks trained in the feature selection regime and networks trained with linear probing (only training second-layer weights). Moreover, in Fig. 7d we show that the L1 norm of the solution increases with the training sample size, consistent with a bias towards L1 minimization. There is an interesting discrepancy between the behavior of the sparsity of the solutions (nonmonotonic, see Fig. 7c) and the L1 norm (largely monotonic, see Fig. 7d). This is attributable to the discrepancy between the L1 norm (which diagonal linear networks in the rich regime are biased to minimize) and sparsity (for which L1 norm is only a proxy). ", "page_idx": 18}, {"type": "image", "img_path": "UwvjJZWjPT/tmp/8d1682613fe9542cc348ebb7b0b30c63642d6c1da9e93e5682ad4f104c9025d8.jpg", "img_caption": ["Figure 7: a, $\\ell_{1,2}$ norm and $Q$ penalty for MTL, STL, and $\\mathrm{PT+FT}$ networks from Fig. 2b (40/40 overlapping features case). b, Proportion of the weight norm in the 40 dimensions relevant for the auxiliary task, for the networks in Fig. 3b,c. Weight rescaling decreases this overlap. c, Proportion of variance concentrated in the top $k$ weights, as a function of $k$ , for training on a single-task. When both layers are trained from small initialization (STL), this variance decreases much more rapidly than for pure linear readout training (STL (LP)), demonstrating the sparsity of the learned solution. d, L1 norm for STL as a function of the number of samples. e, Proportion of variance across different overlaps and for different learning setups (see also Fig. 2b). The rapid decrease in variance demonstrates the sparsity of the learned solutions both for $\\mathrm{PT+FT}$ and MTL. f, Proportion of weight norm in the 40 dimensions relevant for the auxiliary task (see also Fig. 2b). "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "In Fig. 7e we show the sparsity of networks trained on teacher with different values of the number of overlapping features between main and auxiliary tasks (each of which uses 40 features). We find that learned solutions across a range of overlaps are as sparse as using single-task learning (see Fig. 7c) ", "page_idx": 19}, {"type": "text", "text": "when task features do not overlap (0/40 case) and more sparse otherwise (on account of the bias toward reuse of the sparse features learned during the auxiliary task, see next paragraph). ", "page_idx": 20}, {"type": "text", "text": "In Fig. 7f we show the overlap between features active on either task, finding that learned main task solutions are biased to share auxiliary task features when few samples are available. ", "page_idx": 20}, {"type": "text", "text": "D.2 ReLU networks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We adopt a clustering-based approach to analyzing the effective sparse structure of learned task solutions. Specifically, for a given trained network, we perform $\\boldsymbol{\\mathrm{k}}$ -means clustering with a predetermined value of $K$ clusters on the normalized input weights to each hidden-layer neuron in the network3. We measure the extent to which $K$ cluster centers are able to explain the variance in input weights across hidden units; the fraction of variance left unexplained is commonly referred to as the \u201cinertia.\u201d For values of $K$ at which the inertia is close to zero, we can say that (to a good approximation) the network effectively makes use of at most $K$ nonlinear features. ", "page_idx": 20}, {"type": "text", "text": "D.2.1 Single-task learning: rich inductive bias yields clusters of similarly tuned neurons that approximate sparse ground-truth features ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In the single-task learning case, we measure the inertia of trained networks as a function of $K$ . We find that for networks in the rich regime (small initialization scale), for tasks with sparse ground-truth (six units in the ReLU teacher network), the networks do indeed learn solutions that make use of approximately six nonlinear features (Fig. 8a). For tasks with many (1000) units in the teacher network, the network finds solutions that use a small number of feature clusters when main task samples are limited, but gradually uses more clusters as the number of samples is increased (Fig. 8a), at which point the network matches the teacher function very well. This bias towards sparser-thanground-truth solutions given insufficient data corroborates our claim of an inductive bias towards sparse solutions. By contrast, networks in the lazy learning regime (large initialization scale) display no such bias, corroborating our claim that the sparse $\\ell_{1}$ -like inductive bias is a property of the rich regime but not the lazy regime. Interestingly, in the sparse ground-truth case learned solutions are relatively less sparse for an intermediate number of training examples. This may arise because an $\\ell_{1}$ -like inductive bias is not exactly the same as a bias toward sparse solutions over nonlinear features, particularly when training data is limited. We leave an in-depth investigation of this phenomenon to future work. ", "page_idx": 20}, {"type": "text", "text": "Our clustering analysis allows us to measure the extent to which the effective features employed by the network (cluster centers) are aligned with the ground-truth task features. Specifically, for each teacher unit, we compute an \u201calignment score\u201d between teacher and student networks by taking each teacher unit, measuring its cosine similarity with all the cluster centers, choosing the maximum value, and averaging this quantity across all teacher units. We find that the learned feature clusters are indeed highly aligned with the ground-truth teacher features in the sparse ground-truth case, and moreso as the number of main task samples (and consequently task performance) increases (Fig. 8b). ", "page_idx": 20}, {"type": "text", "text": "D.2.2 Pretraining+finetuning finds sparse solutions and improves alignment of feature clusters learned during pretraining ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We find that pretraining+finetuning improves performance over single-task learning when main and auxiliary task features are shared (or correlated), and maintains an apparent bias toward sparsity in new task-specific features. To corroborate these claims, we performed our clustering analysis on the solutions learned through $\\mathrm{PT+FT}$ . We find that the solutions learned are indeed quite sparse (comparable to the sparsity of solutions learned by single-task learning), even when the auxiliary task and main task features are disjoint (Fig. 8c). Moreover, we find that MTL also learns sparse solutions (Fig. 8d). In particular, as expected, the effective features on tasks with overlapping features is equal to the number of total unique features. Moreover, we observe that when main task and auxiliary task features are shared, $\\mathrm{PT+FT}$ and MTL networks exhibit higher alignment between learned features and ground-truth features than single-task-trained networks, especially when main task samples are limited (Fig. 8e). This provides a mechanistic underpinning for the relationship between the inductive bias of $\\mathrm{PT+FT}$ that we describe in the main text and its performance benefits. ", "page_idx": 20}, {"type": "image", "img_path": "UwvjJZWjPT/tmp/67bb216196a313b904748a2d37979ab7245965c7e353a4c7cc35c4baf3e0222d.jpg", "img_caption": ["Figure 8: Analysis of effective sparsity of learned ReLU network solutions. a Inertia (k-means reconstruction error for clustering of hidden-unit input weights) as a function of the number of clusters used for $k$ -means, for different numbers of main task samples and ground-truth teacher network units, in single-task learning. b Alignment score \u2013 average alignment (across teacher units) of the best-aligned student network cluster uncovered via $\\mathbf{k}$ -means. c, Inertia for networks trained using $\\mathrm{PT+FT}$ for the tasks of Fig. 2d,e and Fig. 4a. d, Same as panel $c$ but for networks trained with MTL. e, Alignment score for networks trained with MTL, $\\mathrm{PT+FT}$ , and STL on the same tasks as in panels $c$ and $d$ . f Inertia (using $k=1$ clusters) for networks trained on an auxiliary task that relies on only one ground-truth feature, which is one of the six ground-truth features used in the auxiliary task (as in Fig. 3e,f), using MTL or $\\mathrm{PT+FT}$ with various rescaling factors applied to the weights prior to finetuning. g Alignment score for the networks and task in panel $f.$ "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "D.2.3 Nested feature selection regime allows network to prioritize a sparse subset of feature clusters learned during pretraining ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In the main text we describe the \u201cnested feature selection\u201d regime, which occurs at intermediate values of the ratio between ground-truth main task feature coefficients and pretrained network weight scale. In this regime, networks can more efficiently learn main tasks that make use of a subset of the features used in the auxiliary task. Importantly, they still maintain a bias towards reusing features from the auxiliary task, as we show for diagonal linear networks and ReLU networks in Fig. 9. Here we show that networks in this regime (obtained most clearly in the shallow ReLU network case when networks are rescaled by a value of 1.0 after pretraining) indeed learn very sparse (effectively 1-feature) solutions when the ground-truth main task consists of a single auxiliary task features (Fig. 8e, right). By contrast, networks with weights rescaled by a factor of 10.0 following pretraining exhibit no such nested sparsity bias (consistent with lazy-regime behavior). Similarly, multi-task networks cannot exhibit such a bias in their internal representation as they still need to maintain the features needed for the main task (Fig. 8e, left). Additionally, supporting the idea that the nested feature selection regime allows networks to benefit from feature reuse, we find that networks in this regime exhibit a higher alignment score with the ground-truth teacher network when the main task features are a subset of the auxiliary task features compared to when they are disjoint from the auxiliary task features (Fig. 8g). This alignment benefti is mostly lost when networks are rescaled by a factor of 0.1 following pretrainning (a signature of rich-regime-like behavior). ", "page_idx": 21}, {"type": "image", "img_path": "UwvjJZWjPT/tmp/c02a399d4843570f20159e8096fdd33af0b444f7b981f94df41d58cf1cbdb5dc.jpg", "img_caption": ["Figure 9: Comparison between tasks with sparse main task features that are either subsets of the auxiliary task features or new features. Networks are trained with MTL or with $\\mathrm{PT+FT}$ , potentially with rescaling (as indicated by the number). a, Diagonal linear networks trained on five main task features. b, ReLU networks trained on a teacher network with one feature. We see that MTL (to some extent) and $\\mathrm{PT+FT}$ can benefti from such an overlap, but for small rescaling values, this benefti becomes smaller. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "UwvjJZWjPT/tmp/50535ea8e4f7212b89fa75b71060ebf917517ba52dc2d2f52e070815f7574f96.jpg", "img_caption": ["Figure 10: Results for finetuning deep convolutional networks trained on ImageNet, with/without weight rescaling (factor of 0.5) prior to finetuning. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "E Further evaluations of the rescaling method for finetuning ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To evaluate the robustness / general-purpose utility of our suggested approach of rescaling network weights following pretraining, we experimented with finetuning convolutional networks pretrained on ImageNet on downstream classification tasks: pretrained ResNet-18 finetuned on CIFAR100, pretrained VGG11 finetuned on CIFAR100, and pretrained ResNet-18 finetuned on STL-10. We experimented both with finetuning on the full multi-way classification task, and also on binary classification tasks obtained by subsampling pairs of classes from the main task dataset (which we found exposes performance differences more strongly). Due to computational constraints, we did not sweep over the choice of the rescaling factor, but simply used a factor of 0.5 in all cases. We find that rescaling improves finetuning performance, to varying degrees, in all of our experiments (Fig. 10). ", "page_idx": 23}, {"type": "text", "text": "F Analysis of representations learned in the nested feature selection regime: bridging the gap from shallow to deep networks ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We were interested in testing whether our theoretical understanding of shallow networks is truly responsible for the behavior of deeper networks (with more direct evidence than performance results / sample complexity). Specifically, we sought to understand whether the observed benefti of rescaling network weights following pretraining (Fig. 5b, Appendix E) relates to the nested feature selection regime we characterized in shallow networks. Doing so is challenging, as the space of \u201cfeatures\u201d learnable by a deep network is difficult to characterize explicitly (making the feature clustering analysis employed in Appendix D inapplicable). To circumvent this issue, we propose a signature of nested feature selection that can be characterized without knowledge of the underlying feature space. Specifically, we propose to measure (1) the dimensionality of the network representation pre- and post-finetuning, and (2) the extent to which the representational structure post-finetuning is shared with / inherited from that of the network following pretraining prior to finetuning. ", "page_idx": 23}, {"type": "text", "text": "We employ the commonly used participation ratio [PR; 52] as a measure of dimensionality. For an $n\\times p$ matrix $\\mathbf{X}$ representing $n$ mean-centered samples of p-dimensional network responses, with a $p\\times p$ covariance matrix $\\begin{array}{r}{\\mathbf{C}\\bar{\\mathbf{\\chi}}=\\frac{1}{n}\\mathbf{X}^{\\top}\\mathbf{X}}\\end{array}$ , the participation ratio is defined as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P R(X)=\\frac{\\left(\\sum_{i=1}^{p}\\lambda_{i}\\right)^{2}}{\\sum_{i=1}^{p}\\lambda_{i}^{2}}=\\frac{t r a c e\\left(\\mathbf{C}_{X}\\right)^{2}}{t r a c e\\left(\\mathbf{C}_{X}^{2}\\right)}}\\\\ {=\\frac{t r a c e\\left(\\mathbf{X}^{\\top}\\mathbf{X}\\right)^{2}}{t r a c e\\left(\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{X}^{\\top}\\mathbf{X}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\lambda_{i}$ are the eigenvalues of the covariance matrix $\\mathbf{C}_{X}$ . The PR scales from 1 to $p$ and measures the extent to which the covariance structure of responses $\\mathbf{X}$ is dominated by a few principal components or is spread across many. We argue that low-dimensional representations are a signature of networks that use a sparse set of features. We confirm that this is the case in our teacher-student setting: networks in the rich regime, which are biased towards sparse solutions, learn representations with lower PR than networks in the lazy regime, which are not biased toward sparse solutions (Fig. 11a). ", "page_idx": 23}, {"type": "text", "text": "Our measure of shared dimensionality between two representations is the effective number of shared dimensions (ENSD) introduced by Giaffar et al. [53]. The ENSD for an $n\\times p$ matrix of responses $\\mathbf{X}$ from one network and an $n\\times p$ matrix of responses $\\mathbf{Y}$ from another network, denoted $E N S D(X,Y)$ , is given by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{t r a c e\\left(\\mathbf{Y}^{\\top}\\mathbf{X}\\mathbf{X}^{\\top}\\mathbf{Y}\\right)\\cdot t r a c e\\left(\\mathbf{X}^{\\top}\\mathbf{X}\\right)\\cdot t r a c e\\left(\\mathbf{Y}^{\\top}\\mathbf{Y}\\right)}{t r a c e\\left(\\mathbf{X}^{\\top}\\mathbf{X}\\mathbf{X}^{\\top}\\mathbf{X}\\right)\\cdot t r a c e\\left(\\mathbf{Y}^{\\top}\\mathbf{Y}\\mathbf{Y}^{\\top}\\mathbf{Y}\\right)}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This measure is equal to the centered kernel alignment (CKA), a measure of similarity of two network representations [56], multiplied by the geometric mean of the participation ratios of the two representations. It measures an intuitive notion of \u201cshared dimensions\u201d \u2014 for example, if $\\mathbf{X}$ consists of 10 uncorrelated units, if $\\mathbf{Y}$ is taken from a subset of five of those units, the ENSD(X, Y) will be 5. ", "page_idx": 23}, {"type": "image", "img_path": "UwvjJZWjPT/tmp/ce69f3937b94d18bf83cae7f2254f68760c5acf829fe7adbb6f5a4a2ca14148d.jpg", "img_caption": ["Figure 11: Dimensionality of the network representations before and after finetuning. a, Participation ratio of the ReLU networks\u2019 internal representation after training on a task with six teacher units. b, Participation ratio of the network representation after finetuning on the nested sparsity task with different weight rescalings. c, Participation ratio before (left panel) and after finetuning (middle panel) and the effective number of shared dimensions between the two representations (right panel). Small weight scaling decreases the participation ratio after training. d, The same quantities for ResNet18 before and after finetuning (see also Fig. 5c). "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "If $\\mathbf{Y}$ is taken to be five uncorrelated units that are themselves uncorrelated with all those in $\\mathbf{X}$ , the ENSD(X, Y) will be zero. ", "page_idx": 24}, {"type": "text", "text": "Intuitively, the PR and ENSD of network representations pre- and post-finetuning capture the key phenomena of the nested feature selection regime. In a case in which the main task uses a subset of the features of the auxiliary task, if the network truly extracts this sparse subset of features, we expect the dimensionality of network after finetuning to be lower than after pretraining $(P R({\\mathbf{X}}_{F T})<P R({\\mathbf{X}}_{P T}))$ , and for nearly all of the representational dimensions expressed by the network post-finetuning to be inherited from the network state after pretraining $(E N\\bar{S}D({\\bf X}_{P T},{\\bf\\dot{X}}_{F T})\\approx P R\\bar{(}{\\bf X}_{F T}))$ . By contrast, networks not in the nested feature selection regime should exhibit an $\\ell_{2}$ -like rather than $\\ell_{1}$ -like bias with respect to features inherited from pretraining and thus not exhibit a substantial decrease in dimensionality during finetuning. ", "page_idx": 24}, {"type": "text", "text": "We show that this description holds in our nonlinear teacher-student experiments. Networks that we identified as being in the \u201cnested feature selection\u201d regime (weights rescaled by 1.0 following pretraining), and also networks in the rich regime, exhibit decreased PR following finetuning (Fig. 11b). By contrast, lazy networks (weights rescaled by 10.0 following pretraining) exhibit no dimensionality decrease during finetuning. Additionally (see Fig. 11c), the ENSD between pretrained (PT) and finetuned (FT) networks is almost identical to the dimensionality of the finetuned representation (PR FT). ", "page_idx": 24}, {"type": "text", "text": "Strikingly, we observe very similar behavior in our ResNet-18 model pretrained on 98 CIFAR-100 classes and finetuned on the 2 remaining classes (Fig. 11d), when we apply our method of rescaling weights post-finetuning. Analyzing the PR and ENSD of the outputs of different stages of the network following pretraining and following finetuning, we see that dimensionality decreases with finetuning, and ENSD between the pretrained and finetuned networks is very close to the PR of the finetuned network. Moreover, this phenomenology is only observed when we apply the weight rescaling method; finetuning the raw pretrained network yields no dimensionality decrease. These results suggest that the success of our rescaling method may indeed be attributable to pushing the network into the nested feature selection regime. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We characterize the inductive bias associated with MTL and $\\mathrm{PT+FT}$ by characterizing existing penalties and deriving a new penalty for $\\mathrm{PT+FT}$ in ReLU networks, which we do in Section 3. We then test these inductive biases to understand how they manifest in practice in Section 4 and find that they are relevant for large-scale neural network architectures in Section 5. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We discuss limitations in the latter half of Section 6. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We derive 1) the penalty associated with MTL in ReLU networks in Appendix A.1, 2) the penalty associated with $\\mathrm{PT+FT}$ in ReLU networks in Appendix A.3, and 3) the conservation law (Prop. 4) in Appendix A.4. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We describe the way in which we generate the teacher-student experiments in Section 3.3 and further discuss our methods in detail in Appendix B. Finally we are providing a codebase to reproduce all figure in the main text. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 26}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have attached a codebase that enables reproduction of all figures in the main text. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide these details in Appendix B ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: For all experiments, we report the mean $\\pm$ two standard deviations. Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We describe these resources in Appendix B.5. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: This manuscript presents foundational research without direct path to negative applications. Further, the manuscript does not present any experiments involving human subjects. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: This manuscript presents foundational research without direct societal impact. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: This manuscript presents foundational research and there is no clear potential risk of misuse. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We use two standard vision datasets: CIFAR-100 and Imagenet. For CIFAR100, we were unable to find an associated license. For Imagenet, the data is available for free to researchers for non-commercial use. Both websites hosting these datasets request a citation of the relevant paper which we provided. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]