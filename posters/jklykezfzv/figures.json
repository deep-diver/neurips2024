[{"figure_path": "jKLyKeZfzv/figures/figures_1_1.jpg", "caption": "Figure 1: Post-training rank correlation for randomly chosen 1000 candidates on NASBench-201. The predictive performance of the proposed two terms gradually improves as epochs increase.", "description": "This figure displays the Kendall's tau correlation between the ranking of 1000 randomly selected architectures from the NASBench-201 dataset based on different performance estimation methods and their actual ranking after 200 training epochs.  The x-axis represents the number of training epochs. The y-axis shows the correlation coefficient. Four methods are compared: NTK (Neural Tangent Kernel), TSE (Training Speed Estimation), the proposed landscape term, and the proposed speed term. The figure shows that, while NTK shows an unstable correlation, the proposed landscape and speed terms show a gradually improving correlation with the actual performance as the number of epochs increases, highlighting their effectiveness in predicting actual DNN performance.", "section": "Abstract"}, {"figure_path": "jKLyKeZfzv/figures/figures_2_1.jpg", "caption": "Figure 2: (a) The landscape term draws the slice of loss landscape to capture its macroscopic non-convex nature of the candidate architecture. The speed term analyzes the training changes over the training time, providing microscopic insights into the candidate's convergence speed. (b) Comparison of MOTE-NAS and an Evaluation-Free version MOTE-NAS-EF against other recent efficient NAS methods on NASBench-201 (CIFAR-100).", "description": "Figure 2(a) shows how the MOTE estimate combines a macro-level loss landscape term with a micro-level training speed term to more accurately estimate the performance of neural network architectures. Figure 2(b) compares the performance of MOTE-NAS and its evaluation-free variant to other state-of-the-art efficient neural architecture search (NAS) methods on the CIFAR-100 subset of the NASBench-201 benchmark.  The results demonstrate the superior accuracy and efficiency of MOTE-NAS.", "section": "3 MOTE-NAS"}, {"figure_path": "jKLyKeZfzv/figures/figures_4_1.jpg", "caption": "Figure 3: The generation pipelines of accuracy (upper part) and MOTE (bottom part). The proposed reduced architecture and dataset, MOTE, are colored red in their respective sections.", "description": "This figure illustrates the process of generating accuracy (top) and MOTE (bottom) estimations.  The top section shows the standard process of using a meta-architecture (NAS-201) and datasets (CIFAR-10, CIFAR-100, ImageNet) to obtain an accuracy measurement. The bottom section details the proposed method using a reduced architecture and dataset to generate the MOTE estimate.  The reduced architecture and dataset are highlighted in red, indicating the core components of the proposed efficiency improvement. The reduced architecture simplifies the process to significantly reduce computation time needed.", "section": "3.2 Reduction Strategies for MOTE Generation"}, {"figure_path": "jKLyKeZfzv/figures/figures_5_1.jpg", "caption": "Figure 4: After encoding the images of CIFAR-100 through VGG, the encodings for each label are obtained by averaging image embedding codes. Then we used K-Means and Farthest Point Sampling (FPS) to select a representative set of r labels, forming the reduced dataset.", "description": "This figure illustrates the process of creating a reduced dataset from the CIFAR-100 dataset.  First, images from CIFAR-100 are passed through a pre-trained VGG-16 model to extract image embedding codes. These codes are then averaged for each label, resulting in a single embedding code per label.  Next, K-means clustering is used to group similar labels, and farthest point sampling selects the most representative labels from each cluster. The images corresponding to these selected labels form the final reduced dataset, which is significantly smaller than the original CIFAR-100 dataset but retains its key characteristics.", "section": "3.2 Reduction Strategies for MOTE Generation"}, {"figure_path": "jKLyKeZfzv/figures/figures_5_2.jpg", "caption": "Figure 5: The proposed terms via aggressive reduction strategies on NASBench-101 and NASBench-201. RA means reduced architecture, RD means reduced dataset", "description": "This figure shows the impact of applying reduced architecture (RA) and reduced dataset (RD) strategies on the correlation between estimated and actual performance metrics.  It compares the correlation of test accuracy (early stopping vs. 200 epochs) and MOTE components (landscape and speed terms) against the average time cost per cell.  The results demonstrate that while aggressive reduction strategies significantly reduce computational costs, they also decrease the accuracy of performance prediction using early stopping. However, MOTE components (especially the speed term) show robust correlation even under aggressive reductions.", "section": "3.2 Reduction Strategies for MOTE Generation"}, {"figure_path": "jKLyKeZfzv/figures/figures_6_1.jpg", "caption": "Figure 6: The left side depicts MOTE-NAS's search stage, utilizing MOTE for architecture selection through an evolutionary loop, terminating at 10 + k iterations. On the right side is the evaluation stage, where MOTE selects the top-k architectures for evaluation. MOTE-NAS-EF simplifies this by relying solely on MOTE to choose the top-1 architecture without the evaluation stage.", "description": "This figure illustrates the workflow of the MOTE-NAS algorithm.  The left side shows the search stage, where an evolutionary approach iteratively refines a pool of architectures using MOTE (Multi-Objective Training-based Estimate) for selection. After a set number of iterations (10 + k), the top k architectures are passed to the evaluation stage. The right side shows the evaluation stage, where the top-performing architecture among those k candidates is selected using validation.  MOTE-NAS-EF simplifies this process by directly selecting the best architecture using MOTE without the validation stage.", "section": "3.3 Integrating MOTE with Evolutionary Search"}, {"figure_path": "jKLyKeZfzv/figures/figures_6_2.jpg", "caption": "Figure 7: The Kendall's Tau Correlation comparison of the proposed speed term, landscape term and MOTE with other estimates on NASBench-101 and NASBench-201. Note that the \"(s)\" is the GPU seconds per cell cost.", "description": "This figure compares the Kendall's Tau correlation, a measure of rank correlation, between different performance estimation methods and the actual performance ranking on NASBench-101 and NASBench-201 datasets.  The methods include SynFlow, TSE, TE-NAS, KNAS, Zen-Score, LGA, ZICO, the proposed landscape term, the proposed speed term, and the proposed MOTE.  The x-axis represents each method, while the y-axis shows the Kendall's Tau correlation for each dataset (NB201-CIFAR10, NB201-CIFAR100, NB201-ImageNet16, and NB101). The numbers in parentheses indicate the GPU seconds per cell cost for each method, showcasing the efficiency of different approaches. The higher the correlation, the better the method's ability to predict actual performance ranks.", "section": "4.1 Comparison of MOTE and Other Estimates"}, {"figure_path": "jKLyKeZfzv/figures/figures_7_1.jpg", "caption": "Figure 8: Comparison of the distribution of MOTE (red) and KNAS (green) on NASBench-201 (CIFAR-100).", "description": "This figure compares the performance of MOTE and KNAS in ranking candidate architectures on the NASBench-201 benchmark using CIFAR-100.  Each point represents a candidate architecture. The x-axis shows the rank assigned by either MOTE or KNAS, while the y-axis shows the actual rank determined by the test accuracy after 200 epochs of training.  The plot reveals that MOTE's rankings are more tightly clustered around the diagonal (perfect correlation) than KNAS, indicating that MOTE is a more accurate predictor of actual performance. The clustering of points in the lower-left (high accuracy, high estimate rank) and upper-right (low accuracy, low estimate rank) quadrants also illustrates the accuracy of MOTE and KNAS in identifying high and low performing candidates.", "section": "Visualization of MOTE and NTK based Estimate"}, {"figure_path": "jKLyKeZfzv/figures/figures_15_1.jpg", "caption": "Figure A1: The mutation step of the evolutionary process in the search. When the iteration count is 0, as opposed to being greater than 0, the number of samples taken from the search space varies. The sampling size B, increases with the number of iterations.", "description": "This figure illustrates the mutation process within the evolutionary search algorithm of MOTE-NAS.  The process starts by selecting the top 10% of candidates from the pool (P) based on the MOTE estimate. These selected candidates are then encoded into adjacency (A) and operation (X) matrices.  The mutation step involves creating modified architectures (S') by altering these matrices, ensuring that the Euclidean distance between the original and modified matrices remains below a threshold of 1.  The number of samples drawn from the search space dynamically increases as the iteration count rises, starting with B samples at iteration 0.  Finally, these newly generated candidates are added back into the pool P for further consideration.", "section": "A.3 Integrating MOTE with Evolutionary Search"}, {"figure_path": "jKLyKeZfzv/figures/figures_15_2.jpg", "caption": "Figure A2: Encoding a cell into the adjacency and operation matrix. This entire procedure closely resembles the predictor-based NAS.", "description": "This figure illustrates how a cell in a cell-based neural architecture search space is represented using two matrices: an adjacency matrix and an operation matrix. The adjacency matrix shows the connections between nodes in the cell, while the operation matrix indicates the type of operation performed at each node. This encoding method simplifies the representation of complex architectures, making it easier to perform architecture search. The encoding method closely resembles that of predictor-based NAS methods.", "section": "A.3 Integrating MOTE with Evolutionary Search"}]