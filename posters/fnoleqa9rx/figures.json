[{"figure_path": "fNoleQa9RX/figures/figures_1_1.jpg", "caption": "Figure 1: Given an upstream dataset of general real image-text pairs, we aim to curate a targeted dataset to train a learner on some target task. We can either (1) retrieve targeted real images directly from the upstream dataset, or we can (2) first train an intermediate generative model and then synthesize targeted synthetic images. By comparing these two approaches, our paper seeks to measure what value training on generated synthetic data adds.", "description": "This figure illustrates the two approaches used in the paper to create a targeted dataset for training a machine learning model.  The first approach involves retrieving targeted real images directly from an existing, large dataset. The second approach involves training a generative model on the large dataset and then using the model to synthesize new, targeted images. The goal is to compare the effectiveness of these two methods for training a model on a specific task.", "section": "3 Problem Setting and Method"}, {"figure_path": "fNoleQa9RX/figures/figures_4_1.jpg", "caption": "Figure 2: We adapt a pretrained CLIP image encoder (dashed purple line) to different downstream image classification tasks, using either (a) targeted synthetic data (orange triangles) generated from a Stable Diffusion model trained on LAION-2B or using (b) targeted real data (blue circles) directly retrieved from LAION-2B. We measure performance via downstream zero-shot (ZS) and linear probing (LP) accuracy, aggregating results over at least 3 seeds (error bars indicate \u00b11 standard deviation). Overall, while adapting CLIP with targeted synthetic data can sometimes improve performance over an off-the-shelf model, synthetic data is universally outperformed or matched by targeted real data. This gap persists even when we scale the sample size of the synthetic adaptation dataset beyond the maximum amount of (finite) targeted real data considered (gray shaded regions).", "description": "This figure shows the results of adapting a pretrained CLIP model to five different downstream image classification tasks using either targeted synthetic data generated by Stable Diffusion or targeted real data retrieved from LAION-2B.  It compares the zero-shot and linear probing accuracy of the model trained on synthetic vs. real data across various dataset sizes.  The key finding is that real data consistently outperforms synthetic data, even when the size of the synthetic dataset is significantly larger.", "section": "Main Experiments"}, {"figure_path": "fNoleQa9RX/figures/figures_6_1.jpg", "caption": "Figure 3: We visualize retrieved real images and synthetic images from our targeted adaptation datasets for FGVC-Aircraft (top two rows) and ImageNet-1K (bottom two rows), alongside ground truth images (left column) for reference. Compared to retrieved real images, synthetic images often (1) contain generator artifacts (e.g., the blur on the edges of the \u201cCessna 172\u201d, the eyes and mouth of the \u201cTabby Cat\u201d) and also (2) distort class-relevant visual content, such as the engine configuration of a true \u201cAirbus A320\u201d (i.e., exactly one engine per wing) and the entire visual appearance of a \u201cFlute\u201d. We hypothesize that both factors contribute to synthetic training data\u2019s underperformance versus real training data.", "description": "This figure shows a comparison of real images retrieved from LAION-2B and synthetic images generated by Stable Diffusion for four different classes (Airbus A320, Cessna 172, Flute, Tabby Cat).  The left column displays a ground truth image for each class. The next column displays several examples of real images that were retrieved from LAION-2B dataset. The right column presents synthetic images generated by Stable Diffusion model. The figure aims to illustrate the presence of artifacts and distortions of class-relevant details in synthetic images as compared to their real counterparts. The authors hypothesize that these artifacts and distortions in the synthetic images are the reasons for the underperformance of synthetic training data when compared to real training data.", "section": "5.1 Why does synthetic data lag retrieved real data?"}, {"figure_path": "fNoleQa9RX/figures/figures_7_1.jpg", "caption": "Figure 4: We use Stable Diffusion to synthetically perturb real images according to a noise strength parameter \u03b3\u2208 [0, 1], where larger \u03b3 increases the severity of generator-specific artifacts added by the perturbation. When \u03b3 \u2265 0.6, the introduced artifacts can be strong enough to damage task-relevant visual details for finegrained tasks like FGVC-Aircraft (e.g., the airplane's engine and rear wheels). For broad tasks like ImageNet, artifacts have a lesser impact on class-relevant details; the \u201cTabby Cat\u201d is recognizable as a cat even after perturbing with high \u03b3.", "description": "This figure visualizes the effect of synthetically perturbing real images using Stable Diffusion with varying noise strength (\u03b3).  The top row shows an \"Airbus A320\" image and the bottom row shows a \"Tabby Cat\" image, both subjected to increasing levels of perturbation (\u03b3 = 0.2, 0.4, 0.6, 0.8). The goal is to demonstrate how increasing noise affects image quality and detail, especially for fine-grained recognition tasks.  As \u03b3 increases, noticeable artifacts and distortions appear, impacting the visual details important for correct classification. The impact of artifacts is more pronounced in fine-grained classification (Airbus) compared to broader categories (Cat).", "section": "5.1 Why does synthetic data lag retrieved real data?"}, {"figure_path": "fNoleQa9RX/figures/figures_7_2.jpg", "caption": "Figure 2: We adapt a pretrained CLIP image encoder (dashed purple line) to different downstream image classification tasks, using either (a) targeted synthetic data (orange triangles) generated from a Stable Diffusion model trained on LAION-2B or using (b) targeted real data (blue circles) directly retrieved from LAION-2B. We measure performance via downstream zero-shot (ZS) and linear probing (LP) accuracy, aggregating results over at least 3 seeds (error bars indicate \u00b11 standard deviation). Overall, while adapting CLIP with targeted synthetic data can sometimes improve performance over an off-the-shelf model, synthetic data is universally outperformed or matched by targeted real data. This gap persists even when we scale the sample size of the synthetic adaptation dataset beyond the maximum amount of (finite) targeted real data considered (gray shaded regions).", "description": "The figure shows the results of adapting a pretrained CLIP image encoder to five different downstream image classification tasks using either targeted synthetic data generated from a Stable Diffusion model or targeted real data retrieved from the LAION-2B dataset.  The results demonstrate that using real data consistently outperforms synthetic data across various dataset sizes and evaluation metrics (zero-shot and linear probing accuracy).", "section": "4 Main Experiments"}, {"figure_path": "fNoleQa9RX/figures/figures_8_1.jpg", "caption": "Figure 2: We adapt a pretrained CLIP image encoder (dashed purple line) to different downstream image classification tasks, using either (a) targeted synthetic data (orange triangles) generated from a Stable Diffusion model trained on LAION-2B or using (b) targeted real data (blue circles) directly retrieved from LAION-2B. We measure performance via downstream zero-shot (ZS) and linear probing (LP) accuracy, aggregating results over at least 3 seeds (error bars indicate \u00b11 standard deviation). Overall, while adapting CLIP with targeted synthetic data can sometimes improve performance over an off-the-shelf model, synthetic data is universally outperformed or matched by targeted real data. This gap persists even when we scale the sample size of the synthetic adaptation dataset beyond the maximum amount of (finite) targeted real data considered (gray shaded regions).", "description": "This figure compares the performance of fine-tuning a pre-trained CLIP model on targeted synthetic data generated from Stable Diffusion versus targeted real data retrieved from LAION-2B across five downstream image classification tasks.  The results show that while synthetic data can sometimes improve performance over an off-the-shelf model, it's consistently outperformed or matched by real data retrieved directly from LAION-2B, even when scaling the amount of synthetic data.", "section": "Main Experiments"}, {"figure_path": "fNoleQa9RX/figures/figures_9_1.jpg", "caption": "Figure 2: We adapt a pretrained CLIP image encoder (dashed purple line) to different downstream image classification tasks, using either (a) targeted synthetic data (orange triangles) generated from a Stable Diffusion model trained on LAION-2B or using (b) targeted real data (blue circles) directly retrieved from LAION-2B. We measure performance via downstream zero-shot (ZS) and linear probing (LP) accuracy, aggregating results over at least 3 seeds (error bars indicate \u00b11 standard deviation). Overall, while adapting CLIP with targeted synthetic data can sometimes improve performance over an off-the-shelf model, synthetic data is universally outperformed or matched by targeted real data. This gap persists even when we scale the sample size of the synthetic adaptation dataset beyond the maximum amount of (finite) targeted real data considered (gray shaded regions).", "description": "This figure compares the performance of fine-tuning a pretrained CLIP model on targeted synthetic data generated from a Stable Diffusion model versus fine-tuning on targeted real data retrieved directly from the LAION-2B dataset.  The results show that across various downstream image classification tasks and dataset sizes, the real data consistently outperforms or matches the synthetic data, even when the synthetic dataset is significantly larger.", "section": "Main Experiments"}, {"figure_path": "fNoleQa9RX/figures/figures_15_1.jpg", "caption": "Figure 2: We adapt a pretrained CLIP image encoder (dashed purple line) to different downstream image classification tasks, using either (a) targeted synthetic data (orange triangles) generated from a Stable Diffusion model trained on LAION-2B or using (b) targeted real data (blue circles) directly retrieved from LAION-2B. We measure performance via downstream zero-shot (ZS) and linear probing (LP) accuracy, aggregating results over at least 3 seeds (error bars indicate \u00b11 standard deviation). Overall, while adapting CLIP with targeted synthetic data can sometimes improve performance over an off-the-shelf model, synthetic data is universally outperformed or matched by targeted real data. This gap persists even when we scale the sample size of the synthetic adaptation dataset beyond the maximum amount of (finite) targeted real data considered (gray shaded regions).", "description": "This figure compares the performance of fine-tuning a pre-trained CLIP image encoder on both targeted synthetic and real data for five downstream image classification tasks.  The results demonstrate that using real data consistently outperforms or matches the performance of synthetic data, even when the amount of synthetic data significantly exceeds the amount of real data available.  Zero-shot and linear probing accuracies are presented for various dataset sizes.", "section": "Main Experiments"}, {"figure_path": "fNoleQa9RX/figures/figures_16_1.jpg", "caption": "Figure 2: We adapt a pretrained CLIP image encoder (dashed purple line) to different downstream image classification tasks, using either (a) targeted synthetic data (orange triangles) generated from a Stable Diffusion model trained on LAION-2B or using (b) targeted real data (blue circles) directly retrieved from LAION-2B. We measure performance via downstream zero-shot (ZS) and linear probing (LP) accuracy, aggregating results over at least 3 seeds (error bars indicate \u00b11 standard deviation). Overall, while adapting CLIP with targeted synthetic data can sometimes improve performance over an off-the-shelf model, synthetic data is universally outperformed or matched by targeted real data. This gap persists even when we scale the sample size of the synthetic adaptation dataset beyond the maximum amount of (finite) targeted real data considered (gray shaded regions).", "description": "This figure shows the results of adapting a pretrained CLIP model to five different downstream image classification tasks using either targeted synthetic data generated by Stable Diffusion or targeted real data retrieved from LAION-2B.  The results, shown as zero-shot and linear probing accuracy across various dataset sizes, consistently demonstrate that using real data significantly outperforms using synthetic data, even when the synthetic dataset is much larger than the available real data.", "section": "Main Experiments"}, {"figure_path": "fNoleQa9RX/figures/figures_17_1.jpg", "caption": "Figure 2: We adapt a pretrained CLIP image encoder (dashed purple line) to different downstream image classification tasks, using either (a) targeted synthetic data (orange triangles) generated from a Stable Diffusion model trained on LAION-2B or using (b) targeted real data (blue circles) directly retrieved from LAION-2B. We measure performance via downstream zero-shot (ZS) and linear probing (LP) accuracy, aggregating results over at least 3 seeds (error bars indicate \u00b11 standard deviation). Overall, while adapting CLIP with targeted synthetic data can sometimes improve performance over an off-the-shelf model, synthetic data is universally outperformed or matched by targeted real data. This gap persists even when we scale the sample size of the synthetic adaptation dataset beyond the maximum amount of (finite) targeted real data considered (gray shaded regions).", "description": "This figure compares the performance of fine-tuning a pre-trained CLIP model on two types of data for various downstream image classification tasks: targeted synthetic images generated by Stable Diffusion and targeted real images retrieved from LAION-2B.  The results show that even though synthetic data can sometimes improve performance, real data consistently outperforms synthetic data across all tasks and dataset sizes.", "section": "Main Experiments"}]