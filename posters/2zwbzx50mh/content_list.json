[{"type": "text", "text": "Compact Proofs of Model Performance via Mechanistic Interpretability ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jason Gross\u2217 Rajashree Agrawal Thomas Kwa\u2020 Euan Ong\u2020 Chun Hei Yip\u2020 ", "page_idx": 0}, {"type": "text", "text": "Alex Gibson\u2021 Soufiane Noubir\u2021 Lawrence Chan ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We propose using mechanistic interpretability \u2013 techniques for reverse engineering model weights into human-interpretable algorithms \u2013 to derive and compactly prove formal guarantees on model performance. We prototype this approach by formally proving accuracy lower bounds for a small transformer trained on Max-of$K$ , validating proof transferability across 151 random seeds and four values of $K$ We create 102 different computer-assisted proof strategies and assess their length and tightness of bound on each of our models. Using quantitative metrics, we find that shorter proofs seem to require and provide more mechanistic understanding. Moreover, we find that more faithful mechanistic understanding leads to tighter performance bounds. We confirm these connections by qualitatively examining a subset of our proofs. Finally, we identify compounding structureless errors as a key challenge for using mechanistic interpretability to generate compact proofs on model performance. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "One approach to ensuring the safety and reliability of powerful AI systems is via formally verified proofs of model performance [48, 11]. If we hope to deploy formal verification on increasingly large models [24, 27] with powerful emergent capabilities [56] across more diverse and broader domains [5, 46], we will need compact proofs of generalization bounds on specific models that certify global robustness. However, existing approaches tend to use proof strategies that suffer from bad asymptotic complexity, while verifying either generalization properties of training procedures or local robustness properties of specific models. ", "page_idx": 0}, {"type": "text", "text": "One key challenge to verification is that neural network architectures are highly expressive [51, 58], and models with similar training procedure and performance may still have learned significantly different weights [38, 9]. This expressivity makes it difficult to adequately compress explanations of global model behavior in ways that correspond closely enough to the model\u2019s actual mechanisms to be useful for efficient verification without being too lossy, especially when using only knowledge of the architecture or training procedure. We propose verifying model performance using understanding derived from mechanistic interpretability (Section 2) \u2013 that is, reverse engineering the specific implementation of the algorithm from the learned weights of particular models. Knowledge of the specific implementation allows us to construct less lossy simplifications of the model, and more efficiently reason about model performance over possible inputs. ", "page_idx": 0}, {"type": "text", "text": "In this work, we provide a case study of translating mechanistic interpretations into compact proofs. We train an attention-only transformer on a Max-of- $\\cal{K}$ task with 151 random seeds (Section 3), and then reverse engineer the models using standard mechanistic interpretability techniques. We use our understanding to define a set of 102 different computer-assisted proof strategies with varying tightness of bound and with different asymptotic complexity and number of required FLOPs (Section 4).4We validate our technique against an additional 604 models for varying values of $K$ (Appendix A.2.1). ", "page_idx": 0}, {"type": "image", "img_path": "2zWbzx50mH/tmp/9894865199125c971357c45df42992f2e0476948bcc8d1d6d4ee867d508d0bf4.jpg", "img_caption": ["Figure 1: We construct proofs using different degrees of mechanistic interpretation. (Left) The models we consider in this paper are one-layer attention-only transformers, and so contain three \u201cpaths\u201d: the OV circuit, the QK circuit, and the direct path. (Right) For the brute-force proof (Section 4.3.1), we treat the model as a black box and thus need to check all possible combinations of inputs. For the cubic proof (Section 4.3.1), we decompose the model into its three corresponding paths, but still check the correctness of each path via brute force. Finally, in some subcubic proofs (Section 4.3), we use all parts of the mechanistic interpretation presented in Section 3. (Bottom) For each of the three categories of proof, we report the number of FLOPs used in computing the certificate (lower=better, Appendix A.6), lower bound on model accuracy (higher=better), effective dimension of the unexplained parts of the model (lower=better, Appendix A.5), and asymptotic complexity of the proof strategy as we scale the inputs and model (lower=better). Significantly more compact proofs have vacuous accuracy bounds by default. Using more mechanistic understanding allows us to recover some, but not all, of the accuracy bounds on these more compact proofs, as our understanding is not fully faithful to the model internals. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We define a quantitative metric to assess the mechanistic understanding used in a proof strategy by the dimensionality of the function space that the proof strategy must consider, which we deem the unexplained dimensionality of the proof strategy (Sections 5.1, and A.5). Using this metric, we find a negative relationship between proof length and degree of understanding. We qualitatively examine proof strategies to confirm and explain this relationship, finding that more compact proofs both require and provide more mechanistic understanding. We also find suggestive evidence that the trade-off between proof length and tightness of bound is modulated by the faithfulness of the mechanistic understanding used to derive the proof (Section 5.2).5 ", "page_idx": 1}, {"type": "text", "text": "However, we also identify compounding structureless error terms as a key challenge for generating compact proofs on model behavior (Sections 5.3, and G.2.5). The implementation of algorithms inside of neural networks may contain components that defy mechanistic understanding and appear to us as \u201cnoise\u201d. When we don\u2019t know how noise composes across model components, establishing a bound requires pessimizing over the ways the composition could occur. Worst-case noise can quickly grow even when the empirical noise is small, leading to vacuous performance bounds. ", "page_idx": 1}, {"type": "text", "text": "2 Mechanistic interpretability for proofs ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Generalization bounds on global performance In the style of prior mechanistic interpretability evaluation work [6], we target theorem templates that establish bounds on the expected global performance of the model. Let $\\mathcal{M}:X\\rightarrow Y$ be a model (here assumed to be a neural network), $\\mathcal{D}$ be a probability distribution over input-label pairs $(l,\\mathbf{t})\\in L\\times X$ , notated as $\\mathcal{D}\\vert_{X}$ when marginalized over labels, and $f:L\\times Y\\to\\mathbb{R}$ be a scoring function for evaluating the performance of the model. Then, we seek to establish lower bounds $b$ on the expected $\\bar{s}$ as the form: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\bar{s}:=\\mathbb{E}_{(l,\\mathbf{t})\\sim\\mathcal{D}}\\left[f(l,\\mathcal{M}(\\mathbf{t}))\\right]\\geq b.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "As $f$ can be any metric, this is a fully general template for theorems that can capture any aspect of model performance for which we have a formal specification. However, in this work we restrict $f$ to be the accuracy and $\\mathcal{D}\\vert_{X}$ to be uniform, so our theorems lower bound the accuracy of the model. Our proof methodology generalizes straightforwardly to other input distributions (Appendix A.8), and only a little work is required to generalize from accuracy to log-loss (Appendix A.11). ", "page_idx": 2}, {"type": "text", "text": "Proof template The proofs of model performance in this work have two components: a computational component $C$ : model weights $\\rightarrow\\mathbb{R}$ and a non-computational component $Q$ arguing that for any model $\\mathcal{M}^{\\prime}$ , $C(\\mathcal{M}^{\\prime})\\,\\le\\,\\mathbb{E}_{(l,\\mathbf{t})\\sim\\mathcal{D}}\\,f(l,\\mathcal{M}^{\\prime}(\\mathbf{t}))$ , thus implying that $C$ generates a valid lower bound for the performance of $\\mathcal{M}$ . The whole proof is $Q$ paired with a trace of running $C$ that certifies its output on $\\mathcal{M}$ .6 Here, $b=C(\\mathcal{M})$ . As even the size of the model parameters is much larger than any reasonable $Q$ , we approximate the length of a proof pair $C,Q$ by the length of a trace of $C(\\mathcal{M})$ . ", "page_idx": 2}, {"type": "text", "text": "Proof compactness vs. tightness of bound Different proof strategies make different tradeoffs between compactness and tightness of bound. For example, consider two extreme proof strategies: We can \u201cprove\u201d a vacuous bound using a null proof. On the other hand, in the brute-force proof, we simply run the model on the entirety of $\\mathcal{D}$ to achieve $b=\\bar{s}$ , albeit with a very long proof. ", "page_idx": 2}, {"type": "text", "text": "We quantify the length of $C(\\mathcal{M})$ using two metrics: the asymptotic time complexity of $C$ as we scale the size of the model and the input $\\mathbf{t}$ , as well as the empirical average number of floating point operations required to evaluate $C(\\mathcal{M}^{\\prime})$ over a given set of models $\\{\\mathcal{M}_{i}\\bar{\\}$ . We measure tightness of bound of $C(\\mathcal{M})$ using the ratio of the bound to the true accuracy: $b/\\bar{s}$ . ", "page_idx": 2}, {"type": "text", "text": "Proof as pessimal ablation A standard way of assessing the faithfulness of mechanistic interpretability is by ablating the parts of the model that your interpretation does not explain [54, 6, 23]. In this framework, proofs can be thought of as performing a pessimal ablation over the unexplained parts of the model \u2013 we set the remaining components of the model (the \u201cnoise\u201d or error terms) to values over $X$ that minimize the performance of the model. However, the number of ablations required for a complete argument might be quite high. Thus, we construct relaxations (Appendix A.4) over input sequences, such that performing pessimal ablations on a smaller number of relaxed input sequences is sufficient to lower bound the performance on $\\mathcal{D}$ . ", "page_idx": 2}, {"type": "text", "text": "3 Experimental setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We study our approach to generating compact proofs in a simple toy setting: Max-of- $K$ ", "page_idx": 2}, {"type": "text", "text": "Model Architecture We study one-layer, one-head, attention-only transformers with no biases but with learned positional embeddings, with vocabulary size $d_{\\mathrm{vocab}}$ , model and head dimension $d=d_{\\mathrm{model}}=d_{\\mathrm{head}}^{\\ \\^}$ , and context length $n_{\\mathrm{ctx}}:=k$ . The model parameters consist of the $n_{\\mathrm{ctx}}\\!\\times\\!d_{\\mathrm{model}}$ positional embedding $P$ ; the $d_{\\mathrm{vocab}}\\times d_{\\mathrm{model}}$ token embed $E$ ; the $d_{\\mathrm{model}}\\times d_{\\mathrm{model}}$ query, key, value, and output matrices of the attention head $Q,K,V$ , and $O$ ; as well as the $d_{\\mathrm{model}}\\times d_{\\mathrm{vocab}}$ unembed matrix $U$ . We assume (as is standard in language modeling) that $d_{\\mathrm{model}}<d_{\\mathrm{vocab}}$ . ", "page_idx": 2}, {"type": "text", "text": "For an $n_{\\mathrm{ctx}}\\,\\times\\,d_{\\mathrm{vocab}}$ one-hot encoding $\\mathbf{x}~=~\\left[x_{0},x_{1},\\ldots,x_{n_{\\mathrm{ctx}}-1}\\right]$ of an input sequence $\\mathrm{~\\bf~t~}=$ $[t_{0},t_{1},.~.~.~,t_{n_{\\mathrm{ctx}}-1}]$ , we compute the logits of the model as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{h^{(0)}=\\mathbf{x}E+P}&&{\\qquad\\mathrm{Initial~residual~stream}\\;(n_{\\mathrm{ctx}}\\times d_{\\mathrm{model}})}\\\\ &{\\quad\\alpha=h^{(0)}Q K^{T}{h^{(0)}}^{T}/\\sqrt{d}}&&{\\qquad\\mathrm{Attention~matrix}\\;(n_{\\mathrm{ctx}}\\times n_{\\mathrm{ctx}})}\\\\ &{h^{(1)}=\\sigma^{*}(\\alpha)\\cdot h^{(0)}V O+h^{(0)}}&&{\\qquad\\mathrm{Final~residual~stream}\\;(n_{\\mathrm{ctx}}\\times d_{\\mathrm{model}})}\\\\ &{M(\\mathbf{t})=\\ell=h_{n_{\\mathrm{ctx}}-1}^{(1)}U}&&{\\qquad\\mathrm{Final~seq.\\;position~logits}\\;(d_{\\mathrm{vocab}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\sigma^{*}$ is the masked softmax function used in causal attention. Because we only look at outputs of the model above the final sequence position $i=n_{\\mathrm{ctx}}-1$ , we also denote this position as the \u201cquery position\u201d and the value of the token in this position as $t_{\\mathrm{query}}$ , one-hot encoded as $x_{\\mathrm{{query}}}$ . The model\u2019s prediction is the token corresponding to the max-valued logit $\\ell_{\\mathrm{max}}$ . ", "page_idx": 2}, {"type": "text", "text": "Task Specifically, we study the setting with $n_{\\mathrm{ctx}}=k=4$ because it is the largest sequence length for which we can feasibly evaluate the brute-force proof. We set hidden dimension $d_{\\mathrm{model}}=32$ and a vocabulary of size $d_{\\mathrm{vocab}}=64$ comprising integers between 0 and 63 inclusive. For an input sequence $\\mathbf{t}$ , we denote the true maximum of the sequence by $t_{\\mathrm{max}}$ . Outputting the correct behavior is equivalent to outputting logits $\\ell$ such that $\\Delta\\ell_{t^{*}}:=\\ell_{t^{*}}-\\ell_{\\operatorname*{max}}<0$ for all $t^{*}\\neq t_{\\mathrm{max}}$ . We trained 151 models on this task. Models achieved average accuracy $0.9992\\pm0.0015$ over the entire distribution. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Path decomposition Following prior work [13], we expand the logits of the model and split the paths through the model into three components \u2013 the QK circuit, the OV circuit, and the direct path: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{M(\\mathbf{t})=\\sigma^{*}\\Big(\\underbrace{\\left(x_{\\mathrm{query}}E+P_{\\mathrm{query}}\\right)Q K^{T}\\left(\\mathbf{x}E+P\\right)^{T}}_{\\mathrm{QK~eircuit}}/\\sqrt{d}\\Big)\\cdot\\underbrace{\\left(\\mathbf{x}E+P\\right)V O U}_{\\mathrm{ov~eircuit}}+\\underbrace{\\left(x_{\\mathrm{query}}E+P_{\\mathrm{query}}\\right)U_{\\mathrm{phy}}}_{\\mathrm{direct~path}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Intuitively, the QK circuit determines which tokens the model attends to from a particular query token and sequence position, while the OV circuit processes the tokens and sequence positions the model attends to. The direct path is simply the skip connection around the attention head. ", "page_idx": 3}, {"type": "text", "text": "We further divide the QK and OV circuits into token (position-independent) and position-dependent components. Let $\\begin{array}{r}{P_{\\mathrm{avg}}\\,=\\,\\sum_{i}P_{i}/n_{\\mathrm{ctx}}}\\end{array}$ be the average position embeds across positions (of size $d_{\\mathrm{model}})$ , and let $\\bar{\\mathbf{P}}$ denote either $\\mathbf{1}_{n_{\\mathrm{ctx}}}\\otimes P_{\\mathrm{avg}}$ or $\\mathbf{1}_{d_{\\mathrm{vocab}}}\\otimes P_{\\mathrm{avg}}$ depending on context, the result of broadcasting $P_{\\mathrm{avg}}$ back into the shape of $P$ or $E$ (that is, $n_{\\mathrm{ctx}}\\times\\bar{d}_{\\mathrm{model}}$ or $d_{\\mathrm{vocab}}\\times d_{\\mathrm{model}})$ . Similarly, let $\\mathbf{P}_{q}=\\mathbf{1}_{d_{\\mathrm{vocab}}}\\otimes P_{\\mathrm{query}}$ be the result of broadcasting $\\ensuremath{P_{\\mathrm{{query}}}}$ . Then for one-hot encoded $\\mathbf{x}$ , we can rewrite the QK and OV circuits, as well as the direct path, as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{QK}\\,\\mathrm{circuit}=x_{\\mathrm{query}}\\Big(\\underbrace{\\mathbf{E}_{q}Q K^{T}\\bar{\\mathbf{E}}^{T}}_{\\mathrm{EQKE}}\\mathbf{x}^{T}+\\underbrace{\\mathbf{E}_{q}Q K^{T}\\hat{\\mathbf{P}}^{T}}_{\\mathrm{EQKP}}\\Big)}\\\\ &{\\mathrm{OV}\\,\\mathrm{circuit}=\\mathbf{x}\\underbrace{\\bar{\\mathbf{E}}V O U}_{\\mathrm{EVOU}}+\\underbrace{\\hat{\\mathbf{P}}V O U}_{\\mathrm{PVOU}}\\qquad\\mathrm{Direct}\\,\\mathrm{Path}=x_{\\mathrm{query}}\\underbrace{\\mathbf{E}_{q}U}_{\\mathrm{EU}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{\\mathbf{P}}=P-\\bar{\\mathbf{P}}$ and $\\bar{\\mathbf{E}}=E+\\bar{\\mathbf{P}}$ and ${\\bf E}_{q}=E+{\\bf P}_{q}$ (since $h^{(0)}=\\mathbf{x}\\bar{\\mathbf{E}}+\\hat{\\mathbf{P}})$ ). ", "page_idx": 3}, {"type": "text", "text": "3.1 Mechanistic interpretation of learned models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Using standard empirical mechanistic interpretability techniques, we interpret one of our learned models (our \u201cmainline\u201d model) by independently examining the QK and OV circuits and the direct path.7 We find that the model outputs the largest logit on the true max token $t_{\\mathrm{max}}$ by attending more to larger tokens via the QK circuit and copying the tokens it attends to via the OV circuit. We then quantitatively confirm that these interpretations hold for all 151 models by reporting the mean plus minus standard deviation for various summary statistics. Plots for this section are available in Appendix B.2. ", "page_idx": 3}, {"type": "image", "img_path": "2zWbzx50mH/tmp/2dfee8c17ab7507041223d8eb4e1c3449397dcaade1163257c97867692da2982.jpg", "img_caption": ["Figure 2: The models in our setting implement Max-of- $\\cdot K$ by attending exponentially more to larger tokens and copying the attended-to tokens (Section 3.1). "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "QK circuit By qualitatively examining the positionindependent QK component EQKE, we find the amount of pre-softmax attention paid to a key token is approximately independent of the value of the query token $\\ensuremath{t_{\\mathrm{query}}}$ , and increases monotonically based on the size of the key token. We confirm this hypothesis by performing a singular-value decomposition (SVD) of the EQKE matrices (Appendix G.2.3), and find that it contains a single large rank-one component with singular value around $7800\\pm380$ , around $620\\pm130$ times larger than the second largest component with singular value $13\\pm3$ . The left (query-side) singular vector is approximately constant in all dimensions, with value $0.1243\\pm0.0003\\approx V_{8}=1/\\sqrt{d_{\\mathrm{vocab}}}$ . The right (key-side) singular ve\u221actor of this component is monotonically increasing as we increase the size of the key token, with $(1/{\\sqrt{d}}$ -scaled) pre-softmax attention increasing by an average of $1.236\\pm0.056$ when the key token increases by 1.8 ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "In comparison, each $1/\\sqrt{d}$ -scaled entry of the position-dependent QK component EQKP has negligible size (average $0.31\\pm0.18)$ , suggesting that EQKP is unimportant to the functioning of the model. ", "page_idx": 3}, {"type": "text", "text": "We confirm this by zero ablating EQKP, which changes the models\u2019 accuracies from $0.9992{\\scriptstyle\\pm0.0015}$ to $0.9993\\pm0.0011$ . Combined with our interpretation of EQKE, this implies that the attention pattern of the model depends only on the token values and not the ordering of the sequence. ", "page_idx": 4}, {"type": "text", "text": "OV circuit Then, by qualitatively examining the position-independent OV component EVOU, we see that it has large positive entries along the diagonal. In fact, the entry along the diagonal is the largest in the row for all rows corresponding to $t>6.6\\pm1.2$ . Since each entry in the sequence is uniformly sampled and $d_{\\mathrm{vocab}}=64$ , this means that EVOU is a good approximation for the identity matrix for all but $\\approx(7/64)^{4}\\approx1.2\\times10^{-2}\\,\\%$ of the sequences. ", "page_idx": 4}, {"type": "text", "text": "As with the position-dependent QK component, the position-dependent OV component PVOU also has negligible size and is unimportant to model performance. Taken together with the above results on EVOU, this suggests that the attention head copies the tokens it attends to. ", "page_idx": 4}, {"type": "text", "text": "Direct path As with the two position-dependent components, the entries in EU have small absolute magnitude $2.54\\pm0.20$ ,9 and contribute negligibly to model performance. ", "page_idx": 4}, {"type": "text", "text": "4 Proofs of model performance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section we describe intuitions for three categories of proof that are developed around different mechanistic interpretations and methods for using the interpretations. The strategies result in proofs of different complexities with varying bound tightness (Table 1). We provide detailed theorem statements, proofs, algorithms, and explanations of proof search in Appendices C, D, E, F, and G. ", "page_idx": 4}, {"type": "text", "text": "Our theorem statements for $Q$ will all be of the form ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall\\mathcal{M}^{\\prime},C_{s p e c i f i c\\;s t r a t e g y}(\\mathcal{M}^{\\prime})\\leq\\mathbb{E}_{\\mathbf{t}\\sim\\mathcal{D}\\mid x}\\,f\\big(t_{\\operatorname*{max}},\\mathcal{M}^{\\prime}(\\mathbf{t})\\big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We leave implicit the traces of running Cspecific strategy on our specific models to give the overall theorem. We report the computational complexity or estimated FLOPs of running Cspecific strategy as approximations for our proof lengths. ", "page_idx": 4}, {"type": "text", "text": "4.1 The brute-force baseline ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We start by considering the brute-force proof (Appendix D), which treats the model as a black box and evaluates it on all possible sequences.10 However, this proof strategy has bad asymptotic complexity and is untenable for larger models and larger input distributions. So in subsequent sections, we use knowledge of the model drawn from the interpretation in Section 3.1 to derive more compact proofs. ", "page_idx": 4}, {"type": "text", "text": "4.2 A cubic proof ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Next, we use the fact that the model is composed of the direct path and the QK and OV circuits (Section 3) to decrease the number of sequences that we need to consider, and the fact that only the position-independent components EQKE and EVOU contribute meaningfully to performance (Section 3.1) to pessimize over sequence ordering. ", "page_idx": 4}, {"type": "text", "text": "First, let a pure sequence $\\xi$ be a sequence with at most three distinct tokens: the max token $t_{\\mathrm{max}}$ , the final token $t_{\\mathrm{query}}\\leq t_{\\mathrm{max}}$ , and optionally a third token $t^{\\prime}<t_{\\mathrm{max}}$ , and let $\\Xi^{\\mathrm{pure}}$ be the set of all pure sequences in $X$ .11 For a given input sequence t, define the adjacent pure sequences $\\operatorname{Adj}(\\mathbf{t})$ as the set of sequences that share the same max and query token, and only take on values in $\\mathbf{t}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{Adj}(\\mathbf t)=\\left\\{\\xi\\in\\Xi^{\\mathrm{pure}}\\,\\bigg|\\,\\operatorname*{max}_{i}\\xi_{i}=t_{\\mathrm{max}},\\;\\xi_{\\mathrm{query}}=t_{\\mathrm{query}},\\forall i<n_{\\mathrm{ctx}},\\;\\xi_{i}\\in\\mathbf t\\right\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Using the convexity of softmax and the fact that the model contains three paths, we can show that one-layer attention-only transformers satisfies a variant of the following convexity property: for a given $\\mathbf{t}$ , if $\\mathcal{M}(\\xi)$ is correct for all $\\xi\\in\\mathrm{Adj}(\\mathbf{t})$ , then $\\mathcal{M}(\\mathbf{t})$ is correct. That is, for these transformers, we can bound the accuracy on all sequences by evaluating $\\mathcal{M}$ on only the $O(d_{\\mathrm{vocab}}^{3}(n_{\\mathrm{ctx}}-1)!)$ ", "page_idx": 4}, {"type": "table", "img_path": "2zWbzx50mH/tmp/d797417e66b2ff8a21bd5518ac0ba3a0ac08056cb41c95df27cc200576acc65a.jpg", "table_caption": ["Table 1: We report the proof complexity, accuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5). We round the FLOP and unexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound averaged across all 151 models. As we include more aspects of the mechanistic interpretation (reflected by a lower number of unexplained dimensions), we get more compact proofs (in terms of both asymptotic complexity and FLOPs), albeit with worse bounds. For space reasons, we use $k:=n_{\\mathrm{ctx}}$ , $d:=d_{\\mathrm{model}}$ , and $v:=d_{\\mathrm{vocab}}$ . "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "pure sequences. This allows us to bound the accuracy of our actual $\\mathcal{M}$ on all ${d_{\\mathrm{vocab}}}^{n_{\\mathrm{ctx}}}$ nctx sequences, while evaluating it on $O(d_{\\mathrm{vocab}}^{3}(n_{\\mathrm{ctx}}-1)!)$ ) sequences. ", "page_idx": 5}, {"type": "text", "text": "We can reduce the number of sequences that we need to evaluate by pessimizing over the order of a sequence. For a given tuple of $(t_{\\mathrm{max}},t_{\\mathrm{query}},t^{\\prime})$ , there are $(n_{\\mathrm{ctx}}-1)!$ pure sequences, corresponding to the permutations of the tuple. Pessimizing over the order of sequences reduces the number of sequences to consider for each $(t_{\\mathrm{max}},t_{\\mathrm{query}},t^{\\prime})$ tuple to the number of $t^{\\prime}$ in the pure sequence, and the total number of sequences to $O(d_{\\mathrm{vocab}}{}^{3}n_{\\mathrm{ctx}})$ . By precomputing the five component matrices EU, EQKE, EQKP, EVOU, PVOU and cleverly caching intermediate outputs, we can reduce the additional work of each sequence to the ${\\cal O}(n_{\\mathrm{ctx}})$ required to compute the softmax over $n_{\\mathrm{ctx}}$ elements, resulting in asymptotic complexity $O(d_{\\mathrm{vocab}}{^3}n_{\\mathrm{ctx}}{^2})$ (Theorem 12, additional details in Appendix E). ", "page_idx": 5}, {"type": "text", "text": "4.3 Sub-cubic proofs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now consider proofs that are more compact than $O(d_{\\mathrm{vocab}}{}^{3})$ . These require avoiding iteration over any set of size $O(d_{\\mathrm{vocab}}{}^{3})$ (e.g. the set of pure sequences) and performing operations that take $O(d_{\\mathrm{vocab}})$ time on each of $O(d_{\\mathrm{vocab}}{}^{2})$ combinations. Unfortunately, some methods of avoiding these operations can lead to vacuous bounds (i.e. accuracy lower bounds near $0\\%$ ). In order to recover non-vacuous bounds, we introduce two tricks: the \u201cmean+diff trick\u201d to better approximate the sum of two components with unequal variance, and the \u201cmax row diff trick\u201d to improve upon the low-rank approximations for EU and EQKE. We consider applying variants of these tricks at different locations in the na\u00efve subcubic proof, leading to 100 distinct subcubic proof strategies. See Appendix G.2 for a formal description of these strategies. ", "page_idx": 5}, {"type": "text", "text": "4.3.1 Removing cubic-time computations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Reducing the number of cases by pessimizing over sufficiently small tokens Previously, we considered $\\Theta(d_{\\mathrm{vocab}}{}^{3}n_{\\mathrm{ctx}})$ pure sequences $\\xi$ , with $\\xi$ parameterized by $(t_{\\mathrm{max}},t_{\\mathrm{query}},t^{\\prime},c)$ . Recall from our mechanistic interpretation in Section 3.1 that the pre-softmax attention paid from $t_{\\mathrm{query}}$ to a key token $t^{\\prime}$ is broadly invariant in $t_{\\mathrm{query}}$ and increases roughly linearly with the size of $\\bar{t^{\\prime}}$ . This allows us to pessimize over the OV circuit over all \u201csufficiently small\u201d tokens. ", "page_idx": 5}, {"type": "text", "text": "More formally, suppose we are given some gap $g\\in\\mathbb{N}$ . For each pure sequence $\\xi$ with max token $t_{\\mathrm{max}}$ , query token $t_{\\mathrm{query}}$ , such that $t_{\\mathrm{query}}\\le t_{\\mathrm{max}}-g$ , and $c$ copies of the third token type $t^{\\prime}\\le t_{\\mathrm{max}}-g$ , we pessimally ablate the OV circuit over the set $\\Xi^{\\mathrm{pure}}(t_{\\mathrm{max}},t_{\\mathrm{query}},c;g)$ of pure sequences $\\xi^{\\prime}$ with the same max and query tokens and $c$ copies of the third token type $t^{\\prime}$ . If the model gets all sequences in $\\Xi^{\\mathrm{pure}}(t_{\\mathrm{max}},t_{\\mathrm{query}},c;g)$ correct, then we can conclude that it gets $\\xi$ correct, otherwise, we treat the model as having gotten $\\xi$ wrong. This means that it suffices to only consider the $O(d_{\\mathrm{vocab}}{}^{2}n_{\\mathrm{ctx}})$ pessimal pure sequences of each of the $O(d_{\\mathrm{vocab}}{}^{2}n_{\\mathrm{ctx}})$ sets of the form $\\Xi^{\\mathrm{pure}}(t_{\\mathrm{max}},t_{\\mathrm{query}},c;g)$ . ", "page_idx": 6}, {"type": "text", "text": "Decoupling and pessimizing computations that require $O(d_{\\mathrm{vocab}}{}^{3})$ computations Many parts of our cubic certificate require iterating through $O(d_{\\mathrm{vocab}}{}^{2})$ cases parameterized by $(t_{\\mathrm{max}},t_{\\mathrm{query}})$ or $(t_{\\mathrm{max}},t^{\\prime})$ . For example, as part of the pessimization procedure over pure sequences, for each of the $d_{\\mathrm{vocab}}$ possible values of $t_{\\mathrm{max}}$ , we need to consider the relative effects on the $d_{\\mathrm{vocab}}$ -sized logits of attending to each of the $O(d_{\\mathrm{vocab}})$ other tokens $t^{\\prime}<t_{\\mathrm{max}}$ , and for each $t_{\\mathrm{max}}$ and $t_{\\mathrm{query}}$ , we need to check that the contribution of the direct path on logits $x_{\\mathrm{{query}}}\\mathrm{{EU}}$ is not sufficiently large as to overwhelm the contribution from $x_{\\mathrm{max}}\\mathrm{EVOU}$ . We independently pessimize over each of these components over one of the $d_{\\mathrm{vocab}}$ -sized axes: for example, instead of computing $x_{\\mathrm{max}}\\mathrm{EVOU}+$ $x_{\\mathrm{{query}}}\\mathrm{{EU}}$ for each $t_{\\mathrm{max}}$ , $t_{\\mathrm{query}}$ pair, we first pessimally ablate the direct path along the query token (which takes $O(d_{\\mathrm{vocab}}{}^{2})$ time as it does not depend on the $t_{\\mathrm{max}}$ , and then consider the sum $x_{\\mathrm{max}}\\mathrm{EVOU}+\\mathrm{max}_{x^{\\prime}}\\:\\chi^{\\prime}\\mathrm{EU}$ . Since this sum no longer depends on $t_{\\mathrm{query}}$ , we only need to perform it $O(d_{\\mathrm{vocab}})$ times, for a total cost of $O(d_{\\mathrm{vocab}}{}^{2})$ . ", "page_idx": 6}, {"type": "text", "text": "Low rank approximations to EQKE and EU Recall from Section 3.1 that EQKE is approximately rank 1, where the sole direction of variation is the size of the key token. By computing only the low rank approximation to EQKE, we can more cheaply compute the most significant component of the behavior in the QK circuit. To bound the remaining error, we can use the fact that after pulling off the first principal component from each of the four matrices we multiply, very little structure remains. ", "page_idx": 6}, {"type": "text", "text": "We can find the rank $1/2$ approximations by performing SVD on EQKE. We can efficiently compute the SVD in $\\mathcal{O}(d_{\\mathrm{vocab}}d_{\\mathrm{model}}{}^{2})$ time by using the fact that EQKE can be written as the product of a $d_{\\mathrm{vocab}}\\times d_{\\mathrm{model}}$ matrix and a $d_{\\mathrm{model}}\\times d_{\\mathrm{vocab}}$ matrix. This allows us to avoid performing the $\\mathcal{O}(d_{\\mathrm{vocab}}{}^{2}d_{\\mathrm{model}})$ -cost matrix multiplications to explicitly compute EQKE. ", "page_idx": 6}, {"type": "text", "text": "Similarly, we can more efficiently check that the direct path EU contributes negligibly to the model outputs, by using SVD to decompose EU into a sum of rank 1 products (which we can evaluate exactly) and a high-rank error term that we can cheaply bound. ", "page_idx": 6}, {"type": "text", "text": "4.3.2 Additional subcubic proof strategies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Tighter bounds for sums of variables with unequal variance via the \u201cmean+diff trick\u201d Suppose we want to lower bound the minimum of the sum of two functions over three variables $h(x,y,z)=$ $f(x,y)+g(y,z)$ , while only iterating over two variables at a time. The na\u00efve way is to minimize $f(x,y)$ and $g(x,y)$ independently: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x,y,z}h(x,y,z)\\geq\\operatorname*{min}_{x,y}f(x,y)+\\operatorname*{min}_{y,z}g(y,z)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, the error comes from setting the $y\\mathbf{s}$ in $f$ and $g$ to different values. But in cases where $g(y,z)$ varies significantly with $y$ and only slightly with $z$ , rewriting $g$ as a sum of a component that is independent of $z$ (only varying along $y$ ), and one that depends on $z$ , yields a better lower bound: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x,y,z}h(x,y,z)\\geq\\operatorname*{min}_{x,y}\\left(f(x,y)+\\mathbb{E}_{z}^{\\prime}g(y,z^{\\prime})\\right)+\\operatorname*{min}_{y,z}(g(y,z)-\\mathbb{E}_{z}^{\\prime}g(y,z^{\\prime}))\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This estimate will have error at most $\\varepsilon$ , while the na\u00efve estimator can have arbitrarily large error. We refer to this rewrite as the \u201cmean+diff trick\u201d.12 From the mechanistic interpretation in Section 3.1, we know that some of the components barely vary among one or more axes. So we can apply the mean+diff trick to get tighter lower bounds. ", "page_idx": 6}, {"type": "text", "text": "Avoiding matrix multiplications using the \u201cmax row-diff trick\u201d Using properties of linear algebra, we derive a cheap approximation to the max row-diff for the product of matrices $A B$ in terms of the product of the max row-diff of $B$ and the absolute value of $A$ , which we deem the \u201cmax row-diff\u201d ", "page_idx": 6}, {"type": "text", "text": "trick. We apply this trick to get a better cheap bound on the error terms of low-rank approximations, without having to multiply out the full matrices. See Appendices G.2.2, and F for more details. ", "page_idx": 7}, {"type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We run each of 151 transformers on the various proof strategies of different asymptotic complexity, and analyze these proofs to empirically examine the relationship between proof length, bound tightness, and degree of understanding. For each proof on each transformer, we approximate the length of the proof by estimating the number of FLOPs used, and plot this against the ratio of certified bound the true accuracy $b/\\bar{s}$ (Equation 2) in Figure 3. There exists a clear trade-off between bound tightness and compactness of the proof \u2013 more compact proofs yield looser bounds, and tighter bounds are associated with more expensive proofs. ", "page_idx": 7}, {"type": "text", "text": "5.1 Compact proofs both require and provide mechanistic understanding ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Quantifying mechanistic understanding using unexplained dimensionality We first quantify the amount of mechanistic understanding used in a proof by measuring its unexplained dimensionality \u2013 the number of free parameters required to fully describe model behavior, assuming the structural assumptions of the proof are correct. More detailed interpretations leave fewer free parameters needing to be fliled in via empirical observation (Appendix A.5). In Figure 5, we plot these axes and find a suggestive correlation: proofs based on less mechanistic understanding are longer. ", "page_idx": 7}, {"type": "text", "text": "More mechanistic understanding allows for more compact proofs In addition to the constructions in Section 4, the parts of proofs we were unable to compact seem to correspond to components that we do not mechanistically understand. For example, we could not cheaply bound the behavior of EVOU without multiplying out the matrices, and this seems in part because we do have a mechanistic understanding of how EVOU implements low-rank copying. ", "page_idx": 7}, {"type": "text", "text": "Compact proofs seem to provide understanding By examining compact proofs, we can extract understanding about the model. For example, the fact that replacing each row of EU with its average across rows has little effect on the bound implies that EU does not vary much based on $\\ensuremath{t_{\\mathrm{query}}}$ . ", "page_idx": 7}, {"type": "text", "text": "5.2 Proof length vs. bound tightness trade-off is modulated by faithfulness of interpretation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Compact proofs are less faithful to model internals To derive more compact proofs, we use our mechanistic understanding to simplify the model computation in ways that diverge from the original model internals. For example, in some subcubic proofs (Section 4.3), we approximate EQKE with a rank-1 approximation corresponding to the \u201csize direction\u201d. However, while other components are small, they\u2019re nonzero; this approximation harms model internals. ", "page_idx": 7}, {"type": "text", "text": "Less faithful interpretations lead to worse bounds on performance To confirm that faithfulness of understanding affects the tightness of bound independent of proof length, we plot the normalized accuracy bound of subcubic proofs that perform a rank-1 approximation to EQKE, versus the ratio of the first two singular components. A larger ratio between the components implies that the rank-1 approximation is more faithful. In Figure 4, we see a positive correlation between the two axes: when the interpretation is more faithful, the bounds are tighter, even at a fixed proof length. ", "page_idx": 7}, {"type": "image", "img_path": "2zWbzx50mH/tmp/d8d35e49ff39020240dbba817ca97b37b167c036570d472c46b4443e75a6bcf9.jpg", "img_caption": ["Figure 4: We plot the normalized accuracy bound versus the ratio of first and second singular values of EQKE, for various types of subcubic proofs that depend on a rank-1 approximation EQKE. For each class of proof, the closer EQKE is to rank-1, the tighter the accuracy bound. This suggests that more faithful interpretations lead to tighter bounds even holding proof length fixed. See Appendix H.2.2 for a detailed description of proof strategies. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "2zWbzx50mH/tmp/99ee55d2eba29d78d3f8d729fbb89464085f4a2606ca896f3e7dde1e0dd8027a.jpg", "img_caption": ["Figure 5: We plot, for each proof, the approximate number of flops required to evaluate the proof, versus the unexplained dimensionality (Section 5.1). More mechanistic understanding leaves fewer dimensions unexplained. We observe that more compact proofs seem to leave fewer unexplained dimensions, which is indicative of the relationship of mechanistic understanding and compact proofs. See Appendix H.2.1 for more detail. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Compounding structureless noise is a big challenge for compacting global-behavior proofs ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Pessimal error terms compound in the absence of known structure The rank-1 approximation of EQKE has small error. However, when making rank-1 approximations of each of the constituent matrices $E,Q,K$ , pessimizing over the worst way to composing the individual small error terms leads to a bound on the error term of EQKE that is orders of magnitude larger than the actual error term. Because we don\u2019t understand how the matrices compose in a way that doesn\u2019t cause errors to compound (without just multiplying out the matrices), this approximation leads to a trivial bound on performance (Appendix G.2.5). We speculate that in many cases, there is no short human-interpretable description for why random noise or approximation errors do not compound across layers of neural networks (e.g., see the error correction results on randomly initialized neural networks from H\u00e4nni et al. [21]), and thus that compounding structureless errors may be an issue in practice. ", "page_idx": 8}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Generalization Bounds Prior work in the PAC-Bayes framework [58, 36, 12] proves generalization bounds over learning procedures, which are similar to the global performance bounds we consider in this work. These proofs tend to provide statistical guarantees [25, 26] about the outputs of a known stochastic training procedure, while we seek to bound the performance of particular trained models. ", "page_idx": 8}, {"type": "text", "text": "Formally verifying neural networks Most prior work formally verifies neural networks either via model checking [28, 7] or by relaxing the problem setting and taking an automated theorem proving approach [17, 50, 18, 35, 43] to verify local robustness properties. These proof strategies tend to be derived by examining only the network architecture. We take an approach more akin to interactive theorem proving [22] and verify global performance properties by reverse-engineering the weights. ", "page_idx": 8}, {"type": "text", "text": "Mechanistic Interpretability Finally, mechanistic interpretability is the subfield of the broader field of understanding model internals [45], which is too large to faithfully summarize. Our work takes most direct inspiration from efforts to deeply understand how either toy models [38, 9, 53, 2] or small pretrained text transformers [54, 20] implement algorithmic tasks, generally by performing ablations and SVD. In contrast, we formally prove that a transformer implements an algorithm. ", "page_idx": 8}, {"type": "text", "text": "Nichani et al. [39] proves that, in a significantly simplified 2-layer, 1-head attention-only transformer model and for the task of in-context bigram statistics, gradient descent will create induction heads [40]. Our results concern transformers with fixed weights. In concurrent work, Michaud et al. [34] use techniques inspired by mechanistic interpretability to perform automated program synthesis on 2-dimensional RNNs, while our work works with significantly larger transformer models. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Summary In this work, we used a Max-of- $K$ setting to prototype the use of mechanistic interpretability to derive compact proofs of model behavior. Using varying amounts of understanding, we derived more efficient proof computations lower bounding model accuracy. We found preliminary evidence that mechanistic understanding can compactify proofs. Moreover, we observed that the tightness of the lower bound offered by various proof strategies can be used to grade the faithfulness our mechanistic interpretation. Finally, we identified compounding structureless errors as a key obstacle to deriving compact proofs of model behavior. ", "page_idx": 9}, {"type": "text", "text": "Limitations and future work We study one-layer attention-only transformers on a toy algorithmic task. Future work should explore the viability of deriving proofs via interpretability using larger models featuring MLPs or layernorm on more complex domains. In addition, we were unable to significantly compact the part of the proof involving the OV circuit, which future work can explore. The proofs we explored in this work also did not lead to qualitatively novel insights; future work may be able to derive such insights with improved techniques. Finally, future work can address the problem of compounding structureless errors, perhaps by relaxing from worst-case pessimal ablations to typical-case heuristic guarantees [8]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We are immensely grateful to Paul Christiano for providing the initial support for this project and for his invaluable research advice, encouragement, and feedback throughout its duration. ", "page_idx": 9}, {"type": "text", "text": "Additionally, we are thankful for clarifying discussions and feedback from Jacob Hilton, Matthew Coudron, Adri\u00e0 Garriga-Alonso, Aryan Bhatt, Leo Gao, Jenny Nitishinskaya, Somsubhro Bagchi, Gabriel Wu, Erik Jenner, Ryan Greenblatt, Ronak Mehta, Louis Jaburi, and many others. Louis Jaburi in particular contributed the text of the final proof of Theorem 11 in Appendix E. ", "page_idx": 9}, {"type": "text", "text": "We are indebted to various organizations for their support: Alignment Research Center for funding this project and making it possible at all; Mentorship for Alignment Research Students (MARS) program of the Cambridge AI Safety Hub (CAISH) for setting up the collaboration between a subset of authors, and providing funding for compute and in-person research sprints; Constellation and FAR Labs for hosting a subset of the authors and providing an excellent research environment, including as part of the Visiting Fellows Program and Astra Fellowship. ", "page_idx": 9}, {"type": "text", "text": "Author Contributions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Jason Gross led the project, including managing the team and conceptualizing the proofs approach. He ran the Max-of-4 experiments, devised the proof strategies, and wrote up the formal proofs. He worked on various case studies and developed general methodology for computing complexity and length bounds for proofs. He also developed the particular convex relaxations presented in the paper. ", "page_idx": 9}, {"type": "text", "text": "Rajashree Agrawal was invaluable in steering the direction of the project, including contributing to the preliminary experiment on Max-of-2 and developing the pessimal ablation approach. She worked on framing the results, and contributed text to the paper. ", "page_idx": 9}, {"type": "text", "text": "Thomas Kwa and Euan Ong extended the preliminary experiments to larger values of $k$ and contributed substantially to the cubic proof. Chun Hei Yip, Alex Gibson, and Soufiane Noubir worked on case studies other than the Max-of- $K$ task and informed discussion on proof complexity. ", "page_idx": 9}, {"type": "text", "text": "Lawrence Chan spearheaded the writing of the paper, including turning informal claims into formal theorem statements, creating figures, and writing the core text. He also developed the unexplained dimensionality metric for clarifying the takeaway of the paper. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Behzad Akbarpour, Amr Abdel-Hamid, Sofi\u00e8ne Tahar, and John Harrison. Verifying a synthesized implementation of IEEE-754 floating-point exponential function using HOL. The Computer Journal, 53:465\u2013488, May 2010. doi: 10.1093/comjnl/bxp023. [2] Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models, 2022.   \n[3] Andrew Appel and Ariel Kellison. VCFloat2: Floating-point error analysis in Coq. In Proceedings of the 13th ACM SIGPLAN International Conference on Certified Programs and Proofs, CPP 2024, pages 14\u201329, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400704888. doi: 10.1145/3636501.3636953.   \n[4] Sylvie Boldo and Guillaume Melquiond. Flocq: A unified library for proving floating-point algorithms in Coq. In 2011 IEEE 20th Symposium on Computer Arithmetic, pages 243\u2013252, July 2011. doi: 10.1109/ARITH.2011.40.   \n[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165.   \n[6] Lawrence Chan, Adri\u00e0 Garriga-Alonso, Nicholas Goldwosky-Dill, Ryan Greenblatt, Jenny Nitishinskaya, Ansh Radhakrishnan, Buck Shlegeris, and Nate Thomas. Causal scrubbing, a method for rigorously testing interpretability hypotheses. AI Alignment Forum, 2022. URL https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causalscrubbing-a-method-for-rigorously-testing.   \n[7] Chih-Hong Cheng, Georg N\u00fchrenberg, and Harald Ruess. Maximum resilience of artificial neural networks. In Automated Technology for Verification and Analysis: 15th International Symposium, ATVA 2017, Pune, India, October 3\u20136, 2017, Proceedings 15, pages 251\u2013268. Springer, 2017.   \n[8] Paul Christiano, Eric Neyman, and Mark Xu. Formalizing the presumption of independence. arXiv preprint arXiv:2211.06738, 2022. doi: 10.48550/arxiv.2211.06738.   \n[9] Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse engineering how networks learn group operations, 2023.   \n[10] Edmund M. Clarke, William Klieber, Milo\u0161 Nov\u00e1c\u02c7ek, and Paolo Zuliani. Model Checking and the State Explosion Problem, pages 1\u201330. Springer Berlin Heidelberg, Berlin, Heidelberg, 2012. ISBN 978-3-642-35746-6. doi: 10.1007/978-3-642-35746-6_1. URL https://doi.org/10. 1007/978-3-642-35746-6_1.   \n[11] David Dalrymple, Joar Skalse, Yoshua Bengio, Stuart Russell, Max Tegmark, Sanjit Seshia, Steve Omohundro, Christian Szegedy, Ben Goldhaber, Nora Ammann, et al. Towards guaranteed safe AI: A framework for ensuring robust and reliable AI systems. arXiv preprint arXiv:2405.06624, 2024.   \n[12] Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2016, August 11\u201315, 2017, Sydney, NSW, Australia, 2017.   \n[13] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. URL https://transformer-circuits.pub/2021/framework/index.html.   \n[14] Mart\u00edn H. Escard\u00f3. Synthetic topology of data types and classical spaces. Electronic Notes in Theoretical Computer Science, 87:21\u2013156, November 2004.   \n[15] Mart\u00edn H. Escard\u00f3. Infinite sets that admit fast exhaustive search. In Proceedings of the 22nd Annual IEEE Symposium on Logic in Computer Science (LICS 2007), Wroc\u0142aw, Poland, July 2007.   \n[16] Mart\u00edn H. Escard\u00f3. Seemingly impossible functional programs, 2007. URL https://math. andrej.com/2007/09/28/seemingly-impossible-functional-programs/. Accessed: 2024-05-15.   \n[17] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev. AI2: Safety and robustness certification of neural networks with abstract interpretation. In 2018 IEEE Symposium on Security and Privacy $(S P)$ , pages 3\u201318, Los Alamitos, CA, USA, May 2018. IEEE Computer Society. doi: 10.1109/SP.2018.00058.   \n[18] Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.   \n[19] Jason S. Gross. Performance Engineering of Proof-Based Software Systems at Scale. PhD thesis, Massachusetts Institute of Technology, February 2021. URL https://dspace.mit. edu/handle/1721.1/130763.   \n[20] Michael Hanna, Ollie Liu, and Alexandre Variengien. How does GPT-2 compute greater-than. Interpreting mathematical abilities in a pre-trained language model, 2:11, 2023.   \n[21] Kaarel H\u00e4nni, Jake Mendel, Dmitry Vaintrob, and Lawrence Chan. Mathematical models of computation in superposition. In ICML 2024 Workshop on Mechanistic Interpretability, 2024. URL https://openreview.net/forum?id $\\cdot=$ OcVJP8kClR.   \n[22] John Harrison, Josef Urban, and Freek Wiedijk. History of interactive theorem proving. In Handbook of the History of Logic, volume 9, pages 135\u2013214. Elsevier, 2014.   \n[23] Stefan Heimersheim and Neel Nanda. How to use and interpret activation patching. arXiv preprint arXiv:2404.15255, 2024.   \n[24] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.   \n[25] Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese Thamo, Min Wu, and Xinping Yi. A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability. arXiv preprint arXiv:1812.08342, 2018.   \n[26] Xiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie Jin, Yi Dong, Changshun Wu, Saddek Bensalem, Ronghui Mu, Yi Qi, Xingyu Zhao, Kaiwen Cai, Yanghao Zhang, Sihao Wu, Peipei Xu, Dengyu Wu, Andre Freitas, and Mustafa A. Mustafa. A survey of safety and trustworthiness of large language models through the lens of verification and validation, 2023.   \n[27] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.   \n[28] Guy Katz, Clark Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. Reluplex: An efficient SMT solver for verifying deep neural networks. In Computer Aided Verification: 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30, pages 97\u2013117. Springer, 2017.   \n[29] A. E. Kellison, A. W. Appel, M. Tekriwal, and D. Bindel. LAProof: A library of formal proofs of accuracy and correctness for linear algebra programs. In 2023 IEEE 30th Symposium on Computer Arithmetic (ARITH), pages 36\u201343, Los Alamitos, CA, USA, September 2023. IEEE Computer Society. doi: 10.1109/ARITH58626.2023.00021.   \n[30] Gerwin Klein, Kevin Elphinstone, Gernot Heiser, June Andronick, David Cock, Philip Derrin, Dhammika Elkaduwe, Kai Engelhardt, Rafal Kolanski, Michael Norrish, Thomas Sewell, Harvey Tuch, and Simon Winwood. seL4: Formal verification of an OS kernel. In Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles, SOSP \u201909, pages 207\u2013 220, New York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605587523. doi: 10.1145/1629575.1629596. URL https://doi.org/10.1145/1629575.1629596.   \n[31] Xavier Leroy. A formally verified compiler back-end. Journal of Automated Reasoning, 43: 363\u2013446, 2009.   \n[32] Chuan Li. OpenAI\u2019s GPT-3 language model: A technical overview, June 2020. URL https: //lambdalabs.com/blog/demystifying-gpt-3. Lambda Labs Blog, accessed October 30, 2024.   \n[33] Wes McKinney. Data Structures for Statistical Computing in Python. In St\u00e9fan van der Walt and Jarrod Millman, editors, Proceedings of the 9th Python in Science Conference, pages 56\u201361, 2010. doi: 10.25080/Majora-92bf1922-00a.   \n[34] Eric J. Michaud, Isaac Liao, Vedang Lad, Ziming Liu, Anish Mudide, Chloe Loughridge, Zifan Carl Guo, Tara Rezaei Kheirkhah, Mateja Vukeli\u00b4c, and Max Tegmark. Opening the AI black box: program synthesis via mechanistic interpretability, 2024.   \n[35] Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for provably robust neural networks. In International Conference on Machine Learning, pages 3578\u20133586. PMLR, 2018.   \n[36] Vaishnavh Nagarajan and J. Zico Kolter. Uniform convergence may be unable to explain generalization in deep learning. Advances in Neural Information Processing Systems, 32, 2019.   \n[37] Neel Nanda and Joseph Bloom. TransformerLens. https://github.com/ TransformerLensOrg/TransformerLens, 2022.   \n[38] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. arXiv preprint, 2023. doi: 10.48550/ arXiv.2301.05217.   \n[39] Eshaan Nichani, Alex Damian, and Jason D. Lee. How transformers learn causal structure with gradient descent. arXiv preprint, 2024. doi: 10.48550/arXiv.2402.14735.   \n[40] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. URL https://transformer-circuits.pub/2022/in-contextlearning-and-induction-heads/index.html.   \n[41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performancedeep learning library. Advances in neural information processing systems, 32, 2019.   \n[42] Plotly Technologies Inc. Collaborative data science, 2015. URL https://plot.ly.   \n[43] Aditi Raghunathan, Jacob Steinhardt, and Percy S. Liang. Semidefinite relaxations for certifying robustness to adversarial examples. Advances in neural information processing systems, 31, 2018.   \n[44] Tahina Ramananandro, Paul Mountcastle, Beno\u0131\u02c6t Meister, and Richard Lethin. A unified Coq framework for verifying C programs with floating-point computations. In Proceedings of the 5th ACM SIGPLAN Conference on Certified Programs and Proofs, CPP 2016, pages 15\u201326, New York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450341271. doi: 10.1145/2854065.2854066.   \n[45] Tilman R\u00e4uker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward transparent AI: A survey on interpreting the inner structures of deep neural networks. In First IEEE Conference on Secure and Trustworthy Machine Learning, 2022. doi: 10.48550/arxiv.2207.13243.   \n[46] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL https://arxiv.org/abs/2205.06175.   \n[47] Alex Rogozhnikov. Einops: Clear and reliable tensor manipulations with Einstein-like notation. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id $\\cdot=$ oapKSVM2bcj.   \n[48] Sanjit A. Seshia, Dorsa Sadigh, and S. Shankar Sastry. Toward verified artificial intelligence making AI more trustworthy with a formal methods-based approach to AI system verification and validation.   \n[49] Alex K. Simpson. Lazy functional algorithms for exact real functionals. In Lubo\u0161 Brim, Jozef Gruska, and Ji\u02c7r\u00ed Zlatu\u0161ka, editors, Mathematical Foundations of Computer Science 1998, pages 456\u2013464, Berlin, Heidelberg, 1998. Springer Berlin Heidelberg. ISBN 978-3-540-68532-6.   \n[50] Gagandeep Singh, Timon Gehr, Markus P\u00fcschel, and Martin Vechev. An abstract domain for certifying neural networks. Proc. ACM Program. Lang., 3(POPL), January 2019. doi: 10.1145/3290354.   \n[51] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.   \n[52] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St\u00e9fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C. J. Carey, \u02d9Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Ant\u00f4nio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental algorithms for scientific computing in Python. Nature Methods, 17:261\u2013272, 2020. doi: 10.1038/s41592-019-0686-2.   \n[53] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151\u201335174. PMLR, 2023.   \n[54] Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. arXiv preprint, 2022. doi: 10.48550/arXiv.2211.00593.   \n[55] Michael L. Waskom. seaborn: statistical data visualization. Journal of Open Source Software, 6 (60):3021, 2021. doi: 10.21105/joss.03021.   \n[56] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.   \n[57] Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International conference on machine learning, pages 5286\u20135295. PMLR, 2018.   \n[58] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3): 107\u2013115, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Subtleties of our approach ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we address some subtleties and frequently asked questions about our approach. ", "page_idx": 14}, {"type": "text", "text": "A.1 Why study this simple task? ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Formal reasoning is computationally expensive; very few large software projects have ever been verified [31, 30], none of them comparable to large transformer models [10, 19]. Separately, there is a high fixed cost to taking on any verification project, regardless of computational efficiency of the verification itself. Thus, we picked the simplest setting to study the question of interest: Is it even possible to formally reason more efficiently than by brute force about model behavior? ", "page_idx": 14}, {"type": "text", "text": "A.2 Scalability ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we address concerns about the scalability of our approach. ", "page_idx": 14}, {"type": "text", "text": "A.2.1 Larger input spaces ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We demonstrate that our proof strategies can be reused on larger input spaces while scaling better than the brute force approach does. ", "page_idx": 14}, {"type": "text", "text": "We applied our proof strategies to models trained for Max-of-5, Max-of-10, and Max-of-20. While running the brute force proof on Max-of-20 would require approximately $2^{148}$ FLOPs, which is about $2^{70}\\times$ the cost of training GPT-3 [32], our cubic proof achieves bounds of $(94.1\\pm1.1)\\,\\%$ (Max-of-5), $(91.4\\pm2.1)\\,\\%$ (Max-of-10), and $(88.4\\pm4.0)\\,\\%$ (Max-of-20) in under two minutes. See Tables 2, 3, 4, and 5 for more detailed numbers, and Figures 6, and 7 for visualizations. These results demonstrate that proof strategies can be reused on larger input spaces while scaling better than the brute force approach does. ", "page_idx": 14}, {"type": "text", "text": "A.2.2 Different tasks ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this paper, we worked on highly optimizing our relaxation to make our bounds as tight as possible when incorporating as little understanding as possible. This is not necessary for deriving proofs. Our general formalization of mechanistic interpretability is replicable: (1) theorem statements are exact expressions for the difference between the actual behavior of the model and the purported behavior, and (2) proofs are computations that bound the expression. Furthermore, our convexity theorems and proofs are applicable much more generally generally to element retrieval tasks. ", "page_idx": 14}, {"type": "text", "text": "A.2.3 More complicated architectures ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We worked on a simple model studied in A Mathematical Framework for Transformer Circuits [13]. In follow-up work, we will extend this approach to proving bounds on 1L transformers with ReLU MLP trained on modular addition. ", "page_idx": 14}, {"type": "text", "text": "A.2.4 Larger models ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "It is an open question whether or not the mechanistic interpretability approach to proofs can scale to larger models. However, a large part of this question lies in the feasibility of deriving a high degree of faithful mechanistic understanding from large models \u2014 that is, whether mechanistic interpretability itself will scale. This is widely recognized in the field, and scaling interpretability approaches while getting both a high degree of mechanistic understanding and assurances that said understanding is faithful to the model is an active area of research. Broadly, we see the compact proofs approach as a metric on the quality of mechanistic understanding \u2014 we are not purporting to have a general solution to the problem of scaling interpretability, but instead claim that the challenges in proofs are in fact challenges in understanding networks. ", "page_idx": 14}, {"type": "table", "img_path": "2zWbzx50mH/tmp/d3ab41d218833c22e93f7bb85c7e8ad9654ecf30c1a77e2714057c97f0de79a6.jpg", "table_caption": ["Table 2: Version of Table 1 from Section 4.1 with $n_{\\mathrm{ctx}}=5$ , $d_{\\mathrm{vocab}}=64$ . We report the proof complexity, accuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5). Unlike Table 1, which computes the brute force bound exactly, we instead use importance sampling to estimate the bound; estimated FLOPs are reported for what the full brute force proof would take. We round the FLOP and unexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound averaged across all 151 models. For space reasons, we use $k:=n_{\\mathrm{ctx}}$ , $d:=d_{\\mathrm{model}}$ , and $v:=d_{\\mathrm{vocab}}$ . "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "2zWbzx50mH/tmp/fd7dff007e2a46ef69583c42ec50356c03d3a9968c14c9bfc70dd84853b14476.jpg", "table_caption": ["Table 3: Version of Table 1 from Section 4.1 with $n_{\\mathrm{ctx}}=10$ , $d_{\\mathrm{vocab}}=64$ . We report the proof complexity, accuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5). Unlike Table 1, which computes the brute force bound exactly, we instead use importance sampling to estimate the bound; estimated FLOPs are reported for what the full brute force proof would take. We round the FLOP and unexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound averaged across all 151 models. For space reasons, we use $k:=n_{\\mathrm{ctx}}$ , $d:=d_{\\mathrm{model}}$ , and $v:=d_{\\mathrm{vocab}}$ . "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "2zWbzx50mH/tmp/f52519865380a9100d240748a0b0da2ae726fabe83268d6831d8319ed8fda202.jpg", "table_caption": ["Table 4: Version of Table 1 from Section 4.1 with $n_{\\mathrm{ctx}}=10$ and $d_{\\mathrm{vocab}}=128$ . We report the proof complexity, accuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5). Unlike Table 1, which computes the brute force bound exactly, we instead use importance sampling to estimate the bound; estimated FLOPs are reported for what the full brute force proof would take. We round the FLOP and unexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound averaged across all 151 models. For space reasons, we use $k:=n_{\\mathrm{ctx}}$ , $d:=d_{\\mathrm{model}}$ , and $v:=d_{\\mathrm{vocab}}$ . "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "2zWbzx50mH/tmp/9a0b1fdf166350dce4748b568f92c916dfd9ac17399ea969ae030769777e21e5.jpg", "table_caption": ["Table 5: Version of Table 1 from Section 4.1 with $n_{\\mathrm{ctx}}=20$ , $d_{\\mathrm{vocab}}=64$ . We report the proof complexity, accuracy bound, and estimated flops required (Equation 2), as well as unexplained dimensionality (Section 5). Unlike Table 1, which computes the brute force bound exactly, we instead use importance sampling to estimate the bound; estimated FLOPs are reported for what the full brute force proof would take. We round the FLOP and unexplained dimension counts to the closest power of 2, and report the mean/standard deviation of the bound averaged across all 151 models. For space reasons, we use $k:=n_{\\mathrm{ctx}}$ , $d:=d_{\\mathrm{model}}$ , and $v:=d_{\\mathrm{vocab}}$ . "], "table_footnote": [], "page_idx": 16}, {"type": "equation", "text": "$$\n(\\mathbf{d})\\,n_{\\mathrm{ctx}}=20,d_{\\mathrm{vocab}}=64\n$$", "text_format": "latex", "page_idx": 17}, {"type": "image", "img_path": "2zWbzx50mH/tmp/f9daf6fd3b281eb0dfac325f5847ce98da03e71f8cd79fedcef75d9412f7f2af.jpg", "img_caption": ["Figure 6: Version of Figure 3 from page 8 with varying $n_{\\mathrm{ctx}}$ and $d_{\\mathrm{vocab}}$ . The brute-force proof (Section 4.1) computes the exact performance uses orders of magnitude more compute than other approaches; unlike in Figure 3, here we use importance sampling to estimate the bound. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "2zWbzx50mH/tmp/f8d494bcfeb5d7ea818f2b82fcbbaf8c93c1c656abdeb97be0084b3e82e7ec69.jpg", "img_caption": ["(a) $n_{\\mathrm{ctx}}=5$ , $d_{\\mathrm{vocab}}=64$ . The \u201csvd\u201d proof strategy best-fti line has equation $b/\\bar{s}=0.000\\,17(\\sigma_{1}/\\sigma_{2})+0.56$ , $R^{2}=0.71$ . ", ""], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "2zWbzx50mH/tmp/056e29cd6807f6b5d591a9a75f4dc6ff4a6d505b8b50f43091053ef01bbd4679.jpg", "img_caption": ["(b) $n_{\\mathrm{ctx}}=10$ , $d_{\\mathrm{vocab}}=64$ . The \u201csvd\u201d proof strategy best-fti line has equation $b/\\bar{s}=0.000\\,083(\\sigma_{1}/\\sigma_{2})\\substack{+0.56}$ , $R^{2}=0.47$ . ", ""], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "2zWbzx50mH/tmp/925cf69d6f3be7c43ecbba9099d9422a4308784e497ca3921dd33cdd4f273e11.jpg", "img_caption": ["(c) $n_{\\mathrm{ctx}}=10$ and $d_{\\mathrm{vocab}}=128$ . The \u201csvd\u201d proof strategy best-fti line has equation $b/\\bar{s}=0.000\\,11(\\sigma_{1}/\\sigma_{2})\\,+$ 0.49, $R^{2}=0.75$ . ", ""], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "2zWbzx50mH/tmp/ab659a2b31a092e957ec977523ad84a73d60c6375ce2722422240e147692e30c.jpg", "img_caption": ["(d) $n_{\\mathrm{ctx}}=20$ , $d_{\\mathrm{vocab}}=64$ . The \u201csvd\u201d proof strategy best-fti line has equation $b/\\bar{s}=0.000\\,098({\\sigma}_{1}/{\\sigma}_{2}){+}0.44$ , $R^{2}=0.25$ . ", "EPQKE Singular Ratio: \u03c31/\u03c32 "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 7: Version of Figure 4 from page 9 with varying $n_{\\mathrm{ctx}}$ and $d_{\\mathrm{vocab}}$ . Note that the \u201csvd\u201d proof strategy has a clear upward trend, especially on early points. ", "page_idx": 18}, {"type": "text", "text": "A.3 Why is more mechanistic understanding correlated with worse bounds? ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Figure 3 exhibits Simpson\u2019s Paradox: although more faithful mechanistic understanding is correlated with better bounds within each class of proof (and moreover the most extensive mechanistic understanding results in the greatest improvement in bound tightness over baseline), when we aggregate across all proof strategies, we find that more mechanistic understanding is correlated with worse bounds. ", "page_idx": 19}, {"type": "text", "text": "This relationship is summarized in Figure 8. ", "page_idx": 19}, {"type": "text", "text": "From the compression perspective, more mechanistic understanding is about having more compression. Unless the model is losselessly compressible, we should expect that more compression will inherently be more lossy, no matter how good our compression scheme is. Correspondingly, using more understanding to get more compression will often result in a weaker bound, no matter how good our understanding is. ", "page_idx": 19}, {"type": "text", "text": "Conversely, we can think of the quality of proofs (the combination of tightness of bound, and length of proof) as a metric for how good our mechanistic understanding is. From this perspective, the fact that mechanistic-interpretability-derived bounds are bad suggests gaps in our mechanistic understanding. As the field matures and we develop tools that enable more faithful and complete understanding of model behavior, we expect that the quality of bounds we derive from mechanistic understanding will improve. ", "page_idx": 19}, {"type": "text", "text": "A.4 Convex relaxation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this work, we construct convex relaxations to perform the pessimal ablations for our proofs. ", "page_idx": 19}, {"type": "text", "text": "In what sense are we using \u201cconvexity\u201d? The intuition is that we are attempting to optimize a function $f$ over its domain $X$ by incrementally making local changes to the input sequence, such as replacing one token by another, or by changing the order of tokens. The reason that convex optimization problems are easy to solve is that all local extrema are global extrema. This is not the case for our optimization problem, so we find a relaxation of $f$ and its domain such that all local extrema are in fact global extrema. ", "page_idx": 19}, {"type": "text", "text": "Furthermore, most convex optimizers perform optimization at runtime by repeatedly stepping towards extrema. In this work, we \u201coptimize by hand\u201d, performing the optimization in the proof of our general theorems. The computation of the bound then only needs to instantiate the precomputed possible extrema with the actual values of the model\u2019s parameters to determine the the extrema actually are. ", "page_idx": 19}, {"type": "text", "text": "We now give a formal description of what we mean by \u201cconvex relaxation\u201d. ", "page_idx": 19}, {"type": "text", "text": "For a set of inputs $X_{i}$ , we define a set of \u201crelaxed inputs\u201d $X_{i}^{\\mathrm{relaxed}}$ with an injection $T_{i}:X_{i}\\hookrightarrow$ $\\mathcal{P}(X_{i}^{\\mathrm{relaxed}})$ mapping input to the model to the set of corresponding relaxed inputs. On the relaxed input, we define a function $h_{i}:X_{i}^{\\mathrm{relaxed}}\\rightarrow\\mathbb{R}$ such that for all $\\mathbf{t}\\in X_{i}$ and all labels $l$ for which $(l,\\mathbf{t})$ is supported by (has non-zero probability in) $\\mathcal{D}$ , we can find $\\mathbf{t}^{\\mathrm{relaxed}}\\in T_{i}(\\mathbf{t})$ with $f(l,\\mathcal{M}(\\mathbf{t}))\\geq$ $h_{i}(\\mathbf{t}^{\\mathrm{relaxed}})$ . We proceed by finding a small subset of \u201cboundary\u201d examples $B_{i}\\subset X_{i}^{\\mathrm{relaxed}}$ , proving that if $h_{i}(\\mathbf{t}^{\\mathrm{relaxed}})\\geq b_{i}$ for all $\\mathbf{t}^{\\mathrm{relaxed}}\\in B_{i}$ then $h_{i}(\\mathbf{t}^{\\mathrm{relaxed}})\\geq b_{i}$ for all $\\mathbf{t}^{\\mathrm{relaxed}}\\in X_{i}^{\\mathrm{relaxed}}$ . ", "page_idx": 19}, {"type": "text", "text": "Then, the computational component $C$ of the proof validates that that $h_{i}(\\mathbf{t}^{\\mathrm{relaxed}})\\geq b_{i}$ for some $b_{i}$ for all $\\mathbf{t}^{\\mathrm{relaxed}}\\in X_{i}^{\\mathrm{relaxed}}$ . This allows us to conclude that $f(l,\\mathcal{M}(\\mathbf{t}))\\geq b_{i}$ for all $\\mathbf{t}\\in X_{i}$ . ", "page_idx": 19}, {"type": "text", "text": "A.5 Computing unexplained dimensionality ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We claim in Figure 5 that we can use unexplained dimensionality as a metric for understanding. Here we describe how we compute the unexplained dimensionality of a proof strategy. ", "page_idx": 19}, {"type": "text", "text": "As in Figure 1, for any given proof, we can separate our treatment of transformer components into \u201cblack-box\u201d (e.g., matrix multiplication) and \u201cwhite-box\u201d components (e.g., specifying that the QK circuit is approximately rank one; pessimizing over non-max tokens). Considering the performance score as a large white-box component which may reference black-boxes internally, we define the unexplained dimensionality of a single black-box computation as the log-cardinality of it function space (so, e.g, $2\\cdot64$ for a function $64\\bar{\\to}\\,\\mathbb{R}^{2}$ , whose cardinality is $(\\mathbb{R}^{2})^{\\underline{{64}}}$ , where 64 denotes the finite set on 64 elements). The unexplained dimensionality of the entire proof is the sum of the unexplained dimensions of all black-box components. ", "page_idx": 19}, {"type": "image", "img_path": "2zWbzx50mH/tmp/28cf9ecded108297714a2b5480b3547fcabb50bf70dc01bcc454a9244c707094.jpg", "img_caption": ["(a) The baseline of using inference to generate proofs. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "2zWbzx50mH/tmp/cf6c6a97a3052a69fe9d1bd4330b8f7d2072b0fdef0fc29f5aaf542441ef40c1.jpg", "img_caption": ["(b) Shorter proofs by default have a worse performance bound $^b$ . Faithful understanding allows us to recover significant \u2014 but not complete \u2014 bound tightness with minimal proof-length overhead. ", "Figure 8: The theoretical relationship between proof length and bound tightness. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Intuitively speaking, unexplained dimensionality tries to capture the degrees of freedom that we have to check via brute enumeration over black-box computations. Proofs with less unexplained dimensionality contain more mechanistic understanding, and vice versa. ", "page_idx": 20}, {"type": "text", "text": "A.6 Computing approximate FLOPs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Figure 3 and Table 1 on page 6 and on page 8, we display approximate floating point operations. We instrument our code to execute on phantom tensors that track their shape and accumulate an approximate count of floating point operations. We compute matrix additions and multiplications in the obvious way. We take the instruction count of SVD to be the cost of verifying that the output of SVD is a valid decomposition: that we have a pair of orthonormal bases which when multiplied out give the original basis. ", "page_idx": 20}, {"type": "text", "text": "A.7 IEEE 754 vs. $\\mathbb{R}$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Section 2 we defined $C$ and $Q$ and glossed over whether we were reasoning over reals or floats.   \nHere we clarify this point that we\u2019ve so far been sweeping under the rug. ", "page_idx": 20}, {"type": "text", "text": "Let $\\mathbb{F}$ denote the set of the relevant flavor of IEEE 754 Floating Point numbers (generally 32-bit for our concrete models, but everything would hold just as well for 64-bit). Let $\\mathbb{F}^{*}$ denote $\\mathbb{F}$ restricted to finite numbers (that is, without NaNs and without $\\pm\\infty,$ ). ", "page_idx": 20}, {"type": "text", "text": "We parameterize $C,\\;M$ , and $\\mathcal{D}$ over the real field13 they operate on, so that, e.g., $C_{F}$ : model weights $\\rightarrow\\ \\ F$ . Then we have $Q$ establishing that for any model $\\mathcal{M}^{\\prime}$ , $C_{\\mathbb{R}}(\\mathcal{M}_{\\mathbb{R}}^{\\prime})~\\le$ $\\mathbb{E}_{(l,\\mathbf{t})\\sim\\mathcal{D}_{\\mathbb{R}}}f_{\\mathbb{R}}(l,\\mathcal{M}_{\\mathbb{R}}^{\\prime}(\\mathbf{t}))$ , and we have a trace demonstrating that $C_{\\mathbb{F}}(\\mathcal{M}_{\\mathbb{F}})=b$ . ", "page_idx": 20}, {"type": "text", "text": "Let $i:\\mathbb{F}^{*}\\to\\mathbb{R}$ be any injection that maps each floating point number to some real number that it is \u201cclosest to\u201d. Supposing that $b\\in\\mathbb{F}^{*}$ and thus $b\\in\\mathbb{R}$ , we need two additional components of the proof. We need to find $\\bar{\\varepsilon},\\varepsilon^{\\prime}\\in\\mathbb R^{+}$ prove that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{C_{\\mathbb{R}}(\\mathcal{M}_{\\mathbb{R}})-i\\left(C_{\\mathbb{F}}(\\mathcal{M}_{\\mathbb{F}})\\right)|<\\varepsilon\\qquad\\mathrm{and}\\qquad\\big|\\left(\\mathbb{E}_{(l,\\mathbf{t})\\sim\\mathcal{D}_{\\mathbb{F}}}f_{\\mathbb{R}}(l,\\mathcal{M}_{\\mathbb{R}}(\\mathbf{t}))\\right)-i\\left(\\mathbb{E}_{(l,\\mathbf{t})\\sim\\mathcal{D}_{\\mathbb{F}}}f_{\\mathbb{F}}(l,\\mathcal{M}_{\\mathbb{F}}(\\mathbf{t}))\\right)\\big|}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then we can chain these proofs to prove that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{i\\left(\\mathbb{E}_{(l,\\mathbf{t})\\sim\\mathcal{D}_{\\mathbb{F}}}f_{\\mathbb{F}}(l,\\mathcal{M}_{\\mathbb{F}}(\\mathbf{t}))\\right)\\geq b-\\varepsilon-\\varepsilon^{\\prime}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Such $\\varepsilon$ -ball robustness proofs should be well within the scope of existing approaches to formal methods on neural nets, see, e.g., [44, 3, 4, 29, 1, 57]. We leave actually dealing with the gap between floating point numbers and real numbers to future work. ", "page_idx": 21}, {"type": "text", "text": "A.8 Non-uniform distributions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In Equation 1 in Section 2 we defined the expected model performance as the expectation of the distribution $\\mathcal{D}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bar{s}:=\\mathbb{E}_{(l,\\mathbf{t})\\sim\\mathcal{D}}\\left[f(l,\\mathcal{M}(\\mathbf{t}))\\right]\\geq b.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We then immediately specialized to the case where the marginalization $\\mathcal{D}\\vert_{X}$ of $\\mathcal{D}$ over labels is uniform. As we\u2019ll see in Theorem 1 in Appendix $\\mathrm{D}$ and Algorithm 3 in Appendix E, the bound computation is modularized between a function that bounds the performance $f(l,\\mathcal{M}(\\mathbf{t}))$ over a restricted collection of inputs, and a much simpler function that combines the bounds on individual cases into a bound on the expectation over the entire distribution. The per-input bound computation is CORRECTNESS in Algorithm 1 and RELAXED-CORRECTNESS-PESSIMIZING-OVER-POSITION in Algorithm 3; the expectation computation is BRUTE-FORCE in Algorithm 1 and CUBIC in Algorithm 3. ", "page_idx": 21}, {"type": "text", "text": "Since the expectation computation is modularized, it is straightforward to extend our approach to non-uniform distributions simply by adjusting the weighting of each region of inputs. However, if the distribution is too far off from the uniform training distribution, the bound we get may not be very good, as we may not be allocating adequate computation to the high-probability regions of the input space. ", "page_idx": 21}, {"type": "text", "text": "A.9 Adversarial robustness via flexibility in $\\mathcal{D}$ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "There is flexibility inherent in Equation 1. Normally, by out-of-distribution (OOD) or adversarial inputs, we suppose that there is a distribution $\\mathcal{D}_{\\mathrm{in}}$ that\u2019s used for training and (in-distribution) validation, and another distribution $\\mathcal{D}^{\\prime}$ that is the deployment distribution or generated by an adversary. If we had knowledge of $\\mathcal{D}^{\\prime}$ , we could compute the expected performance from inputs sampled from $\\mathcal{D}^{\\prime}$ . Even if we don\u2019t have exact knowledge of $\\mathcal{D}^{\\prime}$ , we can still define a very broad distribution $\\mathcal{D}$ that covers possible $\\mathcal{D}^{\\prime}\\mathbf{s}$ . ", "page_idx": 21}, {"type": "text", "text": "In this work, $\\mathcal{D}$ is the distribution of all $64^{4}$ possible valid input sequences. In addition, as our proofs partition $\\mathcal{D}$ into subdistributions, and bound the performance on each subdistribution, we can bound the model\u2019s performance on any possible distribution over valid input sequences. ", "page_idx": 21}, {"type": "text", "text": "A.10 Infinite distributions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In the brute force proof in Section 4.1, we run the model on the entirety of $\\mathcal{D}$ . This operation is straightforward when $X$ is finite. Perhaps surprisingly, we can do this even if $X$ is infinite as long as the PDF $L\\times X\\to\\mathbb{R}$ of $\\mathcal{D}$ is computable and the natural computational topology of $X$ is compact [16, 15, 14], because integration of computable functions on computable reals is computable [49]. ", "page_idx": 21}, {"type": "text", "text": "A.11 Using alternate loss functions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Building on the point from Appendix A.8, it is also relatively straightforward to extend our approach from bounding expected accuracy to bounding log-loss. We will see in Figure 12 that the accuracy and log-loss share a subterm $\\Delta\\ell_{i}$ . Since we compute this subterm in all of our algorithms, we can easily extend our approach to log-loss by combining $\\Delta\\ell_{i}$ directly rather than merely checking that the value is negative as we currently do in RELAXED-CORRECTNESS-PESSIMIZING-OVER-POSITION in Algorithm 3. Although this is sufficient for the brute-force and cubic proofs, for the subcubic proof using Algorithm 6 in Appendix F, we would additionally have to compute a log-loss bound for the sequences where the largest non-max token is \u201ctoo close\u201d to the max token, which we currently neglect by considering the model to get them wrong in the worst case. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "A.12 Proving upper bounds ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this work, we focus on proving lower bounds on model performance. Most of our theorems, for example in Appendices E, and F, prove two-sided bounds. Most of the other theorems can be straightforwardly adapted to proving upper bounds by swapping uses of min and max. Therefore, we expect that proving upper bounds on model performance should be straightforward. ", "page_idx": 22}, {"type": "text", "text": "A.13 What proof system? ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Length of proof depends on what proof system we use. We permit any proof system where proofchecking time is linear in the length of the proof. This excludes dependently typed proof systems such as Martin-L\u00f6f type theory, but such proof systems can easily be accommodated by considering a proof-checking-trace rather than the proof object itself. Alternatively, a more conventional proof system like ZF, ZFC, or the proof system underlying Isabelle/HOL should suffice. ", "page_idx": 22}, {"type": "text", "text": "B Experimental details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "B.1 Training details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To train each model, we generate 384,000 random sequences of 4 integers picked uniformly at random, corresponding to less than $2.5\\%$ of the input distribution. We use AdamW with batch_size $=128$ , $\\mathrm{lr}=0.001$ , betas $=(0.9,0.999)$ , weight_decay left at the default 0.01. We train for 1 epoch (3000 steps). Over our 151 seeds, models trained with this procedure achieve $(99.92\\pm0.15)\\,\\%$ train accuracy and a loss of $(4\\pm8)\\times10^{-3}$ .14 When qualitatively examining a single model (for example in Section 3.1 or Appendix H.1), we use the model with config seed 123, model seed15 613947648. ", "page_idx": 22}, {"type": "text", "text": "As our models as sufficiently small, we did not have to use any GPUs to accelerate training our inference. Each training run takes less than a single CPU-hour to complete. In total, the experiments in this paper took less than 1000 CPU-hours. ", "page_idx": 22}, {"type": "text", "text": "We use the following software packages in our work: Paszke et al. [41], Plotly Technologies Inc. [42], Nanda and Bloom [37], Rogozhnikov [47], Virtanen et al. [52], McKinney [33], Waskom [55] ", "page_idx": 22}, {"type": "text", "text": "B.2 Additional details supporting our mechanistic interpretation of the model ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We provide heatmaps of the matrices corresponding to the five components described/defined in Section 3, for the mainline model. ", "page_idx": 22}, {"type": "image", "img_path": "2zWbzx50mH/tmp/478010c45e281e6fce9b7e3fd7bd6de4668eae21b6c0ce52b4e3553d6ea718d5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{\\Psi}(\\mathbf{a})\\operatorname{EQKE}=\\mathbf{E}_{q}Q K^{T}\\bar{\\mathbf{E}}^{T}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 23}, {"type": "image", "img_path": "2zWbzx50mH/tmp/fdc6e01eaaf16410162f902d6f25653d1619e963ba03025b04568d55a2a065c2.jpg", "img_caption": ["Figure 9: The QK circuit can be decomposed into the position-independent and position-dependent components EQKE and EQKP. It computes the pre-softmax attention score for the model. The positional contribution to the attention score, as shown in Figure (b), is minimal. In Figure (a), the gradient from left to right along the key axis indicates that the single attention head pays more attention to larger tokens. The uniformity along the query axis suggests that this behavior is largely independent of the query token. Further, the light and dark bands imply that some queries are better than others at focusing more on larger tokens. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "2zWbzx50mH/tmp/6f5b5f5bf708f3d96f5bcf7b1cbdc6b4639f397fbec0a0a60e8ce47d90b6d5c8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "2zWbzx50mH/tmp/5bd07b49eb92b99c3a7bfd498ebe63b3bfee822e918389cdc54286445287db02.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "2zWbzx50mH/tmp/aed4938e74ae6f462efe2c0e36d39d4d668c30291a96c73e82f6bfe49813cfab.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "equation", "text": "$$\n)\\operatorname{EVOU}={\\bar{\\mathbf{E}}}V O U\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{\\Delta}(\\mathbf{b})\\operatorname{PVOU}=\\hat{\\mathbf{P}}V O U\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n({\\bf c})\\,\\mathrm{Direct}\\,\\mathrm{Path}={\\bf E}_{q}U\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Figure 10: The OV circuit is a sum of EVOU and PVOU. In Figure (a) we see that EVOU \u201ccopies\u201d \u2014 with the exception of input tokens $\\leq5$ ( $6.6\\pm1.2$ across all models) \u2014 by virtue of the fact that above 5, the diagonal is larger than all the other elements in the same row. We see that the range on Figure (b) is much smaller than Figure (a), indicating that positional contribution to the copying is minimal. In Figure (c) we see that direct path values matter a bit more than PVOU, being only $\\approx20\\times$ smaller than the typical EVOU difference. They don\u2019t matter that much, though, being so small. Additionally, the vertical banding indicates that the primary effect of this is a largely-query-independent bias towards larger numbers, reflecting the fact that the input distribution is biased towards larger numbers being the maximum. The weak diagonal pattern indicates a slight bias towards upweighting the query token itself as a (possible) maximum token. ", "page_idx": 23}, {"type": "text", "text": "B.3 Distribution of model mechanisms ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We provide some analysis of the distribution of the mechanisms of the models trained on the same configuration. At a glance, there is not that much variation across models. ", "page_idx": 23}, {"type": "text", "text": "The statistics of interest are: (1) $\\sigma_{1}/\\sigma_{2}$ , the ratio of the first two singular values of EQKE, a measure of the extent to which the attention score computation is low-rank; (2) $\\bar{s}$ , the average score (accuracy) of the model across the entire input distribution; (3) $b_{\\mathrm{cubic}}/\\bar{s}$ , the percent-score-recovered accuracy bound achieved by the cubic proof from Section 4.2; (4) $b_{\\mathrm{{subcubic}}}/\\bar{s}$ , the percent-score-recovered accuracy bound achieved by the (per-model best)16 subcubic proof from Section 4.3. ", "page_idx": 23}, {"type": "text", "text": "For each statistic of interest, Table 6 presents an eleven-number summary of the statistic. Plots, seeds, and statistic values are shown for models whose values are closest to each of the corresponding summary statistics.17 Additionally, each group contains a boxplot of the summary: ", "page_idx": 23}, {"type": "text", "text": "\u2022 the minimum, maximum; the first and third quartiles; the median and mean; percentiles $2.15\\,\\%$ , $97.85\\,\\%$ , $8.87\\,\\%$ , and $91.13\\,\\%$ ; these are displayed as: \u2022 top and bottom of the vertical whisker lines; top and bottom of the box; horizontal line inside the box, and the square; horizontal whisker lines and whisker crosshatches. ", "page_idx": 23}, {"type": "text", "text": "Table 6: Plots of various models. The statistics of interest are: (1) $\\sigma_{1}/\\sigma_{2}$ , the ratio of the first two singular values of EQKE, a measure of the extent to which the attention score computation is low-rank; (2) $\\bar{s}$ , the average score (accuracy) of the model across the entire input distribution; (3) $b_{\\mathrm{cubic}}/\\bar{s}$ , the percent-score-recovered accuracy bound achieved by the cubic proof from Section 4.2; and (4) $b_{\\mathrm{{subcubic}}}/\\bar{s}$ , the percent-score-recovered accuracy bound achieved by the (per-model best) subcubic proof from Section 4.3. The $y$ axes are: for EQKE and EQKP, the query token; for EVOU, the input token; for ${\\bf E}_{q}U$ , the input query token; for PVOU, the input position. The $_x$ axes are: for EQKE, the key token; for EQKP, the key position; for EVOU, PVOU, and $\\mathbf{\\bar{E}}_{q}U$ , the output logit token. All token axes range from 0 at the top (or left) to $d_{\\mathrm{vocab}}-1=63$ at the bottom (or right). All position axes range from 0 at the top (or left) to $n_{\\mathrm{ctx}}-1=3$ at the bottom (or right). ", "page_idx": 24}, {"type": "image", "img_path": "2zWbzx50mH/tmp/9a2d299bbe20379439f62a563e314712591e5ac2c115012c76b81fbf77326f2a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "C Mathematical definitions ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We provide a detailed breakdown of the mathematical notation used in the appendix. ", "page_idx": 25}, {"type": "text", "text": "Let   \n$\\underline{n}$ be the finite set on $n$ elements; we write   \nN<n $:=\\{0,1,\\ldots,n-1\\}$ when we care about the elements of $\\underline{n}$   \n$\\sigma(\\mathbf{v})$ be the softmax function $e^{\\mathbf{v}}/\\sum_{i}e^{v_{i}}$   \n$\\boldsymbol{\\sigma}^{\\ast}(\\mathbf{v})$ be the casually-masked softmax function, $\\sigma^{*}(\\mathbf{v})_{i}:=e^{v_{i}}/\\left\\dot{\\sum}_{j\\leq i}\\,e^{v_{j}}$   \ndvocab be the size of the vocabulary   \n$d$ be the dimension of the attention head, in our case, equal to the hidden dimension of the model $d_{\\mathrm{model}}$ (assumption: $d<d_{\\mathrm{vocab}})$ )   \nnctx be the context length, the number of tokens in the input sequence, equal to $K$ in Max-of- $\\mathcal{K}$   \n$P$ be the $n_{\\mathrm{ctx}}\\times d_{\\mathrm{model}}$ positional embedding   \n$E$ be the $d_{\\mathrm{vocab}}\\times d_{\\mathrm{model}}$ token embed   \n$Q,K$ , be the $d_{\\mathrm{model}}\\times d_{\\mathrm{model}}$ query, key, value, and output   \n$V,O$ matrices of the attention head   \n$U$ be the $d_{\\mathrm{model}}\\times d_{\\mathrm{vocab}}$ unembed matrix   \n$\\mathbf{t}$ be the input token sequence $[t_{0},t_{1},\\ldots,t_{n_{\\mathrm{ctx}}-1}]$   \n$\\mathbf{x}$ be the $n_{\\mathrm{ctx}}\\times d_{\\mathrm{vocab}}$ one-hot-encoded input token sequence $[x_{0},x_{1},\\ldots,x_{n_{\\mathrm{ctx}}-1}]$   \nxquery $:=x_{-1}:=x_{n_{\\mathrm{ctx}}-1}$ be the query token   \n$t_{\\mathrm{query}}$ : $:=t_{-1}:=t_{n_{\\mathrm{ctx}}-1}$ be the one-hot encoded query token   \ntmax be the true maximum token in the input sequence, $\\operatorname*{max}_{i}t_{i}$ ", "page_idx": 25}, {"type": "text", "text": "of type $X\\rightarrow Y$ be the model18; sometimes we write for the logits of the model $\\mathcal{M}(\\mathbf{t})$ be a probability distribution over input-label pairs $(\\bar{\\mathbf t^{,l}})\\in X\\times L$ be $\\mathcal{D}$ marginalized to a distribution over $\\mathbf{t}\\in X$ of type $L\\,\\times\\,Y\\,\\rightarrow\\,\\mathbb{R}$ be a scoring function for evaluating the performance of the model be the average position embeds across positions (of size $d_{\\mathrm{model}})$ ), $\\textstyle{\\frac{1}{n_{\\mathrm{ctx}}}}\\sum_{i}P_{i}$ be either $\\mathbf{1}_{n_{\\mathrm{ctx}}}\\otimes P_{\\mathrm{avg}}$ or $\\mathbf{1}_{d_{\\mathrm{vocab}}}\\otimes P_{\\mathrm{avg}}$ depending on context \u2013 that is the result of broadcasting $P_{\\mathrm{avg}}$ back into the shape of $P$ or $E$ (that is, $n_{\\mathrm{ctx}}^{\\mathrm{~\\,~-~}}\\times d_{\\mathrm{model}}$ or $d_{\\mathrm{vocab}}\\times d_{\\mathrm{model}})$ be $1_{d_{\\mathrm{vocab}}}\\otimes P_{\\mathrm{query}}$ , the broadcasting of $\\ensuremath{P_{\\mathrm{{query}}}}$ $\\hat{\\mathbf{P}},\\bar{\\mathbf{E}},\\mathbf{E}_{q}$ be $P-\\bar{\\mathbf{P}},E+\\bar{\\mathbf{P}}$ , and $E+\\mathbf{P}_{q}$ respectively ", "page_idx": 25}, {"type": "text", "text": "For any vector-valued function $\\mathbf{v}$ of length $d_{\\mathrm{vocab}}$ , parameterized over the input sequence $\\mathbf{t}$ , let   \n$\\Delta v_{i}:=v_{i}\\!-\\!v_{t_{\\operatorname*{max}}}$ be the difference between the $i^{\\mathrm{th}}$ element of $\\mathbf{v}$ and the element of $\\mathbf{v}$ corresponding to the true maximum of the input sequence tmax.   \nFor our particular model, we will have $\\begin{array}{r l}&{d_{\\mathrm{vocab}}:=64\\qquad d=d_{\\mathrm{model}}:=32\\qquad n_{\\mathrm{ctx}}:=4}\\\\ &{X:=(\\mathbb{N}_{<d_{\\mathrm{vocab}}})^{n_{\\mathrm{ctx}}}\\cong\\underline{{\\Theta}}4^{4}\\qquad L:=\\mathbb{N}_{<d_{\\mathrm{vocab}}}\\cong\\underline{{\\Theta}}4}\\end{array}$   \n$\\mathcal{D}|_{X}:=U(0,1,\\ldots,d_{\\mathrm{vocab}}-1)^{n_{\\mathrm{ctx}}}$ , the uniform distribution ", "page_idx": 25}, {"type": "text", "text": "Figure 11: Preliminary model definitions ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\underbrace{\\mathbb{E}_{(l,\\mathbf{t})\\sim\\mathcal{D}}\\left[f(l,\\mathcal{M}(\\mathbf{t}))\\right]}_{\\bar{s}\\,(\\mathrm{average\\,model\\,score})}\\,\\underbrace{b}_{\\mathrm{lower\\,bound}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We define the two typical performance functions corresponding to accuracy and log-loss. Note the shared subterm $\\Delta\\ell_{i}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f^{\\mathrm{accuracy}}(t_{\\mathrm{max}},\\ell):=\\mathbb{1}[\\underset{i}{\\mathrm{argmax}}\\,\\ell_{i}=t_{\\mathrm{max}}]=\\mathbb{1}[0>\\underset{i\\neq t_{\\mathrm{max}}}{\\mathrm{max}}\\,\\underbrace{\\ell_{i}-\\ell_{t_{\\mathrm{max}}}}_{\\Delta\\ell_{i}}]}\\\\ &{f^{\\mathrm{log-loss}}(t_{\\mathrm{max}},\\ell):=(\\sigma(\\ell))_{t_{\\mathrm{max}}}=\\log(\\sum_{i}\\exp(\\underbrace{\\ell_{i}-\\ell_{t_{\\mathrm{max}}}}_{\\Delta\\ell_{i}}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We present the model definition in four different regroupings to define via underbrace labels various useful quantities: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}(\\mathbf{t})=\\ell(\\mathbf{t})=\\sigma^{*}\\left(\\underbrace{\\left(x_{\\mathfrak{q u e r y}}E+P_{\\mathfrak{q u e r y}}\\right)Q K^{T}(\\mathbf{x}E+P)^{T}}_{\\mathfrak{q u e r i o n t}}\\right)\\cdot\\underbrace{\\left(\\mathbf{x}E+P\\right)V O U}_{\\mathfrak{o v e r i o n t}}+\\underbrace{\\left(x_{\\mathfrak{q u e r y}}E\\right.}_{\\mathrm{ain~op}}}\\\\ &{=\\underbrace{\\sigma^{*}\\left(x_{\\mathfrak{q u e r y}}\\left(\\underbrace{\\mathbf{E}_{q}Q K^{T}\\mathbf{E}^{T}}_{\\mathrm{EQKE}}\\mathbf{x}^{T}+\\underbrace{\\mathbf{E}_{q}Q K^{T}\\hat{\\mathbf{F}}^{T}}_{\\mathrm{EQKP}}\\right)/\\sqrt{d}\\right)}_{\\mathfrak{q u e r}(\\mathbf{t})}\\cdot\\underbrace{\\left(\\underbrace{\\mathbf{x}\\bar{\\mathbf{F}}O U}_{\\mathfrak{U U}}+\\underbrace{\\hat{\\mathbf{F}}V O U}_{\\mathfrak{p v e r o r y}}+x_{\\mathfrak{p v o r y}}\\right)}_{\\mathrm{EPVOU}(\\mathbf{t})}+x}\\\\ &{=\\underbrace{\\alpha^{*}(\\mathbf{t})\\cdot\\mathbf{x}\\bar{\\mathbf{E}}V O U}_{\\mathfrak{e x p}(\\mathbf{t})}+\\underbrace{\\alpha^{*}(\\mathbf{t})\\cdot\\hat{\\mathbf{P}}V O U}_{\\mathfrak{e v e r}(\\mathbf{t})}+\\underbrace{x_{\\mathfrak{q u e r y}}\\mathbf{E}_{q U}}_{\\mathrm{effver~}(\\mathbf{z}_{\\mathfrak{q u e r y}})}}\\\\ &{=\\left(\\sum_{i=0}^{n_{\\mathrm{test-1}}}\\underbrace{\\left(\\underbrace{\\alpha^{*}(\\mathbf{t})_{i}x_{i}\\bar{\\mathbf{F}}O U}_{\\mathfrak{e x p}(\\mathbf{t})}+\\underbrace{(\\mathbf{x}^{*}(\\mathbf{t})_{i})\\hat{\\mathbf{F}}V O U}_{\\mathfrak{e x p}(\\mathbf{t})}\\right)}_{\\ell(\\mathbf{t})}+\\underbrace{x_{\\mathfrak{q u e r y}}\\mathbf{E}_{q U}}_{\\ell(\\mathbf{t})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "D Brute-force proof ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Theorem 1. For BRUTE-FORCE $(d_{\\mathrm{vocab}},n_{\\mathrm{ctx}},\\mathcal{M})$ as defined in Algorithm 1, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{t}\\sim U(0,1,\\dots,d_{\\mathrm{vocab}}-1)^{n_{\\mathrm{ctx}}}}\\left[\\operatorname{argmax}_{i}(\\mathcal{M}(\\mathbf{t}))_{i}=\\operatorname*{max}_{i}t_{i}\\right]\\ge\\mathtt{B R U T E}\\mathbf{-}\\mathtt{F O R C E}(d_{\\mathrm{vocab}},n_{\\mathrm{ctx}},\\mathcal{M})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. In fact the two sides of the inequality are equal by definition. Hence the inequality follows by reflexivity of $\\geq$ . \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Algorithm 1 Counting Correct Sequences By Brute Force   \n1: function CORRECTNESS( $\\mathcal{M}$ , input-sequence)   \n2: return MODEL-BEHAVIOR $\\mathcal{M}$ , input-sequence) $==$ MAX(input-sequence)   \n3: end function   \n4: function BRUTE-FORCE( $\\mathrm{{\\Delta}}d_{\\mathrm{{vocab}}}$ , $n_{\\mathrm{ctx}}$ , $\\mathcal{M})$ )   \n5: return $\\frac{1}{d_{\\mathrm{vocab}}{}^{n_{\\mathrm{ctx}}}}$ SUM(CORRECTNESS( $\\mathcal{M}$ , tokens) for tokens $\\in\\big(\\mathrm{RANGE}\\big(d_{\\mathrm{vocab}}\\big)\\big)^{n_{\\mathrm{ctx}}}\\big)$   \n6: end function ", "page_idx": 26}, {"type": "text", "text": "E Details of cubic proof ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we prove formally the result used in Section 4.2, A cubic proof. ", "page_idx": 26}, {"type": "text", "text": "At its heart, the convexity of softmax19 is an extension to a simple idea: a weighted average of scalar values is extremized by putting $100\\%$ of the weight on an extremal value. ", "page_idx": 26}, {"type": "text", "text": "Using this simple version of the theorem, however, gives a useless bound of $0\\%$ accuracy: if we pay no attention to the maximum of the sequence, of course we\u2019re going to get the wrong answer. Since in fact the space of possible weightings we may see in practice is much smaller (finite, in fact, with at most ${d_{\\mathrm{vocab}}}^{n_{\\mathrm{ctx}}}$ values), we may look for a more general version of this idea that gives us tighter bounds that still cover the space of possible weightings. ", "page_idx": 26}, {"type": "text", "text": "The weights are not linearly independently choosable (softmax is non-linear), so extremal values do not necessarily result from putting maximal attention on the worst token. It may be, when trying to find the worst case, that some positions are so dis-preferred that it makes more sense to choose a token that is \u201cless bad\u201d for those positions, if it draws enough attention away from the correct token. See Lemma 3 for details. ", "page_idx": 26}, {"type": "text", "text": "We thus spend this section characterizing a relaxation of the constraints on weights: ", "page_idx": 26}, {"type": "text", "text": "1. that contains all actually possible weightings,   \n2. that is extremized at weights that still correspond to some notion of \u201cput the most weight on the extremal tokens\u201d, and   \n3. for which computing the extremal weightings is computationally efficient. ", "page_idx": 26}, {"type": "text", "text": "Before diving in, let\u2019s recall the proof that a weighted average of scalar values is extremized by putting $100\\%$ of the weight on extremal values: ", "page_idx": 26}, {"type": "text", "text": "Theorem 2 (Warmup: Extremizing weighted averages). Fix a set of values $v_{i}\\in\\mathbb{R}$ . The weighted average is bounded by the extremal values: for any $w_{i}$ such that $\\textstyle\\sum_{i}w_{i}=1$ and $0\\leq w_{i}\\leq1$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{i}v_{i}\\leq\\sum_{i}w_{i}v_{i}\\leq\\operatorname*{max}_{i}v_{i}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. The proof is simple. We have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{i}w_{i}v_{i}-\\operatorname*{min}_{i}v_{i}=\\sum_{i}w_{i}\\big(v_{i}-\\operatorname*{min}_{j}v_{j}\\big)\\geq0\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i}v_{i}-\\sum_{i}w_{i}v_{i}=\\sum_{i}w_{i}(\\operatorname*{max}_{j}v_{j}-v_{i})\\geq0\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "so the result follows. ", "page_idx": 27}, {"type": "text", "text": "E.1 Proof strategy ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The model computes the true maximum $t_{\\mathrm{max}}$ when its outputs logits $\\ell$ are such that $\\Delta\\ell_{t^{*}}:=$ $\\ell_{t^{*}}\\,-\\,\\ell_{t_{\\operatorname*{max}}}\\,<\\,0$ for all $t^{\\ast}\\,\\ne\\,t_{\\mathrm{max}}$ .20 As a result, it suffices to lower-bound the proportion of sequences where (an upper bound on) $\\Delta\\ell_{t^{*}}$ is negative for all $t^{*}\\,\\ne\\,t_{\\mathrm{max}}$ . In particular, we will upper-bound the contribution from incorrect tokens $t$ in positions $i$ to the difference $\\Delta\\ell^{i}$ between incorrect $(t^{*})$ and correct $\\left(t_{\\operatorname*{max}}\\right)$ output tokens $\\Delta\\ell_{t^{*}}^{i}=\\ell_{t^{*}}^{i}-\\ell_{t_{\\operatorname*{max}}}^{i}$ . ", "page_idx": 27}, {"type": "text", "text": "We do this by arguing that the logit difference $\\boldsymbol{\\Delta}\\ell_{t^{*}}$ satisfies a certain notion of convexity over the space of a relaxation of sequences (Theorem 6), and constructing a set of $\\Theta(d_{\\mathrm{vocab}}{}^{3}n_{\\mathrm{ctx}})$ \u201cextremal\u201d relaxed sequences where the position and token embedding components of attention are pessimized independently. ", "page_idx": 27}, {"type": "text", "text": "We start by first rewriting the contribution of each token through the attention head to the logit difference into the contributions involving PVOU and EVOU: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Delta\\ell_{t^{*}}^{t}(\\mathbf{t})=\\Delta\\ell^{\\mathrm{PVOU},i}{}_{t^{*}}(\\mathbf{t})+\\Delta\\ell^{\\mathrm{EVOU},i}{}_{t^{*}}(\\mathbf{t})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We then upper bound $\\Delta\\ell^{\\mathrm{PVOU},i}{}_{t^{*}}(\\mathbf{t})$ by noting that because the softmax attention is a weighted average of PVOU, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\ell^{\\mathrm{PVOU},i}{}_{t^{*}}(\\mathbf{t})=\\ell^{\\mathrm{PVOU},i}(\\mathbf{t})_{t^{*}}-\\ell^{\\mathrm{PVOU},i}(\\mathbf{t})_{\\operatorname*{max}_{j}t_{j}}}\\\\ &{\\qquad\\qquad\\qquad=\\alpha_{i}^{*}(\\mathbf{t})\\mathrm{PVOU}_{i,t^{*}}-\\alpha_{i}^{*}(\\mathbf{t})\\mathrm{PVOU}_{i,\\operatorname*{max}_{j}t_{j}}}\\\\ &{\\qquad\\qquad=\\alpha_{i}^{*}(\\mathbf{t})\\left(\\mathrm{PVOU}_{i,t^{*}}-\\mathrm{PVOU}_{i,\\operatorname*{max}_{j}t_{j}}\\right)}\\\\ &{\\qquad\\qquad\\leq\\alpha_{i}^{*}(\\mathbf{t})\\operatorname*{max}_{i}\\left(\\mathrm{PVOU}_{i,t^{*}}-\\mathrm{PVOU}_{i,\\operatorname*{max}_{j}t_{j}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since $\\textstyle\\sum_{i}\\alpha_{i}^{*}(\\mathbf{t})=1$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{n_{\\mathrm{ctx}}-1}\\Delta\\ell^{\\mathrm{PVOU},i}{}_{t^{*}}(\\mathbf{t})\\leq\\operatorname*{max}_{i}\\left(\\mathrm{PVOU}_{i,t^{*}}-\\mathrm{PVOU}_{i,\\operatorname*{max}_{j}t_{j}}\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We then construct a set $\\Xi^{\\mathrm{pure}}$ of \u201cpure sequences\u201d consisting of only three types of tokens in one of two orders, and show that for each input sequence $\\mathbf{t}$ and readoff logit $t^{*}$ , we bound the logit difference from the token embeddings $\\Delta\\ell^{\\mathrm{EVOU},i}{}_{t^{*}}$ (t) using a small subset $\\mathcal{X}$ of $\\Xi^{\\mathrm{pure}}$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{n_{\\mathrm{ctx}}-1}\\Delta\\ell^{\\mathrm{EVOU},i}{}_{t^{*}}(\\mathbf{t})\\leq\\operatorname*{max}_{\\xi\\in\\mathcal{X}}\\sum_{i=0}^{n_{\\mathrm{ctx}}-1}\\Delta\\ell^{\\mathrm{EVOU},i}{}_{t^{*}}(\\xi)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We construct a set $X^{\\mathrm{relaxed}}$ of relaxed sequences, where each relaxed sequence trelaxed consists of a sequence and a position $(\\mathbf{t},i)$ , where $\\Delta\\ell_{t^{*}}(\\mathbf{t},i)$ is evaluated by separately considering the positional contribution through attention (that is, the attention weighted PVOU) and the token contribution (that is, the attention-weighted EVOU) and direct contribution (the logit difference through the skip connection EU). Note that $i$ indicates the position that we pay $100\\%$ of the attention to for the PVOU contribution. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "We argue that $\\Delta\\ell_{t^{*}}(\\mathbf{t},i)$ satisfies a certain notion of convexity over mixtures of sequences, such that we can evaluate it only on a set of $\\Theta(d_{\\mathrm{vocab}}{}^{3}n_{\\mathrm{ctx}})$ \u201cextremal\u201d sequences in a way that takes $O(d_{\\mathrm{vocab}}{}^{3}n_{\\mathrm{ctx}})$ total time to bound $\\Delta\\ell_{t^{*}}(\\mathbf{t},i)$ for every possible input sequence. We then use the extremal sequences that the model gets correct to lower bound the proportion of all sequences that the model will get correct. Specifically, we argue that Algorithm 3 provides a valid lower bound on the proportion of sequences the model gets correct. ", "page_idx": 28}, {"type": "text", "text": "E.2 Proof outline ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We now proceed to the main results of this section. ", "page_idx": 28}, {"type": "text", "text": "Math fact: For each token $t^{*}$ , the logit difference $\\Delta\\ell_{t^{*}}$ for any sequence t can be decomposed into the direct contribution from the embeds $\\ell^{\\mathrm{EU}}$ , the attention-weighted position contribution (PVOU), and the attention-weighted token contribution (EVOU). Therefore, it suffices to upper bound each of the three components independently, since summing these upper bounds gives a valid upper bound on the logit difference. ", "page_idx": 28}, {"type": "text", "text": "We can compute the direct contribution $\\ell^{\\mathrm{EU}}$ exactly by first computing $\\operatorname{EU}=\\mathbf{E}_{q}U$ and then, for each max, subtracting the logit of the max token from each row of the matrix. No theorems needed. For each max token, we can bound the position contribution by its maximum over positions (Theorem 6). ", "page_idx": 28}, {"type": "text", "text": "In order to upper bound the token contribution, we argue that any mixed sequence will be upper bounded by the maximum of the corresponding pure sequences (Theorem 7). We then argue that for pure sequences, it suffices to consider orderings where same tokens appear contiguously (Theorem 4). ", "page_idx": 28}, {"type": "text", "text": "E.3 Formal proof ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For this subsection, all theorems are parameterized over the following quantities. ", "page_idx": 28}, {"type": "text", "text": "Definition 1 (Common theorem parameters). Fix a token value function (\u00e0 la a row difference in EVOU) $v\\,:\\,\\mathbb{N}_{<d_{\\mathrm{vocab}}}\\,\\to\\,\\mathbb{R}$ and a token attention function (\u00e0 la EQKE for a fixed query token) $a:\\mathbb{N}_{<d_{\\mathrm{vocab}}}\\rightarrow\\mathbb{R}$ . Fix a position value function ( $\\grave{a}$ la a row difference in PVOU) $w:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow\\mathbb{R}$ and a position attention function (\u00e0 la EQKP for a fixed query token) $b:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow\\mathbb{R}$ . ", "page_idx": 28}, {"type": "text", "text": "In practice, we\u2019ll take, for fixed query token $t_{\\mathrm{query}}$ , fixed output token of interest $t^{*}$ , and fixed maximum token tmax, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{v_{t}=\\mathrm{EVOU}_{t,t^{*}}-\\mathrm{EVOU}_{t,t_{\\mathrm{max}}}\\qquad\\qquad}&{a_{t}=\\mathrm{EQKE}_{t_{\\mathrm{query}},t}/\\sqrt{d}}\\\\ &{w_{i}=\\mathrm{PVOU}_{i,t^{*}}-\\mathrm{PVOU}_{i,t_{\\mathrm{max}}}\\qquad\\qquad}&{b_{i}=\\mathrm{EQKP}_{t_{\\mathrm{query}},i}/\\sqrt{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Definition 2 (of a sequence via sorted tokens and a position permutation). We can define a sequence of tokens via sorted tokens and a position permutation by specifying a non-decreasing sequence of tokens $t_{0}\\le\\cdot\\cdot\\cdot\\le t_{n_{\\mathrm{ctx}}-1}\\in\\mathbb{N}_{<d_{\\mathrm{vocab}}}$ paired with a permutation $\\sigma:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow\\mathbb{N}_{<n_{\\mathrm{ctx}}}$ . ", "page_idx": 28}, {"type": "text", "text": "Definition 3 (sequence score). Given a non-decreasing sequence of tokens $t_{0}\\le\\cdot\\cdot\\cdot\\le t_{n_{\\mathrm{ctx}}-1}\\in$ $\\mathbb{N}_{<d_{\\mathrm{vocab}}}$ and a permutation $\\sigma:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow\\mathbb{N}_{<n_{\\mathrm{ctx}}}$ define the sequence score $S t_{0},...,t_{n_{\\mathrm{ctx}}-1},\\sigma$ as: ", "page_idx": 28}, {"type": "equation", "text": "$$\ns_{t_{0},\\dots,t_{n_{\\mathrm{ctx}}-1},\\sigma}:=\\sum_{0\\le i<n_{\\mathrm{ctx}}}v_{t_{i}}e^{a_{t_{i}}+b_{\\sigma(i)}}\\Biggl/\\sum_{0\\le i<n_{\\mathrm{ctx}}}e^{a_{t_{i}}+b_{\\sigma(i)}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We will drop the token subscript, writing only $s_{\\sigma}$ , when the token values are unambiguous by context. ", "page_idx": 28}, {"type": "text", "text": "The sequence score here will be computing $\\Delta\\ell^{\\mathrm{EVOU}}{}_{t^{*}}$ for some fixed $t^{*}$ and $t_{\\mathrm{max}}$ . The way we\u2019ve set up our definitions, high scores predict $t^{*}$ (and are thus bad), negative scores predict $t_{\\mathrm{max}}$ (and are thus good), and more negative the scores, the stronger the prediction of $t_{\\mathrm{max}}$ . ", "page_idx": 28}, {"type": "text", "text": "Definition 4 (swap permutation). Given a permutation $\\sigma:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow\\mathbb{N}_{<n_{\\mathrm{ctx}}}$ of the $n_{\\mathrm{ctx}}$ positions and two indices $0\\leq i,j<n_{\\mathrm{ctx}},$ , define the swap permutation $\\sigma_{i\\leftrightarrow j}$ to be the permutation that is $\\sigma$ ", "page_idx": 28}, {"type": "text", "text": "except swapping $i$ and $j$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sigma_{i\\leftrightarrow j}(k)=\\left\\{{\\sigma}(i)\\quad i f k=j\\atop\\sigma(j)\\quad i f k=i\\atop\\sigma(k)\\quad o t h e r w i s e}\\right.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Define $\\Delta_{\\sigma,i\\leftrightarrow j}$ to be the difference in sequence scores when you swap $i$ and $j$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Delta_{\\sigma,i\\leftrightarrow j}:=s_{\\sigma_{i\\leftrightarrow j}}-s_{\\sigma}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma 3 (Characterization of swapping tokens). Fix a non-decreasing sequence of tokens $t_{0}\\leq\\cdot\\cdot\\leq$ $t_{n_{\\mathrm{ctx}}-1}\\in\\mathbb{N}$ . Fix $\\sigma:\\mathbb{N}\\rightarrow\\mathbb{N}$ be a permutation of the $n_{\\mathrm{ctx}}$ positions. Fix indices $0\\leq i,j<n_{\\mathrm{ctx}}$ . ", "page_idx": 29}, {"type": "text", "text": "Then there are two cases for $\\mathrm{sign}\\left(\\Delta_{\\sigma,i\\leftrightarrow j}\\right)$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota_{j}\\ t h e n\\mathrm{\\sign}\\left(\\Delta_{\\sigma,i\\leftrightarrow j}\\right)=-\\mathrm{sign}\\left(b_{\\sigma(i)}-b_{\\sigma(j)}\\right)\\mathrm{sign}\\left(v_{t_{i}}-v_{t_{j}}\\right).}\\\\ &{;\\mathrm{sign}\\left(\\Delta_{\\sigma,i\\leftrightarrow j}\\right)=\\mathrm{sign}\\left(a_{t_{i}}-a_{t_{j}}\\right)\\mathrm{sign}\\left(b_{\\sigma(i)}-b_{\\sigma(j)}\\right)\\mathrm{sign}\\left(s_{\\sigma}-\\frac{v_{t_{i}}e^{a_{t_{i}}}-v_{t_{j}}e^{a_{t_{j}}}}{e^{a_{t_{i}}}-e^{a_{t_{j}}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Intuitively, Lemma 3 says that, if the token contribution to attention is equal between tokens $t_{i}$ and $t_{j}$ , then the impact of swapping their positions $\\sigma(i)$ and $\\sigma(j)$ is entirely determined by how much attention is paid to the positions of $i$ and $j$ and the relative difference in their value. (Notably, by swapping these tokens, we don\u2019t affect the attention paid on other tokens, and so the effect of the change does not depend on the values of the other tokens.) Alternatively, if the attentions are not equal, then swapping the positions changes the allocation of attention to other tokens in the sequence, and so it may the case that this change in allocation in attention dominates the attention-weighted values of these two tokens. ", "page_idx": 29}, {"type": "text", "text": "Proof. First note that the theorem is trivial for $i=j$ . ", "page_idx": 29}, {"type": "text", "text": "For the rest of the proof, we take $i\\neq j$ . ", "page_idx": 29}, {"type": "text", "text": "The proof proceeds just by algebraic manipulation with no deep insight. We first list the facts we use, the proceed to computing sig $\\mathrm{n}\\left(\\Delta_{\\sigma,i\\leftrightarrow j}\\right)$ . We abbreviate $\\sigma_{i\\leftrightarrow j}$ as $\\sigma^{\\prime}$ for brevity. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{sign}\\left(e^{b_{\\sigma\\left(i\\right)}}-e^{b_{\\sigma\\left(j\\right)}}\\right)=\\mathrm{sign}\\left(b_{\\sigma\\left(i\\right)}-b_{\\sigma\\left(j\\right)}\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{sign}\\left(\\Delta_{\\sigma,i\\leftrightarrow j}\\right)=\\mathrm{sign}\\left(s_{\\sigma^{\\prime}}-s_{\\sigma}\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\mathrm{sign}\\left(\\frac{\\sum_{0\\leq p<n_{\\mathrm{ctx}}}v_{t_{p}}e^{a_{t_{p}}+b_{\\sigma^{\\prime}(p)}}}{\\sum_{0\\leq p<n_{\\mathrm{ctx}}}e^{a_{t_{p}}+b_{\\sigma^{\\prime}(p)}}}-s_{\\sigma}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now multiply through by the denominator, which is positive ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathrm{sign}\\left(\\displaystyle\\sum_{0\\leq p<r_{0},\\ldots,n_{k}}\\psi_{\\varepsilon}e^{\\alpha_{k}+h_{\\varepsilon}(r_{1})}-\\delta_{\\sigma}\\sum_{i=1}^{n}e^{\\alpha_{k}+h_{\\varepsilon}(r_{1})}\\right)}\\\\ &{=\\mathrm{sign}\\left(\\displaystyle\\sum_{0\\leq p<r_{0},\\ldots,n_{k}}\\psi_{\\varepsilon}e^{\\alpha_{k}+h_{\\varepsilon}(r_{1})}-v_{1,\\varepsilon}e^{\\alpha_{k}}(e^{\\beta_{\\varepsilon}(r_{1})}-e^{b_{\\varepsilon}(r_{1})})-v_{1,\\varepsilon}e^{\\alpha_{1}}\\,\\,\\left(e^{h_{\\varepsilon}(r_{1})}-e^{b_{\\varepsilon}(r_{1})}\\right)\\right.}\\\\ &{\\qquad\\qquad\\left.-\\delta_{\\sigma}\\sum_{i=1}^{n}e^{\\alpha_{k}}(e^{h_{\\varepsilon}(r_{1})}+\\delta_{\\sigma}e^{\\alpha_{k}}\\,\\,\\left(e^{h_{\\varepsilon}(i)}-e^{b_{\\varepsilon}(r_{1})}\\right)+s_{\\sigma}e^{\\alpha_{1}}\\,\\,\\left(e^{h_{\\varepsilon}(i)}-e^{b_{\\varepsilon}(r_{1})}\\right)\\right)}\\\\ &{=\\mathrm{sign}\\left(\\displaystyle\\sum_{0\\leq p<r_{0},\\ldots,n_{k}}\\psi_{\\varepsilon}e^{\\alpha_{k}+h_{\\varepsilon}(r_{1})}-v_{1,\\varepsilon}e^{\\alpha_{1}}\\,\\,\\left(e^{h_{\\varepsilon}(i)}-e^{b_{\\varepsilon}(r_{1})}\\right)-v_{1,\\varepsilon}e^{\\alpha_{1}}\\,\\,\\left(e^{h_{\\varepsilon}(i)}-e^{h_{\\varepsilon}(i)}\\right)\\right.}\\\\ &{\\qquad\\qquad\\left.-\\displaystyle\\sum_{g\\leq r<r_{0},\\ldots,n_{k}}e^{\\alpha_{g}t_{1}+g\\sqrt{g_{r_{1}}(r_{1})}+\\delta_{\\sigma}e^{\\alpha_{g}t_{1}}}+e^{\\alpha_{g}t_{1}}\\,\\,\\left(e^{h_{\\varepsilon}(i)}-e^{h_{\\varepsilon}(i)}\\right)+s_{\\sigma}e^{\\alpha_{g}t_{1}}\\,\\,\\left(e^{h_{\\varepsilon}(i)}-e^{h_{\\varepsilon}(i)}\\right)\\right) \n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathrm{sign}\\left(\\left(v_{t_{j}}e^{a_{t_{j}}}-v_{t_{i}}e^{a_{t_{i}}}\\right)\\left(e^{b_{\\sigma(i)}}-e^{b_{\\sigma(j)}}\\right)+s_{\\sigma}\\left(e^{a_{t_{i}}}-e^{a_{t_{j}}}\\right)\\left(e^{b_{\\sigma(i)}}-e^{b_{\\sigma(j)}}\\right)\\right)}\\\\ &{=\\mathrm{sign}\\left(e^{b_{\\sigma(i)}}-e^{b_{\\sigma(j)}}\\right)\\mathrm{sign}\\left(\\left(v_{t_{j}}e^{a_{t_{j}}}-v_{t_{i}}e^{a_{t_{i}}}\\right)+s_{\\sigma}\\left(e^{a_{t_{i}}}-e^{a_{t_{j}}}\\right)\\right)}\\\\ &{=\\mathrm{sign}\\left(b_{\\sigma(i)}-b_{\\sigma(j)}\\right)\\mathrm{sign}\\left(s_{\\sigma}\\left(e^{a_{t_{i}}}-e^{a_{t_{j}}}\\right)-\\left(v_{t_{i}}e^{a_{t_{i}}}-v_{t_{j}}e^{a_{t_{j}}}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Divide through by non-zero values when possible ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\;}&{=\\mathrm{sign}\\left(b_{\\sigma(i)}-b_{\\sigma(j)}\\right)}\\\\ &{\\quad\\cdot\\left\\{\\begin{array}{l l}{\\mathrm{sign}\\left(v_{t_{i}}-v_{t_{j}}\\right)}&{\\mathrm{if}\\;a_{t_{i}}=a_{t_{j}}}\\\\ {\\mathrm{sign}\\left(e^{a_{t_{i}}}-e^{a_{t_{j}}}\\right)\\mathrm{sign}\\left(s_{\\sigma}-\\frac{v_{t_{i}}e^{a_{t_{i}}}-v_{t_{j}}e^{a_{t_{j}}}}{e^{a_{t_{i}}}-e^{a_{t_{j}}}}\\right)}&{\\mathrm{otherwise}}\\end{array}\\right.}\\\\ &{=\\left\\{\\begin{array}{l l}{-\\mathrm{sign}\\left(b_{\\sigma(i)}-b_{\\sigma(j)}\\right)\\mathrm{sign}\\left(v_{t_{i}}-v_{t_{j}}\\right)}&{\\mathrm{if}\\;a_{t_{i}}=a_{t_{j}}}\\\\ {\\mathrm{sign}\\left(a_{t_{i}}-a_{t_{j}}\\right)\\mathrm{sign}\\left(b_{\\sigma(i)}-b_{\\sigma(j)}\\right)\\mathrm{sign}\\left(s_{\\sigma}-\\frac{v_{t_{i}}e^{a_{t_{i}}}-v_{t_{j}}e^{a_{t_{j}}}}{e^{a_{t_{i}}}-e^{a_{t_{j}}}}\\right)}&{\\mathrm{otherwise}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Definition 5 ( $\\bar{\\sigma}$ fixes $F$ ). Fix a set of fixed indices $F\\subseteq\\mathbb{N}_{<n_{\\mathrm{ctx}}}$ and an assignment of token values to each of the fixed positions $t_{F}:F\\to\\mathbb{N}_{<d_{\\mathrm{vocab}}}$ . $F$ is the set of positions for which we are not pessimizing over the value of the token in that position.) Fix a non-decreasing sequence of tokens $t_{0}\\le\\cdot\\cdot\\cdot\\le t_{n_{\\mathrm{ctx}}-1}\\in\\mathbb{N}.$ . ", "page_idx": 30}, {"type": "text", "text": "Given a permutation $\\sigma:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow\\mathbb{N}^{n_{\\mathrm{ctx}}}$ , say that $\\sigma$ fixes $F$ (relative to $t_{0},\\ldots,t_{n_{\\mathrm{ctx}}-1})$ if $t_{i}=$ $t_{F}(\\sigma(i))$ whenever $\\sigma(i)\\in F$ . ", "page_idx": 30}, {"type": "text", "text": "Note that in this section, for the cubic proofs, we will in fact generally take $F=\\{n_{\\mathrm{ctx}}-1\\}$ , so that we are fixing the final query token, though in Theorems 7, 8, and 9 $F$ will also contain all positions with the maximum token $t_{\\mathrm{max}}$ . In Appendix F, we will take $F=\\emptyset$ or $F$ to be the set of positions of the maximum token. However, none of these theorems are specific to $F$ being subsingleton, and we prove them in generality. ", "page_idx": 30}, {"type": "text", "text": "Definition 6 (position-sorting permutation). Fix a set of fixed indices $F\\subseteq\\mathbb{N}_{<n_{\\mathrm{ctx}}}$ and an assignment of token values to each of the fixed positions $t_{F}:F\\to\\mathbb{N}_{<d_{\\mathrm{vocab}}}$ . ", "page_idx": 30}, {"type": "text", "text": "Define the position-sorting permutation fixing indices in $F~\\sigma_{s}~:~\\mathbb{N}_{<n_{\\mathrm{ctx}}}~\\to~\\mathbb{N}_{<n_{\\mathrm{ctx}}}$ to be the permutation that sorts the indices not in $F$ according to $b$ : for $0\\leq i,j<n_{\\mathrm{ctx}}\\;w i t h\\;i,j\\notin F,\\,b_{i}\\leq b_{j}$ whenever $\\sigma_{s}(i)<\\sigma_{s}(j)$ ; and $\\sigma_{s}(i)=i$ for $i\\in F$ . ", "page_idx": 30}, {"type": "text", "text": "Definition 7 (contiguous on equal tokens). Fix a set of fixed indices $F\\subseteq\\mathbb{N}_{<n_{\\mathrm{ctx}}}$ and an assignment of token values to each of the fixed positions $t_{F}:F\\to\\mathbb{N}_{<d_{\\mathrm{vocab}}}$ . Fix a non-decreasing sequence of tokens $t_{0}\\le\\cdot\\cdot\\cdot\\le t_{n_{\\mathrm{ctx}}-1}\\in\\mathbb{N}$ . ", "page_idx": 30}, {"type": "text", "text": "Say that the sequence represented by a permutation $\\sigma:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow\\mathbb{N}_{<n_{\\mathrm{ctx}}}$ is contiguous on equal tokens $i f,$ for all $0\\leq i,j,k<n_{\\mathrm{ctx}}$ with $t_{i}=t_{j}\\ne t_{k}$ and $i,j,k\\not\\in\\sigma^{-1}(F),$ , it is never the case that $\\sigma_{s}(\\sigma(i))<\\sigma_{s}(\\sigma(k))<\\sigma_{s}(\\sigma(j))$ . ", "page_idx": 30}, {"type": "text", "text": "Theorem 4 (Pessimization over sequence ordering is possible and results in contiguous sequences). Fix a set of fixed indices $F\\subseteq\\mathbb{N}_{<n_{\\mathrm{ctx}}}$ and an assignment of token values to each of the fixed positions $t_{F}:F\\to\\mathbb{N}_{<d_{\\mathrm{vocab}}}$ . Fix a non-decreasing sequence of tokens $t_{0}\\le\\cdot\\cdot\\cdot\\le t_{n_{\\mathrm{ctx}}-1}\\in\\mathbb{N}$ . ", "page_idx": 30}, {"type": "text", "text": "Let $\\sigma_{\\mathrm{min}}$ , $\\sigma_{\\mathrm{max}}:\\mathbb{N}\\to\\mathbb{N}$ be permutations of the $n_{\\mathrm{ctx}}$ positions, fixing positions in $F$ , satisfying the following property: For all $\\sigma:\\mathbb{N}\\rightarrow\\mathbb{N}\\,.$ a permutation fixing $F$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\ns_{\\sigma_{\\mathrm{min}}}\\le s_{\\sigma}\\le s_{\\sigma_{\\mathrm{max}}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "(Such permutations are guaranteed to exist because the permutation group on $n_{\\mathrm{ctx}}$ elements is finite.) ", "page_idx": 30}, {"type": "text", "text": "Then $\\sigma_{\\mathrm{max}}$ and $\\sigma_{\\mathrm{min}}$ may be taken to be contiguous on equal tokens. That is, there exist $\\sigma_{\\mathrm{max}}$ and $\\sigma_{\\mathrm{min}}$ satisfying the property of Equation $I O$ which additionally satisfy the definition of Definition 7. ", "page_idx": 30}, {"type": "text", "text": "The basic idea is that we will assume that one of $\\sigma_{\\mathrm{max}}$ and $\\sigma_{\\mathrm{min}}$ cannot be contiguous on equal tokens and derive a contradiction. We will pick the extremal permutation that is closest to being contiguous, take a contiguity violation, and then show that either we can correct the contiguity violation without changing the score\u2014thus violating the presumption that the permutation is closest to being contiguous\u2014or we will find one swap of indices that decreases the score and another swap of indices that increases the score, thus violating the presumption of extremality. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "In slightly more detail, but still informally, we will consider the sign of the difference between scores of our purported extremal permutation and a permutation that has swapped some indices. The theorem follows from showing that there exists a triple of indices $i$ $i,j,k$ such that the sign of the score difference from swapping $i$ and $j$ is different from the sign of the score difference from swapping $j$ and $k$ . ", "page_idx": 31}, {"type": "text", "text": "First, a definition and some helpful facts about it. ", "page_idx": 31}, {"type": "text", "text": "Definition 8 (contiguous on equally-attended positions). Fix a set of fixed indices $F\\subseteq\\mathbb{N}_{<n_{\\mathrm{ctx}}}$ and an assignment of token values to each of the fixed positions $t_{F}:F\\to\\mathbb{N}_{<d_{\\mathrm{vocab}}}$ . Fix a non-decreasing sequence of tokens $t_{0}\\le\\cdot\\cdot\\cdot\\le t_{n_{\\mathrm{ctx}}-1}\\in\\mathbb{N}$ . ", "page_idx": 31}, {"type": "text", "text": "Say that a permutation $\\sigma$ is contiguous on equally-attended positions $i f,$ for all $0\\leq i<n_{\\mathrm{ctx}}$ with $i\\not\\in\\sigma^{-1}(\\bar{F})$ , the sorting order according $\\sigma_{s}$ on the contiguous block of positions with contribution to the attention score equal to that of $\\cdot(i),\\,\\{\\sigma(j)\\,\\vert\\,b_{\\sigma(j)}=b_{\\sigma(i)}$ and $\\sigma(j)\\notin F\\}$ , is the same as the sorting order according to the fraction of tokens equal to $t_{j}$ with $b$ -values greater than $b_{\\sigma(i)}$ , with ties broken by the value of $t_{j}$ . Equationally, this second sorting order is defined by the score ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left(\\big|\\{k\\;\\big|\\;t_{k}=t_{j}\\;a n d\\;b_{\\sigma(k)}>b_{\\sigma(i)}\\;a n d\\;\\sigma(k)\\notin F\\}\\big|+\\frac{t_{j}}{d_{\\mathrm{vocab}}}\\right)\\left/\\left|\\{k\\;\\big|\\;t_{k}=t_{j}\\;a n d\\;\\sigma(k)\\notin F\\}\\right|\\,.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Most importantly, any permutation that is contiguous on equally-attended positions has the property that for any indices $0\\,\\le\\,i,j,k\\,<\\,n_{\\mathrm{ctx}}$ with $i\\overline{{,}}j,k\\not\\in\\sigma^{-\\dot{1}}(F)$ and $t_{i}\\,=\\,t_{j}\\,\\ne\\,t_{k}$ and $\\sigma_{s}(\\sigma(i))\\,<$ $\\sigma_{s}(\\sigma(k))\\,<\\,\\sigma_{s}(\\sigma(j))$ , we will have the strict inequality $b_{\\sigma(i)}<b_{\\sigma(k)}<\\dot{b}_{\\sigma(j)}$ . Additionally, we may always sort equally-attended positions to make any permutation contiguous on equally-attended positions. ", "page_idx": 31}, {"type": "text", "text": "We will define an additional notion of contiguity-violations which we avoid up-front by arbitrarily swapping involved indices without changing the score $s_{\\sigma}$ . ", "page_idx": 31}, {"type": "text", "text": "Definition 9 (needlessly non-contiguous). Fix a set of fixed indices $F\\subseteq\\mathbb{N}_{<n_{\\mathrm{ctx}}}$ and an assignment of token values to each of the fixed positions $t_{F}:F\\to\\mathbb{N}_{<d_{\\mathrm{vocab}}}$ . Fix a non-decreasing sequence of tokens $t_{0}\\le\\cdot\\cdot\\cdot\\le t_{n_{\\mathrm{ctx}}-1}\\in\\mathbb{N}.$ . ", "page_idx": 31}, {"type": "text", "text": "Say that a permutation $\\sigma$ is needlessly non-contiguous at $i$ , j, $k$ (for $i,j,k\\not\\in\\sigma^{-1}(F))$ if $\\Delta_{\\sigma,i\\leftrightarrow k}=0$ or $\\Delta_{\\sigma,j\\leftrightarrow k}\\,=\\,0,$ , for $0\\,\\leq\\,i,j,k\\,<\\,n_{\\mathrm{ctx}}\\;w i t h\\;i,j,k\\,\\notin\\,\\sigma^{-1}(F)$ with $t_{i}=t_{j}\\ne t_{k}$ and $\\sigma_{s}(\\sigma(i))<$ $\\sigma_{s}(\\sigma(k))<\\sigma_{s}(\\sigma(j))$ . ", "page_idx": 31}, {"type": "text", "text": "Say that a permutation $\\sigma$ is needlessly non-contiguous if it is needlessly non-contiguous at any $i,j,k\\not\\in\\sigma^{-1}(F)$ . ", "page_idx": 31}, {"type": "text", "text": "Lemma 5. Fix a set of fixed indices $F\\subseteq\\mathbb{N}_{<n_{\\mathrm{ctx}}}$ and an assignment of token values to each of the fixed positions $t_{F}:F\\to\\mathbb{N}_{<d_{\\mathrm{vocab}}}$ . Fix a non-decreasing sequence of tokens $t_{0}\\le\\cdot\\cdot\\cdot\\le t_{n_{\\mathrm{ctx}}-1}\\in\\mathbb{N}$ ", "page_idx": 31}, {"type": "text", "text": "Any needlessly non-contiguous sequence $\\sigma$ which fixes $F$ can be made into a sequence $\\sigma^{\\prime}$ which still fixes $F$ and is both simultaneously contiguous on equally-attended positions and not needlessly non-contiguous, and for which $s_{\\sigma}=s_{\\sigma^{\\prime}}$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. First, sort regions of equally-attended positions to make $\\sigma$ contiguous on equally-attended positions. If the resulting permutation is not needlessly non-contiguous, then we are done. ", "page_idx": 31}, {"type": "text", "text": "Otherwise, we have $\\Delta_{\\sigma,i\\leftrightarrow k}\\,=\\,0$ or $\\Delta_{\\sigma,j\\leftrightarrow k}\\,=\\,0$ for some $i,\\,j,\\,k$ , for $0\\,\\le\\,i,j,k\\,<\\,n_{\\mathrm{ctx}}$ with $i,j,n\\not\\in\\sigma^{-1}(F)$ and $t_{i}=t_{j}\\ne t_{k}$ and $\\sigma_{s}(\\sigma(i))\\,<\\,\\sigma_{s}(\\sigma(k))\\,<\\,\\sigma_{s}(\\sigma(j))$ . Since the sequence is contiguous on equally-attended positions, we have the strict inequality $b_{\\sigma(i)}<b_{\\sigma(k)}<b_{\\sigma(j)}$ . ", "page_idx": 31}, {"type": "text", "text": "By Lemma 3, we have two cases. Noting that $t_{i}=t_{j}$ , we can write them as ", "page_idx": 31}, {"type": "text", "text": "1. $v_{t_{k}}=v_{t_{i}}$ and $a_{t_{i}}=a_{t_{k}}$   \n2. $a_{t_{i}}\\neq a_{t_{k}}$ and $\\begin{array}{r}{s_{\\sigma}=\\frac{v_{t_{i}}e^{a t_{i}}-v_{t_{k}}e^{a_{t_{k}}}}{e^{a_{t_{i}}}-e^{a_{t_{k}}}}}\\end{array}$ ", "page_idx": 31}, {"type": "text", "text": "In the first case, we may fully freely interchange tokens equal to $t_{i}$ with tokens equal to $t_{k}$ without changing the score; in this case we may use the token value as a sorting tie-breaker and swap tokens until there are no more needlessly non-contiguous triples falling into case (1). ", "page_idx": 32}, {"type": "text", "text": "In the second case, since swapping tokens does not change $s_{\\sigma}$ , the property will continue to hold for these tokens after the swap. We may then swap tokens, again using token value as a tie-breaker, until there are no more needlessly non-contiguous triples falling into case (2). \u53e3 ", "page_idx": 32}, {"type": "text", "text": "We can now finally make our argument for Theorem 4 more precise. ", "page_idx": 32}, {"type": "text", "text": "Proof of Theorem 4. Choose $\\sigma_{\\mathrm{max}}$ and $\\sigma_{\\mathrm{min}}$ to be contiguous on equally-attended positions and not needlessly non-contiguous, and suppose that we have $\\sigma\\,\\in\\,\\left\\lbrace\\sigma_{\\operatorname*{max}},\\sigma_{\\operatorname*{min}}\\right\\rbrace$ such that for some $0\\leq i,j,k<n_{\\mathrm{ctx}}$ with $i,\\dot{j},k\\not\\in\\sigma^{-1}(F)$ and $t_{i}=t_{j}\\ne t_{k}$ , we have $b_{\\sigma(i)}<b_{\\sigma(k)}<b_{\\sigma(j)}$ . We will derive a contradiction with the presumption that $\\sigma$ is extremal by showing that we can swap $i$ and $k$ to change the score in one direction and that we can swap $j$ and $k$ to change the score in the other direction. ", "page_idx": 32}, {"type": "text", "text": "Take $\\sigma_{0}^{\\prime}$ to be $\\sigma$ but swapping $i$ and $k$ , and take $\\sigma_{1}^{\\prime}$ to be $\\sigma$ but swapping $j$ and $k$ . ", "page_idx": 32}, {"type": "text", "text": "Now we will consider the cases for the sign of the score difference $\\Delta_{0}:=s_{\\sigma_{0}^{\\prime}}\\!-\\!s_{\\sigma}$ and $\\Delta_{1}:=s_{\\sigma_{1}^{\\prime}}\\!-\\!s_{\\sigma}$ . By the presumption of not being needlessly non-contiguous, $\\Delta_{z}\\neq0$ for $z\\in\\{0,1\\}$ . If we can show that the sign of $\\Delta_{0}$ is distinct from the sign of $\\Delta_{1}$ , then we will have a contradiction with extremality because we will have either $s_{\\sigma_{0}^{\\prime}}<s_{\\sigma}<s_{\\sigma_{1}^{\\prime}}$ or $s_{\\sigma_{1}^{\\prime}}<s_{\\sigma}<s_{\\sigma_{0}^{\\prime}}$ . That is, we would be able to swap $i\\leftrightarrow k$ and $j\\leftrightarrow k$ to get a lower and higher score, making $\\sigma$ not extremal. ", "page_idx": 32}, {"type": "text", "text": "Noting that $t_{i}=t_{j}$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{sign}\\left(\\Delta_{0}\\right)=\\mathrm{sign}\\left(b_{\\sigma\\left(i\\right)}-b_{\\sigma\\left(k\\right)}\\right)\\left\\{\\mathrm{sign}\\left(v_{t_{k}}-v_{t_{i}}\\right)\\right.}\\\\ &{\\left.\\mathrm{sign}\\left(\\Delta_{0}\\right)=\\mathrm{sign}\\left(b_{\\sigma\\left(i\\right)}-b_{\\sigma\\left(k\\right)}\\right)\\left\\{\\mathrm{sign}\\left(a_{t_{i}}-a_{t_{k}}\\right)\\mathrm{sign}\\left(s_{\\sigma}-\\frac{v_{t_{i}}e^{a_{t_{i}}}-v_{t_{k}}e^{a_{t_{k}}}}{e^{a_{t_{i}}}-e^{a_{t_{k}}}}\\right)\\right.\\right.\\left.}\\\\ &{\\left.\\mathrm{sign}\\left(\\Delta_{1}\\right)=\\mathrm{sign}\\left(b_{\\sigma\\left(j\\right)}-b_{\\sigma\\left(k\\right)}\\right)\\left\\{\\mathrm{sign}\\left(v_{t_{k}}-v_{t_{i}}\\right)\\right.}\\\\ &{\\left.\\mathrm{sign}\\left(a_{t_{i}}-a_{t_{k}}\\right)\\left.\\left\\{\\mathrm{sign}\\left(a_{t_{i}}-a_{t_{k}}\\right)\\mathrm{sign}\\left(s_{\\sigma}-\\frac{v_{t_{i}}e^{a_{t_{i}}}-v_{t_{k}}e^{a_{t_{k}}}}{e^{a_{t_{i}}}-e^{a_{t_{k}}}}\\right)\\right.\\right.\\left.\\mathrm{otherwise}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Noting that the product is non-zero by presumption, that right multiplicand is equal for $\\Delta_{0}$ and $\\Delta_{1}$ , and $\\mathrm{si}\\overline{{{\\mathrm{g}}}}\\mathrm{n}\\left(b_{\\sigma(i)}\\stackrel{\\cdot}{-}b_{\\sigma(k)}\\right)=-1$ and $\\mathrm{sign}\\left(b_{\\sigma(j)}\\dot{-}b_{\\sigma(k)}\\right)=\\dot{1}$ , we have our desired contradiction. ", "page_idx": 32}, {"type": "text", "text": "Note that the proof of Theorem 4 does not go through if we include the position value function $w$ in the score, because we may trade off the position value function against the token value function. We now show that we can independently pessimize over positional attention. ", "page_idx": 32}, {"type": "text", "text": "Definition 10 (full sequence score). Given a non-decreasing sequence of tokens $t_{0}\\le\\cdot\\cdot\\cdot\\le t_{n_{\\mathrm{ctx}}-1}\\in$ $\\mathbb{N}_{<d_{\\mathrm{vocab}}}$ and a permutation $\\sigma:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow\\mathbb{N}_{<n_{\\mathrm{ctx}}}$ define the full sequence score $s_{t_{0},\\ldots,t_{n_{\\mathrm{ctx}}-1},\\sigma}^{\\prime}\\,c$ as: ", "page_idx": 32}, {"type": "equation", "text": "$$\ns_{t_{0},\\ldots,t_{n_{\\mathrm{ctx}}-1},\\sigma}^{\\prime}:=\\sum_{0\\leq i<n_{\\mathrm{ctx}}}(v_{t_{i}}+w_{\\sigma(i)})e^{a_{t_{i}}+b_{\\sigma(i)}}\\Biggl/\\sum_{0\\leq i<n_{\\mathrm{ctx}}}e^{a_{t_{i}}+b_{\\sigma(i)}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We will drop the token subscript, writing only $s_{\\sigma}^{\\prime}$ , when the token values are unambiguous by context. ", "page_idx": 32}, {"type": "text", "text": "The sequence score here will be computing $\\Delta\\ell_{t^{*}}^{\\mathrm{EPVOU}}:=\\Delta\\ell^{\\mathrm{EVOU}}{}_{t^{*}}+\\Delta\\ell^{\\mathrm{PVOU}}{}_{t^{*}}$ for some fixed $t^{*}$ and $t_{\\mathrm{max}}$ . As with Definition 3, with the way we\u2019ve set up our definitions, high scores predict $t^{*}$ (and are thus bad), negative scores predict $t_{\\mathrm{max}}$ (and are thus good), and more negative the scores, the stronger the prediction of $t_{\\mathrm{max}}$ . ", "page_idx": 32}, {"type": "text", "text": "Definition 11 (relaxed sequence score). Given a non-decreasing sequence of tokens $t_{0}\\leq\\dots\\leq$ $t_{n_{\\mathrm{ctx}}-1}\\in\\mathbb{N}_{<d_{\\mathrm{vocab}}}$ and a permutation $\\sigma:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow\\mathbb{N}_{<n_{\\mathrm{ctx}}}$ define the relaxed sequence scores $r_{t_{0},\\dots,t_{n_{\\mathrm{ctx}}-1,\\sigma,\\mathrm{min}}}$ and $r_{t_{0},...,t_{n_{\\mathrm{ctx}}-1},\\sigma,\\mathrm{max}}\\;a$ s: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r_{t_{0},\\dots,t_{n_{\\mathrm{ctx}}-1},\\sigma,\\operatorname*{min}}:=s_{t_{0},\\dots,t_{n_{\\mathrm{ctx}}-1},\\sigma}+\\operatorname*{min}\\boldsymbol{w}_{i}}\\\\ &{r_{t_{0},\\dots,t_{n_{\\mathrm{ctx}}-1},\\sigma,\\operatorname*{max}}:=s_{t_{0},\\dots,t_{n_{\\mathrm{ctx}}-1},\\sigma}+\\operatorname*{max}\\boldsymbol{w}_{i}}\\\\ &{r_{t_{0},\\dots,t_{n_{\\mathrm{ctx}}-1},\\sigma,\\operatorname*{max}}:=s_{t_{0},\\dots,t_{n_{\\mathrm{ctx}}-1},\\sigma}+\\operatorname*{max}\\boldsymbol{w}_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We will drop the token subscript, writing only $r_{\\sigma,\\mathrm{min}}\\,o r\\,r_{\\sigma,\\mathrm{max}}$ , when the token values are unambiguous by context. ", "page_idx": 32}, {"type": "text", "text": "Theorem 6 (Independent pessimization over positional contributions is possible). Fix non-decreasing sequences of tokens $t_{0}\\,\\le\\,\\cdot\\cdot\\,\\le\\,t_{n_{\\mathrm{ctx}}-1}\\,\\in\\,\\mathbb{N}$ and $t_{0}^{\\prime}\\;\\le\\;\\cdot\\cdot\\;\\le\\;t_{n_{\\mathrm{ctx}}-1}^{\\prime}\\;\\in\\;\\mathbb{N}$ and permutations $\\sigma,\\sigma^{\\prime}:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow\\mathbb{N}_{<n_{\\mathrm{ctx}}}$ . Let $r_{\\sigma,\\mathrm{min}}$ and $r_{\\sigma,{\\mathrm{max}}}$ denote $r_{t_{0},\\dots,t_{n_{\\mathrm{ctx}}-1},\\sigma,\\mathrm{min}}$ and $r_{t_{0},...,t_{n_{\\mathrm{ctx}}-1},\\sigma,\\mathrm{max}},$ ; let $s_{\\sigma}$ denote $s_{t_{0},\\ldots,t_{n_{\\mathrm{ctx}}-1},\\sigma}$ ; and let $s_{\\sigma^{\\prime}}$ and $s_{\\sigma^{\\prime}}^{\\prime}$ denote $s_{t_{0}^{\\prime},\\ldots,t_{n_{\\mathrm{ctx}}-1}^{\\prime},\\sigma^{\\prime}}$ and $s_{t_{0}^{\\prime},\\dots,t_{n_{\\mathrm{ctx}}-1}^{\\prime},\\sigma^{\\prime}}^{\\prime}$ . ", "page_idx": 33}, {"type": "text", "text": "Then we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq i<n_{\\mathrm{ctx}}}w_{i}=r_{\\sigma,\\mathrm{min}}-s_{\\sigma}\\leq s_{\\sigma^{\\prime}}^{\\prime}-s_{\\sigma^{\\prime}}\\leq r_{\\sigma,\\mathrm{max}}-s_{\\sigma}=\\operatorname*{max}_{0\\leq i<n_{\\mathrm{ctx}}}w_{i}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "That is, the difference between the relaxed sequence score and the sequence score of any given sequence always bounds the difference between the full sequence score and the sequence score for any (related or unrelated) sequence. ", "page_idx": 33}, {"type": "text", "text": "Proof. This proof follows straightforwardly from the softmax weighting being an affine weighting. We have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r_{\\sigma,\\operatorname*{min}}-s_{\\sigma}=\\underset{0\\leq i<n_{\\mathbf{c}\\mathbf{x}}}{\\operatorname*{min}}\\boldsymbol{\\upsilon}_{\\sigma(i)}=\\underset{0\\leq i<n_{\\mathbf{c}\\mathbf{x}}}{\\operatorname*{min}}}\\\\ &{r_{\\sigma,\\operatorname*{max}}-s_{\\sigma}=\\underset{0\\leq i<n_{\\mathbf{c}\\mathbf{x}}}{\\operatorname*{max}}\\boldsymbol{\\upsilon}_{\\sigma(i)}=\\underset{0\\leq i<n_{\\mathbf{c}\\mathbf{x}}}{\\operatorname*{max}}\\boldsymbol{\\upsilon}_{i}}\\\\ &{\\quad s_{\\sigma^{\\prime}}^{\\prime}-s_{\\sigma^{\\prime}}=\\sum_{v\\in\\mathcal{V}_{i}\\times\\mathcal{V}_{i}}^{\\infty}\\boldsymbol{\\upsilon}_{v^{\\prime}(i)}e^{a_{\\ell_{i}^{\\prime}}+b_{\\sigma^{\\prime}(i)}}\\Biggl/\\underset{0\\leq i<n_{\\mathbf{c}\\mathbf{t}}}{\\sum_{\\mathbf{c}}e^{a_{\\ell_{i}^{\\prime}}+b_{\\sigma^{\\prime}(i)}}}}\\\\ &{\\quad\\quad\\quad\\quad=\\underset{0\\leq i<n_{\\mathbf{c}\\mathbf{x}}}{\\sum}\\boldsymbol{\\upsilon}_{v^{\\prime}(i)}\\frac{e^{a_{\\ell_{i}^{\\prime}}+b_{\\sigma^{\\prime}(i)}}}{\\sum_{0\\leq j<n_{\\mathbf{c}\\mathbf{x}}}e^{a_{\\ell_{j}^{\\prime}}+b_{\\sigma^{\\prime}(i)}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Since $e^{x}$ is non-negative for all real $x$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\le j<n_{\\mathrm{ctx}}}w_{\\sigma^{\\prime}(j)}\\frac{\\sum_{0\\le i<n_{\\mathrm{ctx}}}e^{a_{t_{i}^{\\prime}}+b_{\\sigma^{\\prime}(i)}}}{\\sum_{0\\le j<n_{\\mathrm{ctx}}}e^{a_{t_{j}^{\\prime}}+b_{\\sigma^{\\prime}(j)}}}\\le s_{\\sigma^{\\prime}}^{\\prime}-s_{\\sigma^{\\prime}}\\le\\operatorname*{max}_{0\\le j<n_{\\mathrm{ctx}}}w_{\\sigma^{\\prime}(j)}\\frac{\\sum_{0\\le i<n_{\\mathrm{ctx}}}e^{a_{t_{i}^{\\prime}}+b_{\\sigma^{\\prime}(i)}}}{\\sum_{0\\le j<n_{\\mathrm{ctx}}}e^{a_{t_{j}^{\\prime}}+b_{\\sigma^{\\prime}(j)}}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus we get as desired ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq i<n_{\\mathrm{ctx}}}w_{i}\\leq s_{\\sigma^{\\prime}}^{\\prime}-s_{\\sigma^{\\prime}}\\leq\\operatorname*{max}_{0\\leq i<n_{\\mathrm{ctx}}}w_{i}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that we could prove a more fine-grained theorem, that pessimizes over attention paid to positions only for sequences compatible with the chosen fixed tokens $F$ and $t_{F}$ , but since the positional contribution is so small we do not bother. ", "page_idx": 33}, {"type": "text", "text": "Theorem 7 (For a fixed ordering, softmax is convex over token counts and only pure sequences need be considered). Fix a set of fixed indices $F\\subseteq\\mathbb{N}_{<n_{\\mathrm{ctx}}}$ and an assignment of token values to each of the fixed positions $t_{F}:F\\to\\mathbb{N}_{<d_{\\mathrm{vocab}}}$ . Fix a set $S\\subseteq\\mathbb{N}_{<d_{\\mathrm{vocab}}}$ of valid other tokens in the sequence. (In our uses of this theorem, $S$ will be the largest subset of $\\mathbb{N}_{<t_{\\operatorname*{max}}}$ for which we can guarantee that the model behaves correctly on all sequences compatible with $F$ and $t_{\\mathrm{max}}$ and with tokens otherwise drawn from $S.$ .) ", "page_idx": 33}, {"type": "text", "text": "Define a comparison on non-negative integers less than $d_{\\mathrm{vocab}}$ : ", "page_idx": 33}, {"type": "equation", "text": "$$\nc:=\\sum_{i\\in F}v_{t_{F}(i)}e^{a_{t_{F}(i)}+b_{i}}\\qquad\\qquad d:=\\sum_{i\\in F}e^{a_{t_{F}(i)}+b_{i}}\\qquad\\qquad f:=\\sum_{0\\leq i\\leq n_{\\mathrm{etx}}}e^{b_{i}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathrm{cmp}(x,y):=\\mathrm{sign}\\left(d(e^{a_{x}}v_{x}-e^{a_{y}}v_{y})-c(e^{a_{x}}-e^{a_{y}})+f e^{a_{x}+a_{y}}\\left(v_{x}e^{a_{x}+a_{y}}-v_{y}e^{a_{x}+a_{y}}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Let $t_{\\mathrm{cmp\\,min}}$ and $t_{\\mathrm{cmp\\,max}}$ be the minimum and maximum elements of $S$ according to cmp.21 ", "page_idx": 33}, {"type": "text", "text": "For a given choice of a non-decreasing sequence of tokens $t_{0}\\le\\cdot\\cdot\\cdot\\le t_{n_{\\mathrm{ctx}}-1}\\in\\mathbb{N}$ compatible with $F$ and $S$ and $a$ given choice of permutation $\\sigma:\\mathbb{N}\\rightarrow\\mathbb{N}$ of the $n_{\\mathrm{ctx}}$ positions fixing $F$ $(t_{i}=t_{F}(\\sigma(i))$ ", "page_idx": 33}, {"type": "text", "text": "for $\\sigma(i)\\,\\in\\,F$ ; and $t_{i}\\ \\in\\ S$ for $\\sigma(i)\\notin\\mathbf{\\textit{F}}$ : let $s_{\\sigma,\\mathrm{{min}}}$ (and $s_{\\sigma,\\mathrm{max}})$ denote $S t_{0},...,t_{n_{\\mathrm{ct}\\mathbf{x}}-1},\\sigma$ when $t_{i}=t_{\\mathrm{cmp\\,min}}$ for all $\\sigma(i)\\notin F$ (or $t_{\\mathrm{cmp\\,max}}$ , respectively). ", "page_idx": 34}, {"type": "text", "text": "Then for all such choices of sequence-permutation pairs, ", "page_idx": 34}, {"type": "equation", "text": "$$\ns_{\\sigma,\\mathrm{min}}\\le s_{t_{0},\\dots,t_{n_{\\mathrm{ctx}}-1},\\sigma}\\le s_{\\sigma,\\mathrm{max}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This theorem follows by chaining two lemmas: that scores are extremized by considering pure sequences, and that the extremal pure sequences match the comparison function defined in the theorem statement. ", "page_idx": 34}, {"type": "text", "text": "Lemma 8 (Sequences scores are extremized on purer sequences). Fix all the same quantities as in Theorem 7. ", "page_idx": 34}, {"type": "text", "text": "For any indices $0\\leq i<j<n_{\\mathrm{ctx}},$ token values $x,y\\in S$ , the score for a sequence with $t_{i}=x\\neq$ $y=t_{j}$ is bounded on both sides by sequences with $t_{i}=t_{j}=x$ and $t_{i}=t_{j}=y$ . ", "page_idx": 34}, {"type": "text", "text": "Proof. Let $s_{\\alpha,\\beta}$ be the sequence score with $t_{i}=\\alpha$ and $t_{j}\\,=\\,\\beta$ , and define the score differences $\\Delta_{x}:=s_{x,x}-s_{x,y}$ and $\\Delta_{y}:=s_{y,y}-s_{x,y}$ . It suffices to show that $\\mathrm{sign}(\\Delta_{x}\\Delta_{y})\\leq0.$ . To show this, we must only compute the sign of $\\Delta_{\\alpha}$ for $\\alpha\\in\\{x,y\\}$ and show that whenever both $\\Delta_{x}$ and $\\Delta_{y}$ are non-zero, they have opposite signs. ", "page_idx": 34}, {"type": "text", "text": "We proceed by computation after defining some convenience variables for brevity: ", "page_idx": 34}, {"type": "equation", "text": "$$\nC:=\\sum_{\\stackrel{0\\leq k<n_{\\mathrm{ctx}}}{k\\neq i,j}}v_{t_{k}}e^{a_{t_{k}}+b_{\\sigma(k)}}\\qquad\\qquad\\qquad D:=\\sum_{\\stackrel{0\\leq k<n_{\\mathrm{ctx}}}{k\\neq i,j}}e^{a_{t_{k}}+b_{\\sigma(k)}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\tilde{\\alpha}:=\\left\\{\\begin{array}{l l}{x}&{\\mathrm{if~}\\alpha=y}\\\\ {y}&{\\mathrm{if~}\\alpha=x}\\end{array}\\right.\\qquad\\quad i_{\\alpha}:=\\left\\{\\begin{array}{l l}{i}&{\\mathrm{if~}\\alpha=x}\\\\ {j}&{\\mathrm{if~}\\alpha=y}\\end{array}\\right.\\qquad\\quad i_{\\tilde{\\alpha}}:=\\left\\{\\begin{array}{l l}{i}&{\\mathrm{if~}\\tilde{\\alpha}=x}\\\\ {j}&{\\mathrm{if~}\\tilde{\\alpha}=y}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{sign}\\left(\\Delta_{\\alpha}\\right)=\\mathrm{sign}\\left(\\frac{v_{\\alpha}e^{a_{\\alpha}+b_{\\sigma(i)}}+v_{\\alpha}e^{a_{\\alpha}+b_{\\sigma(j)}}+C}{e^{a_{\\alpha}+b_{\\sigma(i)}}+e^{a_{\\alpha}+b_{\\sigma(j)}}+D}-\\frac{v_{x}e^{a_{x}+b_{\\sigma(i)}}+v_{y}e^{a_{y}+b_{\\sigma(j)}}+C}{e^{a_{x}+b_{\\sigma(i)}}+e^{a_{y}+b_{\\sigma(j)}}+D}\\right)}\\\\ &{\\qquad\\qquad=\\mathrm{sign}\\left(\\frac{v_{\\alpha}e^{a_{\\alpha}+b_{\\sigma(i_{\\alpha})}}+v_{\\alpha}e^{a_{\\alpha}+b_{\\sigma(i_{\\alpha})}}+C}{e^{a_{\\alpha}+b_{\\sigma(i_{\\alpha})}}+e^{a_{\\alpha}+b_{\\sigma(i_{\\alpha})}}+D}-\\frac{v_{\\alpha}e^{a_{\\alpha}+b_{\\sigma(i_{\\alpha})}}+v_{\\alpha}e^{a_{\\tilde{\\alpha}}+b_{\\sigma(i_{\\tilde{\\alpha}})}}+C}{e^{a_{\\alpha}+b_{\\sigma(i_{\\alpha})}}+e^{a_{\\tilde{\\alpha}}+b_{\\sigma(i_{\\tilde{\\alpha}})}}+D}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Multiply through by positive denominators and simplify ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathrm{sign}\\left(C\\left(e^{a_{\\tilde{\\alpha}}+b_{\\sigma\\left(i_{\\tilde{\\alpha}}\\right)}}-e^{b_{\\sigma\\left(i_{\\tilde{\\alpha}}\\right)}+a_{\\alpha}}\\right)+D\\left(v_{\\alpha}e^{b_{\\sigma\\left(i_{\\tilde{\\alpha}}\\right)}+a_{\\alpha}}-v_{\\tilde{\\alpha}}e^{a_{\\tilde{\\alpha}}+b_{\\sigma\\left(i_{\\tilde{\\alpha}}\\right)}}\\right)\\right.}\\\\ &{\\qquad\\qquad+\\left.v_{\\alpha}\\left(e^{b_{\\sigma\\left(i_{\\tilde{\\alpha}}\\right)}}+e^{b_{\\sigma\\left(i_{\\alpha}\\right)}}\\right)e^{a_{\\tilde{\\alpha}}+b_{\\sigma\\left(i_{\\tilde{\\alpha}}\\right)}+a_{\\alpha}}-v_{\\tilde{\\alpha}}\\left(e^{b_{\\sigma\\left(i_{\\tilde{\\alpha}}\\right)}}+e^{b_{\\sigma\\left(i_{\\alpha}\\right)}}\\right)e^{a_{\\tilde{\\alpha}}+b_{\\sigma\\left(i_{\\tilde{\\alpha}}\\right)}+a_{\\alpha}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Pulling out $e^{b_{\\sigma(i_{\\tilde{\\alpha}})}}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n=\\mathrm{sign}\\left(e^{a_{\\bar{\\alpha}}+a_{\\alpha}}\\left(e^{b_{\\sigma\\left(i_{\\bar{\\alpha}}\\right)}}+e^{b_{\\sigma\\left(i_{\\alpha}\\right)}}\\right)\\left(v_{\\alpha}-v_{\\bar{\\alpha}}\\right)+C\\left(e^{a_{\\bar{\\alpha}}}-e^{a_{\\alpha}}\\right)+D\\left(e^{a_{\\alpha}}v_{\\alpha}-e^{a_{\\bar{\\alpha}}}v_{\\bar{\\alpha}}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Note that swapping $\\alpha$ and $\\tilde{\\alpha}$ negates the sign. Hence, we have $\\mathrm{sign}(\\Delta_{x})=-\\,\\mathrm{sign}(\\Delta_{y})$ and hence $s_{x,x}\\le s_{x,y}\\le s_{y,y}$ or $s_{y,y}\\leq s_{x,y}\\leq s_{x,x}$ as desired. ", "page_idx": 34}, {"type": "text", "text": "Lemma 9 (Pure sequences are sorted according to cmp in Theorem 7). Fix all the same quantities as in Theorem 7. ", "page_idx": 34}, {"type": "text", "text": "Fix tokens $x,y\\in S$ . Let $n:=n_{\\mathrm{ctx}}-|F|$ be the number of non-fixed tokens. Fix sequences with n copies of $x$ and y respectively: $\\hbar x\\;t_{x,0}\\leq\\cdot\\cdot\\cdot\\leq t_{x,n_{\\mathrm{ctx}}-1}\\in\\mathbb{N}$ and $t_{y,0}\\,\\leq\\,\\cdot\\,\\cdot\\,\\leq\\,t_{y,n_{\\mathrm{ctx}}-1}\\in\\mathbb{N}$ compatible with $F$ and $S$ and given choices of permutations $\\sigma_{x},\\sigma_{y}:\\mathbb{N}\\rightarrow\\mathbb{N}$ of the $n_{\\mathrm{ctx}}$ positions fixing $F$ : $t_{x,i}=t_{F}\\bigl(\\sigma_{x}(i)\\bigr)$ for $\\sigma_{x}(i)\\in F$ ; $t_{y,i}=t_{F}\\big(\\sigma_{y}(i)\\big)$ for $\\sigma_{y}(i)\\in F$ ; $t_{x,i}=x$ for $\\sigma_{x}(i)\\not\\in F$ ; and $t_{y,i}=y$ for $\\sigma_{y}(i)\\notin F$ . ", "page_idx": 34}, {"type": "text", "text": "Then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{sign}((s_{\\sigma_{x},t_{x,0},\\ldots,t_{x,n_{\\mathrm{ctx}}-1}})-(s_{\\sigma_{y},t_{y,0},\\ldots,t_{y,n_{\\mathrm{ctx}}-1}}))=\\mathrm{cmp}(x,y)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{EQKE}(t_{-1},t_{i}):=t_{-1}\\mathbf{E}_{q}Q K^{T}\\bar{\\mathbf{E}}^{T}t_{i}^{T}/\\sqrt{d}}\\\\ &{\\mathrm{EQKP}(t_{-1},i):=t_{-1}\\mathbf{E}_{q}Q K^{T}\\hat{\\mathbf{P}}_{i}^{T}/\\sqrt{d}}\\\\ &{\\quad\\quad\\mathrm{EVOU}(t_{i}):=t_{i}\\bar{\\mathbf{E}}V O U}\\\\ &{\\quad\\quad\\quad\\mathrm{PVOU}(i):=\\hat{\\mathbf{P}}_{i}V O U}\\\\ &{\\quad\\quad\\quad\\quad\\ell^{\\mathrm{EU}}(t_{-1}):=t_{-1}\\mathbf{E}_{q}U}\\\\ &{\\quad\\quad\\quad\\Delta\\ell^{\\mathrm{EU}}t^{*}(t_{-1},\\underset{i}{\\operatorname*{max}}t_{i}):=\\ell^{\\mathrm{EU}}(t_{-1})_{t^{*}}-\\ell^{\\mathrm{EU}}(t_{-1})_{\\operatorname*{max}_{i}t_{i}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Figure 13: Recapitulation of some relevant definitions from Figure 12, parameterized by the arguments they actually depend on. ", "page_idx": 35}, {"type": "text", "text": "Proof. The proof goes by straightforward computation. ", "page_idx": 35}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\operatorname{sign}((s_{\\sigma_{x},t_{x,0},\\ldots,t_{x,n_{\\mathrm{ctx}}-1}})-(s_{\\sigma_{y},t_{y,0},\\ldots,t_{y,n_{\\mathrm{ctx}}-1}}))}\\\\ &{=\\operatorname{sign}\\left({\\frac{v_{x}e^{a_{x}}f+c}{e^{a_{x}}f+d}}-{\\frac{v_{y}e^{a_{y}}f+c}{e^{a_{y}}f+d}}\\right)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Multiply through by non-negative denominators ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\operatorname{sign}\\left(\\left(v_{x}e^{a_{x}}f+c\\right)\\left(e^{a_{y}}f+d\\right)-\\left(v_{y}e^{a_{y}}f+c\\right)\\left(e^{a_{x}}f+d\\right)\\right)}\\\\ &{=\\operatorname{sign}\\left(-c f e^{a_{x}}+c f e^{a_{y}}+d f v_{x}e^{a_{x}}-d f v_{y}e^{a_{y}}+f^{2}v_{x}e^{a_{x}+a_{y}}-f^{2}v_{y}e^{a_{x}+a_{y}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Use $f>0$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{=\\operatorname{sign}\\left(-c e^{a_{x}}+c e^{a_{y}}+d v_{x}e^{a_{x}}-d v_{y}e^{a_{y}}+f v_{x}e^{a_{x}+a_{y}}-f v_{y}e^{a_{x}+a_{y}}\\right)}\\\\ &{=\\operatorname{sign}\\left(c\\left(e^{a_{y}}-e^{a_{x}}\\right)+d\\left(v_{x}e^{a_{x}}-v_{y}e^{a_{y}}\\right)+f\\left(v_{x}e^{a_{x}+a_{y}}-v_{y}e^{a_{x}+a_{y}}\\right)\\right)}\\\\ &{=\\operatorname{cmp}(x,y)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Corollary 10. Define the relation $\\le_{\\mathrm{cmp}}$ by $x\\leq\\exp\\ y$ if and only $f\\,\\mathrm{cmp}(x,y)\\,\\in\\,\\{-1,0\\}$ . The relation $\\le_{\\mathrm{cmp}}$ is always transitive. ", "page_idx": 35}, {"type": "text", "text": "Proof. Note that by Lemma 9, cmp is comparing two sequence scores. Since $\\leq$ is transitive over the reals, the relation $\\le_{\\mathrm{cmp}}$ is also transitive. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "Finally, we combine the previous lemmas to complete our proof of Theorem 7: ", "page_idx": 35}, {"type": "text", "text": "Proof of Theorem 7. Extremal sequences with scores $s_{\\sigma,\\mathrm{{min}}}$ and $s_{\\sigma,\\mathrm{max}}$ are guaranteed to exist because there are only finitely many elements of $S$ and therefore only finitely many sequences. By Lemma 8, the extremal sequences must be pure (have $t_{i}=t_{j}$ whenever $\\bar{\\sigma(i)},\\sigma(\\bar{j)}\\,\\notin\\,\\bar{F})$ . By Lemma 9, the extremal sequences must have tokens that are extremal according to cmp. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "We now have all the tools necessary to prove the following theorem. We refer to Algorithm 3 and Algorithm 4 or the proof of Theorem 11 for a definition of the CUBIC algorithm. ", "page_idx": 35}, {"type": "text", "text": "Theorem 11. ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{t}\\sim U(0,1,\\dots,d_{\\mathrm{vocab}}-1)^{n_{\\mathrm{ctx}}}}\\left[\\operatorname{argmax}_{i}(\\mathcal{M}(\\mathbf{t}))_{i}=\\operatorname*{max}_{i}t_{i}\\right]\\ge\\mathbf{CUBIC}\\big(d_{\\mathrm{vocab}},n_{\\mathrm{ctx}},\\mathcal{M}\\big)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Before we give the proof of this theorem, we introduce some helpful notation. ", "page_idx": 35}, {"type": "text", "text": "Definition 12. Fix an element $(r_{m},r_{q},c)\\in\\{0,...,d_{\\mathrm{vocab}}\\}^{2}\\times\\{0,...3\\}$ such that $r_{m}\\geq r_{q}$ . We define $X_{(r_{m},r_{q},c)}$ to be the set of tokens t such that ", "page_idx": 35}, {"type": "text", "text": "1. The max token $t_{\\mathrm{max}}$ is equal to $r_{m}$ ", "page_idx": 35}, {"type": "text", "text": "Algorithm 2 Counting Correct Sequences in Cubic Time: Preliminaries ", "page_idx": 36}, {"type": "text", "text": "1: function CORRECTNESS( $\\mathcal{M}$ , input-sequence)   \n2: return MODEL-BEHAVIOR( $\\mathcal{M}$ , input-sequence) $==$ MAX(input-sequence)   \n3: end function   \n4: function MODEL-BEHAVIOR( $\\mathcal{M}$ , input-sequence)   \nRequire: input-sequence is a tensor of shape $(n_{\\mathrm{ctx}},)$ with values in $\\mathbb{N}_{<d_{\\mathrm{vocab}}}$   \n5: $t_{\\mathrm{max}}\\leftarrow\\mathbf{MAX}$ (input-sequence) $>t_{\\mathrm{max}}\\gets$ max-token   \n6: $\\mathbf{t}\\gets$ input-sequence   \n7: skip- $\\begin{array}{r}{\\cdot\\mathrm{score}_{t^{*}}\\,\\stackrel{.}{\\leftarrow}\\Delta\\ell^{\\mathrm{EU}}{}_{t^{*}}(t_{n_{\\mathrm{ctx}}-1},t_{\\mathrm{max}})}\\end{array}$   \n8: attn-weights-unscale $\\mathbf{1}_{i}\\gets\\mathrm{EQKE}(t_{n_{\\mathrm{ctx}}-1},t_{i})+\\mathrm{EQKP}(t_{n_{\\mathrm{ctx}}-1},i)$   \n9: attn-weights $\\leftarrow$ SOFTMAX(attn-weights-unscaled/ ${\\sqrt{d}}_{\\alpha}$ )   \n10: $\\boldsymbol{v}_{t}\\gets\\mathrm{E}\\bar{\\mathrm{V}}\\mathrm{OU}(t)$   \n11: $w_{i}\\gets\\mathrm{PVOU}(i)$   \n12: \u2206vt,t\u2217\u2190vt,t\u2217\u2212vt,tmax   \n13: $\\Delta w_{i,t^{*}}\\gets w_{i,t^{*}}-w_{i,t_{\\operatorname*{max}}}$   \n14: return $\\begin{array}{r}{\\operatorname*{max}_{t^{*}\\neq t_{\\mathrm{max}}}(\\mathrm{skip-score}_{t^{*}}+\\sum_{i=0}^{n_{\\mathrm{ctx}}-1}(\\Delta v_{i,t^{*}}+\\Delta w_{i,t^{*}})\\cdot\\mathrm{attn-weights}_{i})}\\end{array}$   \n15: end function   \n16: function CORRECTNESS-PESSIMIZING-OVER-POSITION-SLOW( $\\mathcal{M}$ , input-sequence)   \n17: $\\mathbf{t}\\gets$ input-sequence   \n18: return ALL(CORRECTNESS $(\\mathcal{M},\\mathfrak{p e r m}+[t_{-1}])$ for all perm $\\in$ PERMUTATIONS $(t_{0:-1})$ )   \n19: end function ", "page_idx": 36}, {"type": "text", "text": "2. The query token $t_{\\mathrm{query}}$ is equal to $r_{q},$ ", "page_idx": 36}, {"type": "text", "text": "3. The cardinality of tokens that are not at the query position and not equal to $t_{\\mathrm{max}}$ is equal to c. ", "page_idx": 36}, {"type": "text", "text": "For clarity, we list all the possible cases. We always take $t_{\\mathrm{query}}\\leq t_{\\mathrm{max}}$ and let $S_{3}$ act on sequences by permuting the first three factors (i.e. keeping the query position fixed). ", "page_idx": 36}, {"type": "text", "text": "1. If $c=0$ , then $X_{(t_{\\mathrm{max}},t_{\\mathrm{query}},0)}=\\{[t_{\\mathrm{max}},t_{\\mathrm{max}},t_{\\mathrm{max}},t_{\\mathrm{query}}]\\},$   \n2. If $c=1$ , then $X_{(t_{\\mathrm{max}},t_{\\mathrm{query}},1)}=S_{3}.\\{[t_{1},t_{\\mathrm{max}},t_{\\mathrm{max}},t_{\\mathrm{query}}]\\ |\\ t_{1}<t_{\\mathrm{max}}$   \n3. If $c=2$ , then $X_{(t_{\\mathrm{max}},t_{\\mathrm{query}},2)}=S_{3}.\\{[t_{1},t_{2},t_{\\mathrm{max}},t_{\\mathrm{query}}]\\ |\\ t_{i}<t_{\\mathrm{max}}\\}$ ,   \n4. If $c=3$ , then $X_{(t_{\\operatorname*{max}},t_{\\operatorname*{max}},3)}=S_{3}.\\{[t_{1},t_{2},t_{3},t_{\\operatorname*{max}}]\\mid t_{i}<t_{\\operatorname*{max}}\\}.$ ", "page_idx": 36}, {"type": "text", "text": "Definition 13. Let $\\mathbf{t}\\in X$ be a sequence. We say $t$ is pure, if it has at most three distinct tokens: the max token $t_{\\mathrm{max}}$ , the query token $t_{\\mathrm{query}}$ , and optionally a third token $t^{*}<t_{\\mathrm{max}}$ . ", "page_idx": 36}, {"type": "text", "text": "We denote by $X^{p u r e}$ the subset of pure tokens. For any subset $Y\\subset X$ , we set $Y^{p u r e}:=Y\\cap X^{p u r e}$ . ", "page_idx": 36}, {"type": "text", "text": "We now come to the proof of Theorem 11. We will show how to use the previous theorems to get explicit bounds and explain how CUBIC $(d_{\\mathrm{vocab}},n_{\\mathrm{ctx}},\\mathcal{M})$ computes these bounds. ", "page_idx": 36}, {"type": "text", "text": "Proof of Theorem $_{l l}$ . First of all, we note that the algorithm $\\mathrm{CUBIC}\\;=\\;\\mathrm{CUBIC}\\big(d_{\\mathrm{vocab}},n_{\\mathrm{ctx}},\\mathcal{M}\\big)$ yields a lower bound for the accuracy on the set $X_{(t_{\\mathrm{max}},t_{\\mathrm{query}},c)}$ . We can therefore compute the bound soun $\\begin{array}{r}{X=\\prod_{(t_{\\mathrm{max}},t_{\\mathrm{query}},c)}X_{(t_{\\mathrm{max}},t_{\\mathrm{query}},c)}}\\end{array}$ by computing it for each such choice $(t_{\\mathrm{max}},t_{\\mathrm{query}},c)$ and ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{t}\\sim U(0,1,\\dots,d_{\\mathrm{vocab}}-1)^{n_{\\mathrm{ctx}}}}\\left[\\arg\\!\\operatorname*{max}_{i}(\\mathcal{M}(\\mathbf{t}))_{i}=\\operatorname*{max}_{i}t_{i}\\right]\\ge\\sum_{(t_{\\mathrm{max}},t_{\\mathrm{query},c})}\\mathrm{CUBIC}\\big(X_{(t_{\\mathrm{max}},t_{\\mathrm{query},c})}\\big).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "So from now on we will fix one such subset X(tmax,tquery,c). ", "page_idx": 36}, {"type": "text", "text": "We begin by defining a map ", "page_idx": 36}, {"type": "equation", "text": "$$\nf:X_{(t_{\\operatorname*{max}},t_{\\mathrm{query}},c)}\\rightarrow\\{0,...,d_{\\mathrm{vocab}}\\}^{c}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which sends a sequence to the subsequence of elements which are not at the query position and not equal to $t_{\\mathrm{max}}$ . Then Theorem 7 can be restated as follows22: ", "page_idx": 36}, {"type": "text", "text": "22In fact, the theorem yields a stronger result, but we will only need the following formulation. ", "page_idx": 36}, {"type": "text", "text": "Algorithm 3 Counting Correct Sequences in Cubic Time, Part I. Lines are annotated with comments indicating the parameters for a cache to avoid duplicate computations. ", "page_idx": 37}, {"type": "text", "text": "1: function MODEL-BEHAVIOR-RELAXED( $\\mathcal{M}$ , query-tok, max-tok, non-max-tok, n-copiesnonmax) 2: $t_{\\mathrm{query}}\\leftarrow$ query-tok, $t_{\\operatorname*{max}}\\gets$ max-tok, $t^{\\prime}\\gets$ non-max-tok, $c\\gets\\mathfrak{n}$ -copies-nonmax   \nRequire: $0\\leq t_{\\mathrm{query}}\\leq t_{\\mathrm{max}}<d_{\\mathrm{vocab}}$ , $0\\leq t^{\\prime}\\leq t_{\\mathrm{max}}<d_{\\mathrm{vocab}}$ , $0\\leq c<n_{\\mathrm{ctx}}$   \nRequire: if n-copies-nonmax $=0$ then non-max-tok $=$ max-tok   \nRequire: if query-tok $\\neq$ max-tok then n-copies-nonmax $<n_{\\mathrm{ctx}}-1$   \nEnsure: return $\\geq$ MODEL-BEHAVIOR $(\\mathcal{M},\\mathbf{t})$ for all t with specified $t_{\\mathrm{query}}$ , $c$ copies of $t^{\\prime}$ in nonquery positions, and the remainder of the tokens equal to $t_{\\mathrm{max}}$   \n3: skip-s $\\mathrm{core}_{t^{*}}\\gets\\Delta\\ell^{\\mathrm{EU}}{}_{t^{*}}(t_{\\mathrm{query}},t_{\\mathrm{max}})$ \u25b7Cache by $t_{\\mathrm{max}}$ , $\\ensuremath{t_{\\mathrm{query}}}$ , $t^{*}$   \n4: $w_{i}\\gets\\mathrm{PVOU}(i)$ for 0 \u2264i < nctx $\\triangleright$ Cache by $i$   \n5: $\\Delta w_{\\mathrm{max},t^{*}}\\gets\\operatorname*{max}_{0\\leq i<n_{\\mathrm{ctx}}}(w_{i,t^{*}}-w_{i,t_{\\mathrm{max}}})$ $\\triangleright$ Cache by $t_{\\mathrm{max}}$ , $t^{*}$   \n6: vt \u2190EVOU(t), \u2206vt,t\u2217\u2190vt,t\u2217\u2212vt,tmax for t \u2208{tquery, tmax, t\u2032} \u25b7Cache by $t_{\\mathrm{max}}$ , $t$ , $t^{*}$   \n7: $a_{t}\\gets\\mathrm{EQKE}(t_{\\mathrm{query}},t)/\\sqrt{d}\\qquad\\mathbf{for}\\;t\\in\\{t_{\\mathrm{query}},t_{\\mathrm{max}},t^{\\prime}\\}$ \u25b7Cache by $t_{\\mathrm{query}}$ , $t$   \n8: bnctx\u22121 \u2190EQKP(tquery, nctx \u22121)/ d\u221a \u25b7Cache by tquery   \n9: $b_{0,:-1}\\gets\\mathrm{soRT}(\\mathrm{EQKP}(t_{\\mathrm{query}},:-1))/\\sqrt{d}$ \u25b7Cache by $t_{\\mathrm{query}}$ , $i$   \n10: $b_{1,:-1}\\gets\\textsc{R E V E R S E}(b_{0,:-1})$   \n11: attn-weights-unscaled: $,n_{\\mathrm{ctx}}\\!-\\!1\\leftarrow a_{t_{\\mathrm{query}}}+b_{n_{\\mathrm{ctx}}-1}$ \u25b7Cache by tquery   \n12: attn-weights-unscaled $_{0,i}\\gets a_{t_{\\mathrm{max}}}+b_{0,i}$ for $0\\leq i<n_{\\mathrm{ctx}}-c-1$ \u25b7Cache by $t_{\\mathrm{max}}$ , c, i, tquery   \n13: attn-weights-unscaled $_1_{,i}\\leftarrow a_{t_{\\operatorname*{max}}}+b_{1,i}$ for $0\\leq i<n_{\\mathrm{ctx}}-c-1$ \u25b7Cache by $t_{\\mathrm{max}}$ , c, i, query   \n14: attn-weights-unscaled $\\mathbf{\\lvert0,}i\\gets a_{t^{\\prime}}+b_{0,i}$ for $n_{\\mathrm{ctx}}-c-1\\leq i<n_{\\mathrm{ctx}}-1\\triangleright\\mathrm{C}$ ache by $t^{\\prime},c,i$ , tquery   \n15: attn-weights-unscaled $_{1,i}\\gets a_{t^{\\prime}}+b_{1,i}$ for $n_{\\mathrm{ctx}}-c-1\\leq i<n_{\\mathrm{ctx}}-1\\triangleright($ Cache by $t^{\\prime},c,i$ , query   \n16: attn-weights $_0\\gets$ SOFTMAX(attn-weights-unscaled ) $\\triangleright$ Cache by $t_{\\mathrm{max}}$ , $t^{\\prime}$ , c, i, tquery   \n17: attn-weights $_1\\leftarrow$ SOFTMAX(attn-weights-unscaled1) \u25b7Cache by tmax, t\u2032, c, i, tquery   \n18: if $c=0$ then $\\triangleright$ In this case, attn-weights $_{0,i}=$ attn-weights $^{1,i}$ , so we drop the first subscript   \n19: return maxt\u2217\u0338=tmax(skip-scoret\u2217 + \u2206wmax,t\u2217 + $\\Delta v_{t_{-1},t^{*}}$ attn-weights\u22121 $^+$ $\\Delta v_{t_{\\operatorname*{max}},t^{*}}\\sum_{i=0}^{n_{\\mathrm{ctx}}-2}$ attn-weights)   \n20: else   \n21: $\\begin{array}{r l}&{\\Delta v_{i,t^{*}}\\gets\\Delta v_{t_{\\operatorname*{max}},t^{*}}\\quad\\mathbf{for}\\ 0\\leq i<n_{\\mathrm{ctx}}-c-1}\\\\ &{\\Delta v_{i,t^{*}}\\gets\\Delta v_{t^{\\prime},t^{*}}\\quad\\mathbf{for}\\ n_{\\mathrm{ctx}}-c-1\\leq i<n_{\\mathrm{ctx}}-1}\\\\ &{\\Delta v_{n_{\\mathrm{ctx}}-1,t^{*}}\\gets\\Delta v_{t_{\\operatorname*{max}},\\tau,\\mathrm{act}-1}}\\\\ &{\\mathbf{return}\\mathbf{max}_{t^{*}\\neq t_{\\operatorname*{max}},\\mathrm{skip}\\setminus\\mathrm{score}_{t^{*}}}\\!+\\!\\operatorname*{max}\\left\\{\\sum_{i=0}^{n_{\\mathrm{ctx}}-1}\\!\\operatorname*{max}_{t^{*}\\neq t_{\\operatorname*{max}}}(\\Delta w_{\\operatorname*{max},t^{*}}+\\Delta v_{i,t^{*}})\\cdot\\mathrm{att}\\!\\!\\!0\\!\\!\\cdot\\!\\!\\mathbf{x}\\!\\!\\!\\in\\!\\mathrm{igh}\\!\\!\\!\\backslash\\!\\!\\mathbf{s}_{0,t^{*}}\\!\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}$   \n22:   \n2243:: i i   \n25: end if   \n26: end function ", "page_idx": 37}, {"type": "text", "text": "", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "27: function RELAXED-CORRECTNESS-PESSIMIZING-OVER-POSITION $\\mathsf{I}(\\mathcal{M},t_{\\mathrm{query}},t_{\\mathrm{max}},t^{\\prime},c)$   \n28: $\\triangleright$ runs the model on a relaxed variant of input sequences compatible with the arguments ", "page_idx": 37}, {"type": "text", "text": "Ensure: return is False if CORRECTNESS-PESSIMIZING-OVER-POSITION-SLOW $(\\mathcal{M},\\mathbf{t})$ is False for any t with specified $t_{\\mathrm{query}}$ , $c$ copies of $t^{\\prime}$ in non-query positions, and the remainder of the tokens equal to tmax ", "page_idx": 37}, {"type": "text", "text": "29: return MODEL-BEHAVIOR-RELAX $\\Im(\\mathcal{M},t_{\\mathrm{query}},t_{\\mathrm{max}},t^{\\prime},c)<0$   \n30: end function ", "page_idx": 37}, {"type": "table", "img_path": "2zWbzx50mH/tmp/6ba95f8c49e9e9b480aafb06f32746b09bb068a1a032674d5f94640b1b52f45b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 38}, {"type": "text", "text": "Let $S\\subset\\{0,...,d_{\\mathrm{vocab}}\\}$ . Then full accuracy $f^{-1}(S^{c})^{p u r e}:=X_{(t_{\\mathrm{max}},t_{\\mathrm{query}},c)}^{p u r e}\\cap f^{-1}(S^{c})$ X(tmax,tquery,c) \u2229f \u22121(Sc), implies full accuracy on $f^{-1}(S^{c})$ . ", "page_idx": 38}, {"type": "text", "text": "Now instead of computing the output of the model for every element $f^{-1}(S^{c})^{p u r e}$ , we use Theorem 4 (combined with Theorem 6) to run a relaxed version of this. In particular, we may assume that the pure sequence is contiguous on equal tokens. Here contiguous on equal tokens means that for the positional part of the attention (i.e. the EQKP part), we have either $b_{t_{\\operatorname*{max}}}<\\{b_{i},b_{j}\\}$ or $b_{t_{\\operatorname*{max}}}>\\{b_{i},b_{j}\\}$ , where $i,j\\in\\{0,...,n_{\\mathrm{ctx}}-1\\}$ are indices of tokens not equal to $t_{\\mathrm{max}}$ . ", "page_idx": 38}, {"type": "text", "text": "For the algorithm CUBIC $(d_{\\mathrm{vocab}},n_{\\mathrm{ctx}},\\mathcal{M})$ we fix a $t^{*}\\in\\{0,...,t_{\\operatorname*{max}}{-1}\\}$ (unless $c=0$ , in which case there is no such choice). We then run the relaxed accuracy computation $\\mathbf{RCPOP}(t_{\\mathrm{query}},t_{\\mathrm{max}},t^{\\prime},c)$ as described in Theorem 6. If $\\mathrm{RCPOP}(t_{\\mathrm{query}},t_{\\mathrm{max}},t^{\\prime},c)<0$ , we add $t^{\\prime}$ to $S$ . If we do, we add $t^{*}$ to $S$ . Therefore by construction of $S$ we know that we get full accuracy on $f^{-1}(S^{c})^{p u r e}$ and therefore we get full accuracy on $f^{-1}(S^{c})$ . ", "page_idx": 38}, {"type": "text", "text": "Now we count the cardinality of $f^{-1}(S^{c})$ and add it to the count of correct sequences. ", "page_idx": 38}, {"type": "text", "text": "Theorem 12. The running time of Algorithm 3, after using caching to avoid duplicate computations, is $\\mathcal{O}(d_{\\mathrm{vocab}}{^3{n_{\\mathrm{ctx}}}}^{2})$ . ", "page_idx": 38}, {"type": "text", "text": "Proof. The nested loops in CUBIC execute the innermost body ${\\mathcal{O}}(d_{\\mathrm{vocab}}{}^{2}n_{\\mathrm{ctx}})$ times, and the summation on Line 13 costs ${\\mathcal{O}}(n_{\\mathrm{ctx}})$ per iteration. What remains is to show that the call to RELAXED-CORRECTNESS-PESSIMIZING-OVER-POSITION $(\\mathcal{M},t_{\\mathrm{query}},t_{\\mathrm{max}},t^{\\prime},c)$ costs ${\\mathcal{O}}(n_{\\mathrm{ctx}})$ when $c\\neq0$ and at most $\\mathcal{O}(d_{\\mathrm{vocab}}n_{\\mathrm{ctx}})$ when $c=0$ and $t^{\\prime}=t_{\\mathrm{max}}$ . ", "page_idx": 38}, {"type": "text", "text": "The matrix multiplications in EQKE, EQKP, EVOU, PVOU, and $\\ell^{\\mathrm{EU}}$ can be cached upfront, costing $\\mathcal{O}(\\operatorname*{max}(\\dot{d}_{\\mathrm{vocab}},d_{\\mathrm{model}},\\dot{n_{\\mathrm{ctx}}})^{2}d_{\\mathrm{model}})\\leq\\mathcal{O}(d_{\\mathrm{vocab}}{}^{3})$ since we assume $d_{\\mathrm{vocab}}>d_{\\mathrm{model}}$ and $d_{\\mathrm{vocab}}>n_{\\mathrm{ctx}}$ . ", "page_idx": 38}, {"type": "text", "text": "The sorting on Line 9 can also be cached upfront (per $\\ensuremath{t_{\\mathrm{query}}}.$ ), costing $O(d_{\\mathrm{vocab}}n_{\\mathrm{ctx}}\\log n_{\\mathrm{ctx}})$ . ", "page_idx": 38}, {"type": "text", "text": "Note that each variable assignment in RELAXED-CORRECTNESS-PESSIMIZING-OVER-POSITION can be cached into a table parameterized over at most three variables which range over $d_{\\mathrm{vocab}}$ and over at most two variables that range over $n_{\\mathrm{ctx}}$ . ", "page_idx": 38}, {"type": "text", "text": "What remains is the return statements. ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "When $\\mathrm{~\\textit~{~c~}~}=\\mathrm{~\\textit~{~0~}~}$ , we have on Line 19: return $\\operatorname*{max}_{t^{*}\\neq t_{\\operatorname*{max}}}(\\mathrm{skip}\\mathrm{-}\\mathrm{score}_{t^{*}}\\ +\\ \\Delta w_{\\operatorname*{max},t^{*}}\\ +$ $\\Delta v_{t_{-1},t^{*}}$ attn-weights $_{-1}+\\Delta v_{t_{\\operatorname*{max}},t^{*}}\\sum_{i=0}^{n_{\\mathrm{ctx}}-2}$ attn-weightsi). This is $\\mathcal{O}(d_{\\mathrm{vocab}}n_{\\mathrm{ctx}})$ as desired. ", "page_idx": 38}, {"type": "text", "text": "When $c\\neq0$ , we have on Line 24: ", "page_idx": 39}, {"type": "text", "text": "return $\\operatorname*{max}_{t^{*}\\neq t_{\\mathrm{max}}}\\mathrm{skip-score}_{t^{*}}+\\operatorname*{max}\\left\\{\\sum_{i=0}^{n_{\\mathrm{ctx}}-1}\\operatorname*{max}_{t^{*}\\neq t_{\\mathrm{max}}}(\\Delta w_{\\mathrm{max},t^{*}}+\\Delta v_{i,t^{*}})\\right.+\\left.\\Pi_{i=1}\\right\\}$ \u00b7 attn-weights $^{0,i}$ ", "page_idx": 39}, {"type": "text", "text": "We can cache $\\operatorname*{max}_{t^{*}\\neq t_{\\operatorname*{max}}}$ $\\mathrm{skip-score}_{t},$ per $t_{\\mathrm{max}}$ and $t_{\\mathrm{query}}$ , costing ${\\mathcal O}(d_{\\mathrm{vocab}}{}^{3}n_{\\mathrm{ctx}})$ . We can cache $\\operatorname*{max}_{t^{*}\\neq t_{\\operatorname*{max}}}(\\Delta w_{\\operatorname*{max},t^{*}}+\\Delta v_{i,t^{*}})$ per $t_{\\mathrm{max}}$ and $t^{\\prime}$ costing $\\mathcal{O}(d_{\\mathrm{vocab}}{}^{3})$ , since each $\\Delta{v}_{i,t^{*}}$ will be $\\Delta{v}_{t,t^{*}}$ for some $t\\in\\{t_{\\mathrm{query}},t_{\\mathrm{max}},t^{\\prime}\\}$ . Finally, we can compute the summation in cost $O(n_{\\mathrm{ctx}})$ per loop iteration, as required. \u53e3 ", "page_idx": 39}, {"type": "text", "text": "F Quadratic counting for a sub-cubic proof ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In this section we fill in the details lacking from Section 4.3. ", "page_idx": 39}, {"type": "text", "text": "In Appendix E we proved an intricate version of convexity of softmax where, modulo pessimizing in unrealistic ways over the attention paid to positions for the computation done on positional encodings, all extremal relaxed sequences correspond to actual sequences. ", "page_idx": 39}, {"type": "text", "text": "When we only get a budget of ${\\mathcal{O}}(d_{\\mathrm{vocab}}{}^{2}n_{\\mathrm{ctx}})$ extremal relaxed cases to consider, though, we must pessimize more, which gives us a simpler version of the convexity theorem and proof. Notably, when we restrict our sequences to have only two tokens (the max token $t_{\\mathrm{max}}$ and the non-max token $t^{\\prime}$ ), most of the theorems from Appendix E.3 get significantly simpler. ", "page_idx": 39}, {"type": "text", "text": "Additionally, we must pessimize separately over the token value $(v)$ and token attention $(b)$ computations in order to allow efficient computation (Theorem 15). ", "page_idx": 39}, {"type": "text", "text": "F.1 Proof of baseline sub-cubic result ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "For this subsection, all theorems are parameterized over the following quantities. ", "page_idx": 39}, {"type": "text", "text": "Definition 14 (Common theorem parameters). Fix a total number of tokens $n_{\\mathrm{ctx}}$ . Fix a token value function ( $\\grave{a}$ la a row-difference in EVOU) $v:\\mathbb{N}_{<d_{\\mathrm{vocab}}}\\rightarrow\\mathbb{R}$ and a token attention function (\u00e0 la EQKE for a fixed query token) $a:\\mathbb{N}_{<d_{\\mathrm{vocab}}}\\rightarrow\\mathbb{R}$ . Fix a position value function (\u00e0 la a row-difference in PVOU) $w:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow\\mathbb{R}$ and a position attention function $\\grave{a}$ la EQKP for a fixed query token) $b:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow\\mathbb{R}$ . ", "page_idx": 39}, {"type": "text", "text": "In practice, as in Appendix E.3, we\u2019ll take, for fixed query token $t_{\\mathrm{query}}$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{v_{t}=\\mathrm{EVOU}_{t,t^{*}}-\\mathrm{EVOU}_{t,t_{\\mathrm{max}}}\\qquad\\qquad}&{a_{t}=\\mathrm{EQKE}_{t_{\\mathrm{query}},t}/\\sqrt{d}}\\\\ &{w_{i}=\\mathrm{PVOU}_{i,t^{*}}-\\mathrm{PVOU}_{i,t_{\\mathrm{max}}}\\qquad\\qquad}&{b_{i}=\\mathrm{EQKP}_{t_{\\mathrm{query}},i}/\\sqrt{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Note that unlike in Appendix E.3, we pessimize independently over the query token and the non-max token, so the \u201cfixed\u201d query token may not in fact appear in any key-side position in the relaxed sequence we consider. ", "page_idx": 39}, {"type": "text", "text": "Definition 15 (of a sequence via mapping from positions). We can define a sequence of tokens via mapping from positions by specifying a subset of valid tokens $S\\subseteq\\mathbb{N}_{<d_{\\mathrm{vocab}}}$ paired with a function $T:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow S$ specifying which token is in each position. ", "page_idx": 39}, {"type": "text", "text": "Definition 16 (sequence score). Given a subset of valid tokens $S\\ \\subseteq\\ \\mathbb{N}_{<d_{\\mathrm{vocab}}}$ and a function $T:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow S$ specifying which token is in each position, define the sequence score ", "page_idx": 39}, {"type": "equation", "text": "$$\ns_{T}:={\\sum_{0\\le i<n_{\\mathrm{ctx}}}}v_{T(i)}e^{a_{T(i)}+b_{i}}\\Biggl/\\sum_{0\\le i<n_{\\mathrm{ctx}}}e^{a_{T(i)}+b_{i}}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Definition 17 (swapped mapping). Given a subset of valid tokens $S\\subseteq\\mathbb{N}_{<d_{\\mathrm{vocab}}}$ and a function $T:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow S$ specifying which token is in each position and two indices $0\\leq i,j<n_{\\mathrm{ctx}},$ , define the swapped mapping $T_{i\\leftrightarrow j}$ be the function that is $T$ except swapping $i$ and $j$ : ", "page_idx": 39}, {"type": "equation", "text": "$$\nT_{i\\leftrightarrow j}(k)=\\left\\{{\\begin{array}{l l}{T(i)}&{i f k=j}\\\\ {T(j)}&{i f k=i}\\\\ {T(k)}&{o t h e r w i s e}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Lemma 13 (Characterization of swapping tokens in a two-token sequence). Fix two tokens $t_{0}<$ $t_{1}\\in\\mathbb{N}$ and $a$ function $T:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow\\bar{\\{t}_{0},{\\bar{t}_{1}}\\}$ specifying which token is in each position. ", "page_idx": 40}, {"type": "text", "text": "Define $\\Delta_{T,i\\leftrightarrow j}$ to be the difference in sequence scores when you swap $i$ and $j$ : ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\Delta_{T,i\\leftrightarrow j}:=s_{T_{i\\leftrightarrow j}}-s_{T}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Then ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathrm{sign}\\left(\\Delta_{T,i\\leftrightarrow j}\\right)=-\\,\\mathrm{sign}\\left(b_{i}-b_{j}\\right)\\mathrm{sign}\\left(v_{T(i)}-v_{T(j)}\\right)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. Lemma 3 gives us the result directly when $\\boldsymbol{a}_{T(i)}=\\boldsymbol{a}_{T(j)}$ . Otherwise, we get ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathrm{sign}\\left(\\Delta_{T,i\\leftrightarrow j}\\right)=\\mathrm{sign}\\left(a_{T(i)}-a_{T(j)}\\right)\\mathrm{sign}\\left(b_{i}-b_{j}\\right)\\mathrm{sign}\\left(s_{T}-\\frac{v_{T(i)}e^{a_{T(i)}}-v_{T(j)}e^{a_{T(j)}}}{e^{a_{T(i)}}-e^{a_{T(j)}}}\\right)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Hence all that remains is to show that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathrm{sign}\\left(s_{T}\\left(e^{a_{T\\left(i\\right)}}-e^{a_{T\\left(j\\right)}}\\right)-v_{T\\left(i\\right)}e^{a_{T\\left(i\\right)}}+v_{T\\left(j\\right)}e^{a_{T\\left(j\\right)}}\\right)=-\\mathrm{sign}\\left(v_{T\\left(i\\right)}-v_{T\\left(j\\right)}\\right)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Define $\\bar{v}\\,:=\\,\\textstyle{\\frac{1}{2}}\\bigl(v_{T(i)}+v_{T(j)}\\bigr)$ and define $\\Delta\\boldsymbol{v}\\,:=\\,\\frac{1}{2}\\bigl(\\boldsymbol{v}_{T(i)}-\\boldsymbol{v}_{T(j)}\\bigr)$ so that $v_{T(i)}\\,=\\,\\bar{v}\\,+\\,\\Delta v$ and $v_{T(j)}=\\bar{v}-\\Delta v$ . Assume WLOG that $T(i)=0$ and $T(j)=1$ so that $v_{T(p)}=\\bar{v}+(-1)^{T(p)}\\Delta v$ for all $p$ . ", "page_idx": 40}, {"type": "text", "text": "Then we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{lim}\\{\\operatorname*{lim}\\{\\operatorname*{lim}\\{\\operatorname*{lim}\\{u^{*}\\}(x^{*}(1-\\theta^{n+1})-x(\\tau^{n+1}-\\tau^{*}))\\}(x^{*}(1-\\eta^{*})(x+\\tau^{*}))\\}}\\\\ &{=\\operatorname*{lim}\\{\\operatorname*{lim}\\{\\operatorname*{lim}\\{0\\},\\quad\\forall x\\in[0,\\pi],\\quad\\forall x\\in[0,\\pi]\\}\\}}\\\\ &{=\\operatorname*{lim}\\left(\\frac{\\operatorname*{lim}\\{x\\}\\operatorname*{lim}\\{0\\}}{\\operatorname*{min}\\{0\\}}{\\operatorname*{min}\\{0\\}}{\\operatorname*{min}\\{0\\}}\\right)}\\\\ &{=\\operatorname*{lim}\\left(\\frac{\\operatorname*{lim}\\{x\\}\\operatorname*{lim}\\{0\\}}{\\operatorname*{min}\\{0\\}}{\\operatorname*{min}\\{0\\}}{\\operatorname*{min}\\{0\\}}\\right)}\\\\ &{=\\operatorname*{lim}\\{\\operatorname*{lim}\\{\\operatorname*{lim}\\{\\operatorname*{lim}\\{0\\}}}}\\\\ &{=\\operatorname*{lim}\\left(\\frac{\\operatorname*{lim}\\{x\\}\\operatorname*{lim}\\{0\\}}{\\operatorname*{min}\\{0\\}}{\\operatorname*{min}\\{0\\}}{\\operatorname*{min}\\{0\\}}\\right)}\\\\ &{=\\operatorname*{lim}\\{\\operatorname*{lim}\\{\\operatorname*{lim}\\{0\\}}}\\\\ &{=\\operatorname*{lim}\\left(\\frac{\\operatorname*{lim}\\{0\\}}{\\operatorname*{min}\\{0\\}}{\\operatorname*{min}\\{0\\}}{\\operatorname*{min}\\frac{\\forall x\\in[0,\\pi]\\}{\\operatorname*{min}\\{0\\}}}\\left({\\operatorname*{min}\\ }-{\\operatorname*{appe}\\ }\\right)-\\frac{\\alpha(\\ell^{n+1})-x(\\tau^{*})}{\\operatorname*{min}{\\operatorname*{min}}{\\operatorname*{min}}}-{\\operatorname*{apper}\\ }\\right)-\\Delta x(\\ell^{n})}\\\\ &{=\\operatorname*{lim}\\{\\operatorname*{lim}\\{0\\}}\\left(\\frac{\\operatorname*{lim}\\{0\\}}{\\operatorname*{min}\\{0\\}}{\\operatorname*{min}\\{0\\}}{\\operatorname*{min}\\frac{\\forall x\\in[0,\\pi]\\}{\\operatorname*{min}\\{0\\}}}\\left({\\operatorname*{min}\\{0\\}}-{\\operatorname*{apper}\\ }\\right)-\\Delta x(\\ell^{n})-\\alpha^{n}\\operatorname*{mart}\\right)}\\\\ &{=\\operatorname*{lim}\\{\\operatorname*\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Define ", "page_idx": 40}, {"type": "equation", "text": "$$\nP_{i}:=\\sum_{\\stackrel{0\\leq p<n_{\\mathrm{ctx}}}{T(p)=T(i)}}e^{b_{p}}\\qquad\\qquad\\qquad\\qquad\\quad P_{j}:=\\sum_{\\stackrel{0\\leq p<n_{\\mathrm{ctx}}}{T(p)=T(j)}}e^{b_{p}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "so that we get ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathrm{sign}\\left(s_{T}\\left(e^{a_{T(i)}}-e^{a_{T(j)}}\\right)-v_{T(i)}e^{a_{T(i)}}+v_{T(j)}e^{a_{T(j)}}\\right)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\n=\\mathrm{sign}(v_{T(i)}-v_{T(j)})\\,\\mathrm{sign}\\left(\\frac{e^{a_{T(i)}}P_{i}-e^{a_{T(j)}}P_{j}}{e^{a_{T(i)}}P_{i}+e^{a_{T(j)}}P_{j}}\\left(e^{a_{T(i)}}-e^{a_{T(j)}}\\right)-e^{a_{T(i)}}-e^{a_{T(j)}}\\right)\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Multiply through by the positive denominator and expand out so that we get ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\;}&{=\\mathrm{sign}(v_{T(i)}-v_{T(j)})\\,\\mathrm{sign}\\left(-2e^{a_{T(i)}+a_{T(j)}}P_{i}-2e^{a_{T(i)}+a_{T(j)}}P_{j}\\right)}\\\\ {\\;}&{=-\\,\\mathrm{sign}(v_{T(i)}-v_{T(j)})\\,\\mathrm{sign}\\left(e^{a_{T(i)}+a_{T(j)}}P_{i}+e^{a_{T(i)}+a_{T(j)}}P_{j}\\right)}\\\\ {\\;}&{=-\\,\\mathrm{sign}(v_{T(i)}-v_{T(j)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Theorem 14 (Pessimization over sequence ordering for two-token sequences is simple). Let $\\sigma_{s}$ : $\\mathbb{N}\\to\\mathbb{N}$ denote a permutation of the $n_{\\mathrm{ctx}}$ positions that sorts them according to $b$ : for $0\\leq i,j<n_{\\mathrm{ctx}},$ , $b_{i}\\leq b_{j}$ whenever $\\sigma_{s}(i)<\\sigma_{s}(j)$ . Fix two tokens $t_{0}<t_{1}\\in\\mathbb{N}$ . ", "page_idx": 41}, {"type": "text", "text": "Let $n_{t_{0}}$ be the number of $p\\in[0,n_{\\mathrm{ctx}})$ with $T(p)=t_{0}$ and let $n_{1}$ be the number of $p\\in[0,n_{\\mathrm{ctx}})$ with $T(p)=t_{t_{1}}$ . Note that $n_{t_{0}}+n_{t_{1}}=n_{\\mathrm{ctx}}$ . ", "page_idx": 41}, {"type": "text", "text": "Define $t_{\\mathrm{min}}:=\\mathrm{argmin}_{t\\in\\{t_{0},t_{1}\\}}\\,v_{t}$ and define $t_{\\mathrm{max}}:=\\mathrm{argmax}_{t\\in\\{t_{0},t_{1}\\}}\\,v_{t}$ . ", "page_idx": 41}, {"type": "text", "text": "Define $T_{\\mathrm{min}},T_{\\mathrm{max}}:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow\\{t_{0},t_{1}\\}$ to be the assignment of tokens to positions that pays the least (respectively, most) attention to $t_{\\mathrm{max}}$ : ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{T_{\\mathrm{min}}(i):=\\biggl\\{t_{\\mathrm{max}}}&{i f0\\leq\\sigma_{s}(i)<n_{t_{\\mathrm{max}}}}\\\\ {t_{\\mathrm{min}}}&{i f n_{t_{\\mathrm{max}}}\\leq\\sigma_{s}(i)<n_{\\mathrm{ctx}}}\\\\ {T_{\\mathrm{max}}(i):=\\biggl\\{t_{\\mathrm{min}}}&{i f0\\leq\\sigma_{s}(i)<n_{t_{\\mathrm{min}}}}\\\\ {t_{\\mathrm{max}}}&{i f n_{t_{\\mathrm{min}}}\\leq\\sigma_{s}(i)<n_{\\mathrm{ctx}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Then we have that ", "page_idx": 41}, {"type": "equation", "text": "$$\ns_{T_{\\mathrm{min}}}\\leq s_{T}\\leq s_{T_{\\mathrm{max}}}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. The extremality of $s_{T_{\\mathrm{min}}}$ and $s_{T_{\\mathrm{max}}}$ follows straightforwardly from Theorem 4. ", "page_idx": 41}, {"type": "text", "text": "All that remains is $s_{T_{\\mathrm{min}}}\\leq s_{T_{\\mathrm{max}}}$ . ", "page_idx": 41}, {"type": "text", "text": "This follows from noting by Lemma 13 that swapping two tokens in $T_{\\mathrm{min}}$ increases the sequence score, while the reverse is true of $s_{T_{\\mathrm{max}}}$ , thus showing that it must be $s_{T_{\\mathrm{min}}}$ that is the minimum and $s_{T_{\\mathrm{max}}}$ that is the maximum and not vice versa. \u53e3 ", "page_idx": 41}, {"type": "text", "text": "Definition 18 (full sequence score). Given a subset of valid tokens $S\\subseteq\\mathbb{N}_{<d_{\\mathrm{vocab}}}$ and a function $T:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow S$ specifying which token is in each position define the full sequence score $s_{T}^{\\prime}$ : ", "page_idx": 41}, {"type": "equation", "text": "$$\ns_{T}^{\\prime}:={\\sum_{0\\leq i<n_{\\mathrm{ctx}}}(v_{T(i)}+w_{i})e^{a_{T(i)}+b_{i}}}\\Biggl/{\\sum_{0\\leq i<n_{\\mathrm{ctx}}}e^{a_{T(i)}+b_{i}}}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Theorem 15 (Independent pessimization over positional contributions and token attention and token value is possible). Fix two tokens $t_{0}<t_{1}\\in\\mathbb{N}$ . Let $T_{\\mathrm{min}},T_{\\mathrm{max}}:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow\\{t_{0},t_{1}\\}$ and $t_{\\mathrm{max}},\\,t_{\\mathrm{min}}$ be as in Theorem 14. Fix a set $S$ of valid tokens with $t_{0},t_{1}\\in S$ . ", "page_idx": 41}, {"type": "text", "text": "Define relaxed versions $T_{\\mathrm{max}}^{\\prime},T_{\\mathrm{min}}^{\\prime}:\\mathbb{N}_{<n_{\\mathrm{ctx}}}\\rightarrow S$ of $T_{\\mathrm{max}}$ and $T_{\\mathrm{min}}$ : ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\mathrm{max}}^{\\prime}(i):=\\left\\{\\underset{\\substack{j\\neq l\\mathrm{max}}}{T_{\\mathrm{max}}}(i)\\right.}&{i f T_{\\mathrm{max}}(i)=t_{\\mathrm{max}}}\\\\ &{T_{\\mathrm{min}}^{\\prime}(i):=\\left\\{\\underset{\\substack{j\\neq l\\mathrm{max}}}{\\operatorname{argmin}}\\,\\,\\,\\,\\,\\mu_{j}\\right.}&{o t h e r w i s e}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "That is, $T_{\\mathrm{max}}^{\\prime}$ replaces $t_{\\mathrm{min}}$ with whatever token in $S$ draws the least attention away from $t_{\\mathrm{max}}$ , while $T_{\\mathrm{min}}^{\\prime}$ replaces $t_{\\mathrm{min}}$ with whichever token in $S$ draws the most attention away from $t_{\\mathrm{max}}$ . ", "page_idx": 41}, {"type": "text", "text": "Define relaxed extremal sequence scores rTmax, rTmin: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r_{T_{\\mathrm{min}}}:=\\underset{0\\leq i<n_{\\mathrm{ctx}}}{\\mathrm{min}}\\,w_{i}+\\left(\\underset{0\\leq i<n_{\\mathrm{ctx}}}{\\sum}v_{T_{\\mathrm{min}}(i)}e^{a_{T_{\\mathrm{min}}^{\\prime}(i)}+b_{i}}\\Bigg/\\underset{0\\leq i<n_{\\mathrm{ctx}}}{\\sum\\sum}e^{a_{T_{\\mathrm{min}}^{\\prime}(i)}+b_{i}}\\right)}\\\\ &{r_{T_{\\mathrm{max}}}:=\\underset{0\\leq i<n_{\\mathrm{ctx}}}{\\mathrm{max}}\\,w_{i}+\\left(\\underset{0\\leq i<n_{\\mathrm{ctx}}}{\\sum}v_{T_{\\mathrm{min}}(i)}e^{a_{T_{\\mathrm{max}}^{\\prime}(i)}+b_{i}}\\Bigg/\\underset{0\\leq i<n_{\\mathrm{ctx}}}{\\sum\\sum}e^{a_{T_{\\mathrm{max}}^{\\prime}(i)}+b_{i}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then rTmin \u2264s\u2032Tmin and s\u2032Tmax $s_{T_{\\mathrm{max}}}^{\\prime}\\leq r_{T_{\\mathrm{max}}}$ ", "page_idx": 42}, {"type": "text", "text": "Proof. (sketch) Essentially the same as the proof of Theorem 6. ", "page_idx": 42}, {"type": "text", "text": "Note that in practice, we take $S$ to be the set of all tokens less than $t_{\\operatorname*{max}}-g$ for some minimum gap $g$ . This allows us to share computation across the various maximum tokens to reduce overall computational complexity. ", "page_idx": 42}, {"type": "text", "text": "Algorithm 5 Counting Correct Sequences in Subcubic Time, Preliminaries ", "text_level": 1, "page_idx": 42}, {"type": "table", "img_path": "2zWbzx50mH/tmp/e352445a32d7b24257613964a4b7353b8e60a2d2848b8c61b79dab2f605ca3b8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 42}, {"type": "text", "text": "1: function MODEL-BEHAVIOR-RELAXED-OVER-GAP( $\\mathcal{M}$ , $t_{\\mathrm{max}}$ , tquery, c, g, g\u2217)   \nEnsure: CORRECTNESS-PESSIMIZING-OVER-GAP-SLOW is False $\\Longrightarrow$ result is False   \nRequire: $0\\leq g^{*}\\leq g\\leq t_{\\operatorname*{max}}$   \nRequire: if $c=0$ then $t_{\\mathrm{query}}=t_{\\mathrm{max}}$   \n2: skip-score $\\gets\\operatorname*{max}_{t}$ \u2217 $\\begin{array}{r}{\\ell^{\\mathrm{EU}}(t_{\\mathrm{query}})_{t^{*}}-\\operatorname*{min}_{t^{*}}\\ell^{\\mathrm{EU}}(t_{\\mathrm{query}})_{t^{*}}}\\end{array}$ \u25b7Cache by tquery   \n3: $v_{t}\\gets\\mathrm{EVOU}(t)$   \n4: $w_{i}\\gets\\mathrm{PVOU}(i)$   \n5: \u2206wmax,t\u2217\u2190maxi wi,t\u2217\u2212wi,tmax \u25b7Cache by $t_{\\mathrm{max}}$ , $t^{*}$   \n6: \u2206wmax,max \u2190maxt\u2217\u2206wmax,t\u2217 \u25b7Cache by $t_{\\mathrm{max}}$   \n7: \u2206vt \u2190maxt\u2217vt,t\u2217\u2212mint\u2217vt,t\u2217 $\\triangleright$ Cache by $t$   \n8: \u2206vmax \u2190max0\u2264t\u2264tmax\u2212g\u2217\u2206vt \u25b7Cache by $t_{\\operatorname*{max}}-g^{*}$   \n9: \u2206v\u2217m \u2190vtmax,t\u2217\u2212vtmax,tmax $\\triangleright$ Cache by $t_{\\mathrm{max}}$   \n10: \u2206vtmmaaxx \u2190maxt\u2217\u0338=tmax \u2206vtt\u2217max $\\triangleright$ Cache by $t_{\\mathrm{max}}$   \n11:   \n12: $\\ell_{t^{*}}\\gets\\ell^{\\mathrm{EU}}(t_{\\operatorname*{max}})_{t^{*}}+v_{t_{\\operatorname*{max}},t^{*}}+\\Delta w_{\\operatorname*{max},t^{*}}$   \n13: return $\\mathrm{max}_{t^{*}\\ne t_{\\mathrm{max}}}\\left(\\ell_{t^{*}}-\\ell_{t_{\\mathrm{max}}}\\right)$   \n14: end if   \n15: $b_{:,n_{\\mathrm{ctx}}-1}\\gets\\mathrm{EQKP}(t_{\\mathrm{query}},n_{\\mathrm{ctx}}-1)/\\sqrt{d}$ \u25b7Cache by tquery   \n16: $\\rangle_{0,:-1}\\gets\\mathrm{soRT}(\\mathrm{EQKP}(t_{\\mathrm{query}},:-1))/\\sqrt{d}$ $\\triangleright$ Cache by $t_{\\mathrm{query}}$ , $i$   \n17: $b_{1,:-1}\\gets\\textsc{R E V E R S E}(b_{0,:-1})$   \n18: $a_{t}\\gets\\mathrm{EQKE}(t_{\\mathrm{query}},t)/\\sqrt{d}$ $\\triangleright$ Cache by $t_{\\mathrm{query}}$ , $t$   \n19: $a_{\\mathrm{min},t}\\gets\\operatorname*{min}_{0\\leq t^{\\prime\\prime}\\leq t}a_{t^{\\prime\\prime}}$ $\\triangleright$ Cache by $t_{\\mathrm{query}},t$ , compute in amortized $\\mathcal{O}(d_{\\mathrm{vocab}}{}^{2})$   \n20: $a_{\\mathrm{max},t}\\gets\\operatorname*{max}_{0\\leq t^{\\prime\\prime}\\leq t}a_{t^{\\prime\\prime}}$ $\\triangleright$ Cache by $t_{\\mathrm{query}},t$ , compute in amortized $\\mathcal{O}(d_{\\mathrm{vocab}}{}^{2})$   \n21: \u2206amax \u2190atmax \u2212amin,tmax\u2212g $\\triangleright$ Cache by $t_{\\mathrm{query}},t_{\\mathrm{max}},c$   \n22: $\\Delta a_{\\mathrm{min}}\\leftarrow a_{t_{\\mathrm{max}}}-a_{\\mathrm{max},t_{\\mathrm{max}}-q}$ $\\triangleright$ Cache by $t_{\\mathrm{query}}$ , $t_{\\mathrm{max}}$ , $c$   \n23: i $|\\mathrm{x-set}\\gets\\{0,\\dots,n_{\\mathrm{ctx}}-c-1\\}$ if $t_{\\mathrm{max}}\\neq t_{\\mathrm{query}}$ else $\\{0,\\dots,n_{\\mathrm{ctx}}-c-2,n_{\\mathrm{ctx}}^{-}-1\\}$   \n24: attn-weights-unscaled $\\mathsf{l}_{0,i}\\gets b_{0,i}+(\\Delta a_{\\operatorname*{min}}$ if $i\\in$ idx-set else 0)   \n25: attn-weights-unscaled $\\cdot_{1,i}\\gets b_{1,i}+(\\Delta a_{\\operatorname*{max}}$ if $i\\in$ idx-set else 0) \u25b7Cache by $t_{\\mathrm{query}},\\,t_{\\mathrm{max}},\\,i,$ , $c$   \n26: attn-weight $\\mathrm{{\\dot{\\rho}_{0}}\\gets\\Delta S O F T M A X}$ (attn-weights-unscaled0) $\\triangleright$ Cache by $t_{\\mathrm{query}}$ , $t_{\\mathrm{max}}$ , i, $c$   \n27: attn-weight $_{1}\\leftarrow\\mathrm{SOFTMAX}.$ (attn-weights-unscaled1) $\\triangleright$ Cache by tquery, $t_{\\mathrm{max}}$ , i, $c$   \n28: attn- $\\begin{array}{r}{\\cdot\\mathrm{max}_{0}\\gets\\sum_{i\\in\\mathrm{idx-set}}}\\end{array}$ attn-weights0,i   \n29: attn- $\\begin{array}{r}{\\operatorname*{max}_{1}\\leftarrow\\sum_{i\\in\\mathrm{idx-set}}}\\end{array}$ attn-weights $^{1,i}$   \n3310:: ratettnu-rmna sx $\\leftarrow$ sacttonr- $\\cdot\\mathbf{max}_{0}$ $\\Delta v_{\\operatorname*{max}}^{t_{\\operatorname*{max}}}\\ge\\Delta v_{\\operatorname*{max}}$ xe l\u00b7 $+\\,\\Delta w_{\\mathrm{max,max}}+$ $\\Delta v_{\\mathrm{max}}^{t_{\\mathrm{max}}}+(1-\\mathrm{attn-max})\\Delta v_{\\mathrm{max}}$   \n32: end function   \n33: function RELAXED-CORRECTNESS-PESSIMIZING-OVER-GAP(M, dvocab, $n_{\\mathrm{ctx}}$ , $t_{\\mathrm{max}}$ , tquery, c, $g,g^{\\ast})$   \n34: $\\triangleright$ runs the model on a relaxed variant of input sequences compatible with the arguments   \nEnsure: CORRECTNESS-PESSIMIZING-OVER-GAP-SLOW is False $\\Longrightarrow$ result is False   \nEnsure: return is False if CORRECTNESS-PESSIMIZING-OVER-GAP-SLOW $(\\mathcal{M},\\mathbf{t})$ is False for any t with specified $t_{\\mathrm{max}}$ , $\\ensuremath{t_{\\mathrm{query}}}$ , and $c$ tokens not equal to $t_{\\mathrm{max}}$   \n35: return MODEL-BEHAVIOR-RELAXED-OVER-GAP $(\\mathcal{M},t_{\\mathrm{max}},t_{\\mathrm{query}},c,g,g^{*})<0$   \n36: end function ", "page_idx": 43}, {"type": "text", "text": "Theorem 16. For all $G$ , ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{t}\\sim U(0,1,\\ldots,d_{\\mathrm{vocab}}-1)^{n_{\\mathrm{ctx}}}}\\left[\\operatorname{argmax}_{i}(\\mathcal{M}(\\mathbf{t}))_{i}=\\operatorname*{max}_{i}t_{i}\\right]\\ge\\operatorname{SUBCUBIC}(d_{\\mathrm{vocab}},n_{\\mathrm{ctx}},\\mathcal{M},G)\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof. (sketch) Apply preceding lemmas and theorems to Algorithm 6 ", "page_idx": 44}, {"type": "text", "text": "Theorem 17. The running time of Algorithm $\\textit{6}$ , after using caching to avoid duplicate computations, is $\\mathcal{O}(d_{\\mathrm{vocab}}^{\\phantom{\\dagger}}{}^{2}d_{\\mathrm{model}}+d_{\\mathrm{vocab}}^{\\phantom{\\dagger}}{}^{2}n_{\\mathrm{ctx}}^{\\phantom{\\dagger}}{}^{2})$ ). ", "page_idx": 44}, {"type": "text", "text": "Proof. (sketch) Sum the complexities indicated along the right side of Algorithm 3. The $d_{\\mathrm{vocab}}{}^{2}d_{\\mathrm{model}}$ term comes from the precomputing EVOU, EU, and EQKP. The $d_{\\mathrm{vocab}}{}^{2}n_{\\mathrm{ctx}}{}^{2}$ term comes from the softmax over $n_{\\mathrm{ctx}}$ tokens for $\\bar{O}(d_{\\mathrm{vocab}}{}^{2}n_{\\mathrm{ctx}})$ pessimized pure sequences. Confirming that none of the complexities on the right side exceeds $\\bar{\\mathcal{O}}(d_{\\mathrm{vocab}}2{d_{\\mathrm{model}}}+\\bar{d}_{\\mathrm{vocab}}2n_{\\mathrm{ctx}}{}^{2})$ completes the proof. ", "page_idx": 44}, {"type": "text", "text": "G Subcubic proof strategies ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "In this section, we present a number of proof strategies that we use to reduce the computational cost of the proof, ultimately driving down the cost of EU and EQKE verification to $\\mathcal{O}(d_{\\mathrm{vocab}}d_{\\mathrm{model}})$ , while unfortunately leaving the cost of EVOU verification at $\\dot{\\mathcal{O}}(d_{\\mathrm{vocab}}{^2d}_{\\mathrm{model}})$ . ", "page_idx": 44}, {"type": "text", "text": "The three main tricks we cover are the mean $^{+}$ diff trick (Appendix G.1), the max row-diff trick (Appendix G.2.2), and the rank one / rank two SVD decomposition of EQKE (Appendix G.2.3). While the mean+diff trick is useful for getting slightly better bounds, the SVD decomposition of EQKE is the place where we get to insert the most understanding (without which we\u2019d have no hope of non-vacuous bounds below $\\mathcal{O}(d_{\\mathrm{vocab}}{}^{2}d_{\\mathrm{model}}))$ , and the max row-diff trick is the workhorse that allows us to drive down the error term computations from cubic to quadratic without getting completely vacuous bounds. ", "page_idx": 44}, {"type": "text", "text": "G.1 The mean+diff trick ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Suppose we have quantities $f_{x,y}$ and $g_{y,z}$ and we want to pessimize (WLOG, suppose minimize) the quantity $f_{x,y}+g_{y,z}$ over $x,y$ , and $z$ in time less than $O(n_{x}n_{y}n_{z})$ , say we allow $O(n_{x}n_{y}+n_{y}n_{z}+$ $n_{x}n_{z})$ . Also suppose the variation of $f$ over the $y$ axis is much larger than the variation of f over the $\\mathbf{X}$ -axis. ", "page_idx": 44}, {"type": "text", "text": "We can of course say ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x,y}f_{x,y}+\\operatorname*{min}_{y,z}g_{y,z}\\leq f_{x,y}+g_{y,z}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "But we can do better! ", "page_idx": 44}, {"type": "text", "text": "Note that ", "page_idx": 44}, {"type": "equation", "text": "$$\nf_{x,y}=\\mathbb{E}_{x}f_{x,y}+(f_{x,y}-\\mathbb{E}_{x}f_{x,y})\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Suppose that $f_{x,y}$ varies much less over $x$ than it does over $y$ , and much less than $g_{y,z}$ varies over either of $y$ and $z$ . This will make the following bound a good approximation, though the bound is sound even without this assumption. We can write ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{f_{x,y}+g_{y,z}\\geq\\operatorname*{min}_{\\varepsilon,y}[f_{x,y}+g_{y,z}]}\\quad}&{}\\\\ &{=\\operatorname*{min}_{x,y,z}\\mathbb{1}_{\\Sigma}f_{x}\\mathfrak{f}_{x,y}+g_{y,z}+f_{x,y}-\\mathbb{E}_{x}f_{x,y}]}\\\\ &{\\geq\\displaystyle\\operatorname*{min}_{x,y,z}[\\mathbb{E}_{x}f_{x,y}+g_{y,z}]+\\displaystyle\\operatorname*{min}_{x,y,z}[f_{x,y}-\\mathbb{E}_{x}f_{x,y}]}\\\\ &{=\\displaystyle\\operatorname*{min}_{y,z}[\\mathbb{E}_{x}f_{x,y}+g_{y,z}]+\\displaystyle\\operatorname*{min}_{x,y}[f_{x,y}-\\mathbb{E}_{x}f_{x,y}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "By averaging the variation over certain axes, we have ", "page_idx": 44}, {"type": "text", "text": "Theorem 18 (Mean+Diff). ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x,y,z}{\\operatorname*{min}}\\;f_{x,y}+g_{y,z}\\geq\\underset{y,z}{\\operatorname*{min}}[\\mathbb{E}_{x}f_{x,y}+g_{y,z}]+\\underset{x,y}{\\operatorname*{min}}[f_{x,y}-\\mathbb{E}_{x}f_{x,y}]}\\\\ &{\\underset{x,y,z}{\\operatorname*{max}}\\;f_{x,y}+g_{y,z}\\leq\\underset{y,z}{\\operatorname*{max}}[\\mathbb{E}_{x}f_{x,y}+g_{y,z}]+\\underset{x,y}{\\operatorname*{max}}[f_{x,y}-\\mathbb{E}_{x}f_{x,y}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and the RHSs can be computed in time $\\mathcal{O}(n_{x}n_{y}+n_{y}n_{z}+n_{x}n_{z})$ for $n_{x},\\,n_{y}$ , and $n_{z}$ the number of possible values of x, y, and $z$ , respectively. ", "page_idx": 45}, {"type": "text", "text": "Example for how this helps with small variation: ", "page_idx": 45}, {"type": "text", "text": "Take any function $k(y)$ and then take ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f_{x,y}:=k(y)+\\varepsilon_{1}(x,y)}\\\\ {g_{y,z}:=-k(y)+\\varepsilon_{2}(y,z)}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Then we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\operatorname*{min}_{x,y,z}[f_{x,y}+g_{y,z}]=\\displaystyle\\operatorname*{min}_{x,y,z}[\\varepsilon_{1}(x,y)+\\varepsilon_{2}(y,z)]}}\\\\ {{\\displaystyle\\operatorname*{min}_{x,y}f_{x,y}+\\operatorname*{min}_{y,z}\\upsilon_{y,z}=\\displaystyle\\operatorname*{min}_{y}k(y)+\\displaystyle\\operatorname*{min}_{y}-k(y)+\\displaystyle\\operatorname*{min}_{x,y}\\varepsilon_{1}(x,y)+\\displaystyle\\operatorname*{min}_{y,z}\\varepsilon_{2}(y,z)}}\\\\ {{\\displaystyle=\\operatorname*{min}_{y}k(y)-\\displaystyle\\operatorname*{max}_{y}k(y)+\\displaystyle\\operatorname*{min}_{x,y}\\varepsilon_{1}(x,y)+\\displaystyle\\operatorname*{min}_{y,z}\\varepsilon_{2}(y,z)}}\\\\ {{\\displaystyle\\operatorname*{min}_{x,y}[f_{x,y}-\\mathbb{E}_{x}f_{x,y}]+\\displaystyle\\operatorname*{min}_{y,z}[g_{y,z}+\\mathbb{E}_{x}f_{x,y}]=\\displaystyle\\operatorname*{min}_{x,y}\\varepsilon_{1}(x,y)+\\displaystyle\\operatorname*{min}_{y,z}[\\varepsilon_{2}(y,z)+\\mathbb{E}_{x}\\varepsilon_{1}(x,y)]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "If $\\varepsilon_{1}$ and $\\varepsilon_{2}$ are small compared to $\\,\\mathrm{min}_{y}\\,k(y)-\\mathrm{max}_{y}\\,k(y)$ , then using $\\mathbb{E}_{x}f_{x,y}$ gives a much better bound. ", "page_idx": 45}, {"type": "text", "text": "Note, though, that this could be a worse bound if the assumption of small variation does not hold. ", "page_idx": 45}, {"type": "text", "text": "Note also that this trick is not restricted to adding and subtracting $\\mathbb{E}_{x}f_{x,y}$ . If $f$ is a matrix indexed by $x$ and $y$ , we might also try taking SVD and using the first principal component instead. A basic application of the triangle inequality gives the following, more general, result: ", "page_idx": 45}, {"type": "text", "text": "Theorem 19 (Summarize+Diff). For any $h_{y}$ which can be computed in time $\\mathcal{O}(n_{h})$ , ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{min}_{x,y,z}\\ f_{x,y}+g_{y,z}\\geq\\displaystyle\\operatorname*{min}_{y,z}[h_{y}+g_{y,z}]+\\displaystyle\\operatorname*{min}_{x,y}[f_{x,y}-h_{y}]}\\\\ {\\displaystyle\\operatorname*{max}_{x,y,z}f_{x,y}+g_{y,z}\\leq\\displaystyle\\operatorname*{max}_{y,z}[h_{y}+g_{y,z}]+\\displaystyle\\operatorname*{max}_{x,y}[f_{x,y}-h_{y}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and the RHSs can be computed in time $O(n_{x}n_{y}+n_{y}n_{z}+n_{h})$ for $n_{x},\\;n_{y},$ , and $n_{z}$ the number of possible values of $x,\\,y,$ , and $z$ , respectively. ", "page_idx": 45}, {"type": "text", "text": "We see that if the variation of $f$ in the $x$ -axis is indeed much smaller than the variation in the $y$ -axis, then letting ", "page_idx": 45}, {"type": "equation", "text": "$$\nf_{x,y}=h_{y}+\\varepsilon_{x,y}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "we get ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x,y,z}{\\mathrm{min}}\\,f_{x,y}+g_{y,z}-\\underset{y,z}{\\mathrm{min}}[h_{y}+g_{y,z}]-\\underset{x,y}{\\mathrm{min}}[f_{x,y}-h_{y}]}\\\\ &{\\le\\left|\\underset{x,y,z}{\\mathrm{min}}[f_{x,y}+g_{y,z}]-\\underset{y,z}{\\mathrm{min}}[h_{y}+g_{y,z}]\\right|+\\left|\\underset{x,y}{\\mathrm{min}}[\\varepsilon_{x,y}]\\right|}\\\\ &{\\le2\\underset{x,y}{\\mathrm{max}}\\,|\\varepsilon_{x,y}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "so indeed this bound isn\u2019t too much worse and we are able to compute it in quadratic rather than cubic time. ", "page_idx": 45}, {"type": "text", "text": "G.2 Details of SVD of QK proof ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "As discussed in Section 4.3.1, to further reduce the computation cost of proof, we need to avoid computing the residual stream, EVOU, and EPQKE matrices fully. Using mechanistic insight or otherwise, we observe that these matrices (apart from EVOU) can be well-approximated by rank one matrices. This will remove the dominant computation cost of $\\mathcal{O}(d_{\\mathrm{vocab}}{^2}\\cdot\\dot{d}_{\\mathrm{model}}^{-})$ . ", "page_idx": 45}, {"type": "text", "text": "G.2.1 Comments on relationship between mechanistic insight and proof size ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Up to this point, we haven\u2019t really said much in our proofs about what the model is doing. All the mechanistic insight has been of the form \u201cthe model varies more along this axis than this other axis\u201d or \u201cthe input data is distributed such that handling these inputs is more important than handling these other inputs\u201d or, at best, \u201cthe model computes the answer by attending to the maximum token of the sequence; everything else is noise\u201d. ", "page_idx": 46}, {"type": "text", "text": "Here, finally, our proof-size constraints are tight enough that we will see something that we could plausibly call \u201chow the model pays attention to the maximum token more than anything else\u201d, i.e., (if we squint a bit) \u201cthe model pays more attention to larger tokens in general. ", "page_idx": 46}, {"type": "text", "text": "G.2.2 The max row-diff trick ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "As stated above, we are breaking matrices into their rank one approximation and some error term. To bound the error, i.e. to bound expressions of the form $\\textstyle\\prod_{i}(A_{i}\\,{\\overline{{+}}}\\,E_{i})-\\prod_{i}A_{i}$ , where $E_{i}$ denote the matrix errors, we can use the following trick: ", "page_idx": 46}, {"type": "text", "text": "Lemma 20 (Max Row-Diff (vector-matrix version)). For a row vector a and a matrix $B$ , ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i,j}\\left((\\mathbf{a}B)_{i}-(\\mathbf{a}B)_{j}\\right)\\leq\\sum_{k}\\left|a_{k}\\right|\\operatorname*{max}_{i,j}\\left(B_{k,i}-B_{k,j}\\right)\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Moreover, for a collection of n row vectors $A_{r}$ , if the shape of $B$ is $m\\times p_{\\mathrm{:}}$ , the right hand side can be computed for all $r$ in time $O(n m+m p)$ . ", "page_idx": 46}, {"type": "text", "text": "Proof. ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{max}_{i,j}(\\mathrm{aB})_{i}-(\\mathrm{aB})_{j}}&{}\\\\ {\\displaystyle=\\operatorname*{max}_{i,j}\\sum_{k}a_{k}\\left(B_{k,i}-B_{k,j}\\right)}&{}\\\\ {\\displaystyle\\leq\\sum_{k}\\displaystyle\\operatorname*{max}_{i,j}a_{k}\\left(B_{k,i}-B_{k,j}\\right)}\\\\ {\\displaystyle=\\sum_{k}a_{k}\\left\\{\\operatorname*{max}_{i,j}\\left(B_{k,i}-B_{k,j}\\right)\\quad\\mathrm{if}\\ a_{k}\\geq0}\\\\ &{=\\displaystyle\\sum_{k}a_{k}\\left\\{\\operatorname*{max}_{i,j}\\left(B_{k,i}-B_{k,j}\\right)\\quad\\mathrm{if}\\ a_{k}\\geq0}\\\\ {\\displaystyle=\\sum_{k}a_{k}\\left\\{\\operatorname*{max}_{i,j}\\left(B_{k,i}-B_{k,j}\\right)\\quad\\mathrm{if}\\ a_{k}<0}\\\\ {\\displaystyle=\\sum_{k}\\left\\{a_{k}\\operatorname*{max}_{i,j}\\left(B_{k,i}-B_{k,j}\\right)\\quad\\mathrm{if}\\ a_{k}<0}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "The asymptotic complexity of computing the result follows from caching the computation of $\\operatorname*{max}_{i,j}\\left(B_{k,i}-B_{k,j}\\right)$ for each $k$ independently of $r$ , as the computation does not depend on $A_{r}$ . ", "page_idx": 46}, {"type": "text", "text": "Theorem 21 (Max Row-Diff). For matrices $A$ and $B$ , ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{r,i,j}\\left((A B)_{r,i}-(A B)_{r,j}\\right)\\leq\\operatorname*{max}_{r}\\sum_{k}\\left|A_{r,k}\\right|\\operatorname*{max}_{i,j}\\left(B_{k,i}-B_{k,j}\\right)\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof. By taking the max of Lemma 20 over rows $r$ of $A$ . ", "page_idx": 46}, {"type": "text", "text": "Lemma 20 can also be applied recursively for a product of more than two matrices. ", "page_idx": 46}, {"type": "text", "text": "Lemma 22 (Max Row-Diff (vector-matrix recursive version)). For a row vector a and a sequence of n matrices Bp of shapes rp \u00d7 cp, ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i,j}\\left(\\left(\\mathbf{a}\\prod_{p}B_{p}\\right)_{i}-\\left(\\mathbf{a}\\prod_{p}B_{p}\\right)_{j}\\right)\\leq\\sum_{k_{0}}|a_{k_{0}}|\\cdot\\cdot\\cdot\\sum_{k_{n}}\\left|(B_{n-1})_{k_{n-1},k_{n}}\\right|\\operatorname*{max}_{i,j}\\left((B_{n})_{k_{n},i}-(B_{n})_{k_{n-1},k_{n}}\\right)\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Moreover, for a collection of q row vectors $A_{\\alpha}$ , the right hand side can be computed for all $\\alpha$ in time $\\begin{array}{r}{\\mathcal{O}(q r_{0}+\\sum_{p}r_{p}c_{p})}\\end{array}$ . ", "page_idx": 46}, {"type": "text", "text": "Proof. We proceed by induction on $n$ . ", "page_idx": 47}, {"type": "text", "text": "For $n=1$ , the statement is identical to Lemma 20. ", "page_idx": 47}, {"type": "text", "text": "Suppose the theorem holds for all positive $n=s$ ; we show the theorem holds for $n=s+1$ . We reassociate the matrix multiplication as ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{i,j}\\left(\\left(\\mathbf{a}\\prod_{p=1}^{s+1}B_{p}\\right)_{i}-\\left(\\mathbf{a}\\prod_{p=1}^{s+1}B_{p}\\right)_{j}\\right)}\\\\ &{\\displaystyle=\\operatorname*{max}_{i,j}\\left((\\mathbf{a}B_{1})\\left(\\left(\\prod_{p=2}^{s+1}B_{p}\\right)_{i}-\\left(\\prod_{p=2}^{s+1}B_{p}\\right)_{j}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Using the induction hypothesis gives ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\leq\\sum_{k_{1}}\\left|\\sum_{k_{0}}a_{k_{0}}(B_{1})_{k_{0},k_{1}}\\right|\\sum_{k_{2}}|(B_{2})_{k_{1},k_{2}}|\\cdot\\cdot\\cdot\\sum_{k_{s+1}}\\left|(B_{s})_{k_{s},k_{s+1}}\\right|\\operatorname*{max}_{i,j}\\left((B_{s+1})_{k_{s+1},i}-(B_{s+1})_{k_{s+1},j}\\right)\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "The triangle inequality gives ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\leq\\sum_{k_{1}}\\sum_{k_{0}}|a_{k_{0}}(B_{1})_{k_{0},k_{1}}|\\sum_{k_{2}}|(B_{2})_{k_{1},k_{2}}|\\cdot\\cdot\\cdot\\sum_{k_{s+1}}\\left|(B_{s})_{k_{s},k_{s+1}}\\right|\\operatorname*{max}_{i,j}\\left((B_{s+1})_{k_{s+1},i}-(B_{s+1})_{k_{s+1},j}\\right)\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and algebra gives ", "page_idx": 47}, {"type": "equation", "text": "$$\n=\\sum_{k_{0}}|a_{k_{0}}|\\sum_{k_{1}}|(B_{1})_{k_{0},k_{1}}|\\sum_{k_{2}}|(B_{2})_{k_{1},k_{2}}|\\cdot\\cdot\\sum_{k_{s+1}}\\left|(B_{s})_{k_{s},k_{s+1}}\\right|\\operatorname*{max}_{i,j}\\left((B_{s+1})_{k_{s+1},i}-(B_{s+1})_{k_{s+1},j}\\right)\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "The asymptotic complexity of computing the right hand side also follows straightforwardly by induction. ", "page_idx": 47}, {"type": "text", "text": "Theorem 23 (Max Row-Diff (recursive)). For a sequence of $n+1$ matrices $A_{0},\\ldots,A_{n}$ , ", "page_idx": 47}, {"type": "equation", "text": "$$\n:\\left(\\left(\\prod_{p}A_{p}\\right)_{r,i}-\\left(\\prod_{p}A_{p}\\right)_{r,j}\\right)\\leq\\operatorname*{max}_{r}\\sum_{k_{0}}|(A_{0})_{r,k_{0}}|\\cdot\\cdot\\cdot\\sum_{k_{n}}\\left|(A_{n-1})_{k_{n-1},k_{n}}\\right|\\operatorname*{max}_{i,j}\\left((A_{n})_{k_{n},i}-(A_{n-1})_{k_{n-1},k_{n}}\\right)\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proof. By taking the max of Lemma 22 over rows $r$ of $A_{0}$ . ", "page_idx": 47}, {"type": "text", "text": "Note that Theorem 21 is compatible with the mean+diff trick of Appendix G.1. ", "page_idx": 47}, {"type": "text", "text": "Theorem 24 (Combined Mean+Diff and Max Row-Diff). For matrices $A$ and $B$ , and any columnwise summary vector $H_{k}$ of $A$ (for example we may take $H_{k}:=\\mathbb{E}_{r}A_{r,k},$ ) ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\gamma,i,j}\\left((A B)_{r,i}-(A B)_{r,j}\\right)\\leq\\left(\\operatorname*{max}_{i,j}\\sum_{k}H_{k}\\left(B_{k,i}-B_{k,j}\\right)\\right)\\operatorname*{max}_{r}\\sum_{k}\\left|A_{r,k}-H_{k}\\right|\\operatorname*{max}_{i,j}\\left(B_{k,i}-B_{k,j}\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proof. ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\operatorname*{max}_{r,i,j}\\left(\\left(A B\\right)_{r,i}-\\left(A B\\right)_{r,j}\\right)}}\\\\ &{=\\operatorname*{max}_{r,i,j}\\sum_{k}A_{r,k}\\left(B_{k,i}-B_{k,j}\\right)}\\\\ &{=\\displaystyle\\operatorname*{max}_{r,i,j}\\sum_{k}\\left(H_{k}+\\left(A_{r,k}-H_{k}\\right)\\right)\\left(B_{k,i}-B_{k,j}\\right)}\\\\ &{=\\displaystyle\\operatorname*{max}_{i,j}\\left(\\sum_{k}H_{k}\\left(B_{k,i}-B_{k,j}\\right)+\\operatorname*{max}_{r}\\sum_{k}\\left(A_{r,k}-H_{k}\\right)\\left(B_{k,i}-B_{k,j}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\left(\\operatorname*{max}_{i,j}\\sum_{k}H_{k}\\left(B_{k,i}-B_{k,j}\\right)\\right)+\\operatorname*{max}_{r}\\sum_{k}\\operatorname*{max}_{i,j}\\left(A_{r,k}-H_{k}\\right)\\left(B_{k,i}-B_{k,j}\\right)}\\\\ {\\displaystyle\\leq\\left(\\operatorname*{max}_{i,j}\\sum_{k}H_{k}\\left(B_{k,i}-B_{k,j}\\right)\\right)+\\operatorname*{max}_{r}\\sum_{k}\\left|A_{r,k}-H_{k}\\right|\\operatorname*{max}_{i,j}\\left(B_{k,i}-B_{k,j}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Theorem 25 (Combined Mean $^+.$ Diff and Vector-Matrix Recursive Max Row-Diff). For a row vector a, a vector of summaries h corresponding to a (for example, $i f$ a is a row of a matrix, h might be the average of the rows), a sequence of $n$ matrices $B_{p}$ of shapes $r_{p}\\times c_{p}$ , and a corresponding sequence of column-wise summary vectors $\\mathbf{h}_{\\mathbf{p}}$ of $B_{p}$ (for example we may take $(h_{p})_{k}:=\\vec{\\mathbb{E}_{r}}(B_{p})_{r,k})$ , ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{i,j}{\\operatorname*{max}}\\left(\\left(\\mathbf a\\prod_{p}B_{p}\\right)_{i}-\\left(\\mathbf a\\prod_{p}B_{p}\\right)_{j}\\right)}\\\\ &{\\le\\underset{i,j}{\\operatorname*{max}}\\left(\\sum_{b_{0}}h_{b_{0}}\\cdots\\sum_{k_{n}}(B_{n-1})_{k_{n-1},k_{n}}\\left((B_{n})_{k_{n},i}-(B_{n})_{k_{n},j}\\right)\\right)}\\\\ &{\\quad+\\displaystyle\\sum_{k_{0}}|a_{k_{0}}-h_{k_{0}}|\\cdots\\left(\\left(\\underset{i,j}{\\operatorname*{max}}\\sum_{k_{n}}(h_{n-1})_{k_{n}}\\left((B_{n})_{k_{n},i}-(B_{n})_{k_{n},j}\\right)\\right)\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.+\\sum_{k_{n}}|(B_{n-1})_{k_{n}-1,k_{n}}-(h_{n-1})_{k_{n}}|\\underset{i,j}{\\operatorname*{max}}\\left((B_{n})_{k_{n},i}-(B_{n})_{k_{n},j}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Moreover, for a collection of $q$ row vectors $A_{\\alpha}$ , the right hand side can be computed for all $\\alpha$ in time $\\mathcal{O}(q r_{0}+\\dot{\\sum}_{p}\\,r_{p}c_{p})$ . ", "page_idx": 48}, {"type": "text", "text": "Proof sketch. Apply the triangle inequality recursively, fusing the proofs of Lemmas 22, and 24. ", "page_idx": 48}, {"type": "text", "text": "Theorem 26 (Combined Mean $^{+}$ Diff and Recursive Max Row-Diff). For a sequence of $n\\!+\\!1$ matrices $A_{0},\\ldots,A_{n}$ , and corresponding column-wise summary vectors $\\mathbf{h_{0}},\\ldots,\\mathbf{h_{n-1}}$ of $A_{0}$ , ..., $A_{n-1}$ , ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\operatorname*{nax}}{r_{i}\\delta_{j}}\\left(\\left(\\prod_{p}A_{p}\\right)_{r_{i},i}-\\left(\\prod_{p}A_{p}\\right)_{r_{j},j}\\right)}\\\\ &{\\le\\operatorname*{max}_{r_{i},i,j}\\left(\\displaystyle\\sum_{k_{0}}(h_{0})_{k_{0}}\\cdots\\sum_{k_{n}}(A_{n-1})_{k_{n-1},k_{n}}\\left((A_{n})_{k_{n},i}-(A_{n})_{k_{n},j}\\right)\\right)}\\\\ &{\\quad+\\displaystyle\\sum_{k_{0}}|(A_{0})_{k_{0}}-(h_{0})_{k_{0}}|\\cdots\\left(\\left(\\mathop{\\operatorname*{max}}_{i,j}\\sum_{k_{n}}(h_{n-1})_{k_{n}}\\left((A_{n})_{k_{n},i}-(A_{n})_{k_{n},j}\\right)\\right)\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.+\\displaystyle\\sum_{k_{0}}\\left|(A_{n-1})_{k_{n-1},k_{n}}-(h_{n-1})_{k_{n}}\\right|\\operatorname*{max}_{i,j}\\left((A_{n})_{k_{n},i}-(A_{n})_{k_{n},j}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Moreover, if the matrices $A_{p}$ have shapes $r_{p}\\times c_{p}$ , the right hand side can be computed in time $\\mathcal{O}(\\sum_{p}r_{p}c_{p})$ . ", "page_idx": 48}, {"type": "text", "text": "Proof. By taking the max of Theorem 25 over rows $r$ of $A_{0}$ . ", "page_idx": 48}, {"type": "text", "text": "G.2.3 Exploring rank one approximation via SVD ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Let us first look at ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathrm{EQKE:}=\\mathbf{E}_{q}Q K^{T}\\bar{\\mathbf{E}}^{T}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "From Figure 9a, we see that there is not much variation along long query token direction. We can confirm this by performing a singular value decomposition (SVD) on EQKE, as seen in Figure 14. ", "page_idx": 48}, {"type": "image", "img_path": "2zWbzx50mH/tmp/19478406086dfebb87b8b9cc9a61a009936f96e60e10a50272159e0b30e4a53e.jpg", "img_caption": ["Figure 14: SVD of EQKE for seed 123, with principal component vectors scaled by the square root of the corresponding singular value. This scaling allows us to see visually that there is not much going on beyond the first singular component. Numerically: the first singular value is just over 7440, while the second singular value is just under 15. "], "img_footnote": [], "page_idx": 49}, {"type": "text", "text": "The first singular value is just over 7440 $7800\\pm380$ across all seeds), while the second singular value is just under 15 $[13.1\\pm2.8$ across all seeds). The ratio across all seeds is $620\\pm130$ . There\u2019s really not much going on here beyond the first singular component.23 ", "page_idx": 49}, {"type": "text", "text": "Call the first singular component of EQKE the \u201cquery direction\u201d $d_{q}$ and the \u201csize direction\u201d $d_{k}$ on the query-side and key-side, respectively. ", "page_idx": 49}, {"type": "text", "text": "There are two ways that we can decompose EQKE into a low-rank component that we can compute exactly, and a full-rank error term that we approximate bounds for. ", "page_idx": 49}, {"type": "text", "text": "G.2.4 The simple SVD decomposition of QK ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "In time $\\mathcal{O}(d_{\\mathrm{vocab}}d_{\\mathrm{model}}{}^{2})$ we can perform SVD on each of the four component matrices $\\mathbf{E}_{q},Q,K$ , $\\bar{\\bf E}$ and perform low-rank SVD on the matrix product ${\\bf E}_{q}Q{\\cal K}^{T}\\bar{\\bf E}^{T}$ . ", "page_idx": 49}, {"type": "text", "text": "We can then bound the difference between two elements in the same row of EQKE by computing exactly the difference between the two elements in the same row of the rank one approximation of EQKE, and adding to that a bound on the difference between the two elements in the same row of the error term. ", "page_idx": 49}, {"type": "text", "text": "That is, we can decompose $E$ into a part parallel to $d_{q}$ and a part orthogonal to $d_{q}$ , say ${\\bf E}_{q}=E_{q}+E_{q}^{\\perp}$ , and similarly $\\bar{\\mathbf{E}}=E_{k}+E_{k}^{\\perp}$ . Note that $E_{q}$ and $E_{k}$ are both rank one, and hence can be multiplied with other matrices of shape $d_{\\mathrm{model}}\\times a$ in time $\\mathcal{O}(d_{\\mathrm{model}}a)$ rather than time $\\mathcal{O}(d_{\\mathrm{vocab}}d_{\\mathrm{model}}\\bar{a})$ . ", "page_idx": 49}, {"type": "text", "text": "Hence we can define $\\mathrm{EQKE_{-}e r r_{1}}$ (subscript one for \u201crank one\u201d) and decompose EQKE as ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathrm{EQKE}=E_{q}Q K^{T}(E_{k})^{T}+\\mathrm{EQKE\\mathrm{_{-}e r r_{1}}}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Define for any vector $v$ ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\Delta_{i,j}v:=v_{i}-v_{j}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "so that we get ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{i,j}(E_{q}Q K^{T}(E_{k})^{T})_{t_{\\mathrm{query}}}+\\underset{i\\neq j}{\\operatorname*{min}}\\,\\Delta_{i,j}(\\mathrm{EQKE\\mathrm{\\underline{{~}}}}_{-}\\mathrm{err_{1}})_{t_{\\mathrm{query}}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\Delta_{i,j}\\mathrm{EQKE}_{t_{\\mathrm{query}}}\\leq}\\\\ &{\\Delta_{i,j}(E_{q}Q K^{T}(E_{k})^{T})_{t_{\\mathrm{query}}}+\\underset{i\\neq j}{\\operatorname*{max}}\\,\\Delta_{i,j}(\\mathrm{EQKE\\mathrm{\\underline{{~}}}}_{-}\\mathrm{err_{1}})_{t_{\\mathrm{query}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Then we may use any method we please to pessimize $\\Delta_{i,j}(\\mathrm{EQKE_{-}e r r_{1}})_{t_{\\mathrm{query}}}$ quickly. For example, since for any matrix $M$ we have $\\sigma_{1}(M)=\\operatorname*{sup}_{x}\\|M x\\|\\,/\\,\\|x\\|$ , considering vectors with one 1, one ", "page_idx": 49}, {"type": "image", "img_path": "2zWbzx50mH/tmp/d7779f36336d436ec1b4351d218006567d0f93399c3f78c04c1ed2c4e525f3a9.jpg", "img_caption": ["Figure 15: The error term EQKE_err for seed 123. "], "img_footnote": [], "page_idx": 50}, {"type": "text", "text": "$-1$ , and zero elsewhere, the maximum difference between elements in a row upper bounded by ${\\sqrt{2}}\\sigma_{1}(M)$ : ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\left|\\Delta_{i,j}\\mathrm{EQKE\\_err}_{1t_{\\mathrm{query}}}\\right|\\leq\\sqrt{2}\\sigma_{1}(\\mathrm{EQKE\\_err}_{1})\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "G.2.5 The complicated SVD decomposition of QK ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "While the \u201cmost mechanistic\u201d interpretation would proceed with the analysis in terms of $E_{q}$ and $E_{k}$ , perhaps decomposing them further, we can get more bang for our buck by extracting out all the low-rank structure available $E,Q$ , and $K$ , so as to make our error bounds as tight as possible. ", "page_idx": 50}, {"type": "text", "text": "To this end, we perform SVD on $E_{q}^{\\perp},E_{k}^{\\perp},$ , $Q$ , and $K$ and peel off the first singular components so as to get the decomposition ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf E_{q}=E_{q}+E_{q,2}+E_{q,2}^{\\perp}}\\\\ &{\\bar{\\mathbf E}=E_{k}+E_{k,2}+E_{k,2}^{\\perp}}\\\\ &{Q=Q_{0}+Q^{\\perp}}\\\\ &{K=K_{0}+K^{\\perp}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Then EQKE, a product of these four matrices, can be expressed as a sum of $2^{2}3^{2}\\mathrm{~-~}1\\;=\\;35$ rank one products and one high-rank error term. We can compute the sum of the rank one products in time $\\mathcal{O}(d_{\\mathrm{vocab}}{}^{2})$ and express EQKE as, say, $\\mathrm{EQKE}_{2}+E_{q,2}^{\\perp}Q^{\\perp}(E_{k,2}^{\\perp}K^{\\perp})^{T}$ . Call the second term EQKE_err (Figure 15). We must now bound for each $q$ and $m$ the quantity $\\operatorname*{max}_{i\\leq m-G}$ $\\mathrm{EQKE\\_err}[q,i]-\\mathrm{EQKE\\_err}[q,m]$ . ", "page_idx": 50}, {"type": "text", "text": "How big is this? ", "page_idx": 50}, {"type": "text", "text": "Even if we relax to $\\mathrm{max}_{i,j}$ $\\mathrm{EQKE\\_err}[q,i]-\\mathrm{EQKE\\_err}[q,j]$ , the maximum such value across all rows is under 1.85 ( $1.99\\pm0.68$ across all seeds). And the rows don\u2019t have any particular structure to them; the maximum absolute element of the entire matrix is just barely over 1 $1.12\\pm0.40$ across all seeds), so doubling that doesn\u2019t give too bad an estimate. ", "page_idx": 50}, {"type": "text", "text": "But we somehow need to compute this value without multiplying out the four matrices. ", "page_idx": 50}, {"type": "text", "text": "One option is to try to use singular value decomposition again. Since $\\sigma_{1}(M)=\\operatorname*{sup}_{x}\\|M x\\|\\,/\\,\\|x\\|$ , considering vectors with one 1, one $-1$ , and zero elsewhere, the maximum difference between elements in a row upper bounded by ${\\sqrt{2}}\\sigma_{1}(M)$ . The largest singular value of EQKE_err (Figure 16) is just under 7.6 $(8.4\\pm2.0\\$ across all seeds), giving a row-diff bound of about 10.7 $11.8\\pm2.8$ across all seeds), which is large but not unusably so. ", "page_idx": 50}, {"type": "text", "text": "If we perform SVD before multiplying out the matrices (Figure 17), however, their first singul\u221aar values are about 4, 1.4, 1.4, and 4, giving a product of about 30, which when multiplied by $\\sqrt{2}$ is about 43. (Across all seeds, these numbers are $)\\pm0.12,1.525\\pm0.067,1.513\\pm0.073.$ , and $3.78\\pm0.12$ , giving a product of about $33.1\\pm2.9$ , which when multiplied by $\\sqrt{2}$ is about $46.8\\pm4.2.)$ This works because $\\sigma_{1}(A B)\\leq\\sigma_{1}(A)\\sigma_{1}(B)$ , but note that we can do factored SVD without needing to use this technique. This bound is still usable, but pretty big. ", "page_idx": 50}, {"type": "text", "text": "Can we use Frobenius? Note that using anything close to this method to drop below $d_{\\mathrm{vocab}}d_{\\mathrm{model}}2$ might seem infeasible (it\u2019ll eventually turn out not to be). For example, the best bound we know on the largest singular value that can be verified even in the worst-case in strictly less time than it takes to compute the full SVD is the Frobenius norm, which is defined as $\\mathrm{tr}(M M^{T})$ , can be computed in $d_{\\mathrm{model}}d_{\\mathrm{vocab}}$ time, and is equal to the square root of the sum of the squares of the singular values. While the Frobenius norm of EQKE_err is only about 12 (giving a bound of about 17 on the row-diff), the Frobenius norms of the four multiplicand matrices are a bit over 10, 4, 4, and 10, giving a product of 1932 and a bound of 2732(!). (Across all seeds, the Frobenius norm of EQKE_err is about $13.1\\pm1.9$ (giving a bound of about $18.6\\pm2.7$ on the row-diff), the Frobenius norms of the four multiplicand matrices are a bit over $9.92\\pm0.19,4.43\\pm0.01,4.361\\pm0.095$ , and $9.85\\pm0.19$ , giving a product of $1888\\pm99$ and a bound of $2670\\pm140.$ ) This is unusably large. ", "page_idx": 50}, {"type": "image", "img_path": "2zWbzx50mH/tmp/4ce7bdaedd8d7b0dd145ab2216ba67279302eb4b87e78b0d9694e2f22f217445.jpg", "img_caption": ["Figure 16: SVD of EQKE_err for seed 123. "], "img_footnote": [], "page_idx": 51}, {"type": "image", "img_path": "2zWbzx50mH/tmp/1d3dc58199cab2089ac09cd24751158b4009eb547cde9681b20cff68933aa12f.jpg", "img_caption": ["Figure 17: SVD of the four component matrices of EQKE_err for seed 123. Matrices look like noise. "], "img_footnote": [], "page_idx": 51}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "However, we can get a much better bound on the max row-diff of EQKE_err without having to multiply out all four matrices. We can use an approach vaguely similar to the mean $^{+}$ diff trick, as follows. ", "page_idx": 51}, {"type": "text", "text": "If we want to compute the max row-diff of a product of matrices $A B$ , we can compute by Theorem 21 ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{r,i,j}\\left((A B)_{r,i}-(A B)_{r,j}\\right)\\leq\\operatorname*{max}_{r}\\sum_{k}\\left|A_{r,k}\\right|\\operatorname*{max}_{i,j}\\left(B_{k,i}-B_{k,j}\\right)\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "or by combining this approximation with Theorem 18 via Theorem 24 we may compute ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{max}_{r,i,j}\\left((A B)_{r,i}-(A B)_{r,j}\\right)}\\\\ {\\displaystyle}\\\\ {\\displaystyle\\leq\\left(\\operatorname*{max}_{i,j}\\sum_{k}\\mathbb{E}_{r}A_{r,k}\\left(B_{k,i}-B_{k,j}\\right)\\right)+\\operatorname*{max}_{r}\\sum_{k}\\left|A_{r,k}-\\mathbb{E}_{r}A_{r,k}\\right|\\operatorname*{max}_{i,j}\\left(B_{k,i}-B_{k,j}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "taking whichever bound is better. ", "page_idx": 51}, {"type": "text", "text": "The first gives us a bound of 7.94 on the maximum row-diff, which is better than we can get by doing SVD on the product of the matrices! We can get an even better bound by peeling off the first two singular values of all four matrices before multiplying them; this gives us a bound of 5.67. Combining it with the avg+diff trick wouldn\u2019t give us much (8.05 and 5.66 respectively), as we\u2019ve effectively already done this by peeling off the leading singular contributions; the mean of EQKE_err over dimension zero has norm 0.025 ( $0.030\\pm0.012$ across all seeds). ", "page_idx": 51}, {"type": "text", "text": "Although this error bound is no longer the leading asymptotic bottleneck, we can peek ahead to what we get if we want to be linear in parameter count. In this case, we can apply the recursive version of Equation 12 via Theorem 23, giving a bound of 97.06 on the maximum row-diff. ", "page_idx": 51}, {"type": "text", "text": "The mechanistic understanding we get here is roughly \u201cfor any given basis vector of the residual stream, the difference between the overlap of any two input tokens with this direction is small once we factor out the first two singular components\u201d, and this is sufficient to drive a low error term overall if we factor out the leading singular components in other places. We don\u2019t mechanistically understand how to combine the ${\\bf E}_{q}\\bar{Q}K^{T}$ (without multiplying them out) in a way that allows getting a good bound, though, which corresponds to our inability to drop below $d_{\\mathrm{vocab}}d_{\\mathrm{model}}2$ here. ", "page_idx": 52}, {"type": "text", "text": "If we use this trick on QK only, and use the mean $^{+}$ diff trick on final attention handling (without which we lose about $19\\,\\%$ ), we can achieve a bound of 0.7840 $(0.784\\pm0.016$ across all seeds). ", "page_idx": 52}, {"type": "text", "text": "If we use this trick on the skip connection (EU) only, we can achieve a bound of 0.6768 $(0.652{\\scriptstyle\\pm0.060}$ across all seed). ", "page_idx": 52}, {"type": "text", "text": "Using this trick on both EU and QK drops us down only to 0.6354 $(0.617\\pm0.059$ across all seeds). ", "page_idx": 52}, {"type": "text", "text": "If we use this trick on EU and use the recursive version of this trick on QK, we get a bound of 0.2927 $(0.283\\pm0.036$ across all seeds). ", "page_idx": 52}, {"type": "text", "text": "Unfortunately, it\u2019s not clear how this trick would apply to EVOU. A fancier convex hull checking algorithm seems required, and an analysis thereof is in progress. ", "page_idx": 52}, {"type": "text", "text": "G.3 The algorithm ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "We now put all of these tricks together into the subcubic algorithm Algorithm 7, which is the full version of Algorithm 6. The format we give here is parameterized over the summarization strategy (from Theorem 19 in Appendix G.1), the decomposition of EQKE, and the handling of EQKE_err and EU. ", "page_idx": 52}, {"type": "text", "text": "1: function MODEL-BEHAVIOR-RELAXED-OVER-GAP( $\\mathcal{M}$ , $t_{\\mathrm{max}}$ , tquery, c, g, g\u2217)   \nEnsure: CORRECTNESS-PESSIMIZING-OVER-GAP-SLOW is False $\\Longrightarrow$ result is False   \nRequire: $0\\leq g^{*}\\leq g\\leq t_{\\operatorname*{max}}$   \nRequire: if $c=0$ then $t_{\\mathrm{query}}=t_{\\mathrm{max}}$   \n2: 3: $\\begin{array}{r l}&{\\mathrm{skip\\!\\cdot\\!score}_{t^{*}}\\gets\\mathrm{sUM\\!\\mathsf{M}A\\!R}\\mathrm{IZ\\!E}_{\\mathbb{E}\\mathbb{U},t_{\\mathrm{query}}}\\,\\big(\\ell^{\\mathrm{-\\!\\infty}}(t_{\\mathrm{query}})t_{t^{*}}\\big)}\\\\ &{\\mathrm{skip\\!\\cdot\\!score}\\gets\\mathrm{max}_{t^{*}}\\,\\ell^{\\mathrm{EU}}\\big(t_{\\mathrm{query}}\\big)_{t^{*}}-\\mathrm{min}_{t^{*}}\\,\\ell^{\\mathrm{EU}}\\big(t_{\\mathrm{query}}\\big)_{t^{*}}}\\\\ &{v_{t}\\gets\\mathrm{EVOU}(t)}\\\\ &{w_{i}\\gets\\mathrm{PVOU}(i)}\\\\ &{\\Delta w_{\\mathrm{max},t^{*}}\\gets\\operatorname*{max}_{i}w_{i,t^{*}}-w_{i,t_{\\mathrm{max}}}}\\\\ &{\\Delta w_{\\mathrm{max},\\mathrm{max}}\\gets\\operatorname*{max}_{t^{*}}\\Delta w_{\\mathrm{max},t^{*}}}\\\\ &{\\Delta v_{t}\\gets\\operatorname*{max}_{t^{*}}v_{t,t^{*}}-\\operatorname*{min}_{t^{*}}v_{t,t^{*}}}\\\\ &{\\Delta v_{\\mathrm{max}}\\gets\\operatorname*{max}_{0\\leq t\\leq t_{\\mathrm{max}}-g^{*}}\\Delta v_{t}}\\\\ &{\\Delta v_{t^{*}}^{t_{\\mathrm{max}}}\\gets v_{\\mathrm{\\{max},t^{*}}}\\,-v_{t_{\\mathrm{max}},t_{\\mathrm{max}}}}\\\\ &{\\Delta v_{\\mathrm{max}}^{t_{\\mathrm{max}}}\\gets\\operatorname*{max}_{t^{*}\\neq t_{\\mathrm{max}}}\\Delta v_{t_{\\mathrm{max}}}}\\end{array}$ \u25b7Cache by tquery \u25b7Cache by $t^{*}$   \n4:   \n5:   \n6: \u25b7Cache by $t_{\\mathrm{max}}$ , $t^{*}$   \n7: $\\triangleright$ Cache by $t_{\\mathrm{max}}$   \n8: $\\triangleright$ Cache by $t$   \n9: \u25b7Cache by $t_{\\operatorname*{max}}-g^{*}$   \n10: $\\triangleright$ Cache by $t_{\\mathrm{max}}$   \n11: \u25b7Cache by tmax   \n12: if c = 0 then   \n13: $\\begin{array}{r l}&{\\ell_{t^{*}}\\gets\\ell^{\\mathrm{EU}}(t_{\\operatorname*{max}})_{t^{*}}+v_{t_{\\operatorname*{max}},t^{*}}+\\Delta w_{\\operatorname*{max},t^{*}}}\\\\ &{\\mathbf{return}\\operatorname*{max}_{t^{*}\\neq t_{\\operatorname*{max}}}\\left(\\ell_{t^{*}}-\\ell_{t_{\\operatorname*{max}}}\\right)}\\end{array}$   \n14:   \n15: end if   \n16: $b_{:,n_{\\mathrm{ctx}}-1}\\gets\\mathrm{EQKP}(t_{\\mathrm{query}},n_{\\mathrm{ctx}}-1)/\\sqrt{d}$ \u25b7Cache by tquery   \n17: b0,:\u22121 \u2190SORT(EQKP(tquery, : \u22121))/ d $\\triangleright$ Cache by $t_{\\mathrm{query}}$ , $i$   \n18: b1,:\u22121 \u2190REVERSE(b0,:\u22121)   \n19: EQKE(1) $\\mathrm{',EQKE_{-}e r r\\leftarrow D E C O M P O S E(E Q K E)}$   \nRequ $\\mathbf{ire:}\\mathrm{\\EQKE}^{(1)}(t_{\\mathrm{query}},t)-\\mathrm{EQKE}^{(1)}(t_{\\mathrm{query}},t_{\\mathrm{max}})-\\mathrm{EQKE}_{-}\\mathrm{err}_{t_{\\mathrm{ouery}}}\\leq\\mathrm{EQKE}(t_{\\mathrm{query}},t)-\\mathrm{EQKE}^{(1)}(t_{\\mathrm{query}},t_{\\mathrm{max}})$   \n$\\mathrm{EQKE}(t_{\\mathrm{query}},t_{\\mathrm{max}})\\leq\\mathrm{EQKE}^{(1)}(t_{\\mathrm{query}},t)-\\mathrm{EQKE}^{(1)}(t_{\\mathrm{query}},t_{\\mathrm{max}})+\\mathrm{EQKE}_{-}\\mathrm{err}_{t_{\\mathrm{query}}}$   \n20: $a_{t}\\gets\\mathrm{EQKE}^{(1)}(t_{\\mathrm{query}},t)/\\sqrt{d}$ \u25b7Cache by tquery, $t$   \n21: $a_{\\mathrm{min},t}\\gets\\operatorname*{min}_{0\\leq t^{\\prime\\prime}\\leq t}a_{t^{\\prime\\prime}}$ \u25b7Cache by $\\ensuremath{t_{\\mathrm{query}}},$ $t$ , compute in amortized $\\mathcal{O}(d_{\\mathrm{vocab}}{}^{2})$   \n22: $a_{\\mathrm{max},t}\\gets\\operatorname*{max}_{0\\leq t^{\\prime\\prime}\\leq t}a_{t^{\\prime\\prime}}$ $\\triangleright$ Cache by $t_{\\mathrm{query}},t$ , compute in amortized $\\mathcal{O}(d_{\\mathrm{vocab}}{}^{2})$   \n23: $\\Delta a_{\\mathrm{max}}\\leftarrow a_{t_{\\mathrm{max}}}-a_{\\mathrm{min},t_{\\mathrm{max}}-g}+\\mathrm{EQKE}_{-}\\mathrm{err}_{t_{\\mathrm{querv}}}$ $\\triangleright$ Cache by tquery, $t_{\\mathrm{max}}$ , c   \n24: $\\Delta a_{\\mathrm{min}}\\leftarrow a_{t_{\\mathrm{max}}}-a_{\\mathrm{max},t_{\\mathrm{max}}-g}-\\mathrm{EQKE}_{-}\\mathrm{err}_{t_{\\mathrm{ouerv}}}$ EQKE_errtquery $\\triangleright$ Cache by $t_{\\mathrm{query}}$ , $t_{\\mathrm{max}}$ , $c$   \n25: idx-s ${\\mathfrak{q}}\\gets\\{0,\\dots,n_{\\mathrm{ctx}}-c-1\\}$ if $t_{\\mathrm{max}}\\neq t_{\\mathrm{query}}$ else $\\left\\{0,\\dots,n_{\\mathrm{ctx}}-c-2,n_{\\mathrm{ctx}}-1\\right\\}$   \n26: attn-weights-unscaled $_{0,i}\\gets b_{0,i}+(\\Delta a_{\\operatorname*{min}}$ if $i\\in$ idx-set else 0)   \n27: attn-weights-unscaled $\\cdot_{1,i}\\gets b_{1,i}+(\\Delta a_{\\operatorname*{max}}$ if $i\\in$ idx-set else 0) \u25b7Cache by $t_{\\mathrm{query}},\\,t_{\\mathrm{max}},\\,i,$ ,   \n$c$   \n28: attn-weight $\\mathrm{{s_{0}}\\gets\\mathrm{{SOFTMAX}}}$ (attn-weights-unscaled0) $\\triangleright$ Cache by tquery, $t_{\\mathrm{max}}$ , i, c   \n29: attn-weight $\\mathrm{\\Delta_{S_{1}}}\\gets\\mathrm{SOFTMAX}.$ (attn-weights-unscaled1) $\\triangleright$ Cache by $\\ensuremath{t_{\\mathrm{query}}}$ , $t_{\\mathrm{max}}$ , i, c   \n30: attn- $\\begin{array}{r}{\\operatorname*{max}_{0}\\leftarrow\\sum_{i\\in\\mathrm{idx-set}}}\\end{array}$ attn-weights $_{0,i}$   \n31: attn- $\\begin{array}{r}{\\cdot\\mathrm{max}_{1}\\gets\\sum_{i\\in\\mathrm{idx-set}}}\\end{array}$ attn-weights $^{1,i}$   \n32: attn-ma $\\mathtt{X}\\gets\\mathsf{a t t m}.$ -max0 if $\\Delta v_{\\operatorname*{max}}^{t_{\\operatorname*{max}}}\\ge\\Delta v_{\\operatorname*{max}}$ else attn-max1   \n33: $\\overline{{\\mathrm{attn-max}}}\\gets\\mathrm{SUMMARIZE}_{\\mathrm{attn},t_{\\mathrm{query}}}$ (attn-max) $\\triangleright$ Cache by $t_{\\mathrm{max}}$ , $c$   \n34: attn-max $^\\prime\\gets$ attn-max \u2212attn-max   \n35: $\\begin{array}{r}{\\mathrm{summary}_{t^{*}}\\gets\\Delta w_{\\mathrm{max},t^{*}}+\\overline{{\\mathrm{skip}\\mathrm{-}\\mathrm{score}_{t^{*}}}}+\\overline{{\\mathrm{attn}\\mathrm{-}\\mathrm{max}}}\\Delta v_{t^{*}}^{t_{\\mathrm{max}}}+(1-\\overline{{\\mathrm{attn}\\mathrm{-}\\mathrm{max}}})\\Delta v_{\\mathrm{max}}}\\end{array}$ \u25b7   \nCache by tmax, $t^{*}$   \n36: $\\begin{array}{r}{\\mathbf{return}\\;\\widehat{\\mathrm{skip}}\\mathrm{-score}+\\mathrm{att}\\mathrm{n-max^{\\prime}}\\cdot\\Delta v_{\\mathrm{max}}^{t_{\\mathrm{max}}}+(1-\\mathrm{attn-max^{\\prime}})\\Delta v_{\\mathrm{max}}+\\mathrm{max}_{t^{*}}\\;\\mathrm{summary}_{t^{*}}}\\end{array}$   \n37: end function ", "page_idx": 53}, {"type": "text", "text": "H Comparison of proof strategies ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "In this section, we compare the various proof strategies that we have developed in Appendix G. We do some traditional mechanistic interpretability analysis to justify that the choices that we made could be expected to lead to reasonably good bounds in Appendix H.1. We then compare the complexities and performance of various proof strategies in Appendix H.2 to line up with the legends of Figures 3, and 4. We close with a figure relating the various categories of proof strategies. ", "page_idx": 54}, {"type": "text", "text": "H.1 Justification of pessimization choices ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "In Sections 4.3, F, and G we make a number of choices about which axes of variation are more or less important to track at various points in the bound computation. ", "page_idx": 54}, {"type": "text", "text": "Here we do some more traditional mechanistic interpretability analysis to justify that the choices that we made could be expected to lead to reasonably good bounds. ", "page_idx": 54}, {"type": "text", "text": "H.1.1 Justifying the gap ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "We take advantage of the fact that attention is mostly monotonically increasing in input integers and that for most sequences, the attentional contribution of the particular query token matters much more than the particular non-max token in the sequence. ", "page_idx": 54}, {"type": "text", "text": "We justify this as follows. ", "page_idx": 54}, {"type": "text", "text": "We can look at the typical diff, when attending to the max token, between the largest non-max logit and the max logit. As shown in Figure 18a, the largest difference between an off-diagonal entry of EVOU and the diagonal of that row is typically at most $-7$ .24 The typical worst contribution to the wrong logit from a non-max token (this is typical over non-max tokens, worst over choice of output token-logit index) is around 43, as shown in Figure 18b. ", "page_idx": 54}, {"type": "text", "text": "The difference in attention between tokens is approximately linear in the gap between the tokens, as seen in Figure 19. The slope of the line, that is, the difference in pre-softmax attention scores divided by the gap between the key token and the max token, is approximately 1.2. ", "page_idx": 54}, {"type": "text", "text": "Exponentiating, the post-softmax attention paid to the max is typically about $3\\times$ larger than to the token one below the max; here the logit difference between the max and non-max token is significant, typically being around 13 (43/3) for the worst output logit. But by the time the gap is 3, this difference has dropped to about 1.1, and by the time the gap is 4 it is around 0.3. ", "page_idx": 54}, {"type": "image", "img_path": "2zWbzx50mH/tmp/f725e67785792e9ae1559e39ada86d9da8f67980464b8e38022c1c98c0027bb0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 54}, {"type": "image", "img_path": "2zWbzx50mH/tmp/09c5ea41e4f4594b18e6694056791451e0a50139063ef09a1235b0c79b8b8a7f.jpg", "img_caption": ["(a) The attention computation weighted by the number (b) Histogram of the maximum difference between of sequences with the particular max. The computation two logits contributed by a single row of EVOU. is $\\mathrm{max}_{j}$ s.t. $j\\neq i$ $\\mathrm{EVOU}_{i,j}-\\mathrm{EVOU}_{i,i}$ . $\\mu\\pm\\sigma$ : $-9.9\\pm$ The computation is, for each $i$ , $\\mathrm{\\Deltamax}_{h}\\mathrm{\\EVOU}_{i,j}\\mathrm{\\Delta}-$ 2.1; range: $(-0\\pm120)\\times10^{-1}$ $\\operatorname*{min}_{j}\\mathrm{\\EVOU}_{i,j}$ . $\\mu\\pm\\sigma$ : $43.4\\pm9.0$ ; range: $52\\pm20$ ", "Figure 18: Plots of the difference in logit for the attention computation, $E\\mathrm{VOU}:=\\bar{\\mathbf{E}}V O U$ for seed 123. "], "img_footnote": [], "page_idx": 54}, {"type": "image", "img_path": "2zWbzx50mH/tmp/3b8d3c31bf7ad718b8d65fc72ca6e67956296f44b51d7859956289969725e687.jpg", "img_caption": ["Figure 19: Plots of attention difference vs. token gap, for $\\mathrm{EQKE:}=\\mathbf{E}_{q}Q K^{T}\\bar{\\mathbf{E}}^{T}$ for seed 123. The difference in attention between tokens is approximately linear in the gap between the tokens. "], "img_footnote": [], "page_idx": 55}, {"type": "image", "img_path": "2zWbzx50mH/tmp/efdb3675ecb880995f161b031bb39400547f6e60f279429518584daaebdbfb4a.jpg", "img_caption": ["Figure 20: Histogram of the minimum gap between the max token and the largest non-max token, for the seed 123. "], "img_footnote": [], "page_idx": 55}, {"type": "text", "text": "So for sequences where the largest non-max and the max are close together, the particular structure of the non-max EVOU matters a lot; but when the max is separated from the largest non-max by a modest gap, the structure of the non-max EVOU does not matter so much. ", "page_idx": 55}, {"type": "text", "text": "The upshot is that to handle most sequences, we need only ask an oracle for the minimum gap $g>0$ between the max token $t_{\\mathrm{max}}$ and largest non-max tokens $t^{\\prime}\\neq t_{\\mathrm{max}}$ , such that the model outputs the correct answer for all sequences where the non-max, non-query tokens have value at most $t_{\\operatorname*{max}}-g$ . ", "page_idx": 55}, {"type": "text", "text": "While computing this gap may be expensive (and indeed the na\u00efve computation of the oracle takes longer than the brute-force proof\u2014though it should be very easy to optimize), we don\u2019t have to pay the cost of computing the gap in the size of the proof, only the cost of storing the gap table $\\bar{(O(d_{\\mathrm{vocab}})^{2}n_{\\mathrm{ctx}}))}$ and of verifying the gap. Empirically, gaps are typically 1\u20135, as seen in Figure 20. ", "page_idx": 55}, {"type": "text", "text": "If we rely on the gaps, this results in leaving behind about $6.9\\,\\%$ of sequences. ", "page_idx": 55}, {"type": "text", "text": "Picking up more sequences In this paragraph / bulleted list, we sketch out how we might go about picking up more sequences to get a tighter bound. This is not coded up, and is left as future work. We propose computing the following quantities: ", "page_idx": 55}, {"type": "text", "text": "\u2022 First, we could build in time $(O(d_{\\mathrm{vocab}}{}^{2}))$ ) a table indexed on pairs $(t,t_{\\mathrm{max}})$ of the maximum token and a non-maximum token: the table would store pessimal logit contributions from $t$ to maximum output tokens $\\leq$ the $t_{\\mathrm{max}}$ parameter. The table could be further split to pessimize separately for tokens within and outside of the gap window.   \n\u2022 Compute a table of pre-softmax attention differences between tokens $t$ and $t+1$ in time $(O(\\bar{d_{\\mathrm{vocab}}}^{2}))$ ).   \n\u2022 Next sort the queries by overlap with the query direction.   \n\u2022 Compute for each number of queries handled (where we assume we handle all queries with greater overlap than the current one) and for each maximum input token $t_{\\mathrm{max}}$ , how many of the query tokens $\\ensuremath{t_{\\mathrm{query}}}$ fall strictly below the max $t_{\\mathrm{max}}$ (and whether or not the model succeeds when $t_{\\mathrm{max}}=t_{\\mathrm{query}}.$ ). This will tell us how many query tokens we can count for a given maximum token.   \n\u2022 Compute a table indexed on pairs of $\\#$ of queries handled and input tokens $t$ which stores the smallest difference in more attention paid to $t+1$ than to $t$ $(\\dot{O}(d_{\\mathrm{vocab}}{}^{2}))$ ).   \n\u2022 Compute a table indexed on pairs $t_{\\mathrm{max}}$ , $t$ storing an upper bound on amount more attention paid to non-maximum tokens than to $t_{\\mathrm{max}}$ by Oracle-permitted query tokens (the Oracle is indexed only on $t_{\\mathrm{max}}$ ) $(O(d_{\\mathrm{vocab}}2))$ ).   \n\u2022 For each # queries permitted: compute for each $t_{\\operatorname*{max}},\\,t,\\,c,$ if the non-maximum token $t$ contributes little enough to incorrect logits that even with the worst skip connection the model still gets the correct answer. ", "page_idx": 55}, {"type": "image", "img_path": "2zWbzx50mH/tmp/70deddb4eade357ea45898cfa2bce61929b57ac704f846689e9204d887f9f7e4.jpg", "img_caption": ["Figure 21: The distribution of entries of the four residual matrices (after removing two principal components from $\\mathbf{E}_{q}$ and $\\bar{\\bf E}$ and one principal component from $Q$ and $K$ ). Distributions look pretty close to normal. Plots are for the seed 123. "], "img_footnote": [], "page_idx": 56}, {"type": "text", "text": "", "page_idx": 56}, {"type": "text", "text": "H.1.2 Stopping after 1\u20132 principal components of QK ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Did we miss out on any structure in the error term of EQKE? The distribution of entries of the four matrices looks pretty close to normal as seen in Figure 21. ", "page_idx": 56}, {"type": "image", "img_path": "2zWbzx50mH/tmp/d78e2a79e7a82c964b69602ae4f3ae4658fc7833ce163a8bb2df7df4d494922a.jpg", "img_caption": ["Figure 22: Recreations of Figure 3 for ease of viewing of the legend. Top is a strict recreation; bottom includes points not on the Pareto frontier. "], "img_footnote": [], "page_idx": 57}, {"type": "text", "text": "If we replace the entries of $E_{q,2}^{\\perp}$ , $E_{k,2}^{\\perp}$ , $Q^{\\perp}$ , and $K^{\\perp}$ with randomly sampled values, we get (sample size 100) that the maximum row-diff of the product of the matrices is approximately $1.31\\pm0.13$ (sampling without replacement from the empirical distribution) or $1.31\\pm0.14$ (sampling from the normal distribution). So in fact our max row-diff is unusually high (by about $4\\sigma$ ).25 ", "page_idx": 57}, {"type": "text", "text": "H.2 How various combinations of tricks perform ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Recall Figures 3 and 4 on page 8 and on page 9 from Section 5, recapitulated here without captions for convenience as Figures 22, and 23. ", "page_idx": 57}, {"type": "text", "text": "We describe what each subcubic proof strategy in the legend means. Note that all subcubic proof strategies (that is, all proof strategies except for \u201cbrute force\u201d and \u201ccubic\u201d) use the quadratic counting algorithm of Appendix F. ", "page_idx": 57}, {"type": "text", "text": "H.2.1 Proof strategies grouped by complexity ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "In Figures 3, and 22, proof strategies are grouped by computational complexity. ", "page_idx": 57}, {"type": "text", "text": "The 102 proof strategies break down into $1+1+2\\times5\\times10\\times2$ strategies. ", "page_idx": 57}, {"type": "text", "text": "The brute force and cubic proofs $(1+1)$ were fully covered in Appendices D, and E. ", "page_idx": 57}, {"type": "text", "text": "There are 5 options for handling EU: ", "page_idx": 57}, {"type": "text", "text": "direct-quadratic refers to handling EU in time $\\mathcal{O}(d_{\\mathrm{vocab}}d_{\\mathrm{model}})$ with either the max row-diff trick (Appendix ${\\mathrm{G}}.2.2)^{26}$ or the max row-diff trick fused with mean $^{+}$ diff or some other summary statistic (Theorem $24)^{27}$ . ", "page_idx": 57}, {"type": "text", "text": "When direct is not mentioned, this indicates that we handle EU in time $\\mathcal{O}(d_{\\mathrm{vocab}}{}^{2}d_{\\mathrm{model}})$ by first multiplying out ${\\bf E}_{q}U$ and then either taking the maximum row-diff in each row28 or by taking the maximum row-diff across all rows29. The latter is included purely for comparison\u2019s sake, and never gives a tighter bound than the former. ", "page_idx": 57}, {"type": "text", "text": "There are 10 options for handling the high-rank attention error term EQKE_err: ", "page_idx": 57}, {"type": "image", "img_path": "2zWbzx50mH/tmp/5fdc99f93a5f534490709367f3a229308e9610216e62d04dd8638da35b7e4b84.jpg", "img_caption": ["Figure 23: Recreation of Figure 4 for ease of viewing of the legend. "], "img_footnote": [], "page_idx": 58}, {"type": "text", "text": "attention-quadratic refers to handling the high-rank attention error term EQKE_err from Appendix G.2.5 in time $\\mathcal{O}(d_{\\mathrm{vocab}}d_{\\mathrm{model}})$ either with the recursive max row-diff trick (Theorem 23)30 or with the recursive max row-diff trick fused with the mean+diff trick either just on the query side31 or throughout32 (Theorem 26). ", "page_idx": 58}, {"type": "text", "text": "attention- $d_{\\mathrm{vocab}}d_{\\mathrm{model}}2$ indicates that we use one of the various $\\mathcal{O}(d_{\\mathrm{vocab}}d_{\\mathrm{model}}{}^{2})$ strategies for handlin\u221ag $\\mathrm{EQKE_{-}e r r_{1}}$ from Appendix G.2.4 or EQKE_err from Appendix G.2.5. These include using $\\sqrt{2}\\sigma_{1}$ \u2014computed via low-rank SVD\u2014as the bound (Equation $11)^{33}$ , considering all ways of multiplying out a subset of the matrices and taking the maximum row-diff of the resulting pair of matrices34 (Theorem 21), or fusing the max row-diff trick with the mean $^+$ diff trick35 (Theorem 24). ", "page_idx": 58}, {"type": "text", "text": "When attention is not mentioned, this indicates that we handle the attention error term in time $\\mathcal{O}(d_{\\mathrm{vocab}}{}^{2}d_{\\mathrm{model}})$ , either by taking the per-row maximum row-diff36 or by using the full rank EQKE matrix and taking the per-row maximum row diff37. ", "page_idx": 58}, {"type": "text", "text": "Finally, note that in combining the rank one attention computation with EVOU, PVOU, and EU, we may either use the mean+diff trick38 (Appendix G.1) or not39; this makes up the final factor of 2. ", "page_idx": 58}, {"type": "text", "text": "H.2.2 Proof strategies grouped by attention handling ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "This section slightly reorganizes the information just covered in Appendix H.2.1, for convenience of legend correspondence. Here we group by the strategy used to handle the attention error term. Strategies that involve using the full rank EQKE matrix are elided. The dashed descriptors here correspond to underscore-joined descriptors in footnotes of Appendix H.2.1. ", "page_idx": 58}, {"type": "text", "text": "max-diff-exact $(\\mathcal{O}(d_{\\mathrm{vocab}}{}^{2}d_{\\mathrm{model}}))$ corresponds to taking the full rank EQKE_ $\\mathrm{err}_{1}$ term and taking the maximum row-diff in each row. ", "page_idx": 58}, {"type": "text", "text": "mean+max-diff-subproduct $(\\mathcal{O}(d_{\\mathrm{vocab}}d_{\\mathrm{model}}))$ ) corresponds to fusing the max row-diff trick with the mean+diff trick (Theorem 24) and considering all ways of associating the multiplication of EQKE_err. ", "page_idx": 58}, {"type": "text", "text": "max-diff-subproduct $\\langle\\mathcal{O}(d_{\\mathrm{vocab}}d_{\\mathrm{model}})\\rangle$ ) corresponds to using the max row-diff trick (Theorem 21) and considering all ways of associating the multiplication of EQKE_err. ", "page_idx": 58}, {"type": "text", "text": "max-diff $(O(d_{\\mathrm{vocab}}d_{\\mathrm{model}}{}^{2}))$ corresponds to using the max row-diff trick (Theorem 21) on the factored SVD of $\\mathrm{EQKE_{-}e r r_{1}}$ . ", "page_idx": 58}, {"type": "text", "text": "mean+max-diff $\\mathcal{O}(d_{\\mathrm{vocab}}d_{\\mathrm{model}}{}^{2}))$ corresponds to fusing the max row-diff trick with the mean+diff trick (Theorem 24) and applying it on the factored SVD of $\\mathrm{EQKE_{-}e r r_{1}}$ . ", "page_idx": 58}, {"type": "text", "text": "svd $\\langle{\\mathcal O}(d_{\\mathrm{vocab}}d_{\\mathrm{model}}{}^{2})\\rangle$ corresponds to using $\\sqrt{2}\\sigma_{1}$ \u2014computed via low-rank SVD\u2014as the bound (Equation 11). ", "page_idx": 59}, {"type": "text", "text": "mean+max-diff-subproduct-recursive $\\langle O(d_{\\mathrm{vocab}}d_{\\mathrm{model}}))$ corresponds to handling the high-rank attention error term EQKE_err from Appendix G.2.5 with the recursive max row-diff trick fused with the mean $^{+}$ diff trick on the query-side only (Theorem 26, taking all but the first summary vector to be zero). ", "page_idx": 59}, {"type": "text", "text": "max-diff-subproduct-recursive $(\\mathcal{O}(d_{\\mathrm{vocab}}d_{\\mathrm{model}}))$ corresponds to handling the high-rank attention error term EQKE_err from Appendix G.2.5 with the recursive max row-diff trick (Theorem 23). ", "page_idx": 59}, {"type": "text", "text": "mean-recursive+max-diff-subproduct-recursive $(\\mathcal{O}(d_{\\mathrm{vocab}}d_{\\mathrm{model}}))$ ) corresponds to handling the high-rank attention error term EQKE_err from Appendix G.2.5 with the recursive max row-diff trick recursively fused with the mean+diff trick (Theorem 26). ", "page_idx": 59}, {"type": "text", "text": "H.2.3 What understanding do we get from each proof strategy? ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Throughout most of this paper, we talk about doing mechanistic interpretability and using understanding to allow more compact proofs to have tighter bounds. We can also look at the reverse problem: we can take a collection of proof strategies, check by brute force which strategies give the tightest bounds for each model, and ask what this implies about how that model works. We do this here. ", "page_idx": 59}, {"type": "text", "text": "In general, which proof methods perform best is an indication of where structure exists in the model. For example, in quadratic EU proofs, when max_diff performs worse than mean_query+max_diff and svd_query+max_diff, this indicates that $E$ has a relatively strong behavioral component shared across query tokens that $U$ is not that good at filtering out. Similarly, when, e.g., mean_recursive+max_diff_subproduct_recursive performs better than max_diff_subproduct_recursive, this indicates that even after removing the first one or two principle components from $\\mathbf{E}_{q}$ , $Q,K$ , and $\\bar{\\bf E}$ , there is still enough common structure that it is worth factoring out the mean behavior. ", "page_idx": 59}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: We present the key challenges in the field of formal verification for neural networks, and describe our proposed solution. Then we summarize our experimental setup and results. ", "page_idx": 60}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: See Section 7. ", "page_idx": 60}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: We provide intuitions for our proof constructions in the main body of the paper. In the supplemental material, we lay out in full detail theorem statements and proofs. Along with this, we provide algorithms and plots that are useful for understanding how the proofs were constructed. ", "page_idx": 60}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: We directly link to a codebase with the specific implementation of our case study. ", "page_idx": 60}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: In supplementary materials, we provide the full details of our model training set up. In the main body of the paper, we describe our experimental setting and provide reasoning for why we chose this experimental setup as a very simple case study of our theoretical work. ", "page_idx": 60}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: We run our experiment on models trained with 151 different hyperparameters. For most reported computations, we provide statistical significance information. We do not need to perform explicit t-tests or such since it is not relevant to our setup. ", "page_idx": 60}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] Justification: See Appendix B.1. ", "page_idx": 61}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: We confirm that we have read the Code of Ethics. Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 61}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 61}, {"type": "text", "text": "Justification: This paper seeks to advance the fields of mechanistic interpretability and formal verification of machine learning systems. While there are many indirect societal consequences of our work through the impacts on these fields, we feel that none are sufficiently consequential as to be highlighted here. ", "page_idx": 61}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 61}, {"type": "text", "text": "Justification: This paper does not involve any such assets. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: We list all important Python packages used in the paper in Appendix B.1. ", "page_idx": 61}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: We introduce no new assets except for the codebase needed to reproduce our experiments, which does contain appropriate documentation. ", "page_idx": 61}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 61}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 62}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 62}]