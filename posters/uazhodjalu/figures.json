[{"figure_path": "uAzhODjALU/figures/figures_3_1.jpg", "caption": "Figure 1: Transferring Transformer to Mamba. Weights, in orange, are initialized from the Transformer (Linear projections for Q, K, and V are initialized using linear projection for C, B, and X respectively). We replace individual attention heads with Mamba heads, and then finetune Mamba blocks while freezing the MLP blocks. Shapes are kept mainly the same. Weights in green are added. New parameters are introduced for the learned A and A parameters.", "description": "This figure illustrates the process of transferring weights from a Transformer model to a Mamba model.  The orange weights in the Mamba architecture are directly initialized from corresponding weights in the Transformer's attention mechanism.  The Mamba model replaces the Transformer's attention blocks, while the MLP layers remain unchanged (frozen during training).  Additional parameters (green) are introduced for the Mamba model to learn more complex functions and improve performance. The figure visually depicts this weight transfer and adaptation.", "section": "Attention-to-Mamba Initialization and Hybrid Stepwise Training"}, {"figure_path": "uAzhODjALU/figures/figures_5_1.jpg", "caption": "Figure 1: Transferring Transformer to Mamba. Weights, in orange, are initialized from the Transformer (Linear projections for Q, K, and V are initialized using linear projection for C, B, and X respectively). We replace individual attention heads with Mamba heads, and then finetune Mamba blocks while freezing the MLP blocks. Shapes are kept mainly the same. Weights in green are added. New parameters are introduced for the learned A and A parameters.", "description": "This figure illustrates the process of transferring weights from a Transformer model to a Mamba model.  Weights from the Transformer's linear projections (Q, K, V) are directly initialized into the corresponding Mamba blocks (C, B, X).  The existing Transformer MLP blocks are frozen during fine-tuning, and only new parameters (shown in green) for the Mamba layers (A) are trained. This hybrid architecture, where Mamba blocks replace Transformer attention heads, aims for efficient inference while retaining performance.", "section": "2.3 Attention-to-Mamba Initialization and Hybrid Stepwise Training"}, {"figure_path": "uAzhODjALU/figures/figures_6_1.jpg", "caption": "Figure 3: Performance of the multi-step SSM kernel for generating 32 tokens.", "description": "This figure shows the performance comparison between the multi-step and single-step SSM (State Space Model) kernels in generating 32 tokens. The x-axis represents the step size (K), and the y-axis represents the time in milliseconds.  The multi-step kernel demonstrates significantly faster generation times, especially at smaller step sizes, while the single-step kernel shows relatively constant generation time regardless of step size. This illustrates the efficiency gains achieved by the multi-step speculative decoding algorithm.", "section": "4.3 Speculation Analysis and Hardware Specific Optimization"}]