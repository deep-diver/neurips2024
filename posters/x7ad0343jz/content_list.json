[{"type": "text", "text": "Limits of Transformer Language Models on Learning to Compose Algorithms ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jonathan Thomm1,2\u2217 Giacomo Camposampiero1,2 Aleksandar Terzic1,2 jthomm@ethz.ch giacomo.camposampiero1@ibm.com aleksandar.terzic1@ibm.com ", "page_idx": 0}, {"type": "text", "text": "Michael Hersche1 Bernhard Sch\u00f6lkopf2,3 Abbas Rahimi1 michael.hersche@ibm.com bs@tuebingen.mpg.de abr@zurich.ibm.com ", "page_idx": 0}, {"type": "text", "text": "1IBM Research \u2013 Zurich, 2ETH Zurich, 3MPI T\u00fcbingen ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We analyze the capabilities of Transformer language models in learning compositional discrete tasks. To this end, we evaluate training LLaMA models and prompting GPT-4 and Gemini on four tasks demanding to learn a composition of several discrete sub-tasks. In particular, we measure how well these models can reuse primitives observable in the sub-tasks to learn the composition task. Our results indicate that compositional learning in state-of-the-art Transformer language models is highly sample inefficient: LLaMA requires more data samples than relearning all sub-tasks from scratch to learn the compositional task; in-context prompting with few samples is unreliable and fails at executing the sub-tasks or correcting the errors in multi-round code generation. Further, by leveraging complexity theory, we support these findings with a theoretical analysis focused on the sample inefficiency of gradient descent in memorizing feedforward models. We open source our code at https://github.com/IBM/ limitations-lm-algorithmic-compositional-learning. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While Large Language Models (LLMs) are known to perform well on natural language generation tasks [1, 2], they exhibit failures on reasoning [3, 4, 5, 6, 7, 8, 9, 10], mathematics [11, 12], causal inference [13, 14, 15], and algorithmic tasks [16, 17, 18]. Many interesting algorithmic tasks rely on function composition, which is an act of combining simple functions (e.g., primitive sub-tasks) to build more complicated ones. In this paper, we dive into the question of how sample-efficient Transformer-based [19] language models are when learning to compose as well as to decompose algorithmic procedures. We empirically approach the aforementioned question by analyzing the performance of Transformer language models on a set of compositional algorithmic tasks. Given that a Transformer language model has enough samples to learn all primitive sub-tasks, we define four hypotheses on how well it learns the composition task within the same training routine: ", "page_idx": 0}, {"type": "text", "text": "$\\mathcal{H}_{3}$ . A Transformer language model learns a compositional task with fewer samples than the sum of the samples needed to learn every sub-task. ", "page_idx": 1}, {"type": "text", "text": "$\\mathcal{H}_{4}$ . A Transformer language model needs more data samples than in $\\mathcal{H}_{3}$ . ", "page_idx": 1}, {"type": "text", "text": "This paper makes the following main contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 In Section 3, we introduce a family of two new algorithmic tasks with a compositional structure that is well-suited for creating systematic sub-tasks and testing compositionality. We specifically design these synthetic tasks with independently observable sub-tasks such that the composition is easily inferable from the sub-tasks but harder to learn from scratch. ", "page_idx": 1}, {"type": "text", "text": "\u2022 In Section 4, we train LLaMA models [2] on this family of the tasks as well as two tasks investigated by Dziri et al. [16]. We ensure that all necessary sub-tasks are learned and test how efficiently the models can learn their compositional re-combinations. We show that training LLaMA models from scratch fails to compose all learned sub-tasks under hypotheses $\\mathcal{H}_{2}$ and $\\mathcal{H}_{3}$ , making $\\mathcal{H}_{4}$ the most plausible hypothesis for all four tasks. Furthermore, we propose a formal bound, showing that current supervised gradient descent training fails in compositional learning in the limit. \u2022 In Section 5, we investigate GPT-4 and Gemini on all tasks and observe their failures to perform the tasks, or multi-round code generation, with task description and various chain-of-thought examples. This shows that in-context learning with few samples is unreliable and fails to compose knowledge from sub-tasks and that $\\mathcal{H}_{1}$ does not hold for the investigated tasks. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Computational graph Let $A$ be a deterministic algorithm and let ${\\mathcal{F}}_{A}$ be a set of deterministic primitive operations that can be used by $A$ during execution. Given an input $x$ , we define the computational graph for the algorithm $A$ as $G_{A(x)}=(V,E)$ . $G_{A(x)}$ is a weakly connected, directed acyclic graph. The nodes $v\\in V$ represent all intermediate variables\u2019 values during $A$ \u2019s execution, while the edges $e\\in E$ represent the function arguments involved in the computation of their target nodes. One source node $s$ describes the input $x$ , while the (single) sink node $t$ represents the output $A(x)=t$ . For every non-source node $v$ , $o p(v)\\in\\mathcal{F}_{A}$ denotes the operation applied to compute $v$ . Figure 1 shows an example with a toy input for one of the tasks (PEN) later introduced in Section 3. ", "page_idx": 1}, {"type": "text", "text": "Sub-task definition While Dziri et al. [16] define one sub-task for each operation occurring in the nodes of $G_{A(x)}$ , we relax this constraint and also consider sub-tasks composed of multiple operations from ${\\mathcal{F}}_{A}$ . However, we require every operation in the set of primitives ${\\mathcal{F}}_{A}$ to be independently observable. Given the set $\\boldsymbol{S}$ of all the defined sub-task graphs, we say that an operation $f=o p(v)$ is independently observable in a sub-task graph $G_{A\\left(x\\right)}^{\\prime}$ if either: ", "page_idx": 1}, {"type": "text", "text": "\u2022 $v$ is the only non-source node in $G_{A\\left(x\\right)}^{\\prime}$ (i.e., there is only one operation in the sub-graph). \u2022 $v$ is not the only non-source node in $G_{A\\left(x\\right)}^{\\prime}$ and all other operations in the nodes of $G_{A\\left(x\\right)}^{\\prime}$ are independently observable in the other sub-task graphs in ${\\mathcal{S}}\\backslash\\{G_{A(x)}^{\\prime}\\}$ . ", "page_idx": 1}, {"type": "text", "text": "By using this constraint on the definition of the sub-tasks, we aim to achieve a middle-ground between completely synthetic settings (where each primitive is presented in isolation) and real-world settings (where the primitives are often grouped and correlated). ", "page_idx": 1}, {"type": "image", "img_path": "x7AD0343Jz/tmp/edb07974142ee1a6ab8748a1d053e0521e0ec86312b6c18b5117f9b5f3ff3642.jpg", "img_caption": ["Figure 1: Translation of a compositional algorithmic task $A$ , PEN (see Section 3 for details), into its corresponding compositional graph $G_{A(x)}$ , for the input $x=$ \u201cab xy ab4fq wv7ql\u201d. The operations (edges) are color-matched with the respective operations in the pseudo-code of the algorithmic task $A(x)$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "3 Probing compositionality with algorithmic tasks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Pointer execution tasks are a family of algorithmic tasks initially proposed to benchmark the generalization capabilities of deep learning models [20]. They are designed to limit the number of confounders in the data and force the model to learn an algorithmic (general) solution, making solving the task with statistical shortcuts impossible. Furthermore, being algorithmic tasks, they are particularly suited for testing compositional learning, as they can be naturally decomposed into atomic operations. Hence, the introduction of this family of tasks would allow operating in a fully controlled environment, limiting the impact of exogenous factors on the empirical observations while having the possibility to stress-test compositionality. ", "page_idx": 2}, {"type": "text", "text": "Motivated by these premises, we introduce two novel pointer execution tasks for testing compositional learning, Pointer Execution\u2019s neighbor (PEN) and Pointer Execution Reverse Multicount (PERM). To ensure the validity of our empirical evaluation, we then further extend our experiments to two well-established tasks for testing compositionality in Transformer-based language models, Highest Subsequence Sum (HSS) and Multiplication (MUL) [16]. We decompose each task in a set of sub-tasks $\\boldsymbol{S}$ , such that every primitive operation of the task is independently observable in at least one sub-task. A more detailed exposition of the properties of $\\boldsymbol{S}$ is included in Appendix B.1. ", "page_idx": 2}, {"type": "text", "text": "3.1 Pointer execution\u2019s neighbor (PEN) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce the Pointer Execution\u2019s neighbor (PEN) task, where the goal is to jump between different words in the sequence according to a matching criterion while outputting the right neighbors of the matched words. This task is inspired by C-PVR, a task recently introduced by Abnar et al. [17] and itself based on the Pointer Value Retrieval task [20]. A sketch of the task is shown in Figure 2 (left). We identify three primitive operations to be left (get the green left neighbor), match (get the matching green word), and right (get the right yellow neighbor). We, therefore, split PEN into three sub-tasks that guarantee independent observability for each primitive: copy (Cpy), where the solver has to copy an input sequence of words (making the right primitive observable); reverse copy (RCpy), where the solver has to copy an input sequence of words in the reversed order (i.e. the last word first, making the left primitive observable); Pointer Execution (PE), where the solver has to match words in a sequence (making the match primitive observable). Additionally, we define the sub-task Pointer Execution Verbose (PEV) to facilitate the learning of the task. PEV requires solving the same problem as PEN with the addition of outputting both the matching words and their neighbors, making it less abstract (see Figure 2). More details on PEN can be found in Appendix B.2. ", "page_idx": 2}, {"type": "text", "text": "To make the task more challenging, we add \u201cattention traps\u201d to the input sequences. These traps add spurious matches between neighboring (yellow) tokens (two spurious matches per yellow neighbor), such that the model could be tricked into matching the wrong sequence of tokens (yellow instead of green). An example of this kind of trapping mechanism is included in Figure 2 (left column), and in a more explicit visualization in Figure B.5. The traps are not added to the main (green) tokens, where the sequence of matches is always deterministic. ", "page_idx": 2}, {"type": "text", "text": "3.2 Pointer execution reverse multicount (PERM) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce the Pointer Execution Reverse Multicount (PERM) task. This task is conceptually similar to the PEN task. However, instead of matching forward and predicting the current word (or its neighbor), the solver has to output the reversed matched sequence. To increase the difficulty of the task, we enrich it with additional operations on the indices. In particular, for each element in the matched sequence, the solver needs to collect the number of matches and the number of left matches up to that point, multiply them, and output the result. A visualization of the task is presented in Figure 2. We omit attention traps because there are no longer neighbor tokens in the input sequence. ", "page_idx": 2}, {"type": "text", "text": "We identify the primitive operations of the PERM task to be match (common to PEN), reverse (reverse a sequence), and multicount (indexes operations, which includes counting the number of matches and left matches, as well as multiplying the two counts together). The primitives left and right are no longer needed, as we drop the concept of neighbors in this task. To make every primitive independently observable and learn PERM, we formulate three sub-tasks: Pointer Execution (PE), inherited from PEN; Pointer Execution Reverse (PER), where the solver has to match a sequence and reverse it, combining the primitives match and reverse (making the latter observable); Pointer ", "page_idx": 2}, {"type": "text", "text": "Execution Multicount (PEM), where the model has to match the sequence and compute the index operations, combining the primitives match and multicount (making the latter observable). ", "page_idx": 3}, {"type": "image", "img_path": "x7AD0343Jz/tmp/45aba616b07d3cd7a7d4b347033c5fcd184c8d06ea9869efa440682e59b1cb86.jpg", "img_caption": ["Figure 2: Introduced compositional algorithmic tasks. Left: The Pointer Execution (PE)\u2019s neighbor (PEN), together with the Pointer Execution (PE) and Pointer Execution Verbose (PEV) sub-tasks. Starting left, the output is obtained by matching words and predicting the current word (in PE) or its neighbor (in PEV and PEN). Our matching criterion is that the two end characters of the current word are equal to the first two characters of the matched word. By ensuring that there are no ambiguities in the input string, an attention mechanism can find the match by retrieving the last two characters of the word and matching it with the (unique) word that starts with them. Right: The Pointer Execution Reverse Multicount (PERM), together with the Pointer Execution (PE) and Pointer Execution Reverse (PER) sub-tasks. PERM first outputs the last word in the matching sequence and then goes backward. The number in the answer for each word is the count of matches times the count of left matches (i.e., arrow to the left in the forward matching sequence). "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.3 Highest subsequence sum (HSS) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given a sequence of numbers, the Highest Subsequence Sum (HSS) task [16] consists in finding the highest sum of a number subsequence where no two numbers are neighbors. For this task, there exists a simple linear-time, constant-space dynamic programming (DP) solution which we use to generate sub-tasks. The dynamic programming recurrence is ", "page_idx": 4}, {"type": "equation", "text": "$$\nd p(i+1)=\\operatorname*{max}(d p(i-1)+\\mathrm{number}_{i+1}),d p(i)),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where numbe $_{i+1}$ is the $i+1$ -th number in the input sequence. The final answer is $d p(n)$ with $n$ being the length of the input sequence. We identify one fundamental primitive used in this task inspired by this formulation of the problem, dp_step, presented in Equation 1. Hence, we define a single sub-task, the Subsequence Sum Execution (SSE) sub-task, to make the primitive dp_step observable. In this sub-task, the solver is required to execute the DP recurrence, explicitly computing $d p(i)$ for the position $i$ in the output (including if the new number was taken or not). ", "page_idx": 4}, {"type": "text", "text": "3.4 Multiplication (MUL) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The Multiplication (MUL) task [16] involves multiplying two multi-digit numbers in base 10. It is often used to assess the symbolic and compositional capabilities of LLMs [21, 22]. Dziri et al. [16] use this task to find that LLMs do not generalize well to out-of-domain computation graph depth and width. We identify two main primitive operations: digit_mul (digit multiplication between a number in base 10 and a digit) and add (addition between numbers). We then formulate two corresponding sub-tasks to guarantee that each one of these operations is observable: digitmultiplication (MUL), where the solver has to solve multiplications between a number in base 10 and a digit (making digit_mul observable) and addition (ADD), where the solver has to add numbers in base 10 (making add observable). We do not explicitly train on shifting the numbers with zeros before adding them up. ", "page_idx": 4}, {"type": "text", "text": "4 Sample efficiency on compositional learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Training LLaMA models for testing compositionality ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we investigate compositional learning on the tasks presented in Section 3. For each one of them, we encode its multiple sub-tasks by adding unique identifiers at the beginning of the samples. We then train a LLaMA model from scratch on all sub-tasks concurrently (sampling from them uniformly at random). We use a character tokenizer and apply cross-entropy loss on all tokens, including both the input and the output sequences, as is usual in autoregressive language modeling. Computing the loss only on the answer part, as done in other works [23, 24], led to worse results in our experiments. We consider an algorithmic task to be learned only when (almost) perfect in-distribution accuracy is achieved. Any solution resulting in lower accuracy is, on the other hand, considered wrong, as it fails to fully capture the operations of the algorithm underlying the task. We conducted additional ablations on the model architecture (e.g., a LLaMa architectural variation based on Universal Transformer [17]), included in Appendix C.5. However, the unsatisfactory results deterred us from conducting further systematic analysis on it. ", "page_idx": 4}, {"type": "text", "text": "Learning the PEN task. We train LLaMA with all four sub-tasks (Cpy, RCpy, PE, PEV) and PEN. As shown in Figure 3 (detailed numbers are in Table C.2), the model successfully learns all the sub-tasks containing the primitives needed for PEN. However, it fails to properly compose them for solving the task under hypotheses $\\mathcal{H}_{2}$ (no. of samples needed for the most difficult sub-task) and $\\mathcal{H}_{3}$ (no. of samples needed for relearning all sub-tasks). Increasing the sample size to $1000\\,\\mathrm{K}$ (roughly $10\\times$ more compared to the sum of the samples needed to learn each sub-task independently) allows the model to learn the PEN task. This makes hypothesis $\\mathcal{H}_{4}$ (a Transformer language model needs more data samples than the sum of samples needed for re-learning all sub-tasks separately) the most plausible. ", "page_idx": 4}, {"type": "text", "text": "Furthermore, by training the model on PEN only (without the sub-tasks), we observe that LLaMA obtains no significant benefti from learning the sub-tasks (see w/o sub-tasks vs. w/ sub-tasks in Figure 3). Based on these results, we speculate that the sub-tasks are not properly reused, and the tasks are rather learned every time from scratch. We interpret this as evidence that the model is not learning the task compositionally. ", "page_idx": 4}, {"type": "text", "text": "To set those results into context: our dataset of $1000\\,\\mathrm{K}$ samples overall has roughly $6\\times$ more tokens than the number of words a typical 13-year-old would have heard in their life [25]. Furthermore, we can train a discrete hill-climbing learning algorithm containing 121 discrete parameters to perfectly learn the PEN task from a single sample, given the sub-tasks as primitives (see Appendix C.2). ", "page_idx": 5}, {"type": "text", "text": "We also propose an ablation with a smaller model $28\\,\\mathrm{M}$ instead of $150\\,\\mathrm{M}$ parameters, \u201caux. loss\u201d in Figure 3, and more detailed Table C.3). We add two additional prediction heads after a fraction of $^1/2$ and ${^3}/{4}$ of the layers, respectively, and train them using auxiliary losses. The first loss is used to train the first head to predict, at step $i$ , the $i$ -th (yellow) element of the answer. The second loss is used to train the second head to predict the (green) element to the left of the one predicted by the first head. The auxiliary losses model can learn PEN with $300\\,\\mathrm{K}$ samples, while the bigger LLaMA $(150\\,\\mathrm{M})$ obtains $0\\%$ accuracy with the same dataset size. However, this result also falls into the $\\mathcal{H}_{4}$ hypothesis. ", "page_idx": 5}, {"type": "text", "text": "Learning the PERM task. We train LLaMA models on the PERM task together with its sub-tasks (PE, PER, and PEM). The training setting used to train the model on this task is identical to the one used for PEN. As reported in Figure 3 (and with more detail in Table C.4), LLaMA can learn with perfect accuracy all the individual sub-tasks but fails to learn their composition (PERM) within hypotheses $\\mathcal{H}_{2}$ , and $\\mathcal{H}_{3}$ . To achieve perfect accuracy, we need to scale up the dataset size by almost one order of magnitude compared to the $\\mathcal{H}_{3}$ setting (the required number of training samples is equal to the sum of the training samples of all the sub-task datasets). This provides evidence that hypothesis $\\mathcal{H}_{4}$ is the most plausible for PERM too. As for PEN, we study the impact of providing the sub-tasks together with the main task during training. Comparing the performance of LLaMA when given the sub-tasks and not (cf. Figure 3 top right), we observe that LLaMA seems to not benefti significantly from the sub-tasks. We speculate that also in this case this is a sign that the model is not re-using the primitives learned from the sub-tasks to solve the compositional task. ", "page_idx": 5}, {"type": "text", "text": "Learning the HSS task and MUL tasks. We investigate HSS and MUL (related to [16]). Figure 3 (bottom row) visualizes the results. As with our PEN and PERM tasks above, we observe that LLaMA does not exhibit strong compositional learning and learns under hypothesis $\\mathcal{H}_{4}$ . Also for these tasks, it is possible to see that learning the sub-tasks along with the composition task did not significantly improve the performance. ", "page_idx": 5}, {"type": "image", "img_path": "x7AD0343Jz/tmp/f7d8437670eb1e46d6e60bcbe6c0ebf0c2cab4122febaebebe15a82392e70a17.jpg", "img_caption": ["Figure 3: Accuracy of LLaMA models on PEN, PERM, HSS, and MUL, and their respective sub-tasks. While LLaMa achieves perfect accuracy on all the individual sub-tasks, it needs much larger amounts of training data to learn their composition. This observation makes hypothesis $\\mathcal{H}_{2}$ (learning the composition requires less samples than the hardest sub-task, green) and $\\mathcal{H}_{3}$ (learning the composition requires less samples than the sum of the sub-tasks, yellow) impossible to achieve on every task, where LLaMa seems to always fall into $\\mathcal{H}_{4}$ (learning the composition requires more samples than the sum of the sub-tasks, red). Moreover, training together with the sub-tasks does not seem to perform sensibly better than training only on the main task (i.e., w/o sub-tasks). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.2 Testing decomposition by learning sub-tasks from a pre-trained LLaMA model ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We additionally evaluate LLaMA\u2019s abilities on decompositionality, i.e., learning a sub-task like PEV after being trained only on the full compositional task (PEN). We observe that, when learning the decomposition task, the models benefit from being previously pre-trained on the composition task (see Table C.10). However, to get maximum accuracy on decomposing PEV from PEN, we need between $25\\,\\mathrm{K}$ and $50\\,\\mathrm{K}$ samples to fine-tune a PEN-pretrained model. This is a significant fraction of the data to learn the PEV task from scratch, making $\\mathcal{H}_{1}$ also implausible for decompositionality on the PEN task. Similar observations hold for decomposing PE from PERM (i.e., a PERM-pretrained model fine-tuned on the PE task; see Table C.11). We speculate that this might be an indication that the model does learn operations relevant for the sub-tasks when trained only on the composite task (since learning the decomposition gets easier), and the learned solution might be to some extent compositional, even if the learning process is not compositional (and very inefficient). ", "page_idx": 6}, {"type": "text", "text": "4.3 Asymptotic limitations on compositional learning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In general, compositional learning can be split into two skills: (1) identifying and learning the primitives from the sub-tasks, and (2) assembling the learned primitives in the correct way to construct the solution to the composition task. In Appendix A we focus on (2) and enrich our empirical results with a theoretical perspective; a gist of it is included in the following paragraphs. ", "page_idx": 6}, {"type": "text", "text": "Pre-training LLMs is sample inefficient in compositional learning. We prove that asymptotically, meaning from an unknown size onwards, feedforward models on gradient descent are data inefficient on combinatorial (including compositional) problems. We use a reduction over the required asymptotic compute time to prove the following result. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1 (Informal Theorem A.7). There exist (many) series of train-test datasets, such that feedforward models on gradient descent that memorize samples will need $O(n^{k})$ times more data than an optimal learner. Otherwise, they will not generalize to the test set. $k$ is any positive number and n is the index in the series. ", "page_idx": 6}, {"type": "text", "text": "Proof sketch. Many combinatorial problems are assumed to be hard to solve. $N P$ hard problems provide an extreme case, where under the $P\\neq N P$ assumption the solution algorithm requires more than polynomial time. By constructing a train and test dataset where the problem instance of a combinatorial problem (e.g., SAT) is in the training set and the solution can easily be extracted from the labels of the test set, we can make the following reduction: if the learning algorithm finishes in polynomial time, then it cannot answer the test samples correctly, as this would build an algorithm which can solve the problem faster than possible. If our learning algorithm, however, tends to memorize the training set fast and, after memorization, training does not improve model test performance anymore, we can conclude that many samples will be needed to be able to generalize to the test set. At the same time, we can simulate a slow but data-efficient algorithm that extracts the problem instance from the training data, computes the answer, and naturally generalizes it to the test set. The same reduction applies to non- $.N P$ -hard problems, in which case the $k$ in Theorem 4.1 becomes bounded. While the classical reduction requires an out-of-distribution test set, the reduction can be made in the in-distribution case as well using hardness assumptions from public key cryptography (shown in Theorem A.9). ", "page_idx": 6}, {"type": "text", "text": "5 Compositional learning via in-context prompting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we investigate the performance of pre-trained LLMs, GPT-4 and Gemini-Pro, on the PEN and PERM tasks with various prompting methods. Concretely, we operate under the assumption that the models have learned to execute a sufficient set of operations during pre-training, and we test whether these latent abilities can be utilized to accurately solve the tasks given prompts of various complexitites. We experiment with a wide range of prompts, ranging from simple few-shot examples to state-of-the-art methods [26, 27, 28]. Besides the task test accuracy, we also report two partial correctness metrics, match accuracy and termination accuracy. Match accuracy measures how many steps in the output were a correct left-match-right step, regardless of the last word being correct. ", "page_idx": 6}, {"type": "text", "text": "Termination accuracy checks if the last output word\u2019s left neighbor in the sequence matches the input sequence. We include ablations on possible confounding factors (such as tokenization), different task definitions, and number of in-context examples in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "5.1 GPT-4 and Gemini prompting on PEN and PERM ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Prompting to solve PEN. We first provide the models with simple few-shot prompts demonstrating eight input-output examples for the task. The exact format is shown in Figure D.16. We find that this is not sufficient to obtain non-zero task accuracy. We then prepend a natural language description of the task to the few-shot prompt as shown in Figure D.17, but we find that this does not change the scores in any significant way. We proceed further by introducing Chain-of-Thought (CoT) prompting [26]. The prompt now consists of a high-level description of the task, eight input-output examples, and a request for the model to perform the task step-by-step (Figure D.18). With this prompting method, the models still exhibit zero task accuracy. We perform a similar experiment using a more recent and advanced CoT query, analogical CoT [28], which prompts the model to generate its own few-shot examples by asking it to recall several examples of similar algorithmic problems it has been exposed to during training (Figure D.19). This still does not help us achieve non-zero task accuracy. ", "page_idx": 7}, {"type": "text", "text": "We then provide more explicit guidance in the few-shot examples using two different approaches. In the first approach (Few-shot CoT, Figure D.20), the few-shot examples demonstrate each step in the pointer matching procedure alongside with the corresponding outputs (right neighbors) at each step. In the second approach (Sub-task CoT, Figure D.21), the few-shot examples consist of two phases: first, we match the entire sequence given an initial input, and then we output the right neighbors of the elements of the previously matched sequence. While none of the two approaches achieve non-zero task accuracy, sub-task CoT prompts induce a higher match accuracy in the Gemini-Pro model, reaching $33\\%$ . ", "page_idx": 7}, {"type": "text", "text": "We experiment with one further ablation, in which we remove the additional attention traps in the yellow tokens (see Appendix B.2), such that every yellow token only matches once. Although this ", "page_idx": 7}, {"type": "table", "img_path": "x7AD0343Jz/tmp/8acfca946f92c96526dc5251275caec6e8feb3fbac004eeae7144a1c8341e660.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 1: Results of GPT-4 and Gemini-Pro with various prompt settings (including chain-of-thought (CoT), analogical CoT [28], and code interpreter as a multi-round prompting technique [27]). Despite providing strong hints, few-shot CoT, task description, and without double traps (single only), both GPT-4 and Gemini fail on PEN ( $0\\%$ task accuracy). On PERM, the trend is similar and the main obstacle is multi-counting correctly (cf. the high match accuracy in the last two rows). GPT-4 reaches $42\\%$ task accuracy with a hand-crafted sub-task CoT. Gemini-Pro reaches to $9\\%$ in this setting. ", "page_idx": 7}, {"type": "text", "text": "does not help GPT-4 or Gemini to obtain a correct answer (still $0\\%$ task accuracy), it more often chooses the correct word to match within its answer sequence, reaching $41\\%$ match accuracy. This showcases how susceptible GPT-4 is to adversarial or unfortunate task-irrelevant correlations, also within chain of thought. ", "page_idx": 8}, {"type": "text", "text": "We also ablate on the newly available o1-preview model from OpenAI. While it cannot infer the task only from examples, it reaches $70\\%$ accuracy given a description of how to perform the task and CoT; see Appendix D.4 for more details. ", "page_idx": 8}, {"type": "text", "text": "Prompting to solve PERM. The same progression of prompt refinements is also applied for PERM, and the results are consistent with the observations on the PEN task. The results are shown in Table 1. When asked for CoT, GPT-4 does not find a good way to calculate the numbers for each word and tries to determine directly whether a match is to the left or not, something it seemingly has trouble doing. The best performing method is \u201cSub-task CoT\u201d (Figure D.22). In this method, we begin by explicitly enumerating all of the words. With this, a left match can be determined by comparing two numbers. We conclude the few-shot examples with an explicit computation of the multicount numbers. This format increases the GPT-4 task accuracy to $42\\%$ , which is significantly higher (by $29\\%$ ) than all of the other results. However, this setting deviates from identifying and leveraging the compositional structure of the tasks, since it rather corresponds to executing a hand-crafted algorithm. Based on the presented results, we can conclude that on the investigated example compositions, in-context learning with few samples is unreliable and fails to compose knowledge from sub-tasks and solve the main compositional task. ", "page_idx": 8}, {"type": "text", "text": "5.2 GPT-4 code interpreter on PEN and PERM ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We experiment with asking GPT-4 to write code, which is a suitable prompting method for our algorithmic tasks. We use the official multi-round code interpreter of GPT-4\u2019s assistant API. While it enables GPT-4 to sometimes output a correct solution, it is not systematic and often fails at finding its errors. An example prompt for PEN is shown in Figure D.23. ", "page_idx": 8}, {"type": "text", "text": "To get a deeper understanding of how the code generation of GPT-4 fails, we investigate 20 random answers from our results of the multi-round code interpreter in Table 1. We find that the model always outputs executable code and copies the task sequence correctly. While $10\\%$ of the answers were correct, $65\\%$ of the answers were a wrong answer sequence, and in the remaining $24\\%$ , the model tried 10 attempts of code programs and messages in between before being killed. In $35\\%$ of the cases, the model initially produced code which results in an infinite loop when executed on the sample. In those cases, the model can fix this problem once $(13\\%)$ , three times $(38\\%)$ it tries to correct itself until shutdown, and three times $(38\\%)$ it catches the problem but still outputs a wrong answer. Once it catches the error and gives a correct answer $(13\\%)$ . Further, it is interesting that the model never leverages the example question $^+$ answer given (it could verify its solution code with that before submitting an answer). ", "page_idx": 8}, {"type": "text", "text": "6 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Dziri et al. [16] and others [9, 29, 30] investigate limitations of Transformer language models on compositionality with prompting and finetuning. Differently from them, we train state-of-the-art language models from scratch and analyze how well the models can reuse and reorder sub-tasks learned. This gives new insights into compositional learning and provides a view into some limitations of current language model pre-training. Similarly, Razeghi et al. [31] find a correlation between performance and the frequency of how often task instances occur in the training data. ", "page_idx": 8}, {"type": "text", "text": "Apart from the tasks we use in this work, there exist many benchmarks on compositional learning for language models [3, 5, 8, 11, 21, 32, 33, 34, 35]. However, compared to them, our training setup and synthetic tasks specifically target compositional learning abilities subject to sample efficiency. ", "page_idx": 8}, {"type": "text", "text": "Many works tackled the problem of improving models\u2019 compositionality already [9, 36, 37, 38, 39]. However, their focus is mostly on the improvement of the model\u2019s behavior in specific benchmarks, while model-inherent systematic compositional learning remains extremely sample inefficient. ", "page_idx": 8}, {"type": "text", "text": "Feng et al. [40] point out that constant-size logarithmic precision Transformers can implement mathematical and dynamic programming problems. In order to learn a concept with CoT, the data has to lay out the respective step-by-step solution. Connecting to our bound, the concept needs to be made \u201cobvious\u201d to learn. Various works [17, 41, 42, 43] present approaches to instead allow Transformers to change their depth. This could make the model computationally more powerful as it allows the model to take arbitrary computation time for difficult samples. Our theoretical result instead focuses on fixed-depth Transformers, as they currently are most present in LLMs, and focuses on tasks where the given depth of the Transformers is already sufficient to solve the task. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "There has been extensive research on the expressive power of language models [40, 44, 41]. Judd [45] already showed that finding weights for feedforward neural networks such that the neural network correctly predicts at least $^2/3$ of the training examples is NP-hard in general [46]. Our theoretical bounds work focuses on overparameterized models that memorize. Other follow-ups construct concrete example network architectures being hard to train [46, 47, 48]. Kearns [49] presents results on the complexity of learning, with a focus on learning from distributions, compared to us investigating data and model efficiency. Abbe and Sandon [50] show that decision problems learnable in polynomial time are learnable by gradient descent in polynomial time. Different from us, they give their learning algorithm arbitrarily many samples and distinguish poly-time vs. non-poly-time. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions, limitations, and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Limitations. We do not propose practical ideas to tackle the issues identified with the investigated algorithmic tasks. Hence, it remains for future research to find effective means to incentivize compositional learning in Transformer-based language models, possibly by incorporating more inductive biases towards compositionality in the model architecture or the training procedure. Additionally, while the investigated algorithmic tasks allow to work in a fully controlled environment with limited confounding factors, they still represent a rather limited collection of benchmarks. Its expansion with more natural tasks, as well as the ablation of different sub-task definitions, could strengthen the observations and conclusions proposed in this work. ", "page_idx": 9}, {"type": "text", "text": "Conclusions and future work. In this work, we analyzed the capabilities of Transformer language models in learning compositional algorithmic tasks. To do so, we formulated a set of hypotheses to characterize the efficiency of a model when learning compositional tasks, from efficient $(\\mathcal{H}_{1})$ to inefficient $(\\mathcal{H}_{4})$ regimes. We introduced a new set of algorithmic tasks based on pointer execution to benchmark compositional learning in large language models in a more controlled setting. On these novel tasks, as well as other well-known benchmarks from previous works, we observe that Transformer-based language models struggle at compositional learning on tasks demanding to learn the composition of several discrete sub-tasks, making our hypothesis $\\mathcal{H}_{4}$ most plausible (learning the compositional task requires more samples than the sum of those required to learn the individual sub-tasks). Further, we show (also theoretically) that learning certain (compositional) concepts with feedforward models on gradient descent is very data inefficient, adding further support to reject $\\mathcal{H}_{1}-\\mathcal{H}_{3}$ . Finally, we empirically rule out also the possibility that these tasks can be learned in a few-shot fashion by state-of-the-art LLMs such as GPT-4 and Gemini-Pro. This evaluation of current state-of-the-art Transformer language models may provide some directions to improvements in compositional capabilities which would help models to be more reliable and improve on complex algorithmic structures like reasoning or mathematics. Addressing the current limitations of this work might also be an interesting avenue for future research. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. Dissociating language and thought in large language models. Trends in Cognitive Sciences, 2024. [2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [3] Arsenii Kirillovich Moskvichev, Victor Vikram Odouard, and Melanie Mitchell. The ConceptARC benchmark: Evaluating understanding and generalization in the ARC domain. Transactions on Machine Learning Research, 2023.   \n[4] Wenting Zhao, Justin Chiu, Claire Cardie, and Alexander Rush. Abductive commonsense reasoning exploiting mutually exclusive explanations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, July 2023. Association for Computational Linguistics. [5] Daniel Keysers, Nathanael Sch\u00e4rli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, and Olivier Bousquet. Measuring compositional generalization: A comprehensive method on realistic data. In International Conference on Learning Representations (ICLR), 2020. [6] Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu, and Ben He. ChatGPT is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models. arXiv preprint arXiv:2303.16421, 2023.   \n[7] Michael Hersche, Francesco di Stefano, Thomas Hofmann, Abu Sebastian, and Abbas Rahimi. Probabilistic abduction for visual abstract reasoning via learning rules in vector-symbolic architectures. In Proceedings of the NeurIPS Workshop on Mathematical Reasoning and AI, 2023.   \n[8] Najoung Kim and Tal Linzen. COGS: A compositional generalization challenge based on semantic interpretation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, November 2020.   \n[9] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 2023. Association for Computational Linguistics.   \n[10] Giacomo Camposampiero, Lo\u00efc Houmard, Benjamin Estermann, Jo\u00ebl Mathys, and Roger Wattenhofer. Abstract visual reasoning enabled by language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2023.   \n[11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-ffith Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.   \n[12] Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, and Chi Wang. An empirical study on challenging math problem solving with GPT-4. In International Conference on Learning Representations (ICLR), 2024.   \n[13] Zhijing Jin, Jiarui Liu, Zhiheng LYU, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona T. Diab, and Bernhard Sch\u00f6lkopf. Can large language models infer causation from correlation? In International Conference on Learning Representations (ICLR), 2024.   \n[14] Cheng Zhang, Stefan Bauer, Paul Bennett, Jiangfeng Gao, Wenbo Gong, Agrin Hilmkil, Joel Jennings, Chao Ma, Tom Minka, Nick Pawlowski, et al. Understanding causality with large language models: Feasibility and opportunities. arXiv preprint arXiv:2304.05524, 2023.   \n[15] Z. Jin, Y. Chen, F. Leeb, L. Gresele, O. Kamal, Z. Lyu, K. Blin, F. Gonzalez, M. Kleiman-Weiner, M. Sachan, and B. Sch\u00f6lkopf. CLadder: A benchmark to assess causal reasoning capabilities of language models. Advances in Neural Information Processing Systems (NeurIPS), 36, 2023.   \n[16] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang (Lorraine) Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena Hwang, Soumya Sanyal, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems (NeurIPS), 36, 2023.   \n[17] Samira Abnar, Omid Saremi, Laurent Dinh, Shantel Wilson, Miguel Angel Bautista, Chen Huang, Vimal Thilak, Etai Littwin, Jiatao Gu, Josh Susskind, and Samy Bengio. Adaptivity and modularity for efficient generalization over task complexity. arXiv preprint arXiv:2310.08866, 2023.   \n[18] Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. In International Conference on Learning Representations (ICLR), 2023.   \n[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS), 30, 2017.   \n[20] Chiyuan Zhang, Maithra Raghu, Jon Kleinberg, and Samy Bengio. Pointer value retrieval: A new benchmark for understanding the limits of neural network generalization. arXiv preprint arXiv:2107.12580, 2021.   \n[21] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. ArXiv, abs/1904.01557, 2019.   \n[22] Jacob Russin, Roland Fernandez, Hamid Palangi, Eric Rosen, Nebojsa Jojic, Paul Smolensky, and Jianfeng Gao. Compositional processing emerges in neural networks solving math problems. 43rd Annual Meeting of the Cognitive Science Society, 2021.   \n[23] Gregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A Ortega. Neural networks and the Chomsky hierarchy. In International Conference on Learning Representations (ICLR), 2023.   \n[24] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In International Conference on Learning Representations (ICLR), 2023.   \n[25] Alex Warstadt, Leshem Choshen, Aaron Mueller, Adina Williams, Ethan Wilcox, and Chengxu Zhuang. Call for papers\u2013The BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus. arXiv preprint arXiv:2301.11796, 2023.   \n[26] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems (NeurIPS), 36, 2022.   \n[27] Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, and Hongsheng Li. Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification. In International Conference on Learning Representations (ICLR), 2024.   \n[28] Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H. Chi, and Denny Zhou. Large language models as analogical reasoners. In International Conference on Learning Representations (ICLR), 2024.   \n[29] Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, and Xiang Ren. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. In International Conference on Learning Representations (ICLR), 2024.   \n[30] Zhuoyan Xu, Zhenmei Shi, and Yingyu Liang. Do large language models have compositional ability? an investigation into limitations and scalability. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024.   \n[31] Yasaman Razeghi, IV RobertL.Logan, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot numerical reasoning. In Conference on Empirical Methods in Natural Language Processing, 2022.   \n[32] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[33] Lorenzo Bertolini, Julie Weeds, and David Weir. Testing large language models on compositionality and inference with phrase-level adjective-noun entailment. In Proceedings of the 29th International Conference on Computational Linguistics. International Committee on Computational Linguistics, 2022.   \n[34] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \n[35] Brenden M. Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International Conference on Machine Learning (ICML), 2017.   \n[36] Meriem Beloucif, Mihir Bansal, and Chris Biemann. Using Wikidata for enhancing compositionality in pretrained language models. In Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing, 2023.   \n[37] Jiaao Chen, Xiaoman Pan, Dian Yu, Kaiqiang Song, Xiaoyang Wang, Dong Yu, and Jianshu Chen. Skills-in-context prompting: Unlocking compositionality in large language models. arXiv preprint arXiv:2308.00304, 2023.   \n[38] Andrew Drozdov, Nathanael Sch\u00e4rli, Ekin Aky\u00fcrek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, and Denny Zhou. Compositional semantic parsing with large language models. In International Conference on Learning Representations (ICLR), 2023.   \n[39] Chenhao Zheng, Jieyu Zhang, Aniruddha Kembhavi, and Ranjay Krishna. Iterated learning improves compositionality in large vision-language models. arxiv preprint arxiv:2404.02145, 2024.   \n[40] Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: A theoretical perspective. Advances in Neural Information Processing Systems (NeurIPS), 36, 2023.   \n[41] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and \u0141ukasz Kaiser. Universal transformers. In International Conference on Learning Representations (ICLR), 2019.   \n[42] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019.   \n[43] Andrea Banino, Jan Balaguer, and Charles Blundell. Pondernet: Learning to ponder. In 8th ICML Workshop on Automated Machine Learning (AutoML), 2021.   \n[44] Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In The Second Blogpost Track at ICLR 2023, 2021.   \n[45] Stephen Judd. On the complexity of loading shallow neural networks. Journal of Complexity, 4 (3):177\u2013192, 1988.   \n[46] Avrim L. Blum and Ronald L. Rivest. Training a 3-node neural network is NP-complete. Neural Networks, 5(1):117\u2013127, 1992.   \n[47] Digvijay Boob, Santanu S Dey, and Guanghui Lan. Complexity of training ReLU neural network. Discrete Optimization, 44:100620, 2022.   \n[48] Bhaskar DasGupta, Hava T. Siegelmann, and Eduardo Sontag. On the Intractability of Loading Neural Networks, pages 357\u2013389. Springer US, Boston, MA, 1994.   \n[49] Michael J Kearns. The computational complexity of machine learning. MIT press, 1990.   \n[50] Emmanuel Abbe and Colin Sandon. Polynomial-time universality and limitations of deep learning. Communications on Pure and Applied Mathematics, 76(11):3493\u20133549, 2023.   \n[51] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.   \n[52] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arxiv preprint arxiv:2001.08361, 2020.   \n[53] Arturs Backurs and Piotr Indyk. Edit distance cannot be computed in strongly subquadratic time (unless seth is false). SIAM Journal on Computing, 47(3):1087\u20131097, 2018.   \n[54] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.   \n[55] Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ippolito, Christopher A Choquette-Choo, Eric Wallace, Florian Tram\u00e8r, and Katherine Lee. Scalable extraction of training data from (production) language models. arXiv preprint arXiv:2311.17035, 2023.   \n[56] Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization without overfitting: Analyzing the training dynamics of large language models. Advances in Neural Information Processing Systems (NeurIPS), 35, 2022.   \n[57] David Maxwell Chickering. Learning bayesian networks is np-complete. Learning from data: Artificial intelligence and statistics V, pages 121\u2013130, 1996.   \n[58] Yury Kaminsky and Irina Deeva. BigBraveBN: algorithm of structural learning for bayesian networks with a large number of nodes. Procedia Computer Science, 212:191\u2013200, 2022.   \n[59] Thomas Eiter and Thomas Lukasiewicz. Complexity results for structure-based causality. Artificial Intelligence, 142(1):53\u201389, 2002.   \n[60] L.G. Valiant and V.V. Vazirani. NP is as easy as detecting unique solutions. Theoretical Computer Science, 47:85\u201393, 1986.   \n[61] R\u00f3bert Csord\u00e1s, Kazuki Irie, and Juergen Schmidhuber. The devil is in the detail: Simple tricks improve systematic generalization of transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Feedforward models on gradient descent can only learn the obvious 16 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Details on the theory . . 18   \nA.2 Intuition: Learning-inference gap . . 18   \nA.3 Special case: SAT proof on algorithmic learning . . 19   \nA.4 General proof on algorithmic learning . . . . 20   \nA.5 Algorithmic learning in the in-distribution setting . . . 21   \nA.6 Proof of the corollary A.7 . . . 22 ", "page_idx": 14}, {"type": "text", "text": "B Specifications of PEN, PERM, HSS, and MUL 23 ", "page_idx": 14}, {"type": "text", "text": "B.1 Properties of the sub-task definition 23   \nB.2 Details on the PEN task . 23 ", "page_idx": 14}, {"type": "text", "text": "C Training experiments details 24 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 LLaMA training hyperparameters . . 24   \nC.2 Details on the discrete solver of the PEN Task 24   \nC.3 Accuracy tables . . . 25   \nC.4 Performance of LLaMA on decompositionality . . . 28   \nC.5 Model ablation: Performance of UT-style LLaMA on PERM . . 29 ", "page_idx": 14}, {"type": "text", "text": "D Prompting experiments details 30 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Input sequences tokenization 30   \nD.2 Natural-PEN . . 30   \nD.3 Many-shot prompting . . . 30   \nD.4 Prompting OpenAI o1-preview 31   \nD.5 Prompt examples . . 32 ", "page_idx": 14}, {"type": "text", "text": "As in our empirical results, we focus on model size and dataset size, not compute, which e.g. is the focus of [51, 52, 50]. This has the rationale, that concepts or tasks which do not have unlimited amounts of data and are hard to learn, can in theory still be learned while gradient descent on feedforward models will be limited. In particular, in this part we show classes of tasks that are learnable from a single sample each (i.e., learnable within hypothesis $\\mathcal{H}_{1}$ ), but on which feedforward models trained with gradient descent will learn within hypothesis $\\mathcal{H}_{4}$ , needing up to exponentially many samples. See also Figure A.4 for an intuition. ", "page_idx": 15}, {"type": "text", "text": "From complexity theory, we know that finding the proof of membership of a string in a language is in general less efficient than verifying its membership given a proof. We use this structure: ", "page_idx": 15}, {"type": "text", "text": "Definition A.1 $(\\kappa)$ . Let $L$ be any recursive decision problem with solution in space $O(n)$ and being verifiable in time $O(n)$ . Let us look at all algorithms solving $L$ and let $k(n)$ be a (high) lower bound of them. We denote $\\kappa$ to be the set of choosing one upper bound $k(n)$ per such decision problem. ", "page_idx": 15}, {"type": "text", "text": "Some examples: Assuming the exponential time hypothesis, SAT solution algorithms need exponential time (i.e. $k(n)$ can be an arbitrarily high polynomial), and verification is in $O(n)$ . Under the same assumption, the edit distance algorithm needs $\\Omega(n^{2-\\epsilon})$ for any $\\epsilon>0$ while being verifiable in $O(n)$ [53]. Usually, no lower bounds are proven for such problems, but the fact that we can not find faster algorithms makes assumptions reasonable. ", "page_idx": 15}, {"type": "text", "text": "Definition A.2 (Concept, Concept Class). A concept $\\mathcal{C}=(U_{\\mathcal{C}},f_{\\mathcal{C}},n_{\\mathcal{C}})$ consists of an algorithm $f$ defined on a set of inputs $U$ and a natural number $n_{\\mathcal{C}}$ . We call $n_{\\mathcal{C}}$ the \u201cdifficulty\" of $\\mathcal{C}$ . A concept class $\\mathbb{C}$ is an infinite set of concepts where each $n\\in\\mathbb N$ occurs only finitely many times. ", "page_idx": 15}, {"type": "text", "text": "The intuition: A concept class is a set of concepts that are connected by some common basis, e.g. they all pose certain challenges (e.g. finding a formula assignment) to an overarching problem to learn (e.g. the SAT problem). We need a class of such problems because complexity theory makes asymptotic statements. ", "page_idx": 15}, {"type": "text", "text": "Similar definitions are used by Kearns [49], here we use a slightly different definition of concepts as functions instead of subsets of an object set to better suit today\u2019s applications. ", "page_idx": 15}, {"type": "text", "text": "Definition A.3 (Application of a Concept). Given a concept $\\mathcal{C}=(U,f,n)$ , an application of $\\mathcal{C}$ is a pair $(T,E)$ of nonempty sets of samples (i.e. input-output pairs) of $f$ . We call $T$ the training set and $E$ the test set or examination set. ", "page_idx": 15}, {"type": "text", "text": "Here we deviate from classical learning theory in that we do not define a distribution. Instead, we define the more general case of having a training regime and testing regime since we focus on systematic generalization. One can additionally require $E$ and $T$ to be statistically dependent to recover classical train-test distributions, as we, for example, show in Appendix A.5. ", "page_idx": 15}, {"type": "text", "text": "Definition A.4 (Learning Algorithm). Let $\\mathbb{S}$ denote the set of all finite datasets (i.e., sets of inputoutput pairs) and $\\Theta$ the set of all input-bounded and runtime-bounded algorithms. A learning algorithm is an algorithm $\\mathcal{L}:\\mathbb{S}\\rightarrow\\Theta$ . ", "page_idx": 15}, {"type": "text", "text": "We say a $\\mathcal{L}$ learns a concept $\\mathcal{C}\\,=\\,(U_{\\mathcal{C}},f_{\\mathcal{C}},n_{\\mathcal{C}})$ on a dataset $T_{\\mathcal{C}}$ , if $\\mathcal{M}(\\boldsymbol{x})\\,=\\,f(\\boldsymbol{x})\\,\\forall\\boldsymbol{x}\\,\\in\\,U$ with $\\mathcal{M}:=\\mathcal{L}(T_{\\mathcal{C}})$ . ", "page_idx": 15}, {"type": "text", "text": "Let $\\mathrm{TIME}({\\mathcal{A}},x)$ denote the runtime of an algorithm $\\boldsymbol{\\mathcal{A}}$ on an input $x$ . For a concept class $\\mathbb{C}$ and a learning algorithm $\\mathcal{L}$ , we denote: ", "page_idx": 15}, {"type": "equation", "text": "$$\nz_{\\mathbb{C}}^{\\mathcal{L}}(n_{0})=\\operatorname*{max}_{(U,f,n_{0})\\in\\mathbb{C}}\\ \\mathrm{TIME}\\big(\\mathcal{L}\\big(T_{(U,f,n_{0})}\\big),x\\big)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "I.e. the maximum inference runtime on a test sample for concepts of difficulty $n_{0}$ . ", "page_idx": 15}, {"type": "text", "text": "Theorem A.5. For each $k\\in\\mathcal{K}$ there is a (different) concept class $\\mathbb{C}_{:}$ , with applications $(T_{\\mathcal{C}},E_{\\mathcal{C}})$ (i.e., train and test datasets) for each ${\\mathcal{C}}\\in\\mathbb{C}$ , with the following properties: ", "page_idx": 15}, {"type": "text", "text": "1. There is a learning algorithm $\\mathcal{L}^{\\ast}$ that can learn each concept from a single sample in $O(k(n_{C}))$ time and with $z_{\\mathbb{C}}^{\\mathcal{L}^{*}}(n_{\\mathcal{C}})=O(n_{\\mathcal{C}})$ ", "page_idx": 15}, {"type": "text", "text": "2. For any learning algorithm $\\mathcal{L},\\,\\mathcal{L}$ one of the following will hold: ", "page_idx": 16}, {"type": "text", "text": "\u2022\u201c $\\mathcal{L}$ does not learn\u201d: $\\mathcal{L}$ is not able to learn infinitely many concepts in $\\mathbb{C}$ . \u2022\u201cL learns during inference\u201d: With $\\begin{array}{r}{\\mathcal{M}=\\mathcal{L}(T\\boldsymbol{c}),\\,\\sum_{(\\boldsymbol{x},\\boldsymbol{y})\\in E\\boldsymbol{c}}\\,\\mathrm{TIME}(\\mathcal{M},\\boldsymbol{x})=\\Omega(k(n\\boldsymbol{c}))}\\end{array}$ \u2022\u201cL does learn\u201d: $\\mathrm{TIME}(\\mathcal{L},T_{\\mathcal{C}})=\\Omega(k(n_{\\mathcal{C}}))$ ", "page_idx": 16}, {"type": "text", "text": "The intuitive claim of this theorem is, that there are concepts, which require a $\\Omega(k(n_{C}))$ time (i.e. a lot) to learn while being fast at inference. ", "page_idx": 16}, {"type": "text", "text": "A special case for SAT is carried out in Appendix A.3 and the general case in Appendix A.4. The proof focuses on training on a minimal dataset (one sample) and testing on an out-of-distribution \u201cexam\" set. For completeness, in Appendix A.5 we provide a proof for similar claims in an in-distribution training-testing setting. ", "page_idx": 16}, {"type": "text", "text": "We now introduce a memorization assumption which assumes that our learning algorithm memorizes fast. ", "page_idx": 16}, {"type": "text", "text": "Definition A.6 (Constant Memorization). Given a concept class $\\mathbb{C}$ with applications $T_{\\mathcal{C}},E_{\\mathcal{C}}$ for each ${\\mathcal{C}}\\in\\mathbb{C}$ and a learning algorithm $\\mathcal{L}$ . We say $\\mathcal{L}$ memorizes constantly under $\\mathbb{C}$ , if there is a constant $B$ , such that for any $T_{\\mathcal{C}},E_{\\mathcal{C}}$ , $\\mathcal{L}$ runs in time $B*|T_{\\mathcal{C}}|*Z$ with $Z$ being the longest runtime of $\\mathcal{M}:=\\mathcal{L}(T_{\\mathcal{C}})$ on a sample in $E$ . ", "page_idx": 16}, {"type": "text", "text": "With this, we obtain: ", "page_idx": 16}, {"type": "text", "text": "Corollary A.7. Let $k\\in\\mathcal{K}$ . Then there exists a concept class $\\mathbb{C}$ with applications $T_{\\mathcal{C}},E_{\\mathcal{C}}$ such that: k TC, EC ", "page_idx": 16}, {"type": "text", "text": "1. Any constantly memorizing learning algorithm $\\mathcal{L}$ learning on an (arbitrary) augmented dataset $T_{\\mathcal{C}}^{\\prime}$ generated by a generator T C\u2032 = G(TC) with G running in time o(k(nC)): If zCL(nC) = o( k(nnCC )), then $\\mathcal{L}$ needs $\\Omega\\big(\\frac{k(n c)}{z_{\\mathbb{C}}^{\\mathcal{L}}(n c)}\\big)$ many samples to learn each concept $\\mathcal{C}=(U_{\\mathcal{C}},f_{\\mathcal{C}},n_{\\mathcal{C}})\\in\\mathbb{C}$ . ", "page_idx": 16}, {"type": "text", "text": "2. There exists a learning algorithm $\\mathcal{L}^{\\ast}$ learning each concept from a single sample with $z_{\\mathbb{C}}^{\\mathcal{L}^{*}}(n_{\\mathcal{C}})=$ $O(n_{C})$ . ", "page_idx": 16}, {"type": "text", "text": "We use the notion of $G$ (which is an algorithm free to choose) to make sure that bigger datasets do not make things \u201cobvious\u201d, that is, leak too much information about the exam $E_{\\mathcal{C}}$ . ", "page_idx": 16}, {"type": "text", "text": "If we think about the PEN task, it is a combination of sub-tasks, i.e., $\\begin{array}{r l}{P E N(x)}&{{}=}\\end{array}$ $C p y(P E(R C p y(x)))$ . There are combinations for three of the sub-tasks chained together. Already in such a restricted setting, learning the task from scratch is extremely inefficient, requiring a multiplicative factor of one million samples more than the discrete optimization algorithm described in Appendix B.2, which in this case corresponds to the optimal learner mentioned in Corollary 5.7. This confirms the outcome that our theoretical framework would predict, i.e. LLaMA fails at learning from few examples, although the existence of the optimal learner shows that it would be possible. ", "page_idx": 16}, {"type": "text", "text": "The theory allows us to extend this result beyond the limit of the empirically verifiable. When more primitives are available (e.g. in general-purpose datasets like The Pile [54], finding the right functional composition of the sub-tasks quickly becomes a computationally hard combinatorial task. With such problems, the theory says that plain gradient descent will tend to just memorize the few samples of the compositional task, instead of (combinatorially) finding the right composition. ", "page_idx": 16}, {"type": "text", "text": "Nasr et al. [55] show that one can extract more memorized data from larger models. Tirumala et al. [56] have shown that larger language models memorize faster than smaller models. Based on this, we can conservatively assume a constant number of steps for memorizing the fixed-size answer to a sample. This fits definition A.6 for a learning algorithm $\\textstyle{\\mathcal{L}}^{F}$ learning with (growing) constant-depth feedforward models, depending on the input size in the training data, as one gradient step has the time complexity of a forward pass. ", "page_idx": 16}, {"type": "text", "text": "Therefore, Corollary A.7 can be interpreted as follows: There are classes of concepts, such that to learn them reliably, either the feedforward deep learning model needs to be unfavorably larger than needed for inference $(z_{\\mathbb{C}}^{\\mathcal{L}^{F}}(n)>>n)$ , or the data inefficiency is very high (by a factor $\\bar{\\frac{k(n)}{z_{\\mathbb{C}}^{\\mathcal{L}^{F}}(n)}}\\bar{)}$ ) ). Note that in practice, memorization does not necessarily mean, we stop optimizing, here we implicitly assume that after memorization, nothing useful happens anymore, e.g., the gradient signal magnitude is 0 and the model does not change further. ", "page_idx": 16}, {"type": "text", "text": "Much research has been done on the complexity of various problems. Here we want to point out a few further concept classes that need significant computation to learn and therefore underlie the above statements: ", "page_idx": 17}, {"type": "text", "text": "A fundamental capability for safe, explainable, and intelligent systems, is to be able to infer causal relationships. It has been shown, however, that learning such structural causal models is NP-hard in the number of graph nodes [57, 58]. Therefore: Let our complexity class contain the functions applying all structural causal models and generate observations for each concept. Now let us test those functions by an exam after training (i.e. by just asking for the structural causal model). Then, for this specific concept class, Theorem A.5 and Corollary A.7 hold for $k(n)$ being a super-polynomial function. ", "page_idx": 17}, {"type": "text", "text": "Further, it has been shown [59] that given a structural causal model, it is NP-hard to infer whether one particular event always leads to another. Such inference might be important for a few selected cases. Again, by constructing a concept class containing functions such that the structural model is given or inferable and the always-cause is asked or needed for predictions in the inference phase, it follows by Theorem A.5 and Corollary A.7 that learning such concepts requires a lot of computation - and many samples for feedforward neural networks although theoretically unnecessary. ", "page_idx": 17}, {"type": "text", "text": "A.1 Details on the theory ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For brevity \u201calgorithm\u201d refers to always terminating algorithms. Let us fix a standard random access memory computational model. We also fix a universal encoding and decoding function ENC, DEC that can encode/decode any algorithm or object we define below (imagine, for example, a digital processor model where everything can be encoded in 0s and 1s). From now on we omit all encodings and decodings and instead use defined objects and their encodings interchangeably for the sake of simple notation. ", "page_idx": 17}, {"type": "text", "text": "A.2 Intuition: Learning-inference gap ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "See Figure A.4. ", "page_idx": 17}, {"type": "image", "img_path": "x7AD0343Jz/tmp/4244c183432e4f8c4dca340dd89df6bea6a68b0cf8c620dc3e0e89f5059a3877.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure A.4: Intuition for our statement: Based on widely accepted assumptions, there are problems requiring much more learning computation time than usage computation time (here: per sample available). Gradient Descent on Feedforward Networks under the assumption of constant-stepmemorization however, will only be able to learn problems as hard to learn as they are during inference. Therefore, the model size needs to be much larger than necessary, or the training samples need to be much larger than necessary, or gradient descent will have trouble learning. ", "page_idx": 17}, {"type": "text", "text": "A.3 Special case: SAT proof on algorithmic learning ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For a better reader\u2019s experience, let us restate the theorem: ", "page_idx": 18}, {"type": "text", "text": "Theorem. For each $k\\in\\mathcal{K}$ there is a (different) concept class $\\mathbb{C}$ , with applications $(T_{\\mathcal{C}},E_{\\mathcal{C}})$ (i.e., train and test datasets) for each ${\\mathcal{C}}\\in\\mathbb{C}$ , with the following properties: ", "page_idx": 18}, {"type": "text", "text": "1. There is a learning algorithm $\\mathcal{L}^{\\ast}$ that can learn each concept from a single sample in $O(k(n_{C}))$ time and with $z_{\\mathbb{C}}^{\\mathcal{L}^{*}}(n_{\\mathcal{C}})=O(n_{\\mathcal{C}})$   \n2. For any learning algorithm $\\mathcal{L},\\,\\mathcal{L}$ one of the following will hold: \u2022\u201c $\\mathcal{L}$ does not learn\u201d: $\\mathcal{L}$ is not able to learn infinitely many concepts in $\\mathbb{C}$ . \u2022 $^{\\epsilon}\\mathcal{L}$ learns during inference\u201d: With $\\begin{array}{r}{\\mathcal{M}=\\mathcal{L}(T_{\\mathcal{C}}),\\,\\sum_{(x,y)\\in E_{\\mathcal{C}}}\\mathrm{TIME}(\\mathcal{M},x)=\\Omega(k(n_{\\mathcal{C}}))}\\end{array}$ \u2022\u201c $\\mathcal{L}$ does learn\u201d: $\\mathrm{TIME}(\\mathcal{L},T_{\\mathcal{C}})=\\Omega(k(n_{\\mathcal{C}}))$ ", "page_idx": 18}, {"type": "text", "text": "For simplicity, we first prove a special case where $k$ is the minimal runtime of a SAT (3-satisfiability) problem (i.e. given $P\\neq N P$ exponential in $n$ ). ", "page_idx": 18}, {"type": "text", "text": "Proof. Valiant and Vazirani [60] have shown, that given a formula which is either uniquely satisfiable or not satisfiable, deciding this is NP-hard. From this follows, that given a uniquely satisfiable formula, it is (assuming $P\\neq N P,$ ) not possible in polynomial time, to find a proof for it, i.e. the unique satisfying variable assignment (this holds, because checking such a proof is fast). ", "page_idx": 18}, {"type": "text", "text": "We choose $\\mathbb{C}$ to be a set constructed from the set of unambiguously satisfiable SAT formulas: Given a uniquely satisfiable formula $A$ and the unique solution $y^{\\bar{A}}$ , let $\\dot{U}^{A}=\\{A,1,\\dots,|y^{A}|\\}$ be the set containing the formula and the variable names. Let $f^{A}(\\bar{A})=1$ and $f^{A}(i)=y_{i}^{A}$ . Our concept is now $\\mathcal{C}^{A}=(U^{A},f^{A},|A|)$ . We choose the training dataset of the concepts to be $\\{(A,1)\\}$ and the examination dataset to be $\\{(i,y_{i}^{A})|i\\in1,\\ldots,|y^{A}|\\}$ . This means we train on a uniquely satisfiable formula, and the test set tests whether the model learned the unique solution to it. ", "page_idx": 18}, {"type": "text", "text": "We now construct the learning algorithm $\\mathcal{L}^{\\ast}$ learning ${\\mathcal{C}}\\in\\mathbb{C}$ from one sample. Let us fix an arbitrary $(U_{\\mathcal{C}},f_{\\mathcal{C}},n_{\\mathcal{C}})\\in\\mathbb{C}$ . ", "page_idx": 18}, {"type": "text", "text": "Given the training dataset $T c=\\{(A c,1)\\}$ corresponding to $\\mathcal{C}$ , $\\mathcal{L}^{\\ast}$ builds an algorithm $\\mathcal{M}$ of the form: ", "page_idx": 18}, {"type": "text", "text": "\u2022 ${\\mathcal{M}}(A)={\\mathrm{CHECK}}(A,x)$ for a formula $A$ of length $n_{\\mathcal{C}}$ where $x,|x|\\leq n c$ is a stored (i.e. hardcoded) constant. CHECK returns 1 iff $x$ is a satisfying variable assignment for the variables of $A$ . Note that $n_{\\mathcal{C}}$ can be inferred by $\\mathcal{L}^{\\ast}$ as it is the length of the single sample in $T_{\\mathcal{C}}$ .   \n\u2022 $\\mathcal{M}(i)=x_{i}$ , i.e. the $i$ -th variable assignment.   \n\u2022 On all other inputs, $\\mathcal{M}$ can implement any algorithm (e.g. some pre-trained knowledge). ", "page_idx": 18}, {"type": "text", "text": "Note that $\\mathcal{M}$ has a linear running time in $n_{\\mathcal{C}}$ on all samples in $T_{\\mathcal{C}}$ and $E_{\\mathcal{C}}$ . Now, $\\mathcal{L}^{\\ast}$ finds the satisfying variable assignment $x$ (using the minimal-runtime algorithm) for $A_{\\mathcal{C}}$ and stores it in $x$ . Note that this procedure runs in $k_{S A T}(n_{C})$ for $k_{S A T}(n_{C})$ being the minimal runtime for finding a solution to uniquely satisfiable formulas (which is assumed to be exponential under the exponential time hypothesis). ", "page_idx": 18}, {"type": "text", "text": "Now to the second step: Assume there is a learning algorithm $\\mathcal{L}$ and assume that it can learn all concepts except finitely many and do so in $o(k_{S A T}(n_{C}))$ (i.e. strictly less) time and with $\\mathcal{M}=\\mathcal{L}(T_{c})$ needing $o(k_{S A T}(n c)/n c)$ time on each sample in $E_{\\mathcal{C}}$ . Then one can simulate the learning setting to find a solution to a formula $A$ , $n:=|A|$ (except finitely many times): First, simulate $\\mathcal{L}$ on $\\{(A,{\\bar{1}})\\}$ to obtain $\\mathcal{M}=\\mathcal{L}(\\{(A,1)\\})$ . Then, simulate $\\mathcal{M}$ on all variable indices $i$ as inputs to obtain the satisfying assignment $y^{A}$ . This algorithm finds $y^{A}$ in $\\begin{array}{r}{o(k_{S A T}(n))+o(n*\\frac{k_{S A T}(n)}{n})=o(k_{S A T}(n))}\\end{array}$ time, contradiction. ", "page_idx": 18}, {"type": "text", "text": "A.4 General proof on algorithmic learning ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This proof is analogous to the special SAT case. We recommend the reader to read the special case first. ", "page_idx": 19}, {"type": "text", "text": "Proof. Let $k\\,\\in\\,\\kappa$ . Let $\\mathcal{P}$ be the problem $k$ corresponds to (e.g. SAT). Let $\\mathcal{A}_{s}$ denote the solving algorithm for $\\mathcal{P}$ running in time $k(n)$ , i.e. an algorithm that computes a solution to each instance of a class of problem cases, where the solution is in $O(n)$ space. Let $\\mathcal{A}_{c}$ be the checking algorithm that can verify if such a solution is correct, running in $O(n)$ . We choose the concept class $\\mathbb{C}$ to be a set constructed from the set of positive instances in $\\mathcal{P}$ (e.g. in the SAT special case this would be a uniquely satisfiable CNF formula): Given such a positive input instance $X$ , let $U^{X}\\;=\\;\\{X,1,\\ldots,|\\hat{\\cal{A}_{s}}(\\bar{X_{}})|\\}$ . Let $f^{X}(X)\\,=\\,1\\,\\$ and $f^{X}(i)\\,=\\,{\\mathcal{A}}_{s}(\\mathbf{\\dot{X}})_{i}$ . Our concept is now $\\mathcal{C}^{X}\\;=\\;(U^{X},f^{X},|X|)$ . We choose the training dataset of the concepts to be $\\{(X,1)\\}$ and the examination dataset to be $\\{(i,y_{i}^{X}|i\\in1,\\ldots,|y^{X}|\\}$ . ", "page_idx": 19}, {"type": "text", "text": "We now construct the learning algorithm $\\mathcal{L}^{\\ast}$ learning ${\\mathcal{C}}\\in\\mathbb{C}$ from one sample. Let us fix an arbitrary $(U_{\\mathcal{C}},f_{\\mathcal{C}},n_{\\mathcal{C}})\\in\\mathbb{C}$ . ", "page_idx": 19}, {"type": "text", "text": "Given the training dataset $T_{\\mathcal{C}}=\\{(X,1)\\}$ corresponding to $\\mathcal{C}$ , $\\mathcal{L}^{\\ast}$ builds an algorithm $\\mathcal{M}$ of the form: ", "page_idx": 19}, {"type": "text", "text": "\u2022 $\\mathcal{M}(\\boldsymbol{X})\\;=\\;\\mathrm{CHECK}(\\boldsymbol{X},\\boldsymbol{s})$ for a problem instance $X$ of the problem $\\mathcal{P}$ with $|X|\\ \\leq\\ n_{C}$ . $\\vert s\\vert\\,=\\,O(n c)$ is a stored (i.e. hardcoded) constant. CHECK returns 1 iff $s$ is a proof for the problem instance $X$ (which exists by Definition A.1). Values for $s$ will be candidate solutions for $X$ , and $\\mathcal{L}^{\\ast}$ will search for the solution.   \n\u2022 $\\mathcal{M}(i)=s_{i}$ , i.e. the $i$ -th value (e.g. bit) of $s$ .   \n\u2022 On all other inputs, $\\mathcal{M}$ can implement any algorithm (e.g. some pre-trained knowledge). ", "page_idx": 19}, {"type": "text", "text": "Note that $\\mathcal{M}$ has a linear runtime in $n_{\\mathcal{C}}$ on all samples in $T_{\\mathcal{C}}$ and $E_{\\mathcal{C}}$ . Now, $\\mathcal{L}^{\\ast}$ computes $s^{*}=\\mathcal{A}_{s}(X)$ (i.e. the solution to $\\mathcal{P}$ ) and stores it in $s$ , i.e. $s=s^{*}$ . Note that this procedure runs in TIME $(A_{s},X)$ time (which can be $k(n_{C})$ if $k(n_{C})$ is tight). ", "page_idx": 19}, {"type": "text", "text": "Now to the second step: Assume there is a learning algorithm $\\mathcal{L}$ and assume that it can learn all concepts except finitely many and do so in $o(k(n c))$ (i.e. strictly less) time and with $\\mathcal{M}=\\mathcal{L}(T_{c})$ needing $o(k(n\\bar{c})/n c)$ time on each sample in $E_{\\mathcal{C}}$ . Then one can simulate the learning setting to find a solution to a problem case $X\\in\\mathcal{P}$ , $n:=|X|$ (except finitely many times): First, simulate $\\mathcal{L}$ on $\\{(X,1)\\}$ to obtain $M=\\mathcal{L}(\\{(X,1)\\})$ . Then, simulate $\\mathcal{M}$ on all solution indices as inputs to get the proof $y^{X}$ for $X$ . This algorithm finds $y^{X}$ in $\\begin{array}{r}{o(k(n))+o(n*\\frac{k(n)}{n})=o(k(n))}\\end{array}$ time, contradiction. ", "page_idx": 19}, {"type": "text", "text": "A.5 Algorithmic learning in the in-distribution setting ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The underlying idea of this proof is to use one-way functions that are assumed to be hard even when one sees many examples. Such functions are known from cryptography. By using public-key cryptography we enable an attacker to simulate our learning setting which then allows us to show that this learning setting cannot be successful easily while we make sure, the setting is principally learnable [49]. For more elaborate motivations, we refer to this work. ", "page_idx": 20}, {"type": "text", "text": "Assumption $A.8$ is a standard assumption about cryptography being safe. ", "page_idx": 20}, {"type": "text", "text": "Assumption A.8 (Randomized RSA is CPA Secure). Let $p k,p k^{-1}$ denote an RSA encryption scheme with padding the plaintext with zeros and random bits before encryption. Assume that this encryption algorithm is CPA secure. ", "page_idx": 20}, {"type": "text", "text": "Theorem A.9. Let $k(n)$ be any polynomial function. Assuming Assumption A.8, there exists a randomized concept class $\\mathbb{C}$ containing exactly one concept $\\ensuremath{{\\mathcal{C}}}_{n}$ per $\\bar{n}\\,\\in\\,\\mathbb{N}^{+}$ , with applications $(T_{\\mathcal{C}},E_{\\mathcal{C}})$ (i.e. train and test datasets) where $T_{\\mathcal{C}}$ and $E_{\\mathcal{C}}$ are drawn from the same distribution, with the following properties: ", "page_idx": 20}, {"type": "text", "text": "\u2022 There is a learning algorithm that can learn each concept from a single sample. \u2022 For any learning algorithm $\\mathcal{L}$ and any $n_{i}$ , $\\mathcal{L}$ will either have negligible (in n) probability of learning concept $\\ensuremath{{\\mathcal{C}}}_{n}$ . Otherwise, $\\mathcal{M}=\\mathcal{L}(T_{\\mathcal{C}_{n}})$ needs $\\Omega\\big(\\frac{k(n)}{n}\\big)$ computation time on a sample in $E_{\\mathcal{C}_{n}}$ , or $\\mathcal{L}$ needs $\\Omega(k(n))$ computation time to learn. ", "page_idx": 20}, {"type": "text", "text": "Proof. Let us denote it by $p k,p k^{-1}$ our randomized RSA encryption scheme. For each $n\\in\\mathbb N$ , we now build one concept: ", "page_idx": 20}, {"type": "text", "text": "First let $s\\in\\{0,1\\}^{n}$ be the randomly chosen secret for $p k,p k^{-1}$ . We define: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{+}=\\{(x,p k(x),p k)|x\\in\\{0,1\\}^{n}\\}}\\\\ &{U_{-}=\\{(y,p k(x),p k)|y\\in\\{0,1\\}^{m},x\\in\\{0,1\\}^{n}\\}\\setminus U_{+}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $m$ denotes the ciphertext size of $p k$ . Note that the encryption is randomized and $U_{+}$ contains all combinations $p k(x)$ for the random bits. We define $f_{n}\\bar{(}e\\bar{)}\\,=\\,1$ if $e\\,\\in\\,U_{+}$ and $f_{n}(e)\\,=\\,0$ if $e\\,\\in\\,U_{-}$ . The concept is now $(U_{+}\\cup U_{-},f_{n},n)$ . Let us fix some constant size for $T_{n},E_{n}$ each (one is sufficient). Each sample in $T_{n}$ and $E_{n}$ is drawn with probability $\\frac{1}{2}$ from $U_{+}$ u.a.r. and with probability $\\frac{1}{2}$ from $U_{-}$ u.a.r. ", "page_idx": 20}, {"type": "text", "text": "With $\\mathbb{C}$ being defined, let us construct the learning algorithm $\\mathcal{L}^{\\ast}$ learning from one sample. Let $n$ be fixed and sample $T_{n},E_{n}$ . $\\mathcal{L}^{\\ast}$ chooses an arbitrary sample $(x,p k(x),p k)$ and brute-forces the secret key $d$ of the RSA encryption scheme. Note that this key is unique given the public key information $p k.\\,\\mathcal{L}^{*}$ now builds an algorithm $\\mathcal{M}$ for the form: ", "page_idx": 20}, {"type": "text", "text": "\u2022 $\\mathcal{M}(x,c,p k)=1$ iff the decoding of $c$ is $x$ and 0 otherwise \u2022 On all other inputs, $\\mathcal{M}$ can implement any algorithm (e.g. some pre-trained knowledge). ", "page_idx": 20}, {"type": "text", "text": "Note that those operations are efficiently doable, much faster than breaking the RSA secret key. ", "page_idx": 20}, {"type": "text", "text": "Now to the second part: Let $k$ be any polynomial function and fix some $n$ . Assume there is a learning algorithm $\\mathcal{L}$ and assume that it learns the concept $C_{n}$ with non-negligible probability in $n$ . Further assume that L needs o(k(n)) time and M = L(TCn) spends o( k(nn ) ) for each sample s \u2208EC. Then, we can break RSA encryption in polynomial time: Let our encryption-breaking algorithm $\\mathcal{E}$ be given a public key configuration (including $n$ ). It samples a training dataset $T$ with the public key. Then it simulates $\\mathcal{L}$ on it and obtains $\\mathcal{M}=\\mathcal{L}(T)$ . For the IND-CPA game, it chooses words $m_{0}\\neq m_{1}$ randomly and gets the (randomized) encryption $c$ of one of them back. $\\mathcal{E}$ sets $\\{(m_{0},c,p k)\\}$ as the test set. $\\mathcal{E}$ now computes the answer by simulating the trained model and choosing $m_{0}$ if $\\mathcal{M}((m_{0},c,p k))=1$ . This will give $\\mathcal{E}$ non-negligible success probability for the IND-CPA game in polynomial time, contradiction. ", "page_idx": 20}, {"type": "text", "text": "A.6 Proof of the corollary A.7 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Corollary. Let $k\\in\\mathcal{K}$ . Then there exists a concept class $\\mathbb{C}$ with applications $T_{\\mathcal{C}},E_{\\mathcal{C}}$ such that: ", "page_idx": 21}, {"type": "text", "text": "\u2022 Any constantly memorizing learning algorithm $\\mathcal{L}$ learning on an (arbitrary) augmented dataset $T_{\\mathcal{C}}^{\\prime}$ generated by a generator ${\\cal T}_{\\cal C}^{\\prime}\\;=\\;{\\cal G}({\\cal T}{\\cal c})$ with $G$ running in time $\\bar{o(k(n_{C}))}$ : If zCL(nC) = o( k(nnCC ) ), then L needs \u2126( z $\\Omega\\big(\\frac{k(n_{C})}{z_{\\mathbb{C}}^{\\mathcal{L}}(n_{C})}\\big)$ many samples to learn each concept $\\mathcal{C}=(U_{\\mathcal{C}},f_{\\mathcal{C}},n_{\\mathcal{C}})\\in\\mathbb{C}.$ .   \n\u2022 There exists a learning algorithm $\\mathcal{L}^{\\ast}$ learning each concept from a single sample with $z_{\\mathbb{C}}^{\\mathcal{L}^{*}}(n_{\\mathcal{C}})=O(n_{\\mathcal{C}})$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Let us fix $k\\in\\mathcal{K}$ and define our concept class $\\mathbb{C}$ with applications $E_{\\mathcal{C}},T_{\\mathcal{C}}$ as given by Theorem A.5. ", "page_idx": 21}, {"type": "text", "text": "The second part follows immediately by Theorem A.5. ", "page_idx": 21}, {"type": "text", "text": "The first part: Let $\\mathcal{L}$ be a constantly memorizing learning algorithm $\\mathcal{L}$ with a generator $G$ . Assume that $\\begin{array}{r}{z_{\\mathbb{C}}^{\\mathcal{L}}(n\\boldsymbol{c})=o(\\frac{k(n_{c})}{n_{c}})}\\end{array}$ and, $\\mathcal{L}$ learns with $\\begin{array}{r}{\\overline{{\\cal T}}_{\\mathcal{C}}^{\\prime}|=o\\big(\\frac{k(\\bar{n_{C}})}{z_{\\mathbb{C}}^{\\mathcal{L}}(n_{C})}\\big)}\\end{array}$ . Then by Definition A.6, there exists a constant $B$ such that for every ${\\mathcal{C}}\\in\\mathbb{C}$ , $\\mathcal{L}$ runs in time: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{B*|T_{c}^{\\prime}|*Z_{c}\\leq B*|T_{c}^{\\prime}|*z_{\\mathbb{C}}^{\\mathcal{L}}(n_{\\mathcal{C}})=o(\\frac{k(n_{c})}{z_{\\mathbb{C}}^{\\mathcal{L}}(n_{c})}*z_{\\mathbb{C}}^{\\mathcal{L}}(n_{\\mathcal{C}}))=o(k(n_{c})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, we can summarize that $G$ runs in $o(k(n_{\\mathcal{C}}))$ , $\\mathcal{L}$ runs in $o(k(n_{\\mathcal{C}}))$ , and $\\mathcal{M}=\\mathcal{L}(T_{c}^{\\prime})$ runs in $z_{\\mathbb{C}}^{\\mathcal{L}}(n c)=o(\\frac{k(n_{c})}{n_{c}})$ . This can\u2019t be, as Theorem A.5 states that we need more computation time. ", "page_idx": 21}, {"type": "text", "text": "B Specifications of PEN, PERM, HSS, and MUL ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "B.1 Properties of the sub-task definition ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this Appendix section, we expose more details about our definition of the set of sub-tasks $\\boldsymbol{S}$ and the properties that it shows. In particular, we characterize it through different angles, namely: ", "page_idx": 22}, {"type": "text", "text": "\u2022 atomicity: each sub-task is an atomic unit roughly corresponding to a basic block of operations in the equivalent procedural programming language. \u2022 minimality: among all the possible definitions of sub-tasks, we define $\\boldsymbol{S}$ to be the minimal set of sub-tasks, where every primitive operation is made observable in exactly one sub-task. ", "page_idx": 22}, {"type": "text", "text": "\u2022 usefulness: the defined set of primitives is useful to learn the compositional task. We validate this in our experiments on task de-composition (Section 4.2), which shows that the model learns and compose the given set of primitives when it is trained on the full compositional task. ", "page_idx": 22}, {"type": "text", "text": "\u2022 uniqueness: by imposing independent observability on the different primitives, we can relax the constraint on the bijection between primitives and sub-tasks, allowing for sub-tasks that contain more than a single primitive. As a result, the definition of $\\boldsymbol{S}$ is not unique. However, this does not undermine the validity of the investigation, since we are measuring a relative phenomenon. The inefficiency of Transformer-based language models that we observe is always relative to the considered set of chosen sub-tasks. However, considering all the properties above, we speculate that similar results could be also measured for alternative definitions of the sub-tasks set $\\boldsymbol{S}$ . ", "page_idx": 22}, {"type": "text", "text": "B.2 Details on the PEN task ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In addition to the task description in Section 3, there are some additional constraints, visualized in Figure B.5: ", "page_idx": 22}, {"type": "text", "text": "1. The (yellow) neighbors of the green chain build a chain themselves, starting at the second word in the sequence (i.e. the first yellow word).   \n2. The full sequence is at least double the length of the green matching sequence. With all other \u2019free\u2019 green words, we build another matching sequence starting at a random \u201cfree\u201d green position (we call it the \u201cfree\u201d matching sequence, see the blue arrows in Figure B.5). This is important because then the model cannot easily tell apart the green positions of the correct matching sequence with the green positions of the \u201cfree\u201d matching sequence without either matching all green tokens from the start or looking at the answer words and starting matching from their left neighbors.   \n3. Each yellow word next to a true green matching word (except the terminating word in the yellow matching sequence) has a doppelganger (which we also call attention trap) next to a \u201cfree\u201d green word. We add this to \u201cconfuse\u201d the model: since the model learns to match words to words, it could be tempted to match yellow words, which would give it the wrong order of neighbors. To make them distinguishable in the answer, we define the doppelganger to match equally to the previous and next word but have a different \u201cdata\u201d number in the middle. ", "page_idx": 22}, {"type": "text", "text": "Figure B.6: How a language model with next-token-prediction can solve the PEN task. Take the most recent word in the answer, find its left neighbor (\u201cleft\u201d), find the match (\u201cmatch\u201d), and retrieve its right neighbor (\u201cright\u201d). This corresponds to the sub-tasks RCpy (e.g., in this task, the model has to find left neighbors), PE, and Cpy. The data efficiency in de-compositionality (see Tables C.10, C.11) provide some evidence that this is the natural way a model solves the task when being close to $100\\%$ performance and without being trained on the sub-tasks. ", "page_idx": 23}, {"type": "text", "text": "C Training experiments details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "C.1 LLaMA training hyperparameters ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We use a batch size of 384 samples, which corresponds to ca. $250\\,K$ tokens per batch for the PEN task and $50\\,K$ for the PERM task. Our $150\\,M$ parameter model contains of 12 layers and a hidden size of 1024. The learning rate is $10^{-4}$ , as in the original paper [2]. ", "page_idx": 23}, {"type": "text", "text": "C.2 Details on the discrete solver of the PEN Task ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our discrete learning algorithm which can learn the PEN task from a single example is a 2-gram model over 11 words resulting in 121 parameters with 8 values each. This results in a search space of size $1.9*10^{109}$ . Each value is an action function that manipulates the current word in the forward pass of the model (at the beginning of the forward pass it is set to \u201cerror\u201d) and gives back one of the 11 action outcomes. The last two action outcomes (initially (BOS,BOS)) result in the next 2-tuple determining the next discrete parameter. As this execution could loop or run over many parameters, we limit the depth to 12. The actions and their outcomes are: ", "page_idx": 23}, {"type": "text", "text": "\u2022 EOS: retrieves the eos constant (outcome: EOS)   \n\u2022 LAST-OUTPUT: retrieves the last output word (or the start word if it is the beginning of the answer) (outcome: LAST-OUTPUT)   \n\u2022 MATCH: matches, if there is no next match, this will return error (outcome: MATCH)   \n\u2022 LEFT: get the left neighbor of the current word (outcome: LEFT)   \n\u2022 RIGHT: get the right neighbor of the current word (outcome: RIGHT)   \n\u2022 IS-START: is it the beginning of the answer (outcomes: IS-START-TRUE, IS-START-FALSE)   \n\u2022 IS-ERROR: is the current state error (outcomes: IS-ERROR-TRUE,IS-ERROR-FALSE)   \n\u2022 OUTPUT: output the current word (outcome: OUTPUT)   \n\u2022 For an entry point we define an additional action outcome: BOS ", "page_idx": 23}, {"type": "text", "text": "The search algorithm uses a simple hill-climbing heuristic with restarts towards getting as many answer words in the sample right as possible. To search more effectively, we add a penalty to changing the same parameter over and over. The most challenging part of our novel pointer execution tasks is to learn the left-match-right structure, which is easily found by this algorithm. ", "page_idx": 23}, {"type": "text", "text": "C.3 Accuracy tables ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The numbers are visualized in Section 4. Each row in the tables C.2, C.3 C.4, C.5, C.6, and C.8 corresponds to one run with the respective data mix. ", "page_idx": 24}, {"type": "table", "img_path": "x7AD0343Jz/tmp/dd3b6d74415ebbc31c3c767681c2314860a5fee0c8cc148d94598b1351e4f027.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table C.2: Performance of LLaMA on in-distribution test samples of the Pointer Execution\u2019s neighbor (PEN) task and its sub-tasks. Training is until convergence. We used the standard language modeling loss on every next token in the input, which performed best. The best validation performance is taken for each task separately. As shown, while models learn all sub-tasks Cpy, RCpy, and PE very well, on the PEN task (which is a composition of the sub-tasks Cpy, RCpy, and PE) they need much larger amounts of data than on any of the sub-tasks. This observation makes hypothesis $\\mathcal{H}_{4}$ most plausible for PEN. ", "page_idx": 24}, {"type": "table", "img_path": "x7AD0343Jz/tmp/ff7a387d71caebbec37ca44afbc4c37fffd6b573f2ebd0275a26899668affa50.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table C.3: Results of LLaMA models trained only on the Pointer Execution\u2019s neighbor (PEN) task. Compared to Table C.2, we observe that the model does not seem to systematically profit from the compositionality given there. ", "page_idx": 24}, {"type": "table", "img_path": "x7AD0343Jz/tmp/fb7dc5f3e1df5b5e56d04a997ad7f78594ba492bbb0418ef3d8028ea7162c015.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table C.4: Performance of LLaMA models on in-distribution test samples of the Pointer Execution Reverse Multicount (PERM) task and its sub-tasks. Similar to Table C.2, we observe that although all sub-tasks are learned perfectly by the models, their composition does not learn fully until including an almost 10-fold increase of the data needed for all sub-tasks together, making hypothesis $\\mathcal{H}_{4}$ most plausible for this task too. ", "page_idx": 24}, {"type": "table", "img_path": "x7AD0343Jz/tmp/d062f8a05f9c4d011c612478d93f7ae36c3185441377d1eb7491d7a51ffa4d35.jpg", "table_caption": ["PERM PERM "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "x7AD0343Jz/tmp/f864fb3fc9345f9ace527c8eb2ff72893acdef195858cfee59306ad7376ba5dc.jpg", "table_caption": ["Table C.5: Results of LLaMA models trained only on the PERM task. Compared to Table C.4, we observe that the model does not seem to systematically profti from the compositionality given there, similar to PEN. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "x7AD0343Jz/tmp/d5c9ba8f72d7dcb7f9053f388773f10391945bace82a6576ba8a9b58e957a3a4.jpg", "table_caption": ["Table C.6: Performance of LLaMA models on Highest Subsequence Sum and Highest Subsequence Execution. Hypothesis $\\mathcal{H}_{4}$ is the most plausible. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table C.7: Performance of LLaMA models only on the Highest Subsequence Sum task. ", "page_idx": 25}, {"type": "table", "img_path": "x7AD0343Jz/tmp/57e97b30c65260ee9be655342e69e000985a881628929b2869d8db62081d9fc4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Table C.8: Performance of LLaMA models on Digit Multiplication (DMUL), addition (ADD), and multiplication (MUL). Hypothesis $\\mathcal{H}_{4}$ is the most plausible. ", "page_idx": 26}, {"type": "table", "img_path": "x7AD0343Jz/tmp/d6cd5ad1627503b49651264a3f9e5a5fa6ec13fc5816b1fe58bab53dd385c9f5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Table C.9: Performance of LLaMA models only on multiplication (MUL). ", "page_idx": 26}, {"type": "text", "text": "C.4 Performance of LLaMA on decompositionality ", "text_level": 1, "page_idx": 27}, {"type": "table", "img_path": "x7AD0343Jz/tmp/038f2cb6666d522b316d4acd63e9d9839166efac38242efead6dee9f7dc0fb3c.jpg", "table_caption": ["In Tables C.10 and C.11 we include more details results about experiments on decompositionality with LLaMa. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table C.10: Performance of LLaMA models pre-trained on Pointer Execution\u2019s neighbor (PEN), and after finetuning on Pointer Execution Verbose (PEV) until convergence (i.e., $\\mathrm{PEN}{\\rightarrow}\\mathrm{PEV}$ ). We observe that decompositionality significantly improves data efficiency compared to training a model on the finetuning task from scratch. At the same time, however, for learning the task to full performance, it still seems to need more than a low constant number of demonstrations $(\\geq50\\,\\mathrm{K})$ , making the $\\mathcal{H}_{1}$ -equivalent (i.e. \u201cA Transformer language model learns a decomposition with a constant number of samples\") rather implausible for decompositionality as well. ", "page_idx": 27}, {"type": "table", "img_path": "x7AD0343Jz/tmp/b2730cfd31c7930f25e058b7ad51da210e0b0b8834f481670de2009792a92b32.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table C.11: Performance of LLaMA models pre-trained on Pointer Execution Reverse Multicount (PERM), and after finetuning on Pointer Execution (PE) until convergence (i.e., PERM $\\rightarrow\\mathrm{PE}$ ). As shown similarly in Table C.10, decompositionality significantly improves data efficiency compared to training a model on the finetuning task from scratch. For learning the task to full performance, however, it still seems to need more than a low constant number of demonstrations $(5{-}20\\,\\mathrm{K})$ , making the $\\mathcal{H}_{1}$ -equivalent \u201cA Transformer language model learns a decomposition with a constant number of samples\" rather implausible for decompositionality. ", "page_idx": 27}, {"type": "text", "text": "We ablate a variation of our model architecture based on the Universal Transformer (UT) Csord\u00e1s et al. [61] and the more recent Hyper-UT [17], for both of which there is evidence of better compositional generalization than the original Transformer. For instance, a UT-style transformer displayed good performance on compositional tasks like SCAN Csord\u00e1s et al. [61]. The model based on UT reached a similar performance as the original LLaMA model, i.e. a behavior that could be classified in $\\mathcal{H}_{4}$ . We then also experimented with a Hyper-UT-style LLaMA, based on the aforementioned Hyper-UT work [17], in which each layer selects its weights from a common weight embedding pool to realize a parameter-efficient model. This model consistently achieved a lower accuracy compared to LLaMA on a large set of small algorithmic tasks. Thus, we chose not to systematically explore its performance in a more challenging compositional algorithmic setting. The results are shown in table C.12. The hyperparameters used to train the model are the same as the ones used for the standard models (see Appendix C.1). ", "page_idx": 28}, {"type": "image", "img_path": "x7AD0343Jz/tmp/a259c4243fff24d7094323d9eb91fb0611eb6ff6d5e0fbd3bab2e7e1c088102f.jpg", "img_caption": ["Table C.12: Performance of LLaMA with weight sharing across all transformer layers on the PERM task and its sub-tasks. We see similar results compared to Table C.4, i.e., hypothesis $\\mathcal{H}_{4}$ is most plausible for the PERM task on this model. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "D Prompting experiments details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "D.1 Input sequences tokenization ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Unlike open-source models, such as the LLaMa, closed-source LLMs do not allow to customize the tokenization process. Hence, tokenization could potentially represent a major confounder for our prompting experiments. If fact, an erroneous mapping between words and tokens would inevitably undermine the ability of the model to correctly perform matching operations in the input sequence, making it impossible to successfully reconstruct the correct path in pointer execution tasks. In this Appendix section, we provide more details on a set of control experiments that were designed to ensure that this phenomenon did not play a role in our empirical evaluation. ", "page_idx": 29}, {"type": "text", "text": "As a preliminary step, we ablated in our experiments different strategies to structure our input components. However, we found that alternative designs for the tasks structure (e.g. LL- $\\cdot\\mathbb{N}$ -LL, where L represents a lower-case Latin letter and N a one-digit number) were always detrimental for the task accuracy compared to the current sequence design (i.e. LLNLL). To ensure that the input sequences of our pointer execution tasks were effectively tokenized as expected, we run some quantitative experiments with GPT-4\u2019s open-source tokenizer (tiktoken). Unfortunately, we could not find an open-source tokenizer for Gemini-Pro; the available token counter only provides limited information about the actual tokenization. ", "page_idx": 29}, {"type": "text", "text": "Analyzing the GPT-4 tokenizer results, we found that the digit delimiter ensures safe splitting within a word in $100\\%$ of the samples, i.e. LLNLL always yields [LL, N, LL]. Additionally, we observed that $13.2\\%$ of the 2-grams were split into two different tokens (e.g., bq is tokenized to b and q), which may affect the attention mechanism. As an additional experiment, we removed these 2-grams from the dataset. However, removing the \u201csplittable\u201d 2-grams did not result in an improvement of the performance (the accuracy decreased from 0.19 to 0.05 on PEN) when using the best-performing prompting technique with GPT-4 (Code Interpreter). ", "page_idx": 29}, {"type": "text", "text": "D.2 Natural-PEN ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We additionally designed a natural variation of the PEN task, dubbed Natural-PEN, where we replaced the synthetic 2-gram sequences used for the matching in the original tasks with 3-gram, in-distribution, natural English words. Natural-PEN inherits a similar structure of the components from the original PEN task, WNW, where $\\boldsymbol{\\mathsf{W}}$ in this case are 3-characters English words. In practice, we flitered all valid 3-gram words from Scrabble3 that were translated to a single token when using GPT-4\u2019s tokenizer. This gave us a set of 707 words (out of the original 1338 words). Similar to the experiment in the previous paragraph, we find that the GPT-4\u2019s performance using the best prompting techniques does not improve on this in-distribution, natural English words, yielding even a lower accuracy on Natural-PEN (Table D.13). ", "page_idx": 29}, {"type": "table", "img_path": "x7AD0343Jz/tmp/4211132804f10ea3402da4f8c526f17e305e0c122f299f2504468bde23f23ec6.jpg", "table_caption": ["Table D.13: Result of GPT-4 on the Natural-PEN task. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "D.3 Many-shot prompting ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We additionally ablated the number of shot provided to the LLMs to perform in-context learning. We increased the number of shots from 8 examples up to 32 examples (64 did not fit into the $8\\mathbf{k}$ context window of the old GPT-4 version we are benchmarking). However, we did not observe any performance improvement, as reported in Table D.14. ", "page_idx": 29}, {"type": "table", "img_path": "x7AD0343Jz/tmp/b479b553a08a0b898104cd1547098a2605a89c72950239a2f8fdcf7697aa71c8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table D.14: Ablation on the number of shots used to do in-context learning in PEN and PERM, for both GPT-4 and Gemini. In particular, the number of shots is increase from 8 examples to 32 examples. Despite increasing the partial metrics, providing more shots does not improve the overall task accuracy in both PEN (same, $0\\%$ ) and PERM (the accuracy goes to $0\\%$ for both models). ", "page_idx": 30}, {"type": "text", "text": "D.4 Prompting OpenAI o1-preview ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section, we include further results with the newly-released model from OpenAI, o1-preview4, specifically trained to tackle complex tasks in science, coding, and math. The results are reported in Table D.15. ", "page_idx": 30}, {"type": "image", "img_path": "x7AD0343Jz/tmp/894e96f7a5cea69e91fad80ecaeefcba0e253d9d74df474eb9b01893af1d5f0f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table D.15: Results of o1 with various prompt settings. While o1-preview cannot infer the task from examples only, it achieves much higher accuracy when given a description of how to perform the task. Our hand-crafted step-by-step solutions achieve higher accuracy than the solutions that o1 finds when asked for Chain of Thought (CoT) only. ", "page_idx": 30}, {"type": "text", "text": "D.5 Prompt examples ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we include some exemplary prompts that were used in the PEN and PERM experiments with pre-trained LLMs. We start with the vanilla few-shot (8) prompt, shown in Table D.16. ", "page_idx": 31}, {"type": "text", "text": "EXAMPLE: xv ke vu7bh sb0fz xy5ih eo7sf ay7of xd3nj zs7bt eo1sf jn6yc xd5nj od3nk br2ny yc2pr ls5sg nv1zs sb5fz uy7vu sf1zv bh6ia sg5dg ux6oc zv4xd ya1yk br5ny wc4xy ke5fm jw1dx ny7sb wq2mm fz6eo nk2nv sf5zv pr3ya fz4eo yk0dk fm4br oc4wc nj0ls ih1uy di7fw mm2pq zv7xd of7wq nj4ls xv7gn ls6sg dx0ux vz7uc ah7od sg4dg sn2jw ae5ce ia7jn zw4ed bt5ay fm6br pq6kw ny3sb gn4ah ke0fm   \nANSWER: ke ls6sg ke0fm sg4dg br2ny sf5zv sb5fz eo1sf fm6br xd3nj nj4ls fz6eo zv7xd ny3sb   \nEXAMPLE: wk qp ld2ki lt1ui te3ob et4mm ss1hi lt0ui ki3ay ad2dh zk1cc cl6lt zz3zs cl1lt hi3ct ap4ys sf1sz bc4yh ct4gf gn5ap mt5zd mm5gn sr1ne yh3et up0vl lu2cl no1ps ys3lu vm6us et6mm us4ha qp1ad zd6sf dh7bc ps3ld bc6yh sv5ch ad4dh qe4hd mm6gn ha0qe dh3bc vl3mt ys4lu ob2zk qp0ad wk7zz lu0cl ch7up ac2hh ne1te qo4qz cc2rp gx2yq xv3vm gn4ap rp2ss sd5gc zs6no yh4et tl7sv xs5jm sz0sr ww1rb ay6xv ap2ys   \nANSWER: qp lu0cl cl1lt yh4et ys3lu bc6yh lt1ui ad2dh ap2ys gn4ap et6mm qp1ad dh3bc mm6gn   \nEXAMPLE: fy kn fz2jy st6zr wk4hz zo3st gz0nt ri2ru dt0rh ri4ru kq0lt st0zr pp3da ru2io lw0vk zo1st sx5xe ej2sj er1fz kn2ej fd2pp ej6sj mc7gs io0zo si4lp ns1jr xe6nb zn5ns vk4fd sj0im lt7lw kn4ej nb0lg sj2im lg0qi ru6io gs7yf zr3zn jy1wk im1ri fy5er ns7jr qi5dt io1zo yf7gz zn4ns nt6do im6ri lp6mc mk2ym do2kq zh3vh ox5s fk0sk hz3sx zr2zn   \nANSWER: kn ns7jr kn2ej st6zr im1ri zo3st zr2zn ej2sj zn5ns sj2im ru6io io1zo ri4ru   \nEXAMPLE: xr ql hq6za co0gt yv6md sk5zi xr3kq zi0co zl4om mc7ha gy3ej xj0qp ud2hq ha0km yu7xa gt6nb rb4mi xj4qp li3ib km1cf lj2ek qp6sk ib4dv ha5km fc5gy gx1xd om7hb tx1mc al0lw gt4nb lw5fc xd6nj ek1tr nb5tx rz3yv ql1xj xu4it km0cf mi5vo gx5xd xa1in xd7nj vo6xz tx6mc dv4al co6gt kq2li sk6zi tr1zl cf7gx md0je mc4ha hh7rz cf2gx gp7yu zi2co je0gp nb0tx in2rb qp0sk it7hh lq4cl ej6lj ql0xj xz0ud sa2sc   \nANSWER: ql zi0co sk6zi km1cf ha5km co6gt gt4nb xd6nj gx1xd xj0qp ql0xj qp6sk nb5tx cf7gx mc7ha tx1mc   \nEXAMPLE: bn xy fv6wc tn0jp sk2qb vy7mw od1hf ju6vs xf1kc xy6zn ue0dr zn7vy ya5fv ls4tn qb2km tn6jp yw6yn zn6vy pc5yw jp4vc af6rc mw7ls hf4om mw5ls kc4wt vv7ju hk3hn vy4mw hn3af vc3vv wt6pc vc6vv yn1sm ls2tn pe4pq jk3gb km7ep zv5gv om4vk mb7mz dr7hk jp0vc ep2pe is5ie sm2sk th7ba vz3ya ju1vs wc2ue xy2zn at5xd wq2wv bn5vz vv4ju vk3at gr0xr xd2xf up2vq   \nANSWER: xy vv4ju ju1vs ls4tn tn0jp xy2zn zn7vy jp0vc vy4mw vc3vv mw7ls   \nEXAMPLE: kt nn fz2kv rv6wb kh6vu js4wk et7lx zk0sb ie3zb wb6mx vu5et nn7zx zb6zn pp3js kv4hk gt6rv bf4vr mx3zk sv5ok zk6sb gk1rx pp1js vg5kh wk5gt vr3fz zx2pp zu4ac nn3zx qe6rl gt4rv ac2tp rv5wb kt6zu sb4ic hk2xe sb1ic rx1xm mx6zk zn6vg az3vp xm4qe zx0pp dm1lt hl4li ok0ja wk3gt bc6bf tl2eb jn7dm ve3jl lt1bc tg5gp tp6sv wb4mx xe6ie ot2pj ja6gk js7wk   \nANSWER: nn sb4ic nn3zx rv5wb wb4mx zk6sb wk3gt js7wk pp1js mx6zk zx0pp gt4rv   \nEXAMPLE: pq gg vs1dy cr4yf xt5kz gg1gn tg0fn gn4cr pq4jj tc1ts qh0ek yf2tc sk2al tc4ts jl3xi cr3yf ub1zh ts7lh tp2vb ex4px kz2kn gn3cr ez7up gg4gn dn2ez jm4ex ek1sz lh0eu kn5cg jm6ex jj5fp yf3tc il5wn eu3jm bd5fc ex7px wn5yw ir0xf fc3vs lh1eu fp0bd eu2jm fn6jl pa4zf zh4ku ep6jf dy4xt ts6lh vc5sk dv5nw sz2ub di7yp vb2dn tk0as ku0km mw7mx km0tp hi3pl bg4il nl1hv xi0bg kk2ao al1tg dx2ca up4vc ru0dc   \nANSWER: gg tc1ts yf3tc eu2jm ex7px lh1eu cr4yf ts6lh gg1gn gn3cr jm6ex   \nEXAMPLE: if xe gw0is zl5jz if2mv wa5xs vs1st nz2wc ox0qs ko5xq xj4ue ko3xq hx7ym ca6zl is0bf xq6ed gs7ga xs0yg mv5ox ca5zl bg0bk xq1ed ry2gw rt7ca bf7lx xe1wa ym3vs xs7yg qs1ee wc6cb pe6ag ed3nz xn4dt zl7jz on6oc xe7wa ps0xj yg7ko st5ps cb2rt ag3xn wc2cb bk7hx rt2ca ee1ry cb4rt ra4on wa2xs lx7az ed0nz dt1ra be3hb rc4gs nz5wc az7rc yg0ko ue2pe or4fg   \nANSWER: xe wa5xs ca5zl ko5xq wc6cb cb4rt rt7ca zl5jz xq6ed xe1wa ed0nz yg0ko nz5wc xs0yg   \nYOUR QUESTION: vq sk zg5gr ne7vc co7os hc6cv pq3ss dp4je vq3sx pu0dp wn2co vc6hc dw5vy ne6vc vy0oe gp6zm ka1rw vc1hc tz5mx je3hy ab2pq pu7dp oe3ka on5is lf1ju hy6gp gr5jp gp5zm os2dq sk6ne dq7cn on0is ju0qq cv2on mx1mb is0pu sx1wn cv1on rw4sz je6hy jp1tz dp0je io1lf is4pu ss5dw sk2ne sz4ij hc1cv qq6ab uk0af cn4zg hy2gp   \nClearly mark your answer by writing \u2019Answer: <your answer>\u2019 as last line. ", "page_idx": 31}, {"type": "text", "text": "Table D.16: GPT4 and Gemini prompt for Pointer Execution\u2019s Neighbor (PEN) task with only few-shot examples of the task. The tasks are encoded so that every word start and word end of a word in the input are encoded as separate tokens, so a model can pattern-match the respective token to do the matching operation. ", "page_idx": 31}, {"type": "text", "text": "Then, we introduce a natural language description of the task, prepended before the examples of the task, as shown in Table D.17. ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "I give you a sequence of words. Each word has four characters plus a middle, words are separated by spaces. Start with the leftmost word. Output its neighbor. Then, match the last two characters of the current word (i.e. not the neighbor) to the word starting with those two characters. Again, output the neighbor. Do this until your current word (not the neighbor) has no match anymore.   \nEXAMPLE: xv ke vu7bh sb0fz xy5ih eo7sf ay7of xd3nj zs7bt eo1sf jn6yc xd5nj od3nk br2ny yc2pr ls5sg nv1zs sb5fz uy7vu sf1zv bh6ia sg5dg ux6oc zv4xd ya1yk br5ny wc4xy ke5fm jw1dx ny7sb wq2mm fz6eo nk2nv sf5zv pr3ya fz4eo yk0dk fm4br oc4wc nj0ls ih1uy di7fw mm2pq zv7xd of7wq nj4ls xv7gn ls6sg dx0ux vz7uc ah7od sg4dg sn2jw ae5ce ia7jn zw4ed bt5ay fm6br pq6kw ny3sb gn4ah ke0fm Answer: ke ls6sg ke0fm sg4dg br2ny sf5zv sb5fz eo1sf fm6br xd3nj nj4ls fz6eo zv7xd ny3sb   \n${<}7$ MORE EXAMPLES>   \nYOUR QUESTION: vq sk zg5gr ne7vc co7os hc6cv pq3ss dp4je vq3sx pu0dp wn2co vc6hc dw5vy ne6vc vy0oe gp6zm ka1rw vc1hc tz5mx je3hy ab2pq pu7dp oe3ka on5is lf1ju hy6gp gr5jp gp5zm os2dq sk6ne dq7cn on0is ju0qq cv2on mx1mb is0pu sx1wn cv1on rw4sz je6hy jp1tz dp0je io1lf is4pu ss5dw sk2ne sz4ij hc1cv qq6ab uk0af cn4zg hy2gp   \nClearly mark your answer by writing \u2019Answer: <your answer>\u2019 as last line. ", "page_idx": 32}, {"type": "text", "text": "Table D.17: GPT4 and Gemini prompt for Pointer Execution\u2019s Neighbor (PEN) task with few-shot examples of the task and a task description. ", "page_idx": 32}, {"type": "text", "text": "On top of this, we also add a request to reason step-by-step (CoT), as shown in Table D.18. ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "I give you a sequence of words. Each word has four characters plus a middle, words are separated by spaces. Start with the leftmost word. Output its neighbor. Then, match the last two characters of the current word (i.e. not the neighbor) to the word starting with those two characters. Again, output the neighbor. Do this until your current word (not the neighbor) has no match anymore.   \nEXAMPLE: xv ke vu7bh sb0fz xy5ih eo7sf ay7of xd3nj zs7bt eo1sf jn6yc xd5nj od3nk br2ny yc2pr ls5sg nv1zs sb5fz uy7vu sf1zv bh6ia sg5dg ux6oc zv4xd ya1yk br5ny wc4xy ke5fm jw1dx ny7sb wq2mm fz6eo nk2nv sf5zv pr3ya fz4eo yk0dk fm4br oc4wc nj0ls ih1uy di7fw mm2pq zv7xd of7wq nj4ls xv7gn ls6sg dx0ux vz7uc ah7od sg4dg sn2jw ae5ce ia7jn zw4ed bt5ay fm6br pq6kw ny3sb gn4ah ke0fm Answer: ke ls6sg ke0fm sg4dg br2ny sf5zv sb5fz eo1sf fm6br xd3nj nj4ls fz6eo zv7xd ny3sb   \n${<}7$ MORE EXAMPLES>   \nYOUR QUESTION: vq sk zg5gr ne7vc co7os hc6cv pq3ss dp4je vq3sx pu0dp wn2co vc6hc dw5vy ne6vc vy0oe gp6zm ka1rw vc1hc tz5mx je3hy ab2pq pu7dp oe3ka on5is lf1ju hy6gp gr5jp gp5zm os2dq sk6ne dq7cn on0is ju0qq cv2on mx1mb is0pu sx1wn cv1on rw4sz je6hy jp1tz dp0je io1lf is4pu ss5dw sk2ne sz4ij hc1cv qq6ab uk0af cn4zg hy2gp   \nReason step by step. Clearly mark your answer by writing \u2019Answer: <your answer>\u2019 as last line. ", "page_idx": 32}, {"type": "text", "text": "Table D.18: GPT4 and Gemini prompt for Pointer Execution\u2019s neighbor task with a task description, few-shot examples of the task, and a request to reason step-by-step. ", "page_idx": 32}, {"type": "text", "text": "We ablate the latter with a variation from Yasunaga et al. [28]. ", "text_level": 1, "page_idx": 32}, {"type": "table", "img_path": "x7AD0343Jz/tmp/4882750a2a04f2606972da5ee1b34446c9602818a910b3ad6f99859f298b2c22.jpg", "table_caption": [], "table_footnote": ["Table D.19: GPT4 and Gemini prompt for PEN with the prompting technique introduced by Yasunaga et al. [28]. "], "page_idx": 32}, {"type": "text", "text": "We further improved the level of help in the prompt by providing few-shot chain-of-thought examples and sub-task few-shot chain-of-thought examples in Table D.20 and D.21, respectively. ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "I give you a sequence of words. Each word has four characters plus a middle, words are separated by spaces. Start with the leftmost word. Output its neighbor. Then, match the last two characters of the current word (i.e. not the neighbor) to the word starting with those two characters. Again, output the neighbor. Do this until your current word (not the neighbor) has no match anymore.   \nEXAMPLE: kt gu vv1vk xd4sp ra3kn fc6cd aq2oa co4wn wb7rr fl4tg zc1rk la7kb di1hn by3kj rk1fy zu6qf un6nb fe6fw fb4wb gu3f lug5fb wn2kz hn5vv qy2cp nb7di ku6ah zh7qm ob1co rr6ti hi4ob fy1ra lc2sh oa6bw nk3pr ti2aq kz1nk kt2ug tg1hi op0zc ln0jz vk4op qk3ev bw4zh pr4ju   \nThe leftmost word is $\"\\mathrm{kt}\"$ . Its right neighbor is \"gu\", so the first output word is $\\\"\\mathrm{gu\"}$ .   \nNow, we need to find a word that starts with $\"\\mathbf{k}\\mathbf{t}\"$ . The word is \"kt2ug\". Its right neighbor is \"tg1hi\", so the next output word is \"tg1hi\".   \nNow, we need to find a word that starts with $\"\\mathrm{ug^{\\prime\\prime}}$ . The word is \"ug5fb\". Its right neighbor is \"wn2kz\", so the next output word is \"wn2kz\".   \nNow, we need to find a word that starts with $\"\\mathrm{fb}\"$ . The word is \"fb4wb\". Its right neighbor is \"gu3fl\", so the next output word is \"gu3fl\".   \nNow, we need to find a word that starts with \"wb\". The word is $\\mathrm{^\\prime\\,wb7rr^{\\prime\\prime}}$ . Its right neighbor is \"fl4tg\", so the next output word is \"fl4tg\".   \nNow, we need to find a word that starts with $\"\\mathbf{r}\\mathbf{r}\"$ . The word is \"rr6ti\". Its right neighbor is \"hi4ob\", so the next output word is \"hi4ob\".   \nNow, we need to find a word that starts with \"ti\". The word is \"ti2aq\". Its right neighbor is \"kz1nk\", so the next output word is \"kz1nk\".   \nNow, we need to find a word that starts with \"aq\". The word is \"aq2oa\". Its right neighbor is \"co4wn\", so the next output word is \"co4wn\".   \nNow, we need to find a word that starts with \"oa\". The word is \"oa6bw\". Its right neighbor is \"nk3pr\", so the next output word is \"nk3pr\".   \nNow, we need to find a word that starts with \"bw\". The word is \"bw4zh\". Its right neighbor is \"pr4ju\", so the next output word is \"pr4ju\".   \nNow, we need to find a word that starts with \"zh\". The word is \"zh7qm\". Its right neighbor is \"ob1co\", so the next output word is \"ob1co\".   \nThere is no word that starts with \"qm\", so we are done with the matching.   \nTherefore the answer is: \"gu tg1hi wn2kz gu3f lfl4tg hi4ob kz1nk co4wn nk3pr pr4ju ob1co\"   \n$^{<7}$ MORE EXAMPLES $;>$ YOUR QUESTION: jm cq nu7nr th6cc du5ij ki7am nr0ad cq6th md4lk am4rw iu0lq bj7mu rp3ja rk1cs od3dm se3vv iw1hz si4rf dd5du mu4of wd0rt en2yt vw0al nb0ir qp6do ni0ff rt0ik sw2ji do5gn sh1bk kf0iu mm5mi ij2md cc4pq ad6dd rw0mm gn0rp ox5uc al2qp bo1fq hz3wd la1rb dm5vw de0ko jm7kf mi5bj wr1iw lu7lj lq7nu pq3ki ik6od da5gp   \nReason step by step. Clearly mark your answer by writing \u2019Answer: <your answer>\u2019 as last line. ", "page_idx": 33}, {"type": "text", "text": "Table D.20: GPT4 and Gemini prompt for the Pointer Execution\u2019s neighbor task with few-shot chain of thought (Few-shot CoT) examples and a description. ", "text_level": 1, "page_idx": 33}, {"type": "image", "img_path": "x7AD0343Jz/tmp/1abe6d5e396ade833535efa5b8dcebf7ffbce2864fd41a730020d72216353440.jpg", "img_caption": ["<7 MORE EXAMPLES> "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "YOUR QUESTION: zg kt tm2wb sj1re lg1at pb4sx la3ve qd3wa sc0iu my2sb wb6lg my1sb uq3ms zj1qd da2tm sx7sj xy6ny kt0dq ny7tn re4my os5xu sj2re tx5kp dq3zp yx5da sb2zj na3sc re5my gs5na sb4zj ly4qx qd0wa tn2ma dq6zp ma3gp zp3pb du2tx zj6qd fa4ly tz3cc vp3xy ry7ie ve6gs pb7sx do1ti ow5yy at6uq yk3wc gp7fa gg5eb ti5vp cv0dh ms6do bz0ut qx5bc jd3qb zg6os sx0sj iu6du kt6dq xu6la zp2pb bc4oz yb6vn oe4yx li5fz   \nReason step by step. Clearly mark your answer by writing \u2019Answer: <your answer>\u2019 as last line. ", "page_idx": 33}, {"type": "text", "text": "Table D.21: GPT4 and Gemini prompt for the Pointer Execution\u2019s neighbor task with sub-task chain of thought (Sub-task CoT) examples and a description. ", "page_idx": 33}, {"type": "image", "img_path": "x7AD0343Jz/tmp/d7c158c84303716f8c189124e0ab590135bba4d2284c135e666c031b8752fd86.jpg", "img_caption": ["Table D.22: GPT4 and Gemini prompt for Pointer Execution Reverse Multicount with description and sub-task few-shot chain of thought (Sub-task CoT). This chain-of-thought makes sure that the language model can easily execute all operations. In our experiments, for GPT4 to reliably determine whether in which order two words occur in a sequence needed enumeration. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "In Table D.23, we include the prompt that was used for the code interepreter. ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "I give you a sequence of words. Each word has four characters plus a middle, words are separated by spaces. Start with the leftmost word. Output its neighbor. Then, match the last two characters of the current word (i.e. not the neighbor) to the word starting with those two characters. Again, output the neighbor. Do this until your current word (not the neighbor) has no match anymore.   \nEXAMPLE: bn oi fe6zt vj4ks yk3us fi6mk lo0ox yd5pt dc2rk mk3vj wj2fe wk0bm yo6kt ks5wk ox1yk wk6bm gc7kj oi1f iah7gc ks0wk vq6ah gv7yd as4vq bm3gv vs3ho vj0ks kj0au yd6pt bn7dd mk2vj qg5lx pt4xv ho7lo oi5f iau7qg fi0mk zt3as nd4zd kt1vs gv0yd lx2dc fa0ut dd5ry pt3xv ry7yo bm0gv ANSWER: oi mk2vj pt3xv bm0gv ks5wk gv0yd vj0ks oi5f iyd5pt wk6bm fi6mk YOUR QUESTION: bc ek vq7ze pe7mu vu5pf nu1sb zg2uf vd5vn ep3qt ek2oj ng6ep ig7is bc4sx is2nu sx3lq pe2mu lq4oo sb1vd wh0lw oj7ax qb2vq mu4xm hq6eu nu0sb pf3qb ax2pe yh7bm sb0vd gz5vu vd1vn fa5sj oj0ax eu0zg ek4oj ze3gx xm3ig oo5hq mu7xm bp7gz is7nu lw3ng vf7ko sj3jq xm4ig xp0fa ax6pe gx3wh qc6tc uf5xp ig3is qt5yh rs5lk   \nReason step by step. Then, use the code interpreter to solve the task. Clearly mark your answer by writing \u2019Answer: <your answer>\u2019 as last line. I give you a sequence of words. The last word (after the \"|\") is the word to start with. Now match match the last two characters of the current word to the word starting with those two characters. If this match was going to the left, i.e. the matched word is left of the current word in the sequence, increase a variable counting the number of left matchings. Do this until your current word has no match anymore. Finally, output this sequence of words, in reverse order in the format word.x where $\\mathbf{X}$ is the number of left matchings until the output word times the number of matchings until the output word. Example answer: abcd.4 efab.1 ghef.0   \nEXAMPLE: ud0xg wp0mr yy0uo xg0yy sr0mw pg0yg oq0zt mw0oq uo0bt ep0ep rs0av bt0oi oi0ud mr0pg oc0wp av0oc tz0tb yg0tz tb0rs vk0sx | sr0mw   \nStarting with \"sr0mw\", let\u2019s match and calculate:   \n\"sr0mw\" matches with $\\\"\\mathrm{mw}0\\mathrm{oq}\"$ . No left match, 0 left matches so far.   \n\"mw0oq\" matches with \"oq0zt\". This is a left match, 1 left matches so far.   \nThere are no further matches for \"oq0zt\", so we end the sequence here.   \nReversing the sequence and formatting it as per the instructions, we get:   \noq0zt.2   \nmw0oq.0   \nsr0mw.0   \nThus, the answer is: \"oq0zt.2 mw0oq.0 sr0mw.0\". EXAMPLE: vs0ep xv0tx me0xt xx0ds ds0re re0rm tx0tg sh0vs xt0bw pf0ss gh0sh ep0me bw0xv rm0gh ss0pf kh0of | xx0ds   \nStarting with $\"\\mathbf{x}\\mathbf{x}0\\mathrm{ds}\"$ , let\u2019s match and calculate:   \n\"xx0ds\" matches with \"ds0re\". No left match, 0 left matches so far.   \n\"ds0re\" matches with \"re0rm\". No left match, 0 left matches so far.   \n\"re0rm\" matches with $^{1\\prime}\\mathrm{rm}0\\mathrm{gh^{\\prime\\prime}}$ . No left match, 0 left matches so far.   \n\"rm0gh\" matches with \"gh0sh\". This is a left match, 1 left matches so far.   \n\"gh0sh\" matches with \"sh0vs\". This is a left match, 2 left matches so far.   \n\"sh0vs\" matches with \"vs0ep\". This is a left match, 3 left matches so far.   \n\"vs0ep\" matches with \"ep0me\". No left match, 3 left matches so far.   \n\"ep0me\" matches with \"me0xt\". This is a left match, 4 left matches so far.   \n\"me0xt\" matches with \"xt0bw\". No left match, 4 left matches so far.   \n\"xt0bw\" matches with \"bw0xv\". No left match, 4 left matches so far.   \n\"bw0xv\" matches with \"xv0tx\". This is a left match, 5 left matches so far.   \n\"xv0tx\" matches with $\"\\mathrm{tx}0\\mathrm{tg\"}$ . No left match, 5 left matches so far.   \nThere are no further matches for \"tx0tg\", so we end the sequence here.   \nReversing the sequence and formatting it as per the instructions, we get:   \ntx0tg.60   \nxv0tx.55   \nbw0xv.40   \nxt0bw.36   \nme0xt.32   \nep0me.21   \nvs0ep.18   \nsh0vs.10   \ngh0sh.4   \nrm0gh.0   \nre0rm.0   \nds0re.0   \nxx0ds.0   \nThus, the answer is: \"tx0tg.60 xv0tx.55 bw0xv.40 xt0bw.36 me0xt.32 ep0me.21 vs0ep.18 sh0vs.10 gh0sh.4 rm0gh.0 re0rm.0 ds0re.0 xx0ds.0\" ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "YOUR QUESTION: lw0ws gs0gs eq0eq yz0pt ws0lw um0qd ea0ea tf0um df0df uo0tf fl0uo pt0yz po0ec | fl0uo Reason step by step, Clearly mark your answer by writing \u2019Answer: <your answer>\u2019 as last line. ", "page_idx": 35}, {"type": "text", "text": "Table D.24: GPT4 and Gemini prompt for Pointer Execution Reverse Multicount with description and fewshot-chain-of-thought we found to be similar to the chain of thought the model chooses naturally. The 0s in the input words are inserted to make sure, GPT-4 encodes each word start and word end as one token and therefore can match more easily. ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The abstract describes testing LLMs on compositional tasks and describes the most important results. It also mentions theoretical contributions. By describing the key points of the paper, the scope is reflected. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer:[Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper explicitly discusses the limitations in section 7 and tries to lay out all information for critical reading. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: The paper provides the proofs and assumptions in Appendix A. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provide extensive information to allow the replication of our experiments in Appendix B, Appendix C, and Appendix D. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We open source our code at https://github.com/IBM/ limitations-lm-algorithmic-compositional-learning. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: Details on the training procedure of our models are included in Appendix C.1. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [No] ", "page_idx": 38}, {"type": "text", "text": "Justification: To make our statements, a certain trend is shown (high data inefficiency). Multiple runs could enhance the precision of our data points, but it seems irrelevant to our main statements, showing trends of data inefficiency and is also handled this way in other papers training LLMs from scratch. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] ", "page_idx": 39}, {"type": "text", "text": "Justification: This information is inferable by looking at our hyperparameter settings, e.g. in C.1 and depends on the precise framework used (e.g. Pytorch Lightning), which we refer to other resources. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, agree with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: To the best of our knowledge, our work has no societal impact besides advancing knowledge on compositionality in LLMs. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: To the best of our knowledge, our work has no societal impact besides advancing knowledge on compositionality in LLMs. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: We work with randomly generated synthetic textual data which does not contain any information which would make it susceptible to misuse. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We cite every work that contributed to our work. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The data generation hyperparameters are given, and the code is simple to use. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: We do not use crowdsourcing or human experiments. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: We do not use crowdsourcing or human experiments. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 41}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}]