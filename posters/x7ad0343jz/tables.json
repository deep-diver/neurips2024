[{"figure_path": "x7AD0343Jz/tables/tables_7_1.jpg", "caption": "Table 1: Results of GPT-4 and Gemini-Pro with various prompt settings (including chain-of-thought (CoT), analogical CoT [28], and code interpreter as a multi-round prompting technique [27]). Despite providing strong hints, few-shot CoT, task description, and without double traps (single only), both GPT-4 and Gemini fail on PEN (0% task accuracy). On PERM, the trend is similar and the main obstacle is multi-counting correctly (cf. the high match accuracy in the last two rows). GPT-4 reaches 42% task accuracy with a hand-crafted sub-task CoT. Gemini-Pro reaches to 9% in this setting.", "description": "This table summarizes the results of experiments using GPT-4 and Gemini-Pro on two algorithmic tasks, PEN and PERM, with various prompting methods.  The results show that even with detailed prompts and chain-of-thought prompting, both models struggle to achieve high accuracy. The main challenge seems to be in composing multiple sub-tasks required for the overall task.  Hand-crafted, sub-task-based prompting provides slightly better performance, particularly for GPT-4.", "section": "5.1 GPT-4 and Gemini prompting on PEN and PERM"}, {"figure_path": "x7AD0343Jz/tables/tables_24_1.jpg", "caption": "Table C.2: Performance of LLaMA on in-distribution test samples of the Pointer Execution's neighbor (PEN) task and its sub-tasks. Training is until convergence. We used the standard language modeling loss on every next token in the input, which performed best. The best validation performance is taken for each task separately. As shown, while models learn all sub-tasks Cpy, RCpy, and PE very well, on the PEN task (which is a composition of the sub-tasks Cpy, RCpy, and PE) they need much larger amounts of data than on any of the sub-tasks. This observation makes hypothesis H4 most plausible for PEN.", "description": "This table presents the results of training a 150M parameter LLaMA model on the PEN task and its subtasks (Cpy, RCpy, PE, PEV).  It shows the accuracy achieved at convergence for each task given different numbers of training samples. The key observation is that while the model achieves near-perfect accuracy on the individual subtasks, it requires significantly more data to learn the compositional PEN task.  This finding supports the hypothesis H4 which states that LLMs are highly sample inefficient when learning compositional tasks.", "section": "Training experiments details"}, {"figure_path": "x7AD0343Jz/tables/tables_24_2.jpg", "caption": "Table C.3: Results of LLaMA models trained only on the Pointer Execution\u2019s neighbor (PEN) task. Compared to Table C.2, we observe that the model does not seem to systematically profit from the compositionality given there.", "description": "This table presents the results of training LLAMA models only on the PEN task, without training on the sub-tasks.  It compares the accuracy at convergence for different numbers of training samples to the results from Table C.2, where LLAMA was trained on both the PEN task and its constituent subtasks. The comparison highlights whether pre-training on sub-tasks provides any benefit for learning the main task, addressing the question of compositional learning efficiency.", "section": "4.1 Training LLaMA models for testing compositionality"}, {"figure_path": "x7AD0343Jz/tables/tables_24_3.jpg", "caption": "Table C.4: Performance of LLaMA models on in-distribution test samples of the Pointer Execution Reverse Multicount (PERM) task and its sub-tasks. Similar to Table C.2, we observe that although all sub-tasks are learned perfectly by the models, their composition does not learn fully until including an almost 10-fold increase of the data needed for all sub-tasks together, making hypothesis H4 most plausible for this task too.", "description": "This table presents the results of training LLAMA models on the PERM task and its subtasks (PE, PEM, PER). It shows the accuracy achieved at convergence for each task at different sample sizes.  The results demonstrate that while the model learns individual sub-tasks effectively, it struggles to learn the composition of these subtasks efficiently.  A significantly larger dataset is required to successfully learn the full PERM task compared to the sum of samples needed to train its individual sub-tasks.", "section": "C Training experiments details"}, {"figure_path": "x7AD0343Jz/tables/tables_25_1.jpg", "caption": "Table C.5: Results of LLaMA models trained only on the PERM task. Compared to Table C.4, we observe that the model does not seem to systematically profit from the compositionality given there, similar to PEN.", "description": "This table shows the performance of LLAMA models trained only on the PERM task.  It compares the accuracy achieved at convergence for different numbers of training samples (500, 1000, 1500, and 2000 thousand). The results are compared to the results in Table C.4, where the model was trained on both the PERM task and its sub-tasks.  The comparison highlights that there's no significant improvement in performance by including sub-tasks during training, further demonstrating the inefficiency of compositional learning in LLAMAs.", "section": "C Training experiments details"}, {"figure_path": "x7AD0343Jz/tables/tables_25_2.jpg", "caption": "Table C.6: Performance of LLaMA models on Highest Subsequence Sum and Highest Subsequence Execution. Hypothesis H4 is the most plausible.", "description": "This table presents the results of training 150M parameter LLaMA models on two tasks: Highest Subsequence Sum (HSS) and Highest Subsequence Execution (SSE).  The table shows the accuracy achieved at convergence for different numbers of training samples.  It demonstrates that the model needs significantly more samples to learn the compositional task (HSS) than to learn the subtask (SSE), supporting hypothesis H4, which states that a Transformer language model requires more samples for compositional tasks than the sum of samples needed for each subtask.  The results are consistent across different sample sizes. ", "section": "C Training experiments details"}, {"figure_path": "x7AD0343Jz/tables/tables_25_3.jpg", "caption": "Table C.7: Performance of LLaMA models only on the Highest Subsequence Sum task.", "description": "This table presents the accuracy of the 150M parameter LLaMA model on the Highest Subsequence Sum (HSS) task, trained only on this task and without the use of subtasks, at different training sample sizes (in thousands).  The results demonstrate that the model's accuracy improves with increasing training data.  This table is part of a larger analysis examining the sample efficiency of LLaMA on compositional algorithmic tasks.", "section": "C Training experiments details"}, {"figure_path": "x7AD0343Jz/tables/tables_26_1.jpg", "caption": "Table C.8: Performance of LLaMA models on Digit Multiplication (DMUL), addition (ADD), and multiplication (MUL). Hypothesis H4 is the most plausible.", "description": "This table presents the results of training LLaMA models on four different tasks: Digit Multiplication (DMUL), addition (ADD), multiplication (MUL), and a composite task that combines the three. The table shows the accuracy achieved by the models at convergence for each task and various training sample sizes.  The results support hypothesis H4, stating that the models need more training samples to learn the composite task than the sum of samples needed for each subtask.", "section": "Training experiments details"}, {"figure_path": "x7AD0343Jz/tables/tables_26_2.jpg", "caption": "Table C.9: Performance of LLaMA models only on multiplication (MUL).", "description": "This table shows the accuracy of the 150M parameter LLaMA model on the multiplication task (MUL) at convergence for various training dataset sizes (200k, 400k, 800k, and 1600k samples).  It highlights the model's performance on this single task without the benefit of training on related sub-tasks, showing how sample efficiency varies with dataset size.", "section": "C Training experiments details"}, {"figure_path": "x7AD0343Jz/tables/tables_27_1.jpg", "caption": "Table C.10: Performance of LLaMA models pre-trained on Pointer Execution's neighbor (PEN), and after finetuning on Pointer Execution Verbose (PEV) until convergence (i.e., PEN\u2192PEV). We observe that decompositionality significantly improves data efficiency compared to training a model on the finetuning task from scratch. At the same time, however, for learning the task to full performance, it still seems to need more than a low constant number of demonstrations (\u2265 50 K), making the H\u2081-equivalent (i.e. \u201cA Transformer language model learns a decomposition with a constant number of samples\u201d) rather implausible for decompositionality as well.", "description": "This table shows the results of pre-training LLAMA models on the PEN task and then fine-tuning them on the PEV task.  It demonstrates that while pre-training improves sample efficiency for fine-tuning, a substantial number of samples (at least 50,000) are still needed to achieve high accuracy, suggesting that hypothesis H1 (constant number of samples for compositional learning) is unlikely to hold.", "section": "C Training experiments details"}, {"figure_path": "x7AD0343Jz/tables/tables_27_2.jpg", "caption": "Table C.11: Performance of LLaMA models pre-trained on Pointer Execution Reverse Multicount (PERM), and after finetuning on Pointer Execution (PE) until convergence (i.e., PERM\u2192PE). As shown similarly in Table C.10, decompositionality significantly improves data efficiency compared to training a model on the finetuning task from scratch. For learning the task to full performance, however, it still seems to need more than a low constant number of demonstrations (\u2265 5-20K), making the H\u2081-equivalent (i.e., \u201cA Transformer language model learns a decomposition with a constant number of samples\u201d) rather implausible for decompositionality as well.", "description": "This table shows the results of pre-training a LLAMA model on the PERM task and then fine-tuning it on the PE task.  It compares the accuracy achieved at different numbers of fine-tuning samples, highlighting the impact of pre-training on sample efficiency for decompositional learning.", "section": "C Training experiments details"}, {"figure_path": "x7AD0343Jz/tables/tables_29_1.jpg", "caption": "Table 1: Results of GPT-4 and Gemini-Pro with various prompt settings (including chain-of-thought (CoT), analogical CoT [28], and code interpreter as a multi-round prompting technique [27]). Despite providing strong hints, few-shot CoT, task description, and without double traps (single only), both GPT-4 and Gemini fail on PEN (0% task accuracy). On PERM, the trend is similar and the main obstacle is multi-counting correctly (cf. the high match accuracy in the last two rows). GPT-4 reaches 42% task accuracy with a hand-crafted sub-task CoT. Gemini-Pro reaches to 9% in this setting.", "description": "This table summarizes the performance of GPT-4 and Gemini-Pro on the PEN and PERM tasks using various prompting techniques.  It shows that even with strong hints and advanced prompting methods, both models struggle to achieve high accuracy, particularly on the PEN task.  The results highlight the challenges in compositional learning for LLMs even with in-context learning.", "section": "5.1 GPT-4 and Gemini prompting on PEN and PERM"}, {"figure_path": "x7AD0343Jz/tables/tables_30_1.jpg", "caption": "Table 1: Results of GPT-4 and Gemini-Pro with various prompt settings (including chain-of-thought (CoT), analogical CoT [28], and code interpreter as a multi-round prompting technique [27]). Despite providing strong hints, few-shot CoT, task description, and without double traps (single only), both GPT-4 and Gemini fail on PEN (0% task accuracy). On PERM, the trend is similar and the main obstacle is multi-counting correctly (cf. the high match accuracy in the last two rows). GPT-4 reaches 42% task accuracy with a hand-crafted sub-task CoT. Gemini-Pro reaches to 9% in this setting.", "description": "This table presents the results of using various prompting methods (few-shot, chain-of-thought, analogical chain-of-thought, code interpreter) with GPT-4 and Gemini-Pro on two algorithmic tasks, PEN and PERM.  The table shows the task accuracy, match accuracy, and termination accuracy achieved by each model under different prompting techniques. Notably, even with detailed prompts, both models struggled significantly on PEN, achieving 0% task accuracy, while performance on PERM was somewhat better but still limited, highlighting challenges in compositional learning and multi-round reasoning.", "section": "5.1 GPT-4 and Gemini prompting on PEN and PERM"}, {"figure_path": "x7AD0343Jz/tables/tables_32_1.jpg", "caption": "Table 1: Results of GPT-4 and Gemini-Pro with various prompt settings (including chain-of-thought (CoT), analogical CoT [28], and code interpreter as a multi-round prompting technique [27]). Despite providing strong hints, few-shot CoT, task description, and without double traps (single only), both GPT-4 and Gemini fail on PEN (0% task accuracy). On PERM, the trend is similar and the main obstacle is multi-counting correctly (cf. the high match accuracy in the last two rows). GPT-4 reaches 42% task accuracy with a hand-crafted sub-task CoT. Gemini-Pro reaches to 9% in this setting.", "description": "This table presents the results of experiments using GPT-4 and Gemini-Pro on two tasks, PEN and PERM, with various prompting methods.  The results are broken down by prompting technique (few-shot, CoT, analogical CoT, code interpreter) and indicate the task accuracy, match accuracy, and termination accuracy achieved.  The table highlights the difficulties both models face in achieving high accuracy, particularly on the PEN task, even with sophisticated prompting strategies.  The results suggest that simple few-shot prompting or even detailed descriptions are insufficient, and more complex, multi-step reasoning methods are needed for successful performance.", "section": "5.1 GPT-4 and Gemini prompting on PEN and PERM"}]