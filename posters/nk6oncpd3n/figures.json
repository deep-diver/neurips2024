[{"figure_path": "nK6OnCpd3n/figures/figures_1_1.jpg", "caption": "Figure 1: Our proposed Text-Aware Diffusion for Policy Learning (TADPoLe) framework leverages frozen, pretrained text-aware diffusion models to automatically craft dense text-conditioned rewards for policy learning. Here we visualize TADPoLe achieving diverse text-conditioned goals in the Humanoid, Dog, and Meta-World environments.", "description": "This figure shows examples of the diverse text-conditioned goals achieved by the TADPoLe model in different simulated environments.  The environments include a humanoid robot, a dog, and robotic arms in a Meta-World setting. Each image shows a different task successfully completed by the AI agent, based solely on the given textual description, such as \"a person in lotus position\", \"a person doing splits\", etc.  The figure highlights the framework's ability to learn policies for novel behaviors directly from natural language instructions without requiring manually-designed reward functions or expert demonstrations.", "section": "1 Introduction"}, {"figure_path": "nK6OnCpd3n/figures/figures_2_1.jpg", "caption": "Figure 2: A policy \u03c0\u03b8 that interacts with an environment can be treated as an agent-centric implicit video representation, where the arrow of time is actuated by the agent's actions and the pixels are rendered by the environment. The rendered behaviors can then be evaluated by a text-aware diffusion model to produce dense rewards, thereby providing text-conditioned update signals to the policy.", "description": "This figure illustrates the core idea of TADPoLe.  It shows how a policy interacting with an environment can be seen as implicitly generating a video. Each frame in this video is a result of the agent's actions and the environment's rendering. This video is then evaluated by a text-conditioned diffusion model. The model compares the generated frames to what it would generate given a text description of the desired behavior. This comparison generates a reward signal, guiding the learning process of the policy to produce videos/behaviors that are aligned with the text input. The left side shows how the policy generates frames sequentially over time, while the right side visualizes how the diffusion model uses those frames to provide text-conditioned reward signals for policy optimization.", "section": "3 Method"}, {"figure_path": "nK6OnCpd3n/figures/figures_3_1.jpg", "caption": "Figure 3: An illustration of the TADPoLe pipeline, which computes text-conditioned rewards for policy learning through a pretrained, frozen diffusion model. At each timestep, the subsequent frame rendered through the environment is corrupted with a sampled Gaussian source noise vector \u03f50. The pretrained text-conditioned diffusion model then predicts the source noise that was added. The reward is designed to be large when the selected action produces frames well-aligned with the text prompt.", "description": "This figure illustrates the Text-Aware Diffusion for Policy Learning (TADPoLe) pipeline.  The process starts with an agent (\u03c0) interacting with an environment.  The environment's render function produces a frame (ot+1) at each timestep. This frame is then corrupted by adding Gaussian noise (\u03f50). This noisy frame is input, along with the text prompt, to a pretrained text-conditioned diffusion model. The diffusion model predicts the noise that was added (\u03f5\u03c6). Finally, a reward (rt) is calculated based on the difference between the predicted noise and the actual noise. A higher reward indicates better alignment between the generated frame and the text prompt. The reward is then used to update the agent's policy.", "section": "3 Method"}, {"figure_path": "nK6OnCpd3n/figures/figures_6_1.jpg", "caption": "Figure 1: Our proposed Text-Aware Diffusion for Policy Learning (TADPoLe) framework leverages frozen, pretrained text-aware diffusion models to automatically craft dense text-conditioned rewards for policy learning. Here we visualize TADPoLe achieving diverse text-conditioned goals in the Humanoid, Dog, and Meta-World environments.", "description": "This figure showcases the capabilities of the proposed Text-Aware Diffusion for Policy Learning (TADPoLe) method. It demonstrates the model's ability to achieve various text-conditioned goals across different simulated environments.  The images depict successful zero-shot policy learning for diverse tasks specified in natural language, highlighting the method's flexibility and potential for complex behavior generation.", "section": "1 Introduction"}, {"figure_path": "nK6OnCpd3n/figures/figures_8_1.jpg", "caption": "Figure 5: Episode return curves for a Humanoid agent trained with Video-TADPoLe, using the prompt \u201ca person walking\u201d. We observe that the Video-TADPoLe reward signal (left) is positively correlated with the agent\u2019s performance as measured with ground-truth reward during training (middle) and evaluation (right). Shaded regions denote the standard deviation across five random seeds.", "description": "This figure displays three graphs showing the training and evaluation performance of a humanoid agent trained using Video-TADPoLe with the text prompt \"a person walking\". The left graph shows the Video-TADPoLe reward during training. The middle graph shows the ground truth reward during training, and the right graph shows the ground truth reward during evaluation. The shaded areas represent the standard deviation across five random seeds. The figure demonstrates a positive correlation between the Video-TADPoLe reward and the ground truth reward, indicating the effectiveness of the proposed method in learning locomotion behaviors.", "section": "4.3 Continuous Locomotion"}, {"figure_path": "nK6OnCpd3n/figures/figures_13_1.jpg", "caption": "Figure A1: Noise range intuition for a fixed image but two distinct prompts (left), and for a fixed prompt but two distinct images (right). Through visualization, we verify that U(400, 500) is a reasonable range from which to sample noise levels that can meaningfully distinguish vision-text alignment for arbitrarily rendered frames.", "description": "This figure shows the results of an experiment to determine the optimal noise level range for the TADPoLe reward function. The experiment involved varying the noise level applied to rendered images and comparing the resulting reward signals for both well-aligned (text and image match) and misaligned (text and image mismatch) pairs. The results suggest that a noise level range of U(400, 500) provides the best discrimination between well-aligned and misaligned cases.", "section": "A Intuition Regarding A Reasonable Noise Level Range"}, {"figure_path": "nK6OnCpd3n/figures/figures_16_1.jpg", "caption": "Figure A2: Meta-World Tasks. We select 12 robotic arm tasks from Meta-World suite as our evaluation task set, balanced in terms of diversity and complexity.", "description": "This figure visualizes the twelve robotic manipulation tasks selected from the Meta-World benchmark for evaluating the performance of TADPoLe.  The tasks represent a diverse range of challenges in terms of complexity and the required motor skills. The image shows a different setup for each of the 12 tasks, showcasing the variety of environments and object interactions involved.", "section": "C Meta-World Tasks"}, {"figure_path": "nK6OnCpd3n/figures/figures_17_1.jpg", "caption": "Figure A3: TADPoLe training curves for a variety of text-conditionings, with intermediate visualizations. The frames displayed are always the last frame achieved by the policy, at that particular training step.", "description": "This figure shows the training curves for four different text prompts used with TADPoLe: \"a person standing with hands above head\", \"a person standing with hands on hips\", \"a person kneeling\", and \"a dog chasing its tail\". Each plot shows the episode return over training steps (in millions).  Below each plot are four images showing the last frame of the video generated by the policy at different training steps, visualizing the progress over training. This helps illustrate the effectiveness of the algorithm in learning complex behaviors from textual descriptions.", "section": "D Training Curves"}, {"figure_path": "nK6OnCpd3n/figures/figures_17_2.jpg", "caption": "Figure 5: Episode return curves for a Humanoid agent trained with Video-TADPoLe, using the prompt \u201ca person walking\u201d. We observe that the Video-TADPoLe reward signal (left) is positively correlated with the agent\u2019s performance as measured with ground-truth reward during training (middle) and evaluation (right). Shaded regions denote the standard deviation across five random seeds.", "description": "This figure displays the training and evaluation performance of a humanoid agent trained using the Video-TADPoLe method. The left panel shows the episode return (a measure of performance) obtained during training using the Video-TADPoLe reward. The middle panel shows the episode return using ground truth reward during training, and the right panel shows the episode return during evaluation with ground truth reward.  The positive correlation between the Video-TADPoLe reward and the agent's performance indicates that the method effectively guides the agent towards the desired behavior. The shaded regions represent the standard deviation, suggesting the variability in the results.", "section": "4.3 Continuous Locomotion"}, {"figure_path": "nK6OnCpd3n/figures/figures_19_1.jpg", "caption": "Figure A3: TADPoLe training curves for a variety of text-conditionings, with intermediate visualizations. The frames displayed are always the last frame achieved by the policy, at that particular training step.", "description": "This figure shows training curves for four different text prompts used with TADPoLe.  The curves illustrate the progress of the agent's learning over time, while the accompanying images display the agent's behavior at various stages of training. This demonstrates the evolution of the policy from initial attempts to the final behavior.  Each row represents a different text prompt:  \"a person standing with hands above head\", \"a person standing with hands on hips\", \"a person kneeling\", and \"a dog chasing its tail\".", "section": "Appendix D Training Curves"}, {"figure_path": "nK6OnCpd3n/figures/figures_19_2.jpg", "caption": "Figure A3: TADPoLe training curves for a variety of text-conditionings, with intermediate visualizations. The frames displayed are always the last frame achieved by the policy, at that particular training step.", "description": "This figure shows the training curves for several text-conditioned policies.  The plots display the episode return over training steps (in millions).  Below each plot are a series of images showing the last frame of the video generated by the trained policy at various stages during training. This allows visualization of how the learned policy's behavior changes over the course of training and how it gradually aligns with the text prompt. ", "section": "Appendix D Training Curves"}, {"figure_path": "nK6OnCpd3n/figures/figures_19_3.jpg", "caption": "Figure A5: Visualizing the denoising of a good query trajectory from a noise level of 500 to completion using per-frame StableDiffusion.", "description": "This figure visualizes the denoising process of a successful dog walking trajectory using Stable Diffusion.  It shows a sequence of images, starting from a noisy version of the original video frames at a noise level of 500, and progressively denoising to reconstruct the original clean frames. Each row represents a frame from the sequence, with the top row being the most noisy version and the bottom row the cleanest reconstruction, demonstrating how Stable Diffusion effectively removes noise while retaining the essential visual characteristics of the dog's motion.", "section": "E Motivating Visualizations"}, {"figure_path": "nK6OnCpd3n/figures/figures_19_4.jpg", "caption": "Figure A3: TADPoLe training curves for a variety of text-conditionings, with intermediate visualizations. The frames displayed are always the last frame achieved by the policy, at that particular training step.", "description": "This figure shows the training curves for four different text prompts: \u201ca person standing with hands above head\u201d, \u201ca person standing with hands on hips\u201d, \u201ca person kneeling\u201d, and \u201ca dog chasing its tail\u201d.  For each prompt, the figure displays the episode return over time (training steps in millions) along with four example frames from the last frame of the achieved video at several training step checkpoints: 500k, 1M, 1.5M, and 2M. This visualizes how the policy learns over time, demonstrating the evolution of the agent's behavior from an initial state to a state that aligns with the text prompt.", "section": "D Training Curves"}, {"figure_path": "nK6OnCpd3n/figures/figures_20_1.jpg", "caption": "Figure A5: Visualizing the denoising of a good query trajectory from a noise level of 500 to completion using per-frame StableDiffusion.", "description": "This figure shows the denoising process of a successful dog walking trajectory using Stable Diffusion. The top row displays the original frames from the trajectory. The middle row shows the noisy frames after adding Gaussian noise with a noise level of 500. The bottom row presents the denoised frames produced by Stable Diffusion.  The consistent and successful reconstruction of the dog walking across the frames demonstrates the model's ability to accurately reconstruct good query trajectories.", "section": "E Motivating Visualizations"}, {"figure_path": "nK6OnCpd3n/figures/figures_20_2.jpg", "caption": "Figure A3: TADPoLe training curves for a variety of text-conditionings, with intermediate visualizations. The frames displayed are always the last frame achieved by the policy, at that particular training step.", "description": "This figure shows training curves and intermediate results of TADPoLe for four different text prompts: \"a person standing with hands above head\", \"a person standing with hands on hips\", \"a person kneeling\", and \"a dog chasing its tail\". Each subplot displays the episode return over training steps (x-axis) and the corresponding last frame of the video generated by the policy at different training steps (bottom). The figure visually demonstrates how the policy learns to achieve the desired behavior over time, with each frame showcasing progress towards the final goal.", "section": "D Training Curves"}, {"figure_path": "nK6OnCpd3n/figures/figures_20_3.jpg", "caption": "Figure A3: TADPoLe training curves for a variety of text-conditionings, with intermediate visualizations. The frames displayed are always the last frame achieved by the policy, at that particular training step.", "description": "This figure visualizes training curves of TADPoLe for various text prompts and shows the last frames of the resulting videos at different training steps (500k, 1M, 1.5M, and 2M).  This allows for a visual understanding of how the learned policies evolve over training and how well they match the intended text prompt at different stages.", "section": "D Training Curves"}, {"figure_path": "nK6OnCpd3n/figures/figures_20_4.jpg", "caption": "Figure A3: TADPoLe training curves for a variety of text-conditionings, with intermediate visualizations. The frames displayed are always the last frame achieved by the policy, at that particular training step.", "description": "This figure shows training curves for four different text prompts used with TADPoLe. For each prompt, the figure displays a training curve and a sequence of images showing the final frame of each policy at specific training steps (500k, 1M, 1.5M, and 2M). The images illustrate how the agent's behavior evolves over the training process, progressing from initially random movements to those increasingly aligned with the intended text prompt.", "section": "D Training Curves"}]