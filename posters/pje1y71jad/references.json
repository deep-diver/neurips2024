{"references": [{"fullname_first_author": "Alon Talmor", "paper_title": "CommonsenseQA: A question answering challenge targeting commonsense knowledge", "publication_date": "2019-06-01", "reason": "This paper introduced a benchmark dataset, CommonsenseQA, which is used for evaluating commonsense reasoning capabilities of models and is frequently used in KBQA research."}, {"fullname_first_author": "Robyn Speer", "paper_title": "ConceptNet 5.5: An open multilingual graph of general knowledge", "publication_date": "2016-01-01", "reason": "ConceptNet is a large knowledge graph frequently used in KBQA research providing common sense knowledge and relationships between concepts."}, {"fullname_first_author": "Todor Mihaylov", "paper_title": "Can a suit of armor conduct electricity?: A new dataset for open book question answering", "publication_date": "2018-10-01", "reason": "This paper introduced another important benchmark dataset, OpenBookQA, focusing on scientific knowledge and multiple-choice questions, commonly used to evaluate KBQA models."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-10-01", "reason": "BERT is a foundational large language model (LLM) that has significantly impacted various NLP tasks, including KBQA, by providing strong contextualized word representations."}, {"fullname_first_author": "Yinhan Liu", "paper_title": "RoBERTa: A robustly optimized BERT pretraining approach", "publication_date": "2019-07-01", "reason": "RoBERTa is an improved version of BERT, offering enhanced performance and robustness, and is another widely used LLM in many KBQA approaches."}]}