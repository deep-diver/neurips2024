[{"figure_path": "fT1RkAgrC3/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of performance on the GLUE benchmark (in percent). The terms \"+SVD\" and \"+OPDF\" represent the use of different over-parameterization methods in a KD model. \"# Train Params\" and \"# Inference Params\" refer to the total number of parameters during training and inference, respectively. Numbers marked with * indicate tasks not tested in the original studies; results here are reproduced from the published code. The best result for each task is highlight in bold. For all the results, we report the mean values of five runs using different random seeds.", "description": "This table compares the performance of different knowledge distillation (KD) models on the GLUE benchmark.  It shows the results for several baseline KD methods (BERT-of-Theseus, LGTM, DBKD, AD-KD) and how their performance changes when the student model is over-parameterized using either Singular Value Decomposition (SVD) or the proposed Matrix Product Operator (MPO) method. The table highlights the average performance across all GLUE tasks and also provides a breakdown of the performance on each individual task.  The number of trainable parameters and inference parameters for each model is also included.", "section": "5.2 Main Experimental Results"}, {"figure_path": "fT1RkAgrC3/tables/tables_7_1.jpg", "caption": "Table 1: Comparison of performance on the GLUE benchmark (in percent). The terms \"+SVD\" and \"+OPDF\" represent the use of different over-parameterization methods in a KD model. \"# Train Params\" and \"# Inference Params\" refer to the total number of parameters during training and inference, respectively. Numbers marked with * indicate tasks not tested in the original studies; results here are reproduced from the published code. The best result for each task is highlight in bold. For all the results, we report the mean values of five runs using different random seeds.", "description": "This table compares the performance of different knowledge distillation (KD) methods on the GLUE benchmark.  It shows the impact of adding Singular Value Decomposition (SVD) and the proposed Over-Parameterized Distillation Framework (OPDF) to several baseline KD methods.  The table presents accuracy and F1 scores for various tasks, along with the number of parameters used during training and inference.  The best performing method for each task is highlighted.", "section": "5.2 Main Experimental Results"}, {"figure_path": "fT1RkAgrC3/tables/tables_8_1.jpg", "caption": "Table 1: Comparison of performance on the GLUE benchmark (in percent). The terms \"+SVD\" and \"+OPDF\" represent the use of different over-parameterization methods in a KD model. \"# Train Params\" and \"# Inference Params\" refer to the total number of parameters during training and inference, respectively. Numbers marked with * indicate tasks not tested in the original studies; results here are reproduced from the published code. For all the results, we report the mean values of five runs using different random seeds.", "description": "This table presents a comparison of the performance of different knowledge distillation (KD) methods on the GLUE benchmark.  It shows the results for baseline KD methods (BERT-of-Theseus, LGTM, DBKD, and AD-KD), as well as those same methods enhanced by the addition of SVD and OPDF over-parameterization techniques. The table reports the accuracy, F1-score, or correlation coefficient for each task in the GLUE benchmark, and also notes the number of training and inference parameters for each model.  The best result for each task is highlighted in bold.", "section": "5.2 Main Experimental Results"}, {"figure_path": "fT1RkAgrC3/tables/tables_16_1.jpg", "caption": "Table 1: Comparison of performance on the GLUE benchmark (in percent). The terms \"+SVD\" and \"+OPDF\" represent the use of different over-parameterization methods in a KD model. \"# Train Params\" and \"# Inference Params\" refer to the total number of parameters during training and inference, respectively. Numbers marked with * indicate tasks not tested in the original studies; results here are reproduced from the published code. The best result for each task is highlight in bold. For all the results, we report the mean values of five runs using different random seeds.", "description": "This table presents a comparison of the performance of different knowledge distillation (KD) models on the GLUE benchmark.  It compares baseline KD methods (BERT-of-Theseus, LGTM, DBKD, AD-KD) against versions of these methods that incorporate either SVD or the proposed OPDF (Over-Parameterization Distillation Framework) over-parameterization technique. The table shows accuracy, F1 scores, and correlation coefficients for various sub-tasks within GLUE, along with the number of parameters used during training and inference for each model.  The results highlight the performance improvements achieved by integrating the OPDF method with existing KD techniques.", "section": "5.2 Main Experimental Results"}, {"figure_path": "fT1RkAgrC3/tables/tables_16_2.jpg", "caption": "Table 1: Comparison of performance on the GLUE benchmark (in percent). The terms \"+SVD\" and \"+OPDF\" represent the use of different over-parameterization methods in a KD model. \"# Train Params\" and \"# Inference Params\" refer to the total number of parameters during training and inference, respectively. Numbers marked with * indicate tasks not tested in the original studies; results here are reproduced from the published code. For all the results, we report the mean values of five runs using different random seeds.", "description": "This table compares the performance of different knowledge distillation (KD) methods on the GLUE benchmark.  It shows the impact of two over-parameterization techniques (+SVD and +OPDF) on various baseline KD models (BERT-of-Theseus, LGTM, DBKD, and AD-KD). The table presents accuracy, F1 scores, and correlation coefficients for multiple downstream tasks (RTE, MRPC, STS-B, COLA, SST-2, QNLI, QQP, and MNLI).  The number of trainable parameters and inference parameters for each model are also included, highlighting the efficiency of the proposed OPDF method.", "section": "5.2 Main Experimental Results"}, {"figure_path": "fT1RkAgrC3/tables/tables_17_1.jpg", "caption": "Table 1: Comparison of performance on the GLUE benchmark (in percent). The terms \"+SVD\" and \"+OPDF\" represent the use of different over-parameterization methods in a KD model. \"# Train Params\" and \"# Inference Params\" refer to the total number of parameters during training and inference, respectively. Numbers marked with * indicate tasks not tested in the original studies; results here are reproduced from the published code. For all the results, we report the mean values of five runs using different random seeds.", "description": "This table compares the performance of various knowledge distillation (KD) methods on the GLUE benchmark.  It shows the performance of baseline models (BERT-of-Theseus, LGTM, DBKD, AD-KD) and the improvement achieved when using either SVD or the proposed OPDF method for over-parameterization of the student model.  The table reports accuracy and F1 scores for different GLUE tasks, along with the number of trainable and inference parameters for each configuration.  The results highlight the improved performance when using OPDF for over-parameterization, particularly exceeding the performance of the teacher model in some cases.", "section": "5.2 Main Experimental Results"}, {"figure_path": "fT1RkAgrC3/tables/tables_17_2.jpg", "caption": "Table 1: Comparison of performance on the GLUE benchmark (in percent). The terms \"+SVD\" and \"+OPDF\" represent the use of different over-parameterization methods in a KD model. \"# Train Params\" and \"# Inference Params\" refer to the total number of parameters during training and inference, respectively. Numbers marked with * indicate tasks not tested in the original studies; results here are reproduced from the published code. The best result for each task is highlight in bold. For all the results, we report the mean values of five runs using different random seeds.", "description": "This table compares the performance of different knowledge distillation (KD) methods on the GLUE benchmark.  It shows the impact of integrating over-parameterization techniques (+SVD and +OPDF) with various baseline KD methods. The table reports accuracy (or F1 score) on different GLUE tasks, the number of parameters used during training and inference, and highlights the best performing method for each task. The results represent the average of five runs.", "section": "5.2 Main Experimental Results"}, {"figure_path": "fT1RkAgrC3/tables/tables_18_1.jpg", "caption": "Table 1: Comparison of performance on the GLUE benchmark (in percent). The terms \"+SVD\" and \"+OPDF\" represent the use of different over-parameterization methods in a KD model. \"\\# Train Params\" and \"\\# Inference Params\" refer to the total number of parameters during training and inference, respectively. Numbers marked with * indicate tasks not tested in the original studies; results here are reproduced from the published code. The best result for each task is highlight in bold. For all the results, we report the mean values of five runs using different random seeds.", "description": "This table compares the performance of different knowledge distillation (KD) methods on the GLUE benchmark.  It shows the impact of adding SVD and OPDF (the proposed over-parameterization method) to various existing KD approaches (BERT-of-Theseus, LGTM, DBKD, AD-KD).  The table includes accuracy, F1 score, and Matthews Correlation Coefficient for various tasks, along with the number of parameters during training and inference. The bold values represent the best performance for each task.", "section": "5.2 Main Experimental Results"}, {"figure_path": "fT1RkAgrC3/tables/tables_19_1.jpg", "caption": "Table 1: Comparison of performance on the GLUE benchmark (in percent). The terms \"+SVD\" and \"+OPDF\" represent the use of different over-parameterization methods in a KD model. \"# Train Params\" and \"# Inference Params\" refer to the total number of parameters during training and inference, respectively. Numbers marked with * indicate tasks not tested in the original studies; results here are reproduced from the published code. The best result for each task is highlight in bold. For all the results, we report the mean values of five runs using different random seeds.", "description": "This table compares the performance of different knowledge distillation (KD) models on the GLUE benchmark.  It shows the impact of two over-parameterization methods (+SVD and +OPDF) on various baseline KD models (BERT-of-Theseus, LGTM, DBKD, AD-KD).  The table includes the accuracy and F1 scores for each task, the number of trainable parameters during training and inference, and highlights the best-performing model for each task.  The results are averaged over five runs with different random seeds.", "section": "5.2 Main Experimental Results"}, {"figure_path": "fT1RkAgrC3/tables/tables_19_2.jpg", "caption": "Table 1: Comparison of performance on the GLUE benchmark (in percent). The terms \"+SVD\" and \"+OPDF\" represent the use of different over-parameterization methods in a KD model. \"# Train Params\" and \"# Inference Params\" refer to the total number of parameters during training and inference, respectively. Numbers marked with * indicate tasks not tested in the original studies; results here are reproduced from the published code. The best result for each task is highlight in bold. For all the results, we report the mean values of five runs using different random seeds.", "description": "This table compares the performance of different knowledge distillation (KD) models on the GLUE benchmark. It shows the impact of two over-parameterization methods (+SVD and +OPDF) on the performance of several KD methods (BERT-of-Theseus, LGTM, DBKD, AD-KD). The table includes the accuracy and F1 scores for each task, the number of parameters during training and inference, and highlights the best performance achieved for each task. The results are averaged over five runs with different random seeds. ", "section": "5.2 Main Experimental Results"}]