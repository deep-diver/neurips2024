[{"type": "text", "text": "Symbolic Regression with a Learned Concept Library ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Arya Grayeli\u2217 Atharva Sehgal\u2217 Omar Costilla-Reyes UT Austin, Foundry Technologies UT Austin MIT ", "page_idx": 0}, {"type": "text", "text": "Miles Cranmer University of Cambridge ", "page_idx": 0}, {"type": "text", "text": "Swarat Chaudhuri UT Austin ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We present a novel method for symbolic regression (SR), the task of searching for compact programmatic hypotheses that best explain a dataset. The problem is commonly solved using genetic algorithms; we show that we can enhance such methods by inducing a library of abstract textual concepts. Our algorithm, called LASR, uses zero-shot queries to a large language model (LLM) to discover and evolve abstract concepts occurring in known high-performing hypotheses. We discover new hypotheses using a mix of standard evolutionary steps and LLM-guided steps (obtained through zero-shot LLM queries) conditioned on discovered concepts. Once discovered, hypotheses are used in a new round of concept abstraction and evolution. We validate LASR on the Feynman equations, a popular SR benchmark, as well as a set of synthetic tasks. On these benchmarks, LASR substantially outperforms a variety of state-of-the-art SR approaches based on deep learning and evolutionary algorithms. Moreover, we show that LASR can be used to discover a new and powerful scaling law for LLMs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Symbolic regression (SR) [33] is the task of finding succinct programmatic hypotheses \u2014 written in a flexible, domain-specific programming language \u2014 that best explain a dataset. Initially proposed in the 1970s, SR has recently emerged as a prominent approach to automated scientific discovery, with applications in domains from astrophysics [30, 12] to chemistry [2, 22] to medicine [51]. ", "page_idx": 0}, {"type": "text", "text": "Computational complexity is a fundamental challenge in SR, as the space of hypotheses that an SR algorithm must search is discrete and exponential. Previous work has approached this challenge using methods like genetic programming [41, 10], neural-guided search [11, 43], deep reinforcement learning [38] and hybrid algorithms [28]. However, new tools to enhance the scalability of SR remain a critical need for applications in SR and scientific discovery. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we show that abstraction and knowledge-directed discovery can be powerful principles in building such scaling tools in SR. State-of-the-art genetic algorithms for SR [10] evolve pools of candidate hypotheses using random mutation and crossover operations. By contrast, a human scientist does not just randomly mutate their explanations of data. Instead, they synthesize background knowledge and empirical observations into abstract concepts, then use these concepts to derive new explanations. We show that zero-shot queries to large language models (LLMs) can be used to implement such a discovery process on top of a standard SR algorithm. ", "page_idx": 0}, {"type": "text", "text": "Concretely, we present a new method for symbolic regression, called LASR, that discovers a library of abstract, reusable and interpretable textual concepts and uses it to accelerate SR. LASR alternates between three phases: (i) concept-directed hypothesis evolution, where standard genetic operations ", "page_idx": 0}, {"type": "image", "img_path": "B7S4jJGlvl/tmp/5fbd85f637e87b249a4259599fc4a817d30070bcb7b1e6e6b4d7b3a9ae6bb46c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: An overview of LASR. LASR iteratively refines a library of interpretable textual concepts which are used to bias the search for hypotheses for scientific discovery tasks. This involves three distinct phases: (Top) finding optimal hypotheses within a concept-directed hypothesis evolution, (Right) leveraging the optimal hypotheses to find new concept abstractions, and (Left) iterating on learned concepts to discover new concepts to accelerate hypothesis evolution. LASR introduces an orthogonal direction of improvement over current symbolic regression algorithms [10] (in gray). ", "page_idx": 1}, {"type": "text", "text": "over hypotheses are interleaved with LLM-guided mutation and crossover operations conditioned on known library concepts; (ii) the LLM-based abstraction of patterns in known high-performing hypotheses into new concepts; and (iii) the LLM-directed evolution of concepts into more succinct and general forms. Together, these three steps form an open-ended alternating maximization loop that combines evolutionary exploration with the exploitation of the LLM\u2019s background knowledge and in-context learning ability. ", "page_idx": 1}, {"type": "text", "text": "We experimentally compare LASR on Feynman Equations [26] \u2014 a popular SR benchmark in which the goal is to discover 100 equations from the Feynman Lectures in Physics \u2014 against several stateof-the-art genetic and deep learning approaches. LASR can discover 66 of the 100 target equations, while the best existing approach can solve 59. To address the concern that LASR\u2019s performance could be attributed to test set leakage, we compare LASR with a state-of-the-art genetic approach on a suite of synthetic benchmarks. We show that LASR substantially outperforms the baseline. Finally, we support the contribution of LASR to the research community by evaluating the methodology in a case study where the goal is to find new scaling laws for LLMs. ", "page_idx": 1}, {"type": "text", "text": "In summary, the contributions of this paper are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We pose the problem of discovering an open-ended, reusable concept library that can accelerate solutions to downstream SR tasks.   \n\u2022 We present LASR, a method for combining zero-shot LLM queries and standard evolutionary operations to simultaneously induce a concept library and high-performing hypotheses. LASR\u2019s strategy of using LLMs to accelerate evolutionary algorithms may have future applications in settings beyond SR.   \n\u2022 We offer promising experimental results, including a demonstration that LASR outperforms stateof-the-art algorithms in standard SR tasks and synthetic domains, as well as a case study that uses LASR to discover a novel LLM scaling law. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Symbolic Regression. We formulate symbolic regression (SR) as a program synthesis [6] problem. The inputs to this problem include a language $\\mathcal{L}$ of programmatic hypotheses and a dataset $\\mathcal{D}:=$ $\\{(\\mathbf{x}_{i},\\bar{\\mathbf{y}_{i}})\\}_{i=1}^{N}$ of input-output examples. The syntax of $\\mathcal{L}$ is described by a context-free grammar [24]. The grammar allows each hypothesis $\\pi$ to be represented using a set of mathematical operators (e.g., addition, multiplication, trigonometric functions) that facilitate the composition of simpler hypotheses into more complex ones. We abstractly define the ftiness of a hypothesis $\\pi$ as the likelihood $p_{\\mathcal{L}}(\\mathcal{D}\\mid\\pi)$ that it generates $\\mathcal{D}$ . ", "page_idx": 2}, {"type": "text", "text": "In order to prevent finding non-useful solutions, we impose a prior probability distribution $p_{\\mathcal{L}}(\\pi)$ over hypotheses $\\pi$ that penalizes syntactically complex hypotheses. We now pose SR as the task of finding a hypothesis $\\pi^{\\star}$ that maximizes the fitness while minimizing syntactic complexity. The problem can be expressed as a maximum a posteriori (MAP) estimation problem [15]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi^{\\star}=\\arg\\operatorname*{max}_{\\pi}p_{\\mathcal{L}}(\\pi|\\mathcal{D})=\\arg\\operatorname*{max}_{\\pi}\\underbrace{p_{\\mathcal{L}}(\\mathcal{D}|\\pi)}_{\\mathrm{optimization}}\\cdot\\underbrace{p_{\\mathcal{L}}(\\pi)}_{\\mathrm{regularization}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Recent work leverages large language models (LLMs) for program synthesis [8, 19, 32]. Large language models (LLMs) approach program synthesis as a token prediction problem, directly approximating the likelihood of programs by training on internet-scale datasets. That is, ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\mathcal{L}}(\\pi|\\mathcal{D})\\approx p_{\\mathrm{LLM}}(\\langle\\pi\\rangle\\mid\\langle\\mathcal{L}\\rangle,\\mathsf{d e s c}(\\mathcal{D})),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\langle\\pi\\rangle$ and $\\langle{\\mathcal{L}}\\rangle$ are, respectively, textual representations of $\\pi$ and a specification of the syntax of $\\mathcal{L}$ , and the task description desc $(\\mathcal{D})$ is a few-shot serialization of a subset of the examples in $\\mathcal{D}$ . ", "page_idx": 2}, {"type": "text", "text": "Symbolic Regression with Latent Concept Libraries. Classical symbolic regression typically assumes no prior knowledge or intuition about the problem. In contrast, human scientific discovery often leverages empirical patterns [52] and intuitions derived from previously observed data. For example, recognizing a \u2018power law relationship between variables\u2019 has led to the formulation of fundamental empirical laws across various fields, such as the Arrhenius equation in Chemistry, the Rydberg formula in Physics, Zipf\u2019s law in Linguistics, and Moore\u2019s law in Computer Science. ", "page_idx": 2}, {"type": "text", "text": "We model such empirical patterns as natural-language concepts drawn from a latent concept library $\\mathcal{C}$ . We frame the relationship between the concept library and programs as a Hierarchical Bayesian model consisting of: (i) a prior $p(\\mathcal{C})$ representing the natural distribution over concept libraries; (ii) a model $p_{\\mathcal{L}}(\\pi\\mid\\mathcal{C})$ that quantifies the likelihood of various hypotheses for a given concept library $\\mathcal{C}$ ; and (iii) the previously mentioned fitness function $p_{\\mathscr{L}}(\\mathscr{D}\\mid\\pi)$ for programs $\\pi$ . We assume that the distributions $p(\\mathcal{C})$ and $p_{\\mathcal{L}}(\\pi\\mid\\mathcal{C})$ can be approximated using LLMs. That is, we can prompt an LLM to generate interesting concepts, and we can prompt an LLM with a set of concepts to generate token-sequence representations of hypotheses that adhere to the concepts. Now we state the problem of symbolic regression with latent concept learning as one of simultaneously inducing an optimal concept library and an optimal programmatic hypothesis: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{max}_{\\pi,c}p(\\pi,\\mathcal{C}|\\mathcal{D})=\\arg\\operatorname*{max}_{\\pi,c}\\;\\underbrace{p(\\mathcal{D}|\\pi)}_{\\pi}\\,\\cdot\\,\\underbrace{p(\\pi|\\mathcal{C})}_{\\mathrm{~\\approx~}}\\cdot\\underbrace{p(\\mathcal{C})}_{\\mathrm{~\\approx~}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "LASR performs a two-stage evolution over natural-language concepts and programmatic hypotheses. The two stages follow an alternating maximization strategy shown in Figure 1: (1) Hypothesis evolution: We fix the set of concepts and focus on maximizing the hypotheses\u2019 ftiness to the dataset, and (2) Concept abstraction and evolution: We leverage the best hypotheses found to induce a new library of concepts. ", "page_idx": 2}, {"type": "text", "text": "In the rest of this section, we first describe PySR, the SR algorithm [10] that LASR extends. Next, we show how to modify this algorithm into one guided by natural-language concepts. Finally, we show how these concepts can be naturally extracted and evolved into new concepts. The full LASR algorithm is presented in Algorithm 1 and visualized in Figure 2. LASR is built in Julia with an additional Python interface2 and uses an open-source, optimized framework for LLM inference [25]. ", "page_idx": 2}, {"type": "image", "img_path": "B7S4jJGlvl/tmp/408be4096391104173f7cf558ad1647c32842833a923538892faa1d04b3c5e20.jpg", "img_caption": ["Figure 2: A single step of LASR. LASR induces multiple hypothesis populations that are evolved using a scalable evolutionary algorithm. Concept guidance is provided by randomly replacing symbolic operations with concept-directed LLM operations with probability $p$ . After each iteration, the top-performing programs are summarized into natural language concepts, which are evolved to form new concepts that are sampled to guide the search in the next iteration. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Base Algorithm: PySR. LASR builds on PySR [10], a scalable, parallelizable genetic search algorithm for SR. The search in PySR maintains multiple populations $\\{\\Pi_{1},\\dotsc,\\Pi_{k}\\}$ of hypotheses, with each hypothesis represented as an expression tree. In its initialization step, captured by a procedure INITIALIZEPOPULATIONS, PySR creates a new expression at random to insert into a population. After running this step, PySR runs a genetic search, encapsulated in a procedure SRCYCLE, which evolve these populations in parallel, simplifies and optimizes the constants of the resulting hypotheses, and then migrates top-performing hypotheses between populations. ", "page_idx": 3}, {"type": "text", "text": "Like other evolutionary algorithms, the search in PySR uses symbolic mutation and crossover operations. The mutation step is broken into many categories, each with distinct weighting, to either mutate a constant, mutate an operator, add a node (append, prepend, insert), delete a subtree of an expression tree, simplify the tree, initialize a new tree, or do nothing. One of these operations is randomly selected at each call to a mutation request, and each operation executes itself at random but within user-provided constraints. For example, deleting a subtree is done by choosing a random node to replace with a randomly-generated leaf node such as a feature or constant. The crossover step involves swapping random subtrees of two expressions in a population. ", "page_idx": 3}, {"type": "text", "text": "LLM-guided Hypothesis Evolution. LASR speeds up PySR by injecting natural language priors into its search procedure. To do this, we modify the INITIALIZEPOPULATIONS procedure to use an LLM-augmented initialization operation, and the SRCYCLE routine to use LLM-augmented versions of its symbolic mutation and crossover operations. The altered procedures are named LLMINIT, LLMMUTATE, and LLMCROSSOVER, respectively. These operations do not replace their standard genetic counterparts. Instead, we introduce a hyperparameter $p$ that, with a fixed probability, substitutes the standard genetic operation with the LLM-based operation. This enables \u201cdoping\u201d each population with a program that respects the language priors, while ensuring that we do not bottleneck the local exploration of the search space. ", "page_idx": 3}, {"type": "text", "text": "The LLM-guided operations follow the same base format: they sample multiple concepts from the concept library, concatenate these concepts with the task-specific variable names and language operations, and append a specialized prompt for each task. We employ zero-shot prompts (see Appendix A.2 for more details) to avoid sampling biases. In further detail: ", "page_idx": 3}, {"type": "text", "text": "\u2022 LLMINIT: The LLMINIT function takes an initial set of concepts and uses them to initialize the populations for the evolutionary search step. The initial set of concepts can either be instantiated from an optional set of user-provided \u201chints\u201d or generated by the LLM. \u2022 LLMMUTATE: For mutation within a population, we sample a set of $l$ concepts from the concept library $C$ , and construct a prompt that uses this set of concepts to mutate an expression $\\pi_{i}$ into $\\pi_{j}$ . The prompt to the LLM takes inspiration from the standard genetic mutation operation, and asks it to mutate the expression given the concepts sampled from the library. ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Pseudocode for LASR. LASR takes as input an optional set of user-provided hints $\\mathcal{C}_{0}$ , a dataset of input-output pairs of high-dimensional data $\\mathcal{D}$ , and four hyperparameters: the number of iterations $I$ , the number of populations $K$ , the number of steps for concept evolution $M$ , and the mixture probability of using LLM-based or GP-based evolutionary operators $p$ . LASR produces two artifacts: the evolved library of concepts $\\mathcal{C}$ and the expression with the highest fitness score $\\pi^{\\star}$ . ", "page_idx": 4}, {"type": "table", "img_path": "B7S4jJGlvl/tmp/1b52f9a2d91e04095d51a15c6bb2f2d200ff195ac82a793ffe74e58f11d2f503.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "\u2022 LLMCROSSOVER: The LLMCROSSOVER function also samples a set of $l$ concepts from the concept library along with two hypotheses $\\pi_{i}$ and $\\pi_{j}$ to construct a new expression $\\pi_{k}$ , which reuses sub-expression trees from the two hypotheses while respecting the sampled concepts. Our implementation is inspired by prior work [40] \u2014 see Figure 4. ", "page_idx": 4}, {"type": "text", "text": "Concept Abstraction. After each iteration of symbolic regression, we use a function EXTRACTPARETOFRONTIER to collect: (i) the hypotheses, across all populations, that are Pareto-optimal with respect to the criteria of syntactic simplicity and dataset loss; (ii) the hypotheses with the worst loss across all populations. The resulting set of hypotheses $\\mathcal{F}=\\{\\pi_{1}^{\\star}.,\\pi_{a}^{\\star}.\\cdot.\\,\\pi_{1}^{-}.,\\pi_{b}^{-}\\}$ captures the trends that were most helpful and most detrimental to performance during hypothesis search. Now we use the CONCEPTABSTRACTION function, which uses a zero-shot prompt to extract a natural language concept $c^{\\star}$ that summarizes the positive trends while eschewing negative trends. This concept is subsequently added to the concept library. The prompt for the function is presented in Figure 5. ", "page_idx": 4}, {"type": "text", "text": "Concept Evolution. Each concept in $\\mathcal{C}$ represents trends that were useful at a previous state in the search process. After adding new concepts into the library, we use a function CONCEPTEVOLUTION to evolve the library to include new ideas that logically follow from the ideas in the current library. The implementation of this function follows that of the LLMCROSSOVER operation in that we are using multiple concepts as a reference to generate new ones, with the key distinction that, unlike in the LLMCROSSOVER operation, the ftiness of each generated concept here is difficult to quantify. Thus, we include all the generated responses in the concept library. While these concepts may sometimes be inaccurate, they increase the evolutionary algorithm\u2019s exploration ability. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We demonstrate the effectiveness of LASR on multiple tasks integral to scientific discovery. First, we evaluate LASR\u2019s performance on the Feynman Equation dataset, a widely adopted scientific discovery benchmark, under a variety of ablations and additional priors. Second, we measure the effect of data leakage by evaluating LASR\u2019s performance on a procedurally generated synthetic dataset of challenging equations. Finally, we conduct a case study using LASR to discover LLM scaling laws with data from the BIG-Bench evaluation suite [16]. ", "page_idx": 4}, {"type": "text", "text": "LASR\u2019s main focus is to serve as a practical toolkit for scientists. Therefore, our evaluation primarily targets slightly noisy environments, using exact solution rate to gauge performance rather than statistical similarity measures like correlation $R^{2}$ , which are less relevant to scientific discovery applications. Additional experiments characterizing the practical behavior of LASR are in Appendix A.5. Information regarding compute usage is in Appendix A.3.1. ", "page_idx": 4}, {"type": "table", "img_path": "B7S4jJGlvl/tmp/91431f7480e4ccafd5f0c574307008cc7d32482f464b0a019fbde387bb307bf5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "B7S4jJGlvl/tmp/c12a95d70d4a344c9d81d59fc4f2b056a308e3d058b53413ec935f862b43bfd3.jpg", "table_caption": ["Table 1: Results on 100 Feynman equations from [49]. We report exact match solve rate for all models. LASR achieves the best exact match solve rate using the same hyperparameters as PySR. "], "table_footnote": ["Table 2: Evaluation results on Feynman dataset by cascading LASR\u2019s LLM backbone (llama3-8b, gpt-3.5-turbo) and changing the probability of calling the model $(p=[0.01,0.05,0.10])$ in the order of increasing concept guidance. LASR outperforms PySR even with minimal concept guidance using an open-source LLM. "], "page_idx": 5}, {"type": "text", "text": "4.1 Comparison against baselines in the Feynman Equation Dataset ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Dataset: The Feynman Equations dataset is a well established benchmark for Symbolic Regression algorithms [49]. This dataset consists of 100 physics equations extracted from the Feynman lectures on Physics. We compare against performance reported on SRBench [26]: a continuously updated public benchmark of SR methods on many datasets. Specifically, we compare against GPlearn, AFP, AFP-FE, DSR, uDSR, PySR, and AI Feynman [42, 47, 49, 28, 38]. Within this subset PySR represents an ablation of our model without the LLM genetic operations and the concept evolution (Section 3). We evaluate on a slightly noisy version of this dataset in order to simulate experimental errors common in scientific discovery domains. More details are presented in Appendix A.4.1. ", "page_idx": 5}, {"type": "text", "text": "Setup: We instantiate LASR using gpt-3.5-turbo-0125 [4] as the backbone LLM and calling it with $p=0.01$ for 40 iterations, and compare our results with PySR which uses the same default hyperparameters. For the other baselines, we use the numbers reported in SRBench with one exception being uDSR [28], for which we couldn\u2019t find any benchmarking numbers. For this method, we derive the exact solve rate from [37]. ", "page_idx": 5}, {"type": "text", "text": "Results: We showcase results in Table 1. We draw three observations from this experiment. First, LASR achieves a higher exact solve rate than all other baselines. Second, both PySR and LASR outperform the other baselines by a wide margin, indicating that scalable and efficient synthesis is imperative to practical scientific discovery algorithms. Finally, and most notably, a subset of the equations LASR finds could not be derived with any of the previous methods. ", "page_idx": 5}, {"type": "text", "text": "4.2 Cascading Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "LASR\u2019s performance is inherently bottlenecked by the reasoning capabilities of the backbone LLMs and the frequency of their invocation in each iteration. To evaluate the effect of the backbone LLM on LASR\u2019s performance, we instantiate a model cascade over two of LASR\u2019s hyperparameters: the backbone model (llama3-8b [17], gpt-3.5-turbo-0125) and the probability $p$ with which we call that model in the evolution step $(p=[1\\%,5\\%,10\\%])$ . ", "page_idx": 5}, {"type": "text", "text": "Setup: Our cascade operates as a tournament. We start LASR with the configuration that provides the least language guidance (llama3-8b at $p=1\\%$ ) and progressively increase the value of $p$ and then the backbone model. Each subsequent model is only evaluated on the problems that the previous model could not solve. We compare this against PySR\u2019s performance on the Feynman equation dataset. To ensure a fair comparison, we cascade PySR using the same procedure but find it does not solve any additional equations. For this experiment, we tag each equation with a qualitative rating comparing the equation to the ground truth form (Exact Solve, Almost Solve, Close, and Not Close). An in-depth discussion on this metric is presented in Section A.7. ", "page_idx": 5}, {"type": "text", "text": "Results: Our results are presented in 2. We draw two key observations from these results. First, LASR outperforms PySR even with minimal concept guidance (llama3-8b at $p=1\\%$ ). Second, increasing the backbone model size and the mixture probability substantially enhances LASR\u2019s performance, indicating that as the language reasoning capabilities of LLMs improve, so will our performance. ", "page_idx": 5}, {"type": "image", "img_path": "B7S4jJGlvl/tmp/5059308837b419039c1837b666ee8c17e4aee7916c234efeb42f7fb95c83efc4.jpg", "img_caption": ["Figure 3: Evaluation results for ablations/extensions of LASR. (Left): We ablate three components of LASR: Concept Evolution, Concept Library, and variable names and evaluate their MSE solve rate performance on the Feynman dataset over 40 iterations. We find that each component contributes to accelerating search at different stages in the search process. (Right): We extend LASR by providing an initial concept library $\\scriptstyle{\\mathcal{C}}_{0}$ in the form of user provided hints. We find that natural language hints significantly increases the speed of solving equations. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.3 Ablation Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct ablations on the use of Concept Evolution (skip phase three from Figure 1), Concept Library (skip phase two and three), variable names, and user hints. Figure 3 shows how these ablations affect performance over 40 iterations. We designate an equation as \u201csolved\u201d if, after $N$ iterations, the MSE of our predicted equation is less than $\\mathrm{\\dot{10^{-11}}}$ . This metric differs from \u2019Exact Solved\u2019 as defined in the prior experiments: an equation can be \u2019exactly solved\u2019 yet have an MSE higher than $10^{-11}$ due to the noise floor in the target variables, and an equation can have low loss but not be an exact match. We observe from the results that: (1) Removing variable names results in a substantial performance drop, as we lose semantic meaning provided by variables (for instance, observing $\\theta$ could suggest employing trigonometric functions on $\\theta$ ). (2) Learning a concept library enables faster convergence to solutions. Without the concept library, task convergence is slower, and widens under higher concept guidance conditions $(p>0.1\\%)$ ). ", "page_idx": 6}, {"type": "text", "text": "4.4 Qualitative Analysis and User Hints ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The concept library provides an interpretable window into our evolutionary search process. To showcase the concepts learned by LASR, we take a sample equation from the Feynman dataset, the electric field of a dipole $\\begin{array}{r}{E_{f}=\\frac{3\\dot{p_{d}}\\cos\\theta\\sin\\theta}{4\\pi\\epsilon r^{3}}}\\end{array}$ and comment on the libraries learned at various intervals. We see rudimentary concepts emerge in the second iteration: ", "page_idx": 6}, {"type": "text", "text": "\u201cThe presence of basic trigonometric functions like sin in the good expressions contributes to their quality, indicating a connection to physical concepts such as waveforms or periodic phenomena.\u201d ", "page_idx": 6}, {"type": "text", "text": "And, in subsequent iterations, the concepts become even more refined: ", "page_idx": 6}, {"type": "text", "text": "\u201cThe good mathematical expressions exhibit a balance between mathematical operations such as multiplication, division, and trigonometric functions, which are known to have physical interpretations and relevance in various scientific phenomena.\u201d ", "page_idx": 6}, {"type": "text", "text": "This iterative refinement helps LASR consistently maintain high-quality concepts, allowing it to converge to an exact match within 40 iterations. By contrast, PySR and the concept library ablations fail to converge on an exact match solution, returning equations that \u2014 while low-loss \u2014 involve many extra terms absent from the ground truth. This reinforces our hypothesis that injecting semantic meaning into the search process not only improves search efficiency, but also regularizes against complex equations \u2014 as the LLM-generated concepts help filter out irrelevant terms. A deeper qualitative analysis is in Appendix A.8. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Extending LASR with Hints: A benefit of LASR is that its search can be initialized with a set of user-specified, natural-language \u201chints.\u201d To evaluate this capability, we generate hints for each equation based on variations of the chapter title of the Feynman lecture that the equation belongs to. We intentionally keep the hints vague to see if knowledge about just the general field is sufficient in improving LASR\u2019s performance. We showcase results in Figure 3. We observe a noticeable boost in performance from injecting these hints, even for our weakest performing model, indicating that even minimal user input can substantially enhance LASR\u2019s effectiveness in discovering equations. ", "page_idx": 7}, {"type": "text", "text": "4.5 Data Leakage Validation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "An important consideration in using LLMs for existing SR problems is the possibility that the LLM was exposed to the hold-out problems in the validation set, presenting an unfair advantage to LLMs trained on massive datasets. Intuitively, LASR generates its own concepts which are conditioned on suboptimal programs, which are unlikely to be within the LLM\u2019s memorized responses. To validate this, we generate a dataset of 41 synthetic equations that are engineered to deviate from common physical and mathematical structures and have arbitrary variables. For example, one such equation is $\\begin{array}{r}{\\dot{y}=\\frac{0.782x_{3}+0.536}{x_{2}e^{x_{1}}(\\log{x_{2}}-x_{2}e^{\\cos{x_{1}}})}}\\end{array}$ . We find that PySR struggles to solve equations with these characteristics (given 400 iterations). Hence, solving such equations hinges on the language guidance components. ", "page_idx": 7}, {"type": "table", "img_path": "B7S4jJGlvl/tmp/fa48464dcf7588977861409346049b01b65e568abf950d0dd15eee1a9a673ae2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "We run LASR with Llama3-8B at $0.1\\%$ . We then compare our synthesized program\u2019s test set $R^{2}$ with that of PySR\u2019s. We justify using correlation instead of exact-match as we are not motivated by the application of results for scientific discovery in this experiment. Our results are summarized in Table 3 and show that LASR\u2019s concept-guided synthesis still provides a considerable performance boost compared to PySR \u2013 demonstrating that LASR can outperform PySR even when data leaking is not possible. ", "page_idx": 7}, {"type": "text", "text": "Table 3: Evaluation results of data leakage. We present the test set $R^{2}$ of PySR and of LASR on a synthetic symbolic regression dataset. Higher $\\overline{{R}}^{2}$ is better. ", "page_idx": 7}, {"type": "text", "text": "4.6 Using LASR to discover LLM Scaling Laws ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "So far, we have demonstrated that LASR can discover equations that are practical but already known (Feynman Dataset) and equations that are novel but aren\u2019t practical (Synthetic Dataset). To investigate LASR\u2019s utility in finding novel and practical empirical trends, we investigate whether LASR can discover novel LLM scaling laws on the BigBench dataset [16]. More details on this experiment are presented in Section A.6. ", "page_idx": 7}, {"type": "text", "text": "Traditionally, to identify an LLM scaling law, practitioners must first manually posit a \u201cskeleton equation\u201d with a fixed set of known variables and unknown free parameters, and then optimize the unknown parameters based on a dataset of model hyperparameters and resulting dataset fitness [23, 1, 5]. Instead of starting with a predefined equation, we use LASR to discover the skeleton equation that best fits various subsets of the BigBench dataset. ", "page_idx": 7}, {"type": "text", "text": "Setup. BigBench contains 204 tasks with scored responses from 55 language models trained with different hyperparameters. We evaluate on the subset of tasks where the preferred metric is \u2018Multiple choice grade\u2019 (53,812 samples). Our goal is to find the equation that best predicts the test score given the model hyperparameters and the dataset hyperparameters. We run LASR with 3840 populations of 200 candidates each for 7 hours (overnight). The runtime of LASR is comparable to other SR algorithms for this experiment as the slowest operation isn\u2019t generating candidate equations but rather optimizing and evaluating candidate equations. ", "page_idx": 7}, {"type": "text", "text": "Results. LASR discovers the following scaling law on the subset of BigBench: ", "page_idx": 7}, {"type": "equation", "text": "$$\n{\\mathsf{s c o r e}}={\\frac{A}{\\left({\\frac{\\mathsf{t r a i n}_{-}\\mathrm{steps}}{B}}\\right)^{\\#{\\mathrm{shots}}}}}+E\n$$", "text_format": "latex", "page_idx": 7}, {"type": "table", "img_path": "B7S4jJGlvl/tmp/11c32487cdc9c5a984297708801a92ffc204250414ff048ffe0dad8b1f9b4172.jpg", "table_caption": ["Table 4: Preliminary results on evaluating different LLM scaling laws. We measure MSE loss on a held out subset of BigBench [16]. The equation discovered with LASR performs as well as the Chinchilla equation [23] on BigBench while using less free parameters. The residual term skeleton equation ${\\bf{s c o r e}}=E_{\\parallel}$ ) also performs well. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "where score is the MCQ grade, train_steps is the number of training steps for the model, and #shots is the number of in-context examples provided during inference. The fitted free parameters are presented in Equation 5. ", "page_idx": 8}, {"type": "text", "text": "Qualitative Evaluation: Equation 4 describes an empirical relationship between training hyperparameters (training steps) and inference hyperparameters (number of shots). It asserts that increasing the number of shots exponentially increases the model\u2019s performance for low-resource models, while having diminishing gains as the number of training steps of the model increase. This observation is consistent with work in scaling test-time compute [46]. However, if the number of shots is an even number, the exponentially increasing trend reflects over the line ${\\tt s c o r e}=E$ and turns into an exponentially decreasing trend. A manual inspection of the training data reveals the majority of samples had an odd number of shots (1, 3, ...) which causes this generalization error. ", "page_idx": 8}, {"type": "text", "text": "As the output artifacts of LASR are interpretable, we can integrate this empirical relationship between training steps and number of shots into known scaling laws. Specifically, we can augment the chinchilla scaling law as follows: ", "page_idx": 8}, {"type": "equation", "text": "$$\n{\\begin{array}{r l r}&{{\\mathrm{score}}={\\frac{A}{\\left({\\mathrm{train}}_{-}{\\mathrm{steps}}\\cdot{\\mathrm{batch}}_{-}{\\mathrm{size}}\\right)^{\\alpha}}}+{\\frac{B}{\\left({\\mathrm{trarams}}\\right)^{\\beta}}}+E}&{{\\mathrm{(Chinchilla~}}[23]{\\mathrm{)}}}\\\\ &{{\\mathrm{score}}={\\frac{A}{\\left({\\mathrm{train}}_{-}{\\mathrm{steps}}\\cdot{\\mathrm{batch}}_{-}{\\mathrm{size}}\\right)^{\\alpha\\cdot{\\mathrm{sibats}}}}}+{\\frac{B}{\\left({\\mathrm{tparams}}\\right)^{\\beta}}}+E}&{{\\mathrm{(Modified~Chinchilla)}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Quantitative Evaluation: We fit the free parameters of each equation on the training set (43, 049 samples) and measure the MSE loss between the actual grade and the predicted grade on the validation set (10, 763 samples). The results are presented in Table 4. We find that the Equation 4\u2019s performance, as well as modified Chinchilla\u2019s performance, is competitive with that of Chinchilla\u2019s in predicting the MCQ grade. However, the horizontal line ${\\mathsf{s c o r e}}=E$ demonstrates acceptable performance as well. We believe increasing the scale of these preliminary experiments (with richer datasets or longer search horizon) will lead to additional empirical findings. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Symbolic Regression. The field of SR started in the 1970s [18, 29] and has recently become a prominent approach to AI-for-science [33, 34, 40]. Two algorithmic themes here are: ", "page_idx": 8}, {"type": "text", "text": "Non-parametric Algorithms: Most work on SR focuses on improving search efficiency using heuristics or parallelization. Specifically, PySR [10] builds a multi-population evolutionary algorithm that incorporates various preexisting heuristics [39], and introduces novel ones such as simulated annealing, an evolve-simplify-optimize loop, and an adaptive parsimony metric. PySR has been successfully applied to study problems in domains such as cosmology [12], international economics [50], and climate modeling [20]. LASR extends PySR to enable the discovery of latent concepts. ", "page_idx": 8}, {"type": "text", "text": "Parametric Algorithms: Recent work in SR and program synthesis has often used neural networks to accelerate search [43, 40, 38, 28, 34, 13, 35]. The interplay between the neural and the symbolic components in these works can be abstracted into two categories: (1) leveraging LLMs to induce program scaffolds [34, 40, 35], and (2) learning a neural policy to accelerate search [38, 28, 43, 13]. We highlight two methods from the first category: Funsearch [40] and LLM-SR [45]. Funsearch [40] uses a pretrained LLM to implement a mutation operator on a database of executable programs under a fixed specification to find super-optimized programs in extremal combinatorics. LASR is a generalization of FunSearch: while FunSearch conditions program generation on a static \u201cspecification\u201d (analogous to our concept library), we discover the concept library in the course of the algorithm. We do not compare against FunSearch due to resource constraints. As for LLM-SR [45], it leverages a pretrained LLM for generating program sketches [36]. The sketch parameters are optimized and cached in a database, which is in turn used to generate new sketches. Our work is an orthogonal direction of improvement. It is technically possible to \u201cplug\u201d the LLM-SR framework (or other LLM-based search algorithms [35]) into LASR and use our generated concepts to guide the lower-level search component. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "The second category includes methods like DSR [38], which, just like LASR, frame SR as a sequence modeling problem. However, the search in LASR leverages a learned concept library and the language and code biases in LLMs, instead of relying on amortization alone. ", "page_idx": 9}, {"type": "text", "text": "Program Synthesis with Foundation Models. Recent work in program synthesis models program generation as a sequence prediction problem. Under this paradigm, the DSL and the input-output specification is serialized in the prompt and a code-generation foundation model [31, 7, 4] is leveraged to autoregressively generate candidate programs. This approach has been impactful in many areas including spreadsheet formula prediction [13, 8], competitive programming [32], and visual programming [48, 21, 9]. LASR is similar to work in this area in that the LLM Mutate, LLM Crossover, and LLM Initialization functions all follow the sequence prediction paradigm to synthesize mathematical equations, relying on guidance from the concept library. ", "page_idx": 9}, {"type": "text", "text": "Program Synthesis with Library Learning. Deploying classical program synthesizers in a new domain often necessitate hand-engineering DSLs to enable scalable synthesis. This severely limits the generality and practicality of such methods. An emerging direction of research \u2013 called library learning \u2013 attempts to learn the DSL and the programs simultaneously [15, 3, 19, 27, 53, 14, 44, 54]. This is typically framed as a hierarchical Bayesian optimization problem over the space of programs and the space of library functions that generate those programs. Notably, [19] uses LLM guidance to assist in program induction and in auto-documenting learned library modules and [53] considers learning programs under a latent distribution over the space of natural language and the space of the DSL. LASR shares a similar problem formulation to these works, but optimizes over the space of programs and over the space of natural language descriptions of these programs. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have presented LASR, a framework that uses zero-shot queries to an LLM to induce abstract, reusable concepts that can be used to accelerate SR. We have shown that LASR outperforms stateof-the-art approaches on the standard Feynman equation task. We have also used the algorithm to discover a novel scaling law for LLMs. ", "page_idx": 9}, {"type": "text", "text": "A key benefti of LASR is that its capabilities are ultimately bottlenecked by those of the underlying LLM. LLMs are rapidly gaining capability and getting cheaper, and future versions of LASR should be able to tap into this progress. ", "page_idx": 9}, {"type": "text", "text": "Many directions of research remain open. First, our strategy of accelerating evolutionary search with LLM-based concept induction may be applicable beyond the SR setting. Future research should explore such applications. Second, while our approach here was entirely based on in-context learning, it is worth exploring if finetuning improves the performance of the LLM. Finally, we evaluated the learned concept library exclusively on the downstream SR task. However, the library may also be valuable in other tasks such as clustering or explanation synthesis. Exploring these other tasks is an attractive topic for future work. ", "page_idx": 9}, {"type": "text", "text": "Limitations. The current instantiation of LASR has several limitations. First, it cannot guarantee that the concepts it learns are correct or insightful. Even a concept that leads to strong performance in downstream SR tasks may do so because of quirks of the model and data, and end up misleading scientists using the method in a discovery process. Also, we do not currently have a way to ensure that the learned concepts are mutually consistent. Finally, our evaluation here was constrained by our compute budgets for LLMs and search. Whether the trends we see generalize to higher-compute regimes remains to be seen. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements: This research was partially supported by the NSF Expeditions in Computing Award #CCF-1918651, the NSF National AI Institute for Foundations of Machine Learning (IFML), and ARO award #W911NF-21-1-0009. We thank Foundry Technologies for providing substantial computational resources for our experiments. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "[1] Ibrahim Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[2] Rohit Batra, Le Song, and Rampi Ramprasad. Emerging materials intelligence ecosystems propelled by machine learning. Nature Reviews Materials, 6(8):655\u2013678, 2021. [3] Matthew Bowers, Theo X Olausson, Lionel Wong, Gabriel Grand, Joshua B Tenenbaum, Kevin Ellis, and Armando Solar-Lezama. Top-down synthesis for library learning. Proceedings of the ACM on Programming Languages, 7(POPL):1182\u20131213, 2023.   \n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[5] Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. In The Eleventh International Conference on Learning Representations, 2023.   \n[6] Swarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, Yisong Yue, et al. Neurosymbolic programming. Foundations and Trends\u00ae in Programming Languages, 7(3):158\u2013243, 2021.   \n[7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.   \n[8] Xinyun Chen, Petros Maniatis, Rishabh Singh, Charles Sutton, Hanjun Dai, Max Lin, and Denny Zhou. Spreadsheetcoder: Formula prediction from semi-structured context. In International Conference on Machine Learning, pages 1661\u20131672. PMLR, 2021.   \n[9] Mia Chiquier, Utkarsh Mall, and Carl Vondrick. Evolving interpretable visual classifiers with large language models, 2024.   \n[10] Miles Cranmer. Interpretable machine learning for science with pysr and symbolicregression. jl. arXiv preprint arXiv:2305.01582, 2023.   \n[11] Miles Cranmer, Alvaro Sanchez-Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel, and Shirley Ho. Discovering symbolic models from deep learning with inductive biases. In Neural Information Processing Systems, 2020.   \n[12] Benjamin L Davis and Zehao Jin. Discovery of a planar black hole mass scaling relation for spiral galaxies. The Astrophysical Journal Letters, 956(1):L22, 2023.   \n[13] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Robustfill: Neural program learning under noisy I/O. In ICML, 2017.   \n[14] Kevin Ellis, Lucas Morales, Mathias Sabl\u00e9-Meyer, Armando Solar-Lezama, and Josh Tenenbaum. Learning libraries of subroutines for neurally\u2013guided Bayesian program induction. In Advances in Neural Information Processing Systems, pages 7805\u20137815, 2018.   \n[15] Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lucas Morales, Luke Hewitt, Armando Solar-Lezama, and Joshua B Tenenbaum. Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning. arXiv preprint arXiv:2006.08381, 2020.   \n[16] Aarohi Srivastava et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023.   \n[17] Abhimanyu Dubey et al. The llama 3 herd of models, 2024.   \n[18] Donald Gerwin. Information processing, data inferences, and scientific generalization. Behavioral Science, 19(5):314\u2013325, 1974.   \n[19] Gabriel Grand, Lionel Wong, Matthew Bowers, Theo X Olausson, Muxin Liu, Joshua B Tenenbaum, and Jacob Andreas. Lilo: Learning interpretable libraries by compressing and documenting code. arXiv preprint arXiv:2310.19791, 2023.   \n[20] Arthur Grundner, Tom Beucler, Pierre Gentine, and Veronika Eyring. Data-driven equation discovery of a cloud cover parameterization. Journal of Advances in Modeling Earth Systems, 16(3):e2023MS003763, 2024.   \n[21] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14953\u201314962, 2023.   \n[22] Alberto Hernandez, Adarsh Balasubramanian, Fenglin Yuan, Simon AM Mason, and Tim Mueller. Fast, accurate, and transferable many-body interatomic potentials by symbolic regression. npj Computational Materials, 5(1):112, 2019.   \n[23] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.   \n[24] John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman. Introduction to automata theory, languages, and computation, 3rd Edition. Pearson international edition. Addison-Wesley, 2007.   \n[25] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.   \n[26] William La Cava, Patryk Orzechowski, Bogdan Burlacu, Fabr\u00edcio Olivetti de Fran\u00e7a, Marco Virgolin, Ying Jin, Michael Kommenda, and Jason H Moore. Contemporary symbolic regression methods and their relative performance. arXiv preprint arXiv:2107.14351, 2021.   \n[27] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332\u20131338, 2015.   \n[28] Mikel Landajuela, Chak Shing Lee, Jiachen Yang, Ruben Glatt, Claudio P Santiago, Ignacio Aravena, Terrell Mundhenk, Garrett Mulcahy, and Brenden K Petersen. A unified framework for deep symbolic regression. Advances in Neural Information Processing Systems, 35:33985\u2013 33998, 2022.   \n[29] Pat Langley. Bacon: A production system that discovers empirical laws. In International Joint Conference on Artificial Intelligence, 1977.   \n[30] Pablo Lemos, Niall Jeffrey, Miles Cranmer, Shirley Ho, and Peter Battaglia. Rediscovering orbital mechanics with machine learning. Machine Learning: Science and Technology, 4(4):045002, 2023.   \n[31] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.   \n[32] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092\u20131097, 2022.   \n[33] Nour Makke and Sanjay Chawla. Interpretable scientific discovery with symbolic regression: a review. Artificial Intelligence Review, 57(1):2, 2024.   \n[34] Matteo Merler, Nicola Dainese, and Katsiaryna Haitsiukevich. In-context symbolic regression: Leveraging language models for function discovery. arXiv preprint arXiv:2404.19094, 2024.   \n[35] Elliot Meyerson, Mark J. Nelson, Herbie Bradley, Adam Gaier, Arash Moradi, Amy K. Hoover, and Joel Lehman. Language model crossover: Variation through few-shot prompting, 2024.   \n[36] Vijayaraghavan Murali, Letao Qi, Swarat Chaudhuri, and Chris Jermaine. Neural sketch learning for conditional program generation. ICLR, 2018.   \n[37] Deep Symbolic Optimization Organization. Srbench symbolic solution. https: //github.com/dso-org/deep-symbolic-optimization/blob/master/images/ srbench_symbolic-solution.png. Accessed: 2024-05-22.   \n[38] Brenden K Petersen, Mikel Landajuela, T Nathan Mundhenk, Claudio P Santiago, Soo K Kim, and Joanne T Kim. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. arXiv preprint arXiv:1912.04871, 2019.   \n[39] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. In Proceedings of the aaai conference on artificial intelligence, volume 33, pages 4780\u20134789, 2019.   \n[40] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large language models. Nature, 625(7995):468\u2013475, 2024.   \n[41] Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. Science, 324(5923):81\u201385, 2009.   \n[42] Michael D. Schmidt and Hod Lipson. Age-fitness pareto optimization. In Annual Conference on Genetic and Evolutionary Computation, 2010.   \n[43] Ameesh Shah, Eric Zhan, Jennifer J Sun, Abhinav Verma, Yisong Yue, and Swarat Chaudhuri. Learning differentiable programs with admissible neural heuristics. In Advances in Neural Information Processing Systems, 2020.   \n[44] Richard Shin, Miltiadis Allamanis, Marc Brockschmidt, and Oleksandr Polozov. Program synthesis and semantic parsing with learned code idioms. In Advances in Neural Information Processing Systems, pages 10825\u201310835, 2019.   \n[45] Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan K Reddy. Llm-sr: Scientific equation discovery via programming with large language models. arXiv preprint arXiv:2404.18400, 2024.   \n[46] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024.   \n[47] Trevor Stephens. gplearn: Genetic programming in python, with a scikit-learn inspired api, 2024. Accessed: 2024-05-22.   \n[48] D\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023.   \n[49] Silviu-Marian Udrescu and Max Tegmark. Ai feynman: A physics-inspired method for symbolic regression. Science Advances, 6(16):eaay2631, 2020.   \n[50] Sergiy Verstyuk and Michael R Douglas. Machine learning the gravity equation for international trade. Available at SSRN 4053795, 2022.   \n[51] Marco Virgolin, Ziyuan Wang, Tanja Alderliesten, and Peter AN Bosman. Machine learning for the prediction of pseudorealistic pediatric abdominal phantoms for radiation dose reconstruction. Journal of Medical Imaging, 7(4):046501\u2013046501, 2020.   \n[52] Eugene P Wigner. The unreasonable effectiveness of mathematics in the natural sciences. In Mathematics and science, pages 291\u2013306. World Scientific, 1990.   \n[53] Catherine Wong, Kevin M Ellis, Joshua Tenenbaum, and Jacob Andreas. Leveraging language to learn program abstractions and search heuristics. In International conference on machine learning, pages 11193\u201311204. PMLR, 2021.   \n[54] Eric Zelikman, Qian Huang, Gabriel Poesia, Noah Goodman, and Nick Haber. Parsel: Algorithmic reasoning with language models by composing decompositions. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Broader Societal Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We have presented LASR: a symbolic regression framework that leverages concept guidance to accelerate symbolic regression. We hope that LASR helps accelerate the search for empirical laws in the broader scientific community. In this section, we discuss the broader societal impacts and ethical considerations of our work. ", "page_idx": 14}, {"type": "text", "text": "Potential for Misuse: As with other ML techniques, symbolic regression can be leveraged by bad actors to inflict societal harm. Our experiments show that LASR accelerates the search for empirical laws from raw observations. In our setting, we are restricted to observations about physical phenomena. However, a malicious actor could misuse LASR to find patterns in datasets that violate personal rights. ", "page_idx": 14}, {"type": "text", "text": "Privacy Concerns: As mentioned before, LASR enables finding patterns in raw observations. We hope that LASR is leveraged by scientists to explain physical phenomena. However, it is possible to use such models to learn behavioral profiles without the active knowledge or explicit consent of the subjects. ", "page_idx": 14}, {"type": "text", "text": "Bias and Fairness: LASR generates two artifacts: a hypothesis that maximizes a fitness function (represented as an equation) and a library of concepts that helped discover that hypothesis. LASR ensures fairness and lack of bias in the generated equation as long as the fitness function is free of biases as well. However, we leverage foundation models to induce our library of concepts which could be trained on biased data which may reflect in our concept library. Furthermore, we cannot directly evaluate the efficacy of the concept library and its factual correctness. This doesn\u2019t affect equation generation \u2013 since equations are quantitatively evaluated. However, a human analyzing the concepts LASR learns might misinterpret trends that the model picks up on. ", "page_idx": 14}, {"type": "text", "text": "A.2 LLM Prompts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Note that in the prompts in Figure 4, Figure 5, and Figure 6, we refer to our hypothesis as expressions and the concepts as hypotheses and suggestions. This prompting style was found to work best for the LLM. ", "page_idx": 14}, {"type": "text", "text": "A.3 Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.3.1 Compute Usage ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We run all experiments on a server node with 8xA100 GPUs with 80 GB of VRAM each. However, our experiments can be reproduced with a GPU with 16 GB of VRAM. We were even able to run LASR on a laptop utilizing a quantized model hosted locally 3. Moreover, certain models are hosted on external servers (such as gpt-3-turbo-0125) which allows running LASR on machines without GPUs. For this project, we chose to run llama3-8b using vLLM [25]. However, our framework is compatible with any LLM inference framework that allows hosting an OpenAI compliant RESTful server. For reference, each iteration makes around 60, 000 calls. Each call to the LLM is just under 1000 tokens. This gives an upper bound on total compute of 60, 000, 000 tokens per iteration if $p=100\\%$ . Hence, running our model at $p=1\\%$ for 40 iterations would result in just under 25M tokens for each equation (around 4 hours on a single A100). ", "page_idx": 14}, {"type": "text", "text": "A.3.2 Concept Sampling ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In order to determine which concepts from the concept library we sample for the LLM Hypothesis Evolution, we randomly choose the top-K most recent concepts in the library. This ensures that we use the latest concepts, which are generally reflective of more informed hypotheses, and thus better to use. In practice, we set $K\\,=\\,20$ . Additionally, for Concept Evolution, we exclude the top-K most recent concepts from being used, and rather use older concepts. This is motivated by the desire to not have the concept library converge on a few ideas, rather we want diversity of thought. Our concepts are intended to be longer lasting than the hypotheses that generated them, similar to how observational data comes and goes, but the conclusions from them are more persistent. ", "page_idx": 14}, {"type": "text", "text": "\\`\\`json   \n[\"expr1\",   \n\"expr2\",   \n\"expr{{N}}\" ", "page_idx": 15}, {"type": "text", "text": "(Assistant) Generated Explanation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Based on the provided suggestions and expressions, I propose the following five recombined expressions: ", "page_idx": 15}, {"type": "text", "text": "1. \\`log(exp(C - sqrt(theta ^ C)) $+\\texttt{c}$ )\\`: This expression combines the exponential function from Expression 2 with the logarithmic function from Expression 1. The \\`- sqrt(theta $\\rightsquigarrow$ )\\` term is taken from Expression 2 and added to the exponential function, while the $\\because\\mathbf{c}$ term is taken from Expression 1. ", "page_idx": 15}, {"type": "text", "text": "2. \\`(sin(C - sqrt(theta)) $\\star$ (theta ^ log(theta $\\prime$ theta)) $+\\texttt{c}$ )\\`: This expression combines the sine function from Expression 1 with the power operation from the same expression. The \\`(theta ^ log(theta $\\dot{\\boldsymbol{\\prime}}$ theta))\\` term is taken from Expression 1, and the \\`sin(C - sqrt(theta))\\` term is also from Expression 1. The $\\because\\textbf{c}$ term is added at the end. ", "page_idx": 15}, {"type": "text", "text": "3. \\`exp(C - sqrt(theta)) $\\star$ sin(C - sqrt(theta))\\`: This expression combines the exponential function from Expression 2 with the sine function from Expression 1. Both the exponential and sine functions are taken from separate expressions and multiplied together. ", "page_idx": 15}, {"type": "text", "text": "4. \\`log(theta ^ log(theta / theta)) $+\\textbf{c}$ - sqrt(theta $\\textbf{\\textsf{n}}\\mathbf{c}$ )\\`: This expression combines the logarithmic function from Expression 1 with the power operation from the same expression. The \\`(theta ^ log(theta / theta))\\` term is taken from Expression 1, and the \\`- sqrt(theta $\\bullet\\,\\,{\\mathfrak{c}}_{.}$ )\\` term is taken from Expression 2. The $\\because\\textbf{c}$ term is added at the end. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{ant)}\\left\\{\\begin{array}{l l}{\\mathrm{~|{ere~is~the~ist~of~proposed~expressions~in~}|{S O N}\\mathrm{~format:~}}}&{}\\\\ {\\cdots\\cdot\\mathrm{~json~}}&{}\\\\ {\\mathrm{~|{\\Psi^{*}~l o g}\\left({e x p\\left({\\mathbb{C}~-~s q r t}\\left(t h e t a~\\textbf{\\textsf{A}}~{C}\\right)}\\right)~+~{\\mathbb{C}}\\right)^{\\prime}~},}\\\\ {\\mathrm{~\\forall\\,\\mathrm{~\\verb~\"~(sin({\\mathbb{C}~-~s q r t}\\left(t h e t a\\right))~\\neq~(t h e t a~\\textbf{\\textsf{A}}~{\\mathrm{~1og}(t h e t a)}~/~t h e t a))~+~{\\mathbb{C}}\\right)^{\\prime}~},}}\\\\ {\\mathrm{~\\forall\\,\\mathrm{~\\verb~\"~{exp}\\left({\\mathbb{C}~-~s q r t}\\left(t h e t a\\right)~\\right)~\\star~\\sin\\left({\\mathbb{C}~-~s q r t}\\left(t h e t a\\right)\\right)\"~},}}\\\\ {\\mathrm{~\\forall\\,\\mathrm{~\\verb~\"~{log}\\left(t h e t a~\\textbf{\\textsf{A}}~{\\mathrm{~1og}\\left(t h e t a\\right)~/~t h e t a}\\right)~+~{\\mathbb{C}}~-~s q r t}\\left(t h e t a~\\mathrm{~\\textsf{A}~}~{\\mathbb{C}}\\right)\"~},}\\\\ {\\cdots\\left(\\mathrm{~\\textbf{sin}({\\mathbb{C}~-~s q r t}\\left(t h e t a\\right)~\\right)~\\star~\\left(t h e t a~\\mathrm{~\\textsf{A}~}~{\\mathrm{~1og}\\left(t h e t a~/~t h e t a~\\right)}~+~{\\mathbb{C}}\\right)^{\\prime}~}\\right)}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Figure 4: LLMCROSSOVER prompt with an example output. LLMMUTATION and LLMINIT follow the same structure but with slightly different wording and with one and no reference expressions, respectively. Variables within double braces are replaced with the instance specific arguments. These prompts are available in prompts/\\*.txt in the linked repository. ", "page_idx": 15}, {"type": "table", "img_path": "B7S4jJGlvl/tmp/50f3848e2b03f196f167358eb308dd9bc8e858788c9988796ac6593e474a22ac.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 5: LLM Concept Abstraction prompt with an example output. The LLM Concept Crossover function follows a similar structure, with a modified task description for crossover on concepts. ", "page_idx": 16}, {"type": "table", "img_path": "B7S4jJGlvl/tmp/a42aadd389f849739df139dbd4ab7b5658d8557dff4c59ffe0eb3bbbadb36d04.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 6: LLM Concept Evolution prompt with an example output. The LLM Concept Evolution prompt follows a similar structure to the concept abstraction and LLM operation prompts with slight modifications. ", "page_idx": 17}, {"type": "text", "text": "niterations $=48$ \uff0c   \nncyclesperiteratior $1=550$ \uff0c   \npopulations $=\\!15$ \uff0c   \npopulation_size $=33$ \uff0c   \nmaxsize $1=38$ \uff0c   \nbinary_operators $:=$ [\"+\", \u201c\\*\", \"-\", \u201c/\", \u201c\u25b3\"],   \nunary_operators $:=$ [\"exp\",\"log\",\"sqrt\",\"sin\",\"cos\"],   \nweight_randomi $z e{=}0,1$   \nnested_constraints={\"sin\":{\"sin\":O\uff0c\"cos\": 0}, \"cos\":{\"sin\": O\uff0c\"cos\":0}, \"exp\": {\"exp\":0, \"log\": 0}, \"log\": {\"exp\": 0, \"log\": 0}, \"sqrt\": {\"sqrt\": 0}},   \nconstraints={\"sin\": 10,\"cos\": 10, \"exp\":20\uff0c\"log\":20\uff0c\"sqrt\":20, \"pow\":\uff08-1\uff0c20)}, ", "page_idx": 18}, {"type": "text", "text": "A.3.3 Hyperparameters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "xFigure 7 showcases the hyperparameters used for all our experiments. Wherever possible, we use the default PySR parameters. Additionally, LASR introduces three new hyperparameters: (1) $\\%$ of LLM calls, (2) List of user hints, and (3) a dictionary of parameters pertaining to backend LLM communication. Following other methods in SRBench, we utilize only a subset of the necessary operators for solving the Feynman equations, excluding special operators like arcsin and arctan. These operators are seldom required, and removing them speeds up the search process. We generally set the number of iterations to 40. However, certain experiments may demand more or less iterations. ", "page_idx": 18}, {"type": "text", "text": "A.4 Dataset Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A.4.1 Feynman Equations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Dataset: The Feynman Equation dataset is a widely adopted benchmark for scientific discovery [49]. The dataset consists of 100 physics equations extracted from the Feynman lectures on Physics. Each equation is in the form $y\\doteq\\bar{f}(x_{1},x_{2}\\overset{\\cdot}{,}\\ldots)$ . The number of input variables ranges from two to ten, and the dataset provides 100,000 samples for each equation. We compare against publically available methods benchmarked on SRBench [26]. SRBench is a continuously updated benchmark which catalogs the performance of various methods on the Feynman dataset as well as other symbolic regression problems. Specifically, we compare against GPlearn, AFP, AFP-FE, DSR, uDSR, PySR, and the original AI Feynman algorithm [42, 47, 49, 28, 38]. Within this subset, notably, PySR represents an ablation of our model without the LLM genetic operations and the concept evolution (Section 3). We evaluate on a slightly noisy version of this dataset in order to simulate experimental errors common in scientific discovery domains. Specifically, we compare numbers against those reported in and reproduced by SRBench with a target noise of 0.001. ", "page_idx": 18}, {"type": "text", "text": "Methodology: For the Feynman dataset, we took the equations and the bounds at which each variable was sampled at and generated our dataset. Then, we added additional noise of 0.001 to our target variable, following the noise formula detailed in the Appendix A.4 of [26], as well as additional random noise variables with arbitrary names to force the model for proper feature selection. We then evaluate exact matches by looking at if the predicted equation symbolically simplifies into the ground truth equation. For the ablation graphs, we used the PySR hyperparameter \"early_stop_condition\" to check if there is a \"solution\" after $N$ iterations. ", "page_idx": 18}, {"type": "text", "text": "A.4.2 Synthetic Dataset ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For the synthetic dataset, we ran a script that generates uncommon mathematical hypotheses that satisfy our constraints at random. Then, we ran PySR for 400 iterations and found all the equations that PySR performed poorly in, i.e. MSE loss greater than 1, while having a complexity less than 20. ", "page_idx": 18}, {"type": "table", "img_path": "B7S4jJGlvl/tmp/fd524ed8fb97a8e120996ed1486bb93d8143dafd1ee35ff43a51e818a7b26796.jpg", "table_caption": [], "table_footnote": ["Table 5: An asymmetric comparison of $\\mathrm{PySR}$ and LASR on the Feynman equations dataset (A.5.1). We run $\\mathrm{PySR}$ for 10 hours per equation (thresholded to $10^{6}$ iterations) and compare the exact solve rate with that of LASR run for 40 iterations. We find that PySR is able to discover three more equations, but LASR still substantially outperforms PySR. "], "page_idx": 19}, {"type": "text", "text": "For these 41 remaining equations, we then compared LASR and PySR after 20 iterations using the average of their test set $R^{\\dot{2}}$ for each hypothesis. ", "page_idx": 19}, {"type": "text", "text": "A.5 Additional Experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This section highlights additional experiments on various benchmarks to further characterize LASR\u2019s performance. ", "page_idx": 19}, {"type": "text", "text": "A.5.1 Asymmetric comparison with PySR ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "A common concern with evaluating genetic optimization algorithms w.r.t number of iterations is that the \u2019hard stop\u2019 after a certain number of iterations might yield populations that haven\u2019t fully converged to their optimal values. ", "page_idx": 19}, {"type": "text", "text": "To account for this discrepancy in performance, we conduct an asymmetric comparison with PySR on the Feynman Equations dataset. Specifically, we allow PySR to run uninterrupted for 10 hours per equation, with the maximum number of iterations set to $10^{6}$ . We compare the results with that of LASR run interrupted for 40 iterations per equation (hence the asymmetric comparison). The results are detailed in Table 5. ", "page_idx": 19}, {"type": "text", "text": "Overall, we find that, despite running PySR substantially longer than LASR, LASR still substantially outperforms PySR. This is because PySR, like other evolutionary algorithms, is susceptible to falling in local minima and converges to this local minima extremely fast. This is why supplementing \u201clocal\u201d search strategies with LLM guidance is useful. ", "page_idx": 19}, {"type": "text", "text": "A.5.2 Qualitative comparision of Synthetic Dataset equations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Table 3, we reported $R^{2}$ test set performance of LASR and PySR on the synthetic dataset. In this section, we qualitatively compare the equations discovered by LASR and PySR on the procedurally generated dataset of equations. None of the algorithms recover the ground truth form, but we find that LASR\u2019s equations fit much better to the ground truth data than PySR\u2019s equations. These equations are presented in Table 3. ", "page_idx": 19}, {"type": "text", "text": "A.5.3 Stochasticity of LASR and PySR ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "A common concern with using LLM\u2019s for scientific discovery is that the equations could be obtained due to dataset memorization rather than pure reasoning and learning on the equations form. To explore this further, we run LASR with the same hyperparameters twice (with different seeds). If the LLM purely relies on memorization, we expect the model to find the same equation form in both experiments. We present results in Table 7. ", "page_idx": 19}, {"type": "text", "text": "Overall, both equations fit well to the underlying dataset and reduce to the ground truth. Yet, despite using the same hyperparameters, the functional forms exhibit sharp differences. This further reinforces our hypothesis that LASR\u2019s performance is not simply the result of regurgitated memorized responses. ", "page_idx": 19}, {"type": "text", "text": "A.6 Using LASR to find an LLM Scaling Law ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "So far, we have used LASR to discover equations that are already well-established in prior work. In this section, we investigate LASR\u2019s utility in making novel empirical discoveries. Specifically, we investigate whether LASR can discover novel scaling laws for LLMs. ", "page_idx": 19}, {"type": "table", "img_path": "B7S4jJGlvl/tmp/8f0f9d7e7b837ccc8d77613f7889027ee1e58b68db85afb4939fc94655efc2c4.jpg", "table_caption": [], "table_footnote": ["Table 6: Qualitative evaluation of LASR on the synthetic equations dataset (A.5.2). We attempt to recover the ground truth equation from a slightly noisy dataset generated from the ground truth equations. None of the algorithms we tested (including LASR) are able to recover the ground truth equations, underscoring the challenge of exact symbolic match on this dataset. A study on the $R^{2}$ performance is presented in Table 3. "], "page_idx": 20}, {"type": "table", "img_path": "B7S4jJGlvl/tmp/036f3158e8cb50fdbc329d0f02d7e2916658ae136ca51744dcb31ac614de604a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 7: Performance of LASR on successive runs with the same hyperparameters on Equation 53 (ian nt hien vFeersyen smqauna reeq luaatwi ownist hd agtraosuetn (d Atr.5u.t3h) .f oTrhmisu leatqiuoant:i $\\begin{array}{r}{h=\\frac{P}{4\\pi R^{2}.}}\\end{array}$ )t.h eW hee eatv afllouawt ef rLoAm Sa Rp obiyn tr usnonuirncge it twice with identical hyperparameters but different random seeds to assess whether the model has memorized the equation\u2019s form. In both runs, LASR consistently discovers high-performing solutions. Moreover, the resulting functional forms show significant variation between runs, supporting the hypothesis that LASR\u2019s success is not rooted in memorization. ", "page_idx": 20}, {"type": "text", "text": "Motivation: Traditionally, to identify an LLM scaling law, practitioners must first manually posit a \u201cskeleton equation\u201d with a fixed set of known variables and unknown free parameters, and then optimize the unknown parameters based on a dataset of model hyperparameters and resulting dataset ftiness [23, 1, 5]. Instead of starting with a predefined equation, we use LASR to discover the skeleton equation that best fits various subsets of the BigBench dataset. Removing the need to manually posit a skeleton equation simplifies the methodology for finding scaling laws in many ways. First, it removes human preconceptions about the expected relationships between hyperparameters. Second, it increases the number of variables and the type of variables human practitioners can jointly reason about. Finally, it enables positing equations of much higher complexity and variable interdependence than otherwise possible. ", "page_idx": 21}, {"type": "text", "text": "Dataset: BigBench is a collaborative benchmark intended to probe large language models and extrapolate their future capabilities [16]. It contains 204 tasks drawing upon problems from linguistics, cognitive science, math, physics, biology, software development, etc. For each task, Bigbench measures the performance on many model families (OpenAI\u2019s GPT models, Google\u2019s dense transformer architectures, and Switch-style sparse transformers). Each BigBench task is evaluated on a preferred metric (chosen by dataset creators). In our experiments, we consider a subset of tasks where the preferred metric is \u2018multiple choice grade.\u2019 This subset contains the highest diversity of tasks and around 53,812 total data points. ", "page_idx": 21}, {"type": "text", "text": "Methodology: We run LASR with 3840 populations (parallelized across 16 cores) with each population evolving 200 candidates evaluated with the Zygote autodifferentiation backend. We ran LASR overnight (for 7 hours). BigBench necessitates optimizing a matrix of parameters rather than singular scalar constants. This shifts the compute bottleneck from generating a pool of candidates (in which LASR is slower than PySR) to evaluating a pool of candidates (which is equally slow for both algorithms). ", "page_idx": 21}, {"type": "text", "text": "Results: LASR discovers the following scaling law on the subset of BigBench: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathsf{s c o r e}=\\frac{-0.015540}{\\left(\\frac{\\mathrm{train}_{-}\\mathrm{steps}}{-127140.499}\\right)^{\\#\\mathrm{shots}}}+0.363219\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Where score is the MCQ grade, train_steps is the number of training steps for the model, and #shots is the number of in-context examples provided during inference. ", "page_idx": 21}, {"type": "text", "text": "Qualitative Evaluation: Equation 4 has some interesting properties. First, it describes an empirical relationship between training hyperparameters (training steps) and inference hyperparameters (number of shots). It asserts that increasing the number of shots exponentially increases the model\u2019s performance for low-resource models while having diminishing gains as the number of training steps of the model increase. Second, Equation 4 only requires three free parameters (Chinchilla [23] required five free parameters). This might allow LASR \u2019s equation to better generalize to different LLM operation regimes. However, the equation has some issues as well. If the number of shots is an even number, the exponentially increasing trend reflects over the value of A and turns into an exponentially decreasing trend. A manual inspection of the training data reveals the majority of samples had an odd number of shots (1, 3, ...) which causes the generalization error. ", "page_idx": 21}, {"type": "text", "text": "Quantitative Evaluation: We compare Equation 4, Chinchilla [23], Modified Chinchilla, and a single free parameter equation on their ability to explain the score of a held out validation set of 10,763 data points. We fit the free parameters of each equation to the training set data and measure the MSE loss between the actual grade and the predicted grade on the validation set. The results are presented in Table 4. Overall, we find that the Equation 4\u2019s performance, as well as modified Chinchilla\u2019s performance, is competitive with that of Chinchilla\u2019s in predicting the MCQ grade. ", "page_idx": 21}, {"type": "text", "text": "We hope our preliminary results provide further insight into the practical value of LASR in addressing real-world challenges, including those in machine learning ", "page_idx": 21}, {"type": "text", "text": "A.7 Metrics for Cascading experiment ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For the cascading experiment, we aim to evaluate the progression of different configuration towards solving equations. The quantitative metric used in the SRBench comparison experiment, Exact Solve, does not allow for such fine-grained analysis. Therefore, we categorize the synthesized equations into four buckets: Exact Solve, Almost Solve, Close, and Not Close. Exact Solve is quantitatively evaluated using a symbolic match. An equation is tagged as \u2019Almost Solve\u2019 if the dataset loss is small, but the generated equation has an extra term or lacks one term. A Close equation captures the general structure of the solution (such as a square root nested in an exponential) but not more than that, and Not Close includes all equations that are far from the solution. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "A.8 Further Qualitative Analysis ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "LASR generates two artifacts: the best fit program, and the library of natural language concept that helped find that program. These artifacts provide a unique window into the inner workings of LASR. This section goes over a qualitative study of how LASR and PySR go about discovering Coulomb\u2019s law $\\begin{array}{r}{F=\\frac{q_{1}\\bar{q_{2}}}{4\\pi r^{2}\\epsilon}}\\end{array}$ from data. Both methods are able to find an answer to this equation. However, their approach to finding the best fit equation as well as the form of the equation they discover differs significantly. ", "page_idx": 22}, {"type": "text", "text": "Setup: Coulomb\u2019s law is equation $\\#10$ in the Feynman equation dataset. It describes how the force between two point charges changes with respect to the distance between the charges, the magnitudes of the charges, and the permittivity of free space constant. The corresponding data for this equation has a target noise of 0.001 to simulate experimental errors. ", "page_idx": 22}, {"type": "text", "text": "By analyzing the form of the equation and relationships between variables in Coulomb\u2019s law, we can uncover several interesting properties: First, observe that this is an inverse square law (The force $F$ varies inversely with the square of the distance $r$ between the charged particles). Second, notice that the $F$ is directly proportional to the magnitude of the charges $q_{1}$ and $q_{2}$ . Third, observe that the resultant force is symmetric with respect to the magnitude of the charged particles (i.e.: The magnitude of the $F$ doesn\u2019t change if the magnitude of the charged particles is swapped). ", "page_idx": 22}, {"type": "text", "text": "PySR Solution: PySR finds the following solution to this equation: ", "page_idx": 22}, {"type": "equation", "text": "$$\nF=\\frac{\\left(\\left(\\left(\\left(\\left(\\left(\\frac{q_{2}\\cdot3.382}{r}\\right)-\\left(\\frac{\\sin\\left(\\frac{q_{3}\\cdot017}{\\exp(B)}\\right)}{\\exp(C)}\\right)\\right)/0.712\\right)\\cdot q_{1}\\right)\\cdot0.087\\right)/\\epsilon\\right)\\cdot0.191\\right)}{r}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This equation has a complexity of 26 and achieves a loss of $2.191505\\times10^{-12}$ on the dataset.   \nObtaining a simplification of this solution is rather painstaking. ", "page_idx": 22}, {"type": "text", "text": "LASR\u2019s Solution: LASR finds the following solution to this equation. We also present three steps of simplification: ", "page_idx": 22}, {"type": "text", "text": "(Substitute constant) ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{F}=\\frac{\\boldsymbol{q}_{1}}{\\left(\\frac{r}{q_{2}}\\right)\\,\\left(r+\\frac{1.9181636\\times10^{-5}}{q_{2}}\\right)\\,\\boldsymbol{\\epsilon}}\\cdot\\boldsymbol{0.07957782}}\\\\ &{\\quad=\\frac{\\boldsymbol{q}_{1}}{\\left(\\frac{r}{q_{2}}\\right)\\,\\left(r+\\frac{1.9181636\\times10^{-5}}{q_{2}}\\right)\\,\\boldsymbol{\\epsilon}}\\cdot\\frac{1}{4\\pi}}\\\\ &{\\quad=\\frac{\\boldsymbol{q}_{1}q_{2}}{r\\,\\left(r+\\frac{1.9181636\\times10^{-5}}{q_{2}}\\right)\\,\\boldsymbol{\\epsilon}}\\cdot\\frac{1}{4\\pi}}\\\\ &{\\quad\\approx\\frac{\\boldsymbol{q}_{1}q_{2}}{r\\,\\left(r\\right)\\,\\boldsymbol{\\epsilon}}\\cdot\\frac{1}{4\\pi}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "(Simplify denominator) ", "page_idx": 22}, {"type": "text", "text": "This equation has a complexity of 15 and achieves a much lower loss of $4.6709058\\times10^{-14}$ on the accompanying dataset. We can see with just three steps of simplification how this equation might be reduced to the ground truth. ", "page_idx": 22}, {"type": "text", "text": "Let\u2019s examine some essential concepts from various iterations in the search process. Keep in mind that an LLM operates on tokens in each concept. Consequently, even small relevant substrings can positively influence future LLM inference calls, despite full concepts appearing verbose to humans. ", "page_idx": 22}, {"type": "table", "img_path": "B7S4jJGlvl/tmp/876414db97fe8d0d2884e999d4f5c1ea61f3c91f72c504b86d748d772cfb6373.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 8: Seven equations discovered by LASR on the Feynman Equations dataset (over PYSR). The equations are presented in the form discovered by LASR, and usually reduce to the ground truth equations after some simplification steps. Note that there are minor discrepancies in the variable names between the ground truth equations in the online lectures (https://www.feynmanlectures. caltech.edu) and the equations in our Feynman equations dataset. ", "page_idx": 23}, {"type": "text", "text": "1. Iteration 2 The good mathematical expressions exhibit a clear and coherent relationship between the variables involved, with a focus on power functions and trigonometric functions that can be easily related to physical concepts.   \n2. Iteration 6 The good mathematical expressions exhibit a certain level of symmetry or regularity in their form, possibly reflecting underlying patterns or relationships between the variables and constants.   \n3. Iteration 24: The good mathematical expressions have a clear and consistent structure involving the variables q1, q2, epsilon, C, and r, with a specific pattern of division and multiplication. ", "page_idx": 23}, {"type": "text", "text": "A.9 Subset of equations discovered by LaSR ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 24}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 24}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 24}, {"type": "text", "text": "\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 24}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 24}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 24}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 24}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 24}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The abstract and introduction spell out the paper\u2019s contributions. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We discuss the limitations in the conclusion section. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: No theoretical results. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. ", "page_idx": 25}, {"type": "text", "text": "\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide a link to an open source implementation of LASR in Julia. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Same answer as above. We also include a README to start the codebase.   \nEventually, we\u2019d like to merge our contributions into a larger repository (PySR). ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 26}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We\u2019ve put our best efforts in ensuring the research work is comprehensive enough that all experiments can be replicated. Nevertheless, for any missing details, we refer the reader to our open sourced repository and are happy to make changes to enhance comprehensiveness. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Our main evaluation metric is exact match of a synthesized equation w.r.t a reference equation. This statistic doesn\u2019t permit a level of granularity at which we can show error bars. An important source of uncertainty is our backbone LLM. We\u2019ve painstaikingly engineered our experiments to use local LLMs at the cost of modest performance to enable reproducibility. However, certain experiments necessitate proprietary models whose internal details may change. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We mention our experimental setup details as well as other experimental setups we have successfully tried. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Yes. We\u2019ve made sure our submission follows with the NeurIPS code of ethics. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We talk about broader impacts in the appendix. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake proflies, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our contributions do not produce artifacts like trained neural network parameters that can be misused. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our codebase is built on top of many prior works in SR (PySR, PromptingTools.jk, and SymbolicRegression.jl). We\u2019ve attributed them appropriately in accordance with their respective licences. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We release all code and provide a README of how to instantiate the codebase. We hope to merge our tool into an existing state-of-the-art symbolic regression framework, which necessitates a comprehensible organization as well. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]