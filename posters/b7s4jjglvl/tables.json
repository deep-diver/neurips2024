[{"figure_path": "B7S4jJGlvl/tables/tables_4_1.jpg", "caption": "Table 1: Results on 100 Feynman equations from [49]. We report exact match solve rate for all models. LASR achieves the best exact match solve rate using the same hyperparameters as PySR.", "description": "This table presents the results of various symbolic regression models on a benchmark dataset of 100 Feynman equations.  The \"exact match solve rate\" indicates the percentage of equations each model correctly solved.  The table highlights that LASR outperforms other state-of-the-art methods (GPlearn, AFP, AFP-FE, DSR, UDSR, AIFeynman, and PySR) by achieving the highest exact match solve rate. Notably, LASR uses the same hyperparameters as PySR, demonstrating its improvement is not simply due to hyperparameter tuning.", "section": "4 Experiments"}, {"figure_path": "B7S4jJGlvl/tables/tables_5_1.jpg", "caption": "Table 1: Results on 100 Feynman equations from [49]. We report exact match solve rate for all models. LASR achieves the best exact match solve rate using the same hyperparameters as PySR.", "description": "This table presents the results of various symbolic regression algorithms on the Feynman equations dataset. The algorithms are compared based on their exact match solve rate, which represents the percentage of equations correctly solved. LASR outperforms all other algorithms in this benchmark.", "section": "4 Experiments"}, {"figure_path": "B7S4jJGlvl/tables/tables_5_2.jpg", "caption": "Table 2: Evaluation results on Feynman dataset by cascading LASR's LLM backbone (llama3-8b, gpt-3.5-turbo) and changing the probability of calling the model (p = [0.01, 0.05, 0.10]) in the order of increasing concept guidance. LASR outperforms PySR even with minimal concept guidance using an open-source LLM.", "description": "This table presents the results of an experiment evaluating the impact of different LLMs (Llama3-8B and GPT-3.5) and varying the probability (p) of using the LLM for concept guidance on the performance of LASR in solving Feynman equations.  The results are compared against the performance of PySR (without LLM guidance).  The table shows the number of equations solved in four categories: Exact Solve, Almost Solve, Close, and Not Close, illustrating that LASR achieves better performance than PySR across different LLM and probability settings.", "section": "4.1 Comparison against baselines in the Feynman Equation Dataset"}, {"figure_path": "B7S4jJGlvl/tables/tables_7_1.jpg", "caption": "Table 3: Evaluation results of data leakage. We present the test set R\u00b2 of PySR and of LASR on a synthetic symbolic regression dataset. Higher R\u00b2 is better.", "description": "This table presents the results of a data leakage experiment designed to assess the fairness of using LLMs in symbolic regression.  A synthetic dataset of 41 equations, engineered to be unlike those typically seen in the LLM's training data, was created. The R\u00b2 (R-squared) values, which measures the goodness of fit of a model, are compared for both PySR (a standard genetic programming algorithm) and LASR.  The significantly higher R\u00b2 for LASR suggests that LASR's performance is not due to memorization of equations from the LLM's training set, but rather that it can successfully leverage the LLM's knowledge to aid in generating new equations.", "section": "4.5 Data Leakage Validation"}, {"figure_path": "B7S4jJGlvl/tables/tables_8_1.jpg", "caption": "Table 4: Preliminary results on evaluating different LLM scaling laws. We measure MSE loss on a held out subset of BigBench [16]. The equation discovered with LASR performs as well as the Chinchilla equation [23] on BigBench while using less free parameters. The residual term skeleton equation (score = E) also performs well.", "description": "This table presents the Mean Squared Error (MSE) loss and the number of free parameters for four different LLM scaling law skeletons.  The first row shows the results for the scaling law discovered by LASR, demonstrating comparable performance to the established Chinchilla equation but with fewer parameters. The table also includes results for the original Chinchilla equation and a modified version, along with a simple baseline of only using a residual term.  This highlights LASR's ability to discover competitive scaling laws efficiently.", "section": "4.6 Using LASR to discover LLM Scaling Laws"}, {"figure_path": "B7S4jJGlvl/tables/tables_16_1.jpg", "caption": "Table 1: Results on 100 Feynman equations from [49]. We report exact match solve rate for all models. LASR achieves the best exact match solve rate using the same hyperparameters as PySR.", "description": "This table presents the results of applying various symbolic regression methods to a dataset of 100 Feynman equations, including the proposed LASR method and several state-of-the-art baselines (GPlearn, AFP, AFP-FE, DSR, UDSR, AIFeynman, PySR). The metric used is the exact match solve rate, which represents the percentage of equations correctly solved by each method.  LASR demonstrates the highest exact match solve rate, outperforming all other methods. Notably, it achieves this using the same hyperparameters as PySR, indicating its improved efficiency.", "section": "4 Experiments"}, {"figure_path": "B7S4jJGlvl/tables/tables_17_1.jpg", "caption": "Table 1: Results on 100 Feynman equations from [49]. We report exact match solve rate for all models. LASR achieves the best exact match solve rate using the same hyperparameters as PySR.", "description": "This table presents the results of applying various symbolic regression methods (GPlearn, AFP, AFP-FE, DSR, UDSR, AI Feynman, PySR, and LASR) to a dataset of 100 Feynman equations. The \"exact match solve rate\" metric is used, indicating the percentage of equations each algorithm correctly solved. LASR outperforms other methods, achieving the highest solve rate and matching PySR's hyperparameters for a fair comparison.", "section": "4 Experiments"}, {"figure_path": "B7S4jJGlvl/tables/tables_19_1.jpg", "caption": "Table 5: An asymmetric comparison of PySR and LASR on the Feynman equations dataset (A.5.1). We run PySR for 10 hours per equation (thresholded to 106 iterations) and compare the exact solve rate with that of LASR run for 40 iterations. We find that PySR is able to discover three more equations, but LASR still substantially outperforms PySR.", "description": "This table compares the performance of PySR and LASR on the Feynman Equations dataset using an asymmetric approach. PySR ran for 10 hours per equation (up to 106 iterations), while LASR ran for only 40 iterations.  The metric used is the exact solve rate, measuring the percentage of equations correctly discovered. Despite the significantly longer runtime, LASR achieved a higher solve rate than PySR, demonstrating its efficiency.", "section": "A.5 Additional Experiments"}, {"figure_path": "B7S4jJGlvl/tables/tables_20_1.jpg", "caption": "Table 6: Qualitative evaluation of LASR on the synthetic equations dataset (A.5.2). We attempt to recover the ground truth equation from a slightly noisy dataset generated from the ground truth equations. None of the algorithms we tested (including LASR) are able to recover the ground truth equations, underscoring the challenge of exact symbolic match on this dataset. A study on the R\u00b2 performance is presented in Table 3.", "description": "This table presents a qualitative comparison of the ground truth equations and the equations predicted by LASR on a synthetic dataset.  The dataset consists of equations with added noise to simulate real-world experimental error.  The results show that while neither LASR nor other tested algorithms perfectly recovered the ground truth equations, they produced equations that captured some aspects of the ground truth. A quantitative comparison based on the R-squared metric is provided in Table 3.", "section": "4.2 Cascading Experiments"}, {"figure_path": "B7S4jJGlvl/tables/tables_20_2.jpg", "caption": "Table 7: Performance of LASR on successive runs with the same hyperparameters on Equation 53 in the Feynman equations dataset (A.5.3). This equation defines the heat flow from a point source (an inverse square law with ground truth formulation: h = 4\u03c0R2). We evaluate LASR by running it twice with identical hyperparameters but different random seeds to assess whether the model has memorized the equation's form. In both runs, LASR consistently discovers high-performing solutions. Moreover, the resulting functional forms show significant variation between runs, supporting the hypothesis that LASR's success is not rooted in memorization.", "description": "This table presents the results of two separate runs of LASR on the same equation using identical hyperparameters but different random seeds. This experiment aims to verify that LASR's success is not due to memorization of the equation. The results show that while both runs yield high-performing solutions, the resulting functional forms are quite different, supporting the claim that the model's success is based on reasoning and not memorization.", "section": "A.5 Additional Experiments"}, {"figure_path": "B7S4jJGlvl/tables/tables_23_1.jpg", "caption": "Table 8: Seven equations discovered by LASR on the Feynman Equations dataset (over PySR). The equations are presented in the form discovered by LASR, and usually reduce to the ground truth equations after some simplification steps. Note that there are minor discrepancies in the variable names between the ground truth equations in the online lectures (https://www.feynmanlectures.caltech.edu) and the equations in our Feynman equations dataset.", "description": "This table showcases seven equations that LASR successfully discovered, comparing them to their ground truth counterparts from the Feynman Lectures.  LASR's discovered equations often require simplification to match the ground truth, highlighting the algorithm's ability to find functional forms that closely approximate the target solutions. Note that minor discrepancies exist in variable names between the online lectures and the dataset used in this paper.", "section": "A.9 Subset of equations discovered by LaSR"}]