[{"figure_path": "1ELFGSNBGC/figures/figures_1_1.jpg", "caption": "Figure 1: Multiview Scene Graph (MSG). The task of MSG takes unposed RGB images as input and outputs a place+object graph. The graph contains place-place edges and place-object edges. Connected place nodes represent images taken at the same place. The same object recognized from different views is associated and merged as one node and connected to the corresponding place nodes.", "description": "This figure illustrates the concept of Multiview Scene Graph (MSG). The input is a set of unposed RGB images from the same scene. The output is a graph where nodes represent places (images taken from the same location) and objects. Edges connect place nodes if they represent images taken from the same place and connect places to the objects observed in those places. The task is challenging as it involves place recognition, object detection, and association.", "section": "1 Introduction"}, {"figure_path": "1ELFGSNBGC/figures/figures_4_1.jpg", "caption": "Figure 2: The AoMSG model. Places and objects queries are obtained by cropping the image feature map using corresponding bounding boxes. The queries are then fed into the Transformer decoder to obtain the final places and objects embeddings. Bounding boxes are in different colors for clarity. The parameters in the Transformer decoder and the linear projector heads are trained with supervised contrastive learning. Image encoder and object detector are pretrained and frozen.", "description": "This figure illustrates the architecture of the Attention Association Multiview Scene Graph (AoMSG) model.  The model takes unposed RGB images as input. It uses a pretrained image encoder (like DINOv2) and object detector to extract image features and object bounding boxes. These bounding boxes are used to crop the feature maps, generating place and object queries. These queries are fed into a Transformer decoder.  The decoder jointly learns place and object embeddings, which are then projected through linear heads for final embeddings used in the MSG generation.  The pre-trained image encoder and object detector parts of the model are frozen during training; only the Transformer decoder and projection heads are trained using a supervised contrastive loss function.", "section": "4 Our Baseline: Attention Association MSG Generation"}, {"figure_path": "1ELFGSNBGC/figures/figures_6_1.jpg", "caption": "Figure 3: Performance of different encoder backbones. We report results from the base models for both ConvNext [40] and ViT [21].", "description": "This figure shows the performance comparison of different encoder backbones, specifically ConvNeXt and Vision Transformer (ViT), on the task of Multiview Scene Graph (MSG) generation.  The x-axis represents different backbones (ConvNeXt, ViT, and different sizes of DINOv2), while the y-axis shows the IoU (Intersection over Union) scores for both place-place edges (PP IoU) and place-object edges (PO IoU). The results are split into direct (without the proposed AoMSG model) and AoMSG (with the proposed AoMSG model). This visualization helps to understand the impact of the choice of backbone on the overall performance of the MSG generation task, highlighting the effectiveness of the DINOv2 model.", "section": "5.2 Baselines"}, {"figure_path": "1ELFGSNBGC/figures/figures_6_2.jpg", "caption": "Figure 1: Multiview Scene Graph (MSG). The task of MSG takes unposed RGB images as input and outputs a place+object graph. The graph contains place-place edges and place-object edges. Connected place nodes represent images taken at the same place. The same object recognized from different views is associated and merged as one node and connected to the corresponding place nodes.", "description": "This figure illustrates the task of Multiview Scene Graph (MSG) generation.  The input is a set of unposed RGB images from a single scene. The output is a graph where nodes represent places (images taken from similar viewpoints) and objects. Edges connect places to each other (if they are visually similar) and connect places to the objects observed in those places.  The key is that the same object detected in multiple images is represented as a single node connected to all the places where it's seen. This topological representation aims to capture spatial relationships without explicit metric information (like distances or poses).", "section": "3 Multiview scene graph"}, {"figure_path": "1ELFGSNBGC/figures/figures_8_1.jpg", "caption": "Figure 5: Object embedding visualization using t-SNE [61]. SepMSG-Direct, SepMSG-Linear, and AoMSG-2 are shown in each row respectively. Results from the same scene are aligned vertically. Colors indicate different objects. Each point is an appearance of an object. It is best viewed in color.", "description": "This figure visualizes the learned object embeddings using t-SNE, a dimensionality reduction technique. It compares three different models: SepMSG-Direct, SepMSG-Linear, and AoMSG-2. Each row represents a model, and each column represents a different scene. Points of the same color represent the same object, showing how well the models cluster appearances of the same object together.  The visualization helps assess how effectively each model separates different objects in the embedding space, which is crucial for object association.", "section": "5.4 Results"}, {"figure_path": "1ELFGSNBGC/figures/figures_8_2.jpg", "caption": "Figure 6: Local 3D reconstruction from 2D MSG using off-the-shelf model Dust3r [63]. The 3D meshes of two scenes are shown side by side, with 3 subgraphs circled in gray and reconstructed on the top of each scene.", "description": "This figure showcases a real-world application of the proposed MSG method. It demonstrates how MSG can be combined with an off-the-shelf 3D reconstruction model (Dust3r) to create local 3D reconstructions of scenes. The left and right columns display two different scenes, each containing a 3D reconstruction, topological map generated from the MSG, and a 3D mesh. The MSG identifies subgraphs, and these subgraphs are used by Dust3r to create the 3D reconstructions. This approach is particularly useful when dealing with large-scale datasets since it breaks down the task into smaller, manageable subproblems.  The figure highlights the complementary nature of MSG and 3D reconstruction models. MSG provides a topological map of the scene, which guides the 3D reconstruction process to produce more accurate and efficient local models.", "section": "6 Discussion"}, {"figure_path": "1ELFGSNBGC/figures/figures_16_1.jpg", "caption": "Figure 7: Relative pose distribution in histograms on the test set. Blue is for the connected and red is for the not connected. The green dashed lines are the spatial thresholds.", "description": "This figure visualizes the relative pose distributions (orientation and translation) for connected and non-connected nodes in the MSG graph on a test set.  The histograms show the frequency of various orientation and translation differences between node pairs. The blue bars represent connected nodes (i.e., images taken at the same place in the scene), and red bars show non-connected nodes.  The green dashed lines indicate the thresholds used to determine if two images are considered to be at the same place; pairs with pose differences exceeding these thresholds are classified as non-connected. The distributions clearly show separation between connected and non-connected nodes, illustrating the model's ability to distinguish spatial proximity accurately.", "section": "C Additional Analysis"}, {"figure_path": "1ELFGSNBGC/figures/figures_16_2.jpg", "caption": "Figure 10: Visualization for the place nodes. Every 3 images shown side by side are those connected in the MSG, meaning they are considered from the same place.", "description": "This figure visualizes the \"place nodes\" from the generated Multiview Scene Graph (MSG).  It demonstrates how the model groups images taken at the same physical location.  Sets of three images are shown together; these images are connected as nodes in the MSG because the model infers that they depict the same place in the scene, despite potential differences in viewpoint or lighting.", "section": "5.1 Data"}, {"figure_path": "1ELFGSNBGC/figures/figures_18_1.jpg", "caption": "Figure 9: Qualitative real-world experiment. Top: results visualization. \u201cpx", "description": "This figure shows a qualitative real-world experiment using the proposed AoMSG model. The top part displays example images from a real-world video, with object instances labeled using their predicted IDs. The bottom part presents an interactive graph visualization of the scene, illustrating the connections between place nodes (images) and object nodes, showcasing the model's ability to create a scene graph from a real-world video.", "section": "6 Discussion"}, {"figure_path": "1ELFGSNBGC/figures/figures_19_1.jpg", "caption": "Figure 10: Visualization for the place nodes. Every 3 images shown side by side are those connected in the MSG, meaning they are considered from the same place.", "description": "This figure visualizes the \"place\" nodes in the Multiview Scene Graph (MSG).  It groups sets of three images together. The images within each group are visually similar and were determined by the model to have been taken at the same location in the scene. This helps demonstrate the model's ability to recognize and cluster images captured from the same viewpoint, despite variations in camera angle or lighting.", "section": "5.1 Data"}, {"figure_path": "1ELFGSNBGC/figures/figures_20_1.jpg", "caption": "Figure 1: Multiview Scene Graph (MSG). The task of MSG takes unposed RGB images as input and outputs a place+object graph. The graph contains place-place edges and place-object edges. Connected place nodes represent images taken at the same place. The same object recognized from different views is associated and merged as one node and connected to the corresponding place nodes.", "description": "This figure illustrates the Multiview Scene Graph (MSG) task.  Given a set of unordered RGB images from a single scene, the goal is to generate a graph representing the scene's spatial structure.  The graph consists of two types of nodes: place nodes (representing locations in the scene, each corresponding to an image) and object nodes (representing objects, with the same object appearing in multiple images merged into a single node).  Edges connect place nodes that are spatially close to each other and connect place nodes to the object nodes they contain. This graph explicitly represents the spatial relationships between places and objects.", "section": "3 Multiview scene graph"}]