{"references": [{"fullname_first_author": "Z. Allen-Zhu", "paper_title": "Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives", "publication_date": "2016-00-00", "reason": "This paper is foundational for the use of SVRG-type variance reduction in the context of adaptive gradient methods."}, {"fullname_first_author": "A. Attia", "paper_title": "SGD with AdaGrad Stepsizes: Full Adaptivity with High Probability to Unknown Parameters, Unbounded Gradients and Affine Variance", "publication_date": "2023-00-00", "reason": "This paper is highly relevant to the theoretical analysis of AdaGrad stepsizes in stochastic optimization, providing state-of-the-art results."}, {"fullname_first_author": "K. Y. Levy", "paper_title": "Online Adaptive Methods, Universality and Acceleration", "publication_date": "2018-00-00", "reason": "This paper provides important theoretical insights into adaptive methods, their universality property, and shows the acceleration techniques."}, {"fullname_first_author": "O. Devolder", "paper_title": "First-order methods of smooth convex optimization with inexact oracle", "publication_date": "2013-00-00", "reason": "This paper establishes a key theoretical framework for dealing with inexact oracles in optimization, which is critical for analyzing the stochastic gradient methods."}, {"fullname_first_author": "A. Rodomanov", "paper_title": "Universal Gradient Methods for Stochastic Convex Optimization", "publication_date": "2024-00-00", "reason": "This paper presents a unified analysis of AdaGrad stepsizes for various settings of stochastic optimization, and it is directly relevant to the work presented in the current paper."}]}