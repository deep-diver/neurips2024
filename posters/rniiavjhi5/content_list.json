[{"type": "text", "text": "Universality of AdaGrad Stepsizes for Stochastic Optimization: Inexact Oracle, Acceleration and Variance Reduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anton Rodomanov Xiaowen Jiang Sebastian Stich CISPA\u2217 Saarland University and CISPA\u2217 CISPA\u2217 anton.rodomanov@cispa.de xiaowen.jiang@cispa.de stich@cispa.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We present adaptive gradient methods (both basic and accelerated) for solving convex composite optimization problems in which the main part is approximately smooth (a.k.a. $(\\delta,L\\bar{)}$ -smooth) and can be accessed only via a (potentially biased) stochastic gradient oracle. This setting covers many interesting examples including H\u00f6lder smooth problems and various inexact computations of the stochastic gradient. Our methods use AdaGrad stepsizes and are adaptive in the sense that they do not require knowing any problem-dependent constants except an estimate of the diameter of the feasible set but nevertheless achieve the best possible convergence rates as if they knew the corresponding constants. We demonstrate that AdaGrad stepsizes work in a variety of situations by proving, in a unified manner, three types of new results. First, we establish efficiency guarantees for our methods in the classical setting where the oracle\u2019s variance is uniformly bounded. We then show that, under more refined assumptions on the variance, the same methods without any modifications enjoy implicit variance reduction properties allowing us to express their complexity estimates in terms of the variance only at the minimizer. Finally, we show how to incorporate explicit SVRG-type variance reduction into our methods and obtain even faster algorithms. In all three cases, we present both basic and accelerated algorithms achieving state-of-the-art complexity bounds. As a direct corollary of our results, we obtain universal stochastic gradient methods for H\u00f6lder smooth problems which can be used in all situations. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Motivation. Gradient methods are among the most popular and efficient optimization algorithms for solving machine learning problems. To achieve the best convergence speed for these algorithms, their stepsizes needs to be chosen properly. While there exist various theoretical recommendations, dictated by the convergence analysis, on how to select stepsizes based on various problem-dependent parameters, they are usually impractical because the corresponding constants may be unknown or their worst-case estimates might be too pessimistic. Furthermore, every applied problem usually belongs to multiple problem classes at the same time, and it is not always evident in advance which of them better suits the concrete problem instance one works with. For classical optimization algorithms, this problem is typically resolved by using a line search. This is a simple yet powerful mechanism which automatically chooses the best stepsize by checking at each iteration a certain condition involving the objective value, its gradient, etc. ", "page_idx": 0}, {"type": "text", "text": "However, the line-search approach is usually unsuitable for problems of stochastic optimization, where gradients are observed with random noise (unless some extra assumptions are made, see [57]). For these problems, it is common instead to apply so-called adaptive methods which set up their stepsizes by simply accumulating on-the-fly certain information about observed stochastic gradients. The first such an algorithm, AdaGrad [17, 39], was obtained from theoretical considerations but quickly inspired several other heuristic methods like RMSProp [56] and Adam [32] that are now at the forefront of training machine learning models. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Excellent practical performance of adaptive methods on various applied problems naturally sparked a lot of theoretical interest in these algorithms. An important observation was done by Levy, Yurtsever, and Cevher [34] who showed that AdaGrad possesses a certain universality property, in the sense that it works for several problem classes simultaneously. Specifically, they showed that AdaGrad converges both for nonsmooth problems with bounded gradient and also for smooth problems with Lipschitz gradient, without needing to know neither the corresponding Lipschitz constants, nor the oracle\u2019s variance but enjoying the rates which are characteristic for algorithms which have the knowledge of these constants. They also presented an accelerated version of AdaGrad with similar properties. An independent version of the accelerated AdaGrad including diagonal scaling was proposed by Deng, Cheng, and Lan [12]. Further improvements and generalization of these ideas were considered in [18, 28, 30]. ", "page_idx": 1}, {"type": "text", "text": "Nonsmooth and smooth problems are the extremes of the more general H\u00f6lder class of problems. The fact that AdaGrad methods simultaneously work for these two extreme cases does not seem to be a coincidence and suggests that these algorithms should work more generally for any problem with intermediate level of smoothness. Some further confirmations to this were recently provided in [48] although in a rather restricted setting of deterministic problems and only for the basic AdaGrad method. The stochastic case and acceleration were constituting an open problem which was recently resolved in [49] for a slightly modified AdaGrad stepsize (see (4)). ", "page_idx": 1}, {"type": "text", "text": "All the previously discussed results were proved only for the classical stochastic optimization setting where the variance of stochastic gradients is assumed to be uniformly bounded. In a recent work, Attia and Koren [2] showed that the basic AdaGrad method for smooth problems works under the more general assumption when the variance is bounded by a constant plus a multiple of the squared gradient norm. On a related note, it was also shown recently that AdaGrad stepsizes can be used inside gradient methods with SVRG-type variance-reduction. The first such an algorithm was proposed in [16]. The accelerated SVRG method enjoying optimal worst-case oracle complexity for smooth finite-sum optimization problems was later presented in [36]. ", "page_idx": 1}, {"type": "text", "text": "Contributions. In this work, we further extend the results mentioned above by demonstrating that AdaGrad stepsizes are even more universal than was shown previously in the literature. Specifically, we consider the composite optimization problem where the main part is approximately smooth (a.k.a. $(\\delta,L)$ -smooth) and can be accessed only via a (potentially biased) stochastic gradient oracle. This setting is more general than typically considered in the literature on adaptive methods and covers many interesting examples, including smooth, nonsmooth and, more generally, H\u00f6lder smooth problems, problems in which the objective function is given itself as another optimization problem whose solution can be computed only approximately, etc. ", "page_idx": 1}, {"type": "text", "text": "Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We start, in Section 3, with identifying the key property of AdaGrad stepsizes, which allows us to apply these stepsizes, in a unified manner, in a variety of situations we consider later. We present our two mains algorithms, UniSgd and UniFastSgd which are the classical stochastic gradient method (SGD) and its accelerated version, respectively, equipped with AdaGrad stepsizes.   \n2. We then establish, in Section 4, efficiency guarantees for these methods in the classical setting where the oracle\u2019s variance is assumed to be uniformly bounded.   \n3. In Section 5, we complement these results by showing that, under additional assumptions that the variance is itself approximately smooth w.r.t. the objective function, the same UniSgd and UniFastSgd without any modifications enjoy implicit variance reduction properties allowing us to express their complexity estimates in terms of the variance only at the minimizer.   \n4. Under the additional assumption that one can periodically compute the full (inexact) gradient of the objective function, we show, in Section 6, how to incorporate explicit SVRG-type variance reduction into our methods, obtaining new UniSvrg and UniFastSvrg algorithms which enjoy even faster convergence rates by completely eliminating the variance. ", "page_idx": 1}, {"type": "text", "text": "Our results are summarized in Table 1 (in the BigO-notation). In all the situations, we present both basic and accelerated algorithms whose only essential parameter is an estimate $D$ of the diameter of the feasible set; the methods automatically adapt to all other problem-dependent constants. In a number of special cases, our algorithms achieve known state-of-the-art complexity bounds, but not restricted to those special cases. In Section 7, we illustrate the significance of our results by demonstrating that complexities for our methods on stochastic optimization problems with H\u00f6lder smooth components can be obtained as simple corollaries from our main results. ", "page_idx": 1}, {"type": "table", "img_path": "rniiAVjHi5/tmp/d70d534e1a21717dd9b927c9526e946ab2d7ce53d43c22e31ab5bf265555bbf0.jpg", "table_caption": ["Table 1: Summary of main results for solving problem (1) with our methods. \u201cConvergence rate\u201d is expressed in terms of the expected function residual at iteration $k$ (or $t$ , depending on the method). \u201cSO complexity\u201d denotes the cumulative stochastic-oracle complexity of the method since its start and up to iteration $k$ (or $t)$ ), which is defined as the number of queries to the stochastic oracle $\\widehat g$ ; for SVRG methods, we assume that querying the (inexact) full-gradient oracle $\\overline{{g}}$ is $n$ times more expensive than $\\widehat g$ , and define the SO complexity as $N_{\\hat{g}}+n N_{\\bar{g}}$ , where $N_{\\widehat{g}}$ and $N_{\\bar{g}}$ are the number of queries to $\\widehat g$ and $\\overline{{g}}$ , respectively. The second and third columns sphould bse understoopd in tersms of the BigO-notation which we omit for brevity. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. We work in the space $\\mathbb{R}^{d}$ equipped with the standard inner product $\\langle\\cdot,\\cdot\\rangle$ and a certain Euclidean norm: $\\|x\\|:=\\langle B x,x\\rangle^{1/2}$ , where $B$ is a fixed positive definite matrix. The dual norm is defined in the standard way: $\\|s\\|_{*}:=\\operatorname*{max}_{\\|x\\|=1}\\langle s,x\\rangle=\\langle s,B^{-1}s\\rangle^{1/2}$ . ", "page_idx": 2}, {"type": "text", "text": "For a convex function $\\psi\\colon\\mathbb{R}^{d}\\rightarrow\\mathbb{R}\\cup\\{+\\infty\\}$ , its (effective) domain is the following set: $\\operatorname{dom}\\psi:=$ $\\{x\\in\\mathbb{R}^{d}:\\psi(x)<+\\infty\\}$ . By $\\partial\\psi(x)$ , we denote the subdifferential of $\\psi$ at a point $x\\in\\operatorname{dom}\\psi$ ; the specific subgradients are typically denoted by $\\nabla\\psi({\\boldsymbol{x}})$ . ", "page_idx": 2}, {"type": "text", "text": "A convex function $f\\colon\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}$ is called $(\\nu,H)$ -H\u00f6lder smooth for some $\\nu\\in[0,1]$ and $H\\ge0$ iff $\\|\\nabla f(x)-\\nabla f(y)\\|_{*}\\,\\leq\\,H\\|x-y\\|^{\\nu}$ for all $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{d}$ and all $\\nabla f(x)\\,\\in\\,\\partial f(x),\\,\\nabla f(y)\\,\\in\\,\\partial f(y)$ . Apart from the special case of $\\nu=0$ , such a function $f$ is differentiable at every point, i.e., $\\partial f(x)$ is a singleton. A $(1,L)$ -H\u00f6lder smooth function is usually called $L$ -smooth. ", "page_idx": 2}, {"type": "text", "text": "For a convex function $\\psi\\colon\\mathbb{R}^{d}\\to\\mathbb{R}\\cup\\{+\\infty\\}$ , point $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , vector $\\boldsymbol{g}\\in\\mathbb{R}^{d}$ , and coefficient $M\\geq0$ , by $\\begin{array}{r}{\\operatorname{Prox}_{\\psi}(x,g,M):=\\operatorname{argmin}_{y\\in\\operatorname{dom}\\psi}\\{\\langle g,y\\rangle+\\psi(y)+\\frac{M}{2}\\|y-x\\|^{2}\\}}\\end{array}$ , we denote the proximal mapping. When $M=0$ , we allow the solution to be chosen arbitrarily. ", "page_idx": 2}, {"type": "text", "text": "For a convex function $f\\colon\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ , points $x,y\\in\\mathbb{R}^{d}$ and $\\nabla f(x)\\in\\partial f(x)$ , we denote the Bregman distance by $\\beta_{f}^{\\nabla f(x)}(x,y):=f(y)-f(x)-\\langle\\nabla f(x),y-x\\rangle\\,(\\ge0).$ . When the specific subgradient $\\nabla f(x)$ is clear from the context, we use the simplified notation $\\beta_{f}(x,y)$ . ", "page_idx": 2}, {"type": "text", "text": "The positive part of $t\\in\\mathbb R$ is $[t]_{+}:=\\operatorname*{max}\\{t,0\\}$ . For $\\tau>0$ , we also use $\\log_{+}\\tau:=\\operatorname*{max}\\{1,\\log\\tau\\}$ . ", "page_idx": 2}, {"type": "text", "text": "Problem Formulation. In this paper, we consider the composite optimization problem ", "page_idx": 2}, {"type": "equation", "text": "$$\nF^{*}:=\\operatorname*{min}_{x\\in\\operatorname{dom}\\psi}\\big[F(x):=f(x)+\\psi(x)\\big],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is a convex function, and $\\psi\\colon\\mathbb{R}^{d}\\rightarrow\\mathbb{R}\\cup\\{+\\infty\\}$ is a proper closed convex function which is assumed to be sufficiently simple in the sense that the proximal mapping $\\mathrm{Prox}_{\\psi}$ can be easily computed. We assume that this problem has a solution which we denote by $x^{*}$ . ", "page_idx": 2}, {"type": "text", "text": "To quantify the smoothness level of the objective function, we use the following assumption: ", "page_idx": 2}, {"type": "text", "text": "$\\overline{{\\mathbf{Algorithm\\1}\\left(\\bar{x}_{N},x_{N},M_{N}\\right)}}\\cong\\mathrm{UniSgd}_{\\hat{g},\\psi}(x_{0},M_{0},N;D)$ ", "page_idx": 3}, {"type": "text", "text": "Input: Oracle $\\widehat g$ , comp. part $\\psi$ , point $x_{0}\\in\\operatorname{dom}\\psi$ , coefficient $M_{0}$ , iteration limit $N$ , diameter $D$ .   \n1: $g_{0}\\cong{\\widehat{g}}(x_{0})$ .   \n2: for $k=0,\\ldots,N-1$ do   \n3: $x_{k+1}=\\operatorname*{Prox}_{\\psi}(x_{k},g_{k},M_{k}),\\ g_{k+1}\\cong{\\widehat{g}}(x_{k+1})$ .   \n4: $M_{k+1}=M_{+}(M_{k},D^{2},x_{k},x_{k+1},g_{k},g_{k+1})$ $\\begin{array}{r}{\\triangleright\\ e.g.,\\frac{(3)}{}\\overset{(3)}{=}\\sqrt{M_{k}^{2}+\\frac{1}{D^{2}}\\|g_{k+1}-g_{k}\\|_{*}^{2}}}\\end{array}$ .   \n5: return $(\\bar{x}_{N},x_{N},M_{N})$ , where $\\begin{array}{r}{\\overline{{x}}_{N}:=\\frac{1}{N}\\sum_{i=1}^{N}x_{i}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. The function $f$ in problem (1) is approximately smooth: there exist constants $L_{f},\\delta_{f}\\,\\ge\\,0$ and $\\bar{f}\\colon\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ , $\\dot{\\overline{{g}}}\\colon\\mathbb R^{\\bar{d}}\\to\\mathbb R^{d}$ such that, for any $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{d}$ , $\\beta_{f,\\bar{f},\\bar{g}}(x,y):=f(y)\\;-$ $\\bar{f}(x)-\\langle\\bar{g}(x),y-x\\rangle$ satisfies  tshe following inequality: $\\begin{array}{r}{0\\le\\beta_{f,\\bar{f},\\bar{g}}(x,y)\\le\\frac{L_{f}}{2}\\|x-y\\|^{2}+\\delta_{f}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumptison 1 is well-known in the literature under the name $(\\delta,L)$ -oracle and was originally introduced in [15]. It covers many interesting examples. For instance, if $f$ is $L$ -smooth, then Assumption 1 is satisfied with $\\bar{f}=\\bar{f},\\bar{g}=\\nabla f,\\check{\\delta_{f}}=0$ and $L_{f}=L$ . More generally, if the function $f$ is $(\\nu,H_{f}(\\nu))$ -H\u00f6lder smooth,  tshen As ssumption 1 is satisfied with $\\bar{f}=f,\\bar{g}=\\nabla f$ (arbitrary selection of subgradients), any $\\delta_{f}>0$ and $\\begin{array}{r}{L_{f}:=\\big[\\frac{1-\\nu}{2(1+\\nu)\\delta_{f}}\\big]^{\\frac{1-\\nu}{1+\\nu}}[H_{f}(\\nu)]^{\\frac{2}{1+\\nu}}}\\end{array}$ (se es Theorem 13). If $f$ can be uniformly approximated by an $L$ -smooth function $\\phi$ , i.e., $\\phi(x)\\leq f(x)\\leq\\phi(x)\\!+\\!\\delta$ , then Assumption 1 is satisfied with ${\\bar{f}}=\\phi$ , $\\overline{{g}}=\\nabla\\phi$ and $\\delta_{f}=\\delta$ . If $f$ represents another auxiliary optimization problem with a strongly c osncave objective, e.g., $\\bar{f}(x)=\\operatorname*{max}_{u}\\Psi(x,u)$ , whose solution $\\bar{u}(x)$ can only be found with accuracy $\\delta$ , then $f$ satisfies Assumption 1 with $\\bar{f}(x)=\\Psi(x,\\bar{u}(x))$ , $\\boldsymbol{\\overline{{g}}}(\\boldsymbol{x})=\\nabla_{\\boldsymbol{u}}\\Psi(\\boldsymbol{x},\\boldsymbol{\\overline{{u}}}(\\boldsymbol{x}))$ and $\\delta_{f}=\\delta$ . For more details and other interesting exampsles, we refer the reader to [15]. ", "page_idx": 3}, {"type": "text", "text": "In what follows, we assume that we have access to an unbiased stochastic oracle $\\widehat g$ for $\\overline{{g}}$ . Formally, this is a pair $\\widehat{\\boldsymbol{g}}\\,=\\,\\left(\\boldsymbol{g},\\boldsymbol{\\xi}\\right)$ consisting of a random variable $\\xi$ and a mapping $g\\colon\\ensuremath{\\mathbb{R}}^{d}\\times\\ensuremath{\\mathrm{Im}}\\,\\xi\\,\\to\\,\\ensuremath{\\mathbb{R}}^{d}$ (with $\\operatorname{Im}\\xi$ being the image of $\\xi$ ). When queried at a point $x$ , the oracle automatically generates an independent copy $\\xi$ of its randomness and then returns $\\widehat{g}_{x}=g(x,\\xi)$ (notation: ${\\widehat{g}}_{x}\\cong{\\widehat{\\widehat{g}}}{\\bar{(x)}}$ ). We call $g$ and $\\xi$ the function component and the random variable component of $\\widehat g$ , respectively. At this point, we only assume that our stochastic oracle $\\widehat g$ is un unbiased estimator of $\\overline{{g}}$ , and later make various assumptions on its variance. ", "page_idx": 3}, {"type": "text", "text": "Another important assumption on problem (1), that we need in our analysis, is the boundedness of the feasible set $\\operatorname{dom}\\psi$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. There exists $D>0$ such that $\\|x-y\\|\\leq D$ for any $x,y\\in\\operatorname{dom}\\psi$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 is rather standard in the literature on adaptive methods for stochastic convex optimization (see [16, 18, 30, 34, 36, 49]) and can always be ensured with $D\\:=\\:2R_{0}$ whenever one has the knowledge of an upper bound $R_{0}$ on the distance from the initial point $x_{0}$ to the solution $x^{*}$ . To that end, it suffices to rewrite the problem (1) in the following equivalent form: $\\mathrm{min}_{x\\in\\mathrm{dom}\\,\\psi_{D}}[f(x)+\\psi_{D}(x)]$ , where $\\psi_{D}$ is the sum of $\\psi$ and the indicator function of the ball $B_{0}:=\\{x\\in\\mathbb{R}^{d}:\\|x-x_{0}\\|\\leq R_{0}\\}$ . Note that this transformation keeps the function $\\psi_{D}$ reasonably simple as its proximal mapping can be computed via that of $\\psi$ by solving a certain one-dimensional nonlinear equation, which can be done very efficiently by Newton\u2019s method (at no extra queries to the stochastic oracle); in some special cases, the corresponding nonlinear equation can even be solved analytically, e.g., when $\\psi=0$ , the proximal mapping of $\\psi_{D}$ is simply the projection on $B_{0}$ . ", "page_idx": 3}, {"type": "text", "text": "Throughout this paper, we refer to $D$ from Assumption 2 as the diameter of the feasible set, and assume that its value is known to us. This will be the only essential parameter in our methods. ", "page_idx": 3}, {"type": "text", "text": "3 Main Algorithms and Stepsize Update Rules ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now present our two main algorithms for solving problem (1): UniSgd (Algorithm 1), and its accelerated version, UniFastSgd (Algorithm 2). Except the specific choice of the stepsize coefficients $M_{k}$ , both algorithms are rather standard: the first one is the classical SGD method, and the second one is the classical accelerated gradient method for stochastic optimization [33], also known as the Method of Similar Triangles (see, e.g., Section 6.1.3 in [46]). ", "page_idx": 3}, {"type": "text", "text": "Both methods are expressed in terms of a certain abstract stepsize update rule $M_{+}(\\cdot)$ defined as follows. Given the current stepsize coefficient $M\\geq0$ , constant $\\Omega>0$ (the scaled squared diameter), ", "page_idx": 3}, {"type": "text", "text": "$\\overline{{\\mathbf{Algorithm\\2\\UniFastSgd}_{\\hat{g},\\psi}(x_{0};D)}}$ ", "page_idx": 4}, {"type": "text", "text": "Input: Stochastic oracle $\\widehat g$ p, composite part $\\psi$ , point $x_{0}\\in\\operatorname{dom}\\psi$ , diameter $D$ .   \n1: $v_{0}=x_{0}$ , $M_{0}=A_{0}=0$ .   \n2: for $k=0,1,\\ldots$ do   \n3: $\\begin{array}{r l r l}&{a_{k+1}=\\frac{1}{2}(k+1),A_{k+1}=A_{k}+a_{k+1}.}\\\\ &{y_{k}=\\frac{A_{k}}{A_{k+1}}x_{k}+\\frac{a_{k+1}}{A_{k+1}}v_{k},\\ g_{y_{k}}\\cong\\widehat{g}(y_{k}).}\\\\ &{v_{k+1}=\\operatorname{Prox}_{\\psi}(v_{k},g_{y_{k}}),\\frac{M_{k}}{a_{k+1}}).}\\\\ &{x_{k+1}=\\frac{A_{k}}{A_{k+1}}x_{k}+\\frac{a_{k+1}}{A_{k+1}}v_{k+1},\\ g_{x_{k+1}}\\cong\\widehat{g}(x_{k+1}).}\\\\ &{M_{k+1}=\\frac{a_{k+1}^{2}}{A_{k+1}}M_{+}\\big(\\frac{A_{k+1}}{a_{k+1}^{2}}M_{k},\\frac{a_{k+1}^{2}}{A_{k+1}^{2}}D^{2},y_{k},x_{k+1},g_{y_{k}},g_{x_{k+1}}\\big)}&&{\\mathtt{p e}.\\underline{{g}},\\underline{{\\cdot}}\\overline{{\\cdot}}\\overline{{\\times}}\\overline{{M_{k}^{2}+\\frac{a_{k+1}^{2}}{D^{2}}\\|g_{x_{k+1}}-g_{y_{k}}\\|^{2}}}.}\\end{array}$   \n4:   \n5:   \n6:   \n7: ", "page_idx": 4}, {"type": "text", "text": "current point $x\\in\\operatorname{dom}\\psi$ with the stochastic gradient ${\\widehat{g}}_{x}\\cong{\\widehat{g}}(x)$ , next iterate $\\hat{x}_{+}=x_{+}(\\hat{g}_{x})\\in\\mathrm{dom}\\,\\psi$ (which is the result of the deterministic function applied to ${\\widehat{g}}_{x}$ ), and the corresponding stochastic gradient $\\widehat{g}_{x_{+}}\\cong\\widehat{g}(\\widehat{x}_{+})$ , the update rule computes $\\widehat{M}_{+}=M_{+}(M,\\Omega,x,\\widehat{x}_{+},\\widehat{g}_{x},\\widehat{g}_{x_{+}})$ (deterministic function  opf its ar gpupments) such that $\\widehat{M}_{+}\\geq M$ and  thxe following inequali tpy h oplds  pfor any $\\overline{{M}}>c_{2}L_{f}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\widehat{\\Delta}(\\widehat{M}_{+})+(\\widehat{M}_{+}-M)\\Omega+\\beta_{f,\\bar{f},\\bar{g}}(\\widehat{x}_{+},x)]}\\\\ &{\\quad\\leq\\frac{c_{1}}{\\overline{{M}}-c_{2}L_{f}}\\mathbb{E}[\\mathrm{Var}_{\\bar{g}}(\\widehat{x}_{+})+\\mathrm{Var}_{\\bar{g}}(x)]+c_{3}\\delta_{f}+c_{4}\\,\\mathbb{E}\\bigl\\{\\bigl[\\operatorname*{min}\\{\\widehat{M}_{+},\\overline{{M}}\\}-M\\bigr]_{+}\\Omega\\bigr\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\widehat{\\Delta}(\\widehat{M}_{+}):=\\beta_{f,\\bar{f},\\bar{g}}(x,\\widehat{x}_{+})+\\langle\\bar{g}(x)-\\widehat{g}_{x},\\widehat{x}_{+}-x\\rangle-\\frac{\\widehat{M}_{+}}{2}\\|\\widehat{x}_{+}-x\\|^{2},c_{1},c_{2},c_{3},c_{4}>0}\\end{array}$ are some absolu tpe coxnstants, a nsds $\\mathrm{Var}_{\\hat{g}}(x):=\\mathbb{E}_{\\xi}[\\|g(x,\\xi)-\\bar{g}(x)\\|_{*}^{2}]$ is tphe variance of $\\widehat g$ . The expectations in (2) are taken w.r.t. the randpomness $(\\xi,\\xi_{+})$ coming from $\\bar{\\hat{g}}_{x}\\equiv g(x,\\xi)$ , $\\hat{g}_{x_{+}}\\equiv g(\\hat{x}_{+},\\xi_{+})$ . ", "page_idx": 4}, {"type": "text", "text": "The main example is the following AdaGrad rule: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\boxed{\\widehat{M}_{+}=\\sqrt{M^{2}+\\frac{1}{\\Omega}\\|\\widehat{g}_{x_{+}}-\\widehat{g}_{x}\\|_{*}^{2}}.}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For this rule, we have $\\begin{array}{r}{c_{1}=\\frac{5}{2}}\\end{array}$ , $c_{2}=4$ , $c_{3}=6$ , $c_{4}=2$ (see Lemma 20). Another interesting example recently suggested in [49] is $\\widehat{M}_{+}$ found from the equation ", "page_idx": 4}, {"type": "equation", "text": "$$\n(\\widehat{M}_{+}-M)\\Omega=\\Big[\\langle\\widehat{g}_{x_{+}}-\\widehat{g}_{x},\\widehat{x}_{+}-x\\rangle-\\frac{\\widehat{M}_{+}}{2}\\|\\widehat{x}_{+}-x\\|^{2}\\Big]_{+}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This equation admits a unique solutipon whipch  pcan be easily wriptten down in closed form (see Lemma E.1 in [49]). For this rule, we have $c_{1}=1$ , $c_{2}=2$ , $c_{3}=6$ , $c_{4}=2$ (see Lemma 21). ", "page_idx": 4}, {"type": "text", "text": "Inequality (2) is the only property we need from the stepsize update rule to establish all forthcoming results. This inequality is exactly what is typically used inside the convergence proofs for stochastic gradient methods with predefined stepsizes $M_{k}\\,\\equiv\\,\\overline{{M}}$ (in which case $\\dot{M}\\,=\\,\\widehat{\\cal M}_{+}\\,=\\,\\overline{{{M}}}\\,\\mathrm{~\\,~}$ ), where $\\overline{{M}}$ depends on problem-dependent constants. The ke\u010ey property of AdaGrad  stxepsize s \u010e(either (3) o\u010er (4)) is that they ensure the same inequality but now $\\overbar{M}$ is the virtual stepsize existing only in the theoretical analysis. The price for this is the ex tra\u010e error term $\\operatorname{\\lbrackmin\\{\\widehat{M}_{+},\\overline{{M}}\\}\\ -\\ M\\rbrack}_{+}\\Omega$ appearing in the right-hand side of (2). The crucial property of this error term is txhat it \u010eis telescopic, $\\begin{array}{r}{\\sum_{i=0}^{k}[\\operatorname*{min}\\{M_{i+1},\\overline{{M}}\\}-M_{i}]_{+}\\Omega=[\\operatorname*{min}\\{M_{k+1},\\overline{{M}}\\}-M_{0}]_{+}\\Omega}\\end{array}$ (see Lemma 18) and therefore its total cumulative im p\u010eact is always bounded by the  co\u010entrollable constantM\u2126. Although a number of other works on theoretical analysis of AdaGrad methods for smooth optimization use some similar ideas about the virtual stepsize (e.g., [30, 34, 36]), this is the first time one has abstracted away all the technical details and identified the specific inequality (2) responsible for the universality of AdaGrad. ", "page_idx": 4}, {"type": "text", "text": "4 Uniformly Bounded Variance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we assume that the variance of our stochastic oracle is uniformly bounded. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3. For the stochastic oracle $\\widehat{g}_{\\mathrm{.}}$ , we have $\\begin{array}{r}{\\sigma^{2}:=\\operatorname*{sup}_{x\\in\\operatorname{dom}\\psi}\\operatorname{Var}_{\\hat{g}}(x)\\,<\\,+\\infty,}\\end{array}$ , where $\\mathrm{Var}_{\\hat{g}}(x):=\\mathbb{E}_{\\xi}[\\|g(x,\\xi)-\\bar{g}(x)\\|_{*}^{2}]$ . ", "page_idx": 4}, {"type": "text", "text": "Under this assumption, we can establish the following efficiency estimates for our UniSgd and UniFastSgd methods (the proofs are deferred to Appendix C). ", "page_idx": 5}, {"type": "text", "text": "Theorem 4. Let Algorithm $^{\\,l}$ with $M_{0}=0$ be applied to problem (1) under Assumptions 1\u20133. Then, for the point ${\\bar{x}}_{N}$ generated by the algorithm, we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\bar{x}_{N})]-F^{*}\\le\\frac{c_{2}c_{4}L_{f}D^{2}}{N}+2\\sigma D\\sqrt{\\frac{2c_{1}c_{4}}{N}}+c_{3}\\delta_{f}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 5. Let Algorithm s2 be applied to problem (1) under Assumptions $_{I-3}$ . Then, for any $k\\geq1$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(x_{k})]-F^{*}\\le\\frac{4c_{2}c_{4}L_{f}D^{2}}{k(k+1)}+4\\sigma D\\sqrt{\\frac{2c_{1}c_{4}}{3k}}+\\frac{c_{3}}{3}(k+2)\\delta_{f}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We see that, in contrast to UniSgd, the accelerated algorithm UniFastSgd is not robust to the oracle\u2019s errors: it accumulates them with time at the rate of $O(k\\delta)$ . This is not surprising since the same phenomenon also occurs in the classical accelerated gradient method, even when the oracle is deterministic and the algorithm has the knowledge about all constants (see [15]). ", "page_idx": 5}, {"type": "text", "text": "The complexity results from Theorems 4 and 5 are similar to those from [13]. However, it is important that our methods are adaptive and do not require knowing the constants $L_{f}$ and $\\sigma$ . ", "page_idx": 5}, {"type": "text", "text": "In the specific case when $\\delta_{f}=0$ , we recover the same convergence rates as in [30, 34], although our methods work for the more general composite optimization problem and, in contrast to [34], do not require that $\\nabla f(x^{*})=0$ . ", "page_idx": 5}, {"type": "text", "text": "5 Implicit Variance Reduction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The assumption of uniformly bounded variance may not hold for some problems, or the corresponding constant $\\sigma^{\\dot{2}}$ might be quite large, which is why there has recently been a growing interest in various alternative variance bound assumptions [5, 22, 24, 29, 42, 54, 59]. One interesting option is expressing complexity bounds via the variance at the minimizer, $\\sigma_{*}^{2}:=\\mathrm{Var}_{\\hat{g}}(x^{*})$ , assuming that the stochastic oracle $\\widehat g$ satisfies some extra smoothness conditions. Let us show that, for our Algorithms 1 and 2, we can also establish such bounds, moreover, this can be done without any modifications to the algorithms. ", "page_idx": 5}, {"type": "text", "text": "In this section, we study problem (1) under Assumptions 1 and 2 and also under the following additional smoothness assumption on the variance: ", "page_idx": 5}, {"type": "text", "text": "Assumption 6. There exist $\\delta_{\\hat{g}},L_{\\hat{g}}\\;\\geq\\;0$ such that $\\mathrm{Var}_{\\hat{g}}(x,y)\\,\\le\\,2L_{\\hat{g}}[\\beta_{f,\\bar{f},\\bar{g}}(x,y)+\\delta_{\\hat{g}}]$ for any $x,y\\in\\mathbb{R}^{d}$ , where $\\mathrm{Var}_{\\hat{g}}(x,y):=\\mathbb{E}_{\\xi}[\\|[g(x,\\xi)-g(y,\\xi)]-[\\overline{{g}}(x)-\\overline{{g}}(y)]\\|_{*}^{2}]$ . ", "page_idx": 5}, {"type": "text", "text": "Note that $\\operatorname{Var}_{\\hat{g}}(x,y)$ is the usual variance of the estimators $g(x,\\xi)-g(y,\\xi)$ which uses the same randomness $\\xi$ p for both arguments. Hence, $\\begin{array}{r}{\\mathrm{Var}_{\\hat{g}}(x,y)\\,\\le\\,\\mathbb{E}[\\|g(x,\\xi)\\,-\\,g(y,\\xi)\\|_{*}^{2}]}\\end{array}$ for any $x,y$ Furthermore, if $\\widehat{g}_{b}$ is the mini-batch version of $\\widehat g$ ofp size $b$ (i.e., the average of $b$ i.i.d. samples of ${\\widehat{g}}(x)$ at any point $x$ ), then $\\begin{array}{r}{\\operatorname{Var}_{\\hat{g}_{b}}(x,y)=\\frac{1}{b}\\operatorname{Var}_{\\hat{g}}(x,\\breve{y})}\\end{array}$ for any $x,y$ . ", "page_idx": 5}, {"type": "text", "text": "For instance, if $f(x)=\\mathbb{E}_{\\xi}[f_{\\xi}(x)]$ , wherep each function $f_{\\xi}$ is convex and $(\\delta_{\\xi},L_{\\xi})$ -approximately smooth with components $(\\bar{f}_{\\xi},\\bar{g}_{\\xi})$ , then, the stochastic gradient oracle $\\widehat g$ , defined by $g(x,\\xi):=\\overline{{g}}_{\\xi}(x)$ satisfies Assumption 6 wi tsh $\\bar{f}(x)\\,=\\,\\mathbb{E}_{\\xi}[\\bar{f}_{\\xi}(x)]$ , $\\overline{{g}}(x)\\,=\\,\\mathbb{E}_{\\xi}[\\overline{{g}}_{\\xi}(x)]$ ,  pand $\\begin{array}{r}{\\delta_{\\widehat{g}}\\,=\\,\\frac{1}{L_{\\mathrm{max}}}\\,\\mathbb{E}_{\\xi}[L_{\\xi}\\delta_{\\xi}]}\\end{array}$ $\\backslash$ $\\mathbb{E}_{\\xi}[\\delta_{\\xi}])$ , $L_{\\hat{g}}=L_{\\mathrm{max}}$ , where $L_{\\mathrm{max}}:=\\operatorname*{sup}_{\\xi}L_{\\xi}$ (se es Lemma 16)s. Furthermorep, if $\\widehat{g}_{b}$ is the mini-batch version of $\\widehat g$ of size $b$ , then $\\widehat{g}_{b}$ satisfies Assumption 6 with the same $\\delta_{\\hat{g}_{b}}=\\delta_{\\hat{g}}$ but $\\begin{array}{r}{L_{\\widehat{g}_{b}}=\\frac{1}{b}L_{\\widehat{g}}=\\frac{1}{b}L_{\\operatorname*{max}}}\\end{array}$ which can be much smaller than $L_{\\mathrm{max}}$ when $b$ is large enough. ", "page_idx": 5}, {"type": "text", "text": "Under the new assumption on the variance, UniSgd enjoys the following convergence rate (see Appendix D.1 for the proof). ", "page_idx": 5}, {"type": "text", "text": "Theorem 7. Let Algorithm $^{\\,I}$ with $M_{0}=0$ be applied to problem (1) under Assumptions $^{\\,l}$ , 2 and $^{6}$ , and let $\\sigma_{*}^{2}:=\\mathrm{Var}_{\\hat{g}}(x^{*})$ . Then, for the point ${\\bar{x}}_{N}$ produced by the method, we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\bar{x}_{N})]-F^{*}\\leq\\frac{c_{4}(c_{2}L_{f}+12c_{1}L_{\\hat{g}})D^{2}}{N}+2\\sigma_{*}D\\sqrt{\\frac{6c_{1}c_{4}}{N}}+c_{3}\\delta_{f}+\\frac{4}{3}\\delta_{\\hat{g}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Comparing the above result with Theorem 4, we see that we have essentially replaced the uniform bound $\\sigma$ with the more refined one $\\sigma_{*}$ at the cost of replacing $L_{f}$ with $L_{f}+L_{\\widehat{g}}$ and $\\delta_{f}$ with $\\delta_{f}+\\delta_{\\widehat{g}}$ . ", "page_idx": 5}, {"type": "table", "img_path": "rniiAVjHi5/tmp/acc81b817be86e93cdce0328da5fb908e0038bce3bd4bbf0ebf6719c8dbf3d67.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "This corresponds to classical results on the usual SGD for which we know all problem dependentconstants. However, our method is universal and works automatically under both assumptions from the previous section and the current one, and therefore enjoys the best among the rates given by Theorems 4 and 7. ", "page_idx": 6}, {"type": "text", "text": "For the accelerated algorithm, we have the following result (whose proof is located in Appendix D.2). Theorem 8. Let Algorithm 2 be applied to problem (1) under Assumptions $^{\\,l}$ , 2 and 6, and let $\\sigma_{*}^{2}:=\\mathrm{Var}_{\\hat{g}}(x^{*})$ . Then, for any $k\\geq1$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(x_{k})]-F^{*}\\le\\frac{4c_{2}c_{4}L_{f}D^{2}}{k(k+1)}+\\frac{24c_{1}c_{4}L_{\\widehat{g}}D^{2}}{k+1}+4\\sigma_{*}D\\sqrt{\\frac{2c_{1}c_{4}}{k}}+\\frac{c_{3}}{3}(k+2)\\delta_{f}+\\frac{4}{3}\\delta_{\\widehat{g}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Comparing our previous complexity bound for UniFastSgd under the assumption on uniformly bounded variance (Theorem 5) with the bound from Theorem 8, we see that, instead of simply replacing $\\sigma$ with $\\sigma_{*}$ , $L_{f}$ with $L_{f}+L_{\\widehat{g}}$ and $\\delta_{f}$ with $\\delta_{f}+\\delta_{\\widehat{g}}$ , which was the case for the basic method, the situation is now not that simple. pSpecifically, the $L_{f}$ and $L_{\\hat{g}}$ terms now converge at different rates: $\\begin{array}{r}{O\\big(\\frac{1}{k^{2}}\\big)}\\end{array}$ and $O(\\frac{1}{k})$ , respectively. While this may seem strangpe at first, this behavior is actually unavoidable, at least in the case when $\\delta_{f}=\\delta_{\\widehat{g}}=0$ (see, e.g., Section E in [59]). For the case when $\\delta_{f}=\\delta_{\\widehat{g}}=0$ , the complexity result from Theoprem 8 is similar to the results for the Accelerated SGD algorithpm from [59]. However, the latter paper studies a specific setting where $f(x)=\\mathbb{E}[f_{\\xi}(x)]$ , where each component $f_{\\xi}$ is $L_{\\mathrm{max}}$ -smooth and then assumes that $f$ is also $L_{\\mathrm{max}}$ -smooth, instead of working with the constant $L_{f}$ which can be much smaller than $L_{\\mathrm{max}}$ . A similar separation of the constants $L_{f}$ and $L_{\\hat{g}}$ , which we do, was recently considered in [24], where the authors obtained some similar rates to our pTheorem 8. However, it is important that, unlike the algorithms considered in [24, 59], our UniFastSgd is universal and does not require knowing any problem-dependent constants except $D$ . Furthermore, our results are more general because we allow the oracle to be inexact. ", "page_idx": 6}, {"type": "text", "text": "6 Explicit Variance Reduction with SVRG ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Let us now show that we can also incorporate explicit SVRG-type variance reduction into our methods. In this section, we consider problem (1) under Assumptions 1, 2 and 6. All the proofs are deferred to Appendix E. ", "page_idx": 6}, {"type": "text", "text": "In addition to the stochastic oracle $\\widehat g$ , we now assume that we can also compute the (approximate) full-gradient oracle $\\overline{{g}}$ . This allows us to define the following auxiliary SVRG oracle induced by $\\widehat g$ with center $\\tilde{x}\\in\\mathbb{R}^{d}$ (snotation $\\widehat{G}=\\mathrm{SvrgOrac}_{\\widehat{g},\\overline{{{g}}}}(\\widetilde{x}))$ as the oracle with the same random variab lpe component $\\xi$ as $\\widehat g$ and the func tipon component pgisven by $G(x,\\xi)=g(x,\\xi)-g(\\tilde{x},\\xi)+\\bar{g}(\\tilde{x})$ . ", "page_idx": 6}, {"type": "text", "text": "Our UniSvrg m epthod is presented in Algorithm 3. This is the classical epoch-based SVR sG algorithm which can be seen as the adaptive version of the $\\mathrm{SVRG++}$ method from [1]. A similar scheme was suggested in [16], however, instead of accumulating gradient differences as in (3), their method accumulates gradients and therefore does not work without the additional assumption of $\\nabla f(x^{*})=0$ (which may not hold for constrained optimization). ", "page_idx": 6}, {"type": "text", "text": "Let us now present the complexity guarantees. To do so, we first need to introduce, one more assumption we need in our analysis. ", "page_idx": 6}, {"type": "text", "text": "Assumption 9. The variance of $\\widehat g$ satisfies $\\mathrm{Var}_{\\hat{g}}(x,y)\\leq4L_{\\hat{g}}[\\beta_{f}^{\\nabla f(x)}(x,y)+2\\delta_{\\hat{g}}]f o r\\,a n y\\,x,y\\in\\mathbb{R}^{d}$ and any $\\nabla f(x)\\in\\partial f(x)$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 9 is very similar to Assumption 6. The only difference between them is that the former contains the standard Bregman distance in the right-hand side, while the latter contains its approximation $\\beta_{f,\\bar{f},\\bar{g}}(x,y)$ involving the approximate function value ${\\bar{f}}(x)$ and the approximate gradient $\\overline{{g}}(x)$ . Neve rsthseless, both assumptions are actually satisfied f osr the main examples we discussed after introducing Assumption 6 (see Lemma 16). ", "page_idx": 6}, {"type": "table", "img_path": "rniiAVjHi5/tmp/5e85ba11a00f4bd3e89d7cd60968f9e6dfc8d0a03c6a3ea8fb9c21a7e1191029.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Theorem 10. Let UniSvrg (as defined by Algorithm 3) be applied to problem (1) under Assumptions $^{\\,l}$ 2, 6 and 9. Then, for any $t\\geq1$ and $\\overline{{c}}_{3}:=\\operatorname*{max}\\{c_{3},1\\}$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\tilde{x}_{t})]-F^{*}\\le\\frac{[(c_{2}c_{4}+1)L_{f}+48c_{1}c_{4}L_{\\hat{g}}]D^{2}}{2^{t}}+2\\bar{c}_{3}\\delta_{f}+\\frac{8}{3}\\delta_{\\hat{g}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "To construct $\\tilde{x}_{t}$ , the algorithm needs to make $O(2^{t})$ queries to $\\widehat g$ and $O(t)$ queries to $\\overline{{g}}$ . ", "page_idx": 7}, {"type": "text", "text": "We now present an accelerated version of UniSvrg, see Alg oprithm 4. As UniSvr gs, this method is also epoch-based, and its epoch is very similar to UniFastSgd (Algorithm 4) in the sense that it also iterates similar-triangle steps. However, the triangles in UniTriSvrgEpoch are of the form $(\\tilde{x},v_{k},v_{k+1})$ , i.e., they always share the common vertex $\\tilde{x}$ , in contrast to the triangles $(x_{k},v_{k},v_{k+1})$ in UniFastSgd (in UniTriSvrgEpoch, the role of the average points $y_{k}$ is played by $x_{k}$ ). We note that our UniFastSvrg is essentially the primal version of the VRADA method from [53], but equipped with AdaGrad stepsizes. Alternative accelerated SVRG schemes with AdaGrad stepsizes (3) were recently proposed in [36]; however, they seem to be much more complicated. ", "page_idx": 7}, {"type": "text", "text": "The special choice of the initial reference point $\\scriptstyle{\\tilde{x}}_{0}$ at Line 1 is rather standard and motivated by the desire to keep the initial function residual appropriately bounded: $\\begin{array}{r}{F(\\tilde{x}_{0})-F^{*}\\le\\frac{1}{2}L_{f}D^{2}+\\delta_{f}}\\end{array}$ ; the simplest way to achieve this is to make the full gradient step from any feasible point (see Lemma 34). ", "page_idx": 7}, {"type": "text", "text": "Theorem 11. Let UniFastSvrg (Algorithm 4) be applied to problem (1) under Assumptions $^{\\,l}$ , 2 and $6$ , and let $N\\geq9$ . Then, for any $t\\geq t_{0}:=\\lceil\\log_{2}\\log_{3}N\\rceil-1\\left(\\geq0\\right)$ , it holds that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\tilde{x}_{t})]-F^{*}\\leq\\frac{9[(c_{2}c_{4}+\\frac12)L_{f}+6c_{1}c_{4}L_{\\hat{g}}]D^{2}}{N(t-t_{0}+1)^{2}}+(c_{3}t+1)\\delta_{f}+\\frac53t\\delta_{\\hat{g}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "To construct $\\tilde{x}_{t}$ , the algorithm needs to make $O(N t)$ queries to $\\widehat g$ and $O(t)$ queries to $\\overline{{g}}$ . Assuming that the complexity of querying $\\overline{{g}}$ is $n$ times bigger than that of querying $\\widehat g$ and choosing $N=\\Theta(n)$ , we get the total stochastic-oracle complexity of $O(n t)$ . ", "page_idx": 7}, {"type": "text", "text": "Note that Theorem 11, unlike Theorem 10, does not require the extra Assumption 9. This suggests that Assumption 9 might be somewhat artificial and could potentially be removed from Theorem 10 as well. However, we do not know how to do it, even in the simplest case when $\\delta_{f}=\\delta_{\\widehat{g}}=0$ and the algorithm has the knowledge of the constants $L_{f}$ and $L_{\\hat{g}}$ from Assumptions 1 and 6. ", "page_idx": 7}, {"type": "text", "text": "7 Application to H\u00f6lder Smooth Problems ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To illustrate how powerful our results are, let us quickly consider the specific example of solving the stochastic optimization problem with H\u00f6lder smooth components. ", "page_idx": 7}, {"type": "text", "text": "Example 12. Suppose that the function $f$ in problem (1) is the expectation of other functions, $f(x)\\bar{=\\mathbb{E}_{\\xi}[f_{\\xi}(x)]}$ , where each function $f_{\\xi}$ is convex and $(\\nu,H_{\\xi}(\\nu))$ -H\u00f6lder smooth. Consider the standard mini-batch stochastic gradient oracle $\\widehat{g}_{b}$ of size $b$ , defined by $\\begin{array}{r}{g_{b}(x,\\xi_{[b]})=\\frac{1}{b}\\sum_{j=1}^{b}\\nabla f_{\\xi_{j}}(x)}\\end{array}$ where $\\xi_{[b]}:=(\\xi_{1},\\dots,\\xi_{b})$ with $b$ i.i.d. copies  opf $\\xi$ , and $\\nabla f_{\\xi}(x)\\in\\partial f_{\\xi}(x)$ is an arbitrary selection of subgradients for each $\\xi$ . We define $H_{f}(\\nu)$ as the H\u00f6lder constant for the function $f$ and $H_{\\mathrm{max}}(\\nu):=$ $\\bar{\\operatorname{sup}_{\\xi}H_{\\xi}}(\\nu)$ as the worst among H\u00f6lder constants for each $f_{\\xi}$ . Note that we always have $H_{f}(\\nu)\\leq$ $\\mathbb{E}_{\\xi}[\\dot{H}_{\\xi}(\\nu)]$ but $H_{f}(\\nu)$ can, in principle, be much smaller than the right-hand side. Also, define $\\begin{array}{r}{\\sigma^{2}\\;:=\\;\\operatorname*{sup}_{x\\in\\dim\\psi}\\operatorname{Var}_{\\hat{g}_{1}}(x)\\;\\equiv\\;\\operatorname*{sup}_{x\\in\\dim\\psi}\\mathbb{E}_{\\xi}[\\|\\nabla f_{\\xi}(x)\\,-\\,\\nabla f(x)\\|_{*}^{2}]}\\end{array}$ and $\\sigma_{*}^{2}\\;:=\\;\\mathrm{Var}_{\\widehat{g}_{1}}(x^{*})\\;\\equiv\\;$ $\\mathbb{E}_{\\xi}[\\|\\nabla f_{\\xi}(x^{*})-\\nabla f(x^{*})\\|_{*}^{2}]$ . We assume that the computation of $\\widehat{g}_{b}$ can be parallelizepd and the computation of $\\nabla f$ is $n_{b}$ times more expensive than that of $\\widehat{g}_{b}$ . ", "page_idx": 7}, {"type": "table", "img_path": "rniiAVjHi5/tmp/e9117181ed2b5ad8d37a87e62d204af27aeb3fded23ccb7932b96e50ba47177e.jpg", "table_caption": ["Table 2: Corollaries of our results for the case when problem (1) has H\u00f6lder smooth components, as defined in Example 12. \u201cSO complexity\u201d is the stochastic-oracle complexity for reaching accuracy $\\epsilon$ in terms of the expected function residual, defined as in Table 1 but with $\\widehat{g}=\\bar{\\widehat{g}}_{b},\\,\\overline{{g}}=\\nabla f$ , $n=n_{b}$ . "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "To solve the above problem, we can apply any of the meth opds we presented before. The resulting oracle complexities (in terms of the BigO-notation) are summarized in Table 2; the precise statements the corresponding results and their proofs are deferred to Appendix F. ", "page_idx": 8}, {"type": "text", "text": "Note that our problem is characterized by a large number of parameters, $\\nu$ , $H_{f}(\\nu)$ , $H_{\\mathrm{max}}(\\nu),\\sigma,\\sigma_{\\ast}$ For each combination of these parameters, we get a certain complexity guarantee for each of our methods, and it is impossible to say in advance which combination results in the smaller complexity bound. However, it is not important for our methods since none of them needs to know any of these constants to ensure the corresponding bound. This means that our algorithms are universal: they automatically figure out the best problem class for a specific problem given to them. ", "page_idx": 8}, {"type": "text", "text": "8 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Let us illustrate the performance of our methods in preliminary numerical experiments2 on solving ", "page_idx": 8}, {"type": "equation", "text": "$$\nf^{*}:=\\operatorname*{min}_{\\|x\\|\\leq R}\\Bigl\\{f(x):=\\frac{1}{n}\\sum_{i=1}^{n}[\\langle a_{i},x\\rangle-b_{i}]_{+}^{q}\\Bigr\\},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $a_{i},b_{i}\\in\\mathbb{R}^{d}$ , $q\\in[1,2]$ and $R>0$ . ", "page_idx": 8}, {"type": "text", "text": "This test problem covers several interesting applications. Indeed, if $q=2$ , we get the classical Least squares problem. If $q=1$ , this is the well-known Support-Vector Machines (SVM) problem. In both cases, the ball-constraint $\\|x\\|\\leq R$ acts as a regularizer, and problem (5) is, in fact, equivalent to $\\begin{array}{r}{\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}[f(x)+\\frac{\\mu}{2}\\|x\\|^{2}]}\\end{array}$ for a certain $\\mu\\geq0$ (this follows, e.g., from the KKT optimality conditions) such that $\\mu$ decreases when $R$ increases. ", "page_idx": 8}, {"type": "text", "text": "Another interesting application of (5), which we consider in this section, is the polyhedron feasibility problem: find $x^{*}\\in\\bar{\\mathbb{R}}^{d}$ , $\\|x^{*}\\|\\leq R$ , inside the polyhedron $P=\\{x:\\langle a_{i},x\\rangle\\leq b_{i}$ , $i=1,\\dots,n\\}$ . Such a point exists iff $f^{*}=0$ . Note that (5) is a problem with H\u00f6lder smooth components of degree $\\nu=q-1$ . By varying $q$ in (5), we can therefore check the adaptivity of different methods to the unknown to them H\u00f6lder characteristics of the objective function. ", "page_idx": 8}, {"type": "text", "text": "The data for our problem is generated randomly. First, we generate $x^{*}$ uniformly from the sphere of radius $0.95R$ centered at the origin. Then, we generate i.i.d. vectors $a_{i}$ with components uniformly distributed on $[-1,1]$ . We then make sure that $\\left\\langle a_{n},x^{*}\\right\\rangle<0$ by inverting the sign of $a_{n}$ if necessary. Next, we generate positive reals $s_{i}$ uniformly in $[0,-0.1c_{\\mathrm{min}}]$ , where $c_{\\operatorname*{min}}:=\\operatorname*{min}_{i}\\langle a_{i},x^{*}\\rangle<0$ , and set $b_{i}\\,\\bar{=}\\,\\langle a_{i},x^{\\bar{*}}\\rangle+s_{i}$ . By construction, $x^{*}$ is a solution of our problem with $f^{*}=0$ , and the origin $x_{0}\\,=\\,0$ lies outside the polyhedron since there exists $j$ (corresponding to $c_{\\operatorname*{min}}$ ) such that $b_{j}=c_{\\operatorname*{min}}+s_{j}\\le0.9c_{\\operatorname*{min}}<0$ . ", "page_idx": 8}, {"type": "image", "img_path": "rniiAVjHi5/tmp/38e034fc4e3e6ebeaa509790c883b68801202e3230474c8f0f57d14a41e825e4.jpg", "img_caption": ["Figure 1: Comparison of different methods on the polyhedron feasibility problem (5). "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "rniiAVjHi5/tmp/33efd461487306952a5d9f2af7a7d4cbb76fe98382367f1efc08337ca1a0dd9f.jpg", "img_caption": ["Figure 2: Impact of mini-batch size on performance of our methods. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "We compare UniSvrg (Algorithm 3) against AdaSVRG [16] (with parameters $K=3$ and $\\eta=D=$ $2R_{c}$ ). We next compare UniFastSvrg (Algorithm 4) against AdaVRAE and AdaVRAG [36]. We also compare it with the FastSvrg method with constant stepsize, which is the primal version of the VRADA method from [53]; the stepsize is selected by doing a grid search over $\\{\\bar{1}0^{j}:j=-3,\\ldots,4\\}$ and choosing the best value in the sense that the algorithm is neither too slow nor has a large error. We report UniSgd (Algorithm 1) and UniFastSgd (Algorithm 2) together with these methods. For UniFastSvrg, contrary to the theoretical recommendation of choosing $\\scriptstyle{\\tilde{x}}_{0}$ as the result of the full gradient step, we found it slightly more useful to simply set $\\tilde{x}_{0}=x_{0}$ . For all our methods, we use the AdaGrad stepsize (3); the other stepsize (4) works very similarly (see Appendix H.2 for a detailed comparison). For all methods, we use the standard mini-batch stochastic oracle of size $b=256$ . ", "page_idx": 9}, {"type": "text", "text": "The results are shown in Fig. 1, where we fix $n=10^{4}$ , $d=10^{3}$ , $R=10^{6}$ and consider different values of $q\\in\\{1,1.3,1.6,2\\}$ . We plot the total number of stochastic oracle calls against the function residual. We treat one mini-batch oracle computation as one stochastic oracle call. If we compute the full gradient, we count this as $n/b$ stochastic oracle calls where $n$ is the total number of samples and $b$ denotes the mini-batch size. ", "page_idx": 9}, {"type": "text", "text": "We see that, except the AdaSVRG method, all SVRG algorithms typically converge much faster than the usual SGD methods without explicit variance reduction, at least after a few computations of the full gradient. Among the non-accelerated SVRG methods, UniSvrg converges consistently faster than AdaSVRG, while UniFastSvrg performs the best across the accelerated ones. Note that FastSvrg with constant stepsize is not converging when the problem is not Lipschitz smooth $(q<2)$ ), in contrast to our universal methods. ", "page_idx": 9}, {"type": "text", "text": "In Fig. 2, we also illustrate the impact of the mini-batch size $b$ on the convergence of our methods. We consider the same values of $n$ , d, $R$ as before and fix $q=1.5$ . As we can see, in the idealized situation, when one can implement the mini-batch oracle computations by perfect parallelism, there is a significant speedup in convergence when increasing the mini-batch size, as predicted by our theory. ", "page_idx": 9}, {"type": "text", "text": "For additional experiments, including the discussion of implicit variance reduction, see Appendix $\\mathrm{H}$ . ", "page_idx": 9}, {"type": "text", "text": "9 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we showed that AdaGrad stepsizes can be applied, in a unified manner, in a large variety of situations, leading to universal methods suitable for multiple problem classes at the same time. Note that this does not come for free. We still need to know one parameter, the diameter $D$ of the feasible set. While it is not necessary to know this parameter precisely, the cost of underestimating or overestimating it, can be high (all complexity bounds would be multiplied by the ratio between our guess and the true $D$ ). At the same time, there already exist some parameter-free methods which are based on AdaGrad and aim to solve precisely this problem [6, 11, 25, 31, 41]. It is therefore interesting to consider extensions of our results to these more advanced algorithms. Another interesting direction is, of course, nonconvex problems. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors are thankful to the anonymous reviewers for their comments and suggestions. Sebastian Stich acknowledges funding support from the Meta Research Award and the Google Research Award. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Z. Allen-Zhu and Y. Yuan. Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives. In International Conference on Machine Learning, pages 1080\u20131089, 2016.   \n[2] A. Attia and T. Koren. SGD with AdaGrad Stepsizes: Full Adaptivity with High Probability to Unknown Parameters, Unbounded Gradients and Affine Variance. In International Conference on Machine Learning, pages 1147\u20131171, 2023.   \n[3] R. Babanezhad Harikandeh, M. O. Ahmed, A. Virani, M. Schmidt, J. Konec\u02c7ny\\`, and S. Sallinen. StopWasting My Gradients: Practical SVRG. Advances in Neural Information Processing Systems, 28, 2015.   \n[4] D. P. Bertsekas. Stochastic optimization problems with nondifferentiable cost functionals. Journal of Optimization Theory and Applications, 12(2):218\u2013231, 1973.   \n[5] L. Bottou, F. E. Curtis, and J. Nocedal. Optimization Methods for Large-Scale Machine Learning. SIAM Review, 60(2):223\u2013311, 2018.   \n[6] Y. Carmon and O. Hinder. Making SGD Parameter-Free. In Conference on Learning Theory, pages 2360\u20132389, 2022.   \n[7] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1\u201327:27, 3, 2011.   \n[8] A. Cotter, O. Shamir, N. Srebro, and K. Sridharan. Better mini-batch algorithms via accelerated gradient methods. Advances in Neural Information Processing Systems, 24, 2011.   \n[9] A. Cutkosky and K. Boahen. Online Learning Without Prior Information. In Conference on Learning Theory, pages 643\u2013677, 2017.   \n[10] A. Cutkosky and F. Orabona. Black-Box Reductions for Parameter-free Online Learning in Banach Spaces. In Conference On Learning Theory, pages 1493\u20131529, 2018.   \n[11] A. Defazio and K. Mishchenko. Learning-Rate-Free Learning by D-Adaptation. In International Conference on Machine Learning, pages 7449\u20137479, 2023.   \n[12] Q. Deng, Y. Cheng, and G. Lan. Optimal Adaptive and Accelerated Stochastic Gradient Descent. arXiv preprint arXiv:1810.00553, 2018.   \n[13] O. Devolder. Stochastic first order methods in smooth convex optimization. CORE Discussion Papers, 2011/70, 2011.   \n[14] O. Devolder. Exactness, Inexactness and Stochasticity in First-Order Methods for Large-Scale Convex Optimization. PhD thesis, Universit\u00e9 catholique de Louvain (UCL), 2013.   \n[15] O. Devolder, F. Glineur, and Y. Nesterov. First-order methods of smooth convex optimization with inexact oracle. Mathematical Programming, 146:37\u201375, 2013. DOI: 10.1007/s10107- 013-0677-5.   \n[16] B. Dubois-Taine, S. Vaswani, R. Babanezhad, M. Schmidt, and S. Lacoste-Julien. SVRG meets AdaGrad: painless variance reduction. Machine Learning, 111(12):4359\u20134409, 2022.   \n[17] J. Duchi, E. Hazan, and Y. Singer. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12(7), 2011.   \n[18] A. Ene, H. L. Nguyen, and A. Vladu. Adaptive Gradient Methods for Constrained Convex Optimization and Variational Enequalities. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35 of number 8, pages 7314\u20137321, 2021.   \n[19] E. Gorbunov, F. Hanzely, and P. Richt\u00e1rik. A Unified Theory of SGD: Variance Reduction, Sampling, Quantization and Coordinate Descent. In International Conference on Artificial Intelligence and Statistics, pages 680\u2013690, 2020.   \n[20] R. M. Gower, P. Richt\u00e1rik, and F. Bach. Stochastic quasi-gradient methods: variance reduction via Jacobian sketching. Mathematical Programming, 188(1):135\u2013192, 2021.   \n[21] R. M. Gower, M. Schmidt, F. Bach, and P. Richt\u00e1rik. Variance-Reduced Methods for Machine Learning. Proceedings of the IEEE, 108(11):1968\u20131983, 2020.   \n[22] R. M. Gower, N. Loizou, X. Qian, A. Sailanbayev, E. Shulgin, and P. Richt\u00e1rik. SGD: General Analysis and Improved Rates. In International Conference on Machine Learning, pages 5200\u2013 5209, 2019.   \n[23] G. N. Grapiglia and Y. Nesterov. Regularized Newton Methods for Minimizing Functions with H\u00f6lder Continuous Hessians. SIAM Journal on Optimization, 27(1):478\u2013506. DOI: 10.1137/ 16M1087801.   \n[24] S. Ilandarideva, A. Juditsky, G. Lan, and T. Li. Accelerated stochastic approximation with state-dependent noise. arXiv preprint arXiv:2307.01497, 2023.   \n[25] M. Ivgi, O. Hinder, and Y. Carmon. DoG is SGD\u2019s Best Friend: A Parameter-Free Dynamic Step Size Schedule. In International Conference on Machine Learning, pages 14465\u201314499, 2023.   \n[26] A. Jacobsen and A. Cutkosky. Unconstrained Online Learning with Unbounded Losses. In International Conference on Machine Learning, pages 14590\u201314630, 2023.   \n[27] R. Johnson and T. Zhang. Accelerating Stochastic Gradient Descent using Predictive Variance Reduction. Advances in Neural Information Processing Systems, 26, 2013.   \n[28] P. Joulani, A. Raj, A. Gyorgy, and C. Szepesv\u00e1ri. A Simpler Approach to Accelerated Optimization: Iterative Averaging Meets Optimism. In International Conference on Machine Learning, pages 4984\u20134993, 2020.   \n[29] A. Juditsky, A. Kulunchakov, and H. Tsyntseus. Sparse recovery by reduced variance stochastic approximation. Information and Inference: A Journal of the IMA, 12(2):851\u2013896, 2023.   \n[30] A. Kavis, K. Y. Levy, F. Bach, and V. Cevher. UniXGrad: A Universal, Adaptive Algorithm with Optimal Guarantees for Constrained Optimization. Advances in Neural Information Processing Systems, 32, 2019.   \n[31] A. Khaled, K. Mishchenko, and C. Jin. DoWG Unleashed: An Efficient Universal ParameterFree Gradient Descent Method. Advances in Neural Information Processing Systems, 36:6748\u2013 6769, 2023.   \n[32] D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. International Conference on Learning Representations (ICLR), 2015.   \n[33] G. Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 133(1):365\u2013397, 2012.   \n[34] K. Y. Levy, A. Yurtsever, and V. Cevher. Online Adaptive Methods, Universality and Acceleration. Advances in Neural Information Processing Systems, 31, 2018.   \n[35] C. Liu and M. Belkin. Accelerating SGD with momentum for over-parameterized learning. arXiv preprint arXiv:1810.13395, 2018.   \n[36] Z. Liu, T. D. Nguyen, A. Ene, and H. Nguyen. Adaptive Accelerated (Extra-)Gradient Methods with Variance Reduction. In International Conference on Machine Learning, pages 13947\u2013 13994, 2022.   \n[37] S. Ma, R. Bassily, and M. Belkin. The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning. In International Conference on Machine Learning, pages 3325\u20133334, 2018.   \n[38] M. Mahdavi, L. Zhang, and R. Jin. Mixed Optimization for Smooth Functions. Advances in Neural Information Processing Systems, 26, 2013.   \n[39] H. B. McMahan and M. Streeter. Adaptive Bound Optimization for Online Convex Optimization. arXiv preprint arXiv:1002.4908, 2010.   \n[40] Z. Mhammedi and W. M. Koolen. Lipschitz and Comparator-Norm Adaptivity in Online Learning. In Conference on Learning Theory, pages 2858\u20132887, 2020.   \n[41] K. Mishchenko and A. Defazio. Prodigy: An Expeditiously Adaptive Parameter-Free Learner. arXiv preprint arXiv:2306.06101, 2023.   \n[42] E. Moulines and F. Bach. Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning. Advances in Neural Information Processing Systems, 24, 2011.   \n[43] I. Necoara, Y. Nesterov, and F. Glineur. Linear convergence of first order methods for nonstrongly convex optimization. Mathematical Programming, 175:69\u2013107, 2019.   \n[44] D. Needell, R. Ward, and N. Srebro. Stochastic Gradient Descent, Weighted Sampling, and the Randomized Kaczmarz Algorithm. Advances in Neural Information Processing Systems, 27, 2014.   \n[45] Y. Nesterov. Universal gradient methods for convex optimization problems. Mathematical Programming, 152:381\u2013404, 2015. DOI: 10.1007/s10107-014-0790-0.   \n[46] Y. Nesterov. Lectures on Convex Optimization, volume 137. Springer, 2nd edition, 2018.   \n[47] F. Orabona. Simultaneous Model Selection and Optimization through Parameter-free Stochastic Learning. Advances in Neural Information Processing Systems, 27, 2014.   \n[48] F. Orabona. Normalized Gradients for All. arXiv preprint arXiv:2308.05621, 2023.   \n[49] A. Rodomanov, A. Kavis, Y. Wu, K. Antonakopoulos, and V. Cevher. Universal Gradient Methods for Stochastic Convex Optimization. arXiv preprint arXiv:2402.03210, 2024.   \n[50] M. Schmidt, N. Le Roux, and F. Bach. Minimizing finite sums with the stochastic average gradient. Mathematical Programming, 162:83\u2013112, 2017.   \n[51] M. Schmidt and N. L. Roux. Fast Convergence of Stochastic Gradient Descent under a Strong Growth Condition. arXiv preprint arXiv:1308.6370, 2013.   \n[52] S. Shalev-Shwartz and T. Zhang. Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization. Journal of Machine Learning Research, 14(1), 2013.   \n[53] C. Song, Y. Jiang, and Y. Ma. Variance Reduction via Accelerated Dual Averaging for Finite-Sum Optimization. In Advances in Neural Information Processing Systems, volume 33, pages 833\u2013844, 2020.   \n[54] S. U. Stich. Unified Optimal Analysis of the (Stochastic) Gradient Method. arXiv preprint arXiv:1907.04232, 2019.   \n[55] M. Streeter and H. B. McMahan. No-Regret Algorithms for Unconstrained Online Convex Optimization. In Proceedings of the 25th International Conference on Neural Information Processing Systems, volume 2, pages 2402\u20132410, 2012.   \n[56] T. Tieleman. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. Coursera: Neural Networks for Machine Learning, 4(2):26, 2012.   \n[57] S. Vaswani, A. Mishkin, I. Laradji, M. Schmidt, G. Gidel, and S. Lacoste-Julien. Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates. Advances in Neural Information Processing Systems, 32, 2019.   \n[58] C. Wang, X. Chen, A. J. Smola, and E. P. Xing. Variance Reduction for Stochastic Gradient Optimization. Advances in Neural Information Processing Systems, 26, 2013.   \n[59] B. E. Woodworth and N. Srebro. An Even More Optimal Stochastic Optimization Algorithm: Minibatching and Interpolation Learning. Advances in Neural Information Processing Systems, 34:7333\u20137345, 2021.   \n[60] L. Zhang, M. Mahdavi, and R. Jin. Linear Convergence with Condition Number Independent Access of Full Gradients. Advances in Neural Information Processing Systems, 26, 2013. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A General Auxiliary Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Approximately Smooth Functions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem 13 (Lemma 2 in [45]). Let $f\\colon\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}$ be a convex $(\\nu,H)$ -H\u00f6lder smooth function for some $\\nu\\in[0,1]$ and $H\\ge0$ . Then, for any $\\delta>0$ , any $x,y\\in\\mathbb{R}^{d}$ and any $\\nabla f(x)\\in\\partial f(x)$ , it holds that \u03b2f\u2207 $\\begin{array}{r}{\\beta_{f}^{\\nabla f(x)}(x,y)\\le\\frac{L}{2}\\|x-y\\|^{2}+\\delta}\\end{array}$ with $\\begin{array}{r}{L=\\left[\\frac{1-\\nu}{2(1+\\nu)\\delta}\\right]^{\\frac{1-\\nu}{1+\\nu}}H^{\\frac{2}{1+\\nu}}}\\end{array}$ (with the convention that $0^{0}=1$ ", "page_idx": 13}, {"type": "text", "text": "Theorem 14. Let $f\\colon\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ be a $(\\delta,L)$ -approximately smooth convex function with components $({\\bar{f}},{\\overline{{g}}})$ , i.e., for any $x,y\\in\\mathbb{R}^{d}$ and $\\beta_{f,\\bar{f},\\bar{g}}(x,\\bar{y}):=f(y)-\\bar{f}(x)-\\langle\\bar{g}(x),y-x\\rangle$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n0\\leq\\beta_{f,\\bar{f},\\bar{g}}(x,y)\\leq\\frac{L}{2}\\|x-y\\|^{2}+\\delta.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, for any $x,y\\in\\mathbb{R}^{d}$ and any $\\nabla f(x)\\in\\partial f(x)$ , the following inequalities hold: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\bar{f}(x)\\leq f(x)\\leq\\bar{f}(x)+\\delta,}\\\\ {\\displaystyle\\langle\\bar{g}(x)-\\bar{g}(y),x-y\\rangle\\leq\\beta_{f,\\bar{f},\\bar{g}}(x,y)+\\beta_{f,\\bar{f},\\bar{g}}(y,x)\\leq\\langle\\bar{g}(x)-\\bar{g}(y),x-y\\rangle+2\\delta,}\\\\ {\\displaystyle\\langle\\bar{g}(x)-\\bar{g}(y),x-y\\rangle\\leq L\\|x-y\\|^{2}+2\\delta,}\\\\ {\\displaystyle\\|\\bar{g}(x)-\\bar{g}(y)\\|_{*}^{2}\\leq2L(\\beta_{f,\\bar{f},\\bar{g}}(x,y)+\\delta),}\\\\ {\\|\\nabla f(x)-\\bar{g}(y)\\|_{*}^{2}\\leq2L(\\beta_{f}^{\\nabla f(x)}(x,y)+\\delta),}\\\\ {\\|\\bar{g}(x)-\\bar{g}(y)\\|_{*}^{2}\\leq L^{2}\\|x-y\\|^{2}+4L\\delta,}\\\\ {\\|\\bar{g}(x)-\\bar{g}(y)\\|_{*}^{2}\\leq4L(\\beta_{f}^{\\nabla f(x)}(x,y)+2\\delta),}\\\\ {\\displaystyle\\beta_{f}^{\\nabla f(x)}(x,y)\\leq L\\|x-y\\|^{2}+2\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Inequality (7) follows immediately from (6) by substituting $y=x$ . ", "page_idx": 13}, {"type": "text", "text": "To prove (8), we rewrite ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\beta_{f,\\bar{f},\\bar{g}}(x,y)+\\beta_{f,\\bar{f},\\bar{g}}(y,x)=\\langle\\bar{g}(x)-\\bar{g}(y),x-y\\rangle+[f(x)-\\bar{f}(x)]+[f(y)-\\bar{f}(y)],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and then apply (7). ", "page_idx": 13}, {"type": "text", "text": "Using the first part of (8) and applying (6) twice, we obtain (9). ", "page_idx": 13}, {"type": "text", "text": "To prove (10) and (11), let us fix some $\\overline{{f_{1}}}(x)\\ \\in\\ \\mathbb{R}$ and $\\overline{{g}}_{1}(x)\\,\\in\\,\\mathbb{R}^{d}$ such that $\\beta_{f,\\bar{f}_{1},\\bar{g}_{1}}(z)\\;:=\\;$ $f(z)-{\\bar{f}}_{1}(x)-\\langle{\\bar{g}}_{1}(x),z-x\\rangle\\geq0$ for any $z\\in\\mathbb{R}^{d}$ . Note th ats we can choose either $(\\bar{f}_{1},\\overline{{g}}_{1})=(\\bar{f},\\overline{{g}})$ or $(\\overline{{f}}_{1},\\overline{{g}}_{1})=(f,\\nabla f)$ . In view of (6), for any $z\\in\\mathbb{R}^{d}$ , we can write the following in esqualities: ", "page_idx": 13}, {"type": "equation", "text": "$$\n0\\leq\\beta_{f,\\bar{f}_{1},\\bar{g}_{1}}(z)\\leq\\bar{f}(y)-\\bar{f}_{1}(x)-\\langle\\bar{g}_{1}(x),y-x\\rangle+\\langle\\bar{g}(y)-\\bar{g}_{1}(x),z-y\\rangle+\\frac{L}{2}\\|z-y\\|^{2}+\\delta.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Minimizing the right-hand side in $z\\in\\mathbb{R}^{d}$ and rearranging, we conclude that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{2L}\\|\\bar{g}(y)-\\bar{g}_{1}(x)\\|_{*}^{2}\\le\\bar{f}(y)-\\bar{f}_{1}(x)-\\langle\\bar{g}_{1}(x),y-x\\rangle+\\delta\\le\\beta_{f,\\bar{f}_{1},\\bar{g}_{1}}(x,y)+\\delta,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the final inequality is due to (7). Substituting now either $\\left(\\overline{{f}}_{1},\\overline{{g}}_{1}\\right)\\,=\\,(\\overline{{f}},\\overline{{g}})$ or $(\\bar{f}_{1},\\bar{g}_{1})\\;=\\;$ $(f,\\nabla f)$ , we obtain either (10) or (11), respectively. ", "page_idx": 13}, {"type": "text", "text": "Inequality (12) follows immediately from (6) and (10). Inequality (13) follows from (11): ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\bar{g}(x)-\\bar{g}(y)\\|_{*}^{2}\\le2\\|\\nabla f(x)-\\bar{g}(y)\\|_{*}^{2}+2\\|\\bar{g}(x)-\\nabla f(x)\\|_{*}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\le4L(\\beta_{f}^{\\nabla f(x)}(x,y)+\\delta)+4L\\delta=4L(\\beta_{f}^{\\nabla f(x)}(x,y)+2\\delta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "To prove (14), we proceed as follows using first (6), then (7), and then (11): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{f}^{\\nabla f(x)}(x,y)\\equiv f(y)-f(x)-\\langle\\nabla f(x),y-x\\rangle}\\\\ &{\\phantom{\\beta_{f}^{\\nabla f(x)}(x,y)\\equiv f(x)-f(x)-\\langle\\nabla f(x),y-x\\rangle+\\frac{L}{2}\\|y-x\\|^{2}+\\delta}}\\\\ &{\\phantom{\\beta_{f}^{\\nabla f(x)}(x,y)\\equiv f(x)-\\nabla f(x)-\\langle\\nabla f(x),y-x\\rangle+\\frac{L}{2}\\|y-x\\|^{2}+\\delta}}\\\\ &{\\phantom{\\beta_{f}^{\\nabla f(x)}(x,y)\\equiv f(x)-\\nabla f(x),y-x\\rangle+\\frac{L}{2}\\|y-x\\|^{2}+\\delta}}\\\\ &{\\phantom{\\beta_{f}^{\\nabla f(x)}(x,y)\\equiv f(x)-\\nabla f(x)+\\frac{L}{2}\\|y-x\\|^{2}+\\delta}}\\\\ &{\\phantom{\\beta_{f}^{\\nabla f(x)}(x,y)=f(x)+\\nabla\\delta}=\\Big(\\sqrt{\\frac{L}{2}}\\|y-x\\|+\\sqrt{\\delta}\\Big)^{2}\\leq L\\|y-x\\|^{2}+2\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the final inequality is $(a+b)^{2}\\leq2a^{2}+2b^{2},a,b\\geq0.$ ", "page_idx": 14}, {"type": "text", "text": "Remark 15. Some of the inequalities from Theorem 14, namely, (7), (10) and (12), were established already in [15]. We nevertheless prefer to present the corresponding proofs since they are rather simple, and we use the associated ideas for proving the other new inequalities. ", "page_idx": 14}, {"type": "text", "text": "Lemma 16. Let $f\\colon\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}$ be the function $f(x):=\\mathbb{E}_{\\xi}[f_{\\xi}(x)]$ , where each $f_{\\xi}\\colon\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}$ is convex and $(\\delta_{\\xi},L_{\\xi})$ -approximately smooth with components $(f_{\\xi},\\overline{{g}}_{\\xi})$ . Further, let $\\widehat g$ be the stochastic oracle defined by $g(x,\\xi):=\\bar{g}_{\\xi}(x),$ , and let $\\bar{f}(x):=\\mathbb{E}_{\\xi}[\\bar{f}_{\\xi}(x)]$ s, $\\overline{{g}}(x):=\\mathbb{E}_{\\xi}[\\overline{{g}}_{\\xi}(x)]$ . Then, $\\widehat g$ is an unbiased oracle forg and, for a sny x, y \u2208Rd,  Lsmax := sup \u03bes L\u03be a nsd\u03b4 :=Lm1ax s $\\begin{array}{r}{\\overline{{\\delta}}:=\\frac{1}{L_{\\mathrm{max}}}\\operatorname{\\mathbb{E}}_{\\xi}[L_{\\xi}\\delta_{\\xi}].}\\end{array}$ , it  hpolds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{Var}_{\\hat{g}}(x,y)\\leq2L_{\\mathrm{max}}[\\beta_{f,\\bar{f},\\bar{g}}(x,y)+\\overline{{\\delta}}].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Furthermore, for any $x,y\\in\\mathbb{R}^{d}$ and any $\\nabla f(x)\\in\\partial f(x)$ , it also h olds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{Var}_{\\hat{g}}(x,y)\\leq4L_{\\operatorname*{max}}[\\beta_{f}^{\\nabla f(x)}(x,y)+2\\overline{{\\delta}}].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. According to our definition of $\\overline{{g}}$ , we have $\\mathbb{E}_{\\xi}[\\overline{{g}}_{\\xi}(x)]\\,=\\,\\overline{{g}}(x)$ for any $x$ , so $\\widehat g$ is indeed an unbiased oracle for $\\overline{{g}}$ . Further, for any $x,y\\in\\mathbb{R}^{d}$ , we can estimate ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}_{\\hat{g}}(x,y)\\equiv\\mathbb{E}_{\\xi}\\big[\\|\\big[\\bar{g}_{\\xi}(x)-\\bar{g}_{\\xi}(y)\\big]-\\big[\\bar{g}(x)-\\bar{g}(y)\\big]\\|_{*}^{2}\\big]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{E}_{\\xi}\\big[\\|\\bar{g}_{\\xi}(x)-\\bar{g}_{\\xi}(y)\\|_{*}^{2}\\big]\\leq\\mathbb{E}_{\\xi}\\big[2L_{\\xi}\\big(\\beta_{f_{\\xi},\\bar{f}_{\\xi},\\bar{g}_{\\xi}}(x,y)+\\delta_{\\xi}\\big)\\big]}\\\\ &{\\qquad\\qquad\\leq2L_{\\operatorname*{max}}\\big(\\mathbb{E}_{\\xi}[\\beta_{f_{\\xi},\\bar{f}_{\\xi},\\bar{g}_{\\xi}}(x,y)]+\\bar{\\delta}\\big)=2L_{\\operatorname*{max}}[\\beta_{f,\\bar{f},\\bar{g}}(x,y)+\\bar{\\delta}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\overline{{\\delta}}$ is as defined in the statement; the second ineq uality follows from Theore m 14 (inequality (10 )s), and the final identity is due to the linearity of $\\beta_{f,\\bar{f},\\bar{g}}(x,y)$ in $(f,\\bar{f},\\bar{g})$ and the fact that, by our definitions, $\\mathbb{E}_{\\xi}[f_{\\xi}(x)]=f(x),\\mathbb{E}_{\\xi}[\\bar{f}_{\\xi}(x)]=\\bar{f}(x),\\mathbb{E}_{\\xi}[\\bar{g}_{\\xi}(x)]=\\bar{g}(x)$ fo rs  asny $x$ . This proves (15). ", "page_idx": 14}, {"type": "text", "text": "The proof of (16) is similar but now w es apply (1 3s) insteads of (10): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}_{\\hat{g}}(x,y)\\leq\\mathbb{E}_{\\xi}\\big[\\|\\bar{g}_{\\xi}(x)-\\bar{g}_{\\xi}(y)\\|_{*}^{2}\\big]\\leq\\mathbb{E}_{\\xi}\\big[4L_{\\xi}\\big(\\beta_{f_{\\xi}}^{\\nabla f_{\\xi}(x)}(x,y)+2\\delta_{\\xi}\\big)\\big]}\\\\ &{\\qquad\\qquad\\qquad\\leq4L_{\\operatorname*{max}}\\big(\\mathbb{E}_{\\xi}[\\beta_{f_{\\xi}}^{\\nabla f_{\\xi}(x)}(x,y)]+2\\bar{\\delta}\\big)=4L_{\\operatorname*{max}}[\\beta_{f}^{\\nabla f(x)}(x,y)+2\\bar{\\delta}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we have used the fact that $\\partial f(x)=\\mathbb{E}_{\\xi}[\\partial f_{\\xi}(x)]$ (see Proposition 2.2 in [4]), meaning that, for any $\\nabla f(x)\\in\\partial f(x)$ , we can find a selection of $\\bar{\\nabla}f_{\\xi}\\bar{(x)}\\in\\partial f_{\\xi}(x)$ such that $\\nabla f(x)=\\mathbb{E}_{\\xi}\\bar{[}\\nabla f_{\\xi}(x)]$ . ", "page_idx": 14}, {"type": "text", "text": "A.2 Miscellaneous ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 17. Let $\\psi\\colon\\mathbb{R}^{d}\\rightarrow\\mathbb{R}\\cup\\{+\\infty\\}$ be a proper closed convex function, $x\\in\\mathrm{dom}\\,\\psi,\\,g\\in\\mathbb{R}^{d}$ , $M\\ge0,$ , and let ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{+}:=\\mathrm{Prox}_{\\psi}(x,g,M).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, for any $y\\in\\dim\\psi$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\langle g,y-x_{+}\\rangle+\\psi(y)+\\frac{M}{2}\\|x-y\\|^{2}\\geq\\psi(x_{+})+\\frac{M}{2}\\|x-x_{+}\\|^{2}+\\frac{M}{2}\\|x_{+}-y\\|^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Indeed, by definition, $x_{+}$ is the minimizer of the function $h\\colon\\mathbb{R}^{d}\\rightarrow\\mathbb{R}\\cup\\{+\\infty\\}$ given by $\\begin{array}{r}{h(y):=\\langle g,y\\rangle+\\psi(y)+\\frac{M}{2}\\|x-y\\|^{2}}\\end{array}$ , which is strongly convex with parameter $M$ (or simply convex if $M=0$ ). Hence, for any $y\\in\\dim\\psi$ $(=\\dim h)$ ), we have $\\begin{array}{r}{h(y)\\geq h(x_{+})+\\frac{M}{2}\\|y-x_{+}\\|^{2}}\\end{array}$ , which is exactly the claimed inequality. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma 18. Let $N\\geq1$ be an integer, $(M_{k})_{k=0}^{N}$ be a nondecreasing nonnegative sequence of reals, and let $\\overline{{M}}\\geq0$ . Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{N-1}[\\operatorname*{min}\\{M_{k+1},\\overline{{M}}\\}-M_{k}]_{+}=[\\operatorname*{min}\\{M_{N},\\overline{{M}}\\}-M_{0}]_{+}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. It suffices to prove the identity only in the special case when $N=2$ , i.e., to show that $\\gamma_{0}{+}\\gamma_{1}=$ $\\Gamma$ , where $\\mathfrak{o}:=[\\operatorname*{min}\\{M_{1},\\overline{{M}}\\}-M_{0}^{\\bullet}]_{+},\\dot{\\gamma}_{1}:=[\\operatorname*{min}\\{M_{2},\\overline{{M}}\\}-M_{1}]_{+},\\Gamma:=[\\operatorname*{min}\\{M_{2},\\overline{{M}}\\}-\\dot{M_{0}}]_{+}$ . The general case then easi ly\u010e follows by induction. ", "page_idx": 15}, {"type": "text", "text": "To prove the identity, we use our assumption that $M_{0}\\,\\leq\\,\\underline{{M_{1}}}\\,\\leq\\,M_{2}$ and consider three possible cases. If $M_{1}\\geq\\overline{{M}}$ , then $\\gamma_{0}+\\gamma_{\\underline{{{1}}}}=[\\overline{{{M}}}-M_{\\underline{{{0}}}}]_{+}+0=[\\overline{{{M}}}-M_{0}]_{+}=\\Gamma$ . If $M_{1}\\,<\\,\\overrightarrow{M}\\,\\leq\\,M_{2}$ , then \u03b30 + $\\gamma_{1}=\\left(M_{1}-M_{0}\\right)+\\left(\\overline{{M}}-M_{1}\\right)=\\overline{{M}}-M_{0}=\\Gamma$ .\u010e Finally, if $M_{2}<\\overline{{M}}$ , then $\\gamma_{0}+\\gamma_{1}=$ $\\left(M_{1}-M_{0}\\right)+\\left(M_{2}-M_{1}\\right)=M_{2}-M_{0}=\\Gamma$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma 19. Let $\\widehat g$ be a stochastic oracle in $\\mathbb{R}^{d}$ . Then, for any $x,y,z\\in\\mathbb{R}^{d}$ and any $\\tau>0$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{Var}_{\\hat{g}}(x)\\leq\\left(1+\\tau\\right)\\operatorname{Var}_{\\hat{g}}(y)+\\left(1+\\tau^{-1}\\right)\\operatorname{Var}_{\\hat{g}}(x,y),\\quad}\\\\ {\\operatorname{Var}_{\\hat{g}}(x,y)\\leq\\left(1+\\tau\\right)\\operatorname{Var}_{\\hat{g}}(x,z)+\\left(1+\\tau^{-1}\\right)\\operatorname{Var}_{\\hat{g}}(y,z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Both inequalities are direct consequences of the standard inequality $\\lVert s_{1}+s_{2}\\rVert_{*}^{2}\\,\\leq\\,(1+$ $\\tau)\\|s_{1}\\|_{*}^{2}+(1+\\tau^{-1})\\|s_{2}\\|_{*}^{2}$ which is valid for any $s_{1},s_{2}\\,\\in\\,\\mathbb{R}^{d}$ and any $\\tau\\ >\\ 0$ . Indeed, let $g$ and $\\xi$ be, respectively, the function and the random variable components of $\\widehat g$ , and let $\\Delta(x,\\xi):=$ $g(x,\\xi)-\\mathbb{E}[g(x,\\xi)]$ for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ . Then, for any $x,y,z\\in\\mathbb{R}^{d}$ and $\\tau>0$ , we can estimate ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}_{\\hat{g}}(x)\\equiv\\mathbb{E}[\\|\\Delta(x,\\xi)\\|_{*}^{2}]=\\mathbb{E}[\\|\\Delta(y,\\xi)+[\\Delta(x,\\xi)-\\Delta(y,\\xi)]\\|_{*}^{2}]}\\\\ &{\\qquad\\qquad\\leq(1+\\tau)\\,\\mathbb{E}[\\|\\Delta(y,\\xi)\\|_{*}^{2}]+(1+\\tau^{-1})\\,\\mathbb{E}[\\|\\Delta(x,\\xi)-\\Delta(y,\\xi)\\|_{*}^{2}]}\\\\ &{\\qquad\\qquad\\equiv(1+\\tau)\\,\\mathrm{Var}_{\\hat{g}}(y)+(1+\\tau^{-1})\\,\\mathrm{Var}_{\\hat{g}}(x,y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}_{\\hat{g}}(x,y)\\equiv\\mathbb{E}\\big[\\|\\Delta(x,\\xi)-\\Delta(y,\\xi)\\|_{*}^{2}\\big]=\\mathbb{E}\\big[\\|[\\Delta(x,\\xi)-\\Delta(z,\\xi)]-[\\Delta(y,\\xi)-\\Delta(z,\\xi)]\\|_{*}^{2}\\big]}\\\\ &{\\qquad\\qquad\\leq(1+\\tau)\\,\\mathbb{E}\\big[\\|\\Delta(x,\\xi)-\\Delta(z,\\xi)\\|_{*}^{2}\\big]+(1+\\tau^{-1})\\,\\mathbb{E}\\big[\\|\\Delta(y,\\xi)-\\Delta(z,\\xi)\\|_{*}^{2}\\big]}\\\\ &{\\qquad\\qquad\\equiv(1+\\tau)\\,\\mathrm{Var}_{\\hat{g}}(x,z)+(1+\\tau^{-1})\\,\\mathrm{Var}_{\\hat{g}}(y,z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B Omitted Proofs for Section 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 20 (AdaGrad stepsize). Let function $f$ satisfy Assumption 1. Consider the stepsize update rule $\\widehat{M}_{+}=M_{+}(M,\\Omega,x,\\stackrel{-}{x}_{+},\\widehat{g}_{x},\\widehat{g}_{x_{+}})$ defined by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{M}_{+}:=\\sqrt{M^{2}+\\frac{1}{\\Omega}\\|\\widehat{g}_{x_{+}}-\\widehat{g}_{x}\\|_{*}^{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, this stepsize update rules satisfies (2) with $\\begin{array}{r}{c_{1}=\\frac{5}{2}}\\end{array}$ , $c_{2}=4$ , $c_{3}=6$ , $c_{4}=2$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{\\widehat{\\Delta}(\\overline{{M}}):=\\beta_{f,\\bar{f},\\bar{g}}(x,\\widehat{x}_{+})+\\langle\\bar{g}(x)-\\widehat{g}_{x},\\widehat{x}_{+}-x\\rangle-\\frac{\\overline{{M}}}{2}\\|\\widehat{x}_{+}-x\\|^{2}}\\end{array}$ . From our Assumption 1 and Theore mp 14\u010e (inequali tsy s(8)) , pit followss that $\\beta_{f,\\bar{f},\\bar{g}}(x,\\widehat{x}_{+})+\\beta_{f,\\bar{f},\\bar{g}}(\\widehat{x}_{+},x)\\leq\\langle\\overline{{{g}}}(\\widehat{x}_{+})\\!-\\!\\overline{{{g}}}(x),\\widehat{x}_{+}-$ ${\\boldsymbol{x}}\\rangle+2\\delta_{f}$ . Hence, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{S}[\\widehat{\\Delta}(\\widehat{M}_{+})+\\beta_{f,\\bar{f},\\bar{g}}(\\widehat{x}_{+},x)]\\leq\\mathbb{E}\\Big[\\langle\\bar{g}(\\widehat{x}_{+})-\\widehat{g}_{x},\\widehat{x}_{+}-x\\rangle-\\frac{\\widehat{M}_{+}}{2}\\|\\widehat{x}_{+}-x\\|^{2}\\Big]+2\\delta_{f}=\\mathbb{E}[\\widehat{\\Delta}_{1}(\\widehat{M}_{+})]+2\\delta_{f},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{\\widehat{\\Delta}_{1}(\\widehat{M}_{+}):=\\langle\\widehat{g}_{x_{+}}-\\widehat{g}_{x},\\widehat{x}_{+}-x\\rangle-\\frac{\\widehat{M}_{+}}{2}\\|\\widehat{x}_{+}-x\\|^{2}}\\end{array}$ . Hence, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Gamma:=\\mathbb{E}[\\widehat{\\Delta}(\\widehat{M}_{+})+(\\widehat{M}_{+}-M)\\Omega+\\beta_{f,\\bar{f},\\bar{g}}(\\widehat{x}_{+},x)]\\leq\\mathbb{E}[\\widehat{\\Delta}_{1}(\\widehat{M}_{+})+(\\widehat{M}_{+}-M)\\Omega]+2\\delta_{f}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From the definition of $\\widehat{M}_{+}$ , it follows that $\\lVert\\widehat{g}_{x_{+}}-\\widehat{g}_{x}\\rVert_{*}^{2}=(\\widehat{M}_{+}^{2}-M^{2})\\Omega=(\\widehat{M}_{+}+M)(\\widehat{M}_{+}-M)\\Omega.$ Since $\\widehat{M}_{+}\\geq M$ , this  mxeans that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{2\\widehat{M}_{+}}\\|\\widehat{g}_{x_{+}}-\\widehat{g}_{x}\\|_{*}^{2}\\leq(\\widehat{M}_{+}-M)\\Omega\\leq\\frac{1}{\\widehat{M}_{+}}\\|\\widehat{g}_{x_{+}}-\\widehat{g}_{x}\\|_{*}^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let us now upper bounxd $\\widehat{\\Gamma}:=\\widehat{\\Delta}_{1}(\\widehat{M}_{+})+(\\widehat{M}_{+}-M)\\Omega$ . Foxr this, let us fix an arbitrary constant $\\overline{{M}}\\geq0$ and consider two  cpases.  pIf $\\widehat{M}_{+}\\geq\\overline{{M}}$ , xwe can bound ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{\\Gamma}\\leq\\widehat{\\Delta}_{1}(\\widehat{M}_{+})+\\frac{1}{\\widehat{M}_{+}}\\|\\widehat{g}_{x_{+}}-\\widehat{g}_{x}\\|_{*}^{2}\\leq\\widehat{\\Delta}_{1}(\\overline{{M}})+\\frac{1}{\\overline{{M}}}\\|\\widehat{g}_{x_{+}}-\\widehat{g}_{x}\\|_{*}^{2}=:\\widehat{\\Gamma}(\\overline{{M}}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If $\\widehat{M}_{+}\\leq\\overline{{M}}$ , we can bound ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{\\Gamma}\\leq\\frac{1}{2\\widehat{M}_{+}}\\|\\widehat{g}_{x_{+}}-\\widehat{g}_{x}\\|_{*}^{2}+(\\widehat{M}_{+}-M)\\Omega\\leq2(\\widehat{M}_{+}-M)\\Omega=2[\\operatorname*{min}\\{\\widehat{M}_{+},\\overline{{M}}\\}-M]_{+}\\Omega.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining txhe two cases, we get $\\widehat{\\Gamma}\\leq[\\widehat{\\Gamma}(\\overline{{M}})]_{+}+2[\\operatorname*{min}\\{\\widehat{M}_{+},\\overline{{M}}\\}-M]_{+}\\Omega.$ . Thus, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Gamma\\leq\\mathbb{E}[\\widehat{\\Gamma}]+2\\delta_{f}\\leq\\mathbb{E}\\big\\{[\\widehat{\\Gamma}(\\overline{{M}})]_{+}\\big\\}+2\\,\\mathbb{E}\\big\\{[\\operatorname*{min}\\{\\widehat{M}_{+},\\overline{{M}}\\}-M]_{+}\\Omega\\big\\}+2\\delta_{f}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let us now estimate the first term. Denote ${\\widehat{S}}:={\\widehat{g}}_{x}-{\\overline{{g}}}(x)$ and $\\widehat{S}_{+}:=\\widehat{g}_{x_{+}}-\\overline{{g}}(\\widehat{x}_{+})$ . Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\widehat\\Gamma(\\overline{{M}})\\equiv\\langle\\widehat g_{x_{+}}-\\widehat g_{x},\\widehat x_{+}-x\\rangle-\\displaystyle\\frac{\\overline{{M}}}{2}\\|\\widehat x_{+}-x\\|^{2}+\\displaystyle\\frac{1}{\\overline{{M}}}\\|\\widehat g_{x_{+}}-\\widehat g_{x}\\|_{*}^{2}}\\\\ {\\leq\\langle\\bar{g}(\\widehat x_{+})-\\bar{g}(x),\\widehat x_{+}-x\\rangle+\\displaystyle\\frac{2}{\\overline{{M}}}\\|\\bar{g}(\\widehat x_{+})-\\bar{g}(x)\\|_{*}^{2}}\\\\ {\\displaystyle\\qquad+\\,\\langle\\widehat S_{+}-\\widehat S,\\widehat x_{+}-x\\rangle+\\displaystyle\\frac{2}{\\overline{{M}}}\\|\\widehat S_{+}-\\widehat S\\|_{*}^{2}-\\displaystyle\\frac{\\overline{{M}}}{2}\\|\\widehat x_{+}-x\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using now our Assumption 1 and Theor epm 14 (inequa\u010elities (9) and (12)), wpe can continue as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle^\\mathrm{\\hat{\\tau}}(\\overline{{M}})\\leq L_{f}\\|\\hat{x}_{+}-x\\|^{2}+2\\delta_{f}+\\frac{2}{M}(L_{f}^{2}\\|\\hat{x}_{+}-x\\|^{2}+4L_{f}\\delta_{f})}\\\\ {\\displaystyle\\quad\\quad+\\,\\hat{\\langle S_{+}-\\hat{S},\\hat{x}_{+}-x\\rangle}+\\frac{2}{M}\\|\\hat{S}_{+}-\\hat{S}\\|_{*}^{2}-\\frac{\\overline{{M}}}{2}\\|\\hat{x}_{+}-x\\|^{2}}\\\\ {\\displaystyle\\leq\\hat{\\langle S_{+}-\\hat{S},\\hat{x}_{+}-x\\rangle}+\\frac{2}{M}\\|\\hat{S}_{+}-\\hat{S}\\|_{*}^{2}-\\frac{\\overline{{M}}-2L_{f}(1+\\frac{2L_{f}}{M})}{2}\\|\\hat{x}_{+}-x\\|^{2}+2\\Big(1+\\frac{4L_{f}}{\\overline{{M}}}\\Big)\\delta_{f}}\\\\ {\\displaystyle\\leq\\Big(\\frac{2}{M}+\\frac{1}{2[\\overline{{M}}-2L_{f}(1+\\frac{2L_{f}}{M})]}\\Big)\\|\\hat{S}_{+}-\\hat{S}\\|_{*}^{2}+2\\Big(1+\\frac{4L_{f}}{\\overline{{M}}}\\Big)\\delta_{f}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Consequentl\u010ey, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\{[\\hat{\\Gamma}(\\overline{{M}})]_{+}\\}\\leq\\left(\\frac{2}{\\overline{{M}}}+\\frac{1}{2[\\overline{{M}}-2L_{f}(1+\\frac{2L_{f}}{M})]}\\right)\\mathbb{E}[\\|\\hat{S}_{+}-\\hat{S}\\|_{*}^{2}]+2\\Big(1+\\frac{4L_{f}}{\\overline{{M}}}\\Big)\\delta_{f}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In particular, for $\\overline{{M}}>4L_{f}$ , \u010ewe can e\u010estimate $\\begin{array}{r}{\\frac{2}{M}\\!+\\!\\frac{1}{2[\\overline{{M}}-2L_{f}(1+\\frac{2L_{f}}{M})]}\\leq\\frac{2}{M}\\!+\\!\\frac{1}{2(\\overline{{M}}-4L_{f})}\\leq\\frac{5}{2(\\overline{{M}}-4L_{f})}.}\\end{array}$ Therefore, for a ny\u010e $\\overline{{M}}>4L_{f}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathsf{z}\\{[\\hat{\\Gamma}(\\overline{{M}})]_{+}\\}\\leq\\frac{5}{2(\\overline{{M}}-4L_{f})}\\mathbb{E}[\\|\\hat{S}_{+}-\\hat{S}\\|_{*}^{2}]+4\\delta_{f}=\\frac{5}{2(\\overline{{M}}-4L_{f})}\\mathbb{E}[\\mathrm{Var}_{\\hat{g}}(\\hat{x}_{+})+\\mathrm{Var}_{\\hat{g}}(x)]+4\\delta_{f},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the final iden\u010etity follows from the fact that $\\mathbb{E}[\\|\\widehat{S}_{+}\\,-\\,\\widehat{S}\\|_{*}^{2}]\\;=\\;\\mathbb{E}[\\|\\widehat{S}_{+}\\|_{*}^{2}]\\,+\\,\\mathbb{E}[\\|\\widehat{S}\\|_{*}^{2}]\\;=$ $\\mathbb{E}[\\mathrm{Var}_{\\hat{g}}(\\hat{x}_{+})]+\\mathrm{Var}_{\\hat{g}}(x)$ (because ${\\widehat S}_{+}$ , conditioned on thpe randpomness $\\xi$ depfining $\\hat{g}_{x}\\,\\equiv\\,g(x,\\xi)$ , has zerpo mean). ", "page_idx": 16}, {"type": "text", "text": "Combining everything together, we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Gamma\\leq\\frac{5}{2(\\overline{{M}}-4L_{f})}\\mathbb{E}[\\mathrm{Var}_{\\widehat{g}}(\\widehat{x}_{+})+\\mathrm{Var}_{\\widehat{g}}(x)]+6\\delta_{f}+2\\mathbb{E}\\big\\{\\big[\\operatorname*{min}\\{\\widehat{M}_{+},\\overline{{M}}\\}-M\\big]_{+}\\Omega\\big\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This is exactly (2) with $\\begin{array}{r}{c_{1}={\\frac{5}{2}}}\\end{array}$ , $c_{2}=4$ , $c_{3}=6$ , $c_{4}=2$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma 21. Let function $f$ satisfy Assumption $^{\\,l}$ . Consider the stepsize update rule $\\widehat{M}_{+}~=~$ $M_{+}(M,\\Omega,x,\\widehat{x}_{+},\\widehat{g}_{x},\\widehat{g}_{x_{+}})$ defined as the solution of the following equation: ", "page_idx": 17}, {"type": "equation", "text": "$$\n(\\widehat{M}_{+}-M)\\Omega=[\\widehat{\\Delta}_{1}(\\widehat{M}_{+})]_{+},\\qquad\\widehat{\\Delta}_{1}(\\widehat{M}_{+}):=\\langle\\widehat{g}_{x_{+}}-\\widehat{g}_{x},\\widehat{x}_{+}-x\\rangle-\\frac{\\widehat{M}_{+}}{2}\\|\\widehat{x}_{+}-x\\|^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, this stepsize update rules satisfies (2) with $c_{1}=1$ , $c_{2}=2$ , $c_{3}=6$ , $c_{4}=2$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Let us define $\\begin{array}{r}{\\hat{\\Delta}(\\overline{{M}}):=\\beta_{f,\\bar{f},\\bar{g}}(x,\\hat{x}_{+})+\\langle\\bar{g}(x)-\\hat{g}_{x},\\hat{x}_{+}-x\\rangle-\\frac{\\overline{{M}}}{2}\\|\\hat{x}_{+}-x\\|^{2}.}\\end{array}$ . Starting as in the proof of Lemma  2p0, \u010ewe see tha ts ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Gamma:=\\mathbb{E}[\\widehat{\\Delta}(\\widehat{M}_{+})+(\\widehat{M}_{+}-M)\\Omega+\\beta_{f,\\bar{f},\\bar{g}}(\\widehat{x}_{+},x)]\\leq\\mathbb{E}[\\widehat{\\Delta}_{1}(\\widehat{M}_{+})+(\\widehat{M}_{+}-M)\\Omega]+2\\delta_{f},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with the same $\\widehat{\\Delta}_{1}(\\cdot)$ as defined in the statement. ", "page_idx": 17}, {"type": "text", "text": "Let us now up pper bound $\\widehat{\\Gamma}:=\\widehat{\\Delta}_{1}(\\widehat{M}_{+})+(\\widehat{M}_{+}-M)\\Omega$ . For this, let us fix an arbitrary constant $\\overline{{M}}\\geq0$ and consider two  cpases.  pIf $\\widehat{M}_{+}\\geq\\overline{{M}}$ , xwe can bound, using the monotonicity of $\\widehat{\\Delta}_{1}(\\cdot)$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{\\Gamma}=\\widehat{\\Delta}_{1}(\\widehat{M}_{+})+[\\widehat{\\Delta}_{1}(\\widehat{M}_{+})]_{+}\\leq\\widehat{\\Delta}_{1}(\\overline{{M}})+[\\widehat{\\Delta}_{1}(\\overline{{M}})]_{+}\\leq2[\\widehat{\\Delta}_{1}(\\overline{{M}})]_{+}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If $\\widehat{M}_{+}\\leq\\overline{{M}}$ , we can bound ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{\\Gamma}\\leq[\\widehat{\\Delta}_{1}(\\widehat{M}_{+})]_{+}+(\\widehat{M}_{+}-M)\\Omega=2(\\widehat{M}_{+}-M)\\Omega=2[\\operatorname*{min}\\{\\widehat{M}_{+},\\overline{{{M}}}\\}-M]_{+}\\Omega.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining the two cases, we get $\\widehat{\\Gamma}\\leq2[\\widehat{\\Delta}_{1}(\\overline{{{M}}})]_{+}+2[\\operatorname*{min}\\{\\widehat{M}_{+},\\overline{{{M}}}\\}-M]_{+}\\Omega,$ and hence ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Gamma\\leq\\mathbb{E}[\\widehat{\\Gamma}]+2\\delta_{f}\\leq2\\,\\mathbb{E}\\big\\{[\\widehat{\\Delta}_{1}(\\overline{{M}})]_{+}\\big\\}+2\\,\\mathbb{E}\\big\\{[\\operatorname*{min}\\{\\widehat{M}_{+},\\overline{{M}}\\}-M]_{+}\\Omega\\big\\}+2\\delta_{f}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let us now estimate the first term. According to our Assumption 1 and Theorem 14 (inequality (9)), we have $\\langle\\overline{{g}}(\\widehat{x}_{+})-\\overline{{g}}(x),\\widehat{x}_{+}-x\\rangle\\leq L_{f}\\|\\widehat{x}_{+}-x\\|^{2}+2\\delta_{f}$ . Hence, denoting ${\\widehat{S}}:={\\widehat{g}}_{x}-{\\overline{{g}}}(x)$ and $\\widehat{S}_{+}:=\\widehat{g}_{x_{+}}-\\overline{{g}}(\\widehat{x}_{+})$ s, we  cpan estimate, for apny $\\overline{{M}}>2L_{f}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\widehat\\Delta_{1}(\\overline{{M}})=\\langle\\bar{g}(\\widehat x_{+})-\\bar{g}(x),\\widehat x_{+}-x\\rangle+\\langle\\hat{S}_{+}-\\hat{S},\\hat{x}_{+}-x\\rangle-\\frac{M}{2}\\|\\hat{x}_{+}-x\\|^{2}}\\\\ {\\displaystyle\\leq\\langle\\hat{S}_{+}-\\hat{S},\\hat{x}_{+}-x\\rangle-\\frac{\\overline{{M}}-2L_{f}}{2}\\|\\hat{x}_{+}-x\\|^{2}+2\\delta_{f}\\leq\\frac{1}{2(\\overline{{M}}-2L_{f})}\\|\\hat{S}_{+}-\\hat{S}\\|_{*}^{2}+2\\delta_{f}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathfrak{C}\\{[\\hat{\\Delta}_{1}(\\overline{{M}})]_{+}\\}\\le\\frac{1}{2(\\overline{{M}}-2L_{f})}\\mathbb{E}[\\|\\hat{S}_{+}-\\hat{S}\\|_{*}^{2}]+2\\delta_{f}=\\frac{1}{2(\\overline{{M}}-2L_{f})}\\mathbb{E}[\\mathrm{Var}_{\\hat{g}}(\\hat{x}_{+})+\\mathrm{Var}_{\\hat{g}}(x)]+2\\delta_{f},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the final identity follows from the fact that $\\mathbb{E}[\\|\\widehat{S}_{+}\\,-\\,\\widehat{S}\\|_{*}^{2}]\\;=\\;\\mathbb{E}[\\|\\widehat{S}_{+}\\|_{*}^{2}]\\,+\\,\\mathbb{E}[\\|\\widehat{S}\\|_{*}^{2}]\\;=$ $\\mathbb{E}[\\mathrm{Var}_{\\hat{g}}(\\hat{x}_{+})]+\\mathrm{Var}_{\\hat{g}}(x)$ (because ${\\widehat S}_{+}$ , conditioned on thpe randpomness $\\xi$ depfining $\\hat{g}_{x}\\,\\equiv\\,g(x,\\xi)$ , has zerpo mean). ", "page_idx": 17}, {"type": "text", "text": "Thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Gamma\\leq\\frac{1}{\\overline{{M}}-2L_{f}}\\mathbb{E}[\\mathrm{Var}_{\\widehat{g}}(\\widehat{x}_{+})+\\mathrm{Var}_{\\widehat{g}}(x)]+6\\delta_{f}+2\\,\\mathbb{E}\\big\\{\\big[\\mathrm{min}\\{\\widehat{M}_{+},\\overline{{M}}\\}-M\\}_{+}\\Omega\\big\\},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is exactly (2) with $c_{1}=1$ , $c_{2}=2$ , $c_{3}=6$ , $c_{4}=2$ . ", "page_idx": 17}, {"type": "text", "text": "C Omitted Proofs for Section 4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Universal SGD ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma 22 (Stochastic Gradient Step). Consider problem (1) under Assumption $^{\\,l}$ . Let $\\widehat g$ be an unbiased oracle for $\\overline{{g}}$ . Let $x\\in\\operatorname{dom}\\psi$ be a point, $M\\geq0$ be a coefficient, ${\\widehat{g}}_{x}\\cong{\\widehat{g}}(x)$ , and let ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{x}_{+}=\\mathrm{Prox}_{\\psi}(x,\\hat{g}_{x},M).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Denote $\\begin{array}{r}{\\widehat{\\Delta}(M):=\\beta_{f,\\bar{f},\\bar{g}}(x,\\widehat{x}_{+})+\\langle\\bar{g}(x)-\\widehat{g}_{x},\\widehat{x}_{+}-x\\rangle-\\frac{M}{2}\\|\\widehat{x}_{+}-x\\|^{2}}\\end{array}$ . Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[F(\\widehat{x}_{+})-F^{*}+\\frac{M}{2}\\|\\widehat{x}_{+}-x^{*}\\|^{2}\\Big]+\\beta_{f,\\bar{f},\\bar{g}}(x,x^{*})\\leq\\frac{M}{2}\\|x-x^{*}\\|^{2}+\\mathbb{E}[\\widehat{\\Delta}(M)].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If further Assumption 2 is satisfied, and $\\widehat{M}_{+}\\geq M$ is a random coefficient (possibly dependent on $\\widehat{g}_{x}$ ), then, we also have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\natural\\Big[F(\\widehat{x}_{+})-F^{*}+\\frac{\\widehat{M}_{+}}{2}\\|\\widehat{x}_{+}-x^{*}\\|^{2}\\Big]+\\beta_{f,\\bar{f},\\bar{g}}(x,x^{*})\\leq\\frac{M}{2}\\|x-x^{*}\\|^{2}+\\mathbb{E}\\big[\\widehat{\\Delta}(\\widehat{M}_{+})+(\\widehat{M}_{+}-M)D^{2}\\big].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. From Lemma 17, it follows that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\bar{f}(x)+\\langle\\widehat{g}_{x},\\widehat{x}_{+}-x\\rangle+\\psi(\\widehat{x}_{+})+\\frac{M}{2}\\|\\widehat{x}_{+}-x^{*}\\|^{2}+\\frac{M}{2}\\|\\widehat{x}_{+}-x\\|^{2}}\\\\ {\\displaystyle\\leq\\bar{f}(x)+\\langle\\widehat{g}_{x},x^{*}-x\\rangle+\\psi(x^{*})+\\frac{M}{2}\\|x-x^{*}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Passing to expectations and rewriting ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\bar{f}(x)+\\langle\\hat{g}_{x},x^{*}-x\\rangle+\\psi(x^{*})]=\\bar{f}(x)+\\langle\\bar{g}(x),x^{*}-x\\rangle+\\psi(x^{*})=F(x^{*})-\\beta_{f,\\bar{f},\\bar{g}}(x,x^{*}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{f}(x)+\\langle\\widehat{g}_{x},\\widehat{x}_{+}-x\\rangle+\\psi(\\widehat{x}_{+})=F(\\widehat{x}_{+})-\\left[f(\\widehat{x}_{+})-\\bar{f}(x)-\\langle\\widehat{g}_{x},\\widehat{x}_{+}-x\\rangle\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=F(\\widehat{x}_{+})-\\left[\\beta_{f,\\bar{f},\\bar{g}}(x,\\widehat{x}_{+})+\\langle\\bar{g}(x)-\\widehat{g}_{x},\\widehat{x}_{+}-x\\rangle\\right]\\!,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "we obtain the first of the claimed inequalities. ", "page_idx": 18}, {"type": "text", "text": "To prove the second one, we simply add to both sides of the already proved first inequality the expected value of ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\widehat M_{+}-M}{2}\\|\\widehat x_{+}\\,-\\,x^{*}\\|^{2}\\,+\\,\\widehat\\Delta(M)\\,-\\,\\widehat\\Delta(\\widehat M_{+})\\ =\\ \\frac{\\widehat M_{+}-M}{2}\\bigl(\\|\\widehat x_{+}\\,-\\,x^{*}\\|^{2}\\,+\\,\\|\\widehat x_{+}\\,-\\,x\\|^{2}\\bigr)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and then bound $\\|\\widehat{x}_{+}\\,-\\,x^{*}\\|\\;\\leq\\;D,\\;\\|\\widehat{x}_{+}\\,-\\,x\\|\\;\\leq\\;D$ using our Assumption 2 and the fact that $x,\\hat{x}_{+},x^{*}\\in\\mathrm{dom}\\,\\psi$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Le pmma 23 (Universal Stochastic Gradient Step). Consider problem (1) under Assumptions $^{\\,l}$ and 2. Let $\\widehat g$ be an unbiased oracle for $\\overline{{g}}$ . Further, let $x\\,\\in\\,\\operatorname{dom}\\psi$ be a point, $M\\ge0$ be a coefficient, ${\\widehat{g}}_{x}\\cong{\\widehat{g}}(x),$ , and let ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{x}_{+}=\\operatorname{Prox}_{\\psi}(x,\\widehat{g}_{x},M),\\quad\\widehat{g}_{x_{+}}\\cong\\widehat{g}(\\widehat{x}_{+}),\\quad\\widehat{M}_{+}=M_{+}(M,D^{2},x,\\widehat{x}_{+},\\widehat{g}_{x},\\widehat{g}_{x_{+}}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, for pany $\\overline{{M}}>c_{2}L_{f}$ ,  pit holds thapt ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\Big[F(\\widehat{x}_{+})-F^{*}+\\frac{\\widehat{M}_{+}}{2}\\|\\widehat{x}_{+}-x^{*}\\|^{2}+\\beta_{f,\\bar{f},\\bar{g}}(\\widehat{x}_{+},x)\\Big]+\\beta_{f,\\bar{f},\\bar{g}}(x,x^{*})}\\\\ &{\\leq\\frac{M}{2}\\|x-x^{*}\\|^{2}\\!+\\!\\frac{c_{1}}{\\widehat{M}-c_{2}L_{f}}\\mathbb{E}[\\mathrm{Var}_{\\hat{g}}(\\widehat{x}_{+})\\!+\\!\\mathrm{Var}_{\\hat{g}}(x)]\\!+\\!c_{3}\\delta_{f}\\!+\\!c_{4}\\,\\mathbb{E}\\big\\{\\![\\mathrm{min}\\{\\widehat{M}_{+},\\overline{{M}}\\}\\!-\\!M]_{+}D^{2}\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. According to Lemma 22, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\natural\\Big[F(\\widehat{x}_{+})-F^{*}+\\frac{\\widehat{M}_{+}}{2}\\|\\widehat{x}_{+}-x^{*}\\|^{2}\\Big]+\\beta_{f,\\bar{f},\\bar{g}}(x,x^{*})\\leq\\frac{M}{2}\\|x-x^{*}\\|^{2}+\\mathbb{E}\\big[\\widehat{\\Delta}(\\widehat{M}_{+})+(\\widehat{M}_{+}-M)D^{2}\\big],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{\\widehat{\\Delta}(\\widehat{M}_{+}):=\\beta_{f,\\bar{f},\\bar{g}}(x,\\widehat{x}_{+})+\\langle\\bar{g}(x)-\\widehat{g}_{x},\\widehat{x}_{+}-x\\rangle-\\frac{\\widehat{M}_{+}}{2}\\|\\widehat{x}_{+}-x\\|^{2}}\\end{array}$ . At the same time, according to the  mpainx requireme snts (2)  pon the sstepsizep up dpate rule, for anpy $\\overline{{M}}>c_{2}L_{f}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\big[\\widehat{\\Delta}(\\widehat{M}_{+})+(\\widehat{M}_{+}-M)D^{2}+\\beta_{f,\\overline{{f}},\\overline{{g}}}(\\widehat{x}_{+},x)\\big]}}\\\\ &{}&{\\leq\\frac{c_{1}}{M-c_{2}L_{f}}\\,\\mathbb{E}\\big[\\mathrm{Var}_{\\widehat{g}}(\\widehat{x}_{+})+\\mathrm{Var}_{\\widehat{g}}(x)\\big]+c_{3}\\delta_{f}+c_{4}\\,\\mathbb{E}\\big\\{\\big[\\operatorname*{min}\\{\\widehat{M}_{+},\\overline{{M}}\\}-M\\big]_{+}D^{2}\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining the t\u010ewo displays, we get the claim. ", "page_idx": 19}, {"type": "text", "text": "Lemma 24 (Universal SGD: General Guarantee). Consider problem (1) under Assumptions $^{\\,I}$ and 2. Let $\\widehat g$ be an unbiased oracle for $\\overline{{g}}$ . Further, let $x\\in\\operatorname{dom}\\psi$ be a point, $M\\geq0$ be a coefficient, $N\\geq1$ be an integer, and let ", "page_idx": 19}, {"type": "equation", "text": "$$\n(\\overline{{x}}_{N},x_{N},M_{N})\\cong\\mathrm{UniSgd}_{\\hat{g},\\psi}(x_{0},M_{0},N;D),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as defined by Algorithm $^{l,}$ , sand let $x_{0},\\ldots,x_{N}$ be thpe corresponding points generated inside the algorithm. Then, for any $\\overline{{M}}>c_{2}L_{f}$ , it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\Big[N[F(\\bar{x}_{N})-F^{*}]+\\frac{M_{N}}{2}\\|x_{N}-x^{*}\\|^{2}+\\displaystyle\\sum_{k=0}^{N-1}[\\beta_{f,\\bar{f},\\bar{g}}(x_{k+1},x_{k})+\\beta_{f,\\bar{f},\\bar{g}}(x_{k},x^{*})]\\Big]}}\\\\ &{}&{\\leq\\displaystyle\\frac{M_{0}}{2}\\|x_{0}-x^{*}\\|^{2}+\\frac{c_{1}}{\\overline{{M}}-c_{2}L_{f}}\\displaystyle\\sum_{k=0}^{N-1}\\mathbb{E}[\\mathrm{Var}_{\\hat{g}}(x_{k+1})+\\mathrm{Var}_{\\hat{g}}(x_{k})]+c_{3}N\\delta_{f}}\\\\ &{}&{\\qquad+\\,c_{4}\\mathbb{E}\\big\\{\\big[\\mathrm{min}\\{M_{N},\\overline{{M}}\\}-M_{0}\\big]_{+}D^{2}\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Each iteration $k$ of the algorithm, when conditioned on $x_{k}$ , follows the construction from Lemma 23 (with $x=x_{k},\\widehat{g}_{x}=g_{k},\\,M=M_{k},\\widehat{x}_{+}=x_{k+1},\\widehat{g}_{x_{+}}=g_{k+1},\\widehat{M}_{+}=M_{k+1})$ . Hence, we can write, after passing to full expectations, that, for each $k=0,\\ldots,N-1$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\quad\\mathbb{E}\\Big[F(x_{k+1})-F^{*}+\\frac{M_{k+1}}{2}\\|x_{k+1}-x^{*}\\|^{2}+\\beta_{f,\\bar{f},\\bar{g}}(x_{k+1},x_{k})+\\beta_{f,\\bar{f},\\bar{g}}(x_{k},x^{*})\\Big]}\\\\ &{\\leq\\mathbb{E}\\Big[\\frac{M_{k}}{2}\\|x_{k}-x^{*}\\|^{2}\\!+\\!\\frac{c_{1}}{\\bar{M}-c_{2}L_{f}}[\\mathrm{Var}_{\\bar{g}}(x_{k+1})\\!+\\!\\mathrm{Var}_{\\bar{g}}(x_{k})]\\!+\\!c_{4}[\\operatorname*{min}\\{M_{k+1},\\overline{{M}}\\}\\!-\\!M_{k}]_{+}D^{2}\\Big]\\!+\\!c_{3}\\delta_{f},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\overline{{M}}>2L_{f}$ is an ar\u010ebitrary constant. Telescoping the above inequalities (using Lemma 18) and then b ou\u010ending $\\begin{array}{r}{\\dot{N}[F({\\overline{{x}}}_{N})-F^{*}]\\leq\\sum_{k=1}^{N}[F(x_{k})-F^{*}]}\\end{array}$ (using the convexity of $F$ and our choice of $\\begin{array}{r}{\\bar{x}_{N}=\\frac{1}{N}\\sum_{k=1}^{N}x_{k})}\\end{array}$ , swe get the claim. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Tsheorem 4. Let Algorithm $^{\\,l}$ with $M_{0}=0$ be applied to problem (1) under Assumptions 1\u20133. Then, for the point ${\\bar{x}}_{N}$ generated by the algorithm, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\bar{x}_{N})]-F^{*}\\le\\frac{c_{2}c_{4}L_{f}D^{2}}{N}+2\\sigma D\\sqrt{\\frac{2c_{1}c_{4}}{N}}+c_{3}\\delta_{f}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Applying Lemma 24, substituting our choice of $M_{0}\\,=\\,0$ , estimating $\\mathrm{Var}_{\\hat{g}}(\\cdot)\\,\\leq\\,\\sigma^{2}$ and dropping the nonnegative $\\beta_{f,\\bar{f},\\bar{g}}(\\cdot,\\cdot)$ terms, we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\bar{x}_{N})]-F^{*}\\le\\frac{1}{N}\\Big(c_{4}\\overline{{M}}D^{2}+\\frac{2c_{1}\\sigma^{2}N}{\\overline{{M}}-c_{2}L_{f}}+c_{3}N\\delta_{f}\\Big)=\\frac{c_{4}\\overline{{M}}D^{2}}{N}+\\frac{2c_{1}\\sigma^{2}}{\\overline{{M}}-c_{2}L_{f}}+c_{3}\\delta_{f},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\overline{{M}}>2L_{f}$ is an arbitrary constant. The optimal $\\overline{{M}}$ which minimizes the right-hand side is $\\begin{array}{r}{\\overline{{M}}=c_{2}L_{f}+\\frac{\\sigma}{D}\\sqrt{\\frac{2c_{1}}{c_{4}}N}}\\end{array}$ . Substituting this choice into t he\u010e above display, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[F(\\bar{x}_{N})]-F^{*}\\leq\\displaystyle\\frac{c_{4}D^{2}}{N}\\Big(c_{2}L_{f}+\\displaystyle\\frac{\\sigma}{D}\\sqrt{\\displaystyle\\frac{2c_{1}}{c_{4}}N}\\Big)+\\displaystyle\\frac{2c_{1}\\sigma^{2}}{\\displaystyle\\frac{\\sigma}{D}\\sqrt{\\displaystyle\\frac{2c_{1}}{c_{4}}N}}+c_{3}\\delta_{f}}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\frac{c_{2}c_{4}L_{f}D^{2}}{N}+2\\sigma D\\sqrt{\\displaystyle\\frac{2c_{1}c_{4}}{N}}+c_{3}\\delta_{f}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C.2 Universal Fast SGD ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma 25 (Stochastic Triangle Step). Consider problem (1) under Assumption $^{\\,l}$ . Let $\\widehat g$ be an unbiased oracle for $\\overline{{g}}_{\\mathrm{:}}$ , let $x,v\\in\\operatorname{dom}\\psi$ be points and $M,A\\geq0,$ , $a>0$ be coefficients. Further, for $A_{+}:=A+a_{*}$ , let ", "page_idx": 20}, {"type": "equation", "text": "$$\ny=\\frac{A x+a v}{A_{+}},\\quad\\hat{g}_{y}\\cong\\hat{g}(y),\\quad\\hat{v}_{+}=\\operatorname{Prox}_{\\psi}(v,\\hat{g}_{y},M/a),\\quad\\hat{x}_{+}=\\frac{A x+a\\hat{v}_{+}}{A_{+}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Denote $\\begin{array}{r}{\\widehat{\\Delta}(M):=\\beta_{f,\\bar{f},\\bar{g}}(y,\\widehat{x}_{+})+\\langle\\bar{g}(y)-\\widehat{g}_{y},\\widehat{x}_{+}-y\\rangle-\\frac{M A_{+}}{2a^{2}}\\|\\widehat{x}_{+}-y\\|^{2}}\\end{array}$ . Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}\\Big[A_{+}[F(\\widehat{x}_{+})-F^{*}]+\\displaystyle\\frac{M}{2}\\|\\widehat{v}_{+}-x^{*}\\|^{2}\\Big]+A\\beta_{f,\\bar{f},\\bar{g}}(y,x)+a\\beta_{f,\\bar{f},\\bar{g}}(y,x^{*})}}\\\\ &{\\leq A[F(x)-F^{*}]+\\displaystyle\\frac{M}{2}\\|v-x^{*}\\|^{2}+A_{+}\\mathbb{E}[\\widehat{\\Delta}(M)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If further Assumption 2 is satisfied, and $\\widehat{M}_{+}\\geq M$ is a random coefficient (possibly dependent on ${\\widehat{g}}_{y}.$ ), then we also have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\Big[A_{+}[F(\\widehat{x}_{+})-F^{*}]+\\frac{\\widehat{M}_{+}}{2}\\|\\widehat{v}_{+}-x^{*}\\|^{2}\\Big]+A\\beta_{f,\\bar{f},\\bar{g}}(y,x)+a\\beta_{f,\\bar{f},\\bar{g}}(y,x^{*})}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\leq A[F(x)-F^{*}]+\\frac{M}{2}\\|v-x^{*}\\|^{2}+\\mathbb{E}[A_{+}\\widehat{\\Delta}(\\widehat{M}_{+})+(\\widehat{M}_{+}-M)D^{2}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Denoting $\\theta:=A\\beta_{f,\\bar{f},\\bar{g}}(y,x)+a\\beta_{f,\\bar{f},\\bar{g}}(y,x^{*})$ and using the fact that $\\mathbb{E}[\\widehat{g}_{y}]=\\overline{{g}}(y)$ , we can rewrite ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle A F(x)+a F(x^{*})+\\frac{M}{2}\\|v-x^{*}\\|^{2}}\\\\ {\\displaystyle\\qquad=A[\\bar{f}(y)+\\langle\\bar{g}(y),x-y\\rangle+\\beta_{f,\\bar{f},\\bar{g}}(y,x)+\\psi(x)]}\\\\ {\\displaystyle\\qquad\\qquad+a[\\bar{f}(y)+\\langle\\bar{g}(y),x^{*}-y\\rangle+\\beta_{f,\\bar{f},\\bar{g}}(y,x^{*})+\\psi(x^{*})]+\\frac{M}{2}\\|v-x^{*}\\|^{2}}\\\\ {\\displaystyle\\qquad=A_{+}\\bar{f}(y)+\\langle\\bar{g}(y),A x+a x^{*}-A_{+}y\\rangle+A\\psi(x)+a\\psi(x^{*})+\\frac{M}{2}\\|v-x^{*}\\|^{2}+\\theta}\\\\ {\\displaystyle\\qquad=\\mathbb{E}\\Big[A_{+}\\bar{f}(y)+\\langle\\hat{g}_{y},A x+a x^{*}-A_{+}y\\rangle+A\\psi(x)+a\\psi(x^{*})+\\frac{M}{2}\\|v-x^{*}\\|^{2}\\Big]+\\theta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Further, by the definition of ${\\widehat{v}}_{+}$ and Lemma 17, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\langle\\widehat{g}_{y},x^{*}-\\widehat{v}_{+}\\rangle+\\psi(x^{*})+\\frac{M}{2a}\\|v-x^{*}\\|^{2}\\geq\\psi(\\widehat{v}_{+})+\\frac{M}{2a}\\|v-\\widehat{v}_{+}\\|^{2}+\\frac{M}{2a}\\|\\widehat{v}_{+}-x^{*}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This mepans that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{+}\\displaystyle\\bar{f}(y)+\\langle\\hat{g}_{y},A x+a x^{*}-A_{+}y\\rangle+A\\psi(x)+a\\psi(x^{*})+\\displaystyle\\frac{M}{2}\\|v-x^{*}\\|^{2}}\\\\ &{\\~\\geq A_{+}\\displaystyle\\bar{f}(y)+\\langle\\hat{g}_{y},A x+a\\hat{v}_{+}-A_{+}y\\rangle+A\\psi(x)+a\\psi(\\hat{v}_{+})+\\displaystyle\\frac{M}{2}\\|v-\\hat{v}_{+}\\|^{2}+\\displaystyle\\frac{M}{2}\\|\\hat{v}_{+}-x^{*}\\|^{2}}\\\\ &{~\\geq A_{+}\\displaystyle[\\bar{f}(y)+\\langle\\hat{g}_{y},\\hat{x}_{+}-y\\rangle+\\psi(\\hat{x}_{+})]+\\displaystyle\\frac{M}{2}\\|v-\\hat{v}_{+}\\|^{2}+\\displaystyle\\frac{M}{2}\\|\\hat{v}_{+}-x^{*}\\|^{2}}\\\\ &{~=A_{+}F(\\hat{x}_{+})+\\displaystyle\\frac{M}{2}\\|\\hat{v}_{+}-x^{*}\\|^{2}-A_{+}\\hat{\\Delta}(M),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the sepcond inequaplity is due to the  definition of $\\widehat{x}_{+}$ and the convexity of $\\psi$ , and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\widehat{\\Delta}(M):=f(\\widehat{x}_{+})-\\bar{f}(y)-\\langle\\widehat{g}_{y},\\widehat{x}_{+}-y\\rangle-\\displaystyle\\frac{M}{2A_{+}}\\|v-\\widehat{v}_{+}\\|^{2}}}\\\\ {{\\ \\ \\ \\ \\ \\ \\ =\\beta_{f,\\bar{f},\\bar{g}}(y,\\widehat{x}_{+})+\\langle\\bar{g}(y)-\\widehat{g}_{y},\\widehat{x}_{+}-y\\rangle-\\displaystyle\\frac{M A_{+}}{2a^{2}}\\|\\widehat{x}_{+}-y\\|^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "since $\\begin{array}{r}{\\widehat{x}_{+}-y=\\frac{a}{A_{+}}(\\widehat{v}_{+}-v)}\\end{array}$ (by t hpe definitsions of $y$ a npd $\\widehat{x}_{+}$ ). Substitutingp the above inequality into the fi rspt display and reparranging, we get the first of the cl apimed inequalities. ", "page_idx": 20}, {"type": "text", "text": "To prove the second one, we simply add to both sides of the already proved first inequality the expected value of ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\widehat{M}_{+}-M}{2}\\|\\widehat{v}_{+}-x^{*}\\|^{2}+A_{+}[\\widehat{\\Delta}(M)-\\widehat{\\Delta}(\\widehat{M}_{+})]=\\frac{\\widehat{M}_{+}-M}{2}\\Big(\\|\\widehat{v}_{+}-x^{*}\\|^{2}+\\frac{A_{+}^{2}}{a^{2}}\\|\\widehat{x}_{+}-y\\|^{2}\\Big)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and then bounpd, using the fact that $\\begin{array}{r}{\\widehat{x}_{+}-y=\\frac{a}{A_{+}}(\\widehat{v}_{+}-v)}\\end{array}$ togetherp with our Assumptipon 2, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\widehat{v}_{+}-x^{*}\\|^{2}+\\frac{A_{+}^{2}}{a^{2}}\\|\\widehat{x}_{+}-y\\|^{2}=\\|\\widehat{v}_{+}-x^{*}\\|^{2}+\\|\\widehat{v}_{+}-v\\|^{2}\\leq2D^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma 26 (Univpersal Stochastic Tripangle Step). Cponsider problepm (1) under Assumptions $^{\\,I}$ and 2, and let $\\widehat g$ be an unbiased oracle for $\\overline{{g}}$ . Let $x,v\\in\\operatorname{dom}\\psi$ be points, $M,A\\geq0,$ , $a>0$ be coefficients. Further, for $A_{+}:=A+a_{;}$ , let ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{A x+a v}{A_{+}},\\quad\\widehat{g}_{y}\\cong\\widehat{g}(y),\\quad\\widehat{v}_{+}=\\operatorname*{Prox}_{\\psi}(v,\\widehat{g}_{y},M/a),\\quad\\widehat{x}_{+}=\\frac{A x+a\\widehat{v}_{+}}{A_{+}},}\\\\ {\\displaystyle\\widehat{g}_{x_{+}}\\cong\\widehat{g}(\\widehat{x}_{+}),\\quad\\widehat{M}_{+}=\\frac{a^{2}}{A_{+}}M_{+}\\Big(\\frac{A_{+}}{a^{2}}M,\\displaystyle\\frac{a^{2}}{A_{+}^{2}}D^{2},y,\\widehat{x}_{+},\\widehat{g}_{y},\\widehat{g}_{x_{+}}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, for any $\\begin{array}{r}{\\overline{{M}}>c_{2}L_{f}\\frac{a^{2}}{A_{+}}}\\end{array}$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\Big[A_{+}[F(\\widehat{x}_{+})-F^{*}]+\\displaystyle\\frac{\\widehat{M}_{+}}{2}\\|\\widehat{v}_{+}-x^{*}\\|^{2}+A_{+}\\beta_{f,\\bar{f},\\bar{g}}(\\widehat{x}_{+},y)\\Big]+A\\beta_{f,\\bar{f},\\bar{g}}(y,x)+a\\beta_{f,\\bar{f},\\bar{g}}(y,x^{*})}}\\\\ &{}&{\\leq A[F(x)-F^{*}]+\\displaystyle\\frac{M}{2}\\|v-x^{*}\\|^{2}+\\displaystyle\\frac{c_{1}a^{2}}{\\overline{{M}}-c_{2}L_{f}\\frac{a^{2}}{A_{+}}}\\mathbb{E}[\\mathrm{Var}_{\\bar{g}}(\\widehat{x}_{+})+\\mathrm{Var}_{\\bar{g}}(y)]\\quad\\quad\\quad}\\\\ &{}&{\\quad+\\,c_{3}A_{+}\\delta_{f}+c_{4}\\,\\mathbb{E}\\big\\{[\\operatorname*{min}\\{\\widehat{M}_{+},\\overline{{M}}\\}-M]_{+}D^{2}\\big\\}.\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. According to Lemma 25 (together with the fact that $\\widehat{M}_{+}\\geq M$ which is guaranteed by the requirement on the stepsize update rule), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\Big[A_{+}[F(\\widehat{x}_{+})-F^{*}]+\\frac{\\widehat{M}_{+}}{2}\\|\\widehat{v}_{+}-x^{*}\\|^{2}\\Big]+A\\beta_{f,\\bar{f},\\bar{g}}(y,x)+a\\beta_{f,\\bar{f},\\bar{g}}(y,x^{*})\\quad}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\leq A[F(x)-F^{*}]+\\frac{M}{2}\\|v-x^{*}\\|^{2}+\\mathbb{E}\\big[A_{+}\\widehat{\\Delta}(\\widehat{M}_{+})+(\\widehat{M}_{+}-M)D^{2}\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{\\widehat{\\Delta}(\\widehat{M}_{+}):=\\beta_{f,\\bar{f},\\bar{g}}(y,\\widehat{x}_{+})+\\langle\\bar{g}(y)-\\widehat{g}_{y},\\widehat{x}_{+}-y\\rangle-\\frac{\\widehat{M}_{+}A_{+}}{2a^{2}}\\|\\widehat{x}_{+}-y\\|^{2}}\\end{array}$ . Further, according t2o the main r epquirxement (2)  osns the  sptepsize supdate prul ep (applied in the varpiables $\\begin{array}{r}{M^{\\prime}:=\\frac{A_{+}}{a^{2}}M,\\Omega:=\\frac{a^{2}}{A_{+}^{2}}D^{2}}\\end{array}$ , $\\begin{array}{r}{\\widehat{M}_{+}^{\\prime}:=\\frac{A_{+}}{a^{2}}\\widehat{M}_{+}}\\end{array}$ , $\\begin{array}{r}{\\overline{{M}}^{\\prime}:=\\frac{A+}{a^{2}}\\overline{{M}}}\\end{array}$ for which we have $\\begin{array}{r}{M^{\\prime}\\Omega=M\\frac{D^{2}}{A+}}\\end{array}$ , $\\widehat{M}_{+}^{\\prime}\\Omega=\\widehat{M}_{+}\\frac{D^{2}}{A_{+}}$ , $\\begin{array}{r}{\\overline{{M}}^{\\prime}\\Omega=\\overline{{M}}\\frac{D^{2}}{A+})}\\end{array}$ , itx holds tha t ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big[\\widehat\\Delta(\\widehat M_{+})+(\\widehat M_{+}-M)\\frac{D^{2}}{A_{+}}+\\beta_{f,\\bar{f},\\bar{g}}(\\widehat x_{+},y)\\Big]}\\\\ &{\\quad\\leq\\frac{c_{1}}{\\frac{A_{+}}{a^{2}}\\overline{M}-c_{2}L_{f}}\\mathbb{E}[\\mathrm{Var}_{\\widehat{g}}(\\widehat x_{+})+\\mathrm{Var}_{\\widehat{g}}(y)]+c_{3}\\delta_{f}+c_{4}\\,\\mathbb{E}\\Big\\{[\\mathrm{min}\\{\\widehat M_{+},\\overline{M}\\}-M]_{+}\\frac{D^{2}}{A_{+}}\\Big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\overline{{{M}}}>c_{2}L_{f}\\frac{a^{2}}{A_{+}}$ is an arbitrary constant. Multiplying both sides of the above display by $A_{+}$ and adding  t\u010ehe result to the first display, we obtain the claim. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Lemma 27 (Universal Fast SGD: General Guarantee). Consider Algorithm 2 applied to problem (1) under Assumptions $^{\\,l}$ and 2. Then, for any $k\\geq1$ and any $\\overline{{M}}>c_{2}\\breve{L_{f}}$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\Big[A_{k}[F(x_{k})-F^{*}]+\\sum_{i=0}^{k-1}[A_{i+1}\\beta_{f,\\bar{f},\\bar{g}}(x_{i+1},y_{i})+a_{i+1}\\beta_{f,\\bar{f},\\bar{g}}(y_{i},x^{*})]\\Big]}}\\\\ &{}&{\\leq c_{4}\\overline{{M}}D^{2}+\\frac{c_{1}}{\\overline{{M}}-c_{2}L_{f}}\\sum_{i=0}^{k-1}a_{i+1}^{2}\\mathbb{E}\\big[\\mathrm{Var}_{\\bar{g}}(x_{i+1})+\\mathrm{Var}_{\\bar{g}}(y_{i})\\big]+c_{3}\\delta_{f}\\sum_{i=1}^{k}A_{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{\\frac{k}{\\omega_{-}}\\frac{1}{2}k,\\,A_{k}=\\frac{1}{4}k(k+1),\\,\\sum_{i=1}^{k}a_{i}^{2}=\\frac{1}{24}k(k+1)(2k+1),\\,\\sum_{i=1}^{k}A_{i}=\\frac{1}{12}k(k+1)(k+2)}\\end{array}$ for each $k\\geq1$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. Each iteration $k$ of the algorithm, when conditioned on $(x_{k},v_{k})$ , follows the construction from Lemma 26 (with $x\\,=\\,x_{k}$ , $v\\,=\\,v_{k}$ , $M\\,=\\,M_{k}$ , $A\\,=\\,A_{k}$ , $a\\,=\\,a_{k+1}$ , $A_{+}\\,=\\,A_{k+1}$ , $y\\,=\\,y_{k}$ , $\\widehat{g}_{y}=g_{y_{k}}$ , $\\widehat{v}_{+}=v_{k+1}$ , $\\widehat{x}_{+}=x_{k+1}$ , $\\widehat{g}_{x_{+}}=g_{x_{k+1}}$ , $\\widehat{M}_{+}=M_{k+1})$ , where $A_{k}$ and $a_{k}$ are the following $\\textstyle a_{k}={\\frac{1}{2}}k$ , $\\begin{array}{r}{A_{k}=\\sum_{i=1}^{k}a_{i}=\\frac{1}{4}k(k+1)}\\end{array}$ .w eA tphpelryeifnogr eL eombtmaian ,2 f6o (r deraocphp , nonnegative $\\beta_{f,\\bar{f},\\bar{g}}(y,x)$ $k\\geq0$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big[A_{k+1}[F(x_{k+1})-F^{*}]+\\displaystyle\\frac{M_{k+1}}{2}\\|v_{k+1}-x^{*}\\|^{2}+A_{k+1}\\beta_{f,\\bar{f},\\bar{g}}(x_{k+1},y_{k})+a_{k+1}\\beta_{f,\\bar{f},\\bar{g}}(y_{k},x^{*})\\Big]}\\\\ &{\\quad\\le\\mathbb{E}\\Big[A_{k}[F(x_{k})-F^{*}]+\\displaystyle\\frac{M_{k}}{2}\\|v_{k}-x^{*}\\|^{2}+\\displaystyle\\frac{c_{1}a_{k+1}^{2}}{\\overline{{M}}-c_{2}L_{f}\\displaystyle\\frac{a_{k+1}^{2}}{A_{k+1}}}[\\mathrm{Var}_{\\hat{g}}(x_{k+1})+\\mathrm{Var}_{\\hat{g}}(y_{k})]\\Big]}\\\\ &{\\quad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+c_{3}A_{k+1}\\delta_{f}+c_{4}\\mathbb{E}\\big\\{[\\operatorname*{min}\\{M_{k+1},\\overline{{M}}\\}-M_{k}]_{+}D^{2}\\big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "whereM is an arbitrary constant such thatM > c2LfAakk++11 . Note however that, for our sequences ak and A k,\u010e we have Aa2k = k 1k4( kk+1) =kk+1 \u2264 \u010e1. Therefore, we can replace $\\frac{c_{1}a_{k+1}^{2}}{\\overline{{M}}-c_{2}L_{f}\\frac{a_{k+1}^{2}}{A_{k+1}}}$ in the above display with $\\frac{c_{1}a_{k+1}^{2}}{M\\!-\\!c_{2}L_{f}}$ under the requirement that $\\overline{{M}}>c_{2}L_{f}$ . Doing this \u010eand then telescoping the above inequali\u010eties (applying Lemma 18), and usin g\u010e the fact that $M_{0}=A_{0}=0$ , we get the claimed inequality. ", "page_idx": 22}, {"type": "text", "text": "It remains to do some standard computations to see that $\\begin{array}{r}{\\sum_{i=1}^{k}a_{i}^{2}\\equiv\\frac{1}{4}\\sum_{i=1}^{k}i^{2}=\\frac{1}{24}k(k\\!+\\!1)(2k\\!+\\!1)}\\end{array}$ and $\\begin{array}{r}{\\sum_{i=1}^{k}A_{i}\\equiv\\frac{1}{4}\\sum_{i=1}^{k}i(i+1)=\\frac{1}{4}(\\frac{1}{6}k(k+1)(2k+1)+\\frac{1}{2}k(k+1))=\\frac{1}{12}k(k+1)(k+2).}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "Theorem 5. Let Algorithm 2 be applied to problem (1) under Assumptions $_{I-3}$ . Then, for any $k\\geq1$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(x_{k})]-F^{*}\\le\\frac{4c_{2}c_{4}L_{f}D^{2}}{k(k+1)}+4\\sigma D\\sqrt{\\frac{2c_{1}c_{4}}{3k}}+\\frac{c_{3}}{3}(k+2)\\delta_{f}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Let $k\\,\\geq\\,1$ be arbitrary and $F_{k}\\,:=\\,\\mathbb{E}[F(x_{k})]\\,-\\,F^{*}$ . Applying Lemma 27, dropping the nonnegative $\\beta_{f,\\bar{f},\\bar{g}}(\\cdot,\\cdot)$ terms and bounding $\\bar{\\mathrm{Var}}_{\\hat{g}}(\\cdot)\\,\\leq\\,\\sigma^{2}$ , we obtain, for an arbitrary constant M > c2Lf, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{F_{k}\\leq\\displaystyle\\frac{1}{A_{k}}\\Big(c_{4}\\overline{{M}}D^{2}+\\frac{2c_{1}\\sigma^{2}}{\\overline{{M}}-c_{2}L_{f}}\\displaystyle\\sum_{i=1}^{k}a_{i}^{2}+c_{3}\\delta_{f}\\displaystyle\\sum_{i=1}^{k}A_{i}\\Big)}}\\\\ {{=\\displaystyle\\frac{4}{k(k+1)}\\Big(c_{4}\\overline{{M}}D^{2}+\\frac{c_{1}k(k+1)(2k+1)\\sigma^{2}}{12(\\overline{{M}}-c_{2}L_{f})}+\\frac{c_{3}}{12}k(k+1)(k+2)\\delta_{f}\\Big)}}\\\\ {{=\\displaystyle\\frac{4c_{4}\\overline{{M}}D^{2}}{k(k+1)}+\\frac{c_{1}(2k+1)\\sigma^{2}}{3(\\overline{{M}}-c_{2}L_{f})}+\\delta_{k},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\begin{array}{r}{\\delta_{k}:=\\frac{c_{3}}{3}(k+2)\\delta_{f}}\\end{array}$ . We now choose $\\overline{{M}}>c_{2}L_{f}$ which minimizes the right-hand side. This is $\\begin{array}{r}{\\overline{{M}}=c_{2}L_{f}+\\frac{\\sigma}{2D}\\sqrt{\\frac{c_{1}}{3c_{4}}k(k+1)(2k+1)}}\\end{array}$ ,  fo\u010er which we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{k}\\le\\displaystyle\\frac{4c_{4}D^{2}}{k(k+1)}\\bigg(c_{2}L_{f}+\\displaystyle\\frac{\\sigma}{2D}\\sqrt{\\frac{c_{1}}{3c_{4}}k(k+1)(2k+1)}\\bigg)+\\displaystyle\\frac{c_{1}(2k+1)\\sigma^{2}}{3\\displaystyle\\frac{\\sigma}{2D}\\sqrt{\\frac{c_{1}}{3c_{4}}k(k+1)(2k+1)}}+\\delta_{k}}\\\\ &{\\quad=\\displaystyle\\frac{4c_{2}c_{4}L_{f}D^{2}}{k(k+1)}+4\\sigma D\\sqrt{\\frac{c_{1}c_{4}(2k+1)}{3k(k+1)}}+\\delta_{k}\\le\\displaystyle\\frac{4c_{2}c_{4}L_{f}D^{2}}{k(k+1)}+4\\sigma D\\sqrt{\\frac{2c_{1}c_{4}}{3k}}+\\delta_{k}.\\qquad\\qquad[\\mathrm{B}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "D Omitted Proofs for Section 5 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "D.1 Universal SGD ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Theorem 7. Let Algorithm $^{\\,I}$ with $M_{0}=0$ be applied to problem (1) under Assumptions 1, 2 and 6, and let $\\sigma_{*}^{2}:=\\mathrm{Var}_{\\hat{g}}(x^{*})$ . Then, for the point ${\\bar{x}}_{N}$ produced by the method, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\bar{x}_{N})]-F^{*}\\leq\\frac{c_{4}(c_{2}L_{f}+12c_{1}L_{\\hat{g}})D^{2}}{N}+2\\sigma_{*}D\\sqrt{\\frac{6c_{1}c_{4}}{N}}+c_{3}\\delta_{f}+\\frac{4}{3}\\delta_{\\hat{g}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Let $x_{0},\\ldots,x_{N}$ be the points generated inside the method and let ${\\cal F}_{N}:=\\mathbb{E}[{\\cal F}(\\bar{x}_{N})]-F^{*}$ . Using Lemma 19 and Assumption 6, we can estimate, for any $0\\leq k\\leq N-1$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}_{\\hat{g}}(x_{k+1})+\\mathrm{Var}_{\\hat{g}}(x_{k})\\leq3\\,\\mathrm{Var}_{\\hat{g}}(x_{k})+2\\,\\mathrm{Var}_{\\hat{g}}(x_{k+1},x_{k})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq6\\sigma_{*}^{2}+6\\,\\mathrm{Var}_{\\hat{g}}(x_{k},x^{*})+2\\,\\mathrm{Var}_{\\hat{g}}(x_{k+1},x_{k})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq6\\sigma_{*}^{2}+12L_{\\hat{g}}\\big[\\beta_{f,\\bar{f},\\bar{g}}(x_{k},x^{*})+\\delta_{\\hat{g}}\\big]+4L_{\\hat{g}}\\big[\\beta_{f,\\bar{f},\\bar{g}}(x_{k+1},x_{k})+\\delta_{\\hat{g}}\\big]}\\\\ &{\\qquad\\qquad\\qquad=6\\sigma_{*}^{2}+4L_{\\hat{g}}\\big[3\\beta_{f,\\bar{f},\\bar{g}}(x_{k},x^{*})+\\beta_{f,\\bar{f},\\bar{g}}(x_{k+1},x_{k})+4\\delta_{\\hat{g}}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Substituting this bound into the general guparant eses given by Lem msas 24 (and taking inpto account the fact that $M_{0}=0$ ), we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle N F_{N}+\\sum_{k=0}^{N-1}\\mathbb E[\\beta_{f,\\bar{f},\\bar{g}}(x_{k+1},x_{k})+\\beta_{f,\\bar{f},\\bar{g}}(x_{k},x^{*})]}}\\\\ {{\\displaystyle\\leq c_{4}\\overline{M}D^{2}+\\frac{6c_{1}\\sigma_{*}^{2}N}{\\overline{M}-c_{2}L_{f}}+\\alpha\\sum_{k=0}^{N-1}\\mathbb E[\\beta_{f,\\bar{f},\\bar{g}}(x_{k+1},x_{k})+3\\beta_{f,\\bar{f},\\bar{g}}(x_{k},x^{*})]+N(c_{3}\\delta_{f}+4\\alpha\\delta_{\\hat{g}}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\overline{{M}}\\,>\\,c_{2}L_{f}$ is an arbitrary constant and $\\begin{array}{r}{\\alpha\\,:=\\,\\frac{4c_{1}L_{\\hat{g}}}{\\overline{{M}}-c_{2}L_{f}}}\\end{array}$ . Requiring now that $3\\alpha\\,\\leq\\,1$ or, equiva le\u010ently, that $\\overline{{M}}\\geq c_{2}L_{f}+12c_{1}L_{\\widehat{g}}=:\\overline{{M}}_{\\operatorname*{min}}$ , we ca\u010en cancel the nonnegative $\\beta_{f,\\bar{f},\\bar{g}}(\\cdot,\\cdot)$ terms on both sides and obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\nF_{N}\\leq\\frac{c_{4}\\overline{{M}}D^{2}}{N}+\\frac{6c_{1}\\sigma_{*}^{2}}{\\overline{{M}}-c_{2}L_{f}}+\\delta,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\delta\\,:=\\,c_{3}\\delta_{f}\\,+\\,\\textstyle{\\frac{4}{3}}\\delta_{\\widehat{g}}$ . The optimal coefficient $\\overline{{M}}_{\\ast}$ minimizing the right-hand side is $\\overline{{M}}_{\\ast}\\,=$ $\\begin{array}{r}{c_{2}L_{f}+\\frac{\\sigma_{*}}{D}\\sqrt{\\frac{6c_{1}N}{c_{4}}}}\\end{array}$ . Hopwever, we still need to respe c\u010et the constraint $\\overline{{M}}\\geq\\overline{{M}}_{\\operatorname*{min}}$ . Choosin g ${\\overline{{M}}}=$ $\\begin{array}{r}{c_{2}L_{f}+12c_{1}L_{\\widehat{g}}+\\frac{\\sigma_{*}}{D}\\sqrt{\\frac{6c_{1}N}{c_{4}}}}\\end{array}$ , we conclude that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{F_{N}\\leq\\displaystyle\\frac{c_{4}D^{2}}{N}\\Big(c_{2}L_{f}+12c_{1}L_{\\hat{g}}+\\frac{\\sigma_{*}}{D}\\sqrt{\\frac{6c_{1}N}{c_{4}}}\\Big)+\\frac{6c_{1}\\sigma_{*}^{2}}{\\frac{\\sigma_{*}}{D}\\sqrt{\\frac{6c_{1}N}{c_{4}}}}+\\delta}}\\\\ {{=\\displaystyle\\frac{c_{4}\\big(c_{2}L_{f}+12c_{1}L_{\\hat{g}}\\big)D^{2}}{N}+2\\sigma_{*}D\\sqrt{\\frac{6c_{1}c_{4}}{N}}+\\delta.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "D.2 Universal Fast SGD ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Theorem 8. Let Algorithm 2 be applied to problem (1) under Assumptions 1, 2 and 6, and let $\\sigma_{*}^{2}:=\\mathrm{Var}_{\\hat{g}}(x^{*})$ . Then, for any $k\\geq1$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(x_{k})]-F^{*}\\le\\frac{4c_{2}c_{4}L_{f}D^{2}}{k(k+1)}+\\frac{24c_{1}c_{4}L_{\\widehat{g}}D^{2}}{k+1}+4\\sigma_{*}D\\sqrt{\\frac{2c_{1}c_{4}}{k}}+\\frac{c_{3}}{3}(k+2)\\delta_{f}+\\frac{4}{3}\\delta_{\\widehat{g}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Let $k\\geq1$ be arbitrary and $F_{k}:=\\mathbb{E}[F(x_{k})]-F^{*}$ . Using Lemma 19 and Assumption 6, we can estimate, for each $i$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}_{\\hat{g}}(x_{i+1})+\\mathrm{Var}_{\\hat{g}}(y_{i})\\leq3\\,\\mathrm{Var}_{\\hat{g}}(y_{i})+2\\,\\mathrm{Var}_{\\hat{g}}(x_{i+1},y_{i})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq6\\sigma_{*}^{2}+6\\,\\mathrm{Var}_{\\hat{g}}(y_{i},x^{*})+2\\,\\mathrm{Var}_{\\hat{g}}(x_{i+1},y_{i})}\\\\ &{\\qquad\\qquad\\qquad\\leq6\\sigma_{*}^{2}+12L_{\\hat{g}}\\big[\\beta_{f,\\bar{f},\\bar{g}}(y_{i},x^{*})+\\delta_{\\hat{g}}\\big]+4L_{\\hat{g}}\\big[\\beta_{f,\\bar{f},\\bar{g}}(x_{i+1},y_{i})+\\delta_{\\hat{g}}\\big]}\\\\ &{\\qquad\\qquad\\qquad=6\\sigma_{*}^{2}+4L_{\\hat{g}}\\big[3\\beta_{f,\\bar{f},\\bar{g}}(y_{i},x^{*})+\\beta_{f,\\bar{f},\\bar{g}}(x_{i+1},y_{i})+4\\delta_{\\hat{g}}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Substituting this bound into the guarantee given by Lemma 27, we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle A_{k}F_{k}+\\sum_{i=0}^{k-1}\\mathbb{E}[A_{i+1}\\beta_{f,\\bar{f},\\bar{g}}(x_{i+1},y_{i})+a_{i+1}\\beta_{f,\\bar{f},\\bar{g}}(y_{i},x^{*})]}}\\\\ {{\\displaystyle\\leq c_{4}\\overline{{M}}D^{2}+\\sum_{i=0}^{k-1}\\alpha_{i+1}\\mathbb{E}[\\beta_{f,\\bar{f},\\bar{g}}(x_{i+1},y_{i})\\!+\\!3\\beta_{f,\\bar{f},\\bar{g}}(y_{i},x^{*})\\!+\\!4\\delta_{\\bar{g}}]\\!+\\!\\frac{6c_{1}\\sigma_{*}^{2}}{\\overline{{M}}-c_{2}L_{f}}\\sum_{i=1}^{k}a_{i}^{2}\\!+\\!c_{3}\\delta_{f}\\sum_{i=1}^{k}A_{i},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\begin{array}{r}{\\alpha_{i+1}:=\\frac{4c_{1}L_{\\hat{g}}a_{i+1}^{2}}{\\overline{{M}}-c_{2}L_{f}}}\\end{array}$ , $\\textstyle a_{i}={\\frac{1}{2}}i$ , $\\begin{array}{r}{A_{k}=\\frac{1}{4}k(k+1),\\sum_{i=1}^{k}a_{i}^{2}=\\frac{1}{24}k(k+1)(2k+1),\\sum_{i=1}^{k}A_{i}=}\\end{array}$ $\\textstyle{\\frac{1}{12}}k(k+1)(k+2)$ . Requiring now that $3\\alpha_{i+1}\\leq a_{i+1}$ for all $i=0,\\ldots,k-1$ or, equivalently, that $\\overline{{M}}\\geq c_{2}L_{f}+{12c_{1}L_{\\hat{g}}a_{k}}\\equiv c_{2}L_{f}+6c_{1}L_{\\hat{g}}k$ , we can cancel the nonnegative $\\beta_{f,\\bar{f},\\bar{g}}(\\cdot,\\cdot)$ terms on both si\u010edes and obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{F_{k}\\leq\\displaystyle\\frac{1}{A_{k}}\\Big(c_{4}\\overline{{M}}D^{2}+\\frac{6c_{1}\\sigma_{*}^{2}}{\\overline{{M}}-c_{2}L_{f}}\\displaystyle\\sum_{i=1}^{k}a_{i}^{2}+c_{3}\\delta_{f}\\displaystyle\\sum_{i=1}^{k}A_{i}+\\frac{4}{3}A_{k}\\delta_{\\hat{g}}\\Big)}}\\\\ {{\\displaystyle~~=\\frac{4}{k(k+1)}\\Big(c_{4}\\overline{{M}}D^{2}+\\frac{c_{1}\\sigma_{*}^{2}k(k+1)(2k+1)}{4(\\overline{{M}}-c_{2}L_{f})}+\\frac{c_{3}}{12}\\delta_{f}k(k+1)(k+2)\\Big)+\\frac{4}{3}\\delta_{\\hat{g}}}}\\\\ {{\\displaystyle~~=\\frac{4c_{4}\\overline{{M}}D^{2}}{k(k+1)}+\\frac{c_{1}\\sigma_{*}^{2}(2k+1)}{\\overline{{M}}-c_{2}L_{f}}+\\delta_{k},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\begin{array}{r}{\\delta_{k}:=\\frac{c_{3}}{3}(k+2)\\delta_{f}+\\frac{4}{3}\\delta_{\\widehat{g}}}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "The minimizer of the right-hand side is $\\begin{array}{r}{\\overline{{M}}_{\\ast}=c_{2}L_{f}+\\frac{\\sigma_{\\ast}}{2D}\\sqrt{\\frac{c_{1}}{c_{4}}k(k+1)(2k+1)}}\\end{array}$ . However, recall that we also need to satisfy the constrain t $\\overline{{M}}\\ge c_{2}L_{f}+6c_{1}\\dot{L}_{\\widehat{g}}k$ . Choosing $\\overline{{M}}=c_{2}L_{f}+6c_{1}L_{\\widehat{g}}k+$ 2\u03c3D\u2217 cc14 k(k + 1)(2k + 1), we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\tau}_{k}\\leq\\frac{4c_{4}D^{2}}{k(k+1)}\\Big(c_{2}L_{f}+6c_{1}L_{\\hat{g}}k+\\frac{\\sigma_{*}}{2D}\\sqrt{\\frac{c_{1}}{c_{4}}k(k+1)(2k+1)}\\Big)+\\frac{c_{1}\\sigma_{*}^{2}(2k+1)}{\\frac{\\sigma_{*}}{2D}\\sqrt{\\frac{c_{1}}{c_{4}}k(k+1)(2k+1)}}+\\delta_{k}}\\\\ &{\\quad=\\frac{4c_{2}c_{4}L_{f}D^{2}}{k(k+1)}+\\frac{24c_{1}c_{4}L_{\\hat{g}}D^{2}}{k+1}+4\\sigma_{*}D\\sqrt{\\frac{c_{1}c_{4}(2k+1)}{k(k+1)}}+\\delta_{k}}\\\\ &{\\quad\\leq\\frac{4c_{2}c_{4}L_{f}D^{2}}{k(k+1)}+\\frac{24c_{1}c_{4}L_{\\hat{g}}D^{2}}{k+1}+4\\sigma_{*}D\\sqrt{\\frac{2c_{1}c_{4}}{k}}+\\delta_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "E Omitted Proofs for Section 6 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma 28 (Basic property of SVRG oracle). Let $\\widehat g$ be a stochastic oracle in $\\mathbb{R}^{d}$ , and let ${\\widehat{G}}\\,=$ $\\operatorname{SvrgOrac}_{\\hat{g}}(\\tilde{x})$ for some $\\tilde{x}\\in\\mathbb R^{d}$ . Then, for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , the mean value of $\\hat{G}$ at $x$ is the same a s pthat of $\\widehat g$ at $x$ , wphile $\\operatorname{Var}_{\\hat{G}}(x)=\\operatorname{Var}_{\\hat{g}}(x,\\tilde{x})$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Let $g$ and $\\xi$ be, respectively, the function and the random variable components of $\\widehat g$ , and let $g(x):=\\mathbb{E}_{\\xi}[g(x,\\xi)],g(\\tilde{x}):=\\mathbb{E}_{\\xi}[g(\\tilde{x},\\xi)]$ . Then, by definition, $\\hat{G}$ is the oracle with the same random variable component $\\xi$ and the function component $G$ defined by $G(x,\\xi)=g(x,\\xi)-g(\\tilde{x},\\xi)+g(\\tilde{x})$ . Consequently, $\\mathbb{E}_{\\xi}[G(x,\\xi)]=g(x)$ , and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}_{\\hat{G}}(x)=\\mathbb{E}_{\\xi}\\big[\\|G(x,\\xi)-g(x)\\|_{*}^{2}\\big]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{\\xi}\\big[\\|[g(x,\\xi)-g(\\tilde{x},\\xi)]-[g(x)-g(\\tilde{x})]\\|_{*}^{2}\\big]=\\mathrm{Var}_{\\hat{g}}(x,\\tilde{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "E.1 Universal SVRG ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma 29 (Universal SVRG Epoch). Consider problem (1) under Assumptions 1, 2, $^{\\sc6}$ and 9. Let $x,\\tilde{x}\\in\\mathrm{dom}\\,\\psi$ be points, $M\\geq0$ be a coefficient, $N\\geq1$ be an integer, $\\hat{G}=\\mathrm{SvrgOrac}_{\\hat{g}}(\\tilde{x})$ , and let ", "page_idx": 24}, {"type": "equation", "text": "$$\n(\\tilde{x}_{+},x_{+},M_{+})\\cong\\mathrm{UniSgd}_{\\hat{G},\\psi}(x,M,N;D),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "as defined by Algorithm 1. Then, for any $\\overline{{M}}\\geq c_{2}L_{f}+12c_{1}L_{\\widehat{g}}$ , $\\begin{array}{r}{\\alpha:=\\frac{4c_{1}L_{\\widehat{g}}}{\\overline{{M}}-c_{2}L_{f}}}\\end{array}$ , and any $\\nabla f(x^{*})\\in$ $\\partial f(x^{*})$ , it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\Big[N[F(\\tilde{x}_{+})-F^{*}]+\\displaystyle\\frac{M_{+}}{2}\\|x_{+}-x^{*}\\|^{2}\\Big]}\\\\ &{\\leq6\\alpha N\\beta_{f}^{\\nabla f(x^{*})}(x^{*},\\tilde{x})+\\displaystyle\\frac{M}{2}\\|x-x^{*}\\|^{2}+N(c_{3}\\delta_{f}+16\\alpha\\delta_{\\tilde{g}})+c_{4}D^{2}\\mathbb{E}\\big\\{\\big[\\operatorname*{min}\\{M_{+},\\overline{{M}}\\}-M\\big]_{+}\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Since $\\widehat g$ is an unbiased oracle for $\\overline{{g}}$ , so is $\\hat{G}$ (Lemma 28). Therefore, we can apply Lemma 24 to get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big[N[F(\\tilde{x}_{+})-F^{*}]+\\frac{M_{+}}{2}\\|x_{+}-x^{*}\\|^{2}+\\displaystyle\\sum_{k=0}^{N-1}[\\beta_{f,\\bar{f},\\bar{g}}(x_{k+1},x_{k})+\\beta_{f,\\bar{f},\\bar{g}}(x_{k},x^{*})]\\Big]}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{M}{2}\\|x-x^{*}\\|^{2}+\\frac{c_{1}}{\\bar{M}-c_{2}L_{f}}\\displaystyle\\sum_{k=0}^{N-1}\\mathbb{E}[\\mathrm{Var}_{\\hat{G}}(x_{k+1})+\\mathrm{Var}_{\\hat{G}}(x_{k})]+c_{3}N\\delta_{f}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+c_{4}\\mathbb{E}\\big\\{[\\mathrm{min}\\{M_{+},\\overline{{{M}}}\\}-M]_{+}D^{2}\\big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\overline{{M}}>c_{2}L_{f}$ is an arbitrary constant and $x_{k}$ are the points generated inside  \u010eUniSgd. ", "page_idx": 25}, {"type": "text", "text": "Apply in\u010eg now Lemmas 19 and 28 and Assumptions 6 and 9, we can estimate, for each $k$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}_{\\hat{G}}(x_{k+1})+\\mathrm{Var}_{\\hat{G}}(x_{k})=\\mathrm{Var}_{\\hat{g}}(x_{k+1},\\tilde{x})+\\mathrm{Var}_{\\hat{g}}(x_{k},\\tilde{x})\\leq2\\,\\mathrm{Var}_{\\hat{g}}(x_{k+1},x_{k})+3\\,\\mathrm{Var}_{\\hat{g}}(x_{k},\\tilde{x})}\\\\ &{\\qquad\\leq2\\,\\mathrm{Var}_{\\hat{g}}(x_{k+1},x_{k})+6\\,\\mathrm{Var}_{\\hat{g}}(x_{k},x^{*})+6\\,\\mathrm{Var}_{\\hat{g}}(x^{*},\\tilde{x})}\\\\ &{\\qquad\\leq4L_{\\hat{g}}[\\beta_{f,\\tilde{f},\\tilde{g}}(x_{k+1},x_{k})+\\delta_{\\hat{g}}]+12L_{\\hat{g}}[\\beta_{f,\\tilde{f},\\tilde{g}}(x_{k},x^{*})+\\delta_{\\hat{g}}]+24L_{\\hat{g}}[\\beta_{f}^{\\nabla f(x^{*})}(x^{*},\\tilde{x})+2\\delta_{\\hat{g}}]}\\\\ &{\\qquad=4L_{\\hat{g}}[\\beta_{f,\\tilde{f},\\tilde{g}}(x_{k+1},x_{k})+3\\beta_{f,\\tilde{f},\\tilde{g}}(x_{k},x^{*})+6\\beta_{f}^{\\nabla f(x^{*})}(x^{*},\\tilde{x})+16\\delta_{\\hat{g}}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\nabla f(x^{*})\\in\\partial f(x^{*})$ is arbitrary. Denoting $\\begin{array}{r}{\\alpha:=\\frac{4c_{1}L_{\\hat{g}}}{\\overline{{M}}-c_{2}L_{f}}}\\end{array}$ M\u22121c2gpLf , we thus obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\Big[N[F(\\tilde{x}_{+})-F^{*}]+\\frac{M_{+}}{2}\\|x_{+}-x^{*}\\|^{2}+\\displaystyle\\sum_{k=0}^{N-1}[\\beta_{f,\\bar{f},\\bar{g}}(x_{k+1},x_{k})+\\beta_{f,\\bar{f},\\bar{g}}(x_{k},x^{*})]\\Big]}}\\\\ &{}&{\\leq6\\alpha N\\beta_{f}^{\\nabla f(x^{*})}(x^{*},\\tilde{x})+\\displaystyle\\frac{M}{2}\\|x-x^{*}\\|^{2}+N(c_{3}\\delta_{f}+16\\alpha\\delta_{\\bar{g}})+c_{4}\\,\\mathbb{E}\\big\\{\\big[\\operatorname*{min}\\{M_{+},\\overline{{{M}}}\\}-M\\big]_{+}D^{2}\\big\\}}\\\\ &{}&{\\qquad+\\displaystyle\\alpha\\sum_{k=0}^{N-1}\\mathbb{E}[\\beta_{f,\\bar{f},\\bar{g}}(x_{k+1},x_{k})+3\\beta_{f,\\bar{f},\\bar{g}}(x_{k},x^{*})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Requiring now $\\overline{{M}}\\,\\geq\\,c_{2}L_{f}\\,+12c_{1}L_{\\widehat{g}}$ , we get $\\alpha\\,\\leq\\,{\\frac{1}{3}}$ which allows us to cancel the nonnegative $\\beta_{f,\\bar{f},\\bar{g}}(\\cdot,\\cdot)$ terms \u010eon both sides. The clpaim now follows. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Th esosrem 10. Let UniSvrg (as defined by Algorithm 3) be applied to problem (1) under Assumptions $^{\\,l}$ , 2, 6 and 9. Then, for any $t\\geq1$ and $\\overline{{c}}_{3}:=\\operatorname*{max}\\{c_{3},1\\}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\tilde{x}_{t})]-F^{*}\\le\\frac{[(c_{2}c_{4}+1)L_{f}+48c_{1}c_{4}L_{\\hat{g}}]D^{2}}{2^{t}}+2\\bar{c}_{3}\\delta_{f}+\\frac{8}{3}\\delta_{\\hat{g}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "To construct $\\tilde{x}_{t}$ , the algorithm needs to make $O(2^{t})$ queries to $\\widehat g$ and $O(t)$ queries to $\\overline{{g}}$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. The algorithm iterates $(\\widetilde x_{t+1},x_{t+1},M_{t+1})\\cong\\operatorname{UniSgd}_{\\hat{G}_{t},\\psi}(x_{t},M_{t},2^{t+1};D)$ for $t\\geq0$ , where $\\widehat{G}_{t}=\\mathrm{SvrgOrac}_{\\widehat{g}}(\\tilde{x}_{t})$ . Applying Lemma 29 with $\\overline{{M}}:=c_{2}L_{f}+48c_{1}L_{\\widehat{g}}$ (for which $\\begin{array}{r}{\\alpha=\\frac{1}{12}}\\end{array}$ so that $6\\alpha2^{t+1}=2^{t}$ ) anpd passing to full expectations, we  o\u010ebtain, for any $t\\geq0$ ,p ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\Big[2^{t+1}\\big[F(\\tilde{x}_{t+1})-F^{*}\\big]+\\frac{M_{t+1}}{2}\\|x_{t+1}-x^{*}\\|^{2}\\Big]}\\\\ {\\le\\mathbb{E}\\Big[2^{t}\\beta_{t}+\\frac{M_{t}}{2}\\|x_{t}-x^{*}\\|^{2}+c_{4}\\big[\\operatorname*{min}\\{M_{t+1},\\overline{{M}}\\}-M_{t}\\big]_{+}D^{2}\\Big]+2^{t+1}\\Big(c_{3}\\delta_{f}+\\frac{4}{3}\\delta_{\\tilde{g}}\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\beta_{t}:=\\beta_{f}^{\\nabla f(x^{*})}(x^{*},\\tilde{x}_{t})$ and $\\nabla f(x^{*})\\in\\partial f(x^{*})$ can be chosen arbitrarily. Rewriting $F_{t+1}:=$ $F(\\tilde{x}_{t+1})-F^{*}$ as $F_{t+1}\\,=\\,\\beta_{t+1}+\\left(F_{t+1}\\,-\\,\\beta_{t+1}\\right)$ and telescoping the above inequalities (using, Lemma 18), we get, for any $t\\geq1$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big[2^{t}\\beta_{t}+\\displaystyle\\sum_{i=1}^{t}2^{i}(F_{i}-\\beta_{i})+\\displaystyle\\frac{M_{t}}{2}\\|x_{t}-x^{*}\\|^{2}\\Big]}\\\\ &{\\quad\\le\\beta_{0}+\\displaystyle\\frac{M_{0}}{2}\\|x_{0}-x^{*}\\|^{2}+\\Big(c_{3}\\delta_{f}+\\displaystyle\\frac{4}{3}\\delta_{\\hat{g}}\\Big)\\displaystyle\\sum_{i=1}^{t}2^{i}+c_{4}\\,\\mathbb{E}\\big\\{\\big[\\mathrm{min}\\{M_{t},\\overline{{M}}\\}-M_{0}\\big]_{+}D^{2}\\big\\}}\\\\ &{\\quad\\le\\beta_{0}+2(2^{t}-1)\\Big(c_{3}\\delta_{f}+\\displaystyle\\frac{4}{3}\\delta_{\\hat{g}}\\Big)+c_{4}\\overline{{M}}D^{2}=:\\Phi_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the final inequality is due to the fact that $M_{0}=0$ , while $\\begin{array}{r}{\\sum_{i=1}^{t}2^{i}=2(2^{t}-1)}\\end{array}$ . According to Lemma 30, we can choose $\\nabla f(x^{*})\\in\\partial f(x^{*})$ such that $\\beta_{i}\\leq F(\\tilde{x}_{i})-F^{*}$ for all $i\\geq0$ . Dropping now various nonnegative terms from the left-hand side of the above display, we conclude that ", "page_idx": 26}, {"type": "equation", "text": "$$\n2^{t}\\operatorname{\\mathbb{E}}[F_{t}]\\leq\\Phi_{0}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let us estimate $\\Phi_{0}$ . Using our Assumptions 1 and 2 and Theorem 14 (inequality (14)), we can bound $\\beta_{0}\\leq L_{f}\\|\\tilde{x}_{0}-x^{*}\\|^{2}+\\bar{2}\\delta_{f}\\leq L_{f}{D^{2}}^{*}+2\\delta_{f}.$ . Therefore, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Phi_{0}\\le L_{f}D^{2}+2\\delta_{f}+c_{4}\\overline{{M}}D^{2}+2(2^{t}-1)(c_{3}\\delta_{f}+\\textstyle\\frac{4}{3}\\delta_{\\hat{g}})\\le L D^{2}+2(\\overline{{c}}_{3}\\delta_{f}+\\textstyle\\frac{4}{3}\\delta_{\\hat{g}})\\cdot2^{t}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $L:=L_{f}+c_{4}\\overline{{{M}}}\\equiv(c_{2}c_{4}+1)L_{f}+48c_{1}c_{4}L_{\\widehat{g}}$ and $\\overline{{c}}_{3}:=\\operatorname*{max}\\{c_{3},1\\}$ . sThus, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}[F_{t}]\\leq\\frac{\\Phi_{0}}{2^{t}}\\leq\\frac{L D^{2}}{2^{t}}+2\\bar{c}_{3}\\delta_{f}+\\frac{8}{3}\\delta_{\\hat{g}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which proves the claimed convergence rate. ", "page_idx": 26}, {"type": "text", "text": "Let us now estimate the number of oracle queries. At each iteration $t$ , the algorithm first queries $\\overline{{g}}$ to construct the SVRG oracle $\\widehat{G}_{t}$ (by precomputing $\\overline{{g}}(\\tilde{x}_{t}))$ . All other queries are then done on lsy to $\\widehat{G}_{t}$ or, equivalently, to $\\widehat g$ ins ipde $\\mathrm{UniSgd}_{\\hat{G}_{t},\\psi}$ whic hs is run for $N_{t+1}\\,=\\,2^{t+1}$ iterations and thus re qupiring $O(N_{t+1})$ queri eps to $\\widehat g$ . Summin gp up, after $T$ iterations, we obtain the total number of $\\begin{array}{r}{\\sum_{t=1}^{T}O(N_{t})=\\sum_{t=1}^{T}O(2^{t})=O(2^{T})}\\end{array}$ queries to $\\widehat g$ , and $T$ queries to $\\overline{{g}}$ . \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Helper Lemmas ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Lemma 30. Let $F\\colon\\mathbb{R}^{d}\\rightarrow\\mathbb{R}\\cup\\{+\\infty\\}$ be the function $F(x):=f(x)+\\psi(x),$ , where $f\\colon\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ is $a$ convex function, and $\\psi\\colon\\mathbb{R}^{d}\\rightarrow\\mathbb{R}\\cup\\{+\\infty\\}$ is a proper closed convex function. Let $x^{*}$ be a minimizer of $F$ and let $F^{*}:=F(x^{*})$ . Then, there exists $\\nabla f(x^{*})\\in\\partial f(x^{*})$ such that, for any $x\\in\\operatorname{dom}\\psi$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\nF(x)-F^{*}\\geq\\beta_{f}^{\\nabla f(x^{*})}(x^{*},x).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Since $x^{*}$ is a minimizer of $F$ , we have $0\\in\\partial F(x^{*})=\\partial f(x^{*})+\\partial\\psi(x^{*})$ . In other words, there exists $\\nabla f(x^{*})\\in\\partial f(x^{*})$ such that $\\nabla\\psi(x^{*}):=-\\nabla f(x^{*})\\in\\partial\\psi(x^{*})$ . Consequently, for any $x\\in\\operatorname{dom}\\psi$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(x)-F^{*}=f(x)-f(x^{*})+[\\psi(x)-\\psi(x^{*})]\\geq f(x)-f(x^{*})+\\langle\\nabla\\psi(x^{*}),x-x^{*}\\rangle}\\\\ &{\\qquad\\qquad=f(x)-f(x^{*})-\\langle\\nabla f(x^{*}),x-x^{*}\\rangle=\\beta_{f}^{\\nabla f(x^{*})}(x^{*},x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "E.2 Universal Fast SVRG ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Lemma 31 (Universal Triangle SVRG Step). Consider problem (1) under Assumptions $I,2$ and $^{6}$ . Let $\\tilde{x},v\\in\\mathrm{dom}\\,\\psi$ be points, $M\\geq0$ and $A,a>0$ be coefficients, $\\widehat{G}:=\\mathrm{SvrgOrac}_{\\widehat{g}}(\\widetilde{x})$ . Further, let, for $A_{+}:=A+a_{;}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle x:=\\frac{A\\widetilde{x}+a v}{A_{+}},\\quad\\widehat{G}_{x}\\cong\\widehat{G}(x),\\quad\\widehat{v}_{+}=\\operatorname*{Prox}_{\\psi}(v,\\widehat{G}_{x},M/a),\\quad\\widehat{x}_{+}=\\frac{A\\widetilde{x}+a\\widehat{v}_{+}}{A_{+}},}\\\\ {\\displaystyle\\widehat{G}_{x_{+}}\\cong\\widehat{G}(\\widehat{x}_{+}),\\quad\\widehat{M}_{+}=\\frac{a^{2}}{A_{+}}M_{+}\\Big(\\frac{A_{+}}{a^{2}}M,\\displaystyle\\frac{a^{2}}{A_{+}^{2}}D^{2},x,\\widehat{x}_{+},\\widehat{G}_{x},\\widehat{G}_{x_{+}}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, for $\\begin{array}{r}{\\overline{{M}}:=c_{2}L_{f}\\frac{a^{2}}{A_{+}}+6c_{1}L_{\\widehat{g}}\\frac{a^{2}}{A}}\\end{array}$ , it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\Big[A_{+}[F(\\widehat{x}_{+})-F^{*}]+\\frac{\\widehat{M}_{+}}{2}\\|\\widehat{v}_{+}-x^{*}\\|^{2}\\Big]}\\\\ {\\displaystyle\\;\\;\\leq A[F(\\widetilde{x})-F^{*}]+\\frac{M}{2}\\|v-x^{*}\\|^{2}+c_{4}D^{2}\\,\\mathbb{E}\\big\\{[\\operatorname*{min}\\{\\widehat{M}_{+},\\overline{{M}}\\}-M]_{+}\\big\\}+c_{3}A_{+}\\delta_{f}+\\frac{5}{3}A\\delta_{\\widehat{g}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Since $\\widehat g$ is an unbiased oracle for $\\overline{{g}}$ , so is $\\hat{G}$ (Lemma 28). Therefore, we can apply Lemma 26 to obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\Big[A_{+}[F(\\widehat{x}_{+})-F^{*}]+\\displaystyle\\frac{\\widehat{M}_{+}}{2}\\|\\widehat{v}_{+}-x^{*}\\|^{2}+A_{+}\\beta_{f,\\bar{f},\\bar{g}}(\\widehat{x}_{+},x)\\Big]+A\\beta_{f,\\bar{f},\\bar{g}}(x,\\widetilde{x})}}\\\\ &{}&{\\leq A[F(\\widetilde{x})-F^{*}]+\\displaystyle\\frac{M}{2}\\|v-x^{*}\\|^{2}+\\displaystyle\\frac{c_{1}a^{2}}{\\overline{{M}}-c_{2}L_{f}\\frac{a^{2}}{A_{+}}}\\mathbb{E}[\\mathrm{Var}_{\\widehat{G}}(\\widehat{x}_{+})+\\mathrm{Var}_{\\widehat{G}}(x)]\\quad\\quad\\quad}\\\\ &{}&{\\quad+\\,c_{3}A_{+}\\delta_{f}+c_{4}\\,\\mathbb{E}\\big\\{[\\operatorname*{min}\\{\\widehat{M}_{+},\\overline{{M}}\\}-M]_{+}D^{2}\\big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\overline{{{M}}}>c_{2}L_{f}\\frac{a^{2}}{A_{+}}$ is an arbitrary coefficient. Using Lemmas 19 and 28 and A ssumption 6, we can furthe r b\u010eound ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{Var}_{\\hat{G}}(\\hat{x}_{+})+\\operatorname{Var}_{\\hat{G}}(x)=\\operatorname{Var}_{\\hat{g}}(\\hat{x}_{+},\\tilde{x})+\\operatorname{Var}_{\\hat{g}}(x,\\tilde{x})\\leq2\\operatorname{Var}_{\\hat{g}}(\\hat{x}_{+},x)+3\\operatorname{Var}_{\\hat{g}}(x,\\tilde{x})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2L_{\\hat{g}}[2\\beta_{f,\\bar{f},\\bar{g}}(\\hat{x}_{+},x)+3\\beta_{f,\\bar{f},\\bar{g}}(x,\\tilde{x})+5\\delta_{\\hat{g}}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Denoting M2\u2212cc12LLgpfa Aa2+, we thus obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big[A_{+}[F(\\widehat{x}_{+})-F^{*}]+\\displaystyle\\frac{\\widehat{M}_{+}}{2}\\|\\widehat{v}_{+}-x^{*}\\|^{2}+A_{+}\\beta_{f,\\bar{f},\\bar{g}}(\\widehat{x}_{+},x)\\Big]+A\\beta_{f,\\bar{f},\\bar{g}}(x,\\widetilde{x})}\\\\ &{\\quad\\le A[F(\\widetilde{x})-F^{*}]+\\displaystyle\\frac{M}{2}\\|v-x^{*}\\|^{2}+c_{3}A_{+}\\delta_{f}+5\\alpha\\delta_{\\widehat{g}}+c_{4}\\,\\mathbb{E}\\big\\{\\big[\\operatorname*{min}\\{\\widehat{M}_{+},\\overline{{{M}}}\\}-M\\big]_{+}D^{2}\\big\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,2\\alpha\\,\\mathbb{E}[\\beta_{f,\\bar{f},\\bar{g}}(\\widehat{x}_{+},x)]+3\\alpha\\beta_{f,\\bar{f},\\bar{g}}(x,\\widetilde{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Choosing now $\\begin{array}{r}{\\overline{{M}}=c_{2}L_{f}\\frac{a^{2}}{A_{+}}+6c_{1}L_{\\hat{g}}\\frac{a^{2}}{A}}\\end{array}$ , we get $\\begin{array}{r}{\\alpha=\\frac{1}{3}A\\left(\\leq\\frac{1}{3}A_{+}\\right)}\\end{array}$ , pwhich allows us to drop the nonnegative $\\beta_{f,\\bar{f},\\bar{g}}(\\cdot,\\cdot)$ terms from botph sides. The claim now follows. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Lemma 32 (Universal Triangle SVRG Epoch). Consider problem (1) under Assumptions $^{\\,l}$ , 2 and $6$ . Let $\\tilde{x},v\\in\\mathrm{dom}\\,\\psi$ be points, $M\\geq0$ and $A,a>0$ be coefficients, $N\\geq1$ be an integer, and let ", "page_idx": 27}, {"type": "equation", "text": "$$\n(\\tilde{x}_{+},v_{+},M_{+})\\cong\\mathrm{UniTriSvrgEpoch}_{\\hat{g},\\psi}(\\tilde{x},v,M,A,a,N;D),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "as defined by Algorithm 5. Then, for $A_{+}:=A+a$ and $\\begin{array}{r}{\\overline{{M}}:=c_{2}L_{f}\\frac{a^{2}}{A_{+}}+6c_{1}L_{\\hat{g}}\\frac{a^{2}}{A}}\\end{array}$ , it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\Big[A_{+}N[F(\\tilde{x}_{+})-F^{*}]+\\frac{M_{+}}{2}\\|v_{+}-x^{*}\\|^{2}\\Big]}\\\\ &{\\leq A N[F(\\tilde{x})-F^{*}]+\\displaystyle\\frac{M}{2}\\|v-x^{*}\\|^{2}+c_{4}D^{2}\\,\\mathbb{E}\\big\\{\\big[\\operatorname*{min}\\{M_{+},\\overline{{M}}\\}-M\\}_{+}\\big\\}+N\\Big(c_{3}A_{+}\\delta_{f}+\\displaystyle\\frac{5}{3}A\\delta_{\\tilde{g}}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Each iteration $k$ of the algorithm, when conditioned on $v_{k}$ , follows the construction from Lemma 31 (with $v\\ =\\ v_{k}$ , $M\\,=\\,M_{k}$ , $A\\,=\\,A_{k}$ , $a\\,=\\,a_{k+1}$ , $A_{+}\\,=\\,A_{k+1}$ , $x\\ =x_{k}$ , $\\hat{G}_{x}\\,=\\,G_{x_{k}}$ , $\\widehat{v}_{+}=v_{k+1},\\widehat{x}_{+}=x_{k+1},\\widehat{G}_{x_{+}}=G_{x_{k+1}},\\widehat{M}_{+}=M_{k+1}).$ Hence, we can write, after pa spsing to full expectations, for each $k=0,\\ldots,N-1$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big[A_{+}[F(x_{k+1})-F^{*}]+\\displaystyle\\frac{M_{k+1}}{2}\\|v_{k+1}-x^{*}\\|^{2}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\leq A[F(\\tilde{x})-F^{*}]+\\mathbb{E}\\Big[\\displaystyle\\frac{M_{k}}{2}\\|v_{k}-x^{*}\\|^{2}+c_{4}[\\operatorname*{min}\\{M_{k+1},\\overline{{M}}\\}-M_{k}]_{+}D^{2}\\Big]+\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\begin{array}{r}{\\delta:=c_{3}A_{+}\\delta_{f}+\\frac{5}{3}A\\delta_{\\widehat{g}}}\\end{array}$ . Telescoping the above inequalities (using Lemma 18), we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\Big[A_{+}\\sum_{k=1}^{N}[F(x_{k})-F^{*}]+\\frac{M_{N}}{2}\\|v_{N}-x^{*}\\|^{2}\\Big]}\\\\ {\\displaystyle\\leq A N[F(\\tilde{x})-F^{*}]+\\frac{M_{0}}{2}\\|v_{0}-x^{*}\\|^{2}+c_{4}D^{2}\\,\\mathbb{E}\\big\\{[\\operatorname*{min}\\{M_{N},\\overline{{M}}\\}-M_{0}]_{+}\\big\\}+N\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The claim now follows from the convexity of F and our definitions x\u02dc+ =xN = N1 kN=1 xk, $v_{+}=v_{N}$ , $M_{+}=M_{N}$ , $M_{0}=M$ , $v_{0}=v$ . ", "page_idx": 28}, {"type": "text", "text": "Theorem 11. Let UniFastSvrg (Algorithm 4) be applied to problem (1) under Assumptions $^{\\,l}$ , 2 and $6$ , and let $N\\geq9$ . Then, for any $t\\geq t_{0}:=\\lceil\\log_{2}\\log_{3}N\\rceil-1\\left(\\geq0\\right)$ , it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\tilde{x}_{t})]-F^{*}\\leq\\frac{9[(c_{2}c_{4}+\\frac12)L_{f}+6c_{1}c_{4}L_{\\hat{g}}]D^{2}}{N(t-t_{0}+1)^{2}}+(c_{3}t+1)\\delta_{f}+\\frac53t\\delta_{\\hat{g}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "To construct $\\tilde{x}_{t}$ , the algorithm needs to make $O(N t)$ queries to $\\widehat g$ and $O(t)$ queries to $\\overline{{g}}$ . Assuming that the complexity of querying $\\overline{{g}}$ is $n$ times bigger than that of querying $\\widehat g$ and choosing $N=\\Theta(n)$ , we get the total stochastic-oracle complexity of $O(n t)$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. By our definition, the algorithm iterates for $t\\geq0$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n(\\tilde{x}_{t+1},v_{t+1},M_{t+1})\\cong\\operatorname{UniTriSvrgEpoch}_{\\hat{g},\\bar{g},\\psi}(\\tilde{x}_{t},v_{t},M_{t},A_{t},a_{t+1},N;D),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $A_{t}$ and $a_{t+1}$ are deterministic coefficients satisfpyisng the following equations: ", "page_idx": 28}, {"type": "equation", "text": "$$\nA_{t+1}=A_{t}+a_{t+1},\\qquad a_{t+1}={\\sqrt{A_{t}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In particular, for any t \u22650, we haveM t\u2032 := c2LfAatt++11 $\\begin{array}{r}{\\overline{{M}}_{t}^{\\prime}:=c_{2}L_{f}\\frac{a_{t+1}^{2}}{A_{t+1}}+6c_{1}L_{\\widehat{g}}\\frac{a_{t+1}^{2}}{A_{t}}\\leq c_{2}L_{f}+6c_{1}L_{\\widehat{g}}=:\\overline{{M}}}\\end{array}$ , and hence $[\\operatorname*{min}\\{M_{t+1},\\overline{{M}}_{t}^{\\prime}\\}-M_{t}]_{+}\\leq[\\operatorname*{min}\\{M_{t+1},\\overline{{M}}\\}-M_{t}]_{+}$ (pbecause, for any fixedp $a$ an d\u010e $b$ , the function $[\\operatorname*{min}\\{a,\\cdot\\}-b]_{+}$ is nondecreasing as th e\u010e composition of two nondecreasing functions). Applying now Lemma 32 and passing to full expectations, we therefore obtain, for any $t\\geq0$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\Big[A_{t+1}N[F(\\tilde{x}_{t+1})-F^{*}]+\\displaystyle\\frac{M_{t+1}}{2}\\|v_{t+1}-x^{*}\\|^{2}\\Big]}\\\\ &{\\leq\\mathbb{E}\\Big[A_{t}N[F(\\tilde{x}_{t})\\!-\\!F^{*}]\\!+\\!\\displaystyle\\frac{M_{t}}{2}\\|v_{t}\\!-\\!x^{*}\\|^{2}\\!+\\!c_{4}\\big[\\operatorname*{min}\\{M_{t+1},\\overline{{M}}\\}\\!-\\!M_{t}\\big]_{+}D^{2}\\Big]\\!+\\!N\\Big(c_{3}A_{t+1}\\delta_{f}\\!+\\!\\frac{5}{3}A_{t}\\delta_{\\hat{g}}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Telescoping the above inequalities (using, in particular, Lemma 18), we obtain, for any $t\\geq1$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle A_{t}N\\mathbb{E}[F(\\tilde{x}_{t})-F^{*}]\\leq A_{0}N[F(\\tilde{x}_{0})-F^{*}]+\\frac{M_{0}}{2}\\|v_{0}-x^{*}\\|^{2}}\\\\ {\\displaystyle\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +c_{4}\\mathbb{E}\\bigl\\{\\bigl[\\operatorname*{min}\\{M_{t},\\overline{{M}}\\}-M_{0}\\bigr]_{+}D^{2}\\bigr\\}+N\\Bigl(c_{3}\\delta_{f}\\underset{i=1}{\\overset{t}{\\sum}}A_{i}+\\frac{5}{3}\\delta_{\\tilde{\\phi}}\\underset{i=0}{\\overset{t-1}{\\sum}}A_{i}\\Bigr)}\\\\ {\\displaystyle\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leq A_{0}N[F(\\tilde{x}_{0})-F^{*}]+c_{4}\\overbar{M}D^{2}+N S_{t}(c_{3}\\delta_{f}+\\frac{5}{3}\\delta_{\\tilde{\\phi}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where, for the last inequality, we have used the fact that $M_{0}=0$ and  denoted $\\begin{array}{r}{S_{t}:=\\sum_{i=1}^{t}A_{i}}\\end{array}$ . Thus, for any $t\\geq1$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\tilde{x}_{t})]-F^{*}\\le\\frac{1}{A_{t}}\\Big(A_{0}[F(\\tilde{x}_{0})-F^{*}]+\\frac{c_{4}\\overline{{M}}D^{2}}{N}\\Big)+\\frac{S_{t}}{A_{t}}\\Big(c_{3}\\delta_{f}+\\frac{5}{3}\\delta_{\\hat{g}}\\Big).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "At the same time, according to (17), $A_{t+1}-A_{t}={\\sqrt{A_{t}}}$ for any $t\\geq0$ . Hence, by Lemma 33 (and our assumption on $A_{0}$ ), we can estimate $\\begin{array}{r}{A_{t}\\geq\\frac{1}{9}(t-t_{0}+1)^{2}}\\end{array}$ for any $\\begin{array}{r}{t\\geq t_{0}:=\\lceil\\log_{2}\\log_{3}\\frac{1}{A_{0}}\\rceil-1\\left(\\geq0\\right)}\\end{array}$ . Further, since the sequence $A_{t}$ is increasing, we can estimate $\\begin{array}{r}{S_{t}\\equiv\\sum_{i=1}^{t}A_{i}\\leq t A_{t}}\\end{array}$ , so that $\\begin{array}{r}{\\frac{S_{t}}{A_{t}}\\leq t}\\end{array}$ . Substituting these bounds into the above display and using our formula for $A_{0}$ , we obtain, for any $t\\geq t_{0}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[F(\\tilde{x}_{t})]-F^{*}\\le\\rho_{t}[F(\\tilde{x}_{0})-F^{*}+c_{4}\\overline{{M}}D^{2}]+t(c_{3}\\delta_{f}+\\frac{5}{3}\\delta_{\\hat{g}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\begin{array}{r}{\\rho_{t}:=\\frac{9}{N(t-t_{0}+1)^{2}}\\le1}\\end{array}$ . By our choice of $\\scriptstyle{\\tilde{x}}_{0}$ , it holds that $\\begin{array}{r}{F(\\tilde{x}_{0})-F^{*}\\le\\frac{1}{2}L_{f}D^{2}+\\delta_{f}}\\end{array}$ (see Lemma 34). Denoting $\\begin{array}{r}{L:=\\frac12L_{f}+c_{4}\\overline{{M}}\\equiv\\big(c_{2}c_{4}+\\frac{1}{2}\\big)L_{f}+6c_{1}c_{4}L_{\\widehat{g}}}\\end{array}$ , we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[F(\\tilde{x}_{t})]-F^{*}\\le\\rho_{t}(L D^{2}+\\delta_{f})+t(c_{3}\\delta_{f}+\\frac{5}{3}\\delta_{\\hat{g}})\\le\\rho_{t}L D^{2}+(c_{3}t+1)\\delta_{f}+\\frac{5}{3}t\\delta_{\\hat{g}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which is exactly the claimed convergence rate bound. ", "page_idx": 29}, {"type": "text", "text": "Let us now estimate the number of oracle queries. At the beginning, the algorithm makes 1 query to $\\overline{{g}}$ to compute $\\scriptstyle{\\tilde{x}}_{0}$ . All other queries to the oracles are then done, at each iteration $t$ , only inside the call to UniTriSvrgEpoch (Algorithm 5). Each such a call needs only one query to $\\overline{{g}}$ to construct the SVRG oracle $\\hat{G}$ (by precomputing $\\overline{{g}}(\\tilde{x})$ ), and $O(N)$ queries to $\\widehat g$ (which impleme nsts each query to $\\hat{G}$ ). Summing  pup, we get, after $t$ i tserations, the total number  opf $O(N t)$ queries to $\\widehat g$ and $O(t)$ qu epries to $\\overline{{g}}$ . \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Helper Lemmas ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Lemma 33 (c.f. Lemma 1.1 in [23]). Let $A_{t}$ be a positive sequence such that ", "page_idx": 29}, {"type": "equation", "text": "$$\nA_{t+1}-A_{t}\\geq{\\sqrt{\\gamma A_{t}}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for all $t\\geq0$ , where $\\gamma>0$ , and let $\\begin{array}{r}{A_{0}\\leq\\frac{1}{9}\\gamma.}\\end{array}$ . Then, for any $t\\geq0$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\nA_{t}\\geq\\left\\{\\gamma(\\frac{A_{0}}{\\gamma})^{1/2^{t}},\\begin{array}{l}{\\!\\qquad i f t<t_{0},}\\\\ {\\!\\qquad\\frac{\\gamma}{9}(t-t_{0}+1)^{2},\\quad\\!i f t\\geq t_{0},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\begin{array}{r}{t_{0}:=\\lceil\\log_{2}\\log_{3}\\frac{\\gamma}{A_{0}}\\rceil-1\\left(\\geq0\\right)}\\end{array}$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. By replacing $A_{t}$ with $A_{t}^{\\prime}=A_{t}/\\gamma$ , we can assume w.l.o.g. that $\\gamma=1$ . ", "page_idx": 29}, {"type": "text", "text": "For any $t\\geq0$ , we have $A_{t+1}\\geq\\sqrt{A_{t}}$ , and hence ", "page_idx": 29}, {"type": "equation", "text": "$$\nA_{t}\\geq A_{0}^{1/2^{t}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In particular, for $t_{0}$ (as defined in the statement), we get $t_{0}\\geq\\log_{2}\\log_{3}{\\frac{1}{A_{0}}}-1$ , so $\\begin{array}{r}{2^{t_{0}}\\geq\\frac{1}{2}\\log_{3}\\frac{1}{A_{0}}}\\end{array}$ , and hence ", "page_idx": 29}, {"type": "equation", "text": "$$\nA_{t_{0}}\\geq A_{0}^{2/\\log_{3}(1/A_{0})}=\\left(3^{-\\log_{3}(1/A_{0})}\\right)^{2/\\log_{3}(1/A_{0})}=3^{-2}={\\frac{1}{9}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "(recall that $\\begin{array}{r}{A_{0}\\leq\\frac{1}{9}\\leq1;}\\end{array}$ ). ", "page_idx": 29}, {"type": "text", "text": "On the other hand, for any $t\\geq t_{0}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{A_{t+1}}-\\sqrt{A_{t}}\\geq\\sqrt{A_{t}+\\sqrt{A_{t}}}-\\sqrt{A_{t}}=\\frac{\\sqrt{A_{t}}}{\\sqrt{A_{t}+\\sqrt{A_{t}}}+\\sqrt{A_{t}}}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{\\sqrt{1+\\frac{1}{\\sqrt{A_{t}}}}+1}\\geq\\frac{1}{\\sqrt{1+3}+1}=\\frac{1}{3},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we have used the fact that $\\begin{array}{r}{A_{t}\\geq A_{t_{0}}\\geq\\frac{1}{9}}\\end{array}$ since $A_{t}$ is monotonically increasing. Telescoping these inequalities and rearranging, we get, for any $t\\geq t_{0}$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\nA_{t}\\geq\\left({\\frac{1}{3}}(t-t_{0})+{\\sqrt{A_{t_{0}}}}\\right)^{2}\\geq\\left({\\frac{1}{3}}(t-t_{0})+{\\frac{1}{3}}\\right)^{2}={\\frac{1}{9}}(t-t_{0}+1)^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma 34. Consider problem (1) under Assumptions $^{\\,l}$ and 2. Let $x\\,\\in\\,\\operatorname{dom}\\psi$ , and let $x_{+}:=$ ${\\mathrm{Prox}}_{\\psi}(x,\\bar{g}(x),0)$ . Then, $\\begin{array}{r}{F(x_{+})-F^{*}\\le\\frac12L_{f}D^{2}+\\delta_{f}}\\end{array}$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. From the first-order optimality condition for the point $x_{+}$ (see Lemma 17), it follows that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\langle\\bar{g}(x),x^{*}-x_{+}\\rangle+\\psi(x^{*})\\geq\\psi(x_{+}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Combining the above inequality first with $\\begin{array}{r}{f(x_{+})\\leq\\bar{f}(x)+\\langle\\bar{g}(x),x_{+}-x\\rangle+\\frac{L_{f}}{2}\\|x_{+}-x\\|^{2}+\\delta_{f}}\\end{array}$ and then with $\\bar{f}(x)+\\langle\\bar{g}(x),x^{*}-x\\rangle\\leq f(x^{*})$ (which sare both sdue to our Assumption 1), we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(x_{+})=f(x_{+})+\\psi(x_{+})\\leq f(x_{+})+\\langle\\overline{{g}}(x),x^{*}-x_{+}\\rangle+\\psi(x^{*})}\\\\ &{\\quad\\quad\\leq\\overline{{f}}(x)+\\langle\\overline{{g}}(x),x^{*}-x\\rangle+\\psi(x^{*})+\\displaystyle\\frac{L_{f}}{2}\\|x_{+}-x\\|^{2}+\\delta_{f}}\\\\ &{\\quad\\quad\\leq F^{*}+\\displaystyle\\frac{L_{f}}{2}\\|x_{+}-x\\|^{2}+\\delta_{f}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "It remains to bound $\\|x_{+}-x\\|\\leq D$ . ", "page_idx": 30}, {"type": "text", "text": "F Omitted Proofs for Section 7 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We start with the observation that for our specific example all our main assumptions are satisfied. Remark 35. Under the setting from Example 12, Assumptions 1, 6 and 9 are satisfied with ${\\bar{f}}=f$ , $\\overline{{g}}(x)=\\nabla f(x):=\\mathbb{E}_{\\xi}[\\nabla f_{\\xi}(x)]$ , any $\\delta_{f},\\delta_{\\hat{g}}>0$ and ", "page_idx": 30}, {"type": "equation", "text": "$$\nL_{f}=\\left[\\frac{1-\\nu}{2(1+\\nu)\\delta_{f}}\\right]^{\\frac{1-\\nu}{1+\\nu}}[H_{f}(\\nu)]^{\\frac{2}{1+\\nu}},\\qquad L_{\\hat{g}}=\\frac{1}{b}\\bigg[\\frac{1-\\nu}{2(1+\\nu)\\delta_{\\hat{g}}}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}[H_{\\operatorname*{max}}(\\nu)]^{\\frac{2}{1+\\nu}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Further, the oracle $\\widehat{g}_{b}$ satisfies Assumption 3 with $\\begin{array}{r}{\\sigma_{b}^{2}:=\\operatorname*{sup}_{x\\in\\operatorname*{dom}\\psi}\\operatorname{Var}_{\\hat{g}_{b}}(x)=\\frac{1}{b}\\sigma^{2},}\\end{array}$ , and $\\sigma_{*,b}^{2}:=$ $\\begin{array}{r}{\\mathrm{Var}_{\\hat{g}_{b}}(x^{*})=\\frac{1}{b}\\sigma_{*}^{2}}\\end{array}$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. For $b=1$ , this follows from Theorem 13 and Lemma 16 and our definitions of $\\sigma^{2}$ and $\\sigma_{*}$ . The general case $b\\geq1$ follows from the fact that the standard mini-batching of size $b$ reduces each of the variances $\\operatorname{Var}_{\\widehat{g}_{1}}(\\cdot)$ and $\\operatorname{Var}_{\\hat{g}_{1}}(\\cdot,\\cdot)$ in $b$ times. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "The following auxiliary result will be useful throughout this section: ", "page_idx": 30}, {"type": "text", "text": "Lemma 36. Let $a,b,p>0$ be real. Then, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t>0}\\Bigl\\{\\frac{a}{t^{p}}+b t\\Bigr\\}=(p+1)a^{\\frac{1}{p+1}}\\biggl(\\frac{b}{p}\\biggr)^{\\frac{p}{p+1}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. The expression inside the min is a convex function in $t>0$ . Differentiating and setting its derivative to zero, we see that the minimum is attained at $\\begin{array}{r}{t_{*}=\\left(\\frac{a p}{b}\\right)^{\\frac{1}{p+1}}}\\end{array}$ . Hence, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t>0}\\Bigl\\{\\frac{a}{t^{p}}+b t\\Bigr\\}=a\\biggl(\\frac{b}{a p}\\biggr)^{\\frac{p}{p+1}}+b\\biggl(\\frac{a p}{b}\\biggr)^{\\frac{1}{p+1}}=(p+1)a^{\\frac{1}{p+1}}\\biggl(\\frac{b}{p}\\biggr)^{\\frac{p}{p+1}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "F.1 Uniformly Bounded Variance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Corollary 37. Consider problem (1) under the setting from Example $^{12}$ and also under Assumption 2. Let Algorithm 1 be applied to this problem with the oracle $\\widehat{g}=\\widehat{g}_{b}$ and initial coefficient $M_{0}=0$ . Then, for the point ${\\bar{x}}_{N}$ generated by the algorithm, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\bar{x}_{N})]-F^{*}\\le\\frac{(2c_{2}c_{4})^{\\frac{1+\\nu}{2}}c_{3}^{\\frac{1-\\nu}{2}}}{1+\\nu}\\frac{H_{f}(\\nu)D^{1+\\nu}}{N^{\\frac{1+\\nu}{2}}}+2\\sigma D\\sqrt{\\frac{2c_{1}c_{4}}{b N}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "To reach $\\mathbb{E}[F({\\overline{{x}}}_{N})]-F^{*}\\leq\\epsilon$ for any $\\epsilon>0$ , it suffices to make $\\begin{array}{r}{O\\big([\\frac{H_{f}(\\nu)}{\\epsilon}]^{\\frac{2}{1+\\nu}}D^{2}+\\frac{\\sigma^{2}D^{2}}{b\\epsilon^{2}}\\big)}\\end{array}$ \u03c3b\u03f5D2  queries to $\\widehat{g}_{b}$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. Denote for brevity $\\boldsymbol{H}_{f}:=\\boldsymbol{H}_{f}(\\nu)$ . Taking into account Remark 35 and applying Theorem 4, we get, for any $\\delta_{f}>0$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\nF_{N}:=\\mathbb{E}[F(\\bar{x}_{N})]-F^{*}\\leq\\frac{c_{2}c_{4}H_{f}^{\\frac{2}{1+\\nu}}D^{2}}{N}\\bigg[\\frac{1-\\nu}{2(1+\\nu)\\delta_{f}}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}+c_{3}\\delta_{f}+\\sigma_{N},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\begin{array}{r}{\\sigma_{N}:=2\\sigma_{b}D\\sqrt{\\frac{2c_{1}c_{4}}{N}}=2\\sigma D\\sqrt{\\frac{2c_{1}c_{4}}{b N}}}\\end{array}$ . Minimizing the right-hand side in $\\delta_{f}$ (using Lemma 36 with $\\begin{array}{r}{p=\\frac{1-\\nu}{1+\\nu}}\\end{array}$ for which $\\textstyle p+1={\\frac{2}{1+\\nu}})$ 1+2\u03bd ), we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{F_{N}\\leq\\displaystyle\\frac{2}{1+\\nu}\\bigg(\\frac{c_{2}c_{4}H_{f}^{\\frac{2}{1+\\nu}}D^{2}}{N}\\bigg[\\frac{1-\\nu}{2(1+\\nu)}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}\\bigg)^{\\frac{1+\\nu}{2}}\\bigg(\\displaystyle\\frac{1+\\nu}{1-\\nu}c_{3}\\bigg)^{\\frac{1-\\nu}{2}}+\\sigma_{N}}}\\\\ {{=\\displaystyle\\frac{(2c_{2}c_{4})^{\\frac{1+\\nu}{2}}c_{3}^{\\frac{1-\\nu}{2}}}{1+\\nu}\\displaystyle\\frac{H_{f}D^{1+\\nu}}{N^{\\frac{1+\\nu}{2}}}+\\sigma_{N}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This proves the claimed convergence rate, and the oracle complexity bound easily follows since each iteration of the algorithm requires only 1 query to $\\widehat{g}_{b}$ . \u53e3 ", "page_idx": 31}, {"type": "text", "text": "Corollary 38. Consider problem (1) under the setting from Example $^{12}$ and also under Assumption 2. Let Algorithm 2 be applied to this problem with the oracle ${\\widehat{g}}={\\widehat{g}}_{b}$ . Then, for any $k\\geq1$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(x_{k})]-F^{*}\\le\\frac{2^{2+\\nu}(c_{2}c_{4})^{\\frac{1+\\nu}{2}}(\\frac{c_{3}}{3})^{\\frac{1-\\nu}{2}}}{1+\\nu}\\frac{H_{f}(\\nu)D^{1+\\nu}}{k^{\\frac{1+3\\nu}{2}}}+4\\sigma D\\sqrt{\\frac{2c_{1}c_{4}}{3b k}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To reach E[F(xk)] \u2212F \u2217\u2264\u03f5 for any \u03f5 > 0, it suffices to make O [ Hf (\u03bd)\u03f5D1+\u03bd] $\\begin{array}{r}{O\\big([\\frac{H_{f}(\\nu)D^{1+\\nu}}{\\epsilon}]^{\\frac{2}{1+3\\nu}}+\\frac{\\sigma^{2}D^{2}}{b\\epsilon^{2}}\\big)}\\end{array}$ \u03c3b\u03f5D2  queries to $\\widehat{g}_{b}$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. Let $k\\geq1$ be arbitrary and denote for brevity $\\boldsymbol{H}_{f}:=\\boldsymbol{H}_{f}(\\nu)$ . Taking into account Remark 35 and applying Theorem 5, we get, for any $\\delta_{f}>0$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\nF_{k}:=\\mathbb{E}[F(x_{k})]-F^{*}\\leq\\frac{4c_{2}c_{4}H_{f}^{\\frac{2}{1+\\nu}}D^{2}}{k(k+1)}\\bigg[\\frac{1-\\nu}{2(1+\\nu)\\delta_{f}}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}+\\frac{c_{3}}{3}(k+2)\\delta_{f}+\\sigma_{k},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\begin{array}{r}{\\sigma_{k}:=4\\sigma_{b}D\\sqrt{\\frac{2c_{1}c_{4}}{3k}}=4\\sigma D\\sqrt{\\frac{2c_{1}c_{4}}{3b k}}}\\end{array}$ . Minimizing the right-hand side in $\\delta_{f}$ (using Lemma 36) and estimating $k+2\\leq2(k+1)$ , we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{k}\\le\\displaystyle\\frac{2}{1+\\nu}\\bigg(\\frac{4c_{2}c_{4}H_{f}^{\\frac{2}{1+\\nu}}D^{2}}{k(k+1)}\\bigg[\\frac{1-\\nu}{2(1+\\nu)}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}\\bigg)^{\\frac{1+\\nu}{2}}\\bigg(\\frac{1+\\nu}{1-\\nu}\\frac{2c_{3}(k+1)}{3}\\bigg)^{\\frac{1-\\nu}{2}}+\\sigma_{k}}\\\\ &{\\quad=\\displaystyle\\frac{2(4c_{2}c_{4})^{\\frac{1+\\nu}{2}}(\\frac{c_{3}}{3})^{\\frac{1-\\nu}{2}}}{1+\\nu}\\frac{H_{f}D^{1+\\nu}}{k^{\\frac{1+\\nu}{2}}(k+1)^{\\nu}}+\\sigma_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This proves the claimed convergence rate, and the oracle complexity bound easily follows since each iteration of the algorithm requires only $O(1)$ queries to $\\widehat{g}_{b}$ . \u53e3 ", "page_idx": 31}, {"type": "text", "text": "Remark 39. The efficiency guarantees given by Corollaries 37 and 38 are exactly the same as those from [49], up to absolute constants. ", "page_idx": 31}, {"type": "text", "text": "F.2 Implicit Variance Reduction ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Corollary 40. Consider problem (1) under the setting from Example $^{12}$ and also under Assumption 2. Let Algorithm $^{\\,l}$ be applied to this problem with the oracle $\\widehat{g}=\\widehat{g}_{b}$ and initial coefficient $M_{0}=0$ Then, for the point ${\\bar{x}}_{N}$ generated by the algorithm, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\bar{x}_{N})]-F^{*}\\leq\\frac{c_{f}(\\nu)H_{f}(\\nu)D^{1+\\nu}}{N^{\\frac{1+\\nu}{2}}}+\\frac{c_{\\hat{g}}(\\nu)H_{\\operatorname*{max}}(\\nu)D^{1+\\nu}}{(b N)^{\\frac{1+\\nu}{2}}}+2\\sigma_{*}D\\sqrt{\\frac{6c_{1}c_{4}}{b N}},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "$\\mathbb{E}[F({\\overline{{x}}}_{N})]-F^{*}\\leq\\epsilon$ $\\begin{array}{r l r}{c_{f}(\\nu)\\!\\!}&{{}:=}&{\\!\\!\\frac{(2c_{2}c_{4})^{\\frac{1+\\nu}{2}}c_{3}^{\\frac{1-\\nu}{2}}}{1+\\nu}\\;=\\;{\\cal O}(1)}\\end{array}$ for 1a+n\u03bdy $\\epsilon>0,$ = O(1) and cg(\u03bd) := (24c4)11+2+\u03bd\u03bd( 43 )12\u03bd , it suffices to makpe $\\begin{array}{r}{O\\big([\\frac{H_{f}(\\nu)}{\\epsilon}]^{\\frac{2}{1+\\nu}}D^{2}+\\frac{1}{b}[\\frac{H_{\\hat{g}}(\\nu)}{\\epsilon}]^{\\frac{2}{1+\\nu}}D^{2}+\\frac{\\sigma_{\\ast}^{2}D^{2}}{b\\epsilon^{2}}\\big)}\\end{array}$ queries to $\\widehat{g}_{b}$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. Denote for brevity ${\\cal F}_{N}:=\\mathbb{E}[{\\cal F}(\\bar{x}_{N})]-F^{*}$ , $\\boldsymbol{H}_{f}:=\\boldsymbol{H}_{f}(\\boldsymbol{\\nu})$ and $H_{\\mathrm{max}}:=H_{\\mathrm{max}}(\\nu)$ . Taking into account Remark 35 and applying Theorem 7, we get, for any $\\delta_{f},\\delta_{\\hat{g}}>0$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\nF_{N}\\leq\\frac{c_{2}c_{4}H_{f}^{\\frac{2}{1+\\nu}}D^{2}}{N}\\bigg[\\frac{1-\\nu}{2(1+\\nu)\\delta_{f}}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}+\\frac{12c_{4}H_{\\mathrm{max}}^{\\frac{2}{1+\\nu}}D^{2}}{b N}\\bigg[\\frac{1-\\nu}{2(1+\\nu)\\delta_{\\hat{g}}}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}+c_{3}\\delta_{f}+\\frac{4}{3}\\delta_{\\hat{g}}+\\sigma_{N},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\begin{array}{r}{\\sigma_{N}:=2\\sigma_{*,b}D\\sqrt{\\frac{6c_{1}c_{4}}{N}}=2\\sigma_{*}D\\sqrt{\\frac{6c_{1}c_{4}}{b N}}}\\end{array}$ . Minimizing the right-hand side in $\\delta_{f}$ and $\\delta_{\\widehat{g}}$ (using Lemma 36 twice), we get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{N}\\leq\\displaystyle\\frac{2}{1+\\nu}\\bigg(\\frac{c_{2}c_{4}H_{f}^{\\frac{2}{1+\\nu}}D^{2}}{N}\\bigg[\\frac{1-\\nu}{2(1+\\nu)}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}\\bigg)^{\\frac{1+\\nu}{2}}\\bigg(\\frac{(1+\\nu)c_{3}}{1-\\nu}\\bigg)^{\\frac{1-\\nu}{2}}}\\\\ &{\\qquad\\qquad+\\displaystyle\\frac{2}{1+\\nu}\\bigg(\\frac{12c_{4}H_{\\mathrm{max}}^{\\frac{2}{1+\\nu}}D^{2}}{b N}\\bigg[\\frac{1-\\nu}{2(1+\\nu)}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}\\bigg)^{\\frac{1+\\nu}{2}}\\bigg(\\frac{(1+\\nu)\\frac{4}{3}}{1-\\nu}\\bigg)^{\\frac{1-\\nu}{2}}+\\sigma_{N}}\\\\ &{\\qquad=\\frac{(2c_{2}c_{4})^{\\frac{1+\\nu}{2}}c_{3}^{\\frac{1-\\nu}{2}}}{1+\\nu}\\frac{H_{f}D^{1+\\nu}}{N^{\\frac{1+\\nu}{2}}}+\\frac{(24c_{4})^{\\frac{1+\\nu}{2}}(\\frac{4}{3})^{\\frac{1-\\nu}{2}}}{1+\\nu}\\frac{H_{\\operatorname*{max}}D^{1+\\nu}}{(b N)^{\\frac{1+\\nu}{2}}}+\\sigma_{N}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This proves the claimed convergence rate, and the oracle complexity bound easily follows since each iteration of the algorithm requires only 1 query to $\\widehat{g}_{b}$ . \u53e3 ", "page_idx": 32}, {"type": "text", "text": "Corollary 41. Consider problem (1) under the sett ipng from Example $^{12}$ and also under Assumption 2. Let Algorithm 2 be applied to this problem with the oracle $\\widehat{g}=\\widehat{g}_{b}$ . Then, for any $k\\geq1$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(x_{k})]-F^{*}\\le\\frac{c_{f}(\\nu)H_{f}(\\nu)D^{1+\\nu}}{k^{\\frac{1+3\\nu}{2}}}+\\frac{c_{\\hat{g}}(\\nu)H_{\\operatorname*{max}}(\\nu)D^{1+\\nu}}{(b k)^{\\frac{1+\\nu}{2}}}+4\\sigma_{*}D\\sqrt{\\frac{2c_{1}c_{4}}{b k}},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "$\\begin{array}{r}{c_{f}(\\nu)\\,:=\\,\\frac{(8c_{2}c_{4})^{\\frac{1+\\nu}{2}}(\\frac23c_{3})^{\\frac{1-\\nu}{2}}}{1+\\nu}=O(1)}\\end{array}$ = O(1) and cg(\u03bd) := (48c1c4)1+2\u03bd( 43 ) 2 . To reach E[F(xk)] \u2212F \u2217\u2264\u03f5 for any \u03f5 > 0, it suffices to mapke O [ Hf (\u03bd)\u03f5D1+\u03bd $O\\big([\\frac{H_{f}(\\nu)D^{1+\\nu}}{\\epsilon}]^{\\frac{2}{1+3\\nu}}+\\frac{1}{b}[\\frac{H_{\\operatorname*{max}}(\\nu)}{\\epsilon}]^{\\frac{2}{1+\\nu}}D^{2}+$ $\\frac{\\sigma_{*}^{2}D^{2}}{b\\epsilon^{2}}\\Big)$ queries to $\\widehat{g}_{b}$ . ", "page_idx": 32}, {"type": "text", "text": "Proof. Let $k\\geq1$ be arbitrary and denote for brevity $F_{k}:=\\mathbb{E}[F(x_{k})]\\,-\\,F^{*}$ , $\\boldsymbol{H}_{f}\\,:=\\,\\boldsymbol{H}_{f}(\\boldsymbol{\\nu})$ and $H_{\\mathrm{max}}\\,:=\\,H_{\\mathrm{max}}(\\nu)$ . Taking into account Remark 35 and applying Theorem 8, we get, for any $\\delta_{f},\\delta_{\\hat{g}}>0$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{}&{F_{k}\\leq\\frac{4c_{2}c_{4}H_{f}^{\\frac{2}{1+\\nu}}D^{2}}{k(k+1)}\\bigg[\\frac{1-\\nu}{2(1+\\nu)\\delta_{f}}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}+\\frac{24c_{1}c_{4}H_{\\operatorname*{max}}^{\\frac{2}{1+\\nu}}D^{2}}{b k}\\bigg[\\frac{1-\\nu}{2(1+\\nu)\\delta_{\\hat{g}}}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}}\\\\ {\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{}&{+\\,\\frac{c_{3}}{3}(k+2)\\delta_{f}+\\frac{4}{3}\\delta_{\\hat{g}}+\\sigma_{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\begin{array}{r}{\\sigma_{k}:=4\\sigma_{*,b}D\\sqrt{\\frac{2c_{1}c_{4}}{k}}=4\\sigma_{*}D\\sqrt{\\frac{2c_{1}c_{4}}{b k}}}\\end{array}$ . Minimizing the right-hand side in $\\delta_{f}$ and $\\delta_{\\widehat{g}}$ (using Lemma 36 twice) and estimating $\\begin{array}{r}{\\frac{1}{3}(k+2)\\leq\\frac{2}{3}(k+1)}\\end{array}$ , we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{k}\\leq\\cfrac{2}{1+\\nu}\\bigg(\\cfrac{4c_{2}c_{4}H_{f}^{\\frac{2}{1+\\nu}}D^{2}}{k(k+1)}\\bigg[\\cfrac{1-\\nu}{2(1+\\nu)}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}\\bigg)^{\\frac{1+\\nu}{2}}\\bigg(\\cfrac{(1+\\nu)\\frac{2c_{3}}{3}(k+1)}{1-\\nu}\\bigg)^{\\frac{1-\\nu}{2}}}\\\\ &{\\qquad+\\cfrac{2}{1+\\nu}\\bigg(\\cfrac{24c_{1}c_{4}H_{\\mathrm{max}}^{\\frac{2}{1+\\nu}}D^{2}}{b k}\\bigg[\\cfrac{1-\\nu}{2(1+\\nu)}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}\\bigg)^{\\frac{1+\\nu}{2}}\\bigg(\\cfrac{(1+\\nu)\\frac{4}{3}}{1-\\nu}\\bigg)^{\\frac{1-\\nu}{2}}+\\sigma_{k}}\\\\ &{=\\frac{(8c_{2}c_{4})^{\\frac{1+\\nu}{2}}(\\frac{2}{3}c_{3})^{\\frac{1-\\nu}{2}}}{1+\\nu}\\frac{H_{f}D^{1+\\nu}}{k^{\\frac{1+\\nu}{2}}(k+1)^{\\nu}}+\\frac{(48c_{1}c_{4})^{\\frac{1+\\nu}{2}}(\\frac{4}{3})^{\\frac{1-\\nu}{2}}}{1+\\nu}\\frac{H_{\\operatorname*{max}}D^{1+\\nu}}{(b k)^{\\frac{1+\\nu}{2}}}+\\sigma_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This proves the claimed convergence rate, and the oracle complexity bound easily follows since each iteration of the algorithm requires only $O(1)$ queries to $\\widehat{g}_{b}$ . \u53e3 ", "page_idx": 32}, {"type": "text", "text": "Remark 42. In the proof of Corollary 41, it was impo rtpant that $\\delta_{f}$ and $\\delta_{\\widehat{g}}$ were allowed to be two separate constants. If we were not paying attention to such a separation anpd simply used the same $\\delta$ everywhere, we would end up with the much weaker rate of $\\begin{array}{r}{O(\\frac{H_{\\mathrm{max}}(\\nu)D^{1+\\nu}}{b^{\\frac{1+\\nu}{2}}k^{\\nu}})}\\end{array}$ for the second term. ", "page_idx": 32}, {"type": "text", "text": "F.3 Explicit Variance Reduction with SVRG ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Corollary 43. Consider problem (1) under the setting from Example 12 and also under Assumption 2. Let UniSvrg (as defined by Algorithm 3) be applied to this problem with the stochastic oracle $\\widehat{g}=\\widehat{g}_{b}$ and the full-gradient oracle $\\overline{{g}}=\\nabla f$ . Then, for any $t\\geq1$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\tilde{x}_{t})]-F^{*}\\leq\\frac{c_{f}(\\nu)H_{f}(\\nu)D^{1+\\nu}}{(2^{t})^{\\frac{1+\\nu}{2}}}+\\frac{c_{\\hat{g}}(\\nu)H_{\\operatorname*{max}}(\\nu)D^{1+\\nu}}{(b2^{t})^{\\frac{1+\\nu}{2}}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "$\\begin{array}{r l}&{c_{f}(\\nu)\\;:=\\;\\frac{[2(c_{2}c_{4}+1)]^{\\frac{1+\\nu}{2}}(2\\bar{c}_{3})^{\\frac{1-\\nu}{2}}}{1+\\nu}\\;=\\;O(1),\\;c_{\\hat{g}}(\\nu)\\;:=\\;\\frac{(96c_{1}c_{4})^{\\frac{1+\\nu}{2}}(\\frac{8}{3})^{\\frac{1-\\nu}{2}}}{1+\\nu}\\;=\\;O(1)}\\end{array}$ $\\overline{{c}}_{3}\\ :=$ $\\operatorname*{max}\\{c_{3},1\\}$ . To get $\\mathbb{E}[F(\\tilde{x}_{t})]\\!-\\!F^{*}\\leq\\epsilon$ , it suffices to mapke $O(N_{\\nu}(\\epsilon))$ queries to $\\widehat{g}_{b}$ and $O(\\log_{+}N_{\\nu}(\\epsilon))$ queries to $\\nabla f$ , where $\\begin{array}{r}{N_{\\nu}(\\epsilon):=[\\frac{H_{f}(\\nu)}{\\epsilon}]^{\\frac{2}{1+\\nu}}D^{2}+\\frac{1}{b}[\\frac{H_{\\operatorname*{max}}(\\nu)}{\\epsilon}]^{\\frac{2}{1+\\nu}}D^{2}}\\end{array}$ . Assumin gp that the complexity of querying $\\overline{{g}}_{b}$ is $n_{b}$ times bigger than that of querying $\\nabla f$ , we get the total stochastic-oracle complexity of $O(N_{\\nu}(\\epsilon)+n_{b}\\log_{+}N_{\\nu}(\\epsilon))$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. Let $t\\geq1$ be arbitrary and denote for brevity $F_{t}\\,:=\\,\\mathbb{E}[F(\\tilde{x}_{t})]\\,-\\,F^{*}$ , $H_{f}\\,:=\\,H_{f}(\\nu)$ and $H_{\\mathrm{max}}\\,:=\\,H_{\\mathrm{max}}(\\nu)$ . Taking into account Remark 35 and applying Theorem 10, we get, for any $\\delta_{f},\\delta_{\\hat{g}}>0$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{C}_{t}\\leq\\frac{(c_{2}c_{4}+1)H_{f}^{\\frac{2}{1+\\nu}}D^{2}}{2^{t}}\\bigg[\\frac{1-\\nu}{2(1+\\nu)\\delta_{f}}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}+\\frac{48c_{1}c_{4}H_{\\operatorname*{max}}^{\\frac{2}{1+\\nu}}D^{2}}{b2^{t}}\\bigg[\\frac{1-\\nu}{2(1+\\nu)\\delta_{\\tilde{g}}}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}+2\\bar{c}_{3}\\delta_{f}+\\frac{8}{3}\\delta_{\\tilde{g}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Minimizing the right-hand side in $\\delta_{f},\\delta_{\\widehat{g}}$ (using Lemma 36 twice), we obtain ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{t}\\leq\\displaystyle\\frac{2}{1+\\nu}\\bigg(\\frac{(c_{2}c_{4}+1)H_{f}^{\\frac{2}{1+\\nu}}D^{2}}{2^{t}}\\bigg[\\frac{1-\\nu}{2(1+\\nu)}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}\\bigg(\\frac{(1+\\nu)2\\bar{c}_{3}}{1-\\nu}\\bigg)^{\\frac{1-\\nu}{2}}}\\\\ &{\\qquad\\qquad+\\displaystyle\\frac{2}{1+\\nu}\\bigg(\\frac{48c_{1}c_{4}H_{\\mathrm{max}}^{\\frac{2}{1+\\nu}}D^{2}}{b2^{t}}\\bigg[\\frac{1-\\nu}{2(1+\\nu)}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}\\bigg)^{\\frac{1+\\nu}{2}}\\bigg(\\frac{(1+\\nu)\\frac{8}{3}}{1-\\nu}\\bigg)^{\\frac{1-\\nu}{2}}}\\\\ &{=\\displaystyle\\frac{c_{f}H_{f}D^{1+\\nu}}{(2^{t})^{\\frac{1+\\nu}{2}}}+\\frac{c_{\\hat{g}}H_{\\mathrm{max}}D^{1+\\nu}}{(b2^{t})^{\\frac{1+\\nu}{2}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where cf := [2(c2c4+1)]1+2\u03bd(2c3)1\u22122\u03bd and $\\begin{array}{r}{c_{\\widehat{g}}:=\\frac{(96c_{1}c_{4})^{\\frac{1+\\nu}{2}}(\\frac{8}{3})^{\\frac{1-\\nu}{2}}}{1+\\nu}}\\end{array}$ . This proves the claimed convergence rate. ", "page_idx": 33}, {"type": "text", "text": "Let us now estimate the oracle complexity. From the already proved convergence rate bound, we see that $F_{t}\\le\\epsilon$ once $2^{t}\\geq O(1)N(\\epsilon)$ , where $\\begin{array}{r}{N(\\epsilon):=[\\frac{H_{f}}{\\epsilon}]^{\\frac{\\zeta}{1+\\nu}}D^{2}+\\frac{1}{b}[\\frac{H_{\\operatorname*{max}}}{\\epsilon}]^{\\frac{2}{1+\\nu}}D^{2}}\\end{array}$ . At the same time, according to Theorem 10, to generate the corresponding $\\tilde{x}_{t}$ , the algorithm needs to make $O(2^{t})$ queries to $\\widehat{g}_{b}$ and $O(t)$ queries to $\\nabla f$ . Combining these two facts together, we get the claimed $\\bar{O}(N(\\epsilon))$ queries to $\\widehat{g}_{b}$ and $O(\\log_{2}N(\\epsilon)+1)=O(\\log_{+}N(\\epsilon))$ queries to $\\nabla f$ . \u53e3 ", "page_idx": 33}, {"type": "text", "text": "Corollary 44. Cons ipder problem (1) under the setting from Example $^{12}$ and also under Assumption 2. Let UniFastSvrg (Algorithm 4) be applied to this problem with the stochastic oracle $\\widehat{g}\\ =\\ \\widehat{g}_{b}$ , the full-gradient oracle ${\\overline{{g}}}\\,=\\,\\nabla f$ , and the epoch length $N\\geq9$ . Then, for any $t~\\geq~2t_{0}$ , where $t_{0}:=\\lceil\\log_{2}\\log_{3}N\\rceil-1\\left(\\geq0\\right)\\rceil$ , it holds that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\tilde{x}_{t})]-F^{*}\\leq\\frac{c_{f}(\\nu)H_{f}(\\nu)D^{1+\\nu}}{N^{\\frac{1+\\nu}{2}}(t+1)^{\\frac{1+3\\nu}{2}}}+\\frac{c_{\\hat{g}}(\\nu)H_{\\operatorname*{max}}(\\nu)D^{1+\\nu}}{(b N)^{\\frac{1+\\nu}{2}}(t+1)^{\\frac{1+3\\nu}{2}}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "$\\begin{array}{r l}&{c_{f}(\\nu)\\;:=\\;\\frac{[72(c_{2}c_{4}+\\frac{1}{2})]^{\\frac{1+\\nu}{2}}\\bar{c}_{3}^{\\frac{1-\\nu}{2}}}{1+\\nu}\\;=\\;O(1),\\;c_{\\hat{g}}(\\nu)\\;:=\\;\\frac{(432c_{1}c_{4})^{\\frac{1+\\nu}{2}}(\\frac{5}{3})^{\\frac{1-\\nu}{2}}}{1+\\nu}\\;=\\;O(1),\\;\\bar{c}_{3}\\;:=\\;0.}\\end{array}$ $\\operatorname*{max}\\{c_{3},1\\}$ . To get $\\mathbb{E}[F(\\tilde{x}_{t})]-F^{*}\\leq\\epsilon$ , it suffices top make $O(N T_{\\nu}(\\epsilon))$ queries to $\\widehat{g}_{b}$ and ${\\cal O}(T_{\\nu}(\\epsilon))$ queries to \u2207f, where T\u03bd(\u03f5) := [ Hf (\u03bd1)+D\u03bd1+\u03bd] $\\begin{array}{r}{T_{\\nu}(\\epsilon):=[\\frac{H_{f}(\\nu)D^{1+\\nu}}{N^{\\frac{1+\\nu}{2}}\\epsilon}]^{\\frac{2}{1+3\\nu}}+[\\frac{H_{\\operatorname*{max}}(\\nu)D^{1+\\nu}}{(b N)^{\\frac{1+\\nu}{2}}\\epsilon}]^{\\frac{2}{1+3\\nu}}+\\log_{2}\\log_{3}N.}\\end{array}$ H(mbaNx()\u03bd1)+2D\u03bd1\u03f5+\u03bd]1+23\u03bd +log2 log3  Np. Assuming that the complexity of querying $\\overline{{g}}_{b}$ is $n_{b}$ times bigger than that of querying $\\nabla f$ and choosing $N=\\Theta(n_{b})$ , we get the total stochasti cs-oracle complexity of O [ nb Hf (\u03f5\u03bd)D $\\begin{array}{r}{O\\big([\\frac{n_{b}^{\\nu}H_{f}(\\nu)D^{1+\\nu}}{\\epsilon}]^{\\frac{2}{1+3\\nu}}+[\\frac{n_{b}^{\\nu}H_{\\operatorname*{max}}(\\nu)D^{1+\\nu}}{b^{(1+\\nu)/2}\\epsilon}]^{\\frac{2}{1+3\\nu}}+}\\end{array}$ $n_{b}\\log\\log n_{b}$ ", "page_idx": 33}, {"type": "text", "text": "Proof. Let $t\\geq2t_{0}$ be arbitrary, $F_{t}:=\\mathbb{E}[F(\\tilde{x}_{t})]-F^{*}$ , $\\boldsymbol{H}_{f}:=\\boldsymbol{H}_{f}(\\boldsymbol{\\nu})$ , $H_{\\mathrm{max}}:=H_{\\mathrm{max}}(\\nu)$ . Taking into account Remark 35 and applying Theorem 11, we get, for any $\\delta_{f},\\delta_{\\hat{g}}>0$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{t}\\leq\\frac{9(c_{2}c_{4}+\\frac{1}{2})H_{f}^{\\frac{2}{1+\\nu}}D^{2}}{N(t-t_{0}+1)^{2}}\\Big[\\frac{1-\\nu}{2(1+\\nu)\\delta_{f}}\\Big]^{\\frac{1-\\nu}{1+\\nu}}+\\frac{54c_{1}c_{4}H_{\\mathrm{max}}^{\\frac{2}{1+\\nu}}D^{2}}{b N(t-t_{0}+1)^{2}}\\Big[\\frac{1-\\nu}{2(1+\\nu)\\delta_{\\hat{\\mathcal{G}}}}\\Big]^{\\frac{1-\\nu}{1+\\nu}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\left(c_{3}t+1\\right)\\delta_{f}+\\frac{5}{3}t\\delta_{\\hat{\\mathcal{G}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Since $t\\geq2t_{0}$ , we can estimate $\\begin{array}{r}{t-t_{0}+1=\\frac{1}{2}t+\\frac{1}{2}t-t_{0}+1\\geq\\frac{1}{2}(t+1)}\\end{array}$ , which gives us ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F_{t}\\leq\\frac{36(c_{2}c_{4}+\\frac{1}{2})H_{f}^{\\frac{2}{1+\\nu}}D^{2}}{N(t+1)^{2}}\\bigg[\\frac{1-\\nu}{2(1+\\nu)\\delta_{f}}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}+\\frac{216c_{1}c_{4}H_{\\operatorname*{max}}^{\\frac{2}{1+\\nu}}D^{2}}{b N(t+1)^{2}}\\bigg[\\frac{1-\\nu}{2(1+\\nu)\\delta_{\\hat{g}}}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}}\\\\ {+\\,\\bar{c}_{3}(t+1)\\delta_{f}+\\frac{5}{3}(t+1)\\delta_{\\hat{g}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Minimizing the right-hand side in $\\delta_{f},\\delta_{\\widehat{g}}$ (using Lemma 36 twice), we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{t}\\le\\displaystyle\\frac{2}{1+\\nu}\\bigg(\\frac{36(c_{2}c_{4}+\\frac{1}{2})H_{f}^{\\frac{2}{1+\\nu}}D^{2}}{N(t+1)^{2}}\\bigg[\\frac{1-\\nu}{2(1+\\nu)}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}\\bigg(\\frac{(1+\\nu)\\overline{{c}}_{3}(t+1)}{1-\\nu}\\bigg)^{\\frac{1-\\nu}{2}}}\\\\ &{\\qquad\\qquad+\\displaystyle\\frac{2}{1+\\nu}\\bigg(\\frac{216c_{1}c_{4}H_{\\operatorname*{max}}^{\\frac{2}{1+\\nu}}D^{2}}{b N(t+1)^{2}}\\bigg[\\frac{1-\\nu}{2(1+\\nu)}\\bigg]^{\\frac{1-\\nu}{1+\\nu}}\\bigg)^{\\frac{1+\\nu}{2}}\\bigg(\\frac{(1+\\nu)\\frac{5}{3}(t+1)}{1-\\nu}\\bigg)^{\\frac{1-\\nu}{2}}}\\\\ &{=\\displaystyle\\frac{c_{f}H_{f}D^{1+\\nu}}{N^{\\frac{1+\\nu}{2}}(t+1)^{\\frac{1+3\\nu}{2}}}+\\frac{c_{\\bar{g}}H_{\\operatorname*{max}}D^{1+\\nu}}{(b N)^{\\frac{1+\\nu}{2}}(t+1)^{\\frac{1+3\\nu}{2}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\begin{array}{r}{c_{f}:=\\frac{[72(c_{2}c_{4}+\\frac12)]^{\\frac{1+\\nu}{2}}\\bar{c}_{3}^{\\frac{1-\\nu}{2}}}{1+\\nu}}\\end{array}$ and $\\begin{array}{r}{c_{\\widehat{g}}:=\\frac{(432c_{1}c_{4})^{\\frac{1+\\nu}{2}}(\\frac{5}{3})^{\\frac{1-\\nu}{2}}}{1+\\nu}}\\end{array}$ This proves the claimed convergence rate. ", "page_idx": 34}, {"type": "text", "text": "Let us now estimate the number of oracle queries. In view of the above convergence rate bound, we have $F_{t}~\\le~\\epsilon$ once $t\\ \\geq\\ T(\\epsilon)\\ :=\\ T_{1}(\\mathit{\\dot{\\epsilon}})\\,+\\,2t_{0}\\ =\\,O\\big(T_{1}(\\epsilon)\\,+\\,\\log\\log N\\big)$ , where $T_{1}(\\epsilon)\\;:=\\;$ $\\begin{array}{r}{\\big[\\frac{H_{f}D^{1+\\nu}}{N^{(1+\\nu)/2}\\epsilon}\\big]^{\\frac{2}{1+3\\nu}}+\\big[\\frac{H_{\\mathrm{max}}D^{1+\\nu}}{(b N)^{(1+\\nu)/2}\\epsilon}\\big]^{\\frac{2}{1+3\\nu}}=\\frac{T_{2}(\\epsilon)}{N^{\\frac{1+\\nu}{1+3\\nu}}}}\\end{array}$ , where $\\begin{array}{r}{T_{2}(\\epsilon):=[\\frac{H_{f}D^{1+\\nu}}{\\epsilon}]^{\\frac{2}{1+3\\nu}}+[\\frac{H_{\\operatorname*{max}}D^{1+\\nu}}{b^{(1+\\nu)/2}\\epsilon}]^{\\frac{2}{1+3\\nu}}}\\end{array}$ does not depend on $N$ . Combining this with Theorem 11 saying that, to generate the corresponding $\\tilde{x}_{t}$ , the algorithm needs to make $O(N t)$ queries to $\\widehat{g}_{b}$ and $O(t)$ queries to $\\nabla f$ , we get the claimed $O(N T(\\epsilon))$ queries to $\\widehat{g}_{b}$ and $O(T(\\epsilon))$ queries to $\\nabla f$ . ", "page_idx": 34}, {"type": "text", "text": "Assuming now that th pe complexity of querying $\\nabla f$ is $n_{b}$ times bigger than that of querying $\\widehat{g}_{b}$ , we get the total stochastic-oracle complexity of $\\begin{array}{r}{O\\big((N+n_{b})T(\\epsilon)\\big)\\,=\\,O\\big((N+n_{b})\\big[\\frac{T_{2}(\\epsilon)}{N^{(1+\\nu)/(1+3\\nu)}}\\,+\\,}\\end{array}$ $\\log\\log N]$ . Ignoring the doubly-logarithmic term, we get the expression of the form $\\begin{array}{r l r}{\\lefteqn{(N+n_{b})\\frac{1}{N^{q}}=}}\\end{array}$ $\\begin{array}{r}{N^{1-q}+\\frac{n_{b}}{N^{q}}}\\end{array}$ with $\\begin{array}{r}{q:=\\frac{1+\\nu}{1+3\\nu}\\in[0,1]}\\end{array}$ , whose minimal value is achieved at $N=\\Theta(n_{b})$ . Substituting this value into our complexity bound, we get the stochastic-oracle complexity of $O\\big(n_{b}\\big(\\frac{T_{2}(\\epsilon)}{n_{b}^{(1+\\nu)/(1+3\\nu)}}+$ $\\log\\log n_{b})\\big)=O\\big(n_{b}^{\\frac{2\\nu}{1+3\\nu}}T_{2}(\\epsilon)+n_{b}\\log\\log n_{b}\\big).$ ", "page_idx": 34}, {"type": "text", "text": "G Additional Discussion of Related Work ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Inexact Oracle and Approximate Smoothness. Devolder, Glineur, and Nesterov [15] introduced the notion of the inexact first-order oracle and analyzed the behaviour of several first-order methods for smooth convex optimization using such an oracle. Although their work was motivated by the desire to present the general definition of an inexact oracle covering many different applications, it was also observed that this oracle model is suitable for studying weakly smooth problems. This insight was later used in [45] to develop universal gradient methods for H\u00f6lder smooth problems. First stochastic gradient methods for approximately smooth functions with inexact oracle were proposed in [13]. These algorithms however are not adaptive and require the knowledge of problem-dependent constants. For more details on the subject, see [14]. ", "page_idx": 34}, {"type": "image", "img_path": "rniiAVjHi5/tmp/c15d0c893931f5fbb7e690f618ad01aa91263fe165723489ee27428a0ee504cc.jpg", "img_caption": ["Figure 3: Comparison of various methods on the logistic regression problem with real-world data. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Parameter-Free Methods. Parameter-free algorithms originating from the literature on online learning [9, 10, 26, 40, 47, 55] is another popular type of adaptive methods. They are usually endowed with mechanisms helping achieving efficiency bounds that are almost insensitive (typically, with logarithmic dependency) to the error of estimating certain problem parameters, such as the diameter of the feasible set [6, 11, 25, 31, 41]. ", "page_idx": 35}, {"type": "text", "text": "Variance Reduction. Variance reduction techniques encompass a set of strategies that enhance the convergence speed of SGD when multiple passes are possible over the training dataset. Various researchers simultaneously introduced methods to reduce variance around the same period [27, 38, 50, 52, 58, 60]. The consideration of mini-batching in the context of these methods is documented in [3], while, in [20], it is shown that the convergence rate is influenced by both the average and the maximum smoothness of individual components. For further details, see [21] and the references therein. ", "page_idx": 35}, {"type": "text", "text": "Sometimes, it is even not necessary to use an explicit variance reduction mechanism. SGD may converge fast in the so-called over-parameterized regime, or when the stochastic noise is small at the optimal solution [8, 35, 37, 43, 44, 51]. In this work, we call this effect implicit variance reduction. Such a situation is also considered in [19, 54] and, more recently, Woodworth and Srebro [59] proposed an accelerated SGD algorithm for this setting, under the assumption that the smoothness and noise constants are known. ", "page_idx": 35}, {"type": "text", "text": "H Additional Experiments ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "H.1 Logistic Regression with Real-World Data ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In this section, we present experiments on the logistic regression problem: ", "page_idx": 35}, {"type": "equation", "text": "$$\nf^{*}=\\operatorname*{min}_{\\|\\boldsymbol{x}\\|\\leq R}\\Bigl\\{f(\\boldsymbol{x}):=\\frac{1}{n}\\sum_{i=1}^{n}\\log\\bigl(1+e^{-b_{i}\\langle a_{i},\\boldsymbol{x}\\rangle}\\bigr)\\Bigr\\},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $a_{i}\\in\\mathbb{R}^{d}$ and $b_{i}\\in\\{-1,1\\}$ are features and labels taken from diverse real-world datasets from LIBSVM [7]: mushrooms $[d\\ll n]$ , w8a $\\textstyle d\\ll n)$ ), leu $(d\\gg n)$ and colon-cancer $(d\\gg n)$ ). The dataset leu is quite special because it satisfies the so-called interpolation condition, meaning that the variance at the optimum is zero. We fix $R=1$ and use the mini-batch size of $b=32$ for the first two datasets and $b=1$ for the last two. ", "page_idx": 35}, {"type": "text", "text": "Figure 3 shows the results of our experiments. The solid lines and the shaded area for each method represent, respectively, the mean and the region between the minimum and the maximum values after three independent runs of the algorithm. We see that, on the leu dataset, UniSgd and UniFastSgd converge as fast as the best non-accelerated and accelerated SVRG methods, respectively, which confirms our theory on implicit variance reduction. Otherwise, these two SGD methods are typically much slower than the SVRG algorithms. Our UniSvrg method performs consistently better than AdaSVRG across all the datasets. Overall, all adaptive accelerated SVRG methods demonstrate comparable performance for solving these smooth problems. ", "page_idx": 35}, {"type": "image", "img_path": "rniiAVjHi5/tmp/ac2e212c336a45baa424f5d0c79e52a2218b83be511fd3fb0cd24cdf5b449889.jpg", "img_caption": ["Figure 4: Comparison of our methods for different stepsize update rules on the polyhedron feasibility problem. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "H.2 Comparison between Stepsize Update Rules ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this section, we compare the AdaGrad stepsize rule (3) with the other rule (4) for UniSgd (Algorithm 1), UniFastSgd (Algorithm 2), UniSvrg (Algorithm 3), and UniFastSvrg (Algorithm 4). We consider the polyhedron feasibility and logistic regression problems under the same setups as in Section 8 and Appendix H.1. ", "page_idx": 36}, {"type": "text", "text": "The results are shown in Figs. 4 and 5, where we plot the function residual and the stepsize (inverse of $M$ ) against stochastic oracle calls. We see that the two stepsize rules work very similarly across all test cases, which was not evident from the theory alone. ", "page_idx": 36}, {"type": "image", "img_path": "rniiAVjHi5/tmp/3e83a5d1bb5c0402fad6f30013dd0894b67bcfc2ff45e9413499e04fc28d83dd.jpg", "img_caption": ["Figure 5: Comparison of our methods for different stepsize update rules on the logistic regression problem with real-world data. "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Claims ", "page_idx": 37}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Each claim is justified by a theorem. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 37}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Each theorem is followed by a discussion. For the numerical experiments in Section 8 we carefully discuss the performance of the method. Additional experiments are presented in Appendix H. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201cLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 38}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Yes. This is a theory paper. All proofs are provided in the appendix. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 38}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: We provide the pseudocode and the hyperparameter settings for all algorithms. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: We provide the GitHub link to our code. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Everything is carefully described in Section 8 and Appendix H. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [No] ", "page_idx": 40}, {"type": "text", "text": "Justification: Different runs of the same algorithm for the same hyperparameter settings result in almost the same performance in our experiments. Therefore, we plot only one curve for each method, focusing more on highlighting the (more significant) differences between various methods. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 40}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [No] ", "page_idx": 41}, {"type": "text", "text": "Justification: This is not important for the results we present since the algorithms are compared in terms of oracle calls and not the actual time of execution. All experiments are run on a standard MacBook Pro laptop and do not require a lot of computational power. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 41}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: This is a theory paper. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: This is a theory paper. We do not expect immediate societal impact. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: Does not apply. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 42}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We cite the LIBSVM dataset. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 42}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: We do not release any new assets in this paper. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. ", "page_idx": 42}, {"type": "text", "text": "\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 43}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: No such experiments or research. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 43}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: Does not apply. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 43}]