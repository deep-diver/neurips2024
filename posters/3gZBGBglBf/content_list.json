[{"type": "text", "text": "Beware of Overestimated Decoding Performance Arising from Temporal Autocorrelations in Electroencephalogram Signals ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Researchers have reported high decoding accuracy $(>\\!95\\%)$ using non-invasive   \n2 Electroencephalogram (EEG) signals for brain-computer interface (BCI) decod  \n3 ing tasks like image decoding, emotion recognition, auditory spatial attention   \n4 detection, etc. Since these EEG data were usually collected with well-designed   \n5 paradigms in labs, the reliability and robustness of the corresponding decoding   \n6 methods were doubted by some researchers, and they argued that such decoding   \n7 accuracy was overestimated due to the inherent temporal autocorrelation of EEG   \n8 signals. However, the coupling between the stimulus-driven neural responses and   \n9 the EEG temporal autocorrelations makes it difficult to confirm whether this overestimation exists in truth. Furthermore, the underlying pitfalls behind overestimated decoding accuracy have not been fully explained due to a lack of appropriate formulation. In this work, we formulate the pitfall in various EEG decoding tasks in a unified framework. EEG data were recorded from watermelons to remove stimulus-driven neural responses. Labels were assigned to continuous EEG according to the experimental design for EEG recording of several typical datasets, and then the decoding methods were conducted. The results showed the label can be successfully decoded as long as continuous EEG data with the same label were split into training and test sets. Further analysis indicated that high accuracy of various BCI decoding tasks could be achieved by associating labels with EEG intrinsic temporal autocorrelation features. These results underscore the importance of choosing the right experimental designs and data splits in BCI decoding tasks to prevent inflated accuracies due to EEG temporal correlations. The watermelon EEG dataset collected in this work can be obtained at Zenodo: https://zenodo.org/records/11238929, and all the codes of this work can be obtained in the supplementary materials. ", "page_idx": 0}, {"type": "text", "text": "10   \n11   \n12   \n13   \n14   \n15   \n16   \n17   \n18   \n19   \n20   \n21   \n22   \n23   \n24   \n25 ", "page_idx": 0}, {"type": "text", "text": "26 1 Introduction and related works ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "27 A brain-computer interface (BCI) is a type of human-machine interaction that bridges a pathway   \n28 from the brain to external devices [1]. Electroencephalogram (EEG) has emerged as a valuable tool   \n29 for BCI because of its high time resolution, low cost, and good portability [2], and algorithms of   \n30 neural decoding from EEG signals play a role in its practical applications. Recently, deep learning   \n31 methods have been developed widely for various EEG decoding tasks, and high decoding accuracy   \n32 was reported. For example, in the task of decoding image classes with EEG recordings, when   \n33 subjects were required to watch images of different classes, a decoding accuracy of $82.90\\%$ was   \n34 reported for the 40-way classification by Spampinato et al. [3]. With their EEG dataset, subsequent   \n35 studies reported a higher decoding accuracy $(98.30\\%$ , [4]), high performance on image retrieval, and   \n36 even image generation from EEG [5, 6, 7].   \n37 However, it remains unclear what kind of EEG features are learned by the DNN-based models. Some   \n38 researchers have posited that the high decoding accuracy on the image-evoked EEG dataset was   \n39 attributed to the block-design paradigm during EEG recording [8, 9, 10], in which 50 images with the   \n40 same class label were presented to the subject continuously in one block, and the 40 image-classes   \n41 were presented as 40 separate blocks. Due to the existence of temporal autocorrelation of EEG   \n42 signals, i.e., the temporally nearby data is more similar than the temporally distal [11, 12, 13, 14],   \n43 the models could learn the block-related features rather than the image-related.   \n44 To verify their concerns, Li et al. [8] recorded EEG with two experimental designs: block design   \n45 and rapid-event design. For the rapid-event design, images across the 40 classes were presented   \n46 alternately and randomly. When the same DNN model was used, it was found that the decoding   \n47 accuracy was close to Spampinato et al. [3] with the block-design EEG data, but it was dramati  \n48 cally decreased to the chance-level $(2.50\\%)$ with the rapid-event design data. Subsequent work also   \n49 confirmed the low decoding accuracy for EEG recorded with rapid-event design [9, 10]. However,   \n50 Palazzo et al. [15] proposed that temporal autocorrelations only play a marginal role in EEG de  \n51 coding tasks because they found that EEG data recorded during rest periods (temporal proximity to   \n52 adjacent blocks) could not be successfully classified as the preceding block label or the succeeding   \n53 block label. They also argued that the rapid-event design seemed to weaken the image-related neural   \n54 responses due to the possible cognitive load and fatigue effect compared to the block design. Some   \n55 researchers [15, 16, 17, 18] pointed out that block design is essential because humans tend to react   \n56 more consistently and respond faster when conditions are presented in blocks [19, 20]. Wilson et   \n57 al. [18] advised that classification work that decodes from block design datasets is the most suitable   \n58 approach until advances are made to reduce noise.   \n59 Although the pitfall of overestimated decoding accuracy has been mainly discussed in image neural   \n60 decoding tasks, we noticed that similar pitfalls might also exist in various EEG decoding tasks such   \n61 as in auditory spatial attention detection (ASAD) tasks [21, 22, 23, 24], which involves decoding   \n62 the subjects auditory attention locus from neural data, and in emotion recognition task [25, 26, 27],   \n63 which involves recognizing the subjects emotion type from neural data. Researchers have also found   \n64 that splitting a continuous EEG from a specific experimental condition into training and test sets   \n65 would bring higher decoding accuracy in epilepsy detection tasks [28], motor imagery decoding   \n66 tasks [29], and so on. All those high decoding accuracy works share the common characteristic:   \n67 continuously recorded EEG data of a specific class (condition) label are divided into training and   \n68 test sets (see the top-left of Figure 1).   \n69 Although some studies have mentioned the overestimated decoding accuracy and tried to remind   \n70 the possible pitfall [8, 30], it is difficult to discriminate the influence of the inherent temporal auto  \n71 correlation in EEG signals due to the coupling of stimuli-driven neural responses and the temporal   \n72 autocorrelations. More importantly, due to the lack of an effective formalization, there is not an   \n73 adequate explanation of how models utilize temporal autocorrelation features for decoding. Further  \n74 more, their concerns only focused on one specific decoding task, and the results and conclusions   \n75 cannot be generalized to general BCI decoding tasks.   \n76 In this work, the pitfall of various EEG decoding tasks was formulated with a unified framework.   \n77 To completely decouple the temporal autocorrelation features from stimuli-driven neural responses,   \n78 EEG data were collected from 10 watermelons in this work to construct \"Watermelon EEG\". This   \n79 method is known as phantom EEG in previous studies [31, 32, 33, 34, 35, 36], and the EEG data   \n80 exclude stimulus-driven neural responses while reserving the temporal autocorrelation features. For   \n81 comparison, a human EEG dataset was also adopted. The watermelon EEG and human EEG   \n82 were reorganized into three classic neural decoding EEG datasets following their EEG experimen  \n83 tal paradigm: image classification (CVPR, [3]), emotion classification (DEAP, [37]), and auditory   \n84 spatial attention decoding (KUL, [38]), resulting in six EEG datasets. A sample CNN-based decod  \n85 ing model was used to complete the decoding tasks with the corresponding EEG dataset, and the   \n86 experimental results revealed that:   \n87 1. When the pitfall was formulated with a unique framework, and the temporal autocorre  \n88 lation was defined as domain features, high decoding accuracy of various BCI decoding   \n89 tasks could be achieved by associating labels with EEG intrinsic temporal autocorrelation   \n90 features.   \n91 2. The pitfall exists not only in classification but also widely in EEG-image joint training   \n92 without explicit labels and even image generation.   \n93 3. Splitting a continuous EEG with the same class label into training and test sets should never   \n94 be used in future BCI decoding works. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "95 2 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "96 The section is organized by: the pitfall is formulated in Subsection 2.1, and the datasets used are   \n97 introduced in Subsection 2.2. Then, the methods to finish different classification tasks are introduced   \n98 in Subsection 2.3, and joint training and image generation from EEG are introduced in Subsection   \n99 2.4. Some implementation details and statistical analysis method are described in Subsection 2.5. ", "page_idx": 2}, {"type": "image", "img_path": "3gZBGBglBf/tmp/0fe7aef4ee10c5b9c5228e63d003e49254f0fe30292eb28ca388009bbd023896.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Overestimated decoding performance in BCI works. (a) Continuous EEG data in a certain experimental condition (with the same class label) are split into training and test sets for decoder training and evaluation. (b) With the test EEG sample input, the decoder gives output in the forms of classification, retrieval, and generation. (c) Decoders may use both domain features or class-related features for decoding. ", "page_idx": 2}, {"type": "text", "text": "100 2.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "101 In some BCI works on domain generalization [39], all EEG data from a dataset [40] or from a subject   \n102 [41] are usually regarded as a domain to emphasize EEG pattern distribution differences between   \n103 datasets or subjects. Adopted from this concept, we regard a period of continuous EEG data with   \n104 the same class label as a domain. In some BCI works [3, 4, 21, 22, 23, 24, 25, 26, 27], researches   \n105 segment the EEG data from the same domain into samples and further split the samples into training   \n106 and test data (as shown in Figure 1a) and complete decoding task, such as classification, retrieval   \n107 and generation (as shown in Figure 1b). In these cases, the models used in these works would learn   \n108 the coupled features containing the class-related feature and domain feature (as shown in the middle   \n109 of the Figure 1c). The underlying assumption of these works is that the domain feature plays only a   \n110 margin role in EEG decoding tasks as shown in the left of the Figure 1c. However, we assumed that   \n111 the domain feature contributes to the high decoding accuracy as shown in the right of the Figure 1c,   \n112 which is the pitfall we mentioned in Section 1.   \n113 To validate our assumption, we need to formulate the pitfall. Denote $D$ as the domain set, and each   \n114 domain $d\\in D$ contains many samples. We use $S^{d}$ to denote the sample set of the domain $d$ . The   \n115 notation $\\boldsymbol{x}_{i}^{d}$ represents the $i$ -th sample (e.g., a 0.5-second EEG data corresponding to watching a   \n116 specific image) of domain $d$ , which is associated with class $y_{i}^{d}$ (e.g., the class label panda of the   \n117 watched image). Considering the temporal autocorrelation of the EEG data, the domain features of   \n118 data within the same domain are more similar, while the domain features of data in different domains   \n119 are more distinct.   \n120 For EEG decoding tasks, we assume the data is generated from a two-stage process. First, each   \n121 domain is modeled as a latent factor $z$ sampled from some meta domain distribution $p(\\cdot)$ . Second,   \n122 each data sample $x$ is sampled from a sample distribution conditioned on the domain $z$ and class $y$ : ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nz\\sim p(\\cdot),x\\sim p(\\cdot|z,y)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "123 Given the sample $x$ , the aim of a specific EEG decoding task is to uncover its true class label using   \n124 the posterior $p(y|x)$ . The quantity can be factorized by the domain factor $z$ as, ", "page_idx": 3}, {"type": "equation", "text": "$$\np(y|x)=\\int p(y,z|x)d z=\\int p(y|x,z)p(z|x)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "125 When we use the Watermelon EEG dataset or use a dataset that is completely unrelated to the   \n126 current task (e.g., decoding images from an auditory EEG dataset), the class-related feature has   \n127 none possibility to exist in EEG samples. In this condition, $p(y|x,z)=p(y|z)$ and the equation (2)   \n128 can be modified as: ", "page_idx": 3}, {"type": "equation", "text": "$$\np(y|x)=\\int p(y,z|x)d z=\\int p(y|z)p(z|x)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "129 The assumption of this work is that the model could also deduce $p(y|x)$ by learning $p(y|z)$ and   \n130 $p(z|x)$ even there is none class-related feature exists. In other words, we assumed that it could also   \n131 achieve high decoding accuracy on different EEG decoding tasks when using the Watermelons EEG   \n132 dataset. ", "page_idx": 3}, {"type": "text", "text": "133 2.2 Dataset ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "134 Watermelon EEG Dataset Ten watermelons were selected as subjects. EEG data were recorded   \n135 with a NeuroScan SynAmps2 system (Compumedics Limited, Victoria, Australia), using a 64-   \n136 channel $\\mathrm{Ag/AgCl}$ electrodes cap with a 10/20 layout. An additional electrode was placed on the   \n137 lower part of the watermelon as the physiological reference, and the forehead served as the ground   \n138 site (see Appendix A.1 for photography). The inter-electrode impedances were maintained under   \n139 $20\\,\\mathrm{kOhm}$ . Data were recorded at a sampling rate of $1000\\,\\mathrm{{Hz}}$ . EEG recordings for each watermelon   \n140 lasted for more than 1 hour to ensure sufficient data for the decoding task. We refer to the dataset   \n141 consisting of EEG recordings of 10 watermelons as the Watermelon EEG Dataset.   \n142 SparrKULee Dataset SparrKULee dataset[42] is a speech-evoked EEG dataset from the KU Leu  \n143 ven University containing 64-channel EEG recordings from 85 participants, each of whom listened   \n144 to 90-150 minutes of natural speech. We used this dataset because EEG recordings were longer than   \n145 1 hour to ensure a sufficient amount of data for each subject. To match the number of subjects in   \n146 the Watermelon EEG Dataset, EEG data from 10 subjects (ID: Sub7-Sub16) from the SparrKULee   \n147 Dataset were used.   \n148 Dataset reorganization and dataset segmentation The term \"reorganization\" refers to segmenting   \n149 continuous EEG into samples and assigning each sample a class label and a domain label according   \n150 to the referenced experimental design. Here, we follow the experimental designs of three classical   \n151 published EEG datasets to reorganize the Watermelon EEG Dataset and SparrKULee Dataset. These   \n152 three datasets were collected respectively for image decoding, emotion recognition, and ASAD   \n153 tasks.   \n154 For the image decoding task, we referred to the experimental design of the CVPR dataset [3]. For   \n155 the CVPR dataset, 40 classes of images were presented in a block-design paradigm. Specifically, 50   \n156 different images of the same class were presented continuously in a block, with each image lasting   \n157 for 0.5 second, resulting in 40 blocks of presentation for each subject. The 0.5-second length EEG   \n158 data of the same class were split into training, validation, and test sets in a ratio of 8:1:1 [4, 3].   \n159 Following this experimental design and dataset segmentation, we segment continuous EEG from   \n160 the Watermelon EEG Dataset and SparrKULee Dataset into blocks and assign a unique class label   \n161 and a unique domain label for each block. The interval between adjacent blocks is set to 10 seconds   \n162 to match the rest time of the subjects during the EEG recording in the CVPR dataset. Then, EEG   \n163 data in each block are further segmented into $50\\;0.5\\mathrm{-s}$ length samples. Since the EEG data in the   \n164 CVPR dataset has 128 channels, we replicated our 64-channel EEG in the channel dimension. The   \n165 reorganized datasets for Watermelon Dataset and SparrKULee Dataset are called WM-CVPR and   \n166 SK-CVPR, respectively. Here, we use the \"A-B\" naming format, where the left side of \"-\" represents   \n167 the source dataset (WM: watermelon dataset, SK: SparrKULee Dataset), and the right side of \"-\"   \n168 represents the dataset of which the experimental design is referenced. For the emotion recognition   \n169 task and ASAD task, the DEAP dataset and the KUL dataset are used as the referenced dataset,   \n170 resulting in WM-DEAP, SK-DEAP, WM-KUL, and SK-KUL. More details for reorganization can   \n171 be found in Appendix A.2. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "172 2.3 Classification tasks ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "173 Model. To demonstrate that domain features are strong and easy to be learned by the network,   \n174 we used a simple CNN (or some parts of this CNN) to complete all classification tasks mentioned   \n175 in this work. The CNN network includes a layer-norm layer, a 2D-convolutional layer (output   \n176 channel: 100), an averaging pooling layer, and two fully connected layers. The kernel size of the   \n177 2D-convolutional layer depends on the channel number and sampling frequency of the input EEG.   \n178 The node number of the output fully connected layer depends on the number of classes.   \n179 Decoding the domain feature To demonstrate that the model can predict the domain factor $z$ from   \n180 EEG input sample $x$ , which relates to learning posterior $p(z|x)$ , a domain label classification was   \n181 adopted on the six datasets (i.e., WM-CVPR, WM-DEAP, WM-KUL, SK-CVPR, SK-DEAP and   \n182 SK-KUL dataset) with a simple CNN classifier. The splitting strategy leave-samples-out was used,   \n183 which means that all sample were randomly split into training set, validation set and test set. The   \n184 outputs after the averaging pooling layer were selected as domain feature representation, and t-SNE   \n185 was utilized for dimensionality reduction and visualization.   \n186 Decoding the class label from the domain feature To demonstrate that the model can predict   \n187 the class label $y$ from the domain factor $z$ , which relates to learning posterior $p(y|z)$ , a class label   \n188 classification was adopted on the four datasets (classification on the WM-CVPR dataset and SK  \n189 CVPR dataset are unnecessary since domain labels and class labels are one-to-one correspondence)   \n190 using a single network with two linear layers and an intermediate sigmoid function.   \n191 End-to-end classification To demonstrate that the model can predict the class label $y$ from the EEG   \n192 input sample $x$ directly when samples in the training set and test set are from common domains,   \n193 a class label classification was adopted on the six datasets with the simple CNN classifier. The   \n194 splitting strategy leave samples out was used. Classification on the WM-CVPR dataset and SK  \n195 CVPR dataset is the same since domain labels and class labels in the two datasets are one-to-one   \n196 correspondence. To demonstrate that the model indeed used the domain feature to complete the   \n197 end-to-end classification, the splitting strategy leave domains out was used on the four datasets (i.e.,   \n198 WM-DEAP, WM-KUL, SK-DEAP, and SK-KUL dataset) in which samples in the same domain   \n199 only appear in the training set or the test set.   \n200 Zero-shot classification In a recent work [4], EEG data from 34 classes within the CVPR2017   \n201 dataset were used to train an EEG encoder, and the remaining 6 unseen classes were used for test  \n202 ing. The results showed that features of different unseen classes clustered in distinct groups on the   \n203 two-dimensional t-SNE plane. Similar analyses were conducted on the SK-CVPR and WM-CVPR   \n204 datasets. Six classes were selected for testing, and the remaining 34 classes were for training. The   \n205 simple CNN was used to predict class labels from input EEG samples, and the outputs from the av  \n206 erage pooling layer were chosen as the EEG feature representation. Two strategies were employed   \n207 for selecting the 6 test classes: random selection and first-six selection. For random selection, the 6   \n208 test classes are randomly chosen from the 40 classes. For the first-six selections, the first presented   \n209 6 classes in the EEG experiment are chosen. During the test stage, since the training set does not in  \n210 clude classes corresponding to the test EEG data, the model could not give the corresponding labels   \n211 and could only output the most probable classes among the 34 seen during training. Therefore, we   \n212 proposed two evaluation metrics: $A c c_{n e a r}$ and $A c c_{7t h}$ . $A c c_{n e a r}$ represents the proportion of EEG   \n213 data classified into temporally adjacent classes, while $A c c_{7t h}$ represents the proportion classified   \n214 into the category presented seventh in time. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "215 2.4 Joint training and image generation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "216 To demonstrate that the model can utilize domain features to accomplish retrieval and generation   \n217 besides classification, EEG-image joint training and image generation on WM-CVPR and SK-CVPR   \n218 were conducted.   \n219 Joint training In the EEG-image joint training, a pre-trained image encoder was typically utilized   \n220 to extract image representation, while an EEG encoder was employed to extract EEG features to   \n221 align with the image representation. During the decoding process, a retrieval task was applied.   \n222 Specifically, given a test EEG sample and a collection of images containing the target and the non  \n223 target. The image representation was reconstructed from the EEG with the EEG encoder. The   \n224 similarity between the reconstructed image representation and all candidate image representations   \n225 in the collection is calculated. The decoded output image is selected based on the ranking of these   \n226 similarities. Usually, the Top-k accuracy and normalized Rank accuracy are used as evaluation   \n227 metrics. In this work, the simple CNN described in Subsection 2.3 is used as an EEG encoder. The   \n228 detailed implementation can be found in Appendix A.3.   \n229 Image generation The image generation aims to generate images seen by the subjects from their   \n230 EEG data. This task commonly uses a two-stage process: EEG encoding and image generation.   \n231 In the EEG encoding stage, a model is built to encode EEG data into a latent representation. In   \n232 the image generation stage, a pre-trained image generator is used. The generator is fine-tuned with   \n233 EEG representation and corresponding images. In this work, the EEG data are first encoded into   \n234 image representation with a simple CNN described in Subsection 2.3. Following previous work[43],   \n235 a latent diffusion model conditioned on image representation was used. The metric of n-way top- $\\cdot\\mathbf{k}$   \n236 accuracy was used for evaluating the semantic correctness of generated images [44]. The detailed   \n237 implementation can be found in Appendix A.4. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "238 2.5 Implement details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "239 The neural networks were implemented with the Pytorch and trained on a single high-performance   \n240 computing node with 8 A800 GPU. For the classification task, the AdamW [45] optimizer was em  \n241 ployed to minimize the cross-entropy loss function with a learning rate of $10^{-3}$ . For the joint training   \n242 and image generation, the AdamW optimizer was used with a learning rate of $10^{-3}$ and $5\\times10^{-\\overline{{4}}}$   \n243 for each task respectively. More details can be found in our codes. All the experiments mentioned   \n244 in this work were trained within the subjects (i.e., models were trained for each subject respectively)   \n245 except special annotation (unseen subject decoding results were only presented in Appendix A.5).   \n246 For statistical analysis, the one-sample t-test was used to check whether the reported results were   \n247 significantly higher than the chance level. Bonferroni correction was used to adjust the $p$ -value. A   \n248 $p$ -value of 0.05 or lower was considered statistically significant. ", "page_idx": 5}, {"type": "text", "text": "249 3 Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "250 3.1 Classification tasks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "251 The results shown in Table 1 present that classification accuracy in domain label classification and   \n252 class label classification are all significantly above the chance level. This shows that the domain   \n253 feature can be extracted effectively with a simple CNN, and the label class can be decoded from   \n254 the extracted domain features or from EEG directly. In contrast, the decoding accuracy drops to the   \n255 chance level when using the splitting strategy leave-domains-out, further supporting domain feature  \n256 induced high decoding accuracy. The standard error of the mean calculated over the subjects level   \n257 is reported for accuracy in this work.   \n258 Figures 2a and 2b show the t-SNE plot for domain label classification and end-to-end class label   \n259 classification. As shown in Figure 2a, 8 distinct clusters exist, each corresponding to one domain.   \n260 In Figure 2b, 8 distinct clusters also exist, with four corresponding to class label 1 and the other   \n261 four corresponding to class label 2. This indicates that the high decoding accuracy results from   \n262 associating class labels with domain features. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Table 1: Classification accuracy $(\\%)$ on the six datasets. DLC is for domain label classification. TLC-DF is for class label classification from domain features. TLC-EEG is for end-to-end class label classification. TLC-EEG-woDO is for class label classification direct from EEG when samples in the training set and test set are from different domains. ", "page_idx": 6}, {"type": "table", "img_path": "3gZBGBglBf/tmp/b69bc9c10fa399a5220631cb8c2c60a962ccc8b3713cbf7346b3a7061c75cc42.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "3gZBGBglBf/tmp/10eaa728494fab1821649ab67b478064407c384bc320649c096c93ea7d6a21a3.jpg", "img_caption": ["Figure 2: t-SNE plot for (a) domain label classification, (b) end-to-end class label classification, and (c) zero-shot class label classification "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "263 The experimental results for zero-shot classification are displayed in Table 2. It can be observed   \n264 that the model tended to classify test samples into temporally adjacent classes. Figure 2c shows the   \n265 t-SNE visualization of the unseen EEG features extracted from the decoder. Despite being unseen,   \n266 different domains of features clustered in distinct groups. This suggests that the decoder just learned   \n267 to extract EEG domain features during training and distinguish unseen EEG responses from the   \n268 domain features. ", "page_idx": 6}, {"type": "table", "img_path": "3gZBGBglBf/tmp/aba93325858be3515089985a4a99cddffe17c00e84b3cbd23bf918dea310f1ab.jpg", "table_caption": ["Table 2: Zero-shot EEG classification accuracy $(\\%)$ on WM-CVPR and SK-CVPR datasets. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "269 3.2 Joint training and image generation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "270 For EEG-image joint training, Table 3 displays the accuracy for the retravel task on the test set. The   \n271 table shows that, for both types of loss functions, decoding accuracy is far above the chance level,   \n272 demonstrating that the model can utilize domain features to align EEG with image features. Table 3   \n273 Result for joint training on WM-CVPR and SK-CVPR with a loss function of cosine similarity (CS)   \n274 or InfoNCE. ", "page_idx": 6}, {"type": "text", "text": "Table 3: Accuracy $(\\%)$ for joint training on WM-CVPR and SK-CVPR with a loss function of cosine similarity (CS) or InfoNCE. ", "page_idx": 6}, {"type": "table", "img_path": "3gZBGBglBf/tmp/d5be30b055445e9b2413444060f2db8ca1d9f5033b4f8767d30430260c911f0f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "275 For image generation, Table 4 displays the n-way top-k accuracy for the generated images on the   \n276 WM-CVPR and SK-CVPR datasets. The metrics are significantly above the chance level, indicating   \n277 that the generated images have correct semantics. Figure 3 shows some generated images on the   \n278 WM-CVPR dataset. As shown in the figure, the model can exactly generate the correct images. The   \n279 results on EEG-image joint training and image generation show that in addition to classification   \n280 tasks, retrieval, and generation can also achieve high performance by leveraging domain features   \n281 shared by the test and training sets. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "3gZBGBglBf/tmp/97af3616dfd761592d93c0a24af08495e63526455e5dc415f1ff8f9d54639f27.jpg", "table_caption": ["Table 4: Accuracy $(\\%)$ for semantic correctness. The repeated times N was set to 50. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "3gZBGBglBf/tmp/f482ae75e6a68824c8d241a2725db06669bb96744db073a58f89ffd8ce5b2118.jpg", "img_caption": ["Figure 3: EEG-generated image from a typical watermelon subject, where the first column of each panel represents the real images \"watched\" by the watermelon subject, and the following five columns show the images generated by the model. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "282 4 Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "283 4.1 Relying on the domain features for EEG decoding ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "284 While many works on EEG decoding have reported high-performance results, we proposed that   \n285 some of these high-performance may rely on temporal autocorrelation of EEG data. The pitfall may   \n286 involve different EEG decoding tasks. To clarify this pitfall, the concept of domain was adopted   \n287 to describe the temporal autocorrelation of a continuous EEG with the same label. EEG data were   \n288 collected from watermelon as the phantom to exclude the contribution of stimuli-driven neural re  \n289 sponses to decoding results. The results showed that a simple CNN network could well learn domain   \n290 features from EEG data and could associate class labels with domain features.   \n91 To avoid the pitfalls, a feasible approach is to adopt a reasonable data-splitting strategy to avoid train  \n92 ing and test sets sharing the common domain features, i.e., a leave-domains-out splitting strategy.   \n93 For instance, a leave-subjects-out data-splitting strategy can be adopted, which entails designating   \n94 the data from certain participants for training and data from others for testing. Alternatively, for   \n95 datasets that do not follow a block design, a leave-trials-out strategy may be applied. Prior research   \n96 has consistently demonstrated that employing a leave-subjects-out splitting strategy precipitates a   \n97 notable decline in decoding performance [46]. In some cases, it has been reported that decoding   \n98 accuracy dropped to the chance level [47, 8]. The prevalent interpretation is that inter-individual   \n99 variability [46] hampers the generalizability across different subjects. However, we posit that the   \n00 observed decrement in decoding accuracy is attributable to model overfitting to domain features.   \n01 Although the leave-subjects-out partitioning strategy is designed to prevent the leakage of domain   \n02 features, the presence of these domain features in the training set can still lead the model to inadver  \n03 tently exploit them to differentiate between categories during the training phase. The methods and   \n04 results further support the conclusion can be found in Appendix A.5   \n305 Palazzo et al. [15] proposed that the EEG temporal correlation related to baseline drift could be al  \n306 leviated by high-pass filtering. However, our further experiment proved that the domain feature still   \n307 exists and that high decoding accuracy could be achieved in any frequency band (see Appendix A.6).   \n308 We argue that the focus should not be exclusively on the elimination of EEG autocorrelation through   \n309 filtering. Instead, greater emphasis should be placed on the experimental paradigms of EEG record  \n310 ing and the methods employed for dataset splitting. By addressing these aspects, we can proactively   \n311 prevent the overestimated decoding accuracy arising from EEG temporal autocorrelations.   \n312 It is worth noting that we do not want to create an illusion that all BCI works utilize EEG temporal   \n313 autocorrelation features for decoding. In fact, there are many works that do not rely on EEG temporal   \n314 autocorrelation features for decoding in image decoding [48, 49, 50] emotion recognition [51], sleep   \n315 detection [40, 41] and ASAD [52]. These works demonstrated the feasibility of various BCI tasks. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "316 4.2 Potential sources of domain features ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "317 In this work, we have demonstrated the existence of EEG temporal autocorrelation in the water  \n318 melon EEG, which consists of no neural activities, and in the human EEG data. Li et al. [8] believed   \n319 the model decodes by utilizing the baseline drift in the CVPR2017 dataset. They found that when   \n320 the EEG data is filtered with a bandpass fliter, the decoding accuracy dropped greatly. Palazzo et   \n321 al. [15] also claimed that temporal correlation was strong only in low frequency. However, we have   \n322 demonstrated in Appendix A.4 that the domain feature still exists and that high decoding accuracy   \n323 can be achieved in any frequency band. In addition to baseline drift, some neuroscience works have   \n324 shown that temporal autocorrelation existed in neural oscillation, which could be reflected in EEG   \n325 in various frequency bands. This is referred to as Long-Range Temporal Correlations (LRTC) in   \n326 neuroscience research [11, 12, 13, 14]. Linkenkaer-Hansen et al. [13] first calculated the LRTC in   \n327 resting-state EEG data. They found that spontaneous alpha, mu, and beta oscillations result in signif  \n328 icant LRTC for at least several hundred seconds during resting conditions. Subsequent neuroscience   \n329 research further demonstrated that significant LRTC exists in the theta [11] and gamma [12] bands.   \n330 While baseline drift can be removed through filtering, the frequency range of the LRTC overlaps   \n331 with the frequency range of stimuli-driven neural responses, making it impossible to remove this   \n332 domain feature through filtering. Temporal correlation analysis on human EEG in the SparrKULee   \n333 Dataset showed the existence of strong LRTC in all frequency bands, and the LRTC in a narrowband   \n334 is sufficient to complete the corresponding decoding task. The methods and results further support   \n335 the conclusion can be found in Appendix A.7. ", "page_idx": 8}, {"type": "text", "text": "336 4.3 Limitation and future work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "337 Although direct evidence of overestimated decoding accuracy attributable to domain feature across   \n338 various brain-computer interface (BCI) tasks have been provided in the current work, no solution has   \n339 been proposed to mitigate overfitting to domain features in the training set. Some works have already   \n340 used domain adaptation [2, 53, 54] or domain generalization [40, 41] method to improve decoding   \n341 accuracy under leave-subjects-out data splitting in BCI tasks. This may also help alleviate the ad  \n342 verse effects of domain features on decoding tasks. It is also noteworthy to highlight the remarkable   \n343 efficacy of large-scale EEG model in various BCI decoding tasks [55, 56, 57]. Given that domain   \n344 features are pervasive in extensive EEG datasets and do not necessitate manually annotated labels,   \n345 self-supervised pre-trained large EEG models may be especially adept at discerning and neutralizing   \n346 domain features, thereby facilitating more robust and generalizable decoding performance. ", "page_idx": 8}, {"type": "text", "text": "347 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "348 In this work, the \u201coverestimated decoding accuracy pitfall\u201d in various EEG decoding tasks is for  \n349 mulated in a unified framework by adopting the concept of \u201cdomain\u201d. Some typical EEG decoding   \n350 tasks (image decoding, emotion recognition, and auditory spatial attention detection) are conducted   \n351 on the self-collected watermelon EEG dataset. The results showed that EEG data from different   \n352 domains have distinctive domain features induced by EEG temporal autocorrelations. Using the in  \n353 appropriate data partitioning strategy, high decoding accuracy is achieved by associating class labels   \n354 with domain features. The results will draw attention to the high decoding performance caused by   \n355 EEG temporal correlation and guide the development of BCI in a positive direction. ", "page_idx": 8}, {"type": "text", "text": "356 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "357 [1] Yue-Ting Pan, Jing-Lun Chou, and Chun-Shu Wei. Matt: A manifold attention network for   \n358 eeg decoding. Advances in Neural Information Processing Systems, 35:31116\u201331129, 2022.   \n359 [2] Reinmar Kobler, Jun-ichiro Hirayama, Qibin Zhao, and Motoaki Kawanabe. Spd domain  \n360 specific batch normalization to crack interpretable unsupervised domain adaptation in eeg. Ad  \n361 vances in Neural Information Processing Systems, 35:6219\u20136235, 2022.   \n362 [3] C. Spampinato, S. Palazzo, I. Kavasidis, D. Giordano, N. Souly, and M. Shah. Deep learning   \n363 human mind for automated visual classification. In 2017 IEEE Conference on Computer Vision   \n364 and Pattern Recognition (CVPR), pages 4503\u20134511, 2017.   \n365 [4] Prajwal Singh, Dwip Dalal, Gautam Vashishtha, Krishna Miyapuram, and Shanmuganathan   \n366 Raman. Learning robust deep visual representations from eeg brain recordings. In Proceedings   \n367 of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 7553\u20137562,   \n368 2024.   \n369 [5] Isaak Kavasidis, Simone Palazzo, Concetto Spampinato, Daniela Giordano, and Mubarak Shah.   \n370 Brain2image: Converting brain signals into images. In Proceedings of the 25th ACM inter  \n371 national conference on Multimedia, MM 17, pages 1809\u20131817, New York, NY, USA, 2017.   \n372 Association for Computing Machinery.   \n373 [6] S. Palazzo, C. Spampinato, I. Kavasidis, D. Giordano, and M. Shah. Generative adversarial   \n374 networks conditioned by brain signals. In 2017 IEEE International Conference on Computer   \n375 Vision (ICCV), pages 3430\u20133438, 2017.   \n376 [7] Praveen Tirupattur, Yogesh Singh Rawat, Concetto Spampinato, and Mubarak Shah.   \n377 Thoughtviz: Visualizing human thoughts using generative adversarial network. In Proceed  \n378 ings of the 26th ACM international conference on Multimedia, MM 18, pages 950\u2013958, New   \n379 York, NY, USA, 2018. Association for Computing Machinery.   \n380 [8] Ren Li, Jared S. Johansen, Hamad Ahmed, Thomas V. Ilyevsky, Ronnie B. Wilbur, Hari M.   \n381 Bharadwaj, and Jeffrey Mark Siskind. The perils and pitfalls of block design for eeg classifica  \n382 tion experiments. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(1):316\u2013   \n383 333, 2021.   \n384 [9] Hamad Ahmed, Ronnie B. Wilbur, Hari M. Bharadwaj, and Jeffrey Mark Siskind. Object   \n385 classification from randomized eeg trials. In 2021 IEEE/CVF Conference on Computer Vision   \n386 and Pattern Recognition (CVPR), pages 3844\u20133853, 2021.   \n387 [10] Hari M Bharadwaj, Ronnie B. Wilbur, and Jeffrey Mark Siskind. Still an ineffective method   \n388 with supertrials/erpscomments on decoding brain representations by multimodal learning of   \n389 neural activity and visual features. IEEE Transactions on Pattern Analysis and Machine Intel  \n390 ligence, 45(11):14052\u201314054, 2023.   \n391 [11] Luc Berthouze, Leon M. James, and Simon F. Farmer. Human eeg shows long-range temporal   \n392 correlations of oscillation amplitude in theta, alpha and beta bands across a wide age range.   \n393 Clinical Neurophysiology, 121(8):1187\u20131197, 2010.   \n394 [12] Mona Irrmischer, Simon-Shlomo Poil, Huibert D. Mansvelder, Francesca Sangiuliano Intra,   \n395 and Klaus Linkenkaer-Hansen. Strong long-range temporal correlations of beta/gamma oscil  \n396 lations are associated with poor sustained visual attention performance. European Journal of   \n397 Neuroscience, 48(8):2674\u20132683, 2018.   \n398 [13] Klaus Linkenkaer-Hansen, Vadim V. Nikouline, J. Matias Palva, and Risto J. Ilmoniemi. Long  \n399 range temporal correlations and scaling behavior in human brain oscillations. Journal of Neu  \n400 roscience, 21(4):1370\u20131377, 2001.   \n401 [14] Vadim V. Nikulin and Tom Brismar. Long-range temporal correlations in alpha and beta oscil  \n402 lations: effect of arousal level and testretest reliability. Clinical Neurophysiology, 115(8):1896\u2013   \n403 1908, 2004.   \n404 [15] Simone Palazzo, Concetto Spampinato, Joseph Schmidt, Isaak Kavasidis, Daniela Giordano,   \n405 and Mubarak Shah. Correct block-design experiments mitigate temporal correlation bias in   \n406 eeg classification. arXiv preprint arXiv:2012.03849, 2020.   \n407 [16] Jacopo Cavazza, Waqar Ahmed, Riccardo Volpi, Pietro Morerio, Francesco Bossi, Cesco   \n408 Willemse, Agnieszka Wykowska, and Vittorio Murino. Understanding action concepts from   \n409 videos and brain activity through subjects consensus. Scientific Reports, 12(11):19073, 2022.   \n410 [17] Alankrit Mishra, Nikhil Raj, and Garima Bajwa. Eeg-based image feature extraction for vi  \n411 sual classification using deep learning. In 2022 International Conference on Intelligent Data   \n412 Science Technologies and Applications (IDSTA), pages 181\u2013188, 2022.   \n413 [18] Holly Wilson, Xi Chen, Mohammad Golbabaee, Michael J. Proulx, and Eamonn ONeill. Fea  \n414 sibility of decoding visual information from eeg. Brain-Computer Interfaces, 0(0):1\u201328, 2023.   \n415 [19] Lauren E. Ethridge, Shefali Brahmbhatt, Yuan Gao, Jennifer E. Mcdowell, and Brett A.   \n416 Clementz. Consider the context: Blocked versus interleaved presentation of antisaccade tri  \n417 als. Psychophysiology, 46(5):1100\u20131107, 2009.   \n418 [20] Nelson A. Roque, Timothy J. Wright, and Walter R. Boot. Do different attention capture   \n419 paradigms measure different types of capture? Attention, Perception & Psychophysics,   \n420 78(7):2014\u20132030, 2016.   \n421 [21] Enze Su, Siqi Cai, Longhan Xie, Haizhou Li, and Tanja Schultz. Stanet: A spatiotemporal   \n422 attention network for decoding auditory spatial attention from eeg. IEEE Transactions on   \n423 Biomedical Engineering, 69(7):2233\u20132242, 2022.   \n424 [22] Saurav Pahuja, Siqi Cai, Tanja Schultz, and Haizhou Li. Xanet: Cross-attention between eeg   \n425 of left and right brain for auditory attention decoding. In 2023 11th International IEEE/EMBS   \n426 Conference on Neural Engineering (NER), pages 1\u20134, 2023.   \n427 [23] Xiran Xu, Bo Wang, Yujie Yan, Xihong Wu, and Jing Chen. A densenet-based method for   \n428 decoding auditory spatial attention with eeg. In IEEE International Conference on Acoustics,   \n429 Speech and Signal Processing (ICASSP), pages 1946\u20131950, 2024.   \n430 [24] Qinke Ni, Hongyu Zhang, Cunhang Fan, Shengbing Pei, Chang Zhou, and Zhao Lv. Dbpnet:   \n431 Dual-branch parallel network with temporal-frequency fusion for auditory attention detection.   \n432 In International Joint Conference on Artificial Intelligence (IJCAI), 2024.   \n433 [25] Liangliang Hu, Congming Tan, Jiayang Xu, Rui Qiao, Yilin Hu, and Yin Tian. Decoding   \n434 emotion with phaseamplitude fusion features of eeg functional connectivity network. Neural   \n435 Networks, 172:106148, 2024.   \n436 [26] Jiayang Xu, Wenxia Qian, Liangliang Hu, Guangyuan Liao, and Yin Tian. Eeg decoding   \n437 for musical emotion with functional connectivity features. Biomedical Signal Processing and   \n438 Control, 89:105744, 2024.   \n439 [27] Zhi Zhang, Shenghua Zhong, and Yan Liu. Beyond mimicking under-represented emotions:   \n440 Deep data augmentation with emotional subspace constraints for eeg-based emotion recog  \n441 nition. Proceedings of the AAAI Conference on Artificial Intelligence, 38(99):10252\u201310260,   \n442 2024.   \n443 [28] Geoffrey Brookshire, Jake Kasper, Nicholas M. Blauch, Yunan Charles Wu, Ryan Glatt,   \n444 David A. Merrill, Spencer Gerrol, Keith J. Yoder, Colin Quirk, and Ch\u00e9 Lucero. Data leak  \n445 age in deep learning studies of translational eeg. Frontiers in Neuroscience, 18, 2024.   \n446 [29] Hamdi Altaheri, Ghulam Muhammad, Mansour Alsulaiman, Syed Umar Amin, Ghadir Ali   \n447 Altuwaijri, Wadood Abdul, Mohamed A. Bencherif, and Mohammed Faisal. Deep learning   \n448 techniques for classification of electroencephalogram (eeg) motor imagery (mi) signals: a re  \n449 view. Neural Computing and Applications, 35(20):14681\u201314722, 2023.   \n450 [30] Iustina Rotaru, Simon Geirnaert, Nicolas Heintz, Iris Van de Ryck, Alexander Bertrand, and   \n451 Tom Francart. What are we really decoding? unveiling biases in eeg-based decoding of the   \n452 spatial focus of auditory attention. Journal of Neural Engineering, 21(1):016017, 2024.   \n453 [31] Mukund Balasubramanian, William M. Wells, John R. Ives, Patrick Britz, Robert V. Mulk  \n454 ern, and Darren B. Orbach. Rf heating of gold cup and conductive plastic electrodes during   \n455 simultaneous eeg and mri. The Neurodiagnostic Journal, 57(1):69\u201383, 2017.   \n456 [32] Maximillian K. Egan, Ryan Larsen, Jonathan Wirsich, Brad P. Sutton, and Sepideh Sadaghiani.   \n457 Safety and data quality of eeg recorded simultaneously with multi-band fmri. PLOS ONE,   \n458 16(7):e0238485, 2021.   \n459 [33] Dominik Freche, Jodie Naim-Feil, Avi Peled, Nava Levit-Binnun, and Elisha Moses. A quan  \n460 titative physical model of the tms-induced discharge artifacts in eeg. PLOS Computational   \n461 Biology, 14(7):e1006177, 2018.   \n462 [34] Johan N. van der Meer, Yke B. Eisma, Ronald Meester, Marc Jacobs, and Aart J. Nederveen.   \n463 Effects of mobile phone electromagnetic fields on brain waves in healthy volunteers. Scientific   \n464 Reports, 13(1):21758, 2023.   \n465 [35] Tuomas Mutanen, Hanna M\u00e4ki, and Risto J. Ilmoniemi. The effect of stimulus parameters on   \n466 tmseeg muscle artifacts. Brain Stimulation, 6(3):371\u2013376, 2013.   \n467 [36] Limin Sun and Hermann Hinrichs. Simultaneously recorded eegfmri: Removal of gradient   \n468 artifacts by subtraction of head movement related average artifact waveforms. Human Brain   \n469 Mapping, 30(10):3361\u20133377, 2009.   \n470 [37] Sander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan Yazdani,   \n471 Touradj Ebrahimi, Thierry Pun, Anton Nijholt, and Ioannis Patras. Deap: A database for emo  \n472 tion analysis;using physiological signals. IEEE Transactions on Affective Computing, 3(1):18\u2013   \n473 31, 2012.   \n474 [38] Neetha Das, Tom Francart, and Alexander Bertrand. Auditory attention detection dataset kuleu  \n475 ven, 2020.   \n476 [39] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen,   \n477 Wenjun Zeng, and Philip S. Yu. Generalizing to unseen domains: A survey on domain gener  \n478 alization. IEEE Transactions on Knowledge and Data Engineering, 35(8):8052\u20138072, 2023.   \n479 [40] Jiquan Wang, Sha Zhao, Haiteng Jiang, Shijian Li, Tao Li, and Gang Pan. Generalizable sleep   \n480 staging via multi-level domain alignment. Proceedings of the AAAI Conference on Artificial   \n481 Intelligence, 38(11):265\u2013273, 2024.   \n482 [41] Chaoqi Yang, M. Brandon Westover, and Jimeng Sun. Manydg: Many-domain generalization   \n483 for healthcare applications. In The Eleventh International Conference on Learning Represen  \n484 tations, 2023.   \n485 [42] Bernd Accou, Lies Bollens, Marlies Gillis, Wendy Verheijen, Hugo Van Hamme, and Tom   \n486 Francart. Sparrkulee: A speech-evoked auditory response repository of the ku leuven, contain  \n487 ing eeg of 85 participants. bioRxiv preprint bioRxiv: 2023.07.24.550310, 2023.   \n488 [43] Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, and Juan Helen Zhou. Seeing beyond   \n489 the brain: Conditional diffusion model with sparse masked modeling for vision decoding. In   \n490 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages   \n491 22710\u201322720, 2023.   \n492 [44] Yunpeng Bai, Xintao Wang, Yan-pei Cao, Yixiao Ge, Chun Yuan, and Ying Shan. Dreamdiffu  \n493 sion: Generating high-quality images from brain eeg signals. arXiv preprint arXiv:2306.16934,   \n494 2023.   \n495 [45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint   \n496 arXiv:1711.05101, 2017.   \n497 [46] Xinke Shen, Xianggen Liu, Xin Hu, Dan Zhang, and Sen Song. Contrastive learning of subject  \n498 invariant eeg representations for cross-subject emotion recognition. IEEE Transactions on   \n499 Affective Computing, 14(3):2496\u20132511, 2023.   \n500 [47] Ivine Kuruvila, Jan Muncke, Eghart Fischer, and Ulrich Hoppe. Extracting the auditory atten  \n501 tion in a dual-speaker scenario from eeg using a joint cnn-lstm model. Frontiers in Physiology,   \n502 12, 2021.   \n503 [48] Changde Du, Kaicheng Fu, Jinpeng Li, and Huiguang He. Decoding visual neural representa  \n504 tions by multimodal learning of brain-visual-linguistic features. IEEE Transactions on Pattern   \n505 Analysis and Machine Intelligence, 45(9):10760\u201310777, 2023.   \n506 [49] Yonghao Song, Bingchuan Liu, Xiang Li, Nanlin Shi, Yijun Wang, and Xiaorong Gao. Decod  \n507 ing natural images from eeg for object recognition. In The Twelfth International Conference   \n508 on Learning Representations, 2024.   \n509 [50] Zesheng Ye, Lina Yao, Yu Zhang, and Sylvia Gustin. Self-supervised cross-modal visual   \n510 retrieval from brain activities. Pattern Recognition, 145:109915, 2024.   \n511 [51] Yiming Wang, Bin Zhang, and Yujiao Tang. Dmmr: Cross-subject domain generalization for   \n512 eeg-based emotion recognition via denoising mixed mutual reconstruction. Proceedings of the   \n513 AAAI Conference on Artificial Intelligence, 38(11):628\u2013636, 2024.   \n514 [52] Servaas Vandecappelle, Lucas Deckers, Neetha Das, Amir Hossein Ansari, Alexander   \n515 Bertrand, and Tom Francart. Eeg-based detection of the locus of auditory attention with con  \n516 volutional neural networks. eLife, 10:e56481, 2021.   \n517 [53] Theo Gnassounou, R\u00e9mi Flamary, and Alexandre Gramfort. Convolution monge mapping   \n518 normalization for learning on sleep data. Advances in Neural Information Processing Systems,   \n519 36, 2023.   \n520 [54] Johanna Wilroth, Bo Bernhardsson, Frida Heskebeck, Martin A. Skoglund, Carolina Bergeling,   \n521 and Emina Alickovic. Improving eeg-based decoding of the locus of auditory attention through   \n522 domain adaptation\\*. Journal of Neural Engineering, 20(6):066022, 2023.   \n523 [55] Weibang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large brain model for learning generic   \n524 representations with tremendous eeg data in bci. In The Twelfth International Conference on   \n525 Learning Representations, 2024.   \n526 [56] Chaoqi Yang, M. Westover, and Jimeng Sun. Biot: Biosignal transformer for cross-data learn  \n527 ing in the wild. In Advances in Neural Information Processing Systems, volume 36, 2023.   \n528 [57] Ke Yi, Yansen Wang, Kan Ren, and Dongsheng Li. Learning topology-agnostic eeg representa  \n529 tions with geometry-aware modeling. In Advances in Neural Information Processing Systems,   \n530 volume 36, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "3gZBGBglBf/tmp/d25fcf600cd99b8d16a657142958db7351de74aaf72be9d09db0d68061eebf81.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 4: Photos of watermelons used in the experiment. Each watermelon\u2019s ID is marked on the watermelon, with IDs ranging from 1 to 10. ", "page_idx": 13}, {"type": "text", "text": "533 A.2 Reorganization for KUL dataset and DEAP dataset ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "534 For the emotion recognition task, we referred to the experimental design of DEAP dataset [37]. In   \n535 this dataset, the EEG data were recorded while subjects are presented with 40 audio-visual clips of   \n536 60 seconds in length, with each corresponding to one of four emotion classes. We only used the first   \n537 32 channels of the EEG to match the EEG channel numbers in the DEAP dataset. The watermelon   \n538 EEG data and SparrKULee EEG data were down-sampled to $128\\;\\mathrm{Hz}$ and then were segmented into   \n539 40 60-second segments. The interval between adjacent segments is set to 40 seconds to match the   \n540 rest time of the subjects during the EEG recording in the KUL dataset. Each segment was assigned   \n541 a unique domain label and a class label in accordance with the DEAP dataset, and each segment was   \n542 further segmented into 2-second samples [25]. The reorganized datasets for the Watermelon EEG   \n543 Dataset and SparrKULee Dataset are called WM-DEAP and SK-DEAP, respectively.   \n544 For the ASAD task, we referred to the experimental design of the KUL dataset [38]. In this dataset,   \n545 8 clips of two-talker mixed speech are presented to subjects, with each lasting for 6 minutes. Each   \n546 speech clip contains a left talker and a right talker. Subjects are instructed to attend left or right talker   \n547 during the entire duration of one clip presentation. The watermelon EEG data and SparrKULee EEG   \n548 data were down-sampled to $128~\\mathrm{Hz}$ and then were epoch into 8 6-minute segments. The interval   \n549 between adjacent segments is set to 1-2 minutes to match the rest time of the subjects during the EEG   \n550 recording in the KUL dataset. Each segment was assigned a unique domain label and a class label   \n551 in accordance with the KUL dataset and was further segmented into 1-second samples [22, 21, 23].   \n552 The reorganized datasets for Watermelon Dataset and SparrKULee Dataset are called WM-KUL and   \n553 SK-KUL, respectively. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "554 A.3 Detailed implementation of joint training ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "555 The joint training was performed on the WM-CVPR and SK-CVPR datasets. All EEG samples   \n556 were randomly divided into the training set, validation set, and test set in a ratio of 8:1:1. The image   \n557 encoder of the CLIP (CLIP VIT-L/14) model 1 is chosen to extract image representation, yielding   \n558 768-dimensional vectors from the image inputs. The structure of the EEG encoder is similar to the   \n559 model introduced in Subsection 2.3, with an augmentation from 40 to 768 output nodes to match   \n560 the dimension of the image representation. The network is trained using either a cosine similarity   \n561 (CS) loss or an InfoNCE contrastive loss (with a temperature parameter set to 0.07). The evaluation   \n562 metrics selected are Top-1 accuracy, Top-5 accuracy, and Rank accuracy, where the Top-1 accuracy   \n563 metric is equivalent to the classification accuracy in the classification task. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "564 A.4 Detailed implementation of image generation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "565 We take an approach similar to previous works [44] 2. We used a CLIP image encoder to extract   \n566 image representation and trained an EEG encoder with cosine similarity loss to reconstruct image   \n567 representation from EEG. This process is the same as described in Joint training with image features.   \n568 The reconstructed features are then serviced as a conditional input of an image generator. To match   \n569 the reconstructed features, we employ the pre-trained StableDiffusion model 3 as our generator. This   \n570 model uses a fixed pre-trained image encoder (CLIP VIT-L/14) to extract image features, which   \n571 then guide the Latent Diffusion models generation process in the latent space. The diffusion model   \n572 gradually generates images from a random noise distribution that corresponds to the conditional   \n573 features during its iterative process. To improve the generation performance, we fine-tuned the   \n574 generator with the reconstructed image features and the corresponding images. Experiments were   \n575 done on the WM-CVPR and SK-CVPR datasets. All EEG samples were randomly divided into   \n576 training set, validation set, and test set in a ratio of 8:1:1.   \n577 Consistent with previous work [1], we evaluate the semantic correctness of the generated images   \n578 using N-way Top-1 and Top-5 accuracy classification tasks. Specifically, given a generated image   \n579 input, a pre-trained ImageNet1K classifier is used to output a classification logit probability among   \n580 1000 classes. Among the 1000 classes, N-1 random classes and the correct class are selected, and   \n581 the Top-1 and Top-5 classification accuracy are calculated. To avoid randomness, this operation is   \n582 repeated 50 times for each generated image, with the average value taken as the accuracy. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "583 A.5 leave-subjects-out data splitting strategy ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "584 In this subsection, we employed the leave-subjects-out data splitting strategy. This refers to using   \n585 data from a subset of subjects for training, while data from the remaining subjects are used for   \n586 testing. Within the training data, there are two further data partitioning methods: leave-samples-out   \n587 and leave-subjects-out. The former involves randomly dividing all samples of the training data into   \n588 training and validation sets, whereas the latter uses data from a subset of subjects for the training set,   \n589 with the remaining subjects data allocated for the test set. Table 5 presents the decoding accuracy   \n590 for six datasets (i.e., WM-CVPR, WM-DEAP, WM-KUL, SK-CVPR, SK-DEAP, and SK-KUL).   \n591 It can be observed that when the leave-samples-out splitting strategy was used within the training   \n592 data, both the training and validation sets achieved very high decoding accuracy, but the accuracy   \n593 only reached the chance level on the test set. Such results are similar to those reported by [46, 47, 8],   \n594 which corroborates the argument that while the leave-subjects-out approach may avert the domain   \n595 features leakage, it cannot prevent overfitting of the domain features during the training stage, as   \n596 discussed in Subsection 4.1. Moreover, when the leave-subjects-out data splitting strategy was used   \n597 within the training dataset, the validation set performance was only at chance level despite high   \n598 accuracy on the training set. This further demonstrates that decoding that relies on domain features   \n599 cannot be generalized to practical application scenarios. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Table 5: Decoding accuracy $(\\%)$ for the six datasets on training, validation and test set. Leavesubjects-out data splitting strategy is used for training and test data. Leave-samples-out and leavesubjects-out data splitting strategy is used for training and validation set. The mean accuracy and standard deviation are calculated over subjects level with a five-fold cross-validation. ", "page_idx": 15}, {"type": "table", "img_path": "3gZBGBglBf/tmp/0f9b28fceb674798520dd7b20109e47ff5d36241590a5a30f9f490ec7d63b922.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "600 A.6 Results on different frequency band ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "601 To demonstrate that domain features are not solely due to baseline drift, we conducted an analysis on   \n602 seven frequency bands across six datasets. These seven frequency bands are delta $(0{-}4\\,\\mathrm{Hz})$ , theta (4-   \n603 $8\\,\\mathrm{Hz},$ ), alpha $\\scriptstyle(8-12\\,\\mathrm{{Hz})}$ , beta $_{(12-32\\,\\mathrm{Hz})}$ ), low gamma $(32{-}45\\ \\mathrm{Hz})$ ), and high gamma $(55{-}95\\,\\mathrm{Hz})$ . High   \n604 gamma frequency band results for DEAP and KUL datasets are not presented due to the sampling   \n605 rate of $128~\\mathrm{Hz}$ (i.e., only frequency under $64\\ \\mathrm{Hz}$ is available according to the Nyquist sampling   \n606 theorem). Tables 6, 7, 8, and 9 show the decoding accuracy for domain label classification (DLC  \n607 EEG), class label classification from domain features (TLC-DF), class label classification directly   \n608 from EEG (TLC-EEG), and class label classification directly from EEG when samples in the training   \n609 set and test set are from different domains (TLC-EEG-woDO), respectively. As expected, the highest   \n610 decoding accuracy is observed for both the low-frequency band (delta band) and the full-frequency   \n611 EEG data. However, other frequency bands also exhibited decoding accuracy significantly higher   \n612 than the chance level. This suggests that baseline correction through filtering does not eliminate   \n613 domain features. Consequently, any experimental designs and data partitioning strategies that could   \n614 lead to the leakage of domain information should be meticulously avoided. ", "page_idx": 15}, {"type": "table", "img_path": "3gZBGBglBf/tmp/20f5404886403faa34d2b538af8bf2a8f29b5a9b0f5030fb9cb26a2a014c0bae.jpg", "table_caption": ["Table 6: Decoding accuracy $(\\%)$ using different EEG bands for domain label classification (DLCEEG) "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "3gZBGBglBf/tmp/f208f4f0aa0f15dfcb8838e1f31705b286733db42c962916066b41b32849534b.jpg", "table_caption": ["Table 7: Decoding accuracy $(\\%)$ using different EEG bands for class label classification from domain features (TLC-DF) "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "3gZBGBglBf/tmp/94a6f5495ac490398afe93644033ceb718b6946b84208177916bd0d58a7d69cc.jpg", "table_caption": ["Table 8: Decoding accuracy $(\\%)$ using different EEG bands for class label classification directly from EEG (TLC-EEG) "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "3gZBGBglBf/tmp/d33dbdefad288f442f78a9c1a541fb18ea1b94874a66cb1843f66053b7f6123e.jpg", "table_caption": ["Table 9: Decoding accuracy $(\\%)$ using different EEG bands for class label classification directly from EEG when samples in the training set and test set are from different domains (TLC-EEGwoDO) "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "615 A.7 LRTC ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "616 The autocorrelation analysis was used to evaluate long range temporal correlation in EEG data from   \n617 the Watermelon and SparrKULee datasets, similar to the approach taken by previous study. For a   \n618 lengthy segment of single-channel EEG, the Morlet wavelet transform was employed to extract the   \n619 time-varying amplitude envelope $W_{f}(t)$ at a given frequency $f$ . The autocorrelation function $A C F_{f}$   \n620 for $W_{f}(t)$ is defined as: ", "page_idx": 16}, {"type": "equation", "text": "$$\nA C F_{f}(\\tau)=c o r r(W_{f}(t),W_{f}(t+\\tau))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "621 In the above equation, $c o r r(,)$ denotes the Pearson correlation coefficient between two time series,   \n622 and $\\tau$ represents the time lag.   \n623 In our analysis, the original EEG data were down-sampled to $200\\,\\mathrm{Hz}$ . Ninety-five analysis frequen  \n624 cies were distributed linearly and evenly between $1{\\-}95\\ \\mathrm{Hz}$ . Two hundred autocorrelation time lags   \n625 were logarithmically spaced between $0.5\\mathrm{~s~}$ and $500\\;\\mathrm{s}$ . For each subject in the Watermelon dataset,   \n626 continuous EEG recordings were divided into five segments of equal length (with each segment   \n627 ranging from 15 to 20 minutes), and autocorrelation analysis was completed on each segment. For   \n628 each subject in the SparrKULee dataset, the autocorrelation analysis was carried out separately on   \n629 each of their ten trials. Figure 5 shows the results of the autocorrelation analysis for the Watermelon   \n630 and SparrKULee datasets. The figure illustrates the magnitude of correlation at different frequen  \n631 cies and time lags (represented by color). The correlation values were obtained by averaging the   \n632 results across all subjects, segments (trials), and electrodes. Black lines represent the contour lines   \n633 where $p\\,=\\,0.01$ , as determined by statistical analysis. Statistical significance was assessed using   \n634 single-sample t-test at the subject-electrode level. Specifically, for each electrode of each subject, the   \n635 averaged Pearson correlation coefficient across all segments (trials) was used as the value for the t  \n636 test. Additionally, $p$ -values were corrected for multiple comparisons using the Benjamini-Hochberg   \n637 False Discovery Rate (BH-FDR) to type I error.   \n638 As demonstrated in Figure 5, EEG data from both Watermelon and SparrKULee datasets show   \n639 significant LRTC across multiple frequency bands. For the EEG data from the Watermelon dataset,   \n640 significant bands of LRTC are primarily distributed in the low-frequency range $(<\\!8\\;\\mathrm{Hz})$ and around   \n641 $50\\,\\mathrm{Hz}$ , with these correlations spanning over 500 seconds. This indicates that baseline drifts and line   \n642 noise contribute to the temporal correlation observed in the Watermelon dataset. For the EEG data   \n643 from the SparrKULee dataset, LRTCs are significant across the entire frequency range. Similarly,   \n644 LTRCs are most prominent at low frequencies $(<\\!5\\,\\mathrm{Hz})$ and around $50\\,\\mathrm{Hz}$ , consistent with the findings   \n645 from the Watermelon dataset. Notably, for SparrKULee dataset, there is also a significant presence   \n646 of LTRC around $10\\:\\mathrm{Hz}$ , which aligns with previous research findings [13], suggesting the temporal   \n647 correlation of alpha oscillations in human subjects. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "image", "img_path": "3gZBGBglBf/tmp/893734c57e6b2288c5cab9a724588570d78632e05528d19d0a94bf97a4accdc6.jpg", "img_caption": ["Figure 5: Autocorrelation analysis result on (a) Watermelon and (b) SparrKULee datasets. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "648 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "55 Guidelines:   \n56 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n57 made in the paper.   \n58 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n59 contributions made in the paper and important assumptions and limitations. A No or   \n60 NA answer to this question will not be perceived well by the reviewers.   \n61 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n62 much the results can be expected to generalize to other settings.   \n63 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these   \n64 goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "65 2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "0 \u2022 The answer NA means that the paper has no limitation while the answer No means   \n1 that the paper has limitations, but those are not discussed in the paper.   \n72 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n3 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n4 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n5 model well-specification, asymptotic approximations only holding locally). The au  \n6 thors should reflect on how these assumptions might be violated in practice and what   \n7 the implications would be.   \n78 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n9 only tested on a few datasets or with a few runs. In general, empirical results often   \n0 depend on implicit assumptions, which should be articulated.   \n1 \u2022 The authors should reflect on the factors that influence the performance of the ap  \n82 proach. For example, a facial recognition algorithm may perform poorly when image   \n3 resolution is low or images are taken in low lighting. Or a speech-to-text system might   \n4 not be used reliably to provide closed captions for online lectures because it fails to   \n85 handle technical jargon.   \n86 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n7 and how they scale with dataset size.   \n88 \u2022 If applicable, the authors should discuss possible limitations of their approach to ad  \n9 dress problems of privacy and fairness.   \n90 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n1 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n92 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n3 judgment and recognize that individual actions in favor of transparency play an impor  \n4 tant role in developing norms that preserve the integrity of the community. Reviewers   \n5 will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "696 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "697 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n698 a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "00 Justification: The paper does not include any theoretical results.   \n01 Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "713 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "14 Question: Does the paper fully disclose all the information needed to reproduce the main   \n15 experimental results of the paper to the extent that it affects the main claims and/or conclu  \n16 sions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Justification: the information needed to reproduce all the experimental results of this paper could be found in Section 2.2, Section 2.3, Section 2.4, Section 2.5 and Appendix A. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "752 5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "753 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n754 tions to faithfully reproduce the main experimental results, as described in supplemental   \n755 material?   \n756 Answer: [Yes]   \n757 Justification: the collected Watermelon EEG dataset could be available in Zenodo and the   \n758 human dataset used in this work could also be downloaded in the link provided in supple  \n759 mentary materials. All the codes to reproduce this work can be found in supplementary   \n760 materials.   \n761 Guidelines:   \n762 \u2022 The answer NA means that paper does not include experiments requiring code.   \n763 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n764 public/guides/CodeSubmissionPolicy) for more details.   \n765 \u2022 While we encourage the release of code and data, we understand that this might not   \n766 be possible, so No is an acceptable answer. Papers cannot be rejected simply for not   \n767 including code, unless this is central to the contribution (e.g., for a new open-source   \n768 benchmark).   \n769 \u2022 The instructions should contain the exact command and environment needed to run to   \n770 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n771 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n772 \u2022 The authors should provide instructions on data access and preparation, including how   \n773 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n774 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n775 proposed method and baselines. If only a subset of experiments are reproducible, they   \n776 should state which ones are omitted from the script and why.   \n777 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n778 versions (if applicable).   \n779 \u2022 Providing as much information as possible in supplemental material (appended to the   \n780 paper) is recommended, but including URLs to data and code is permitted.   \n781 6. Experimental Setting/Details   \n782 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n783 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n784 results?   \n785 Answer: [Yes]   \n786 Justification: the experimental setting and details could also be found in Section 2.2, Sec  \n787 tion 2.3, Section 2.4, Section 2.5, and could also be found in the codes.   \n788 Guidelines:   \n789 \u2022 The answer NA means that the paper does not include experiments.   \n790 \u2022 The experimental setting should be presented in the core of the paper to a level of   \n791 detail that is necessary to appreciate the results and make sense of them.   \n792 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n793 material.   \n794 7. Experiment Statistical Significance   \n795 Question: Does the paper report error bars suitably and correctly defined or other appropri  \n796 ate information about the statistical significance of the experiments?   \n797 Answer: [Yes]   \n798 Justification: the standard error of the mean is reported for all results. As we only com  \n799 pared the result against chance level, one-sample t-test was used for statistical analysis to   \n800 check whether the reported results are significant high above the chance level. Given that   \n801 decoding analyses was conducted on multiple frequency bands, Bonferroni correction was   \n802 used to adjust the p-value to reduce the risk of type-I error. A p-value of 0.05 or lower is   \n803 considered statistically significant.   \n804 Guidelines:   \n805 \u2022 The answer NA means that the paper does not include experiments.   \n806 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n807 dence intervals, or statistical significance tests, at least for the experiments that support   \n808 the main claims of the paper.   \n809 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n810 example, train/test split, initialization, random drawing of some parameter, or overall   \n811 run with given experimental conditions).   \n812 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n813 call to a library function, bootstrap, etc.)   \n814 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n815 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n816 of the mean.   \n817 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should prefer  \n818 ably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of   \n819 Normality of errors is not verified.   \n820 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n821 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n822 error rates).   \n823 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n824 they were calculated and reference the corresponding figures or tables in the text.   \n825 8. Experiments Compute Resources   \n826 Question: For each experiment, does the paper provide sufficient information on the com  \n827 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n828 the experiments?   \n829 Answer: [Yes]   \n830 Justification: the neural networks were implemented with the Pytorch and trained on a   \n831 single HPC node with 8 A800 GPU.   \n832 Guidelines:   \n833 \u2022 The answer NA means that the paper does not include experiments.   \n834 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n835 or cloud provider, including relevant memory and storage.   \n836 \u2022 The paper should provide the amount of compute required for each of the individual   \n837 experimental runs as well as estimate the total compute.   \n838 \u2022 The paper should disclose whether the full research project required more compute   \n839 than the experiments reported in the paper (e.g., preliminary or failed experiments   \n840 that didn\u2019t make it into the paper).   \n841 9. Code Of Ethics   \n842 Question: Does the research conducted in the paper conform, in every respect, with the   \n843 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n844 Answer: [Yes]   \n845 Justification: The collected dataset was released in an anonymous form, and all codes do   \n846 not contain any identity information.   \n847 Guidelines:   \n848 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n849 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n850 deviation from the Code of Ethics.   \n851 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n852 eration due to laws or regulations in their jurisdiction).   \n853 10. Broader Impacts   \n854 Question: Does the paper discuss both potential positive societal impacts and negative   \n855 societal impacts of the work performed?   \n857 Justification: The purpose of this paper is to let researchers beware of overestimated de  \n858 coding performance arising from temporal autocorrelations in EEG signals. This work   \n859 formalizes and proves the pitfalls existing in current EEG decoding tasks. We believe that   \n860 this will not generate any significant societal impact.   \n861 Guidelines:   \n862 \u2022 The answer NA means that there is no societal impact of the work performed.   \n863 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n864 impact or why the paper does not address societal impact.   \n865 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n866 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n867 (e.g., deployment of technologies that could make decisions that unfairly impact spe  \n868 cific groups), privacy considerations, and security considerations.   \n869 \u2022 The conference expects that many papers will be foundational research and not tied   \n870 to particular applications, let alone deployments. However, if there is a direct path to   \n871 any negative applications, the authors should point it out. For example, it is legitimate   \n872 to point out that an improvement in the quality of generative models could be used to   \n873 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n874 that a generic algorithm for optimizing neural networks could enable people to train   \n875 models that generate Deepfakes faster.   \n876 \u2022 The authors should consider possible harms that could arise when the technology is   \n877 being used as intended and functioning correctly, harms that could arise when the   \n878 technology is being used as intended but gives incorrect results, and harms following   \n879 from (intentional or unintentional) misuse of the technology.   \n880 \u2022 If there are negative societal impacts, the authors could also discuss possible mitiga  \n881 tion strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n882 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n883 feedback over time, improving the efficiency and accessibility of ML).   \n884 11. Safeguards   \n885 Question: Does the paper describe safeguards that have been put in place for responsible   \n886 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n887 image generators, or scraped datasets)?   \n888 Answer: [NA]   \n889 Justification: the paper poses no such risks.   \n890 Guidelines:   \n891 \u2022 The answer NA means that the paper poses no such risks.   \n892 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n893 necessary safeguards to allow for controlled use of the model, for example by re  \n894 quiring that users adhere to usage guidelines or restrictions to access the model or   \n895 implementing safety filters.   \n896 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n897 should describe how they avoided releasing unsafe images.   \n898 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n899 not require this, but we encourage authors to take this into account and make a best   \n900 faith effort.   \n901 12. Licenses for existing assets   \n902 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n903 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n904 properly respected?   \n905 Answer: [Yes]   \n906 Justification: The existing assets we used are the SparrKULee dataset, which is licensed   \n907 under CC-BY-NC-4.0.   \n908 Guidelines:   \n909 \u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Justification: We have released an anonymous Watermelon EEG dataset, which can be accessed at https://zenodo.org/records/11238929 ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: we collected the Watermelon EEG dataset, but watermelon is not a human subject. Nonetheless, we provided experiment details, which could be found in section Section 2.2. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "962 Answer: [NA]   \n963 Justification: the paper does not involve crowdsourcing nor research with human subjects   \n964 Guidelines:   \n965 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research   \n966 with human subjects.   \n967 \u2022 Depending on the country in which research is conducted, IRB approval (or equiva  \n968 lent) may be required for any human subjects research. If you obtained IRB approval,   \n969 you should clearly state this in the paper.   \n970 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n971 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n972 guidelines for their institution.   \n973 \u2022 For initial submissions, do not include any information that would break anonymity   \n974 (if applicable), such as the institution conducting the review. ", "page_idx": 24}]