[{"type": "text", "text": "AdjointDEIS: Efficient Gradients for Diffusion Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zander W. Blasingame Chen Liu Clarkson University Clarkson University blasinzw@clarkson.edu cliu@clarkson.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The optimization of the latents and parameters of diffusion models with respect to some differentiable metric defined on the output of the model is a challenging and complex problem. The sampling for diffusion models is done by solving either the probability flow ODE or diffusion SDE wherein a neural network approximates the score function allowing a numerical ODE/SDE solver to be used. However, na\u00efve backpropagation techniques are memory intensive, requiring the storage of all intermediate states, and face additional complexity in handling the injected noise from the diffusion term of the diffusion SDE. We propose a novel family of bespoke ODE solvers to the continuous adjoint equations for diffusion models, which we call AdjointDEIS. We exploit the unique construction of diffusion SDEs to further simplify the formulation of the continuous adjoint equations using exponential integrators. Moreover, we provide convergence order guarantees for our bespoke solvers. Significantly, we show that continuous adjoint equations for diffusion SDEs actually simplify to a simple ODE. Lastly, we demonstrate the effectiveness of AdjointDEIS for guided generation with an adversarial attack in the form of the face morphing problem. Our code will be released on our project page https://zblasingame.github.io/AdjointDEIS/. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models are a large family of state-of-the-art generative models which learn to map samples drawn from white Gaussian noise into the data distribution [1, 2]. These diffusion models have achieved state-of-the-art performance on prominent tasks such as image generation [3\u20135], audio generation [6, 7], or video generation [8]. Often, the state-of-the-art models are quite large and training them is prohibitively expensive [9]. As such, it is fairly common to adapt a pre-trained model to a specific task for post-training. This way, the generative model can learn new concepts, identities, or tasks without needing to train the whole model [10\u201312]. Additional work has also proposed algorithms for guiding the generative process of the diffusion models [13, 14]. ", "page_idx": 0}, {"type": "text", "text": "One method to guide or direct the generative process is to solve an optimization problem w.r.t. some guidance function $\\mathcal{L}$ defined on the image space $\\mathbb{R}^{d}$ . This guidance function works on the output of the diffusion model and assesses how \u201cgood\u201d the output is. However, the diffusion model works by iteratively removing noise until a clean sample is reached. As such, we need to be able to efficiently backpropagate gradients through the entire generative process. As Song et al. [15] showed, the diffusion SDE can be simplified to an associated ODE, and as such, many efficient ODE/SDE solvers have been developed for diffusion models [16\u201318]. However, na\u00efvely applying backpropagation to the diffusion model is inflexible and memory intensive; moreover, such an approach is not trivial to apply to the diffusion models that used an SDE solver instead of an ODE solver. ", "page_idx": 0}, {"type": "image", "img_path": "fAlcxvrOEX/tmp/6059cb1a63c5b9e68ba514aa69a3920963fcedc169cf8c9451f37388136582fd.jpg", "img_caption": ["Figure 1: A high-level overview of the AdjointDEIS solver to the continuous adjoint equations for diffusion models. The sampling schedule consists of $\\{t_{n}\\}_{n=0}^{N}$ timesteps for the diffusion model and $\\{\\tilde{t}_{n}\\}_{n=0}^{M}$ timesteps for AdjointDEIS. The gradients ${\\bf a}_{\\bf x}(T)$ can be used to optimize $\\mathbf{x}_{T}$ to find some optimal $\\mathbf{x}_{T}^{*}$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Inspired by the work of [19, 20] we study the application of the continuous adjoint equations to diffusion models, with a focus on training-free guided generation with diffusion models. We introduce several theoretical contributions and technical insights to both improve the ability to perform certain guided generation tasks and to gain insight into guided generation with diffusion models. ", "page_idx": 1}, {"type": "text", "text": "First, we introduce AdjointDEIS a bespoke family of ODE solvers which can efficiently solve the continuous adjoint equations for both diffusion ODEs and SDEs. To the best of our knowledge, this is the first general backpropagation technique designed for diffusion SDEs. Moreover, we show that the continuous adjoint equations for diffusion SDEs simplify to a mere ODE. ", "page_idx": 1}, {"type": "text", "text": "Next, we show how to calculate the continuous adjoint equation for conditional information which evolves with time (rather than being constant). To the best of our knowledge, we are the first to consider conditional information which evolves with time for neural ODEs. ", "page_idx": 1}, {"type": "text", "text": "Overall, multiple theoretical contributions and technical insights are provided to bring a new family of techniques for the guided generation of diffusion models, which we evaluate experimentally on the task of face morphing. ", "page_idx": 1}, {"type": "text", "text": "2 Diffusion Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we provide a brief overview of diffusion models. Diffusion models learn a generative process by first perturbing the data distribution into an isotropic Gaussian by progressively adding Gaussian noise to the data distribution. Then, a neural network is trained to perform denoising steps, allowing for sampling of the data distribution via sampling of a Gaussian distribution [2, 9]. Assume we have an $n$ -dimensional random variable $\\mathbf{x}\\in\\mathbb{R}^{n}$ with some distribution $p_{d a t a}(\\mathbf{x})$ . Then diffusion models begin by diffusing $p_{d a t a}(\\mathbf{x})$ according to the diffusion SDE [2], an It\u00f4 SDE given as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=f(t)\\mathbf{x}_{t}\\;\\mathrm{d}t+g(t)\\;\\mathrm{d}\\mathbf{w}_{t}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $t\\ \\in\\ [0,T]$ denotes time with fixed constant $T~>~0$ , $f(\\cdot)$ and $g(\\cdot)$ denote the drift and diffusion coefficients, and $\\mathbf{w}_{t}$ denotes the standard Wiener process. The trajectories of $\\mathbf{x}_{t}$ follow the distributions $p_{t}(\\mathbf{x}_{t})$ with $p_{0}(\\mathbf{x}_{0})\\,\\equiv\\,p_{\\mathrm{data}}(\\mathbf{x})$ and $p_{T}(\\mathbf{x}_{T})\\,\\approx\\,\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . Under some regularity conditions Song et al. [15] showed that Equation (2.1) has a reverse process as time runs backwards from $T$ to 0 with initial marginal distribution $p_{T}(\\mathbf{x}_{T})$ governed by ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=\\left[f(t)\\mathbf{x}_{t}-g^{2}(t)\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x}_{t})\\right]\\mathrm{d}t+g(t)\\;\\mathrm{d}\\bar{\\mathbf{w}}_{t}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\bar{\\bf w}_{t}$ is the standard Wiener process as time runs backwards. Solving Equation (2.2) is what allows diffusion models to draw samples from $p_{d a t a}(\\mathbf{x})$ by sampling $p_{T}(\\mathbf{x}_{T})$ . The unknown term in Equation (2.2) is the score function $\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x}_{t})$ , which in practice is modeled by a neural network that estimates the scaled score function, $\\begin{array}{r}{\\epsilon_{\\theta}(\\mathbf{x}_{t},t)\\approx-\\bar{\\sigma_{t}}\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x}_{t})}\\end{array}$ , or some closely related quantity like $\\mathbf{x}_{\\mathrm{0}}$ -prediction [1, 2, 21]. ", "page_idx": 2}, {"type": "text", "text": "2.1 Probability Flow ODE ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The practical choice of a step size when discretizing SDEs is limited by the randomness of the Wiener process as a large step size, i.e., a small number of steps, can cause non-convergence, particularly in high-dimensional spaces [16]. Sampling an equivalent Ordinary Differential Equation (ODE) over an SDE would enable faster sampling. Song et al. [15] showed there exists an Ordinary Differential Equation (ODE) whose marginal distribution at time $t$ is identical to that of Equation (2.2) given as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}=f(t)\\mathbf{x}_{t}-\\frac{1}{2}g^{2}(t)\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x}_{t}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The ODE in Equation (2.3) is known as the probability flow $O D E$ [15]. As the noise prediction network, $\\epsilon_{\\theta}(\\mathbf{x}_{t},t)$ , is trained to model the scaled score function, Equation (2.3) can be parameterized as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}=f(t)\\mathbf{x}_{t}+\\frac{g^{2}(t)}{2\\sigma_{t}}\\epsilon_{\\theta}(\\mathbf{x}_{t},t)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "w.r.t. the noise prediction network. For brevity, we refer to this as a diffusion ODE. ", "page_idx": 2}, {"type": "text", "text": "While there exist several popular choices for the drift and diffusion coefficients, we opt to use the de facto choice which is known as the Variance Preserving (VP) type diffusion SDE [1, 15, 22]. The coefficients for VP-type SDEs are given as ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(t)=\\frac{\\mathrm{d}\\log\\alpha_{t}}{\\mathrm{d}t},\\quad g^{2}(t)=\\frac{\\mathrm{d}\\sigma_{t}^{2}}{\\mathrm{d}t}-2\\frac{\\mathrm{d}\\log\\alpha_{t}}{\\mathrm{d}t}\\sigma_{t}^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which corresponds to sampling $\\mathbf{x}_{t}$ from the distribution $q(\\mathbf{x}_{t}\\mid\\mathbf{x}_{0})=\\mathcal{N}(\\alpha_{t}\\mathbf{x}_{0},\\sigma_{t}^{2}\\mathbf{I})$ ", "page_idx": 2}, {"type": "text", "text": "3 Adjoint Diffusion ODEs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Problem statement. Given the diffusion ODE in Equation (2.4), we wish to solve the following optimization problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underset{{\\bf x}_{T},{\\bf z},\\theta}{\\arg\\operatorname*{min}}\\ \\mathcal{L}\\bigg({\\bf x}_{T}+\\int_{T}^{0}f(t){\\bf x}_{t}+\\frac{g^{2}(t)}{2\\sigma_{t}}\\epsilon_{\\theta}({\\bf x}_{t},{\\bf z},t)\\ \\mathrm{d}t\\bigg).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "I.e., we wish to find the optimal $\\mathbf{x}_{T},\\,\\mathbf{z},$ , and $\\theta$ which satisfy our guidance function $\\mathcal{L}$ . ", "page_idx": 2}, {"type": "text", "text": "Unlike GANs which can update the latent representation through GAN inversion [23, 24], as seen in Equation (3.1) diffusion models require more care as they model an ODE or SDE and require numerical solvers. Therefore, to update the latent representation, model parameters, and conditional information we must backpropagate the gradient of loss defined on the output, $\\partial\\mathcal{L}(\\mathbf{x}_{0})/\\partial\\mathbf{x}_{0}$ through the whole ODE or SDE. ", "page_idx": 2}, {"type": "text", "text": "A key insight of this work is the connection between the adjoint ODE used in neural ODEs by Chen et al. [19] and specialized ODE/SDE solvers by [16\u201318] for diffusion models. It has been well observed that diffusion models are a type of neural ODE [15, 25]. Since a diffusion model can be thought of as a neural ODE, then we can solve the continuous adjoint equations [20] to find useful gradients for guided generation. We can then exploit the unique structure of diffusion models to develop efficient bespoke ODE solvers for the continuous adjoint equations. ", "page_idx": 2}, {"type": "text", "text": "Let $\\scriptstyle f_{\\theta}$ describe a parameterized neural field of the probability flow ODE, i.e., the R.H.S of Equation (2.4), defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\nf_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)=f(t)\\mathbf{x}_{t}+\\frac{g^{2}(t)}{2\\sigma_{t}}\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then $\\boldsymbol{f}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)$ describes a neural ODE which admits an adjoint state, $\\mathbf{a}_{\\mathbf{x}}:=\\partial\\mathcal{L}/\\partial\\mathbf{x}_{t}$ (and likewise for ${\\bf a}_{\\bf z}(t)$ and ${\\bf a}_{\\theta}(t))$ , which solve the continuous adjoint equations [20, Theorem 5.2] in the form of the following Initial Value Problem (IVP): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{a}_{\\mathbf{x}}(0)=\\displaystyle\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{x}_{0}},}\\\\ &{\\mathbf{a}_{\\mathbf{z}}(0)=\\mathbf{0},\\qquad\\qquad\\qquad\\qquad\\frac{\\mathrm{d}\\mathbf{a}_{\\mathbf{x}}}{\\mathrm{d}t}(t)=-\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial f_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\theta},}\\\\ &{\\mathbf{a}_{\\theta}(0)=\\mathbf{0},\\qquad\\qquad\\qquad\\qquad\\frac{\\mathrm{d}\\mathbf{a}_{\\mathbf{z}}}{\\mathrm{d}t}(t)=-\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial f_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{z}},}\\\\ &{\\mathbf{a}_{\\theta}(0)=\\mathbf{0},\\qquad\\qquad\\qquad\\qquad\\frac{\\mathrm{d}\\mathbf{a}_{\\theta}}{\\mathrm{d}t}(t)=-\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial f_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\theta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We refer to this system of equations in Equation (3.3) as the adjoint diffusion $O D E^{1}$ as it describes the continuous adjoint equations for the empirical probability flow ODE. ", "page_idx": 3}, {"type": "text", "text": "N.B., in the literature of diffusion models the sampling process is often done in reverse-time, i.e., the initial noise is $\\mathbf{x}_{T}$ and the final sample is $\\mathbf{x}_{\\mathrm{0}}$ . Due to this convention solving the adjoint diffusion ODE backwards actually means integrating forwards in time. Thus while diffusion models learn to compute $\\mathbf{x}_{t}$ from $\\mathbf{x}_{s}$ with $s>t$ , the adjoint diffusion ODE seeks to compute $\\mathbf{a}_{\\mathbf{x}}(s)$ from ${\\bf a}_{\\bf x}(t)$ . ", "page_idx": 3}, {"type": "text", "text": "3.1 Simplified Formulation of the Empirical Adjoint Probability Flow ODE ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We show that rather than treating $\\scriptstyle f_{\\theta}$ as a black box, the specific structure of the probability flow ODE is carried over to the adjoint probability flow ODE, allowing the adjoint probability flow ODE to be simplified into a special exact formulation. ", "page_idx": 3}, {"type": "text", "text": "By evaluating the gradient of $\\scriptstyle f_{\\theta}$ w.r.t. $\\mathbf{x}_{t}$ for each term in Equation (3.2) we can rewrite the adjoint diffusion ODE for ${\\bf a}_{\\bf x}(t)$ in Equation (3.3) as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{a}_{\\mathbf{x}}}{\\mathrm{d}t}(t)=-f(t)\\mathbf{a}_{\\mathbf{x}}(t)-\\frac{g^{2}(t)}{2\\sigma_{t}}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{x}_{t}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Due to the gradient of the drift term in Equation (3.4), further manipulations are required to put the empirical adjoint probability flow ODE into a sufficiently \u201cnice\u201d form. We follow the approach used by [16, 18] to simplify the empirical probability flow ODE with the use of exponential integrators and a change of variables. By applying the integrating factor $\\begin{array}{r}{\\exp\\left(\\int_{0}^{t}f(\\tau)\\;\\mathrm{d}\\tau\\right)}\\end{array}$ to Equation (3.4), the following ODE is expressed as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\biggl[e^{\\int_{0}^{t}f(\\tau)\\;\\mathrm{d}\\tau}\\mathbf{a}_{\\mathbf{x}}(t)\\biggr]=-e^{\\int_{0}^{t}f(\\tau)\\;\\mathrm{d}\\tau}\\frac{g^{2}(t)}{2\\sigma_{t}}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{x}_{t}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then, the exact solution at time $s$ given time $t<s$ is found to be ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{a}_{\\mathbf{x}}(s)=\\underbrace{e^{\\int_{s}^{t}f(\\tau)\\;\\mathrm{d}\\tau}\\mathbf{a}_{\\mathbf{x}}(t)}_{\\mathrm{linear}}-\\underbrace{\\int_{t}^{s}e^{\\int_{s}^{u}f(\\tau)\\;\\mathrm{d}\\tau}\\frac{g^{2}(u)}{2\\sigma_{u}}\\mathbf{a}_{\\mathbf{x}}(u)^{\\top}\\frac{\\epsilon_{\\theta}(\\mathbf{x}_{u},\\mathbf{z},u)}{\\partial\\mathbf{x}_{u}}\\;\\mathrm{d}u}_{\\mathrm{non.linear}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Like with solvers for diffusion models which leverage exponential integrators, we are able to transform the adjoint diffusion ODE into a non-stiff form by separating the linear and non-linear component. Moreover, we can compute the linear in closed form, thereby eliminating the discretization error in the linear term. However, we still need to approximate the non-linear term which consists of a difficult integral about our complex neural network. This is where the insight of Lu et al. [16] to integrate in the log-SNR domain becomes invaluable. Let $\\lambda_{t}:=\\log(\\alpha_{t}/\\sigma_{t})$ be one half of the log-SNR. Then, with using this new variable and computing the drift and diffusion coefficients in closed form, we can rewrite Equation (3.6) as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{a}_{\\mathbf{x}}(s)=\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{a}_{\\mathbf{x}}(t)+\\frac{1}{\\alpha_{s}}\\int_{t}^{s}\\alpha_{u}\\sigma_{u}\\frac{\\mathrm{d}\\lambda_{u}}{\\mathrm{d}u}\\mathbf{a}_{\\mathbf{x}}(u)^{\\top}\\frac{\\epsilon_{\\theta}(\\mathbf{x}_{u},\\mathbf{z},u)}{\\partial\\mathbf{x}_{u}}~\\mathrm{d}u.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As $\\lambda_{t}$ is a strictly decreasing function w.r.t. $t$ and therefore it has an inverse function $t_{\\lambda}$ which satisfies $t_{\\lambda}(\\lambda_{t})=t$ , and, with abuse of notation, we let $\\mathbf{x}_{\\lambda}:=\\mathbf{x}_{t_{\\lambda}(\\lambda)}$ , ${\\bf a}_{\\bf x}(\\lambda):={\\bf a}_{\\bf x}(t_{\\lambda}(\\lambda))$ , &c and let the reader infer from context if the function is mapping the log-SNR back into the time domain or already in the time domain. Then by rewriting Equation (3.7) as an exponentially weighted integral and performing an analogous derivation for $\\mathbf{a}_{\\mathbf{z}}(t)$ and ${\\bf a}_{\\theta}(t)$ , we arrive at: ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.1 (Exact solution of adjoint diffusion ODEs). Given initial values $[\\mathbf{a}_{\\mathbf{x}}(t),\\mathbf{a}_{\\mathbf{z}}(t),\\mathbf{a}_{\\theta}(t)]$ at time $t\\,\\in\\,(0,T)$ , the solution $[\\mathbf{a}_{\\mathbf{x}}(s),\\mathbf{a}_{\\mathbf{z}}(s),\\mathbf{a}_{\\theta}(s)]$ at time $s\\in(t,T]$ of adjoint diffusion $O D E s$ in Equation (3.3) is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{{\\bf a}_{{\\bf x}}(s)=\\frac{\\alpha_{t}}{\\alpha_{s}}{\\bf a}_{{\\bf x}}(t)+\\frac{1}{\\alpha_{s}}\\int_{\\lambda_{t}}^{\\lambda_{s}}{\\alpha_{\\lambda}^{2}e^{-\\lambda}{\\bf a}_{{\\bf x}}(\\lambda)^{\\top}\\frac{{\\bf\\epsilon}_{\\theta}({\\bf x}_{\\lambda},{\\bf z},\\lambda)}{\\partial{\\bf x}_{\\lambda}}\\mathrm{~d}\\lambda},}}\\\\ {\\displaystyle{{\\bf a}_{{\\bf z}}(s)={\\bf a}_{{\\bf z}}(t)+\\int_{\\lambda_{t}}^{\\lambda_{s}}{\\alpha_{\\lambda}e^{-\\lambda}{\\bf a}_{{\\bf x}}(\\lambda)^{\\top}\\frac{{\\partial}\\epsilon_{\\theta}({\\bf x}_{\\lambda},{\\bf z},\\lambda)}{{\\partial}{\\bf z}}\\mathrm{~d}\\lambda},}}\\\\ {\\displaystyle{{\\bf a}_{\\theta}(s)={\\bf a}_{\\theta}(t)+\\int_{\\lambda_{t}}^{\\lambda_{s}}{\\alpha_{\\lambda}e^{-\\lambda}{\\bf a}_{{\\bf x}}(\\lambda)^{\\top}\\frac{{\\partial}\\epsilon_{\\theta}({\\bf x}_{\\lambda},{\\bf z},\\lambda)}{{\\partial}\\theta}\\mathrm{~d}\\lambda}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The full derivations of Proposition 3.1 can be found in Appendix A.1. ", "page_idx": 4}, {"type": "text", "text": "There is a nice symmetry between Equations (3.8) to (3.10), while the adjoint of the solution trajectories evolves with a weighting of $\\alpha_{t}/\\alpha_{s}$ in the linear term and the integral term is weighted by $\\alpha_{t}^{2}/\\alpha_{s}^{2}$ reflecting the double partial $\\partial\\mathbf{x}_{t}$ in the adjoint and Jacobian terms. Conversely, the adjoint state for the conditional information and model parameters evolves with no weighting on the linear term and the integral is only weighted by $\\alpha_{t}/\\alpha_{s}$ . This follows from the vector fields being independent of $\\mathbf{a_{z}}$ and $\\mathbf{a}_{\\theta}$ . These equations while reflecting the special nature of this formulation of diffusion models also have an appealing parallel with the exact solution for diffusion ODEs Lu et al. [16, Proposition 3.1]. ", "page_idx": 4}, {"type": "text", "text": "3.2 Numerical Solvers for AdjointDEIS ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The numerical solver for the adjoint empirical probability flow ODE, now in light of Equation (3.8), only needs to focus on approximating the exponentially weighted integral of $\\epsilon_{\\theta}$ from $\\lambda_{t}$ to $\\lambda_{s}$ , a well-studied problem in the literature on exponential integrators [26, 27]. To approximate this integral, we evaluate the Taylor expansion of the vector Jacobian product to further simplify the ODE. For $k\\geq1$ , the $(k-1)$ -th Taylor expansion at $\\lambda_{t}$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{a}_{\\mathbf{x}}(\\lambda)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{\\lambda},\\mathbf{z},\\lambda)}{\\partial\\mathbf{x}_{\\lambda}}=\\sum_{n=0}^{k-1}\\frac{(\\lambda-\\lambda_{t})^{n}}{n!}\\frac{\\mathrm{d}^{n}}{\\mathrm{d}\\lambda^{n}}\\bigg[\\alpha_{\\lambda}^{2}\\mathbf{a}_{\\mathbf{x}}(\\lambda)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{\\lambda},\\mathbf{z},\\lambda)}{\\partial\\mathbf{x}_{\\lambda}}\\bigg]_{\\lambda=\\lambda_{t}}+\\mathcal{O}((\\lambda-\\lambda_{t})^{k}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Plugging this expansion into Equation (3.8) and letting $h=\\lambda_{s}-\\lambda_{t}$ yield ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{a_{x}}(s)=\\displaystyle\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{a_{x}}(t)}&{~+\\displaystyle\\frac{1}{\\alpha_{s}}\\sum_{n=0}^{k-1}\\displaystyle\\frac{\\mathrm{d}^{n}}{\\Omega\\!\\!\\!\\!\\!\\slash\\left[\\alpha_{\\lambda}^{2}\\mathbf{a_{x}}(\\lambda)^{\\top}\\displaystyle\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{\\lambda},\\mathbf{z},\\lambda)}{\\partial\\mathbf{x}_{\\lambda}}\\right]_{\\lambda=\\lambda_{t}}}\\underbrace{\\int_{\\lambda_{t}}^{\\lambda_{s}}\\frac{(\\lambda-\\lambda_{t})^{n}}{n!}e^{-\\lambda}\\,\\mathrm{d}\\lambda}_{\\mathrm{andyically\\;computed}}}\\\\ &{~+\\underbrace{\\mathcal{O}(h^{k+1})}_{\\mathrm{onited}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "With this expansion, the number of terms which need to be estimated is further reduced as the exponentially weighted integral $\\begin{array}{r}{\\int_{\\lambda_{t}}^{\\lambda_{s}}\\frac{(\\lambda-\\lambda_{t})^{n}}{n!}e^{-\\lambda}}\\end{array}$ d\u03bb can be solved analytically by applying $n$ times integration-by-parts [16, 28]. Therefore, the only errors in solving this ODE occur in the approximation of the $n$ -th order total derivatives of the vector-Jacobian product and the higher-order error terms $O(h^{k+1})$ . By dropping the ${\\mathcal{O}}(h^{k+1})$ error term and approximating the first $(k-1)$ -th derivatives of the vector-Jacobian product, we can derive $k$ -th order solvers for adjoint diffusion ODEs. We decide to name such solvers as Adjoint Diffusion Exponential Integrator Sampler (AdjointDEIS) reflecting our use of the exponential integrator to simplify the ODEs and pay homage to DEIS from [18] that explored the use of exponential integrators for diffusion ODEs. Consider the case of $k=1$ , by dropping the error term ${\\mathcal{O}}(h^{2})$ we construct the AdjointDEIS-1 solver with the following algorithm. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "AdjointDEIS-1. Given an initial augmented adjoint state $[\\mathbf{a}_{\\mathbf{x}}(t),\\mathbf{a}_{\\mathbf{z}}(t),\\mathbf{a}_{\\theta}(t)]$ at time $t\\in(0,T)$ , the solution $[\\mathbf{a}_{\\mathbf{x}}(s),\\mathbf{a}_{\\mathbf{z}}(s),\\mathbf{a}_{\\theta}(s)]$ at time $s\\in(t,T]$ is approximated by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{a}_{\\mathbf{x}}(s)=\\displaystyle\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{a}_{\\mathbf{x}}(t)+\\sigma_{s}(e^{h}-1)\\frac{\\alpha_{t}^{2}}{\\alpha_{s}^{2}}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{x}_{t}},}\\\\ &{\\mathbf{a}_{\\mathbf{z}}(s)=\\mathbf{a}_{\\mathbf{z}}(t)+\\sigma_{s}(e^{h}-1)\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{z}},}\\\\ &{\\mathbf{a}_{\\theta}(s)=\\mathbf{a}_{\\theta}(t)+\\sigma_{s}(e^{h}-1)\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\theta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "higher-order expansions of Equation (3.11) require estimations of the $n$ -th order derivatives of the vector Jacobian product which can be approximated via multi-step methods, such as Adams-Bashforth methods [29]. This has the added benefit of reduced computational overhead, as the multi-step method just reuses previous values to approximate the higher-order derivatives. Moreover, multi-steps are empirically more efficient than single-step methods [29]. Combining the Taylor expansions in Equation (3.11) with techniques for designing multi-step solvers, we propose a novel multi-step second-order solver for the adjoint empirical probability flow ODE which we call AdjointDEIS-2M. This algorithm combines the previous values of the vector Jacobian product at time $t$ and time $r$ to predict ${\\bf a}_{s}$ without any additional intermediate values. ", "page_idx": 5}, {"type": "text", "text": "AdjointDEIS-2M. We assume having a previous solution $\\mathbf{a}_{\\mathbf{x}}(r)$ and model output $\\epsilon_{\\theta}(\\mathbf{x}_{r},\\mathbf{z},r)$ at time $r<t<s$ , let $\\rho$ denote $\\begin{array}{r}{\\rho=\\frac{\\lambda_{t}-\\lambda_{r}}{h}}\\end{array}$ . Then the solution ${\\bf a}_{s}$ at time $s$ to Equation (3.4) is estimated to be ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf a}_{\\bf x}(s)=\\frac{\\alpha_{t}}{\\alpha_{s}}{\\bf a}_{\\bf x}(t)+\\sigma_{s}(e^{h}-1)\\frac{\\alpha_{t}^{2}}{\\alpha_{s}^{2}}{\\bf a}_{\\bf x}(t)^{\\top}\\frac{\\partial\\epsilon({\\bf x}_{t},{\\bf z},t)}{\\partial{\\bf x}_{t}}}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~+\\sigma_{s}\\frac{e^{h}-1}{2\\rho}\\bigg(\\frac{\\alpha_{t}^{2}}{\\alpha_{s}^{2}}{\\bf a}_{\\bf x}(t)^{\\top}\\frac{\\partial\\epsilon({\\bf x}_{t},{\\bf z},t)}{\\partial{\\bf x}_{t}}-\\frac{\\alpha_{r}^{2}}{\\alpha_{s}^{2}}{\\bf a}_{\\bf x}(r)^{\\top}\\frac{\\partial\\epsilon({\\bf x}_{r},{\\bf z},r)}{\\partial{\\bf x}_{r}}\\bigg).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For brevity, we omit the details of the AdjointDEIS-2M solver for ${\\bf a}_{\\bf z}(t)$ and ${\\bf a}_{\\theta}(t)$ ; rather, we provide the full derivation and details in Appendix A. Likewise, the full algorithm can be found in Appendix F.1. The advantage of a higher-order solver is that it is generally more efficient, requiring fewer steps due to its higher convergence order. We show that AdjointDEIS- $k$ is a $k$ -th order solver, as stated in the following theorem. The proof is in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1 (AdjointDEIS- ${\\cdot k}$ as a $k$ -th order solver). Assume the function $\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)$ and its associated vector-Jacobian products follow the regularity conditions detailed in Appendix $B$ , then for $k=1,2$ , AdjointDEIS- $k$ is a $k$ -th order solver for adjoint diffusion ODEs, i.e., for the sequence $\\{\\widetilde{\\mathbf{a}}_{\\mathbf{x}}(t_{i})\\}_{i=1}^{M}$ computed by AdjointDEIS- $k$ , the global truncation error at time $T$ satisfies $\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{M})\\mathbf{\\Omega}-$ $\\mathbf{a}_{\\mathbf{x}}(T)=\\bar{\\mathcal{O}}(h_{m a x}^{2})$ , where $h_{m a x}=\\operatorname*{max}_{1\\leq j\\leq M}\\!\\left(\\lambda_{t_{i}}-\\lambda_{t_{i-1}}\\right)$ . Likewise, AdjointDEIS- $k$ is a $k$ -th order solver for the estimated gradients w.r.t. z and $\\theta$ . ", "page_idx": 5}, {"type": "text", "text": "As previous work has shown that higher-order solvers may be unsuitable for large guidance scales [16\u2013 18] we do explicitly construct or analyze any solvers for $k>2$ and leave such explorations for future study. ", "page_idx": 5}, {"type": "text", "text": "3.3 Scheduled Conditional Information ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Thus far, we have held the conditional information constant across time, i.e., the same text conditioning like a prompt \u201cfire dragon\u201d can be fed to the noise prediction network. In guided generation tasks it is not uncommon to take advantage of the iterative nature of diffusion models by scheduling the conditional information to exhibit different values at different timesteps. $E.g.$ , blending concepts by alternating between the prompt for \u201cfire dragon\u201d and the prompt for \u201cice dragon\u201d at each timestep to create a picture of a dragon with both the qualities of ice and fire\u2014a technique which has been popularized by Stable Diffusion [3]. ", "page_idx": 5}, {"type": "text", "text": "We show that converting a constant $\\mathbf{z}$ into a scheduled $\\mathbf{z}_{t}$ does not actually change the continuous adjoint equation for $\\mathbf{z}_{t}$ , meaning we can still apply the AdjointDEIS- $k$ solvers derived above to find $\\mathbf{a}_{\\mathbf{z}}(t)$ by simply replacing $\\mathbf{z}$ with $\\mathbf{z}_{t}$ . We state this observation more formally in the following theorem. The proof is in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Theorem 3.2 (Continuous adjoint equations for time-dependent conditional information). Suppose there exists a function $\\mathbf{z}:\\mathbb{R}\\rightarrow\\mathbb{R}^{z}$ which is continuously differentiable in $t$ and is the conditional input into a parameterized vector field $\\pmb{f}_{\\theta}:\\mathbb{R}^{d}\\times\\mathbb{R}^{z}\\stackrel{\\cdot}{\\times}\\mathbb{R}^{\\ast}\\rightarrow\\mathbb{R}^{d}$ be continuous in $t$ , uniformly Lipschitz in x, and continuously differentiable in x. Let $\\mathbf{x}:\\mathbb{R}\\to\\mathbb{R}^{d}$ be the unique solution for the ODE ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}=f_{\\theta}(\\mathbf{x}_{t},\\mathbf{z}_{t},t)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with initial condition $\\mathbf{x}_{\\mathrm{0}}$ . Then there exists a unique solution $\\mathbf{a_{z}}:\\mathbb{R}\\rightarrow\\mathbb{R}^{z}$ to the following $I V P$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{a}_{\\mathbf{z}}(0)=\\mathbf{0},\\qquad\\frac{\\mathrm{d}\\mathbf{a}_{\\mathbf{z}}}{\\mathrm{d}t}(t)=-\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial f_{\\theta}(\\mathbf{x}_{t},\\mathbf{z}_{t},t)}{\\partial\\mathbf{z}_{t}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "While motivated by the case of scheduled conditional information in guided generation with diffusion models, this result applies to neural ODEs more generally, which could open future research directions. ", "page_idx": 6}, {"type": "text", "text": "4 Adjoint Diffusion SDEs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As recent work [30, 31] has shown, diffusion SDEs have useful properties over probability flow ODEs for image manipulation and editing. In particular, it has been shown that probability flow ODEs are invariant in Nie et al. [31, Theorem 3.2] and that diffusion SDEs are contractive in Nie et al. [31, Theorem 3.1], i.e., any gap in the mismatched prior distributions $p_{t}(\\mathbf{x}_{t})$ and $\\tilde{p}_{t}(\\mathbf x_{t})$ for the true distribution $p_{t}$ and edited distribution $\\tilde{p}_{t}$ will remain between $p_{0}(\\mathbf{x}_{0})$ and $\\tilde{p}_{0}(\\mathbf{x}_{0})$ , whereas for diffusion SDEs the gap can be reduced between $\\tilde{p}_{t}(\\mathbf x_{t})$ and $p_{t}(\\mathbf{x}_{t})$ as $t$ tends towards 0. Motivated by this reasoning, we present a framework for solving the adjoint diffusion SDE using exponential integrators. ", "page_idx": 6}, {"type": "text", "text": "The diffusion SDE with noise prediction model is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=\\Big[f(t)\\mathbf{x}_{t}+\\frac{g^{2}(t)}{\\sigma_{t}}\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)\\Big]\\ \\mathrm{d}t+g(t)\\ \\mathrm{d}\\bar{\\mathbf{w}}_{t},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\,{\\mathrm{~\\hat{~}{~d}}}t^{\\bullet}$ is an infinitesimal negative timestep. Note how the drift term of the SDE looks remarkably similar to the probability flow ODE sans a missing factor of $1/2$ in front of the noise prediction model. This is due to differing manipulations of the forward Kolomogorov equations\u2014which describe the evolution of $p_{t}(\\mathbf{x}_{t})$ \u2014used by Anderson [32] to derive the reverse-time SDE and later by Song et al. [15] to derive the probability-flow ODE. This connection is very important as it enables one to simplify the AdjointDEIS solvers for the adjoint diffusion SDE. ", "page_idx": 6}, {"type": "text", "text": "We show that for the special case of Stratonovich SDEs 2 with a diffusion coefficient ${\\pmb g}(t)$ which does not depend on the process state $\\mathbf{x}_{t}$ , then the adjoint process has a unique strong solution that evolves with what is essentially an ODE. Intuitively, this tracks as the stochastic term $\\bar{\\pmb{g}^{}}(t)\\circ\\mathrm{d}\\mathbf{w}_{t}$ has nothing to do with $\\mathbf{x}_{t}$ . We state this observation somewhat informally in the following theorem. The proof can be found in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1. Let $\\pmb{f}:\\mathbb{R}^{d}\\times\\mathbb{R}\\rightarrow\\mathbb{R}^{d}$ be in $\\mathcal{C}_{b}^{\\infty,1}$ and $\\pmb{g}:\\mathbb{R}\\rightarrow\\mathbb{R}^{d\\times w}$ be in $\\mathcal{C}_{b}^{1}$ . Let $\\mathcal{L}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be $a$ scalar-valued differentiable function. Let $\\mathbf{w}_{t}:[0,T]\\rightarrow\\mathbb{R}^{w}$ be a $w$ -dimensional Wiener process. Let $\\mathbf x:[0,T]\\rightarrow\\mathbb R^{d}$ solve the Stratonovich SDE ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=\\pmb{f}(\\mathbf{x}_{t},t)\\;\\mathrm{d}t+\\pmb{g}(t)\\circ\\mathrm{d}\\mathbf{w}_{t},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with initial condition $\\mathbf{x}_{\\mathrm{0}}$ . Then the adjoint process $\\mathbf{a}_{\\mathbf{x}}(t):=\\partial\\mathcal{L}(\\mathbf{x}_{T})/\\partial\\mathbf{x}_{t}$ is a strong solution to the backwards-in-time $O D E$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname{d}\\!\\mathbf{a}_{\\mathbf{x}}(t)=-\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\mathbf{f}}{\\partial\\mathbf{x}_{t}}(\\mathbf{x}_{t},t)\\operatorname{d}\\!t.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This is a boon for us as diffusion models model use only a mere scalar diffusion coefficient, $g(t)$ . Therefore, the continuous adjoint equations for the diffusion SDE just simplify to an ODE. Not only that, but as mentioned before, the drift term of the diffusion SDE and probability flow ODE differ only by a factor of 2 in the term with the noise prediction network. As only the drift term of the diffusion SDE is used when constructing the continuous adjoint equations, it follows that the only difference between the continuous adjoint equations for the probability flow ODE and diffusion SDE is a factor of 2. Therefore, the exact solutions are given by: ", "page_idx": 7}, {"type": "text", "text": "Proposition 4.1 (Exact solution of adjoint diffusion SDEs). Given initial values $[\\mathbf{a}_{\\mathbf{x}}(t),\\mathbf{a}_{\\mathbf{z}}(t),\\mathbf{a}_{\\theta}(t)]$ at time $t\\in(0,T)$ , the solution $[\\mathbf{a}_{\\mathbf{x}}(s),\\mathbf{a}_{\\mathbf{z}}(s),\\mathbf{a}_{\\theta}(s)]$ at time $s\\in(t,T]$ of adjoint diffusion SDEs is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf a}_{{\\bf x}}(s)=\\frac{\\alpha_{t}}{\\alpha_{s}}{\\bf a}_{{\\bf x}}(t)+\\frac{2}{\\alpha_{s}}\\int_{\\lambda_{t}}^{\\lambda_{s}}{\\alpha_{\\lambda}^{2}e^{-\\lambda}{\\bf a}_{{\\bf x}}(\\lambda)^{\\top}\\frac{\\epsilon_{\\theta}({\\bf x}_{\\lambda},{\\bf z},\\lambda)}{\\partial{\\bf x}_{\\lambda}}\\,\\mathrm{d}\\lambda},}}\\\\ {{\\displaystyle{\\bf a}_{{\\bf z}}(s)={\\bf a}_{{\\bf z}}(t)+2\\int_{\\lambda_{t}}^{\\lambda_{s}}{\\alpha_{\\lambda}e^{-\\lambda}{\\bf a}_{{\\bf x}}(\\lambda)^{\\top}\\frac{\\partial\\epsilon_{\\theta}({\\bf x}_{\\lambda},{\\bf z},\\lambda)}{\\partial{\\bf z}}\\,\\mathrm{d}\\lambda},}}\\\\ {{\\displaystyle{\\bf a}_{\\theta}(s)={\\bf a}_{\\theta}(t)+2\\int_{\\lambda_{t}}^{\\lambda_{s}}{\\alpha_{\\lambda}e^{-\\lambda}{\\bf a}_{{\\bf x}}(\\lambda)^{\\top}\\frac{\\partial\\epsilon_{\\theta}({\\bf x}_{\\lambda},{\\bf z},\\lambda)}{\\partial\\theta}\\,\\mathrm{d}\\lambda}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark 4.1. While the adjoint diffusion SDEs evolve with an ODE the same cannot be said for the underlying state, $\\mathbf{x}_{t}$ . Rather this evolves with a backwards SDE (more details in Appendix $D$ ) which requires the same realization of the Wiener process used to sample the image as the one used in the backwards SDE. ", "page_idx": 7}, {"type": "text", "text": "4.1 Solving Backwards Diffusion SDEs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Lu et al. [17] propose the following first-order solver for diffusion SDEs ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{x}_{s}-2\\sigma_{t}(e^{h}-1)\\epsilon_{\\theta}(\\mathbf{x}_{s},s)+\\sigma_{t}\\sqrt{e^{2h}-1}\\epsilon_{s},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\pmb{\\epsilon}_{s}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . To solve the SDE backwards in time we follow the approach initially proposed by $\\mathrm{W}\\mathbf{u}$ and la Torre [33] and used by later works [31]. Given a particular realization of the Wiener process that admits $\\mathbf{x}_{t}\\ \\sim\\mathcal{N}(\\alpha_{t}\\mathbf{x}_{0}\\ \\mid\\ \\sigma_{t}^{2}\\mathbf{I})$ , then for two samples $\\mathbf{x}_{t}$ and $\\mathbf{x}_{s}$ the noise $\\epsilon_{s}$ can be calculated by rearranging Equation (4.6) to find ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\epsilon_{s}=\\frac{\\mathbf{x}_{t}-\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{x}_{s}+2\\sigma_{t}(e^{h}-1)\\epsilon_{\\theta}(\\mathbf{x}_{s},\\mathbf{z},s)}{\\sigma_{t}\\sqrt{e^{2h}-1}}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "With this the sequence $\\{\\epsilon_{t_{i}}\\}_{i=1}^{N}$ of added noises can be calculated which will exactly reconstruct the original input from the initial realization of the Wiener process. This technique is referred to as Cycle-SDE after the CycleDiffusion paper [33]. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To illustrate the efficacy of our technique, we examine the application of guided generation for the face morphing attack. The face morphing attack is a new emerging attack on Face Recognition (FR) systems. This attack works by creating a singular morphed face image $\\mathbf{x}_{0}^{(a b)}$ that shares biometric information with the two contributing faces x(0a) and $\\mathbf{x}_{0}^{(b)}$ [34\u201336]. A successfully created morphed face image can trigger a false accept with either of the two contributing identities in the targeted Face Recognition (FR) system, see Figure 2 for an illustration. Recent work in this space has explored the use of diffusion models to generate these powerful attacks [34, 37, 38]. All prior work on diffusionbased face morphing used a pre-trained diffusion autoencoder [39] trained on the FFHQ [40] dataset at a $256\\times256$ resolution. We illustrate the use of the AdjointDEIS solvers by modifying the Diffusion Morph (DiM) architecture proposed by Blasingame and Liu [34] to use the AdjointDEIS solvers to find the optimal initial noise $\\mathbf{x}_{T}^{(\\bar{a}b)}$ and conditional $\\mathbf{z}_{a b}$ . The AdjointDEIS solvers are used to calculate the gradients with respect to the identity loss [38] defined as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{I D}=d(v_{a b},v_{a})+d(v_{a b},v_{b}),\\quad\\mathcal{L}_{d i f f}=\\big|d(v_{a b},v_{a})-d(v_{a b},v_{b})\\big)\\big|,}\\\\ &{\\mathcal{L}_{I D}^{*}=\\mathcal{L}_{I D}+\\mathcal{L}_{d i f f},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "fAlcxvrOEX/tmp/f85a5d8991d01f7e9ab5fbc8ec551543ead78facab402a0d35025975db359682.jpg", "img_caption": ["Figure 2: Example of guided morphed face generation with AdjointDEIS on the FRLL dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "fAlcxvrOEX/tmp/14b559c589a71c7b3e140f08e0cd6cf5dc4029fa5c941b2fe118fe78195db69b.jpg", "img_caption": ["Figure 3: Comparison of DiM morphs on the FRLL dataset. From left to right, identity $a$ , DiM-A, Fast-DiM, Morph-PIPE, AdjointDEIS (ODE), AdjointDEIS (SDE), and identity $b$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "where $v_{a}\\,=\\,F(\\mathbf{x}_{0}^{(a)}),v_{b}\\,=\\,F(\\mathbf{x}_{0}^{(b)}),v_{a b}\\,=\\,F(\\mathbf{x}_{0}^{(a b)})$ , and $F:\\mathcal{X}\\rightarrow V$ is an FR system which embeds images into a vector space $V$ which is equipped with a measure of distance, $d$ . We used the ArcFace [41] FR system for the identity loss. ", "page_idx": 8}, {"type": "text", "text": "We compare against three preexisting DiM methods, the original DiM algorithm [34], Fast-DiM [37], and Morph-PIPE [38] as well as a GAN-inversion based face morphing attack, MIPGAN-I and MIPGAN-II [42] based on the StyleGAN [40] and StyleGAN2 [43] architectures respectively. FastDiM improves DiM by using high-order ODE solvers to decrease the number of sampling steps required to create a morph. Morph-PIPE performs a very simple version of guided generation by generating a large batch of morphed images derived from a discrete set of interpolations between $\\bar{\\mathbf{x}}_{T}^{(a)}$ and $\\bar{\\mathbf{x}}_{T}^{(b)}$ , and $\\mathbf{z}_{a}$ and $\\mathbf{z}_{b}$ . For reference purposes, we compare against a reference GAN-based method [42] which uses GAN-inversion w.r.t.to the identity loss to find the optimal morphed face, and we include prior state-of-the-art Webmorph, a commercial off-the-shelf system [44]. ", "page_idx": 8}, {"type": "text", "text": "We run our experiments on the SYN-MAD 2022 [44] morphed pairs which are constructed from the Face Research Lab London dataset [45], more details in Appendix G.4. The morphed images are evaluated against three FR systems, the ArcFace [41], ElasticFace [46], and AdaFace [47] models, further details are found in Appendix G.5. To measure the efficacy of a morphing attack, the Mated Morph Presentation Match Rate (MMPMR) metric [48] is used. The MMPMR metric as proposed by Scherhag et al. [48] is defined as ", "page_idx": 8}, {"type": "equation", "text": "$$\nM(\\delta)=\\frac{1}{M}\\sum_{m=1}^{M}\\left\\{\\left[\\operatorname*{min}_{n\\in\\{1,...,N_{m}\\}}S_{m}^{n}\\right]>\\delta\\right\\}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\delta$ is the verification threshold, $S_{m}^{n}$ is the similarity score of the $n$ -th subject of morph $m$ , $N_{m}$ is the total number of contributing subjects to morph $m$ , and $M$ is the total number of morphed images. ", "page_idx": 8}, {"type": "text", "text": "In our experiments, we used a learning rate of 0.01, $N\\,=\\,20$ sampling steps, $M=20$ steps for AdjointDEIS, and 50 optimization steps for gradient descent. ", "page_idx": 8}, {"type": "table", "img_path": "fAlcxvrOEX/tmp/47ae1519fc56bc585d83337ae2e14cf5256965b84960f90ec055e8e190ca09c6.jpg", "table_caption": ["Table 1: Vulnerability of different FR systems across different morphing attacks on the SYN-MAD 2022 dataset. $\\mathrm{FMR}=0.1\\%$ . "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "In Table 1 we present the effectiveness of the morphing attacks against the three FR systems. Guided generation with AdjointDEIS massively increases the performance of DiM, supplanting the old state-of-the-art for face morphing. Interestingly, the SDE variant did not fare as well as the ODE variant. This is likely due to the difficulty in discretizing SDEs with large step sizes [15\u201317]. We present further results in Appendix E which explore the impact of the choice of learning rate and number of discretization steps for AdjointDEIS. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present a unified view on guided generation by updating latent, conditional, and model information of diffusion models with a guidance function using the continuous adjoint equations. We propose AdjointDEIS, a family of solvers for the continuous adjoint equations of diffusion models. We exploit the unique construction of diffusion models to create efficient numerical solvers by using exponential integrators. We prove the convergence order of solvers and show that the continuous adjoint equations for diffusion SDEs evolve with an ODE. Furthermore, we show how to handle conditional information that is scheduled in time, further expanding the generalizability of the proposed technique. Our results in face morphing show that the gradients produced by AdjointDEIS can be used for guided generation tasks. ", "page_idx": 9}, {"type": "text", "text": "Limitations. There are several limitations. Empirically, we only explored a small subset of the true potential AdjointDEIS by evaluating on a single scenario, i.e., face morphing. Likewise, we only explored a few different hyperparameter options. In particular, we did not explore much the impact of the number of optimization steps and the number of sampling steps for diffusion SDEs on the visual quality of the generated face morphs. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact. Guided generation techniques can be misused for a variety of harmful purposes. In particular, our approach provides a powerful tool for adversarial attacks. However, better knowledge of such techniques should hopefully help direct research in hardening systems against such kinds of attacks. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 6840\u20136851. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf. 1, 3, 30   \n[2] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id $=$ St1giarCHLP. 1, 2, 3   \n[3] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF ", "page_idx": 9}, {"type": "text", "text": "Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684\u201310695, June 2022. 1, 6 ", "page_idx": 10}, {"type": "text", "text": "[4] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional Image Generation with CLIP Latents. arXiv e-prints, art. arXiv:2204.06125, April 2022. doi: 10.48550/arXiv.2204.06125.   \n[5] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-toimage diffusion models with deep language understanding. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 36479\u201336494. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf. 1 [6] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D. Plumbley. AudioLDM: Text-to-Audio Generation with Latent Diffusion Models. arXiv e-prints, art. arXiv:2301.12503, January 2023. doi: 10.48550/arXiv.2301.12503. 1   \n[7] Scott H. Hawley. Pictures of midi: Controlled music generation via graphical prompts for image-based diffusion inpainting, 2024. URL https://arxiv.org/abs/2407.01499. 1 [8] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. arXiv e-prints, art. arXiv:2304.08818, April 2023. doi: 10.48550/arXiv.2304. 08818. 1 [9] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Proc. NeurIPS, 2022. 1, 2   \n[10] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022. 1   \n[11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion. arXiv e-prints, art. arXiv:2208.01618, August 2022. doi: 10.48550/ arXiv.2208.01618.   \n[12] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 1   \n[13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. URL https://openreview. net/forum?id=qw8AKxfYbI. 1   \n[14] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal Guidance for Diffusion Models. arXiv e-prints, art. arXiv:2302.07121, February 2023. doi: 10.48550/arXiv.2302.07121. 1   \n[15] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=PxTIG12RRHS. 1, 2, 3, 7, 10, 30   \n[16] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan LI, and Jun Zhu. Dpmsolver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 5775\u20135787. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 260a14acce2a89dad36adc8eefe7c59e-Paper-Conference.pdf. 1, 3, 4, 5, 6, 17, 18, 19   \n[17] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models, 2023. 8, 10, 19   \n[18] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In International Conference on Learning Representations, 2023. 1, 3, 4, 5, 6   \n[19] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/ 2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf. 2, 3   \n[20] Patrick Kidger. On Neural Differential Equations. PhD thesis, Oxford University, 2022. 2, 3, 4   \n[21] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id $=$ TIdIXIpzhoI. 3   \n[22] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Curran Associates Inc., Red Hook, NY, USA, 2019. 3   \n[23] R. Abdal, Y. Qin, and P. Wonka. Image2stylegan: How to embed images into the stylegan latent space? In IEEE/CVF Int\u2019l Conf. on Comp. Vision (ICCV), pages 4431\u20134440, 2019. doi: 10.1109/ICCV.2019.00453. 3   \n[24] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan $^{++}$ : How to edit the embedded images? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8296\u20138305, 2020. 3   \n[25] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t. 3   \n[26] Marlis Hochbruck and Alexander Ostermann. Exponential integrators. Acta Numerica, 19: 209\u2013286, 2010. doi: 10.1017/S0962492910000048. 5, 17   \n[27] Iyabo Ann Adamu. Numerical approximation of SDEs & the stochastic Swift-Hohenberg equation. PhD thesis, Heriot-Watt University, 2011. 5   \n[28] Martin Gonzalez, Nelson Fernandez Pinto, Thuy Tran, elies Gherbi, Hatem Hajri, and Nader Masmoudi. Seeds: Exponential sde solvers for fast high-quality sampling from diffusion models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 68061\u201368120. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ d6f764aae383d9ff28a0f89f71defbd9-Paper-Conference.pdf. 5, 17   \n[29] K. Atkinson, W. Han, and D.E. Stewart. Numerical Solution of Ordinary Differential Equations. Pure and Applied Mathematics: A Wiley Series of Texts, Monographs and Tracts. Wiley, 2011. ISBN 9781118164525. URL https://books.google.com/books?id=QzjGgLlKCYQC. 6   \n[30] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. 7   \n[31] Shen Nie, Hanzhong Allan Guo, Cheng Lu, Yuhao Zhou, Chenyu Zheng, and Chongxuan Li. The blessing of randomness: SDE beats ODE in general diffusion-based image editing. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=DesYwmUG00. 7, 8   \n[32] Brian D.O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326, 1982. ISSN 0304-4149. doi: https://doi.org/10.1016/ 0304-4149(82)90051-5. URL https://www.sciencedirect.com/science/article/ pii/0304414982900515. 7   \n[33] Chen Henry Wu and Fernando De la Torre. A latent space of stochastic diffusion models for zero-shot image editing and guidance. In ICCV, 2023. 8   \n[34] Zander W. Blasingame and Chen Liu. Leveraging diffusion for strong and high quality face morphing attacks. IEEE Transactions on Biometrics, Behavior, and Identity Science, 6(1): 118\u2013131, 2024. doi: 10.1109/TBIOM.2024.3349857. 8, 9, 10, 27, 28   \n[35] R. Raghavendra, K. B. Raja, and C. Busch. Detecting morphed face images. In IEEE 8th Int\u2019l Conf. on Biometrics Theory, Applications and Systems (BTAS), pages 1\u20137, 2016. doi: 10.1109/BTAS.2016.7791169.   \n[36] Eklavya Sarkar, Pavel Korshunov, Laurent Colbois, and S\u00e9bastien Marcel. Are gan-based morphs threatening face recognition? In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 2959\u20132963, 2022. doi: 10.1109/ ICASSP43922.2022.9746477. 8, 28   \n[37] Zander W. Blasingame and Chen Liu. Fast-dim: Towards fast diffusion morphs. IEEE Security & Privacy, 22(4):103\u2013114, June 2024. doi: 10.1109/MSEC.2024.3410112. 8, 9, 10   \n[38] Haoyu Zhang, Raghavendra Ramachandra, Kiran Raja, and Busch Christoph. Morph-pipe: Plugging in identity prior to enhance face morphing attack based on diffusion model. In Norwegian Information Security Conference (NISK), 2023. 8, 9, 10, 28   \n[39] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders: Toward a meaningful and decodable representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10619\u201310629, June 2022. 8   \n[40] T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial networks. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4396\u20134405, 2019. doi: 10.1109/CVPR.2019.00453. 8, 9   \n[41] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4690\u20134699, 2019. 9, 28   \n[42] Haoyu Zhang, Sushma Venkatesh, Raghavendra Ramachandra, Kiran Raja, Naser Damer, and Christoph Busch. Mipgan\u2014generating strong and high quality morphing attacks using identity prior driven gan. IEEE Transactions on Biometrics, Behavior, and Identity Science, 3(3): 365\u2013383, 2021. doi: 10.1109/TBIOM.2021.3072349. 9, 10, 28   \n[43] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila. Analyzing and improving the image quality of stylegan. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8107\u20138116, 2020. doi: 10.1109/CVPR42600.2020.00813. 9   \n[44] Marco Huber, Fadi Boutros, Anh Thi Luu, Kiran Raja, Raghavendra Ramachandra, Naser Damer, Pedro C. Neto, Tiago Gon\u00e7alves, Ana F. Sequeira, Jaime S. Cardoso, Jo\u00e3o Tremo\u00e7o, Miguel Louren\u00e7o, Sergio Serra, Eduardo Cerme\u00f1o, Marija Ivanovska, Borut Batagelj, Andrej Kronov\u0161ek, Peter Peer, and Vitomir \u0160truc. Syn-mad 2022: Competition on face morphing attack detection based on privacy-aware synthetic training data. In 2022 IEEE International Joint Conference on Biometrics (IJCB), pages 1\u201310, 2022. doi: 10.1109/IJCB54206.2022.10007950. 9, 10, 28   \n[45] Lisa DeBruine and Benedict Jones. Face Research Lab London Set. 5 2017. doi: 10. $6084/\\mathrm{m}9$ .figshare.5047666.v5. URL https://figshare.com/articles/dataset/Face_ Research_Lab_London_Set/5047666. 9, 28   \n[46] Fadi Boutros, Naser Damer, Florian Kirchbuchner, and Arjan Kuijper. Elasticface: Elastic margin loss for deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 1578\u20131587, June 2022. 9, 28   \n[47] Minchul Kim, Anil K Jain, and Xiaoming Liu. Adaface: Quality adaptive margin for face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 9, 28   \n[48] Ulrich Scherhag, Andreas Nautsch, Christian Rathgeb, Marta Gomez-Barrero, Raymond N. J. Veldhuis, Luuk Spreeuwers, Maikel Schils, Davide Maltoni, Patrick Grother, Sebastien Marcel, Ralph Breithaupt, Raghavendra Ramachandra, and Christoph Busch. Biometric systems under morphing attacks: Assessment of morphing techniques and vulnerability reporting. In 2017 International Conference of the Biometrics Special Interest Group (BIOSIG), pages 1\u20137, 2017. doi: 10.23919/BIOSIG.2017.8053499. 9   \n[49] John Charles Butcher. Numerical methods for ordinary differential equations. John Wiley & Sons, 2016. 22   \n[50] Xuechen Li, Ting-Kam Leonard Wong, Ricky T. Q. Chen, and David Duvenaud. Scalable gradients for stochastic differential equations. In Silvia Chiappa and Roberto Calandra, editors, Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 3870\u20133882. PMLR, 26\u201328 Aug 2020. URL https://proceedings.mlr.press/v108/li20i.html. 22, 23   \n[51] Hiroshi Kunita. Stochastic differential equations and stochastic flows. Stochastic Flows and Jump-Diffusions, pages 77\u2013124, 2019. 22, 23   \n[52] Zander W. Blasingame and Chen Liu. Greedy-dim: Greedy algorithms for unreasonably effective face morphs. In 2024 IEEE International Joint Conference on Biometrics (IJCB), pages 1\u201310, September 2024. 27, 28, 29   \n[53] Ionut Cosmin Duta, Li Liu, Fan Zhu, and Ling Shao. Improved residual networks for image and video recognition. In 2020 25th International Conference on Pattern Recognition (ICPR), pages 9415\u20139422, 2021. doi: 10.1109/ICPR48806.2021.9412193. 28   \n[54] Xiang An, Xuhan Zhu, Yuan Gao, Yang Xiao, Yongle Zhao, Ziyong Feng, Lan Wu, Bin Qin, Ming Zhang, Debing Zhang, and Ying Fu. Partial fc: Training 10 million identities on a single machine. In 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), pages 1445\u20131449, 2021. doi: 10.1109/ICCVW54120.2021.00166. 28   \n[55] Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. Freedom: Trainingfree energy-guided conditional diffusion model. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 28, 29   \n[56] Xingchao Liu, Lemeng Wu, Shujian Zhang, Chengyue Gong, Wei Ping, and Qiang Liu. Flowgrad: Controlling the output of generative odes with gradients. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24335\u201324344, 2023. 28, 29   \n[57] Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil Naik. End-to-end diffusion latent optimization improves classifier guidance, 2023. 28, 29   \n[58] Jiachun Pan, Jun Hao Liew, Vincent Tan, Jiashi Feng, and Hanshu Yan. AdjointDPM: Adjoint sensitivity method for gradient backpropagation of diffusion probabilistic models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=y33lDRBgWI. 28, 29   \n[59] Pierre Marion, Anna Korba, Peter Bartlett, Mathieu Blondel, Valentin De Bortoli, Arnaud Doucet, Felipe Llinares-L\u00f3pez, Courtney Paquette, and Quentin Berthet. Implicit diffusion: Efficient optimization through stochastic sampling. arXiv preprint arXiv:2402.05468, 2024. 28, 30   \n[60] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22532\u201322541, 2023. 29 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Derivation of AdjointDEIS ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section we provide the full derivations for the family of AdjointDEIS solvers. First recall the full definition of the continuous adjoint equations for the empirical probability flow ODE: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{a}_{\\mathbf{x}}(0)=\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{x}_{0}},}\\\\ {\\displaystyle\\mathbf{a}_{\\mathbf{z}}(0)=\\mathbf{0},}\\\\ {\\displaystyle\\mathbf{a}_{\\theta}(0)=\\mathbf{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\mathrm{d}\\mathbf{a}_{\\mathbf{x}}}{\\mathrm{d}t}(t)=-\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial{\\{{\\mathbf{f}}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{x}_{t}}}}{\\partial\\mathbf{x}_{t}},}\\\\ {\\frac{\\mathrm{d}\\mathbf{a}_{\\mathbf{z}}}{\\mathrm{d}t}(t)=-\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial{{\\mathbf{f}}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}}{\\partial\\mathbf{z}},}\\\\ {\\frac{\\mathrm{d}\\mathbf{a}_{\\theta}}{\\mathrm{d}t}(t)=-\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial{{\\mathbf{f}}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}}{\\partial\\theta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We can simplify the equations by explicitly solving gradients of the neural vector field $\\scriptstyle f_{\\theta}$ for the drift term to obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\frac{\\mathrm{d}\\mathbf{a}_{\\mathbf{x}}}{\\mathrm{d}t}(t)=-f(t)\\mathbf{a}_{\\mathbf{x}}(t)-\\frac{g^{2}(t)}{2\\sigma_{t}}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{x}_{t}},}&\\\\ &{\\frac{\\mathrm{d}\\mathbf{a}_{\\mathbf{z}}}{\\mathrm{d}t}(t)=}&{\\mathbf{0}-\\frac{g^{2}(t)}{2\\sigma_{t}}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{z}},}&\\\\ &{\\frac{\\mathrm{d}\\mathbf{a}_{\\theta}}{\\mathrm{d}t}(t)=}&{\\mathbf{0}-\\frac{g^{2}(t)}{2\\sigma_{t}}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\theta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Remark A.1. The last two equations in Equations (A.2) the vector fields are independent of $\\left(\\mathbf{a}_{\\mathbf{z}},\\mathbf{a}_{\\theta}\\right)$ , reducing these equations to mere integrals; however, it is often useful to compute the whole system ${\\bf a}_{a u g}=({\\bf a}_{\\bf x},{\\bf a}_{\\bf z},{\\bf a}_{\\theta})$ as an augmented $O D E$ . ", "page_idx": 14}, {"type": "text", "text": "Remark A.2. Likewise, the last two equations in Equations (A.2) are functionally identical with a simple swap of z for $\\theta$ or vice versa. ", "page_idx": 14}, {"type": "text", "text": "As such, for the sake of brevity, the derivations for the AdjointDEIS solvers for $(\\mathbf{a}_{\\mathbf{z}},\\mathbf{a}_{\\theta})$ will only explicitly include the derivations for $\\mathbf{a_{z}}$ . ", "page_idx": 14}, {"type": "text", "text": "A.1 Simplified Formulation of the Continuous Adjoint Equations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Focusing first on the continuous adjoint equation for $\\mathbf{a_{x}}$ we apply the integrating factor $\\begin{array}{r l}{\\exp\\big(\\int_{0}^{t^{\\phantom{\\dagger}}}\\!f(\\tau)\\;\\mathrm{d}\\tau\\big)}\\end{array}$ to Equation (A.2) to find ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\biggl[e^{\\int_{0}^{t}f(\\tau)\\;\\mathrm{d}\\tau}\\mathbf{a}_{\\mathbf{x}}(t)\\biggr]=-e^{\\int_{0}^{t}f(\\tau)\\;\\mathrm{d}\\tau}\\frac{g^{2}(t)}{2\\sigma_{t}}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{x}_{t}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, the exact solution at time $s$ given time $t<s$ is found to be ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{e^{\\int_{0}^{s}f(\\tau)\\;\\mathrm{d}\\tau}\\mathbf{a}_{\\mathbf{x}}(s)=e^{\\int_{0}^{t}f(\\tau)\\;\\mathrm{d}\\tau}\\mathbf{a}_{\\mathbf{x}}(t)-\\int_{t}^{s}e^{\\int_{0}^{u}f(\\tau)\\;\\mathrm{d}\\tau}\\frac{g^{2}\\left(u\\right)}{2\\sigma_{u}}\\mathbf{a}_{\\mathbf{x}}(u)^{\\top}\\frac{\\epsilon_{\\theta}\\left(\\mathbf{x}_{u},\\mathbf{z},u\\right)}{\\partial\\mathbf{x}_{u}}\\;\\mathrm{d}u}\\\\ {\\mathbf{a}_{\\mathbf{x}}(s)=e^{\\int_{s}^{t}f(\\tau)\\;\\mathrm{d}\\tau}\\mathbf{a}_{\\mathbf{x}}(t)-\\int_{t}^{s}e^{\\int_{s}^{u}f(\\tau)\\;\\mathrm{d}\\tau}\\frac{g^{2}\\left(u\\right)}{2\\sigma_{u}}\\mathbf{a}_{\\mathbf{x}}(u)^{\\top}\\frac{\\epsilon_{\\theta}\\left(\\mathbf{x}_{u},\\mathbf{z},u\\right)}{\\partial\\mathbf{x}_{u}}\\;\\mathrm{d}u}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To simplify Equation (A.4), recall that $f(t)$ is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(t)={\\frac{\\mathrm{d}\\log\\alpha_{t}}{\\mathrm{d}t}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for VP type SDEs. Furthermore, let $\\lambda_{t}:=\\log(\\alpha_{t}/\\sigma_{t})$ be one half of the log-SNR. Then the diffusion coefficient can be simplified using the log-derivative trick such that ", "page_idx": 14}, {"type": "equation", "text": "$$\ng^{2}(t)=\\frac{\\mathrm{d}\\sigma_{t}^{2}}{\\mathrm{d}t}-2\\frac{\\mathrm{d}\\log\\alpha_{t}}{\\mathrm{d}t}\\sigma_{t}^{2}=2\\sigma_{t}^{2}\\bigg(\\frac{\\mathrm{d}\\log\\sigma_{t}}{\\mathrm{d}t}-\\frac{\\mathrm{d}\\log\\alpha_{t}}{\\mathrm{d}t}\\bigg)=-2\\sigma_{t}^{2}\\frac{\\mathrm{d}\\lambda_{t}}{\\mathrm{d}t}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using this updated expression of $g^{2}(t)$ along with computing the integrating factor in closed form enables us to express Equation (A.4) as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{a}_{\\mathbf{x}}(s)=\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{a}_{\\mathbf{x}}(t)+\\frac{1}{\\alpha_{s}}\\int_{t}^{s}\\alpha_{u}\\sigma_{u}\\frac{\\mathrm{d}\\lambda_{u}}{\\mathrm{d}u}\\mathbf{a}_{\\mathbf{x}}(u)^{\\top}\\frac{\\epsilon_{\\theta}(\\mathbf{x}_{u},\\mathbf{z},u)}{\\partial\\mathbf{x}_{u}}~\\mathrm{d}u.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lastly, by rewriting the integral in terms of an exponentially weighted integral $\\alpha_{u}\\sigma_{u}=\\alpha_{u}^{2}\\sigma_{u}/\\alpha_{u}=$ $\\alpha_{u}^{2}e^{-\\lambda_{u}}$ we find ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{a}_{\\mathbf{x}}(s)=\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{a}_{\\mathbf{x}}(t)+\\frac{1}{\\alpha_{s}}\\int_{\\lambda_{t}}^{\\lambda_{s}}\\alpha_{\\lambda}^{2}e^{-\\lambda}\\mathbf{a}_{\\mathbf{x}}(\\lambda)^{\\top}\\frac{\\epsilon_{\\theta}(\\mathbf{x}_{\\lambda},\\mathbf{z},\\lambda)}{\\partial\\mathbf{x}_{\\lambda}}\\;\\mathrm{d}\\lambda.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This change of variables is possible as $\\lambda_{t}$ is a strictly decreasing function w.r.t. $t$ and therefore it has an inverse function $t_{\\lambda}$ which satisfies $t_{\\lambda}(\\lambda_{t})=t$ , and, with abuse of notation, we let $\\mathbf{x}_{\\lambda}:=\\mathbf{x}_{t_{\\lambda}(\\lambda)}$ , $\\mathbf{a}_{\\mathbf{x}}(\\lambda):=\\mathbf{a}_{\\mathbf{x}}(t_{\\lambda}(\\lambda))$ , &c and let the reader infer from context if the function is mapping the log-SNR back into the time domain or already in the time domain. ", "page_idx": 15}, {"type": "text", "text": "Now we will show the derivations to find a simplified form of the continuous adjoint equation for the conditional information. Using the continuous adjoint equation from Equations (A.2) for ${\\bf a}_{\\bf z}(t)$ along with the log-SNR, we can express the evolution of $\\mathbf{a}_{\\mathbf{z}}(t)$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{a}_{\\mathbf{z}}}{\\mathrm{d}t}(t)=\\sigma_{t}\\frac{\\mathrm{d}\\lambda_{t}}{\\mathrm{d}t}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{z}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As we would like to express this as an exponential integrator, we simply multiply $\\sigma_{t}$ by $\\alpha_{t}/\\alpha_{t}$ to obtain $\\alpha_{t}\\cdot\\sigma_{t}/\\alpha_{t}=\\alpha_{t}e^{-\\lambda_{t}}$ , as such we can rewrite Equation (A.9) as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{a}_{\\mathbf{z}}}{\\mathrm{d}t}(t)=\\alpha_{t}e^{-\\lambda_{t}}\\frac{\\mathrm{d}\\lambda_{t}}{\\mathrm{d}t}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{z}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using Equations (A.8) and (A.10), we arrive at Proposition 3.1 from the main paper. ", "page_idx": 15}, {"type": "text", "text": "Proposition A.1. Given initial values $[\\mathbf{a}_{\\mathbf{x}}(t),\\mathbf{a}_{\\mathbf{z}}(t),\\mathbf{a}_{\\theta}(t)]$ at time $t\\ \\in\\ (0,T)$ , the solution $[\\mathbf{a}_{\\mathbf{x}}(s),\\mathbf{a}_{\\mathbf{z}}(s),\\mathbf{a}_{\\theta}(s)]$ at time $s\\;\\in\\;(t,T]$ of the adjoint empirical probability flow ODE in Equation (A.4) is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{a}_{\\mathbf{x}}(s)=\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{a}_{\\mathbf{x}}(t)+\\frac{1}{\\alpha_{s}}\\int_{\\lambda_{t}}^{\\lambda_{s}}{\\alpha_{\\lambda}^{2}e^{-\\lambda}\\mathbf{a}_{\\mathbf{x}}(\\lambda)^{\\top}\\frac{\\epsilon_{\\theta}(\\mathbf{x}_{\\lambda},\\mathbf{z},\\lambda)}{\\partial\\mathbf{x}_{\\lambda}}\\;\\mathrm{d}\\lambda},}\\\\ {\\displaystyle\\mathbf{a}_{\\mathbf{z}}(s)=\\mathbf{a}_{\\mathbf{z}}(t)+\\int_{\\lambda_{t}}^{\\lambda_{s}}{\\alpha_{\\lambda}e^{-\\lambda}\\mathbf{a}_{\\mathbf{x}}(\\lambda)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{\\lambda},\\mathbf{z},\\lambda)}{\\partial\\mathbf{z}}\\;\\mathrm{d}\\lambda},}\\\\ {\\displaystyle\\mathbf{a}_{\\theta}(s)=\\mathbf{a}_{\\theta}(t)+\\int_{\\lambda_{t}}^{\\lambda_{s}}{\\alpha_{\\lambda}e^{-\\lambda}\\mathbf{a}_{\\mathbf{x}}(\\lambda)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{\\lambda},\\mathbf{z},\\lambda)}{\\partial\\theta}\\;\\mathrm{d}\\lambda}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then to find the AdjointDEIS solvers we take a $k$ -th order Taylor expansion about $\\lambda_{t}$ and integrate in the log-SNR domain. ", "page_idx": 15}, {"type": "text", "text": "A.2 Taylor Expansion ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For $k\\geq1$ , the $(k-1)$ -th Taylor expansion at $\\lambda_{t}$ of the inner term of the exponentially weighted integral in Equation (A.11) is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{a}_{\\mathbf{x}}(\\lambda)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{\\lambda},\\mathbf{z},\\lambda)}{\\partial\\mathbf{x}_{\\lambda}}=\\sum_{n=0}^{k-1}\\frac{(\\lambda-\\lambda_{t})^{n}}{n!}\\frac{\\mathrm{d}^{n}}{\\mathrm{d}\\lambda^{n}}\\bigg[\\alpha_{\\lambda}^{2}\\mathbf{a}_{\\mathbf{x}}(\\lambda)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{\\lambda},\\mathbf{z},\\lambda)}{\\partial\\mathbf{x}_{\\lambda}}\\bigg]_{\\lambda=\\lambda_{t}}+\\mathcal{O}((\\lambda-\\lambda_{t})^{k}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then plugging this into Equation (A.11) yields ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{a}_{\\mathbf{x}}(s)=\\displaystyle\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{a}_{\\mathbf{x}}(t)}&{~+\\displaystyle\\frac{1}{\\alpha_{s}}\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda}\\sum_{n=0}^{k-1}\\frac{(\\lambda-\\lambda_{t})^{n}}{n!}\\frac{\\mathrm{d}^{n}}{\\mathrm{d}\\lambda^{n}}\\bigg[\\alpha_{\\lambda}^{2}\\mathbf{a}_{\\mathbf{x}}(\\lambda)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{\\lambda},\\mathbf{z},\\lambda)}{\\partial\\mathbf{x}_{\\lambda}}\\bigg]_{\\lambda=\\lambda_{t}}\\,\\mathrm{d}\\lambda}\\\\ &{~+\\mathcal{O}(h^{k+1})}\\\\ {=\\displaystyle\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{a}_{\\mathbf{x}}(t)}&{~+\\displaystyle\\frac{1}{\\alpha_{s}}\\sum_{n=0}^{k-1}\\frac{\\mathrm{d}^{n}}{\\mathrm{d}\\lambda^{n}}\\bigg[\\alpha_{\\lambda}^{2}\\mathbf{a}_{\\mathbf{x}}(\\lambda)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{\\lambda},\\mathbf{z},\\lambda)}{\\partial\\mathbf{x}_{\\lambda}}\\bigg]_{\\lambda=\\lambda_{t}}\\underbrace{\\int_{\\lambda_{t}}^{\\lambda_{s}}\\frac{(\\lambda-\\lambda_{t})^{n}}{n!}e^{-\\lambda}\\,\\mathrm{d}\\lambda}_{\\lambda}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $h=\\lambda_{s}-\\lambda_{t}$ . ", "page_idx": 16}, {"type": "text", "text": "The exponentially weighted integral $\\begin{array}{r}{\\int_{\\lambda_{t}}^{\\lambda_{s}}\\frac{(\\lambda-\\lambda_{t})^{n}}{n!}e^{-\\lambda}\\,\\mathrm{d}\\lambda}\\end{array}$ can be solved analytically by applying $n$ times integration by parts [16, 28] such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda}\\frac{(\\lambda-\\lambda_{t})^{n}}{n!}\\;\\mathrm{d}\\lambda=\\frac{\\sigma_{s}}{\\alpha_{s}}h^{n+1}\\varphi_{n+1}(h),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with the special $\\varphi$ -functions [26]. These functions are defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\varphi_{n+1}(h):=\\int_{0}^{1}e^{(1-u)h}\\frac{u^{n}}{n!}\\ \\mathrm{d}u,\\qquad\\varphi_{0}(h)=e^{h},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which satisfy the recurrence relation $\\varphi_{k+1}(h)\\,=\\,(\\varphi_{n}(h)-\\varphi_{n}(0))/h$ and have closed forms for $k=1,2$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\varphi_{1}(h)=\\frac{e^{h}-1}{h},}\\\\ {\\displaystyle\\varphi_{2}(h)=\\frac{e^{h}-h-1}{h^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Likewise, the Taylor expansion of the exponentially weighted integral in Equation (A.12) yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{a}_{\\mathbf{z}}(s)=\\mathbf{a}_{\\mathbf{z}}(t)+\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{-\\lambda}\\displaystyle\\sum_{n=0}^{k-1}\\frac{(\\lambda-\\lambda_{t})^{n}}{n!}\\frac{\\mathrm{d}^{n}}{\\mathrm{d}\\lambda^{n}}\\bigg[\\alpha_{\\lambda}\\mathbf{a}_{\\mathbf{x}}(\\lambda)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{\\lambda},\\mathbf{z},\\lambda)}{\\partial\\mathbf{z}}\\bigg]_{\\lambda=\\lambda_{t}}\\,\\mathrm{d}\\lambda+\\mathcal{O}(h^{k+1})}\\\\ {=\\mathbf{a}_{\\mathbf{z}}(t)+\\displaystyle\\sum_{n=0}^{k-1}\\underbrace{\\mathrm{d}\\lambda^{n}}_{\\displaystyle\\mathrm{d}\\lambda^{n}}\\bigg[\\alpha_{\\lambda}\\mathbf{a}_{\\mathbf{x}}(\\lambda)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{\\lambda},\\mathbf{z},\\lambda)}{\\partial\\mathbf{z}}\\bigg]_{\\lambda=\\lambda_{t}}\\int_{\\lambda_{t}}^{\\lambda_{s}}\\frac{(\\lambda-\\lambda_{t})^{n}}{n!}e^{-\\lambda}\\,\\mathrm{d}\\lambda+\\underbrace{\\mathcal{O}(h^{k+1})}_{\\displaystyle\\mathrm{d}\\lambda}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.3 AdjointDEIS-1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For $k=1$ and omitting the higher-order error term, Equation (A.15) becomes: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{a}_{\\mathbf{x}}(s)=\\displaystyle\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{a}_{\\mathbf{x}}(t)+\\frac{1}{\\alpha_{s}}\\alpha_{t}^{2}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{x}_{t}}\\int_{\\lambda_{t}}^{\\lambda_{s}}\\frac{(\\lambda-\\lambda_{t})^{0}}{0!}e^{-\\lambda}\\ \\mathrm{d}\\lambda}\\\\ &{\\qquad=\\displaystyle\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{a}_{\\mathbf{x}}(t)+\\sigma_{s}(e^{h}-1)\\frac{\\alpha_{t}^{2}}{\\alpha_{s}^{2}}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{x}_{t}}\\qquad\\mathrm{By~Equation~}(\\mathrm{A}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Likewise, the continuous adjoint equation for $\\mathbf{z}$ , Equation (A.20), becomes when $k=1$ by omitting the higher-order error term: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{a}_{\\mathbf{z}}(s)=\\mathbf{a}_{\\mathbf{z}}(t)+\\alpha_{t}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{z}}\\int_{\\lambda_{t}}^{\\lambda_{s}}\\frac{(\\lambda-\\lambda_{t})^{0}}{0!}e^{-\\lambda}\\;\\mathrm{d}\\lambda}\\\\ &{\\qquad=\\mathbf{a}_{\\mathbf{z}}(t)+\\sigma_{s}(e^{h}-1)\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{z}}\\qquad\\mathrm{By~Equation~(A.16).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "And the first-order solver for ${\\bf a}_{\\theta}(t)$ can be found in a similar fashion, thus we have derived the AdjointDEIS-1 solvers. ", "page_idx": 16}, {"type": "text", "text": "A.4 AdjointDEIS-2M ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For notational convenience let $\\mathbf{V}(\\mathbf{x};t)$ denote the scaled vector-Jacobian product of the adjoint state ${\\bf a}_{\\bf x}(t)$ and the gradient of the model w.r.t. $\\mathbf{x}_{t}$ , i.e., ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{V}(\\mathbf{x};t)=\\alpha_{t}^{2}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{x}_{t}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Consider the following definition of the limit in the log-SNR domain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\lambda}\\bigg[\\alpha_{\\lambda}^{2}\\mathbf{a}_{\\mathbf{x}}(\\lambda)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{\\lambda},\\mathbf{z},\\lambda)}{\\partial\\mathbf{x}_{\\lambda}}\\bigg]=\\operatorname*{lim}_{\\lambda_{r}\\rightarrow\\lambda_{t}}\\frac{\\mathbf{V}(\\mathbf{x};\\lambda_{t})-\\mathbf{V}(\\mathbf{x};\\lambda_{r})}{\\rho h},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{\\rho=\\frac{\\lambda_{t}-\\lambda_{r}}{h}}\\end{array}$ with $h=\\lambda_{s}-\\lambda_{t}$ and where $r$ is some previous step $r<t<s$ . Again $\\mathbf{V}(\\mathbf{x};\\lambda_{t})$ is overloaded to mean $\\mathbf{V}(\\mathbf{x};t_{\\lambda}(\\lambda_{t}))$ . Then by omitting higher-order error $O(h^{k+1})$ , Equation (A.15) becomes: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{a}_{\\mathbf{x}}(s)=\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{a}_{\\mathbf{x}}(t)+\\frac{1}{\\alpha_{s}}\\bigg[\\mathbf{V}(\\mathbf{x};\\lambda_{t})\\int_{\\lambda_{t}}^{\\lambda_{s}}\\frac{(\\lambda-\\lambda_{t})^{0}}{0!}\\;\\mathrm{d}\\lambda+\\mathbf{V}^{(1)}(\\mathbf{x};\\lambda_{t})\\int_{\\lambda_{t}}^{\\lambda_{s}}\\frac{(\\lambda-\\lambda_{t})^{1}}{1!}\\;\\mathrm{d}\\lambda\\bigg]}\\\\ &{\\qquad=\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{a}_{\\mathbf{x}}(t)+\\frac{1}{\\alpha_{s}}\\bigg[\\frac{\\sigma_{s}}{\\alpha_{s}}(e^{h}-1)\\mathbf{V}(\\mathbf{x};\\lambda_{t})+\\frac{\\sigma_{s}}{\\alpha_{s}}(e^{h}-h-1)\\mathbf{V}^{(1)}(\\mathbf{x};\\lambda_{t})\\bigg].\\qquad\\qquad(\\mathbf{A}<\\mathbf{V}<\\mathbf{V})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By applying the same approximation used in Lu et al. [16] of ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{e^{h}-h-1}{h}\\approx\\frac{e^{h}-1}{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "then we can rewrite the second term of the Taylor expansion as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\sigma_{s}}{\\alpha_{s}}(e^{h}-h-1)\\mathbf{V}^{(1)}(\\mathbf{x};\\lambda_{t})\\approx\\frac{\\sigma_{s}}{\\alpha_{s}}(e^{h}-h-1)\\frac{\\mathbf{V}(\\mathbf{x};\\lambda_{t})-\\mathbf{V}(\\mathbf{x};\\lambda_{r})}{\\rho h}\\qquad}&{}&{\\mathrm{By~Equation~(A.24)}}\\\\ &{}&{\\approx\\frac{\\sigma_{s}}{\\alpha_{s}}\\frac{e^{h}-1}{2\\rho}\\big(\\mathbf{V}(\\mathbf{x};\\lambda_{t})-\\mathbf{V}(\\mathbf{x};\\lambda_{r})\\big)\\qquad}&{\\mathrm{By~Equation~(A.26)}}\\\\ &{}&{=\\frac{\\sigma_{s}}{\\alpha_{s}}\\frac{e^{h}-1}{2\\rho}\\bigg(\\alpha_{t}^{2}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{x}_{t}}-\\alpha_{r}^{2}\\mathbf{a}_{\\mathbf{x}}(r)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{r},\\mathbf{z},r)}{\\partial\\mathbf{x}_{r}}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then Equation (A.25) becomes ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{a}_{\\mathbf{x}}(s)=\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{a}_{\\mathbf{x}}(t)+\\sigma_{s}(e^{h}-1)\\frac{\\alpha_{t}^{2}}{\\alpha_{s}^{2}}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{x}_{t}}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ {+\\,\\sigma_{s}\\frac{e^{h}-1}{2\\rho}\\bigg(\\frac{\\alpha_{t}^{2}}{\\alpha_{s}^{2}}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{x}_{t}}-\\frac{\\alpha_{r}^{2}}{\\alpha_{s}^{2}}\\mathbf{a}_{\\mathbf{x}}(r)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{r},\\mathbf{z},r)}{\\partial\\mathbf{x}_{r}}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Likewise, consider the scaled vector-Jacobian product of the adjoint state ${\\bf a}_{\\bf x}(t)$ and the gradient of the model w.r.t. z, i.e., ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{V}(\\mathbf{z};t)=\\alpha_{t}\\mathbf{a_{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{z}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "along with a corresponding definition of first-derivative w.r.t. $\\lambda$ as defined in Equation (A.24). As such Equation (A.20), when $k=2$ , becomes the following when omitting the higher-order error term: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{a}_{\\mathbf{z}}(s)=\\mathbf{a}_{\\mathbf{z}}(t)+\\mathbf{V}(\\mathbf{z};\\lambda_{t})\\displaystyle\\int_{\\lambda_{t}}^{\\lambda_{s}}\\frac{(\\lambda-\\lambda_{t})^{0}}{0!}\\;\\mathrm{d}\\lambda+\\mathbf{V}^{(1)}(\\mathbf{z};\\lambda_{t})\\displaystyle\\int_{\\lambda_{t}}^{\\lambda_{s}}\\frac{(\\lambda-\\lambda_{t})^{1}}{1!}\\;\\mathrm{d}\\lambda}\\\\ &{\\qquad=\\mathbf{a}_{\\mathbf{z}}(t)+\\frac{\\sigma_{s}}{\\alpha_{s}}(e^{h}-1)\\mathbf{V}(\\mathbf{z};\\lambda_{t})+\\frac{\\sigma_{s}}{\\alpha_{s}}(e^{h}-h-1)\\mathbf{V}^{(1)}(\\mathbf{z};\\lambda_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The second term of the Taylor expansion can be rewritten as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\sigma_{s}}{\\alpha_{s}}(e^{h}-h-1)\\mathbf{V}^{(1)}(\\mathbf{z};\\lambda_{t})\\approx\\frac{\\sigma_{s}}{\\alpha_{s}}(e^{h}-h-1)\\frac{\\mathbf{V}(\\mathbf{z};\\lambda_{t})-\\mathbf{V}(\\mathbf{z};\\lambda_{r})}{\\rho h}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\approx\\frac{\\sigma_{s}}{\\alpha_{s}}\\frac{e^{h}-1}{2\\rho}\\big(\\mathbf{V}(\\mathbf{z};\\lambda_{t})-\\mathbf{V}(\\mathbf{z};\\lambda_{r})\\big)\\qquad\\mathrm{By~Equation~(A.26)~}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{\\sigma_{s}}{\\alpha_{s}}\\frac{e^{h}-1}{2\\rho}\\bigg(\\alpha_{t}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{z}}-\\alpha_{r}\\mathbf{a}_{\\mathbf{x}}(r)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{r},\\mathbf{z},r)}{\\partial\\mathbf{z}}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then Equation (A.30) becomes ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{a}_{\\mathbf{z}}(s)=\\mathbf{a}_{\\mathbf{z}}(t)+\\sigma_{s}(e^{h}-1)\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{z}}}\\\\ &{\\qquad\\qquad\\qquad+\\sigma_{s}\\frac{e^{h}-1}{2\\rho}\\bigg(\\frac{\\alpha_{t}}{\\alpha_{s}}\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{z}}-\\frac{\\alpha_{r}}{\\alpha_{s}}\\mathbf{a}_{\\mathbf{x}}(r)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{r},\\mathbf{z},r)}{\\partial\\mathbf{z}}\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the corresponding second-order solver for ${\\bf a}_{\\theta}(t)$ can be found in a similar manner. ", "page_idx": 18}, {"type": "text", "text": "B Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For notational brevity we denote the scaled vector-Jacobian products of the solution trajectory of AdjointDEIS as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathbf{V}}(\\mathbf{x};t)=\\alpha_{t}^{2}\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\tilde{\\mathbf{x}}_{t},\\mathbf{z},t)}{\\partial\\tilde{\\mathbf{x}}_{t}},}\\\\ {\\tilde{\\mathbf{V}}(\\mathbf{z};t)=\\alpha_{t}\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\tilde{\\mathbf{x}}_{t},\\mathbf{z},t)}{\\partial\\mathbf{z}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B.1 Assumptions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For the AdjointDEIS solvers, we make similar assumptions to Lu et al. [16]. ", "page_idx": 18}, {"type": "text", "text": "Assumption B.1. The total derivatives of the vector-Jacobian products ${\\bf V}^{(k)}(\\{{\\bf x}_{\\lambda},{\\bf z},\\theta\\},\\lambda)$ as a function of $\\lambda$ exist and are continuous for $0\\leq j\\leq k+1$ (and hence bounded). ", "page_idx": 18}, {"type": "text", "text": "Assumption B.2. The function $\\epsilon_{\\theta}(\\mathbf{x},\\mathbf{z},t)$ is continuous in t and uniformly Lipschitz and continuously differentiable w.r.t. its first parameter $\\mathbf{x}$ . ", "page_idx": 18}, {"type": "text", "text": "Assumption B.3. $\\begin{array}{r}{h_{m a x}:=\\operatorname*{max}_{1\\leq j\\leq M}h_{j}=\\mathcal{O}(1/M).}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Assumption B.4. $\\rho_{i}>c>0$ for all $i=1,\\cdot\\cdot\\cdot M$ and some constant c. ", "page_idx": 18}, {"type": "text", "text": "The first assumption is required by Taylor\u2019s theorem. The second assumption is a mild assumption to ensure Theorem B.1 holds, which is used to replace $\\tilde{\\mathbf{V}}(\\{\\mathbf{x}_{t},\\mathbf{z},\\theta\\},t)$ with ${\\bf V}(\\{{\\bf x}_{t},{\\bf z},\\theta\\},t)\\mathrm{~+~}$ $O(\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t)-\\mathbf{a}_{\\mathbf{x}}(t))$ so the Taylor expansion w.r.t. $\\lambda_{s}$ is applicable. The third assumption is a technical assumption to exclude a significantly large step size. The last assumption is necessary for the case when $k=2$ . For our proofs we follow a similar outline to that taken by Lu et al. [17, Appendix A]. ", "page_idx": 18}, {"type": "text", "text": "B.2 The Vector-Jacobian Product is Lipschitz ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma B.1 (Vector-Jacobian Product is Lipschitz.). Let $\\pmb{f}_{\\theta}:\\mathbb{R}^{d}\\times\\mathbb{R}^{z}\\times[0,T]\\rightarrow\\mathbb{R}^{d}$ be continuous in t and uniformly Lipschitz and continuously differentiable in $\\mathbf{x}$ . Let $\\mathbf x:[0,T]\\rightarrow\\mathbb R^{d}$ be the unique solution to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{x}_{t}}{\\mathrm{d}t}=\\pmb{f}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with initial condition $\\mathbf{x}_{\\mathrm{0}}$ . Then the following map ", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\mathbf{a},t)\\mapsto-\\mathbf{a}^{\\top}\\frac{\\partial\\mathbf{{f}}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial[\\mathbf{x}_{t},\\mathbf{z},\\theta]}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "is Lipschitz in a. Moreover, the Lipschitz constant $L>0$ is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\nL=\\operatorname*{sup}_{t\\in[0,T]}\\bigg|\\frac{\\partial f_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial\\mathbf{x}_{t}}\\bigg|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Now as xt is continuous and f \u03b8 is continuously differentiable in x, so t  \u2192 \u2202[x\u2202tf, z\u03b8,\u03b8]( is a continuous function on the compact set $[0,T]$ , so it is bounded by some $L\\,>\\,0$ . Likewise, for $\\mathbf{a}\\in\\mathbb{R}^{d}$ the map $\\begin{array}{r}{(\\mathbf{a},t)\\mapsto-\\mathbf{a}^{\\top}\\frac{\\partial\\mathbf{f}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\partial[\\mathbf{x}_{t},\\mathbf{z},\\theta]}}\\end{array}$ a\u22a4\u2202f\u2202 [\u03b8x(tx,tz,,z\u03b8,]t) is Lipschitz in a with Lipschitz constant L and this constant is independent of $t$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.3 Proof of Theorem 3.1 when $k=1$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. First we consider the case of the adjoint state ${\\bf a}_{\\bf x}(t)$ . Recall that the AdjointDEIS-1 solver for $\\mathbf{a_{x}}$ with higher-order error terms is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{a}_{\\mathbf{x}}(t_{i+1})=\\frac{\\alpha_{t_{i}}}{\\alpha_{t_{i+1}}}\\mathbf{a}_{\\mathbf{x}}(t_{i})+\\sigma_{t_{i+1}}(e^{h_{i}}-1)\\frac{\\alpha_{t_{i}}^{2}}{\\alpha_{t_{i+1}}^{2}}\\mathbf{a}_{\\mathbf{x}}(t_{i})^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t_{i}},\\mathbf{z},t)}{\\partial\\mathbf{x}_{t_{i}}}+\\mathcal{O}(h_{i}^{2}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we let $t_{i}=t,t_{i+1}=s,h_{i}=\\lambda_{t_{i+1}}-\\lambda_{t_{i}}$ from Equation (A.21). By Theorem B.1 and Equation (A.21) it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{i+1})=\\frac{\\alpha_{t_{i}}}{\\alpha_{t_{i+1}}}\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{i})+\\sigma_{t_{i+1}}(e^{h_{i}}-1)\\frac{\\alpha_{t_{i}}^{2}}{\\alpha_{t_{i+1}}^{2}}\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{i})^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\tilde{\\mathbf{x}}_{t_{i}},\\mathbf{z},t)}{\\partial\\tilde{\\mathbf{x}}_{t_{i}}}}\\\\ &{\\quad\\quad\\quad=\\frac{\\alpha_{t_{i}}}{\\alpha_{t_{i+1}}}\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{i})+\\sigma_{t_{i+1}}(e^{h_{i}}-1)\\frac{\\alpha_{t_{i}}^{2}}{\\alpha_{t_{i+1}}^{2}}\\biggl(\\mathbf{a}_{\\mathbf{x}}(t_{i})^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t_{i}},\\mathbf{z},t)}{\\partial\\mathbf{x}_{t_{i}}}+\\mathcal{O}(\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{i})-\\mathbf{a}_{\\mathbf{x}}(t_{i}))\\biggr)}\\\\ &{\\quad\\quad\\quad=\\frac{\\alpha_{t_{i}}}{\\alpha_{t_{i+1}}}\\mathbf{a}_{\\mathbf{x}}(t_{i})+\\sigma_{t_{i+1}}(e^{h_{i}}-1)\\frac{\\alpha_{t_{i}}^{2}}{\\alpha_{t_{i+1}}^{2}}\\mathbf{a}_{\\mathbf{x}}(t_{i})^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t_{i}},\\mathbf{z},t)}{\\partial\\mathbf{x}_{t_{i}}}+\\mathcal{O}(\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{i})-\\mathbf{a}_{\\mathbf{x}}(t_{i}))}\\\\ &{\\quad\\quad\\quad=\\mathbf{a}_{\\mathbf{x}}(t_{i+1})+\\mathcal{O}(h_{m a x}^{2})+\\mathcal{O}(\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{i})-\\mathbf{a}_{\\mathbf{x}}(t_{i})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Repeat, this argument, from $\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{0})=\\mathbf{a}_{\\mathbf{x}}(0)$ then we find ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{M})=\\mathbf{a}_{\\mathbf{x}}(T)+\\mathcal{O}(M h_{m a x}^{2})=\\mathbf{a}_{\\mathbf{x}}(T)+\\mathcal{O}(h_{m a x}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Although the argument for the adjoint state ${\\bf a}_{\\bf z}(t)$ follows an analogous form to the one above we explicitly state it for completeness. Recall that the AdjointDEIS-1 solver for $\\mathbf{a}_{\\mathbf{z}}$ with higher-order error terms is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{a}_{\\mathbf{z}}(t_{i+1})=\\mathbf{a}_{\\mathbf{z}}(t_{i})+\\sigma_{t_{i+1}}(e^{h_{i}}-1)\\frac{\\alpha_{t_{i}}}{\\alpha_{t_{i+1}}}\\mathbf{a}_{\\mathbf{x}}(t_{i})^{\\top}\\frac{\\partial\\epsilon_{\\theta}(\\mathbf{x}_{t_{i}},\\mathbf{z},t)}{\\partial\\mathbf{z}}+\\mathcal{O}(h_{i}^{2}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By Theorem B.1 and Equation (A.22) it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathbf{a}}_{\\mathbf{z}}(t_{i+1})=\\tilde{\\mathbf{a}}_{\\mathbf{z}}(t_{i})+\\sigma_{t_{i+1}}(e^{h_{i}}-1)\\frac{\\alpha_{t_{i}}}{\\alpha_{t_{i+1}}}\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{i})^{\\top}\\frac{\\partial\\epsilon_{\\theta}\\left(\\tilde{\\mathbf{x}}_{t_{i}},\\mathbf{z},t\\right)}{\\partial\\mathbf{z}}}\\\\ &{\\quad\\quad\\quad=\\tilde{\\mathbf{a}}_{\\mathbf{z}}(t_{i})+\\sigma_{t_{i+1}}(e^{h_{i}}-1)\\frac{\\alpha_{t_{i}}}{\\alpha_{t_{i+1}}}\\biggl(\\mathbf{a}_{\\mathbf{x}}(t_{i})^{\\top}\\frac{\\partial\\epsilon_{\\theta}\\left(\\mathbf{x}_{t_{i}},\\mathbf{z},t\\right)}{\\partial\\mathbf{z}}+\\mathcal{O}(\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{i})-\\mathbf{a}_{\\mathbf{x}}(t_{i}))\\biggr)}\\\\ &{\\quad\\quad\\quad=\\mathbf{a}_{\\mathbf{z}}(t_{i})+\\sigma_{t_{i+1}}(e^{h_{i}}-1)\\frac{\\alpha_{t_{i}}}{\\alpha_{t_{i+1}}}\\mathbf{a}_{\\mathbf{x}}(t_{i})^{\\top}\\frac{\\partial\\epsilon_{\\theta}\\left(\\mathbf{x}_{t_{i}},\\mathbf{z},t\\right)}{\\partial\\mathbf{z}}+\\mathcal{O}(\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{i})-\\mathbf{a}_{\\mathbf{x}}(t_{i}))}\\\\ &{\\quad\\quad\\quad=\\mathbf{a}_{\\mathbf{z}}(t_{i+1})+\\mathcal{O}(h_{m a x}^{2})+\\mathcal{O}(\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{i})-\\mathbf{a}_{\\mathbf{x}}(t_{i})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Repeat, this argument, from $\\tilde{\\mathbf{a}}_{\\mathbf{z}}(t_{0})=\\mathbf{0}$ then we find ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{a}}_{\\mathbf{z}}(t_{M})=\\mathbf{a}_{\\mathbf{z}}(T)+\\mathcal{O}(M h_{m a x}^{2})=\\mathbf{a}_{\\mathbf{z}}(T)+\\mathcal{O}(h_{m a x}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "An identical argument can be constructed for $\\mathbf{a}_{\\theta}$ thereby finishing the proof. ", "page_idx": 19}, {"type": "text", "text": "B.4 Proof of Theorem 3.1 when $k=2$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We prove the discretization error of the AdjointDEIS-2M solver. Note for the AdjointDEIS-2M solver we have $h_{i}=\\lambda_{t_{i+1}}-\\lambda_{t_{i-1}}$ and \u03bbti\u2212h\u03bbti\u22121. Furthermore, let \u2206i = \u2225a\u02dcx(ti) \u2212ax(ti)\u2225. Without loss of generality, we will prove this only for $\\mathbf{a_{x}}$ ; the derivation for $\\mathbf{a}_{\\mathbf{z}}$ and $\\mathbf{a}_{\\theta}$ is analogous. ", "page_idx": 19}, {"type": "text", "text": "Proof. First we consider the case of the adjoint state ${\\bf a}_{\\bf x}(t)$ . Recall that the AdjointDEIS-2, see Equation (A.25), solver for $\\mathbf{a_{x}}$ with higher-order error terms is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{\\iota}_{\\mathbf{x}}(t_{i+1})=\\frac{\\alpha_{t_{i}}}{\\alpha_{t_{i+1}}}\\mathbf{a}_{\\mathbf{x}}(t_{i})+\\frac{1}{\\alpha_{t_{i+1}}}\\biggl[\\frac{\\sigma_{t_{i+1}}}{\\alpha_{t_{i+1}}}(e^{h_{i}}-1)\\mathbf{V}(\\mathbf{x};t_{i})+\\frac{\\sigma_{t_{i+1}}}{\\alpha_{t_{i+1}}}(e^{h_{i}}-h_{i}-1)\\mathbf{V}^{(1)}(\\mathbf{x};t_{i})\\biggr]+\\mathcal{O}(h_{i}^{3}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Taylor\u2019s expansion yields ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\mathbf{a}_{\\mathbf{x}}(t_{i+1})-\\left(\\frac{\\alpha_{t_{i}}}{\\alpha_{t_{i+1}}}\\mathbf{a}_{\\mathbf{x}}(t_{i})+\\frac{1}{\\alpha_{t_{i+1}}}\\Bigl[\\frac{\\sigma_{t_{i+1}}}{\\alpha_{t_{i+1}}}(e^{h_{i}}-1)\\mathbf{V}(\\mathbf{x};t_{i})+\\frac{\\sigma_{t_{i+1}}}{\\alpha_{t_{i+1}}}(e^{h_{i}}-h_{i}-1)\\mathbf{V}^{(1)}(\\mathbf{x};t_{i})\\Bigr]\\right)\\right|\\le C h_{i}^{3},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $C$ is a constant that depends on $\\mathbf{V}^{(2)}(\\mathbf{x}_{t},t)$ . Also note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\bigg\\|\\mathbf{V}^{(1)}(\\mathbf{x};t_{i})-\\frac{1}{\\rho_{i}h_{i}}\\big(\\mathbf{V}(\\mathbf{x};t_{i})-\\mathbf{V}(\\mathbf{x};t_{i-1})\\big)\\bigg\\|\\leq C h_{i}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\rho_{i}$ is bounded away from zero, and $e^{-h_{i}}=1-h_{i}+h_{i}^{2}/2+\\mathcal{O}(h_{i}^{3})$ , we know ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|(e^{h_{i}}-h_{i}+1)\\mathbf{V}^{(1)}(\\mathbf{x};t_{i})-\\frac{e^{h_{i}}-1}{2\\rho_{i}}(\\tilde{\\mathbf{V}}(\\mathbf{x};t_{i})-\\tilde{\\mathbf{V}}(\\mathbf{x};t_{i-1}))\\right\\|}\\\\ &{\\leq C L h_{i}(\\Delta_{i}+\\Delta_{i-1})+C h_{i}^{3}+\\displaystyle\\frac{1}{\\rho_{i}}\\left|\\frac{e^{h_{i}}-1}{2}-\\frac{e^{h_{i}}-h_{i}-1}{h_{i}}\\right|\\|\\mathbf{V}(\\mathbf{x};t_{i})-\\mathbf{V}(\\mathbf{x};t_{i-1})\\|}\\\\ &{\\leq C L h_{i}(\\Delta_{i}+\\Delta_{i-1})+C h_{i}^{3}+C h_{i}^{2}\\|\\mathbf{V}(\\mathbf{x};t_{i})-\\mathbf{V}(\\mathbf{x};t_{i-1})\\|}\\\\ &{\\leq C L h_{i}(\\Delta_{i}+\\Delta_{i-1})+C M_{i}h_{i}^{3},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{M_{i}=1+\\operatorname*{sup}_{t_{i}\\leq t\\leq t_{i+1}}\\|\\mathbf{V}^{(1)}(\\mathbf{x};t)\\|}\\end{array}$ and $L$ are the Lipschitz constants of $\\mathbf{V}(\\mathbf{x};t)$ by Theorem B.1. Then, $\\Delta_{i+1}$ can be estimated as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{i+1}\\leq\\frac{\\alpha_{t_{i}}}{\\alpha_{t_{i+1}}}\\Delta_{i}+\\frac{\\sigma_{t_{i+1}}}{\\alpha_{t_{i+1}}^{2}}L\\Delta_{i}+\\frac{\\sigma_{t_{i+1}}}{\\alpha_{t_{i+1}}^{2}}(C M_{i}h_{i}^{3}+C L h_{i}(\\Delta_{i}+\\Delta_{i+1}))+C h_{i}^{3}}\\\\ {\\leq\\frac{\\alpha_{t_{i}}}{\\alpha_{t_{i+1}}}\\Delta_{i}+\\tilde{C}h_{i}(\\Delta_{i}+\\Delta_{i+1}+h_{i}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, $\\Delta_{i+1}=\\mathcal{O}(h_{m a x}^{2})$ as long as $h_{m a x}$ is sufficiently small and $\\Delta_{0}+\\Delta_{1}=\\mathcal{O}(h_{m a x}^{2})$ , which can be verified via Taylor expansion, thereby finishing the proof. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "C Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For additional clarity, we let $\\mathbf{x}(t)\\equiv\\mathbf{x}_{t}$ and likewise, ${\\bf z}(t)\\equiv{\\bf z}_{t}$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. As ${\\bf z}(t)$ is continuously differentiable w.r.t. $t$ there exists some function $\\mathbf{z}^{\\prime}(t)$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{d}\\mathbf{z}}{\\mathrm{d}t}}(t)=\\mathbf{z}^{\\prime}(t).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Consider the augmented state defined by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left[\\mathbf{\\underline{{x}}}\\right](t)=f_{\\mathrm{aug}}=\\left[\\mathbf{\\underline{{f}}}_{\\theta}(\\mathbf{x}_{t},\\mathbf{z}_{t},t)\\right],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and the associated augmented adjoint state ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{a}_{\\mathrm{aug}}(t):=\\left[\\mathbf{a}_{\\mathbf{x}}\\right](t).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The Jacobian of $\\pmb{f}_{\\mathrm{aug}}$ has the form ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\partial f_{\\mathrm{aug}}}{\\partial[{\\bf x},{\\bf z}]}=\\left[\\frac{\\partial f_{\\theta}({\\bf x},{\\bf z},t)}{\\partial{\\bf x}}\\quad\\frac{\\partial f_{\\theta}({\\bf x},{\\bf z},t)}{\\partial{\\bf z}}\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Recall that ${\\bf a}_{\\bf x}(t)$ evolves with ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{a}_{\\mathbf{x}}}{\\mathrm{d}t}(t)=-\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial{\\pmb{f}}_{\\theta}(\\mathbf{x}(t),\\mathbf{z}(t),t)}{\\partial\\mathbf{x}(t)}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using Equations (C.3) and (C.4) we can define the evolution of the adjoint augmented state as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{a}_{\\mathrm{aug}}}{\\mathrm{d}t}(t)=-\\left[\\mathbf{a}_{\\mathbf{x}}\\quad\\mathbf{a}_{\\mathbf{z}}\\right](t)\\frac{\\partial f_{\\mathrm{aug}}}{\\partial[\\mathbf{x},\\mathbf{z}]}(t)=-\\left[\\mathbf{a}_{\\mathbf{x}}\\frac{\\partial f_{\\theta}(\\mathbf{x},\\mathbf{z},t)}{\\partial\\mathbf{x}}\\quad\\mathbf{a}_{\\mathbf{x}}\\frac{\\partial f_{\\theta}(\\mathbf{x},\\mathbf{z},t)}{\\partial\\mathbf{z}}\\right](t).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, ${\\bf a}_{\\bf z}(t)$ evolves with the ODE ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{a}_{\\mathbf{z}}(0)=0,\\qquad\\frac{\\mathrm{d}\\mathbf{a}_{\\mathbf{z}}}{\\mathrm{d}t}(t)=-\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial f_{\\theta}(\\mathbf{x}(t),\\mathbf{z}(t),t)}{\\partial\\mathbf{z}(t)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We have thus shown the evolution of $\\mathbf{a}_{\\mathbf{z}}(t)$ for some continuously differentiable function ${\\bf z}(t)$ . ", "page_idx": 21}, {"type": "text", "text": "Now we prove the solution is unique and exists. As ${\\bf x}(t)$ is continuous and $\\scriptstyle f_{\\theta}$ is continuously differentiable in $\\mathbf{x}$ , it follows that the map $\\begin{array}{r}{t\\mapsto\\,\\frac{\\partial\\pmb{f}_{\\theta}}{\\partial\\mathbf{x}}(\\mathbf{x}(t),\\mathbf{z}(t),t)}\\end{array}$ is a continuous function on the compact set $[0,T]$ , and therefore it is bounded by some $L\\,>\\,0$ . Correspondingly, for it follows that the map $\\begin{array}{r}{(\\mathbf{a},t)\\mapsto-\\mathbf{a}^{\\top}\\frac{\\partial f_{\\theta}}{\\partial[\\mathbf{x},\\mathbf{z}]}(\\mathbf{x}(t),\\mathbf{z}(t),t)}\\end{array}$ is Lipschitz in a with Lipschitz constant $L$ and this constant is independent of $t$ . Therefore, by the Picard-Lindel\u00f6lf theorem [49, Theorem 110C] the solution ${\\bf a}_{\\bf z}(t)$ exists and is unique. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "D Details on Adjoints for SDEs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we provide further details on the continuous adjoint equations for diffusion SDEs that we omitted from the main paper due to their technical nature and for the purpose of brevity. ", "page_idx": 21}, {"type": "text", "text": "Consider the It\u00f4 integral given by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{x}_{T}=\\int_{0}^{T}\\mathbf{x}_{t}\\;\\mathrm{d}\\mathbf{w}_{t},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mathbf{x}_{t}$ is a continuous semi-martingale adapted to the filtration generated by the Wiener process $\\{\\mathbf{w}_{t}\\}_{t\\in[0,T]}$ , $\\{{\\mathcal{F}}_{t}\\}_{t\\in[0,T]}$ . The following quantity, however, is not defined ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{T}^{0}\\mathbf{x}_{t}\\ \\mathrm{d}\\mathbf{w}_{t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This is because $\\mathbf{x}_{t}$ and $\\mathbf{w}_{t}$ are adapted to $\\{{\\mathcal{F}}_{t}\\}_{t\\in[0,T]}$ which is defined in forwards time. This means $\\mathbf{x}_{t}$ does not anticipate future events only depends on past events. While this is generally sufficient when we wish to integrate backwards in time we want future events to inform past events. ", "page_idx": 21}, {"type": "text", "text": "D.1 Stratonovich Symmetric Integrals and Two-sided Filtration ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Clearly, we need a different tool to model this backwards SDE. As such, taking inspiration from the work on neural SDEs [50], we follow the treatment of Kunita [51] for the forward and backward Fisk-Stratonovich integrals using two-sided flitration. Let $\\{{\\mathcal{F}}_{s,t}\\}_{s\\leq t;s,t\\in[0,T]}$ be a two-sided flitration, where $\\mathcal{F}_{s,t}$ is the $\\sigma$ -algebra generated by $\\left\\{\\mathbf{w}_{v}-\\mathbf{w}_{u}:s\\le u\\le v\\le t\\right\\}$ for $s,t\\in[0,T]$ such that $s\\leq t$ . ", "page_idx": 21}, {"type": "text", "text": "Forward time. For a continuous semi-martingale $\\{\\mathbf{x}_{t}\\}_{t\\in[0,T]}$ adapted to the forward filtration $\\{{\\mathcal F}_{0,t}\\}_{t\\in[0,t]}$ , the Stratonovich stochastic integral is given as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{0}^{T}\\mathbf{x}_{t}\\circ\\mathrm{d}\\mathbf{w}_{t}=\\operatorname*{lim}_{|\\Pi|\\rightarrow0}\\sum_{k=1}^{N}\\frac{\\mathbf{x}_{t_{k}}+\\mathbf{x}_{t_{k-1}}}{2}\\big(\\mathbf{w}_{t_{k}}-\\mathbf{w}_{t_{k-1}}\\big)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\Pi=\\{0=t_{0}<\\cdot\\cdot<t_{N}=T\\}$ is a partition of the interval $[0,T]$ and $|\\Pi|=\\operatorname*{max}_{k}t_{k}-t_{k-1}$ The forward flitration $\\{{\\mathcal F}_{0,t}\\}_{t\\in[0,t]}$ is analogous to the flitration defined in the prior section; therefore, any continuous semi-martingale adapted to it only considers past events and does not anticipate future events. ", "page_idx": 21}, {"type": "text", "text": "Reverse time. Consider the backwards Wiener process $\\Breve{\\mathbf{w}}_{t}\\,=\\,\\mathbf{w}_{t}\\,-\\,\\mathbf{w}_{T}$ that is adapted to the backward flitration $\\{\\mathcal{F}_{s,T}\\}_{s\\in[0,T]}$ , then for a continuous se mqi-martingale $\\{\\widecheck{\\mathbf{x}}_{t}\\}_{t\\in[0,T]}$ adapted to the backward filtration, the backward Stratonovich integral is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{0}^{T}\\check{\\mathbf{x}}_{t}\\circ\\mathrm{d}\\check{\\mathbf{w}}_{t}=\\operatorname*{lim}_{|\\Pi|\\rightarrow0}\\sum_{k=1}^{N}\\frac{\\check{\\mathbf{x}}_{t_{k}}+\\check{\\mathbf{x}}_{t_{k-1}}}{2}\\big(\\check{\\mathbf{w}}_{t_{k-1}}-\\check{\\mathbf{w}}_{t_{k}}\\big)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The backward filtration $\\{\\mathcal{F}_{s,T}\\}_{s\\in[0,T]}$ is the opposite of the forward filtration in the sense that continuous semi-martingales adapted to it only depend on future events and do not anticipate past events. As such, time is effectively reversed. ", "page_idx": 21}, {"type": "text", "text": "Remark D.1. While the Stratonovich symmetric integrals give us a powerful tool for integrating forwards and backwards in time with stochastic integrals, it is important that we use the same realization of the Wiener process. ", "page_idx": 22}, {"type": "text", "text": "D.2 Stochastic Flow of Diffeomorphisms ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Consider the Stratonovich SDE defined as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{x}_{T}=\\mathbf{x}_{0}+\\int_{0}^{T}{\\mathbf{\\mathcal{f}}}(\\mathbf{x}_{t},t)\\;\\mathrm{d}t+\\int_{0}^{T}{\\mathbf{\\mathcal{g}}}(\\mathbf{x}_{t},t)\\circ\\mathrm{d}\\mathbf{w}_{t},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\pmb{f},\\pmb{g}\\in\\mathcal{C}_{b}^{\\infty,1}$ , i.e., they belong to the class of functions with infinitely many bounded derivatives w.r.t. the state and bounded first derivatives w.r.t. time. Thus, the SDE has a unique strong solution. Given a realization of the Wiener process, there exists a smooth mapping $\\Phi$ called the stochastic flow such that $\\Phi_{s,t}(\\mathbf{x}_{s})$ is the solution at time $t$ of the process described in Equation (D.5) started at $\\mathbf{x}_{s}$ at time $s\\leq t$ . This then defines a collection of continuous maps $S=\\{\\Phi_{s,t}\\}_{s\\leq t;s,t\\in[0,T]}$ from $\\mathbb{R}^{d}$ to itself. ", "page_idx": 22}, {"type": "text", "text": "Kunita [51, Theorem 3.7.1] shows that with probability 1 this collection $\\boldsymbol{S}$ satisfies the flow property ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Phi_{s,t}(\\mathbf{x}_{s})=\\Phi_{u,t}(\\Phi_{s,u}(\\mathbf{x}_{s}))\\quad s\\leq u\\leq t,\\mathbf{x}_{s}\\in\\mathbb{R}^{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and that each $\\Phi_{s,t}$ is a smooth diffeomorphism from $\\mathbb{R}^{d}$ to itself. Hence, $\\boldsymbol{S}$ is the stochastic flow of diffeomorphisms generated by Equation (D.5). Moreover, the backward flow $\\check{\\Psi}_{s,t}:=\\Phi_{s,t}^{-1}$ satisfies the backwards SDE: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\check{\\Psi}_{s,t}(\\mathbf{x}_{t})=\\mathbf{x}_{t}-\\int_{s}^{t}\\mathbf{f}\\big(\\check{\\Psi}_{u,t}(\\mathbf{x}_{t}),u\\big)\\;\\mathrm{d}u-\\int_{s}^{t}g\\big(\\check{\\Psi}_{u,t}(\\mathbf{x}_{t}),u\\big)\\circ\\mathrm{d}\\check{\\mathbf{w}}_{u},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all $s,t\\in[0,T]$ such that $s\\leq t$ . This formulation makes intuitive sense as the backwards SDE differs only from the forwards SDE by a negative sign. ", "page_idx": 22}, {"type": "text", "text": "D.3 Continuous Adjoint Equations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Now consider the adjoint flow $\\mathbf{A}_{s,t}(\\mathbf{x}_{s})=\\partial\\mathcal{L}(\\Phi_{s,t}(\\mathbf{x}_{s}))/\\partial\\mathbf{x}_{s}$ , then $\\check{\\mathbf{A}}_{s,t}(\\mathbf{x}_{t})=\\mathbf{A}_{s,t}(\\check{\\Psi}_{s,t}(\\mathbf{x}_{t}))$ . Li et al. [50] show that $\\check{\\mathbf{A}}_{s,t}(\\mathbf{x}_{t})$ satisfies the backward SDE: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\check{\\mathbf{A}}_{s,t}(\\mathbf{x}_{t})=\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{x}_{t}}+\\int_{s}^{t}\\check{\\mathbf{A}}_{u,t}(\\mathbf{x}_{t})\\frac{\\partial f}{\\partial\\mathbf{x}_{u}}(\\check{\\Psi}_{u,t}(\\mathbf{x}_{t}),u)\\;\\mathrm{d}u+\\int_{s}^{t}\\check{\\mathbf{A}}_{u,t}(\\mathbf{x}_{t})\\frac{\\partial g}{\\partial\\mathbf{x}_{u}}(\\check{\\Psi}_{u,t}(\\mathbf{x}_{t}),u)\\circ\\mathrm{d}\\check{\\mathbf{w}}_{u}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As the drift and diffusion coefficient of this SDE are in $\\mathcal{C}_{b}^{\\infty,1}$ , the system has a unique strong solution. ", "page_idx": 22}, {"type": "text", "text": "D.4 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We are now ready to put all of this together to prove the result from the main paper. ", "page_idx": 22}, {"type": "text", "text": "Proof. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=\\pmb{f}(\\mathbf{x}_{t},t)\\;\\mathrm{d}t+\\pmb{g}(t)\\circ\\mathrm{d}\\mathbf{w}_{t}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Equation (D.8) the adjoint state admitted by the flow of diffeomorphisms generated by Equation (D.9) evolves with the SDE ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\check{\\mathbf{A}}_{s,t}(\\mathbf{x}_{t})=\\displaystyle\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{x}_{t}}+\\int_{s}^{t}\\check{\\mathbf{A}}_{u,t}(\\mathbf{x}_{t})\\frac{\\partial f}{\\partial\\mathbf{x}_{u}}(\\check{\\Psi}_{u,t}(\\mathbf{x}_{t}),u)\\;\\mathrm{d}u+\\underbrace{\\int_{s}^{t}\\check{\\mathbf{A}}_{u,t}(\\mathbf{x}_{t})\\frac{\\partial g}{\\partial\\mathbf{x}_{u}}(u)\\circ\\mathrm{d}\\check{\\mathbf{w}}_{u}}_{=0}}\\\\ {\\displaystyle=\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{x}_{t}}+\\int_{s}^{t}\\check{\\mathbf{A}}_{u,t}(\\mathbf{x}_{t})\\frac{\\partial f}{\\partial\\mathbf{x}_{u}}(\\check{\\Psi}_{u,t}(\\mathbf{x}_{t}),u)\\;\\mathrm{d}u.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Clearly, the adjoint state evolves with an ODE revolving around only the drift coefficient, i.e., $\\pmb{f}$ . Therefore, we can rewrite the evolution of the adjoint state as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname{d}\\!\\mathbf{a}_{\\mathbf{x}}(t)=-\\mathbf{a}_{\\mathbf{x}}(t)^{\\top}\\frac{\\partial\\mathbf{f}}{\\partial\\mathbf{x}_{t}}(\\mathbf{x}_{t},t)\\operatorname{d}\\!t.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "D.5 Converting the It\u00f4 SDE to Stratonovich ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The diffusion SDE in Equation (4.1) is defined as an It\u00f4 SDE. However, Theorem 4.1 is defined as Stratonovich SDEs. However, an It\u00f4 SDE can be easily converted into the Stratonovich form, i.e., for some It\u00f4 SDE of the form ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=\\pmb{f}(\\mathbf{x}_{t},t)\\ \\mathrm{d}t+\\pmb{g}(\\mathbf{x}_{t},t)\\ \\mathrm{d}\\mathbf{w}_{t}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with a differentiable function $\\sigma$ , there exists a corresponding Stratonovich SDE of the form ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=\\left[\\mathbf{\\cal{f}}(\\mathbf{x}_{t},t)+\\frac{1}{2}\\frac{\\partial g}{\\partial\\mathbf{x}}(\\mathbf{x}_{t},t)\\cdot\\mathbf{g}(\\mathbf{x}_{t},t)\\right]\\mathrm{d}t+\\mathbf{g}(\\mathbf{x}_{t},t)\\circ\\mathrm{d}\\mathbf{w}_{t}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "As Equation (4.1) is defined such that $g(\\mathbf{x}_{t},t)=g(t)$ and is independent of the state $\\mathbf{x}_{t}$ , then the SDE may be written in Stratonovich form as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=\\left[f(t)\\mathbf{x}_{t}+\\frac{g^{2}(t)}{\\sigma_{t}}\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)\\right]\\,\\mathrm{d}t+g(t)\\circ\\mathrm{d}\\bar{\\mathbf{w}}_{t}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "E Additional Experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we include some additional experiments that did not fit within the main paper. ", "page_idx": 23}, {"type": "image", "img_path": "fAlcxvrOEX/tmp/117575cdf062a06b1715c481b2a43c86f884da0e517331413f30c6e2dfef0bb5.jpg", "img_caption": ["Figure 4: Morphed faces created by guided generation with AdjointDEIS with differing number of discretization steps. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "E.1 Impact of Discretization Steps ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "One of the advantages of AdjointDEIS is that the solver for the diffusion ODE and continuous adjoint equations are distinct. This means that we don\u2019t have to force $N=M$ enabling greater flexibility when using AdjointDEIS. As such we explore the impact of using fewer steps to estimate the gradient while keeping the number of sampling steps $N=20$ fixed. In Figure 4 we illustrate the impact of the change in the number of discretization steps when estimating the gradients. Unsurprisingly, the fewer steps we take the less accurate the gradients are. This matches the empirical data presented in Table 2 which measures the impact of face morphing performance measured in MMPMR. ", "page_idx": 23}, {"type": "table", "img_path": "fAlcxvrOEX/tmp/d31f817b0780471cf381b756983609a0dc9dc9b793820bb02f8ad7c0d7dc922e.jpg", "table_caption": ["Table 2: Impact of number of discretization steps, $M$ , on face morphing with AdjointDEIS. FMR $=$ $0.1\\%$ . "], "table_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "fAlcxvrOEX/tmp/9ff4e8e998349a507664f9dcd9f28ceb2f7c9ed7bdd08dea00201589d98420fc.jpg", "img_caption": ["Figure 5: Morphed faces created by guided generation with AdjointDEIS with different learning rates. All used $M=20$ the ODE variant. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "E.2 Impact of Learning Rate ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We measure the impact of the learning rate on guided generation with AdjointDEIS in Table 2. Unsurprisingly, large learning rates lower performance, especially for less accurate gradients. I.e., when $M$ is small. We illustrate an example of the impact in Figure 5. Clearly, the learning rate of $\\eta=1$ starts to distort the images even if it still fools the FR system. ", "page_idx": 24}, {"type": "table", "img_path": "fAlcxvrOEX/tmp/f809311ea185187f8ac7d8f06c2f1a2d6a7d513af656529af65d2a0720a15d81.jpg", "table_caption": ["Table 3: Impact of learning rate, $\\eta$ , on face morphing with AdjointDEIS. $\\mathrm{FMR}=0.1\\%$ "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "E.3 Number of Steps ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "fAlcxvrOEX/tmp/680fde98a881c0eb3816513e439ef0c77b6da61a1238a2393c6b7922891c3edd.jpg", "img_caption": ["Figure 6: Morphed faces created by guided generation with AdjointDEIS with different number of sampling steps. SDE solver, $M=N$ . "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "As alluded to in the main paper, one of the drawbacks of diffusion SDEs is that they require small step sizes to work properly. We observe that the missing high frequency content is added back in when the step size is increased, see Figure 6. ", "page_idx": 25}, {"type": "text", "text": "F Implementation Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "F.1 AdjointDEIS-2M Algorithm ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For completeness we have the full AdjointDEIS-2M solver implemented in Algorithm 1. ", "page_idx": 25}, {"type": "text", "text": "Algorithm 1 AdjointDEIS-2M.   \nRequire: Initial values ${\\bf a}_{\\bf x}(0)$ , monotonically increasing time steps $\\{t_{i}\\}_{i=0}^{M}$ , and noise prediction   \nmodel $\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)$ .   \n1: Denote $h_{i}:=\\lambda_{t_{i+1}}-\\lambda_{t_{i}}$ , for $i=0,\\dots,M-1$ .   \n2: $\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{0})\\gets\\mathbf{a}_{\\mathbf{x}}(0)\\ '$ $\\triangleright$ Initialize an empty buffer $Q$ .   \n3: $\\tilde{\\mathbf{a}}_{\\mathbf{z}}(t_{0})\\gets\\mathbf{0},\\tilde{\\mathbf{a}}_{\\theta}(t_{0})\\gets\\mathbf{0}$ .   \n4: $Q\\stackrel{\\mathrm{buffer}}{\\leftarrow}\\tilde{\\mathbf{a}}_{\\mathrm{aug}}(t_{0})$   \n$\\begin{array}{r l}&{\\mathbf{\\dot{\\bar{\\rho}}}:\\ \\mathbf{\\tilde{a}_{x}}(t_{1})\\gets\\frac{\\alpha_{t_{i}}}{\\alpha_{t_{1}}}\\mathbf{\\tilde{a}_{x}}(t_{0})+\\sigma_{t_{1}}(e^{h_{0}}-1)\\frac{\\alpha_{t_{0}}^{2}}{\\alpha_{t_{1}}^{2}}\\mathbf{\\tilde{a}_{x}}(t_{0})^{\\top}\\frac{\\partial\\mathbf{\\epsilon}_{\\theta}(\\tilde{\\mathbf{x}}_{t_{0}},\\mathbf{z},t)}{\\partial\\tilde{\\mathbf{x}}_{t_{0}}}}\\\\ &{\\mathbf{\\dot{\\bar{\\rho}}}:\\ \\tilde{\\mathbf{a}}_{\\mathbf{z}}(t_{1})\\gets\\tilde{\\mathbf{a}}_{\\mathbf{z}}(t_{0})+\\sigma_{t_{1}}(e^{h_{0}}-1)\\frac{\\alpha_{t_{0}}}{\\alpha_{t_{1}}}\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{0})^{\\top}\\frac{\\partial\\mathbf{\\epsilon}_{\\theta}(\\tilde{\\mathbf{x}}_{t_{0}},\\mathbf{z},t)}{\\partial\\mathbf{z}}}\\\\ &{\\mathbf{\\bar{\\rho}}:\\ \\tilde{\\mathbf{a}}_{\\theta}(t_{1})\\gets\\tilde{\\mathbf{a}}_{\\theta}(t_{0})+\\sigma_{t_{1}}(e^{h_{0}}-1)\\frac{\\alpha_{t_{0}}}{\\alpha_{t_{1}}}\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{0})^{\\top}\\frac{\\partial\\mathbf{\\epsilon}_{\\theta}(\\tilde{\\mathbf{x}}_{t_{0}},\\mathbf{z},t)}{\\partial\\theta}}\\end{array}$   \n8: $Q\\stackrel{\\mathrm{buffer}}{\\leftarrow}\\tilde{\\mathbf{a}}_{\\mathrm{aug}}(t_{1})$   \n9: for $i\\gets1,2,\\ldots,M-1$ do   \n10: $\\begin{array}{r}{\\rho_{i}\\gets\\frac{h_{i-1}}{h_{i}}}\\end{array}$   \n11: $\\begin{array}{r l}&{\\mathbf{D}_{i}\\gets\\left(1+\\frac{1}{2\\rho_{i}}\\right)\\tilde{\\mathbf{V}}(\\mathbf{x};t_{i})-\\frac{1}{2\\rho_{i}}\\tilde{\\mathbf{V}}(\\mathbf{x};t_{i-1})}\\\\ &{\\mathbf{E}_{i}\\gets\\left(1+\\frac{1}{2\\rho_{i}}\\right)\\tilde{\\mathbf{V}}(\\mathbf{z};t_{i})-\\frac{1}{2\\rho_{i}}\\tilde{\\mathbf{V}}(\\mathbf{z};t_{i-1})}\\\\ &{\\mathbf{F}_{i}\\gets\\left(1+\\frac{1}{2\\rho_{i}}\\right)\\tilde{\\mathbf{V}}(\\boldsymbol{\\theta};t_{i})-\\frac{1}{2\\rho_{i}}\\tilde{\\mathbf{V}}(\\boldsymbol{\\theta};t_{i-1})}\\\\ &{\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{i+1})\\gets\\frac{\\alpha_{t_{i}}}{\\alpha_{t_{i+1}}}\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{i})+\\frac{\\sigma_{t_{i+1}}}{\\alpha_{t_{i+1}}^{2}}(e^{h_{i}}-1)\\mathbf{D}_{i}}\\\\ &{\\tilde{\\mathbf{a}}_{\\mathbf{z}}(t_{i+1})\\gets\\tilde{\\mathbf{a}}_{\\mathbf{z}}(t_{i})+\\frac{\\sigma_{t_{i+1}}}{\\alpha_{t_{i+1}}}(e^{h_{i}}-1)\\mathbf{E}_{i}}\\\\ &{\\tilde{\\mathbf{a}}_{\\boldsymbol{\\theta}}(t_{i+1})\\gets\\tilde{\\mathbf{a}}_{\\boldsymbol{\\theta}}(t_{i})+\\frac{\\sigma_{t_{i+1}}}{\\alpha_{t_{i+1}}}(e^{h_{i}}-1)\\mathbf{F}_{i}}\\end{array}$   \n12:   \n13:   \n14:   \n15:   \n16:   \n17: if i < M \u22121 then   \n18: Qbu\u2190ff\u2212era\u02dcaug(ti+1)   \n19: end if   \n20: end for   \n21: return $\\tilde{\\mathbf{a}}_{\\mathbf{x}}(t_{M}),\\tilde{\\mathbf{a}}_{\\mathbf{z}}(t_{M}),\\tilde{\\mathbf{a}}_{\\theta}(t_{M})$ . ", "page_idx": 25}, {"type": "text", "text": "F.2 Code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Our code for AdjointDEIS will soon be available here at https://github.com/zblasingame/ AdjointDEIS. ", "page_idx": 25}, {"type": "text", "text": "F.3 Repositories Used ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For reproducibility purposes, we provide a list of links to the official repositories of other works used in this paper. ", "page_idx": 25}, {"type": "text", "text": "1. The SYN-MAD 2022 dataset used in this paper can be found at https://github.com/ marcohuber/SYN-MAD-2022.   \n2. The ArcFace models, MS1M-RetinaFace dataset, and MS1M-ArcFace dataset can be found at https://github.com/deepinsight/insightface.   \n3. The ElasticFace model can be found at https://github.com/fdbtrs/ElasticFace.   \n4. The AdaFace model can be found at https://github.com/mk-minchul/AdaFace.   \n5. The official Diffusion Autoencoders repository can be found at https://github.com/ phizaz/diffae.   \n6. The official MIPGAN repository can be found at https://github.com/ ZHYYYYYYYYYYYY/MIPGAN-face-morphing-algorithm. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "G Experimental Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we outline the details for the experiments run in Section 5. ", "page_idx": 26}, {"type": "text", "text": "G.1 DiM Algorithm ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For completeness, we provide the DiM algorithm from [34] following the notation used in [52]. The original bona fide images are denoted $\\mathbf{x}_{0}^{(a)}$ and $\\mathbf{x}_{0}^{(b)}$ . The conditional encoder is $\\mathcal{E}:\\mathcal{X}\\rightarrow\\mathcal{Z}$ , $\\Phi$ is the numerical diffusion ODE solver, $\\Phi^{+}$ is the numerical diffusion ODE solver as time runs forwards from 0 to $T$ . The algorithm is presented in Algorithm 2. ", "page_idx": 26}, {"type": "text", "text": "Algorithm 2 DiM Framework.   \nRequire: Blend parameter $w=0.5$ . Time schedule $\\{t_{i}\\}_{i=1}^{N}\\subseteq[0,T],t_{i}<t_{i+1}.$ .   \n1: $\\mathbf{z}_{a}\\gets\\mathcal{E}(\\mathbf{x}_{0}^{(a)})$ \u25b7Encoding bona fides into conditionals. 2: $\\mathbf{z}_{b}\\gets\\mathcal{E}(\\mathbf{x}_{0}^{(b)})$   \n3: for $i\\gets1,2,\\ldots,N-1$ do   \n4: $\\mathbf{x}_{t_{i+1}}^{(a)}\\leftarrow\\Phi^{+}(\\mathbf{x}_{t_{i}}^{(a)},\\epsilon_{\\theta}(\\mathbf{x}_{t_{i}}^{(a)},\\mathbf{z}_{a},t_{i}),t_{i})\\triangleright\\vdots$ Solving the probability flow ODE as time runs from 5: 0 t $\\mathbf{\\widetilde{x}}_{t_{i+1}}^{(\\dot{b})}\\gets\\Phi^{+}(\\mathbf{x}_{t_{i}}^{(b)},\\epsilon_{\\theta}(\\mathbf{x}_{t_{i}}^{(b)},\\mathbf{z}_{b},t_{i}),t_{i})$   \n67:: xT $\\mathbf{x}_{T}^{(a b)}\\leftarrow\\operatorname{slerp}(\\mathbf{x}_{T}^{(a)},\\mathbf{x}_{T}^{(b)};w)$ $\\triangleright$ Morph initial noise. 8: $\\mathbf{z}_{a b}\\gets\\mathrm{lerp}(\\mathbf{z}_{a},\\mathbf{z}_{b};w)$ $\\triangleright$ Morph conditionals. 9: for $i\\gets N,N-1,\\ldots,2$ do   \n10: $\\mathbf{x}_{t_{i-1}}^{(a b)}\\gets\\Phi(\\mathbf{x}_{t_{i}}^{(a b)},\\epsilon_{\\theta}(\\mathbf{x}_{t_{i}}^{(a b)},\\mathbf{z}_{a b},t_{i}),t_{i})$ $\\triangleright$ Solving the probability flow ODE as time runs from $T$ to 0.   \n11: end for   \n12: return x(0ab) ", "page_idx": 26}, {"type": "text", "text": "G.2 NFE ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In our reporting of the NFE we record the number of times the diffusion noise prediction U-Net is evaluated both during the encoding phase, $N_{E}$ , and solving of the PF-ODE or diffusion SDE, $N$ . We chose to report $N+N_{E}$ over $N+2N_{E}$ as even though two bona fide images are encoded resulting in $2N_{E}$ NFE during encoding, this process can simply be batched together, reducing the NFE down to $N_{E}$ . When reporting the NFE for the Morph-PIPE model, we report $N_{E}+B N$ where $B$ is the number of blends. While a similar argument can be made that the morphed candidates could be generated in a large batch of size $B$ , reducing the NFE of the sampling process down to $N$ , we chose to report $B N$ as the number of blends, $B=21$ , used in the Morph-PIPE is quite large, potentially resulting in Out Of Memory (OOM) errors, especially if trying to process a mini-batch of morphs. Using $N_{E}+N$ reporting over $N_{E}+B N$ , the NFE of Morph-PIPE is 350, which is comparable to DiM. The reporting of NFE for AdjointDEIS was calculated as $N_{E}+n_{o p t}(N+M)$ where $n_{o}p t$ is the number of optimization steps and $M$ is the number of discretization steps for the continuous adjoint equations. ", "page_idx": 26}, {"type": "text", "text": "G.3 Hardware ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "All of the main experiments were done on a single NVIDIA Tesla V100 32GB GPU. On average the guided generation experiments for our approach took between 6 - 8 hours for the whole dataset of ", "page_idx": 26}, {"type": "text", "text": "face morphs with a batch size of 8. Some additional follow-up work for the camera-ready version used an NVIDIA H100 Tensor Core 80GB GPU with a batch size of 16. ", "page_idx": 27}, {"type": "text", "text": "G.4 Datasets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The SYN-MAD 2022 dataset is derived from the Face Research Lab London (FRLL) dataset [45]. FRLL is a dataset of high-quality captures of 102 different individuals with frontal images and neutral lighting. There are two images per subject, an image of a \u201cneutral\u201d expression and one of a \u201csmiling\u201d expression. The ElasticFace [46] FR system was used to select the top 250 most similar pairs, in terms of cosine similarity, of bona fide images for both genders, resulting in a total of 489 bona fide image pairs for face morphing [44], as some pairs did not generate good morphs on the reference set; we follow this minimal subset. ", "page_idx": 27}, {"type": "text", "text": "G.5 FR Systems ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "All three FR systems use the Improved ResNet (IResNet-100) architecture [53] as the neural net backbone for the FR system. The ArcFace model is a widely used FR system [34, 36, 38, 42]. It employs an additive angular margin loss to enforce intra-class compactness and inter-class distance, which can enhance the discriminative ability of the feature embeddings [41]. ElasticFace builds upon the ArcFace model by using an elastic penalty margin over the fixed penalty margin used by ArcFace. This change results in an FR system with state-of-the-art performance [46]. Lastly, the AdaFace model employs an adaptive margin loss by weighting the loss relative to an approximation of the image quality [47]. The image quality is approximated via feature norms and is used to give less weight to misclassified images, reducing the impact of \u201clow\u201d quality images on training. This improvement allows the AdaFace model to achieve state-of-the-art performance in FR tasks. ", "page_idx": 27}, {"type": "text", "text": "The AdaFace and ElasticFace models are trained on the MS1M-ArcFace dataset, whereas the ArcFace model is trained on the MS1M-RetinaFace dataset. N.B., the ArcFace model used in the identity loss is not the same ArcFace model used during evaluation. The model used in the identity loss is an IResNet-100 trained on the Glint360k dataset [54] with the ArcFace loss. We use the cosine distance to measure the distance between embeddings from the FR models. All three FR systems require images of $112\\times112$ pixels. We resize every image, post alignment from dlib which ensures the images are square, to $112\\times112$ using bilinear down-sampling. The image tensors are then normalized such that they take values in $[-1,1]$ . Lastly, the AdaFace FR system was trained on BGR images so the image tensor is shuffled from the RGB format to the BGR format. ", "page_idx": 27}, {"type": "text", "text": "H Additional Related Work ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section we compare several recent methods for training-free guided generation. We broadly classify these techniques into two categories: ", "page_idx": 27}, {"type": "text", "text": "1. Techniques which directly optimize the solution trajectory during sampling [52, 55, 56] 2. Techniques which search for the optimal latents $\\mathbf{x}_{T}$ and or $\\mathbf{z}$ (this can include optimizing the solution trajectory as well) [57, 58]. ", "page_idx": 27}, {"type": "text", "text": "In Table 4 we compare several different techniques for training-free guided diffusion along this category along with whether the formulation is for diffusion ODEs, SDEs, or both. ", "page_idx": 27}, {"type": "table", "img_path": "fAlcxvrOEX/tmp/423a23851db011dc9ff14d00a1483b8ed3dfdaab20575a8eddb754e2eae21f11.jpg", "table_caption": ["Table 4: Comparison of different guidance methods for diffusion models. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "FlowGrad [56] controls the generative process by solving the following optimal control problem ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\displaystyle\\operatorname*{min}_{{\\pmb u}}}&{\\mathcal{L}({\\bf x}_{0})+\\lambda\\int_{T}^{0}\\|{\\pmb u}(t)\\|^{2}~\\mathrm{d}t,}\\\\ {\\mathrm{s.t.}}&{{\\bf x}_{0}={\\bf x}_{T}+\\displaystyle\\int_{T}^{0}f_{\\theta}({\\bf x}_{t},{\\bf z},t)+{\\pmb u}(t)~\\mathrm{d}t}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\textbf{\\em u}$ is the control function. This optimization objective learns to alter the flow, $\\scriptstyle f_{\\theta}$ , by $\\textbf{\\em u}$ . In practice, this amounts to injecting a control step governed by ${\\pmb u}(t)$ for a discretized schedule. This technique does not allow for learning an optimal $\\mathbf{x}_{T}$ , $\\mathbf{z}$ , or $\\theta$ . ", "page_idx": 28}, {"type": "text", "text": "FreeDoM [55] looks at gradient guided generation of images by calculating the gradient w.r.t. $\\mathbf{x}_{t}$ by using the approximated clean image ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbf{x}_{0}\\approx\\frac{\\mathbf{x}_{t}-\\sigma_{t}\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{z},t)}{\\alpha_{t}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "at each timestep. Let $h_{i}=\\lambda_{t_{i}}-\\lambda_{t_{i-1}}$ . The strategy can be described as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\bf{x}}_{t_{i-1}}}=\\frac{{\\alpha_{t_{i-1}}}}{{\\alpha_{t_{i}}}}{\\bf{x}}_{t_{i}}-2{\\sigma_{t_{i-1}}}(e^{{h_{i}}}-1)\\epsilon_{\\theta}({{\\bf{x}}_{t_{i}}},{{\\bf{z}}},{t_{i}})+{\\sigma_{t_{i-1}}}\\sqrt{{e^{2h_{i}}}-1}\\epsilon_{t_{i}},}\\\\ {\\hat{{\\bf{x}}}_{0}=\\frac{{{{\\bf{x}}_{t_{i}}}-{\\sigma_{t_{i}}}\\epsilon_{\\theta}}\\left({{{\\bf{x}}_{t_{i}}},{{\\bf{z}}},{t_{i}}}\\right)}{{\\alpha_{t_{i}}}},}\\\\ {{{{\\bf{g}}_{t_{i}}}=\\frac{{\\partial\\mathcal{L}(\\hat{{\\bf{x}}_{0}})}}{{\\partial{\\bf x}_{t}}},}}\\\\ {{{{\\bf{x}}_{t_{i-1}}}={{\\bf{x}}_{t_{i-1}}}-{\\eta_{t_{i}}}{\\bf{g}}_{t_{i}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\eta_{t_{i}}$ is a learning rate defined per timestep. Importantly, FreeDoM operates on diffusion SDEs. They have an addition algorithm in which the add noise back to the image, in essence going back one timestep, and applying the guidance step again. ", "page_idx": 28}, {"type": "text", "text": "Similar to FreeDoM, greedy guided generation [52] looks to alter the generative trajectory by injecting the gradient defined on the approximated clean image; however, this technique does so w.r.t. the prediction noise, i.e., ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\epsilon_{t_{i}}^{\\prime}=\\mathrm{stopgrad}(\\epsilon_{\\theta}(\\mathbf{x}_{t_{i}},\\mathbf{z},t_{i}))}\\\\ &{\\hat{\\mathbf{x}}_{0}=\\frac{\\mathbf{x}_{t_{i}}-\\sigma_{t_{i}}\\epsilon_{t_{i}}^{\\prime}}{\\alpha_{t_{i}}},}\\\\ &{\\mathbf{g}_{t_{i}}=\\frac{\\partial\\mathcal{L}(\\hat{\\mathbf{x}}_{0})}{\\partial\\epsilon_{t_{i}}},}\\\\ &{\\epsilon_{t_{i}}^{\\prime}=\\epsilon_{t_{i}}^{\\prime}-\\eta_{t_{i}}\\mathbf{g}_{t_{i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The technique would work for either diffusion ODEs or SDEs. ", "page_idx": 28}, {"type": "text", "text": "DOODL [57] looks at gradient calculation based on the invertibility of EDICT [60]. This method can find the gradient w.r.t. $\\mathbf{x}_{T}$ ; however, it cannot for the other quantities. DOODL additionally has further overhead due to the dual diffusion process of EDICT. Further analysis of DOODL compared to continuous adjoint equations for diffusion models can be found in [58]. ", "page_idx": 28}, {"type": "table", "img_path": "fAlcxvrOEX/tmp/e3c876bdb510fd7448a736293e321c0739870cfea083cbd57790168aed3eb079.jpg", "table_caption": ["Table 5: Comparison of adjoint sensitivity algorithms for diffusion models. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "More closely related to our work is the recent AdjointDPM [58] who also explores the use of adjoint sensitivity methods for backpropagation through the probability flow ODE. While they also propose to use the continuous adjoint equations to find gradients for diffusion models, our work differs in several ways which we enumerate in Table 5. For clarity, we use orange to denote their notation. They reparameterize Equation (2.4) as ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathbf{y}}{\\mathrm{d}\\rho}=\\tilde{\\epsilon}_{\\theta}(e^{\\int_{0}^{\\gamma^{-1}(\\rho)}f(\\tau)\\ \\mathrm{d}\\tau}\\mathbf{y},\\gamma^{-1}(\\rho),c)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where c denotes the conditional information, \u03c1 = \u03b3(t), and dd\u03b3t = e\u2212 0t f(\u03c4) d\u03c4 g22\u03c3(tt) . which gives them the following ODE for calculating the adjoint. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\rho}\\bigg[\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{y}_{\\rho}}\\bigg]=-\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{y}_{\\rho}}^{\\top}\\frac{\\partial\\epsilon_{\\theta}(e^{\\int_{0}^{\\gamma^{-1}(\\rho)}f(\\tau)\\;\\mathrm{d}\\tau}\\mathbf{y}_{\\rho},\\mathbf{z},\\gamma^{-1}(\\rho))}{\\partial\\mathbf{y}_{\\rho}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In our approach we integrate over $\\lambda_{t}$ whereas they integrate over $\\rho$ . Moreover, we provide custom solvers designed specifically for diffusion ODEs instead of using a black box ODE solver. Our approach is also interoperable with other forward ODE solvers meaning our AdjointDEIS solver is agnostic to the ODE solver used to generate the output; however, the AdjointDPM model is tightly coupled to its forward solver. Lastly and most importantly our method is more general and supports diffusion SDEs, not just ODEs. ", "page_idx": 29}, {"type": "text", "text": "Although the original paper omitted a closed form expression for $\\gamma^{-1}(\\rho)$ , we provide to give a comparison between both methods, and to ensure AdjointDPM can be fully implemented. In the VP SDE scheme with a linear noise schedule $\\log\\alpha_{t}$ is found to be ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\log\\alpha_{t}=-\\frac{\\beta_{1}-\\beta_{0}}{4}t^{2}-\\frac{\\beta_{0}}{2}t\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "on $t\\in[0,1]$ with $\\beta_{0}=0.1,\\beta_{1}=20$ , following Song et al. [15]. Then $\\gamma^{-1}(\\rho)$ is found to be ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\gamma^{-1}(\\rho)=\\frac{\\beta_{0}-\\sqrt{\\beta_{0}^{2}+4\\log\\frac{1}{\\sqrt{\\frac{1}{\\alpha_{0}^{2}}(\\rho+\\sigma_{0})^{2}+1}}(\\beta_{0}-\\beta_{1})}}{\\beta_{0}-\\beta_{1}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Concurrent work to ours by Marion et al. [59] has also explored the method of adjoint sensitivity for guidance of diffusion models. They, however, focus on an efficient scheme to parallelize the solution to the adjoint ODE from the perspective of bi-level optimization rather than the adjoint technique itself. So while we focused on the details of the continuous adjoint equations, they focused on an efficient implementation of the optimization problem from the perspective of bi-level optimization. ", "page_idx": 29}, {"type": "text", "text": "I Analytic Formulations of Drift and Diffusion Coefficients ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "For completeness, we show how to analytically compute the drift and diffusion coefficients for a linear noise schedule Ho et al. [1] in the VP scenario Song et al. [15]. With a linear noise schedule $\\log\\alpha_{t}$ is found to be ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\log\\alpha_{t}=-\\frac{\\beta_{1}-\\beta_{0}}{4}t^{2}-\\frac{\\beta_{0}}{2}t\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "on $t\\in[0,1]$ with $\\beta_{0}=0.1,\\beta_{1}=20$ , following Song et al. [15]. The drift coefficient becomes ", "page_idx": 29}, {"type": "equation", "text": "$$\nf(t)=-{\\frac{\\beta_{1}-\\beta_{0}}{2}}t-{\\frac{\\beta_{0}}{2}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and as $\\sigma_{t}=\\sqrt{1-\\alpha_{t}^{2}}$ we find ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}\\sigma_{t}^{2}}{\\mathrm{d}t}=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\bigg[1-\\exp{\\bigg(-\\frac{\\beta_{1}-\\beta_{0}}{4}t^{2}-\\frac{\\beta_{0}}{2}t\\bigg)}^{2}\\bigg]}\\\\ {\\displaystyle\\qquad=\\big((\\beta_{1}-\\beta_{0})t+\\beta_{0}\\big)\\exp{\\bigg(-\\frac{\\beta_{1}-\\beta_{0}}{2}t^{2}-2\\beta_{0}t\\bigg)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, the diffusion coefficient $g^{2}(t)$ is ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g^{2}(t)=\\underbrace{((\\beta_{1}-\\beta_{0})t+\\beta_{0})\\exp{\\bigg(-\\frac{\\beta_{1}-\\beta_{0}}{2}t^{2}-2\\beta_{0}t\\bigg)}}_{\\frac{\\mathrm{d}\\sigma_{t}^{2}}{\\mathrm{d}t}}}\\\\ &{\\quad\\underbrace{+\\big((\\beta_{1}-\\beta_{0})t+\\beta_{0}\\big)\\bigg[1-\\exp{\\bigg(-\\frac{\\beta_{1}-\\beta_{0}}{4}t^{2}-\\frac{\\beta_{0}}{2}t\\bigg)^{2}}\\bigg]}_{-2\\frac{\\mathrm{d}\\log{\\alpha_{t}}}{\\mathrm{d}t}\\sigma_{t}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Iapmpproorxtianmtaltyi,o $\\frac{\\mathrm{d}\\sigma_{t}}{\\mathrm{d}t}$ ndeoeeds end otw heexins ts taatr titinmg ef $t\\;=\\;0$ ,s  iansi $\\sigma_{t}$ l  isst edpi.s cIno nptriancutoicues,  aatd tdhinatg  pa osinmt,a lla aton $\\epsilon\\ll1$ $t=0$ should suffice. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We discussed the limitations of this work in Section 6. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The full set of assumptions, derivations, and proofs are found in Appendices A to D. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We presented the implementation details in Appendix F, including the algorithm as well as the repositories used. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The dataset we use are all public dataset, we have provided links to all the repositories used in the experiments in Appendix F. We provide detailed derivations of the AdjointDEIS solvers Appendix A. Interested readers can implement the algorithms themselves. We intend to release our code at https://github.com/zblasingame/AdjointDEIS. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The experimental details are presented in Appendix G. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: Due the computationally demanding nature of the guided generation we do not report error bars. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] Justification: The hardware used in this paper is explained in Appendix G.3. ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We conducted the research conforming in every aspect with the NeurIPS Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We address the broader impacts in Section 6. ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: The dataset used in the experiments are public datasets. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The details of the datasets and models used from other researchers are decsribed in Appendices F and G. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: No new assets were created at the time of submission. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We did not perform crowdsourcing. Human faces are used in the experiments, but the datasest we used are all public dataset. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: Used public datasets, as such no IRB approvals were needed. ", "page_idx": 32}]