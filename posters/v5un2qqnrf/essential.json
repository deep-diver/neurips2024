{"importance": "This paper is crucial for researchers working with large multimodal models (LMMs). It addresses the limitations of current LMMs in handling diverse vision-centric tasks by proposing a novel architecture called Lumen. **Lumen's unique approach of decoupling task-agnostic and task-specific learning processes opens up new avenues for improving the efficiency and versatility of LMMs**, making it highly relevant to current research trends in computer vision and artificial intelligence. The findings are significant and will likely inspire future research into more robust and flexible LMM designs.", "summary": "Lumen: A novel LMM architecture decouples perception learning into task-agnostic and task-specific stages, enabling versatile vision-centric capabilities and surpassing existing LMM-based approaches.", "takeaways": ["Lumen, a novel LMM architecture, decouples the learning of perception capabilities into task-agnostic and task-specific stages.", "Lumen promotes fine-grained vision-language concept alignment and achieves superior performance on various vision-centric tasks.", "Lumen maintains general visual understanding and instruction-following capabilities while enhancing vision-centric tasks."], "tldr": "Current Large Multimodal Models (LMMs) struggle with diverse vision-centric tasks due to their language-oriented design. They often adapt visual task outputs to language formats, overlooking the inductive biases in various visual tasks and hindering the learning of perception capabilities. This limits their potential in handling complex scenarios and versatile vision tasks.\nThe proposed Lumen architecture tackles this issue by decoupling perception learning into two stages: task-agnostic and task-specific.  It first focuses on aligning vision and language concepts, creating a shared representation for various vision tasks. Then, it uses lightweight task-specific decoders, minimizing training efforts.  **Experimental results across various benchmarks demonstrate that Lumen achieves or surpasses the performance of existing LMM-based approaches while maintaining general visual understanding**.", "affiliation": "Fudan University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "v5Un2QqnRf/podcast.wav"}