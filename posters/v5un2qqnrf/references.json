{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a model that learns visual representations from natural language supervision, which is foundational to many of the methods used in the Lumen architecture."}, {"fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-12-01", "reason": "Flamingo is a highly influential large multimodal model that demonstrates strong few-shot learning capabilities on various vision-language tasks, serving as a strong baseline for comparison."}, {"fullname_first_author": "Junnan Li", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-01-01", "reason": "BLIP-2 provides a strong foundation for multimodal understanding, by aligning visual and language modalities effectively, which is crucial for LMMs and directly relevant to the Lumen approach."}, {"fullname_first_author": "Deyao Zhu", "paper_title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models", "publication_date": "2023-04-01", "reason": "MiniGPT-4 is a significant LMM that integrates vision and language in a unified manner, which is directly relevant and compared against Lumen in the paper."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-04-01", "reason": "Visual instruction tuning is a key technique used to improve the performance of LMMs on vision-centric tasks, and the paper explores and directly builds upon this methodology."}]}