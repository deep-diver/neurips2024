[{"figure_path": "SNmuKbU0am/figures/figures_1_1.jpg", "caption": "Figure 1: The case that video and audio labels are different. The video label includes basketball, cheering, and clapping, while the audio label includes speech and cheering.", "description": "This figure illustrates a scenario where the video and audio labels don't perfectly align.  The video shows a basketball game with events like basketball playing, cheering, and clapping. However, the corresponding audio primarily contains speech and cheering, omitting the sound of clapping and the distinct sound of a basketball bounce. This discrepancy highlights the challenge of audio-visual learning where modalities may not always perfectly correlate.  This example motivates the need for a flexible approach that can handle such mismatches and prioritize relevant information from each modality.", "section": "1 Introduction"}, {"figure_path": "SNmuKbU0am/figures/figures_3_1.jpg", "caption": "Figure 2: Method Overview. We propose to inject trainable adapters with the MoE scheme into the frozen pre-trained backbones for dynamically adjusting the strategy. The AVMoE module mainly consists of a router layer and two types of adapters, the router ingests concatenated multimodal tokens and allocates the weights for adapters, and the output of AVMoE module is the weighted sum of all the adapters' predictions.", "description": "This figure illustrates the architecture of the Audio-Visual Mixture of Experts (AVMoE) model.  The AVMoE module takes visual and audio tokens as input, processes them through frozen pre-trained transformer encoders, and then uses a router to dynamically allocate weights to two types of adapters: unimodal and cross-modal. The unimodal adapters process information within a single modality (audio or visual), while the cross-modal adapters integrate information from both modalities. The output of the AVMoE is a weighted sum of the adapter predictions, effectively combining both unimodal and cross-modal information.", "section": "3 Method"}, {"figure_path": "SNmuKbU0am/figures/figures_5_1.jpg", "caption": "Figure 3: The structure of adapters. a) Cross-Modal Adapter (CMA), consisting of token compression, multimodal feature fusion and bottleneck block; b) Unimodal Adapter (UA), consisting of token compression, self-attention and bottleneck block.", "description": "This figure shows the architecture of the two types of adapters used in the AVMoE model: the cross-modal adapter (CMA) and the unimodal adapter (UA).  The CMA integrates information from both audio and visual modalities through token compression, feature fusion, and a bottleneck block. In contrast, the UA focuses on processing information within a single modality (either audio or visual) using token compression and a self-attention mechanism before passing it through a bottleneck block.  Both adapters aim to enhance the model's ability to process audio and visual inputs effectively, with the CMA handling cross-modal interactions and the UA focusing on intra-modal processing.", "section": "3.2 Audio-Visual Adapters"}, {"figure_path": "SNmuKbU0am/figures/figures_8_1.jpg", "caption": "Figure 4: Qualitative examples of AVMoE and DG-SCT under the S4 setting and MS3 setting of the AVS task.", "description": "This figure shows qualitative results comparing the performance of the proposed AVMoE model and the DG-SCT baseline on the Audio-Visual Segmentation (AVS) task.  The comparison is done across two settings: S4 (single sound source) and MS3 (multiple sound sources). Each row represents the results from a different method (DG-SCT, AVMoE, Ground Truth).  The columns showcase different video clips, demonstrating how each method segments the visual content based on the corresponding audio cues. The results visually highlight the differences in performance, accuracy, and robustness of the two methods under varying conditions. ", "section": "4 Experimental Analysis"}, {"figure_path": "SNmuKbU0am/figures/figures_15_1.jpg", "caption": "Figure 5: Applying AVMoE to audio-visual downstream tasks: audio-visual event localization, audio-visual video parsing, audio-visual segmentation, and audio-visual question answering. The purple and blue modules are frozen pre-trained visual and audio models, and the red modules are our proposed trainable adapter modules, and the remaining modules are set up with reference to the baseline model of these tasks, with some parameters trainable.", "description": "This figure illustrates how the proposed Audio-Visual Mixture of Experts (AVMoE) model is applied to four different audio-visual tasks: event localization, video parsing, segmentation, and question answering.  Pre-trained visual and audio models are used as the base, with the AVMoE modules (shown in red) added as trainable adapters to enhance performance.  Each task's architecture is slightly modified to suit its specific needs, but the core AVMoE approach remains consistent.", "section": "Method"}, {"figure_path": "SNmuKbU0am/figures/figures_16_1.jpg", "caption": "Figure 6: The qualitative experimental results on the AVVP task.", "description": "This figure showcases a qualitative comparison of the AVVP task's results between the proposed AVMoE model and the DG-SCT model.  It presents input video frames, the ground truth (GT) labels for both visual and audio tracks, and the predictions made by both models.  The comparison highlights the ability of AVMoE to more accurately segment and label audio and visual events, especially in cases where the audio and visual information do not perfectly align (e.g., instances where the audio contains speech but the visual track only shows a car).", "section": "4.2 Audio-Visual Video Parsing"}, {"figure_path": "SNmuKbU0am/figures/figures_16_2.jpg", "caption": "Figure 2: Method Overview. We propose to inject trainable adapters with the MoE scheme into the frozen pre-trained backbones for dynamically adjusting the strategy. The AVMoE module mainly consists of a router layer and two types of adapters, the router ingests concatenated multimodal tokens and allocates the weights for adapters, and the output of AVMoE module is the weighted sum of all the adapters' predictions.", "description": "This figure illustrates the architecture of the proposed Audio-Visual Mixture of Experts (AVMoE) model.  AVMoE injects trainable adapters into pre-trained models to adapt to audio-visual tasks. The model uses a mixture of experts approach, combining unimodal and cross-modal adapters. A router layer dynamically allocates weights to each expert based on the input. This allows the model to leverage the strengths of each adapter, improving performance in various audio-visual tasks.", "section": "3 Method"}, {"figure_path": "SNmuKbU0am/figures/figures_17_1.jpg", "caption": "Figure 8: Qualitative visualizations of visual and audio features of original and Ours on AVE and AVS tasks.", "description": "This figure visualizes the learned audio and visual features using t-SNE. Each point represents a feature from an individual audio or visual event, with color indicating its category.  The visualization demonstrates that the features extracted by the proposed AVMoE model are more compact within classes and more distinct between classes, showing that AVMoE learns discriminative features for each modality across audio-visual downstream tasks.  The \"Original\" columns show feature visualizations from baseline methods, while the \"Ours\" columns show feature visualizations from the AVMoE method. This comparison highlights how AVMoE improves feature representation.", "section": "F Feature Visualization"}]