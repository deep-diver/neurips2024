[{"figure_path": "bFoQXD7Uls/figures/figures_1_1.jpg", "caption": "Figure 1: The memory overhead for backpropagation on a single layer consists of storing the intermediate activations and the weights. (a) demonstrates that PEFT methods can reduce the memory by using cheap low-rank adapters. (b) VeLoRA additionally compresses the saved intermediate activations to further reduce the memory usage.", "description": "This figure shows a comparison of memory usage for backpropagation between traditional methods and VeLoRA.  Panel (a) illustrates how Parameter-Efficient Fine-Tuning (PEFT) methods reduce memory by using low-rank adapters. Panel (b) demonstrates that VeLoRA further reduces memory by compressing intermediate activations during both the forward and backward passes. This compression is achieved through a rank-1 projection of sub-tokens, reducing the memory footprint needed to store intermediate activation tensors.", "section": "3 Method"}, {"figure_path": "bFoQXD7Uls/figures/figures_2_1.jpg", "caption": "Figure 2: (a) Stable rank for the input activations using a different number of groups, with = 1 indicating no sub-division of the tokens into smaller sub-tokens. (b) Approximate probability of the feature similarity diverging by at least k. (c) visualisation the rank-1 projection of sub-tokens.", "description": "This figure shows three subplots that illustrate different aspects of the VeLoRA algorithm. Plot (a) shows how the stable rank of input activations changes with the number of groups (i.e., the number of sub-tokens). Plot (b) shows the probability that the gradient similarity between two sub-tokens will diverge by at least k, as a function of the standard deviation of the angles between the sub-tokens and the projection vector. Plot (c) provides a visual representation of the rank-1 projection of sub-tokens.", "section": "Method"}]