[{"figure_path": "bFoQXD7Uls/tables/tables_4_1.jpg", "caption": "Table 1: Results on a subset of the VTAB-1k benchmark. All methods use a ViT-Base-224/16 model pre-trained on ImageNet-21k. The batch sizes and ranks are the same across all tasks.", "description": "This table presents the results of different methods on a subset of the VTAB-1k benchmark.  All methods utilize a ViT-Base-224/16 model pre-trained on ImageNet-21k, ensuring consistency. Batch sizes and ranks remain constant across all tasks for a fair comparison. The table showcases the performance of various approaches (Full tuning, Full tuning + VeLoRA, Linear probing, Linear probing + VeLoRA, Hydra, Hydra + VeLoRA, LoRA, LoRA + VeLoRA) on several image classification tasks, highlighting accuracy and memory usage.", "section": "4.2 Vision experiments"}, {"figure_path": "bFoQXD7Uls/tables/tables_6_1.jpg", "caption": "Table 1: Results on a subset of the VTAB-1k benchmark. All methods use a ViT-Base-224/16 model pre-trained on ImageNet-21k. The batch sizes and ranks are the same across all tasks.", "description": "This table presents the results of various methods on a subset of the VTAB-1k benchmark.  All methods use the same ViT-Base-224/16 model, pre-trained on ImageNet-21k, and maintain consistent batch sizes and ranks across all tasks.  The table shows the performance of different methods (full tuning, full tuning + VeLoRA, linear probing, SSF, SSF + VeLoRA, Hydra, Hydra + VeLoRA, LoRA, and LoRA + VeLoRA) on various vision tasks categorized into Natural, Specialized, and Structured, and indicates the GPU memory usage for each method.", "section": "4.2 Vision experiments"}, {"figure_path": "bFoQXD7Uls/tables/tables_6_2.jpg", "caption": "Table 2: Comparison of our method with full fine-tuning, GaLore and LORA on GLUE benchmark using pre-trained ROBERTa-Base. Our method reaches the best overall results while showing significant memory improvements, especially compared to GaLore. We bold the best results from the considered PEFT methods. The GPU memory is measured on-device.", "description": "This table compares the performance of VeLoRA against full fine-tuning, GaLore, and LoRA on the GLUE benchmark using a pre-trained ROBERTa-Base model.  It shows that VeLoRA achieves the highest average accuracy while using significantly less GPU memory, particularly when compared to GaLore.", "section": "4 Comparison with the state-of-the-art"}, {"figure_path": "bFoQXD7Uls/tables/tables_7_1.jpg", "caption": "Table 3: Mean 5-shot MMLU test accuracy for LLaMA models finetuned with adapters on Alpaca. The GPU memory estimate consists of the frozen weights, trainable adapters, and input activations.", "description": "This table presents the results of fine-tuning various sizes of LLaMA models using different methods (LoRA with BFloat16, LoRA with Float4, QLoRA, and VeLORA).  For each method, it reports the Alpaca accuracy and GPU memory usage.  The memory estimate includes the frozen weights, trainable adapters, and input activations.  VeLORA demonstrates memory efficiency while maintaining competitive accuracy compared to other methods.", "section": "4.4 Scaling up to LLAMA"}, {"figure_path": "bFoQXD7Uls/tables/tables_7_2.jpg", "caption": "Table 4: Comparison with low-rank algorithms on pre-training various sizes of LLaMA models on C4 dataset. Validation perplexity is reported, along with the on-device GPU memory usage.", "description": "This table compares the performance of different low-rank algorithms (GaLore, LoRA, FLORA, and VeLORA) against a full-rank baseline in pre-training LLaMA models on the C4 dataset.  It shows validation perplexity (a lower score indicates better performance) and the GPU memory usage for each method across two model sizes (60M and 130M parameters).  The table also provides the token-to-dimension ratio (r/dmodel) and the total number of training tokens for each model size.", "section": "4 Comparison with the state-of-the-art"}, {"figure_path": "bFoQXD7Uls/tables/tables_8_1.jpg", "caption": "Table 5: All three ablations are done using LLama-7B model. (a) VeLoRA has no loss in performance when trained for fewer or more training epochs than QLoRA despite both reducing the memory footprint. (b) Importance in choosing the correct number of sub-token size to find an optimal memory v.s. accuracy trade-off. Using a GPU memory estimate for the input activations only. (c) Ablating various initialisation strategies for a rank-1 projection and with M = D / 32.", "description": "This table presents ablation studies on the VeLoRA model using a 7B parameter LLaMA model.  It shows the impact of training epochs on accuracy and memory, the effect of sub-token size on model performance, and the results of different initialization strategies for the rank-1 projection.  The key finding is that VeLoRA maintains competitive accuracy while significantly reducing memory.", "section": "5 Ablations Studies"}, {"figure_path": "bFoQXD7Uls/tables/tables_8_2.jpg", "caption": "Table 6: Memory v.s. accuracy trade-off for VeLORA on different layers. We use a LLaMA-7B trained on alpaca and evaluated on MMLU. We report the GPU memory estimate from the input activations only.", "description": "This table shows the results of an ablation study on the placement of VeLORA within different layers of a LLaMA-7B model.  It examines the impact of applying VeLORA to the Query, Key, Value, and Down projection layers on both memory usage (in GB) and the model's accuracy (Acc) on the MMLU benchmark. The experiment uses a LLaMA-7B model trained on the Alpaca dataset.  Only the memory usage of the input activations is considered for these results. The first row represents a baseline model without any VeLORA application, while subsequent rows demonstrate the effects of applying VeLORA to different combinations of layers.", "section": "5.4 Choice of layers"}, {"figure_path": "bFoQXD7Uls/tables/tables_9_1.jpg", "caption": "Table 7: On-device training time and memory costs for pre-training LLaMA. Unlike VeLoRA, gradient checkpointing incurs a much more significant training time overhead. Batch size of 1. Our method is 17%, 30%, 30%, 47% faster than gradient checkpointing in LLama 60M, 130M, 7B and 13B. We see that the larger the model, the larger the time performance gain.", "description": "This table compares the training time and memory consumption of VeLoRA against gradient checkpointing for pre-training LLAMA models of different sizes (60M, 130M, 7B, and 13B parameters). It highlights that VeLoRA not only reduces memory usage but also significantly speeds up training compared to gradient checkpointing.", "section": "5.5 Comparison with gradient checkpointing"}, {"figure_path": "bFoQXD7Uls/tables/tables_13_1.jpg", "caption": "Table 8: Hyperparameters of fine-tuning RoBERTa base.", "description": "This table presents the hyperparameters used for fine-tuning the RoBERTa base model on the GLUE benchmark.  It details the batch size, number of epochs, learning rate, and maximum sequence length used for each of the eight tasks within the GLUE benchmark: MNLI, SST-2, MRPC, CoLA, QNLI, QQP, RTE, and STS-B.", "section": "7.2 Roberta Experiments"}, {"figure_path": "bFoQXD7Uls/tables/tables_13_2.jpg", "caption": "Table 9: The optimal task-specific hyper-parameters proposed for the Hydra method [21].", "description": "This table presents the optimal hyperparameters (scale and dropout values) used for the Hydra method in the VTAB-1k experiments, tailored for each specific task or dataset. These values were obtained through a process not described in detail in the paper.  The table aids reproducibility and shows that the experimenters used task-specific tuning for Hydra, making it different than other PEFT methods used, in which task-specific tuning was not performed.", "section": "4.1 Implementation details"}, {"figure_path": "bFoQXD7Uls/tables/tables_14_1.jpg", "caption": "Table 1: Results on a subset of the VTAB-1k benchmark. All methods use a ViT-Base-224/16 model pre-trained on ImageNet-21k. The batch sizes and ranks are the same across all tasks.", "description": "This table presents the results of different methods on a subset of the VTAB-1k benchmark.  All methods use the same ViT-Base-224/16 model, pre-trained on ImageNet-21k, ensuring consistency. The batch sizes and ranks are identical across all tasks for fair comparison. The table shows the performance of each method across various vision tasks, allowing for a comprehensive evaluation of their effectiveness.", "section": "4.2 Vision experiments"}]