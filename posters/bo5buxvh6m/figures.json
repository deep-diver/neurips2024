[{"figure_path": "bO5bUxvH6m/figures/figures_2_1.jpg", "caption": "Figure 1: Latent hierarchical graphs. The dashed circle in (a) indicates that the continuous variable c can be viewed as an exogenous variable. Dashed edges in (b) indicate potential statistical dependence.", "description": "This figure illustrates the hierarchical structure of the proposed latent variable model used for learning discrete concepts from high-dimensional data.  Panel (a) shows the subspaces of the model: a continuous subspace representing attributes (c) and a discrete subspace representing concepts (d), which jointly determine the observed variables (x). Panel (b) demonstrates a 'bottom level' representation where the discrete concepts (d) directly influence the observed variables (x). Finally, panel (c) shows a full hierarchical model where high-level concepts (z) influence the low-level concepts (d), which in turn influence the observed variables (x). This hierarchical structure captures dependencies between concepts at different levels of abstraction.", "section": "3 Discrete Hierarchical Models"}, {"figure_path": "bO5bUxvH6m/figures/figures_5_1.jpg", "caption": "Figure 2: Graphical comparison. Tree Structures permit one undirected path between any two variables. Multi-level DAGs require partitioning variables into levels with edges only between adjacent levels. Our conditions allow multiple paths between variables across levels and include non-leaf observed variables.", "description": "This figure compares three different types of graphical models used to represent hierarchical relationships between variables.  (a) shows a tree structure, where there is only one path between any two nodes. (b) shows a multi-level DAG, where nodes are arranged in levels and connections exist only between adjacent levels. (c) shows the model proposed in the paper, which is more flexible than the previous models. It allows multiple paths between variables, regardless of the level they are in, and it can include non-leaf nodes in the graph.  This increased flexibility allows the model to capture more complex relationships in the data.", "section": "4 Hierarchical Model Identification"}, {"figure_path": "bO5bUxvH6m/figures/figures_8_1.jpg", "caption": "Figure 3: Diffusion models estimate the latent hierarchical model. Different noise levels correspond to different concept levels. To avoid cluttering, we leave out vector quantization.", "description": "This figure illustrates how a latent diffusion model can be interpreted through the lens of a hierarchical concept model.  Different noise levels in the diffusion process correspond to different hierarchical levels in the concept model. The encoder of the diffusion model at a particular noise level extracts a representation that corresponds to a particular level of concepts. At higher noise levels, low-level concepts are lost, resulting in a representation that focuses on higher-level, more abstract concepts. The decoder reconstructs the original representation using the compressed representation and optional text information.", "section": "6 Interpretations of Latent Diffusion"}, {"figure_path": "bO5bUxvH6m/figures/figures_9_1.jpg", "caption": "Figure 4: Recovering concepts and their relationships from LD. (a) The final recovered concept graph among concepts \u201cdog\u201d, \u201ctree\u201d, \u201ceyes\u201d, \u201cears\u201d, \u201cbranch\u201d, and \u201cleaf\u201d. (b) Identifying causal links through \u201cinterventions\u201d. For example, we compare two prompts that vary in \u201cdog\u201d: \u201ca dog with wide eyes and a wilting tree with short branches, in a cartoon style\u201d and \u201ca big dog with wide eyes and a wilting tree with short branches, in a cartoon style\u201d. We observe significant changes in \u201ceyes\u201d but not in \u201cbranch\u201d, indicating a causal link between \u201cdog\u201d and \u201ceyes\u201d but not between \u201cdog\u201d and \u201cbranch\u201d. (c) Identifying concept levels by the last effective diffusion step. For example, we use the base prompt \u201ca tree with long branches, in a cartoon style\u201d and prepend \u201cdog\u201d at steps 0, 5, and 15. Only injecting \u201cdog\u201d at step 0 works. Similarly, injecting \u201cwide eyes\u201d works at both steps 0 and 5, indicating that \u201cdog\u201d is a higher-level concept than \u201ceyes\u201d.", "description": "This figure shows how the authors recovered concepts and their relationships from a Latent Diffusion model.  Part (a) displays the resulting hierarchical graph of concepts. Part (b) demonstrates how interventions on higher-level concepts (e.g., adding 'dog') affect lower-level concepts ('eyes', 'ears'), revealing causal relationships. Part (c) illustrates how the timing of concept injection during the diffusion process reveals the hierarchical order of concepts (higher-level concepts injected earlier).", "section": "7.1 Discovering Hierarchical Concept Structures from Diffusion Models"}, {"figure_path": "bO5bUxvH6m/figures/figures_9_2.jpg", "caption": "Figure 5: Semantic latent space. We modify the diffusion model's representation (UNet encoder's output) along principal directions at steps T and 0.6T. Structure changes indicate the semantics of the representation and manipulation at the early time T induces global shifts. See more examples in Figure A9.", "description": "This figure shows how modifying the diffusion model's representation at different times (early vs late) affects the generated image's semantics.  Early modifications result in global changes (e.g., breed, species, gender), while later changes produce more localized effects (e.g., accessories, minor features). This visually demonstrates the hierarchical structure of concepts within the latent space, where early stages represent higher-level concepts and later stages represent lower-level details.  The results support the paper's interpretation of latent diffusion models as hierarchical concept learners.", "section": "7.2 Diffusion Representation as Concept Embeddings"}, {"figure_path": "bO5bUxvH6m/figures/figures_23_1.jpg", "caption": "Figure 1: Latent hierarchical graphs. The dashed circle in (a) indicates that the continuous variable c can be viewed as an exogenous variable. Dashed edges in (b) indicate potential statistical dependence.", "description": "This figure illustrates the concept of a hierarchical model with continuous and discrete latent variables.  Panel (a) shows the overall model structure, illustrating continuous variables 'c' and discrete variables 'd'.  Panel (b) focuses on the discrete latent variables 'd' as the leaves of the hierarchy.  The dashed lines in (b) represent potential statistical dependence between the concepts represented by the discrete variables, which are further explained by a higher-level concept in (c), showing how concepts are hierarchically related. ", "section": "3 Discrete Hierarchical Models"}, {"figure_path": "bO5bUxvH6m/figures/figures_24_1.jpg", "caption": "Figure 1: Latent hierarchical graphs. The dashed circle in (a) indicates that the continuous variable c can be viewed as an exogenous variable. Dashed edges in (b) indicate potential statistical dependence.", "description": "This figure illustrates the data generating process and the model used in the paper.  Panel (a) shows that the high-dimensional continuous observed variables (x) are generated from discrete latent variables (d) and continuous latent variables (c). Panel (b) zooms in to the 'bottom level' of the model, showing the relationship between the discrete latent variables and the continuous observed variables. Panel (c) shows a hierarchical model composed of both high-level and low-level discrete latent variables. The hierarchical model describes the dependence among different abstraction levels of concepts. ", "section": "3 Discrete Hierarchical Models"}, {"figure_path": "bO5bUxvH6m/figures/figures_25_1.jpg", "caption": "Figure 1: Latent hierarchical graphs. The dashed circle in (a) indicates that the continuous variable c can be viewed as an exogenous variable. Dashed edges in (b) indicate potential statistical dependence.", "description": "This figure illustrates three different graphical representations of latent hierarchical models.  (a) shows the general structure, with continuous and discrete subspaces that are parents to the observed variables x. (b) zooms in on the \"bottom\" level to emphasize the relationship between discrete latent variables d and the continuous observed variables x.  Finally, (c) shows a full hierarchical model where the discrete variables are connected via higher-level concepts, allowing for complex dependencies among the concepts.", "section": "3 Discrete Hierarchical Models"}, {"figure_path": "bO5bUxvH6m/figures/figures_26_1.jpg", "caption": "Figure 2: Graphical comparison. Tree Structures permit one undirected path between any two variables. Multi-level DAGs require partitioning variables into levels with edges only between adjacent levels. Our conditions allow multiple paths between variables across levels and include non-leaf observed variables.", "description": "This figure compares three different types of graphical models representing hierarchical relationships among variables: trees, multi-level directed acyclic graphs (DAGs), and the model proposed in the paper.  Trees allow only one path between any two variables, while multi-level DAGs are more structured and restrict edges to exist only between adjacent levels. The authors' model is more flexible, enabling multiple paths between variables and allowing non-leaf nodes (observed variables) at any level in the hierarchy.", "section": "4.3 Hierarchical Model Identification"}, {"figure_path": "bO5bUxvH6m/figures/figures_27_1.jpg", "caption": "Figure 2: Graphical comparison. Tree Structures permit one undirected path between any two variables. Multi-level DAGs require partitioning variables into levels with edges only between adjacent levels. Our conditions allow multiple paths between variables across levels and include non-leaf observed variables.", "description": "This figure compares three types of graphical models for representing latent variables: trees, multi-level directed acyclic graphs (DAGs), and the model proposed in the paper.  Trees only allow one path between any two nodes, which is too restrictive.  Multi-level DAGs stratify variables into levels, with connections only between adjacent levels, which is also restrictive. The proposed model offers greater flexibility in the relationships between variables, allowing for multiple paths and non-leaf observed variables.", "section": "4.3 Hierarchical Model Identification"}, {"figure_path": "bO5bUxvH6m/figures/figures_27_2.jpg", "caption": "Figure A4: Sparsity patterns in latent diffusion models' attention. We compute the proportions of the attention scores lower than a fixed threshold over the entire model. We can observe that the sparsity increases greatly towards small timesteps, i.e., the lower levels of the hierarchical model, which verifies our theory.", "description": "This figure visualizes the attention sparsity of a latent diffusion model over diffusion steps and specific attention patterns. The results show that the sparsity increases as the generative process progresses, which reflects that the connectivity between the hierarchical level and the bottom concept level becomes sparse and more local as we march down the hierarchical structure. This observation supports the theory proposed in the paper.", "section": "A5 Real-world Experiments"}, {"figure_path": "bO5bUxvH6m/figures/figures_28_1.jpg", "caption": "Figure A5: Hierarchical Concept Ordering. We inject concepts of distinct abstraction levels into the generating process at different time steps. In the top row, the concept injection follows the hierarchical order, which renders injected concepts faithfully. The bottom row reverses the hierarchical order and cannot incorporate concepts properly. More examples in Figure A8.", "description": "This figure demonstrates the hierarchical nature of concepts in a latent diffusion model. Injecting high-level concepts early in the generation process, followed by low-level concepts, results in images that faithfully reflect all injected concepts. Reversing this order leads to incomplete or inaccurate image generation. This supports the hierarchical model's structure, with higher-level concepts influencing lower-level ones.", "section": "A5 Real-world Experiments"}, {"figure_path": "bO5bUxvH6m/figures/figures_29_1.jpg", "caption": "Figure A6: Concepts have varying levels of sparsity. We show that concepts of various abstraction levels correspond to different sparsity levels. For instance, bright weather is appropriately conveyed by a rank-2 LoRA and higher-rank LoRAs alter the background. Inadequate ranks fail to capture the concept faithfully and unnecessary ranks entangle the target concept with other attributes.", "description": "This figure shows the results of an experiment where different ranks of LoRA (Low-Rank Adaptation) were used to modify images with specific concepts.  The experiment demonstrates that the appropriate rank of LoRA is crucial for effectively and faithfully modifying images with the desired concept without introducing unwanted artifacts.  Higher ranks, while potentially offering more detail, may introduce unwanted alterations or distortions.", "section": "A5.4 Causal Sparsity for Concept Extraction"}, {"figure_path": "bO5bUxvH6m/figures/figures_30_1.jpg", "caption": "Figure A7: CLIP/LPIPS evaluation. We evaluate our approach and baselines at individual rank constraints. A high CLIP score is favorable as it indicates semantic alignment. A low LPIPS score is more favorable as it indicates minimal excessive changes. We compare our method \"sparse\" with the optimal fixed rank setting on each concept. For instance, \u201ccastle opt: rank4_scale3\" indicates that the optimal setting for the concept \u201ccastle\u201d is the LoRA of rank 4 and scale 3. With a adaptive rank selection, our approach outperforms or keeps up with the optimal fixed setting across different concepts. We repeat each training over three random seeds.", "description": "This figure shows the results of CLIP and LPIPS evaluations for different concepts using various rank settings.  It compares the performance of a sparse approach with an optimal fixed rank approach, highlighting the effectiveness of adaptive rank selection in achieving high semantic alignment with minimal image alterations.", "section": "A5 Real-world Experiments"}, {"figure_path": "bO5bUxvH6m/figures/figures_31_1.jpg", "caption": "Figure A5: Hierarchical Concept Ordering. We inject concepts of distinct abstraction levels into the generating process at different time steps. In the top row, the concept injection follows the hierarchical order, which renders injected concepts faithfully. The bottom row reverses the hierarchical order and cannot incorporate concepts properly. More examples in Figure A8.", "description": "This figure shows experiments on injecting concepts into a diffusion model at different time steps. The top row shows that injecting high-level concepts first and then low-level concepts produces images that successfully integrate all concepts. The bottom row reverses the order, showing that injecting low-level concepts first results in failure to include the high-level concept in the generated image.  This demonstrates the hierarchical relationship between concepts in the model.", "section": "A5 Real-world Experiments"}, {"figure_path": "bO5bUxvH6m/figures/figures_31_2.jpg", "caption": "Figure A9: More examples for Figure 5.", "description": "This figure provides additional examples to illustrate the concept of editing latent representations at different time steps in latent diffusion models. The top two rows show changes in image generation when manipulating image representations at early versus late diffusion steps. The changes induced by manipulating early steps correspond to shifting global concepts, while changes in later steps correspond to finer-grained changes. The bottom two rows demonstrate similar effects for different image types.", "section": "A5.5 More Examples"}]