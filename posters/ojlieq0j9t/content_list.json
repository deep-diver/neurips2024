[{"type": "text", "text": "Shaping the distribution of neural responses with interneurons in a recurrent circuit model ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "David Lipshutz Center for Computational Neuroscience, Flatiron Institute dlipshutz@flatironinstitute.org ", "page_idx": 0}, {"type": "text", "text": "Eero P. Simoncelli Center for Computational Neuroscience, Flatiron Institute Center for Neural Science, New York University eero.simoncelli@nyu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Efficient coding theory posits that sensory circuits transform natural signals into neural representations that maximize information transmission subject to resource constraints. Local interneurons are thought to play an important role in these transformations, shaping patterns of circuit activity to facilitate and direct information flow. However, the relationship between these coordinated, nonlinear, circuit-level transformations and the properties of interneurons (e.g., connectivity, activation functions, response dynamics) remains unknown. Here, we propose a normative computational model that establishes such a relationship. Our model is derived from an optimal transport objective that conceptualizes the circuit\u2019s input-response function as transforming the inputs to achieve an efficient target response distribution. The circuit, which is comprised of primary neurons that are recurrently connected to a set of local interneurons, continuously optimizes this objective by dynamically adjusting both the synaptic connections between neurons as well as the interneuron activation functions. In an application motivated by redundancy reduction theory, we demonstrate that when the inputs are natural image statistics and the target distribution is a spherical Gaussian, the circuit learns a nonlinear transformation that significantly reduces statistical dependencies in neural responses. Overall, our results provide a framework in which the distribution of circuit responses is systematically and nonlinearly controlled by adjustment of interneuron connectivity and activation functions. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The problem of transforming a signal into a representation with a given target distribution (or within a target set of distributions) is a classical problem whose origins can be traced back more than two centuries [1]. Many methods in statistics, signal processing and machine learning can be interpreted within the context of this problem. For example, data whitening is a common preprocessing step that linearly transforms a signal to have identity covariance [2]. Independent component analysis [ICA; 3] is a signal processing method that linearly transforms a signal so as to minimize higher-order statistical dependencies in addition to removing second-order dependencies. Nonlinear transformations of skewed or heavy-tailed data to approximately Gaussianize their distribution can facilitate statistical analyses [4, 5]. Machine learning methods for density estimation such as Gaussianization [6\u201310] and normalizing flows [11\u201313] nonlinearly transform high-dimensional signals with complex densities into more tractable representations with approximately Gaussian densities. ", "page_idx": 0}, {"type": "image", "img_path": "ojLIEQ0j9T/tmp/7036126f251a5b6669667b1a606c972108c074fe2ae8e3a536755c7e495cca58.jpg", "img_caption": ["Figure 1: Schematic of a recurrent circuit with $N=2$ primary neurons and $K=3$ interneurons. Left: Scatter plot of a 2D input signals ${{\\bf s}}=(s_{1},s_{2})$ with s $\\sim p_{\\mathbf{s}}$ . Center: Primary neurons (black circles), with outputs $\\mathbf{r}=\\left(r_{1},r_{2}\\right)$ , receive external feedforward inputs, s, and recurrent feedback from an auxiliary population of interneurons (purple circles), $-\\mathbf{W}\\mathbf{n}$ , where $\\bar{\\mathbf{n}}=(n_{1},n_{2},n_{3})$ are the interneuron outputs. Projection vectors $\\left\\{\\mathbf{w}_{1},\\mathbf{w}_{2},\\mathbf{w}_{3}\\right\\}$ encode feedforward synaptic weights connecting primary neurons to interneurons $i={1,2,3}$ , with symmetric feedback connections. Inset: The $i^{\\mathrm{th}}$ interneuron (here $i\\,=\\,1$ ) receives weighted inputs $z_{i}:=\\mathbf{r}\\cdot\\mathbf{w}_{i}$ , which is fed through the activation function $f(\\theta_{i},\\cdot)$ and scaled by the gain $g_{i}$ to generate the output $n_{i}:=g_{i}f(\\theta_{i},z_{i})$ . Right: Scatter plot of the 2D circuit responses $\\mathbf{r}=\\left(r_{1},r_{2}\\right)$ with $\\mathbf{r}\\sim p_{\\mathrm{target}}$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "This problem may also lie at the core of sensory processing. Efficient coding theory posits that sensory systems maximize the information they transmit about sensory signals to downstream areas subject to resource constraints [14\u201317]. In one instantiation of this theory, the redundancy reduction hypothesis posits that sensory circuits transform natural signals into representations to minimize or eliminate statistical dependencies between coordinates, essentially producing factorized response distributions [14, 15, 18]. In a separate, but related, instantiation, sparse coding theory posits that population responses are optimized for sparsity [19\u201321], which is naturally interpreted as a constraint on the shape of the distribution of responses. In another line of theoretical work, sensory representations are posited to maximize the Fisher information about the inputs [22\u201324], which can be interpreted as a statement about the joint distribution of the inputs and responses. Importantly, each of these theories can be formulated as a transformation of the signal into a representation with target distribution that is optimal under information theoretic and metabolic constraints. However, it is not clear how neural circuits learn or implement these potentially nonlinear transformations. ", "page_idx": 1}, {"type": "text", "text": "Neural circuits are typically comprised of populations of primary (excitatory) neurons and local (inhibitory) interneurons. Extensive experimental measurements have led to the idea that local interneuron populations allow neural circuits to flexibly shape patterns of primary neuron responses so as to coordinate information flow [25\u201331]. Consequently, local interneurons are natural candidate substrates responsible for shaping circuit responses into efficient representations. However, the precise relationship between the physiological and anatomical properties of local interneurons and the coordinated response properties of populations of primary neurons remains unclear. ", "page_idx": 1}, {"type": "text", "text": "Several normative mechanistic models have been proposed to explain how neural circuits can linearly transform their inputs into a representation whose distribution lies within a target set (e.g., the set of distributions with identity covariance) [32\u201335]. These models are derived from optimization objectives for linear redundancy reduction, including (adaptive) decorrelation and ICA, and the circuit parameters (e.g., gains, synaptic weights) are optimized to match the data distribution. The optimization steps correspond to processes such as gain modulation and synaptic plasticity, thus demonstrating how adjustments of circuit parameters according to local signals can optimize a global, circuit-level objective for redundancy reduction of the neural responses. While linear transformations can remove second-order statistical dependencies, they cannot remove higher-order statistical dependencies that are prominent in sensory signals [7]. Furthermore, early sensory systems exhibit a host of prominent nonlinear response properties [36\u201338] that are not captured by these models. ", "page_idx": 1}, {"type": "text", "text": "Here, we seek a normative model of how circuits nonlinearly transform their inputs to produce responses with a (spherical) target distribution. Starting from an optimal transport objective for transforming the input signal into a neural representation with a given target distribution, we derive an algorithm (Alg. 1) that can be mapped onto a dynamical model of a neural circuit, Fig. 1. The circuit model is comprised of primary neurons that are recurrently connected to local interneurons and the circuit adapts its responses using a combination of Hebbian synaptic plasticity and interneuron adaptation. Complementary computational roles for Hebbian plasticity and interneuron adaptation emerge from this analysis: (i) synapses are updated according to a Hebbian learning rule to identify projections of the signal that are least aligned with the target distribution (essentially, projection pursuit [39]); (ii) interneuron gains and nonlinear activation functions are adjusted to transform the marginal circuit responses along directions defined by the synaptic weights. Together these operations transform the distribution of responses to approximate the target distribution. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "As a primary test case motivated by redundancy reduction theory, we apply our algorithm to the case that the inputs are derived from natural images (using local oriented fliters qualitatively similar to those found in the primary visual cortex) and the target distribution is the spherical Gaussian.1 We find that the algorithm learns a nonlinear transformation that approximately Gaussianizes the responses and significantly reduces statistical dependencies between coordinates. Overall, our results demonstrate how local interneurons may adjust their connectivity and response properties to nonlinearly reshape the distribution of circuit responses, thus facilitating efficient transmission of information. ", "page_idx": 2}, {"type": "text", "text": "2 Circuit objective ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider a neural circuit with $N\\geq2$ primary neurons that transforms input signals $\\mathbf{s}\\in\\mathbb{R}^{N}$ , which are distributed according to a density $p_{\\mathbf{s}}$ , into circuit responses $\\mathbf{r}\\in\\mathbb{R}^{N}$ (Fig. 1). The inputs may represent a direct sensory input (e.g., the rate at which photons are absorbed by a cone) or the weighted sum of multiple inputs (e.g., the postsynaptic current). The responses r represent the firing rate (or the logarithm of the firing rate) of the neuron. For simplicity, we assume that the circuit responses are a deterministic function of the input signals; that is, $\\mathbf{r}=T(\\mathbf{s})$ for a function $T:\\mathbb{R}^{N}\\mapsto\\mathbb{R}^{N}$ . ", "page_idx": 2}, {"type": "text", "text": "2.1 Optimal transport objective ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We explore the possibility that the circuit objective is to transform its inputs s so that the circuit responses $\\mathbf{r}=T(\\mathbf{s})$ follow a (spherical) target distribution $p_{\\mathrm{target}}$ while minimizing the $L^{2}$ -distance between responses and input signals. Mathematically, this corresponds to an optimal transport problem [41] ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{T}\\mathbb{E}\\left[\\|T(\\mathbf{s})-\\mathbf{s}\\|^{2}+\\lambda\\|T(\\mathbf{s})\\|^{2}\\right]\\quad\\mathrm{such~that}\\quad T(\\mathbf{s})\\sim p_{\\mathrm{target}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the minimization is over a suitable class of functions $T$ , $\\lambda\\in\\mathbb R$ is a regularizing term and, unless otherwise noted, expectations are over the input distribution $p_{\\mathbf{s}}$ . Note that the choice of $\\lambda\\in\\mathbb{R}$ does not affect the optimal solution as $\\mathbb{E}[\\lVert T(\\mathbf{s})\\rVert^{2}]$ is fixed provided $T(\\mathbf{s})\\sim p_{\\mathrm{target}}$ ; however, it will affect the optimization algorithm. Assuming $p_{\\mathbf{s}}$ is sufficiently regular, the minimum in eq. (1) is the squared Wasserstein-2 distance between $p_{\\mathbf{s}}$ and $p_{\\mathrm{target}}$ and the input-response transformation $T$ is the so-called optimal transport plan that maps the input distribution $p_{\\mathbf{s}}$ to the target distribution $p_{\\mathrm{target}}$ . ", "page_idx": 2}, {"type": "text", "text": "Our goal is to derive an online algorithm for optimizing the objective in eq. (1) that can be implemented in a neural circuit model. We accomplish this by defining a distance between the response distribution $p_{\\mathbf{r}}$ and the target distribution $p_{\\mathrm{target}}$ that can be estimated in a neural circuit, and then solving the optimization problem by incorporating this measure as a constraint, using the method of Lagrange multipliers. ", "page_idx": 2}, {"type": "text", "text": "2.2 Measuring the discrepancy between the response distribution and the target distribution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Common candidates for measuring the discrepancy between two distributions include KullbackLeibler (KL) divergence or integral probability metrics [42]. The former typically require numerous samples to estimate the density $p_{\\mathbf{r}}$ , whereas neural circuits must operate in the online setting without access to the full history of their responses. Therefore, we use an integral probability metric that quantifies the difference between the random variables when evaluated using constraint functions that can be rapidly estimated online. ", "page_idx": 2}, {"type": "text", "text": "We restrict our solution to use constraint functions that compare the marginal response distributions to the marginals of $p_{\\mathrm{target}}$ , denoted $p_{\\mathrm{marginal}}$ , which are all equal under our assumption that $p_{\\mathrm{target}}$ is spherical. Such constraint functions are well matched to signals that are linear mixtures of independent sources, as in generative models for ICA. Even when the signal statistics are not generated according to a linear mixture model, the Cram\u00e9r and Wold theorem [43] suggests that transforming sufficiently many marginals of the response distribution may effectively transform the multivariate response distribution (though the number of marginals required may be quite large). Our motivation for measuring marginal response distributions is due, in part, by our goal of modeling local interneurons, whose inputs are naturally modeled as weighted sums of primary neuron responses (i.e., their input distributions are marginals of the primary responses). ", "page_idx": 3}, {"type": "text", "text": "To compare the marginal response distributions, we first select a finite set of directions defined by $K\\ge1$ unit vectors $\\mathbf{\\bar{w}}_{1},\\dots,\\mathbf{\\bar{w}}_{K}\\in\\mathbb{R}^{N}$ , which can be randomly sampled, chosen based on prior knowledge of the signal statistics, or learned from data using projection pursuit [39]. We then choose a class of scalar functions $\\{h(\\theta,\\cdot)\\}$ parameterized by $\\theta$ , which defines a semi-metric between the marginal of $\\mathbf{r}$ in the direction w and $p_{\\mathrm{marginal}}$ to be max\u03b8 $\\left|\\mathbb{E}[\\phi(\\theta,\\mathbf{r}\\cdot\\mathbf{w})]\\right|$ , where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi(\\theta,z):=h(\\theta,z)-\\mathbb{E}_{z\\sim p_{\\mathrm{marginal}}}[h(\\theta,z)].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For example, when $\\{h(\\theta,\\cdot)\\}$ parameterizes all Lipschitz-1 (indicator) functions, this induces the Wasserstein-1 (total variation) distance between the marginal response distribution and $p_{\\mathrm{marginal}}$ . Given directions $\\mathbf{w}_{1},\\dots,\\mathbf{w}_{K}$ and constraint functions $\\{h\\bar{(\\theta,\\cdot)}\\}$ , we express the distance between the response density $p_{\\mathbf{r}}$ and the standard Gaussian distribution as the sum of the marginal distances: ", "page_idx": 3}, {"type": "equation", "text": "$$\nd_{\\mathbf{W},\\phi}(p_{\\mathbf{r}}):=\\sum_{i=1}^{K}\\operatorname*{max}_{\\theta_{i}}|\\mathbb{E}_{\\mathbf{r}\\sim p_{\\mathbf{r}}}\\left[\\phi(\\theta_{i},\\mathbf{r}\\cdot\\mathbf{w}_{i})\\right]|,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{W}:=[\\mathbf{w}_{1},\\dots,\\mathbf{w}_{K}]$ is the $N\\times K$ matrix of concatenated unit vectors.2 ", "page_idx": 3}, {"type": "text", "text": "How do we choose the constraint functions $\\{h(\\theta,\\cdot)\\}?$ In general, the choice should be well-suited to the input distribution $p_{\\mathbf{s}}$ and the target distribution $p_{\\mathbf{r}}$ . For example, consider the simple case when the inputs follow a centered Gaussian distribution with unknown covariance structure, and the target distribution is the spherical Gaussian distribution $\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . The Wasserstein-2 distance between a tsmeeracrmtigsoi nno af4 lt .hd2i,e s tdwriiefb fceurtoienonsnic deo efbr  teath ewp eianeranp umttheset r aisnce dcc otlahnseds   smtmaoonmtdieavrnadtts e,nd s obor yma  anqla utdauidrstrarlai tsbiicug tfinuoanln c ${\\mathcal{N}}(0,1)$ $\\begin{array}{r}{h(z)=\\frac{1}{2}z^{2}}\\end{array}$ xspurfefsicseesd.  Iinn ", "page_idx": 3}, {"type": "text", "text": "2.3 Optimization using Lagrange multipliers ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We replace the condition $\\mathbf{r}\\sim p_{\\mathrm{target}}$ in eq. (1) with the condition maxW $d{\\bf w}_{,\\phi}(p_{\\bf r})=0$ , which we enforce using Lagrange multipliers. This results in the minimax optimization problem ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{W}}\\operatorname*{max}_{\\theta}\\operatorname*{max}_{\\mathbf{g}}\\mathbb{E}\\left[\\operatorname*{min}_{\\mathbf{r}}\\mathcal{L}(\\mathbf{W},\\theta,\\mathbf{g},\\mathbf{s},\\mathbf{r})\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{L}$ is defined by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{W},\\pmb{\\theta},\\mathbf{g},\\mathbf{s},\\mathbf{r}):=\\|\\mathbf{r}-\\mathbf{s}\\|^{2}+\\lambda\\|\\mathbf{r}\\|^{2}+\\sum_{i=1}^{K}g_{i}\\phi(\\theta_{i},\\mathbf{r}\\cdot\\mathbf{w}_{i}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $\\pmb{\\theta}~:=~(\\theta_{1},\\ldots,\\theta_{K})$ is the set of concatenated parameters, $\\textbf{g}:=\\ (g_{1},\\ldots,g_{K})$ is a $K$ - dimensional vector of Lagrange multipliers, and the circuit transform is defined by $T(\\mathbf{s})\\ =$ arg minr $\\mathcal{L}(\\mathbf{W},\\pmb{\\theta},\\mathbf{g},\\mathbf{s},\\mathbf{r})$ . The maximization over $\\mathbf{g}$ and $\\pmb{\\theta}$ effectively minimizes the distances between the marginal distributions of the responses $\\mathbf{r}$ along the directions $\\mathbf{w}_{1},\\dots,\\mathbf{w}_{K}$ and $p_{\\mathrm{marginal}}$ , whereas the maximization over the matrix W learns directions along which the marginals of s are least aligned with $p_{\\mathrm{marginal}}$ , essentially performing projection pursuit [39]. ", "page_idx": 3}, {"type": "text", "text": "3 Algorithm and circuit implementation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now derive an online gradient-based algorithm for optimizing the objective in eq. (2), then map the algorithm onto a recurrent neural circuit. Spiking activity operates on a much faster timescale than neural or synaptic adaptation mechanisms, so we assume that the neural activities equilibrate before the neural activations and synapses are updated. ", "page_idx": 4}, {"type": "text", "text": "3.1 Fast recurrent neural dynamics ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For each iteration, the circuit receives a stimulus s. The (discretized) recurrent neural response dynamics (Fig. 1) correspond to gradient-descent minimization of $\\mathcal{L}$ with respect to $\\mathbf{r}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{r}\\leftarrow\\mathbf{r}+\\eta_{r}\\left(\\mathbf{s}-\\mu\\mathbf{r}-\\sum_{i=1}^{K}n_{i}\\mathbf{w}_{i}\\right),\\qquad\\qquad\\qquad n_{i}=g_{i}f(\\theta_{i},z_{i}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\eta_{r}>0$ is a small constant, $\\mu:=1+\\lambda$ represents a leak term, $z_{i}:=\\mathbf{r}\\cdot\\mathbf{w}_{i}$ is the weighted input to the $i^{\\mathrm{th}}$ interneuron, $f(\\theta_{i},\\cdot):=\\partial\\phi(\\theta_{i},\\cdot)/\\partial z$ is the activation function, $g_{i}$ is a multiplicative gain that scales the output, and $n_{i}$ denotes the output. Notably, the interneuron activation response function $g_{i}f(\\theta_{i},\\cdot)$ is parameterized by $(g_{i},\\theta_{i})$ , which can vary across interneurons, so the interneuron responses are heterogeneous. For each $i$ , synaptic weights $\\mathbf{w}_{i}$ connect the primary neurons to the $i^{\\mathrm{th}}$ interneuron and symmetric weights $-\\mathbf{w}_{i}$ connect the $i^{\\mathrm{th}}$ interneuron to the primary neurons. From eq. (4), we see that the neural responses are driven by the signal s, a leak term $-\\mathbf{r}$ , and recurrent weighted feedback from the interneurons $-\\mathbf{W}\\mathbf{n}$ , where $\\mathbf{n}:=(n_{1},...,n_{K})$ . ", "page_idx": 4}, {"type": "text", "text": "Since the neural activities equilibrate before other updates are performed, the responses $\\mathbf{r}$ are a fixed point of $\\mathcal{L}(\\mathbf{W},\\pmb{\\theta},\\mathbf{g},\\mathbf{s},\\cdot)$ . In general, we do not have a closed-form expression for $\\mathbf{r}$ ; however, if $g_{i}\\geq0$ and $\\phi(\\theta_{i},\\cdot)$ are convex, then $\\mathcal{L}(\\mathbf{W},\\pmb{\\theta},\\mathbf{g},\\mathbf{s},\\cdot)$ is convex and we can express the transform as ", "page_idx": 4}, {"type": "equation", "text": "$$\nT(\\mathbf{s})=\\underset{\\mathbf{r}}{\\arg\\operatorname*{min}}\\,\\mathcal{L}(\\mathbf{W},\\boldsymbol{\\theta},\\mathbf{g},\\mathbf{s},\\mathbf{r}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In Appx. A, we show that the transform $T(\\cdot)$ is invertible whenever $g_{i}\\geq0$ and $\\phi(\\theta_{i},\\cdot)$ are convex and it defines a precise relationship between the input distribution $p_{\\mathbf{s}}$ and response distribution $p_{\\mathbf{r}}$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Gain modulation, activation function adaptation and Hebbian plasticity ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "After the neural activities reach equilibrium, we maximize $\\mathcal{L}$ by taking concurrent gradient-ascent steps with respect to $g_{i},\\theta_{i}$ and $\\mathbf{w}_{i}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta\\theta_{i}=\\eta_{\\theta}\\nabla_{\\theta}\\phi(\\theta_{i},z_{i}),\\qquad\\qquad\\Delta\\mathbf{w}_{i}=\\eta_{w}n_{i}\\mathbf{r},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\eta_{g},\\eta_{\\theta},\\eta_{w}\\,\\ge\\,0$ are the respective learning rates, which control the relative speeds of gain modulation, neural adaptation and synaptic plasticity, respectively. For example, at the extremes, we can $\\it{f l x}$ the gains, activation functions or synaptic weights by setting $\\eta_{g}=0$ , $\\eta_{\\theta}=0$ or $\\eta_{w}=0$ , respectively. Notably, while synaptic plasticity [46] and gain modulation [47] are well-studied circuit mechanisms that support learning and adaptation, adjustments of nonlinear neural activation functions are not as well established though there is some emerging evidence that neurons also adapt their activation functions in response to changes in their input statistics [48, 49]. ", "page_idx": 4}, {"type": "text", "text": "The circuit operates online and the updates are local in the sense that the updates to the gain and activation function of the $i^{\\mathrm{th}}$ interneuron only depend on variables $\\theta_{i}$ and $z_{i}$ . The updates to the synapses $\\mathbf{w}_{i}$ $(-\\mathbf{w}_{i})$ are proportional (inversely proportional) to the product of the pre- and postsynaptic activities, so they are both local and Hebbian (anti-Hebbian) [50]. Finally, to ensure that the vectors $\\mathbf{w}_{1},\\dots,\\mathbf{w}_{K}$ have unit norm, we normalize the weights after each update: $\\dot{\\mathbf{w}}_{i}\\gets\\mathbf{w}_{i}/\\lVert\\mathbf{w}_{i}\\rVert$ . This can be viewed as form of homeostatic plasticity such as synaptic scaling [51]. While the feedforward weights $\\mathbf{w}_{i}$ and feedback weights $-\\mathbf{w}_{i}$ are constrained to be symmetric, these can be decoupled due to the symmetry of the Hebbian learning rule. Both theoretical and empirical evidence of this is shown for a related adaptive whitening circuit in [52, appendix E.2]. ", "page_idx": 4}, {"type": "text", "text": "3.3 Online algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Combining the neural dynamics, the interneuron adaptation and synaptic plasticity steps yields our online algorithm (Alg. 1), which we write in vector-matrix notation by defining the normalization function $\\bar{P}(\\mathbf{W}):=\\bar{[\\mathbf{w}_{1}/\\lVert\\mathbf{w}_{1}\\rVert},\\dots,\\mathbf{w}_{K}/\\lVert\\mathbf{w}_{K}\\rVert]$ . ", "page_idx": 4}, {"type": "image", "img_path": "ojLIEQ0j9T/tmp/99fcaec611bb145818ad91c1075820d378062530e37915e9337c99763f5afab1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.4 Relation to existing algorithms ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Algorithm 1 is naturally viewed as a nonlinear extension of existing algorithms for linear data whitening that have neural circuit implementations [34, 52\u201354]. In particular, when the constraint function is quadratic, $\\begin{array}{r}{h(z)\\,=\\,\\frac{1}{2}z^{2}}\\end{array}$ , and the target distribution is the spherical Gaussian, $\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ , then the activation function is the identity, $f(z)=z$ , and the optimization in eq. (2) enforces that the second moments of the responses match the second moments of $\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ , which corresponds to data whitening. If the gains are fixed $(\\eta_{g}\\,=\\,0)$ and $K\\geq N$ (i.e., synaptic adaptation only), then Alg. 1 corresponds to the adaptive whitening algorithm presented in [34, 53]. Alternatively, if the synaptic weights are fixed $\\eta_{w}=0$ ) and $K\\geq N(N+1)/2$ (i.e., interneuron gain adaptation only), then Alg. 1 corresponds to the adaptive whitening algorithm presented in [54]. Finally, if the gains adapt on a fast timescale and the synapses update on a slow timescale (i.e., $\\eta_{g}\\gg\\eta_{w}>0)$ ), Alg. 1 corresponds to the multi-timescale adaptive whitening algorithm presented in [52]. ", "page_idx": 5}, {"type": "text", "text": "When the target distribution is the spherical Gaussian and $\\mathbf{W}$ is constrained to be an orthogonal matrix, Alg. 1 is related to existing iterative algorithms for Gaussianization that alternate between (a) orthogonal transformations and (b) marginal Gaussianization of the coordinates [6, 8]. In the case that the column vectors of W are orthogonal, Gaussianization along one marginal does not affect the responses along other marginals, allowing these operations to be performed independently of one another. In general, we allow the column vectors of W to be non-orthogonal and potentially overcomplete so that marginal Gaussianization along one basis vector affects the other marginal distributions. Consequently, the algorithm is more complicated to analyze mathematically (e.g., obtaining convergence guarantees), but it is more biologically realistic since neural systems are unlikely to be constrained to have orthonormal synaptic weight vectors. ", "page_idx": 5}, {"type": "text", "text": "4 Gaussianization of natural image statistics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We apply our algorithm to the problem of efficient nonlinear encoding of natural signals, specifically oriented filter responses to visual images.3 Redundancy reduction theories posit that early sensory systems transform natural signal into neural representations with reduced statistical redundancies [14, 15, 17]. In support of this hypothesis, early sensory representations exhibit far less spatial and temporal correlations than natural signals [55, 56] and methods such as linear ICA have been used to derive optimal representations of natural signals that are approximately matched to early sensory neuron responses [18, 19, 57]. ", "page_idx": 5}, {"type": "text", "text": "Linear whitening and ICA transforms can eliminate simple forms of statistical dependency, but their responses exhibit higher-order statistical dependencies when applied to natural signals [58], suggesting that sensory systems can more efficiently represent natural signals by implementing nonlinear transforms. Consistent with this, nonlinear phenomenological models of neural responses (e.g., divisive normalization [36, 38]) effectively reduce these higher-order statistical dependencies [37, 59]. However, the circuit mechanisms that support these nonlinear transformations are unknown. ", "page_idx": 5}, {"type": "image", "img_path": "ojLIEQ0j9T/tmp/9a6a3e580791a19cafd5fabe4bcf1eba6caaebd16fb2b8967725226c47f7c663.jpg", "img_caption": ["Figure 2: Gaussianization of local filter responses. A) Three example natural images from the Kodak dataset. B) Histograms of local filter responses (black lines) and fitted generalized Gaussian density (red dashed lines) with scale $\\alpha$ and shape $\\beta$ . C) Learned interneuron activations ${g f(\\theta,z)}$ , with $f(\\theta,z)$ defined as in eq. (6) and learned $g$ and $\\theta$ , and $\\mathbf{D}$ ) corresponding stimulus-response transforms $r=T(s)$ . The optimal activations and transforms are shown as thick gray curves. E) Histograms of the circuit responses (black lines) and the Gaussian density (red dashed lines). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "We consider the case that circuits nonlinearly transform their inputs to produce Gaussian responses. Gaussian responses may be interpreted as firing rate responses, or as their logs (e.g., membrane potentials are Gaussian, and firing rates are exponentiated). From an efficient coding and computation perspective, Gaussian representations are appealing for a variety of reasons. First, among distributions with given covariance structure, Gaussian distributions have maximum entropy. Therefore, if metabolic demands are a function of the (co)variance of the response distribution, then the Gaussian distribution maximizes information transmission under metabolic constraints. Second, compression based on information theoretic objectives often reduces to linear projection when the data distribution is Gaussian [60\u201362], so Gaussianization can be viewed as form of linearization that facilitates downstream computation. In addition, the efficiency of the representation is preserved under orthogonal transformation [7]. Finally, experiments have shown that single neurons in the fly early visual system adaptively Gaussianize their univariate responses [63], and neural populations in early sensory systems decorrelate their multivariate responses [56, 64\u201367]. Therefore, neural circuit models that nonlinearly transform signals to jointly Gaussianize their responses may offer normative, parsimonious explanations of nonlinear transformations in early sensory systems. ", "page_idx": 6}, {"type": "text", "text": "4.1 Description of the input signal ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We computed the responses of a local oriented fliter [68], which captures receptive field selectivity of neurons in primate visual cortex [69], applied to natural images from the Kodak dataset [70]. These local fliter responses are notorious for their sparse heavy-tailed statistical properties that can be well approximated by generalized Gaussian distributions of the form $p_{s}(s)\\stackrel{.}{\\propto}\\exp(-|s/\\alpha|^{\\beta})$ , where $\\alpha$ is referred to as the scale parameter and $\\beta$ is referred to as the shape parameter [71]. Fig. 2AB shows example images and histograms of the local filter responses along with fitted generalized Gaussian distributions whose scale and shape parameters $(\\alpha,\\beta)$ vary across images. While linear methods such as variance normalization are sufficient for rescaling the distribution, adaptive nonlinear transformations are required to reshape these heavy-tailed distributions. ", "page_idx": 6}, {"type": "image", "img_path": "ojLIEQ0j9T/tmp/e0e9710b9778c23477c9b89e52a28b8034ebb815ced4588d31f08bbfb511d411.jpg", "img_caption": ["Figure 3: Evenly spaced contours for the spherical Gaussian distribution are depicted as dashed red circles. For spatial offsets $d=2,8,32$ , the contour plots for (A) the local filter responses, (B) ZCA whitened local filter responses, and $\\mathbf{(CDE)}$ learned circuit responses (with $K=2,3,4$ interneurons) are depicted (in the respective column) as black curves along with the estimated mutual information between coordinates. The learned column vectors of W are indicated by the faint gray lines. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Next, we generated 2-dimensional signals from pairs of the local fliter responses for images at fixed horizontal spatial offsets ranging between $d=2$ and $d=64$ . Contour plots (using kernel density estimation) of the local filter response pairs and symmetric (or ZCA) whitened local filter response pairs for a natural image (specifically, the top-left image from Fig. 2) are shown for select $d$ in Fig. 3AB. To quantify the statistical dependencies between coefficients, we estimated the mutual information between the pairs of coefficients after discretizing them into bins of width 0.5. In Fig. 4, we plot the estimated mutual information between the local filter response pairs (blue line) and ZCA whitened local filter response pairs (orange line) for spatial offsets between $d=2$ and $d=64$ . Note that aside from $d=2$ , the linear ZCA whitening transform does not significantly reduce the mutual information between coordinates. (Similar results have been found when applying linear ICA transforms; see, e.g., [72, Figure 6].) Fig. 4 also suggests that ZCA whitening can even slightly increase the mutual information between coordinates\u2014see also, rows $d=8$ and $d=32$ of Fig. 3AB\u2014though these effects are quite small and may be a consequence of the discretization step when estimating mutual information. ", "page_idx": 7}, {"type": "text", "text": "4.2 Choice of activation functions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "How do we choose the family of activation functions $\\{f(\\theta,\\cdot)\\}?$ One approach is to choose a kernel that can approximate a general class of functions. An alternative approach, adopted here, which is motivated by the efficient coding hypothesis [14, 15, 17], is to choose a family of activation functions that is well matched to the marginal statistics of natural signals. In Fig. 2C, we plot examples of optimal activations for transforming local filter responses from different images (thick gray curves). ", "page_idx": 7}, {"type": "text", "text": "Since the marginals of the local filter responses are well-approximated by generalized Gaussian distributions [71], a sensible approach is to identify a family of interneuron activation functions that are optimal for transforming generalized Gaussian distributions with varying $(\\alpha,\\beta)$ into the standard Gaussian distribution. When $\\mu=0$ , this implies (see Appx. B) that for each choice of scale $\\alpha$ and shape $\\beta$ , there is a gain $g$ and parameter $\\theta$ such that ", "page_idx": 7}, {"type": "equation", "text": "$$\ng f(\\theta,\\cdot)=F_{\\alpha,\\beta}^{-1}\\circ\\Phi(\\cdot),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\Phi(\\cdot)$ is the cdf of $\\mathcal{N}(0,1)$ . However, if we define $f(\\theta,z)$ in terms of the above display, then we do not have a closed-form solution for $\\phi(\\theta,z)$ or $\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{\\phi}(\\boldsymbol{\\theta},z)$ , which are both required to implement ", "page_idx": 7}, {"type": "image", "img_path": "ojLIEQ0j9T/tmp/b880c8ab95bb5a858a2db0add8093d16a913575fa7ff92f4b427bab826155acd.jpg", "img_caption": ["Figure 4: For spatial offsets between $d=2$ and $d=64$ , the estimated mutual information (using bin size 0.5) of the signal with $95\\%$ confidence intervals (estimated across 23 images), ZCA whitened signal, and learned circuit responses (with $K=2,3,4$ interneurons). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Alg. 1. Instead, we found that $F_{\\alpha,\\beta}^{-1}\\circ\\Phi(\\cdot)$ can be well approximated by the simple algebraic form ", "page_idx": 8}, {"type": "equation", "text": "$$\nf(\\theta,z)=a(\\theta)z+b(\\theta)\\operatorname{sign}(z)|z|^{\\theta},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $a(\\theta)$ and $b(\\theta)$ are specified nonnegative functions of $\\theta\\ \\>\\ 1$ . Intuitively, the linear component shapes the marginal density locally around zero, while the higher-order monomial shapes the tails of the marginal distribution. In Appx. B, we show that the monomial activation $f(\\dot{\\theta_{,}}\\,z)=\\mathrm{sign}(z)|z|^{\\theta}$ is optimal for Gaussianizing scalar signals whose marginal tail densities satisfy $p_{s}(s)\\propto|s|^{q-1}\\ensuremath{\\exp(-|s|^{2q})}$ , where $q=1/\\theta$ . This closely resembles the tail densities of generalized Gaussian densities (when $\\alpha=1$ ), suggesting monomial activations are effective for shaping the tails. ", "page_idx": 8}, {"type": "text", "text": "4.3 Marginal density of local filter responses ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We first apply Alg. 1 in the scalar setting $N=K=1$ to demonstrate that our choice of activation function in eq. (6) is indeed well matched to the shape of heavy-tailed marginals of local filter responses. For each image, we ran Alg. 1 on the local filter responses with $\\mu=0$ , learning rates $(\\eta_{g},\\eta_{\\theta})=(10^{-5},10^{-5})$ and batch size 10 for $10^{5}$ iterations. Fig. 2CD shows the learned interneuron activation functions $g f(\\theta,\\cdot)$ and the learned transforms $T(\\cdot)$ . Fig. 2E shows histograms of the circuit responses. Compared to the local filter responses, the circuit responses are visually much closer to Gaussian. We found that the circuit performs worse when the distribution $p_{s}$ is more \u2018peaked\u2019 around zero (i.e., when $s$ is sparser and the ftited shape parameter $\\beta$ is smaller), as evidenced by the mismatch between the response distribution $p_{r}$ and the Gaussian distribution ${\\mathcal{N}}(0,1)$ near zero in the bottom row of Fig. 2E (see Appx. C for more examples). However, even in this case, the interneuron activation and circuit transform are close to optimal (Fig. 2CD, bottom row). ", "page_idx": 8}, {"type": "text", "text": "4.4 Joint density of pairs of local filter responses ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Next, we apply Alg. 1 in the multivariate setting $N\\,=\\,2$ . For each image and spatial offset, we ran Alg. 1 with $K=2,3,4$ interneurons, $\\mu=0$ , learning rates $(\\eta_{g},\\eta_{\\theta},\\eta_{w}^{~\\bullet})=(10^{-4},10^{-6},10^{-4})$ , and batch size 10 for $10^{6}$ iterations. Contour plots of the learned circuit responses for one image are shown in Fig. 3CDE; see Appx. C for more examples. The mutual information between circuit responses are shown in Fig. 4. We see that $K=3$ interneurons significantly reduces the mutual information between circuit responses for spatial offsets less than $d=32$ . The reduction is much greater than obtained using $K\\,=\\,2$ interneurons and about the same as obtained using $K\\,=\\,4$ interneurons. For spatial offsets greater than $d=32$ , the local filter responses already have low mutual information and the circuit does not significantly reduce the mutual information between responses. ", "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We derived a novel online algorithm for transforming a signal to approximate a target distribution, using a recurrent neural circuit with Hebbian synaptic plasticity, gain modulation and adaptation of neural activation functions. Our model draws inspiration from the extensive neuroscience literature on efficient coding [15], Hebbian synaptic plasticity [46], interneuron function [29] and gain modulation [47]. The model proposes complementary roles for different physiological processes: Hebbian synaptic plasticity learns directions that are least matched to the target distribution and interneurons adapt their gains and activation functions to transform the marginal responses along these directions. ", "page_idx": 9}, {"type": "text", "text": "Our circuit model captures many features of biological circuits, including interneurons, neural adaptation and Hebbian synaptic plasticity, providing a bridge between neural anatomy and physiology and a computational objective inspired by efficient coding. In particular, our model suggests precise relationships between marginal statistics of the signal and interneuron input-output functions. The form of the input-output function for the local interneurons\u2014linear-nonlinear with gain modulation\u2014 closely resembles phenomenological models of neurons [73], and the parameters W, \u03b8, g can potentially be fti to neural recordings and compared with the optimal parameters that can be derived from the signal statistics $p_{\\mathbf{s}}$ . Furthermore, our model predicts a relation between the interneuron activation function and the circuit transform; see Fig. 2CD for an example of an expansive interneuron activation function that corresponds to a compressive circuit transformation. ", "page_idx": 9}, {"type": "text", "text": "There are also aspects of our circuit that are not biologically realistic. For example, our model focuses on the role of local interneurons in reshaping the response distribution and, for simplicity, assumes that the primary neurons have linear activation functions. A more realistic model would also include nonlinearity and adaptation in the primary neurons. Moreover, our model only includes synaptic connections between primary neurons and interneurons, which is consistent with some sensory circuits (e.g., olfactory bulb), but cannot account for excitatory-excitatory connections or inhibitoryinhibitory connections in cortical circuits. Finally, the synaptic weights are not sign-constrained, which violates Dale\u2019s law. This can be addressed by modifying the objective in eq. 2 so that the optimization is over non-negative weight matrices $\\mathbf{W}\\geq0$ , which will result in a projected gradient step in Alg. 1; however, the circuit responses will generally be less aligned with the target distribution. ", "page_idx": 9}, {"type": "text", "text": "A limitation of our experiments is that we only test our method on two-dimensional inputs, demonstrating that three interneurons are sufficient to dramatically reduce the redundancy in the circuit responses. However, natural signals are often very high-dimensional and it is not clear how the number of interneurons required to effectively reduce redundancy will scale with the dimension of the signal. We are optimistic that the number of interneurons required will scale reasonably. Visual inputs are highly structured\u2014e.g., statistical dependencies between inputs rapidly decay with the distance between the inputs\u2014so local interneurons only need to connect to neurons with overlapping or adjacent receptive fields, which greatly reduces the number of interneurons that are required as the dimension of the input signal grows. ", "page_idx": 9}, {"type": "text", "text": "There are a number of existing computational models that also explain how neural circuits can implement nonlinear transformations to efficiently encode their inputs. For example, there are several neural circuit models that implement forms of divisive normalization [74\u201376], a transformation that is optimal for efficient encoding of natural signals [37, 59]. In addition, there is a body of work on normative spiking models derived from objectives which maximize the information encoded per spike [77\u201379], which can account for neural adaptation mechanisms such as gain control. Our work differs from these, by proposing a novel framing of sensory circuit computation in terms of transformations of probability distributions, which can be viewed as a population level version of the seminal work by Laughlin [16]. We then demonstrate in a normative circuit model how interneurons can play a critical role in optimizing this objective by measuring the marginal distribution of circuit responses and adjusting their feedback accordingly. ", "page_idx": 9}, {"type": "text", "text": "Finally, our results may also be relevant beyond the biological setting. Gaussianization and normalizing flows are active areas of research [10, 13, 80]. We offer a novel continuous-learning solution inspired by neuroscience that learns using a combination of weight updates and activation function updates (related to trainable activation functions [81]). In low-dimensional settings, when the constraint functions are matched to the signal statistics, we show that Gaussianization can be achieved using relatively few parameters. It is of primary interest to understand how the methods introduced here scale to high-dimensional inputs, where the curse of dimensionality presents significant challenges. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Colin Bredenberg and members of the Center for Computational Neuroscience at the Flatiron Institute for helpful feedback on an earlier draft of this work. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Gaspard Monge. M\u00e9moire sur la th\u00e9orie des d\u00e9blais et des remblais. Mem. Math. Phys. Acad. Royale Sci., pages 666\u2013704, 1781.   \n[2] Agnan Kessy, Alex Lewin, and Korbinian Strimmer. Optimal whitening and decorrelation. The American Statistician, 72(4):309\u2013314, 2018.   \n[3] Aapo Hyv\u00e4rinen and Erkki Oja. Independent component analysis: algorithms and applications. Neural Networks, 13(4-5):411\u2013430, 2000.   \n[4] Mike H Hoyle. Transformations: An introduction and a bibliography. International Statistical Review/Revue Internationale de Statistique, pages 203\u2013223, 1973.   \n[5] Remi M Sakia. The Box-Cox transformation technique: a review. Journal of the Royal Statistical Society Series D: The Statistician, 41(2):169\u2013178, 1992.   \n[6] Scott Chen and Ramesh Gopinath. Gaussianization. Advances in Neural Information Processing systems, 13, 2000.   \n[7] Siwei Lyu and Eero P Simoncelli. Reducing statistical dependencies in natural signals using radial gaussianization. Advances in Neural Information Processing Systems, 2008:1009\u20131016, 2008.   \n[8] Valero Laparra, Gustavo Camps-Valls, and Jes\u00fas Malo. Iterative Gaussianization: from ICA to random rotations. IEEE Transactions on Neural Networks, 22(4):537\u2013549, 2011.   \n[9] Johannes Ball\u00e9, Valero Laparra, and Eero P Simoncelli. Density modeling of images using a generalized normalization transformation. International Conference on Learning Representations, 2016.   \n[10] Chenlin Meng, Yang Song, Jiaming Song, and Stefano Ermon. Gaussianization flows. In International Conference on Artificial Intelligence and Statistics, pages 4336\u20134345. PMLR, 2020.   \n[11] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International conference on machine learning, pages 1530\u20131538. PMLR, 2015.   \n[12] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. International Conference on Learning Representations, 2015.   \n[13] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. The Journal of Machine Learning Research, 22(1):2617\u20132680, 2021.   \n[14] Fred Attneave. Some informational aspects of visual perception. Psychological Review, 61(3): 183\u2013193, 1954.   \n[15] H B Barlow. Possible Principles Underlying the Transformations of Sensory Messages. In Sensory Communication, pages 216\u2013234. The MIT Press, 1961.   \n[16] Simon Laughlin. A simple coding procedure enhances a neuron\u2019s information capacity. Zeitschrift fur Naturforschung C, Journal of Biosciences, pages 910\u2013912, 1981.   \n[17] Eero P Simoncelli and Bruno A Olshausen. Natural image statistics and neural representation. Annual Review of Neuroscience, 24(1):1193\u20131216, 2001.   \n[18] Anthony J Bell and Terrence J Sejnowski. The \u201cindependent components\u201d of natural scenes are edge filters. Vision research, 37(23):3327\u20133338, 1997.   \n[19] Bruno A Olshausen and David J Field. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381(6583):607\u2013609, 1996.   \n[20] Peter Foldiak. Sparse coding in the primate cortex. The Handbook of Brain Theory and Neural Networks, 2003.   \n[21] Bruno A Olshausen and David J Field. Sparse coding of sensory inputs. Current Opinion in Neurobiology, 14(4):481\u2013487, 2004.   \n[22] Alexandre Pouget, Sophie Deneve, Jean-Christophe Ducom, and Peter E Latham. Narrow versus wide tuning curves: What\u2019s best for a population code? Neural Computation, 11(1): 85\u201390, 1999.   \n[23] Kechen Zhang and Terrence J Sejnowski. Neuronal tuning: To sharpen or broaden? Neural Computation, 11(1):75\u201384, 1999.   \n[24] Deep Ganguli and Eero P Simoncelli. Efficient sensory encoding and Bayesian inference with heterogeneous neural populations. Neural computation, 26(10):2103\u20132134, 2014.   \n[25] TF Freund and G Buzs\u00e1ki. Interneurons of the hippocampus. Hippocampus, 6(4):347\u2013470, 1996.   \n[26] Chris J McBain and Andr\u00e9 Fisahn. Interneurons unbound. Nature Reviews Neuroscience, 2(1): 11\u201323, 2001.   \n[27] Henry Markram, Maria Toledo-Rodriguez, Yun Wang, Anirudh Gupta, Gilad Silberberg, and Caizhi Wu. Interneurons of the neocortical inhibitory system. Nature Reviews Neuroscience, 5 (10):793\u2013807, 2004.   \n[28] Carl P Wonders and Stewart A Anderson. The origin and specification of cortical interneurons. Nature Reviews Neuroscience, 7(9):687\u2013696, 2006.   \n[29] Adam Kepecs and Gordon Fishell. Interneuron cell types are fti to function. Nature, 505(7483): 318\u2013326, 2014.   \n[30] Gord Fishell and Adam Kepecs. Interneuron types as attractors and controllers. Annual Review of Neuroscience, 43:1\u201330, 2020.   \n[31] Mattia Chini, Thomas Pfeffer, and Ileana Hanganu-Opatz. An increase of inhibition drives the developmental decorrelation of neural activity. Elife, 11:e78811, 2022.   \n[32] Cristina Savin, Prashant Joshi, and Jochen Triesch. Independent component analysis in spiking neurons. PLoS Computational Biology, 6(4):e1000757, 2010.   \n[33] Paul D King, Joel Zylberberg, and Michael R DeWeese. Inhibitory interneurons decorrelate excitatory cells to drive sparse code formation in a spiking model of V1. Journal of Neuroscience, 33(13):5475\u20135485, 2013.   \n[34] Cengiz Pehlevan and Dmitri B Chklovskii. A normative theory of adaptive dimensionality reduction in neural networks. Advances in Neural Information Processing Systems, 28, 2015.   \n[35] Nikolai M Chapochnikov, Cengiz Pehlevan, and Dmitri B Chklovskii. Normative and mechanistic model of an adaptive circuit for efficient encoding and feature extraction. Proceedings of the National Academy of Sciences, 120(29):e2117484120, 2023.   \n[36] David J. Heeger. Normalization of cell responses in cat striate cortex. Visual neuroscience, 9(2): 181\u201397, 1992. ISSN 0952-5238. doi: 10.1017/S0952523800009640.   \n[37] Odelia Schwartz and Eero P. Simoncelli. Natural signal statistics and sensory gain control. Nature Neuroscience, 4(8):819\u2013825, August 2001.   \n[38] Matteo Carandini and David J Heeger. Normalization as a canonical neural computation. Nature Reviews Neuroscience, 13(1):51\u201362, 2012.   \n[39] Peter J Huber. Projection pursuit. The Annals of Statistics, pages 435\u2013475, 1985. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[40] Gy\u00f6rgy Buzs\u00e1ki and Kenji Mizuseki. The log-dynamic brain: how skewed distributions affect network operations. Nature Reviews Neuroscience, 15(4):264\u2013278, 2014. ", "page_idx": 12}, {"type": "text", "text": "[41] C\u00e9dric Villani. Optimal Transport: Old and New, volume 338. Springer, 2009.   \n[42] Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Sch\u00f6lkopf, and Gert RG Lanckriet. On integral probability metrics, $\\phi$ -divergences and binary classification. arXiv preprint arXiv:0901.2698, 2009.   \n[43] Harald Cram\u00e9r and Herman Wold. Some theorems on distribution functions. Journal of the London Mathematical Society, 1(4):290\u2013294, 1936.   \n[44] Julien Rabin, Gabriel Peyr\u00e9, Julie Delon, and Marc Bernot. Wasserstein Barycenter and its application to texture mixing. In Scale Space and Variational Methods in Computer Vision: Third International Conference, SSVM 2011, Ein-Gedi, Israel, May 29\u2013June 2, 2011, Revised Selected Papers 3, pages 435\u2013446. Springer, 2012.   \n[45] Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen Zhao, David Forsyth, and Alexander G Schwing. Max-sliced Wasserstein distance and its use for GANs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10648\u201310656, 2019.   \n[46] Natalia Caporale and Yang Dan. Spike timing-dependent plasticity: a Hebbian learning rule. Annu. Rev. Neurosci., 31:25\u201346, 2008.   \n[47] Katie A Ferguson and Jessica A Cardin. Mechanisms underlying gain modulation in the cortex. Nature Reviews Neuroscience, 21(2):80\u201392, 2020.   \n[48] Julijana Gjorgjieva, Guillaume Drion, and Eve Marder. Computational implications of biophysical diversity and multiple timescales in neurons and synapses for circuit performance. Current Opinion in Neurobiology, 37:44\u201352, 2016.   \n[49] Alison I Weber, Kamesh Krishnamurthy, and Adrienne L Fairhall. Coding principles in adaptation. Annual Review of Vision Science, 5:427\u2013449, 2019.   \n[50] D O Hebb. The Organization of Behavior: A Neuropsychological Theory. Wiley & Sons, 1949.   \n[51] Gina G Turrigiano. The self-tuning neuron: synaptic scaling of excitatory synapses. Cell, 135 (3):422\u2013435, 2008.   \n[52] Lyndon R Duong, Eero P Simoncelli, Dmitri B Chklovskii, and David Lipshutz. Adaptive whitening with fast gain modulation and slow synaptic plasticity. Advances in Neural Information Processing systems, 2023.   \n[53] David Lipshutz, Cengiz Pehlevan, and Dmitri B Chklovskii. Interneurons accelerate learning dynamics in recurrent neural networks for statistical adaptation. In The Eleventh International Conference on Learning Representations, 2023.   \n[54] Lyndon R Duong, David Lipshutz, David J Heeger, Dmitri B Chklovskii, and Eero P Simoncelli. Adaptive whitening in neural populations with gain-modulating interneurons. Proceedings of the 40th International Conference on Machine Learning, PMLR, 202:8902\u20138921, 2023.   \n[55] Yang Dan, Joseph J Atick, and R Clay Reid. Efficient coding of natural scenes in the lateral geniculate nucleus: experimental test of a computational theory. Journal of Neuroscience, 16 (10):3351\u20133362, 1996.   \n[56] Xaq Pitkow and Markus Meister. Decorrelation and efficient coding by retinal ganglion cells. Nature Neuroscience, 15(4):628\u2013635, 2012.   \n[57] Michael S Lewicki. Efficient coding of natural sounds. Nature Neuroscience, 5(4):356\u2013364, 2002.   \n[58] Bernhard Wegmann and Christoph Zetzsche. Statistical dependence between orientation filter outputs used in a human-vision-based image code. In Visual Communications and Image Processing\u201990: Fifth in a Series, volume 1360, pages 909\u2013923. SPIE, 1990.   \n[59] Siwei Lyu. Dependency reduction with divisive normalization: Justification and effectiveness. Neural Computation, 23(11):2942\u20132973, 2011.   \n[60] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction, volume 2. Springer, 2009.   \n[61] Ralph Linsker. Self-organization in a perceptual network. Computer, 21(3):105\u2013117, 1988.   \n[62] Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss. Information bottleneck for Gaussian variables. Advances in Neural Information Processing Systems, 16, 2003.   \n[63] JH Van Hateren. Processing of natural time series of intensities by the visual system of the blowfly. Vision Research, 37(23):3407\u20133416, 1997.   \n[64] Sonya Giridhar, Brent Doiron, and Nathaniel N Urban. Timescale-dependent shaping of correlation by olfactory bulb lateral inhibition. Proceedings of the National Academy of Sciences, 108(14):5843\u20135848, 2011.   \n[65] Andrea Benucci, Aman B Saleem, and Matteo Carandini. Adaptation maintains population homeostasis in primary visual cortex. Nature Neuroscience, 16(6):724\u2013729, 2013.   \n[66] Adrian A Wanner and Rainer W Friedrich. Whitening of odor representations by the wiring diagram of the olfactory bulb. Nature Neuroscience, 23(3):433\u2013442, 2020.   \n[67] Ariana R Andrei, Alan E Akil, Natasha Kharas, Robert Rosenbaum, Kre\u0161imir Josic\u00b4, and Valentin Dragoi. Rapid compensatory plasticity revealed by dynamic correlated activity in monkeys in vivo. Nature Neuroscience, 2023.   \n[68] Eero P Simoncelli and William T Freeman. The steerable pyramid: A flexible architecture for multi-scale derivative computation. In Proceedings., International Conference on Image Processing, volume 3, pages 444\u2013447. IEEE, 1995.   \n[69] David J Field. Wavelets, vision and the statistics of natural scenes. Philosophical Transactions of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences, 357(1760):2527\u20132542, 1999.   \n[70] Eastman Kodak Company. True color Kodak images dataset, 1993. URL https://www.math. purdue.edu/\\~lucier/PHOTO_CD/.   \n[71] Eero P Simoncelli. Statistical models for images: Compression, restoration and synthesis. In Conference Record of the Thirty-First Asilomar Conference on Signals, Systems and Computers, volume 1, pages 673\u2013678. IEEE, 1997.   \n[72] Siwei Lyu and Eero P Simoncelli. Nonlinear extraction of independent components of natural images using radial gaussianization. Neural Computation, 21(6):1485\u20131519, 2009.   \n[73] Robbe LT Goris, J Anthony Movshon, and Eero P Simoncelli. Partitioning neuronal variability. Nature Neuroscience, 17(6):858\u2013865, 2014.   \n[74] Daniel B Rubin, Stephen D Van Hooser, and Kenneth D Miller. The stabilized supralinear network: a unifying circuit motif underlying multi-input integration in sensory cortex. Neuron, 85(2):402\u2013417, 2015.   \n[75] Matthew Chalk, Paul Masset, Sophie Deneve, and Boris Gutkin. Sensory noise predicts divisive reshaping of receptive fields. PLoS Computational Biology, 13(6):e1005582, 2017.   \n[76] Jes\u00fas Malo, Jos\u00e9 Juan Esteve-Taboada, and Marcelo Bertalm\u00edo. Cortical divisive normalization from Wilson\u2013Cowan neural dynamics. Journal of Nonlinear Science, 34(2):1\u201336, 2024.   \n[77] Veronika Koren and Sophie Den\u00e8ve. Computational account of spontaneous activity as a signature of predictive coding. PLoS computational biology, 13(1):e1005355, 2017.   \n[78] Alireza Alemi, Christian Machens, Sophie Deneve, and Jean-Jacques Slotine. Learning nonlinear dynamics in efficient, balanced spiking networks using local plasticity rules. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.   \n[79] Gabrielle J Gutierrez and Sophie Den\u00e8ve. Population adaptation in efficient balanced networks. eLife, 8:e46926, September 2019.   \n[80] Vincent Stimper, David Liu, Andrew Campbell, Vincent Berenz, Lukas Ryll, Bernhard Sch\u00f6lkopf, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. normflows: A PyTorch package for normalizing flows. Journal of Open Source Software, 8(86):5361, 2023.   \n[81] Andrea Apicella, Francesco Donnarumma, Francesco Isgr\u00f2, and Roberto Prevete. A survey on modern trainable activation functions. Neural Networks, 138:14\u201332, 2021.   \n[82] Aapo Hyv\u00e4rinen and Erkki Oja. Simple neuron models for independent component analysis. International Journal of Neural Systems, 7(06):671\u2013687, 1996. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Analysis of circuit transform ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we analyze the circuit transform $T(\\cdot)$ in the case that $\\mu\\geq0$ (i.e., $\\lambda\\geq-1$ ), $g_{i}\\geq0$ and $\\phi(\\theta_{i},\\cdot)$ are convex so that $\\mathcal{L}(\\mathbf{W},\\pmb{\\theta},\\mathbf{g},\\mathbf{s},\\cdot)$ is convex and ", "page_idx": 15}, {"type": "equation", "text": "$$\nT(\\mathbf{s})=\\underset{\\mathbf{r}}{\\arg\\operatorname*{min}}\\,\\mathcal{L}(\\mathbf{W},\\boldsymbol{\\theta},\\mathbf{g},\\mathbf{s},\\mathbf{r}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When $\\mu>0$ , we can solve for the equilibrium responses using the fixed point iteration: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mu\\mathbf{r}^{(0)}=\\mathbf{s},\\qquad\\qquad\\qquad\\mu\\mathbf{r}^{(n+1)}=\\mathbf{s}-\\mathbf{W}(\\mathbf{g}\\circ f(\\pmb{\\theta},\\mathbf{W}^{\\top}\\mathbf{r}^{(n)})),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $f(\\pmb{\\theta},\\mathbf{z}):=(f(\\theta_{1},z_{1}),\\dots,f(\\theta_{K},z_{K}))$ denotes the vector obtained by applying the function $f$ elementwise to the pairs $(\\theta_{i},z_{i})$ and $\\bullet_{\\circ},$ denotes the elementwise (Hadamard) product. ", "page_idx": 15}, {"type": "text", "text": "A.1 Invertibility of the transform ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "If the constraint functions are twice continuously differentiable, then $T(\\mathbf{s})$ is continuously differentiable and we can relate the response density $p_{\\mathbf{r}}$ to the signal density $p_{\\mathbf{s}}$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\np_{\\mathbf{s}}(\\mathbf{s})=\\vert\\mathbf{det}(J_{T}(\\mathbf{s}))\\vert\\,p_{\\mathbf{r}}(T(\\mathbf{s})),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $J_{T}$ denotes the Jacobian of $T$ with respect to s. In general, we don\u2019t have a closed form solution for $T$ ; however, setting the update in eq. (4) to zero, we see that the inverse transform $T^{-1}$ (when it is well-defined) satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\nT^{-1}(\\mathbf{r})=\\mathbf{s}=\\mu\\mathbf{r}+\\sum_{i=1}^{K}g_{i}f(\\theta_{i},\\mathbf{r}\\cdot\\mathbf{w}_{i})\\mathbf{w}_{i},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we have used the fact that the partial derivative of $\\phi(\\theta,z)$ with respect to $z$ is equal to the partial derivative of $h(\\theta,z)$ with respect to $z$ . It follows that its Jacobian $J_{T^{-1}}$ satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\nJ_{T^{-1}}(\\mathbf{r})=\\mu\\mathbf{I}+\\mathbf{W}\\mathrm{diag}\\left(g_{1}a_{1}(\\mathbf{r}),\\ldots,g_{K}a_{K}(\\mathbf{r})\\right)\\mathbf{W}^{\\top},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $a_{i}(\\mathbf{r}):=\\partial^{2}\\phi(\\theta_{i},\\mathbf{r}\\cdot\\mathbf{w}_{i})/\\partial z^{2}$ . Provided that either (a) $\\mu>0$ or (b) $g_{i}>0$ and $\\phi(\\theta_{i},\\cdot)$ is strictly convex for all $i$ and the column vectors of $\\mathbf{W}$ span $\\mathbb{R}^{N}$ , then the Jacobian of $T^{-1}$ is positive definite everywhere. This implies that the Jacobian of $T$ is positive definite everywhere, and thus $T$ is invertible everywhere. ", "page_idx": 15}, {"type": "text", "text": "A.2 Relation to function class ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Equation (7) establishes a relationship between the set of parameterizable (inverse) transforms and the function class $\\{h(\\theta,\\cdot)\\}$ . To better understand this relationship, consider the scalar setting $N=K=1$ in which case the optimal transformation is given by $T=\\bar{F}_{\\mathrm{marginal}}^{-1}\\circ F_{s}$ , where $F_{\\mathrm{target}}$ and $F_{s}$ are the cumulative distribution functions (cdfs) of $p_{\\mathrm{marginal}}$ and $p_{s}$ , respectively. When $T=F_{\\mathrm{marginal}}^{-1}\\circ F_{s}$ follows from eq. (7) that $h(\\theta,z)$ satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\ng\\frac{\\partial h(\\theta,z)}{\\partial z}=g f(\\theta,z)=F_{s}^{-1}\\circ F_{\\mathrm{marginal}}(z)-\\mu z.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The multidimensional setting is more complicated; however, in the case that the signal is an orthogonal mixture of $N$ independent sources, we can derive a precise relationship between the signal density and $(\\mathbf{W},\\pmb{\\theta},\\mathbf{g})$ Consider the case that the signal is of the form $\\mathbf{s}=\\mathbf{A}\\mathbf{u}$ , where $\\mathbf{A}$ is an $N\\times N$ orthogonal mixing matrix and $\\mathbf{u}=(u_{1},\\ldots,u_{N})$ has statistically independent coordinates. Suppose $\\mathbf{W}=\\mathbf{A}^{\\top}$ . After left multiplying eq. (7) on both sides by $\\mathbf{W}^{\\top}$ , we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{u}=\\mu\\mathbf{W}^{\\top}\\mathbf{r}+\\mathbf{g}\\circ\\mathbf{f}(\\pmb{\\theta},\\mathbf{W}^{\\top}\\mathbf{r}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If $\\mathbf{r}$ is Gaussian, then so is $\\mathbf{z}=\\mathbf{W}^{\\top}\\mathbf{r}$ and ", "page_idx": 15}, {"type": "equation", "text": "$$\nu_{i}=\\mu z_{i}+g_{i}f(\\theta_{i},z_{i}),\\qquad\\qquad\\qquad\\qquad i=1\\ldots,N.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, an optimal solution to eq. (2) is when $\\{\\theta_{i}\\}$ and $\\{g_{i}\\}$ satisfy ", "page_idx": 15}, {"type": "equation", "text": "$$\ng_{i}f_{i}(\\theta_{i},z_{i})=F_{u_{i}}^{-1}\\circ F_{\\mathrm{marginal}}(z_{i})-\\mu z_{i},\\qquad i=1,\\ldots,N,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $F_{u_{i}}$ is the cumulative distribution function for $u_{i}$ . ", "page_idx": 15}, {"type": "text", "text": "B Activation functions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we discuss the activation functions used for the experiments carried out in Sec. 4 of the main text. ", "page_idx": 16}, {"type": "text", "text": "B.1 Choice of activation functions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "How should we choose the family of activation functions $\\{f(\\theta,\\cdot)\\}?$ As stated in the main text, one approach is the choose a family that is well matched to the marginal statistics of the inputs. In this case, the input marginals are well approximated by generalized Gaussian distributions of the form ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{s}(s)=\\frac{\\beta}{2\\alpha\\Gamma(1/\\beta)}\\exp(-|s/\\alpha|^{\\beta})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with varying scale $\\alpha$ and shape $\\beta$ . Here $\\Gamma$ is the gamma function. Therefore, from eq. (8), we see that an optimal family of activation functions $\\{f(\\theta,\\bar{\\cdot})\\}$ is such that for each choice of scale $\\alpha$ and shape $\\beta$ , there is a gain $g$ and parameter $\\theta$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\ng f(\\theta,z)+\\mu z=F_{\\alpha,\\beta}^{-1}\\circ\\Phi(z),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $F_{\\alpha,\\beta}$ and $\\Phi$ denote the cdfs of the generalized Gaussian distribution and the standard Gaussian distribution. In general, defining the family of activation functions directly in terms of the above display leads to challenges implementing Alg. 1 since $\\phi(\\theta,z)$ and $\\nabla_{\\boldsymbol{\\theta}}\\phi({\\boldsymbol{\\theta}},{\\boldsymbol{\\dot{z}}})$ are not readily computable. ", "page_idx": 16}, {"type": "text", "text": "We instead sought a simple algebraic expression that approximates $F_{\\alpha,\\beta}^{-1}\\circ\\Phi(z)$ . First, suppose the activation function takes the form $f(\\theta,z)=\\mathrm{sign}(z)|z|^{\\theta}$ . When $\\mu=0$ , this activation function is optimal for a signal whose cdf $F_{\\mathrm{monomial}}$ satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\ng\\mathrm{sign}(z)|z|^{\\theta}=F_{\\mathrm{monomial}}^{-1}\\circ\\Phi(z).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Rearranging and differentiating with respect to $z$ , we find that the pdf of the signal $p_{\\mathrm{monomial}}$ satsifies ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{\\mathrm{monomial}}(g\\mathrm{sign}(z)|z|^{\\theta})g\\theta|z|^{\\theta-1}=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}z^{2}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Substituting in with $s=g\\mathrm{sign}(z)|z|^{\\theta}$ , we find the following expression for the pdf ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{\\mathrm{monomial}}(s)=\\frac{q}{g\\sqrt{2\\pi}}|s/g|^{q-1}\\exp\\left(-\\frac{1}{2}|s/g|^{2q}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $q=1/\\theta$ . This closely resembles aspects of the pdf for the tails of the generalized Gaussian distribution. To reshape the distribution local when $s\\approx0$ , we included a linear term, resulting in the activation function ", "page_idx": 16}, {"type": "equation", "text": "$$\nf(\\theta,z)=a(\\theta)z+b(\\theta)\\mathrm{sign}(z)|z|^{\\theta}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here $a(\\theta)$ and $b(\\theta)$ are nonnegative functions given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{a(\\theta)=\\exp((2\\theta-3.85)^{1.95})}\\\\ {b(\\theta)=\\exp(\\theta^{2.32}-5.9).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The forms of $a(\\theta)$ and $b(\\theta)$ were chosen to approximately minimize $\\mathrm{min}_{\\theta}\\,\\mathrm{max}_{z}\\,|F_{s}(g f(\\theta,z))\\!-\\!\\Phi(z)|$ when $F_{s}$ is the cdf of a generalized Gaussian distribution with shape parameter $\\beta$ between 0.2 and 1. ", "page_idx": 16}, {"type": "text", "text": "B.2 Activation function updates ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here we derive the updates to the activation functions. Recall from Alg. 1 that the $\\theta$ update is given by $\\theta\\leftarrow\\theta+\\eta_{\\theta}\\nabla_{\\theta}\\phi(\\bar{\\theta},z)$ , where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\phi(\\theta,z):=h(\\theta,z)-\\mathbb{E}_{z\\sim\\mathcal{N}(0,1)}[h(\\theta,z)]=\\frac{a(\\theta)}{2}(z^{2}-1)+\\frac{b(\\theta)}{\\theta+1}(|z|^{\\theta+1}-C(\\theta+1)),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $C(p)$ is the absolute $p$ -moment of a scalar Gaussian random variable: ", "page_idx": 17}, {"type": "equation", "text": "$$\nC(p):=\\mathbb{E}_{z\\sim\\mathcal{N}(0,1)}[|z|^{p}]=\\sqrt{\\frac{2^{p}}{\\pi}}\\Gamma\\left(\\frac{p+1}{2}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that when $\\theta=3$ , the constraint function normalizes the kurtosis of the marginal and the resulting update bears similarity to ones used in neural models of ICA [82]. Differentiating with respect to $\\theta$ , we obtain the updates ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{\\displaystyle\\frac{\\partial\\phi(\\theta,z)}{\\partial\\theta}=\\frac{a^{\\prime}(\\theta)}{2}(z^{2}-1)+\\frac{(\\theta+1)b^{\\prime}(\\theta)-b(\\theta)}{(\\theta+1)^{2}}(|z|^{\\theta+1}-C(\\theta+1))}}&{{}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+\\,\\frac{b(\\theta)}{\\theta+1}\\left(|z|^{\\theta+1}\\log|z|-C^{\\prime}(\\theta+1)\\right),}}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\nC^{\\prime}(p)=\\frac{1}{2}\\sqrt{\\frac{2^{p}}{\\pi}}\\Gamma\\left(\\frac{p+1}{2}\\right)\\left(\\log2+\\psi^{(0)}\\left(\\frac{p+1}{2}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "$\\psi^{(0)}(p)$ is the polygamma function and we have used the fact that the derivative of the gamma function is $\\Gamma^{\\prime}(p)=\\Gamma(p)\\psi^{(0)}(p)$ . ", "page_idx": 17}, {"type": "text", "text": "C Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Experimental set up ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We ran our experiments on a cluster comprised of 40-core Intel Skylake nodes with 768GB of RAM. Each experiment shown in Fig. 2 (i.e., Gaussianization of the coefficients from 1 image) took less than 5 minutes to run. Each experiment shown in Fig. 3 took around 5 hours to run (we did not optimize the choice of hyper-parameters). ", "page_idx": 17}, {"type": "text", "text": "C.2 Additional experimental results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Fig. 5, we provide additional examples of histograms of local filter responses and optimized responses for various images from the Kodak dataset [70] (see Fig. 2 of the main text for a detailed description). We note that when the shape parameter $\\beta$ of the ftited generalized Gaussian distribution is small, the distribution of responses has a characteristic dip near zero. This is likely due to the fact that when $\\beta$ is small, there is more probability mass concentrated near zero and so small discrepancies between the learned activation function $g f(\\theta,z)$ and the optimal activation function $F_{\\alpha,\\beta}^{-1}\\circ\\Phi(z)$ when $z\\approx0$ can lead to large discrepancies between the response distribution and ${\\mathcal{N}}(0,1)$ near zero. This could potentially be resolved by choosing a parameterization of $g f(\\theta,z)$ that better approximates the optimal activation function. ", "page_idx": 17}, {"type": "text", "text": "In Fig. 6, we provide additional examples of contour plots of local filter responses and optimized responses for three additional images from the Kodak dataset (corresponds to top three rows of Fig. 5, see Fig. 3 of the main text for a detailed description). In some of these examples we see that unlike the standard Gaussian distribution $\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ the learned response distribution is clearly non-monotone with respect to $\\|\\mathbf{r}\\|$ . This non-monotonicity is likely inherited from non-optimality of the interneuron activation functions that leads to the discrepancies in the scalar case (see Fig. 5). ", "page_idx": 17}, {"type": "image", "img_path": "ojLIEQ0j9T/tmp/51bbab03bbca4796dc26715868c1f0ce8847022ffb136fb7daae3d4cf6b81179.jpg", "img_caption": ["Figure 5: Gaussianization of local filter responses. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "ojLIEQ0j9T/tmp/53fc3a640cd609c7cc7dfe1d5bef8f8b589423a52dda30a5bc2c2269ef033571.jpg", "img_caption": ["Figure 6: Gaussianization of pairs of local filter responses. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our claims in the abstract and introduction match the theoretical and experimental results in the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We include limitations of our work and potential alternative approaches in the Discussion section. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: While our paper does not include proofs, we do include full details of our analytical results. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We describe the implementation of our algorithm and include a link to code for reproducing our results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We link to a GitHub repository with example code for reproducing the experiments described in the paper. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide full details of our experiments. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We include error bars. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We include the amount of compute used and runtime for the experiments (see Appx. C). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] Justification: Our research conforms with the NeurIPS Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: We do not foresee an immediate societal impact of our work ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We cite the Kodak dataset and include a URL to the asset. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}]