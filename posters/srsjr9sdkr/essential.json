{"importance": "This paper is important because it tackles a critical challenge in AI: **eliciting complex probability distributions from human experts or other information sources**.  Current methods are limited to simple distributions. This work introduces a novel approach using normalizing flows and preferential comparisons, opening new avenues for **building more accurate and flexible models** for AI systems that rely on human knowledge.  This has broad implications for many applications, including reward modelling, Bayesian inference, and decision making.", "summary": "Eliciting high-dimensional probability distributions from experts using only preference comparisons is achieved via normalizing flows and a novel functional prior, resolving the problem of collapsing probability mass.", "takeaways": ["A novel method uses normalizing flows to learn probability distributions from an expert's preferences (pairwise comparison or ranking).", "A new functional prior for normalizing flows addresses the issue of probability mass collapsing or diverging during training, especially when limited data is available.", "The method is validated through experiments using both simulated and real-world data, demonstrating its effectiveness in recovering complex, multi-dimensional belief densities."], "tldr": "Many AI applications require incorporating human knowledge, often represented as probability distributions.  However, **eliciting these distributions directly from human experts is challenging, especially for high-dimensional data** where humans are poor at assessing covariance structures and current methods are limited to simple forms. Existing techniques often fail to accurately capture the belief density due to problems like 'collapsing' or 'diverging' probability mass during the training process.  This paper proposes a solution to this problem.\nThe proposed method uses **normalizing flows**\u2014a type of neural network\u2014to represent the belief density.  It leverages **preferential data**, obtained by asking experts to compare or rank different options, rather than directly querying the probability density.  A key innovation is the introduction of a novel **functional prior for the normalizing flow**, derived from decision-theoretic principles, that prevents the undesirable behavior of the probability mass.  Empirical experiments show that this approach successfully infers the expert's belief density, including for a large language model's prior on real-world data, demonstrating the method's ability to handle both synthetic and real-world datasets. ", "affiliation": "University of Helsinki", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "sRSjr9SDKR/podcast.wav"}