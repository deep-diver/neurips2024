[{"figure_path": "sRSjr9SDKR/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of belief densities elicited from preferential ranking data by a normalizing flow (contour: true density; heatmap: estimated flow; red: preferred points; blue: non-preferred points). (a)-(b): Typical failure modes of collapsing and diverging mass, when training a flow with just n = 10 rankings. (c)-(d): The proposed functional prior resolves the issues, and already with 10 rankings we can learn the correct belief density, matching the result of the flow trained on larger data.", "description": "This figure shows how a normalizing flow can be used to elicit a probability distribution from an expert's preferences. The top row shows failure cases when only 10 rankings are used. The bottom row shows that by introducing a functional prior, the method can recover the correct density even with limited data.", "section": "2 Why learning the density from preferential data is challenging?"}, {"figure_path": "sRSjr9SDKR/figures/figures_4_1.jpg", "caption": "Figure 2: (a) The k-wise winner distribution converges to the belief density as k\u2192\u221e. (b) The k-wise winner distribution can be approximated by a tempered belief density. For example, the tempered belief density with an exponent 1/5 approximates well the pairwise winner distribution.", "description": "This figure shows that the k-wise winner distribution (the distribution of the most preferred point in k-wise comparisons) approaches the true belief density as k increases.  Panel (a) demonstrates this convergence. Panel (b) shows that the k-wise winner distribution can also be approximated by a tempered belief density (a belief density adjusted by raising it to a power), illustrating an alternative way to represent the belief density using the k-wise winner distribution.", "section": "3.2 The k-wise winner distribution as a tilted and tempered belief density"}, {"figure_path": "sRSjr9SDKR/figures/figures_7_1.jpg", "caption": "Figure 3: Cross-plot of selected variables of the estimated flow in the Abalone (left) and LLM knowledge elicitation experiment (middle), and the marginal density of the same variables for the ground truth density in the LLM experiment (right). See Figures C.6 and C.7 for other variables.", "description": "This figure shows the visualization of the learned belief density. The left panel shows the cross-plot of selected variables for the Abalone dataset. The middle panel shows the cross-plot of selected variables for the LLM experiment. The right panel shows the marginal density of the same variables for the ground truth density in the LLM experiment.  Additional variables' plots are available in Figures C.6 and C.7. This visual comparison helps to assess the quality of the learned belief density.", "section": "Experiments"}, {"figure_path": "sRSjr9SDKR/figures/figures_8_1.jpg", "caption": "Figure 4: Illustration of belief densities elicited from pairwise comparisons by a normalizing flow.", "description": "This figure shows two examples of belief densities inferred using a normalizing flow from pairwise comparisons. The left panel (a) displays the results for the Onemoon2D dataset, while the right panel (b) presents the results for the Gaussian6D dataset.  Each panel shows the estimated density (heatmap) alongside a set of points representing expert preferences (red for preferred points and blue for non-preferred points), illustrating the capability of the method in estimating probability densities from limited preferential data. The contour lines represent the true underlying density, allowing a visual comparison of accuracy.", "section": "Experiments"}, {"figure_path": "sRSjr9SDKR/figures/figures_19_1.jpg", "caption": "Figure 3: Cross-plot of selected variables of the estimated flow in the Abalone (left) and LLM knowledge elicitation experiment (middle), and the marginal density of the same variables for the ground truth density in the LLM experiment (right). See Figures C.6 and C.7 for other variables.", "description": "This figure shows a visual comparison of the estimated probability density (using the proposed method) with the ground truth density for selected variables. The left panel shows the Abalone dataset, the middle panel shows the LLM experiment, and the right panel shows the marginal density of the ground truth for the LLM experiment.  The plots illustrate the accuracy of the learned flow in capturing the relationships between variables, and the marginal distributions compared to the ground truth.", "section": "Experiments"}, {"figure_path": "sRSjr9SDKR/figures/figures_20_1.jpg", "caption": "Figure C.2: Gaussian6D experiment. The target distribution is depicted by light blue contour points and its marginal by a pink curve. The learned flow is depicted by dark blue contour sample points and its marginal by a black curve.", "description": "This figure shows the results of applying the proposed method to a 6-dimensional Gaussian distribution.  The contour plots show the true distribution (light blue) and the estimated distribution (dark blue). The marginal distributions are also shown (pink for true, black for estimated). The figure highlights how well the normalizing flow is able to capture the complex, high-dimensional distribution, even from limited data.", "section": "C Experimental details"}, {"figure_path": "sRSjr9SDKR/figures/figures_20_2.jpg", "caption": "Figure 1: Illustration of belief densities elicited from preferential ranking data by a normalizing flow (contour: true density; heatmap: estimated flow; red: preferred points; blue: non-preferred points). (a)-(b): Typical failure modes of collapsing and diverging mass, when training a flow with just n = 10 rankings. (c)-(d): The proposed functional prior resolves the issues, and already with 10 rankings we can learn the correct belief density, matching the result of the flow trained on larger data.", "description": "This figure shows the performance of the proposed method for eliciting high-dimensional probability distributions from an expert using preferential ranking data.  Subfigures (a) and (b) illustrate common problems when training a normalizing flow with limited data: probability mass collapses to a single point or diverges widely.  Subfigures (c) and (d) demonstrate that the proposed functional prior effectively mitigates these issues, leading to accurate density estimation even with only 10 rankings.  The contours represent the true density, while the heatmaps show the estimated density from the normalizing flow.  Red points indicate preferred alternatives, and blue points indicate non-preferred alternatives.", "section": "1 Introduction"}, {"figure_path": "sRSjr9SDKR/figures/figures_21_1.jpg", "caption": "Figure C.4: Twogaussians 10D experiment. The target distribution is depicted by light blue contour points and its marginal by a pink curve. The learned flow is depicted by dark blue contour sample points and its marginal by a black curve.", "description": "This figure shows the results of applying the proposed method to a synthetic 10-dimensional dataset with two Gaussian distributions.  The plot visualizes both the true distribution (light blue) and the learned distribution from the preferential ranking data using normalizing flows (dark blue).  Marginal distributions are also shown (pink and black curves) to provide a comparison of each individual dimension.", "section": "C Experimental details"}, {"figure_path": "sRSjr9SDKR/figures/figures_22_1.jpg", "caption": "Figure C.7: Full result plot for the Abalone7D experiment, complementing the partial plot presented in Figure 3. The target distribution is depicted by light blue contour points and its marginal by a pink curve. The learned flow is depicted by dark blue contour sample points and its marginal by a black curve.", "description": "This figure shows the full result of the Abalone7D experiment, which is a more realistic synthetic dataset created by using a real-world regression dataset.  It complements a smaller visualization shown earlier in the paper.  The plot compares the true density (light blue) and the learned flow density (dark blue). The target and estimated densities are shown as contour lines, while marginal distributions are shown as curves.  The visualization allows for a direct comparison of the learned flow to the true density.", "section": "C.5 Plots of learned belief densities"}, {"figure_path": "sRSjr9SDKR/figures/figures_23_1.jpg", "caption": "Figure 1: Illustration of belief densities elicited from preferential ranking data by a normalizing flow (contour: true density; heatmap: estimated flow; red: preferred points; blue: non-preferred points). (a)-(b): Typical failure modes of collapsing and diverging mass, when training a flow with just n = 10 rankings. (c)-(d): The proposed functional prior resolves the issues, and already with 10 rankings we can learn the correct belief density, matching the result of the flow trained on larger data.", "description": "This figure shows how a normalizing flow can be used to learn a probability distribution from preferential ranking data.  The contour lines represent the true density, and the heatmaps are the densities estimated by the flow. The red points indicate preferred choices, while blue points show non-preferred choices by an expert. Panels (a) and (b) illustrate common problems when training such flows with limited data: the probability mass may collapse to a small area or spread out excessively. Panels (c) and (d) demonstrate that the proposed new functional prior for the flow effectively solves these problems, enabling accurate density estimation even with small amounts of data. This highlights the effectiveness of the authors' method in eliciting high-dimensional belief densities.", "section": "Why learning the density from preferential data is challenging?"}, {"figure_path": "sRSjr9SDKR/figures/figures_23_2.jpg", "caption": "Figure 1: Illustration of belief densities elicited from preferential ranking data by a normalizing flow (contour: true density; heatmap: estimated flow; red: preferred points; blue: non-preferred points). (a)-(b): Typical failure modes of collapsing and diverging mass, when training a flow with just n = 10 rankings. (c)-(d): The proposed functional prior resolves the issues, and already with 10 rankings we can learn the correct belief density, matching the result of the flow trained on larger data.", "description": "This figure shows examples of belief densities inferred from preferential ranking data using normalizing flows.  Panels (a) and (b) illustrate common problems encountered when training flows with limited data: probability mass collapses to a single point or diverges to low-probability regions. Panels (c) and (d) demonstrate that incorporating a novel functional prior addresses these issues, resulting in accurate density estimation even with only 10 rankings.", "section": "2 Why learning the density from preferential data is challenging?"}, {"figure_path": "sRSjr9SDKR/figures/figures_24_1.jpg", "caption": "Figure 1: Illustration of belief densities elicited from preferential ranking data by a normalizing flow (contour: true density; heatmap: estimated flow; red: preferred points; blue: non-preferred points). (a)-(b): Typical failure modes of collapsing and diverging mass, when training a flow with just n = 10 rankings. (c)-(d): The proposed functional prior resolves the issues, and already with 10 rankings we can learn the correct belief density, matching the result of the flow trained on larger data.", "description": "This figure shows examples of belief densities inferred from preferential ranking data using normalizing flows.  Panels (a) and (b) illustrate common problems when using small datasets: probability mass collapsing to a single point or diverging to regions of low probability. Panels (c) and (d) demonstrate how the introduction of a novel functional prior solves these problems, enabling accurate density estimation even with limited data (n=10 rankings).", "section": "Why learning the density from preferential data is challenging?"}, {"figure_path": "sRSjr9SDKR/figures/figures_25_1.jpg", "caption": "Figure 4: Illustration of belief densities elicited from pairwise comparisons by a normalizing flow.", "description": "This figure shows the results of applying the proposed method to elicit belief densities from pairwise comparisons using a normalizing flow.  It illustrates the method's ability to capture the shape of the underlying belief density, even with a limited number of comparisons. Subfigure (a) shows the results for a two-dimensional, one-mode distribution (Onemoon2D), while subfigure (b) displays the results for a six-dimensional Gaussian distribution. The contour plots represent the true densities, the heatmaps show the learned densities, and red and blue points indicate preferred and non-preferred points, respectively. The figure showcases that the method can successfully infer high-dimensional probability distributions from preferential data.", "section": "Experiments"}, {"figure_path": "sRSjr9SDKR/figures/figures_26_1.jpg", "caption": "Figure D.2: Preferential flow fitted via FS-MAP with varying precision levels in the data generation process (in RUM) s<sub>true</sub>, and precision levels in the preferential likelihood s<sub>lik</sub>. The first column shows that a lower precision level in RUM leads to a more spread fitted flow, as expected. The middle plot is the only scenario where both the likelihood and the functional prior are correctly specified, resulting in the best result. Since the prior is misspecified in the bottom-right plot, the best results are not achieved, contrary to expectations. However, this misspecification does not lead to catastrophic performance deterioration but rather to a more spread-out fitted flow.", "description": "This figure shows the impact of different noise levels in the data generation process and likelihood function on the learned flow model.  The results indicate that a mismatch in noise levels (between the data generating process and the likelihood) does not cause a complete failure, but rather a less accurate (more spread-out) result.", "section": "D Ablation studies"}, {"figure_path": "sRSjr9SDKR/figures/figures_27_1.jpg", "caption": "Figure D.3: The Onemoon2D experiment replicated for varying sampling distributions \u03bb from where the candidates are sampled. In original Onemoon2D, \u03bb is a mixture of uniform and Gaussian distribution centered on the mean of the target, with the mixture probability w = 1/3 for the Gaussian. Here, \u03bb \u2208 {Uniform, Gaussian-Uniform Mixture, Target} with letting the mixture probability w to vary in {0.1, 1/4, 1/3, 1/2, 2/3, 3/4, 1.0}. The rest of the details can be found in Section 5, specifically n = 200 and k = 5. The distance between the base density and the target density (1.85) provides a scale reference. The method is robust for the sampling distribution and for broad range of w we reach essentially the same accuracy as when sampling from the target itself.", "description": "This figure shows the robustness of the proposed method to the choice of the candidate sampling distribution.  The Wasserstein distance between the estimated and true densities is plotted against different mixture probabilities (w) of a uniform and Gaussian distribution used for sampling candidates. The results demonstrate that the accuracy remains high even when the sampling distribution is far from the target distribution.", "section": "D Ablation studies"}, {"figure_path": "sRSjr9SDKR/figures/figures_27_2.jpg", "caption": "Figure D.4: The estimated belief densities in the Onemoon2D experiment of Table D.2 (contour: true density; heatmap: estimated flow). While the coverage of the estimated density with n = 1000 is better than with n = 100, there is more spread with n = 1000 than with n = 100, which explains the slight performance deterioration in Table D.2.", "description": "This figure shows the results of the Onemoon2D experiment with different numbers of rankings (n). It displays the true density using contours and the estimated density using a heatmap for n=25, n=100, and n=1000 respectively. The figure demonstrates how increasing n improves the accuracy of the estimated density, but also shows that excessive n might lead to overestimation of the density's spread, resulting in slightly lower accuracy.", "section": "D Ablation studies"}, {"figure_path": "sRSjr9SDKR/figures/figures_28_1.jpg", "caption": "Figure D.5: Cross-plots of selected variables of the estimated flow in the Twogaussians10D experiment of Table D.2 (contour: true density; heatmap: estimated flow). While the coverage of the estimated density with n = 10000 is better than with n = 500, there is more spread with n = 10000 than with n = 500, which explains the slight performance deterioration in Table D.2.", "description": "This figure shows cross-plots of selected variables from the estimated flow in the Twogaussians10D experiment, comparing different numbers of rankings (n).  The true density is represented by contour lines, and the estimated flow density is depicted as a heatmap. The marginal distributions are also shown.  The results indicate that while increasing the number of rankings improves coverage, there's also a slight increase in the spread of the estimated density, which explains a minor decrease in performance.", "section": "D Ablation studies"}, {"figure_path": "sRSjr9SDKR/figures/figures_28_2.jpg", "caption": "Figure 4: Illustration of belief densities elicited from pairwise comparisons by a normalizing flow.", "description": "This figure shows the results of eliciting belief densities using pairwise comparisons with a normalizing flow.  The plots showcase the learned belief densities from pairwise comparisons, illustrating the performance of the proposed method in recovering the underlying probability distributions. The figure highlights the effectiveness of the method in capturing the shape and distribution of the belief densities, even with limited pairwise comparison data. ", "section": "Experiments"}]