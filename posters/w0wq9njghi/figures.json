[{"figure_path": "W0wq9njGHi/figures/figures_1_1.jpg", "caption": "Figure 1: Full parameter sharing confines the policies to be homogeneous. In this example, all predators pursue the same prey, neglecting another prey in the game World. Further game details are in Appendix A.2.", "description": "This figure shows an example of how full parameter sharing in multi-agent reinforcement learning leads to homogeneous policies.  In a predator-prey scenario, all predators follow the same prey, ignoring other available options.  This illustrates a key limitation of full parameter sharing: it prevents the development of diverse and potentially more effective strategies.", "section": "1 Introduction"}, {"figure_path": "W0wq9njGHi/figures/figures_3_1.jpg", "caption": "Figure 2: Overall network architecture of Kaleidoscope. It maintains one set of parameters \u03b8\u2080 with N sets of masks [M\u1d62] for actor networks, and one set of parameters \u03d5\u2080 with K sets of masks [M\u2c7c] for critic ensemble networks, where N is the number of agents, K is the number of ensembles, and \u2299 denotes the Hadamard product.", "description": "This figure illustrates the architecture of the Kaleidoscope model.  It shows how a single set of parameters (\u03b8\u2080 for actors, \u03d5\u2080 for critics) is used across multiple agents.  Each agent has a unique learnable mask (M\u1d62 for actors, M\u2c7c for critics) that selectively gates the shared parameters, allowing for heterogeneity in the policies and value estimations while maintaining high sample efficiency. The Hadamard product (\u2299) combines the shared parameters with the agent-specific masks.", "section": "3 Learnable Masks for Heterogenous MARL"}, {"figure_path": "W0wq9njGHi/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration on resetting mechanisms.", "description": "This figure illustrates the resetting mechanisms used in Kaleidoscope for both actor networks and critic ensembles.  For actors, the weights masked by all agents are reinitialized with probability p, encouraging heterogeneity. For critic ensembles, one set of masks is reset at a time, promoting diversity among critic functions. The figure showcases how these resetting mechanisms prevent excessive sparsity and enhance the representational capacity of the network.", "section": "3 Learnable Masks for Heterogenous MARL"}, {"figure_path": "W0wq9njGHi/figures/figures_7_1.jpg", "caption": "Figure 4: Performance comparison with baselines on MPE and MaMuJoCo benchmarks.", "description": "This figure presents a comparison of the performance of Kaleidoscope and several baseline methods across eight different scenarios from the Multi-Agent Particle Environment (MPE) and Multi-Agent MuJoCo (MaMuJoCo) benchmarks.  Each subplot represents a different scenario, showing the average test return over training timesteps for each algorithm. The shaded areas represent 95% confidence intervals. The results demonstrate the superior performance of Kaleidoscope in most scenarios, highlighting its effectiveness in balancing sample efficiency and policy diversity.", "section": "4.2 Results"}, {"figure_path": "W0wq9njGHi/figures/figures_7_2.jpg", "caption": "Figure 5: Performance comparison with baselines on SMACv2 benchmarks.", "description": "This figure compares the performance of Kaleidoscope against several baselines on three different StarCraft II scenarios (Terran_5_vs_5, Protoss_5_vs_5, and Zerg_5_vs_5).  Each line represents the average test win rate over multiple runs for a particular algorithm.  The x-axis shows the training progress (timesteps), and the y-axis shows the win rate.  The figure visually demonstrates the superior performance of Kaleidoscope compared to other parameter sharing approaches across these complex multi-agent scenarios.", "section": "4.2 Results"}, {"figure_path": "W0wq9njGHi/figures/figures_8_1.jpg", "caption": "Figure 6: Ablation studies.", "description": "The ablation studies compare Kaleidoscope with three variations: one without the diversity regularization term, another without the parameter resetting mechanism, and a final version without critic ensembles using Kaleidoscope.  The results show that the diversity regularization is crucial for Kaleidoscope's performance, while the resetting mechanism offers additional benefits, particularly in later training stages.  The use of critic ensembles with Kaleidoscope also contributes significantly to the overall results.", "section": "3 Learnable Masks for Heterogenous MARL"}, {"figure_path": "W0wq9njGHi/figures/figures_9_1.jpg", "caption": "Figure 8: Visualization on World.", "description": "This figure visualizes the trained policies and pairwise mask differences among agents at different training timesteps in the \"World\" scenario.  (a) shows the trained policies, illustrating the agents' strategies. (b) presents heatmaps showing the pairwise differences between agents' masks at various training stages (1M, 2M, and 3M timesteps), highlighting how these differences evolve during training, indicating the development of distinct policies.", "section": "3.2 Policy diversity regularization"}, {"figure_path": "W0wq9njGHi/figures/figures_18_1.jpg", "caption": "Figure 9: Hyperparameter analysis.", "description": "The plots show the impact of hyperparameters \u03b1 (critic ensembles diversity coefficient) and \u03b2 (actors diversity coefficient) on the performance of Kaleidoscope.  The left plot shows that an excessively small \u03b1 leads to degraded performance due to the critic ensembles collapsing to a single critic, while an excessively large \u03b1 also degrades performance due to increased estimation bias. The right plot shows that a small \u03b2 leads to degraded performance because Kaleidoscope's parameter sharing reduces to full parameter sharing, causing policy homogeneity. An excessively large \u03b2 also negatively impacts performance. Optimal performance is achieved with \u03b1 and \u03b2 values between 0.1 and 1.", "section": "B.2 Hyperparameter Analysis"}, {"figure_path": "W0wq9njGHi/figures/figures_19_1.jpg", "caption": "Figure 8: Visualization on World.", "description": "This figure visualizes the trained policies of Kaleidoscope on the \"World\" environment at different training timesteps (1M, 2M, 3M).  The top panel shows the test return and diversity loss curves over time. The middle panel displays heatmaps representing pairwise mask differences between agents at different timesteps. The bottom panel uses t-SNE to visualize the trajectories of each agent.  The visualizations demonstrate how Kaleidoscope dynamically learns to share parameters (as indicated by the evolving mask differences) and how this leads to diverse and effective agent policies (as seen in their distinct trajectories).", "section": "3.2 Policy diversity regularization"}]