[{"figure_path": "W0wq9njGHi/tables/tables_6_1.jpg", "caption": "Table 1: Methods compared in the experiments. Here, \u201cadaptive\u201d indicates whether the sharing scheme evolves during training.", "description": "This table compares different multi-agent reinforcement learning (MARL) methods used in the paper's experiments.  Each method is categorized by its parameter sharing paradigm (no sharing, full sharing, or partial sharing), the level at which parameters are shared (networks, layers, neurons, or weights), whether the sharing scheme adapts during training, and a brief description of how parameters are shared.  This helps to understand the various approaches to parameter sharing and their differences.", "section": "4.1 Experimental Setups"}, {"figure_path": "W0wq9njGHi/tables/tables_15_1.jpg", "caption": "Table 3: Common hyperparameters used for MATD3 in the MaMuJoCo domain.", "description": "This table lists the common hyperparameters used for the MATD3 algorithm in the MaMuJoCo domain experiments.  It includes the number of layers in the neural network, the size of the hidden layers, the discount factor, the number of rollout threads, the learning rate for the critic and actor networks, exploration noise, batch size, replay buffer size, total number of environment steps, and the n-step parameter for the algorithm. These parameters are crucial in setting up the training process and significantly influence the results of the experiments.", "section": "A.1.3 Network architecture and hyperparameters"}, {"figure_path": "W0wq9njGHi/tables/tables_16_1.jpg", "caption": "Table 4: Common hyperparameters used for QMIX in the MPE domain.", "description": "This table lists the common hyperparameters used for the QMIX algorithm in the Multi-agent Particle Environment (MPE) domain.  It includes parameters such as the number of layers in the neural network, hidden layer sizes, discount factor, learning rate, exploration noise parameters (initial and final epsilon values), batch size, replay buffer size, number of training steps, and whether or not Double Q-learning is used.", "section": "A.1.3 Network architecture and hyperparameters"}, {"figure_path": "W0wq9njGHi/tables/tables_16_2.jpg", "caption": "Table 4: Common hyperparameters used for QMIX in the MPE domain.", "description": "This table shows the common hyperparameters used when applying the QMIX algorithm in the Multi-agent Particle Environment (MPE) domain.  The hyperparameters listed are those used for all algorithms and baselines used in the experiment and include the number of layers in the neural network, the hidden layer sizes, the discount factor (gamma), the learning rate, the initial and final exploration rates (epsilon), the batch size, the replay buffer size, the number of environment steps, and whether the Double Q learning technique was used.", "section": "A.1.3 Network architecture and hyperparameters"}, {"figure_path": "W0wq9njGHi/tables/tables_17_1.jpg", "caption": "Table 6: Hyperparameters used for Kaleidoscope.", "description": "This table lists the hyperparameters used in the Kaleidoscope model, categorized by environment (MaMuJoCo, MPE, and SMACv2) and hyperparameter type (actor diversity coefficient, actor reset probability, actor reset interval, number of critic ensembles, critic ensembles diversity coefficient, and critic reset interval).  Each hyperparameter's value is specified for each environment, demonstrating the model's adaptability to different experimental settings.", "section": "4.1 Experimental Setups"}, {"figure_path": "W0wq9njGHi/tables/tables_17_2.jpg", "caption": "Table 7: Environments details.", "description": "This table lists the experimental environments used in the paper. It includes the environment name (MaMuJoCo, MPE, SMACv2), action space type (continuous or discrete), agent types (homogeneous or heterogeneous), specific scenarios used within each environment and the number of agents involved in each scenario.", "section": "4.1 Experimental Setups"}, {"figure_path": "W0wq9njGHi/tables/tables_17_3.jpg", "caption": "Table 2: Averaged FLOPs (with calculation methods detailed in Appendix A.3) across different methods. Results are first normalized with respect to the FuPS + ID model for each scenario and then averaged across scenarios within each environment (detailed results in Appendix B.1). The lowest costs are highlighted in bold.", "description": "This table compares the computational cost (measured in FLOPs) of different multi-agent reinforcement learning methods. The FLOPs are normalized relative to the \"FuPS + ID\" method for each scenario and then averaged across scenarios within each environment. The lowest FLOPs for each environment are highlighted in bold, indicating the most computationally efficient methods.", "section": "4.2 Results"}]