[{"type": "text", "text": "Kaleidoscope: Learnable Masks for Heterogeneous Multi-agent Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xinran Li Ling Pan Jun Zhang ", "page_idx": 0}, {"type": "text", "text": "Department of Electronic and Computer Engineering The Hong Kong University of Science and Technology xinran.li@connect.ust.hk, lingpan@ust.hk, eejzhang@ust.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In multi-agent reinforcement learning (MARL), parameter sharing is commonly employed to enhance sample efficiency. However, the popular approach of full parameter sharing often leads to homogeneous policies among agents, potentially limiting the performance benefits that could be derived from policy diversity. To address this critical limitation, we introduce Kaleidoscope, a novel adaptive partial parameter sharing scheme that fosters policy heterogeneity while still maintaining high sample efficiency. Specifically, Kaleidoscope maintains one set of common parameters alongside multiple sets of distinct, learnable masks for different agents, dictating the sharing of parameters. It promotes diversity among policy networks by encouraging discrepancy among these masks, without sacrificing the efficiencies of parameter sharing. This design allows Kaleidoscope to dynamically balance high sample efficiency with a broad policy representational capacity, effectively bridging the gap between full parameter sharing and non-parameter sharing across various environments. We further extend Kaleidoscope to critic ensembles in the context of actor-critic algorithms, which could help improve value estimations. Our empirical evaluations across extensive environments, including multi-agent particle environment, multi-agent MuJoCo and StarCraft multi-agent challenge v2, demonstrate the superior performance of Kaleidoscope compared with existing parameter sharing approaches, showcasing its potential for performance enhancement in MARL. The code is publicly available at https://github.com/LXXXXR/Kaleidoscope. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Cooperative multi-agent reinforcement learning (MARL) has demonstrated remarkable effectiveness in solving complex real-world decision-making problems across various domains, such as resource allocation (Ying and Dayong, 2005), package delivery (Seuken and Zilberstein, 2007), autonomous driving (Zhou et al., 2021), and robot control (Swamy et al., 2020). To mitigate the challenges posed by the non-stationary and partially observable environments typical of MARL (Yuan et al., 2023), the centralized training with decentralized execution (CTDE) paradigm (Foerster et al., 2016) has become prevalent, inspiring many influential MARL algorithms such as MADDPG (Lowe et al., 2017), COMA (Foerster et al., 2018), MATD3 (Ackermann et al., 2019), QMIX (Rashid et al., 2020), and MAPPO (Yu et al., 2022). ", "page_idx": 0}, {"type": "text", "text": "Under the CTDE paradigm, parameter sharing among agents is a commonly adopted practice to improve sample efficiency. However, identical network parameters across agents often lead to homogeneous policies, restricting diversity in behaviors and the overall joint policy representational capacity. This limitation can result in undesired outcomes in certain situations (Christianos et al., 2021; Fu et al., 2022; Kim and Sung, 2023), as shown in Figure 1, impeding further performance gains. An alternative approach is the non-parameter sharing scheme, where each agent possesses its own unique parameters. Nevertheless, while this method naturally supports heterogeneous policies, it suffers from reduced sample efficiency, leading to significant training costs. This is particularly problematic given the current trend towards increasingly large model sizes, with some scaling to trillions of parameters (Zhao et al., 2023; Achiam et al., 2023). Therefore, it is imperative to develop a parameter sharing strategy that enjoys both high sample efficiency and broad policy representational capacity, potentially achieving significantly enhanced performance. While several efforts (Christianos et al., 2021; Kim and Sung, 2023) have explored partial parameter sharing initiated at the start of training, such initializations can be challenging to design without detailed knowledge of agent-specific environmental transitions or reward functions (Christianos et al., 2021). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we build upon insights from previous studies (Christianos et al., 2021; Fu et al., 2022; Kim and Sung, 2023) and introduce Kaleidoscope, a novel adaptive partial parameter sharing scheme. It maintains a single set of policy parameters and employs multiple learnable masks to designate the shared parameters. Unlike earlier methods that depend on fixed initializations, Kaleidoscope dynamically learns these masks alongside MARL parameters throughout the training process. This end-to-end training approach inherently integrates environmental information, and its adaptive nature enables Kaleidoscope to dynamically adjust the level of parameter sharing based on the demands of the environment and the learning progress of the agents. The learnable masks facilitate a dynamic balance between full parameter sharing and nonparameter sharing, offering a flexible trade-off between sample efficiency and policy representational capacity through enhanced heterogeneity. Initially, we build Kaleidoscope upon agent networks, where it achieves diverse policies. Following this success, we extend it to multi-agent actor-critic algorithms to encourage heterogeneity among the central critic ensembles for further performance enhancement. ", "page_idx": 1}, {"type": "image", "img_path": "W0wq9njGHi/tmp/54f59a7497493c4c75e7e586ed6bd9ac61bb489f8494a9b5022ea9fa1884bb6a.jpg", "img_caption": ["Figure 1: Full parameter sharing confines the policies to be homogeneous. In this example, all predators pursue the same prey, neglecting another prey in the game World. Further game details are in Appendix A.2. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Just like a kaleidoscope uses the reflective properties of rotating mirrors to transform simple shapes into beautiful patterns, our proposed method leverages learnable masks to map a single set of parameters into diverse policies, thereby enhancing task performance. ", "page_idx": 1}, {"type": "text", "text": "We summarize our contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 To enable policy heterogeneity among agents for better training flexibility, we adapt the soft threshold reparameterization (STR) technique to learn distinct masks for different agent networks while only maintaining one set of common parameters, effectively balancing between full parameter sharing and non-parameter sharing mechanisms. ", "page_idx": 1}, {"type": "text", "text": "\u2022 To enhance policy diversity among agents, we introduce a novel regularization term that encourages the pairwise discrepancy between masks. Additionally, we design resetting mechanisms that recycle masked parameters to preserve the representational capacity of the joint networks. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Through extensive experiments on MARL benchmarks, including multi-agent particle environment (MPE) (Lowe et al., 2017), multi-agent MuJoCo (MAMuJoCo) (Peng et al., 2021) and StarCraft multi-agent challenge v2 (SMACv2) (Ellis et al., 2024), we demonstrate the superior performance of Kaleidoscope over existing parameter sharing approaches. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Multi-agent reinforcement learning (MARL) In MARL, a fully cooperative partially observable multi-agent task is typically formulated as a decentralized partially observable Markov decision process (dec-POMDP) (Oliehoek and Amato, 2016), represented by a tuple $\\mathcal{M}=\\langle\\mathcal{S},A,P,R,\\bar{\\Omega},O,N,\\gamma\\rangle$ . Here, $N$ denotes the number of agents, and $\\gamma\\in\\left(0,1\\right]$ represents the discount factor. At each timestep $t$ , with the environment state as $s^{t}\\in\\mathcal S$ , agent $i$ receives a local observation $o_{i}^{t}\\in\\Omega$ drawn from the observation function $O(s^{t},i)$ and then follows its local policy $\\pi_{i}$ to select an action $a_{i}^{t}\\,\\in\\,A$ . Individual actions form a joint action $\\pmb{a}^{t}\\in A^{N}$ , leading to a state transition to the next state $s^{t+1}\\sim P(s^{t+1}|s^{t},\\pmb{a}^{t})$ and inducing a global reward $r^{t}=R(\\bar{s^{t}},\\mathbf{a}^{t})$ . The overall team objective is to learn the joint policies ${\\boldsymbol{\\pi}}=\\langle\\pi_{1},\\ldots,\\pi_{N}\\rangle$ such that the expectation of discounted accumulated reward $\\begin{array}{r}{G^{t}=\\sum_{t}\\bar{\\gamma^{t}}r^{t}}\\end{array}$ is maximized. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "To learn such policies $\\pi_{\\theta}$ , various MARL algorithms (Lowe et al., 2017; Foerster et al., 2018; Rashid et al., 2020; Yu et al., 2022) have been developed. For instance, the off-policy actor-critic algorithm MATD3 (Ackermann et al., 2019) serves as an example method. Specifically, the critic networks are updated by minimizing the temporal difference (TD) error loss ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c}(\\phi)=\\mathbb{E}_{(s^{t},o^{t},a^{t},r^{t},s^{t+1},o^{t+1})\\sim\\mathcal{D}}\\left[\\left(y^{t}-Q(s^{t},a^{t};\\phi)\\right)^{2}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with ", "page_idx": 2}, {"type": "equation", "text": "$$\ny^{t}=r^{t}+\\gamma\\operatorname*{min}_{j=1,2}Q(s^{t+1},\\pi_{1}(o_{1}^{t+1};\\theta_{1}^{\\prime})+\\epsilon,\\dots,\\pi_{N}(o_{N}^{t+1};\\theta_{N}^{\\prime})+\\epsilon;\\phi_{j}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\phi$ are the parameters for critics, $\\theta$ are the parameters for actor policies and $\\theta^{\\prime}$ are the parameters for target actor policies. And $\\epsilon$ is the clipped Gaussian noise, given as $\\mathrm{clip}(\\mathcal{N}(0,\\sigma),-c,\\bar{c})$ . ", "page_idx": 2}, {"type": "text", "text": "The policy is updated by the deterministic policy gradient algorithm (Silver et al., 2014) ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla\\mathcal{I}(\\theta_{i})=\\mathbb{E}_{(s^{t},o^{t},a^{t},r^{t},s^{t+1},o^{t+1})\\sim\\mathcal{D}}\\left[\\nabla_{\\theta_{i}}\\pi_{i}(o_{i}^{t};\\theta_{i})\\nabla_{a_{i}}Q(s^{t},a_{1},\\cdot\\,\\cdot\\,,a_{N}|_{a_{i}=\\pi_{i}(o_{i}^{t};\\theta_{i})};\\phi_{1})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Soft threshold reparameterization (STR) Originally introduced in the context of model sparsification, STR (Kusupati et al., 2020) is an unstructured pruning method that achieves notable performance without requiring a predetermined sparsity level. Specifically, STR applies a transformation to the original parameters $W$ as follows ", "page_idx": 2}, {"type": "equation", "text": "$$\nS_{g}(W,s)=\\mathrm{sign}(W)\\cdot\\mathrm{ReLU}\\left(|W|-g(s)\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $s$ is a learnable parameter, $\\alpha=g(s)$ serves as the pruning threshold, and $\\mathrm{ReLU}(\\cdot)=\\operatorname*{max}(\\cdot,0)$ . The original supervised learning problem modeled by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{W}\\mathcal{L}(W;\\mathcal{D})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with $\\mathcal{D}$ as the data is now transferred to ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{W,s}\\mathcal{L}(S_{g}(W,s);D).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Overall, this approach optimizes the learnable pruning threshold alongside the model parameters, facilitating dynamic adjustment to the sparsity level during training. ", "page_idx": 2}, {"type": "text", "text": "3 Learnable Masks for Heterogenous MARL ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we propose using learnable masks as a low-cost method to enable network heterogeneity in MARL. The core concept, illustrated in Figure 2, is to learn a single set of shared parameters complemented by multiple masks for distinct agents, specifying which parameters to share. ", "page_idx": 2}, {"type": "text", "text": "Specifically, in Section 3.1, we first adapt STR into a dynamic partially parameter sharing method, unlocking the joint policy network\u2019s capability to represent diverse policies among agents. In Section 3.2, we actively foster policy heterogeneity through a novel regularization term based on the masks. Given that the masking technique could excessively sparsify the network, potentially diminishing its representational capacity, in Section 3.3, we propose a straightforward remedy to periodically reset the parameters based on the outcomes of masking, which additionally mitigates primacy bias. Finally, in Section 3.4, we explore how to further extend this approach within the critic components of actor-critic algorithms to improve value estimations in MARL and further boost performance. ", "page_idx": 2}, {"type": "text", "text": "For the sake of clarity, we integrate the proposed Kaleidoscope with the MATD3 (Ackermann et al., 2019) algorithm to demonstrate the concept within this section. Nevertheless, as a versatile partial parameter-sharing technique, our method can readily be adapted to other MARL algorithms. We defer its integration with other MARL frameworks to Appendix A.1.2 and will evaluate them empirically in Section 4. ", "page_idx": 2}, {"type": "image", "img_path": "W0wq9njGHi/tmp/3634b85045ed898f18dc4b1075bc9bdc1110e9c13ac38618d9163bb74dfe603c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Overall network architecture of Kaleidoscope. It maintains one set of parameters $\\theta_{0}$ with $N$ sets of masks $[M_{i}]_{i=1}^{N}$ for actor networks, and one set of parameters $\\phi_{0}$ with $K$ sets of masks $\\left[M_{j}^{c}\\right]_{j=1}^{K}$ for critic ensemble networks, where $N$ is the number of agents, $K$ is the number of ensembles, and $\\odot$ denotes the Hadamard product. ", "page_idx": 3}, {"type": "text", "text": "3.1 Adaptive partial parameter sharing $\\spadesuit$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The core idea of this work is to learn distinct binary masks $M_{i}$ for different agents to facilitate differentiated policies, ultimately aiming to improve MARL performance. To achieve this, we apply the STR (Kusupati et al., 2020) technique to the policy parameters with different thresholds dedicated to each agent: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\theta_{i}=\\theta_{0}\\odot M_{i},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\theta_{i}$ parameterizes the policy for agent $i$ , $\\theta_{0}$ is the set of learnable parameters shared by all agents, and $M_{i}$ is the learnable mask for agent $i$ . Specifically, assume $\\theta_{0}\\:=\\:\\Big[\\theta_{0}^{(1)},\\cdot\\cdot\\cdot,\\theta_{0}^{(\\bar{N}_{a})}\\Big],$ $\\theta_{i}\\;=\\;\\left[\\theta_{i}^{(1)},\\ldots,\\theta_{i}^{(N_{a})}\\right]$ and $M_{i}\\;=\\;\\left[m_{i}^{(1)},\\ldots,m_{i}^{(N_{a})}\\right]$ with $N_{a}$ being the total parameter count of an agent\u2019s network. In line with STR, we compute each element $m_{i}^{(k)}$ of Mi as mi $m_{i}^{(k)}\\;=\\;$ $\\mathbb{1}\\left[|\\theta_{0}^{(k)}|>\\sigma(s_{i}^{(k)})\\right]$ , where $\\sigma(\\cdot)$ denotes the Sigmoid function. ", "page_idx": 3}, {"type": "text", "text": "The benefits of such a combination are summarized as follows: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Preservation of original MARL learning objectives: Unlike most of the methods in pruning literature, which primarily aim to minimize the discrepancies between pruned and unpruned networks in terms of weights, loss, or activations (Hoefler et al., 2021; Menghani, 2023; Deng et al., 2020), STR maintains the original goal of minimizing task-specific loss, aligning directly with our objectives to enhance MARL performance.   \n\u2022 Flexibility in sparsity: Many classical pruning methods require predefined per-layer sparsity levels (Evci et al., 2020; Ramanujan et al., 2020). Such requirements can complicate our design, with the goal not to gain extreme sparsity but rather to promote heterogeneity through masking. The STR technique is ideal in our case as it does not require predefining sparsity levels, allowing for adaptive learning of the masks.   \n\u2022 Enhanced network representational capacity: Utilizing learnable masks for adaptive partial parameter sharing enhances the network\u2019s representational capacity beyond traditional full parameter sharing. In full parameter sharing, agents\u2019 joint policies are parameterized as $\\begin{array}{r l r}{\\pi^{\\mathrm{ps}}(\\cdot|\\theta_{0})}&{{}=}&{\\langle\\pi_{1}(\\cdot|\\theta_{0}),\\dots,\\pi_{N}(\\cdot|\\theta_{0})\\rangle}\\end{array}$ . In contrast, our proposed adaptive partial parameter sharing mechanism parameterizes the joint policies as $\\pi$ K $\\mathsf{i e i d o s c o p}\\bar{\\mathbf{e}}_{\\left(\\cdot\\right|\\theta_{0},\\mathsf{\\bar{\\b{M}}}\\right)}=\\bar{\\langle}\\pi_{1}(\\cdot|\\theta_{0}\\odot M_{1}),\\bar{\\dots},\\pi_{n}(\\cdot|\\theta_{0}\\odot\\bar{M}_{\\underline{{N}}})\\rangle.$ In the extreme case where all the values in $M_{i}$ are 1s, the function set represented by $\\pi^{\\mathrm{Kaleidoscope}}(\\cdot|\\theta_{0},M)$ degrades to that of $\\pi^{\\mathrm{ps}}(\\cdot|\\theta_{0})$ . In other scenarios, it is a superset of that represented by $\\pi^{\\mathrm{ps}}(\\cdot|\\theta_{0})$ . ", "page_idx": 3}, {"type": "image", "img_path": "W0wq9njGHi/tmp/8aa0a1e14254e9a234c24c232ff2b75d390dfd0156737c426252bfd21a12c2c7.jpg", "img_caption": ["(a) Actors: reinitialize the weights that are masked by (b) Critic ensembles: reset one set of masks at a time. all agents with probability $\\rho$ . ", "Figure 3: Illustration on resetting mechanisms. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.2 Policy diversity regularization \u2663 ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While independently learned masks enable agents to develop distinct policies, without a specific incentive, these policies may still converge to being homogeneous. To this end, we propose to explicitly encourage agent policy heterogeneity by introducing a diversity regularization term maximizing the weighted pairwise distance between network masks, which is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{J}^{\\mathrm{div}}(\\pmb{s})=\\sum_{i=1,\\ldots,n}\\sum_{\\stackrel{j=1,\\ldots,n}{j\\neq i}}\\|\\theta_{0}\\odot(M_{i}-M_{j})\\|_{1}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This term is inherently non-differentiable due to the indicator function $\\mathbb{I}[\\cdot]$ inside $_M$ . To overcome this difficulty, following established practices in the literature (Bengio et al., 2013; Alizadeh et al., 2018), we utilize a surrogate function for gradient approximation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{J}^{\\mathrm{div}}}{\\partial g(\\ensuremath{\\boldsymbol{s}}_{i})}=-\\mathrm{tanh}\\left[\\frac{\\partial\\mathcal{J}^{\\mathrm{div}}}{\\partial M_{i}}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We formally provide the overall training objective for actors in Appendix A.1.1. ", "page_idx": 4}, {"type": "text", "text": "3.3 Periodically reset $\\spadesuit$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As the training with masks proceeds, we observe an increasing sparsity in each agent\u2019s network, potentially reducing the overall network capacity. To remedy the issue, we propose a simple approach to periodically reset the parameters that are consistently masked across all $M_{i}$ with a certain probability $\\rho$ , which is illustrated in Figure 3a. At intervals defined by $t$ mod reset_interval $==0$ , if the parameter index $k$ satisfies $\\forall i,m_{i}^{\\Bar{(k)}}==0$ , we apply the following resetting rule ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta_{0}^{(k)},s_{1}^{(k)},\\ldots,s_{N}^{(k)}\\gets\\left\\{\\!\\!\\begin{array}{l l}{\\mathrm{Reinitialize}[\\theta_{0}^{(k)},s_{1}^{(k)},\\ldots,s_{N}^{(k)}]}&{\\mathrm{with~probability~}\\rho}\\\\ {\\theta_{0}^{(k)},s_{1}^{(k)},\\ldots,s_{N}^{(k)}}&{\\mathrm{with~probability~}1-\\rho}\\end{array}\\!\\!.\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This resetting mechanism recycles the weights masked as zeros by all the masks, preventing the networks from becoming overly sparse. A side benefit of this resetting mechanism is the enhancement of neural plasticity (Lyle et al., 2023; Nikishin et al., 2024), which helps alleviate the primacy bias (Nikishin et al., 2022) in reinforcement learning. Unlike methods that reinitialize entire layers resulting in abrupt performance drops (Nikishin et al., 2022), our resetting approach selectively targets weights as indicated by the learnable masks, thus avoiding significant performance disruptions, as shown in Section 4. ", "page_idx": 4}, {"type": "text", "text": "3.4 Critic ensembles with learnable masks ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In actor-critic algorithm frameworks, we further apply Kaleidoscope to central critics as an efficient way to implement ensemble-like critics. By facilitating dynamic partial parameter sharing, Kaleidoscope enables heterogeneity among critic ensembles. Furthermore, by regularizing the diversity among critic functions, we can control ensemble variances. This approach is elaborated in subsequent paragraphs. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "$\\spadesuit$ Adaptive partial parameter sharing for critic ensembles In the standard MATD3 algorithm (Ackermann et al., 2019), two critics with independent parameters are maintained to mitigate overestimation risks. However, using separate parameters typically results in a low update-to-data (UTD) ratio (Hiraoka et al., 2022). To address this issue, we propose to enhance the UTD ratio by employing Kaleidoscope parameter sharing among ensembles of critics. Specifically, we maintain a single set of parameters $\\phi_{0}$ and $K$ masks $\\left[M_{j}^{c}\\right]_{j=1}^{K}$ to distinguish the critic functions, resulting in $K$ ensembles $[Q(\\cdot;\\phi_{j})]_{j=1}^{K}$ with $\\phi_{j}=\\phi_{0}\\odot M_{j}^{c}$ . ", "page_idx": 5}, {"type": "text", "text": "To be specific, we update the critic networks by minimizing the temporal difference (TD) error loss ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c}(\\boldsymbol{\\phi}_{j})=\\mathbb{E}_{(s^{t},a^{t},s_{t+1})\\sim\\mathcal{D}}\\left[\\left(y^{t}-Q(s^{t},a^{t};\\boldsymbol{\\phi}_{j})\\right)^{2}\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with ", "page_idx": 5}, {"type": "equation", "text": "$$\ny^{t}=r^{t}+\\gamma\\operatorname*{min}_{j=1,\\dots,K}Q(s^{t+1},\\pi_{1}(o_{1}^{t+1};\\theta_{1}^{\\prime})+\\epsilon,\\dots,\\pi_{n}(o_{N}^{t+1};\\theta_{N}^{\\prime})+\\epsilon;\\phi_{j}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "And the policies are updated by the mean estimation of the ensembles as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla{\\mathcal{J}}(\\theta_{i})=\\mathbb{E}_{s^{t}\\sim{\\mathcal{D}}}\\left[\\nabla_{\\theta_{i}}\\pi_{i}(o_{i}^{t};\\theta_{i})\\nabla_{a_{i}}{\\frac{1}{K}}\\sum_{j=1}^{K}\\left[Q(s^{t},a_{1},\\dots,a_{N}|_{a_{i}=\\pi_{i}(o_{i}^{t};\\theta_{i})};\\phi_{j})\\right]\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$\\clubsuit$ Critic ensembles diversity regularization As in Section 3.2, we also apply diversity regularization to critic masks to prevent critics functions from collapsing to identical ones. The diversity regularization to maximize for the critic ensembles is expressed as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{I}_{c}^{\\mathrm{div}}(s^{c})=\\sum_{i=1,\\ldots,K}\\sum_{\\stackrel{j=1,\\ldots,K}{j\\neq i}}\\|\\phi_{0}\\odot(M_{i}^{c}-M_{j}^{c})\\|_{1}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Intuitively, as training progresses, this term encourages divergence among the critic masks, leading to increased model estimation uncertainty. This process fosters a gradual shift from overestimation to underestimation. As discussed in prior research (Hiraoka et al., 2022; Lan et al., 2020; Chen et al., 2021; Wang et al., 2021b), overestimation can encourage exploration, beneficial in early training stages, whereas underestimation alleviates error accumulation (Fujimoto et al., 2018), which is preferred in the late training stage. We formally provide the overall training objective for critic ensembles in Appendix A.1.1. ", "page_idx": 5}, {"type": "text", "text": "$\\spadesuit$ Periodically reset To further promote diversity among critic ensembles and counteract the reduction in network capacity caused by masking, we implement a resetting mechanism similar to that described in Section 3.3. In particular, we sequentially reinitialize the masks $M_{j}^{c}$ following a cyclic pattern, as illustrated in Figure 3b. In this way, each critic function\u2019s mask is trained on distinct data segments, leading to different biases. ", "page_idx": 5}, {"type": "text", "text": "In summary, by adopting Kaleidoscope parameter sharing with learnable masks, we establish a cost-effective implementation for critic ensembles that enjoy a high UTD ratio. Through enforcing distinctiveness among the masks, we subtly control the differences among critic functions, thereby improving the value estimations in MARL. ", "page_idx": 5}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we integrate Kaleidoscope with the value-based MARL algorithm QMIX and the actor-critic MARL algorithm MATD3, and evaluate them across eleven scenarios in three benchmark tasks. ", "page_idx": 5}, {"type": "table", "img_path": "W0wq9njGHi/tmp/f6522c3514703a06962f06e499512cb340df3a5819f3e57983f6a34e42efd4c0.jpg", "table_caption": ["Table 1: Methods compared in the experiments. Here, \u201cadaptive\u201d indicates whether the sharing scheme evolves during training. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setups ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Environment descriptions We test our proposed Kaleidoscope on three benchmark tasks: MPE (Lowe et al., 2017), MaMuJoCo (Peng et al., 2021) and SMACv2 (Ellis et al., 2024). For the discrete tasks MPE and SMACv2, we integrate Kaleidoscope and baselines with QMIX (Rashid et al., 2020) and assess the performance. For the continuous task MaMuJoCo, we employ MATD3 (Ackermann et al., 2019). We use five random seeds for MPE and MaMuJoCo and three random seeds for SMACv2, reporting averaged results and displaying the $95\\%$ confidence interval with shaded areas. The chosen benchmark tasks reflect a mix of discrete and continuous action spaces and both homogeneous and heterogeneous agent types, detailed further in Appendix A.2. ", "page_idx": 6}, {"type": "text", "text": "Baselines In the following, we compare our proposed Kaleidoscope with baselines (Christianos et al., 2021; Kim and Sung, 2023), as listed in Table 1. For both Kaleidoscope and the baselines, in scenarios with fixed agent types (MPE and MaMuJoCo), we assign one mask per agent. For SMACv2, where agent types vary, we assign one mask per agent type. We use official implementations of the baselines where available; otherwise, we closely follow the descriptions from their respective papers, integrating them into QMIX or MATD3. Hyperparameters and further details are provided in Appendix A.1.3. ", "page_idx": 6}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Performance We present the comparative performance of Kaleidoscope and baselines in Figure 4 and Figure 5. Overall, Kaleidoscope demonstrates superior performance, attributable to the flexibility of the learnable masks and the effectiveness of diversity regularization. Additionally, we observe that $\\mathrm{FuPS}\\,+\\,\\mathrm{ID}$ generally outperforms NoPS, except for the $\\mathrm{Ant-v}2{-}4\\mathrm{x}2$ scenario (Figure 4c). This advantage is largely due to FuPS\u2019s higher sample efficiency; a single transition data sample updates the model parameters $N$ times in $\\mathrm{FuPS}+\\mathrm{ID}$ , once for each agent, compared to just once in NoPS. Consequently, $\\mathrm{FuPS}+\\mathrm{ID}$ models learn faster from the same number of transitions. Similarly, Kaleidoscope benefits from this mechanism as it shares weights among agents, allowing a single transition to update the model parameters multiple times. Furthermore, by integrating policy heterogeneity through learnable masks, Kaleidoscope enables diverse agent behaviors, as illustrated in the visualization results in Figure 8. Ultimately, Kaleidoscope effectively balances parameter sharing and diversity, outperforming both full parameter sharing and non-parameter sharing approaches. ", "page_idx": 6}, {"type": "text", "text": "Cost analysis Despite its superior performance, Kaleidoscope does not increase computational complexity at test time compared to the baselines. We report the test time averaged FLOPs comparison of Kaleidoscope and baselines in Table 2. We see that due to the masking technique, Kaleidoscope has lower FLOPs compared to baselines, thereby enjoying a faster inference speed when being deployed. ", "page_idx": 6}, {"type": "image", "img_path": "W0wq9njGHi/tmp/796f659b9596fde57f066e456b2fb491e3d878f70f05bdfacfabf9a0f7623924.jpg", "img_caption": ["Figure 4: Performance comparison with baselines on MPE and MaMuJoCo benchmarks. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "W0wq9njGHi/tmp/955292e7422c7116ad604eb5fd19f1502e4f85f11d20496ce80953d81b74fde4.jpg", "img_caption": ["Figure 5: Performance comparison with baselines on SMACv2 benchmarks. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Ablation studies We conduct ablation studies to assess the impact of key components in Kaleidoscope, with results presented in Figure 6. Specifically, we compare Kaleidoscope with three ablations: 1) Kaleidoscope w/o reg, which lacks the regularization term in Equation (8) that encourages the masks to be distinct. 2) Kaleidoscope w/o reset, which does not reset parameters. 3) Kaleidoscope w/o ce, which does not use Kaleidoscope parameter sharing in critic ensembles and instead maintains two independent sets of parameters for critics. From the results, we observe that diversity regularization contributes the most to the performance of Kaleidoscope. Without it, masking degrades the performance due to the reduced number of parameters in each policy network. Resetting primarily aids learning in the late stages of training when needed, which aligns with the observation made by Nikishin et al. (2022). Notably, even with resetting, the performance does not experience abrupt drops thanks to the guidance provided by the masks on where to reset. When ablating the critic ensembles with Kaleidoscope parameter sharing, we observe inferior performance from the beginning of the training. This is because the critic ensembles with Kaleidoscope parameter sharing enable a higher UTD ratio of the critics, as discussed in Section 3.4. ", "page_idx": 7}, {"type": "text", "text": "Furthermore, we conduct experiments to study the impact of mask designs. The results are shown in Figure Figure 7. Specifically, we compare original Kaleidoscope with two alternative mask design choices: 1) Kaleidoscope w/ neuron masks, where adaptive masking techniques are applied to neurons rather than weights. 2) Kaleidoscope w/ fixed masks, where the masks are initialized at the beginning of training and kept fixed throughout the learning process. The results show that performance drops with either alternative design choice, demonstrating that Kaleidoscope\u2019s superior performance originates from the flexibility of the learnable masks on weights. ", "page_idx": 7}, {"type": "text", "text": "More results on hyperparameter analysis are included in Appendix B.2. ", "page_idx": 7}, {"type": "text", "text": "Table 2: Averaged FLOPs (with calculation methods detailed in Appendix A.3) across different methods. Results are first normalized with respect to the $\\mathrm{FuPS}+\\mathrm{ID}$ model for each scenario and then averaged across scenarios within each environment (detailed results in Appendix B.1). The lowest costs are highlighted in bold. ", "page_idx": 8}, {"type": "image", "img_path": "W0wq9njGHi/tmp/6af4698fff4cf75b4c021e9ccf2bc494eece69af47c5dbc3cf7f80ee92a29b5e.jpg", "img_caption": ["", ""], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Visualization We visualize the trained policies of Kaleidoscope on World, as shown in Figure 8a. The agents exhibit cooperative divide-and-conquer strategies (four red agents divide into two teams and surround the preys), contrasting with the homogeneous policies depicted in Figure 1. We further examine the distinctions in the agents\u2019 masks and present the results in Figure 8b. First, we observe that by the end of the training, each agent has developed a unique mask, revealing that distinct masks facilitate diverse policies by selectively activating different segments of the neural network weights. Second, throughout the training process, we note that the differences among the agents\u2019 masks evolve dynamically. This observation confirms that Kaleidoscope effectively enables dynamic parameter sharing among the agents based on the learning progress, empowered by the adaptability of the learnable masks. More visualization results are provided in Appendix B.3. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Parameter sharing First introduced by Tan (1993), parameter sharing has been widely adopted in MARL algorithms (Foerster et al., 2018; Rashid et al., 2020; Yu et al., 2022), due to its simplicity and high sample efficiency (Grammel et al., 2020). However, schemes without parameter sharing typically offer greater flexibility for policy representation. To balance sample efficiency with policy representational capacity, some research efforts aim to find effective partial parameter sharing schemes. Notably, SePS (Christianos et al., 2021) first clusters agents based on their transitions at the start of training and restricts parameter sharing within these clusters. Subsequently, SNP (Kim and Sung, 2023) enables partial parameter sharing by utilizing the lottery ticket hypothesis (Su et al., 2020) to initialize heterogeneous network structures. Concurrent to our work, AdaPS (Li et al., 2024) combines SNP and SePS by proposing a cluster-based partial parameter sharing scheme. While these methods have shown promise in certain domains, their performance potential is often limited by the static nature of the parameter sharing schemes set early in training. Our proposed Kaleidoscope distinguishes itself by dynamically learning specific parameter sharing configurations alongside the development of MARL policies, thereby offering enhanced training flexibility. ", "page_idx": 8}, {"type": "text", "text": "Agent heterogeneity in MARL To incorporate agent heterogeneity in MARL and enable diverse behaviors among agents, previous methods have explored concepts such as diversity and roles. Specifically, diversity-based approaches aim to enhance pairwise distinguishability among agents based on identities (Jiang and Lu, 2021), trajectories (Li et al., 2021), or credits assignment (Liu et al., 2023; Hu et al., 2023) through contrastive learning techniques. Concurrently, role-based strategies, sometimes referred to as skills (Yang et al., 2020) or subtasks (Yuan et al., 2022), employ conditional policies to differentiate agents by assigning them to various conditions. These conditions may be based on agent identities (Yang et al., 2022), local observations (Yang et al., 2020), local histories (Wang et al., 2020, 2021a; Yuan et al., 2022) or joint histories (Liu et al., 2021; Iqbal et al., 2022; Zeng et al., 2023). This line of researches mainly focus on module design and operate separately from parameter-level adjustments, making them orthogonal to our approach. Nevertheless, integrating these methods with our work could potentially enhance performance further. ", "page_idx": 8}, {"type": "image", "img_path": "W0wq9njGHi/tmp/77008aaa2a334d100e148e8b7e0416ab59ff2ad4c7d9f1443c8418e50874b0c0.jpg", "img_caption": ["Figure 8: Visualization on World. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Sparse networks in deep reinforcement learning (RL) Although relatively few, there are some noteworthy recent attempts to find sparse networks for deep RL. In particular, PoPS (Livne and Cohen, 2020) prunes the dense networks post-training, achieving significantly reduced execution time complexity. Additionally, (Yu et al., 2020) validate the lottery ticket hypothesis within the RL domain, producing high-performance models even under extreme pruning rates. Subsequent efforts, including DST (Sokar et al., 2022), TE-RL\\* (Graesser et al., 2022) and RLx2 (Tan et al., 2023) employ topology evolution (TE) techniques to further decrease the training costs. While these developments utilize sparse training techniques, which are similar to the methods we employ, their primary focus is on reducing training and execution costs in single-agent settings. In contrast, our work leverages sparse network strategies as a means to enhance parameter sharing techniques, aiming to improve MARL performance. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduced Kaleidoscope, a novel adaptive partial parameter sharing mechanism for MARL. It leverages distinct learnable masks to facilitate network heterogeneity, applicable to both agent policies and critic ensembles. Specifically, Kaleidoscope is built on three technical components: STR-empowered learnable masks, network diversity regularization, and a periodic resetting mechanism. When applied to agent policy networks, Kaleidoscope balances sample efficiency and network representational capacities. In the context of critic ensembles, it improves value estimations. By combining our proposed Kaleidoscope with QMIX and MATD3, we have empirically demonstrated its effectiveness across various MARL benchmarks. This study shows great promises in developing adaptive partial parameter sharing mechanisms to enhance the performance of MARL. For future work, it is interesting to further extend Kaleidoscope to other domains such as offline MARL or meta-RL. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the Hong Kong Research Grants Council under the NSFC/RGC Collaborative Research Scheme grant CRS_HKUST603/22. And we thank the anonymous reviewers for their valuable feedback and suggestions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774 (2023). ", "page_idx": 9}, {"type": "text", "text": "Johannes Ackermann, Volker Gabler, Takayuki Osa, and Masashi Sugiyama. 2019. Reducing overestimation bias in multi-agent domains using double centralized critics. arXiv preprint arXiv:1910.01465 (2019). ", "page_idx": 9}, {"type": "text", "text": "Milad Alizadeh, Javier Fern\u00e1ndez-Marqu\u00e9s, Nicholas D Lane, and Yarin Gal. 2018. An empirical study of binary neural networks\u2019 optimisation. In Proceedings of the International Conference on Learning Representations.   \nYoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 (2013).   \nXinyue Chen, Che Wang, Zijian Zhou, and Keith W. Ross. 2021. Randomized Ensembled Double Q-Learning: Learning Fast Without a Model. In Proceedings of the 9th International Conference on Learning Representations.   \nFilippos Christianos, Georgios Papoudakis, Arrasy Rahman, and Stefano V. Albrecht. 2021. Scaling Multi-Agent Reinforcement Learning with Selective Parameter Sharing. In Proceedings of the 38th International Conference on Machine Learning, Vol. 139. PMLR, 1989\u20131998.   \nLei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. 2020. Model compression and hardware acceleration for neural networks: A comprehensive survey. Proc. IEEE 108, 4 (2020), 485\u2013532.   \nBenjamin Ellis, Jonathan Cook, Skander Moalla, Mikayel Samvelyan, Mingfei Sun, Anuj Mahajan, Jakob Foerster, and Shimon Whiteson. 2024. Smacv2: An improved benchmark for cooperative multi-agent reinforcement learning. In Advances in Neural Information Processing Systems, Vol. 36.   \nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. 2020. Rigging the lottery: Making all tickets winners. In Proceedings of the International Conference on Machine Learning. PMLR, 2943\u20132952.   \nJakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. 2016. Learning to communicate with deep multi-agent reinforcement learning. In Advances in Neural Information Processing Systems, Vol. 29.   \nJakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. 2018. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.   \nWei Fu, Chao Yu, Zelai Xu, Jiaqi Yang, and Yi Wu. 2022. Revisiting Some Common Practices in Cooperative Multi-Agent Reinforcement Learning. In Proceedings of the International Conference on Machine Learning. PMLR, 6863\u20136877.   \nScott Fujimoto, Herke van Hoof, and David Meger. 2018. Addressing Function Approximation Error in Actor-Critic Methods. In Proceedings of the 35th International Conference on Machine Learning, Vol. 80. PMLR, 1582\u20131591.   \nLaura Graesser, Utku Evci, Erich Elsen, and Pablo Samuel Castro. 2022. The state of sparse training in deep reinforcement learning. In Proceedings of the International Conference on Machine Learning. PMLR, 7766\u20137792.   \nNathaniel Grammel, Sanghyun Son, Benjamin Black, and Aakriti Agrawal. 2020. Revisiting parameter sharing in multi-agent deep reinforcement learning. arXiv preprint arXiv:2005.13625 (2020).   \nTakuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi Onishi, and Yoshimasa Tsuruoka. 2022. Dropout Q-Functions for Doubly Efficient Reinforcement Learning. In The Tenth International Conference on Learning Representations.   \nTorsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. 2021. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. Journal of Machine Learning Research 22, 241 (2021), 1\u2013124.   \nJian Hu, Siyang Jiang, Seth Austin Harding, Haibin Wu, and Shih wei Liao. 2021. Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning. (2021). arXiv:2102.03479 [cs.LG]   \nZican Hu, Zongzhang Zhang, Huaxiong Li, Chunlin Chen, Hongyu Ding, and Zhi Wang. 2023. Attention-Guided Contrastive Role Representations for Multi-Agent Reinforcement Learning. arXiv preprint arXiv:2312.04819 (2023).   \nShariq Iqbal, Robby Costales, and Fei Sha. 2022. ALMA: Hierarchical learning for composite multi-agent tasks. In Advances in Neural Information Processing Systems, Vol. 35. 7155\u20137166.   \nJiechuan Jiang and Zongqing Lu. 2021. The emergence of individuality. In Proceedings of the International Conference on Machine Learning. PMLR, 4992\u20135001.   \nWoojun Kim and Youngchul Sung. 2023. Parameter Sharing with Network Pruning for Scalable Multi-Agent Deep Reinforcement Learning. In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems. 1942\u20131950.   \nAditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, and Ali Farhadi. 2020. Soft threshold weight reparameterization for learnable sparsity. In Proceedings of the International Conference on Machine Learning. PMLR, 5544\u20135555.   \nQingfeng Lan, Yangchen Pan, Alona Fyshe, and Martha White. 2020. Maxmin Q-learning: Controlling the Estimation Bias of Q-learning. In Proceedings of the 8th International Conference on Learning Representations, Addis Ababa, Ethiopia, April 26-30, 2020.   \nChenghao Li, Tonghan Wang, Chengjie Wu, Qianchuan Zhao, Jun Yang, and Chongjie Zhang. 2021. Celebrating diversity in shared multi-agent reinforcement learning. In Advances in Neural Information Processing Systems, Vol. 34. 3991\u20134002.   \nDapeng Li, Na Lou, Bin Zhang, Zhiwei Xu, and Guoliang Fan. 2024. Adaptive parameter sharing for multi-agent reinforcement learning. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 6035\u20136039.   \nBo Liu, Qiang Liu, Peter Stone, Animesh Garg, Yuke Zhu, and Anima Anandkumar. 2021. Coachplayer multi-agent reinforcement learning for dynamic team composition. In Proceedings of the International Conference on Machine Learning. PMLR, 6860\u20136870.   \nShunyu Liu, Yihe Zhou, Jie Song, Tongya Zheng, Kaixuan Chen, Tongtian Zhu, Zunlei Feng, and Mingli Song. 2023. Contrastive identity-aware learning for multi-agent value decomposition. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 11595\u201311603.   \nDor Livne and Kobi Cohen. 2020. Pops: Policy pruning and shrinking for deep reinforcement learning. IEEE Journal of Selected Topics in Signal Processing 14, 4 (2020), 789\u2013801.   \nRyan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. 2017. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, Vol. 30.   \nClare Lyle, Zeyu Zheng, Evgenii Nikishin, Bernardo Avila Pires, Razvan Pascanu, and Will Dabney. 2023. Understanding plasticity in neural networks. In Proceedings of the International Conference on Machine Learning. PMLR, 23190\u201323211.   \nGaurav Menghani. 2023. Efficient deep learning: A survey on making deep learning models smaller, faster, and better. Comput. Surveys 55, 12 (2023), 1\u201337.   \nEvgenii Nikishin, Junhyuk Oh, Georg Ostrovski, Clare Lyle, Razvan Pascanu, Will Dabney, and Andr\u00e9 Barreto. 2024. Deep reinforcement learning with plasticity injection. In Advances in Neural Information Processing Systems, Vol. 36.   \nEvgenii Nikishin, Max Schwarzer, Pierluca D\u2019Oro, Pierre-Luc Bacon, and Aaron Courville. 2022. The primacy bias in deep reinforcement learning. In Proceedings of the International Conference on Machine Learning. PMLR, 16828\u201316847.   \nFrans A Oliehoek and Christopher Amato. 2016. A concise introduction to decentralized POMDPs. Springer.   \nGeorgios Papoudakis, Filippos Christianos, Lukas Sch\u00e4fer, and Stefano V. Albrecht. 2021. Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. http://arxiv.org/abs/2006.07869   \nBei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr, Wendelin B\u00f6hmer, and Shimon Whiteson. 2021. Facmac: Factored multi-agent centralised policy gradients. In Advances in Neural Information Processing Systems, Vol. 34. 12208\u201312221.   \nVivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad Rastegari. 2020. What\u2019s hidden in a randomly weighted neural network?. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 11893\u201311902.   \nTabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. 2020. Monotonic value function factorisation for deep multi-agent reinforcement learning. The Journal of Machine Learning Research 21, 1 (2020), 7234\u20137284.   \nSven Seuken and Shlomo Zilberstein. 2007. Improved memory-bounded dynamic programming for decentralized POMDPs. In Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence. 344\u2013351.   \nDavid Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. 2014. Deterministic policy gradient algorithms. In Proceedings of the International Conference on Machine Learning. Pmlr, 387\u2013395.   \nGhada Sokar, Elena Mocanu, Decebal Constantin Mocanu, Mykola Pechenizkiy, and Peter Stone. 2022. Dynamic Sparse Training for Deep Reinforcement Learning. In Proceedings of the ThirtyFirst International Joint Conference on Artificial Intelligence. ijcai.org, 3437\u20133443.   \nJingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, and Jason D Lee. 2020. Sanity-checking pruning methods: Random tickets can win the jackpot. In Advances in Neural Information Processing Systems, Vol. 33. 20390\u201320401.   \nGokul Swamy, Siddharth Reddy, Sergey Levine, and Anca D Dragan. 2020. Scaled autonomy: Enabling human operators to control robot fleets. In 2020 IEEE International Conference on Robotics and Automation. IEEE, 5942\u20135948.   \nMing Tan. 1993. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of the 10th International Conference on Machine Learning. 330\u2013337.   \nYiqin Tan, Pihe Hu, Ling Pan, Jiatai Huang, and Longbo Huang. 2023. RLx2: Training a Sparse Deep Reinforcement Learning Model from Scratch. In Proceedings of the Eleventh International Conference on Learning Representations.   \nHang Wang, Sen Lin, and Junshan Zhang. 2021b. Adaptive Ensemble Q-learning: Minimizing Estimation Bias via Error Feedback. In Advances in Neural Information Processing Systems. 24778\u201324790.   \nTonghan Wang, Heng Dong, Victor Lesser, and Chongjie Zhang. 2020. ROMA: multi-agent reinforcement learning with emergent roles. In Proceedings of the 37th International Conference on Machine Learning. 9876\u20139886.   \nTonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and Chongjie Zhang. 2021a. RODE: Learning Roles to Decompose Multi-Agent Tasks. In Proceedings of the 9th International Conference on Learning Representations.   \nJiachen Yang, Igor Borovikov, and Hongyuan Zha. 2020. Hierarchical Cooperative Multi-Agent Reinforcement Learning with Skill Discovery. In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems. 1566\u20131574.   \nMingyu Yang, Jian Zhao, Xunhan Hu, Wengang Zhou, Jiangcheng Zhu, and Houqiang Li. 2022. LDSA: Learning dynamic subtask assignment in cooperative multi-agent reinforcement learning. In Advances in Neural Information Processing Systems, Vol. 35. 1698\u20131710.   \nWang Ying and Sang Dayong. 2005. Multi-agent framework for third party logistics in E-commerce. Expert Systems with Applications 29, 2 (2005), 431\u2013436.   \nChao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. 2022. The surprising effectiveness of ppo in cooperative multi-agent games. In Advances in Neural Information Processing Systems, Vol. 35. 24611\u201324624.   \nHaonan Yu, Sergey Edunov, Yuandong Tian, and Ari S. Morcos. 2020. Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP. In Proceedings of the 8th International Conference on Learning Representations.   \nLei Yuan, Chenghe Wang, Jianhao Wang, Fuxiang Zhang, Feng Chen, Cong Guan, Zongzhang Zhang, Chongjie Zhang, and Yang Yu. 2022. Multi-Agent Concentrative Coordination with Decentralized Task Representation.. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence. 599\u2013605.   \nLei Yuan, Ziqian Zhang, Lihe Li, Cong Guan, and Yang Yu. 2023. A Survey of Progress on Cooperative Multi-agent Reinforcement Learning in Open Environment. arXiv preprint arXiv:2312.01058 (2023).   \nXianghua Zeng, Hao Peng, and Angsheng Li. 2023. Effective and stable role-based multi-agent collaboration by structural information principles. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 11772\u201311780.   \nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).   \nYifan Zhong, Jakub Grudzien Kuba, Xidong Feng, Siyi Hu, Jiaming Ji, and Yaodong Yang. 2024. Heterogeneous-agent reinforcement learning. Journal of Machine Learning Research 25 (2024), 1\u201367.   \nMing Zhou, Jun Luo, Julian Villella, Yaodong Yang, David Rusu, Jiayu Miao, Weinan Zhang, Montgomery Alban, Iman Fadakar, Zheng Chen, et al. 2021. Smarts: An open-source scalable multi-agent rl training school for autonomous driving. In Proceedings of the Conference on Robot Learning. PMLR, 264\u2013285. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Experimental details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Implementation details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1.1 Kaleidoscope with MATD3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Critic ensembles When incorporating Kaleidoscope into the MATD3 algorithm, the overall training loss for the critic ensembles becomes: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c}^{\\mathrm{all}}(\\phi_{0},\\boldsymbol{s}^{c})=\\sum_{j=1,...,K}\\mathcal{L}_{c}^{\\mathrm{all}}(\\phi_{j})=\\sum_{j=1,...,K}\\mathcal{L}_{c}(\\phi_{0},\\boldsymbol{s}_{j}^{c})-\\alpha^{d}\\cdot\\mathcal{I}_{c}^{\\mathrm{div}}(\\boldsymbol{s}^{c}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with $\\mathcal{L}_{c}(\\phi_{0},s_{j}^{c})$ being the original MATD3 loss given in Equation (11), $\\mathcal{I}_{c}^{\\mathrm{div}}(s^{c})$ being the diversity regularization given in Equation (14), and $\\alpha^{d}$ being a coefficient balancing the original MARL objective and the proposed diversity regularization. Note that although $\\mathcal{I}_{c}^{\\mathrm{div}}(s^{\\bar{c}})$ contains parameters $\\phi_{0}$ , we stop the gradients for $\\phi_{0}$ in $\\mathcal{I}_{c}^{\\mathrm{div}}(s^{c})$ . ", "page_idx": 14}, {"type": "text", "text": "In the implementation, we apply layer-wise weights to the diversity regularization term $\\mathcal{I}_{c}^{\\mathrm{div}}(s^{c})$ which is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{J}_{c}^{\\mathrm{div}}(s^{c})=\\sum_{l=1,\\ldots,L}w_{l}\\cdot\\sum_{\\substack{i=1,\\ldots,K\\,j=1,\\ldots,K}}\\|\\phi_{0}\\odot(M_{i,l}^{c}-M_{j,l}^{c})\\|_{1},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $l$ denotes the layer index of the neuron networks, $L$ represents the total number of layers, $M_{i,l}^{c}$ is the mask for agent $i$ at layer $l$ , and the layer-wise weights are set as $w_{l}=2^{l}$ . The intuition behind this choice is that features closer to the output tend to be more compact (Kusupati et al., 2020); consequently, assigning larger regularization weights to these layers may have a more significant impact on the output action decisions. Our initial experiments empirically demonstrate that setting $w_{l}\\stackrel{\\cdot}{=}2^{l}$ improves performance compared to the case where $w_{l}=1$ . Based on these findings, we maintain this design choice throughout all our experiments. ", "page_idx": 14}, {"type": "text", "text": "In practice, we adaptively adjust $\\alpha^{d}$ while maintaining a constant ratio between the MATD3 loss and the diversity loss, which is treated as a hyperparameter: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\alpha^{d}=\\frac{|\\sum_{j=1,\\dots,K}\\mathcal{L}_{c}(\\phi_{0},\\pmb{s}_{j}^{c})|}{|\\mathcal{J}_{c}^{\\mathrm{div}}(\\pmb{s}^{c})|}\\cdot\\alpha,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\alpha$ is a hyperparameter, and the gradients for $\\begin{array}{r}{\\frac{|\\sum_{j=1,\\dots,K}\\mathcal{L}_{c}\\left(\\phi_{0},\\pmb{s}_{j}^{c}\\right)|}{|\\mathcal{J}_{c}^{\\mathrm{div}}\\left(\\pmb{s}^{c}\\right)|}}\\end{array}$ are stopped. ", "page_idx": 14}, {"type": "text", "text": "Actors For the actors, the training objective is to maximize the following term ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{I}^{\\mathrm{all}}(\\theta_{0},s)=\\sum_{i=1,...,n}\\mathcal{I}^{\\mathrm{all}}(\\theta_{i})=\\sum_{i=1,...,n}\\mathcal{I}(\\theta_{0},s_{i})+\\beta^{d}\\cdot\\mathcal{I}^{\\mathrm{div}}(s),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathcal{I}(\\theta_{0},s_{i})$ is the original actor objective defined in Equation (3), $\\mathcal{I}^{\\mathrm{div}}(s)$ is the diversity regularization given in Equation (8) with layer-wise weights as in Equation (16), $\\beta^{d}$ is the regularization coefficient. The value of $\\beta^{d}$ is determined by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\beta^{d}=\\frac{|\\sum_{i=1,\\dots,n}\\mathcal{J}(\\theta_{0},\\pmb{s}_{i})|}{|\\mathcal{J}^{\\mathrm{div}}(\\pmb{s})|}\\cdot\\beta,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\beta$ is a constant hyperparameter, similar to the approach used in Equation (17) for the critic ensembles. ", "page_idx": 14}, {"type": "text", "text": "A.1.2 Kaleidoscope with QMIX ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "When incorporating Kaleidoscope into the QMIX algorithm (Rashid et al., 2020), we apply Kaleidoscope parameter sharing only to the local $\\mathrm{^Q}$ networks. Consequently, the training loss is defined as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{\\mathrm{all}}(\\theta_{0},s)=\\mathcal{L}(\\theta_{0},s)-\\beta^{d}\\cdot\\mathcal{I}^{\\mathrm{div}}(s),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta_{0},s)=\\mathbb{E}_{(s^{t},o^{t},a^{t},r^{t},s^{t+1},o^{t+1})\\sim\\mathcal{D}}\\left[\\left(y^{t o t}-Q_{t o t}(s^{t},o^{t},a^{t};\\theta_{0},s)\\right)^{2}\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with $y^{t o t}\\,=\\,r+\\gamma\\operatorname*{max}_{a}Q_{t o t}\\bigl(s^{t+1},o^{t+1},a;\\theta^{-}\\bigr)$ and $\\theta^{-}$ representing the parameters of a target network as in DQN. ", "page_idx": 14}, {"type": "text", "text": "A.1.3 Network architecture and hyperparameters ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Codebase Our implementation of Kaleidoscope and baseline algorithms are based on the following codebase: ", "page_idx": 15}, {"type": "text", "text": "\u2022 HARL (Zhong et al., 2024) (MATD3 implementation): https://github.com/PKU-MARL/ HARL   \n\u2022 EPyMARL (Papoudakis et al., 2021) (QMIX implementation for MPE): https://github.com/uoe-agents/epymarl   \n\u2022 PyMARL2 (Hu et al., 2021) (QMIX implementation for SMACv2): https://github.com/benellis3/pymarl2   \n\u2022 SePS (Christianos et al., 2021): https://github.com/uoe-agents/seps ", "page_idx": 15}, {"type": "text", "text": "The code for Kaleidoscope is publicly available at https://github.com/LXXXXR/Kaleidoscope. ", "page_idx": 15}, {"type": "text", "text": "Network architecture In line with prior works (Zhong et al., 2024; Papoudakis et al., 2021; Hu et al., 2021), we employ deep neural networks consisting of multilayer perceptrons (MLPs) with rectified linear unit (ReLU) activation functions and gated recurrent units (GRUs) to parameterize the actor and critic networks. Moreover, when the masking technique is applied to critic ensembles, we incorporate layer normalization between the MLP layers and ReLU activations (Hiraoka et al., 2022). In Kaleidoscope, the masking technique is applied to the MLP layers, and the resetting mechanisms described in Sections 3.3 and 3.4 are applied to the last three layers of the respective neural networks, following Nikishin et al. (2022). ", "page_idx": 15}, {"type": "text", "text": "Hyperparameters To ensure a fair comparison, we implement our method and all the baselines using the same codebase with the same set of hyperparameters, with the exception of methodspecific ones. The common hyperparameters are listed in Tables 3 to 5. The Kaleidoscope-specific hyperparameters are provided in Table 6. ", "page_idx": 15}, {"type": "table", "img_path": "W0wq9njGHi/tmp/3d4b5940999e8b5356cc177db3df5c9622d04cea0e5673d68a078c7e8cbca111.jpg", "table_caption": ["Table 3: Common hyperparameters used for MATD3 in the MaMuJoCo domain. "], "table_footnote": ["1 Here we adopt the per-scenario finetuned value for this hyperparameter as provided by HARL. "], "page_idx": 15}, {"type": "text", "text": "A.2 Environmental details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Codebase The environments used in this work are listed below with descriptions in Table 7. ", "page_idx": 15}, {"type": "text", "text": "\u2022 MaMuJoCo (Peng et al., 2021): https://github.com/schroederdewitt/multiagent_ mujoco   \n\u2022 MPE (Lowe et al., 2017; Papoudakis et al., 2021): https://github.com/semitable/ multiagent-particle-envs   \n\u2022 SMACv2 (Ellis et al., 2024): https://github.com/oxwhirl/smacv2 ", "page_idx": 15}, {"type": "text", "text": "Table 4: Common hyperparameters used for QMIX in the MPE domain. ", "page_idx": 16}, {"type": "table", "img_path": "W0wq9njGHi/tmp/e2820ca0e9d05eaa00f7e2e13cec41190f3b277812dabd2adf6fb7ed038392b5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "W0wq9njGHi/tmp/e81180467ceb8673f925bc4400c129da27310bf7a41780c58af73c21d17419d0.jpg", "table_caption": ["Table 5: Common hyperparameters used for QMIX in the SMACv2 domain. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "MPE We extend the scenario settings provided in the original codebase to increase the complexity and challenge of the tasks. In World, we set the number of predators (agents) to 4, the number of prey to 2, the number of obstacles to 1, and the number of forests to 2. The objective of the game is for the predators to approach the prey while avoiding collisions with obstacles. The prey is attracted to the food and can hide from the predators in the forests. In Push, we set the number of agents to 5, the number of adversaries to 2, and the number of landmarks to 2. The goal of the game is for the agents to push the adversaries away from the landmarks. In both scenarios, we pretrain the adversary (prey) policies using the MADDPG algorithm Lowe et al. (2017) and use these pretrained policies to test the performance of different algorithms ", "page_idx": 16}, {"type": "text", "text": "A.3 FLOPs calculation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To calculate the number of floating-point operations (FLOPs) for a single forward pass of a sparse model, we sum the total number of multiplications and additions layer by layer, following the approach in Evci et al. (2020). For a fully-connected layer, the FLOPs are computed as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{FLOPs}=2\\times(1-\\mathrm{Sparsity})\\times\\mathrm{In}\\times\\mathrm{Out}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For a GRU cell, the FLOPs are computed as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{FLOPs}=2\\times\\mathrm{(3\\timesHidden^{2}+3\\times I n\\times H i d d e n+13\\times H i d d e n)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.4 Experimental Infrastructure ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The experiments on the SMACv2 benchmark were conducted using NVIDIA GeForce RTX 3090 GPUs, while the experiments on other benchmarks were performed using NVIDIA GeForce RTX 3080 GPUs. Each experimental run required less than 2 days to complete. ", "page_idx": 16}, {"type": "table", "img_path": "W0wq9njGHi/tmp/33d2884431bb87edcb23e377349ff5338da471eaed110bb450cbaf4fe30c120f.jpg", "table_caption": ["Table 6: Hyperparameters used for Kaleidoscope. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "W0wq9njGHi/tmp/0cd3dd71e3b81a6bdf9cf97f71b64689d90d8d19fbef6c9573b8cdae0d0fb1e1.jpg", "table_caption": ["Table 7: Environments details. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B More results and discussion ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Detailed Costs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We provide per-scenario FLOPs across different methods in Table 8 as a supplement for Table 2. ", "page_idx": 17}, {"type": "text", "text": "Table 8: Averaged FLOPs for different methods. Results are normalized w.r.t. the $\\mathrm{FuPS}+\\mathrm{ID}$ model.   \nThe lowest costs are highlighted in bold. ", "page_idx": 17}, {"type": "table", "img_path": "W0wq9njGHi/tmp/561943044662b03c2e8e5efc88e01ba6d625acab415ec4e55794e6e6e1b47e16.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "W0wq9njGHi/tmp/ac43b9a03137a89d34722ecb4c400a66548fc874dc05c7b19eb15a08e577023a.jpg", "img_caption": ["Figure 9: Hyperparameter analysis. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.2 Hyperparameter Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We conduct further analysis on the hyperparameters $\\alpha$ and $\\beta$ , and present the results in Figure 9. The hyperparameter $\\alpha$ controls the variance of the critic ensembles. As shown in Figure $9\\mathrm{a}$ , we observe that an excessively small $\\alpha$ results in degraded performance because it reduces the critic ensembles to a single critic network, causing the value estimation to suffer from severe overestimation. Conversely, an excessively large $\\alpha$ also deteriorates performance, possibly due to increased estimation bias. For $\\beta$ , as illustrated in Figure $^{9\\mathrm{{b}}}$ , an overly small $\\beta$ leads to degraded performance because it reduces the Kaleidoscope parameter sharing to full parameter sharing, confining the policies to be identical. An overly large $\\beta$ also negatively impacts performance, as it may cause the training objective to deviate too much from minimizing the original MARL loss. ", "page_idx": 18}, {"type": "text", "text": "In general, we recommend setting both hyperparameters between 0.1 and 1. However, the optimal hyperparameter values may vary across different scenarios. For fair comparisons, we maintain the same set of hyperparameters across all scenarios in our experiments. Nevertheless, further tuning of these hyperparameters has the potential to enhance performance. ", "page_idx": 18}, {"type": "text", "text": "B.3 Further Visualization Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To better understand how learnable masks in Kaleidoscope affect the performance through policies, we visualize the pairwise mask differences among agents and the agent trajectories at different training stages in Figure 10. As training progresses, the test return increases and diversity loss decreases, indicating better performance and greater diversity among agent policies. Correspondingly, mask differences among agents increase, and the agent trajectory distribution becomes more diverse. ", "page_idx": 18}, {"type": "text", "text": "B.4 Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Here we discuss some limitations of Kaleidoscope. ", "page_idx": 18}, {"type": "text", "text": "First, as suggested by results in Appendix B.2, the optimal hyperparameters vary from scenario to scenario. Therefore, using the same hyperparameters across all scenarios may not yield the best performance for Kaleidoscope. Developing an automatic scheme that utilizes environmental information to determine these hyperparameters would be beneficial. ", "page_idx": 18}, {"type": "text", "text": "Second, as the environments used in this work contain no more than 10 agents, we assign a distinct mask for each agent. However, when the problem scales to hundreds of agents, this vanilla implementation may fail. In such cases, a possible approach is to cluster $N$ agents into $K$ $\\langle K<N\\rangle$ ) groups and train $K$ masks with Kaleidoscope. This would reduce computational costs and achieve a better trade-off between sample efficiency and diversity. Within the same group, agents share all parameters, while agents from different groups share only partial parameters. Techniques for clustering agents based on experience, as proposed by Christianos et al. (2021), could be useful. ", "page_idx": 18}, {"type": "image", "img_path": "W0wq9njGHi/tmp/1c4c3e7eaed843dbcaeb5b8e9a95498ba412f79dc18f0f60ee3e615331c9264e.jpg", "img_caption": ["Figure 10: Further visualization on World. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Claims ", "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The abstract and introduction accurately reflect the paper\u2019s contributions by proposing a novel parameter sharing technique in MARL (Section 3) and claiming its superior performance compared to existing methods, which is supported by the experimental results (Section 4). ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper. \u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Please see Appendix B.4. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not include theoretical results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The method and implementation details are provided in Section 3 and Appendix A.1, and the experiment settings and environment details are described in Section 4 and Appendix A.2. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The code is available at https://github.com/LXXXXR/Kaleidoscope. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The experiment settings are provided in Section 4 with details elaborated in Appendices A.1 and A.2. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The main results are reported with $95\\%$ confidence error bars in Figures 4 to 6. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Please see Appendix A.4. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Reinforcement Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The experiments in this work are conducted in simulated game environments, thereby presenting minimal risk of misuse. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The papers corresponding to the environments used are cited in Section 4, and the codebases utilized are listed in Appendices A.1.3 and A.2. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]