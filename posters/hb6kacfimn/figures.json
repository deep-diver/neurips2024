[{"figure_path": "HB6KaCFiMN/figures/figures_0_1.jpg", "caption": "Figure 1: Different supervision for 4D generation. MV-VDM shows superior spatiotemporal consistency than previous models. Based on MV-VDM, we propose Animate3D to animate any 3D model.", "description": "This figure compares different methods for generating 4D content (spatiotemporal data).  It shows that the proposed method, MV-VDM, produces significantly better results in terms of spatiotemporal consistency compared to using video diffusion alone (T2V) or combining video and 3D diffusion (SVD + Zero123).  The improved consistency of MV-VDM makes it suitable as the foundation for the Animate3D framework, which can animate any 3D model.", "section": "Abstract"}, {"figure_path": "HB6KaCFiMN/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of our proposed multi-view video diffusion model\u2014MV-VDM (upper part) and our Animate3D framework (lower part). MV-VDM, trained on our presented large-scale 4D dataset MV-Video, can generate spatiotemporal consistent multi-view videos. Animate3D, based on MV-VDM, combines reconstruction and 4D-SDS optimization to animate any static 3D models.", "description": "This figure illustrates the architecture of the proposed multi-view video diffusion model (MV-VDM) and the Animate3D framework. MV-VDM, trained on a large-scale 4D dataset (MV-Video), generates temporally and spatially consistent multi-view videos. The Animate3D framework then leverages MV-VDM for animating any static 3D model via a two-stage pipeline that combines motion reconstruction and 4D Score Distillation Sampling (4D-SDS) for motion refinement and appearance preservation.  The diagram shows the flow of information from text prompt, multi-view renderings of the 3D object, to the generated multi-view videos and final animation.", "section": "3 Method"}, {"figure_path": "HB6KaCFiMN/figures/figures_7_1.jpg", "caption": "Figure 3: Qualitative comparison with state-of-the-art methods.", "description": "This figure compares the animation results of Animate3D against two state-of-the-art methods, 4Dfy and DreamGaussian4D, on three different 3D objects (bear, frog, penguin). Each row represents a different object and shows its animation at different time steps (t=0, t=t1, t=t2). The input 3D model is shown in the first column (t=0), followed by the animation results of 4Dfy, DreamGaussian4D, Reconstruction (Ours), and Ours. The comparison highlights Animate3D's ability to generate high-quality and consistent animations, unlike the other methods that suffer from distortions or blurry effects.", "section": "4.2 Comparison with State-of-the-Art"}, {"figure_path": "HB6KaCFiMN/figures/figures_8_1.jpg", "caption": "Figure 4: Ablation for multi-view video diffusion.", "description": "The figure shows an ablation study on the proposed multi-view video diffusion model.  It demonstrates the impact of removing the spatiotemporal attention module and pre-trained weights on the quality of generated videos.  The results highlight the importance of these components for achieving high-quality spatiotemporal consistency in the generated videos.", "section": "4.3 Ablation"}, {"figure_path": "HB6KaCFiMN/figures/figures_9_1.jpg", "caption": "Figure 6: Visualizations of mesh animation. We present RGB and textureless renderings of two mesh animation results. Best viewed by zooming in.", "description": "This figure shows the results of applying the Animate3D framework to animate 3D meshes.  Two examples are provided: a wooden dragon head shaking from right to left, and a dog running and jumping. For each animation, both color (RGB) and textureless renderings of the mesh are shown at different points in time.  The goal is to demonstrate the framework's ability to directly animate meshes without requiring skeleton rigging or other complex procedures.", "section": "4.4 Mesh Animation"}, {"figure_path": "HB6KaCFiMN/figures/figures_14_1.jpg", "caption": "Figure 7: Qualitative comparison with state-of-the-art methods on reconstructed 3D objects.", "description": "This figure compares the animation results of Animate3D with two state-of-the-art methods, 4Dfy(Gau.) and DreamGaussian4D, on three reconstructed 3D objects: a spiderman, a monstrous dog, and Superman. Each row shows the input static 3D object (t=0) and the animation results at different time steps (t=t1, t=t2) for each method. The results demonstrate that Animate3D achieves better quality in terms of appearance and motion consistency, generating more natural and visually pleasing animations compared to the other two methods.", "section": "4.2 Comparison with State-of-the-Art"}, {"figure_path": "HB6KaCFiMN/figures/figures_15_1.jpg", "caption": "Figure 8: Ablations of MV-VDM and 4D optimization.", "description": "This figure shows the ablation study results of the proposed method.  The top two rows present ablation studies on the multi-view video diffusion model (MV-VDM), showing results without spatiotemporal attention and without pre-trained weights. The bottom two rows show ablation studies on the 4D animation optimization, presenting results without 4D Score Distillation Sampling (4D-SDS) and without As-Rigid-As-Possible (ARAP) loss. Each row displays multiple views of the same object at different time steps, illustrating the impact of each component on the quality and consistency of the animation. The objects used are a pink dinosaur and a blue treasure chest, each undergoing a specific animation (waving and opening, respectively). Comparing the results across rows for each object demonstrates the contributions of each component to the overall performance of Animate3D.", "section": "3 Method"}, {"figure_path": "HB6KaCFiMN/figures/figures_15_2.jpg", "caption": "Figure 8: Ablations of MV-VDM and 4D optimization.", "description": "This figure demonstrates the ablation studies performed on the MV-VDM model and the 4D optimization process.  The top row shows the impact of removing the spatiotemporal attention module and pre-training on the MV-VDM. The bottom row shows the impact of removing the ARAP loss and the SDS loss from the 4D optimization.  The results illustrate the importance of each component for achieving high-quality results in generating consistent and detailed animations.", "section": "4.3 Ablation"}, {"figure_path": "HB6KaCFiMN/figures/figures_17_1.jpg", "caption": "Figure 2: Illustration of our proposed multi-view video diffusion model\u2014MV-VDM (upper part) and our Animate3D framework (lower part). MV-VDM, trained on our presented large-scale 4D dataset MV-Video, can generate spatiotemporal consistent multi-view videos. Animate3D, based on MV-VDM, combines reconstruction and 4D-SDS optimization to animate any static 3D models.", "description": "This figure illustrates the architecture of the proposed MV-VDM model and Animate3D framework. MV-VDM, a multi-view video diffusion model, takes multi-view renderings and text prompts as input to generate spatiotemporally consistent multi-view videos. Animate3D uses MV-VDM to reconstruct and refine motions for animating static 3D models. It uses a two-stage pipeline consisting of motion reconstruction and 4D Score Distillation Sampling (4D-SDS) to achieve high-quality mesh animation.", "section": "3 Method"}, {"figure_path": "HB6KaCFiMN/figures/figures_18_1.jpg", "caption": "Figure 2: Illustration of our proposed multi-view video diffusion model\u2014MV-VDM (upper part) and our Animate3D framework (lower part). MV-VDM, trained on our presented large-scale 4D dataset MV-Video, can generate spatiotemporal consistent multi-view videos. Animate3D, based on MV-VDM, combines reconstruction and 4D-SDS optimization to animate any static 3D models.", "description": "This figure shows the architecture of the proposed method, Animate3D, which consists of two main parts: the multi-view video diffusion model (MV-VDM) and the animation pipeline. MV-VDM takes multi-view renderings of a 3D object as input and generates spatiotemporal consistent multi-view videos. The animation pipeline reconstructs motions from the generated videos and refines them using 4D Score Distillation Sampling (4D-SDS) to create a final animation. The figure also highlights the use of a spatiotemporal attention module and MV2V-Adapter to improve the model's performance.", "section": "3 Method"}, {"figure_path": "HB6KaCFiMN/figures/figures_19_1.jpg", "caption": "Figure 2: Illustration of our proposed multi-view video diffusion model\u2014MV-VDM (upper part) and our Animate3D framework (lower part). MV-VDM, trained on our presented large-scale 4D dataset MV-Video, can generate spatiotemporal consistent multi-view videos. Animate3D, based on MV-VDM, combines reconstruction and 4D-SDS optimization to animate any static 3D models.", "description": "This figure illustrates the architecture of the MV-VDM model (top) and the Animate3D framework (bottom).  MV-VDM, trained on a large dataset of multi-view videos, generates consistent videos across multiple views and over time.  The Animate3D framework leverages MV-VDM to animate static 3D models by combining reconstruction and a novel 4D score distillation sampling (4D-SDS) method for motion refinement.  The diagram shows the flow of data through each component, highlighting the integration of text prompts, multi-view images, and the spatiotemporal attention module in MV-VDM.", "section": "3 Method"}, {"figure_path": "HB6KaCFiMN/figures/figures_20_1.jpg", "caption": "Figure 12: More examples of our MV-Video dataset.", "description": "This figure shows more examples from the MV-Video dataset.  The dataset contains a large variety of animated 3D objects, including animals, plants, humans, and other objects, which are rendered in multiple views for the purpose of training the MV-VDM model in the Animate3D framework. The images showcase different poses and motions of the objects to demonstrate the spatiotemporal diversity present in the dataset.", "section": "D.2 Data Examples"}, {"figure_path": "HB6KaCFiMN/figures/figures_21_1.jpg", "caption": "Figure 1: Different supervision for 4D generation. MV-VDM shows superior spatiotemporal consistency than previous models. Based on MV-VDM, we propose Animate3D to animate any 3D model.", "description": "This figure compares different approaches to 4D generation (generating videos of 3D objects in motion).  It shows three methods: simple video diffusion (T2V), a combined video and 3D diffusion approach (SVD+Zero123), and the authors' proposed multi-view video diffusion model (MV-VDM).  The figure highlights that MV-VDM produces more temporally and spatially consistent results, paving the way for their Animate3D framework which animates any 3D model.", "section": "Abstract"}]