[{"figure_path": "cMwSoXLCVi/figures/figures_3_1.jpg", "caption": "Figure 1: (a) The overall architecture of the proposed One-to-Multiple Progressive Style Transfer Unsupervised Domain-Adaptive framework, which includes a shared generator and a discriminator. The generator is composed of an encoder, a decoder, and multiple style fusion modules. (b) shows the progressive style transfer process, achieved through cascaded style fusion modules.", "description": "This figure shows the architecture of the proposed One-to-Multiple Progressive Style Transfer Unsupervised Domain-Adaptive (PSTUDA) framework.  Panel (a) provides a high-level overview of the framework, illustrating the shared generator and discriminator, as well as the encoder, decoder, and multiple style fusion modules within the generator. Panel (b) delves into the detail of the progressive style transfer process, showing how content and style features are combined at multiple levels using cascaded style fusion modules, point-wise instance normalization, and style dictionaries.", "section": "3 Method"}, {"figure_path": "cMwSoXLCVi/figures/figures_5_1.jpg", "caption": "Figure 2: Architecture of the Multi-Scale Discriminator, composed of multiple residual blocks.", "description": "The figure shows the architecture of a Multi-Scale Discriminator used in the PSTUDA framework.  It's composed of multiple residual blocks. The input image is downsampled through several stages. At each stage, the feature map is processed by four residual blocks and then a 1x1 convolutional layer followed by ReLU activation. The output from each stage is concatenated with a domain label and fed into a final ReLU and convolutional layer. The output of the final layer is then fed into a classification layer that produces the final classification output. The discriminator is designed to be effective in detecting whether an image is real or fake, and it helps improve the quality of generated images by encouraging the generator to produce more realistic images.", "section": "3.4 Generator and Discriminator Architecture"}, {"figure_path": "cMwSoXLCVi/figures/figures_7_1.jpg", "caption": "Figure 3: Qualitative results for T1c \u2192 FS T2W, T2W, and DWI on the MSKT dataset. Blue and red bounding boxes indicate the annotated boundaries of the kidney and tumor, respectively (Same below).", "description": "This figure shows the qualitative results of the image translation task from T1c MRI sequence to FS T2W, T2W, and DWI MRI sequences, using different methods: CycleGAN, MUNIT, SIFA, DEPL, StarGAN v2 and PSTUDA. The results demonstrate the superiority of PSTUDA in maintaining the structural consistency while translating images across different domains. Other methods show distorted structures or missing details.", "section": "4.2 Comparative Study"}, {"figure_path": "cMwSoXLCVi/figures/figures_17_1.jpg", "caption": "Figure 4: Qualitative results for CT \u2192 T1c, FS T2W, T2W, and DWI on the KiTS19 and MSKT datasets.", "description": "This figure shows a qualitative comparison of the results obtained by different methods (CycleGAN, MUNIT, SIFA, DEPL, StarGAN v2, and PSTUDA) for the task of translating CT images to four different MRI sequences (T1c, FS T2W, T2W, and DWI). The source CT images are shown in the first column, and the results of each method are displayed in the subsequent columns. The blue and red bounding boxes indicate the annotated boundaries of the kidney and tumor, respectively. This figure demonstrates the ability of PSTUDA to generate images that are visually similar to the target domain images while maintaining structural consistency and detailed information. ", "section": "C Visualization of the KiTS19 and MSKT Experiments"}]