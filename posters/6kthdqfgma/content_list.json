[{"type": "text", "text": "Fair and Welfare-Efficient Constrained Multi-matchings under Uncertainty ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Elita Lobo,\u2217 Justin Payan,\u2217 Cyrus Cousins, and Yair Zick University of Massachusetts Amherst {elobo, jpayan, cbcousins, yzick}@umass.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study fair allocation of constrained resources, where a market designer optimizes overall welfare while maintaining group fairness. In many large-scale settings, utilities are not known in advance, but are instead observed after realizing the allocation. We therefore estimate agent utilities using machine learning. Optimizing over estimates requires trading-off between mean utilities and their predictive variances. We discuss these trade-offs under two paradigms for preference modeling \u2013 in the stochastic optimization regime, the market designer has access to a probability distribution over utilities, and in the robust optimization regime they have access to an uncertainty set containing the true utilities with high probability. We discuss utilitarian and egalitarian welfare objectives, and we explore how to optimize for them under stochastic and robust paradigms. We demonstrate the efficacy of our approaches on three publicly available conference reviewer assignment datasets. The approaches presented enable scalable constrained resource allocation under uncertainty for many combinations of objectives and preference models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Constrained resource allocation without money underpins many important systems, including reviewer assignment for peer review (our primary example throughout the paper) [4, 16, 31, 45, 54], assigning resources to homeless populations [5, 34, 49], distributing emergency response resources [51, 56, 57], and more [1, 44, 53]. In these settings we assign resources to agents. Agents and resources are constrained; each agent has bounds on the minimum or maximum number of items they receive from different categories, and each item has required minimums and limited total capacity. Each agent has a valuation for every item, and we optimize a welfare function of the agent-item valuations. In the case of reviewer assignment, the reviewer-paper valuations measure the alignment between reviewers and papers, papers must receive a certain number of reviews from unique reviewers, reviewers have upper limits on the number of papers they can review, and conflicts of interest prevent some reviewers from being assigned to certain papers. ", "page_idx": 0}, {"type": "text", "text": "A crucial factor in all of the above settings is the presence of uncertainty. Uncertainty often stems from the fact that agents\u2019 valuations for resources depend on future outcomes. In reviewer assignment, a reviewer-paper pair\u2019s match quality is observed only after the reviewer submits his or her review. Uncertainty may also stem from our limited ability to collect data; for example, in deciding where to target lead pipe mitigation projects based on number of school-aged children per neighborhood, we may have access to imperfect school enrollment records, allowing only an approximate model of the impacts of mitigation on children in each neighborhood [53]. We adopt two possible stances towards uncertainty, depending on the information available. When we have access to $a$ probability distribution over preferences, we optimize the conditional expectation of the distribution at percentiles of interest [33, 50]. When we have access to a set of possible preferences, we adopt the robust approach, which is related to the minimax regret objective used in solving robust assignment problems [3, 10, 11, 32]. Uncertainty-aware optimization approaches can often result in significantly different allocations from the default of optimizing for welfare over a central estimate (see Example 2.1 for an intuitive explanation for this phenomenon). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Typically, we maximize the sum of agent utilities. However, in many of these settings, we are also concerned with fairness to individuals or groups of agents. Groups of agents may represent subject areas of papers in reviewer assignment, demographic groups in poverty alleviation campaigns, or regional groupings of computational resources in bandwidth allocation. Fairness to these groups may be legally required in some cases; in others it is an ethical choice by the decision maker. Although groups are often first-class objects worthy of receiving fair treatment, group fairness is often the smallest granularity of fairness achievable under uncertainty \u2013 in a large dataset uncertainty will always cause some individuals to have vanishing welfare, but group welfare can still be upheld. Although there is much literature on combinatorial optimization under uncertainty [3, 10, 11, 32, 33], to our knowledge it has not addressed the intersection of fairness and uncertainty in the constrained multi-matching problem. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We study the broad problem of fair and efficient constrained multi-matchings under uncertainty about agents\u2019 valuations. We optimize for welfare while simultaneously accounting for the uncertainty inherent in real-world resource allocation problems. Specifically, we develop methods to efficiently optimize the utilitarian and egalitarian welfare objectives using the robust approach [7, 8, 26] and the CVaR approach [50]. Our results are summarized in Table 1. ", "page_idx": 1}, {"type": "text", "text": "For robust optimization, we construct an uncertainty set containing the true preferences with high probability (Section 3). This model is appropriate when building a predictor with statistical error bounds, but without making any assumptions on the full probability distribution over valuations. For utilitarian and egalitarian welfare functions, we robustly maximize welfare over such uncertainty sets. When the uncertainty sets are linear we can efficiently compute the exact optimal allocations for both utilitarian and egalitarian welfare in polynomial time (Corollaries 3.2 and 3.6). Under a single ellipsoidal uncertainty set, we can apply an iterated quadratic programming approach (Corollaries 3.3 and 3.7), while a projected subgradient ascent approach is needed when uncertainty sets consist of multiple ellipsoids (Propositions 3.1 and 3.5). Under general monotonic, concave welfare functions and arbitrary convex uncertainty sets, we apply the relatively expensive adversarial projected subgradient ascent algorithm of Cousins et al. [16]. ", "page_idx": 1}, {"type": "text", "text": "When the market designer can construct a full probability distribution over preferences or sample from such a distribution, we consider stochastic optimization using the concept of Conditional Value at Risk, or CVaR [50]. This approach, laid out in Section 4, selects an allocation that maximizes the conditional expectation of welfare over the left tail of the welfare distribution. We often approximate CVaR objectives using sampling, then solve the resulting linear program or LP (as in Propositions 4.1 and H.3). However, in the case of utilitarian welfare and Gaussian-distributed valuations we present a simple reformulation of the CVaR objective (Proposition 4.3). Optimizing CVaR for general monotonic, concave welfare functions can require solving arbitrary concave optimization problems, even after sampling. ", "page_idx": 1}, {"type": "text", "text": "We also compare these optimization approaches empirically in Section 5 on reviewer assignment data from AAMAS 2015, 2016, and 2021. ", "page_idx": 1}, {"type": "text", "text": "1.2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We discuss the history of prior work on robust and CVaR optimization in Appendix A. ", "page_idx": 1}, {"type": "text", "text": "Some existing work applies stochastic or robust optimization to fair division problems. A line of work studies the minimax regret objective in combinatorial optimization problems, such as constrained resource allocation [3, 10, 11, 32]. This work does not explicitly consider multi-matching problems like those considered here, nor does it address the robust egalitarian welfare problem. Pujol et al. [48] study fair division problems with parameters noised for differential privacy, showing ", "page_idx": 1}, {"type": "text", "text": "Table 1: Summary of optimization algorithms for efficiently computing utilitarian and egalitarian welfare under different robustness concepts. Green highlights indicate problems which require solving a single linear program (low difficulty). Yellow highlights indicate solving a small number of linear or quadratic programs (medium difficulty). Red highlights indicate problems which require solving numerous quadratic programs or arbitrary concave programs. ", "page_idx": 2}, {"type": "table", "img_path": "6KThdqFgmA/tmp/f61ee8e61615a2dcc431065a046723b41df9371c5711a28617bdfbb9edf1af24.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "that the noise can cause unfair allocations; they propose a Monte Carlo approach to mitigate the unfairness with high probability. Peters et al. [46] study envy-free rent division under probabilistic uncertainty. A central mechanism divides rooms and sets room prices for the items to minimize envy. We study a setting without money, both utilitarian and egalitarian objectives, and robust optimization in addition to stochastic optimization. ", "page_idx": 2}, {"type": "text", "text": "Cousins et al. [16] study robust optimization under the utilitarian objective. They propose an adversarial projected subgradient ascent method which requires solving a two quadratic programs (one for the adversary and one for the projection) at each iteration for a large number of iterations. Our empirical analysis in Section 5 demonstrates the inefficiency of this method. Fair machine learning algorithms [17, 22, 23, 43, 59] often employ similar adversarial optimization techniques over an uncertainty set in a machine learning context. Other fair allocation research has studied the case where agent demand or item availability are uncertain but preferences are known [2, 14, 21, 27]. In our case demand and availability are known but preferences are not. Devic et al. [20] consider fair two-sided matching where the fairness constraint is defined with respect to unknown parameters; we assume knowledge of the parameters that define the fairness constraint (i.e., group identities). ", "page_idx": 2}, {"type": "text", "text": "2 Fair Resource Allocation under Uncertainty ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first introduce the problem of resource allocation without uncertainty, then lay out the two approaches we take to deal with the introduction of uncertainty. Our results are summarized in Table 1. ", "page_idx": 2}, {"type": "text", "text": "2.1 Fair Resource Allocation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We have a set of $n$ agents $N\\,=\\,\\{a_{1},\\dots,a_{n}\\}$ , and $m$ item types $I\\,=\\,\\{i_{1},...\\,i_{m}\\}$ . Agents are partitioned into $g$ groups $\\mathcal{G}\\,=\\,\\{G_{1},...\\,G_{g}\\}$ , with each $G\\subseteq N$ and each agent $i$ belonging to exactly one group. ", "page_idx": 2}, {"type": "text", "text": "For any $n\\times m$ matrix $\\mathbf{\\deltaX}$ we use the same lower-case bold letter, i.e., $\\textbf{\\em x}$ to denote the vector representing the vectorized form of the matrix $\\mathbf{\\deltaX}$ , in row-major order. For any group of agents $G$ , we use $\\mathbf{x}_{G}\\in\\bar{\\mathbb{R}}^{|G|m}$ to denote the vector restricted to the agents in $G$ . Given vectors $\\mathbf{x},\\mathbf{y}\\subseteq\\mathbb{R}^{n m}$ and real number $c\\in\\mathbb R$ , let $\\mathbf{x}\\succeq c$ denote that $\\mathrm{x}_{j}\\geq c$ for all $j$ , and let $\\mathbf{x}\\succeq\\mathbf{y}$ denote that $\\mathbf x-\\mathbf y\\succeq0$ . The $\\preceq$ operator is defined analogously. ", "page_idx": 2}, {"type": "text", "text": "We assume a valuation matrix $V^{\\ast}\\,\\in\\,\\mathbb{R}_{0+}^{n\\times m}$ , where $V_{a,i}^{*}$ encodes the true value of assigning item type to agent $a$ . The values of $V^{*}$ are typically unknown; we discuss our approaches to handle this problem in Section 2.2. We use tildes to denote random variables, for example, $\\tilde{x}\\sim\\mathcal{D}_{\\tilde{x}}$ denotes that $\\tilde{x}$ is drawn from distribution $\\mathcal{D}_{\\tilde{x}}$ . ", "page_idx": 2}, {"type": "text", "text": "Given some set of feasible assignments $A\\subseteq\\,\\mathbb{N}^{n\\times m}$ , we aim to find assignments $A\\in{\\mathcal{A}}$ where $A_{a,i}$ indicates the number of items of type $i$ allocated to agent $a$ . For each agent $a\\in N$ , we have upper and lower bounds on assignments of the form $\\begin{array}{r}{\\underline{{\\kappa}}_{a}\\,\\leq\\,\\sum_{i\\in I}A_{a,i}\\,\\leq\\,\\bar{\\kappa}_{a}}\\end{array}$ . For each item $i$ , we have lower and upper bounds on the total assignment of that item; $\\begin{array}{r}{\\underline{{\\psi}}_{i}\\leq\\sum_{a\\in N}{\\bf A}_{a,i}\\leq\\bar{\\psi}_{i}}\\end{array}$ . Finally, we have pairwise limits $C_{a,i}$ for each agent $a$ and item type $i$ , requiring that ${\\pmb A}_{a,i}\\,\\leq\\,C_{a,i}$ . It is always the case that the constraints define a finite set such that $|{\\mathcal{A}}|\\in\\mathbb{N}$ . In the example of reviewer assignment, these constraints reflect the review requirements per paper, load bounds for reviewers, conflicts of interest, and the constraint that no reviewer is assigned twice to any given paper. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Let $\\mathbf{u}:\\mathcal{A}\\times\\mathbb{R}_{0+}^{n\\times m}\\rightarrow\\mathbb{R}^{g}$ be an affine function mapping from allocations to utilities for each group. $\\operatorname{u}_{G}(\\mathbf{a},\\mathbf{v})$ denotes the utility of the group under allocation (recall a is the vectorized version of the assignment $\\pmb{A}$ ). We write $\\mathbf{u}$ instead of $\\mathbf{u}(\\mathbf{a},\\mathbf{v})$ when a and $\\mathbf{v}$ are clear from context. ", "page_idx": 3}, {"type": "text", "text": "We assume that $\\mathbf{u}$ is additive and normalized by group size, so $\\begin{array}{r}{{\\bf u}_{G}=\\frac{{\\bf a}_{G}^{\\sf T}{\\bf v}_{G}}{|G|}}\\end{array}$ . We define a welfare function $\\mathrm{W}:\\mathbb{R}^{g}\\rightarrow\\mathbb{R}$ , where ${\\mathrm{W}}(\\mathbf{u}(\\mathbf{a},\\mathbf{v}))$ denotes the overall welfare of allocation a. The weighted utilitarian social welfare function is defined as $\\mathbf{w}\\cdot\\mathbf{u}$ , where $\\mathbf{w}\\in\\mathbb{R}_{0+}^{g}$ denotes the weights on groups in $\\mathcal{G}$ . When $\\mathrm{w}_{G}=|G|$ for all $G$ , we call this function simply \u201cutilitarian welfare\u201d or \u201cUSW.\u201d The group egalitarian social welfare function (also \u201cgroup egalitarian welfare\u201d or \u201cGESW\u201d) is defined as $\\operatorname*{min}_{G\\in{\\mathcal{G}}}\\mathrm{u}_{G}$ . We do not consider individual egalitarian welfare in this work; under robust and stochastic optimization the egalitarian welfare is zero when the number of items is proportional to the number of agents and uncertainty is non-trivial. ", "page_idx": 3}, {"type": "text", "text": "2.2 Optimizing Allocations under Uncertainty ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider two main approaches to dealing with uncertainty: the robust optimization approach and the Conditional Value at Risk approach. ", "page_idx": 3}, {"type": "text", "text": "In the robust approach (Section 3), we obtain an uncertainty set $\\mathcal{V}$ that contains the true agent valuations $\\mathbf{v}^{*}$ with probability at least $1\\,-\\,\\alpha$ for some confidence parameter $\\alpha~\\in~[0,1)$ . We then optimize the welfare corresponding to the worst valuation matrix in the uncertainty set, i.e., $\\begin{array}{r}{\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A}}\\operatorname*{min}_{\\mathbf{v}\\in\\mathcal{V}}\\operatorname{W}(\\mathbf{u}(\\mathbf{a},\\mathbf{v}))}\\end{array}$ . This approach applies when we do not have access to a full distribution over valuations but have some other way of describing $\\nu$ [16]. ", "page_idx": 3}, {"type": "text", "text": "When we have access to a full distribution over the random variable $\\tilde{\\mathbf{v}}\\in\\mathbb{R}^{n m}$ , we apply a stochastic approach instead. We compute the welfare distribution and optimize the conditional expectation over an $\\alpha$ -percentile of the welfare or Conditional Value at Risk at $\\alpha$ $\\left(\\mathrm{CVaR}_{\\alpha}\\right)$ . Suppose we have a random variable $\\tilde{x}\\,\\sim\\,\\mathcal{D}_{\\tilde{x}}$ . For any risk level $\\alpha\\,\\in\\,(0,0.5)$ , $\\operatorname{CVaR}_{\\alpha}[\\tilde{x}]$ is defined as $\\mathbb{E}_{\\tilde{x}\\sim\\mathcal{D}_{\\tilde{x}}}[\\tilde{x}\\mid$ $\\tilde{x}\\,\\le\\,\\nu_{\\alpha}]$ where $\\nu_{\\alpha}$ denotes the $\\alpha$ -percentile of $\\tilde{x}$ . This approach is only appropriate when $\\mathcal{D}_{\\tilde{x}}$ is fully known, or when we can sample from it. We investigate this regime in Section 4, where we will compute and optimize $\\mathrm{CVaR}_{\\alpha}[\\mathrm{W}(\\mathbf{u}(\\mathbf{a},\\tilde{\\mathbf{v}}))]$ for a random variable $\\tilde{\\mathbf{v}}\\sim\\mathcal{D}_{\\tilde{\\mathbf{v}}}$ . ", "page_idx": 3}, {"type": "text", "text": "Example 2.1 (The Importance of Considering Uncertainty). Consider a simple two-agent, two-item instance, where each agent needs to get exactly one item, and either likes (utility 1) or dislikes it (utility 0). Agent preferences are Bernoulli random variables, where $\\mathrm{Pr}[\\tilde{\\mathrm{v}}_{1,1}=1]=0.8,\\mathrm{Pr}[\\tilde{\\mathrm{v}}_{1,2}=$ $1]=0.9,\\mathrm{Pr}[\\tilde{\\mathrm{v}}_{2,1}=1]=0.5$ , and $\\operatorname*{Pr}[\\tilde{\\mathrm{v}}_{2,2}=1]=0.8$ . If we maximize the expected USW, we would assign $i_{1}$ to $a_{1}$ and $i_{2}$ to $a_{2}$ , for a total expected USW of 1.6. However, consider instead the $\\mathrm{CVaR}_{0.3}$ of USW. When we make the expectation-maximizing assignment, then $\\mathrm{Pr}[\\mathrm{W}(\\mathbf{u})=0]=0.04$ and $\\mathrm{Pr}[\\mathrm{W}(\\mathbf{u})=1]=0.32$ . However, if we assign $i_{2}$ to agent $a_{1}$ and item $i_{1}$ to agent $a_{2}$ , we have that $\\mathrm{Pr}[\\mathrm{W}(\\mathbf{u})=0]=0.05$ and $\\mathrm{Pr}[\\mathrm{W}(\\mathbf{u})=1]\\stackrel{\\cdot}{=}0.5$ . This means that the conditional expectation of welfare at the $30^{t h}$ percentile is higher if we assign $i_{2}$ to $a_{1}$ and $i_{1}$ to $a_{2}$ (it is .32 in the first case and .5 in the second case). If we want to retain welfare in the face of uncertainty, we might well choose to maximize this quantity rather than the expectation of the welfare. ", "page_idx": 3}, {"type": "text", "text": "3 Robust Welfare Optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We construct the optimization problems for utilitarian and egalitarian welfare objectives with the robust approach. Many of these optimization problems are concave-convex max-min problems that can be directly solved using an adversarial projected subgradient ascent technique [16]: in each iteration of the algorithm, the inner minimization problem is solved to optimality, followed by a subgradient step on the allocation a, followed by a projection onto the constraint space $\\boldsymbol{\\mathcal{A}}$ . However, this method does not exploit the structure of these problems and is often computationally expensive or intractable, as demonstrated empirically in Section 5. Despite the inherent complexities of these problems, we show that under specific assumptions, these problems can be reduced to more manageable forms that are easier to optimize. We then discuss a range of algorithms for efficiently optimizing the simplified problems. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Scope: The robust approach detailed in Section 2 assumes the availability of an uncertainty set of the valuation matrix. For the sake of computational tractability, we focus on the class of uncertainty sets defined by linear and ellipsoidal constraints ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{V}=\\left\\{\\mathbf{v}\\in\\mathbb{R}^{n m}\\;\\vert\\;\\forall i\\in[1,\\ell],\\,(\\mathbf{v}-\\bar{\\mathbf{v}}_{i})\\mathbf{S}_{i}^{-1}(\\mathbf{v}-\\bar{\\mathbf{v}}_{i})\\leq r_{i}^{2},Q\\mathbf{v}\\succeq\\mathbf{e},\\mathbf{v}\\succeq0\\right\\}\\;\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the $i^{t h}$ ellipsoidal uncertainty set has center $\\bar{\\mathbf{v}}_{i}\\,\\in\\,\\mathbb{R}_{0+}^{n m}$ , covariance matrix $\\mathbf{S}_{i}\\,\\in\\,\\mathbb{R}^{n m\\times n m}$ with radius $r_{i}\\in\\mathbb{R}$ , $Q\\in\\mathbb{R}^{k\\times n m}$ , and $\\mathbf{e}\\in\\mathbb{R}^{k}$ . ", "page_idx": 4}, {"type": "text", "text": "We will further assume that the covariance matrices corresponding to the ellipsoidal uncertainty sets are positive semi-definite. This limitation on the structure of uncertainty sets is not too restrictive; it is possible to construct such uncertainty sets for linear regression and logistic regression models using statistical bounds, as shown in Appendix D. ", "page_idx": 4}, {"type": "text", "text": "In all of our methods where obtaining an integer allocation is either not feasible or computationally tractable, we relax the set of feasible integer assignments $A\\subseteq\\mathbb{N}^{n\\times m}$ to a set of feasible continuous allocations $A\\subseteq\\mathbb{R}_{0+}^{n\\times m}$ . One can obtain integer allocations satisfying all constraints by applying a randomized rounding technique that generalizes the Birkhoff-von Neumann decomposition [13]. The fractional solutions can thus be interpreted as randomized allocations. ", "page_idx": 4}, {"type": "text", "text": "3.1 Robust Allocation for Utilitarian Welfare ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We consider the problem of finding an allocation that optimizes the weighted utilitarian welfare under the worst valuation matrix in the uncertainty set. We formulate the problem as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A}}\\operatorname*{min}_{\\mathbf{v}\\in\\mathcal{V}}\\mathbf{w}\\cdot\\mathbf{u}(\\mathbf{a},\\mathbf{v})~~.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The objective and constraints of the inner-minimization problem described in (1) are convex. The problem is strictly feasible, which satisfies Slater\u2019s condition [12] for strong duality. Therefore, by taking the dual of the inner-minimization problem, we can simplify the problem in (1) into a single equivalent maximization problem. We provide the dual formation in Proposition 3.1. ", "page_idx": 4}, {"type": "text", "text": "To simplify notation in the results that follow, we assume, without loss of generality, that each group $G$ has weight $\\mathbf{w}_{G}\\,=\\,|G|$ . In practice, if this assumption does not hold, the weights can be incorporated into the valuations $\\mathbf{v}$ with a corresponding adjustment to the parameters of the valuation uncertainty set $\\nu$ . ", "page_idx": 4}, {"type": "text", "text": "In the dual, let $\\beta\\in\\mathbb{R}_{0+}^{k}$ be the dual variable corresponding to the linear constraints $Q\\mathbf{v}\\ \\succeq\\ \\mathbf{e},$ , $\\lambda\\in\\mathbb{R}_{0+}^{\\ell}$ be the dual variable associated with the ellipsoidal constraints, and $\\xi\\,\\in\\,\\mathbb{R}^{n m}$ be the variable that combines the primal variable a with the dual variable of the non-negativity constraint on $\\mathbf{v}$ for variable elimination. We define a set of feasible $\\xi$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Lambda\\doteq A-\\mathbb{R}_{0+}^{n m}=\\left\\{\\pmb\\xi\\in\\mathbb{R}^{n m}\\,\\Big|\\,\\forall a\\in N:\\,\\sum_{i\\in I}\\pmb\\xi_{a m+i}\\le\\bar{\\kappa}_{a},\\forall i\\in I:\\,\\sum_{a\\in N}\\pmb\\xi_{a m+i}\\le\\bar{\\psi}_{i},\\pmb\\xi\\preceq c\\right\\}\\,\\mathrm{~,~}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which is Pareto-dominated by $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.1 (Robust Utilitarian Welfare Dual). The problem in (1) is equivalent to solving ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\boldsymbol{\\xi}\\in\\Lambda,\\lambda\\in\\mathbb{R}_{0+}^{l}}\\mathbf{p}^{\\intercal}\\mathbf{T}\\mathbf{q}+\\beta^{\\intercal}\\mathbf{e}-\\frac{1}{4}\\mathbf{p}^{\\intercal}\\mathbf{T}\\mathbf{p}+\\sum_{i=1}^{\\ell}\\left(\\lambda_{i}\\bar{\\mathbf{v}}_{i}^{\\intercal}\\mathbf{S}_{i}\\bar{\\mathbf{v}}_{i}-\\lambda_{i}r_{i}^{2}\\right)-\\mathbf{q}^{\\intercal}\\mathbf{T}\\mathbf{q}\\parallel_{0+}^{l-1}\\mathbf{q}^{\\intercal}\\mathbf{T}\\mathbf{q}\\quad,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbf{p}=\\pmb{\\xi}-Q^{\\intercal}\\beta,\\,\\mathbf{q}=\\sum_{i=1}^{\\ell}\\lambda_{i}\\mathbf{S}_{i}^{-1}\\bar{\\mathbf{v}}_{i},}\\end{array}$ , and $\\mathbf{T}=(\\sum_{i=1}^{\\ell}\\lambda_{i}\\mathbf{S}_{i}^{-1})^{-1}$ . Let $\\xi^{*}$ be the optimal $\\xi$ in (3). Then the optimal allocation $\\mathbf{a}^{*}$ can be derived from $\\xi^{*}$ by finding a $\\mathbf{\\mu}_{1}\\in A$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall G\\in\\mathcal{G}:\\mathbf{a}_{G}\\preceq\\xi_{G}^{*}\\ \\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proposition 3.1 shows that the optimal allocation for the problem in Equation (1) can be computed by first solving the concave program in Equation (3) to obtain $\\xi^{\\ast}$ and then deriving the optimal allocation $\\mathbf{a}^{*}$ from $\\xi^{\\ast}$ by solving a system of equations. Notably, the problem in Equation (3) is a single maximization problem with fewer variables and constraints as compared to the maxmin problem in (1), making it simpler to solve. We can either solve it using off-the-shelf convex optimization tools, or by applying a projected subgradient ascent approach (without the previously required adversary). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "When the valuation uncertainty set is polyhedral, the problem in (3) simplifies further into a linear program (LP) which can be solved efficiently using standard LP solvers like Gurobi [28]. ", "page_idx": 5}, {"type": "text", "text": "Corollary 3.2 (Utilitarian Welfare with Polyhedral Uncertainty). In the case where the uncertainty set $\\mathcal{V}$ is defined purely by linear constraints, i.e., $\\mathcal{V}=\\left\\{\\mathbf{v}\\in\\mathring{\\mathbb{R}}^{n m}\\mid Q\\mathbf{v}\\succeq\\mathbf{e},\\mathbf{v}\\succeq0\\right\\}$ , the optimal allocation $\\mathbf{a}^{*}$ for the problem in (1) can be computed by solving the linear program ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A},\\beta\\in\\mathbb{R}_{0+}^{k}}\\beta^{\\intercal}\\mathbf{e}\\quad s.t.\\ \\forall G\\in\\mathcal{G}:\\ Q_{G}^{\\intercal}\\beta_{G}\\preceq\\mathbf{a}_{G}\\ .\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "When the valuation uncertainty set has a single ellipsoidal constraint with a non-negativity constraint, we compute the solution using iterated quadratic programming (Iterated QP). ", "page_idx": 5}, {"type": "text", "text": "Corollary 3.3 (Utilitarian Welfare with Ellipsoidal Uncertainty). Suppose that the set $\\nu$ in (1) is defined by a single truncated ellipsoidal constraint i.e., $\\begin{array}{r}{\\mathcal{V}=\\{\\mathbf{\\bar{v}}\\in\\mathbb{R}^{\\bar{n}m}\\mathrm{~}|\\mathrm{~}(\\mathbf{v}-\\bar{\\mathbf{v}})\\mathbf{S}^{-1}(\\mathbf{v}-\\bar{\\mathbf{v}})\\leq}\\end{array}$ $r^{2},\\mathbf{v}\\succeq\\boldsymbol{0}\\}$ . The problem in (1) is equivalent to solving ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\lambda\\in\\mathbb{R}_{0+},\\pmb{\\xi}\\in\\Lambda}\\pmb{\\xi}^{\\mathsf{T}}\\bar{\\mathbf{v}}-\\frac{\\pmb{\\xi}^{\\mathsf{T}}\\mathbf{S}\\pmb{\\xi}}{4\\lambda}-\\lambda r^{2}\\ \\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The exact optimal solution $(\\lambda^{*},\\xi^{*})$ to Equation (4) can be computed by alternately performing two steps until convergence: first, fixing $\\xi$ and optimizing $\\lambda,$ , i.e., $\\lambda=\\xi^{\\top}{\\bf S}\\xi/2r_{\\mathrm{3}}$ , and second, fixing $\\lambda$ and solving a concave quadratic program to optimize $\\xi$ . The optimal allocation $\\mathbf{a}^{*}$ can be computed from $\\xi^{*}$ as in Proposition 3.1. ", "page_idx": 5}, {"type": "text", "text": "3.2 Robust Allocation for Group Egalitarian Welfare ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now consider the problem of maximizing egalitarian welfare under the robust approach. We can represent this problem as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A}}\\operatorname*{min}_{\\mathbf{v}\\in\\mathcal{V}}\\operatorname*{min}_{G\\in\\mathcal{G}}\\;\\mathrm{u}_{G}(\\mathbf{a},\\mathbf{v})\\;\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This problem presents inherent challenges due to the non-smoothness of the inner-minimization problem and the joint constraint on the uncertainties of the valuation matrices of different groups. These factors make it difficult to compute the dual and reduce the problem or efficiently solve the problem using the quadratic program technique described in Corollary 3.3, though the generic adversarial subgradient ascent approach of Cousins et al. [16] can still be applied. For the remainder of this section, we assume that the uncertainty sets for each group $G\\in{\\mathcal{G}}$ are independent of each other. To simplify notation, we assume, without loss of generality, that the valuations $\\mathbf{v}$ and the parameters of the valuation uncertainty set $\\mathcal{V}$ are scaled to incorporate the factor $\\frac{1}{|G|}$ in the representation of the utility of each group $G$ in the corresponding group valuation $\\mathbf{v}_{G}$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 3.4 (Independence of Groups). The uncertainty set $\\nu$ is a Cartesian product of individual groups\u2019 uncertainty sets, $\\mathcal{V}\\doteq\\otimes_{G\\in\\mathcal{G}}\\mathcal{V}_{G}$ . ", "page_idx": 5}, {"type": "text", "text": "This assumption is not unreasonable in practical scenarios. For example, conferences often group papers into disjoint tracks or require paper authors to select a single primary subject area. Although papers may have multiple secondary subject areas, the top-level grouping remains independent. Assumption 3.4 allows us to reorder the two minimization problems without compromising generality: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A}}\\operatorname*{min}_{G\\in\\mathcal{G}}\\operatorname*{min}_{\\mathbf{v}_{G}\\in\\mathcal{V}_{G}}\\mathbf{a}_{G}^{\\mathsf{T}}\\mathbf{v}_{G}\\ \\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We take the dual of the inner minimization problem, then reorder the minimization over groups and the maximization over the dual variables to obtain a single, concave max-min problem. This can be solved with projected subgradient ascent in the general case, or with more efficient approaches in special cases. Proposition 3.5 expresses the general form of the result. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.5 (Robust Group Egalitarian Dual). The problem in (5) is equivalent to solving ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\boldsymbol{\\xi}\\in\\mathcal{A}}\\ \\operatorname*{min}_{G\\in\\mathcal{G}}\\beta_{G}^{\\intercal}\\mathbf{e}_{G}+\\mathbf{p}_{G}^{\\intercal}\\mathbf{T}\\mathbf{q}_{G}-\\frac{1}{4}\\mathbf{p}_{G}^{\\intercal}\\mathbf{T}\\mathbf{p}_{G}+\\sum_{i=1}^{\\ell}\\left(\\lambda_{G,i}\\bar{\\mathbf{v}}_{G,i}^{\\intercal}\\mathbf{T}\\bar{\\mathbf{v}}_{G,i}-\\lambda_{G,i}r_{G,i}^{2}\\right)-\\mathbf{q}_{G}^{\\intercal}\\mathbf{T}\\mathbf{q}_{G}\\ ,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where for each group $\\begin{array}{l l l}{G}&{\\in}&{{\\mathcal G},}\\end{array}$ , ${\\bf p}_{G}\\ =\\ \\pmb{\\xi}_{G}\\ -\\ {\\bf Q}_{G}^{\\intercal}\\beta_{G}$ , $\\begin{array}{r}{{\\bf q}_{G}{\\bf\\phi}={\\bf\\phi}\\sum_{i=1}^{\\ell}\\lambda_{G,i}{\\bf S}_{G,i}^{-1}\\bar{\\bf v}_{G,i},}\\end{array}$ , $\\mathrm{~\\bf~T~}=$ $\\begin{array}{r}{(\\sum_{i=1}^{\\ell},\\,\\lambda_{G,i}\\mathbf{S}_{G,i}^{-1})^{-1},}\\end{array}$ , and $\\Lambda$ is defined as in Equation (2). The optimal allocation $\\mathbf{a}^{*}$ can be computed from $\\xi^{*}$ as in Proposition 3.1. ", "page_idx": 6}, {"type": "text", "text": "The dual variables $\\lambda_{G},\\beta_{G},\\zeta_{G}$ and $\\xi_{G}$ for each group $G$ are interpreted as in Proposition 3.1. The optimization problem in (7) is concave with respect to the dual variables $\\lambda,\\beta$ and $\\xi$ . We can solve it using an off-the-shelf convex programming library or by applying projected subgradient ascent. ", "page_idx": 6}, {"type": "text", "text": "Under polyhedral uncertainty sets, Equation (7) simplifies to a linear program. This is akin to what we observe in the robust utilitarian case (Corollary 3.2). ", "page_idx": 6}, {"type": "text", "text": "Corollary 3.6 (Group Egalitarian Welfare with Polyhedral Uncertainty). In the case where the uncertainty set $\\mathcal{V}$ is defined only by linear constraints, i.e., $\\mathcal{V}=\\left\\{\\mathbf{v}\\in\\mathbb{R}^{n m}\\ |\\ Q\\mathbf{v}\\succeq\\mathbf{e},\\mathbf{v}\\succeq0\\right\\}$ , the max-min-min problem in (5) transforms into a linear program. ", "page_idx": 6}, {"type": "text", "text": "When the valuation uncertainty set is defined by a single ellipsoidal constraint per group, we can employ the iterated quadratic programming (Iterated QP) approach used in Corollary 3.3, alternately fixing $\\lambda$ and optimizing the rest of the dual variables $(\\beta,\\xi)$ until convergence. ", "page_idx": 6}, {"type": "text", "text": "Corollary 3.7 (Group Egalitarian Welfare with Ellipsoidal Uncertainty). Suppose that the set $\\mathcal{V}$ in (5) is defined by a single truncated ellipsoidal constraint per group i.e., $\\mathcal{V}=\\mathrm{\\\"{f}v}\\in\\mathbb{R}^{n m}\\ \\vert\\ \\forall G\\in\\mathcal{G}:$ $(\\mathbf{v}_{G}-\\bar{\\mathbf{v}}_{G})\\mathbf{S}_{G}^{-1}(\\mathbf{v}_{G}-\\bar{\\mathbf{v}}_{G})\\leq r_{G}^{2},\\mathbf{v}\\succeq0\\right\\}$ . Then the problem in (5) is equivalent to solving ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pmb{\\lambda}\\in\\mathbb{R}_{0+}^{g}}\\operatorname*{min}_{G\\in\\mathcal{G}}\\xi_{G}^{\\intercal}\\bar{\\mathbf{v}}_{G}-\\frac{\\xi_{G}^{\\intercal}\\mathbf{S}_{G}\\pmb{\\xi}_{G}}{4\\lambda_{G}}-\\lambda_{G}r_{G}^{2}\\enspace.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The exact optimal solution $(\\lambda^{*},\\xi^{*})$ to Equation (4) can be computed by alternately performing two steps until convergence: first, fixing $\\xi$ and optimizing $\\lambda_{i}$ , i.e., $\\forall G\\,\\in\\,\\mathcal{G}$ , $\\lambda_{G}\\,=\\,\\dot{\\xi_{G}^{\\intercal}}\\mathbf{S}_{G}\\xi_{G}\\big/2r_{G}$ , and second, fixing $\\lambda$ and solving a concave quadratic program to optimize $\\xi$ . The optimal allocation $\\mathbf{a}^{*}$ can be computed from $\\xi^{*}$ as in Proposition 3.1. ", "page_idx": 6}, {"type": "text", "text": "3.3 Robust Allocation for Monotonic Welfare Functions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now extend our findings to a broader class of monotonic welfare functions. Specifically, we show that when optimizing a monotonic welfare objective under Assumption 3.4, we can decompose the problem into sub-problems such that we independently determine the worst valuation in the uncertainty set of each group, while jointly optimizing the allocation over all groups. ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.8 (Decomposition for Monotonic Welfare Functions). Consider an optimization problem of the form ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A}}\\operatorname*{min}_{\\mathbf{v}\\in\\mathcal{V}}\\ \\mathrm{W}_{\\mathrm{M}}\\big(\\mathbf{u}(\\mathbf{a},\\mathbf{v})\\big)\\ \\mathrm{,}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the welfare function $\\mathrm{W}_{\\mathrm{M}}$ is monotonic in the utility of groups. If Assumption 3.4 holds, then (8) simplifies to ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{a}\\in A}\\;\\mathrm{W}_{\\mathrm{M}}\\left(\\operatorname*{min}_{\\mathbf{v}_{G_{1}}\\in\\mathcal{V}_{G_{1}}}\\mathrm{u}_{G_{1}}(\\mathbf{a}_{G_{1}},\\mathbf{v}_{G_{1}}),\\operatorname*{min}_{\\mathbf{v}_{G_{2}}\\in\\mathcal{V}_{G_{2}}}\\mathrm{u}_{G_{2}}(\\mathbf{a}_{G_{2}},\\mathbf{v}_{G_{2}}),\\dots,\\operatorname*{min}_{\\mathbf{v}_{G_{g}}\\in\\mathcal{V}_{G_{g}}}\\mathrm{u}_{G_{g}}(\\mathbf{a}_{G_{g}},\\mathbf{v}_{G_{g}})\\right)\\;\\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proposition 3.8 helps us derive simplified versions of Equation (8), when Assumption 3.4 holds. The egalitarian problem in (5) is an instance of the class of optimization problem described in (8), hence Proposition 3.8 holds under Assumption 3.4 and allows us to derive a single maximization problem (Proposition 3.5). If the allocation and valuation uncertainty sets are convex and compact, the problem in (8) can be solved using constrained convex-concave minimax optimization algorithms [18, 25, 55], or adversarial projected gradient ascent [16]. These approaches do not depend on Assumption 3.4, though the optimization may be simplified if independence does hold. ", "page_idx": 6}, {"type": "text", "text": "4 Stochastic Welfare Optimization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we optimize the CVaR of utilitarian and egalitarian welfare. This approach works when the distribution $\\mathcal{D}_{\\tilde{\\mathbf{v}}}$ over the valuation matrix is known, or when we can sample from $\\mathcal{D}_{\\tilde{\\mathbf{v}}}$ . We demonstrate that when the distribution follows a Gaussian distribution, the CVaR of the utilitarian welfare has a simple representation that can be optimized without sample approximation using a projected gradient ascent method. In all other cases, we can approximately optimize CVaR using a sampling-based approach. In particular, when we have monotone, concave welfare functions, we can always approximate the CVaR objective using sampling. However, unlike in Propositions 4.1 and H.3, where the approximated problem becomes linear, with arbitrary monotone, concave welfare functions the problem may require general concave optimization. ", "page_idx": 7}, {"type": "text", "text": "4.1 CVaR Allocation for Utilitarian Welfare ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We wish to find an allocation that maximizes the $\\mathrm{CVaR}_{\\alpha}$ of the weighted utilitarian welfare. Let $\\widetilde{{\\mathbf v}}$ represent the random valuation vector. For confidence level $\\alpha$ , we formulate the problem as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{a}\\in A}\\mathrm{CVaR}_{\\alpha}\\left[\\mathbf{w}\\cdot\\mathbf{u}(\\mathbf{a},\\tilde{\\mathbf{v}})\\right]\\doteq\\operatorname*{max}_{\\mathbf{a}\\in A,b\\in\\mathbb{R}}\\left\\{b-\\frac{1}{\\alpha}\\underbrace{\\mathbb{E}}_{\\tilde{\\mathbf{v}}\\sim\\mathcal{D}_{\\tilde{\\mathbf{v}}}}\\left[\\left(b-\\mathbf{w}\\cdot\\mathbf{u}(\\mathbf{a},\\tilde{\\mathbf{v}})\\right)_{+}\\right]\\right\\}\\ ,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $(x)_{+}=\\operatorname*{max}(x,0)$ represents the positive part of $x$ [50]. Computing the exact expectation in this problem may not be feasible for every distribution $\\mathcal{D}_{\\tilde{\\mathbf{v}}}$ . Therefore, we adopt a sampling-based approach. We begin by drawing $h$ i.i.d. samples of the valuation matrix from $\\mathcal{D}_{\\tilde{\\mathbf{v}}}$ represented as $\\mathbf{v}^{\\mathbf{1}^{\\mathbf{2}}},\\mathbf{v}^{2},\\mathbf{v}^{3},\\dots,\\mathbf{v}^{\\tilde{h}}$ . We then use these samples to solve the problem described in (9) by solving the linear program outlined in Proposition 4.1. ", "page_idx": 7}, {"type": "text", "text": "Proposition 4.1 (Approximate CVaR of USW). Given h i.i.d samples of $\\tilde{\\mathbf{v}}$ , i.e., $\\mathbf{v}^{1},\\mathbf{v}^{2},\\mathbf{v}^{3},\\ldots,\\mathbf{v}^{h}$ from $\\mathcal{D}_{\\tilde{\\mathbf{v}}}$ , the optimal allocation for the problem in (9) can be approximately computed by solving ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{{\\mathbf{a}}\\in A}\\operatorname*{max}_{y\\in\\mathbb{R}_{0}^{h}}\\left(b-\\frac{1}{\\alpha}\\sum_{j=1}^{h}y_{j}\\right)\\qquad\\forall j\\in[1,h]:\\;y_{j}\\geq\\frac{1}{h}\\left(b-{\\mathbf{w}}\\cdot{\\mathbf{u}}({\\mathbf{a}},{\\mathbf{v}}^{j})\\right)\\ \\ .\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The CVaR estimator used in (10) is a strongly consistent estimator [29]. Therefore, the approximation error of the objective in (10) goes to 0 as $h\\to\\infty$ . In Proposition 4.2, we bound the sample complexity of the problem in (10) when the valuation matrix is sub-Gaussian distributed. ", "page_idx": 7}, {"type": "text", "text": "For any allocation a, let $\\hat{c}_{h,\\alpha}({\\mathbf{a}})$ represent the empirical estimate of $\\mathrm{CVaR}_{\\alpha}[\\mathbf{w}\\cdot\\mathbf{u}(\\mathbf{a},\\tilde{\\mathbf{v}})]$ computed from $h$ samples and $c_{h,\\alpha}(\\mathbf{a})$ represent the corresponding true value. We will use $|{\\mathcal{A}}|$ to denote the number of feasible allocations and $f_{\\mathbf{a}}:\\mathbb{R}\\,\\rightarrow\\,\\mathbb{R}_{0+}$ to denote the density function of the random welfare $\\mathrm{W}(\\mathbf{a},\\tilde{\\mathbf{v}})$ . $\\nu_{\\alpha}$ denotes the $\\alpha$ percentile of $\\mathrm{W}(\\mathbf{a},\\tilde{\\mathbf{v}})$ . ", "page_idx": 7}, {"type": "text", "text": "Proposition 4.2 (Sample Complexity of Approximate CVaR of USW). Suppose that $\\tilde{\\textbf{v}}$ is a multivariate sub-Gaussian random variable with mean $\\bar{\\mathbf{v}}\\,\\in\\,\\mathbb{R}^{n m}$ and covariance proxy $\\mathbf{S}\\in\\mathbb{R}^{n m\\times n m}$ , i.e., for all vectors $z\\,\\in\\,\\mathbb{R}^{n m}\\,:\\,\\mathbb{E}_{\\mathbf{v}\\sim\\mathcal{D}_{\\tilde{\\mathbf{v}}}}\\left[\\exp((\\mathbf{v}-\\bar{\\mathbf{v}})^{\\mathsf{T}}z))\\right]\\,\\le\\,\\exp(z^{\\mathsf{\\tau}}\\mathbf{s}z/2)$ , and that, for any risk level $\\alpha\\in(0,\\frac{1}{2})$ and allocation $\\mathbf{a}\\in{\\mathcal{A}}$ , there exists probability density threshold $\\eta>0$ and radius $\\gamma>0$ , s.t., $f_{\\mathbf{a}}{\\bar{(}}x)>\\eta\\,,\\forall x\\in[\\nu_{\\alpha}-\\gamma,\\nu_{\\alpha}+\\gamma]$ . Set $\\begin{array}{r}{\\forall G\\in\\mathcal{G}:\\mathbf{a}_{G}^{\\prime}=\\frac{\\mathbf{w}_{G}\\cdot\\mathbf{a}_{G}}{|G|}}\\end{array}$ . Then, for any confidence parameter $\\delta\\in(0,1)$ and error tolerance $\\varepsilon>0$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{y_{\\mathrm{\\scriptsize~\\left[\\,\\mathrm{sup}\\,\\big\\vert}\\,\\hat{c}_{h,\\alpha}(\\mathbf{a})-c_{\\alpha}(\\mathbf{a})\\big\\vert\\leq\\varepsilon\\right]}\\geq1-\\delta\\quad f o r\\quad h>\\left[\\frac{8\\operatorname*{max}_{\\mathbf{a}\\in A}\\mathbf{a}^{\\prime\\intercal}\\mathbf{S}\\mathbf{a}^{\\prime},8\\big)\\ln\\left(\\frac{6|A|}{\\delta}\\right)}{\\operatorname*{min}(\\varepsilon^{2},16\\gamma^{2})\\alpha^{2}\\operatorname*{min}(\\eta^{2},1)}\\right]\\enspace.}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proposition 4.2 follows directly from Theorem 3.1 of L.A. et al. [35]. When $\\widetilde{{\\mathbf v}}$ is Gaussian distributed, we can circumvent the sampling approach and instead solve an optimization problem (Proposition 4.3), which depends solely on the mean and covariance of $\\tilde{\\textbf{v}}$ . ", "page_idx": 7}, {"type": "text", "text": "Proposition 4.3 (CVaR of USW for Gaussian Distributions). If $\\tilde{\\textbf{v}}$ is distributed as a multivariate Gaussian, i.e., $\\tilde{\\mathbf{v}}\\sim\\mathcal{N}(\\bar{\\mathbf{v}},\\mathbf{S})$ , then, the optimization problem in (9) simplifies to ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A}}\\mathbf{a}^{\\intercal}\\bar{\\mathbf{v}}-\\frac{\\phi(\\Phi^{-1}(\\alpha))}{\\alpha}\\sqrt{\\mathbf{a}^{\\intercal}\\mathbf{S}\\mathbf{a}}\\ \\,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\phi$ and $\\Phi$ denote the probability density function and cumulative density function, respectively, of the standard normal distribution $\\mathcal{N}(0,1)$ . The problem in (11) is concave and can be solved without sample approximation using the projected gradient ascent method. ", "page_idx": 8}, {"type": "text", "text": "4.2 CVaR Allocation for Group Egalitarian Welfare ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For our final objective, we wish to optimize egalitarian welfare under uncertainty using the CVaR approach. We formulate this optimization problem as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A}}\\mathrm{CVaR}_{\\alpha}\\left[\\operatorname*{min}_{G\\in\\mathcal{G}}\\mathbf{u}_{G}(\\mathbf{a},\\tilde{\\mathbf{v}})\\right]\\doteq\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A},b\\in\\mathbb{R}}\\left\\{b-\\frac{1}{\\alpha}\\underbrace{\\mathbb{E}}_{\\tilde{\\mathbf{v}}\\sim\\mathcal{D}_{\\tilde{\\mathbf{v}}}}\\left[\\left(b-\\operatorname*{min}_{G\\in\\mathcal{G}}\\mathbf{u}_{G}(\\mathbf{a},\\tilde{\\mathbf{v}})\\right)_{+}\\right]\\right\\}~.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "To optimize the problem described in the above equation, we solve a linear program similar to the one used for optimizing the CVaR utilitarian objective in (9). See Proposition H.3 for more details. ", "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We run experiments on three reviewer assignment datasets. The datasets contain bids from the International Conference on Autonomous Agents and Multiagent Systems (AAMAS) 2015, 2016, and 2021 [41, 42]. We consider the papers as the \u201cagents\u201d and the reviewers as the \u201citems.\u201d This is a fairly standard assumption in most recent reviewer assignment approaches, reflecting the primary goal of peer review to assign qualified and interested reviewers to papers [16, 30, 31, 38, 45, 54]. ", "page_idx": 8}, {"type": "text", "text": "Reviewers issue bids of yes, maybe, no, or no response. We run two experiments with this data. In one, we binarize the bids such that yes and maybe are considered affirmative and no is considered negative, while in the other we convert the bids to numerical scores such that yes is 1, maybe is .5, and $\\ n\\bigcirc$ is 0.01. Under the binarized model, we fit a logistic matrix factorization model to predict whether the bid is affirmative or negative, and in the continuous model, we fit a Gaussian process matrix factorization model [36]. We derive probability distributions and uncertainty sets from these models. More details on prediction and uncertainty set construction are in Appendix E. These datasets do not contain groups of papers and reviewers, so we create 4 roughly balanced clusters of reviewers and papers for each dataset using the procedure outlined in Appendix F. We define our valid set of assignments $\\boldsymbol{\\mathcal{A}}$ as follows. For each paper $a\\in N$ , we set $\\underline{{{\\kappa}}}_{a}=\\bar{\\kappa}_{a}=3$ for all $a$ in AAMAS 2015, and $\\underline{{{\\kappa}}}_{a}=\\bar{\\kappa}_{a}=2$ for all $a$ in AAMAS 2016 and 2021. For each reviewer $i$ , we set $\\underline{{\\psi}}_{i}=0$ and $\\bar{\\psi}_{i}=15$ for 2015 and 2016 and 4 for 2021. We optimize and evaluate $\\mathrm{CVaR}_{0.01}$ ; we take 4, 000 samples from the distribution to optimize for CVaR using the sampling-based approach, and we take 10, 000 samples to estimate the CVaR for evaluation. We optimize and evaluate the robust welfare at the $\\alpha\\:=\\:0.3$ level (there is a $70\\%$ chance the true values lie in the uncertainty set). We constrain the na\u00a8\u0131ve and CVaR approaches to select integer allocations, while the robust approach selects fractional allocations without rounding. ", "page_idx": 8}, {"type": "text", "text": "All results are averaged over 5 runs of subsampling $20\\%$ of each dataset. For each run, we construct 6 allocations, maximizing the na\u00a8\u0131ve central estimate, CVaR and robust statistics for USW and GESW respectively. We evaluate each allocation on each metric. For each run, we normalize each metric by the maximum value achieved for that metric by any allocation. We normalize in this manner to highlight that the allocation targeted for a given objective always returns the highest value on that objective, and because the absolute optimal values differ across runs. ", "page_idx": 8}, {"type": "text", "text": "All code is available at https://github.com/justinpayan/RAU2. Overall Performance Table 2 shows the results for the binarized version of AAMAS 2015 bids. Similar tables for the 5 other settings are included in Appendix G. Each row shows the metrics for the allocation produced by the method which optimizes for the objective shown in the left-most column. All non-robust methods have 0 robust welfare, indicating that if robustness to adversarial noise is desired, it is very important to consider this objective explicitly. Relatively little noise is actually present in this dataset, as the $\\mathrm{CVaR}_{0.01}$ is quite high for both the na\u00a8\u0131ve USW-optimal and GESW-optimal in all cases. ", "page_idx": 8}, {"type": "text", "text": "Since the USW-optimal solution has very high GESW, we implement a simulated example to explore when the USW-optimal solution fails to have high GESW. We find that in a number of settings, the GESW of the USW-optimal solution is much lower than the GESW-optimal solution. Appendix I explains the details of the simulation setting and the results. ", "page_idx": 8}, {"type": "image", "img_path": "6KThdqFgmA/tmp/5ebe7fa88a791f910e9113735881a9202375eb8960ac5b8f854d15f7350693db.jpg", "img_caption": ["Figure 1: Left: CVaR as noise increases for AAMAS 2015. Right: Convergence behavior of the Iterated Quadratic Program (Iterated QP) vs. Adversarial Projected Subgradient Ascent approach on AAMAS 2015. "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "6KThdqFgmA/tmp/bb4566edb13e2d4a8ab5c1ae2304550f605cb116cd9feb928405e9ca7827026c.jpg", "table_caption": ["Table 2: Performance of different allocations across each metric on the AAMAS 2015 dataset. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Robustness under Increasing Uncertainty Figure 1 shows the $\\mathrm{CVaR}_{0.01}$ on the Gaussian version of all three datasets as we artificially increase the amount of noise. We multiply the standard deviations of the Gaussian distributions by a scalar and optimize for the CVaR or the na\u00a8\u0131vely-computed USW and GESW. We then plot $\\mathrm{CVaR}_{0.01}$ as noise increases. Although the CVaR approach is less important at low noise levels, the CVaR of welfare decreases for both welfare measures as noise increases. GESW has a sharper decline than USW. We see that as the noise increases, the $\\mathrm{CVaR}_{0.01}$ of the baseline USW and GESW maximizing allocations drops off relative to the same value for the CVaR-optimized allocation. ", "page_idx": 9}, {"type": "text", "text": "We also verify that when we model valuations using a negatively-skewed Gaussian distribution with the same means and variances, we see increasing importance of optimizing for CVaR relative to uncertainty-unaware USW and robust USW. The difference is sharper as the skew parameter gets more negative. Details of this experiment and its results are included in Appendix J. ", "page_idx": 9}, {"type": "text", "text": "Runtime For the robust optimization setting with ellipsoidal uncertainty sets (derived from confidence intervals over the Gaussian process matrix factorization), we compare the Iterated QP approach (Corollary 3.3) to adversarial projected subgradient ascent on the original max-min problem (as in [16]). We find that Iterated QP converges much faster than the adversarial projected subgradient ascent algorithm on both AAMAS 2015 (Figure 1) and 2016 (Figure 3). Adversarial projected subgradient ascent fails to converge in 1, 000 iterations for the robust GESW objective on all datasets and the USW objective on AAMAS 2021. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In conclusion, we explore the stochastic and robust optimization regimes for utilitarian and group egalitarian welfare objectives. The robust optimization algorithms depend on the form of the uncertainty set. We show that when the uncertainty set has linear constraints only, the resulting problem is an LP and can be solved efficiently. Under ellipsoidal constraints, we demonstrate an iterative quadratic programming approach converges much faster than adversarial projected subgradient ascent. In the stochastic regime, we lay out the sample complexity of CVaR for the utilitarian welfare objective. We demonstrate the feasibility of estimating probability distributions and uncertainty sets on three years of bid data from AAMAS, and show that the robust and CVaR approaches demonstrated in this paper combat the uncertainty present in these three datasets. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is generously supported by Army Research Lab DEVCOM Data and Analysis Center - Contract W911QX23D0009 and NSF grant IIS-2327057. Cousins was supported by the University of Massachusetts Center for Data Science Fellowship. This work was performed using high performance computing equipment obtained under a grant from the Collaborative R&D Fund managed by the Massachusetts Technology Collaborative. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Emily L Aiken, Guadalupe Bedoya, Joshua E Blumenstock, and Aidan Coville. Program targeting with machine learning and mobile phone data: Evidence from an anti-poverty intervention in afghanistan. Journal of Development Economics, 161:103016, 2023.   \n[2] Amine Allouah, Christian Kroer, Xuan Zhang, Vashist Avadhanula, Anil Dania, Caner Gocmen, Sergey Pupyrev, Parikshit Shah, and Nicolas Stier. Robust and fair work allocation. arXiv preprint arXiv:2202.05194, 2022.   \n[3] Igor Averbakh. Minmax regret solutions for minimax optimization problems with uncertainty. Operations Research Letters, 27(2):57\u201365, 2000.   \n[4] Haris Aziz, Evi Micha, and Nisarg Shah. Group fairness in peer review. Proceedings of the 37th Annual Conference on Neural Information Processing Systems (NeurIPS), pages 64885\u201364895, 2023.   \n[5] Mohammad Javad Azizi, Phebe Vayanos, Bryan Wilder, Eric Rice, and Milind Tambe. Designing fair, efficient, and interpretable policies for prioritizing homeless youth for housing resources. In Proceedings of the 15th International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research (CPAIOR), pages 35\u201351, 2018.   \n[6] Adelchi Azzalini and Antonella Capitanio. Statistical applications of the multivariate skew normal distribution. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):579\u2013602, 1999.   \n[7] Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimization. Princeton University Press, 2009.   \n[8] Dimitris Bertsimas, David B Brown, and Constantine Caramanis. Theory and applications of robust optimization. SIAM Review, 53(3):464\u2013501, 2011.   \n[9] John R Birge and Francois Louveaux. Introduction to stochastic programming. Springer Science & Business Media, 2011.   \n[10] Craig Boutilier, Relu Patrascu, Pascal Poupart, and Dale Schuurmans. Constraint-based optimization and utility elicitation using the minimax decision criterion. Artificial Intelligence, 170(8-9):686\u2013713, 2006.   \n[11] Craig Boutilier. Computational decision support: Regret-based models for optimization and preference elicitation. In Thomas Zentall and Philip Crowley, editors, Comparative Decision Making, chapter 14, pages 423\u2013453. Oxford University Press, 2013.   \n[12] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge University Press, 2004.   \n[13] Eric Budish, Yeon-Koo Che, Fuhito Kojima, and Paul Milgrom. Implementing random assignments: A generalization of the Birkhoff-von Neumann theorem. In Cowles Summer Conference, 2009.   \n[14] Jan Buermann, Enrico H Gerding, and Baharak Rastegari. Fair allocation of resources with uncertain availability. In Proceedings of the 19th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pages 204\u2013212, 2020.   \n[15] Ioannis Caragiannis, David Kurokawa, Herve\u00b4 Moulin, Ariel D Procaccia, Nisarg Shah, and Junxing Wang. The unreasonable fairness of maximum Nash welfare. ACM Transactions on Economics and Computation (TEAC), 7(3):1\u201332, 2019.   \n[16] Cyrus Cousins, Justin Payan, and Yair Zick. Into the unknown: Assigning reviewers to papers with uncertain affinities. In Proceedings of the 16th International Symposium on Algorithmic Game Theory (SAGT), pages 179\u2013197, 2023.   \n[17] Cyrus Cousins. An axiomatic theory of provably-fair welfare-centric machine learning. In Proceedings of the 35th Annual Conference on Neural Information Processing Systems (NeurIPS), pages 16610\u201316621, 2021.   \n[18] Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of constrained min-max optimization. In Proceedings of the 53rd Annual ACM Symposium on Theory of Computing (STOC), page 1466\u20131478, 2021.   \n[19] Anand Deo and Karthyek Murthy. Efficient black-box importance sampling for VaR and CVaR estimation. In Proceedings of the 2021 Winter Simulation Conference (WSC), pages 1\u201312, 2021.   \n[20] Siddartha Devic, David Kempe, Vatsal Sharan, and Aleksandra Korolova. Fairness in matching under uncertainty. Proceedings of the 40th International Conference on Machine Learning (ICML), pages 7775\u20137794, 2023.   \n[21] Kate Donahue and Jon Kleinberg. Fairness and utilization in allocating resources with uncertain demand. In Proceedings of the 3rd ACM Conference on Fairness, Accountability and Transparency $(F\\!A T^{*})$ , pages 658\u2013668, 2020.   \n[22] Evan Dong and Cyrus Cousins. Decentering imputation: Fair learning at the margins of demographics. In Proceedings of the 2022 ICML Queer in AI Workshop, 2022.   \n[23] John C Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally robust optimization. The Annals of Statistics, 49(3):1378\u20131406, 2021.   \n[24] Corrado Gini. On the measure of concentration with special reference to income and statistics. Colorado College Publication, General Series, 208(1):73\u201379, 1936.   \n[25] Denizalp Goktas and Amy Greenwald. Convex-concave min-max Stackelberg games. Proceedings of the 35th Annual Conference on Neural Information Processing Systems (NeurIPS), pages 2991\u20133003, 2021.   \n[26] Bram L Gorissen, \u02d9Ihsan Yan\u0131kog\u02d8lu, and Dick Den Hertog. A practical guide to robust optimization. Omega, 53:124\u2013137, 2015.   \n[27] Negin Gorlezaei, Patrick Jaillet, and Zijie Zhou. Online resource allocation with samples. arXiv preprint arXiv:2210.04774, 2022.   \n[28] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023. Accessed August 21, 2024.   \n[29] L. Jeff Hong and Guangwu Liu. Monte Carlo estimation of value-at-risk, conditional valueat-risk and their sensitivities. In Proceedings of the 2011 Winter Simulation Conference (WSC), pages 95\u2013107, 2011.   \n[30] Steven Jecmen, Hanrui Zhang, Ryan Liu, Nihar Shah, Vincent Conitzer, and Fei Fang. Mitigating manipulation in peer review via randomized reviewer assignments. In Proceedings of the 34th Annual Conference on Neural Information Processing Systems (NeurIPS), pages 12533\u201312545, 2020.   \n[31] Ari Kobren, Barna Saha, and Andrew McCallum. Paper matching with local fairness constraints. In Proceedings of the 25th International Conference on Knowledge Discovery and Data Mining (KDD), pages 1247\u20131257, 2019.   \n[32] Panos Kouvelis and Gang Yu. Robust discrete optimization and its applications. Springer Science & Business Media, 2013.   \n[33] Pavlo Krokhmal, Jonas Palmquist, and Stanislav Uryasev. Portfolio optimization with conditional value-at-risk objective and constraints. Journal of Risk, 4:43\u201368, 2002.   \n[34] Amanda R Kube, Sanmay Das, and Patrick J Fowler. Fair and efficient allocation of scarce resources based on predicted outcomes: Implications for homeless service delivery. Journal of Artificial Intelligence Research, 76:1219\u20131245, 2023.   \n[35] Prashanth L.A., Krishna Jagannathan, and Ravi Kolla. Concentration bounds for CVaR estimation: The cases of light-tailed and heavy-tailed distributions. In Proceedings of the 37th International Conference on Machine Learning (ICML), pages 5577\u20135586, 2020.   \n[36] Neil D Lawrence and Raquel Urtasun. Non-linear matrix factorization with Gaussian processes. In Proceedings of the 26th International Conference on Machine Learning (ICML), pages 601\u2013608, 2009.   \n[37] Daniel Levy, Yair Carmon, John C Duchi, and Aaron Sidford. Large-scale methods for distributionally robust optimization. In Proceedings of the 34th Annual Conference on Neural Information Processing Systems (NeurIPS), pages 8847\u20138860, 2020.   \n[38] Kevin Leyton-Brown, Mausam, Yatin Nandwani, Hedayat Zarkoob, Chris Cameron, Neil Newman, and Dinesh Raghu. Matching papers and reviewers at large conferences. Artificial Intelligence, 331:104119, 2024.   \n[39] Richard J Lipton, Evangelos Markakis, Elchanan Mossel, and Amin Saberi. On approximately fair allocations of indivisible goods. In Proceedings of the 5th ACM Conference on Electronic Commerce (EC), pages 125\u2013131, 2004.   \n[40] Elita A Lobo, Mohammad Ghavamzadeh, and Marek Petrik. Soft-robust algorithms for batch reinforcement learning. arXiv preprint arXiv:2011.14495, 2020.   \n[41] Nicholas Mattei and Toby Walsh. Preflib: A library of preference data. In Proceedings of the 3rd International Conference on Algorithmic Decision Theory (ADT), pages 259\u2013270, 2013.   \n[42] Nicholas Mattei and Toby Walsh. A preflib.org retrospective: Lessons learned and new directions. Trends in Computational Social Choice. AI Access Foundation, pages 289\u2013309, 2017.   \n[43] Hongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust optimization with f-divergences. Proceedings of the 30th Annual Conference on Neural Information Processing Systems (NeurIPS), pages 2208\u20132216, 2016.   \n[44] W\u0142odzimierz Ogryczak, Adam Wierzbicki, and Marcin Milewski. A multi-criteria approach to fair and efficient bandwidth allocation. Omega, 36(3):451\u2013463, 2008.   \n[45] Justin Payan and Yair Zick. I will have order! Optimizing orders for fair reviewer assignment. In Proceedings of the 31st International Joint Conference on Artificial Intelligence (IJCAI), pages 440\u2013446, 2022.   \n[46] Dominik Peters, Ariel D Procaccia, and David Zhu. Robust rent division. In Proceedings of the 36th Annual Conference on Neural Information Processing Systems (NeurIPS), pages 13864\u2013 13876, 2022.   \n[47] Andra\u00b4s Pre\u00b4kopa. Stochastic programming, volume 324. Springer Science & Business Media, 2013.   \n[48] David Pujol, Ryan McKenna, Satya Kuppam, Michael Hay, Ashwin Machanavajjhala, and Gerome Miklau. Fair decision making using privacy-protected data. In Proceedings of the 2020 ACM Conference on Fairness, Accountability and Transparency (FAT\\*), pages 189\u2013199, 2020.   \n[49] Aida Rahmattalabi, Phebe Vayanos, Kathryn Dullerud, and Eric Rice. Learning resource allocation policies from observational data with an application to homeless services delivery. In Proceedings of the 2022 ACM Conference on Fairness, Accountability and Transparency (FAccT), pages 1240\u20131256, 2022.   \n[50] R Tyrrell Rockafellar and Stanislav Uryasev. Optimization of conditional value-at-risk. Journal of Risk, 2:21\u201342, 2000.   \n[51] Erik Rolland, Raymond A Patterson, Keith Ward, and Bajis Dodin. Decision support for disaster management. Operations Management Research, 3:68\u201379, 2010.   \n[52] Andrzej Ruszczyn\u00b4ski and Alexander Shapiro. Stochastic programming models. Handbooks in Operations Research and Management Science, 10:1\u201364, 2003.   \n[53] Isaac Slavitt. Prioritizing municipal lead mitigation projects as a relaxed knapsack optimization: a method and case study. International Transactions in Operational Research, 30(6):3719\u2013 3737, 2023.   \n[54] Ivan Stelmakh, Nihar B Shah, and Aarti Singh. PeerReview4All: Fair and accurate reviewer assignment in peer review. In Proceedings of the 30th International Conference on Algorithmic Learning Theory (ALT), pages 828\u2013856, 2019.   \n[55] Yuanhao Wang and Jian Li. Improved algorithms for convex-concave minimax optimization. In Proceedings of the 34th Annual Conference on Neural Information Processing Systems (NeurIPS), pages 4800\u20134810, 2020.   \n[56] Felix Wex, Guido Schryen, and Dirk Neumann. A fuzzy decision support model for natural disaster response under informational uncertainty. International Journal of Information Systems for Crisis Response and Management (IJISCRAM), 4(3):23\u201341, 2012.   \n[57] Felix Wex, Guido Schryen, Stefan Feuerriegel, and Dirk Neumann. Emergency response in natural disaster management: Allocation and scheduling of rescue units. European Journal of Operational Research, 235(3):697\u2013708, 2014.   \n[58] Tony Sit Yue Xing and Hoi Ying Wong. Variance reduction for risk measures with importance sampling in nested simulation. Quantitative Finance, 22(4):657\u2013673, 2022.   \n[59] Wenbin Zhang and Jeremy C Weiss. Fair decision-making under uncertainty. In Proceedings of the 21st IEEE International Conference on Data Mining (ICDM), pages 886\u2013895, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Additional Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Gorissen et al. [26] provide an excellent overview of optimization under uncertainty, including techniques used in this work, while Ben-Tal et al. [7], Bertsimas et al. [8] offer additional background on robust optimization. A standard approach in this regime is analyzing the dual of the uncertainty, as we generally do in this work. Stochastic optimization has a wide literature; the books by Birge and Louveaux [9], Levy et al. [37], Pr\u00b4ekopa [47], Ruszczyn\u00b4ski and Shapiro [52] present wide-ranging introductions to the topic. Conditional value at risk (CVaR) can often be approximately optimized by sampling and optimizing over an objective composing the different samples [35, 40, 50]. ", "page_idx": 14}, {"type": "text", "text": "B Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We believe this work has the potential for a significant positive societal impact. Fair resource allocation algorithms are essential for various systems, including assigning reviewers in peer review processes, allocating resources to homeless and low-income populations, distributing emergency response resources during natural disasters, and resettling refugees. In this work, we develop methods for efficiently optimizing allocations of constrained resources under various fairness objectives while addressing uncertainty in resource preferences. These methods can be directly applied to the aforementioned problems. However, we advise users to conduct extensive testing on similar datasets before deploying these algorithms in real-world scenarios. ", "page_idx": 14}, {"type": "text", "text": "C Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The CVaR approach requires solving linear programs with a large number of samples to be effective, which makes them computationally expensive. One potential solution is to leverage importance sampling methods to reduce the variance of the estimator [19, 58]. Future research could benefit from empirically and theoretically analyzing other fairness objectives like Nash welfare [15], Gini index [24], and envy-freeness [39]. ", "page_idx": 14}, {"type": "text", "text": "D Constructing uncertainty sets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section we demonstrate a simple and natural approach to construct an uncertainty set using a logistic regression estimator. Logistic regression models with bounded cross-entropy loss result in polyhedral uncertainty sets. Replacing the logistic regression model with a model with bounded squared-error loss, or simply taking the confidence interval of a multivariate Gaussian, results in truncated ellipsoidal uncertainty sets. We construct uncertainty sets per group in all cases. ", "page_idx": 14}, {"type": "text", "text": "Assume we have a discrete set of $c$ values $L\\subseteq\\mathbb{R}$ , with $L=\\{\\ell_{1},\\ldots,\\ell_{c}\\}$ . For each agent $i$ and item type $j$ we denote the true distribution over values $p^{*}(\\ell|(i,j))$ and the distribution predicted by the logistic regression model is $\\hat{p}(\\ell|(i,j))$ . ", "page_idx": 14}, {"type": "text", "text": "We estimate the cross-entropy loss of the model on a test set $T$ , where $\\left|T\\right|=t$ . This test set can be segmented by the group identity of the agent, such that we have $T_{G_{1}},T_{G_{2}},\\ldots.T_{G_{g}}$ for each of the $g$ groups (with sizes $t_{G_{1}},\\ldots,t_{G_{g}})$ . We assume that the test set comes from the same distribution as the agent-item pairs of the assignment problem; this can be achieved either during dataset construction or by limiting the assignments (through the $_{C}$ constraints) to better reflect the test distribution. We can also apply likelihood reweighting in our uncertainty set construction, as in [16], though we do not do so here. ", "page_idx": 14}, {"type": "text", "text": "For an agent $a$ and item type $i$ , the cross-entropy loss of the distribution $\\hat{p}$ with respect to the distribution $p$ is defined as ", "page_idx": 14}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbb{H}(p(\\ell|(a,i)),\\hat{p}(\\ell|(a,i)))\\overset{.}{=}-\\sum_{\\ell\\in L}p(\\ell|(a,i))\\ln\\hat{p}(\\ell|(a,i))}\\end{array}$ . For each $T_{G}$ , we compute the mean of the cross-entropy loss $\\begin{array}{r}{\\hat{\\xi}_{G}=\\frac{1}{t_{G}}\\sum_{(a,i)\\in T_{G}}\\mathbb{H}(p(\\ell|(a,i)),\\hat{p}(\\ell|(a,i)))}\\end{array}$ , as well as the standard error of the mean \u03b7\u02c6G = t1G (i,j)\u2208TG(H(p(\u2113|(a, i)), p\u02c6(\u2113|(a, i))) \u2212\u03be\u02c6G)2 12 . We model the distribution over cross-entropy losses for group $G$ as $\\mathcal{N}(\\hat{\\xi}_{G},\\hat{\\eta}_{G})$ . We want an uncertainty set $\\mathcal{V}$ such that the true values lie outside $\\nu$ with probability at most $\\alpha$ . Thus, using a union bound, we require each uncertainty set $\\nu_{G}$ for individual groups to contain the true valuations with probability at least $1-$ $\\frac{\\alpha}{g}$ . We can thus give the bound that the cross entropy loss is at most $\\begin{array}{r}{\\Phi^{-1}(1-\\frac{\\alpha}{g},\\hat{\\xi}_{G},\\hat{\\eta}_{G})}\\end{array}$ , where $\\bar{\\Phi}^{-1}(p,\\mu,\\sigma)$ denotes the $p$ percentile of a Gaussian distribution with mean $\\mu$ and standard deviation $\\sigma$ . ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "In our assignment problem, for each group $G$ with agents $N_{G}$ we obtain the uncertainty set ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{t_{G}m}\\sum_{a\\in N_{G},i\\in I}\\mathbb{H}(p(\\ell|(a,i)),\\hat{p}(\\ell|(a,i)))\\leq\\Phi^{-1}(1-\\frac{\\alpha}{g},\\hat{\\xi}_{G},\\hat{\\eta}_{G})\\,\\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The bound can be made tighter if we restrict some pairs using $_{C}$ , in which case the cross-entropy term on the left side is only averaged over the pairs which are not restricted. ", "page_idx": 15}, {"type": "text", "text": "E Logistic and Gaussian Process Matrix Factorization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Both models define probability distributions over outcomes, which we use to compute and evaluate the CVaR of utilitarian and egalitarian welfare. For the logistic model, we build a polyhedral uncertainty set by estimating the cross-entropy loss on a held-out test set, and for the Gaussian process model we simply consider the confidence intervals of the resulting Gaussian distribution. ", "page_idx": 15}, {"type": "text", "text": "For the binarized bids, we first set aside some of the observed bids as a test set. We estimate the missing bids and the bids for the held-out test pairs using logistic matrix factorization. Setting a hidden dimension size $d$ , we construct two matrices $\\mathbf{X}\\in\\overline{{\\mathbb{R}^{n\\times d}}}$ and $\\mathbf{Y}\\in\\mathbb{R}^{m\\times d}$ . We set $d=20$ . Let $\\mathbf{V}^{*}$ denote the true binarized bid matrix, where we observe entries for the training set pairs $(a,i)\\in T$ . We predict the probability of an affirmative bid as $\\sigma((\\mathbf{XY^{\\top}})_{a,i})$ where $\\sigma$ is the logistic sigmoid function. We select $\\mathbf{X}$ and $\\mathbf{Y}$ to minimize the loss function ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{(a,i)\\in T}-\\mathbf{V}_{a,i}^{*}\\ln\\left(\\sigma((\\mathbf{XY}^{\\mathsf{T}})_{a,i})\\right)-\\mathbf{V}_{a,i}^{*}\\ln\\left((\\mathbf{XY^{\\mathsf{T}}})_{a,i}\\right)~~.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For CVaR, we take samples from the distribution defined by $\\sigma(\\mathbf{XY^{\\top}})$ , assuming all pairs are independently-distributed. We also construct an uncertainty set as described in Appendix $\\mathrm{D}$ using the cross-entropy loss on the test pairs. ", "page_idx": 15}, {"type": "text", "text": "Under the Gaussian process matrix factorization model [36], we simply predict a mean and variance of a Gaussian distribution for each reviewer-paper pair. We can then sample values independently for each pair, or give a confidence interval for the joint Gaussian with $m n-1$ degrees of freedom. ", "page_idx": 15}, {"type": "text", "text": "F Grouping Papers and Reviewers ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We group papers and reviewers as follows: given the real-valued bids in the set $\\{0.01,.5,1\\}$ we set unknown bids to be 0. We then construct a graph with all reviewers and papers as nodes, and the bid score between reviewers and papers is the edge weight. All inter-reviewer and inter-paper edges are set to 0 edge weight. We apply spectral embedding with 5 dimensions to transform the nodes into vectors, and cluster the resulting vectors into 4 clusters to obtain 4 groups containing both papers and reviewers. To ensure a balance of reviewers and papers across clusters, we employ Lloyd\u2019s algorithm for KMeans clustering with the modification that during each assignment step we enforce a lower bound on the number of papers and number of reviewers assigned to each cluster. ", "page_idx": 15}, {"type": "text", "text": "G Additional Experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For the binarized AAMAS 2016 and 2021 datasets, Tables 3 and 4 show the performance of the baseline USW and GESW maximizing allocations, the $\\mathrm{CVaR}_{0.01}$ USW and GESW maximizing allocations, and the robust USW and GESW maximizing allocations at the $\\alpha=0.3$ level. Because so many of the bids in AAMAS 2021 are recorded as no, since no is the default bid, we randomly select $90\\%$ of the no bids to be converted to no response. ", "page_idx": 15}, {"type": "text", "text": "Tables 5 to 7 show the same results for the Gaussian matrix factorization version of the 3 datasets, with the $\\mathrm{CVaR}_{0.01}$ estimated by sampling from the estimated Gaussian distribution, and the adversarial welfare computed over the truncated ellipsoidal uncertainty set corresponding to the $1\\mathrm{~-~}\\alpha$ confidence interval of the Gaussian. ", "page_idx": 15}, {"type": "image", "img_path": "6KThdqFgmA/tmp/90c6d702baa0cf1d3228aba7c781e4e2777f07a1acb4f5fdb1e6d01a9809d4ff.jpg", "img_caption": ["Figure 2: $\\mathrm{CVaR}_{0.01}$ as noise increases for AAMAS 2016 and 2021. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "6KThdqFgmA/tmp/165f135978f39a680b2823952ace9bb1e9244267ca22f2260fcec086792d0f0b.jpg", "img_caption": ["Figure 3: Convergence of the Iterated QP vs. adversarial projected subgradient ascent on AAMAS 2016 dataset for the adversarial USW objective. The Iterated QP (in blue) converges much faster. ", "(a) AAMAS 2016 "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "6KThdqFgmA/tmp/e8c3a14dc48d720d26c960a597efb151d8ff7572f7248bcda118caf6980096ff.jpg", "table_caption": ["Table 3: Performance of different allocations across each metric on the AAMAS 2016 dataset. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "6KThdqFgmA/tmp/d1adbdab3b256145d97e0c409801dbe11db5f54f0cbb86832b4ab78f69a56c83.jpg", "table_caption": ["Table 4: Performance of different allocations across each metric on the AAMAS 2021 dataset. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "6KThdqFgmA/tmp/90cf4ca170d6bcd2bc5ca0d5a3ae0a7e73b65d2aa0c77352748c9ded2c0b64bf.jpg", "table_caption": ["Table 5: Performance of different allocations across each metric on the Gaussian AAMAS 2015 dataset. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "6KThdqFgmA/tmp/7178787f73f56f42d916500e85c26fdd1e1238b9e306243c9668492b5db58694.jpg", "table_caption": ["Table 6: Performance of different allocations across each metric on the Gaussian AAMAS 2016 dataset. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "6KThdqFgmA/tmp/9b5a731361b527b098f967d50355b59e64865e58db15db0c1d5942f2aa3d10d5.jpg", "table_caption": ["Table 7: Performance of different allocations across each metric on the Gaussian AAMAS 2021 dataset. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "H Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "H.1 Proof of Proposition 3.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proposition 3.1 (Robust Utilitarian Welfare Dual). The problem in (1) is equivalent to solving ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\boldsymbol{\\xi}\\in\\Lambda,\\lambda\\in\\mathbb{R}_{0+}^{l}}\\mathbf{p}^{\\intercal}\\mathbf{T}\\mathbf{q}+\\beta^{\\intercal}\\mathbf{e}-\\frac{1}{4}\\mathbf{p}^{\\intercal}\\mathbf{T}\\mathbf{p}+\\sum_{i=1}^{\\ell}\\left(\\lambda_{i}\\bar{\\mathbf{v}}_{i}^{\\intercal}\\mathbf{S}_{i}\\bar{\\mathbf{v}}_{i}-\\lambda_{i}r_{i}^{2}\\right)-\\mathbf{q}^{\\intercal}\\mathbf{T}\\mathbf{q}\\enspace,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbf{p}=\\pmb{\\xi}-Q^{\\intercal}\\beta,\\,\\mathbf{q}=\\sum_{i=1}^{\\ell}\\lambda_{i}\\mathbf{S}_{i}^{-1}\\bar{\\mathbf{v}}_{i}}\\end{array}$ , and $\\mathbf{T}=\\left(\\sum_{i=1}^{\\ell}\\lambda_{i}\\mathbf{S}_{i}^{-1}\\right)^{-1}$ . Let $\\xi^{*}$ be the optimal $\\xi$ in (3). Then the optimal alloc ation $\\mathbf{a}^{*}$ can be derived from $\\xi^{*}$ by finding $\\mathbf{a}\\in A$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\forall G\\in\\mathcal{G}:\\mathbf{a}_{G}\\preceq\\xi_{G}^{*}\\ \\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Consider the inner-minimization problem ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\underset{\\mathbf{v}\\in\\mathbb{R}^{n_{m}}}{\\mathrm{min}}\\,\\sum_{G\\in\\mathcal{G}}\\mathbf{a}_{G}^{\\intercal}\\mathbf{v}_{G}}\\\\ &{\\forall i\\in[1,l]:(\\mathbf{v}-\\bar{\\mathbf{v}}_{i})\\mathbf{S}_{i}^{-1}(\\mathbf{v}-\\bar{\\mathbf{v}}_{i})\\leq r_{i}^{2}}\\\\ &{\\qquad\\qquad Q\\mathbf{v}\\succeq\\mathbf{e}}\\\\ &{\\qquad\\qquad\\mathbf{v}\\succeq0\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that the above optimization problem is convex as the objective is an affine combination of $\\mathbf{v}$ , which is convex, and the linear and quadratic constraints are also convex. Thus, from the theory of convex optimization (Section 5.1 in Boyd et al. [12]), we know that maximizing the dual of a convex optimization problem is equivalent to minimizing the primal problem. We will therefore use the Lagrangian method for computing the dual of the above problem. ", "page_idx": 18}, {"type": "text", "text": "The Lagrangian for the above problem is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\therefore(\\mathbf{v},\\lambda\\in\\mathbb{R}_{0+}^{\\ell},\\beta\\in\\mathbb{R}_{0+}^{k},\\zeta\\in\\mathbb{R}_{0+}^{n m})=\\mathbf{a}^{\\mathsf{T}}\\mathbf{v}+\\sum_{i=1}^{\\ell}\\lambda_{i}\\big((\\mathbf{v}-\\bar{\\mathbf{v}}_{i})\\mathbf{S}_{i}^{-1}(\\mathbf{v}-\\bar{\\mathbf{v}}_{i})-r_{i}^{2}\\big)-\\beta^{\\mathsf{T}}(Q\\mathbf{v}-\\mathbf{e})-\\zeta^{\\mathsf{T}}\\mathbf{v}\\ .\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From the first-order optimality conditions, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial L\\left(\\mathbf{v},\\lambda\\in\\mathbb{R}_{0+}^{\\ell},\\beta\\in\\mathbb{R}_{0+}^{k},\\zeta\\in\\mathbb{R}_{0+}^{n m}\\right)}{\\partial\\mathbf{v}}=0}\\\\ {\\mathbf{a}+\\displaystyle\\sum_{i=1}^{\\ell}2\\lambda_{i}\\mathbf{S}_{i}^{-1}(\\mathbf{v}-\\bar{\\mathbf{v}}_{i})-Q^{\\top}\\beta-\\zeta=0}\\\\ {\\Longrightarrow\\mathbf{v}=\\frac{\\displaystyle\\sum_{i=1}^{\\ell}2\\lambda_{i}\\mathbf{S}_{i}^{-1}\\bar{\\mathbf{v}}_{i}-\\left(\\mathbf{a}-Q^{\\top}\\beta-\\zeta\\right)}{\\displaystyle\\sum_{i=1}^{\\ell}2\\lambda_{i}\\mathbf{S}_{i}^{-1}}~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Substituting this value of $\\mathbf{v}$ in (13), we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\lambda\\in\\mathbb R_{0+}^{t}}-\\frac{1}{4}\\big(\\big({\\mathbf a}-Q^{\\top}\\beta-\\zeta\\big)^{\\top}\\big(\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{i}{\\mathbf S}_{i}^{-1}\\big)^{-1}\\big({\\mathbf a}-Q^{\\top}\\beta-\\zeta\\big)\\big)+\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{i}\\bar{\\mathbf v}_{i}^{\\top}\\mathbf S_{i}^{-1}\\bar{\\mathbf v}_{i}}\\\\ &{\\displaystyle\\beta\\in\\mathbb R_{0+}^{n_{0+}}}\\\\ &{\\displaystyle\\zeta\\in\\mathbb R_{0+}^{n_{0}}-\\,\\big(\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{i}{\\mathbf S}_{i}^{-1}\\bar{\\mathbf v}_{i}\\big)\\big(\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{i}{\\mathbf S}_{i}^{-1}\\big)^{-1}\\big(\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{i}{\\mathbf S}_{i}^{-1}\\bar{\\mathbf v}_{i}\\big)}\\\\ &{\\displaystyle\\qquad\\qquad+\\,\\big({\\mathbf a}-Q^{\\top}\\beta-\\zeta\\big)^{\\top}\\big(\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{i}{\\mathbf S}_{i}^{-1}\\big)^{-1}\\big(\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{i}{\\mathbf S}_{i}^{-1}\\bar{\\mathbf v}_{i}\\big)-\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{i}r_{i}^{2}+\\beta^{\\top}\\mathbf e~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, we substitute the above dual problem back into the original problem in (1) to get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\underset{\\mathbf{a}\\in\\mathbb{R}_{+}^{\\ell}}{\\mathrm{max}}\\,-\\,\\frac{1}{4}\\big(\\big(\\mathbf{a}-Q^{\\top}\\beta-\\boldsymbol{\\xi}\\big)^{\\top}\\big(\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{i}\\mathbf{S}_{i}^{-1}\\big)^{-1}\\big(\\mathbf{a}-Q^{\\top}\\beta-\\boldsymbol{\\xi}\\big)\\big)+\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{i}\\bar{\\mathbf{v}}_{i}^{\\top}\\mathbf{S}_{i}^{-1}\\bar{\\mathbf{v}}_{i}}\\\\ &{\\displaystyle\\hat{\\beta}\\in\\mathbb{R}_{0+}^{\\ell}}\\\\ &{\\displaystyle\\zeta\\in\\mathbb{R}_{0+}^{n\\prime}=\\,\\big(\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{i}\\mathbf{S}_{i}^{-1}\\bar{\\mathbf{v}}_{i}\\big)^{\\top}\\big(\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{i}\\mathbf{S}_{i}^{-1}\\big)^{-1}\\big(\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{i}\\mathbf{S}_{i}^{-1}\\bar{\\mathbf{v}}_{i}\\big)}\\\\ &{\\qquad\\qquad+\\,\\big(\\mathbf{a}-Q^{\\top}\\beta-\\boldsymbol{\\xi}\\big)^{\\top}\\big(\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{i}\\mathbf{S}_{i}^{-1}\\big)^{-1}\\big(\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{i}\\mathbf{S}_{i}^{-1}\\bar{\\mathbf{v}}_{i}\\big)-\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{i}r_{i}^{2}+\\beta^{\\top}\\mathbf{e}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that the dual problem is concave in $\\lambda,\\beta$ , and $\\zeta$ (Section 5.1 in [12]). However, it is unclear if the dual is concave in allocation a. In order to guarantee concavity, we use the change of variables $\\pmb{\\xi}=\\mathbf{a}-\\boldsymbol{\\zeta}$ . From affine-composition rule in convex optimization (Section 3.2.2 in [12]), we know that if $f(\\mathbf{x})\\,\\mathbf{x}\\in\\mathbb{R}^{n}$ is convex, then $f(A x+b)\\;A\\in\\bar{\\mathbb{R}}^{n\\times n},\\pmb{b}\\in\\mathbb{R}^{n}$ is also convex in $\\textbf{\\em x}$ . Thus, the variable change $\\pmb{\\xi}=\\mathbf{a}-\\pmb{\\zeta}$ results in a objective that is concave in $\\xi,\\lambda,\\beta$ . The allocation variable a and the dual variable $\\zeta$ only appear in a linear constraint, which is also concave. Thus, the objective and the constraints are concave in a, $\\rho,\\lambda$ , and $\\xi$ , and $\\zeta$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\boldsymbol{\\xi}\\in\\Lambda,\\lambda\\in\\mathbb{R}_{0+}^{l}}\\mathbf{p}^{\\intercal}\\mathbf{T}\\mathbf{q}+\\beta^{\\intercal}\\mathbf{e}-\\frac{1}{4}\\mathbf{p}^{\\intercal}\\mathbf{T}\\mathbf{p}+\\displaystyle\\sum_{i=1}^{\\ell}\\left(\\lambda_{i}\\bar{\\mathbf{v}}_{i}^{\\intercal}\\mathbf{S}_{i}\\bar{\\mathbf{v}}_{i}-\\lambda_{i}r_{i}^{2}\\right)-\\mathbf{q}^{\\intercal}\\mathbf{T}\\mathbf{q}\\ ,}\\\\ &{\\quad\\beta\\in\\mathbb{R}_{0+}^{k}}\\\\ &{\\mathbf{a}\\in\\mathcal{A},\\boldsymbol{\\xi}\\in\\mathbb{R}_{0+}^{n}}\\\\ &{\\quad\\mathrm{~s.t.~}\\boldsymbol{\\xi}=\\mathbf{a}-\\boldsymbol{\\zeta}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{p}=Q^{\\intercal}\\boldsymbol{\\beta}+\\boldsymbol{\\xi}$ , $\\mathbf{T}=(\\sum_{i=1}^{\\ell}\\lambda_{i}\\mathbf{S}_{i}^{-1})^{-1}$ , $\\begin{array}{r}{\\mathbf{q}=\\sum_{i=1}^{\\ell}\\lambda_{i}\\mathbf{S}_{i}^{-1}\\bar{\\mathbf{v}}_{i}}\\end{array}$ , and $\\Lambda=\\pmb{\\mathcal{A}}-\\mathbb{R}_{0+}^{n m}=\\{\\pmb{\\xi}\\in$ $\\begin{array}{r}{\\mathbb{R}^{n m}\\mid\\forall a\\in N:\\sum_{i\\in I}\\xi_{a m+i}\\leq\\bar{\\kappa}_{a},\\forall i\\in I:\\sum_{a\\in N}\\pmb{\\xi}_{a m+i}\\leq\\psi_{i},\\pmb{\\xi}\\preceq c\\}.}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "We can solve the optimization problem in (14) using standard convex optimization techniques [12]. Alternatively, we can further simplify the problem by eliminating the allocation variable a and the dual variable $\\zeta$ and subsequently deriving them from the solution of the resultant problem. ", "page_idx": 19}, {"type": "text", "text": "Note that in (14), $\\mathbf{a}-\\boldsymbol{\\zeta}=\\boldsymbol{\\xi}$ . Let $(\\mathbf{a}^{*},\\zeta^{*})$ represent an optimal $(\\mathbf{a},\\boldsymbol{\\zeta})$ pair for the problem in (14). Now there can be multiple pairs of $(\\mathbf{a},\\boldsymbol{\\zeta})$ that are optimal. To eliminate $\\zeta$ and a, we need to first ensure that there exists a $\\pmb{\\xi}^{\\prime}\\in\\Lambda$ such that $\\xi^{\\prime}=\\mathbf{a}^{*}-\\zeta^{*}$ for at least one optimal pair $(\\mathbf{a}^{*},\\zeta^{*})$ . It is easy to see that if there exists such a $\\pmb{\\xi}^{\\prime}\\in\\Lambda$ , then, $\\xi^{\\prime}$ maximizes the objective in (14). Furthermore, since $\\Lambda$ is defined solely by upper-bound constraints on $\\xi$ imposed by $\\boldsymbol{\\mathcal{A}}$ , we can easily verify that it will contain at least one instance of $\\xi^{\\prime}$ that satisfies $\\xi^{\\prime}=\\mathbf{a}^{*}-\\zeta^{*}$ . ", "page_idx": 19}, {"type": "text", "text": "Thus, we can break down the problem in (14) into two sub-problems. In the first problem, we obtain the optimal value of $\\lambda,\\xi$ and $\\bar{\\boldsymbol\\beta}$ by solving ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\boldsymbol{\\zeta}^{*},\\beta^{*},\\boldsymbol{\\xi}^{*}=\\underset{\\boldsymbol{\\xi}\\in\\mathbb{R}_{0}^{n}}{\\mathrm{arg}\\mathrm{max}}\\;\\mathbf{p}^{\\top}\\mathbf{T}\\mathbf{q}^{\\top}+\\beta^{\\top}\\mathbf{e}-\\frac{1}{4}\\mathbf{p}^{\\top}\\mathbf{T}\\mathbf{p}+\\displaystyle\\sum_{i=1}^{\\ell}\\left(\\lambda_{i}\\bar{\\mathbf{v}}_{i}^{\\top}\\mathbf{S}_{i}\\bar{\\mathbf{v}}_{i}-\\lambda_{i}r_{i}^{2}\\right)-\\mathbf{q}^{\\top}\\mathbf{T}\\mathbf{q}+\\mathbf{p}^{\\top}\\mathbf{T}^{-1}\\mathbf{q}^{\\top}}&{}\\\\ {\\beta\\in\\mathbb{R}_{0}^{n}+}&{}\\\\ {\\varepsilon\\in\\Lambda}&{{}-\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{i}r_{i}^{2}+\\beta^{\\top}\\mathbf{e}~,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{p}=-Q^{\\intercal}\\beta+\\boldsymbol{\\xi}$ , $\\begin{array}{r}{\\mathbf{q}=\\sum_{i=1}^{\\ell}\\lambda_{i}\\mathbf{S}_{i}^{-1}\\bar{\\mathbf{v}}_{i}}\\end{array}$ , and $\\mathbf{T}=(\\sum_{i=1}^{\\ell}\\lambda_{i}\\mathbf{S}_{i}^{-1})^{-1}$ . Then, we can compute the set of optimal $(\\mathbf{a},\\boldsymbol{\\zeta})$ pair s by solving the system of equations: $\\{(\\mathbf{a}^{*},\\zeta^{*})~|~\\mathbf{a}\\in\\mathcal{A},\\,G\\in\\mathcal{G}:$ $\\mathbf{a}_{G}-\\zeta_{G}=\\xi_{G}^{*},\\zeta\\in\\mathbb{R}_{0+}^{n m}\\}$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "H.2 Proof of Corollary 3.2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Corollary 3.2 (Utilitarian Welfare with Polyhedral Uncertainty). In the case where the uncertainty set $\\mathcal{V}$ is defined purely by linear constraints, i.e., $\\mathcal{V}=\\left\\{\\mathbf{v}\\in\\dot{\\mathbb{R}}^{n m}\\ |\\ Q\\mathbf{v}\\geq\\mathbf{e},\\mathbf{v}\\succeq0\\right\\}$ , the optimal allocation $\\mathbf{a}^{*}$ for the problem in (1) can be computed by solving the linear program ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A},\\beta\\in\\mathbb{R}_{0+}^{k}}\\beta^{\\intercal}\\mathbf{e}\\quad s.t.\\ \\forall G\\in\\mathcal{G}:\\ Q_{G}^{\\intercal}\\beta_{G}\\preceq\\mathbf{a}_{G}\\ .\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Now consider the inner-minimization problem: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{\\operatorname*{min}_{{\\bf v}\\in\\mathbb{R}^{n m}}\\,\\sum_{G\\in\\mathcal{G}}{\\bf a}_{G}^{\\sf T}{\\bf v}_{G}}}}\\\\ {{\\displaystyle{\\cal Q}{\\bf v}\\succeq{\\bf e}}}\\\\ {{\\displaystyle{\\bf v}\\succeq0\\ .}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We compute the dual of the above problem using the Lagrangian method. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\mathbf{v},\\lambda\\in\\mathbb{R}_{0+}^{\\ell},\\beta\\in\\mathbb{R}_{0+}^{k},\\zeta\\in\\mathbb{R}_{0+}^{n m})=\\mathbf{a}^{\\mathsf{T}}\\mathbf{v}-\\beta^{\\mathsf{T}}(Q\\mathbf{v}-\\mathbf{e})-\\zeta^{\\mathsf{T}}\\mathbf{v}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=(\\mathbf{a}-Q^{\\mathsf{T}}\\beta-\\zeta)^{\\mathsf{T}}\\mathbf{v}+\\beta^{\\mathsf{T}}\\mathbf{e}}\\\\ &{\\qquad L(\\lambda\\in\\mathbb{R}_{0+}^{\\ell},\\beta\\in\\mathbb{R}_{0+}^{k},\\zeta\\in\\mathbb{R}_{0+}^{n m})=\\left\\{\\beta^{\\mathsf{T}}\\mathbf{e}\\quad\\quad(\\mathbf{a}-Q^{\\mathsf{T}}\\beta-\\zeta)\\succeq0\\quad.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, the dual is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\beta\\in\\mathbb{R}_{0+}^{k},\\zeta\\in\\mathbb{R}_{0+}^{n m}}{\\operatorname*{max}}\\beta^{\\intercal}\\mathbf{e}}\\\\ {Q^{\\intercal}\\beta-\\zeta\\preceq\\mathbf{a}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\zeta$ is non-negative, we can eliminate it to get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}_{\\beta\\in\\mathbb{R}_{0+}^{k}}\\beta^{\\intercal}\\mathbf{e}\\mathrm{~}}\\\\ {Q^{\\intercal}\\beta\\preceq\\mathbf{a}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By combining the dual with the outer-maximization problem in (1), we obtain the final result. ", "page_idx": 20}, {"type": "text", "text": "H.3 Proof of Corollary 3.3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Corollary 3.3 (Utilitarian Welfare with Ellipsoidal Uncertainty). Suppose that the set $\\nu$ in (1) is defined by a single truncated ellipsoidal constraint i.e., $\\begin{array}{r}{\\mathcal{V}=\\{\\mathbf{\\bar{v}}\\in\\mathbb{R}^{\\bar{n}m}\\mathrm{~}|\\mathrm{~}(\\mathbf{v}-\\bar{\\mathbf{v}})\\mathbf{S}^{-1}(\\mathbf{v}-\\bar{\\mathbf{v}})\\leq}\\end{array}$ $r^{2},\\mathbf{v}\\succeq\\dot{0}\\}$ . The problem in (1) is equivalent to solving ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\lambda\\in\\mathbb{R}_{0+},\\pmb{\\xi}\\in\\Lambda}\\pmb{\\xi}^{\\mathsf{T}}\\bar{\\mathbf{v}}-\\frac{\\pmb{\\xi}^{\\mathsf{T}}\\mathbf{S}\\pmb{\\xi}}{4\\lambda}-\\lambda r^{2}\\ \\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The exact optimal solution $(\\lambda^{*},\\xi^{*})$ to Equation (4) can be computed by alternately performing two steps until convergence: first, fixing $\\xi$ and optimizing $\\lambda,$ , i.e., $\\lambda={\\xi^{\\top}\\mathbf S\\pmb\\xi}/{2r}$ , and second, fixing $\\lambda$ and solving a concave quadratic program to optimize $\\xi$ . The optimal allocation $\\mathbf{a}^{*}$ can be computed from $\\xi^{*}$ as in Proposition 3.1. ", "page_idx": 20}, {"type": "text", "text": "Proof. Consider the inner-minimization problem ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{\\mathbf{v}\\in\\mathbb{R}^{n_{m}}}\\,\\sum_{G\\in\\mathcal{G}}\\mathbf{a}_{G}^{\\mathsf{T}}\\mathbf{v}_{G}}\\\\ &{\\quad\\quad\\quad(\\mathbf{v}-\\bar{\\mathbf{v}})^{\\mathsf{T}}\\mathbf{S}^{-1}(\\mathbf{v}-\\bar{\\mathbf{v}})\\leq r^{2}}\\\\ &{\\quad\\quad\\quad\\mathbf{v}\\succeq0\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similar to the approach in Proposition 3.1, we will use the Lagrangian method for computing the dual of the above problem. The Lagrangian for the above problem is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\nL(\\mathbf{v},\\lambda\\in\\mathbb{R}_{0+},\\zeta\\in\\mathbb{R}_{0+}^{n m})=\\mathbf{a}^{\\intercal}\\mathbf{v}+\\lambda\\left((\\mathbf{v}-\\bar{\\mathbf{v}})\\mathbf{S}^{-1}(\\mathbf{v}-\\bar{\\mathbf{v}})-r^{2}\\right)-\\zeta^{\\intercal}\\mathbf{v}\\,\\,\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "From the first-order optimality conditions, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial{\\cal L}\\left({\\bf v},\\lambda\\in\\mathbb{R}_{0+},\\zeta\\in\\mathbb{R}_{0+}^{n m}\\right)}{\\partial{\\bf v}}=0}\\\\ &{\\quad\\quad\\quad\\mathbf{a}+2\\lambda\\mathbf{S}^{-1}({\\bf v}-\\bar{\\bf v})-\\zeta=0}\\\\ &{\\Longrightarrow{\\bf v}=\\frac{2\\lambda{\\bf S}^{-1}\\bar{\\bf v}-({\\bf a}-\\zeta)}{2\\lambda{\\bf S}^{-1}}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Substituting the value of $\\mathbf{v}$ in the above equation in (15), we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\lambda\\in\\mathbb{R}_{0+}^{\\ell}}-\\frac{1}{4}\\left(\\left(\\mathbf{a}-\\zeta\\right)^{\\intercal}\\frac{\\mathbf{S}}{\\lambda}\\left(\\mathbf{a}-\\zeta\\right)\\right)+(\\mathbf{a}-\\zeta)^{\\intercal}\\bar{\\mathbf{v}}-\\lambda r^{2}\\ \\cdot\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From the theory of convex optimization [12], we know that the dual of a convex optimization problem is always concave, and therefore, the above optimization problem is concave in $\\lambda$ and $\\zeta$ . Combining the dual with the outer-maximization problem in (1), we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A}}\\operatorname*{max}_{\\lambda\\in\\mathbb{R}_{0+}^{\\ell}}-\\frac{1}{4}\\left(\\left(\\mathbf{a}-\\zeta\\right)^{\\mathsf{T}}\\frac{\\mathbf{S}}{\\lambda}\\left(\\mathbf{a}-\\zeta\\right)\\right)+(\\mathbf{a}-\\zeta)^{\\mathsf{T}}\\bar{\\mathbf{v}}-\\lambda r^{2}\\ \\cdot\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To further obtain concavity in allocation a, we follow the same procedure as in Proposition 3.1 and use the change of variables $\\pmb{\\xi}=\\mathbf{a}-\\boldsymbol{\\zeta}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A}}\\operatorname*{max}_{\\lambda\\in\\mathbb{R}_{0+},\\pmb{\\xi}\\in\\Lambda}\\pmb{\\xi}^{\\intercal}\\bar{\\mathbf{v}}-\\frac{\\pmb{\\xi}^{\\intercal}\\mathbf{S}\\pmb{\\xi}}{4\\lambda}-\\lambda r^{2}\\ \\cdot}\\\\ &{\\quad\\quad\\quad\\quad\\mathrm{s.t.}\\ \\pmb{\\xi}=\\mathbf{a}-\\zeta\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As a result of the change of variables, the allocation variable a and the dual variable $\\zeta$ now appear only in a linear constraint, which is convex. Furthermore, due to the affine composition property of convex functions (Section 3.2.2 in Boyd et al. [12]), the objective remains retains its concavity. Thus, the above optimization problem is concave in a, $\\lambda,\\zeta$ , and $\\xi$ . ", "page_idx": 21}, {"type": "text", "text": "Similar to the approach used in the proof of Proposition 3.1, we further simplify the problem by eliminating the allocation variables a and the dual variable $\\zeta$ and subsequently deriving them from the solution of the resultant problem. ", "page_idx": 21}, {"type": "text", "text": "From (16), we know that that ${\\bf a}-\\boldsymbol{\\zeta}\\,=\\,\\boldsymbol{\\xi}$ . Let $(\\mathbf{a}^{*},\\zeta^{*})$ represent an optimal $(\\mathbf{a},\\boldsymbol{\\zeta})$ pair for the problem in (14). Note that there can be multiple pairs of $(\\mathbf{a},\\boldsymbol{\\zeta})$ that are optimal. To eliminate $\\zeta$ and a, we need to find a set of feasible $\\xi$ , which we denote by $\\Lambda$ , such that there exists a $\\pmb{\\xi}^{\\prime}\\in\\Lambda$ such that $\\xi^{\\prime}=\\mathbf{a}^{*}-\\zeta^{*}$ for at least one optimal pair $(\\mathbf{a}^{*},\\zeta^{*})$ . It is easy to see that if there exists such a $\\pmb{\\xi}^{\\prime}\\in\\Lambda$ , then, $\\xi^{\\prime}$ maximizes the objective in (16). Furthermore, it easy to verify that $\\Lambda$ satisfies this criteria for optimality, i.e., it contains at least one $\\xi^{\\prime}$ that satisfies $\\xi^{\\prime}=\\mathbf{a}^{*}-\\zeta^{*}$ , for some optimal pair $(\\mathbf{a}^{*},\\zeta^{*})$ . ", "page_idx": 21}, {"type": "text", "text": "Thus, we can efficiently solve the problem in (16) in two steps. First, we obtain the optimal value of $\\lambda$ and $\\xi$ by solving the problem ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\lambda^{*},\\pmb{\\xi}^{*}=\\underset{\\lambda\\in\\mathbb{R}_{0+},\\pmb{\\xi}\\in\\Lambda}{\\arg\\operatorname*{max}}\\ \\pmb{\\xi}^{\\intercal}\\bar{\\mathbf{v}}-\\frac{\\pmb{\\xi}^{\\intercal}\\mathbf{S}\\pmb{\\xi}}{4\\lambda}-\\lambda r^{2}\\ \\cdot\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As in Proposition 3.1, we can compute the set of optimal $(\\mathbf{a},\\boldsymbol{\\zeta})$ pairs by solving the system of equations: $\\{(\\mathbf{a}^{*},\\zeta^{*})\\mid\\mathbf{a}\\in\\mathcal{A},\\,G\\in\\bar{\\mathcal{G}}:\\mathbf{a}_{G}-\\zeta_{G}=\\xi_{G}^{*},\\zeta\\in\\mathbb{R}_{0+}^{n m}\\}$ . ", "page_idx": 21}, {"type": "text", "text": "H.4 Proof of Proposition 3.5 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proposition 3.5 (Robust Group Egalitarian Dual). The problem in (5) is equivalent to solving ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\boldsymbol{\\xi}\\in\\mathcal{A}}\\ \\operatorname*{min}_{G\\in\\mathcal{G}}\\beta_{G}^{\\intercal}\\mathbf{e}_{G}+\\mathbf{p}_{G}^{\\intercal}\\mathbf{T}\\mathbf{q}_{G}-\\frac{1}{4}\\mathbf{p}_{G}^{\\intercal}\\mathbf{T}\\mathbf{p}_{G}+\\sum_{i=1}^{\\ell}\\left(\\lambda_{G,i}\\bar{\\mathbf{v}}_{G,i}^{\\intercal}\\mathbf{T}\\bar{\\mathbf{v}}_{G,i}-\\lambda_{G,i}r_{G,i}^{2}\\right)-\\mathbf{q}_{G}^{\\intercal}\\mathbf{T}\\mathbf{q}_{G}\\ ,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where for each group $\\textit{G}\\in\\textit{g}$ , ${\\bf p}_{G}\\ =\\ \\pmb{\\xi}_{G}\\ -\\ {\\cal Q}_{G}^{\\intercal}\\beta_{G}$ , $\\begin{array}{r}{{\\bf q}_{G}{\\bf\\phi}={\\bf\\phi}\\sum_{i=1}^{\\ell}\\lambda_{G,i}{\\bf S}_{G,i}^{-1}\\bar{\\bf v}_{G,i},}\\end{array}$ $\\mathrm{~\\bf~T~}=$ $\\begin{array}{r}{(\\sum_{i=1}^{\\ell},\\,\\lambda_{G,i}\\mathbf{S}_{G,i}^{-1})^{-1},}\\end{array}$ , and $\\Lambda$ is defined as in Equation (2). The optimal allocation $\\mathbf{a}^{*}$ can be computed from $\\xi^{*}$ as in Proposition 3.1. ", "page_idx": 21}, {"type": "text", "text": "Proof. Consider the following optimization problem. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A}}\\operatorname*{min}_{G\\in\\mathcal{G}}\\;\\operatorname*{min}_{\\mathbf{v}_{G}\\in\\mathcal{V}_{G}}\\;\\mathbf{a}_{G}^{\\intercal}\\mathbf{v}_{G}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{z\\in\\mathcal{V}_{G}}{\\mathrm{min}}\\ \\mathbf{a}_{G}^{\\dagger}\\mathbf{v}_{G}}\\\\ &{\\qquad\\forall i\\in[1,l]:\\forall G\\in\\mathcal{G}:(\\mathbf{v}_{G}-\\bar{\\mathbf{v}}_{i,G})\\mathbf{S}_{i,G}^{-1}(\\mathbf{v}_{G}-\\bar{\\mathbf{v}}_{i,G})\\leq r_{i,G}^{2}}\\\\ &{\\qquad\\forall G\\in\\mathcal{G}:Q_{G}\\mathbf{v}_{G}\\succeq\\mathbf{e}_{G}}\\\\ &{\\quad\\mathbf{v}_{G}\\succeq0\\ \\mathrm{.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It is important to note that the inner-most minimization is a convex optimization problem and the outer-maximization is a concave maximization problem. This is due to the fact that affine functions are either concave or convex and minimum of concave objectives is concave. ", "page_idx": 22}, {"type": "text", "text": "Notice that the inner-most minimization problem for each group is independent of other groups. Thus, we can simply replace each of these minimization problems with their Lagrangian dual counterparts. Furthermore, we note that these duals are computed following the approach outlined in the proof of Proposition 3.1 and are exact equivalents of their respective primal problems [12]. The resultant optimization problem is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{i\\in\\mathcal{A}}\\ \\operatorname*{min}_{\\deg\\alpha\\in\\mathcal{E}_{\\alpha,i}^{\\mathbb{N}}}-\\frac{1}{4}\\left(\\left(\\mathbf{a}_{G}-Q_{G}^{\\intercal}\\beta_{G}-\\zeta_{G}\\right)^{\\intercal}\\left(\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{G,i}\\mathbf{\\tilde{s}}_{G,i}^{-1}\\right)^{-1}\\left(\\mathbf{a}_{G}-Q_{G}^{\\intercal}\\beta_{G}-\\zeta_{G}\\right)\\right)+}\\\\ &{\\displaystyle\\overset{\\mathrm{for~}\\in\\mathbb{R}_{0,+}^{3}}{\\xi_{G}\\in\\mathbb{R}_{0,+}^{9}},}\\\\ &{\\displaystyle\\qquad\\qquad\\sum_{i=1}^{\\ell}\\lambda_{G,i}\\bar{\\mathbf{v}}_{G,i}^{\\intercal}\\bar{\\mathbf{v}}_{G,i}^{-1}\\bar{\\mathbf{v}}_{G,i}-\\left(\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{G,i}\\mathbf{S}_{G,i}^{-1}\\bar{\\mathbf{v}}_{G,i}\\right)^{\\intercal}\\left(\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{G,i}\\mathbf{S}_{G,i}^{-1}\\right)^{-1}\\left(\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{G,i}\\mathbf{S}_{G,i}^{-1}\\bar{\\mathbf{v}}_{G,i}\\right)}\\\\ &{\\displaystyle\\qquad\\qquad+\\left(\\mathbf{a}_{G}-Q_{G}^{\\intercal}\\beta_{G}-\\zeta_{G}\\right)^{\\intercal}\\left(\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{G,i}\\mathbf{S}_{G,i}^{-1}\\right)^{-1}\\left(\\displaystyle\\sum_{i=1}^{\\ell}\\lambda_{G,i}\\bar{\\mathbf{S}}_{G,i}^{-1}\\bar{\\mathbf{v}}_{G,i}\\right)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\displaystyle-\\sum_{i=1}^{\\ell}\\lambda_{G,i}\\bar{\\mathbf{r}}_{G,i}^{2}+\\beta_{G}^{\\intercal}\\mathbf{e}_{G}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Notice that the inner-most maximization problem is concave in $\\lambda,\\beta$ , and $\\zeta$ . We now apply the same procedure as in the proof of Proposition 3.1 to obtain concavity in allocation a. Using the change of variables $\\forall G\\in{\\mathcal{G}}:{\\overline{{\\xi}}}_{G}=\\mathbf{a}_{G}-{\\overline{{\\zeta}}}_{G}$ , we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A}}\\displaystyle\\operatorname*{min}_{G\\in\\mathcal{G}}\\displaystyle\\operatorname*{max}_{\\lambda_{G}\\in\\mathbb{R}^{l}}\\beta_{G}^{\\top}\\mathbf{e}_{G}+\\mathbf{p}_{G}^{\\top}\\mathbf{T}\\mathbf{q}_{G}-\\frac{1}{4}\\mathbf{p}_{G}^{\\top}\\mathbf{T}\\mathbf{p}_{G}+\\sum_{i=1}^{\\ell}\\left(\\lambda_{G,i}\\bar{\\mathbf{v}}_{G,i}^{\\top}\\mathbf{T}\\bar{\\mathbf{v}}_{G,i}-\\lambda_{G,i}r_{G,i}^{2}\\right)-\\mathbf{q}_{G}^{\\top}\\mathbf{T}\\mathbf{q}_{G}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where for any $\\begin{array}{r c l c l c l}{{G}}&{{\\in}}&{{{\\mathcal G},\\mathbf{p}_{G}}}&{{=}}&{{{\\mathbf Q}_{G}^{\\intercal}{\\beta}_{G}\\;+\\;\\xi_{G},\\;\\textbf{T}}}&{{=}}&{{\\left(\\sum_{i=1}^{\\ell}\\lambda_{G,i}\\mathbf{S}_{G,i}^{-1}\\right)^{-1}}}\\end{array}$ , and $\\begin{array}{r l}{\\mathbf{q}_{G}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\sum_{i=1}^{\\ell}\\lambda_{G,i}\\mathbf{S}_{G,i}^{-1}\\bar{\\mathbf{v}}_{G,i}.}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "Since the inner maximization for each group is independent of the other groups, we can re-order the inner minimization over groups and the inner-maximization problem. Thus, without loss of generality, we can write the above optimization problem as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\mathbf{\\Phi}_{\\xi}\\in\\mathbb{R}_{m}^{4}}\\operatorname*{min}_{G\\in\\mathcal{G}}\\beta_{G}^{\\intercal}\\mathbf{e}_{G}+\\mathbf{p}_{G}^{\\intercal}\\mathbf{T}\\mathbf{q}_{G}-\\frac{1}{4}\\mathbf{p}_{G}^{\\intercal}\\mathbf{T}\\mathbf{p}_{G}+\\sum_{i=1}^{\\ell}\\left(\\lambda_{G,i}\\bar{\\mathbf{v}}_{G,i}^{\\intercal}\\mathbf{T}\\bar{\\mathbf{v}}_{G,i}-\\lambda_{G,i}r_{G,i}^{2}\\right)-\\mathbf{q}_{G}^{\\intercal}\\mathbf{T}\\mathbf{q}_{G}}\\\\ &{\\displaystyle\\mathcal{L}\\bar{\\mathbf{e}}_{\\mathrm{p}_{+}}^{\\intercal,m}}\\\\ &{\\displaystyle\\lambda\\in\\mathbb{R}_{0}^{9\\times1}}\\\\ &{\\displaystyle\\beta\\bar{\\mathbf{e}}_{\\mathrm{p}_{+}}^{\\intercal\\bullet\\dagger}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using the same technique as in Appendix H.1, we can simplify the problem by eliminating the variables a and $\\zeta$ in the above problem and then derive them from the optimal $\\xi$ . ", "page_idx": 22}, {"type": "text", "text": "Eliminating $\\zeta$ and a in the above equation, we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{\\nu}^{*},\\beta^{*},\\xi^{*}=\\underset{\\underset{\\mathcal{A}\\in\\mathbb{R}_{+}^{M}}{\\mathbf{1}},\\mathbf{\\xi}_{G\\in\\mathbb{Q}}}{\\arg\\operatorname*{max}\\operatorname*{min}}\\beta_{G}^{\\top}\\mathbf{e}_{G}+\\mathbf{p}_{G}^{\\top}\\mathbf{T}\\mathbf{q}_{G}-\\frac{1}{4}\\mathbf{p}_{G}^{\\top}\\mathbf{T}\\mathbf{p}_{G}+\\displaystyle\\sum_{i=1}^{\\ell}\\left(\\lambda_{G,i}\\bar{\\mathbf{v}}_{G,i}^{\\top}\\mathbf{T}\\bar{\\mathbf{v}}_{G,i}-\\lambda_{G,i}r_{G,i}^{2}\\right)-\\mathbf{q}_{G}^{\\top}\\mathbf{T}\\mathbf{q}_{G}\\,\\,,}\\\\ &{\\qquad\\qquad\\quad\\beta\\in\\mathbb{R}_{+}^{{\\mathbf{1}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\xi\\in\\mathbb{K}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where for each group $\\textit{G}\\in\\textit{g}$ , $\\begin{array}{r c l}{\\mathbf{p}_{G}}&{=}&{(\\pmb{\\xi}_{G}\\ -\\ Q_{G}^{\\intercal}\\beta_{G})}\\end{array}$ , $\\begin{array}{r c l}{\\mathbf{q}_{G}}&{=}&{\\sum_{i=1}^{\\ell}\\lambda_{G,i}\\mathbf{S}_{G,i}^{-1}\\bar{\\mathbf{v}}_{G,i}.}\\end{array}$ , $\\mathrm{~\\bf~T~}=$ $\\begin{array}{r}{(\\sum_{i=1}^{\\ell},\\,\\lambda_{G,i}\\mathbf{S}_{G,i}^{-1})^{-1}}\\end{array}$ , and $\\begin{array}{r}{\\Lambda\\,=\\,\\pm\\,-\\,\\mathbb{R}_{0+}^{n m}\\,=\\,\\{\\pmb{\\xi}\\,\\in\\,\\mathbb{R}^{n m}\\,\\mid\\,\\forall a\\,\\in\\,N\\,:\\,\\sum_{i\\in I}\\pmb{\\xi}_{a m+i}\\,\\le\\,\\bar{\\kappa}_{a},\\forall i\\,\\in\\,I\\}\\,.}\\end{array}$ $\\begin{array}{r}{I:\\sum_{a\\in N}\\xi_{a m+i}\\le\\bar{\\psi}_{i},\\xi\\preceq c\\}.}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "As in Proposition 3.1, we can determine the set of optimal $(\\mathbf{a},\\boldsymbol{\\zeta})$ pairs by solving the system of equations: $\\{(\\mathbf{a}^{*},\\zeta^{*})\\mid\\mathbf{a}\\in\\mathcal{A},\\,G\\in\\mathcal{G}:\\mathbf{a}_{G}-\\zeta_{G}=\\xi_{G}^{*},\\zeta\\in\\mathbb{R}_{0+}^{n m}\\}$ . ", "page_idx": 23}, {"type": "text", "text": "H.5 Proof of Corollary 3.6 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Corollary 3.6 (Group Egalitarian Welfare with Polyhedral Uncertainty). In the case where the uncertainty set $\\mathcal{V}$ is defined only by linear constraints, i.e., $\\mathcal{V}=\\left\\{\\mathbf{v}\\in\\mathbb{R}^{n m}\\mid Q\\mathbf{v}\\succeq\\mathbf{e},\\mathbf{v}\\succeq0\\right\\}$ , the max-min-min problem in (5) transforms into a linear program. ", "page_idx": 23}, {"type": "text", "text": "Proof. Now consider the inner-minimization problem: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{a}\\in A}{\\operatorname*{max}}\\ \\underset{G\\in\\mathcal{G}}{\\operatorname*{min}}\\ \\underset{\\mathbf{v}_{G}\\in\\mathbb{R}_{0+}^{n_{m}}}{\\operatorname*{min}}\\ \\mathbf{a}_{G}{^{\\intercal}}\\mathbf{v}_{G}}\\\\ &{\\qquad\\qquad\\qquad\\forall G\\in\\mathcal{G}:\\ Q_{G}\\mathbf{v}_{G}\\succeq\\mathbf{e}_{G}\\enspace.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We can compute the Lagrangian dual of the inner-most minimization problem for each group independently by following steps outlined in the proof of Corollary 3.2. Note that since these minimization problems are simple linear programs, their corresponding duals are exact equivalents of their primal counterparts (Section 5.1 in Boyd et al. [12]). By substituting the duals in the above problem, we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A},\\beta\\in\\mathbb{R}^{g\\times k}}\\displaystyle\\operatorname*{min}_{G\\in\\mathcal{G}}}&{\\beta_{G}^{\\intercal}\\mathbf{e}_{G}}\\\\ {\\displaystyle Q_{G}^{\\intercal}\\beta_{G}\\preceq\\mathbf{a}_{G}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using simple algebraic manipulations, we can further simplify the above optimization problem as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\operatorname*{max}}\\\\ &{\\qquad{\\mathbf{a}}{\\in}\\mathcal{A},\\beta{\\in}\\mathbb{R}^{g\\times k},t{\\in}\\mathbb{R}}\\\\ &{\\qquad\\quad\\forall G\\in\\mathcal{G}:t\\leq\\beta_{G}^{\\intercal}\\mathbf{e}_{G}}\\\\ &{\\qquad\\quad\\forall G\\in\\mathcal{G}:\\!Q_{G}^{\\intercal}\\beta_{G}\\preceq\\mathbf{a}_{G}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "H.6 Proof of Corollary 3.7 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Corollary 3.7 (Group Egalitarian Welfare with Ellipsoidal Uncertainty). Suppose that the set $\\mathcal{V}$ in (5) is defined by a single truncated ellipsoidal constraint per group i.e., $\\mathcal{V}=\\left\\{\\mathbf{v}\\in\\mathbb{R}^{n m}\\ |\\ \\forall G\\in\\mathcal{G}:\\right.$ $(\\mathbf{v}_{G}-\\bar{\\mathbf{v}}_{G})\\mathbf{S}_{G}^{-1}(\\mathbf{v}_{G}-\\bar{\\mathbf{v}}_{G})\\leq r_{G}^{2},\\mathbf{v}\\succeq0\\right\\}$ . Then the problem in (5) is equivalent to solving ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pm\\in\\mathbb{R}_{0+}^{g}}\\operatorname*{min}_{G\\in\\mathcal{G}}\\xi_{G}^{\\intercal}\\bar{\\mathbf{v}}_{G}-\\frac{\\xi_{G}^{\\intercal}\\mathbf{S}_{G}\\pmb{\\xi}_{G}}{4\\lambda_{G}}-\\lambda_{G}r_{G}^{2}\\enspace.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The exact optimal solution $(\\lambda^{*},\\xi^{*})$ to Equation (4) can be computed by alternately performing two steps until convergence: first, fixing $\\xi$ and optimizing $\\lambda_{i}$ , i.e., $\\forall G\\,\\in\\,\\mathcal{G}$ , $\\lambda_{G}\\,=\\,\\dot{\\xi_{G}^{\\intercal}}\\mathbf{S}_{G}\\xi_{G}\\big/2r_{G}$ , and second, fixing $\\lambda$ and solving a concave quadratic program to optimize $\\xi$ . The optimal allocation $\\mathbf{a}^{*}$ can be computed from $\\xi^{*}$ as in Proposition 3.1. ", "page_idx": 23}, {"type": "text", "text": "Proof. Consider the following optimization problem. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{a}\\in\\mathcal{A}}{\\operatorname*{max}}\\ \\underset{G\\in\\mathcal{G}}{\\operatorname*{min}}\\ \\underset{\\mathbf{v}_{G}\\in\\mathbb{R}_{0+}^{|G|m}}{\\operatorname*{min}}\\,\\mathbf{a}_{G}^{\\intercal}\\mathbf{v}_{G}}\\\\ &{\\qquad\\qquad\\qquad\\quad\\forall G\\in\\mathcal{G}:\\ \\big(\\mathbf{v}_{G}-\\bar{\\mathbf{v}}_{G}\\big)\\mathbf{S}_{G}^{-1}\\big(\\mathbf{v}_{G}-\\bar{\\mathbf{v}}_{G}\\big)\\leq r_{G}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\quad\\mathbf{v}_{G}\\succeq0\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Similar to the general version of the problem in (17), the inner-most minimization is a convex optimization problem and the outer-maximization is a concave maximization problem. This is again ", "page_idx": 23}, {"type": "text", "text": "due to the fact that affine functions are either concave or convex and minimum of concave objectives is concave. ", "page_idx": 24}, {"type": "text", "text": "The inner-most minimization over the uncertainty set of valuation matrices is independent for each group. Therefore, by simply replacing each of these minimization problems with their respective Lagrangian duals, as computed in Corollary 3.3, we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{{\\bf a}\\in{\\cal A}}\\operatorname*{min}_{G\\in{\\cal S}}\\operatorname*{max}_{{\\bf{\\lambda}}_{G}\\in{\\bf R}_{0+}^{\\ell}}-\\frac{1}{4}\\left(({\\bf a}_{G}-\\zeta_{G})^{\\top}{\\bf\\lambda}{\\bf S}\\big({\\bf a}_{G}-\\zeta_{G}\\big)\\right)+({\\bf a}_{G}-\\zeta_{G})^{\\top}\\bar{\\bf v}_{G}-\\sum_{i=1}^{\\ell}\\lambda_{G,i}r_{G,i}^{2}\\ .\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that the dual is computed following the approach outlined in the proof of Proposition 3.1. ", "page_idx": 24}, {"type": "text", "text": "Using the change of variables $\\forall G\\in{\\mathcal{G}}:\\,\\pmb{\\xi}_{G}=\\mathbf{a}_{G}-\\zeta_{G}$ , we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{{\\bf{a}}\\in{\\cal{A}}}{\\operatorname*{max}}\\,\\operatorname*{min}_{G\\in{\\cal{G}}}\\underset{{\\bf{\\lambda}}_{G}\\in\\mathbb{R}_{0+}^{\\ell}}{\\operatorname*{max}}\\ {\\pmb{\\xi}}_{G}^{\\intercal}\\bar{\\bf{v}}_{G}-\\frac{{\\pmb{\\xi}}_{G}^{\\intercal}{\\bf{S}}_{G}{\\pmb{\\xi}}_{G}}{4{\\pmb{\\lambda}}_{G}}-{\\pmb{\\lambda}}_{G}r_{G}^{2}}\\\\ &{\\qquad\\qquad\\quad{\\pmb{\\zeta}}_{G}\\in\\mathbb{R}_{0+}^{n m}\\ {\\bf{s.t.}}\\ {\\pmb{\\xi}}_{G}={\\bf{a}}_{G}-{\\pmb{\\zeta}}_{G}\\ \\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since the inner maximization for each group is independent of the other groups, we can re-order the inner minimization over groups and the inner-maximization problem. Thus, without loss of generality, we can write the above optimization problem as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{{\\bf{a}}\\in{\\cal{A}}_{m}}{\\operatorname*{max}}\\,\\operatorname*{min}_{G\\in\\mathcal{G}}\\xi_{G}^{\\sf T}\\bar{\\bf v}_{G}-\\frac{{\\bf{\\xi}}\\displaystyle{\\bf{\\xi}}_{G}^{\\sf T}{\\bf S}_{G}\\xi_{G}}{4\\lambda_{G}}-\\lambda_{G}r_{G}^{2}}\\\\ &{\\xi_{\\sf{e}^{\\mathbb{R}^{n m}}}^{n_{n m}}}\\\\ &{\\lambda\\in{\\mathbb{R}_{0+}^{g\\times l}}}\\\\ &{\\xi_{\\sf{e}^{\\mathbb{R}^{n m}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Using the same technique as in Appendix H.1, we can simplify the problem by eliminating the variables $\\mathbf{a}$ and $\\zeta$ in the above problem and then derive them from the optimal $\\xi$ . ", "page_idx": 24}, {"type": "text", "text": "Eliminating $\\zeta$ and a in the above optimization problem, we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\lambda^{*},\\xi^{*}=\\underset{\\lambda\\in\\mathbb{R}_{0+}^{g\\times l}}{\\arg\\operatorname*{max}}\\underset{G\\in\\mathcal{G}}{\\operatorname*{min}}\\ \\xi_{G}^{\\top}\\bar{\\mathbf{v}}_{G}-\\frac{\\xi_{G}^{\\top}\\mathbf{S}_{G}\\pmb{\\xi}_{G}}{4\\lambda_{G}}-\\lambda_{G}r_{G}^{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\begin{array}{r}{\\Lambda={\\cal A}-\\mathbb{R}_{0+}^{n m}=\\{\\xi\\in\\mathbb{R}^{n m}~|~\\forall a\\in{\\cal N}:~\\sum_{i\\in I}\\xi_{a m+i}\\le\\bar{\\kappa}_{a},\\forall i\\in I:~\\sum_{a\\in{\\cal N}}\\xi_{a m+i}\\le\\bar{\\kappa}_{a},}\\end{array}$ $\\bar{\\psi}_{i},\\xi\\preceq c\\}$ . ", "page_idx": 24}, {"type": "text", "text": "As in proposition 3.1, we can now determine the set of optimal $(\\mathbf{a},\\boldsymbol{\\zeta})$ pairs by solving the system of equations: $\\{(\\mathbf{a}^{*},\\zeta^{*})\\mid\\mathbf{a}\\in\\mathcal{A},\\,G\\in\\mathcal{G}:\\mathbf{a}_{G}-\\zeta_{G}=\\xi_{G}^{*},\\bar{\\zeta}\\in\\mathbb{R}_{0+}^{n m}\\}$ . ", "page_idx": 24}, {"type": "text", "text": "H.7 Proof of Proposition 3.8 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proposition 3.8 (Decomposition for Monotonic Welfare Functions). Consider an optimization problem of the form ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A}}\\operatorname*{min}_{\\mathbf{v}\\in\\mathcal{V}}\\ \\mathrm{W}_{\\mathrm{M}}(\\mathbf{u}(\\mathbf{a},\\mathbf{v}))\\ \\mathrm{,}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the welfare function $\\mathrm{W}_{\\mathrm{M}}$ is monotonic in the utility of groups. If Assumption 3.4 holds, then (8) simplifies to ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{a}\\in A}\\;\\mathrm{W}_{\\mathrm{M}}\\left(\\operatorname*{min}_{\\mathbf{v}_{G_{1}}\\in\\mathcal{V}_{G_{1}}}\\mathrm{u}_{G_{1}}(\\mathbf{a}_{G_{1}},\\mathbf{v}_{G_{1}}),\\operatorname*{min}_{\\mathbf{v}_{G_{2}}\\in\\mathcal{V}_{G_{2}}}\\mathrm{u}_{G_{2}}(\\mathbf{a}_{G_{2}},\\mathbf{v}_{G_{2}}),\\dots,\\operatorname*{min}_{\\mathbf{v}_{G_{g}}\\in\\mathcal{V}_{G_{g}}}\\mathrm{u}_{G_{g}}(\\mathbf{a}_{G_{g}},\\mathbf{v}_{G_{g}})\\right)\\;\\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. The result directly follows from the monotonic property of the welfare function and the independence of the uncertainty sets across groups. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "H.8 Proof of Proposition 4.1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proposition 4.1 (Approximate CVaR of USW). Given h i.i.d samples of $\\widetilde{{\\mathbf v}}$ , i.e., $\\mathbf{v}^{1},\\mathbf{v}^{2},\\mathbf{v}^{3},\\ldots,\\mathbf{v}^{h}$ from $\\mathcal{D}_{\\tilde{\\mathbf{v}}}$ , the optimal allocation for the problem in (9) can be approximately computed by solving ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{{\\mathbf{a}}\\in A}\\operatorname*{max}_{y\\in\\mathbb{R}_{0}^{h}}\\left(b-\\frac{1}{\\alpha}\\sum_{j=1}^{h}y_{j}\\right)\\qquad\\forall j\\in[1,h]:\\;y_{j}\\geq\\frac{1}{h}\\left(b-{\\mathbf{w}}\\cdot{\\mathbf{u}}({\\mathbf{a}},{\\mathbf{v}}^{j})\\right)\\ \\ .\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Consider the CVaR of utilitarian welfare optimization problem, given by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A}}\\mathrm{CVaR}_{\\alpha}\\left[\\mathbf{w}\\cdot\\mathbf{u}(\\mathbf{a},\\tilde{\\mathbf{v}})\\right]\\doteq\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A},b\\in\\mathbb{R}}\\left\\{b-\\frac{1}{\\alpha}\\underbrace{\\mathbb{E}}_{\\tilde{\\mathbf{v}}\\sim\\mathcal{D}_{\\tilde{\\mathbf{v}}}}\\left[\\left(b-\\mathbf{w}\\cdot\\mathbf{u}(\\mathbf{a},\\tilde{\\mathbf{v}})\\right)_{+}\\right]\\right\\}\\ \\mathrm{,}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Substituting the expectation with the empirical expectation computed from the $h$ samples of random valuation $\\tilde{v}$ , and using a slack variable $\\pmb{y}\\in\\dot{\\mathbb{R}}_{0+}^{h}$ to convert $\\left(b-\\mathbf{w}\\cdot\\mathbf{u}(\\mathbf{a},\\tilde{\\mathbf{v}})\\right)_{+}$ term in the expectation to linear constraints, we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\mathbf{a},A}\\operatorname*{max}_{\\boldsymbol{y}\\in\\mathbb{R}^{h},b\\in\\mathbb{R}}\\left(b-\\frac{1}{\\alpha}\\sum_{j=1}^{h}y_{j}\\right)}\\\\ &{\\displaystyle\\forall j\\in[1,h]:y_{j}\\geq0}\\\\ &{\\displaystyle\\forall j\\in[1,h]:y_{j}\\geq\\frac{1}{h}\\left(b-\\sum_{G\\in\\mathcal{G}}\\frac{\\mathbf{w}_{G}}{|G|}\\cdot\\mathbf{a}_{G}^{\\top}\\mathbf{v}_{G}^{j}\\right)}\\\\ &{\\displaystyle\\mathbf{a}\\in\\mathcal{A}~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Assumption H.1 (L.A. et al. [35]). The random variable $\\tilde{x}$ is continuous with probability density function $f$ that satisfies the following condition: There exists $\\eta,\\gamma>0$ such that $\\forall y\\in[v_{\\alpha}-\\gamma,v_{\\alpha}+$ $\\gamma]:\\;f(y)>\\eta$ , where $v_{\\alpha}=F^{-1}(\\alpha)$ . ", "page_idx": 25}, {"type": "text", "text": "Theorem H.2 (Theorem 3.1 in [35]). Let $(\\tilde{x}_{i})_{i=1}^{h}$ be a sequence of i.i.d random variables. Suppose that $\\tilde{x}_{i},i=1,\\ldots,n$ are $\\sigma{-s u b}$ -Gaussian and Assumption $H.I$ holds. If $c_{\\alpha}$ and $\\hat{c}_{h,\\alpha}$ represent the true CVaR and the empirical CVaR of random variable $\\tilde{x}$ estimated from $h$ samples at confidence level $\\alpha$ respectively, then for any $\\varepsilon>0$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\vert\\hat{c}_{h,\\alpha}-c_{\\alpha}\\vert>\\varepsilon\\right]\\le6\\exp\\left(\\frac{-h\\alpha^{2}\\operatorname*{min}(\\varepsilon^{2},16\\gamma^{2})\\operatorname*{min}(\\eta^{2},1)}{8\\operatorname*{max}(8,\\sigma^{2})}\\right)\\enspace.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "H.9 Proof of Proposition 4.2 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proposition 4.2 (Sample Complexity of Approximate CVaR of USW). Suppose that $\\tilde{\\textbf{v}}$ is a multivariate sub-Gaussian random variable with mean $\\bar{\\mathbf{v}}\\,\\in\\,\\mathbb{R}^{n m}$ and covariance proxy $\\mathbf{S}\\in\\mathbb{R}^{n m\\times n m}$ , i.e., for all vectors $\\pmb{z}\\in\\mathbb{R}^{n m}:\\mathbb{E}_{\\mathbf{v}\\sim\\mathcal{D}_{\\tilde{\\mathbf{v}}}}$ $\\left[\\exp((\\mathbf{v}-\\bar{\\mathbf{v}})\\mathsf{T}z))\\right]\\,\\leq\\,\\exp(z^{\\mathsf{T}}\\mathbf{S}z/2)$ , and that, for any risk level $\\alpha\\in(0,\\frac{1}{2})$ and allocation $\\mathbf{a}\\in{\\mathcal{A}}$ , there exists probability density threshold $\\eta>0$ and radius $\\gamma>0$ , s.t., $f_{\\mathbf{a}}{\\bar{(}}x)>\\eta\\,,\\forall x\\in[\\nu_{\\alpha}-\\gamma,\\nu_{\\alpha}+\\gamma]$ . Set $\\begin{array}{r}{\\forall G\\in\\mathcal{G}:\\mathbf{a}_{G}^{\\prime}=\\frac{\\mathbf{w}_{G}\\cdot\\mathbf{a}_{G}}{|G|}}\\end{array}$ . Then, for any confidence parameter $\\delta\\in(0,1)$ and error tolerance $\\varepsilon>0$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{y_{\\mathrm{\\scriptsize~\\left[\\,\\mathrm{sup}\\,}\\right]}}_{\\mathrm{a\\'e},A}[\\hat{c}_{h,\\alpha}({\\mathbf a})-c_{\\alpha}({\\mathbf a})|\\leq\\varepsilon]\\geq1-\\delta\\quad f o r\\quad h>\\left[\\frac{8\\operatorname*{max}_{\\mathbf{a}\\in A}{\\mathbf a}^{\\prime\\intercal}\\mathbf{S}{\\mathbf a}^{\\prime},\\mathrm{8}\\right)\\ln\\left(\\frac{6|A|}{\\delta}\\right)}{\\operatorname*{min}(\\varepsilon^{2},16\\gamma^{2})\\alpha^{2}\\operatorname*{min}(\\eta^{2},1)}\\right]\\enspace.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. From the assumption, we know that the valuation vector $\\mathbf{v}$ is a sub-Gaussian that satisfies the following condition ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\forall z\\in\\mathbb{R}^{n m}:\\mathbb{E}\\left[\\exp((\\mathbf{v}-\\bar{\\mathbf{v}})^{\\mathsf{T}}z))\\right]\\leq\\exp(z^{\\mathsf{\\tau}_{0}}z/2)\\;\\;.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using the above properties of sub-Gaussian, we can establish that the utilitarian welfare for a given allocation a is also a sub-Gaussian with variance-proxy $=\\sum_{G\\in\\mathcal{G}}\\mathbf{a}_{G}^{\\prime}\\,^{\\sf T}\\mathbf{S}\\mathbf{a}_{G}^{\\prime}$ where $\\forall G\\in\\mathcal{G}:\\,\\bar{\\mathbf{a}}_{G}^{\\prime}=$ $\\frac{\\mathbf{w}_{G}\\!\\cdot\\!\\mathbf{a}}{|G|}$ . ", "page_idx": 26}, {"type": "text", "text": "For any allocation a, let $\\hat{c}_{h,\\alpha}({\\mathbf{a}})$ represent the empirical estimate of CVaR of utilitarian welfare and $c_{h,\\alpha}({\\mathbf{a}})$ represent the corresponding true value. ", "page_idx": 26}, {"type": "text", "text": "Then, using Theorem H.2, we can bound the error of approximating the CVaR of the utilitarian welfare for allocation a as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\left|\\hat{c}_{h,\\alpha}({\\bf a})-c_{\\alpha}({\\bf a})\\right|>\\varepsilon\\right]\\le6\\exp\\left(\\frac{-h\\alpha^{2}\\operatorname*{min}(\\varepsilon^{2},16\\gamma^{2})\\operatorname*{min}(\\eta^{2},1)}{8\\operatorname*{max}(8,\\sum_{G\\in\\mathcal{G}}{\\bf a}_{G}^{\\prime}{}^{\\top}{\\bf S}{\\bf a}_{G}^{\\prime})}\\right)\\ \\ .\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The approximation error for all allocations can be upper-bounded as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{Pr}\\left[\\operatorname*{sup}_{\\mathbf{a}\\in A}\\,|\\hat{c}_{h,\\boldsymbol{\\alpha}}(\\mathbf{a})-c_{\\boldsymbol{\\alpha}}(\\mathbf{a})|>\\varepsilon\\right]\\leq\\displaystyle\\sum_{\\mathbf{a}\\in A}\\operatorname*{Pr}\\left[|\\hat{c}_{h,\\boldsymbol{\\alpha}}(\\mathbf{a})-c_{\\boldsymbol{\\alpha}}(\\mathbf{a})|>\\varepsilon\\right]}&{}\\\\ {\\displaystyle\\leq|A|6\\exp\\left(\\frac{-h\\alpha^{2}\\operatorname*{min}(\\varepsilon^{2},4\\gamma^{2})\\operatorname*{min}(\\eta^{2},1)}{8\\operatorname*{max}(8,\\sum_{G\\in\\mathcal{G}}\\mathbf{a}_{G}^{\\prime}\\mathbf{\\bar{S}}\\mathbf{a}_{G}^{\\prime})}\\right)~~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To obtain confidence guarantee $\\operatorname*{Pr}[\\forall\\mathbf{a}\\in A:\\,|\\hat{c}_{h,\\alpha}(\\mathbf{a})-c_{\\alpha}(\\mathbf{a})|\\leq\\varepsilon]\\geq1-\\delta$ , we set the R.H.S of (18) to be less than $\\delta$ , i.e., ", "page_idx": 26}, {"type": "equation", "text": "$$\n|{\\cal A}|6\\exp\\left(\\frac{-h\\alpha^{2}\\operatorname*{min}(\\varepsilon^{2},4\\gamma^{2})\\operatorname*{min}(\\eta^{2},1)}{8\\operatorname*{max}(8,\\sum_{G\\in\\mathcal{G}}{\\bf a}_{G}^{\\prime}{}^{\\!\\top}{\\bf S}{\\bf a}_{G}^{\\prime}}\\right)<\\delta\\,\\,\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Solving for $h$ , we get ", "page_idx": 26}, {"type": "equation", "text": "$$\nh>\\left[\\left(\\frac{8\\operatorname*{max}(\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A}}\\sum_{G\\in\\mathcal{G}}{\\mathbf{a}_{G}^{\\prime}}^{\\mathsf{T}}\\mathbf{S}\\mathbf{a}_{G}^{\\prime},8)\\ln\\left(\\frac{6|A|}{\\delta}\\right)}{\\operatorname*{min}(\\varepsilon^{2},4\\gamma^{2})\\alpha^{2}\\operatorname*{min}(\\eta^{2},1)}\\right)\\right]\\enspace.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "H.10 Proof of Proposition 4.3 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proposition 4.3 (CVaR of USW for Gaussian Distributions). If $\\widetilde{{\\mathbf v}}$ is distributed as a multivariate Gaussian, i.e., $\\tilde{\\mathbf{v}}\\sim\\mathcal{N}(\\bar{\\mathbf{v}},\\mathbf{S})$ , then, the optimization problem in (9) simplifies to ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A}}\\mathbf{a}^{\\intercal}\\bar{\\mathbf{v}}-\\frac{\\phi(\\Phi^{-1}(\\alpha))}{\\alpha}\\sqrt{\\mathbf{a}^{\\intercal}\\mathbf{S}\\mathbf{a}}\\ \\,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. The proof simply follows from the fact that for any Gaussian distributed random variable x\u02dc \u223cN(\u00b5, \u03c32) with mean \u00b5 \u2208R and standard deviation \u03c3 \u2208R0+, CVaR[x\u02dc] = \u00b5 \u2212\u03d5(\u03a6\u2212\u03b11(\u03b1))\u03c3. The mean of the utilitarian welfare under the assumption that $\\mathbf{v}$ is Gaussian-distributed is given by $\\mathbf{a}^{\\prime\\intercal}\\mathbf{v}$ and variance is given by $\\mathbf{a}^{\\prime\\intercal}\\mathbf{S}\\mathbf{a}^{\\prime}$ , where for any $\\begin{array}{r}{G\\,\\in\\,\\mathcal{G},\\mathbf{a}_{G}^{\\prime}\\,=\\,\\frac{\\mathbf{w}_{G}\\cdot\\mathbf{a}}{|G|}}\\end{array}$ . Substituting these values in the previously mentioned expression of CVaR of a Gaussian random variable, we obtain the results stated above. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "H.11 Linear Program for CVaR of Egalitarian Welfare ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proposition H.3 (Approximate CVaR of GESW). Given $h$ samples of $\\tilde{\\mathbf{v}}_{;}$ , i.e., $\\mathbf{v}^{1},\\mathbf{v}^{2},\\mathbf{v}^{3},\\ldots\\mathbf{v}^{h}$ sampled from $\\mathcal{D}_{\\tilde{\\mathbf{v}}}$ , the optimal allocation for the problem in (12) can be approximately computed by solving ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\mathbf{a}\\in A}\\operatorname*{max}_{\\mathbf{\\Phi}_{y}\\in\\mathbb{R}_{0+}^{h},b\\in\\mathbb{R}}\\left(b-\\frac{1}{\\alpha}\\sum_{j=1}^{h}\\mathbf{\\boldsymbol{y}}_{j}\\right)}\\\\ &{\\forall j\\in[1,h]:\\forall G\\in\\mathcal{G}:\\ \\mathbf{\\Phi}\\mathbf{\\cdot}\\mathbf{\\Phi}\\mathbf{\\mathcal{y}}_{j}\\geq\\displaystyle\\frac{1}{h}\\left(b-\\frac{1}{\\vert G\\vert}\\cdot\\mathbf{a}_{G}^{\\intercal}\\mathbf{\\boldsymbol{v}}_{G}^{j}\\right)\\ \\mathrm{~.~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Consider the CVaR of egalitarian welfare optimization problem, given by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{a}\\in A}\\mathrm{CVaR}_{\\alpha}\\left[\\operatorname*{min}_{G\\in\\mathcal{G}}\\frac{1}{|G|}\\cdot\\mathbf{a}_{G}^{\\top}\\tilde{\\mathbf{v}}_{G}\\right]\\quad\\doteq\\operatorname*{max}_{b\\in\\mathbb{R},\\mathbf{a}\\in\\mathcal{A}}\\left\\{b-\\frac{1}{\\alpha}\\underbrace{\\mathbb{E}}_{\\tilde{\\mathbf{v}}\\sim\\mathcal{D}_{\\tilde{v}}}\\left[\\left(b-\\operatorname*{min}_{G\\in\\mathcal{G}}\\frac{1}{|G|}\\cdot\\mathbf{a}_{G}^{\\top}\\tilde{\\mathbf{v}}_{G}\\right)_{+}\\right]\\right\\}\\quad.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Substituting the expectation in the above problem with the empirical expectation computed from the $h$ samples of the valuation matrices, we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\approx\\operatorname*{max}_{b\\in\\mathbb{R},\\mathbf{a}\\in A}\\left\\{b-\\frac{1}{\\alpha h}\\sum_{i=1}^{h}\\left(b-\\operatorname*{min}_{G\\in\\mathcal{G}}\\frac{1}{\\left|G\\right|}\\cdot\\mathbf{a}_{G}^{\\intercal}\\mathbf{v}_{G}^{i}\\right)_{+}\\right\\}\\;.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Introducing slack variables $\\pmb{y}\\in\\mathbb{R}^{h}$ , we can write the above problem as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{max}_{\\mathbf{a\\in}\\mathcal{A}\\ \\mathbf{y}\\in\\mathbb{R}^{h},b\\in\\mathbb{R}}\\left(b-\\frac{1}{\\alpha}\\sum_{j=1}^{h}\\pmb{y}_{j}\\right)}\\\\ {\\forall j\\in[1,h]:\\ \\pmb{y}_{j}\\geq0}\\\\ {\\forall j\\in[1,h]:\\ \\pmb{y}_{j}\\geq\\frac{1}{h}\\left(b-\\displaystyle\\operatorname*{min}_{G\\in\\mathcal{G}}\\frac{1}{|G|}\\cdot\\mathbf{a}_{G}^{\\top}\\mathbf{v}_{G}^{j}\\right)\\ \\mathrm{~.~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Without loss of generality, we can represent the above problem as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{max}_{\\mathbf{a}\\in\\mathcal{A}}\\displaystyle\\operatorname*{max}_{y\\in\\mathbb{R}^{h},b\\in\\mathbb{R}}\\left(b-\\displaystyle\\frac{1}{\\alpha}\\sum_{j=1}^{h}y_{j}\\right)}\\\\ {\\forall j\\in[1,h]:y_{j}\\geq0}\\\\ {\\forall j\\in[1,h]:\\forall G\\in\\mathcal{G}:\\ y_{j}\\geq\\displaystyle\\frac{1}{h}\\left(b-\\displaystyle\\frac{1}{\\vert G\\vert}\\cdot\\mathbf{a}_{G}^{\\intercal}\\mathbf{v}_{G}^{j}\\right)\\ \\mathrm{~.~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "I GESW vs. USW under Different Scenarios ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We run an experiment to discover scenarios where the USW-optimal solution has very sub-optimal GESW. Using the AAMAS 2015 dataset, we set all of the papers to be group 1, and we create a second, synthetic group of papers by copying and modifying a random subset of the papers. For these papers, we divide the copied valuations by some number, and set to zero all but the top valuations per paper. For each setting we compute the percentage of relative loss in GESW incurred by the maximum USW solution. Figure 4 shows the effects of varying the percentage of papers in the minority group, the number of non-zero entries, and the divisor. We use default values of 2 for the divisor, 5 for the number of nonzero entries, and 150 for the size of the minority group (corresponds to a ratio of roughly $20\\%$ ). Taken together, the results suggest that if one group of papers has overall lower bids than another group, this has a very strong negative effect on the GESW of the USW-optimal solution. ", "page_idx": 27}, {"type": "text", "text": "J CVaR Performance under Skewed Gaussians ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We use the AAMAS 2015 dataset and sample each valuation independently from a skewed-Gaussian distribution with varying skew parameter. We use the means and variances estimated by the Gaussian Process matrix factorization model described in Appendix E. The univariate skewed-Gaussian distribution with mean $\\mu$ , variance $\\sigma^{2}$ , and skew $\\alpha$ is defined by the probability density function ", "page_idx": 27}, {"type": "equation", "text": "$$\np(\\tilde{\\mathrm{x}};\\alpha)=2\\phi(\\tilde{\\mathrm{x}}-\\mu)\\Phi(\\alpha(\\tilde{\\mathrm{x}}-\\mu)/\\sigma),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\phi$ and $\\Phi$ are the probability density function and cumulative density function of a standard Gaussian distribution [6]. ", "page_idx": 27}, {"type": "image", "img_path": "6KThdqFgmA/tmp/91eebe012cac559b12c3c42f21acaee208d502177ae8edd6f5da6f2bc3a5fe18.jpg", "img_caption": ["Figure 4: Relative loss (in GESW) of the maximum USW solution, compared to the optimal GESW solution. Results are reported for a synthetic 2-group example, varying 1) the divisor applied to artificially scale the minority group\u2019s valuations, 2) the ratio of the minority group to the overall number of papers, and 3) the number of valuations per paper that are artifically set to 0. "], "img_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "6KThdqFgmA/tmp/fcf0ab8f5a1274afcf6cca444d046f30606376e4b5b9bb7dae2bcdaadec79759.jpg", "table_caption": ["Table 8: Performance of different allocations for CVaR on the AAMAS 2015 with skewed Gaussians. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 9: Performance of different allocations for CVaR of USW on toy example with skewed Gaussians. ", "page_idx": 29}, {"type": "table", "img_path": "6KThdqFgmA/tmp/a00d29b46cf6df7d73aca7336f5f20409333378ed8a6436a8acc4ae56f39cf68.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "6KThdqFgmA/tmp/20ef1e1baa0883fb88f767308bdc55852eecf16e5d3818fa66b9f36c985ef3df.jpg", "table_caption": ["Table 10: Performance of different allocations for CVaR of GESW on toy example with skewed Gaussians. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "We compare against the uncertainty-unaware and robust USW solution (using the ellipsoid derived from the Gaussian distribution, which is an optimistic uncertainty set). We optimize and evaluate for $\\mathrm{CVaR_{0.3}}$ . Table 8 displays the results. As the skew parameter $\\alpha$ gets more negative, the $\\mathrm{CVaR_{0.3}}$ of welfare of the na\u00a8\u0131ve and robust solutions get much worse compared to the CVaR-optimized solution. ", "page_idx": 29}, {"type": "text", "text": "We also consider a constructed scenario with two agents and four items, where each agent must be assigned one item. Each agent\u2019s valuation for an item is independent and follows a skewed Gaussian distribution. The mean valuations for the items are represented by the matrix ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left[0.39\\quad0.49\\quad0.51\\quad0.53\\right].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The item values have standard deviations $\\left[0.01\\quad0.04\\quad0.05\\quad0.09\\right]$ respectively for both agents, and the skew parameter is $\\alpha\\,=\\,5$ for all item-agent pairs. We sample 20, 000 samples from the skewed Gaussian distibutions. We compute the na\u00a8\u0131ve USW- or GESW-optimal allocations using the mean valuations, and we enumerate all possible allocations to identify the CVaR and robust allocations at a confidence level of 0.04. To estimate the robust statistic at a 0.04 confidence level, we assume independence and reject any samples that fall outside the $0.04/8$ confidence interval upper/lower bounds for any individual entry. We then compute the minimum welfare on the remaining samples. The computed solutions and the $\\mathrm{CVaR}_{0.04}$ of each are displayed in Tables 9 and 10. ", "page_idx": 29}, {"type": "text", "text": "In both the USW and GESW cases, the na\u00a8\u0131ve approach selects items 3 and 4, as they have the highest mean values, but it does not account for the uncertainty in the preferences. The robust approach, being more conservative, chooses items 1 and 2 due to their lower uncertainty. The CVaR approach strikes a balance between these two methods, selecting items 1 and 3. Item 1 has a lower mean value with low uncertainty, while item 3 has a higher mean value but with slightly higher uncertainty than item 2. ", "page_idx": 29}, {"type": "text", "text": "K Machine Specification ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "All experiments were run on Xeon E5-2680 v4 $@$ 2.40GHz machines with 128GB RAM with each experiment consuming at most $32\\ \\mathrm{GB}$ of memory. We ran 1500 experiments in total and each experiment took 3-4 hours. ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The abstract describes the CVaR and robust optimization approaches taken in the paper, the utilitarian and egalitarian objectives studied, and mentions our theoretical and empirical results. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: See Appendix C. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All our formal statements appear with proof in the appendix. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Description of experiment provided in Section 5 and Appendix D. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Link provided to code repository on Github in Section 5, link and recommended citation provided for data. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: See Section 5 and appendix D, and code. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] , ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: All results in the tables Section 5 and Appendix G are averaged over 5 runs of sub-sampling $20\\%$ of each dataset. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] , ", "page_idx": 33}, {"type": "text", "text": "Justification: The details of the compute resources used for the experiments is reported in Appendix K. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We reviewed and abide by the code of ethics. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] , ", "page_idx": 33}, {"type": "text", "text": "Justification: See Appendix B for details. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 34}, {"type": "text", "text": "Justification: We have used publicly available datasets in our experiments. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have only used Gurboi Solver for solving our optimization problems and have properly cited it. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We linked to code in Section 5. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No human subjects. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No human subjects. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}]