[{"figure_path": "NVl4SAmz5c/figures/figures_2_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/\u03bb. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure shows the training loss and sharpness trajectories for two different parameterizations (\u03bcP and SP) of Fully Connected Networks (FCNs) trained on a subset of the CIFAR-10 dataset using gradient descent (GD) with a mean squared error (MSE) loss function.  The sharpness is the maximum eigenvalue of the Hessian of the loss function. The dashed lines in the sharpness plots indicate the instability threshold (\u03b7 > \u03b7c), where \u03b7 is the learning rate.  When the sharpness exceeds this threshold, training becomes unstable. The top row shows the results for the \u03bcP parameterization, while the bottom row shows the results for the SP parameterization.  Each parameterization uses a different target learning rate (\u03b7trgt). The figure illustrates how the learning rate warmup impacts the sharpness and loss curves during training, with the goal of preventing the network from entering unstable regions of the loss landscape.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_5_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the \u03bb/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure displays the training loss and sharpness trajectories of two different fully connected neural networks (FCNs) trained on a subset of the CIFAR-10 dataset using gradient descent (GD) with different initializations (\u03bcP and SP) and learning rate schedules.  It shows how the learning rate affects the sharpness of the loss landscape and how training instability occurs when the learning rate exceeds a critical threshold.  The top row shows results for the \u03bcP initialization, while the bottom row displays results for the SP initialization. The dashed lines in the sharpness plots indicate the threshold.  Similar trends in network behavior are observed across various network architectures, loss functions, and batch sizes.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_6_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure shows the training loss and sharpness trajectories for two different parameterizations (\u03bcP and SP) of Fully Connected Networks (FCNs) trained on a subset of the CIFAR-10 dataset using gradient descent (GD).  The top row displays results for the \u03bcP parameterization, and the bottom row shows results for the SP parameterization.  The plots illustrate the relationship between the learning rate, sharpness, and training stability. The dashed lines in the sharpness plots represent instability thresholds; when the sharpness exceeds these thresholds, training becomes unstable.  The figure shows different regimes of sharpness evolution during the warmup period that depend on initialization and parameterization. ", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_7_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/\u03bb. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure displays the training loss and sharpness trajectories for two different parameterizations (\u00b5P and SP) of Fully Connected Networks (FCNs) trained on a small subset of the CIFAR-10 dataset using Gradient Descent (GD).  The plots show how the training loss and sharpness evolve over training steps for various warmup durations.  The dashed lines in the sharpness plots represent thresholds above which training becomes unstable (\u03b7 > \u03b7c).  The top row shows \u00b5P parameterization with a target learning rate (\u03b7trgt) set at 1/8, while the bottom row displays the SP parameterization with \u03b7trgt set to 32/\u03bb, where \u03bb is the sharpness. The results suggest that similar mechanisms are at play across different network architectures and training settings.  Further details can be found in Appendix E.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_8_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/nt curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure shows the training loss and sharpness trajectories for two different parameterizations (\u03bcP and SP) of Fully Connected Networks (FCNs) trained on a subset of the CIFAR-10 dataset using gradient descent (GD) with different warmup durations. The sharpness plots illustrate the relationship between sharpness, learning rate, and training stability, showing when training surpasses a critical threshold.  The results highlight the impact of parameterization and learning rate schedules on training stability and the effectiveness of warmup in mitigating instabilities. ", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_9_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure displays the training loss and sharpness trajectories for two different parameterizations (\u03bcP and SP) of Fully Connected Networks (FCNs) trained on a subset of the CIFAR-10 dataset using gradient descent (GD).  The sharpness plots show the maximum eigenvalue of the Hessian of the loss function. Dashed lines indicate the instability threshold (\u03b7 > \u03b7c), where \u03b7 is the learning rate.  The figure illustrates how different parameterizations and learning rate warmup strategies impact the network's stability and training dynamics.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_15_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure displays the training loss and sharpness trajectories for two different parameterizations (\u00b5P and SP) of Fully Connected Networks (FCNs) trained on a subset of the CIFAR-10 dataset using gradient descent (GD) with mean squared error (MSE) loss.  The top row shows the \u00b5P parameterization, while the bottom row shows the SP parameterization.  The plots illustrate the relationship between learning rate, training loss, and sharpness (the maximum eigenvalue of the Hessian of the loss).  The dashed lines indicate the instability threshold (\u03b7 > \u03b7c), where the learning rate (\u03b7) is too high, causing training instabilities.  The figure highlights how different parameterizations and learning rate schedules affect training dynamics and stability.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_20_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure shows the training loss and sharpness trajectories for two different parameterizations (\u00b5P and SP) of Fully Connected Networks (FCNs) trained on a subset of CIFAR-10 using Gradient Descent (GD) with different warmup durations. The sharpness is defined as the maximum eigenvalue of the Hessian of the loss function. The dashed lines in the sharpness plots indicate the instability thresholds (\u03b7 > \u03b7c).  The figure demonstrates that warmup allows for training with larger target learning rates by facilitating a reduction in sharpness and moving the model away from unfavorable regions of the loss landscape.  Similar trends are observed for different architectures and minibatch sizes. ", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_21_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/nt curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure shows the training loss and sharpness trajectories for two different parameterizations (\u03bcP and SP) of Fully Connected Networks (FCNs) trained on a subset of CIFAR-10 dataset using Gradient Descent (GD) with different warmup durations.  It demonstrates the relationship between learning rate, sharpness, and training stability, highlighting how warmup helps networks tolerate higher learning rates by forcing them into better-conditioned areas of the loss landscape. The dashed lines in the sharpness plots represent the instability threshold (2/\u03b7). When sharpness exceeds this threshold, training becomes unstable.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_22_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the \u03bb/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/\u03bb. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure shows the training loss and sharpness trajectories for two different parameterizations (\u03bcP and SP) of a fully connected neural network (FCN) trained on a subset of the CIFAR-10 dataset using gradient descent (GD) with a mean squared error (MSE) loss function.  The sharpness is plotted, showing the maximum eigenvalue of the Hessian matrix of the loss function. Dashed lines indicate the instability threshold (\u03b7 > \u03b7c), where \u03b7 is the learning rate.  The plots demonstrate how different parameterizations lead to variations in training dynamics and sharpness evolution, highlighting the concept of self-stabilization in training.", "section": "3 Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_22_2.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/\u03bb. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure shows the training loss and sharpness trajectories for two different parameterizations (\u03bcP and SP) of a Fully Connected Network (FCN) trained on a subset of the CIFAR-10 dataset using gradient descent (GD).  The sharpness, representing the maximum eigenvalue of the Hessian of the loss function, is plotted against the training step. Dashed lines indicate instability thresholds. The figure demonstrates how warmup influences the network's ability to tolerate higher target learning rates by affecting the sharpness. Different initializations and training phases (sharpness increase or decrease) lead to diverse training dynamics.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_23_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/nt curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "The figure shows the training loss and sharpness trajectories of two fully connected networks (FCNs) trained on a subset of the CIFAR-10 dataset using gradient descent (GD) with different warmup durations. The top row shows the results for a network with maximal update parameterization (\u03bcP), and the bottom row shows the results for a network with standard parameterization (SP).  The dashed lines in the sharpness plots indicate the instability threshold.  The figure illustrates how different warmup schedules affect the ability of the network to handle larger learning rates and demonstrates the self-stabilization mechanism.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_23_2.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure shows the training loss and sharpness trajectories for two different parameterizations (\u00b5P and SP) of Fully Connected Networks (FCNs) trained on a subset of CIFAR-10 dataset using Gradient Descent (GD) with different warmup durations.  The top row shows the results for the \u00b5P parameterization, while the bottom row shows the results for the SP parameterization.  The plots illustrate how the learning rate warmup affects the training stability and sharpness of the model, with longer warmup periods leading to greater stability and potentially better final performance. The dashed lines in the sharpness plots represent the theoretical instability threshold.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_24_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/nt curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure shows the training loss and sharpness trajectories for two different network parameterizations (\u03bcP and SP) trained on a subset of CIFAR-10 using gradient descent (GD) with varying warmup durations. The top row shows the results for the \u03bcP parameterization, while the bottom row shows the results for the SP parameterization. The dashed lines in the sharpness plots represent the instability thresholds. When the sharpness is above these thresholds, training becomes unstable. This instability is mitigated by using a warmup period, which allows the network to gradually adjust to the larger learning rate.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_24_2.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the \u03bb/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/\u03bb. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure displays the training loss and sharpness trajectories for two different parameterizations (\u03bcP and SP) of a Fully Connected Network (FCN) trained on a subset of the CIFAR-10 dataset using gradient descent (GD).  The top row shows the results for the \u03bcP parameterization, while the bottom row shows the results for the SP parameterization.  Sharpness is plotted against the training step.  Dashed lines indicate the instability thresholds, illustrating the relationship between learning rate, sharpness and stability during training.  Different warmup durations are shown, and the figure demonstrates the self-stabilization mechanism where training approaches, exceeds, and then recovers from an instability threshold.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_25_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/\u03bb. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure shows the training loss and sharpness trajectories for two different parameterizations (\u03bcP and SP) of fully connected networks (FCNs) trained on a subset of the CIFAR-10 dataset using gradient descent (GD) with different warmup durations.  The sharpness plots illustrate a critical learning rate (\u03b7c) above which training becomes unstable.  The top row demonstrates the \u00b5P parameterization, and the bottom row shows the SP parameterization.  The key takeaway is the relationship between the learning rate, sharpness, and training stability, underscoring the influence of warmup on managing these dynamics.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_26_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/\u03bb. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure shows training loss and sharpness trajectories for fully connected networks (FCNs) trained on a subset of CIFAR-10 dataset.  It compares two different parameterizations (\u03bcP and SP) with different warmup durations (Twrm). The dashed lines in the sharpness plots represent the instability threshold (\u03b7c).  The plots illustrate the self-stabilization mechanism where the training initially becomes unstable (\u03b7 > \u03b7c), then sharpness reduces, leading to stabilization.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_26_2.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/\u03bb. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure displays training loss and sharpness trajectories for two different parameterizations (\u00b5P and SP) of fully connected networks (FCNs) trained on a subset of the CIFAR-10 dataset using gradient descent (GD). The top row shows the \u00b5P parameterization, while the bottom row shows the SP parameterization. The plots illustrate how the learning rate warmup affects the training process by influencing the sharpness of the loss landscape. Dashed lines in the sharpness plots indicate the instability threshold, showing when the learning rate is too high for stable training.  Different warmup durations (Twrm) are compared, illustrating how longer warmups allow the network to handle larger target learning rates.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_27_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure shows the training loss and sharpness trajectories for two different parameterizations (\u03bcP and SP) of a fully connected network (FCN) trained on a small subset of the CIFAR-10 dataset using gradient descent (GD) with a mean squared error (MSE) loss function.  The sharpness is the maximum eigenvalue of the Hessian of the loss.  The plots illustrate how the learning rate warmup affects the sharpness and the loss. The dashed lines represent the instability threshold, and when the sharpness exceeds this threshold, the training becomes unstable.  The different initializations (\u03bcP and SP) affect the relationship between the learning rate and sharpness.  The figure shows similar results across different architectures, mini-batch sizes, and loss functions, as detailed in Appendix E. ", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_27_2.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/\u03bb. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure shows the training loss and sharpness trajectories for two different parameterizations (\u00b5P and SP) of fully connected networks (FCNs) trained on a subset of the CIFAR-10 dataset using gradient descent (GD) with mean squared error (MSE) loss.  The top row displays the results for the \u00b5P parameterization, and the bottom row shows the results for the SP parameterization.  The dashed lines in the sharpness plots indicate the instability threshold (\u03b7 > \u03b7c), where \u03b7 is the learning rate and \u03b7c is the critical learning rate.  The figure demonstrates how different network parameterizations impact the training dynamics and how the warmup period affects the ability of the network to reach and maintain stability at higher learning rates. It highlights the relationship between learning rate, loss, sharpness, and training stability.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_28_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/nt curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure shows the training loss and sharpness trajectories for two different parameterizations (\u03bcP and SP) of a fully connected network (FCN) trained on a subset of the CIFAR-10 dataset using gradient descent (GD).  The plots illustrate how the learning rate warmup affects training stability and sharpness. The dashed lines represent the instability thresholds, showing when the learning rate is too high for stable training. The top row demonstrates the \u03bcP parameterization, where the target learning rate is 1/8, while the bottom row illustrates the SP parameterization with a target learning rate of 32/.  Similar behaviors are observed across different network architectures, loss functions, and batch sizes.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_28_2.jpg", "caption": "Figure 3: Test accuracy heatmaps of WRNs trained on CIFAR-10 using different parameterizations and loss functions using SGD: (a) \u03bcP and MSE loss, (b) \u03bcP and cross-entropy loss, (c) SP and MSE loss, and (d) SP and cross-entropy loss. Empty cells correspond to training divergences. Similar phase diagrams are generically observed for different architectures and datasets, as shown in Appendix F.", "description": "This figure shows the test accuracy for different combinations of network parameterization (\u03bcP or SP), loss function (MSE or cross-entropy), and warmup duration (Twrm) for Wide Residual Networks (WRNs) trained on CIFAR-10 using Stochastic Gradient Descent (SGD).  Empty cells indicate that training diverged. The heatmaps demonstrate how the optimal hyperparameter settings are influenced by the interaction of parameterization, loss function, and warmup duration. Similar trends are observed across other architectures and datasets (see Appendix F for details).", "section": "5 Impact of Warmup on Training and Generalization"}, {"figure_path": "NVl4SAmz5c/figures/figures_29_1.jpg", "caption": "Figure 3: Test accuracy heatmaps of WRNs trained on CIFAR-10 using different parameterizations and loss functions using SGD: (a) \u00b5P and MSE loss, (b) \u00b5P and cross-entropy loss, (c) SP and MSE loss, and (d) SP and cross-entropy loss. Empty cells correspond to training divergences. Similar phase diagrams are generically observed for different architectures and datasets, as shown in Appendix F.", "description": "The figure shows heatmaps illustrating the test accuracy results for WideResNet (WRN) models trained on the CIFAR-10 dataset using different parameterizations (\u00b5P and SP) and loss functions (MSE and cross-entropy).  The heatmaps depict how the test accuracy varies with respect to both the warmup duration (Twrm) and the target learning rate (\u03b7trgt). Empty cells indicate that training diverged for those parameter settings. The results show a similar pattern across different architectures and datasets, as detailed in Appendix F. The primary benefit of warmup is the ability to use larger learning rates.", "section": "5 Impact of Warmup on Training and Generalization"}, {"figure_path": "NVl4SAmz5c/figures/figures_29_2.jpg", "caption": "Figure 3: Test accuracy heatmaps of WRNs trained on CIFAR-10 using different parameterizations and loss functions using SGD: (a) \u03bcP and MSE loss, (b) \u03bcP and cross-entropy loss, (c) SP and MSE loss, and (d) SP and cross-entropy loss. Empty cells correspond to training divergences. Similar phase diagrams are generically observed for different architectures and datasets, as shown in Appendix F.", "description": "This figure shows the test accuracy results for WideResNet (WRN) models trained on the CIFAR-10 dataset using different parameterizations (\u03bcP and SP) and loss functions (MSE and cross-entropy).  The heatmaps illustrate how the test accuracy varies with different target learning rates (\u03b7trgt) and warmup durations (Twrm). Empty cells indicate training divergences. The results demonstrate that the effect of warmup is robust across various architectural choices and loss functions, extending beyond the specific settings explored in the main body of the paper.", "section": "5 Impact of Warmup on Training and Generalization"}, {"figure_path": "NVl4SAmz5c/figures/figures_29_3.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/nt curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure shows the training loss and sharpness trajectories for two different parameterizations (\u03bcP and SP) of Fully Connected Networks (FCNs) trained on a subset of the CIFAR-10 dataset using gradient descent (GD).  The top row displays results for the \u03bcP parameterization, and the bottom row displays results for the SP parameterization.  Different learning rates (\u03b7) are tested and the sharpness \u03bb is plotted against the training steps. The dashed lines represent the stability threshold (\u03b7c = 2/\u03bb), where exceeding this threshold indicates instability.  The figure illustrates how the choice of parameterization and the learning rate affects training stability and the relationship between loss and sharpness.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_30_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure displays the training loss and sharpness trajectories for two different parameterizations (\u03bcP and SP) of a Fully Connected Network (FCN) trained on a subset of the CIFAR-10 dataset using Gradient Descent (GD) with a Mean Squared Error (MSE) loss function.  The sharpness metric indicates the maximum eigenvalue of the Hessian of the loss. The dashed lines represent the instability threshold (\u03b7c).  The plots showcase how the learning rate warmup affects the training dynamics and stability for different network initializations.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_30_2.jpg", "caption": "Figure 4: Test loss heatmaps of Pre-LN Transformers in SP trained on WikiText-103 with cross-entropy loss for a single epoch using (a) Adam, and (b) GI-Adam (introduced in Section 6). Additional results are presented in Appendix F.3.", "description": "This figure displays heatmaps showing test loss of Pre-LN Transformers trained on the WikiText-103 dataset using Adam and GI-Adam optimizers.  Each heatmap presents test loss as a function of the target learning rate (\u03b7<sub>trgt</sub>) and warmup duration (T<sub>wrm</sub>). The plots illustrate the performance differences between the standard Adam and the improved GI-Adam, particularly highlighting how GI-Adam allows for training with higher learning rates and reduces training failures.  Appendix F.3 provides additional results.", "section": "Adaptive Gradient Methods (Adam)"}, {"figure_path": "NVl4SAmz5c/figures/figures_31_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/nt curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with Ntrgt = 1/8, (bottom) SP with Ntrgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure displays the training loss and sharpness trajectories for two different parameterizations (\u00b5P and SP) of fully connected networks (FCNs) trained on a subset of the CIFAR-10 dataset using gradient descent (GD) with a mean squared error (MSE) loss function.  The sharpness is represented by the maximum eigenvalue of the Hessian. The dashed lines in the sharpness plots indicate instability thresholds. The top row shows the \u00b5P parameterization, while the bottom shows the SP parameterization. Each plot shows how the loss and sharpness change over training steps, illustrating how learning rate warmup affects training stability and sharpness.  Similar behavior is observed in different architectures and loss functions.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_31_2.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/nt curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with Ntrgt = 1/8, (bottom) SP with Ntrgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure shows training loss and sharpness trajectories for two different parameterizations (\u00b5P and SP) of fully connected networks (FCNs) trained on a subset of the CIFAR-10 dataset using gradient descent (GD).  The sharpness plots illustrate how sharpness relates to the learning rate, and how a warmup schedule allows for higher target learning rates by avoiding instability.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_32_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/\u03bb. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure displays the training loss and sharpness trajectories for two different parameterizations (\u03bcP and SP) of fully connected networks (FCNs) trained on a subset of the CIFAR-10 dataset.  The plots illustrate how the learning rate warmup affects training stability.  The dashed lines represent the instability threshold; when sharpness exceeds this threshold, training becomes unstable.  The different warmup durations (Twrm) are compared, showing how they influence the sharpness and loss curves. The figure suggests that \u03bcP and SP exhibit different behaviors during the warmup period, which may relate to the sharpness reduction or sharpening phenomena discussed in the paper.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_32_2.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/nt curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure displays the training loss and sharpness trajectories of two different types of fully connected neural networks (FCNs) trained on a subset of the CIFAR-10 dataset using gradient descent (GD) with different learning rates. The top row shows the results for a network initialized using the maximal update parameterization (\u00b5P), while the bottom row shows the results for a network initialized using the standard parameterization (SP).  The sharpness plots illustrate the relationship between the learning rate and the stability of the training process, indicating when the training process becomes unstable and exceeds an instability threshold. The figure shows that the two types of initialization lead to qualitatively different training behaviors and that the stability threshold varies depending on the initialization and learning rate.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_33_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure displays the training loss and sharpness trajectories for two different parameterizations (\u03bcP and SP) of fully connected networks (FCNs) trained on a subset of CIFAR-10 using gradient descent (GD) with varying warmup durations (Twrm). The sharpness, representing the maximum eigenvalue of the Hessian of the loss, is plotted against the training step.  Dashed lines indicate instability thresholds, where exceeding the threshold leads to divergence.  The figure demonstrates how warmup allows for larger target learning rates (\u03b7trgt) by controlling sharpness and preventing divergence.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_34_1.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/nt curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with Ntrgt = 1/8, (bottom) SP with Ntrgt = 32/. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure shows the training loss and sharpness trajectories for two different network parameterizations (\u03bcP and SP) trained on a subset of CIFAR-10 using gradient descent (GD). The plots illustrate the relationship between learning rate, sharpness, and training stability.  When sharpness exceeds a critical threshold, the training becomes unstable.  The different parameterizations lead to distinct training behaviors, demonstrating how warmup can influence these dynamics.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}, {"figure_path": "NVl4SAmz5c/figures/figures_34_2.jpg", "caption": "Figure 1: Training loss and sharpness trajectories of FCNs trained on a 5k subset of CIFAR-10 with MSE loss using GD. In the sharpness plot, the dashed lines represent the 2/\u03b7t curves, and when \u03bb is above these curves, training exceeds the instability threshold (\u03b7 > \u03b7c). (top) \u03bcP with \u03b7trgt = 1/8, (bottom) SP with \u03b7trgt = 32/\u03bb. Similar mechanisms are observed across different architectures, loss functions, and mini-batch sizes, as shown in Appendix E.", "description": "This figure shows the training loss and sharpness trajectories for two different parameterizations (\u03bcP and SP) of a fully connected network (FCN) trained on a subset of the CIFAR-10 dataset using gradient descent (GD) with mean squared error (MSE) loss.  The top row displays the \u03bcP parameterization, while the bottom shows the SP parameterization.  The sharpness plots include dashed lines representing the instability thresholds (\u03b7 > \u03b7c), where \u03b7 is the learning rate.  The plots illustrate how the learning rate warmup affects the sharpness and loss, revealing different training dynamics for the two parameterizations.", "section": "Overview of Training Instabilities and the Self-Stabilization Mechanism"}]