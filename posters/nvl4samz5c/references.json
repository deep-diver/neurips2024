{"references": [{"fullname_first_author": "Jeremy Cohen", "paper_title": "Gradient descent on neural networks typically occurs at the edge of stability", "publication_date": "2021", "reason": "This paper introduces the concept of training instabilities in deep learning, a key element in the analysis of the warmup mechanism."}, {"fullname_first_author": "Alex Damian", "paper_title": "Self-stabilization: The implicit bias of gradient descent at the edge of stability", "publication_date": "2023", "reason": "This paper provides a theoretical model of the self-stabilization mechanism, which helps explain the dynamics observed during warmup."}, {"fullname_first_author": "Justin Gilmer", "paper_title": "A loss curvature perspective on training instabilities of deep learning models", "publication_date": "2022", "reason": "This paper introduces a loss landscape perspective on warmup, linking it to the sharpness of the loss landscape and the ability of the network to tolerate larger learning rates."}, {"fullname_first_author": "Aitor Lewkowycz", "paper_title": "The large learning rate phase of deep learning: the catapult mechanism", "publication_date": "2020", "reason": "This paper introduces the concept of catapults, which are abrupt changes in the loss and sharpness during training, and how warmup can mitigate them."}, {"fullname_first_author": "Liyuan Liu", "paper_title": "On the variance of the adaptive learning rate and beyond", "publication_date": "2020", "reason": "This paper discusses the role of warmup in adaptive optimizers, such as Adam, and how it can mitigate the harmful effects of large variance in the adaptive learning rate."}]}