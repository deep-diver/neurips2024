[{"figure_path": "1067784F6e/tables/tables_4_1.jpg", "caption": "Table 1: Comparison with KL, WD and MMD2, on sample and computational complexities, triangle inequality and connection with the Huber model. dim is the dimension of the random variable/data.", "description": "This table compares four different distance metrics (KL divergence, Wasserstein distance, squared MMD, and MMD) across five criteria: sample complexity (asymptotic and finite sample bounds), computational complexity, whether the triangle inequality holds, and whether it is compatible with the Huber model. The comparison helps to justify the choice of MMD as the best distance metric for data distribution valuation in this paper.", "section": "Model, Problem Statement and MMD"}, {"figure_path": "1067784F6e/tables/tables_8_1.jpg", "caption": "Table 2: Pearson correlations between data sample values and data distribution values for classification.", "description": "This table presents the Pearson correlation coefficients between the rankings of data sample values (obtained from various valuation methods) and the rankings of the true data distribution values for classification tasks.  Higher correlation indicates better agreement between the rankings produced by a method and the true rankings of the distributions. The table includes results for two pairs of datasets: CIFAR10 vs. CIFAR100 and TON vs. UGR16,  and compares several valuation methods (LAVA, DAVINZ, CS, MMD\u00b2, Ours, Ours cond.) including the proposed MMD-based method (Ours and Ours cond.).  Ours cond. represents the method when label information is used.", "section": "5 Empirical Results"}, {"figure_path": "1067784F6e/tables/tables_9_1.jpg", "caption": "Table 2: Pearson correlations between data sample values and data distribution values for classification.", "description": "This table presents the Pearson correlation coefficients between the ranking of data sample values and the ranking of data distribution values for classification tasks.  The table compares the performance of several methods, including the proposed MMD-based method (\"Ours\"), against existing baselines (LAVA, DAVINZ, CS, MMD2) across various datasets (CIFAR10 vs. CIFAR100, TON vs. UGR16). Higher correlation indicates better performance of the valuation method in accurately ranking data distributions.", "section": "5 Empirical Results"}, {"figure_path": "1067784F6e/tables/tables_28_1.jpg", "caption": "Table 4: Datasets, used ML models M and n, mi, Ei.", "description": "This table summarizes the experimental settings used in the empirical evaluation of the proposed data distribution valuation method.  It lists the datasets used for classification and regression tasks, the machine learning models employed for each task, the number of vendors (n), the sample size for each vendor (mi), and the parameter controlling the heterogeneity in the Huber model (\u03b5i). The table provides a concise overview of the experimental setup, facilitating reproducibility and comparison across different settings.", "section": "5 Empirical Results"}, {"figure_path": "1067784F6e/tables/tables_31_1.jpg", "caption": "Table 5: Pearson correlations between data sample values and data distribution values for classification.", "description": "This table presents the Pearson correlation coefficients between the data sample values (obtained using various methods) and the true data distribution values for classification tasks.  The Pearson correlation coefficient measures the linear association between two sets of data; a higher value indicates a stronger positive association. The table shows correlations for different numbers of vendors (n), each with a sample size (mi).  The results provide insights into how well each data valuation method captures the true rankings of data distributions, based on the sample data.", "section": "5 Empirical Results"}, {"figure_path": "1067784F6e/tables/tables_31_2.jpg", "caption": "Table 6: Classification: P* = Credit7, Q = Credit31.", "description": "This table presents the Pearson correlation coefficients between data sample values and data distribution values for a classification task using the Credit7 and Credit31 datasets.  The Pearson correlation is calculated using two different methods of valuation: one which uses a validation set (\u03c1(\u03bd, \u03b6)), and one which approximates the reference distribution (\u03c1\u03cd, \u03b6)). Several baselines (LAVA, DAVINZ, CS, MMD2, Ours, Ours cond.) are compared. The results show the correlation between the valuation of datasets and the true value of the underlying data distributions.  'Ours cond.' refers to the method which utilizes label information.  N.A. indicates that the result is not applicable.", "section": "5.2 Ranking Data Distributions"}, {"figure_path": "1067784F6e/tables/tables_31_3.jpg", "caption": "Table 2: Pearson correlations between data sample values and data distribution values for classification.", "description": "This table presents the Pearson correlation coefficients between the rankings of data sample values and the true rankings of data distributions for classification tasks.  The Pearson correlation measures the strength and direction of the linear relationship between the two rankings. Higher values indicate stronger agreement between the rankings.  The table compares the performance of different valuation methods: LAVA, DAVINZ, CS, MMD\u00b2, Ours, and Ours cond.  Ours cond. leverages label information, while Ours does not.  The results are reported with standard errors over 5 independent trials and for different datasets.  N.A indicates that the result is not applicable for that specific combination.", "section": "5 Empirical Results"}, {"figure_path": "1067784F6e/tables/tables_31_4.jpg", "caption": "Table 8: Classification: P* = MNIST, Q = FaMNIST.", "description": "This table presents the Pearson correlation coefficients between data sample values and data distribution values for a classification task using the MNIST and FaMNIST datasets.  The Pearson correlation is calculated between the rankings produced by different valuation methods and the ground truth ranking of the data distributions, to assess the accuracy of the ranking.  The table shows results for various methods including LAVA, DAVINZ, CS, MMD\u00b2, Ours, and Ours cond., under two conditions: when a validation set is available to help baselines and when it isn't.  Ours cond. (Ours with conditional distributions) is not applicable when a validation set isn't available.", "section": "5 Empirical Results"}, {"figure_path": "1067784F6e/tables/tables_32_1.jpg", "caption": "Table 9: Non-Huber: additive Gaussian noise.", "description": "This table presents the Pearson correlation coefficients between data sample values and data distribution values for a non-Huber setting with additive Gaussian noise.  It compares the performance of several baselines (LAVA, DAVINZ, CS, MMD\u00b2, Ours, and Ours cond.) in ranking data distributions based on their sample datasets. The results are presented separately for when a validation set is available (\u03c1(\u03bd, \u03b6)) and unavailable (\u03c1(\u1fe6, \u03b6)).  The \u2018Ours\u2019 method refers to the proposed MMD-based valuation method in the paper, while \u2018Ours cond.\u2019 incorporates label information. The table highlights the relative performance of different valuation methods under this specific non-Huber data setting.", "section": "5 Empirical Results"}, {"figure_path": "1067784F6e/tables/tables_32_2.jpg", "caption": "Table 2: Pearson correlations between data sample values and data distribution values for classification.", "description": "This table presents the Pearson correlation coefficients between the data sample values and the data distribution values for classification tasks.  The Pearson correlation measures the linear association between the rankings of data sample values (obtained using various valuation methods) and the rankings of data distribution values (ground truth obtained from model performance on a held-out test set).  Higher correlation indicates better agreement between the rankings, reflecting the effectiveness of the valuation methods in identifying the most valuable data distributions. Results are shown for multiple datasets with their corresponding classification methods and the number of vendors considered (n). The table includes results for scenarios both with and without a validation set.", "section": "5 Empirical Results"}, {"figure_path": "1067784F6e/tables/tables_33_1.jpg", "caption": "Table 2: Pearson correlations between data sample values and data distribution values for classification.", "description": "This table presents the Pearson correlation coefficients between the data sample values and the data distribution values for classification tasks.  The Pearson correlation measures the strength and direction of the linear relationship between the rankings of data sample values (obtained using different valuation methods) and the ground truth rankings of data distributions.  Higher values indicate a stronger positive correlation, meaning the valuation method better reflects the true value of the data distributions. The results are shown for four different datasets: CIFAR10 vs. CIFAR100, TON vs. UGR16, and two cases with Dval available and unavailable, respectively.", "section": "5 Empirical Results"}, {"figure_path": "1067784F6e/tables/tables_34_1.jpg", "caption": "Table 12: Average and standard error (over 5 independent trials) of Pearson coefficients with GT for PMNIST and QEMNIST with n = 5, rows i' = 2, 3, 4 corresponding to Fig. 7.", "description": "This table shows the Pearson correlation coefficients between the ground truth data values and the approximated values obtained using MMD2 and the proposed method (Ours). The ground truth values are obtained using the true distribution P* as the reference, while the approximated values are obtained using the uniform mixture Pw as the reference. The results are shown for different values of i', which represents the index of the vendor that is mis-reporting its data. The table shows that both MMD2 and Ours have very high Pearson correlation coefficients with the ground truth values, suggesting that both methods are able to accurately capture the relationship between the true values and the approximated values. The results indicate that the proposed method is effective at identifying the mis-reporting vendor, even when the ground truth is not known.", "section": "5.2 Ranking Data Distributions"}, {"figure_path": "1067784F6e/tables/tables_34_2.jpg", "caption": "Table 12: Average and standard error (over 5 independent trials) of Pearson coefficients with GT for PMNIST and QEMNIST with n = 5, rows i' = 2, 3, 4 corresponding to Fig. 7.", "description": "This table shows the Pearson correlation coefficients between the ground truth data values and the approximated data values obtained using MMD2 and the proposed method for the MNIST and QEMNIST datasets with 5 vendors. The results are presented for the case where vendor i' adds Gaussian noise to the features of their data, and the results are consistent with the ground truth results.", "section": "D.3.5 Preliminary Experimental Results on Incentive Compatibility"}, {"figure_path": "1067784F6e/tables/tables_35_1.jpg", "caption": "Table 12: Average and standard error (over 5 independent trials) of Pearson coefficients with GT for PMNIST and QEMNIST with n = 5, rows i' = 2, 3, 4 corresponding to Fig. 7.", "description": "This table presents the Pearson correlation coefficients between the ground truth data values and the values obtained by using MMD2 and the proposed method (Ours). The ground truth values are obtained using the expected test performance. The results show high correlation (close to 1) between the ground truth and the estimated values, especially for the proposed method, suggesting that the proposed method can achieve approximate incentive compatibility.", "section": "D.3.5 Preliminary Experimental Results on Incentive Compatibility"}, {"figure_path": "1067784F6e/tables/tables_35_2.jpg", "caption": "Table 15: Average and standard error (over 5 independent trials) of Pearson coefficients with GT for PMNIST and QFaMNIST with n = 10, rows i' = 2, 3, 4 corresponding to Fig. 8.", "description": "This table presents the Pearson correlation coefficients between the ground truth data values and the approximated data values obtained using MMD2 and the proposed method (Ours), for different mis-reporting vendors (i').  The results are from an experiment to test incentive compatibility, where a vendor misreports its data by adding Gaussian noise.  High correlation coefficients indicate good agreement between the true and approximated values, suggesting that incentive compatibility is approximately satisfied.", "section": "5.2 Ranking Data Distributions"}, {"figure_path": "1067784F6e/tables/tables_36_1.jpg", "caption": "Table 1: Comparison with KL, WD and MMD2, on sample and computational complexities, triangle inequality and connection with the Huber model. dim is the dimension of the random variable/data.", "description": "This table compares the Maximum Mean Discrepancy (MMD) method against three other methods for data distribution valuation: Kullback-Leibler (KL) divergence, Wasserstein distance (WD), and squared MMD (MMD\u00b2).  The comparison considers sample and computational complexities, whether the triangle inequality holds, and compatibility with the Huber model.  The MMD method is shown to be superior in several aspects, particularly its sample efficiency and theoretical guarantees.", "section": "Model, Problem Statement and MMD"}, {"figure_path": "1067784F6e/tables/tables_36_2.jpg", "caption": "Table 1: Comparison with KL, WD and MMD2, on sample and computational complexities, triangle inequality and connection with the Huber model. dim is the dimension of the random variable/data.", "description": "This table compares four different distance metrics: Kullback-Leibler (KL) divergence, Wasserstein distance (WD), squared maximum mean discrepancy (MMD2), and maximum mean discrepancy (MMD).  The comparison is based on their sample complexity, computational complexity, whether they satisfy the triangle inequality, and their compatibility with the Huber model for data heterogeneity.  MMD is shown to have favorable properties compared to other metrics.", "section": "Model, Problem Statement and MMD"}, {"figure_path": "1067784F6e/tables/tables_36_3.jpg", "caption": "Table 1: Comparison with KL, WD and MMD2, on sample and computational complexities, triangle inequality and connection with the Huber model. dim is the dimension of the random variable/data.", "description": "This table compares four different metrics (KL, WD, MMD2, and MMD) based on several criteria relevant to data distribution valuation, particularly in the context of the Huber model for data heterogeneity.  These criteria include sample complexity (how much data is needed for accurate estimation), computational complexity (how much computation is needed), whether the metric satisfies the triangle inequality (a key property for theoretical analysis), and compatibility with the Huber model (whether it easily integrates into the analysis of mixture distributions).  The results show that MMD excels across several of these criteria, making it particularly suitable for use in the theoretical framework of the paper.", "section": "3 Model, Problem Statement and MMD"}, {"figure_path": "1067784F6e/tables/tables_37_1.jpg", "caption": "Table 1: Comparison with KL, WD and MMD2, on sample and computational complexities, triangle inequality and connection with the Huber model. dim is the dimension of the random variable/data.", "description": "This table compares the Maximum Mean Discrepancy (MMD) method to three other methods (Kullback-Leibler divergence, Wasserstein distance, and squared MMD) across several criteria.  These criteria are sample complexity (asymptotic and finite sample), computational complexity, whether the method satisfies the triangle inequality, and whether it is compatible with the Huber model of data heterogeneity.  The table highlights the advantages of MMD in terms of sample and computational efficiency, and its suitability for theoretical analysis thanks to the triangle inequality property.", "section": "3 Model, Problem Statement and MMD"}, {"figure_path": "1067784F6e/tables/tables_37_2.jpg", "caption": "Table 1: Comparison with KL, WD and MMD2, on sample and computational complexities, triangle inequality and connection with the Huber model. dim is the dimension of the random variable/data.", "description": "This table compares the Maximum Mean Discrepancy (MMD) method to three other methods (Kullback-Leibler divergence, Wasserstein distance, and squared MMD) across several criteria.  These criteria include the asymptotic and computational complexities of each method for both sample and computation, whether the method satisfies the triangle inequality, and whether the method is compatible with the Huber model for data heterogeneity.", "section": "Model, Problem Statement and MMD"}, {"figure_path": "1067784F6e/tables/tables_37_3.jpg", "caption": "Table 1: Comparison with KL, WD and MMD2, on sample and computational complexities, triangle inequality and connection with the Huber model. dim is the dimension of the random variable/data.", "description": "This table compares four different divergence metrics (KL, WD, MMD2, and MMD) in terms of their sample complexity, computational complexity, whether they satisfy the triangle inequality, and their compatibility with the Huber model.  The Huber model is a statistical model used in the paper to represent data heterogeneity.", "section": "3 Model, Problem Statement and MMD"}, {"figure_path": "1067784F6e/tables/tables_38_1.jpg", "caption": "Table 22: Maximum CUDA, RAM and time for DAVINZ with n = 10 data vendors on MNIST with a standard convolutional neural network.", "description": "This table shows the maximum CUDA memory (in MBs), RAM (in MBs), and CPU time (in seconds) used by the DAVINZ method for different sample sizes (mi) on the MNIST dataset.  The experiment uses 10 data vendors and a standard convolutional neural network. The table illustrates the scalability limitations of DAVINZ, particularly with respect to memory usage.", "section": "5 Empirical Results"}]