[{"figure_path": "ufKBRvYxtp/tables/tables_1_1.jpg", "caption": "Table 1: A comparison between sample and oracle complexities (i.e., number of weak learning calls) of the present results and previous works, in each case to achieve \u025b-excess population error. Here we suppress polylogarithmic factors. We make progress on closing the sample complexity gap between ERM, which is computationally inefficient, and boosting-based approaches. The y-weak leaner outputs a hypothesis from the base class B, which is usually substantially smaller than H against which the final agnostic learning guarantee holds. In practice, boosting is used with learners with small values of log |B|. See Definition 1 for details. See the paragraph following Theorem 1 in [KK09] and Section 3.3 in [BCHM20] for derivation of these bounds. See also Theorem 2.14 in [AGHM21] for a bound on the expressivity of the boosted class to derive ERM's sample complexity.", "description": "This table compares the sample and oracle complexities of different agnostic boosting algorithms, including the proposed algorithm, to achieve an error within epsilon of the optimal classifier.  It shows that the new algorithm significantly improves sample complexity without increasing computational complexity.", "section": "1 Introduction"}, {"figure_path": "ufKBRvYxtp/tables/tables_2_1.jpg", "caption": "Table 2: Sample complexity of reinforcement learning given \u03b3-weak learner over the policy class, for two different modes of accessing the underlying MDP, in terms of \u03b5 and \u03b3, suppressing other terms.", "description": "This table compares the sample complexities of two different reinforcement learning approaches (episodic model and rollouts with v-resets) using a \u03b3-weak learner.  The sample complexity is expressed in terms of \u03b5 (excess error) and \u03b3 (weak learner's edge). The table shows that using the proposed algorithm (Theorem 7) significantly reduces the sample complexity compared to a previous approach ([BHS22]).", "section": "1.1 Contributions and technical innovations"}, {"figure_path": "ufKBRvYxtp/tables/tables_9_1.jpg", "caption": "Table 1: A comparison between sample and oracle complexities (i.e., number of weak learning calls) of the present results and previous works, in each case to achieve \u03b5-excess population error. Here we suppress polylogarithmic factors. We make progress on closing the sample complexity gap between ERM, which is computationally inefficient, and boosting-based approaches. The \u03b3-weak leaner outputs a hypothesis from the base class B, which is usually substantially smaller than H against which the final agnostic learning guarantee holds. In practice, boosting is used with learners with small values of log |B|. See Definition 1 for details. See the paragraph following Theorem 1 in [KK09] and Section 3.3 in [BCHM20] for derivation of these bounds. See also Theorem 2.14 in [AGHM21] for a bound on the expressivity of the boosted class to derive ERM's sample complexity.", "description": "This table compares the sample and oracle complexities of several agnostic boosting algorithms, including the proposed one, with the computationally expensive Empirical Risk Minimization (ERM) method.  It highlights the trade-off between sample efficiency and computational cost, showing that the new algorithm significantly improves sample efficiency while maintaining reasonable computational complexity.", "section": "1 Introduction"}]