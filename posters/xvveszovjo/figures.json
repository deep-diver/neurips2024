[{"figure_path": "xvVeSZoVJO/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of noisy camera situations (blurred, occluded and even failed) during collaboration and the perception result w.o./w. RCDN. orange for drivable areas segmentation, blue for lanes and teal for dynamic vehicles.", "description": "This figure illustrates how RCDN handles noisy camera situations during multi-agent collaboration.  It shows three scenarios: a camera failure, occlusion, and blurring. The left side depicts the poor perception results without RCDN in these scenarios, while the right shows how RCDN improves the perception using information from other agents' views to reconstruct the complete scene, resulting in a more accurate segmentation of drivable areas, lanes, and dynamic vehicles.", "section": "1 Introduction"}, {"figure_path": "xvVeSZoVJO/figures/figures_3_1.jpg", "caption": "Figure 2: System overview. The geometry BEV generation module provides feature sampling for later processes. The collaborative static and dynamic fields are performed in parallel to model the background and foreground, respectively. Note that MCP is short for the multi-agents collaborative perception process.", "description": "This figure illustrates the overall architecture of the Robust Camera-Insensitivity collaborative perception system (RCDN).  It shows the flow of data and processing steps, highlighting the key components: geometry BEV generation, collaborative static neural field, and collaborative dynamic neural field.  The geometry BEV generation module first extracts features from multi-agent inputs. These features then feed into both the static and dynamic neural fields. The static field models the background, while the dynamic field focuses on foreground objects. Finally, the outputs of these fields are combined to generate the final aggregated BEV feature. The figure also visually represents the improved perception results achieved using RCDN (Success) compared to those without RCDN (Failed).", "section": "4 RCDN"}, {"figure_path": "xvVeSZoVJO/figures/figures_7_1.jpg", "caption": "Figure 3: Comparison of the performance of other baseline methods w.o/w the proposed RCDN under the random noisy (failed situation) camera numbers from 0 to 3. RCDN can be ported to other baseline methods and stabilize the performance under different level camera failure situations on OPV2V-N dataset.", "description": "This figure compares the performance of different baseline methods with and without RCDN under various levels of random noisy camera failures (from 0 to 3 failed cameras). The results are shown for three different map segmentation tasks: drivable area, lane, and dynamic vehicle.  It demonstrates that RCDN improves robustness across all baselines in the face of increasing camera failures.", "section": "5 Experimental Results"}, {"figure_path": "xvVeSZoVJO/figures/figures_7_2.jpg", "caption": "Figure 4: Effectiveness of dynamic neural field.", "description": "The figure shows a comparison of the performance of the proposed RCDN with and without the time model. The left image shows the result without the time model, while the right image shows the result with the time model. The time model improves the quality of the reconstructed image by reducing the blurriness and noise. This demonstrates that the dynamic neural field is effective in handling the temporal consistency in the collaborative perception process.", "section": "4 RCDN"}, {"figure_path": "xvVeSZoVJO/figures/figures_8_1.jpg", "caption": "Figure 5: Visualization of different baseline methods w. RCDN with one random camera failure.", "description": "This figure visualizes the performance of different baseline methods (AttFusion, F-Cooper, V2VNet, and COBEVT) with and without the proposed RCDN when one camera randomly fails.  It shows the repaired views generated by RCDN, the original views with camera failure, the segmentation results without RCDN, and the results with RCDN.  The origin segmentation map acts as ground truth. The comparison highlights how RCDN improves the segmentation results by recovering information lost due to camera failure.", "section": "5.3 Qualitative Evaluation"}, {"figure_path": "xvVeSZoVJO/figures/figures_8_2.jpg", "caption": "Figure 6: Comparison between existing dynamic field modeling and the proposed RCDN.", "description": "This figure compares the training efficiency and performance (measured by PSNR) of two dynamic field modeling methods: an implicit MLP-based method and the explicit grid-based RCDN proposed in the paper. The RCDN method is shown to achieve significantly faster training (around 24 times faster) and higher PSNR, indicating improved efficiency and quality.", "section": "4.4 Dynamic Collaborative Neural Field"}, {"figure_path": "xvVeSZoVJO/figures/figures_9_1.jpg", "caption": "Figure 5: Visualization of different baseline methods w. RCDN with one random camera failure.", "description": "This figure visualizes the results of different baseline methods (CoBEVT and V2VNet) with and without the proposed RCDN (Robust Camera-Insensitivity collaborative perception) when there is one randomly failing camera.  It shows the impact of RCDN on the accuracy of map segmentation. The left column shows the original (successful) view, the center column is the resulting perception with only one camera failure, and the right column shows the results after RCDN is applied.  The color-coded boxes (orange, blue, teal) in the segmentation maps represent the drivable area, lanes, and dynamic vehicles, respectively. The visual comparison highlights RCDN's ability to improve the robustness and accuracy of collaborative perception under camera failures.", "section": "5.3 Qualitative Evaluation"}]