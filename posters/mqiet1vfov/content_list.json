[{"type": "text", "text": "Boosting Sample Efficiency and Generalization in Multi-agent Reinforcement Learning via Equivariance ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Joshua McClellan Naveed Haghani John Winder JHU APL \u2217 JHU APL JHU APL University of Maryland naveed.haghani@jhuapl.edu john.winder@jhuapl.edu joshmccl@umd.edu ", "page_idx": 0}, {"type": "text", "text": "Furong Huang Pratap Tokekar University of Maryland University of Maryland furongh@umd.edu tokekar@umd.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency and poor generalization [1]. These challenges are partially due to a lack of structure or inductive bias in the neural networks typically used in learning the policy. One such form of structure that is commonly observed in multi-agent scenarios is symmetry. The field of Geometric Deep Learning has developed Equivariant Graph Neural Networks (EGNN) that are equivariant (or symmetric) to rotations, translations, and reflections of nodes. Incorporating equivariance has been shown to improve learning efficiency and decrease error [2]. In this paper, we demonstrate that EGNNs improve the sample efficiency and generalization in MARL. However, we also show that a naive application of EGNNs to MARL results in poor early exploration due to a bias in the EGNN structure. To mitigate this bias, we present Exploration-enhanced Equivariant Graph Neural Networks or E2GN2. We compare E2GN2 to other common function approximators using common MARL benchmarks MPE and SMACv2. E2GN2 demonstrates a significant improvement in sample efficiency, greater final reward convergence, and a $2\\mathrm{x}{-}5\\mathrm{x}$ gain in over standard GNNs in our generalization tests. These results pave the way for more reliable and effective solutions in complex multi-agent systems. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-Agent Reinforcement Learning (MARL) has found success in various applications such as robotics [3, 4, 5], complex strategy games [6, 7, 8] and power grid management [9, 10]. However, MARL algorithms can be slow to train, difficult to tune, and have poor generalization guarantees [1, 11]. This is partially because typical implementations of MARL techniques use neural networks such as Multi-Layer Perceptrons (MLP) that do not take the underlying ", "page_idx": 0}, {"type": "image", "img_path": "MQIET1VfoV/tmp/89eac09d9da6b0f2b6f04aa0422a2ccdf4c74ea1b00030aaca99887914e4b66f.jpg", "img_caption": ["Figure 1: An example of how using an equivariant function approximator shrinks the total search space. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "structure into account. The models learn simple input/output relationships with no constraints or priors on the policies learned. These generic architectures lack a strong inductive bias making them inefficient in terms of the training samples required. ", "page_idx": 0}, {"type": "text", "text": "Symmetries are commonplace in the world. As humans, we exploit symmetries in our daily lives to improve reasoning and learning. It is a basic concept learned by children. Humans do not need to relearn how to eat an apple simply because it has been moved from the right to the left. In soccer, if you have learned to pass to someone on the right, it is easier to learn to pass to someone on the left. Our objective is to develop agents that are guaranteed to adhere to these basic principles, without needing to learn every single scenario from scratch. ", "page_idx": 1}, {"type": "image", "img_path": "MQIET1VfoV/tmp/5ed14bcf8f6bc386143f4fa35d5ff21ab920330e7345c89cd86f688f4fb36535.jpg", "img_caption": ["Figure 2: An example of rotational equivariance/symmetry in MPE simple spread environment. Note as the agent (in red) positions are rotated, the optimal actions (arrows) are also rotated. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Symmetries are particularly common in MARL. These occur in the form of equivariance and invariance. Given a transformation matrix $_T$ , if $f(\\pmb{T}\\boldsymbol{x})=f(\\boldsymbol{x})$ that function is said to be invariant. Similarly, $f(.)$ is equivariant if $f(\\pmb{T}x)=$ $T f(x)$ [12]. Rotational and reflection symmetries (the $O(n)$ symmetry group) are particularly common in Reinforcement Learning (RL) scenarios. For example, consider the Multi-agent Particle Environment (MPE) [13] benchmark for MARL which has agents with simple dynamics and tasks. We note that this scenario adheres to rotational equivariance. As shown in Figure 2 rotating the agent\u2019s positions results in the op", "page_idx": 1}, {"type": "text", "text": "timal actions also being rotated. A policy that is rotationally equivariant effectively shrinks the state-action space for the problem, potentially making the problem easier to learn (Figure 1). ", "page_idx": 1}, {"type": "text", "text": "One way to guarantee equivariance to rotations and reflections in MARL is to use an Equivariant Graph Neural Network [2]. Due to the more complex and equivariant structure of the EGNN, directly adapting EGNNs to MARL is not straightforward. Specifically, consider action spaces typically represented as discrete choices (e.g., up, down, left, shoot). Typically the policy over such an action space is represented using logits (to specify the probability of choosing each action). EGNN outputs a continuous equivariant output. Mapping this output to logits is non-trivial. Therefore, we ask the question What is the correct representation of a complex action space when using Equivariant GNNs? ", "page_idx": 1}, {"type": "text", "text": "Another specific issue we observed in EGNNs is an exploration bias in the early stages of learning due to the specific nature of the equivariant computation. This bias can lead to poor initial exploration, decreasing the probability of the agent finding the optimal policy, and potentially resulting in suboptimal convergence. Therefore we ask the question How can we design a GNN equivariant to rotations, but without early exploration bias? ", "page_idx": 1}, {"type": "text", "text": "The main contribution of this paper is Exploration-enhanced Equivariant Graph Neural Networks (E2GN2). This addresses the two research questions. Specifically, we show how to apply Equivariant GNNs to complex MARL tasks, to mixed discrete/continuous action spaces, and to mitigate the early exploration bias. Our contributions can be summarized as: ", "page_idx": 1}, {"type": "text", "text": "(1) Our approach is the first to successfully demonstrate Equivariant GNNs for MARL on standard MARL benchmarks with complex action spaces. ", "page_idx": 1}, {"type": "text", "text": "(2) We propose E2GN2 which has no bias in early exploration for MARL and is equivariant to rotations and reflections. ", "page_idx": 1}, {"type": "text", "text": "(3) We evaluate E2GN2 on common MARL benchmarks: MPE [13] and Starcraft Multi-agent Challenge v2 [14] using PPO. It learns quicker, outperforming standard GNNs and MLPs by up to $2{\\bf x}\\!-\\!5{\\bf x}^{\\mathrm{~2~}}$ in sample efficiency on terran and protoss challenges. It is worth noting that E2GN2 is an improvement on the function approximation for MARL and thus is compatible with most MARL actor-critic methods. ", "page_idx": 1}, {"type": "text", "text": "(4) We showcase E2GN2\u2019s ability to generalize to scenarios it wasn\u2019t trained on, due to the equivariance guarantees. This results in $5\\mathrm{x}$ performance over GNNs ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A key theoretical foundation for our paper is [15], which formulated the structure for equivariant MDPs. One important takeaway is that if a reward is equivariant to a transformation/symmetry, we want the policy and dynamics to be equivariant to that symmetry. However, this work was limited to very simple dynamics with discrete actions such as cart-pole. [16] followed up with equivariance in multi-agent, but was again limited to simple problems. Their method is specifically formulated and tested on small discrete grid world problems with simple dynamics and discrete up, down,left actions. For example, the trafic control problem has an input of a $7\\mathrm{x7}$ grid. Extending their work to continuous environments with large state spaces, large mixed discrete/continuous action spaces is not straightforward without significant modifications. ", "page_idx": 2}, {"type": "text", "text": "Contemporary to our research, [17] demonstrated E(3) equivariant networks on simple cooperative navigation problems. However, their results on more complex tasks, such as Starcraft, did not excel over the baseline. Additionally, they used SEGNN [18] which can result in very slow training times, making tuning difficult and cumbersome. Others took the approach [19] of attempting to learn symmetries via an added loss term. However, since this approach needed to learn the symmetries in parallel with learning it did not have the same guarantees as Equivariant GNNs, and did not result in significant performance gains. Another work [20] demonstrated rotation equivariance for complex robotic manipulation tasks. This work, while promising, was for single-agent RL and used image observations and many problems don\u2019t have access to image observations. ", "page_idx": 2}, {"type": "text", "text": "AI research in chemistry has taken a particular interest in adding symmetry constraints to GNNs. Works such as EGNN, [2] SEGNN, [18] E3NN, [21] and Equivariant transformers have demonstrated various approaches to encoding symmetries in GNNs. In this paper, we chose to focus on EGNN due to its simplicity, high performance, and quick inference time. Other works took a different approach, such as [22], which proposes Equivariant MLPs, solving a constrained optimization problem to encode a variety of symmetries. Unfortunately, in our experience, the inference time was slow, and we preferred a network with a graph structure such as EGNN. ", "page_idx": 2}, {"type": "text", "text": "3 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 MARL ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Multi-agent reinforcement learning (MARL) considers problems where multiple learning agents interact in a shared environment. The goal is for the agents to learn policies that maximize long-term reward through these interactions. Typically, MARL problems are formalized as Markov games [23]. A Markov game for $N$ agents is defined by a set of states $\\boldsymbol{S}$ , a set of actions $\\mathcal{A}_{1},...,\\mathcal{A}_{N}$ for each agent, transition probabilities $P:\\mathcal{S}\\times\\mathcal{A}_{1}\\times...\\times\\mathcal{A}_{N}\\times\\mathcal{S}\\to[0,1]$ , and reward functions $R_{1},...R_{N}$ mapping each state and joint action to a scalar reward. ", "page_idx": 2}, {"type": "text", "text": "The goal of each agent $i$ is to learn a policy $\\pi_{i}(a_{i}|s)$ that maximizes its expected return: $J(\\pi_{i})=$ $\\begin{array}{r}{\\mathbb{E}\\pi_{1},...,\\pi_{N}\\big[\\sum_{t=0}^{T}\\gamma^{t}R_{i}\\big(s_{t},a_{t}^{1},...,a_{t}^{N}\\big)\\big]}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "Where $T$ is the time horizon, $\\gamma\\,\\in\\,(0,1]$ is a discount factor, and $a_{t}^{j}\\,\\sim\\,\\pi_{j}(\\cdot|s_{t})$ . The presence of multiple learning agents makes this more complex than single-agent RL due to issues such as non-stationarity and multi-agent credit assignment. ", "page_idx": 2}, {"type": "text", "text": "3.2 Equivariance ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Crucial to equivariance is group theory. A group is an abstract algebraic object describing a symmetry. For example, $O(3)$ describes the set of continuous rotation symmetries. A group action is an element of that particular group. To describe how groups o perate on data we use representations of group actions. A representation can be described as a mapping from a group element to a matrix, $\\bar{\\rho}:\\bar{G}\\rightarrow G L(m)$ where $\\rho(g)\\in\\mathbb{R}^{m\\times m}$ Or instead we can more simply use: $L_{g}:X\\to X$ where $g\\in G$ where $L_{g}$ is the matrix representation of the group element $g\\in G$ [24]. ", "page_idx": 2}, {"type": "text", "text": "A function is equivariant to a particular group or symmetry if transforming the input is equivalent to transforming the function output. More formally, $T_{g}f(x)=f(L_{g}x)$ for $g\\in G$ , $L_{g}:X\\to X$ and $T_{g}:Y\\rightarrow Y$ . Related to equivariance is the key concept of invariance, that is a function does not change with a transformation to the input: $f(x)^{'}=f(L_{g}^{'}x)$ . [24] ", "page_idx": 2}, {"type": "text", "text": "Previous work [16] has shown that if a Markov game has symmetries in the dynamics and the reward function then the resulting optimal policy will be equivariant and the value function will be invariant. That is, $V(L_{g}{\\bf s})=V(s)$ and $\\pi(L_{g}\\mathbf{\\dot{s}})=^{\\circ}K_{g}\\pi(s)$ where $L_{g}$ and $K_{g}$ , with $g\\in G$ are transformations to the state and action respectively. ", "page_idx": 3}, {"type": "text", "text": "3.3 Equivariant Graph Neural Network ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Equivariant Graph Neural Networks [2] (EGNN) are an extension of a standard Graph Neural Network. EGNNs are equivariant to the $E(n)$ group, that is, rotations, translations, and reflections in Euclidean space. EGNNs have two vectors of embeddings for each graph node $i$ : the feature embeddings denoted by $h_{i}^{l}$ , and coordinate embeddings denoted by $\\pmb{u}_{i}^{l}$ , where $l$ denotes the neural network layer. The equations describing the forward pass for a single layer are below: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle m_{i j}=\\phi_{e}\\left(h_{i}^{l},h_{j}^{l},\\|\\left(\\boldsymbol{u}_{i}^{l}-\\boldsymbol{u}_{j}^{l}\\right)\\|^{2}\\right)}}\\\\ {{\\displaystyle u_{i}^{l+1}=u_{i}^{l}+C\\sum_{j\\neq i}\\left(u_{i}^{l}-u_{j}^{l}\\right)\\phi_{u}\\left(m_{i j}\\right)}}\\\\ {{\\displaystyle m_{i}=\\sum_{j\\neq i}m_{i j},\\quad h_{i}^{l+1}=\\phi_{h}\\left(h_{i}^{l},m_{i}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here $\\phi$ represents a multi-layer perceptron, where $\\phi_{e}:\\mathbb{R}^{n}\\,\\mapsto\\,\\mathbb{R}^{m}$ , $\\phi_{u}\\;:\\;\\mathbb{R}^{m}\\;\\mapsto\\;\\mathbb{R}$ , and $\\phi_{h}:$ $\\mathbb{R}^{m+p}\\mapsto\\mathbb{R}^{p}$ . The key difference between EGNN and GNN is the addition of coordinate embeddings $\\pmb{u}_{i}$ and equation 2, which serves to update the coordinate embeddings in a manner that is equivariant to transformations from $E(n)$ . Note that $\\pmb{u}_{i}$ will be equivariant and $h_{i}$ will be invariant to these transformations [2]. ", "page_idx": 3}, {"type": "text", "text": "As noted in Section 1, application of EGNN to MARL is not straightforward. In the following section, we discuss these issues in more depth and present our solution towards addressing them. ", "page_idx": 3}, {"type": "text", "text": "4 Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we address both theoretically and empirically how the output of EGNN is biased by the input, leading to suboptimal exploration in RL. To mitigate this issue, we introduce Explorationenhanced Equivariant Graph Neural Networks (E2GN2) as a method that ameliorates this bias, leading to improved exploration. ", "page_idx": 3}, {"type": "text", "text": "4.1 Biased Exploration in EGNN Policies ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "An important component of reinforcement learning is exploration. Practitioners often use a policy parameterized by a Gaussian distribution, where the mean is determined by the policy network output, and the standard deviation is a separately learned parameter. Best practices are that the actions initially have a zero mean distribution to get a good sampling of potential state-action trajectories, i.e., $\\pi(\\mathbf{a}_{i}|s)\\sim N(\\mathbf{0},\\sigma)$ . Below we show that an EGNN will initially have a non-zero mean distribution, which can cause problems in early training. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 Given a layer $l$ of an EGNN with randomly initialized weights, with the equivariant component input vector $\\pmb{u}_{i}^{l}\\ \\in\\ \\mathbb{R}^{n}$ , equivariant output vector $\\pmb{u}_{i}^{l+1^{-}}\\in\\ \\mathbb{R}^{n}$ and the multilayer perceptron $\\phi_{u}~:~\\mathbb{R}^{m}~\\mapsto~\\mathbb{R}_{\\!\\!\\!\\!\\!\\!\\!\\cdot}$ , where the equivariant component is updated as: $u_{i}^{l+1}\\;=\\;$ $\\begin{array}{r}{\\pmb{u}_{i}^{l}+C\\sum_{j\\neq i}\\left(\\pmb{u}_{i}^{l}-\\pmb{u}_{j}^{l}\\right)\\phi_{u}\\left(\\pmb{m}_{i j}\\right)}\\end{array}$ . Then the expected value of the output vector is approximately the expected value of the input vector: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\pmb{u}_{i}^{l+1}\\right]\\approx\\mathbb{E}\\left[\\pmb{u}_{i}^{l}\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Furthermore, given a full EGNN with $L$ layers then the expected value of the network output is approximately the expected value of the network input ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\pmb{u}_{i}^{L}\\right]\\approx\\mathbb{E}\\left[\\pmb{u}_{i}^{0}\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "(See appendix A for proof.) ", "page_idx": 3}, {"type": "text", "text": "Corollary 1.1 Given a policy for agent i represented by an EGNN and parameterized with a Gaussian distribution, and the equivariant component of the state $\\pmb{s}_{i}^{e q}$ the policy will have the following distribution: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{i}(\\pmb{a}_{i}|\\pmb{s})\\sim N(\\pmb{s}_{i}^{e q},\\sigma)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "(See Appendix A for proof.) ", "page_idx": 4}, {"type": "text", "text": "In many cases $s_{i}^{e q}$ is the agent\u2019s position. Corollary 1.1 indicates that an agent\u2019s action distribution will be skewed towards its own position. If the magnitude of the state representation ${\\pmb{s}}_{i}^{e q}$ is significantly larger than $\\sigma$ , the agent may output actions approximately equal to its position. We refer to this phenomenon, where agent actions are biased towards replicating the input state as the action, as the early exploration bias. Such bias is not conducive to effective exploration, potentially limiting the agent\u2019s ability to discover and converge to optimal solutions. ", "page_idx": 4}, {"type": "text", "text": "Examples of early exploration bias Suppose there is an agent placed at the position $\\pmb{u}_{i}=[2,0]$ , and its objective is to reach the origin (i.e. 0). The agent needs an action of less than 0 to move to the left. Since its action using the EGNN is close to Gaussian distributed with a mean of 2, then $P(X<0)\\approx0.02$ . If the agent moves 0.25 with each action, then the probability of the agent reaching the center objective in eight steps is $2.9\\mathrm{e}{-8}$ . On the other hand, the agent has a high likelihood of moving toward infinity as each move away increases the likelihood of moving towards infinity. This example shows that the bias leads to a low probability of finding the solution, which is especially harmful in sparse reward environments. ", "page_idx": 4}, {"type": "image", "img_path": "MQIET1VfoV/tmp/b6573ff691c85e04788270d4be48e1e07e7da5e02318cc628e842ee9b81cd61e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: An example of biased learning in MPE simple spread environment. Left: We observed the behavior of the EGNN agents in this early training phase. Each agent moved away from the origin due to the EGNN bias. Right: Note the very low reward in early training steps due to the biased policies moving away from the goals. ", "page_idx": 4}, {"type": "text", "text": "To confirm our hypothesis on EGNN early exploration bias, we conducted a simple experiment. We trained a Proximal Policy Optimization (PPO) [3] agent on the MPE simple-spread problem. This problem consists of three agents that seek to navigate to three goals cooperatively; the agents are rewarded based on the sum of the minimum distance from each target to an agent. We give further details of this set up and training procedure in the experiments section. Figure 3 shows the results. The EGNN policy biased the agents to move in the direction of their current position, causing them to move away from the origin and receive low rewards. Due to the dense rewards and simplicity of the problem, the EGNN agent was able to overcome this initial ", "page_idx": 4}, {"type": "text", "text": "bias and still solve the problem. However, in more complex problems this early bias could cause more problems. Even if an agent finds an optimal trajectory, it will be a relatively small proportion of the sampled trajectories and may result in sub-optimal convergence. ", "page_idx": 4}, {"type": "text", "text": "4.2 Exploration-Enhanced EGNNs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As discussed previously, EGNN\u2019s severe early exploration bias can decrease learning performance. In this section, we propose our solution to this problem in the form of Exploration-enhanced Equivariant Graph Neural Networks (E2GN2). To create E2GN2 we make the following modification to Equation 2 of the equivariant component of EGNN: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{u}_{i}^{l+1}=\\pmb{u}_{i}^{l}\\phi_{u_{2}}(\\pmb{m}_{i})+C\\sum_{j\\neq i}\\left(\\pmb{u}_{i}^{l}-\\pmb{u}_{j}^{l}\\right)\\phi_{u}\\left(\\pmb{m}_{i j}\\right))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\phi_{u_{2}}(\\pmb{m}_{i}):\\mathbb{R}^{m}\\rightarrow\\mathbb{R}$ is an MLP parameterized by $u_{2}$ . The remaining layer update equations of the EGNN remain the same. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 Given a $L$ layer E2GN2 with randomly initialized weights, where the equivariant component is updated as: $\\begin{array}{r}{\\pmb{u}_{i}^{l+1}=\\pmb{u}_{i}^{l}\\phi_{u2}(\\pmb{m}_{i})+C\\sum_{j\\neq i}\\left(\\pmb{u}_{i}^{l}-\\pmb{u}_{j}^{l}\\right)\\phi_{u}\\left(\\pmb{m}_{i j}\\right))}\\end{array}$ . Then the expected value of the output vector is approximately the expected value of the input vector: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\pmb{u}_{i}^{l+1}\\right]\\approx\\pmb{\\theta}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Corollary 2.1 Given a policy for agent $i$ represented by an E2GN2 and parameterized with a Gaussian distribution the policy will have the following distribution: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi_{i}(\\pmb{a}_{i}|\\pmb{s})\\sim N(\\pmb{\\theta},\\sigma)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thus we see that E2GN2 should have unbiased early exploration for the equivariant component of the actions. The primary difference between EGNN and E2GN2 is the addition of $\\phi_{u_{2}}$ , which serves to offset the bias from the previous layer (or input) $u_{i}^{l}$ To validate that this did indeed solve the exploration bias we tested E2GN2 on the same MPE simple spread environment. We observe in Figure 3 that E2GN2 did indeed have unbiased behavior when it came to early exploration, as the agents had smooth random actions, and the reward did not drastically decrease. ", "page_idx": 5}, {"type": "text", "text": "Analysis of E2GN2 Equivariance Here we verify that E2GN2 still retains EGNN\u2019s guarantee of equivariance to rotations and reflections. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3 E2GN2 is equivariant to transformations from the $O(n)$ group. In other words, it is equivariant to rotations and reflections. ", "page_idx": 5}, {"type": "text", "text": "(See appendix A for proof.) ", "page_idx": 5}, {"type": "text", "text": "Retaining our symmetries to rotations and reflections is important, since it should increase our sample efficiency and generalization. Note that E2GN2 is no longer equivariant to translations. However, in MARL translation equivariance is not necessarily a benefti. For example, consider the MPE problem with a translational equivariant policy. If we shift an agent to be 10 units right, this will add 10 to the action as well, causing it to move to the right! Essentially, this is adding an undesirable bias to the policy output: $\\pi(s+b)=\\pi(s)+b$ However, we can expect $O(n)$ policy equivariance to improve our sample efficiency in MARL, and it is key that E2GN2 retains this guarantee. ", "page_idx": 5}, {"type": "text", "text": "4.3 Adapting Architectures for Complex Action Spaces ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Applying EGNN/E2GN2 to sophisticated MARL problems requires careful consideration. Many MARL environments have discrete action spaces or mixed continuous-discrete action spaces. Some components of these action spaces may be invariant and others may be equivariant. Further complicating the problem, MARL requires the neural networks to output parameters for a probability distribution (which is sampled for use in exploration). For continuous actions, agents typically use a gaussian and for discrete actions agents generally use logits. Unfortunately, it is not straightforward to map the distinct invariant and equivariant coordinate embeddings onto a single distribution in a manner that preserves equivariance. ", "page_idx": 5}, {"type": "text", "text": "A core benefti of the GNN structure in MARL is the scalability afforded by permutation equivariance: the ability to handle a variable number of agents without retraining. For example, [25] demonstrates using a GNN to train $N$ agents, then to control $N+1$ agents without retraining. However, this example did not operate with discrete or mixed action spaces or equivariant structures as we do here. Improperly mapping GNN outputs to logits risks losing this scalability. ", "page_idx": 5}, {"type": "text", "text": "To address these issues, we propose leveraging the GNN\u2019s graph structure to output different components of the action space from different nodes in an equivariant manner: ", "page_idx": 5}, {"type": "text", "text": "\u2022 Discrete Actions & Invariant Components: the invariant feature embeddings $h_{i}$ of each agent\u2019s node are used to output discrete logits/actions and other invariant components. For action spaces with both equivariant and invariant components, this can be used as the \u2019action type selector\u2019 to select which type of action to apply at that type step (ie move action or targeting action).   \n\u2022 Continuous Spatial Actions: the equivariant coordinate embeddings $u_{i}$ of each agent\u2019s node are used for continuous spatial actions such as movement.   \n\u2022 Targeting Specific Entities: for multi-agent environments with entities beyond the learning agents (e.g., enemies), logits for discrete actions pertaining to each entity (e.g., which enemy to target) are output from that entity\u2019s corresponding node. This enables the discrete action space to scale with the number of agents. ", "page_idx": 5}, {"type": "image", "img_path": "MQIET1VfoV/tmp/fa23ebffdae2863893b1afde9459125826d170648ea70bcd4d6a953879f78f54.jpg", "img_caption": ["Figure 4: An example of using an Equivariant Graph Neural Network in MARL. Note that the state must be structured as a graph with each node having an equivariant $u_{i}$ and invariant $h_{i}$ component. As discussed in 4.3, the output of the policy uses $u_{i}$ for equivariant (typically spatial) actions, and the $h_{i}$ for invariant components of the actions "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Each of these components is parameterized as a distribution (ie logits or gaussian), resulting in one to three seperate distributions (depending on the environment and which components are used). After the exploration phase samples from each of these distributions, the final action can be constructed from each of these components via concatenation or the action selection component. ", "page_idx": 6}, {"type": "text", "text": "This formulation enables us to retain the permutation equivariance and scalability of a GNN. An alternative to our approach is to add an MLP at the end of the GNN to convert the GNN outputs to the mixed action spaces. This will lose the scalability/locality of the neural network. For example, if you add two more agents how do you modify the final MLP to expand the action space? In our formulation, whenever an new entity $i$ is added to the environment, we can simply add the node $N+1$ to the graph. The action space of the GNN will now be supplemented with $\\pm_{N+1}$ and $h_{N+1}$ , allowing us to expand the action space without retraining. ", "page_idx": 6}, {"type": "text", "text": "By structuring the GNN output in this manner, we can handle discrete or mixed discrete-continuous action spaces while retaining the equivariance of EGNN/E2GN2 and the flexibility of GNNs to a variable number of agents and entities. This approach allows MARL agents based on equivariant GNNs to be applied to challenging environments with minimal restrictions on the action space complexity. Indeed this approach was key to enabling the success of EGNN/E2GN2 on SMACv2. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We seek to answer the following questions with our experiments: (1) Do rotationally equivariant policy networks and rotationally invariant value networks improve training sample efficiency? (2) Does E2GN2 improve learning and performance over EGNN? (3) Does Equivariance indeed demonstrate improved generalization performance? ", "page_idx": 6}, {"type": "text", "text": "To address these questions, we use common MARL benchmarks: the multi-agent particle-world environment (MPE) [13] and Starcraft Multi-agent Challenge version 2 (SMACv2) [14]. Our experiments show that equivariance does indeed lead to improved sample efficiency, with E2GN2 in particular performing especially well. We also demonstrate that generalization is guaranteed across rotational transformations. ", "page_idx": 6}, {"type": "text", "text": "We want to focus our experiments on the neural networks\u2019 effects on MARL performance. To isolate the impact of the network architecture, we avoid using specialized tips and tricks sometimes employed in MARL [26]. This allows us to demonstrate that our proposed networks can improve performance without relying on these additional techniques. Thus we use a common standardized open source MARL training library RLlib [27]. We use the default multi-agent PPO algorithm, which does not use a centralized critic. We followed the basic hyperparameter tuning guidelines set forth in [26]. That is, we use a large training batch and mini-batch size, low numbers of SGD iterations, and a small clip rate. Further hyperparameter details are found in appendix B. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We compare our results with common neural networks used for MARL benchmarks: multi-layer perceptrons (MLP), and GNNs (we use a GNN structure similar to equations 1, 3, see appendix B for details ). We also compare with the approach from [17] which we refer to as E3AC in our plots. Recall that E3AC also uses neural networks that guarantee equivariance: SEGNN. We integrated E3AC into RLLIB to ensure the RL training procedure remained consistent across our comparisons. Note that the majority of MARL papers on SMAC/SMACv2 use MLPs as the base network. We also compare with GNNs to show the improvement is not primarily due to the graph structure. For the main paper results, we assume full observability since we have not explicitly tackled the partial observability problem yet. In future work, we will seek to remedy this. However, to be thorough we performed experiments with partial observability, resulting in surprising success using E2GN2 (see appendix C). ", "page_idx": 7}, {"type": "text", "text": "5.1 Training Environments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "From the MPE environment ([13]), we use two environments to benchmark our performance: cooperative navigation also known as spread, and predator-prey, also known as tag. The MPE tag environment consists of three RL-controlled agents seeking to catch a third evader agent. For easier comparison, we use a simple heuristic algorithm to control the evader agent. The evader simply moves in the opposite direction of the pursuers. The other MPE environment is the cooperative navigation or simple spread. In this environment, each agent seeks to minimize the distance between all obstacles. To better test equivariance, for the MPE environments we use continuous actions instead of the default discrete actions. ", "page_idx": 7}, {"type": "text", "text": "Next we test on SMACv2, a much more difficult environment than MPE. In SMACv2, the units are heterogenous with different capabilities (with different attack ranges, total health, and sometimes action spaces). The unit types are randomized at the beginning of the scenario. The actions include more components than simply movement (such as in MPE), agents can move and attack. The goals are more complex as well. Instead of simply navigating cooperatively as in MPE, the agents must learn attack formations and strategies. Sometimes it may be optimal to sacrifice health or allies in the purpose of the greater strategic objective. SMACv2 has three basic scenarios defined by the unit types: terran, protoss, and zerg. We use 5 agents for each team, and the initial position configuration termed \"surrounded\" (see fig 7) ", "page_idx": 7}, {"type": "text", "text": "The SMACv2 action space poses an interesting problem for GNN structures. A key advantage of GNNs is the permutation equivariance, which leads to scalability without retraining. The default SMACv2 agents will output simply a discrete action determining movement or target for shooting. For our purposes, we modify the action space to be a mixed action space. This consists of a continuous vector for movement actions, a discrete action determining the attack target, and a boolean determining if the agent should shoot, move or no-op (to be complete, we include plots with a discrete action space in appendix C). As discussed in section 4.3 this will both prove a greater test for our algorithms and allow for the GNN to scale to larger numbers of agents. ", "page_idx": 7}, {"type": "image", "img_path": "MQIET1VfoV/tmp/339dd3eea89e66d9c3f6bf0df68f0d2d5ed3feb819a27e05e73d165e3901e5c4.jpg", "img_caption": ["Figure 5: Comparing PPO learning performance on MPE with various Neural Networks. (TOP) reward as a function of environment steps. We show the standard errors computed across 10 seeds. (BOTTOM) reward as a function of wall clock time "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "MQIET1VfoV/tmp/31690c70932c0cd1330bf667b152fc737f19d7fba09316fed066f8f1603bca8c.jpg", "img_caption": ["Figure 6: Comparing performance of PPO on SMACv2 with various Neural Networks representing the policy and value function. Each chart represents a different race from the SMACv2 environment. We show the standard errors computed across 10 seeds. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2 Training Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present the results from our training in this section. The results for MPE in Figure 5 are averaged across 10 seeds. As discussed previously, EGNN has poor early performance due to the early exploration bias. Despite this poor exploration, EGNN outperforms GNNs and MLPs, demonstrating the power of equivariance in MARL. Similarly, we note that in MPE tag, E2GN2 results in a strong final agent, while EGNN suffers in the initial training phases. We see that E2GN2 is able to greatly outperform EGNN\u2019s final reward, partly due to superior early exploration. ", "page_idx": 8}, {"type": "text", "text": "In figure 5 we also compare the wall clock time required to train each algorithm. Note that these were all trained on the same hardware and using the same training algorithm (RLLIBs PPO). E3AC remains competitive in sample efficiency on MPE spread, but it requires nearly four hours to gather one million time steps, compared to less than one hour for E2GN2. This is likely due to the SEGNN [18] underlying structure used by E3AC, which is slower due to the be slower for inference time. We also note that E2GN2 does manage to outperform E3AC on MPE Tag. ", "page_idx": 8}, {"type": "text", "text": "Next, we review the results from SMACv2 in Figure 6; these results are averaged across 10 seeds. The equivariant networks have a clear improvmement in sample efficiency over the MLP and GNN. On the terran environment, EGNN once again learns slower in the initial phases, but due to equivariance, it outperforms MLP/GNN. For protoss, we note that EGNN performs well but has a high variance for its performance. E3AC struggles to perform well on SMACv2, likely since it was unable to resolve the hurdle we addressed in section 4.3. Instead for SMACv2 E3AC used a simple MLP for the policy and SEGNN for the value function [17]. The training speed and performance for E2GN2 is especially impressive since this environment is much more complex than the MPE. ", "page_idx": 8}, {"type": "text", "text": "5.3 Generalization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Thus far we have demonstrated that equivariance does indeed lead to improved sample efficiency. The next question to answer is whether equivariance leads to improved generalization. We test this by using the \u2019surrounded\u2019 initial configuration from SMACv2 shown in figure 7. By default, the agents are randomly generated in all directions (along specific axes). To test generalization, we only initialize agents on the left side ", "page_idx": 8}, {"type": "image", "img_path": "MQIET1VfoV/tmp/7e1d532b6fd7c4be0cf04ee3797d66d56525e292fd2ad7bdaa4a51b9401f9ee7.jpg", "img_caption": ["Figure 7: SMACv2 initialization schemes used for testing generalization "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "of the map (i.e., surrounded left). We then test to see if the agents are able to generalize when they are tested with initial positions starting on the right (called surrounded right), and with the default surrounded configuration (termed Surrounded All or Surrounded). Theoretically, the equivariant network should see equivalent performance between the training and testing initialization, due to the guarantee of rotational equivariance for movement, and invariance for shooting actions. ", "page_idx": 8}, {"type": "text", "text": "The results of our tests are shown in Table 1. As expected E2GN2 has stellar generalization performance. The win rate remains the same from the training configuration (Surrounded Left) to Surrounded Right. We see an increase in win rate when testing in Surrounded All. This is likely because this Surrounded All is an easier scenarion; the enemy is divided into more groups, so the agents can defeat the smaller groups one at a time. As we mentioned in the abstract, this table indicates that for the Surrounded Right generalization test E2GN2 has ${\\sim}5\\mathrm{x}$ the performance of GNN and MLP. ", "page_idx": 8}, {"type": "table", "img_path": "MQIET1VfoV/tmp/a84dd577baf08a2d7b66457e4dd785c88babaa55eb80609c0f1eeebb73929e69.jpg", "table_caption": ["Table 1: Generalization Win Rate in SMACv2. Note that E2GN2 retains high performance, while GNN and MLP lose performance when generalizing "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Next, in Table 2 we test E2GN2\u2019s ability to scale to various numbers of agents without retraining. We use the agents trained in SMACv2 from Figure 6 for these tests. We do not include the MLP agent since it cannot scale up without retraining. Note that the method described in section 4.3 was essential to enable scaling between agents. For example, if we passed the GNN output to a final MLP to map to the discrete action space this wouldn\u2019t be scalable to more agents. ", "page_idx": 9}, {"type": "text", "text": "The results for scalability confirm that both E2GN2 and GNN are able to maintain performance with larger numbers of agents. As the number of agents gradually increases the win rate does slowly decrease. E2GN2 does seem to have a slightly steeper loss in performance in the Zerg domain, which was also the most difficult domain for the initial training phase. We believe this is due to the higher complexity of the Zerg scenario, with certain unit types (banelings) that can explode to damage many other agents. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have demonstrated that E2GN2 merits strong consideration for many MARL applications. We addressed several important problems including the early exploration bias and how to apply it to complex action spaces. The sample-efficiency has dramatically improved, and there are now guarantees of generalization built into the network. There is still further work to do, such as handling the partial or incomplete symmetries. Furthermore this approach would need to be improved further to be applied to environments with angular momentum and accelerations, which are not included in these benchmarks. Currently, we expect that the improvement gained from E2GN2 in MARL will depend on the amount of rotational symmetry applicable in the observations. We believe that building on this work can be helpful to deploying MARL agents with a greater degree of trust (due to the guarantees). This could be helpful in many fields such as robotics, medicine, and power systems. In summation, the results of this paper provide a solid foundation upon which to build. ", "page_idx": 9}, {"type": "table", "img_path": "MQIET1VfoV/tmp/39407873b11438dacd88d6011f8c14ddbd2817862ac4253a1b007219b51d0405.jpg", "table_caption": ["Table 2: Generalization Win Rate: Testing RL agents ability to scale to different numbers of agents (originally trained with 5 agents) "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Kaiqing Zhang, Zhuoran Yang, and Tamer Ba\u00b8sar. Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms, pages 321\u2013384. Springer International Publishing, Cham, 2021.   \n[2] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks, 2 2021.   \n[3] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017.   \n[4] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for autonomous driving. ArXiv, abs/1610.03295, 2016.   \n[5] Pinxin Long, Tingxiang Fan, Xinyi Liao, Wenxi Liu, Hao Zhang, and Jia Pan. Towards optimally decentralized multi-robot collision avoidance via deep reinforcement learning. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 6252\u20136259, 2018.   \n[6] O. Vinyals, I. Babuschkin, and W.M. et al Czarnecki. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575, 2019.   \n[7] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140\u20131144, 2018.   \n[8] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, L. Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap, and David Silver. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588:604 \u2013 609, 2019.   \n[9] Esmat Samadi, Ali Badri, and Reza Ebrahimpour. Decentralized multi-agent based energy management of microgrid using reinforcement learning. International Journal of Electrical Power I& Energy Systems, 122:106211, 2020.   \n[10] Ziming Yan and Yan Xu. A multi-agent deep reinforcement learning method for cooperative load frequency control of a multi-area power system. IEEE Transactions on Power Systems, 35(6):4599\u20134608, 2020.   \n[11] Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generalization in reinforcement learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 1282\u20131289. PMLR, 09\u201315 Jun 2019.   \n[12] Andrew M. Pitts. Nominal Sets: Names and Symmetry in Computer Science. Cambridge Tracts in Theoretical Computer Science. Cambridge University Press, 2013.   \n[13] Ryan Lowe, YI WU, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[14] Benjamin Ellis, Jonathan Cook, Skander Moalla, Mikayel Samvelyan, Mingfei Sun, Anuj Mahajan, Jakob N. Foerster, and Shimon Whiteson. Smacv2: An improved benchmark for cooperative multi-agent reinforcement learning, 2023.   \n[15] Elise van der Pol, Daniel E. Worrall, Herke van Hoof, Frans A. Oliehoek, and Max Welling. Mdp homomorphic networks: Group symmetries in reinforcement learning. Advances in Neural Information Processing Systems, 2020-December, 6 2020.   \n[16] Elise Van Der Pol, Herke Van Hoof, Uva-Bosch Deltalab, Frans A Oliehoek, and Max Welling. Multi-agent mdp homomorphic networks, 10 2021.   \n[17] Dingyang Chen and Qi Zhang. e(3)-equivariant actor-critic methods for cooperative multi-agent reinforcement learning, 2024.   \n[18] Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J. Bekkers, and Max Welling. Geometric and physical quantities improve E(3) equivariant message passing. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.   \n[19] Xin Yu, Rongye Shi, Pu Feng, Yongkai Tian, Simin Li, Shuhao Liao, and Wenjun Wu. Leveraging partial symmetry for multi-agent reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):17583\u201317590, Mar. 2024.   \n[20] Dian Wang, Robin Walters, and Robert Platt. $\\{S O\\}(2)$ -equivariant reinforcement learning, 3 2022.   \n[21] Mario Geiger and Tess Smidt. e3nn: Euclidean neural networks, 7 2022.   \n[22] Marc Finzi, Max Welling, and Andrew Gordon Wilson. A practical method for constructing equivariant multilayer perceptrons for arbitrary matrix groups, 2021.   \n[23] Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. Morgan Kaufmann, San Francisco (CA), 1994.   \n[24] Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velic\u02c7kovic\u00b4. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges, 2021.   \n[25] Akshat Agarwal, Sumit Kumar, and Katia P. Sycara. Learning transferable cooperative behavior in multi-agent teams. CoRR, abs/1906.01202, 2019.   \n[26] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative, multi-agent games, 2022.   \n[27] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph E. Gonzalez, Michael I. Jordan, and Ion Stoica. RLlib: Abstractions for distributed reinforcement learning. In International Conference on Machine Learning (ICML), 2018.   \n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In 2015 IEEE International Conference on Computer Vision (ICCV), pages 1026\u20131034, 2015.   \n[29] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philip H. S. Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix and Supplemental Material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 EGNN Bias Proof ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Here we prove our assertion that the EGNN layers have biased outputs, and thus poor exploration. ", "page_idx": 12}, {"type": "text", "text": "We start by showing that $\\mathbb{E}\\left[\\phi_{u}(m_{i j})\\right]\\approx0$ . We do this by computing the expected value of a single neural network layer. ", "page_idx": 12}, {"type": "text", "text": "An individual neural network layer is defined as ${\\pmb y}=W{\\pmb x}+{\\pmb b}$ where $^{b}$ is a N-by-1 vector, $W$ is a N-by-M matrix and $\\textbf{\\em x}$ is a M-by-1 vector. Note that similar to [28] we initialize the elements in $W$ to be mutually independent and share the same distribution. $\\textbf{\\em x}$ is initialized similarly. and $\\textbf{\\em x}$ and $W$ are independent. Thus for each element (subscript $l$ ) of the equation, we have: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[y_{l}\\right]=\\sum\\mathbb{E}\\left[w_{l}x_{l}\\right]+\\mathbb{E}\\left[b_{l}\\right]=n_{l}\\mathbb{E}\\left[w_{l}x_{l}\\right]+\\mathbb{E}\\left[b_{l}\\right]=n_{l}\\mathbb{E}\\left[w_{l}\\right]\\mathbb{E}\\left[x_{l}\\right]+\\mathbb{E}\\left[b_{l}\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We can expect $w_{l}$ and $b_{l}$ to be zero mean, since many commonly used initializers use either uniform, or normal distribution, with zero mean [28]. This results in: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{\\mu=}n_{l}*0*\\mathbb{E}\\left[x_{l}\\right]+0=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, we can assume that for an individual layer the output is 0, since the final layer of any mlp is $0$ , we can see that $\\mathbb{E}\\left[\\phi_{u}(m_{i j})\\right]\\approx0$ ", "page_idx": 12}, {"type": "text", "text": "Next we show that E $\\left[{\\pmb u}_{i}^{L}\\right]\\approx\\mathbb{E}\\left[{\\pmb u}_{i}^{0}\\right]$ . Recall that $u_{i}^{L}$ is the $L$ th layer of ith node of the equivariant component of the network. We take the expectation over a sampling of the inputs $u_{i}$ and $h_{i}$ , treating each as a random variable. Note that $u_{i}$ and $u_{j}$ will have identical distributions, since each node will be sampled in the same manner. We start our proof by taking the expected value of the output: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[u_{i}^{l+1}\\right]=\\mathbb{E}\\left[u_{i}^{l}+C\\sum_{j\\neq i}\\left(u_{i}^{l}-u_{j}^{l}\\right)\\phi_{x}\\left(m_{i j}\\right)\\right]=\\mathbb{E}\\left[u_{i}^{l}\\right]+C\\sum_{j\\neq i}\\mathbb{E}\\left[\\left(u_{i}^{l}-u_{j}^{l}\\right)\\phi_{x}\\left(m_{i j}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n=\\mathbb{E}\\left[\\pmb{u}_{i}^{l}\\right]+C\\sum_{j\\neq i}\\mathbb{E}\\left[\\pmb{u}_{i}^{l}\\phi_{u}\\left(\\pmb{m}_{i j}\\right)\\right]-\\mathbb{E}\\left[\\pmb{u}_{j}^{l}\\phi_{u}\\left(\\pmb{m}_{i j}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\approx\\mathbb{E}\\left[\\pmb{u}_{i}^{l}\\right]+C\\sum_{j\\neq i}\\mathbb{E}\\left[\\pmb{u}_{i}^{0}\\right]\\mathbb{E}\\left[\\phi_{u}\\left(\\pmb{m}_{i j}\\right)\\right]-\\mathbb{E}\\left[\\pmb{u}_{j}^{0}\\right]\\mathbb{E}\\left[\\phi_{u}\\left(\\pmb{m}_{i j}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\approx\\mathbb{E}\\left[\\pmb{u}_{i}^{l}\\right]+C\\sum_{j\\neq i}\\pmb{u}_{i}^{l}0-\\pmb{u}_{j}^{l}0=\\pmb{u}_{i}^{l}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\pmb{u}_{i}^{l+1}\\right]\\approx\\mathbb{E}\\left[\\pmb{u}_{i}^{l}\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\boldsymbol{u}_{i}^{L}\\right]\\ldots\\approx\\mathbb{E}\\left[\\boldsymbol{u}_{i}^{1}\\right]\\approx\\mathbb{E}\\left[\\boldsymbol{u}_{i}^{0}\\right]\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Where we used the assumption that $\\mathbb{E}\\left[u_{j}^{l}\\phi_{u}\\left(m_{i j}\\right)\\right]\\approx\\mathbb{E}\\left[u_{j}^{l}\\right]\\mathbb{E}\\left[\\phi_{u}\\left(m_{i j}\\right)\\right]$ ", "page_idx": 12}, {"type": "text", "text": "The corollary: $\\pi_{i}(\\pmb{a}_{i}|\\pmb{s})\\sim N(\\pmb{s}_{i}^{e q},\\sigma)$ is a simple result from the fact that the equivariant action for an agent is the output of the coordinate embedding of the EGNN: $u_{i}^{L}$ . Here we assume the standard deviation is a separately trained parameter and not a function of the EGNN. If this is the case, then the policy mean $E\\left[\\pi(\\dot{\\pmb{a}_{i}}|s)\\right]=\\dot{\\mathbb{E}}\\left[\\pmb{{u}}_{i}^{L}\\right]\\approx\\mathbb{E}\\left[\\pmb{{u}}_{i}^{0}\\right]=\\mathbb{E}\\left[\\pmb{{s}}_{i}^{e q}\\right]$ (by definition) ", "page_idx": 12}, {"type": "text", "text": "A.2 Proof E2GN2 is Unbiased ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here we show that E2GN2 leads to unbiased early exploration. We begin with the equation for the E2GN2 layer: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\pmb{u}_{i}^{l+1}=\\pmb{u}_{i}^{l}\\phi_{u2}(\\pmb{m}_{i j})+C\\sum_{j\\neq i}\\left(\\pmb{u}_{i}^{l}-\\pmb{u}_{j}^{l}\\right)\\phi_{u}\\left(\\pmb{m}_{i j}\\right))\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Next we take the expected value of the output: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle E\\left[{\\pmb u}_{i}^{l+1}\\right]=\\mathbb{E}\\left[{\\pmb u}_{i}^{l+1}\\phi_{u2}({\\pmb m}_{i j})\\right]+C\\sum_{j\\neq i}\\mathbb{E}\\left[{\\pmb u}_{i}^{l}\\phi_{u}\\left({\\pmb m}_{i j}\\right)\\right]-\\mathbb{E}\\left[{\\pmb u}_{j}^{l}\\phi_{u}\\left({\\pmb m}_{i j}\\right)\\right]}}\\\\ {{\\displaystyle\\approx\\mathbb{E}\\left[{\\pmb u}_{i}^{l+1}\\right]\\mathbb{E}\\left[\\phi_{u2}({\\pmb m}_{i j})\\right]+C\\sum_{j\\neq i}\\mathbb{E}\\left[{\\pmb u}_{i}^{l}\\right]\\mathbb{E}\\left[\\phi_{u}\\left({\\pmb m}_{i j}\\right)\\right]-\\mathbb{E}\\left[{\\pmb u}_{j}^{l}\\right]\\mathbb{E}\\left[\\phi_{u}\\left({\\pmb m}_{i j}\\right)\\right]}}\\\\ {{\\displaystyle\\approx\\mathbb{E}\\left[{\\pmb u}_{i}^{l+1}\\right]\\pmb0+C\\sum_{j\\neq i}\\mathbb{E}\\left[{\\pmb u}_{i}^{l}\\right]\\pmb0-\\mathbb{E}\\left[{\\pmb u}_{j}^{l}\\right]\\pmb0=\\pmb0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus we see that for each layer $\\mathbb{E}\\left[\\pmb{u}_{i}^{l+1}\\right]\\approx\\mathbf{0}$ and therefore $\\mathbb{E}\\left[\\pmb{u}_{i}^{L}\\right]\\approx\\mathbf{0}$ . Similar to before, we used the assumption that Where we used the assumption that $\\mathbb{E}\\left[\\pmb{\\mathscr{u}}_{j}^{l}\\bar{\\phi_{u}}\\left(\\pmb{\\mathscr{m}}_{i j}\\right)\\right]\\approx\\mathbb{E}\\left[\\pmb{\\mathscr{u}}_{j}^{l}\\right]\\mathbb{E}\\left[\\phi_{u}\\left(\\pmb{\\mathscr{m}}_{i j}\\right)\\right]$ and $\\mathbb{E}\\left[\\pmb{u}_{j}^{l}\\phi_{u2}(\\pmb{m}_{i j})\\right]\\approx\\mathbb{E}\\left[\\pmb{u}_{j}^{l}\\right]\\mathbb{E}\\left[\\phi_{u2}(\\pmb{m}_{i j})\\right]$ ", "page_idx": 13}, {"type": "text", "text": "Similar to the first corollary the result: $\\pi_{i}(\\mathbf{a}_{i}|s)\\sim N(\\mathbf{0},\\sigma)$ follows mostly from the definition and the above proof. The corollary $\\pi_{i}(\\mathbf{a}_{i}|s)\\sim N(\\mathbf{0},\\sigma)$ is a simple result from the fact that the equivariant action for an agent is the output of the coordinate embedding of the EGNN: $u_{i}^{L}$ . Here we assume the standard deviation is a separately trained parameter and not a function of the EGNN. If this is the case, then the policy mean $E\\left[\\pi(\\stackrel{.}{\\mathbf{a}}_{i}|s)\\right]=\\dot{\\mathbb{E}}\\left[\\pmb{u}_{i}^{L}\\right]\\approx\\pmb{0}$ (by definition) ", "page_idx": 13}, {"type": "text", "text": "A.3 Equivariance and Invariance of E2GN2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We follow the structure of [2] for how to show equivariance. Note that [2] showed that $\\phi_{u}\\left(m_{i j}\\right)$ will be invariant to $E(n)$ transformations. This should still hold in our case as we make no modifications to $m_{i j}$ , similarly $\\phi_{u2}\\left(m_{i j}\\right)$ will be invariant to translations. To show equivariance to rotations and reflections we show that applying a transformation $T$ to the input, results in a tranformation to the output. That is: $f(\\mathbf{T}\\mathbf{u}_{i}^{l})=\\dot{T}\\dot{\\mathbf{u}}_{i}^{l}$ where $f(.)$ is the update equation of E2GN2: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{T u_{i}^{l}\\phi_{u2}(m_{i})+C\\displaystyle\\sum_{j\\neq i}\\left(T u_{i}^{l}-T u_{j}^{l}\\right)\\phi_{u}\\left(m_{i j}\\right)}}\\\\ {{\\displaystyle=T u_{i}^{l}\\phi_{u2}(m_{i})+T C\\displaystyle\\sum_{j\\neq i}\\left(u_{i}^{l}-u_{j}^{l}\\right)\\phi_{u}\\left(m_{i j}\\right)}}\\\\ {{\\displaystyle=T u_{i}^{l}\\phi_{u2}(m_{i})+T C\\displaystyle\\sum_{j\\neq i}\\left(u_{i}^{l}-u_{j}^{l}\\right)\\phi_{u}\\left(m_{i j}\\right)}}\\\\ {{\\displaystyle=T\\left(u_{i}^{l}\\phi_{u2}^{-}(m_{i})+C\\displaystyle\\sum_{j\\neq i}\\left(u_{i}^{l}-u_{j}^{l}\\right)\\phi_{u}\\left(m_{i j}\\right)\\right)=T u_{i}^{l+1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since there are no modifications to E2GN2 update equation for $h_{i}$ , then E2GN2 should retain invariance to transformations from $E(n)$ . More precisely (using the fact that the initial $\\pmb{m}_{i j}$ is invariant to translations from [2]), if we translate the input $\\pmb{u}_{i}$ by $^{b}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n(\\pmb{u}_{i}^{l}+\\pmb{b})\\phi_{u2}(\\pmb{m}_{i})+C\\sum_{j\\neq i}\\left((\\pmb{u}_{i}^{l}+\\pmb{b})-(\\pmb{u}_{j}^{l}+\\pmb{b})\\right)\\phi_{u}\\left(\\pmb{m}_{i j}\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n=b\\phi_{u2}(m_{i})+u_{i}^{l}\\phi_{u2}(m_{i})+C\\sum_{j\\neq i}\\left(u_{i}^{l}-u_{j}^{l}\\right)\\phi_{u}\\left(m_{i j}\\right)=b\\phi_{u2}(m_{i})+u_{i}^{l+1}\\neq b+u_{i}^{l+1}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This shows that E2GN2 is not translation equivariant. ", "page_idx": 13}, {"type": "text", "text": "B Additional Training Details ", "text_level": 1, "page_idx": 14}, {"type": "table", "img_path": "MQIET1VfoV/tmp/be277ef80818a77940f950e1521e463873a6728a821768e43779f46b46e44eac.jpg", "table_caption": [], "table_footnote": ["Table 3: PPO Hyperparameters for SMACv2 "], "page_idx": 14}, {"type": "table", "img_path": "MQIET1VfoV/tmp/bd9c6fffc884508316911712254ebc90bd9dd867457344637621e1e1d5894f52.jpg", "table_caption": [], "table_footnote": ["Table 4: hyperparameters for MPE "], "page_idx": 14}, {"type": "text", "text": "MLP uses two hidden layers of 64 width each for MPE and 128 for SMACv2. All MLPs in the GNNs use 2 layers with a width of 32. For both MPE and GNN structures we use separate networks for the policy and value functions. ", "page_idx": 14}, {"type": "text", "text": "Graph Structure The graph structure for MPE environments is set as a complete graph. The graph structure for SMACv2 is complete among the agent controlled units (or all of the friendly nodes), and each friendly node alse has an edge to each of the enemy controlled units. This was an attempt to model what we imagine a real world scenario to look like (agents see the enemies, communicate with allies, but do not see the enemies communication signals) ", "page_idx": 14}, {"type": "text", "text": "Graph Inputs For MPE environments the input invariant feature for each node $h_{i}^{0}$ is the id (pursuer, evader, or landmark). For SMACv2, $h_{i}^{0}$ is made up of the features: health, shield unit type, and team. For each $\\pmb{u}_{i}^{0}$ is set as the position of node $i$ . For MPE there is also a velocity feature, which we incoporate following the procedure described in [2] ", "page_idx": 14}, {"type": "text", "text": "Graph Outputs for Value function Similar to the visual in section 4.3, the value function output comes from the invariant component of the agent\u2019s node of final layer of the EGNN/E2GN2. In other words the value function is: $\\bar{h_{i}^{L}}$ where is $L$ the final layer, and $i$ is the agent in making the decision. ", "page_idx": 14}, {"type": "text", "text": "Graph Outputs for Policy For MPE environments the actions are forces in a particular direction, thus the action output for agent $i$ is $u_{i}^{L}$ where $L$ is the final layer. ", "page_idx": 14}, {"type": "text", "text": "Section 4.3 discussed applying EGNN to more sophisticated action spaces. Specifically for SMACv2 the discrete action space has an equivariant component (movements) and an invariant component (shoot). The outputs for the equivariant component of EGNN/E2GN2 are continuous values, mapping these values to logits is not straightforward. Furthermore, they would need to be mapped onto the same distribution of logits being represented by the shoot commands. We solve this problem by having three distributions output by the GNN structure: the continuous/equivariant movement gaussian distribution, the discrete/invariant distribution, and a third distribution that determines whether we should move or shoot. After the RL sampling from each distribution is performed for the actions, then the three components of the action can be converted to the final action for either a mixed discrete-continuous action space or a discrete action space. ", "page_idx": 14}, {"type": "text", "text": "Graph Neural Network details We used the same GNN as the EGNN paper: Specifically, we use equations 1 and 3, but update equation 1 to be $m_{i j}=\\phi(h_{i},h_{j},x_{i},x_{j})$ . For convenience we rewrite the GNN equations describing a single layer here: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{m}_{i j}=\\phi_{e}\\left({h}_{i}^{l},{h}_{j}^{l}\\right),\\quad{m}_{i}=\\sum_{j\\neq i}{{m}_{i j}},\\quad{h}_{i}^{l+1}=\\phi_{h}\\left({h}_{i}^{l},{{m}_{i}}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Recall that $\\phi_{h}$ and $\\phi_{e}$ are two layer MLPs. Note that the GNN only has one variable $h_{i}$ per node (compared to the two variables of $h_{i}$ and $\\pmb{u}_{i}$ of EGNN/E2GN2). The features inputs are similar to an EGNN but with the two invariant/equivariant components concatenated together. The output for GNN will all come from the variable $^h$ as there is no need to distinguish between equivariant and invariant outputs (the action may be composed across several nodes, see above and figure 4). ", "page_idx": 15}, {"type": "text", "text": "Hardware For training hardware, we trained the graph-structured networks using various GPUs. Many were trained on an A100, but that is certainly not necessary, they didn\u2019t need that much space. MLPs were trained on CPUs. We typically used 4 rollout workers across 4 CPU threads, so each training run used 5 CPU threads. ", "page_idx": 15}, {"type": "text", "text": "Further Notes/details on SMACv2 SMACv2 ([14]) is a modification of the original SMAC ([29]). Since SMAC had deterministic initialization and static scenarios it turns out policies could memorize the solution. In each scenario, there is a pre-specified number of agents on each team (we use 5 agents for each team). In each scenario, the agents are randomly initialized in a specific formation. For this work, we use the initial position configuration termed \"surrounded\" (see fig 7). Finally, it is important to note that SMACv2 will randomly sample unit types from 3 different units (for each scenario). We use the same policy network for each agent but have the unit type as an input in the observation space, so each policy must learn to condition its behavior on the unit type. ", "page_idx": 15}, {"type": "text", "text": "C Supplementary Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Below we include supplementary results we believe interesting to understanding further E2GN2\u2019s performance. ", "page_idx": 15}, {"type": "image", "img_path": "MQIET1VfoV/tmp/e648ae2e0dc9688a8c9bafc0c064e7d5d17ebdd121424f0d8e870a590dec2a99.jpg", "img_caption": ["Figure 8: These results are using SMACv2 with the \u2019surrounded\u2019 initialization and 5 agents. Specifically, here we map the actions from the mixed discrete-continuous actions back to the default SMACv2 discrete action space. This is done by simply rounding the continuous actions to the nearest axis. These results are using the standard SMACv2 action input. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "MQIET1VfoV/tmp/ecfd9477598f0e1064aff56bb988c9c65b4b92e9dfd910fc075c49b4f880a323.jpg", "img_caption": ["Figure 9: These results are using SMACv2 with the \u2019surrounded\u2019 initialization and 5 agents. Specifically, the observations here are partially observable. Note that E2GN2 still performs well with partial observability. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "MQIET1VfoV/tmp/c9fd00232f8b7571f21be0f89a42ea09ecf2bf59ca94e8dc3492fd60503c77af.jpg", "img_caption": ["Figure 10: These results are using SMACv2 with the \u2019surrounded\u2019 initialization and 5 agents. Specifically, the observations here are partially observable. In all other experiments we recenter the states at 0 (standard in MARL). Here we did perform the recentering. Note how important this step is for EGNN. Without the centering of observations the EGNN is assuming rotational equivariance around the wrong center. Interestingly, this seems to have little impact on E2GN2\u2019s performance. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "MQIET1VfoV/tmp/3c86aaa19828cdebb744022a54e51c0a431fa68af7aa73c384825e2adba5a5e0.jpg", "img_caption": ["Figure 11: These were trained on SMACv2 using the \u2019surrounded\u2019 initialization with 5 agents. We were curious how an MLP would perform using the mixed continuous/discrete action space vs all discrete. Note that our E2GN2 is still able to outperform both. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "MQIET1VfoV/tmp/420275036dba719c74813afc16c4b191894e901066fd283fa542b266d293aaca.jpg", "img_caption": ["Figure 12: SMACv2 has various initial position configurations. Here we show the results using the \u2019Surrounded and Reflect\u2019 initial configuration. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "MQIET1VfoV/tmp/b7dab3d20c21dda6745831cb056a7220d9a63148c0a0109714dbe5ce5fa923d7.jpg", "img_caption": ["Figure 13: SMACv2, Surrounded and Reflect, 5 agents. Here we dive deeper into the performance when trained on individual unit types (instead of a mix of unit types). Top Row: Only Marine, stalker or Zergling units Bottom Row: Only Marauder, Zealot or hydralisk unit "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "MQIET1VfoV/tmp/46d848399d8d8399f5dc9f337a632b767a08ac2ca85578d85d82cb6cf1e0757b.jpg", "table_caption": ["Table 5: Generalization Win Rate: Training on 5 agents in Surrounded-Left configuration, Testing on 7 agents in all configurations "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We believe that our charts and tables reflect the claims made in the abstract and introduction. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Yes, we discuss these briefly in the conclusion and results. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Yes we provide our proofs and rationale in the appendix ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We specifically used an open-source MARL library (RLlib) for our RL training. The code for EGNN is available online. We described any modifications we made to the environments, as well as hyperparameters and training details. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: Much of the necessary code is available online. Part of this work was developed for a private company, and permission for the full code release was not given. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, we mention that we selected hyperparameters following guidelines set by [26], and included the hyperparameters used, the unspecified ones were simply the RLlib default parameters. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The charts all had confidence intervals (95 percent), computed using the standard seaborn package. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We describe this in the appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, we believe this work adheres to the code of ethics. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We discuss this in the conclusion. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We did not release data or code. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We cited the code library used. RLlib uses an Apache license 2.0 Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: There were no new assets introduced. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: There were no human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: No human subjects in this work. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]