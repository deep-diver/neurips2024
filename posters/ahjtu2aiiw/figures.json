[{"figure_path": "AhjTu2aiiW/figures/figures_2_1.jpg", "caption": "Figure 1: First-Explore aims to maximize the cumulative reward of a sequence of n episodes on a target environment distribution. This optimization is achieved by first training two separate policies, and then combining them after training to maximize the total reward obtained. A. First, two separate policies are trained on the distribution of environments: one to explore (produce informative episodes), and one to exploit (maximize current episode return). During training, the explore policy Texplore provides all the context Ci = T1, ..., Ti\u00bf for both policies. This flow of context is visualized by solid arrows \u2192. The exploit policy Texploit takes a context of episodes, and produces a single episode of exploitation. The return of this exploit episode is then used to train both policies, with the feedback to the explore policy visualized by the dotted green arrows ....>. B. After the two policies are trained, different combinations of them are evaluated to find the combination that maximizes total reward. Each combination involves first exploring for k episodes, and then repeatedly exploiting for the remaining n \u2212 k episodes. C. The best combination is then used at inference time: exploring for a fixed number of episodes on new environments, and then exploiting for the remaining episodes.", "description": "This figure illustrates the First-Explore algorithm's three phases: policy training, k-selection, and final policy.  In the training phase, two separate policies (explore and exploit) are trained using context from previous episodes.  The k-selection phase determines the optimal balance between exploration and exploitation by testing different numbers of initial exploration episodes. Finally, the best combination of exploration and exploitation episodes is used for inference.", "section": "4 First-Explore"}, {"figure_path": "AhjTu2aiiW/figures/figures_6_1.jpg", "caption": "Figure 6: Alternative Bandits-with-One-Fixed-Arm Plots with Mean \u00b1 Standard Deviation.", "description": "This figure is an alternative visualization of Figure 2, showing the mean cumulative and average pull rewards with standard deviation across multiple runs for both the deceptive (\u03bc\u2081 = 0.5) and non-deceptive (\u03bc\u2081 = 0) cases.  It displays the performance of First-Explore, UCB, Thompson Sampling, RL2, the oracle (always choosing the best arm), and random action selection. The standard deviations help illustrate the variability in performance across different bandit instances.", "section": "C Detailed Domains"}, {"figure_path": "AhjTu2aiiW/figures/figures_7_1.jpg", "caption": "Figure 3: Mean performance (averaged across sampled treasure rooms) of algorithms for deceptive (left) and non-deceptive (right) versions of the Dark Treasure Room domain. Each method trained 5 independent times, and each such run is plotted individually. The top figures plot the cumulative reward obtained against step and episode number, while the bottom figures provide a proxy for exploration by plotting the number of times agents move against the same. When the domain is deceptive, the cumulative-reward meta-RL methods, RL2 (fuchsia), HyperX (brown), and VariBAD (purple) achieve low total-reward, as the policies learnt to minimize exploration. In contrast, First-Explore (green) performs well on both the deceptive and non-deceptive domains.", "description": "The figure shows the performance comparison between First-Explore and other meta-RL algorithms (RL2, VariBAD, HyperX) on the Dark Treasure Room environment, both in deceptive (p = -4) and non-deceptive (p = 0) settings.  The plots illustrate cumulative reward over time, along with a measure of exploration (total moves made).  It highlights First-Explore's superior performance, particularly in the deceptive case where other methods fail due to their avoidance of exploration.", "section": "Results"}, {"figure_path": "AhjTu2aiiW/figures/figures_8_1.jpg", "caption": "Figure 3: Mean performance (averaged across sampled treasure rooms) of algorithms for deceptive (left) and non-deceptive (right) versions of the Dark Treasure Room domain. Each method trained 5 independent times, and each such run is plotted individually. The top figures plot the cumulative reward obtained against step and episode number, while the bottom figures provide a proxy for exploration by plotting the number of times agents move against the same. When the domain is deceptive, the cumulative-reward meta-RL methods, RL2 (fuchsia), HyperX (brown), and VariBAD (purple) achieve low total-reward, as the policies learnt to minimize exploration. In contrast, First-Explore (green) performs well on both the deceptive and non-deceptive domains.", "description": "The figure compares the performance of First-Explore and three other meta-RL algorithms (RL2, HyperX, VariBAD) on the Dark Treasure Room domain under deceptive and non-deceptive conditions.  It shows that First-Explore significantly outperforms the other algorithms in the deceptive condition where early exploration requires sacrificing immediate reward. The plots illustrate cumulative reward, episode reward, and the number of times the agent moves, highlighting First-Explore's ability to balance exploration and exploitation effectively.", "section": "Results"}, {"figure_path": "AhjTu2aiiW/figures/figures_8_2.jpg", "caption": "Figure 4: Left: Raw agent observations from a sampled ray maze converted to an image. The agent receives the wall distances and the wall types. Portraying this numerical data as an image, goal locations are green, and the two wall orientations are distinguished (east-west teal, and north-south navy). To aid the eye, the floor has been coloured in dark purple, and the sky yellow. Although the goal is visible, it could be a treasure (positive reward) or trap (negative reward). Right: The image produced with direct ray casting (large number of processed lidar measurements) rather than the 15 the agent receives.", "description": "The figure shows a comparison between the raw lidar observations (15 rays) received by the agent in the Ray Maze environment and a complete image of the environment generated using all available lidar data (201 rays). The raw observations are converted into an image where goal locations are shown in green, east-west walls in teal, north-south walls in navy, the floor in dark purple, and the sky in yellow.  This demonstrates the limited perceptual capability of the agent, which must navigate the maze based only on the sparse sensor data.", "section": "Ray Maze"}, {"figure_path": "AhjTu2aiiW/figures/figures_15_1.jpg", "caption": "Figure 2: Mean performance (averaged across sampled bandits) of algorithms for deceptive (left) and non-deceptive (right) versions of the bandit domain. Each method trained 5 independent times, and each such run is plotted individually, so as to faithfully represent the variance between runs (e.g., that multiple of the bandit-domain RL2 training runs achieve exactly the same reward). Appendix C provides alternative plots with mean reward \u00b1 standard deviation. The top figures plot the cumulative reward against the number of arm pulls, while the bottom figures illustrate the reward dynamics by plotting the individual pull rewards against the same. When the domain is deceptive, the cumulative-reward meta-RL method, RL2 (fuchsia), performs extremely poorly, despite the deceptive domain giving strictly higher rewards than the non-deceptive version. In contrast, First-Explore (green) impressively outperforms UCB (purple) and Thompson Sampling (orange) despite them being specialized bandit algorithms, in both the deceptive and non-deceptive settings, with p < 10-5.", "description": "The figure compares the performance of First-Explore and other algorithms (including specialized bandit algorithms) on deceptive and non-deceptive bandit tasks. It shows that First-Explore significantly outperforms other methods, especially in the deceptive case where early exploration sacrifices immediate reward for higher long-term returns.", "section": "Results"}, {"figure_path": "AhjTu2aiiW/figures/figures_16_1.jpg", "caption": "Figure 3: Mean performance (averaged across sampled treasure rooms) of algorithms for deceptive (left) and non-deceptive (right) versions of the Dark Treasure Room domain. Each method trained 5 independent times, and each such run is plotted individually. The top figures plot the cumulative reward obtained against step and episode number, while the bottom figures provide a proxy for exploration by plotting the number of times agents move against the same. When the domain is deceptive, the cumulative-reward meta-RL methods, RL2 (fuchsia), HyperX (brown), and VariBAD (purple) achieve low total-reward, as the policies learnt to minimize exploration. In contrast, First-Explore (green) performs well on both the deceptive and non-deceptive domains.", "description": "This figure compares the performance of First-Explore and several other meta-RL algorithms on the Dark Treasure Room environment under deceptive and non-deceptive conditions. It shows that First-Explore significantly outperforms the other methods in the deceptive case (where exploration requires sacrificing immediate rewards), demonstrating its ability to learn effective exploration strategies despite this challenge. In the non-deceptive case, First-Explore still performs well, although the performance differences are less pronounced.", "section": "Results"}, {"figure_path": "AhjTu2aiiW/figures/figures_17_1.jpg", "caption": "Figure 8: Behavior of First-Explore and the meta-RL controls on the 10 Horizon Dark Treasure Room for various values of p.", "description": "This figure shows the performance of First-Explore and three other meta-reinforcement learning algorithms (RL2, VariBAD, and HyperX) across different values of the parameter 'p' in the 10-horizon Dark Treasure Room environment.  The x-axis represents the episode number, and the y-axis represents the cumulative reward across episodes.  Different values of 'p' represent different levels of difficulty in the environment, with lower values being more difficult because they require the agent to sacrifice immediate reward early in the episode sequence to achieve higher rewards later. This figure demonstrates First-Explore's ability to achieve significantly better performance than other algorithms, especially in challenging environments (lower p-values).", "section": "C.2.1 The p Domain Dynamics"}, {"figure_path": "AhjTu2aiiW/figures/figures_19_1.jpg", "caption": "Figure 3: Mean performance (averaged across sampled treasure rooms) of algorithms for deceptive (left) and non-deceptive (right) versions of the Dark Treasure Room domain. Each method trained 5 independent times, and each such run is plotted individually. The top figures plot the cumulative reward obtained against step and episode number, while the bottom figures provide a proxy for exploration by plotting the number of times agents move against the same. When the domain is deceptive, the cumulative-reward meta-RL methods, RL2 (fuchsia), HyperX (brown), and VariBAD (purple) achieve low total-reward, as the policies learnt to minimize exploration. In contrast, First-Explore (green) performs well on both the deceptive and non-deceptive domains.", "description": "This figure compares the performance of First-Explore against three other meta-RL algorithms (RL2, HyperX, and VariBAD) and random actions on two versions of the Dark Treasure Room environment: deceptive (p = -4) and non-deceptive (p = 0).  The top row shows cumulative reward over episodes and steps, and the bottom row shows the number of times agents move, indicating exploration.  In the deceptive setting, where forgoing immediate rewards is necessary for optimal long-term performance, First-Explore outperforms the other algorithms. However, in the non-deceptive setting, all algorithms perform well. The results highlight First-Explore's ability to succeed in scenarios where other methods fail due to an inability to balance exploration and exploitation.", "section": "Results"}, {"figure_path": "AhjTu2aiiW/figures/figures_23_1.jpg", "caption": "Figure 10: Regardless of training time, the meta-RL controls perform poorly.", "description": "This figure shows the average cumulative reward achieved by RL2, VariBAD, and HyperX across multiple training runs with varying lengths for the deceptive Dark Treasure Room environment.  The top panel displays the performance of all three algorithms across 100% of the training progress.  The bottom panel shows the effect of changing the number of episode steps used during HyperX training (2e6, 2e7, and 2e8). The results demonstrate that even with significantly more training, the algorithms still fail to achieve satisfactory cumulative reward, highlighting a limitation of optimizing directly for cumulative reward in environments requiring reward-sacrificing exploration.  The consistent low performance across different training lengths for HyperX further suggests that the algorithm's exploration bonus schedule, rather than insufficient training, is the primary factor contributing to its poor performance.", "section": "Poor Performance Regardless of Train Time:"}, {"figure_path": "AhjTu2aiiW/figures/figures_24_1.jpg", "caption": "Figure 11: Well-planned sequential exploration can sometimes significantly outperform a sequence of optimal myopic explorations. For example, consider exploring a plain over four days, where each day one must explore by walking from the plain's center. Sequence of Optimal Myopic Explorations one \u2018optimal\u2019 way of exploring is to perform a spiral from the center (e.g., the red spiral on the right). This strategy achieves the optimal amount of exploration on day 1 as one never retraces one\u2019s steps. However, if one does a spiral on day 1 then on day 2, one must retread old ground - wasting time otherwise spent exploring new locations. Each day bee-lining to unseen areas and then spiralling from there is also optimal for that day, however it increases the amount of retreading tomorrow. Optimal Sequence of Explorations: another optimal way of exploring on day 1 is to explore a quadrant, visualized in red on the left. Again, as one does not backtrack, this strategy is optimal on day 1. However, unlike the spiral strategy, this strategy is also part of an optimal sequence of four explorations, as one can explore a new quadrant each of the four days, without ever retreading the same ground.", "description": "This figure illustrates the difference between a sequence of myopically optimal explorations and an optimal sequence of explorations.  A myopic strategy prioritizes immediate reward, leading to suboptimal long-term exploration. In contrast, an optimal sequence plans exploration steps to maximize overall exploration efficiency, even if it means sacrificing immediate gains in some steps.", "section": "G Myopic Exploration"}, {"figure_path": "AhjTu2aiiW/figures/figures_24_2.jpg", "caption": "Figure 12: Demonstration of First-Explore's k-selection phase, for the bandit distribution, with \u03bc\u2081 = 0. Five separate First-Explore runs are plotted. The training runs select different values of k (due to the relative strengths of each runs explore and exploit policy), with the associated selected k corresponding to the peak of each curve (marked by a cross).", "description": "This figure shows the results of the k-selection phase of the First-Explore algorithm. The x-axis represents the number of exploration episodes (k), and the y-axis represents the total meta-rollout return.  Five different training runs are shown, each with a different peak return, indicating the algorithm's sensitivity to the balance between exploration and exploitation.  The selected k value for each run is the one that maximizes the return, shown by the cross on each curve.", "section": "H K-Selection Phase"}, {"figure_path": "AhjTu2aiiW/figures/figures_27_1.jpg", "caption": "Figure 13: A visualization of the dark treasure-room. The agent's position is visualized by the blue square, positive rewards are in green, and negative rewards are in red, with the magnitude of reward being visualized by the colour intensity. When the agent enters a reward location it consumes the reward, and for that timestep is visualized as having the additive mixture of the two colours.", "description": "This figure shows a sample visualization of the Dark Treasure Room environment.  The agent's current position is a blue square.  Green squares represent positive rewards, and red squares represent negative rewards, with intensity representing magnitude. When the agent lands on a square, the color blends to reflect both the agent and reward colors. ", "section": "L Dark Treasure-Room Visualizations"}, {"figure_path": "AhjTu2aiiW/figures/figures_28_1.jpg", "caption": "Figure 1: First-Explore aims to maximize the cumulative reward of a sequence of n episodes on a target environment distribution. This optimization is achieved by first training two separate policies, and then combining them after training to maximize the total reward obtained. A. First, two separate policies are trained on the distribution of environments: one to explore (produce informative episodes), and one to exploit (maximize current episode return). During training, the explore policy Texplore provides all the context Ci = T1, ..., Ti\u00bf for both policies. This flow of context is visualized by solid arrows \u2192. The exploit policy exploit takes a context of episodes, and produces a single episode of exploitation. The return of this exploit episode is then used to train both policies, with the feedback to the explore policy visualized by the dotted green arrows ....>. B. After the two policies are trained, different combinations of them are evaluated to find the combination that maximizes total reward. Each combination involves first exploring for k episodes, and then repeatedly exploiting for the remaining n-k episodes. C. The best combination is then used at inference time: exploring for a fixed number of episodes on new environments, and then exploiting for the remaining episodes.", "description": "The figure illustrates the First-Explore algorithm's workflow. It consists of three phases: Policy Training, k-Selection, and Final Policy.  In the Policy Training phase, two separate policies are trained \u2013 one for exploration and one for exploitation. The exploration policy focuses on generating informative episodes, while the exploitation policy aims to maximize immediate reward.  During training, the exploration policy provides context (past episodes) to both policies. In the k-Selection phase, different combinations of the trained policies are evaluated to determine the optimal balance between exploration and exploitation (number of initial exploration episodes *k* before switching to exploitation).  The Final Policy phase uses the best combination from the k-Selection phase for inference (new environments): exploring for *k* episodes and then exploiting for the rest.", "section": "4 First-Explore"}, {"figure_path": "AhjTu2aiiW/figures/figures_29_1.jpg", "caption": "Figure 1: First-Explore aims to maximize the cumulative reward of a sequence of n episodes on a target environment distribution. This optimization is achieved by first training two separate policies, and then combining them after training to maximize the total reward obtained. A. First, two separate policies are trained on the distribution of environments: one to explore (produce informative episodes), and one to exploit (maximize current episode return). During training, the explore policy Texplore provides all the context Ci = T1, ..., Ti\u22121 for both policies. This flow of context is visualized by solid arrows \u2192. The exploit policy \u03c0exploit takes a context of episodes, and produces a single episode of exploitation. The return of this exploit episode is then used to train both policies, with the feedback to the explore policy visualized by the dotted green arrows ....>. B. After the two policies are trained, different combinations of them are evaluated to find the combination that maximizes total reward. Each combination involves first exploring for k episodes, and then repeatedly exploiting for the remaining n \u2212 k episodes. C. The best combination is then used at inference time: exploring for a fixed number of episodes on new environments, and then exploiting for the remaining episodes.", "description": "This figure illustrates the First-Explore algorithm.  It shows how two separate policies, one for exploration and one for exploitation, are trained and then combined to maximize cumulative reward across multiple episodes. The exploration policy informs the exploitation policy by providing contextual information from previous episodes, and the success of the exploitation policy provides feedback for improving the exploration policy.  Finally, the algorithm determines the optimal balance between exploration and exploitation by evaluating different combinations of the two policies.", "section": "4 First-Explore"}, {"figure_path": "AhjTu2aiiW/figures/figures_30_1.jpg", "caption": "Figure 1: First-Explore aims to maximize the cumulative reward of a sequence of n episodes on a target environment distribution. This optimization is achieved by first training two separate policies, and then combining them after training to maximize the total reward obtained. A. First, two separate policies are trained on the distribution of environments: one to explore (produce informative episodes), and one to exploit (maximize current episode return). During training, the explore policy Texplore provides all the context Ci = T1, ..., Ti\u00bf for both policies. This flow of context is visualized by solid arrows \u2192. The exploit policy Texploit takes a context of episodes, and produces a single episode of exploitation. The return of this exploit episode is then used to train both policies, with the feedback to the explore policy visualized by the dotted green arrows ....>. B. After the two policies are trained, different combinations of them are evaluated to find the combination that maximizes total reward. Each combination involves first exploring for k episodes, and then repeatedly exploiting for the remaining n k episodes. C. The best combination is then used at inference time: exploring for a fixed number of episodes on new environments, and then exploiting for the remaining episodes.", "description": "This figure illustrates the First-Explore algorithm, which maximizes cumulative reward by training two separate policies: an explore policy and an exploit policy.  The explore policy focuses on gathering information, while the exploit policy focuses on maximizing reward. The algorithm combines these policies after training, using a k-selection phase to determine the optimal balance between exploration and exploitation for new environments.", "section": "4 First-Explore"}, {"figure_path": "AhjTu2aiiW/figures/figures_30_2.jpg", "caption": "Figure 1: First-Explore aims to maximize the cumulative reward of a sequence of n episodes on a target environment distribution. This optimization is achieved by first training two separate policies, and then combining them after training to maximize the total reward obtained. A. First, two separate policies are trained on the distribution of environments: one to explore (produce informative episodes), and one to exploit (maximize current episode return). During training, the explore policy Texplore provides all the context Ci = T1, ..., Ti\u22121 for both policies. This flow of context is visualized by solid arrows \u2192. The exploit policy Texploit takes a context of episodes, and produces a single episode of exploitation. The return of this exploit episode is then used to train both policies, with the feedback to the explore policy visualized by the dotted green arrows ....>. B. After the two policies are trained, different combinations of them are evaluated to find the combination that maximizes total reward. Each combination involves first exploring for k episodes, and then repeatedly exploiting for the remaining n \u2212 k episodes. C. The best combination is then used at inference time: exploring for a fixed number of episodes on new environments, and then exploiting for the remaining episodes.", "description": "The figure shows the First-Explore framework for meta-RL. It trains two separate policies: an explore policy and an exploit policy. The explore policy focuses on gathering information, while the exploit policy aims to maximize immediate reward.  These policies are trained separately and then combined optimally to maximize cumulative reward in a sequence of episodes.  The diagram illustrates the training and k-selection phases, emphasizing the flow of information (context) between the two policies.", "section": "4 First-Explore"}, {"figure_path": "AhjTu2aiiW/figures/figures_31_1.jpg", "caption": "Figure 1: First-Explore aims to maximize the cumulative reward of a sequence of n episodes on a target environment distribution. This optimization is achieved by first training two separate policies, and then combining them after training to maximize the total reward obtained. A. First, two separate policies are trained on the distribution of environments: one to explore (produce informative episodes), and one to exploit (maximize current episode return). During training, the explore policy Texplore provides all the context Ci = T1, ..., Ti\u22121 for both policies. This flow of context is visualized by solid arrows \u2192. The exploit policy \u03c0exploit takes a context of episodes, and produces a single episode of exploitation. The return of this exploit episode is then used to train both policies, with the feedback to the explore policy visualized by the dotted green arrows ....>. B. After the two policies are trained, different combinations of them are evaluated to find the combination that maximizes total reward. Each combination involves first exploring for k episodes, and then repeatedly exploiting for the remaining n \u2212 k episodes. C. The best combination is then used at inference time: exploring for a fixed number of episodes on new environments, and then exploiting for the remaining episodes.", "description": "The figure illustrates the First-Explore algorithm's process of maximizing cumulative reward. It involves training two separate policies: one for exploration and one for exploitation. The exploration policy informs the exploitation policy, which then maximizes rewards. Different combinations of exploration and exploitation lengths are tested, and the best combination is selected for inference.", "section": "4 First-Explore"}]