[{"figure_path": "AhjTu2aiiW/tables/tables_5_1.jpg", "caption": "Table 1: Mean cumulative reward \u00b1 standard deviation of First-Explore compared against control algorithms in hard-to-explore domains, with random action (picking actions uniformly at random at each timestep) added for additional reference. In each domain, First-Explore significantly outperforms meta-RL controls. The bandit domain compares to two non-meta-RL baselines, marked \u2020.", "description": "This table presents the mean cumulative reward and standard deviation achieved by First-Explore and other algorithms (including baselines) across three challenging domains: Bandits with One Fixed Arm, Dark Treasure Rooms, and Ray Maze.  The results show that First-Explore significantly outperforms existing cumulative reward meta-RL algorithms (RL2, VariBAD, HyperX) in scenarios where optimal behavior requires sacrificing immediate reward for greater future reward.  Random actions are included for comparison.", "section": "6 Results"}, {"figure_path": "AhjTu2aiiW/tables/tables_18_1.jpg", "caption": "Table 2: Mean cumulative reward \u00b1 standard deviation on 10 Episode Dark Treasure Rooms, for different values of p. p = \u22122\u221ax corresponds to exploration (visiting a random treasure location) requiring more than x 1 revisits. Even when the domain is made significantly less challenging (i.e., for more-positive values of p), First-Explore signficantly outperforms the controls.", "description": "This table shows the mean cumulative reward and standard deviation achieved by First-Explore and three other meta-RL algorithms (RL2, VariBAD, and HyperX) across 10 episodes in the Dark Treasure Room environment. The environment's difficulty is parameterized by p, where a higher absolute value of p makes exploration more challenging, as it requires more consistent exploitation to make exploration worthwhile. The results demonstrate that First-Explore significantly outperforms the other algorithms across all values of p, highlighting its effectiveness in environments that require sacrificing immediate reward for long-term gains.", "section": "6 Results"}, {"figure_path": "AhjTu2aiiW/tables/tables_21_1.jpg", "caption": "Table 3: Bandits with One Fixed Arm Results. The bandit domain compares to two non-meta-RL baselines, marked \u2020.", "description": "This table presents the results of the Bandits with One Fixed Arm experiment, comparing the performance of First-Explore to three other algorithms: RL2, UCB1, and Thompson Sampling. The experiment was conducted with two settings: one where the first arm has an above-average reward (the deceptive setting) and one where it does not (the non-deceptive setting).  The mean and standard deviation of the cumulative reward are reported for each algorithm and setting. The UCB1 and Thompson Sampling algorithms are non-meta-RL baselines included for comparison.", "section": "6 Results"}, {"figure_path": "AhjTu2aiiW/tables/tables_21_2.jpg", "caption": "Table 4: Dark Treasure-Room Results", "description": "This table presents the mean and median cumulative rewards achieved by First-Explore and three other meta-RL algorithms (RL2, VariBAD, and HyperX) in the Dark Treasure Room environment under two different reward settings: a deceptive setting (p = -4) and a non-deceptive setting (p = 0).  The deceptive setting makes exploration less immediately rewarding, creating a challenging scenario for cumulative reward methods.  The table highlights the significant performance difference between First-Explore and the other algorithms, especially in the deceptive setting, demonstrating First-Explore's ability to handle environments where early exploration requires foregoing immediate rewards.", "section": "6 Results"}, {"figure_path": "AhjTu2aiiW/tables/tables_21_3.jpg", "caption": "Table 1: Mean cumulative reward \u00b1 standard deviation of First-Explore compared against control algorithms in hard-to-explore domains, with random action (picking actions uniformly at random at each timestep) added for additional reference. In each domain, First-Explore significantly outperforms meta-RL controls. The bandit domain compares to two non-meta-RL baselines, marked \u2020.", "description": "This table presents the mean cumulative reward and standard deviation achieved by First-Explore and several baseline algorithms across three different challenging domains.  The results demonstrate First-Explore's significant outperformance in scenarios requiring the agent to sacrifice immediate reward for long-term gain, a key challenge that existing meta-RL methods struggle with.  The bandit domain also includes comparisons to non-meta RL baselines.", "section": "6 Results"}, {"figure_path": "AhjTu2aiiW/tables/tables_22_1.jpg", "caption": "Table 6: Compute Usage Per Training Run. Many of the meta-RL controls converged early, and did not improve with longer periods of training time (see Appendix F). Domains where this occurred are marked \u2020. To save compute, these runs were not trained as long as First-Explore.", "description": "This table presents the compute time required for training different models on various tasks.  It highlights that many of the meta-RL control models converged to a solution quickly and didn't benefit from extended training. To save computational resources, the training time for these models was shorter than that of the First-Explore model. The symbol \u2020 indicates tasks where this early convergence occurred.", "section": "Compute Usage"}, {"figure_path": "AhjTu2aiiW/tables/tables_26_1.jpg", "caption": "Table 3: Bandits with One Fixed Arm Results. The bandit domain compares to two non-meta-RL baselines, marked \u2020.", "description": "This table shows the mean cumulative reward and standard deviation for the Bandits with One Fixed Arm environment.  The results for First-Explore are compared against three other algorithms (RL2, UCB-1, and Thompson Sampling).  The table also includes the median cumulative reward for each algorithm.  Two versions of the bandit problem are included: one where early exploration sacrifices immediate reward, and another where it does not. The \u2020 symbol indicates non-meta-RL baselines.", "section": "6 Results"}, {"figure_path": "AhjTu2aiiW/tables/tables_26_2.jpg", "caption": "Table 1: Mean cumulative reward \u00b1 standard deviation of First-Explore compared against control algorithms in hard-to-explore domains, with random action (picking actions uniformly at random at each timestep) added for additional reference. In each domain, First-Explore significantly outperforms meta-RL controls. The bandit domain compares to two non-meta-RL baselines, marked \u2020.", "description": "This table presents the mean cumulative reward and its standard deviation achieved by First-Explore and several control algorithms across three different domains: Bandits with One Fixed Arm, Dark Treasure Rooms, and Ray Maze.  The results demonstrate that First-Explore significantly outperforms the other algorithms in these domains, particularly when exploration requires sacrificing immediate reward for greater future reward.  A random action baseline is included for comparison, and the bandit domain also provides results from two additional non-meta-RL baselines (UCB and Thompson Sampling). The \u2020 symbol marks non-meta-RL baselines.", "section": "6 Results"}, {"figure_path": "AhjTu2aiiW/tables/tables_27_1.jpg", "caption": "Table 9: Training Rollout Hyperparameters", "description": "This table presents the hyperparameters used for training the rollout policies (explore and exploit) in the First-Explore framework.  It shows the settings for three different domains: Bandits, Darkroom, and Ray Maze. The parameters cover temperature for sampling actions in both explore and exploit policies, the frequency of policy updates, the probability of randomly selecting an action (epsilon), the baseline reward used during training, and the total number of training updates performed.  These parameters were carefully chosen to ensure effective training for each of the distinct tasks and complexities of the domains.", "section": "K Training Details"}]