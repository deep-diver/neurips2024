[{"type": "text", "text": "Frequency Adaptive Normalization For Non-stationary Time Series Forecasting ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Weiwei Ye1 Songgaojun Deng2 Qiaosha Zou3 Ning Gui1\u2217 1Central South University 2University of Amsterdam 3Zhejiang Lab wwye155@gmail.com, s.deng@uva.nl, qiaoshazou@zhejianglab.org, ninggui@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Time series forecasting typically needs to address non-stationary data with evolving trend and seasonal patterns. To address the non-stationarity, reversible instance normalization has been recently proposed to alleviate impacts from the trend with certain statistical measures, e.g., mean and variance. Although they demonstrate improved predictive accuracy, they are limited to expressing basic trends and are incapable of handling seasonal patterns. To address this limitation, this paper proposes a new instance normalization solution, called frequency adaptive normalization (FAN), which extends instance normalization in handling both dynamic trend and seasonal patterns. Specifically, we employ the Fourier transform to identify instance-wise predominant frequent components that cover most non-stationary factors. Furthermore, the discrepancy of those frequency components between inputs and outputs is explicitly modeled as a prediction task with a simple MLP model. FAN is a model-agnostic method that can be applied to arbitrary predictive backbones. We instantiate FAN on four widely used forecasting models as the backbone and evaluate their prediction performance improvements on eight benchmark datasets. FAN demonstrates significant performance advancement, achieving $7.76\\%{\\sim}37.90\\%$ average improvements in MSE. Our code is publicly available2. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Time series forecasting plays a key role in various fields such as traffic [8], finance [23] and infectious disease [1], etc. Recent research focuses on deep learning-based methods, as they demonstrate promising capabilities to capture complex dependencies between variables [47, 42, 46]. However, time series with trends and seasonality, also called non-stationary time series [15], create covariate pattern shifts across different time steps. These dynamics pose significant challenges in forecasting. ", "page_idx": 0}, {"type": "text", "text": "To mitigate non-stationarity issues, reversible normalization has been proposed [33, 17] which first removes non-stationary information from the input and returns the information back to rebuild the output. Current work focuses on removing non-stationary signals with statistical measures, e.g., mean and variance in the time domain [10, 28]. However, while these methods have improved prediction accuracy, these statistical measures are only capable of extracting the most prominent component, i.e., the trend, leaving substantial room for improvement. They, we argued, can hardly measure the characteristics of seasonal patterns, which are quite common in many time series. This significantly limits their capability to handle the non-stationarity, especially the seasonal patterns. ", "page_idx": 0}, {"type": "text", "text": "We illustrate an example featuring one of the simplest non-stationary signals in Fig. 1. This graph shows a time-variant signal with a gradually damping frequency, which is widely seen in many passive systems, e.g., spring-mass damper systems. As depicted in Fig. 1, the three input stages ", "page_idx": 0}, {"type": "text", "text": "(highlighted in different background colors) exhibit the same mean and variance but differ in Fourier frequencies. Previous methods that model non-stationary information using means and variances can hardly distinguish this type of change in the time domain. In comparison, changes in periodic signals can be easily identified with the instance-wise Fourier transform $(f_{1}\\neq f_{2}\\neq f_{3})$ . Thus, in this context, the principal Fourier components provide a more effective representation of non-stationarity com", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "T0axIflVDD/tmp/8fab80ed32260fa6fe4fc786dfdcaa9e982a713a7dd01d9a93353b76173fc9e2.jpg", "img_caption": ["Figure 1: A sinusoidal signal with linearly varying frequency which is a common example of a non-stationary time series. In the lower-left corner, we plot the Fourier spectrum for three segments of the signal. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "pared to statistical values such as mean and variance. This simple example also shows that many existing frequency-based solutions, e.g., TimesNet [41], Koopa [27], which assume that the principal frequencies of the input signal are constant, can not identify the evolving principal frequencies. ", "page_idx": 1}, {"type": "text", "text": "With this inspiration, we introduce a novel instance-based normalization method, named Frequency Adaptive Normalization (FAN). Rather than normalizing temporal statistical measures, FAN mitigates the impacts from the non-stationarity by flitering top $K$ dominant components in the Fourier domain for each input instance, this approach can handle unified non-stationary fact composed of both trend and seasonal patterns. Furthermore, as those removed patterns might evolve from inputs to outputs, we employ a pattern adaptation module to forecast future non-stationary information rather than assuming these patterns remain unchanged. ", "page_idx": 1}, {"type": "text", "text": "In summary, our main contributions are: 1) We illustrate the limitations of reversible instance normalization methods in using temporal distribution statistics to remove impacts from non-stationarity. To address this limitation, we introduce a novel reversible normalization method, named FAN, which adeptly addresses both trend and seasonal non-stationary patterns within time series data. 2) We explicitly address pattern evolvement with a simple MLP that predicts the top $K$ frequency signals of the horizon series and applies these predictions to reconstruct the output. 3) We apply FAN to four general backbones for time series forecasting across eight real-world popular benchmarks. The results demonstrate that FAN significantly improves their predictive effectiveness. Furthermore, a comparative analysis between FAN and state-of-the-art normalization techniques underscores the superiority of our proposed solution. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Time series forecasting has been a hot topic of study for many years. This section provides discussions on related work from the following three perspectives. ", "page_idx": 1}, {"type": "text", "text": "2.1 Time Series Forecasting ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Traditional statistical methods, such as ARIMA [2], assume the stationarity of the time series and dependencies between temporal steps. Although these methods provide theoretical guarantees, they typically require data with ideal properties, which is often inconsistent with real-world scenarios [42]. Besides, they can only handle a limited amount of data and features. In recent years, the field has witnessed a significant proliferation in the application of deep learning techniques for multivariate time series forecasting, a development ascribed to their ability in handling high-dimensional datasets. Consequently, various methods have been proposed to model time series data. Work based on recurrent neural networks [36, 4] preserves the current state and models the evolution of time series as a recurrent process. However, they generally suffer from a limited receptive field, which restricts their ability to capture long temporal dependencies [47]. Inspired by their successes in Computer Vision (CV) and Natural Language Processing (NLP), convolutional neural networks and the self-attention mechanism have been extensively utilized in time series forecasting [22, 19, 40, 25]. Nevertheless, those works still face difficulties in handling non-stationary data with covariate pattern shifts. Making an accurate prediction for non-stationary time series remains challenging. ", "page_idx": 1}, {"type": "text", "text": "2.2 Non-stationary Time Series Forecasting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To address non-stationarity, many methods directly model non-stationary phenomena with different modeling techniques. Li et al. [24] utilize a domain-adaptation paradigm to predict data distributions. Du et al. [6] propose an adaptive RNN to alleviate the impact of non-stationary factors through distribution matching. Liu et al. [26] introduce a non-stationary Transformer with de-stationary attention that incorporates non-stationary factors in self-attention mechanisms. To model non-linear dynamic systems, several models based on Koopman theory [21, 29, 37, 44] have been proposed with Fourier transform. To learn different patterns at different scales, Wang et al. [38] employs global and local Koopman operators. Liu et al. [27] model non-stationarity identified with Fourier transform and use Koopman operators to learn those components. However, these solutions typically select fixed frequency components based on the whole sequence rather than frequencies based on inputs. This time-invariant assumption can hardly be true in real-world scenarios. ", "page_idx": 2}, {"type": "text", "text": "2.3 Instance-wise Normalization against Non-stationarity ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To mitigate the time-variant property of non-stationary time series, a set of instance-wise normalization methods have been proposed to remove the impacts from non-stationarity. To reflect instance-wise changes, Ogasawara et al. [31] propose the usage of normalization based on local properties rather than global statistics. Passalis et al. [33] introduce an adaptive and learnable approach to this instancewise normalization paradigm. However, although these methods effectively remove non-stationary components from inputs, they still need to predict the non-stationary time series in the output series, which remains challenging. In response, reversible instance normalization [17] is introduced by reintegrating the removed non-stationary components back to reconstruct the output. However, it still assumes unchanged trends between inputs and outputs. Kim et al. [17] developed RevIN, which mainly addresses evolving trends between input sequences. Recent works [10, 28] explore trends at a finer granularity, e.g., at the sliced level. However, these approaches still model non-stationarity with temporal statistical distribution parameters and fail to account for evolving seasonality, which is a crucial aspect of non-stationarity [35, 11, 45]. ", "page_idx": 2}, {"type": "text", "text": "3 Proposed Method: FAN ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a multivariate time series $\\boldsymbol{\\mathcal{X}}\\in\\mathbb{R}^{N\\times D}$ , where $N$ is the total time steps and $D$ denotes the number of feature dimensions. We use inputs series with length $L$ to predict outputs series within length $H$ . The forecast task can be formulated as: $\\mathcal{X}_{t-L:t}\\rightarrow\\mathcal{X}_{t+1:t+H}$ , where $\\lambda_{t-L:t}^{\\,\\,\\,}\\in\\mathbb{R}^{L\\times D}$ and $\\bar{\\boldsymbol{\\chi}_{t+1:t+H}}\\in\\mathbb{R}^{H\\times D}$ . For a clearer notation, we denote the input and output series as $\\mathbf{X}_{t}\\in\\mathbb{R}^{L\\times D}$ and $\\mathbf{Y}_{t}\\in\\mathbb{R}^{H\\times D}$ respectively. ", "page_idx": 2}, {"type": "text", "text": "Our proposed method, FAN, consists of symmetrically structured instance-wise normalization and denormalization layers, illustrated in Fig. 2. The normalization process removes the impacts of non-stationary signals through frequency domain decomposition (upper left part of Fig. 2), while the denormalization process, supported by a prediction module, addresses potential shifts in frequency components between the input and output (lower part of Fig. 2). ", "page_idx": 2}, {"type": "text", "text": "3.1 Frequency-based Normalization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "First, FAN removes the top $K$ dominant components in the frequency domain for each input instance, so the forecasting backbone can concentrate on the stationary aspects of the input. We term this process as Frequency Residual Learning (FRL). The input at time $t$ , $\\mathbf{X}_{t}$ , is multivariate with $D$ dimension, and each dimension might have different frequency patterns; thus, we apply the FRL to each dimension in a channel independence setting [30]. Here, the FRL is realized by the 1-dim Discrete Fourier Transform (DFT) with $\\mathrm{DFT}(\\cdot)$ towards each input $\\mathbf{X}_{t}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{Z}_{t}=\\mathrm{DFT}(\\mathbf{X}_{t})\\quad{\\mathrm{and}}\\quad K_{t}=\\mathrm{TopK}(\\mathrm{Amp}(\\mathbf{Z}_{t}))\\quad{\\mathrm{and}}\\quad\\mathbf{X}_{t}^{n o n}=\\mathrm{IDFT}(\\mathrm{Filter}(K_{t},\\mathbf{Z}_{t}))\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Equ. 1 shows that $\\mathrm{DFT}(\\cdot)$ transforms an input into Fourier components in complex values, denoted $\\mathbf{Z}_{t}^{\\star}\\in\\mathbb{C}^{T\\times D}$ . Then, $\\mathrm{TopK}(\\cdot)$ selects the frequency set with the top $K$ largest amplitude, which are calculated with $\\mathrm{Amp}(\\cdot)$ function. Filter is the operation to filter out the $\\kappa_{t}$ frequency from $\\mathbf{Z}_{t}$ . To mitigate the impact of non-stationary signals, FRL restores the top $K$ components into time domain components $\\mathbf{X}_{t}^{n o n}$ with $\\mathrm{IDFT}(\\cdot)$ . ", "page_idx": 2}, {"type": "image", "img_path": "T0axIflVDD/tmp/79f29f41c5c0cc3d97a9ba0e8c18d4e7927776d0283c2f9db350c1397d652379.jpg", "img_caption": ["Figure 2: An overview of FAN which consists of normalization, frequency residual learning, denormalization steps, and incorporates a prior loss for non-stationary patterns. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "With $\\mathbf{X}_{t}^{n o n}$ , we can easily normalize the inputs and get the stationary components by removing $\\mathbf{X}_{t}^{n o n}$ from $\\mathbf{X}_{t}$ , which is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{X}_{t}^{r e s}=\\mathbf{X}_{t}-\\mathbf{X}_{t}^{n o n}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\mathrm{DFT}(\\cdot)$ and $\\mathrm{IDFT}(\\cdot)$ can be performed using Fast Fourier Transform (FFT) [3] with a computational complexity of ${\\dot{O}}(L\\log L)$ . And the TopK and Filter operations both exhibit complexity of $O(L+K)$ . It is important to note that all these operations are GPU-friendly and can be fully paralleled. Thus, the impact of applying these operations independently on each dimension can be largely mitigated. GPU-friendly PyTorch pseudocode is in Appendix A.2. After the normalization step, the normalized sequences ${\\bf X}_{t}^{r e s}$ can be more stationary and have a more consistent covariate distribution, the theoretical proof is provided in Appendix C. ", "page_idx": 3}, {"type": "text", "text": "3.2 Forecast & Denormalization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As a result, the normalization layer allows the forecast backbone model $g_{\\theta}$ to focus more on the dynamics within the inputs. Here, following the reversible instance normalization paradigm, the forecast backbone $g_{\\theta}$ receives the transformed data ${\\bf X}_{t}^{r e s}$ as input and forecasts only the stationary part $\\mathbf{Y}_{t}^{r e s}$ of the outputs $\\mathbf{Y}_{t}$ . This design makes it easier for the model to forecast non-stationary time series. Then, we apply the removed non-stationary information back to the output. We define this process as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{Y}}_{t}^{r e s}=g_{\\theta}(\\mathbf{X}_{t}^{r e s})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{Y}}_{t}=\\hat{\\mathbf{Y}}_{t}^{r e s}+\\hat{\\mathbf{Y}}_{t}^{n o n}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{\\mathbf{Y}}_{t}^{n o n}$ is the reconstruct signal of $\\mathbf{X}_{t}^{n o n}$ . We illustrate $\\hat{\\mathbf{Y}}^{n o n}$ as follow: ", "page_idx": 3}, {"type": "text", "text": "Non-stationary shift forecasting. For reverse instance normalization, we need to estimate $\\hat{\\mathbf{Y}}_{t}^{n o n}$ in the outputs. As an input and its corresponding output are rather close, RevIN [17] directly adds $\\mathbf{X}_{t}^{n o n}$ back by assuming $\\mathbf{Y}_{t}^{n o n}$ with exactly the same trend as $\\mathbf{X}_{t}^{n o n}$ . However, this assumption can hardly be true as the non-stationary information between the input and output may evolve. Furthermore, although SAN [28] and Dish-TS [10] predict statistics to address the discrepancy between the input and output, these statistics can only represent the most salient trend patterns. ", "page_idx": 3}, {"type": "text", "text": "To this end, rather than predicting statistics [10, 28], we use a simple MLP model $q_{\\phi}$ to directly predict future values of the composite top $K$ frequency components for $D$ dimensions, defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\bf\\hat{Y}}_{t}^{n o n}=q_{\\phi}({\\bf X}_{t}^{n o n},{\\bf X}_{t})={\\bf W}_{3}\\operatorname{ReLU}\\left({\\bf W}_{2}\\operatorname{Concat}(\\operatorname{ReLU}\\left({\\bf W}_{1}{\\bf X}_{t}^{n o n}\\right),{\\bf X}_{t})\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{W}_{1},\\mathbf{W}_{2},\\mathbf{W}_{3}$ are all learnable parameters. Here, since $\\mathbf{X}_{t}^{n o n}$ only contains top $K$ frequency information, it is difficult to capture variations in other frequencies solely relying on $\\mathbf{X}_{t}^{n o n}$ . Therefore, we concatenate the top $K$ components with the original input $\\mathbf{X}_{t}$ to handle potential frequency variations. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Loss Functions. To help with the residual learning process, we incorporate a prior guidance loss for tphrie opr rgeudiidctainocne  ocfa pnr ibnec icpoanls firdeeqreude nac ym cuoltim-tpaosnke ontpst,i tmhiez aftiinoanl  lporsosb ilse dme [f1in2e]d,  iwnh Eerqe. $\\mathcal{L}_{\\phi}^{n o n s t a t}$ reencsausrt esw $q_{\\phi}$ accurately predict the non-stationary principal frequency component and L\u03b8f,o\u03d5recast guarantees that both model optimizes along the overall forecast accuracy. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi,\\theta=\\arg\\operatorname*{min}_{\\phi,\\theta}\\sum_{t}\\left(\\mathcal{L}_{\\phi}^{n o n s t a t}(\\mathbf{Y}_{t}^{n o n},\\hat{\\mathbf{Y}}_{t}^{n o n})+\\mathcal{L}_{\\theta,\\phi}^{f o r e c a s t}(\\mathbf{Y}_{t},\\hat{\\mathbf{Y}}_{t})\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, the mean square error loss is used for both loss functions. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Experiment Setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Datasets. We use eight popular datasets in multivariate time series forecasting as benchmarks, including: (1-4) ETT (Electricity Transformer Temperature) 3[47] records the oil temperature and load of the electricity transformers from July 2016 to July 2018. Four subsets are included in this dataset, where ETThs are sampled per hour and ETTms per 15 minutes. (5) Electricity 4 contains the electricity consumption of 321 clients from July 2016 to July 2019 per 15 minutes. (6) ExchangeRate 5 contains the daily exchange rates of 8 countries from 1990 to 2016. (7) Traffic 6 includes the hourly traffic load of San Francisco freeways recorded by 862 sensors from 2015 to 2016. (8) Weather 7 is made up of 21 indicators of weather, including air temperature and humidity collected every 10 minutes in 2021. ", "page_idx": 4}, {"type": "text", "text": "For preprocessing, we apply z-score normalization [12] on all datasets to scale different variables to the same scale. Note that z-score normalization is unable to handle non-stationary time series since the statistics remain unchanged for different input instances [17]. The split ratio for training, validation, and test sets is set to 7:2:1 for all the datasets. We report datasets properties in Table 1, including (1) Trend Variation: Differences in the means across different sections of the dataset. (2) Seasonality Variation: We report the average variance over the Fourier spectrum to examine the presence of evolving seasonality. Other dataset details can be found at Appendix B. ", "page_idx": 4}, {"type": "table", "img_path": "T0axIflVDD/tmp/6e170d5eac8fbc87ad29e81a77d7e21f16a4a50c958362eeafb5531d0469e100.jpg", "table_caption": ["Table 1: Properties of datasets and used hyperparameter $K$ of each dataset. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Evaluation. We set the prediction length $H\\in\\{96,168,336,720\\}$ , covering both short- and longterm rediction [30]. A fixed input window length $L=96$ is used for all datasets. We evaluate the performance of baselines using mean squared error (MSE) and mean absolute error (MAE). The MSE and MAE are computed on z-score normalized data to measure different variables on the same scale. We report the final results on the test set for the model that performed optimally on the validation set. ", "page_idx": 4}, {"type": "text", "text": "Backbone Models. FAN is model-agnostic and could be applied to any prediction backbones. To validate its effectiveness, four state-of-the-art time-series forecasting model are used: MLPbased DLinear [46], Transformer-based Informer [47] and FEDformer[48], and convolutional neural network-based SCINet [25]. Notably, FEDformer also employs the Fourier transform for analyzing seasonality. Results later show that FAN continues to make considerable improvements over these frequency-based solutions like FEDformer. ", "page_idx": 4}, {"type": "table", "img_path": "T0axIflVDD/tmp/00389f2fc64b8a981ff93524ee45dff0fe887f45c641d92414720a691dc6adcd.jpg", "table_caption": ["Table 2: Forecasting errors with and without FAN. The bold values indicate the best performance. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Implementation and Settings. For the non-stationary prediction module $q_{\\phi}$ in FAN, the MLP model has hidden sizes [64, 128, 128]. All the experiments are implemented by PyTorch [34] and are conducted for five runs with fixed seeds $\\{1,2,3,4,5\\}$ on NVIDIA RTX 4090 GPU (24GB). For the different baselines, we follow the implementation and settings provided in their official code repository. ADAM [18] as the default optimizer across all the experiments. More experiment details, including training details and hyperparameter, can be found in Appendix A.1. ", "page_idx": 5}, {"type": "text", "text": "Selections of Hyperparameter $K$ . FAN allows for $K$ to be any integer number less than $L$ . Regarding the selection of $K$ , we analyze these benchmarks and found that the main variation frequencies of these datasets are within $10\\%$ of the maximum amplitude. Therefore, we select the value of $K$ based on the average maximum amplitude within $10\\%$ in the training set, the selected $K$ is shown in Table 1. More evidence of this selection strategy is at Sec. 4.4, and we provide a detailed hyperparameter sensitivity analysis at Appendix D.1. ", "page_idx": 5}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We report MAE/MSE forecasting errors of the baselines and FAN in Table 2. Since the performance in ETT datasets shows similar results, only results of ETTm2 are reported. The full results of the ETT benchmarks and further discussion are in Appendix E.2. ", "page_idx": 5}, {"type": "text", "text": "As shown in Table 2, our proposed FAN effectively enhances the performance of all four backbone models, by a large margin, achieving state-of-the-art performance on five datasets. Specifically, on the ETTm2, Electricity, Exchange, Traffic, and Weather datasets, the average MSE performance improvements are rather significant: $10.81\\%$ , $21.49\\%$ , $51.27\\%$ , $21.97\\%$ , and $21.55\\%$ respectively. It clearly shows that frequency residual learning of FAN effectively mitigates the impacts of evolving seasonal and trend patterns and enhances the stationarity that simplifies the prediction for backbones. ", "page_idx": 5}, {"type": "text", "text": "FAN demonstrates increasing performance improvements as the prediction length extends in the Informer backbone, with MSE improvements of $9.87\\%$ , $18.87\\%$ , $36.91\\%$ , $16.26\\%$ , and $20.05\\%$ , from 96 steps to 720 steps. We believe this can be attributed to the fact that the periodicity contained in the prediction series increases with step length, and the FAN\u2019s pattern prediction module helps uncover periodicity in longer step lengths, thereby enhancing long-term prediction effectiveness. It is important to note that even in the models that utilize FFT to analyze seasonal patterns, like FEDformer, we still observe significant performance improvements $(19.81\\%)$ . This conclusion underscores our model\u2019s advantage in handling non-stationary aspects by directly extracting non-stationary seasonality patterns rather than learning these patterns. ", "page_idx": 5}, {"type": "text", "text": "4.3 Comparison With Reversible Instance Normalization Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we compare FAN with three state-of-the-art normalization methods for non-stationary time series forecasting: SAN [28], Dish-TS [10], and RevIN [17], with the same experimental setup as Sect. 4.2. We report the average MSE over all the forecasting lengths of all backbones for all datasets in Table 3. It is evident that FAN generally outperforms the baseline models, except for a few cases with a close margin. Here, SAN generally ranks second as it slices the whole sequence into sub-series which can make seasonal patterns into evolving trends that could be partially predicted with its statistics prediction module. In comparison, RevIN and Dish-TS have much worse performance. Detailed results of all cases and further discussions are provided in Appendix E.4. ", "page_idx": 6}, {"type": "table", "img_path": "T0axIflVDD/tmp/0f4e5911d89d6d247491e4603ceffd510e4d0a45a0dc5ab8caff1c27be08e826.jpg", "table_caption": ["Table 3: The MSE performance averaged across all steps. Bold values indicate the best performance. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Show Cases. Fig. 3 illustrates the forecasting results with DLinear backbone in Traffic to show why FAN has performance advantages. This data has very clear evolving seasonality with daily waveform patterns. FAN can extract trends and seasonal patterns especially the seasonal patterns during weekends while Dish-TS and RevIN only focus on trends statistics. Furthermore, FAN can adaptively adjust frequency pattern forecasting results based on the input main frequency signals, capturing the evolving patterns between the input and horizon. Fig. 3(a) clears shows FAN can identify the seasonal patterns with increasing amplitudes from hour $100\\!\\sim\\!150$ . ", "page_idx": 6}, {"type": "image", "img_path": "T0axIflVDD/tmp/88d7599aca5e4485111149b712a30083c84fa869ec8294325ec6b672dd4c6e10.jpg", "img_caption": ["Figure 3: Visualization of long-term 168 steps forecasting results of a test sample in Traffic dataset, using DLinear enhanced with different normalization methods. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Stationarity Analysis. To verify our model\u2019s effectiveness against non-stationarity, we use the ADF test [39] to examine the stationarity of the data after normalization. The results are shown in Fig. 4(a), smaller value (further from the center) indicates higher stationarity. Compared to previous normalization methods, our model achieves greater stationarity across all datasets, particularly in cases with larger seasonal patterns (Traffic, ETTh1, ETTm1). In some datasets, e.g., Weather, Exchange, despite having less apparent seasonality, our model still enhances stationarity. We attribute this to our method\u2019s ability to adaptively capture the low-frequency trend signals, such as mixed-linear changes, while other methods assume consistent distribution over a period and remove estimated statistics, due to which they might fail to capture these intricate trend patterns. ", "page_idx": 6}, {"type": "text", "text": "Model Efficiency. We compare the performance, training time per iteration, and number of parameters with SAN on Traffic with $D=862$ , $H=720$ . DLinear is used for both as the backbone. The results are shown in Fig. 4(b). FAN and SAN have similar training iteration times, but FAN has $29.79\\%$ less parameters. Moreover, FAN achieves a $15.56\\%$ improvement in MSE and a $15.30\\%$ improvement in MAE. This highlights our model\u2019s effectiveness and efficiency. ", "page_idx": 6}, {"type": "text", "text": "Various Input Length. In time series learning, the non-stationarity of inputs varies with the choice and change of the time window [35], which in turn impacts the performance of deep learning models [27]. Therefore, we compare the performance changes under different input lengths on ", "page_idx": 6}, {"type": "image", "img_path": "T0axIflVDD/tmp/2b16526f32e4d4c1ddb20472c599e14f65a865e98483f669d55d1bdfd1301583.jpg", "img_caption": ["Figure 4: Comparison with other normalization methods. (a) ADF test after normalization, the smaller the value, the higher the stationarity. (b) Model efficiency comparison with SAN, including MSE/MAE, parameters (in millions), and training time per iteration (ms/100). (c) Performance in MSE vs. input length on the ETTm2 dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "ETTm2 dataset and DLinear as the backbone. Fig. 4(c) shows FAN exhibits the best performance across all windowed data. As can be seen, compared to other models, as the input length increases, among these normalizations, the enhancement of increases the most. The MSE enhancement with previous SOTAs increases from $0.49\\%$ in short inputs $L=48$ to $4.37\\%$ in long inputs with $L=336$ . This demonstrates that the instance-wise DFT is capable of extracting more seasonal patterns from the longer input windows. ", "page_idx": 7}, {"type": "text", "text": "4.4 TopK vs. Frequency Distributions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As different datasets might have different non-stationary patterns, it is crucial to select appropriate K frequency components from inputs. We study the relations between the selected TopK and the frequency distribution on the ExchangeRate and Traffic datasets. ", "page_idx": 7}, {"type": "text", "text": "Fig. 5 plots the frequency amplitude distribution for frequency $_{0\\sim32}$ by performing DFT towards the different input instances with $L=96$ of the whole training sequences. Here, we can see the clear relation between the selection of K and the frequency amplitude distributions. As we can see, the Traffic dataset contains rich seasonal signals ranging from $_{0\\sim32}$ while the ExchangeRate dataset only has mainly one principal frequency component with frequency 0 (trend) in the inputs. Thus, the prediction on the ExchangeRate dataset might not benefit from a bigger K while a bigger K indeed helps for the Traffic dataset. Results for more datasets are in Appendix B. ", "page_idx": 7}, {"type": "image", "img_path": "T0axIflVDD/tmp/67a587b48b507088295763c93adf0d60994ef9f1e3c29e6cab3cfbbcd00731b8.jpg", "img_caption": ["Figure 5: Frequency distributions vs. forecast error in MSE with different K and output length $H$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.5 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Main Components. This section aims to evaluate the effectiveness of various FAN\u2019s designs. Three variants are studied: \u201cw/o predict\u201d denotes the removal of the non-stationary pattern prediction module and directly reconstructing $\\hat{\\mathbf{Y}}^{n o n}$ with $\\mathbf{X}^{n o n}$ . \u201cpure backbone\u201d refers to the omission of the reconstruction in the output or \u201cw/o backbone\u201d is the omission of the stationary part. We evaluate their performance on two non-stationary datasets, ETTh1 and Weather. The experimental settings are consistent with those described in Section 4.2. The evaluation results, along with the standard deviations, are presented in Table 4. The results show that FAN achieves best performance across all metrics in all variants. FAN w/o backbone ranks second as the learning model of FAN already learns the principle changes. In comparison, the results from pure backbone is the weakest, as it cannot handle nonstationary signals. FAN w/o predict also has poor performance. Those results clearly show that trends and seasonal patterns do evolve and that our proposed residual frequency learning is crucial in dealing with these changes. ", "page_idx": 7}, {"type": "table", "img_path": "T0axIflVDD/tmp/a9219624b3c01ff8c52f6b46066275f402c9060c494763ffbbcfc4bbe039c3fd.jpg", "table_caption": ["Table 4: Forecasting errors under the multivariant setting with respect to variations of FAN with SCINet backbone. The best performances are highlighted in bold. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Instance-wise vs. Global Fourier Analysis. This section investigates the effect from instance-wise Fourier Analysis of FAN. Previous Fourier-based methods select predominant Fourier signals based on fixed frequencies [41, 27, 43, 9]. However, as shown in Fig. 6, on the Traffic and Electricity datasets, the predominant components from the input-wise view are not fixed but exhibit distinct distribution characteristics and vary across the inputs. The assumption of fixed spectrum and the reality of changing frequency limits their performance, supported with two additional experiments on Fourier-based backbones at Appendix E.3. ", "page_idx": 8}, {"type": "text", "text": "Here, we compare the performance of FAN with fixed frequencies computed using global sequences and original FAN with instance-specific frequencies.The results are shown in Table 5. ", "page_idx": 8}, {"type": "image", "img_path": "T0axIflVDD/tmp/8e7e9257832682531b9153c31fa90ae4db8d4c4f9214d3c5c55a442ef3283bdd.jpg", "img_caption": ["As shown in Table 5, by selecting instance-wise predominant frequencies, FAN achieves an average improvement of $18.50\\%$ and $10.29\\%$ on the Electricity and Traffic datasets respectively. This highlights instance-wise frequency selection rather than assuming fixed frequency patterns. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we study the problem of non-stationary time series prediction. We identify the fact that traditional statistical measurement-based instance-wise normalization can not effectively recover the evolving seasonal patterns. We propose FAN to perform instance normalization for each input window. The Fourier transform is used to remove the main frequency components in the inputs and reconstruct the Fourier basis through denormalization. To address the evolving trend and seasonal patterns between inputs and outputs, we utilize a simple MLP model to predict the changes in the extracted non-stationary pattern. The effectiveness of FAN is verified with a set of experiments on eight widely used benchmark datasets. Compared to other state-of-the-art normalization baselines, FAN significantly improves the prediction performance and outperforms state-of-the-art normalization methods. One potential avenue for improvement involves the autonomous determination of an optimal K for the selection of principal frequency components. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported in part by the National Natural Science Foundation of China (Grant No. 6247075381). We would also like to thank the anonymous reviewers for their constructive feedback and suggestions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Andrea L Bertozzi, Elisa Franco, George Mohler, Martin B Short, and Daniel Sledge. The challenges of modeling and forecasting the spread of covid-19. Proceedings of the National Academy of Sciences, 117(29):16732\u201316738, 2020.   \n[2] George EP Box and Gwilym M Jenkins. Some recent advances in forecasting and control. Journal of the Royal Statistical Society. Series C (Applied Statistics), 17(2):91\u2013109, 1968.   \n[3] E Oran Brigham. The fast Fourier transform and its applications. Prentice-Hall, Inc., 1988.   \n[4] Rahul Dey and Fathi M Salem. Gate-variants of gated recurrent unit (gru) neural networks. In 2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS), pages 1597\u20131600. IEEE, 2017.   \n[5] MM Dodson and AM Silva. Fourier analysis and the sampling theorem. In Proceedings of the Royal Irish Academy. Section A: Mathematical and Physical Sciences, pages 81\u2013108. JSTOR, 1985.   \n[6] Yuntao Du, Jindong Wang, Wenjie Feng, Sinno Pan, Tao Qin, Renjun Xu, and Chongjun Wang. Adarnn: Adaptive learning and forecasting of time series. In Proceedings of the 30th ACM international conference on information & knowledge management, pages 402\u2013411, 2021.   \n[7] Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. Tsmixer: Lightweight mlp-mixer model for multivariate time series forecasting. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 459\u2013469, 2023.   \n[8] Alireza Ermagun and David Levinson. Spatiotemporal traffic forecasting: review and proposed directions. Transport Reviews, 38(6):786\u2013814, 2018.   \n[9] Wei Fan, Shun Zheng, Xiaohan Yi, Wei Cao, Yanjie Fu, Jiang Bian, and Tie-Yan Liu. Depts: Deep expansion learning for periodic time series forecasting. arXiv preprint arXiv:2203.07681, 2022.   \n[10] Wei Fan, Pengyang Wang, Dongkun Wang, Dongjie Wang, Yuanchun Zhou, and Yanjie Fu. Dish-ts: a general paradigm for alleviating distribution shift in time series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 7522\u20137529, 2023.   \n[11] Wei Fan, Kun Yi, Hangting Ye, Zhiyuan Ning, Qi Zhang, and Ning An. Deep frequency derivative learning for non-stationary time series forecasting. arXiv preprint arXiv:2407.00502, 2024.   \n[12] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.   \n[13] Geoffrey Grimmett and David Stirzaker. Probability and random processes. Oxford university press, 2020.   \n[14] Huan He, Owen Queen, Teddy Koker, Consuelo Cuevas, Theodoros Tsiligkaridis, and Marinka Zitnik. Domain adaptation for time series under feature and label shifts. In International Conference on Machine Learning, pages 12746\u201312774. PMLR, 2023.   \n[15] Rob J Hyndman and George Athanasopoulos. Forecasting: principles and practice. OTexts, 2018.   \n[16] Lars Kegel, Martin Hahmann, and Wolfgang Lehner. Feature-based comparison and generation of time series. In Proceedings of the 30th international conference on scientific and statistical database management, pages 1\u201312, 2018.   \n[17] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations, 2021.   \n[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[19] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.   \n[20] Lambert H Koopmans. The spectral analysis of time series. Elsevier, 1995.   \n[21] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st international ACM SIGIR conference on research & development in information retrieval, pages 95\u2013104, 2018.   \n[22] Colin Lea, Michael D Flynn, Rene Vidal, Austin Reiter, and Gregory D Hager. Temporal convolutional networks for action segmentation and detection. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 156\u2013165, 2017.   \n[23] Audeliano Wolian Li and Guilherme Sousa Bastos. Stock market forecasting using deep learning and technical analysis: a systematic review. IEEE access, 8:185232\u2013185242, 2020.   \n[24] Wendi Li, Xiao Yang, Weiqing Liu, Yingce Xia, and Jiang Bian. Ddg-da: Data distribution generation for predictable concept drift adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 4092\u20134100, 2022.   \n[25] Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang Xu. Scinet: Time series modeling and forecasting with sample convolution and interaction. Advances in Neural Information Processing Systems, 35:5816\u20135828, 2022.   \n[26] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring the stationarity in time series forecasting. Advances in Neural Information Processing Systems, 35:9881\u20139893, 2022.   \n[27] Yong Liu, Chenyu Li, Jianmin Wang, and Mingsheng Long. Koopa: Learning non-stationary time series dynamics with koopman predictors. Advances in Neural Information Processing Systems, 36, 2024.   \n[28] Zhiding Liu, Mingyue Cheng, Zhi Li, Zhenya Huang, Qi Liu, Yanhu Xie, and Enhong Chen. Adaptive normalization for non-stationary time series forecasting: A temporal slice perspective. Advances in Neural Information Processing Systems, 36, 2024.   \n[29] Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Deep learning for universal linear embeddings of nonlinear dynamics. Nature communications, 9(1):4950, 2018.   \n[30] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2022.   \n[31] Eduardo Ogasawara, Leonardo C Martinez, Daniel De Oliveira, Geraldo Zimbr\u02dcao, Gisele L Pappa, and Marta Mattoso. Adaptive normalization: A novel data normalization approach for non-stationary time series. In The 2010 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2010.   \n[32] Alan V Oppenheim. Applications of digital signal processing. Englewood Cliffs, 1978.   \n[33] Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gabbouj, and Alexandros Iosifidis. Deep adaptive input normalization for time series forecasting. IEEE transactions on neural networks and learning systems, 31(9):3760\u20133765, 2019.   \n[34] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[35] Maurice Bertram Priestley. Spectral analysis and time series, volume 890. Academic press London, 1981.   \n[36] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27, 2014.   \n[37] Naoya Takeishi, Yoshinobu Kawahara, and Takehisa Yairi. Learning koopman invariant subspaces for dynamic mode decomposition. Advances in neural information processing systems, 30, 2017.   \n[38] Rui Wang, Yihe Dong, Sercan O\u00a8 Arik, and Rose Yu. Koopman neural forecaster for time series with temporal distribution shifts. arXiv preprint arXiv:2210.03675, 2022.   \n[39] Annette Witt, J\u00a8urgen Kurths, and A Pikovsky. Testing stationarity in time series. physical Review E, 58(2):1800, 1998.   \n[40] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in neural information processing systems, 34:22419\u201322430, 2021.   \n[41] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In The eleventh international conference on learning representations, 2022.   \n[42] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang. Connecting the dots: Multivariate time series forecasting with graph neural networks. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 753\u2013763, 2020.   \n[43] Zhijian Xu, Ailing Zeng, and Qiang Xu. Fits: Modeling time series with $10k$ parameters. arXiv preprint arXiv:2307.03756, 2023.   \n[44] Enoch Yeung, Soumya Kundu, and Nathan Hodas. Learning deep neural network representations for koopman operators of nonlinear dynamical systems. In 2019 American Control Conference (ACC), pages 4832\u20134839. IEEE, 2019.   \n[45] Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Ning An, Defu Lian, Longbing Cao, and Zhendong Niu. Frequency-domain mlps are more effective learners in time series forecasting. Advances in Neural Information Processing Systems, 36, 2024.   \n[46] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 11121\u201311128, 2023.   \n[47] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11106\u201311115, 2021.   \n[48] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In International conference on machine learning, pages 27268\u201327286. PMLR, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Reproducibility ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Experiment Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We make our code publicly available 2, including the backbones and baselines; the backbones and baselines code are based on their official public GitHub repositories and we use the default parameter settings or the optimal parameter settings in their paper. We used a batch size of 32, a learning rate of 0.0003, and trained each run for 100 epochs, with an early stopper set to patience as 5. For the experimental results, $K$ is set as the number of frequencies greater than $10\\%$ of the maximum amplitude. For a fair comparison, other normalization methods were also tuned accordingly to ensure optimal results, only the normalization hyperparameters were tuned, and no other experiment parameters are tuned during the experiment phase. ", "page_idx": 12}, {"type": "text", "text": "A.2 Pseudocode of GPU-Friendly Normalization ", "text_level": 1, "page_idx": 12}, {"type": "image", "img_path": "T0axIflVDD/tmp/972a62041fce40ffdf3ce86535f65abb9188c1d5267a3e37d7b7f27f7d859f0c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "B Dataset Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 Fourier Amplitude Distribution ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Frequency amplitude variation and composition are closely related to the non-stationary pattern shift [20]. To analyse its effect, we use $L=96$ to plot the frequency amplitude distribution of all input windows used in our eight benchmarks in Fig. 7. ", "page_idx": 12}, {"type": "text", "text": "In Fig. 7, it can be clearly observed that in many datasets such as ETTh1, ETTh2, ETTm1, Traffic, and Electricity datasets, besides the low-frequency trend patterns, the high-frequency parts also exhibit significant variations, especially in ETTh1, ETTm1, and Traffic datasets. This may also be the reason for our significant improvements on these datasets (with maximum improvements of $19.90\\%$ , $7.02\\%$ , and $18.65\\%$ respectively). However, in the ETTh2, ExchangeRate, and Weather datasets, which have relatively low seasonal variation, our model\u2019s improvement compared to state-of-the-art methods is relatively smaller. This is naturally because these datasets do not contain much seasonal non-stationary information for further improvement. ", "page_idx": 12}, {"type": "text", "text": "B.2 Main Frequency Density ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "FAN select top $K$ amplitude signals, compared to previous methods based on Fourier transform, we do not use a fixed frequency set [41, 27] or randomly select [48] the frequencies. This is aligned with our observations in real data: the principal frequency signals may have a distinct distribution, rather than being composed solely of fixed or pure random frequency signals. We plot the probability of input frequencies being selected into the top 10 signals in the input signal, as shown in Fig. 8. Although the low-frequency trend signals dominate in amplitude, many high-frequency signals still play a dominant role in some inputs, highlighting the importance of considering the entire spectrum, not just the low/high or random selected frequencies. Furthermore, this analysis shows that there may be significant differences of the main frequency components between different inputs. ", "page_idx": 12}, {"type": "image", "img_path": "T0axIflVDD/tmp/12228f1990a56105bac938ccb4d42f64dc1216b0c522a82ceff84477e20e6f58.jpg", "img_caption": ["Figure 7: The distribution of the various Fourier components of the data, we display the first 30 frequencies. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "However, previous methods based on the Fourier transform assumed that the main frequency signal is constant across inputs [41, 27]. In contrast, our model can dynamically extract Fourier-based signals from the inputs which enables better extraction of seasonal information, especially when the input patterns vary a lot. ", "page_idx": 13}, {"type": "image", "img_path": "T0axIflVDD/tmp/65d545db61b1ea38a9396378069bce5ab7b0b0e30b83ff59fb7a8934b21859f2.jpg", "img_caption": ["Figure 8: Probability density of frequencies get selected in the top 10 removal process, we use an input length $L=96$ as the analysis length. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "B.3 Variation of Main/Residual Components ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We examine the relative variations of the normalized main and residual components in Fig. 9. The quantitative results are obtained by calculating the relative amplitude variations of the Fourier components between the input and output in the frequency domain. In particular, across all benchmarks, the variations in the main frequency components are smaller than those in the residuals. We believe that this is the reason why a simple MLP is effective enough to capture the main frequency variations, as its shift is relatively small, as shown in the Appendix D.2. ", "page_idx": 14}, {"type": "image", "img_path": "T0axIflVDD/tmp/a4495e14b37cdeee25ebd788621c980e90cf193d554ee82bae2180f2dd456965.jpg", "img_caption": ["Figure 9: The relative changes in amplitude of the main/residual frequency components in the time domain. The results are evaluated using input length $L=96$ and averaged across the whole dataset. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B.4 Trend/Seasonal Variation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We explain more details of how the trend and seasonal variation in Table 1 are calculated. ", "page_idx": 14}, {"type": "text", "text": "Trend Variation To capture global trend shifts, we calculate the mean values over different regions of the dataset. Specifically, given a timeseries dataset $\\mathcal{X}\\in\\mathbb{R}^{N\\times D}$ , we first chronologically split it into $\\chi^{\\mathrm{train}},\\chi^{\\mathrm{val}}$ , and $\\mathcal{X}^{\\mathrm{test}}$ , representing the training, validation, and testing datasets, respectively. The trend variations are then computed as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{Trend\\;Variation}=\\left|\\frac{\\mathrm{Mean}_{N}(\\chi^{\\mathrm{train}})-\\mathrm{Mean}_{N}(\\chi^{\\mathrm{val,test}})}{\\mathrm{Mean}_{N}(\\chi^{\\mathrm{train}})}\\right|\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the subscripts indicate the dimension of mean, $|\\cdot|$ denotes the absolute value operation, and $\\chi^{\\mathrm{val,test}}$ represents the concatenation of the validation and test sets. Note that, to obtain relative results across different datasets, the trend variation is normalized by dividing by the mean of the training dataset. We obtain the first dimension to be the value inthe main content Table 1. ", "page_idx": 14}, {"type": "text", "text": "Seasonal variations. We evaluate seasonal changes by analyzing the variations in Fourier frequencies across all input instances. Given the inputs, $\\bar{\\boldsymbol{X}}\\breve{\\in}\\breve{\\mathbb{R}}^{\\acute{N}_{i}\\times\\bar{\\boldsymbol{L}}\\times\\acute{\\boldsymbol{D}}}$ where $N_{i}$ is the number of inputs. We first obtain the FFT results of all inputs, denoted as $Z\\in\\mathbb{C}^{N_{i}\\times L\\times D}$ . Then, we calculate the variance across different inputs and normalize this variance by dividing by the mean of each input as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\mathrm{Seasonal~Variation}}={\\frac{\\operatorname{Var}_{N_{i}}[{\\mathrm{Amp}}(Z)]}{\\mathbf{Mean}_{L}(X)}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the subscripts indicate the dimension of the operation. We sum the results across all channels for the value in Table 1. ", "page_idx": 14}, {"type": "text", "text": "C Theoritical Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This section discusses the effect of FAN on stationarity and temporal distribution in a theoretical perspective. We conclude that FAN enhances the stationarity of the input and mitigates distribution in the time domain. ", "page_idx": 14}, {"type": "text", "text": "C.1 Preliminary ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Discrete Fourier Transform. Given a multivariate time series input $\\mathbf{X}$ , we independently apply the 1-dim Fourier transform to each dimension $\\mathbf{x}^{(i)}$ , hence, we illustrate in vector settings. For a discrete ", "page_idx": 14}, {"type": "text", "text": "time series vector $\\mathbf{x}\\in\\mathbb{R}^{L}$ with $L$ time steps, it is transformed into Fourier domain by applying the 1-dim DFT, and can be transformed back using 1-dim IDFT, which can be defined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\mathrm{DFT}:\\ }}&{{\\mathbf{z}[w]=\\displaystyle\\sum_{t=0}^{L-1}\\mathbf{x}[t]\\cdot e^{-i2\\pi{\\frac{w t}{L}}}}}\\\\ {{\\mathrm{IDFT}:\\ }}&{{\\mathbf{x}[t]=\\displaystyle\\frac{1}{L}\\sum_{w=0}^{T-1}\\mathbf{z}[w]\\cdot e^{i2\\pi{\\frac{w t}{L}}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $w$ is current frequency, $t$ is current time step, and $\\mathbf{z}$ represents the Fourier transformation results which is a complex vector with real and imaginary parts, the amplitude and phase can be calculated as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Mag:}\\;\\mathbf{a}[w]=\\frac{\\sqrt{\\mathrm{Re}(\\mathbf{z}[w])^{2}+\\mathrm{Im}(\\mathbf{z}[w])^{2}}}{L}}\\\\ &{\\mathrm{Pha:}\\;\\mathbf{p}[w]=\\tan2(\\mathrm{Im}(\\mathbf{z}[w]),\\mathrm{Re}(\\mathbf{z}[w]))}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathrm{Im}({\\bf z}[m])$ and $\\mathrm{Re}({\\bf z}[m])$ indicate imaginary and real parts of a complex number, and atan2 is the two-argument form of arctan. ", "page_idx": 15}, {"type": "text", "text": "Distribution Of Fourier Components. The distributions of Fourier amplitude and phase can be modeled as Rayleigh distribution and uniform distribution respectively [14], thus the probabilistic density function can be represented as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\begin{array}{c}{f(a,p)=\\operatorname{Rayleigh}\\left(a\\mid\\sigma\\right)\\cdot\\operatorname{U}(p\\mid0,2\\pi)}\\\\ {={\\frac{a}{2\\pi\\sigma^{2}}}\\cdot\\exp\\left(-{\\frac{a^{2}}{2\\sigma^{2}}}\\right)}\\\\ {(a\\geq0,0\\leq p\\leq2\\pi).}\\end{array}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where a and p denotes a amplitude and phase scalar variable, $\\sigma$ is the scale parameter of the distribution. Thus, the frequency domain distribution ( $^T$ non-identically-distributed variables) can be modeled as a joint Rayleigh distribution with different scale parameters: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(\\mathbf{a},\\mathbf{p})=\\operatorname{Rayleigh}\\left(\\mathbf{a}\\mid\\sigma\\right)\\cdot\\operatorname{U}(\\mathbf{p}\\mid0,2\\pi)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C.2 Variance Over Spectrum ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Along with the time series spectral theory [35], a time series with smaller variance in the spectrum is more stationary, in this section, we try to prove the proposed FAN can reduce the variance over spectrum, thus enhance the stationarity of the input data. Hence, we prove that, given an univariate time series real value vector $\\mathbf{x}\\in\\mathbb{R}^{T}$ , after removing main frequency components $\\mathbf{z}[k]\\in\\mathcal{K}$ , the variance on spectrum can be reduced $\\mathrm{Var}\\left(\\mathbf{a}^{r e s}\\right)<\\mathrm{Var}\\left(\\mathbf{a}\\right)$ . ", "page_idx": 15}, {"type": "text", "text": "Here, the marginal distribution of the amplitude vector (the spectrum) a is represented as a joint Rayleigh distribution with different scale parameters: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle f({\\bf a})=\\int f({\\bf a},{\\bf p})d{\\bf p}}}\\\\ {{\\displaystyle~~~~=\\prod_{i=1}^{L}\\frac{a}{\\sigma_{i}^{2}}\\cdot\\exp\\left(-\\frac{a^{2}}{2\\sigma_{i}^{2}}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that although we assume that the frequency components are independent with each other, this assumption is actually widely used [16] since it is quite possible that a specific component change independently, e.g., the daily weekly changes while the monthly periodicity stays the same. Following the principle of additivity of variance for independent variables [13], the variance of the amplitude vector a can be expressed as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left(\\mathbf{a}\\right)=\\sum_{i}^{L}\\frac{4-\\pi}{2}\\sigma_{i}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "after removing frequencies $k\\in\\mathcal{K}$ , the joint distribution actually becomes: ", "page_idx": 16}, {"type": "equation", "text": "$$\nf(\\mathbf{a}^{r e s})=\\prod_{i=1,i\\notin K}^{L}\\frac{a}{\\sigma_{i}^{2}}\\cdot\\exp{\\left(-\\frac{a^{2}}{2\\sigma_{i}^{2}}\\right)}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "thus, the variance of the whole distribution after removing top $K$ -amplitude signals reduces to a smaller number, since the independent variance of of each dimension is positive, which is: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left(\\mathbf{a}^{r e s}\\right)=\\sum_{i=1,i\\notin{K}}^{L}\\frac{4-\\pi}{2}\\sigma_{i}^{2}<\\mathrm{Var}\\left(\\mathbf{a}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C.3 Influence On Temporal Distribution ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Relation with Temporal Statistics. The zero frequency of the Fourier transform divided by $L$ is actually the mean of the statistical measure, and the energy of the Fourier transform of frequency components above zero is equivalent to the variance of the input scaled by $L$ , proved as follow: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[{\\bf x}]=\\frac{1}{L}\\sum_{t=0}^{L-1}{\\bf x}[t]=\\frac{1}{L}{\\bf z}[0]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "According to Parseval theorem [5], for a discrete signal $\\mathbf{x}$ , its energy is identical in both the time and frequency domain: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{L-1}|\\mathbf{x}[t]|^{2}=\\sum_{w=0}^{L-1}|\\mathbf{z}[w]|^{2}=L\\mathbb{E}[\\mathbf{x}^{2}]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus the variance of the input signal can be defined as the energy of Fourier components with frequency above zero. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{Var}[\\mathbf{x}]=\\mathbb{E}[\\mathbf{x}^{2}]-\\mathbb{E}^{2}[\\mathbf{x}]}\\\\ {\\displaystyle=\\frac{1}{L}\\sum_{w=0}^{L-1}|\\mathbf{z}[w]|^{2}-\\frac{1}{L^{2}}|\\mathbf{z}[0]|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Influence On Mean. Since the mean is equal to the zero frequency component in time domain and for any other components, the expectation is zero since they are all number of periodic $\\mathrm{sin}/\\mathrm{cos}$ signals, after removing the zero frequency component, the expectation is then equal to zero. Due to this property, this is also known as the \u2019detrending\u2019 in traditional signal processing [32], proved as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{x}^{r e s}]=\\mathbb{E}[\\mathbf{x}-\\mathrm{IDFT}(\\mathbf{z}[0])]=\\mathbb{E}[\\mathbf{x}-\\frac{1}{L}\\mathbf{z}[0]]=\\mathbb{E}[\\mathbf{x}-\\mathbb{E}[\\mathbf{x}]]=0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Influence On Variance. Since the The normalization step select and remove top $K$ amplitude Fourier components, the Fourier spectrum energy will be significantly diminished, defined as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{w=0,w\\not\\in\\mathcal{K}}^{L-1}|\\mathbf{z}[w]|^{2}\\ll\\sum_{w=0}^{L-1}|\\mathbf{z}[w]|^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "thus, the input variance then can be largely reduced, which is: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Var}[{\\bf x}^{r e s}]\\ll\\mathrm{Var}[{\\bf x}]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In summary, our method can effectively reduce the range of data distribution, which is crucial for enhancing the performance of the backbone model and minimizing the risk of overfitting [12]. ", "page_idx": 16}, {"type": "text", "text": "C.4 Fourier Spectrum Empirical Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The variance in the Fourier spectrum is an important indicator reflecting stationarity [20]. The closer the frequency components are to each other, the smaller the variance between the components, thus the stronger the stationarity [35]. Therefore, we compare the changes in frequency domain components for different methods and present the results in Fig. 10. In Fig. 10, after FAN\u2019s normalization step, the distribution exhibits alignment of the input and output, and the range of the distribution mean has decreased to 8, compared with previous methods which are round 80, 70, 70 respectively for SAN, Dish-TS and RevIN. However, other methods still show significant differences between the input and output distributions, with the range of the frequency domain amplitude distribution reaching up to 80, indicating the presence of strong non-stationary signals. This highlights the effectiveness of our method in handling non-stationarity, especially for seasonal periodic signals, which previous methods have not successfully considered. ", "page_idx": 17}, {"type": "image", "img_path": "T0axIflVDD/tmp/47b2a68c062b21648f43b4815863f1184081c88b745df6ff18c4fdc2e1301069.jpg", "img_caption": ["Figure 10: Fourier spectrum on polar axis of ETTm2 dataset with $L=96$ after various normalization methods. Each point indicates one frequency component averaged across the dataset. The blue dots indicate the input Fourier components, the orange dots represent the output Fourier components. FAN remove top 5 Fourier components, and SAN slice in 12. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D Ablation Study ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Hyperparameter Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our model incorporates a hyperparameter $K$ , which represents the maximum frequency count selection. In this section, we provide a sensitivity analysis for this parameter in Fig. 11. We observe that our proposed FAN achieves stable performance across various parameter settings. ", "page_idx": 17}, {"type": "image", "img_path": "T0axIflVDD/tmp/b9d93d60efe1c13e8238b95bb0ecea1034010992952e7fbf341cf9e8f618835a.jpg", "img_caption": ["Figure 11: Sensitivity analysis of hyper-parameter $K$ , and we select dataset-specific $K$ across the experiments. The purple line denote the selected $K$ through our $10\\%$ of largest magnitude selection rule. We use DLinear as backbone with and use MSE as the evaluation metric, other settings are identical with the main results settings. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Moreover, from Fig. 11, we note the following observations: (1) As the prediction length increases, the need for a larger $K$ becomes more apparent, significantly enhancing performance. This is likely due to the increased prediction steps that bring more frequency information into the model. For example, in the ExchangeRate dataset, when the prediction length is 720, $K\\,=\\,16$ outperforms using only 3 and 6 frequencies. (2) In datasets with higher sampling rates, such as Traffic and ETTm2, larger $K$ values enhance performance at all prediction steps. This could be attributed to the finer granularity of sampling in these datasets (minute-level) compared to yearly sampling in ExchangeRate and hourly in ETTh2, resulting in richer frequency signals in ETTm2 and Traffic, thereby enhancing FAN\u2019s performance on these datasets. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "D.2 Pattern Prediction Module ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To justify our rather simple MLP structure for predicting the future main frequency component, we extended the basic MLP with three additional layers to observe the results. These layers include ", "page_idx": 18}, {"type": "text", "text": "\u2022 $+\\mathbf{M}\\mathbf{L}\\mathbf{P}$ : adding an additional MLP layer on top of the basic MLP.   \n\u2022 $+\\mathrm{GRU}$ [4]: adding a GRU layer, which is a recurrent neural network, mitigates the problem of gradient vanishing through the gating mechanism.   \n\u2022 $^+$ TSMixer [7]: adding a TSMixer layer, which is a state-of-the-art lightweight model that also considers inter-dimensional relationships. ", "page_idx": 18}, {"type": "text", "text": "We perform ablation on ETTh1, ExchangeRate, Weather datasets, under experiment settings of Section 4.2, we report the MAE/MSE evaluation metrics, and the result is shown at Table 6. ", "page_idx": 18}, {"type": "text", "text": "In Table 6, the three-layer MLP of FAN performed best overall on three datasets and more complex models tend to perform worse. We believe this is due to the following reasons: (1) The main frequency signal provides a baseline position for the backbone model, leading to more robust predictions and thus to greater model robustness. (2) The main frequency signal is subject to underlying physical characteristics, resulting in relatively slower changes. This has been verified by observations in Appendix B.3, showing that the main frequency signal changes at least $40.27\\%$ more slowly compared to the residual frequency signal. Therefore, a simple three-layer MLP is sufficient to provide effective and somewhat more robust predictions. However, a four-layer MLP and a GRU also achieved the best performance in some metrics, indicating that there is still room for improvement in future work. ", "page_idx": 18}, {"type": "table", "img_path": "T0axIflVDD/tmp/7f50004930c3882d4151142667de09ca0d43b5b64cb2001a3cf5e515e9b7e141.jpg", "table_caption": ["Table 6: Multivariate Forecasting MAE/MSE results mean and standard deviation, the bold letter indicates the best performance. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "E Full Results And Discussions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "E.1 Experiment On Synthetic Data ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To fully demonstrate the effectiveness of our method on signals with varying non-stationary frequencies, we generated a synthetic multidimensional time-series dataset using composite sinusoidal signals [32] to verify the effectivenss of FAN on evolving non-stationary time series. Each dimension is composed of $i$ superimposed sinusoidal signals with linearly changing amplitude, generated as $\\begin{array}{r}{\\chi_{t}^{(i)}=\\sum_{j=1}^{i}\\!a_{t}\\sin{\\frac{2\\pi}{T_{j}}}t,\\quad i=1,\\dots,D}\\end{array}$ . where $a_{t}$ is the signal amplitude, $T_{i}$ are the periodicities, for example, daily periodicity in hour $T_{i}=24$ , and $t$ is the current time step. ", "page_idx": 18}, {"type": "text", "text": "In the synthetic dataset, each synthetic signal is a combination of multiple sinusoidal signals with linear changes and fixed periods, and varies in the training, validation, and test sets. In Table 7, we list the settings of these signals, every synthetic signal is a composition of these signals, e.g., Syn-5 contains Sig1-5, Syn-9 contains Sig1-9, the generation code can also be found in our code repository2. ", "page_idx": 19}, {"type": "table", "img_path": "T0axIflVDD/tmp/9d626acdd9abfb6e6a6e44d0ba3d8a6dc97f0494ccd7f19ab64a0bbb26539a40.jpg", "table_caption": ["Table 7: Synthetic signal settings, the amplitude changes linearly in train/val/test sets "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "We conduct a 720-step multivariate forecasting experiment on synthetic data using DLinear as the backbone model and compared with other reversible normalization methods. Results are shown in Table 8. We observe averaged improvements ranging from $19.75\\%$ to $47.04\\%$ . As the number of composite frequencies increases (from Syn-5 to Syn-9), the prediction difficulty escalates. Previous normalization methods failed to make further improvements as probably because they can not address seasonality patterns shift. Conversely, our model\u2019s enhancements steadily increase. The MAE/MSE improvements ranges from $19.75\\%$ to $32.45\\%$ and from $23.76\\%$ to $41.75\\%$ . This underscores the effectiveness of FAN in handling intricate seasonality patterns. ", "page_idx": 19}, {"type": "table", "img_path": "T0axIflVDD/tmp/fc8f9ef2cb67077aebbf9c593d920b2c3fee1a3818805f70af14430f63bb9818.jpg", "table_caption": ["Table 8: Forecasting errors under the multivariate setting. The bold values indicate best performance. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "We compare the performance of different normalization methods in Fig. 12. Beyond performance, FAN also surpasses other models in performance on synthetic datasets, requiring only three epochs to achieve a smaller test loss compared to ten epochs required by other normalizations. This demonstrates that varying periodic signals indeed affect the predictive performance of models and the non-stationarity that normalization methods must counteract, with our model proving effective in handling these challenges. ", "page_idx": 19}, {"type": "image", "img_path": "T0axIflVDD/tmp/d78cf87ad7d35f8f266822e5399d5ca5ef8d8e50af305dcc69d3a4ca07b7490c.jpg", "img_caption": ["Figure 12: Performance comparison with different normalization baselines, where FAN clearly outperforms others in this varying frequencies condition. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E.2 Full results of ETT benchmarks ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We present the full results of ETT benchmarks in Table 9. In the entire ETT benchmarks, FAN demonstrates improvements over the original models in 114 out of 128 metrics. Specifically, our model exhibits average enhancements of, $18.43\\%$ , $31.64\\%$ , and $12.04\\%$ for FEDformer, Informer, and SCINet respectively. It\u2019s worth noting that despite FEDformer\u2019s utilization of Fourier transform for seasonality analysis, it still struggles with handling changing seasonality patterns, thus leaving room for our $31.64\\%$ improvement. However, FAN fails to enhance the DLinear backbone in the ETTm1 dataset. We attribute this to the inherently higher stationarity of the ETTm1 dataset and its fewer trend changes (Table 1), which may not align well with the DLinear model which is based on the moving average. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "table", "img_path": "T0axIflVDD/tmp/a4fd025caf39483bfa992eb2baf3bf9ece5ac8045cf2e35280981ab2f7a1f69f.jpg", "table_caption": ["Table 9: Full results on ETT benchmarks. The bold values indicate best performance. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E.3 FAN for Fourier-based Bakcbones ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To demonstrate the effectiveness of our method in extracting non-stationary seasonal patterns, we select two other models based on the Fourier transform and perform additional experiments, including: (1) TimesNet [41], which generates 2D variations of time series data using Fourier Transform; (2) Koopa [27], which employs Fourier Transform for dynamic time series modeling based on koopman theory. It is important to note that although both models extract the top $k$ signals, their main frequency selection is based on the average dimensions of the training set rather than the input-specific, which may limit their models\u2019 ability to handle varying dimensions and inputs. Furthermore, they internally use RevIN [17] in their model implementation, for a fair comparison, we remove this part or replace it with FAN, we present the experiment results in Table 10. ", "page_idx": 20}, {"type": "text", "text": "As in Table 10, Even with state-of-the-art models based on Fourier-transform, our model still demonstrates performance improvements across almost all datasets. Specifically, for long inputs, our model consistently shows performance enhancements. In particular, on the Exchange dataset, our model achieves a maximum improvement of $84.85\\%$ in TimesNet and $26.10\\%$ in Koopa. We believe the significant improvement in TimesNet is due to its lack of handling seasonal non-stationarity compared to Koopa. However, Although Koopa explicitly handle non-stationarity, we still observe improvements in Koopa. For the ETTm2, Electricity, Traffic, and Weather datasets, our model shows stable MSE performance large improvements for short-term 96 steps and long-term 720 steps inputs by $4.71\\%/6.09\\%$ , $1.30\\%/8.57\\%$ , - $.1.96\\%/2.57\\%$ , and - $-1.78\\%/2.43\\%$ , respectively. This improvement is likely due to our model\u2019s approach of learning directly from the changes in primary frequency components and our instance-wise and dimension-specific frequency analysis. ", "page_idx": 20}, {"type": "text", "text": "E.4 Full Results of Baselines Comparison ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Table 11, we provide the detailed experimental results of the comparison between FAN and state-of-the-art normalization methods for non-stationary time series normalization: RevIN [17], Dish-TS [10], and SAN [28]. ", "page_idx": 20}, {"type": "text", "text": "The table clearly shows that FAN outperforms existing approaches in most cases, particularly in the ExchangeRate dataset, our long-term 720-step prediction significantly outperforms other baselines, highlighting the importance of handling seasonal non-stationary information in long-term predictions. However, we fail to make further improvement on Weather and ETTh2 dataset. Considering that in these two datasets, the seasonal variation is very small (as in Appendix B.1), the limited seasonal non-stationary information might have led to the inability to further optimization. ", "page_idx": 20}, {"type": "table", "img_path": "T0axIflVDD/tmp/5f8bee7ba80bf922954c401805f09ec6a78723ee8e1ed9cd2928ca618c2a07f1.jpg", "table_caption": ["Table 10: Mulltivariate long-term forecasting for Fourier-transform based backbones. The bold letter indicates the best result. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "F Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Though FAN shows promising performance, there are still some limitations. First, we removed a significant amount of non-stationary trend and seasonal information. While this is effective in most baselines, it may lead to an over-stationary issue, causing a decline in the backbone\u2019s performance. Second, we largely select the frequency counts $K$ in a search-based manner or based on dataset priors. This approach may lead to incorrect $K$ value selection, resulting in an under-stationary issue or more severe over-stationary issue. Additionally, our non-stationary pattern extraction is based on the Fourier transform, and a finite number of Fourier signals cannot represent all periodic signals, which may hinder our ability to handle some waveforms, e.g. square waves. Therefore, future work can focus on more effective instance-specific $K$ value selection, strict dynamic control of non-stationarity elimination, and other methods for extracting non-stationary waveforms. ", "page_idx": 21}, {"type": "table", "img_path": "T0axIflVDD/tmp/c041f73da8534aeaceb4bf38fff7c54a1e46596060a23b70048208b9944dfe81.jpg", "table_caption": ["Table 11: Forecasting errors under the multivariate setting. The bold values indicate best performance. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 23}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 23}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 23}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 23}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \u201d[Yes] \u201d is generally preferable to \u201d[No] \u201d, it is perfectly acceptable to answer \u201d[No] provided a proper justification is given (e.g., \u201derror bars are not reported because it would be too computationally expensive\u201d or \u201dwe were unable to find the license for the dataset we used\u201d). In general, answering \u201d[No] \u201d or \u201d[NA] \u201d is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. IMPORTANT, please: ", "page_idx": 23}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201dNeurIPS paper checklist\u201d, \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: This paper proposes a novel normalization method for non-stationary time series forecasting that could handle both evolving trend and seasonal patterns. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We discuss the limitation of proposed method, FAN, at Appendix F. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: All assumptions and proofs are clearly stated in our manuscript and are detailed in Appendix C. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The experimental settings are detailed in Sec. 4.2 and Appendix A.1. All experiments can be easily reproduced with our code.   \nGuidelines: ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We make our code publicly available at 2, including data and detail documentation. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 25}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The experimental settings and hyperparameter selection details are in Sec. 4.1. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: See the results with standard deviation in Table 4, 6. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We discuss the experiments compute resources at Sec. 4.1. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Yes, this paper conform the the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper is a foundational research and has no direct negative social impacts. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: No such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All data and methods are explicitly mentioned. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All data, code, and documentation are given in the Supplementary Material and our public repository 2. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve human subjects. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve human subjects. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]