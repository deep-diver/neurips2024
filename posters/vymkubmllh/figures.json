[{"figure_path": "vymkuBMLlh/figures/figures_1_1.jpg", "caption": "Figure 1: (Top: x-ray to report generation task) (a) do(X = x) removes the X \u2192 R bias and makes the generation of R domain invariant. P(r|do(x)) is factorized into c-factors and (b) conditional models ({MV}k) are trained for each factor (shown as boxes). (c) The intervened value X = x is propagated through the merged network and samples from the P(r|do(x)) are generated.", "description": "This figure illustrates the proposed algorithm ID-GEN for generating interventional samples from a high-dimensional distribution.  It shows how a sequence of conditional generative models are used to sample from a desired interventional distribution, specifically P(r|do(x)), which is the interventional distribution of the report (R) given an intervention on the x-ray (X). The top panel illustrates a causal graph showing that hospital location (H) is a confounder between X and R.  Part (a) shows how intervening on X removes the confounding bias introduced by H. Part (b) shows the training of conditional generative models, one for each factor in the factorization of P(r|do(x)). Part (c) depicts the merging of these models to create a sampling network, which is used to generate samples from the desired interventional distribution.", "section": "1 Introduction"}, {"figure_path": "vymkuBMLlh/figures/figures_4_1.jpg", "caption": "Figure 2: \u2194:Unobserved. Left blue samples from Px,w2 (W1,y) = P(W1|x) P(y|x, W1,W2). Right blue samples from Px,w\u2081 (W2) = \u2211x' P(x') P(w2|x', w\u2081). Joint network samples from Px(y).", "description": "This figure illustrates how ID-GEN merges sampling networks to sample from a causal query with latent confounders.  The causal graph has unobserved confounders represented by \u2194.  ID-GEN factorizes the query into sub-problems (represented by the blue samples from  Px,w2 (W1,y) and Px,w\u2081 (W2)). Then, it trains conditional models for each sub-problem and merges them into a single sampling network, to finally generate samples from the desired distribution Px(y).", "section": "3.2 Recursive training of ID-GEN and interventional sampling"}, {"figure_path": "vymkuBMLlh/figures/figures_6_1.jpg", "caption": "Figure 3: (Left: top-down) Pw\u2081 (y) is factorized into Pw1,x,y(W2), Pw1,w2,y(x) and Pw1,w2,x(y) (Step 4). Steps 7, 2, 6 is shown for Pw1,w2,x(y) only. (Right: bottom-up) we combine the sampling networks of each c-factor. For any do(W\u2081 = w\u2081), we use H to get samples from Pw\u2081 (y).", "description": "This figure illustrates the process of ID-GEN algorithm for sampling from a high-dimensional interventional distribution P(y|do(w1)). The left panel shows the top-down factorization of the query according to the ID algorithm steps and illustrates how the algorithm decomposes the problem into subproblems to address high-dimensionality. Each subproblem involves training a set of conditional generative models. The right panel shows the bottom-up merge process of the sampling network, connecting the trained models to build a single network capable of generating samples from the desired interventional distribution.", "section": "3 ID-GEN: generative model-based interventional sampling"}, {"figure_path": "vymkuBMLlh/figures/figures_7_1.jpg", "caption": "Figure 4: (Left:) Causal graph with color and thickness as unobserved. (Center:) FID scores (lower the better) of each algorithm and images generated from them. (Right:) Likelihood calculated from the Px(y) images generated by each algorithm. We closely reflect the true Px(y) with low TVD.", "description": "This figure shows the results of an experiment on a semi-synthetic Colored-MNIST dataset.  The left panel displays a causal graph illustrating the relationships between variables (W1, W2, X, Y) with unobserved confounders (Ucolor and Uthickness). The center panel presents FID scores, a measure of image quality, for different algorithms (Conditional, DCM, NCM, Ours) in generating images of digits 3 and 5 with specific colors.  Lower FID scores indicate better image quality. The right panel shows the total variation distance (TVD) between the likelihoods calculated from generated images and true likelihoods, with lower TVD suggesting closer matches to the ground truth.  The results demonstrate that the proposed approach (Ours) outperforms baseline methods in generating higher-quality images that more accurately reflect the true distribution.", "section": "4.1 ID-GEN performance on napkin-MNIST dataset and baseline comparison"}, {"figure_path": "vymkuBMLlh/figures/figures_7_2.jpg", "caption": "Figure 5: i) Graph and sampling network for PMale(I2). ii) For both causal and non-causal attributes, EGSDE shows high correlation.", "description": "This figure demonstrates the causal graph used to perform interventional sampling on the CelebA dataset.  The graph shows that the image I1 is influenced by attributes Male and Young, with a latent confounder between them. The algorithm trains conditional models for each factor to generate samples from P(I2|do(Male=0)). The bar chart visualizes the correlation between various attributes and the Male attribute, highlighting the effect of intervening on the Male attribute using two different generative models.", "section": "4.2 Evaluating CelebA image translation models with ID-GEN"}, {"figure_path": "vymkuBMLlh/figures/figures_8_1.jpg", "caption": "Figure 6: Left: Baseline vs our causal graph. Right: images for specific prompt w/ and w/o pneumonia. Inferred attributes are shown with their likelihood. Blue indicates changes compared to healthy.", "description": "This figure compares the baseline causal graph with the true causal graph proposed by the authors. The baseline assumes a direct relationship between the report and X-ray image, while the true graph includes latent confounders and intermediate variables such as pneumonia, pleural effusion, and atelectasis, which are more causally related to the X-ray image.  The right side shows sample images generated for different prompts with and without pneumonia, along with the likelihood of each inferred attribute (blue highlighting indicates changes from a healthy case).", "section": "4.3 Invariant prediction with foundation models for chest X-ray generation"}, {"figure_path": "vymkuBMLlh/figures/figures_15_1.jpg", "caption": "Figure 7: do(R = r) removes the R \u2192 X bias and makes prediction of X domain invariant. ID-GEN factorizes P(v|do(r)) into four factors and trains conditional models ({Mv; }\u00bf) for each (blue shades). The intervened value R = r is propagated through the merged network to generate all other variables.", "description": "This figure shows how the proposed algorithm, ID-GEN, handles high-dimensional interventional sampling in the presence of latent confounders.  It illustrates the process for the report-to-X-ray generation task. The algorithm factorizes a complex causal query (P(v|do(r))) into simpler factors, trains conditional generative models for each factor, and merges these models into a neural network. The intervention, do(R=r), is then applied to remove confounding bias. Ancestral sampling through the resulting network generates samples from the desired high-dimensional interventional distribution.", "section": "4 Experiments"}, {"figure_path": "vymkuBMLlh/figures/figures_15_2.jpg", "caption": "Figure 8: \u2194:Confounding. Mw\u2081, My sample from Px,w2 (W1,y) = P(w1|x) P(y|x, W1,W2). Mx', Mw2 sample from Px,w\u2081 (W2) = \u2211x, P(x') P(w2|x', w\u2081). Joint network samples from Px(y).", "description": "This figure demonstrates how ID-GEN addresses cyclic dependency issues that may arise when training generative models for causal inference.  It shows a causal graph with confounding variables and unobserved confounders between X and Y. ID-GEN addresses this issue by factorizing the joint distribution P(x, y) into two factors and training conditional generative models for each factor and merging them to build a sampling network for generating interventional samples.", "section": "Recursive training of ID-GEN and interventional sampling"}, {"figure_path": "vymkuBMLlh/figures/figures_16_1.jpg", "caption": "Figure 9: Suppose we aim to sample from P(m|do(v)), with M(MRI) as high-dimensional. Group 1 includes algorithms (ex: ID) that depend on likelihood estimation such as P(m|v, a,b). Group 2 algorithms with GAN architectures have issues with GAN convergence while Group 3 improves convergence with modularization but both struggle to match the joint distribution. Group 4 utilizes normalizing flow, diffusion models, etc but cannot deal with confounders. Groups 2-4 only do unconditional interventions or costly rejection sampling. Finally, our method can employ classifier-free diffusion models to sample from P(m|do(v)) or conditional P(m|a, do(v)). The causal graph is adapted from Ribeiro et al. [41].", "description": "This figure compares different approaches to high-dimensional causal inference, highlighting the challenges of existing methods in handling high-dimensional data and unobserved confounders.  It shows how the proposed ID-GEN algorithm addresses these challenges by employing conditional generative models, specifically diffusion models, to efficiently sample from interventional distributions.", "section": "Related works"}, {"figure_path": "vymkuBMLlh/figures/figures_17_1.jpg", "caption": "Figure 10: ID-GEN Recursion Tree Example", "description": "This figure illustrates a possible recursive route of the ID-GEN algorithm for a causal query P(y|do(x)). Each node represents a step in the algorithm, with red edges indicating the top-down phase and green edges indicating the bottom-up phase. Rectangular boxes represent functions used by the algorithm. The figure helps to understand the recursion steps of the ID-GEN algorithm.", "section": "3 ID-GEN: generative model-based interventional sampling"}, {"figure_path": "vymkuBMLlh/figures/figures_20_1.jpg", "caption": "Figure 10: ID-GEN Recursion Tree Example", "description": "This figure illustrates a possible recursive trace of the ID-GEN algorithm for a causal query P(y|do(x)). The different colors of edges indicate the direction of the recursion (top-down and bottom-up). Each box represents a function call within the ID-GEN algorithm (ConditionalGMs, MergeNetwork, Update). The numbers within the boxes represent line numbers within the ID-GEN algorithm, indicating the execution flow in the algorithm.", "section": "3 ID-GEN: generative model-based interventional sampling"}, {"figure_path": "vymkuBMLlh/figures/figures_23_1.jpg", "caption": "Figure 10: ID-GEN Recursion Tree Example", "description": "This figure shows a possible recursive route of ID-GEN for a causal query P(y|do(x)). It illustrates the steps ID-GEN takes, the conditions for each step, and how the algorithm recursively breaks down the problem. Red edges represent the top-down phase, while green edges indicate the bottom-up phase. Gray boxes highlight the functions ID-GEN uses for high-dimensional sampling.", "section": "3 ID-GEN: generative model-based interventional sampling"}, {"figure_path": "vymkuBMLlh/figures/figures_26_1.jpg", "caption": "Figure 10: ID-GEN Recursion Tree Example", "description": "This figure shows a possible recursive route of ID-GEN for a causal query P(y|do(x)). At any recursion level, it checks conditions for 7 steps (S1-S7) and enters one step based on the satisfied conditions. Red edges indicate the top-down phase, and green edges indicate the bottom-up phase. Gray boxes represent functions that allow ID-GEN to sample from high-dimensional interventional distributions.  The figure helps readers understand the recursion route.", "section": "3 ID-GEN: generative model-based interventional sampling"}, {"figure_path": "vymkuBMLlh/figures/figures_27_1.jpg", "caption": "Figure 1: (Top: x-ray to report generation task) (a) do(X = x) removes the X \u2192 R bias and makes the generation of R domain invariant. P(r|do(x)) is factorized into c-factors and (b) conditional models ({MV}k) are trained for each factor (shown as boxes). (c) The intervened value X = x is propagated through the merged network and samples from the P(r|do(x)) are generated.", "description": "This figure illustrates the proposed algorithm ID-GEN. It shows how to sample from any identifiable interventional distribution, specifically P(r|do(x)), by factorizing it into c-factors and using conditional generative models.  Panel (a) depicts the problem of x-ray to report generation with unobserved confounding. Panel (b) shows how conditional generative models are trained for each factor. Panel (c) displays how the models are merged to generate samples from P(r|do(x)). The algorithm addresses the challenges of high-dimensional data and unobserved confounders in causal inference.", "section": "1 Introduction"}, {"figure_path": "vymkuBMLlh/figures/figures_31_1.jpg", "caption": "Figure 15: Joint samples from the Napkin-MNIST dataset: Samples from the Napkin-MNIST dataset are visualized as columns above. The first row indicates the latent variable color, the second row indicates the latent variable thickness, and the row labeled W2 is a discrete variable holding a (color, digit), where digit is represented as the number of dots. Notice that the noising process sometimes causes information to not be passed to children.", "description": "This figure visualizes joint samples from the Napkin-MNIST dataset. The first row shows the latent variable color, the second row shows the latent variable thickness, and the third row shows a discrete variable W2 that contains both color and digit information (digit is shown as the number of dots).  The figure demonstrates how noise sometimes prevents information from being passed down the causal graph.", "section": "F.2 Napkin-MNIST dataset"}, {"figure_path": "vymkuBMLlh/figures/figures_33_1.jpg", "caption": "Figure 16: Samples from P(I2|do(Male = 0). Row 1: original I\u2081, Row 2: translated I2 by StarGAN and Row3: translated I2 by EGSDE.", "description": "This figure shows the results of applying the ID-GEN algorithm to translate images from the male domain to the female domain. The first row shows the original male images. The second row shows the images translated using the StarGAN model. The third row shows the images translated using the EGSDE model. The results show that both models are able to successfully translate the images, however, the quality of the translated images varies depending on the model used. The StarGAN model produces blurry images, while the EGSDE model produces sharper images. This demonstrates the ability of ID-GEN to generate high-quality samples from a high-dimensional interventional distribution.", "section": "4.2 Evaluating CelebA image translation models with ID-GEN"}, {"figure_path": "vymkuBMLlh/figures/figures_34_1.jpg", "caption": "Figure 17: Multi-domain image translation by EGSDE. Images in rows 1 and 3 are the original images (male domain) and images in rows 2 and 4 are translated images (female domain). Table 3 shows, 29.16% of the total images are translated as a young person.", "description": "This figure shows the results of multi-domain image translation using the EGSDE model.  The top two rows display the original male images, and the bottom two rows show the corresponding female images generated by EGSDE.  The caption notes that, across all generated images, 29.16% were translated as \"young\". This highlights EGSDE's ability to transform images between domains while also demonstrating some biases in the model's output (e.g., a tendency to generate younger-appearing female faces).", "section": "4.2 Evaluating CelebA image translation models with ID-GEN"}, {"figure_path": "vymkuBMLlh/figures/figures_34_2.jpg", "caption": "Figure 16: Samples from P(I2|do(Male = 0). Row 1: original I\u2081, Row 2: translated I2 by StarGAN and Row3: translated I2 by EGSDE.", "description": "This figure shows the results of applying two different image translation models, StarGAN and EGSDE, to translate male faces (original images) into female faces. The first row displays the original male faces. The second row shows the female faces generated by StarGAN, and the third row displays the female faces generated by EGSDE.  The results illustrate the differences in the image quality and the types of translations performed by each model.  It helps to visually compare the performance of the two models in achieving realistic and varied translations.  This figure is part of the CelebA experiment section, showcasing the algorithm's application to a real-world dataset.", "section": "4.2 Evaluating CelebA image translation models with ID-GEN"}, {"figure_path": "vymkuBMLlh/figures/figures_35_1.jpg", "caption": "Figure 17: Multi-domain image translation by EGSDE. Images in rows 1 and 3 are the original images (male domain) and images in rows 2 and 4 are translated images (female domain). Table 3 shows, 29.16% of the total images are translated as a young person.", "description": "This figure shows the results of a multi-domain image translation experiment using the EGSDE model.  The top row shows the original male images, and the bottom row shows the corresponding female images generated by the model after translation. The model successfully translates the images, changing various attributes such as age and appearance. Notably, a significant percentage (29.16%) of the translated images appear as young.", "section": "4.2 Evaluating CelebA image translation models with ID-GEN"}, {"figure_path": "vymkuBMLlh/figures/figures_37_1.jpg", "caption": "Figure 6: Left: Baseline vs our causal graph. Right: images for specific prompt w/ and w/o pneumonia. Inferred attributes are shown with their likelihood. Blue indicates changes compared to healthy.", "description": "This figure compares the baseline model with the causal graph proposed by the authors. The left panel shows the baseline model, which directly maps the text prompt to the X-ray image without considering the causal relationships between variables. The right panel demonstrates the causal graph proposed by the authors, highlighting the causal relationships between pneumonia (N), pleural effusion (E), atelectasis (A), lung opacity (L), and the x-ray image (X). It shows example images with and without pneumonia and the likelihood of each attribute, based on text prompts. Blue highlights changes compared to a healthy X-ray image.", "section": "4.3 Invariant prediction with foundation models for chest X-ray generation"}, {"figure_path": "vymkuBMLlh/figures/figures_38_1.jpg", "caption": "Figure 4: (Left:) Causal graph with color and thickness as unobserved. (Center:) FID scores (lower the better) of each algorithm and images generated from them. (Right:) Likelihood calculated from the Px(y) images generated by each algorithm. We closely reflect the true Px(y) with low TVD.", "description": "This figure presents the results of an experiment on the Napkin-MNIST dataset. The left panel shows the causal graph used in the experiment, highlighting the unobserved confounders. The center panel compares the FID (Fr\u00e9chet Inception Distance) scores of four different methods for generating images from the interventional distribution P(Y|do(X)). Lower FID scores indicate higher image quality. The right panel illustrates the total variation distance (TVD) between the generated image distributions and the true distribution, showing that the proposed method closely matches the ground truth.", "section": "4.1 ID-GEN performance on napkin-MNIST dataset and baseline comparison"}, {"figure_path": "vymkuBMLlh/figures/figures_38_2.jpg", "caption": "Figure 23: Generated Covid XRay Images: Generated chest XRay images from our diffusion model, separated by class and compared against real data.", "description": "This figure shows a comparison of generated and real chest X-ray images. The images are separated into two classes (C=0 and C=1, likely representing the absence and presence of COVID-19, respectively) and further divided into generated and real images within each class. This visualization helps assess the quality and realism of the generated images compared to real medical data, providing a visual evaluation of the model's performance in generating realistic synthetic medical imagery.", "section": "F.4.3 Covid X-Ray generation"}]