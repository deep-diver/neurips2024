[{"figure_path": "vymkuBMLlh/tables/tables_5_1.jpg", "caption": "Table 1: Evaluations on the Napkin-MNIST generated dataset. V.d, V.c, V.t refer to the digit, color and thickness respectively of variable V. The first column is with respect to samples generated from diffusion model P(X, Y | W1, W2), Image Data is the dataset used to train P, Discrete Data is the empirical distribution according to a discrete Napkin-MNIST, and the ground truth is analytically computed. Ideally all values should be equal across a row. While our synthetic dataset generated from P is not a perfect representation, it is quite close in all attributes except thickness. This is because the classifier for thickness has some inherent error in it, as evidenced by the mismatch between the base data and ground truth in the thickness rows.", "description": "This table compares the results obtained from three different methods for generating samples from the Napkin-MNIST dataset: a diffusion model, the original dataset, and an analytically computed distribution.  The comparison is made using several metrics for the digit, color, and thickness of the generated images, revealing the accuracy of each method.", "section": "4.1 ID-GEN performance on napkin-MNIST dataset and baseline comparison"}, {"figure_path": "vymkuBMLlh/tables/tables_33_1.jpg", "caption": "Table 2: Color probability distribution of the sampled images. P(y.c|x.c = red) is biased towards [R, G, B] while P(y.c|do(x.c = red)) is not.", "description": "This table shows the color probability distribution of generated images from two different methods: a conditional model and the proposed ID-GEN method.  The conditional model shows a bias towards red, green, and blue colors when the input image (x) has a red color.  In contrast, the ID-GEN method demonstrates a more even distribution of colors, indicating that it successfully removes the spurious correlation between the input and output colors.", "section": "4.2 Evaluating CelebA image translation models with ID-GEN"}, {"figure_path": "vymkuBMLlh/tables/tables_33_2.jpg", "caption": "Table 1: Evaluations on the Napkin-MNIST generated dataset. V.d, V.c, V.t refer to the digit, color and thickness respectively of variable V. The first column is with respect to samples generated from diffusion model P(X, Y | W1, W2), Image Data is the dataset used to train P, Discrete Data is the empirical distribution according to a discrete Napkin-MNIST, and the ground truth is analytically computed. Ideally all values should be equal across a row. While our synthetic dataset generated from P is not a perfect representation, it is quite close in all attributes except thickness. This is because the classifier for thickness has some inherent error in it, as evidenced by the mismatch between the base data and ground truth in the thickness rows.", "description": "This table presents the evaluation results on the Napkin-MNIST dataset.  It compares the probabilities of different attributes (digit, color, thickness) generated by a diffusion model with the empirical distribution from the dataset and the analytical ground truth.  Discrepancies highlight the model's limitations, particularly regarding thickness.", "section": "4.1 ID-GEN performance on napkin-MNIST dataset and baseline comparison"}, {"figure_path": "vymkuBMLlh/tables/tables_35_1.jpg", "caption": "Table 3: Additional attributes added (in percentage) in the translated images.", "description": "This table shows the percentage of additional attributes added to the translated images in the CelebA experiment.  The experiment involved translating images from the male domain to the female domain. The table lists several attributes and indicates the percentage of translated images in which these attributes appeared in the translated image but were absent in the original image.  This is used to assess the extent to which the translation models modify non-causal attributes compared to causal attributes.", "section": "4.2 Evaluating CelebA image translation models with ID-GEN"}, {"figure_path": "vymkuBMLlh/tables/tables_35_2.jpg", "caption": "Table 4: (Left) Class-conditional FID scores for generated Covid XRAY images (lower is better). Generated C = c, means we sample from the diffusion model conditioned on c. Real (C = c) refers to a held out test set of approximately 5k images, partitioned based on C-value. Low values on the diagonal and high values on the off-diagonal imply we are sampling correctly from conditional distributions. (Right) Evalution of Interventional Distribution Pc(n). We evaluate the distributions Pc(n = 1) for three cases for the Covid-XRAY dataset. Diffusion uses a learned diffusion model for P(xc), No Diffusion samples P(x|c) empirically from a held out validation set, and no latent evaluates the conditional query assuming no latent confounders in the causal model.", "description": "This table shows the FID scores for generated and real COVID X-ray images, categorized by whether the image is of a patient with or without COVID-19.  It compares the performance of different methods for sampling from the interventional distribution, highlighting the impact of latent confounders. Lower FID scores indicate better image quality.", "section": "F.4.3 Covid X-Ray generation"}, {"figure_path": "vymkuBMLlh/tables/tables_38_1.jpg", "caption": "Table 4: (Left) Class-conditional FID scores for generated Covid XRAY images (lower is better). Generated C = c, means we sample from the diffusion model conditioned on c. Real (C = c) refers to a held out test set of approximately 5k images, partitioned based on C-value. Low values on the diagonal and high values on the off-diagonal imply we are sampling correctly from conditional distributions. (Right) Evalution of Interventional Distribution Pc(n). We evaluate the distributions Pc(n = 1) for three cases for the Covid-XRAY dataset. Diffusion uses a learned diffusion model for P(xc), No Diffusion samples P(x|c) empirically from a held out validation set, and no latent evaluates the conditional query assuming no latent confounders in the causal model.", "description": "This table presents the results of evaluating the performance of the proposed ID-GEN algorithm on a real-world dataset of chest X-rays.  The left side shows the FID scores (Fr\u00e9chet Inception Distance, a metric for evaluating the quality of generated images) for images generated by a diffusion model conditioned on the presence or absence of COVID-19. The diagonal contains the FID scores for images where the model was conditioned on the same class as the real images (e.g., COVID-19 images compared to COVID-19 generated images), while the off-diagonal contains FID scores for images where the model was conditioned on the opposite class. Low values on the diagonal and high values off-diagonal indicate that the model is generating realistic and faithful samples. The right side shows an evaluation of the interventional distribution Pc(n), representing the probability of pneumonia (n) given COVID-19 (c).  Three variations are shown: using a learned diffusion model, empirically sampling from a held-out validation set, and a calculation assuming there are no latent confounders.", "section": "4.3 Invariant prediction with foundation models for chest X-ray generation"}, {"figure_path": "vymkuBMLlh/tables/tables_38_2.jpg", "caption": "Table 4: (Left) Class-conditional FID scores for generated Covid XRAY images (lower is better). Generated C = c, means we sample from the diffusion model conditioned on c. Real (C = c) refers to a held out test set of approximately 5k images, partitioned based on C-value. Low values on the diagonal and high values on the off-diagonal imply we are sampling correctly from conditional distributions. (Right) Evalution of Interventional Distribution Pc(n). We evaluate the distributions Pc(n = 1) for three cases for the Covid-XRAY dataset. Diffusion uses a learned diffusion model for P(xc), No Diffusion samples P(x|c) empirically from a held out validation set, and no latent evaluates the conditional query assuming no latent confounders in the causal model.", "description": "This table presents the results of the experiment conducted on the Covid-XRAY dataset. It compares the FID scores for generated images against real images conditioned on the presence or absence of COVID-19. The table also evaluates the interventional distribution of pneumonia given COVID-19 status, considering different scenarios: with a learned diffusion model, without a diffusion model (empirical samples), and assuming no latent confounders. Lower FID scores indicate higher image quality.", "section": "4.3 Invariant prediction with foundation models for chest X-ray generation"}]