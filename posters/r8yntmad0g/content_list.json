[{"type": "text", "text": "DOPPLER: Differentially Private Optimizers with Low-pass Filter for Privacy Noise Reduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xinwei Zhang\u2217 Zhiqi Bu\u2020 Mingyi Hong University of Southern California Amazon University of Minnesota ", "page_idx": 0}, {"type": "text", "text": "Meisam Razaviyayn\u2217 University of Southern California ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Privacy is a growing concern in modern deep-learning systems and applications. Differentially private (DP) training prevents the leakage of sensitive information in the collected training data from the trained machine learning models. DP optimizers, including DP stochastic gradient descent (DPSGD) and its variants, privatize the training procedure by gradient clipping and $D P$ noise injection. However, in practice, DP models trained using DPSGD and its variants often suffer from significant model performance degradation. Such degradation prevents the application of DP optimization in many key tasks, such as foundation model pretraining. In this paper, we provide a novel signal processing perspective to the design and analysis of DP optimizers. We show that a \u201cfrequency domain\u201d operation called low-pass filtering can be used to effectively reduce the impact of DP noise. More specifically, by defining the \u201cfrequency domain\u201d for both the gradient and differential privacy (DP) noise, we have developed a new component, called DOPPLER. This component is designed for DP algorithms and works by effectively amplifying the gradient while suppressing DP noise within this frequency domain. As a result, it maintains privacy guarantees and enhances the quality of the DP-protected model. Our experiments show that the proposed DP optimizers with a low-pass filter outperform their counterparts without the filter by $3\\bar{\\%}-10\\%$ in test accuracy on various models and datasets. Both theoretical and practical evidence suggest that the DOPPLER is effective in closing the gap between DP and non-DP training. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A rapidly growing number of modern machine learning applications in computer vision, natural language processing, and their mixtures rely on the development of large foundation models, whose performance heavily depends on the huge amounts of data collected from individual users. The leakage of potentially sensitive information in training data has become an increasingly critical issue when releasing and using machine learning models. Unfortunately, modern complex models have a strong ability to memorize the exact training data during the training processing Carlini et al. (2021); Pan et al. (2020). To alleviate the possible privacy leakage in the model training procedure, privacy-preserving optimization has attracted both researchers\u2019 and practitioners\u2019 interests. ", "page_idx": 0}, {"type": "text", "text": "Differential Privacy (DP) Dwork and Roth (2014) provides a strong theoretical guarantee with an easy and nearly plug-and-play mechanism, i.e., gradient clipping and noise injection, for existing optimization algorithms to guarantee the privacy of training procedures Abadi et al. (2016). By directly applying the DP mechanism to existing optimizers, DP optimizers have achieved decent performance in fine-tuning foundation models (Yu et al., 2021; Bu et al., 2024) or training small models (De et al., 2022). However, the performance of the pretraining tasks and training large foundation models using DP optimizers still remain unsatisfactory. This is because, as the DP theory suggests, the amount of injected DP noise is proportional to the number of model parameters and the total update steps (Abadi et al., 2016). Thus, the performance of large foundation models trained with DP optimizers degrades severely. To put it in perspective, pretraining a foundation model for an image classification task on the CIFAR dataset from randomly initialized weights takes around 100 epochs, with $300K$ steps (Dosovitskiy et al., 2020); the pretraining of BERT for natural language processing task takes $1M$ steps (Devlin et al., 2019), and pretraining LLAMA takes more than $250K$ steps (Touvron et al., 2023). The number of trainable parameters is also huge for these tasks, ranging from $300M$ to $70B$ . Therefore, the huge amount of injected DP noise severely degrades the performance of the final model trained with DP optimizers. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To improve the performance of DP optimizers, existing research takes two approaches: 1) designing models that are less sensitive to DP noise, e.g., using group normalization, weight standardization, weight smoothing, and smooth activation layers (De et al., 2022; Papernot et al., 2021; Wang et al., 2020), and 2) designing adaptive DP optimizers that inject relatively smaller noise, e.g., adaptive clipping, sparse gradient, and dynamic privacy budget (Andrew et al., 2021; Yu et al., 2021; Luo et al., 2021; Hong et al., 2022). However, existing methods only work for certain models and tasks at the cost of consuming more privacy budget. Moreover, most of the methods only demonstrate empirical improvements and do not provide theoretical justification for improving DP optimization. Therefore, there is a strong need for an approach that improves the performance of DP optimizers, which has the following properties: 1) has a solid theoretical guarantee, 2) is easy to implement, and 3) is compatible with most existing DP optimization improving methods. ", "page_idx": 1}, {"type": "text", "text": "Motivated by the above needs, in this paper, we develop a module that can easily be integrated into the DP training optimizers. We provide both theoretical and empirical justification for our proposed module. Specifically, our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Frequency-domain analysis: We introduce the notion of frequency domain analysis on (DP) optimizers. This analysis sheds light on how \u201cnoise\u201d affects the \u201csignal\u201d part of the update directions viewed as a sequence of update steps, rather than independent update steps. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Low-pass filter approach: Based on our frequency-domain analysis, we propose a low-pass filtering approach named DOPPLER, that post-processes the privatized gradient to reduce DP noise and improve DP optimizers\u2019 performance. Our low-pass filter reduces noise in the frequency domain, which is orthogonal to existing DP noise reduction approaches in the time domain and, therefore, can be easily combined with other existing techniques to further reduce noise. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Theoretical Analysis: We provide a novel theoretical analysis for the proposed low-pass filtering approach. Specifically, by introducing certain frequency domain assumptions on the gradients, we provide the convergence and privacy guarantee for DPSGD with the low-pass filter. Unlike existing methods that involve trading off noise with bias (e.g., adaptive clipping), or based on approximation (e.g., low-rank decomposition), our proposed algorithm does not introduce extra bias, model modification, or extra privacy cost. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Numerical results: Our extensive numerical experiments compare the performance of a variety of DP optimizers with and without the low-pass filter on different models and datasets. Our results show that DP optimizers equipped with the low-pass filter outperform the ones without the filter. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we discuss notations, assumptions, and some related prior work on DP optimization: ", "page_idx": 1}, {"type": "text", "text": "2.1 Notations $\\pmb{\\&}$ assumptions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we aim to optimize the Empirical Risk Minimization (ERM) problem: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}F(\\mathbf{x}),\\quad\\mathrm{where}\\ \\ F(\\mathbf{x}):=\\frac{1}{N}\\sum_{i=1}^{N}f(\\mathbf{x};\\xi_{i}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here, $\\mathcal{D}=\\{\\xi_{i}\\}_{i=1}^{N}$ is the training dataset with $N$ samples. Further denote the lower bound of the problem as $f^{\\star}=\\operatorname*{inf}{f(\\mathbf{x})}$ . Throughout the paper, we use $(\\cdot)_{t}$ to denote the update steps, $\\mathcal{N}(\\mu,\\sigma^{2})$ ", "page_idx": 1}, {"type": "text", "text": "to denote the Gaussian distribution with mean $\\mu$ and variance $\\sigma^{2}$ . We also assume the problem (1) satisfies the following assumptions. ", "page_idx": 2}, {"type": "text", "text": "A 1 (Smoothness) $F(\\cdot)$ is $L$ -smooth, i.e., $\\|\\nabla F(\\mathbf{x})-\\nabla F(\\mathbf{y})\\|\\leq L\\left\\|\\mathbf{x}-\\mathbf{y}\\right\\|,\\,\\forall\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}.$ ", "page_idx": 2}, {"type": "text", "text": "A 2 (Bounded Variance) The per-sample gradient has bounded variance, i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi\\in\\mathcal{D}}\\left\\|\\nabla f(\\mathbf{x};\\xi)-\\nabla F(\\mathbf{x})\\right\\|^{2}\\leq\\sigma_{S G D}^{2},\\;\\forall\\mathbf{x}\\in\\mathbb{R}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "A 3 (Bounded Gradient) The per-sample gradient has a bounded norm, i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla f(\\mathbf{x};\\boldsymbol{\\xi})\\|\\leq G,\\;\\forall\\mathbf{x}\\in\\mathbb{R}^{d},\\boldsymbol{\\xi}\\in\\mathcal{D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Let us briefly comment on these assumptions: A1 and A2 are standard in non-convex optimization (Allen-Zhu and Hazan, 2016; Zaheer et al., 2018; Abadi et al., 2016); and A3 is commonly used in analyzing the convergence of DP algorithms (Abadi et al., 2016; Wang et al., 2020; Andrew et al., 2021) to avoid introducing the clipping bias. Since the impact of clipping is not the major focus of this paper, we follow the existing analyses and use A3 to simplify our theoretical analysis. ", "page_idx": 2}, {"type": "text", "text": "2.2 Differential privacy (DP) and differentially private SGD (DPSGD) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Differential privacy is a gold standard of privacy to protect the privacy of individuals: ", "page_idx": 2}, {"type": "text", "text": "Definition 1 ( $[\\epsilon,\\delta)$ -DP (Dwork and Roth, 2014)) $A$ randomized mechanism $\\mathcal{M}$ is said to be $(\\epsilon,\\delta)$ -differentially private, if for any two neighboring datasets $\\mathcal{D},\\mathcal{D}^{\\prime}\\,(\\mathcal{D},\\mathcal{D}^{\\prime}$ differ only by one sam$p l e)$ and for any measurable output set $\\boldsymbol{S}$ , it holds that $\\mathrm{Pr}[\\mathcal{M}(\\mathcal{D})\\in\\mathcal{S}]\\leq\\mathrm{e}^{\\epsilon}\\mathrm{Pr}[\\mathcal{M}(\\mathcal{D}^{\\prime})\\in\\mathcal{S}]+\\delta$ . ", "page_idx": 2}, {"type": "text", "text": "A popular practical differentially private approach to finding an (approximate) solution to the ERM optimization problem (1) is Differentially Private Stochastic Gradient Descent (DPSGD) (Abadi et al., 2016) and its variants, including DP-Adam and DP-Lora (Yu et al., 2021). To protect DP, DPSGD considers applying the commonly used Gaussian mechanism (Dwork and Roth, 2014; Abadi et al., 2016) at each iteration of the stochastic gradient descent method. The Gaussian mechanism provides a DP guarantee by injecting additive noise into the algorithm output. ", "page_idx": 2}, {"type": "text", "text": "Definition 2 (Gaussian Mechanism (Dwork and Roth, 2014; Zhao et al., 2019)) Suppose an algorithm $\\mathcal{A}:\\mathcal{D}\\,\\rightarrow\\,\\mathbb{R}^{d}$ has $\\ell_{2}$ sensitivity $\\Delta_{\\mathcal{A}}$ , i.e., $\\operatorname*{max}_{\\mathcal{D},\\mathcal{D^{\\prime}}}\\|\\mathcal{A}(\\mathcal{D})-\\mathcal{A}(\\mathcal{D^{\\prime}})\\|\\ \\leq\\ \\Delta_{A}$ . Then, for any $\\epsilon\\mathrm{~>~0~}$ and $\\delta~\\leq~0.05$ , by adding random Gaussian noise to the output of the algorithm $M(x)=A(x)+\\mathbf{w}$ , with $\\mathbf{w}\\sim\\mathcal{N}(0,\\sigma_{\\mathrm{DP}}^{2}I_{d})$ , where =\u2206A\u221a2 l\u03f5n(1/2\u03b4)+ \u221a\u22062A\u03f5, the algorithm M is $(\\epsilon,\\delta){-}D P.$ ", "page_idx": 2}, {"type": "text", "text": "The DPSGD algorithm, presented in Algorithm 1, first samples a mini-batch $B^{t}$ of size $B$ and computes the per-sample gradient at each step $t$ . Then, it applies the Gaussian mechanism by clipping the per-sample gradient and injecting DP noise. The clipping operation bounds the sensitivity of the stochastic gradients to $C$ , e.g., cli $)\\left(\\vec{\\nabla}f,C\\right)=$ $\\operatorname*{min}\\left\\{1,\\frac{C}{\\|\\nabla f\\|}\\right\\}\\nabla f$ or $\\frac{C}{\\|\\nabla f\\|}\\nabla f$ . Finally, the ", "page_idx": 2}, {"type": "table", "img_path": "r8YntmAd0g/tmp/f34d64367ac5568e6dc93f4478b9beb6f4b22bb8574d40f833c4f18ac4d54e1f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "algorithm updates the model parameter with the privatized mini-batch gradient. It has been shown that DPSGD guarantees $(\\epsilon,\\delta)$ -DP with sufficiently large injected noise (Abadi et al., 2016). ", "page_idx": 2}, {"type": "text", "text": "Theorem 1 (Privacy Guarantee (Abadi et al., 2016)) Given $N,B,T$ and $C$ , there exist positive constants $u,v$ , such that for any $\\begin{array}{r}{\\epsilon\\,<\\,\\frac{u B^{2}T}{N^{2}},\\delta\\,>\\,0,}\\end{array}$ , by choosing $\\begin{array}{r}{\\sigma_{\\mathrm{DP}}^{2}\\,\\geq\\,v\\frac{C^{2}T\\ln\\left(\\frac{1}{\\delta}\\right)}{N^{2}\\epsilon^{2}}}\\end{array}$ v C2NT 2l\u03f5n2( \u03b41 ), Algorithm 1 is guaranteed to be $(\\epsilon,\\delta){-}D P.$ ", "page_idx": 2}, {"type": "text", "text": "2.3 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Effective DP training: Improving the performance of DP training has been widely studied. Adaptive gradient clipping (Andrew et al., 2021) estimates the size of the gradient privately and adaptively changes the clipping threshold to avoid injecting large DP noise; automatic clipping (Bu et al., 2024) replaces the clipping operation with normalization to avoid injecting large DP noise when the gradient becomes small; Hong et al. (2022) proposes using a time-varying privacy budget at each step which injects non-static DP noise based on the gradient to reduce the impact of the DP noise. As the injected DP noise variance scales with the model size, reducing the number of trainable parameters with adapters, low-rank weights, or quantized models has also been used to reduce the DP noise magnitude (Yu et al., 2021; Luo et al., 2021; Yu et al., 2021). De et al. (2022); Papernot et al. (2021); Wang et al. (2020) use special model structures that are less sensitive to DP noise, including group normalization, weight standardization, smoothed activation, and smoothed weights. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "These methods aim to reduce the magnitude of the injected DP noise or make the model and/or the DP algorithm less sensitive to large DP noise. However, the improvement is either empirical or only works for specific model structures and is unable to be generalized to other DP training tasks. ", "page_idx": 3}, {"type": "text", "text": "Signal processing for optimization: A few existing works analyze the optimization procedure from the signal processing perspective. They mainly focus on optimizing strongly convex problems using deterministic algorithms (Hu and Lessard, 2017; An et al., 2018); Gannot (2022) provides stability and convergence analysis from the frequency domain for inexact gradient methods. However, the results are still restricted to non-DP optimization and to strongly convex problems. ", "page_idx": 3}, {"type": "text", "text": "3 A signal processing perspective ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As discussed in Section 2.3, most of the existing works that aim to improve the performance of DP training are reducing the per-iteration injected DP noise. These approaches treat the update directions in each step independently, omitting the underlying dynamics and correlations between the steps. However, the gradient directions typically change smoothly due to the smoothness of the machine-learning model; therefore, the update directions are not independent over time. ", "page_idx": 3}, {"type": "text", "text": "With the intuition that the gradients over iterations are not independent, we provide the frequencydomain analysis of the stochastic gradients. Specifically, in the frequency domain, we treat the (stochastic) gradients from $t=0$ to $t=T$ as a time series and analyze the long-term correlation and dependencies across all gradients. In contrast, the time domain refers to the analyses that only focus on the gradient at step $t$ : for instance, Bu et al. (2024); Yang et al. (2022) leverage the $L$ -Lipschitz smoothness or the second-order Taylor expansion to bound/approximate the objective function in step $t$ after the update is performed. By analyzing the DP optimizers\u2019 updates in the frequency domain, we can make use of our prior knowledge of the correlation among the update directions. ", "page_idx": 3}, {"type": "text", "text": "To explain our motivation in a simplified manner, we temporarily ignore the per-sample gradient clipping and focus our narrative on the noise: suppose $\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta_{t}\\mathbf{g}_{t}$ with $\\mathbf{\\bar{g}}_{t}=\\nabla{\\bar{F}}(\\mathbf{x}_{t})+\\mathbf{w}_{t}$ and ${\\bf w}_{t}$ being the Gaussian noise. We will come back to the clipping in Section 3.2. In what follows, we decompose the sequence of stochastic gradients $\\{\\mathbf{g}_{t}\\}$ into two parts: 1) the gradient signal $\\{\\nabla F({\\bf x}_{t})\\}$ and the noise $\\{\\bar{\\bf g}_{t}-\\nabla F({\\bf x}_{t})\\}$ . We employ the power spectral density (PSD) to characterize the distribution of power into frequency components of a continuous signal. Mathematically, the power spectral density (PSD) of a sequence $\\{s_{t}\\}_{t=0,1,\\ldots}$ is $P_{s}(\\nu)=\\mathcal{F}\\{\\phi_{s}(\\tau)\\}$ , where $\\mathcal{F}$ denotes the Fourier transform from time domain $(\\tau)$ to frequency domain $(\\nu)$ (Oppenheim et al., 1996), and $\\phi$ is the auto-correlation coefficient as $\\phi_{s}(\\tau)=\\mathbb{E}\\left\\langle s_{t},s_{t-\\tau}\\right\\rangle$ . ", "page_idx": 3}, {"type": "text", "text": "On the one hand, the gradient sequence $\\{\\nabla F({\\mathbf x}_{t})\\}_{t=0,1,\\dots}$ . can be treated as a low-frequency signal, where we apply the Cauchy Schwarz inequality to get ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi_{\\nabla f}(\\tau)=\\mathbb{E}\\left\\langle\\nabla F(\\mathbf{x}_{t}),\\nabla F(\\mathbf{x}_{t-\\tau})\\right\\rangle=\\frac{1}{2}\\,\\mathbb{E}\\left[\\left\\Vert\\nabla F(\\mathbf{x}_{t})\\right\\Vert^{2}+\\left\\Vert\\nabla F(\\mathbf{x}_{t-\\tau})\\right\\Vert^{2}-\\left\\Vert\\nabla F(\\mathbf{x}_{t})-\\nabla F(\\mathbf{x}_{t-\\tau})\\right\\Vert^{2}\\right]}\\\\ &{\\qquad\\qquad\\geq\\frac{1}{2}\\,\\mathbb{E}\\left[\\left\\Vert\\nabla F(\\mathbf{x}_{t})\\right\\Vert^{2}+\\left\\Vert\\nabla F(\\mathbf{x}_{t-\\tau})\\right\\Vert^{2}-L^{2}\\eta^{2}\\tau\\sum_{i=1}^{\\tau}\\left\\Vert\\mathbf{g}_{t-i}\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This indicates that as long as the stepsize $\\eta$ is small, the auto-correlation coefficients decrease as $\\tau$ increases (as illustrated in Figure 1a, blue line). Therefore, the PSD also decreases as $\\nu$ increases, i.e., $\\{\\nabla F({\\bf x}_{t})\\}$ is a low-frequency signal (as illustrated in Figure 1b, blue line). ", "page_idx": 3}, {"type": "image", "img_path": "r8YntmAd0g/tmp/60d31ed38820ef6a1f1d8c4f53064c3ee6a04074adbf331a7be935760fecc514.jpg", "img_caption": ["", "Figure 1: An illustration of the auto-correlation $\\phi(\\tau)$ and power spectrum density $P(\\nu)$ of $\\{\\bar{\\nabla}F(\\mathbf{x}^{t})\\}$ and $\\mathbf{w}^{t}$ where $\\phi_{\\nabla f}$ decays proportional to $\\tau^{2}$ and ${\\bf w}_{t}$ is a white noise. (c) illustrates how an ideal low-pass filters out the high-frequency noise and keeps the low-frequency signal. ", "On the other hand, the noise signal $\\left\\{\\mathbf{w}_{t}\\right\\}\\;:=\\;\\left\\{\\mathbf{g}_{t}\\,-\\,\\nabla F(\\mathbf{x}_{t})\\right\\}$ is a white noise, where its autocorrelation is non-zero when $\\tau=0$ and is zero otherwise (as illustrated in Figure 1a, red line): ", "", ""], "img_footnote": [], "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi_{\\mathbf{w}}(\\tau)=\\mathbb{E}\\left\\langle\\mathbf{w}_{t},\\mathbf{w}_{t-\\tau}\\right\\rangle\\left\\{\\sum_{=}^{\\varsigma}d\\sigma_{\\mathrm{DP}}^{2}+\\frac{\\sigma_{\\mathrm{SGD}}^{2}}{B},\\quad\\tau=0\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{P_{\\mathbf{w}}(\\nu)=d\\sigma_{\\mathrm{DP}}^{2}+\\frac{\\sigma_{\\mathrm{SGD}}^{2}}{B},\\forall\\nu}\\end{array}$ (as illustrated in Figure 1b, red line). ", "page_idx": 4}, {"type": "text", "text": "3.1 Low-pass filter and noise reduction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "From the above discussion, we observed that although the gradient and the DP noise are not separable in each step $t$ (time-domain), they are distinguishable in the frequency domain. In particular, the noise power is equally distributed over all frequencies, while the gradient is concentrated around the lower frequencies. Therefore, we can apply the classical signal processing tools, such as frequency domain low-pass filters, to help improve the performance of DP optimization. ", "page_idx": 4}, {"type": "text", "text": "A low-pass filter amplifies the low-frequency component of the signal and suppresses the highfrequency part. Figure 1c shows an ideal low-pass filter that keeps the frequencies where the gradient is larger $(\\nu\\in[-0.6,0.6])$ and blocks the frequencies where the noise is larger $(\\nu<-0.6\\cup\\nu>0.6)$ . Signal-to-Noise Ratio (SNR) is a useful measure to characterize the quality of a noisy signal, i.e., the privatized gradient $\\{\\mathbf{g}_{t}\\}$ in DP optimization. Given the PSD of the gradient and the noise, the SNR of the privatized gradient is $\\frac{\\sum_{\\nu}P_{\\nabla f}(\\nu)}{\\sum_{\\nu}P_{\\mathbf{w}}(\\nu)}$ . As illustrated in Figure 1, when there is no low-pass filter (i.e., in Figure 1b), the SNR is small as the noise dominates in the high-frequency. In contrast, by applying the low-pass filter (i.e., in Figure 1c), most of the signals in the low-frequencies are kept, and the noise in the high frequencies is filtered, so the SNR increases. A linear low-pass filter on $\\{\\mathbf{g}_{t}\\}$ can be written as a recursive linear combination of the history signals: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{m}_{t}=-\\sum_{\\tau=1}^{n_{a}}a_{\\tau}\\mathbf{m}_{t-\\tau}+\\sum_{\\tau=0}^{n_{b}}b_{\\tau}\\mathbf{g}_{t-\\tau},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the sequence $\\left\\{\\mathbf{m}_{t}\\right\\}$ is the filtered output, $\\{a_{\\tau}\\},\\{b_{\\tau}\\}$ are the filter coefficients. Additinally, the \u201corder\u201d of the filter is defined as $\\operatorname*{max}\\{n_{a},n_{b}\\}$ . By carefully designing the coefficients, the low-pass filter can take different shapes and filter different frequencies. ", "page_idx": 4}, {"type": "text", "text": "In contrast to low-pass filters, the existing approaches improving the performance of DP optimization can be viewed as increasing the SNR in the time domain, i.e., reducing the magnitude of the noise injected in each step while preserving most of the gradient signal. Because the low-pass filter reduces noise in the frequency domain, and the existing noise reduction approaches lie in the time domain, the two approaches are orthogonal to each other. Therefore, the low-pass filter can be combined with existing approaches to further improve the DP optimizers\u2019 performance. ", "page_idx": 4}, {"type": "text", "text": "3.2 The impact of per-sample gradient clipping ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The above analysis assumes that the clipping operation is inactive by choosing a large enough clipping threshold $C$ . In practice, the clipping operation is usually active. By assuming the clipped ", "page_idx": 4}, {"type": "text", "text": "gradient $\\nabla F_{C}(\\mathbf{x})$ has zero curl, the DP optimizer optimizes an alternative problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}F_{C}(\\mathbf{x}),\\mathrm{~where~}F_{C}(\\mathbf{x})=\\int_{0}^{1}\\nabla F_{C}(z\\mathbf{x})^{\\top}\\mathbf{x}\\mathrm{d}z,\\;\\nabla F_{C}(\\mathbf{x})=\\frac{1}{N}\\sum_{\\xi\\in\\mathcal{D}}\\mathrm{clip}\\left(\\nabla f(\\mathbf{x};\\xi),C\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then the signal of the DP optimizer becomes the gradient of the alternative problem $\\{\\nabla F_{C}({\\bf x}_{t})\\}$ and the noise becomes $\\left\\lbrace\\mathbf{w}_{t}\\bar{\\right\\rbrace}\\,=\\,\\left\\lbrace\\mathbf{g}_{t}\\,-\\,\\nabla F_{C}(\\mathbf{x}_{t})\\right\\rbrace$ . As clipping is a non-expansive operator, i.e., $\\left\\|\\operatorname{clip}\\left(\\mathbf{x},C\\right)-\\operatorname{clip}\\left(\\mathbf{y},\\bar{C}\\right)\\right\\|\\leq\\left\\|\\mathbf{x}-\\mathbf{y}\\right\\|$ , the alternative problem $F_{C}(\\cdot)$ is also $L^{\\prime}$ -smooth with $L^{\\prime}\\leq$ $L$ . Therefore, a similar argument could be made on the gradient signal $\\{\\nabla F_{C}({\\bf x}_{t})\\}$ and the noise $\\{\\mathbf{w}_{t}\\}\\}$ when the clipping threshold is small and the clipping operation is active. ", "page_idx": 5}, {"type": "text", "text": "4 The proposed DOPPLER approach ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Building on the discussions in Section 3, we proposed a universal approach to improve DP optimization performance: DP OPtimizer with Low-Pass fiLter for noisE Reduction (DOPPLER). Taking DPSGD as an example, by applying DOPPLER, the main steps of the modified DPSGD algorithm are illustrated in Algorithm 2. The key steps of the low-pass filter are described in Lines 6-8. Line 6 computes the filtered update direction ${\\mathbf{m}}_{t}$ as a recursive linear combination of the current gradient, past gradients, and past update directions. ${\\bf m}_{t}$ estimates the first moment of the privatized gradient and can be expanded as a moving average of ${\\bf g}_{t}$ , i.e., $\\begin{array}{r}{\\mathbf{m}_{t}=\\sum_{\\tau=0}^{t+n_{b}}\\kappa_{\\tau}\\mathbf{g}_{t-\\tau}}\\end{array}$ . However, as $\\left\\{\\mathbf{m}_{\\tau}\\right\\},\\left\\{\\mathbf{g}_{\\tau}\\right\\}$ are initialized with zeros (Line 2), ${\\mathbf{m}}_{t}$ is biased towards zero, especially in the early steps. To correct the initialization bias, in Line 7, the optimizer computes the bias correction factor $c_{a,t}$ that is used in Line 8 to guarantee the weights $\\kappa_{\\tau}$ in the moving average are summed to 1. ", "page_idx": 5}, {"type": "text", "text": "Connection to momentum method: The DOPPLER approach is a generalized version of the momentum method from a first-order filter to higher orders. The momentum method uses one buffer ${\\bf m}_{t}$ to store the exponential moving average of ${\\bf g}_{t}$ , while DOPPLER uses multiple buffers $\\{\\mathbf{m}_{t-\\tau}\\}_{\\tau=0}^{n_{a}-1}$ to compute a more complex moving average of . ", "page_idx": 5}, {"type": "text", "text": "Compatibility: Algorithm 2 demonstrates how DOPPLER can be combined with the DPSGD algorithm while it is not restricted to DPSGD. The DOPPLER approach is compatible with other advanced DP optimizers, e.g., Adam (Kingma and Ba, 2015; Tang et al., 2024) and GaLore (Zhao et al., 2024). It serves as a base component for DP optimizers to improve their performance. ", "page_idx": 5}, {"type": "text", "text": "5 Theoretical analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Convergence analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we analyze the convergence of DPSGD with DOPPLER. First, we make the following assumption on the gradient auto-correlation coefficients. ", "page_idx": 5}, {"type": "text", "text": "A 4 (Gradient auto-correlation) For all $t\\in\\{0,\\ldots,T-1\\}$ , there exists sequences $\\{c_{\\tau}\\},\\{c_{-\\tau}\\}$ such that the following condition holds: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla F({\\mathbf x}_{t}),\\nabla F({\\mathbf x}_{t-\\tau})\\rangle\\geq c_{\\tau}\\left\\|\\nabla F({\\mathbf x}_{t})\\right\\|^{2}+c_{-\\tau}\\left\\|\\nabla F({\\mathbf x}_{t-\\tau})\\right\\|^{2},\\quad\\forall\\tau\\geq0,}\\\\ &{c_{-\\tau}\\geq0,\\quad\\forall\\tau\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "table", "img_path": "r8YntmAd0g/tmp/60447db329bd06b30145096da47789b94cf7313b6acaf7c8f3afcb40c1116c2a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Clearly, we have $\\begin{array}{r}{c_{0}=\\frac{1}{2}>0}\\end{array}$ . From the discussion in Section 3, we see that the above assumption can be satisfied as long as $\\eta$ is small enough, i.e., ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{\\sqrt{\\left(1-2c_{-\\tau}\\right)\\left\\Vert\\nabla F(\\mathbf{x}_{t-\\tau})\\right\\Vert^{2}+\\left(1-2c_{\\tau}\\right)\\left\\Vert\\nabla F(\\mathbf{x}_{t})\\right\\Vert^{2}}}{L\\sqrt{\\left\\Vert\\sum_{\\tau_{1}=1}^{\\tau}\\nabla F(\\mathbf{x}_{t-\\tau_{1}})\\right\\Vert^{2}+\\tau(d\\sigma_{\\mathrm{DP}}^{2}+\\sigma_{\\mathrm{SGD}}^{2}/B)}}=\\mathcal{O}\\left(\\sqrt{\\frac{1}{\\tau}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The pattern of the sequence $\\{c_{\\tau}\\}$ characterizes the frequency of the gradients as discussed in Section 3. If $c_{\\tau}$ \u2019s are all positive and slowly decreasing, then $\\nabla F({\\bf x}_{t})$ and $\\nabla F({\\bf x}_{t-\\tau})$ are highly correlated, so $\\{\\nabla F({\\bf x}_{t})\\}$ lies in lower frequencies. However, $c_{\\tau}$ is not necessarily positive. When some of $c_{\\tau}$ \u2019s are negative, or $c_{\\tau}$ \u2019s are oscillating between positive and negative values, it means that $\\nabla F({\\bf x}_{t})$ and $\\nabla F({\\bf x}_{t-\\tau})$ are negatively correlated, and $\\{\\nabla F({\\bf x}_{t})\\}$ may contain high-frequency signals. ", "page_idx": 6}, {"type": "text", "text": "Before we present the theorem, let us define the normalized SNR as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\mathbf{S}\\mathbf{N}\\mathbf{R}}{\\sum_{t=0}^{T-1}\\sum_{\\tau=0}^{t}c_{\\tau}\\kappa_{\\tau}}=\\frac{\\sum_{t=0}^{T-1}\\sum_{\\tau=0}^{t}c_{\\tau}\\kappa_{\\tau}}{\\sum_{t=0}^{T-1}\\sum_{\\tau=0}^{t}\\kappa_{\\tau}^{2}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and define the expanded coefficients $\\kappa_{\\tau}$ as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\kappa_{\\tau}=\\sum_{\\tau_{2}=0}^{\\operatorname*{min}\\{n_{b},\\tau\\}}b_{\\tau_{2}}\\sum_{\\tau_{1}=1}^{n_{a}}z_{a,\\tau_{1}}(p_{a,\\tau_{1}})^{\\tau-\\tau_{2}},\\quad\\mathrm{s.t.}\\,\\sum_{\\tau=1}^{n_{a}}\\frac{z_{a,\\tau}}{1-p_{a,\\tau}x}=\\frac{1}{1+\\sum_{\\tau=1}^{n_{a}}a_{\\tau}x^{\\tau}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which satisfies $\\begin{array}{r}{m_{t}=-\\sum_{\\tau=1}^{n_{a}}a_{\\tau}m_{t-\\tau}+\\sum_{\\tau=0}^{n_{b}}b_{\\tau}g_{t-\\tau}=\\sum_{\\tau=0}^{t}\\kappa_{\\tau}g_{t-\\tau}.}\\end{array}$ Note that $p_{a,\\tau}$ might be complex, but $\\kappa_{\\tau}$ are guaranteed to be real . With A4, we ha ve the following convergence result for Algorithm 2. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Convergence) Assume the problem satisfies A1-A4. By choosing $C~\\geq~G,$ , $\\eta\\ \\leq$ $\\operatorname*{min}\\{\\frac{2c-\\tau}{L\\kappa_{\\tau}}\\}$ , and running Algorithm 2 for $T$ iterations, the algorithm satisfies: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t\\sim P(t)}\\left\\|\\nabla F(\\mathbf{x}_{t})\\right\\|^{2}\\leq\\frac{F(\\mathbf{x}_{0})-F^{\\star}}{\\eta S_{T}}+\\frac{\\eta L}{2\\underline{{\\mathbf{SNR}}}}\\left(d\\sigma_{D P}^{2}+\\frac{\\sigma_{S G D}^{2}}{B}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where we define $\\begin{array}{r}{S_{T}=\\sum_{t=0}^{T-1}\\sum_{\\tau=0}^{t}c_{\\tau}\\kappa_{\\tau}=\\mathcal{O}(T)}\\end{array}$ ; the expectation is taken over $t=0,\\dots,T-1$ such that $\\begin{array}{r}{P(t)=\\frac{\\sum_{\\tau=0}^{t}c_{\\tau}\\kappa_{\\tau}}{S_{T}}}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "The proof of Theorem 2 is given in Appendix B. Compared with vanilla DPSGD (Abadi et al., 2016), by adopting DOPPLER the noise is scaled by a factor of $\\frac{1}{2\\underline{{\\mathbf{S}}}\\underline{{\\mathbf{N}}}\\mathbf{R}}$ . Thus, as long as $\\mathbf{S}\\mathbf{N}\\mathbf{R}>\\textstyle{\\frac{1}{2}}$ the noise is reduced. Next, we will use the above result to obtain privacy-utility tradeoff. ", "page_idx": 6}, {"type": "text", "text": "5.2 Privacy guarantee ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our low-pass filter is post-processing on the privatized gradient. Since DP is immune to postprocessing Dwork and Roth (2014), Algorithm 2 provides the same DP guarantee as DPSGD, satisfying Theorem 1. By directly combining Theorem 1 and Theorem 2, we can obtain the following privacy-utility trade-off for Algorithm 2. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3 (Privacy-utility trade-off) Assume the problem satisfies A1-A4. By choosing $C=G,$ , $\\begin{array}{r}{\\sigma_{D P}^{2}=\\frac{C^{2}T\\ln(1/\\delta)}{N^{2}\\epsilon^{2}}}\\end{array}$ , $\\begin{array}{r}{\\eta\\le\\operatorname*{min}\\{\\frac{2c-\\tau}{L\\kappa_{\\tau}}\\}}\\end{array}$ , and running Algorithm 2 for $\\begin{array}{r}{T=\\mathcal{O}\\left(\\frac{N\\epsilon\\sqrt{\\mathbf{SNR}(F(\\mathbf{x}_{0})-F^{\\star})}}{C\\sqrt{d L\\ln(1/\\delta)}}\\right)}\\end{array}$ iterations, the algorithm satisfies $(\\epsilon,\\delta)$ -DP and the expected gradient satisfies: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t\\sim P(t)}\\left\\|\\nabla F(\\mathbf{x}_{t})\\right\\|^{2}=\\mathcal{O}\\left(\\frac{C\\sqrt{d L(F(\\mathbf{x}_{0})-F^{\\star})\\ln(1/\\delta)}}{\\sqrt{\\underline{{\\mathbf{SNR}}}N}\\nu\\epsilon}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{P(t)=\\frac{\\sum_{\\tau=0}^{t}c_{\\tau}\\kappa_{\\tau}}{\\sum_{t=0}^{T-1}\\sum_{\\tau=0}^{t}c_{\\tau}\\kappa_{\\tau}}}\\end{array}$ and $\\kappa$ , SNR are defined in (6), (5), respectively. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3 implies that DPSGD with DOPPLER shares the same convergence rate $\\mathcal{O}\\left(\\frac{C\\sqrt{d\\ln(1/\\delta)}}{N\\epsilon}\\right)$ as the vanilla DPSGD (Abadi et al., 2016). However, by using the low-pass filter, the performance of DPSGD improves by a constant factor $\\frac{1}{\\sqrt{\\underline{{\\bf S N R}}}}$ , which is discussed next. ", "page_idx": 6}, {"type": "text", "text": "5.3 Impact of the low-pass filter ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Here, we provide $\\mathbf{SNR}$ value for some choices of the filter coefficients and discuss how to design low-pass filters. ", "page_idx": 7}, {"type": "text", "text": "\u2022 For SGD (no filter), we have $\\kappa_{0}~=~1$ , and $\\kappa_{\\tau}\\;=\\;0,\\;\\forall\\tau\\;>\\;0$ . Then, the normalized SNR is $\\begin{array}{r}{\\mathbf{S}\\mathbf{N}\\mathbf{R}=\\frac{1}{2}}\\end{array}$ . This recovers the convergence result for DPSGD in Abadi et al. (2016). \u2022 Momentum-SGD Cutkosky and Mehta (2020) is a special case of the low-pass filter, with filter coefficients: $a_{1}=-0.9,b_{0}=0.1$ . and $\\kappa_{\\tau}=0.1\\times0.9^{\\tau}$ . Then, the normalized SNR is $\\underline{{\\mathbf{S}}}\\underline{{\\mathbf{N}}}\\underline{{\\mathbf{R}}}\\ge$ $\\begin{array}{r}{1.9\\times\\left(\\frac{1}{2}+\\sum_{\\tau=0}^{t-1}0.9^{\\tau}c_{\\tau}\\right)}\\end{array}$ , which is larger than vanilla DPSGD. This indicates that DPSGD with momentum can reduce the impact of DP noise compared with DPSGD w/o momentum. \u2022 We can further improve the SNR by optimizing the filter coefficients under a fixed order: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\{a_{\\tau}\\},\\{b_{\\tau}\\}}\\ \\ \\frac{{\\bf S N R}}{{\\bf S}},\\ \\ \\ \\mathrm{s.t.}\\ \\ \\ \\sum_{\\tau=0}^{n_{b}}b_{\\tau}-\\sum_{\\tau=1}^{n_{a}}a_{\\tau}=1.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "From (8), we observe that the pattern of the auto-correlation coefficients $c_{\\tau}$ determines the choice of the filter coefficients. When $\\kappa_{\\tau}\\propto c_{\\tau}$ , $\\mathbf{SNR}$ is maximized. However, in general, finding the optimal filter coefficients by optimizing (8) before training is difficult, as $\\{c_{\\tau}\\}$ is determined by the problem and the DP optimizer\u2019s updates and can be time-varying. ", "page_idx": 7}, {"type": "text", "text": "Optimal FIR filter: When the filter takes the form of a finite impulse response (FIR) (i.e., $a_{n}=0$ ), we can estimate $\\{c_{\\tau}\\}$ and optimize $b_{\\tau}$ \u2019s according to (8) during training. To estimate $c_{\\tau}$ , we have: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{\\tau}\\overset{A4}{\\leq}\\left(\\langle\\nabla F(\\mathbf{x}_{t}),\\nabla F(\\mathbf{x}_{t-\\tau})\\rangle-c_{-\\tau}\\,\\|\\nabla F(\\mathbf{x}_{t-\\tau})\\|^{2}\\right)/\\left\\|\\nabla F(\\mathbf{x}_{t})\\right\\|^{2}}\\\\ &{\\overset{A2}{\\leq}\\mathbb{E}\\left[\\langle\\mathbf{g}_{t},\\mathbf{g}_{t-\\tau}\\rangle-c_{-\\tau}\\left(\\left\\|\\mathbf{g}_{t-\\tau}\\right\\|^{2}-d\\sigma_{\\mathrm{DP}}^{2}-\\sigma_{\\mathrm{SGD}}^{2}/B\\right)\\right]/\\left\\|\\mathbf{g}\\left[\\left\\|\\mathbf{g}_{t}\\right\\|^{2}-d\\sigma_{\\mathrm{DP}}^{2}-\\sigma_{\\mathrm{SGD}}^{2}/B\\right]}\\\\ &{\\approx\\mathbb{E}\\left[\\langle\\mathbf{g}_{t},\\mathbf{g}_{t-\\tau}\\rangle-\\frac{1}{2}\\operatorname*{max}\\{\\left\\|\\mathbf{g}_{t-\\tau}\\right\\|^{2}-d\\sigma_{\\mathrm{DP}}^{2},0\\}\\right]/\\mathbb{E}\\left[\\operatorname*{max}\\{\\left\\|\\mathbf{g}_{t}\\right\\|^{2}-d\\sigma_{\\mathrm{DP}}^{2},\\boldsymbol{\\epsilon}_{1}\\right\\},\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where in the last approximation we set $\\begin{array}{r}{c_{-\\tau}\\,=\\,\\frac{1}{2}}\\end{array}$ and assume $d\\sigma_{\\mathrm{DP}}^{2}$ dominates $\\sigma_{\\mathrm{SGD}}^{2}/B$ ; the max are taken as $\\left\\|\\cdot\\right\\|^{2}\\ge0$ and we choose $\\epsilon_{1}=10^{-3}$ as a small positive number for numerical stability. After obtaining $c_{\\tau}^{\\prime}\\mathbf{s},\\,b_{\\tau}^{\\prime}\\mathbf{s}$ have a closed-form solution $\\begin{array}{r}{b_{\\tau}=\\frac{c_{\\tau}}{\\sum_{\\tau=0}^{b_{n}}c_{\\tau}}}\\end{array}$ . This estimation only relies on the stored privatized gradients $\\{{\\bf g}_{t-\\tau}\\}_{\\tau=0}^{n_{b}}$ , so it does not spend an extra privacy budget or memory. Therefore, it can also be implemented along with DOPPLER, as an adaptive approach to adjust the filter coefficients for an optimal performance. ", "page_idx": 7}, {"type": "text", "text": "6 Numerical experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we investigate how the low-pass filter affects the performance of various DP optimizers on different datasets, privacy budgets, and models. Due to the page limitation, detailed implementation and extra numerical results are given in Appendix C. The code for the experiments is available at https://anonymous.4open.science/r/Low-pass-SGD-C7A1. ", "page_idx": 7}, {"type": "text", "text": "6.1 Experiment Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Dataset: We conduct experiments on computer vision datasets (MNIST, CIFAR-10, and CIFAR100 (Krizhevsky et al., 2009)) and natural language processing datasets, GLUE (Wang et al., 2018). ", "page_idx": 7}, {"type": "text", "text": "Model: We conduct experiments on various models, including the 5-layer CNN described in De et al. (2022), the modified ResNet in Kolesnikov et al. (2019), EfficientNet with group normalization (Tan and Le, 2019), and ViT (Dosovitskiy et al., 2020) for the CV tasks and RoBERTa-base (Liu et al., 2019) for the GLUE dataset. If not specified, the models are initialized with random weights without pretraining. ", "page_idx": 7}, {"type": "text", "text": "Algorithm: We compared the impact of DOPPLER on several base algorithms, including the DP version of SGD, Adam, and GaLore. The updates of the algorithms are given in Algorithm 2 and Algorithm 3 in Appendix C.2. We use LP- to denote the DP optimizer with DOPPLER. ", "page_idx": 7}, {"type": "image", "img_path": "r8YntmAd0g/tmp/8d817d0ccdcd9647ebb2ec9f6b81f3829b8423f7a252d59143a9b3459b6f67a1.jpg", "img_caption": ["(a) PSD of $\\mathbf{w}_{t}$ and ${\\bf g}_{t}$ "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "r8YntmAd0g/tmp/bba83b194b2d458169e49f3ad0614380256d1013d0843366fe875fc310003946.jpg", "img_caption": ["(b) PSD of ${\\bf g}_{t}$ of SGD and LP-SGD. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 2: The recorded PSD of Gaussian noise $\\left\\{\\mathbf{w}_{t}\\right\\}$ , and the stochastic gradients of SGD and LPSGD of ResNet-50 training on CIFAR-10 dataset. ", "page_idx": 8}, {"type": "image", "img_path": "r8YntmAd0g/tmp/6bf406fde92981263d7c595e25f4589886a92f01e15662b468e8de04fd9e1113.jpg", "img_caption": ["Figure 3: Comparision between DPSGD and LP-DPSGD for pre-training on different datasets. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "r8YntmAd0g/tmp/7bd7e8ac338915b5f400854238488c8d1fef5cc157305b33d9886b6bc541cd9f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: Comparision between DP optimizers w and w/o low-pass filters for pre-training with different $\\epsilon$ \u2019s on CIFAR-10 dataset. ", "page_idx": 8}, {"type": "text", "text": "Hyper-parameters choices: The choices of the filter coefficients $a_{i},b_{i}$ are empirical; specific choices used in the experiments are listed in Table 2 in Appendix C.1. ", "page_idx": 8}, {"type": "text", "text": "The learning rate, batch size, and number of training epochs are tuned for best testing accuracy using grid search. Detailed hyper-parameters and search grids are given in Appendix C.1. For all experiments, we fix the privacy parameter $\\delta=1/N^{1.1}$ to obtain a reasonable privacy notion. ", "page_idx": 8}, {"type": "text", "text": "6.2 Numerical results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "PSD of the stochastic gradient: First, we record the stochastic gradients of SGD and SGD with DOPPLER training ResNet-50 for 40 epochs $T\\,=\\,4000$ steps) on the CIFAR-10 dataset. Then, we compute the PSD of the recorded stochastic gradients. The results are given in Figure 2. We can observe that the recorded PSD of ${\\bf w}_{t}$ is filling all frequencies, and ${\\bf g}_{t}$ is a low-frequency signal. After applying the low-pass filter, the PSD of the filtered gradient lies in the low-frequency domain, and the high-frequency signals (and noise) are suppressed. ", "page_idx": 8}, {"type": "text", "text": "Results for different datasets. The comparisons between LP-DPSGD and DPSGD on different datasets are given in Figure 3. We can observe that LP-DPSGD outperforms DPSGD on MNIST, CIFAR-10, and CIFAR-100 datasets under the same privacy budget $\\epsilon=8$ . The results on the GLUE dataset is in Appendix C.3. ", "page_idx": 8}, {"type": "text", "text": "Results for different algorithms: The comparisons between DP optimizers, including DPSGD, DPAdamBC (Tang et al., 2024), and DPGaLore (an extension of GaLore (Zhao et al., 2024)), with and without DOPPLERis shown in Figure 4. We can observe that all DP optimizers with DOPPLERoutperform the baseline under different levels of privacy budget $\\epsilon^{\\prime}\\mathbf{s}$ . ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion and discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce a signal processing perspective to understand and analyze DP optimizers. By identifying the difference between the gradient and noise signal in the frequency domain, we propose DOPPLER, a low-pass filter approach, to filter out the DP noise and improve the signalto-noise ratio of the privatized gradient. Our proposed filtering method is compatible with existing DP optimizers, and extensive experiments have shown that the low-pass filter could improve DP optimizers\u2019 performance in the case when the DP noise is large, e.g., in the pertaining stage and for training large models. ", "page_idx": 9}, {"type": "text", "text": "Limitations: Designing higher-order filters requires hyper-parameter tuning or prior knowledge of the gradients\u2019 auto-correlation pattern; implementing a high-order low-pass filter is memory inefficient (requires storing $n_{a}+n_{b}$ optimization states for each trainable parameter), which eliminates the usage of the proposed method when optimizing very large-scale foundation models with limited memory resource. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by a gift from the USC-Meta Center for Research and Education in AI, and a gift from Google. Mingyi Hong is supported partially by NSF under the grants EPCN-2311007 and CCF-1910385. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308\u2013318, 2016.   \nZ. Allen-Zhu and E. Hazan. Variance reduction for faster non-convex optimization. In International conference on machine learning, pages 699\u2013707. PMLR, 2016.   \nW. An, H. Wang, Q. Sun, J. Xu, Q. Dai, and L. Zhang. A pid controller approach for stochastic optimization of deep networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8522\u20138531, 2018.   \nG. Andrew, O. Thakkar, B. McMahan, and S. Ramaswamy. Differentially private learning with adaptive clipping. Advances in Neural Information Processing Systems, 34:17455\u201317466, 2021.   \nZ. Bu, Y.-X. Wang, S. Zha, and G. Karypis. Automatic clipping: Differentially private deep learning made easier and stronger. Advances in Neural Information Processing Systems, 36, 2024.   \nN. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson, et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633\u20132650, 2021.   \nC. A. Choquette-Choo, K. D. Dvijotham, K. Pillutla, A. Ganesh, T. Steinke, and A. G. Thakurta. Correlated noise provably beats independent noise for differentially private learning. In The Twelfth International Conference on Learning Representations.   \nA. Cutkosky and H. Mehta. Momentum improves normalized sgd. In International conference on machine learning, pages 2260\u20132268. PMLR, 2020.   \nS. De, L. Berrada, J. Hayes, S. L. Smith, and B. Balle. Unlocking high-accuracy differentially private image classification through scale. arXiv preprint arXiv:2204.13650, 2022.   \nS. Denisov, H. B. McMahan, J. Rush, A. Smith, and A. Guha Thakurta. Improved differential privacy for sgd via optimal private linear operators on adaptive streams. Advances in Neural Information Processing Systems, 35:5910\u20135924, 2022.   \nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, 2019.   \nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.   \nC. Dwork and A. Roth. The algorithmic foundations of differential privacy. Theoretical Computer Science, 9(3-4):211\u2013407, 2014.   \nO. Gannot. A frequency-domain analysis of inexact gradient methods. Mathematical Programming, 194(1):975\u20131016, 2022.   \nJ. Hong, Z. Wang, and J. Zhou. Dynamic privacy budget allocation improves data efficiency of differentially private gradient descent. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 11\u201335, 2022.   \nB. Hu and L. Lessard. Control interpretations for first-order optimization methods. In 2017 American Control Conference (ACC), pages 3114\u20133119. IEEE, 2017.   \nP. Kairouz, B. McMahan, S. Song, O. Thakkar, A. Thakurta, and Z. Xu. Practical and private (deep) learning without sampling or shuffling. In International Conference on Machine Learning, pages 5213\u20135225. PMLR, 2021.   \nD. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), San Diega, CA, USA, 2015.   \nA. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby. Big transfer (bit): General visual representation learning. In European Conference on Computer Vision, 2019.   \nA. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \nX. Li, F. Tramer, P. Liang, and T. Hashimoto. Large language models can be strong differentially private learners. In International Conference on Learning Representations, 2021.   \nY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \nI. Loshchilov and F. Hutter. SGDR: stochastic gradient descent with warm restarts. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview. net/forum?id $\\equiv$ Skq89Scxx.   \nZ. Luo, D. J. Wu, E. Adeli, and L. Fei-Fei. Scalable differential privacy with sparse network finetuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5059\u20135068, 2021.   \nA. V. Oppenheim, A. S. Willsky, and S. H. Nawab. Signals & systems (2nd ed.). Prentice-Hall, Inc., USA, 1996. ISBN 0138147574.   \nX. Pan, M. Zhang, S. Ji, and M. Yang. Privacy risks of general-purpose language models. In 2020 IEEE Symposium on Security and Privacy (SP), pages 1314\u20131331. IEEE, 2020.   \nN. Papernot, A. Thakurta, S. Song, S. Chien, and \u00b4U. Erlingsson. Tempered sigmoid activations for deep learning with differential privacy. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 9312\u20139321, 2021.   \nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211\u2013252, 2015.   \nM. Tan and Q. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 6105\u20136114. PMLR, 2019.   \nQ. Tang, F. Shpilevskiy, and M. Le\u00b4cuyer. Dp-adambc: Your dp-adam is actually dp-sgd (unless you apply bias correction). In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 15276\u201315283, 2024.   \nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozie\\`re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \nA. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\u2013355, 2018.   \nB. Wang, Q. Gu, M. Boedihardjo, L. Wang, F. Barekat, and S. J. Osher. Dp-lssgd: A stochastic optimization method to lift the utility in privacy-preserving erm. In Mathematical and Scientific Machine Learning, pages 328\u2013351. PMLR, 2020.   \nS. Winder. Analog and digital filter design. Elsevier, 2002.   \nX. Yang, H. Zhang, W. Chen, and T.-Y. Liu. Normalized/clipped sgd with perturbation for differentially private non-convex optimization. arXiv preprint arXiv:2206.13033, 2022.   \nD. Yu, S. Naik, A. Backurs, S. Gopi, H. A. Inan, G. Kamath, J. Kulkarni, Y. T. Lee, A. Manoel, L. Wutschitz, et al. Differentially private fine-tuning of language models. In International Conference on Learning Representations, 2021.   \nM. Zaheer, S. Reddi, D. Sachan, S. Kale, and S. Kumar. Adaptive methods for nonconvex optimization. Advances in neural information processing systems, 31, 2018.   \nJ. Zhao, T. Wang, T. Bai, K.-Y. Lam, Z. Xu, S. Shi, X. Ren, X. Yang, Y. Liu, and H. Yu. Reviewing and improving the gaussian mechanism for differential privacy. arXiv preprint arXiv:1911.12060, 2019.   \nJ. Zhao, Z. Zhang, B. Chen, Z. Wang, A. Anandkumar, and Y. Tian. Galore: Memory-efficient llm training by gradient low-rank projection. In 5th Workshop on practical ML for limited/low resource settings, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Additional Background ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we provide the details of the frequency-domain analysis and low-pass filter approach. First, we discuss the basic concepts in signal processing. Then, we discuss the filter method in signal processing. ", "page_idx": 12}, {"type": "text", "text": "A.1 Frequency domain analysis ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In signal processing, frequency domain analysis is used to analyze the periodical or long-term behavior of a (time series) signal/data. In the frequency domain analysis, we use the frequency $\\nu$ as the indices of the signal, e.g., $\\{X(\\nu)\\},X(\\nu)\\in{\\bar{\\mathbb{C}}}$ , where each term $X(\\nu)$ records the amplitude and phase of the sine wave of frequency $\\nu$ that composes the signal; in contrast, in the time domain, we use time $t$ as the indices of a signal, e.g., $\\left\\{x_{t}\\right\\}$ , where each term $x_{t}$ records the value of the signal at a given time $t$ . In this paper, we treat each coordinate $i\\in[1,\\dots,d]$ of the privatized gradient over the iterates as an individual signal, i.e. $\\{g_{1}[i],g_{2}[i],\\dots,g_{T}[i]\\}$ . Thus, the gradient over iterates gives us $d$ one-dimensional signals, and we can look at their frequency domain representation of each signal. ", "page_idx": 12}, {"type": "text", "text": "Benefit of frequency-domain analysis: ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "\u2022 Certain properties of a signal can be hard to observe/characterize in the time domain. For example, a long-time correlation or a cyclic behavior of the signal is not easy to directly observe in the time domain. By converting the signal to the frequency domain, such properties can easily be captured and analyzed. For example, the signal $x_{t}\\,=\\,\\sin(t)$ has nonzero entries in almost all times. However, the frequency domain representation of this signal has only one entry that is nonzero, i.e., $X(1)=1$ and all other entries are zero, i.e., $X(\\nu)=\\stackrel{\\cdot}{0},\\forall\\nu\\not=1$ . This means $x_{t}$ has only one periodic signal in it. ", "page_idx": 12}, {"type": "text", "text": "\u2022 Certain mathematical analyses can be significantly simplified in the frequency domain. For example, linear differential equations in the time domain are converted to algebraic equations in the frequency domain; filters as convolutions in the time domain are converted to point-wise multiplication in the frequency domain. These properties greatly simplify the analysis of the dynamics of the signals and filters (Oppenheim et al., 1996). ", "page_idx": 12}, {"type": "text", "text": "Transform from time to frequency domain: To obtain a frequency domain representation of a discrete signal, one can apply the Discrete Fourier transform (DFT) $({\\mathcal F}\\{x_{t}\\}\\ :\\ X(\\nu)\\ =$ $\\begin{array}{r}{\\sum_{t=0}^{T-1}x(t)e^{\\frac{-2\\pi\\bar{i}t}{T}\\nu}))}\\end{array}$ to the signal. By directly applying DFT to a signal and obtaining $\\{X(\\nu)\\}$ , one can identify how the signal is composed of sin waves of different frequencies with their amplitudes and phases. In the paper, we apply DFT to the auto-correlation of a signal and obtain its power spectrum density (PSD). The PSD of a signal shows the distribution of the power of a signal on different frequencies. For example, the PSD of $x(t)=\\sin(t)$ is $P(\\nu)=1/2$ for $\\nu=\\pm\\frac{1}{2\\pi}$ and 0 elsewhere. ", "page_idx": 12}, {"type": "text", "text": "A.2 Low-pass filter ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Frequency filter: A frequency filter is a transformation of a signal that only allows certain frequencies to pass and blocks/attenuates the remaining frequencies. For example, for a signal $\\begin{array}{r l r}{x(t)\\dot{}}&{{}=}&{\\sin(t)\\ +\\ \\sin(10t)}\\end{array}$ , we can apply an (ideal) low-pass filter $\\begin{array}{r l r}{F(\\nu)}&{{}=}&{1}\\end{array}$ when $\\begin{array}{r l r}{|\\nu|}&{{}\\leq}&{\\frac{1}{2\\pi}}\\end{array}$ and 0 otherwise. Then, after applying the filter, $\\begin{array}{r l r}{\\dot{F}^{\\mathrm{~\\,~}}\\ast\\mathrm{~\\,~}x(t)}&{{}=}&{\\sin(t)}\\end{array}$ , the output signal only keeps the low-frequency signal. ", "page_idx": 12}, {"type": "text", "text": "In this work, we use (time-invariant) linear filters for DP noise reduction. A linear filter attenuates certain frequencies by using a linear combination of the input signal. Considering $g_{t}$ as the time signal, the general form of a linear filter on $g_{t}$ is ", "page_idx": 12}, {"type": "image", "img_path": "r8YntmAd0g/tmp/299c65d4f7bf7a52bccf4bbe47df83bdb3b0d0cac47f2167a6a9afbe19ddd0d0.jpg", "img_caption": ["Figure 5: Illustration of the low-pass filter. "], "img_footnote": [], "page_idx": 12}, {"type": "equation", "text": "$$\nm_{t}=\\sum_{\\tau=0}^{t}\\kappa_{\\tau}g_{t-\\tau}=-\\sum_{\\tau=1}^{n_{a}}a_{\\tau}m_{t-\\tau}+\\sum_{\\tau=0}^{n_{b}}b_{\\tau}g_{t-\\tau},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\kappa_{\\tau}$ are the filter coefficients. The second formula is a recursive way of writing the filter. ", "page_idx": 13}, {"type": "text", "text": "Filter design: The property of the filter depends on the choice of the filter coefficients. Designing a filter consists of the following steps: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Decide filter order/tab $n_{a},n_{b}$ . Larger $n_{a},n_{b}$ give the filter more flexibility and better possible performance, at a cost of more memory consumption. In our experiment, we tested on 0th-3rd order filters, i.e., $\\operatorname*{max}\\{n_{a},n_{b}\\}\\leq3$ . ", "page_idx": 13}, {"type": "text", "text": "\u2022 Decide filter coefficients $\\{a_{\\tau}\\},\\{b_{\\tau}\\}$ . Filter design can, in general, be a complex procedure, and it involves deciding on trade-offs among different properties of the filter (Winder, 2002). Two standard constraints on the filter coefficients are: a) $-\\sum a_{\\tau}+\\sum b_{\\tau}=1$ , to ensure the filter has unit gain, i.e., the mean of the signal remains unchanged; and b)  the solutions $x$ to $1\\!+\\!\\sum a_{\\tau}x^{\\tau}=0$ satisfies $|x|<1$ , to ensure the filter is stable, i.e., $\\sum\\bar{|\\kappa_{t}|}<\\infty$ . In the paper, we directly follow the design of the Chebyshev filter and Butterworth f ilter and tune their cut-off frequency (and ripple) to achieve the best performance and maintain these properties. ", "page_idx": 13}, {"type": "text", "text": "A.3 Additional related work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "DP optimization with correlated noise: DP optimization with correlated noise have been investigated in Kairouz et al. (2021); Denisov et al. (2022); Choquette-Choo et al.. These works treat the DPSGD update as releasing a weighted prefix sum with DP noise, i.e., $A(G_{0:t}+W_{0:t})$ , where $A$ is the prefix sum matrix (a lower-triangular all-one matrix) and $W_{0:t}$ is the i.i.d. DP noise. Kairouz et al. (2021); Denisov et al. (2022) apply certain decomposition $A=B C$ and change the update to $B(C G_{0:t}+W_{0:t})=A G_{0:t}+B W_{0:t}$ , and Choquette-Choo et al. provides a theoretical justification that when $B$ is a high-pass filter, and $g_{t}$ are correlated, the algorithm outperforms original DPSGD. In contrast, our method can be written as $A M(G_{0:t}+W_{0:t})$ , where $M$ is a low-pass filter. ", "page_idx": 13}, {"type": "text", "text": "\u2022 The correlated noise methods and our proposed method can all be viewed as processing the signal in the frequency domain to \u201dseparate\u201d the noise and the gradient. ", "page_idx": 13}, {"type": "text", "text": "\u2022 The existing correlated noise methods 1) pre-process the gradient/noise to separate the gradient and the DP noise in the frequency domain, and therefore require careful design of the matrices $B,C$ for each problem and optimizer, 2) require extra memory $(O(d\\log(t))$ to $O(d t))$ , which is unrealistic for large scale training, and 3) only work for SGD update, since Adam cannot be written as such a prefix sum of privatized gradient. In contrast, our method 1) post-processes the noisy signal to extract the gradient from the noise from the frequency domain, 2) only requires $O(d)$ extra memory, which is independent of $t$ , and 3) is compatible with any first-order optimizer since it just post-processes the gradient. ", "page_idx": 13}, {"type": "text", "text": "B Missing proof details in the main paper ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide the proof for Theorem 2. First, by A1, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F({\\mathbf x}_{t+1})-F({\\mathbf x}_{t})\\leq\\left\\langle\\nabla F({\\mathbf x}_{t}),{\\mathbf x}_{t+1}-{\\mathbf x}_{t}\\right\\rangle+\\displaystyle\\frac{L}{2}\\,\\mathbb{E}\\left[\\left\\|{\\mathbf x}_{t+1}-{\\mathbf x}_{t}\\right\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad=-\\eta\\left\\langle\\nabla F({\\mathbf x}_{t}),{\\mathbf m}_{t}\\right\\rangle+\\displaystyle\\frac{L\\eta^{2}}{2}\\left\\|{\\mathbf m}_{t}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We can expand ${\\bf m}_{t}$ as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{{\\bf m}}_{t}=-\\sum_{\\tau=1}^{n_{a}}a_{\\tau}{\\bf m}_{t-\\tau}+\\sum_{\\tau=0}^{n_{b}}b_{\\tau}{\\bf g}_{t-\\tau}}\\ ~}\\\\ {{\\displaystyle{{\\bf m}}_{t}+\\sum_{\\tau=1}^{n_{a}}a_{\\tau}{\\bf m}_{t-\\tau}=\\sum_{\\tau=0}^{n_{b}}b_{\\tau}{\\bf g}_{t-\\tau}\\ ~}}\\\\ {{\\displaystyle(1+\\sum_{\\tau=1}^{n_{a}}a_{\\tau}z^{-\\tau}){\\bf M}(z)\\left(\\frac{a}{c}\\right)\\sum_{\\tau=0}^{n_{b}}b_{\\tau}z^{-\\tau}){\\bf G}(z)}}\\\\ {{\\displaystyle{{\\bf M}}(z)\\frac{(b)}{=1+\\sum_{\\tau=1}^{n_{a}}a_{\\tau}z^{-\\tau}}(\\sum_{\\tau=0}^{n_{b}}b_{\\tau}z^{-\\tau}){\\bf G}(z)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{M}(z)\\overset{(e)}{\\underset{Y=1}{\\longrightarrow}}\\frac{z_{n_{a}}}{1-\\rho_{a,\\tau}z^{-1}}\\mathbf{\\Phi}_{Y=0}^{(n_{b})}\\mathbf{\\Phi}_{P^{\\perp}z}\\mathbf{\\Phi}^{-\\tau})\\mathbf{G}(z)}\\\\ &{\\mathbf{M}(z)\\overset{(d)}{\\underset{Y=1}{\\longrightarrow}}\\ z_{n_{a},\\tau_{n_{b}}}\\sum_{\\substack{(p_{a},\\tau_{n}-1)\\,\\cdots\\,\\tau_{0}}}^{\\infty}(\\sum_{p_{a}}b_{p_{a}z}\\mathbf{\\Phi}^{-\\tau_{n}})\\mathbf{G}(z)}\\\\ &{\\mathbf{M}(z)\\overset{(e)}{\\underset{Y=1}{\\longrightarrow}}b_{p_{a}}\\mathbf{\\Phi}_{Y=0}^{\\left(n_{b}\\right)}\\ z_{a,\\tau_{n}}\\sum_{\\substack{(p_{a},\\tau_{n})}}^{\\infty}\\left(p_{a,\\tau_{n}}\\right)^{r_{a}-r_{0}-\\tau_{0}}\\mathbf{G}(z)}\\\\ &{\\mathbf{m}_{t}\\frac{(f)}{\\underset{Y=0}{\\overset{n}{\\longrightarrow}}}b_{p_{a}}\\mathbf{\\Phi}_{Y=0}^{\\left(n_{b}\\right)}\\ z_{a,\\tau_{n}}p_{a,\\tau_{n}}^{r_{a}}\\mathbf{g}_{t-r_{0}-\\tau_{2}}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{m}=\\frac{I}{\\underset{Y=0}{\\overset{k}{\\longrightarrow}}}\\mathbf{\\Phi}_{Y=\\mathbf{g}_{t-\\tau}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where in $(a)$ we applies the $z$ -transform $\\mathcal{Z}\\{\\cdot\\}$ to both side of the sequence and use the property that $\\mathcal{Z}\\{x_{t-k}\\}\\,=\\,z^{-k}\\mathbf{X}(z)$ (Oppenheim et al., 1996); $(b)$ divides both sides by $\\begin{array}{r}{1+\\sum_{\\tau=1}^{n_{a}}a_{\\tau}z^{-\\tau}}\\end{array}$ ; in $(c)$ we define $\\{z_{a,\\tau}\\},\\{p_{a,\\tau}\\}$ such that $\\begin{array}{r}{\\sum_{\\tau=1}^{n_{a}}\\frac{z_{a,\\tau}}{1-p_{a,\\tau}z^{-1}}\\,=\\,\\frac{1}{1+\\sum_{\\tau=1}^{n_{a}}a_{\\tau}z^{-\\tau}}}\\end{array}$ ; $(d)$ e xpands $\\scriptstyle{\\frac{1}{1-p}}\\ =$ $\\textstyle\\sum_{t=0}^{\\infty}p^{t}$ ; in (e) we rearrange there terms; in $(f)$ we apply the inverse $z$ -transform and notice that $\\mathbf{g}_{<0}=0$ ; and in the last equation we define \u03c4m2i=n{0nb,\u03c4}b\u03c42 \u03c4n1a=1 za,\u03c41(pa,\u03c41)\u03c4\u2212\u03c42. Plug the expansion of ${\\bf m}_{t}$ back to (9), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\mathbf{x}_{1+1})-F(\\mathbf{x}_{1})\\leq-\\eta\\left\\langle\\nabla F(\\mathbf{x}_{1}),\\mathbf{X}_{\\tau}^{\\top}\\mathbf{x}_{0},\\mathbf{z}_{1}-\\mathbf{z}_{1}\\right\\rangle+\\frac{L\\eta^{2}}{2}\\mathbb{E}\\left\\|\\frac{1}{r_{0}n}\\sum_{k=0}^{K}\\mathbb{I}\\right\\|_{r_{1}}^{2}}\\\\ &{\\overset{(i)}{\\leq}-\\eta\\frac{\\Gamma}{r_{0}n},}\\\\ &{\\overset{(i)}{\\leq}-\\eta\\frac{\\Gamma}{r_{0}n},}\\\\ &{\\overset{(i)}{\\leq}-\\eta\\frac{\\Gamma}{r_{0}n},}\\\\ &{\\vdots\\frac{L\\eta^{2}}{r_{0}n}\\left(\\|\\nabla F(\\mathbf{x}_{1}),\\nabla F(\\mathbf{x}_{-1})\\right\\rangle}\\\\ &{\\quad+\\frac{L\\eta^{2}}{2}\\left(\\left\\|\\sum_{s=0}^{K}\\mathrm{e}^{\\mathrm{i}}\\nabla F(\\mathbf{x}_{1}-\\mathbf{z}_{1})\\right\\|^{2}+\\sum_{s=0}^{K}\\left(\\alpha\\mu_{0}^{2}+\\frac{\\sigma_{0}^{2}\\omega}{2B}\\right)\\right)}\\\\ &{\\overset{(i)}{\\leq}-\\eta\\frac{\\Gamma}{r_{0}n},}\\\\ &{\\vdots-\\eta\\frac{\\Gamma}{r_{0}n},}\\\\ &{\\quad+\\frac{L\\eta^{2}}{r_{0}n}\\left(\\frac{\\Gamma}{r_{0}n},\\|\\nabla F(\\mathbf{x}_{1})\\|^{2}-\\eta\\frac{\\Gamma}{r_{0}n},\\|\\nabla F(\\mathbf{x}_{-1})\\|^{2}\\right.}\\\\ &{\\quad+\\left.\\frac{L\\eta^{2}}{2}\\left(\\frac{\\Gamma}{r_{0}n},\\|\\nabla F(\\mathbf{x}_{1})\\|^{2}+\\sum_{s=0}^{K}\\left(\\alpha\\mu_{0}^{2}+\\frac{\\sigma_{0}^{2}\\omega}{2B}\\right)\\right)}\\\\ &{\\overset{(i i)}{\\leq}-\\eta\\frac{\\Gamma}{r_{0}n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where in $(a)$ we apply A3 and set $C\\,\\geq\\,G$ , so that $\\mathbb{E}[\\mathbf{g}_{t}]\\;=\\;\\nabla F(\\mathbf{x}_{t});\\;($ $(b)$ applies A2 to the last term, and use the fact that $\\mathbf{w}_{t}\\sim\\mathcal{N}(0,\\sigma_{\\mathrm{DP}}^{2}\\cdot I_{d})$ , and the noise are independent; $(c)$ applies A4 to the first term and uses Jensen\u2019s inequality to the second term with $\\left\\Vert\\cdot\\right\\Vert^{2}$ being convex; in $(d)$ we set $\\begin{array}{r}{\\eta\\le\\operatorname*{min}_{\\tau}\\{\\frac{2c-\\tau}{L\\kappa_{\\tau}}\\}}\\end{array}$ . Clearly, we have $\\begin{array}{r}{\\sum_{\\tau=0}^{t}\\kappa_{\\tau}\\leq\\frac{\\sum_{\\tau=0}^{n_{b}}b_{\\tau}}{1-\\sum_{\\tau=1}^{n_{a}}a_{\\tau}}}\\end{array}$ . Averaging over $t=0,\\dots,T-1$ , and deciding both side by $\\eta$ , then the theorem is proved. ", "page_idx": 14}, {"type": "text", "text": "C Missing experiment details in the main paper ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide the missing details for the experiments in the main paper and additional experiments. ", "page_idx": 14}, {"type": "image", "img_path": "r8YntmAd0g/tmp/8a473b9209c4bd4b0b58809b7ecb3febd7b16889973c1717e56dd224a865f400.jpg", "img_caption": ["Figure 6: The time and frequency response of the filters used in the paper. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Experiments compute resources: Each experiment is conducted on an EPYC-7513 CPU with one NVIDIA A100 (80 GB) GPU. The runtime ranges from 1 hour to 48 hours. The codes for the optimizers and the job scripts are given in the supplementary. ", "page_idx": 15}, {"type": "text", "text": "C.1 Hyper-parameter choice ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide the hyper-parameter choices and search grids for different experiment settings. The search grids for the experiments are given in Table 1. ", "page_idx": 15}, {"type": "table", "img_path": "r8YntmAd0g/tmp/3508437a75fe755174d0f0570889a6f18e7e65d474b2a38bb0ca1b7ff685c523.jpg", "table_caption": ["Table 1: Search grids for each hyper-parameter. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "The filter coefficients used in the experiments are given in Table 2. ", "page_idx": 15}, {"type": "text", "text": "Table 2: Possible choice of the coefficients for the filter of different orders. ", "page_idx": 15}, {"type": "table", "img_path": "r8YntmAd0g/tmp/841deb42ab33d38d5624b57fdb3779b25cbd7714fa3a361be6c0982d71b3e468.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C.2 Algorithm variants ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We provide the update rules for different DP optimizer variants used in the main paper in Algorithm 3. Different components of the optimizers are highlighted with different colors. the blue lines are additional steps for the Adam update, and brown lines are the components of the GaLore (Zhao et al., 2024) optimizer. ", "page_idx": 15}, {"type": "image", "img_path": "r8YntmAd0g/tmp/03cecc02fca3669a1bddd6177a112ea3ac35f101864d7f42f4a169604cd02d5e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.3 Additional experiments: the GLUE dataset ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We also conduct experiments on the GLUE dataset (Wang et al., 2018). We fine-tune a RoBERTabase model (Liu et al., 2019) with the pretrained weights from https://huggingface.co/ FacebookAI/roberta-base. We follow the training script provided in Li et al. (2021). The results are shown in Table 3. For comparison, we also include the results from Li et al. (2021). From the result of DPAdamBC and LP-DPAdamBC, we observe a slight accuracy improvement by using DOPPLER. However, in the fine-tuning tasks, we do not see significant improvement by using DOPPLER compared with the result reported in Li et al. (2021). ", "page_idx": 16}, {"type": "table", "img_path": "r8YntmAd0g/tmp/1d204f3e0ecf1c8bac7e3bdf18a9be8cd045c0335116255731d5460b92bbcecc.jpg", "table_caption": ["Table 3: Test accuracy on language tasks with RoBERTa-base, $\\epsilon=\\{3,8\\}$ . "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.4 Additional experiments: ablation studies ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide additional numerical results for several ablation studies w.r.t. the impact of different components in the experiment. ", "page_idx": 16}, {"type": "text", "text": "Results for different models. The results for pretraining different models are given in Figure 7. We observe that DOPPLERuniformly improves the DP pretraining performance for different model structures in pre-training. ", "page_idx": 16}, {"type": "text", "text": "Ablation study on filter coefficients. The results for DPSGD accompanied with different low-pass filter coefficients in Table 2 are given in Figure 8a. We observe that different fliters have different impacts on the algorithm\u2019s performance. For training on the CIFAR-10 dataset, the first-order filter is sufficient to have a good performance, while different coefficient choices for filters of the same order also have different performances. The second-order filter provides an over-smoothing to the gradient, leading to slow convergence in the early stage of training. Although the second-order filter does not have a good performance, it could have a better performance when the number of training steps is longer (e.g., when training on the Imagenet dataset (Russakovsky et al., 2015)). ", "page_idx": 16}, {"type": "image", "img_path": "r8YntmAd0g/tmp/eb9534cfd070be750a8b9a28546e326fa88d0e353db3151a7662631101aaa23b.jpg", "img_caption": ["Figure 7: Comparision between DPSGD LP-DPSGD for pre-training different models on CIFAR-10 dataset with $\\epsilon=8$ . "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "image", "img_path": "r8YntmAd0g/tmp/f9a1f0a87d2e7c3012138e406b18eae018783cd7f387e286ba7add3ab8f686fb.jpg", "img_caption": ["Figure 8: LP-DPSGD for pre-training on CIFAR-10 with different filter coefficients. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "An additional set of first- and second-order filters with different coefficients in Table 4. Coefficient choices f1 and f2 compare different values of $b_{\\tau}$ ; f3 and f4 compare the impact of the values of $a_{\\tau}$ ; f5 and f6 compare the impact of filter orders $n_{b}$ and $n_{a}$ . ", "page_idx": 17}, {"type": "table", "img_path": "r8YntmAd0g/tmp/18356daa033e705f3d7f864e90d4c67ee4da010f5829dba2eb7908ac2f81ad8c.jpg", "table_caption": ["Table 4: Possible choice of the coefficients for the filter of different orders. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Impact of different clipping operations. The results for LP-DPSGD with different clipping methods are given in Figure 9. We observe automatic clipping (Bu et al., 2024) (Norm) is better than vanilla clipping described in (Abadi et al., 2016) (Clip); treating all layers as one vector (Flat) is better than clipping each layer separately (Layer). ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "image", "img_path": "r8YntmAd0g/tmp/2effce8f49a0120d6d2b15ace5003b6f02a43b94bd6b06963f765a32f892daa9.jpg", "img_caption": ["Figure 9: DPLPSGD for pre-training on CIFAR-10 with different clipping strategies. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Impact of learning rate and scheduler: We report part of our hyper-parameter search process. First, we report the test accuracy for different learning rates with fixed epochs (150) and $(8,1/50000^{1.1})$ -DP. The results are shown in Figure 10. The optimal learning rate is $10^{-3}$ ; a larger learning rate speeds up the training in the early stage but hurts the final performance; a smaller learning rate results in slow convergence. In Figure 11, we compare the optimizer with and without a learning rate scheduler. Specifically, we use the Cosine-Annealing with warmup from Loshchilov and Hutter (2017). From the result, we see when the number of epochs is large, the learning rate scheduler improves the training performance, while in the early stage, the scheduler slows down the convergence. ", "page_idx": 18}, {"type": "image", "img_path": "r8YntmAd0g/tmp/d1898e8ead8d8ee1d594327d8df49619d2d23c10350e037493e31e425c90537c.jpg", "img_caption": ["Figure 10: LP-DPAdamBC for pre-training on CIFAR-10 with different learning rates. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "r8YntmAd0g/tmp/8b35ab32563435c7791428663c5a55d80d97d5c811f476289edc16b25a4668b7.jpg", "img_caption": ["Figure 11: LP-DPSGD with and without learning rate scheduler "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Finetuning on CIFAR dataset: We also provide the experiment result for fine-tuning ViT models on the CIFAR-10 dataset under different privacy budgets. As illustrated in Figure 12, the performance of LP-DPSGD improves by less than $1\\%$ for small $\\epsilon^{\\prime}\\mathbf{s}$ . For larger $\\epsilon^{\\prime}\\mathbf{s}$ , the gap is smaller. DOPPLER works has more improvement when the injected DP noise is large (i.e., $\\epsilon$ is small). ", "page_idx": 18}, {"type": "image", "img_path": "r8YntmAd0g/tmp/32ba3a98ff44a966b87e29329a2db47c9e7d4b129b7f50c090887f900889b562.jpg", "img_caption": ["Figure 12: DPSGD and LP-DPSGD for fine-tuning ViT on CIFAR-10 with different $\\epsilon$ \u2019s. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The claim matches with the results in the main paper ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Yes, we provide a discussion at the end of the paper. And the limitation of the assumptions are discussed right after the the definitions. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide assumption in the statement of the theorems and proofs in the appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide the codes in the supplementary and the parameters are given in the experiment section and the appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, we included the code for the experiment in the supplementary Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The parameters are either provided in the citations or given in the appendix Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: We did not report the error bar, partially due to the unknown distribution of the error. In addition, most of our experimental results were done with one run due to the large size nature of them. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The training details are included in the appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: There is no significant societal impact of the work performed. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The code provided contains the license information. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The code of the algorithm is well-documented. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]