[{"figure_path": "r8YntmAd0g/tables/tables_2_1.jpg", "caption": "Table 3: Test accuracy on language tasks with ROBERTa-base, e = {3, 8}.", "description": "This table compares the test accuracy of different models on several language tasks using the ROBERTa-base model.  The results are shown for different privacy budget levels (epsilon = 3 and epsilon = 8).  It allows for comparison of the performance of different DP optimizers (DPAdam, DPAdamBC, and LP-DPAdamBC) under varying privacy constraints.  The tasks evaluated are QQP, QNLI, MNLI, and SST-2.", "section": "C.3 Additional experiments: the GLUE dataset"}, {"figure_path": "r8YntmAd0g/tables/tables_5_1.jpg", "caption": "Table 3: Test accuracy on language tasks with ROBERTa-base, e = {3,8}.", "description": "This table compares the test accuracy of different DP optimizers on various language tasks using the ROBERTa-base model.  The results are presented for two different privacy budget values (epsilon = 3 and epsilon = 8), showing the impact of the privacy budget and the optimizer on the final performance. It specifically highlights the performance differences between different DP optimizers and their enhanced versions utilizing DOPPLER.", "section": "C.3 Additional experiments: the GLUE dataset"}, {"figure_path": "r8YntmAd0g/tables/tables_15_1.jpg", "caption": "Table 1: Search grids for each hyper-parameter.", "description": "This table shows the range of hyper-parameters used in the grid search for different experiments in the paper.  The hyper-parameters include the number of epochs, batch size for CIFAR10/CIFAR100 and GLUE datasets, and the learning rates for SGD, Adam, and GaLore optimizers.", "section": "C.1 Hyper-parameter choice"}, {"figure_path": "r8YntmAd0g/tables/tables_15_2.jpg", "caption": "Table 2: Possible choices of the coefficients for the filter of different orders.", "description": "This table lists the coefficients used for filters of different orders (0th to 2nd order).  The coefficients determine the characteristics of the low-pass filter applied to the gradients in the DOPPLER method.  Different filter orders offer different tradeoffs between computational cost and the effectiveness of noise reduction. The choices provided are based on empirical experimentation.", "section": "C.1 Hyper-parameter choice"}, {"figure_path": "r8YntmAd0g/tables/tables_16_1.jpg", "caption": "Table 3: Test accuracy on language tasks with ROBERTa-base,  = {3,8}.", "description": "This table presents the results of fine-tuning a RoBERTa-base model on the GLUE benchmark dataset using different differentially private (DP) optimization methods.  The table shows test accuracy scores across multiple tasks (MNLI, QQP, QNLI, SST-2) for two different privacy budgets (\u03f5 = 3 and \u03f5 = 8).  The methods compared include DPAdam from existing literature and DPAdamBC and its variant with the low-pass filter (LP-DPAdamBC) proposed in this paper.", "section": "C.3 Additional experiments: the GLUE dataset"}, {"figure_path": "r8YntmAd0g/tables/tables_17_1.jpg", "caption": "Table 4: Possible choice of the coefficients for the filter of different orders.", "description": "This table shows various choices of filter coefficients  (a_\u03c4 and b_\u03c4 ) for low-pass filters of different orders (max{n_a, n_b}).  The filter coefficients influence the frequency response of the filter, impacting how the filter attenuates high-frequency noise and passes low-frequency signal. Different choices lead to various trade-offs between the filter's performance. For example, higher-order filters can provide more flexibility in shaping the frequency response but may require more computational resources.", "section": "C.1 Hyper-parameter choice"}]