[{"figure_path": "vunJCq9PwU/figures/figures_6_1.jpg", "caption": "Figure 2: Cumulative robust accuracy (RA) with varying L2 perturbation level using 500 samples. Note that GREAT Score gives a certified RA for attack-proof robustness, whereas Auto-Attack is an empirical robustness evaluation.", "description": "The figure shows a comparison of cumulative robust accuracy (RA) between GREAT Score and Auto-Attack under varying L2 perturbation levels.  Both methods were tested on 500 samples.  GREAT Score provides a certified lower bound on RA, meaning it guarantees that at a given perturbation level, the model's accuracy will be at least that value.  Conversely, Auto-Attack gives an empirical (measured) RA under the same conditions.  The difference highlights that the GREAT Score provides a guaranteed level of robustness while the Auto-Attack reflects the actual performance against a specific attack strategy.", "section": "4.2 Local and Global Robustness Analysis"}, {"figure_path": "vunJCq9PwU/figures/figures_6_2.jpg", "caption": "Figure 1: Comparison of local GREAT Score and CW attack in L2 perturbation on CIFAR-10 with Rebuffi_extra model [46]. The x-axis is the image id. The result shows the local GREAT Score is indeed a lower bound of the perturbation level found by CW attack.", "description": "This figure compares the local GREAT Score and the L2 perturbation level found by the CW attack on 20 randomly selected images from the CIFAR-10 dataset using the Rebuffi_extra model.  The x-axis represents the image ID, and the y-axis represents the L2 perturbation level.  Each point shows the perturbation level for a single image. The blue triangles represent the perturbation level found by the CW attack (a strong adversarial attack method), and the orange circles represent the local GREAT Score (a certified lower bound on the minimal adversarial perturbation).  The figure visually demonstrates that the local GREAT Score consistently provides a lower bound for the perturbation level found by the CW attack, validating its theoretical properties.", "section": "4.2 Local and Global Robustness Analysis"}, {"figure_path": "vunJCq9PwU/figures/figures_8_1.jpg", "caption": "Figure 3: Comparison of Inception Score and Spearman\u2019s rank correlation to RobustBench using GREAT Score with different GANs.", "description": "This figure shows the results of an ablation study comparing different generative models used in the GREAT Score framework.  The left y-axis shows the Inception Score (IS), a metric evaluating the quality of generated images by GANs. The right y-axis displays the Spearman\u2019s rank correlation between GREAT Score and RobustBench, assessing the consistency of model rankings. Higher IS values generally correlate with higher rank correlation, suggesting that better generative models lead to more reliable global robustness estimates. This demonstrates that the quality of the generative model influences GREAT score performance.", "section": "Ablation Study and Run-time Analysis"}, {"figure_path": "vunJCq9PwU/figures/figures_19_1.jpg", "caption": "Figure 6: The relationship between the approximation error (\u20ac) and sample complexity in Theorem 2, with three different confidence levels: \u03b4 = {5, 15, 25}%.", "description": "This figure shows the relationship between the approximation error (epsilon) and the sample complexity required to achieve a certain level of confidence (delta) in the estimation of the global robustness metric, GREAT Score.  The sample complexity refers to the number of samples needed from the generative model to obtain a reliable estimate.  As expected, smaller errors (epsilon) or higher confidence levels (delta) necessitate a larger sample size (sample complexity). The figure showcases three different confidence levels (delta = 0.05, 0.15, and 0.25), each with its corresponding relationship between the approximation error and sample complexity.", "section": "H Approximation Error and Sample Complexity"}, {"figure_path": "vunJCq9PwU/figures/figures_20_1.jpg", "caption": "Figure 7: The relation of GREAT Score and sample complexity using CIFAR-10 and Rebuffi_extra model over (500-10000) range. The data points refer to the mean value for GREAT Score, and the error bars refers to the standard derivation for GREAT Score.", "description": "This figure shows the relationship between the GREAT Score and the number of samples used in the evaluation.  The x-axis represents the sample complexity, ranging from 500 to 10000. The y-axis represents the GREAT Score.  Each data point shows the average GREAT Score obtained across multiple runs with the given sample size, and the error bars represent the standard deviation. The figure demonstrates the stability of the GREAT Score even with a relatively small number of samples.", "section": "J Sample Complexity and GREAT Score"}, {"figure_path": "vunJCq9PwU/figures/figures_21_1.jpg", "caption": "Figure 7: The relation of GREAT Score and sample complexity using CIFAR-10 and Rebuffi_extra model over (500-10000) range. The data points refer to the mean value for GREAT Score, and the error bars refers to the standard derivation for GREAT Score.", "description": "This figure shows the relationship between the GREAT Score and the number of samples used for its calculation.  It uses the CIFAR-10 dataset and the Rebuffi_extra model.  The x-axis represents the number of samples (sample complexity), ranging from 500 to 10000. The y-axis shows the GREAT Score. For each sample size, a mean GREAT Score and standard deviation are calculated and plotted.  The error bars represent the standard deviation, showing the variability of the GREAT Score across different sample sets of the same size.", "section": "J Sample Complexity and GREAT Score"}, {"figure_path": "vunJCq9PwU/figures/figures_21_2.jpg", "caption": "Figure 9: Generated Images for old subgroup.", "description": "This figure shows a sample of generated images from the InterFaceGAN model, specifically focusing on the 'old' subgroup.  The images showcase the diversity of faces generated by the model while maintaining characteristics consistent with the label. This helps to visualize the quality and variety of the synthetic data used to evaluate the GREAT Score in a privacy-preserving way.", "section": "L Generated Images from Facial GAN Models"}, {"figure_path": "vunJCq9PwU/figures/figures_22_1.jpg", "caption": "Figure 9: Generated Images for old subgroup.", "description": "This figure shows sample images generated by a GAN for the \"old\" subgroup in a facial recognition robustness evaluation.  The images demonstrate the variety of faces produced by the GAN for this particular attribute, showcasing differences in facial features, hair, and other visual characteristics. These images are synthetically generated and used in the evaluation process to assess the robustness of online facial recognition APIs to various types of adversarial attacks.", "section": "L Generated Images from Facial GAN Models"}, {"figure_path": "vunJCq9PwU/figures/figures_22_2.jpg", "caption": "Figure 11: Generated Images for with-eyeglasses subgroup.", "description": "This figure shows a sample of generated images from a GAN model.  The images are all of faces with eyeglasses, demonstrating the model's ability to control the attributes of the generated faces (in this case, the presence of eyeglasses).  This is relevant to the paper because these generated images are used to evaluate the robustness of facial recognition APIs against adversarial attacks.", "section": "L Generated Images from Facial GAN Models"}, {"figure_path": "vunJCq9PwU/figures/figures_22_3.jpg", "caption": "Figure 9: Generated Images for old subgroup.", "description": "This figure shows a sample of generated images from the InterFaceGAN model, specifically those categorized as belonging to the 'old' subgroup.  The images showcase the model's ability to generate diverse facial features and appearances consistent with the aging process, which were used for evaluating the robustness of online facial recognition APIs in a privacy-preserving way.", "section": "L Generated Images from Facial GAN Models"}]