[{"figure_path": "IfZwSRpqHl/figures/figures_5_1.jpg", "caption": "Figure 1: Performance of a 5-layer GAT with various dynamic rescaling (DR) settings using learning rates (lr) 0.001 and 0.01. Across 10 splits, the mean accuracy is reported for roman-empire and amazon-ratings while ROC AUC is reported for minesweeper, questions and tolokers. The case annotated by * indicates training for more than 10k epochs.", "description": "This figure shows the performance of a 5-layer Graph Attention Network (GAT) trained with different dynamic rescaling settings and learning rates.  The results are presented for five different datasets (roman-empire, amazon-ratings, minesweeper, questions, and tolokers). The performance metric used is accuracy for roman-empire and amazon-ratings, and ROC AUC for the other three datasets.  The figure compares performance with no dynamic rescaling (w/o DR), dynamic rescaling based on weight norms (DRW), dynamic rescaling based on relative gradients (DRRG), and a combination of both (DRC).  The results show that applying DRRG or DRC can sometimes lead to improved performance compared to no dynamic rescaling, especially with a learning rate of 0.01. The asterisk (*) indicates that training continued for more than 10,000 epochs in certain cases.", "section": "3.1 Training in balance"}, {"figure_path": "IfZwSRpqHl/figures/figures_5_2.jpg", "caption": "Figure 2: The degree of imbalance, i.e. the R.H.S. quantity of Eq. (5), before and after rebalancing every 10 epochs when training roman-empire on GAT. A value of 0 indicates complete balance.", "description": "This figure shows how balanced the network is, based on a specific criterion (Eq. 5 in the paper), both before and after applying a rebalancing technique. The x-axis represents the training epoch, and the y-axis shows the degree of imbalance.  Each line represents a different neuron in the network.  The goal is to minimize imbalance (bring the value toward zero), which is done by rescaling weights and gradients as described in the paper.  The top row of plots illustrates the imbalance before rebalancing, while the bottom row depicts the situation after rebalancing every 10 epochs. The figure is a visualization of the dynamic rescaling technique used to maintain balance in the network during training.", "section": "3.1 Training in balance"}, {"figure_path": "IfZwSRpqHl/figures/figures_5_3.jpg", "caption": "Figure 1: Performance of a 5-layer GAT with various dynamic rescaling (DR) settings using learning rates (lr) 0.001 and 0.01. Across 10 splits, the mean accuracy is reported for roman-empire and amazon-ratings while ROC AUC is reported for minesweeper, questions and tolokers. The case annotated by * indicates training for more than 10k epochs.", "description": "This figure shows the performance of a 5-layer Graph Attention Network (GAT) trained with different dynamic rescaling settings and learning rates.  The dynamic rescaling methods are denoted by DRW, DRRG, and DRC, representing dynamic rescaling with respect to weight norms, relative gradients, and a combination of both, respectively.  The performance is evaluated using accuracy for the datasets roman-empire and amazon-ratings, and ROC AUC for minesweeper, questions, and tolokers.  The results are averaged across 10 different random train/test splits. The asterisk (*) indicates that training ran for more than 10,000 epochs.", "section": "3.1 Training in balance"}, {"figure_path": "IfZwSRpqHl/figures/figures_6_1.jpg", "caption": "Figure 4: Performance of a five-layer GAT network on synthetic data under varying settings. Standard implies regular training with no constant or dynamic rescaling. L = l for l \u2208 [5] denotes scaling down the parameters of l by a constant (X = 0.002) at initialization followed by regular training. DR denotes dynamic rescaling to balance relative gradients during training with the specified learning rate (lr). Note that the train and test accuracy axis have been zoomed in for clarity and the initial (train or test) accuracy is lower than 0.9 (but rises sharply in the first few epochs). The best strategy (among considered cases) for this task is to (initially) focus the learning more on the first layer.", "description": "This figure shows the performance of a 5-layer GAT network on synthetic data using various training strategies. The standard training shows the baseline performance. L=1 to L=5 represents training where only the layers 1 to 5 are initially scaled-down, respectively, by a constant factor before starting regular training. DR represents dynamic rescaling methods to balance relative gradients during the training process using the specified learning rates.  The results show that focusing training on the first layer is the most effective strategy for this particular synthetic task. The graphs show both the training and test loss, as well as training and test accuracy over the epochs.", "section": "3.2 Learning layers in order"}, {"figure_path": "IfZwSRpqHl/figures/figures_6_2.jpg", "caption": "Figure 5: Impact of training layers of a two-layer GAT network in and out balance for different tasks. The tasks {amazon-ratings, questions, roman-empire, tolokers, minesweeper} are heterophilic and the remaining are homophilic. Homophilic tasks tend to perform better and converge much faster with learning concentrated in the first layer initially (lower weight norms imply larger relative gradients), whereas heterophilic tasks perform better when layers are trained in balance. Interestingly, even freezing the initial values of parameters in the second layer (i.e. only allowing the second layer to learn) does not significantly reduce the performance for homophilic tasks, even without an additional classifier layer. On the contrary, freezing the first layer results in a severe drop in performance for all tasks.", "description": "This figure displays the results of experiments on various datasets, categorized as homophilic or heterophilic. It shows the impact of different training strategies on the performance (test metric and convergence epoch) of a two-layer GAT network. The strategies include training with all layers in balance (BRG and BC), training with initially only the first layer active, training with initially only the second layer active, and standard training. Results indicate that for homophilic tasks, focusing learning on the first layer is beneficial, while for heterophilic tasks, balanced training leads to better performance.", "section": "3.2 Learning layers in order"}, {"figure_path": "IfZwSRpqHl/figures/figures_7_1.jpg", "caption": "Figure 8: Evolution of relative gradients norms in a five-layer GAT network trained on synthetic data under varying settings. Test accuracy (%) @ epoch of maximum validation accuracy is reported in parentheses. The colored heatmap in the background displays log10 of relative gradient \u21132-norm for each layer (left axis) during training. Darker regions correspond to higher relative gradients. The training curves in the foreground show the train and test loss (right axis) for the epoch.", "description": "This figure displays the evolution of relative gradient norms (log scale) and loss curves during training of a five-layer GAT network on synthetic data under various settings. The heatmaps show relative gradient \u21132-norms for each layer, with darker colors indicating larger norms.  The line graphs show the training and test loss curves. The figure demonstrates how different initial conditions and learning rate adjustments influence the training dynamics. It helps visualize the impact of concentrating initial learning on specific layers, followed by rebalancing.", "section": "3.2 Learning layers in order"}, {"figure_path": "IfZwSRpqHl/figures/figures_8_1.jpg", "caption": "Figure 7: Left: Layer-wise relative gradient norm (log10 scale) and loss curves similar to Figure 3 in the paper. Right: Corresponding accuracy of the same run. Grokking-like phenomenon can be induced On a 5-layer GAT using real-world roman-empire dataset by placing initial training focus on layers 4 (top) and 5 (bottom) by scaling down initial parameter norms, followed by rebalancing w.r.t. relative gradients every 10 epochs staring only at epoch 1000. Note the sharp drop in validation/test loss immediately after rebalancing which also translates to more rapid improvement in test accuracy.", "description": "This figure shows the layer-wise relative gradient norms and loss curves for a 5-layer GAT trained on the roman-empire dataset.  The left panel displays a heatmap of log10(relative gradient norms) over epochs, with separate lines for training, validation, and test loss. The right panel displays the corresponding accuracy curves. The experiment induced grokking-like behavior by initially focusing training on layers 4 or 5 (by scaling down initial parameters) and then rebalancing every 10 epochs starting at epoch 1000. Rebalancing leads to a sharp drop in validation and test loss, and a rapid increase in test accuracy, demonstrating a grokking-like phenomenon.", "section": "3.3 Grokking-like phenomena"}, {"figure_path": "IfZwSRpqHl/figures/figures_16_1.jpg", "caption": "Figure 8: Evolution of relative gradients norms in a five-layer GAT network trained on synthetic data under varying settings. Test accuracy (%) @ epoch of maximum validation accuracy is reported in parentheses. The colored heatmap in the background displays log10 of relative gradient l2-norm for each layer (left axis) during training. Darker regions correspond to higher relative gradients. The training curves in the foreground show the train and test loss (right axis) for the epoch.", "description": "This figure shows the evolution of relative gradient norms (in log10 scale) and training/test loss curves for a five-layer GAT network trained on a synthetic dataset under different dynamic rescaling settings. The heatmap visualizes the relative gradient norms across different layers during training, highlighting how these norms change over time and across layers.  The line plots display the training and testing loss curves, indicating the network's performance during training and its generalization ability. The caption indicates test accuracy at the best validation epoch for each setting, providing a quantitative measure of the network's overall performance.", "section": "3.2 Learning layers in order"}]