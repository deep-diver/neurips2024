{"references": [{"fullname_first_author": "David Balduzzi", "paper_title": "The shattered gradients problem: If resnets are the answer, then what is the question?", "publication_date": "2018-00-00", "reason": "This paper introduces the 'shattered gradients problem' in deep learning, a concept relevant to the challenges of training deep neural networks and the potential benefits of rescaling techniques."}, {"fullname_first_author": "Shaked Brody", "paper_title": "How attentive are graph attention networks?", "publication_date": "2022-00-00", "reason": "This paper analyzes the attention mechanism in Graph Attention Networks (GATs), which is crucial for understanding the rescale invariance properties exploited in the current work."}, {"fullname_first_author": "Tianle Cai", "paper_title": "Graphnorm: A principled approach to accelerating graph neural network training", "publication_date": "2021-00-00", "reason": "This paper introduces GraphNorm, a normalization technique for GNNs, which is relevant to the current paper's focus on improving the training dynamics of GNNs."}, {"fullname_first_author": "S. S. Du", "paper_title": "Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced", "publication_date": "2018-00-00", "reason": "This paper explores algorithmic regularization in deep learning, particularly focusing on the concept of balanced layers, which is a key element of the proposed dynamic rescaling method."}, {"fullname_first_author": "Xavier Glorot", "paper_title": "Understanding the difficulty of training deep feedforward neural networks", "publication_date": "2010-05-00", "reason": "This foundational paper discusses challenges in training deep neural networks, providing context for the current work's focus on addressing similar challenges in the context of GNNs."}]}