[{"figure_path": "IfZwSRpqHl/tables/tables_4_1.jpg", "caption": "Table 1: Results of training a 5-layer GAT network with various dynamic rescaling (DR) settings. The mean \u00b195% CI test metric at the epoch of the best validation metric across 10 splits is reported using the best learning rate from {0.01, 0.001, 0.005}. The evaluation metric is accuracy for roman-empire and amazon-ratings, and ROC AUC for the remaining three datasets.", "description": "This table presents the results of training a 5-layer Graph Attention Network (GAT) using four different dynamic rescaling methods: no dynamic rescaling (w/o DR), dynamic rescaling with respect to weight norms (DR<sub>W</sub>), dynamic rescaling with respect to relative gradients (DR<sub>RG</sub>), and a combination of both (DR<sub>C</sub>).  The table shows the mean and 95% confidence interval of the test metric (accuracy for roman-empire and amazon-ratings, ROC AUC for others) achieved at the epoch with the best validation performance, across 10 different random train/test splits. The best learning rate from a set of options was used for each setting.  The asterisk (*) indicates statistically significantly better performance compared to no dynamic rescaling.", "section": "3 Experiments"}, {"figure_path": "IfZwSRpqHl/tables/tables_16_1.jpg", "caption": "Table 1: Results of training a 5-layer GAT network with various dynamic rescaling (DR) settings. The mean \u00b195% CI test metric at the epoch of the best validation metric across 10 splits is reported using the best learning rate from {0.01, 0.001, 0.005}. The evaluation metric is accuracy for roman-empire and amazon-ratings, and ROC AUC for the remaining three datasets.", "description": "This table presents the results of training a 5-layer Graph Attention Network (GAT) using different dynamic rescaling methods.  It shows the mean test accuracy (or ROC AUC) achieved across 10 different random train/test splits for five different datasets (roman-empire, amazon-ratings, tolokers, questions, minesweeper). The best learning rate (from 0.01, 0.001, and 0.005) was selected for each method and dataset.  The methods compared are training without dynamic rescaling (w/o DR), dynamic rescaling with respect to weight norms (DRW), dynamic rescaling with respect to relative gradients (DRRG), and a combination of both (DRC). The table allows comparison of the performance of these methods across various datasets.", "section": "3.1 Training in balance"}]