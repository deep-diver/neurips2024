[{"figure_path": "T826pwZLci/figures/figures_8_1.jpg", "caption": "Figure 1: Per-client regret.", "description": "The figure compares the per-client regret of the Fed-DP-OPE-Stoch algorithm and the Limited Updates algorithm as a function of the time horizon T.  The plot shows that Fed-DP-OPE-Stoch achieves significantly lower regret than Limited Updates, demonstrating the effectiveness of the proposed federated learning approach.  The experiment used parameters m=10, \u03b5=10, and d=100.", "section": "Numerical Experiments"}, {"figure_path": "T826pwZLci/figures/figures_8_2.jpg", "caption": "Figure 2: Per-client regret.", "description": "The figure shows the per-client regret as a function of the time horizon (T) for different algorithms. The algorithms compared are Sparse-Vector (a single-player model), and Fed-SVT (the federated algorithm proposed in the paper) with different communication intervals (N). The results indicate that Fed-SVT outperforms the single-player model significantly, highlighting the benefits of collaborative expert selection in reducing the per-client regret, even with infrequent communication.", "section": "Federated OPE with Oblivious Adversaries: Realizable Setting"}, {"figure_path": "T826pwZLci/figures/figures_17_1.jpg", "caption": "Figure 3: Binary tree with depth j = 2 in Algorithm 3.", "description": "This figure shows a binary tree structure used in the DP-FW algorithm (Algorithm 3).  The root node is labeled \u00d8, representing the initial gradient estimate. Each internal node represents an intermediate gradient estimate calculated from its child nodes.  The leaf nodes represent final gradient estimates calculated using the DP-FW algorithm's iterative process on sets of loss functions assigned to each leaf. The tree's structure determines the hierarchical gradient aggregation in the algorithm, enabling privacy-preserving estimation.", "section": "C.1 DP-FW"}, {"figure_path": "T826pwZLci/figures/figures_33_1.jpg", "caption": "Figure 4: Comparison between Fed-DP-OPE-Stoch and Limited Updates with different random seeds.", "description": "The figure compares the performance of the Fed-DP-OPE-Stoch algorithm and the Limited Updates algorithm, both designed for online prediction from experts under differential privacy constraints with stochastic adversaries. The experiment is repeated six times with different random seeds to demonstrate the consistency and robustness of the results. Each subplot shows the per-client cumulative regret as a function of time horizon (T) for one experiment with different random seeds. The results show that Fed-DP-OPE-Stoch consistently outperforms Limited Updates, illustrating its effectiveness and the benefits of the proposed federated approach.", "section": "6 Numerical Experiments"}, {"figure_path": "T826pwZLci/figures/figures_33_2.jpg", "caption": "Figure 5: Comparison between Fed-SVT and Sparse-Vector with different random seeds.", "description": "This figure compares the performance of the proposed Fed-SVT algorithm against the Sparse-Vector algorithm, a single-player baseline, under different random seeds for an oblivious realizable setting.  It demonstrates the impact of varying communication intervals (N = 1, 30, 50) on the per-client regret of Fed-SVT.  The results visually showcase the significant reduction in regret achieved by Fed-SVT compared to Sparse-Vector, highlighting the benefits of collaborative learning in the realizable scenario. ", "section": "6 Numerical Experiments"}, {"figure_path": "T826pwZLci/figures/figures_34_1.jpg", "caption": "Figure 6: Regret performance with MovieLens dataset. Shaded area indicates the standard deviation.", "description": "This figure compares the performance of the Fed-SVT algorithm with different communication intervals (N = 1, 30, 50) against the Sparse-Vector algorithm on the MovieLens-1M dataset.  The y-axis represents the per-client regret, and the x-axis represents the time horizon (T).  The shaded area represents the standard deviation across multiple runs.  The results demonstrate that Fed-SVT, especially with larger communication intervals, achieves significantly lower regret than Sparse-Vector.", "section": "6 Numerical Experiments"}]