[{"figure_path": "J6NByZlLNj/figures/figures_3_1.jpg", "caption": "Figure 1: A motivating example for the backdoor trigger design on high-frequency components.", "description": "The figure shows an example of adding noise to different frequency components of an image.  It demonstrates that adding noise to the high-frequency component (HH) results in a less perceptible change than adding the same noise to the low-frequency components (LL, LH, HL). This motivates the authors' choice to inject backdoor triggers into the high-frequency component, as it improves the stealthiness of the attack.", "section": "3.2 Motivation"}, {"figure_path": "J6NByZlLNj/figures/figures_3_2.jpg", "caption": "Figure 2: Overview of our attack method WaveAttack.", "description": "This figure illustrates the WaveAttack method.  It starts with input images, which undergo a Discrete Wavelet Transform (DWT) to separate them into four frequency components (LL, LH, HL, HH). The high-frequency component (HH) is then processed by an encoder-decoder network (E and D) to generate residuals.  These residuals are multiplied by a coefficient (\u03b1) to create a modified HH component (HH'). This modified component, along with the other frequency components from the DWT, is then fed into an Inverse Discrete Wavelet Transform (IDWT) to reconstruct the image. The benign samples, samples with added residuals (payload samples), and regularization samples are randomly split and used to train a classifier (C) to generate the backdoored model.", "section": "3.3 Implementation of WaveAttack"}, {"figure_path": "J6NByZlLNj/figures/figures_6_1.jpg", "caption": "Figure 3: Comparison of examples generated by seven backdoor attacks. For each attack, we show the poisoned sample (top) and the magnified (\u00d75) residual (bottom).", "description": "This figure compares the poisoned samples and their residuals generated by different backdoor attack methods, including BadNets, Blend, IAD, WaNet, BppAttack, Adapt-Blend, FTrojan, and WaveAttack. The top row shows the poisoned samples, while the bottom row shows their corresponding residuals magnified five times.  It visually demonstrates the stealthiness of each method by showing how much the poisoned image differs from the original. WaveAttack shows the least visible difference.", "section": "Stealthiness Evaluation (RQ2)"}, {"figure_path": "J6NByZlLNj/figures/figures_6_2.jpg", "caption": "Figure 3: Comparison of examples generated by seven backdoor attacks. For each attack, we show the poisoned sample (top) and the magnified (\u00d75) residual (bottom).", "description": "This figure compares the poisoned samples generated by seven different backdoor attacks, including WaveAttack.  The top row shows the poisoned samples, while the bottom row displays the residuals (magnified 5x) showing the difference between the poisoned and original images. The goal is to visually assess the stealthiness of each attack, with smaller and less noticeable residuals indicating a more stealthy approach. WaveAttack aims to generate samples that are visually indistinguishable from clean samples.", "section": "Stealthiness Evaluation (RQ2)"}, {"figure_path": "J6NByZlLNj/figures/figures_7_1.jpg", "caption": "Figure 5: STRIP normalized entropy of WaveAttack.", "description": "This figure shows the results of applying the STRIP defense method to the WaveAttack backdoor.  The histograms display the normalized entropy of both clean and poisoned samples for three datasets: CIFAR-10, CIFAR-100, and GTSRB. The overlapping distributions of clean and poisoned samples demonstrate that WaveAttack successfully evades detection by STRIP, indicating its strong stealthiness against this particular defense technique.  The x-axis represents the normalized entropy, while the y-axis shows the number of inputs (samples).", "section": "Resistance to Existing Defenses (RQ3)"}, {"figure_path": "J6NByZlLNj/figures/figures_8_1.jpg", "caption": "Figure 6: GradCAM visualization results for both clean and backdoored models.", "description": "This figure presents a comparison of GradCAM heatmaps for clean and WaveAttack backdoored models. GradCAM is a technique used to visualize which parts of an image a neural network focuses on to make a prediction. The left side shows the heatmaps from a clean model, which correctly classifies the images. The right side shows heatmaps from models that have been attacked using WaveAttack. The similarity in heatmaps between the clean and attacked models demonstrates the stealthiness of the WaveAttack method, making it difficult to distinguish between clean and poisoned images using this technique.", "section": "4.4 Resistance to Existing Defenses (RQ3)"}, {"figure_path": "J6NByZlLNj/figures/figures_8_2.jpg", "caption": "Figure 7: ASR comparison against Fine-Pruning.", "description": "This figure shows the performance comparison between WaveAttack and seven SOTA attack methods on CIFAR-10 by resisting Fine-Pruning.  The x-axis represents the number of filters pruned, while the y-axis shows both the Attack Success Rate (ASR) and Benign Accuracy (BA).  The plots illustrate how the ASR and BA of each attack method change as more and more filters are pruned.  This demonstrates the robustness of WaveAttack against the Fine-Pruning defense method.", "section": "4.4 Resistance to Existing Defenses (RQ3)"}, {"figure_path": "J6NByZlLNj/figures/figures_8_3.jpg", "caption": "Figure 9: Defense performance against NC.", "description": "This figure shows a bar chart comparing the anomaly index of clean and backdoored models for four different datasets: CIFAR-10, CIFAR-100, GTSRB, and ImageNet.  The anomaly index is a measure used by the Neural Cleanse defense method to detect backdoors.  Lower anomaly indices suggest higher stealthiness.  The chart indicates that WaveAttack's backdoored models have anomaly indices below the threshold of 2, making them difficult for Neural Cleanse to detect.", "section": "Resistance to Neural Cleanse"}, {"figure_path": "J6NByZlLNj/figures/figures_9_1.jpg", "caption": "Figure 1: A motivating example for the backdoor trigger design on high-frequency components.", "description": "This figure shows an example to motivate the design of the backdoor trigger on high-frequency components. It compares the effects of adding the same noise to different frequency components (LL, LH, HL, HH) of an image obtained through the Haar wavelet transform. The result shows that it is more difficult to identify the difference between the original image and the poisoned counterpart when noise is added to the HH component (high-frequency component) compared to other components. Therefore, the authors argue that it is more suitable to inject triggers into the HH component for backdoor attacks.", "section": "3.2 Motivation"}]