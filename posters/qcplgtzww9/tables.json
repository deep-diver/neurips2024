[{"figure_path": "qcPlGtzwW9/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of our results with state of the art, in terms of individual gradient oracle complexity required to output xout with E[f(xout) - f(x*)] < \u20ac, where \u20ac > 0 is the target error and x* is the optimal solution. Here, \u03c3\u00b2 = 1/n \u03a3n=1 ||\u2207fi(x*)||2, D = ||x0 - x*||2, and generalized linear model refers to objectives of the form f(x) = \u03a3n=1 li(aTx) as defined in Section 3. Parameters L\u00ba, L\u00ba are defined in Section 2 and satisfy L\u00ba < \u03a3n=1 Li and L\u00ba < Lmax. Parameters L, L, and G are defined in Section 3, and are discussed in the text of this section.", "description": "This table compares the convergence rate results of the proposed method with those of the existing methods in terms of the individual gradient oracle complexity required to achieve a target error. The assumptions made and the step size used for each method are also listed.  The table highlights the superior convergence rate obtained by the proposed method, especially in the generalized linear model setting.  The table uses parameters defined and discussed within the paper.", "section": "1.2 Contributions"}, {"figure_path": "qcPlGtzwW9/tables/tables_9_1.jpg", "caption": "Table 1: Comparison of our results with state of the art, in terms of individual gradient oracle complexity required to output xout with E[f(xout) - f(x*)] < \u2208, where \u2208 > 0 is the target error and x* is the optimal solution. Here, \u03c3\u00b2 = 1/n \u03a3n=1 ||\u2207fi(x*)||\u00b2, D = ||x0 - x*||2, and generalized linear model refers to objectives of the form f(x) = \u2211ni=1 li(aTix) as defined in Section 3. Parameters L\u00ba, L\u00ba are defined in Section 2 and satisfy L\u00ba \u2264 \u2211ni=1 Li and L\u00ba \u2264 Lmax. Parameters L, L, and G are defined in Section 3, and are discussed in the text of this section.", "description": "This table compares the convergence rates of the proposed method with existing methods.  It shows the complexity (number of gradient evaluations) required to achieve a target error (\u2208) under different assumptions (smoothness, convexity) and for different shuffling schemes (RR, SO).  The table highlights the improvements in convergence bounds offered by the proposed method, particularly in terms of the dependence on the number of data points (n).", "section": "1.2 Contributions"}]