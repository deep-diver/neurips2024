[{"type": "text", "text": "Tighter Convergence Bounds for Shuffled SGD via Primal-Dual Perspective ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xufeng Cai\\* ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Cheuk Yin Lin\\* ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Computer Sciences University of Wisconsin-Madison xcai74@wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Sciences University of Wisconsin-Madison cylin@cs.wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Jelena Diakonikolas   \nDepartment of Computer Sciences   \nUniversity of Wisconsin-Madison jelena@cs.wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stochastic gradient descent (SGD) is perhaps the most prevalent optimization method in modern machine learning. Contrary to the empirical practice of sampling from the datasets without replacement and with (possible) reshuffling at each epoch, the theoretical counterpart of SGD usually relies on the assumption of sampling with replacement. It is only very recently that SGD using sampling without replacement - shuffed SGD - has been analyzed with matching upper and lower bounds. However, we observe that those bounds are too pessimistic to explain often superior empirical performance of data permutations (sampling without replacement) over vanilla counterparts (sampling with replacement) on machine learning problems. Through fine-grained analysis in the lens of primal-dual cyclic coordinate methods and the introduction of novel smoothness parameters, we present several results for shuffled SGD on smooth and non-smooth convex losses, where our novel analysis framework provides tighter convergence bounds over all popular shuffling schemes (IG, SO, and RR). Notably, our new bounds predict faster convergence than existing bounds in the literature - by up to a factor of $O({\\sqrt{n}})$ , mirroring benefits from tighter convergence bounds using component smoothness parameters in randomized coordinate methods. Lastly, we numerically demonstrate on common machine learning datasets that our bounds are indeed much tighter, thus offering a bridge between theory and practice. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Originally proposed in [38], SGD has been broadly studied in the machine learning literature due to its effectiveness in large-scale settings, where full gradient computations are often computationally prohibitive. When applied to unconstrained finite-sum problems ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{{\\pmb x}\\in\\mathbb{R}^{d}}f({\\pmb x}),\\;\\;\\mathrm{where}\\;\\;f({\\pmb x}):=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}({\\pmb x}),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "SGD performs the update $\\pmb{x}_{t}\\,=\\,\\pmb{x}_{t-1}\\,-\\,\\eta\\nabla f_{i_{t}}(\\pmb{x}_{t-1})$ for $i_{t}\\;\\in\\;[n]\\;([n]\\;:=\\;\\{1,\\dots,n\\})$ , in each iteration $t$ . Traditional theoretical analysis for SGD builds upon the assumption of sampling $i_{t}\\in[n]$ with replacement according to a fixed distribution $\\mathbf{p}\\,=\\,(p_{1},\\hdots,p_{n})^{\\top}$ over $[n]$ , which leads to $\\mathbb{E}_{i_{t}}[\\nabla f_{i_{t}}(\\mathbf{x}_{t-1})/(n p_{i_{t}})]=\\nabla f(\\mathbf{x}_{t-1})$ , and thus much of the (deterministic) gradient descent-style analysis can be transferred to this setting. By contrast, no such connection between the component and the full gradient can be established for shuffled SGD \u2014 which employs sampling without replacement \u2014 making its analysis much more challenging. As a result, despite its fundamental nature, there were no non-asymptotic convergence results for shuffed SGD until a very recent line of work [2, 12, 20, 21, 30, 31, 34, 35, 42]. All existing results consider general finite sum problems, with the same regularity condition constant (Lipschitz constant of $f_{i}$ or its gradient) assumed for all the component functions. As a result, the obtained convergence bounds are typically no better than for (full) gradient descent, and are only better than the bounds for SGD with replacement sampling if the algorithm is run for many full passes over the data [30, 34]. ", "page_idx": 0}, {"type": "image", "img_path": "qcPlGtzwW9/tmp/ef8dfa52ba5acb85804c00fbc7ff677088e972b4f04c5d8b247ae3ee9b9ef0ca.jpg", "img_caption": ["Figure 1: An illustration of the convergence behaviour of shuffled SGD for logistic regression problems on LIBSVM datasets luke, leu and $a9a$ , where we use step sizes from existing bounds and our work. Due to randomness, we average over 20 runs for each plot and include a ribbon around each line to show its variance. However, as suggested by the concentration of $\\hat{L}$ (see Section 4.1 and Appendix E), the variance across multiple runs is negligible, hence the ribbons are not observable. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Furthermore, there is a large gap between the empirical performance of shuffled SGD and the predicted convergence rates from prior work [20, 30]. One cause for this discrepancy are overly pessimistic bounds on the step size in prior work, which are of order $1/(n L_{\\mathrm{max}})$ ,where $L_{\\mathrm{max}}$ is the maximum smoothness constant over components $f_{i}$ in (P). In practice, the step sizes are tuned to achieve better convergence bounds than predicted by the current theory. We illustrate how restrictions on the step size affect convergence of shuffled SGD (with random permutations in each epoch) in Fig. 1, where we plot the resulting optimality gap over full data passes when shuffed SGD is applied to logistic regression problems on standard datasets. To compare the effect of the step size $\\eta$ from prior work and our work, we choose take $\\eta=1/(\\sqrt{2}n L_{\\mathrm{max}})$ based on [30], and $\\eta=1/(n\\sqrt{\\hat{L}\\tilde{L}})$ from our work, where $\\hat{L},\\tilde{L}$ are our novel fine-grained, data-dependent smoothness parameters defined in Section 3 for smooth convex finite-sum problems with linear predictors. As can be observed from Fig. 1, larger step sizes resulting from our theory lead to faster convergence of shuffled SGD and, as a result, our convergence bounds better predict the performance of shuffled SGD. ", "page_idx": 1}, {"type": "text", "text": "Building on these insights, we introduce a fined-grained theoretical analysis to transparently show how the structure of the data and the possibly different Lipschitz constants of the component functions or their gradients affect the performance of shuffed SGD, thus providing a better explanation of the heuristic success of shuffled SGD in modern machine learning. ", "page_idx": 1}, {"type": "text", "text": "1.1  Background and related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "SGD (with replacement) has been extensively studied in many settings (see e.g., [1, 9, 10, 38] for convex optimization). Compared to SGD, shuffled SGD usually exhibits faster convergence in practice [8, 37], and is easier and more efficient to implement [5]. For each epoch $k$ , shuffled SGDstyle algorithms perform incremental gradient updates based on the sample ordering (permutation of the data points) denoted by $\\pi^{(k)}$ . There are three main choices of data permutations: (i) $\\pi^{(k)}\\equiv\\pi$ for some fixed permutation of $[n]$ for all epochs, where shuffed SGD reduces to the incremental gradient (IG) method; (i) $\\pi^{(k)}\\equiv\\tilde{\\pi}$ where $\\tilde{\\pi}$ is randomly chosen only once, at the beginning of the first epoch, referred to as the shuffle-once (SO) scheme; (i) $\\pi^{(k)}$ randomly generated at the beginning of each epoch, referred to as random reshufling (RR). ", "page_idx": 1}, {"type": "text", "text": "For general smooth convex settings, the convergence of shuffled SGD has been established only recently. For the number of epochs $K$ sufficiently large, [31] proved a convergence rate ", "page_idx": 1}, {"type": "table", "img_path": "qcPlGtzwW9/tmp/eee20f6b8ea1a9c6e7d32d446dbcf53a1e2ef8eb9d30eda8ac23c03c13d8b8bd.jpg", "table_caption": ["Table 1: Comparison of our results with state of the art, in terms of individual gradient oracle complexity required to output $x_{\\mathrm{out}}$ with $\\mathbb{E}[f(\\pmb{x}_{\\mathrm{out}})-f(\\pmb{x}_{\\ast})]\\leq\\epsilon$ where $\\epsilon>0$ is the target error and $^{x\\ast}$ is the optimal solution. Here, $\\begin{array}{r}{\\sigma_{*}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla f_{i}(\\pmb{x}_{*})\\|_{2}^{2}}\\end{array}$ $D=\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}$ , and generalized linear model refers to objectives of the form $\\begin{array}{r}{f(\\pmb{x})\\,=\\,\\frac{1}{n}\\sum_{i=1}^{n}\\ell_{i}(\\pmb{a}_{i}^{\\top}\\pmb{x})}\\end{array}$ as defined in Section 3. Parameters $\\hat{L}^{g},\\tilde{L}^{g}$ are defined in Section 2 and satisfy $\\begin{array}{r}{\\hat{L}^{g}\\le\\frac{1}{n}\\sum_{i=1}^{n}L_{i}}\\end{array}$ and $\\tilde{L}^{g}\\leq L_{\\operatorname*{max}}$ Parameters $\\hat{L},\\tilde{L}$ , and $\\bar{G}$ are defined in Section 3, and are discussed in the text of this section. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "$O(1/\\sqrt{n K})$ for RR, which leads to the complexity matching SGD. This result was later improved to $\\mathcal{O}(1/(n^{1/3}K^{2/3}))$ by [12, 30, 34] for $K$ sufficiently large and with bounded variance assumed at the minimizer, while the same rate holds for SO [30]. These results were complemented by matching lower bounds in [12], under sufficiently small step sizes as utilized in prior work. The results in [30, 34] require restricted $\\mathcal{O}(1/(n L))$ step sizes and reduce to ${\\mathcal{O}}(1/K)$ for small $K$ , acquiring the same iteration complexity as full-gradient methods. Unlike in strongly convex settings, we are not aware of any follow-up work with improvements under small $K$ for smooth convex settings. ", "page_idx": 2}, {"type": "text", "text": "The major difficulty in analyzing shuffled SGD comes from characterizing the difference between the intermediate iterate and the iterate after one full data pass, for which current analysis (see e.g., [30] in smooth convex settings) uses the global smoothness constant with a triangle inequality. Such a bound may be too pessimistic and fail capturing the nuances of intermediate progress of shuffled SGD, which leads to a small step size and large $K$ restrictions. To provide a more fine-grained analysis that narrows the theory-practice gap for shuffled SGD, we notice that such a proof difficulty is reminiscent of the analysis of cyclic block coordinate methods relating the partial gradients to the full one. This natural connection was further emphasized in studies of cyclic methods with random permutations [24, 47]; however, these results were limited to convex quadratics. More generally, it is possible to interpret shuffled SGD as a primal-dual method performing cyclic updates on the dual side (see (PD) in Section 2.1 and (PL-PD) in Section 3). We note here that prior work on dual coordinate methods [41] provided theoretical guarantees only for the algorithms that choose the dual coordinate to optimize uniformly at random, while the cyclic variant (related to shuffled SGD) had only been studied numerically up until this work. Further discussion of related work appears in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "1.2 Contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we study the convergence rates of shuffled SGD in various settings through a unified primal-dual perspective, making intriguing connections to cyclic coordinate methods. This analysis framework is novel and allows us to leverage cyclic bias accumulation techniques on the dual side to obtain fine-grained convergence bounds. The obtained bounds mirror the improvements in randomized coordinate methods, which come from different coordinate smoothness parameters. While coordinate methods are no better than full-gradient methods in the worst case, on typical problem instances, they are much faster and the improvements come precisely from a more finegrained view of smoothness. We see a similar phenomenon in our analysis, which highlights the usefulness of the fine-grained smoothness characterizations introduced in our work. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We provide improved bounds for all three popular data permutation strategies RR, SO and IG, in smooth convex settings. When the problem objective narrows to empirical risk minimization with linear predictors, we are able to exploit the data-dependent structure and uncouple the linear and nonlinear parts of the objective function, allowing us to provide tighter data-dependent bounds, up to a factor of $O({\\sqrt{n}})$ . Moreover, we show that our techniques extend to non-smooth convex settings, providing improved bounds over existing work. ", "page_idx": 3}, {"type": "text", "text": "We summarize our results and compare them to the state of the art in Table 1. As is standard, all complexity results in Table 1 are expressed in terms of individual (component) gradient evaluations. They represent the number of gradient evaluations required to construct a solution with (expected) optimalitygap $\\epsilon$ given a target error $\\epsilon>0$ ", "page_idx": 3}, {"type": "text", "text": "Extensions to mini-batching and IG.  When presenting our results for general finite-sum problems (in Section 2), we consider simple updates without mini-batching for ease of presentation and to avoid introducing excessive notation. However, we emphasize that all our results can be extended to shuffled SGD with mini-batching. Our results are also the first to provide convergence bounds that demonstrate benefits of mini-batching in shuffled SGD. For completeness and generality, the proofs in the appendix are carried out for mini-batch settings with arbitrary batch sizes $b\\in\\{1,\\bar{\\cdot}\\cdot\\cdot,n\\}$ Thus, all the results stated in Section 2 can be recovered by setting $b=1$ . Moreover, our framework can provide similar fine-grained convergence bounds for IG. However, as IG is not as commonly used in practice compared to RR and SO and due to space constraints, we only present our results for RR and SO in the main body and include the results for IG in the appendix. ", "page_idx": 3}, {"type": "text", "text": "1.3Notation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider a real $d$ -dimensional Euclidean space $(\\mathbb{R}^{d},\\|\\cdot\\|)$ where $d$ is finite and $\\|\\cdot\\|$ is the $\\ell_{2}$ -norm. For a vector $\\textbf{\\em x}$ we let $\\mathbf{\\boldsymbol{x}}^{j}$ denote its $j$ -th coordinate. For any positive integer $m$ , we use $[m]$ to denote the set $\\{1,2,\\ldots,m\\}$ . Given a matrix $\\pmb{A}$ \uff0c $\\|A\\|:=\\operatorname*{sup}_{\\pmb{x}\\in\\mathbb{R}^{d},\\|\\pmb{x}\\|\\leq1}\\|\\pmb{A}\\pmb{x}\\|$ denotes its operator norm. For a positive definite matrix $\\pmb{\\Lambda}$ \uff0c $\\Vert\\cdot\\Vert_{\\Lambda}$ denotes the Mahalanobis norm, $\\|x\\|_{\\Lambda}:=\\sqrt{\\langle\\Lambda x,x\\rangle}$ We use $\\boldsymbol{\\mathit{I}}$ to denote the identity matrix, and $\\mathrm{diag}(v)$ to denote the diagonal matrix with vector $\\pmb{v}$ on the main diagonal. For any $j~\\in~[n]$ , we define $I_{j\\uparrow}$ as the matrix obtained from the identity matrix $\\boldsymbol{\\mathit{I}}$ by setting the first $j$ diagonal elements to zero, and let $I_{j}$ be the matrix with only the $j$ -th diagonal element nonzero and equal to 1. To handle the cases with random data permutations, we use the following definitions corresponding to the data permution $\\pi=\\{\\pi^{1},\\pi^{\\dot{2}},\\ldots,\\pi^{n}\\}$ $[n]$ $A_{\\pi}:=\\left[\\pmb{a}_{\\pi_{1}},\\pmb{a}_{\\pi_{2}},\\dots,\\pmb{a}_{\\pi_{n}}\\right]^{\\top}$ permuting the rows based on $\\pi$ given a matrix $\\pmb{A}=[\\pmb{a}_{1},\\pmb{a}_{2},\\dots,\\pmb{a}_{n}]^{\\top}$ \uff0c and $\\pmb{v}_{\\pi}:=\\left(\\pmb{v}^{\\pi_{1}},\\pmb{v}^{\\pi_{2}},\\dots,\\pmb{v}^{\\pi_{n}}\\right)^{\\top}$ permuting the coordinates/subvectors based on $\\pi$ given a vector $\\pmb{v}=(\\pmb{v}^{1},\\pmb{v}^{2},\\dots,\\pmb{v}^{n})^{\\top}$ ", "page_idx": 3}, {"type": "text", "text": "2 Primal-Dual Framework for Smooth Convex Finite-Sum Problems ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Throughout this section, we make the following standard assumptions. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. Each $f_{i}$ is convex and $L_{i}$ -smooth, and there exists a minimizer $\\mathbf{\\boldsymbol{x}}_{\\ast}\\in\\mathbb{R}^{d}$ for $f({\\boldsymbol{x}})$ ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 implies that $f$ and all component functions $f_{i}$ are $L$ -smooth, where $L_{\\mathrm{max}}~:=$   \n$\\operatorname*{max}_{i\\in[n]}L_{i}$ It als implies that each convex conjugate $f_{i}^{*}$ $\\frac{1}{L_{i}}$ -strongly convex [3]. In this   \nsection, we define $\\mathbf{A}=\\operatorname{diag}(L_{1},\\ldots,L_{1},\\ldots,L_{n},\\ldots,L_{n})\\in\\mathbb{R}^{n d\\times n d}$ and slightly abuse the no$\\underbrace{\\phantom{\\left(i\\alpha\\mathcal{U}-R e^{-1}D\\right)}}_{d}$   \ntation to use $\\mathbf{A}_{\\pi}=\\mathrm{diag}\\big(\\,L_{\\pi^{1}},\\dots,L_{\\pi^{1}},\\dots,L_{\\pi^{n}},\\dots,L_{\\pi^{n}}\\,\\big)$ given a permutation $\\pi$ of $[n]$ . For the $\\delta\\lfloor\\underbrace{\\alpha\\pi^{1},\\ldots,\\alpha\\pi^{1}}_{d},\\ldots,\\underbrace{\\alpha\\pi^{n},\\ldots,\\alpha\\pi^{n}}_{d}\\big\\rangle$   \npermutation $\\pi_{k}$ at the $k$ -th epoch, we denote ${\\cal\\Lambda}_{k}={\\cal\\Lambda}_{\\pi_{k}}$ , for brevity.   \nWe further assume that the variance at $\\pmb{x}_{*}$ is bounded, same as prior work [30, 34]. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. The quantity $\\begin{array}{r}{\\sigma_{*}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla f_{i}(\\pmb{x}_{*})\\|^{2}}\\end{array}$ is bounded. ", "page_idx": 3}, {"type": "text", "text": "1: Input: Initial point $\\pmb{x}_{0}\\in\\mathbb{R}^{d}$ , step size $\\{\\eta_{k}\\}>0$ , number of epochs $K>0$   \n2: for $k=1$ to $K$ do   \n3:  Generate some permutation $\\pi_{k}$ of $[n]$ (either deterministic or random)   \n4: k-1,1 = k-1   \n5:for $i=1$ to $n$ in the ordering of $\\pi_{k}$ do   \n6: $\\begin{array}{r l}&{y_{k}^{i}=\\arg\\operatorname*{max}_{y^{i}\\in\\mathbb{R}^{d}}\\left\\{\\langle\\tilde{y_{i}^{i}},\\boldsymbol{x}_{k-1,i}\\rangle-f_{i}^{*}(y^{i})\\right\\}}\\\\ &{x_{k-1,i+1}=\\arg\\operatorname*{min}_{\\boldsymbol{x}\\in\\mathbb{R}^{d}}\\left\\{\\langle y_{k}^{i},\\boldsymbol{x}\\rangle+\\frac{1}{2\\eta_{k}}\\lVert\\boldsymbol{x}-\\boldsymbol{x}_{k-1,i}\\rVert^{2}\\right\\}=x_{k-1,i}-\\eta_{k}\\nabla f(\\boldsymbol{x}_{k-1,i})}\\end{array}$   \n7:   \n8: end for   \n9: k = Ck-1,n+1, $\\pmb{y}_{k}=\\left(\\pmb{y}_{k}^{1},\\pmb{y}_{k}^{2},\\ldots,\\pmb{y}_{k}^{n}\\right)^{\\top}$   \n10: ed for $\\begin{array}{r}{\\hat{\\pmb{x}}_{K}=\\sum_{k=1}^{K}{\\eta_{k}\\pmb{x}_{k}}/\\sum_{k=1}^{K}{\\eta_{k}}}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "2.1 Primal-dual view of shuffled SGD ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Problem (P) can be reformulated into a primal-dual form using the standard Fenchel conjugacy argument (see, e.g., [13, 14]), ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\boldsymbol{x}\\in\\mathbb{R}^{d}}\\operatorname*{max}_{\\boldsymbol{y}\\in\\mathbb{R}^{n d}}\\Big\\{\\mathcal{L}(\\boldsymbol{x},\\boldsymbol{y}):=\\frac{1}{n}\\sum_{i=1}^{n}\\Big(\\left\\langle\\pmb{\\mathscr{y}}^{i},\\pmb{\\mathscr{x}}\\right\\rangle-f_{i}^{*}(\\pmb{\\mathscr{y}}^{i})\\Big)\\Big\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we slightly abuse the notation to denote $\\pmb{y}\\,=\\,(\\pmb{y}^{1},\\dots,\\pmb{y}^{n})^{\\top}\\,\\in\\,\\mathbb{R}^{n d}$ and $f_{i}^{*}$ is the convex conjugate of $f_{i}$ defined by $\\begin{array}{r}{f_{i}^{*}(\\pmb{y})=\\operatorname*{sup}_{\\pmb{x}\\in\\mathbb{R}^{d}}\\left\\{\\,\\langle\\pmb{y},\\pmb{x}\\rangle-f_{i}(\\pmb{x})\\right\\}}\\end{array}$ We let $\\pmb{y}_{\\pmb{x}}=(\\pmb{y}_{\\pmb{x}}^{1},\\dots,\\pmb{y}_{\\pmb{x}}^{n})^{\\top}\\in\\mathbb{R}^{n d}$ be the conjugate pair of $\\pmb{x}\\in\\mathbb{R}^{d}$ i.e., $\\pmb{y}_{\\pmb{x}}^{i}=\\arg\\operatorname*{max}_{\\pmb{y}\\in\\mathbb{R}^{d}}\\{\\langle\\pmb{y},\\pmb{x}\\rangle-f_{i}^{*}(\\pmb{y})\\}$ , and we denote $\\mathbf{y}_{\\ast}=\\pmb{y}_{\\pmb{x}_{\\ast}}$ ", "page_idx": 4}, {"type": "text", "text": "Given a primal-dual pair $\\left({\\pmb x},{\\pmb y}\\right)$ , the primal-dual gap of (PD) is defined by $\\begin{array}{r l}{\\mathrm{\\bfGap}(x,y)}&{{}=}\\end{array}$ $\\operatorname*{max}_{(\\boldsymbol{u},\\boldsymbol{v})}\\{\\mathcal{L}(\\boldsymbol{x},\\boldsymbol{v})-\\mathcal{L}(\\boldsymbol{u},\\boldsymbol{y})\\}$ . In particular, we consider the pair $\\left(x,y_{\\ast}\\right)$ for $\\pmb{x}\\in\\mathbb{R}^{d}$ , and bound $\\mathrm{Gap}^{v}(x,y_{\\ast})\\,:=\\,\\mathcal{L}(\\pmb{x},v)-\\mathcal{L}(\\pmb{x}_{\\ast},\\pmb{y}_{\\ast})$ for an arbitrary but fixed $\\pmb{v}$ . To finally obtain the function value gap $f(\\pmb{x})-f(\\pmb{x}_{*})$ for $({\\bf P})$ , we only need to choose $\\pmb{v}=\\arg\\operatorname*{max}_{\\pmb{w}}\\mathcal{L}(\\pmb{x},\\pmb{w})=\\pmb{y}_{x}$ ", "page_idx": 4}, {"type": "text", "text": "Using this primal-dual formulation and standard convex conjugacy arguments, we can equivalently write the standard shuffed SGD algorithm in a primal-dual form as summarized in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Improved bounds with new smoothness constants.  To prove a convergence bound for shuffled SGD in this general setting, we first construct an upper estimate of $\\mathrm{Gap}^{v}(x_{k},y_{*})$ for some fixed $\\pmb{v}$ to be set later, as summarized in the following lemma. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1. Under Assumtion $^{\\,I}$ for any $k\\in[K],$ the iterates $\\{y_{k}^{i}\\}_{i=1}^{n}$ and $\\{{\\pmb x}_{k-1,i}\\}_{i=1}^{n+1}$ generated by Algorithm $^{\\,I}$ satisfy ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathcal E_{k}\\leq\\frac{\\eta_{k}}{n}\\sum_{i=1}^{n}\\big\\langle\\boldsymbol y_{k}^{i},\\boldsymbol x_{k}-\\boldsymbol x_{k-1,i+1}\\big\\rangle+\\frac{\\eta_{k}}{n}\\sum_{i=1}^{n}\\big\\langle\\boldsymbol v_{k}^{i}-\\boldsymbol y_{k}^{i},\\boldsymbol x_{k}-\\boldsymbol x_{k-1,i}\\big\\rangle}}\\\\ {{\\displaystyle\\qquad-\\,\\frac{\\eta_{k}}{2n}\\|\\boldsymbol y_{k}-\\boldsymbol v_{k}\\|_{\\boldsymbol\\Lambda_{k}^{-1}}^{2}-\\frac{\\eta_{k}}{2n}\\|\\boldsymbol y_{k}-\\boldsymbol y_{*,k}\\|_{\\boldsymbol\\Lambda_{k}^{-1}}^{2}-\\frac{1}{2n}\\sum_{i=1}^{n}\\|\\boldsymbol x_{k-1,i+1}-\\boldsymbol x_{k-1,i}\\|^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{E}_{k}:=\\eta_{k}\\mathrm{Gap}^{\\upsilon}(\\pmb{x}_{k},\\pmb{y}_{*})+\\frac{1}{2n}\\|\\pmb{x}_{*}-\\pmb{x}_{k}\\|_{2}^{2}-\\frac{1}{2n}\\|\\pmb{x}_{*}-\\pmb{x}_{k-1}\\|_{2}^{2},}\\end{array}$ ${\\pmb v}_{k}={\\pmb v}_{\\pi^{(k)}}$ and ${\\pmb y}_{\\ast,k}={\\pmb y}_{\\ast,\\pi^{(k)}}$ are the (block-wise) permuted vectors based on the permutation $\\pi_{k}$ at the $k$ -th epoch. ", "page_idx": 4}, {"type": "text", "text": "We note that the frsterm $\\begin{array}{r}{\\mathcal{T}_{1}:=\\frac{\\eta_{k}}{n}\\sum_{i=1}^{n}\\left\\langle\\pmb{y}_{k}^{i},\\pmb{x}_{k}-\\pmb{x}_{k-1,i+1}\\right\\rangle}\\end{array}$ from Lemma1 can be aggregated into the terms capturing the primal progress within one epoch and cancelled by the last term in Eq. (1). The precise bound on $\\mathcal{T}_{1}$ and its proof are provided in Lemma 10 in Appendix C.1. The second term $\\begin{array}{r}{\\mathcal{T}_{2}:=\\frac{\\eta_{k}}{n}\\sum_{i=1}^{n}\\left\\langle\\pmb{v}_{k}^{i}-\\pmb{y}_{k}^{i},\\pmb{x}_{k}-\\pmb{\\bar{x}}_{k-1,i}\\right\\rangle}\\end{array}$ requires us to relate the intermediate iterate $\\mathbf{\\Delta}x_{k-1,i}$ to the iterate $\\pmb{x}_{k}$ after one full data pass, which corresponds to a partial sum of the component gradients, each at different iterates {k-1,j'= . In contrast to prior analyses (e.g., Mishchenko et al. [30]) using the global smoothness and triangle inequality to bound this partial sum, we provide a tighter bound on $\\mathcal{T}_{2}$ that tracks the progress of the cyclic update on the dual side, in the aggregate. ", "page_idx": 4}, {"type": "text", "text": "To simplify the notation in the following lemmas and to clearly compare our results, we introduce the following novel definitions of smoothness constants for shuffed SGD: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\hat{L}_{\\pi}^{g}:=\\frac{1}{n^{2}}\\big\\|\\boldsymbol{\\Lambda}_{\\pi}^{1/2}\\big(\\sum_{i=1}^{n}I_{d(i-1)\\uparrow}\\boldsymbol{E}\\boldsymbol{E}^{\\top}I_{d(i-1)\\uparrow}\\big)\\boldsymbol{\\Lambda}_{\\pi}^{1/2}\\big\\|_{2},}&&{\\hat{L}^{g}=\\underset{\\pi}{\\operatorname*{max}}\\hat{L}_{\\pi}^{g},}\\\\ &{\\tilde{L}_{\\pi}^{g}:=\\big\\|\\boldsymbol{\\Lambda}_{\\pi}^{1/2}\\big(\\sum_{i=1}^{n}I_{(d i)}\\boldsymbol{E}\\boldsymbol{E}^{\\top}I_{(d i)}\\big)\\boldsymbol{\\Lambda}_{\\pi}^{1/2}\\big\\|_{2},}&&{\\tilde{L}^{g}=\\underset{\\pi}{\\operatorname*{max}}\\tilde{L}_{\\pi}^{g},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{I_{(d i)}=\\sum_{j=d(i-1)+1}^{d i}I_{j}}\\end{array}$ and $\\pmb{E}=[\\underbrace{I_{d},\\dots,I_{d}}_{n}]^{\\top}\\in\\mathbb{R}^{n d\\times d}$ Permutation-ependent quantites $\\hat{L}_{\\pi}^{g}$ and ${\\tilde{L}}_{\\pi}^{g}$ defined in (2) are obtained directly from our analysis. We remark that $\\hat{L}^{g}$ is bounded by the average smoothness of $f$ and $\\tilde{L}^{g}$ is bounded by the max of individual smoothness constants of $f_{i}$ .\uff0c see more details in Appendix B. However, as we argue in later sections, these upper bounds on ${\\hat{L}}_{\\pi}^{g}$ and ${\\tilde{L}}_{\\pi}^{g}$ are loose in general, and so the convergence bounds based on $\\hat{L}_{\\pi}^{g}$ and ${\\tilde{L}}_{\\pi}^{g}$ that we obtain align better with the empirical performance of shuffled SGD. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Assuming that a uniformly random data shuffing strategy is used (SO or RR), the resulting bound on $\\mathcal{T}_{2}$ is summarized in Lemma 2, while its proof is deferred to Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Lemma2. Under Assumptions $^{\\,l}$ and2,forany $k\\,\\in\\,[K]$ theiterates $\\{{\\pmb y}_{k}^{i}\\}_{i=1}^{n}$ and $\\{{\\pmb x}_{k-1,i}\\}_{i=1}^{n+1}$ generated by Algorithm with uniformly random shufing (RR/SO) satisfy ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathcal{T}_{2}]\\leq\\mathbb{E}\\Big[\\eta_{k}^{3}n\\hat{L}_{\\pi^{(k)}}^{g}\\tilde{L}_{\\pi^{(k)}}^{g}\\|y_{k}-y_{*,k}\\|_{\\Lambda_{k}^{-1}}^{2}+\\frac{\\eta_{k}}{2n}\\|v_{k}-y_{k}\\|_{\\Lambda_{k}^{-1}}^{2}\\Big]+\\frac{\\eta_{k}^{3}(n+1)\\tilde{L}^{g}}{6}\\sigma_{*}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{T}_{2}:=\\frac{\\eta_{k}}{n}\\sum_{i=1}^{n}\\left\\langle v_{k}^{i}-y_{k}^{i},x_{k}-x_{k-1,i}\\right\\rangle}\\end{array}$ $\\pmb{v}_{k}=\\pmb{v}_{\\pi^{(k)}}$ and ${\\pmb y}_{\\ast,k}={\\pmb y}_{\\ast,\\pi^{(k)}}$ ", "page_idx": 5}, {"type": "text", "text": "With Lemmas 1 and 2 in tow, we are ready to present the main result of this section. ", "page_idx": 5}, {"type": "text", "text": "Theorem1.UnderAssumptions $^{\\,l}$ and 2, ij $\\begin{array}{r}{{^c\\eta_{k}}\\leq\\frac{1}{n\\sqrt{2\\hat{L}_{\\pi^{(k)}}^{g}\\tilde{L}_{\\pi^{(k)}}^{g}}}}\\end{array}$ and $\\begin{array}{r}{H_{K}=\\sum_{k=1}^{K}\\eta_{k}}\\end{array}$ the output $\\hat{\\pmb{x}}_{K}$ of Algorithm $^{\\,I}$ withuniformly random(RR/so) shuffingsatisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[H_{K}(f(\\hat{{\\boldsymbol{x}}}_{K})-f({\\boldsymbol{x}}_{*}))]\\leq\\frac{1}{2n}\\|{\\boldsymbol{x}}_{0}-{\\boldsymbol{x}}_{*}\\|_{2}^{2}+\\sum_{k=1}^{K}\\frac{\\eta_{k}^{3}(n+1)\\tilde{L}^{g}}{6}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "As a consequence, for any $\\epsilon>0$ , there exists a choice of a constant step size $\\eta_{k}=\\eta$ for which E[f(ak)- f(m)\u2264eaferO(mVLo\u221el2 + $\\begin{array}{r}{\\mathcal{O}\\Big(\\frac{n\\sqrt{\\hat{L}^{g}\\tilde{L}^{g}}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{\\epsilon}+\\frac{\\sqrt{n\\tilde{L}^{g}}\\sigma_{*}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{\\epsilon^{3/2}}\\Big)}\\end{array}$ Vniaol ) individual gradient queries. ", "page_idx": 5}, {"type": "text", "text": "3  Tighter Bounds for Convex Finite-Sum Problems with Linear Predictors ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To study the effect of the structure of the data on the convergence of shuffled SGD, we sharpen the focus from general finite-sum problems to convex finite-sum with linear predictors: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{x}\\in\\mathbb{R}^{d}}\\Big\\{f(\\pmb{x}):=\\frac{1}{n}\\sum_{i=1}^{n}\\ell_{i}(\\pmb{a}_{i}^{\\top}\\pmb{x})\\Big\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\pmb{a}_{i}\\in\\mathbb{R}^{d}\\,(i\\in[n])$ are data vectors and $\\ell_{i}:\\mathbb{R}\\rightarrow\\mathbb{R}$ are convex and either smooth or Lipschitz nonsmooth functions associated with the linear predictors $\\langle a_{i},x\\rangle$ for $i\\in[n]$ . In addition to their explicit dependence on the data, it is worth noting that problems of the form (PL) cover most of the standard convex ERM problems where shuffled SGD is commonly applied, such as support vector machines, least absolute deviation, least squares, and logistic regression. ", "page_idx": 5}, {"type": "text", "text": "Problem (PL) admits an explicit primal-dual formulation using the standard Fenchel conjugacy argument (see, e.g., [13, 14]), ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\operatorname*{max}_{y\\in\\mathbb{R}^{n}}\\Big\\{\\mathcal{L}(x,y):=\\frac{1}{n}\\,\\langle A x,y\\rangle-\\frac{1}{n}\\sum_{i=1}^{n}\\ell_{i}^{*}(y^{i})=\\frac{1}{n}\\sum_{i=1}^{n}\\big(a_{i}^{\\top}x y^{i}-\\ell_{i}^{*}(y^{i})\\big)\\Big\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $A=[\\pmb{a}_{1},\\pmb{a}_{2},\\dots,\\pmb{a}_{n}]^{\\top}\\in\\mathbb{R}^{n\\times d}$ is the data matrix and $\\ell_{i}^{*}:\\mathbb{R}\\rightarrow\\mathbb{R}$ is the convex conjugate Oof $\\ell_{i}$ . This observation allows us to again interpret without-replacement SGD updates as cyclic coordinate updates on the dual side. Note that due to the objective structure in (PL), the primal-dual formulation (PL-PD) can decouple the linear $(a_{i}^{\\top}x)$ and the non-linear $(\\ell_{i})$ parts within individual loss functions $f_{i}$ . We redefine the conjugate pair of $\\pmb{x}\\in\\mathbb{R}^{d}$ to be $\\pmb{y}_{x}=(\\pmb{y}_{x}^{1},\\dots,\\pmb{y}_{x}^{n})^{\\top}\\in\\mathbb{R}^{n}$ , with $\\begin{array}{r}{\\pmb{y}_{\\pmb{x}}^{i}=\\arg\\operatorname*{max}_{\\pmb{y}^{i}\\in\\mathbb{R}}\\{\\pmb{y}^{i}\\pmb{a}_{i}^{\\top}\\pmb{x}-\\ell_{i}^{*}(\\pmb{y}^{i})\\}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "In this section, we consider shuffled SGD with mini-batch estimators of size $b$ and assumewithout loss of generality that $n=b m$ for some positive integer $m$ . The detailed primal-dual view of shuffled SGD adapted to (PL-PD) and mini-batch estimators is provided in Alg. 2 in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "3.1  Smooth and convex objectives ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Throughout this subsection, we make the following (standard) assumptions, corresponding to Assumptions 1 and 2 from Section 2. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3. Each $\\ell_{i}$ is convex and $L_{i}$ -smooth $(i\\in[n])$ ,i.e., $|\\ell_{i}^{\\prime}(x)-\\ell_{i}^{\\prime}(y)|\\leq L_{i}|x-y|$ for any $x,y\\in\\mathbb{R}$ There exists a minimizer $\\pmb{x}_{*}\\in\\arg\\operatorname*{min}_{\\pmb{x}\\in\\mathbb{R}^{d}}f(\\pmb{x})$ ", "page_idx": 6}, {"type": "text", "text": "We remark that Assumption 3 implies that both $f$ and each component function $f_{i}(\\pmb{x})=\\ell_{i}(\\pmb{a}_{i}^{\\top}\\pmb{x})$ are $L_{\\mathrm{max}}$ -smooth, where $L_{\\mathrm{max}}=\\operatorname*{max}_{i\\in[n]}L_{i}\\Vert\\mathbf{a}_{i}\\Vert_{2}^{2}$ . Assumption 3 also implies that each convex conjugate $\\ell_{i}^{*}$ .s $\\frac{1}{L_{i}}$ -strongly convex [3]. In the following, we let $\\mathbf{A}=\\operatorname{diag}(L_{1},L_{2},\\ldots,L_{n})$ and $\\mathbf{A}_{\\pi}=\\operatorname{diag}\\bigl(L_{\\pi^{1}},L_{\\pi^{2}},...\\,,L_{\\pi^{n}}\\bigr)$ , given a permutation $\\pi$ of $[n]$ ", "page_idx": 6}, {"type": "text", "text": "We further assume bounded variance at $\\pmb{x}_{*}$ , same as prior work [30, 34, 45, 46]. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4. $\\begin{array}{r}{\\sigma_{*}^{2}:=\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla f_{i}(\\pmb{x}_{*})\\|^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(\\ell_{i}^{\\prime}(\\pmb{a}_{i}^{\\top}\\pmb{x}_{*}))^{2}\\|\\pmb{a}_{i}\\|_{2}^{2}}\\end{array}$ is bounded. ", "page_idx": 6}, {"type": "text", "text": "Improved bounds with new smoothness constants. Our convergence bounds depend on the smoothness parameters defined in Eq. (3) below. We provide a detailed discussion on how these parameters relate to traditional smoothness parameters both in the worst case and on typical datasets, in Section 4.1, with additional numerical results provided in Appendix E. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\hat{L}_{\\pi}:=\\displaystyle\\frac{1}{m n}\\big\\|\\mathbf{A}_{\\pi}^{1/2}\\big(\\sum_{j=1}^{m}I_{b(j-1)\\uparrow}A_{\\pi}A_{\\pi}^{\\top}I_{b(j-1)\\uparrow}\\big)\\mathbf{A}_{\\pi}^{1/2}\\big\\|_{2},\\quad}}&{{\\hat{L}=\\displaystyle\\operatorname*{max}_{\\pi}\\hat{L}_{\\pi},}}\\\\ {{\\tilde{L}_{\\pi}:=\\displaystyle\\frac{1}{b}\\big\\|\\mathbf{A}_{\\pi}^{1/2}\\big(\\sum_{j=1}^{m}I_{(j)}A_{\\pi}A_{\\pi}^{\\top}I_{(j)}\\big)\\mathbf{A}_{\\pi}^{1/2}\\big\\|_{2},\\quad}}&{{\\tilde{L}=\\displaystyle\\operatorname*{max}_{\\pi}\\tilde{L}_{\\pi},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{I_{(j)}:=\\sum_{i=b(j-1)+1}^{b j}I_{i}}\\end{array}$ . In comparison to the smoothnes constants defned in Eqg (2) for general finite-sum problems, we note that the constants in Eq. (3) applying to generalized linear models are tighter and more informative estimates, as the data matrix $\\pmb{A}$ and the smoothness constants from the nonlinear part $\\pmb{\\Lambda}$ are separated in Eq. (3). Thus, the constants $\\hat{L}_{\\pi}$ and ${\\tilde{L}}_{\\pi}$ directly depend on the data matrix, which explicitly demonstrates how the structure of the data affects the convergence of shuffled SGD. The following theorem states the convergence of Algorithm 2 with these new refined smoothness constants, while its proof is provided in Appendix $\\mathbf{C}$ ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Under Assumptions 3 and 4, if $\\begin{array}{r}{\\eta_{k}\\,\\leq\\,\\frac{b}{n\\sqrt{2\\hat{L}_{\\pi^{(k)}}\\tilde{L}_{\\pi^{(k)}}}}}\\end{array}$ and $\\begin{array}{r}{H_{K}=\\sum_{k=1}^{K}\\eta_{k}}\\end{array}$ thenthe output $\\hat{\\pmb{x}}_{K}$ of Alg. 1 with uniformly random (RR/SO) shuffing satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}[H_{K}(f(\\hat{x}_{K})-f(x_{*}))]\\leq\\frac{b}{2n}\\|x_{0}-x_{*}\\|_{2}^{2}+\\sum_{k=1}^{K}\\frac{\\eta_{k}^{3}\\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "As a result, given $\\epsilon>0$ , there exists a constant step size $\\eta_{k}=\\eta$ suchthat $\\mathbb{E}[f({\\hat{\\mathbf{x}}}_{K})-f(\\mathbf{x}_{*})]\\leq\\epsilon$ afer $\\begin{array}{r}{\\mathcal{O}\\big(\\frac{n\\sqrt{\\hat{L}\\tilde{L}}\\|x_{0}-x_{*}\\|_{2}^{2}}{\\epsilon}+\\sqrt{\\frac{(n-b)(n+b)}{n(n-1)}}\\frac{\\sqrt{n\\tilde{L}}\\sigma_{*}\\|x_{0}-x_{*}\\|_{2}^{2}}{\\epsilon^{3/2}}\\big)}\\end{array}$ (mb(n+b) /ni\u221e2) individual gradient queries. ", "page_idx": 6}, {"type": "text", "text": "A few remarks are in order here. When $b=n$ , we recover the standard guarantee of gradient descent, which serves as a sanity check as in this case the algorithm reduces to standard gradient descent. When $\\begin{array}{r}{\\epsilon=\\Omega(\\frac{(n-b)(n+b)\\sigma_{*}^{2}}{n^{2}(n-1)\\hat{L}})}\\end{array}$ th resuling complexit is $\\mathcal{O}\\big(\\frac{n\\sqrt{\\hat{L}\\tilde{L}}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{\\epsilon}\\big)$ Observe that thi ase can happen when either $\\epsilon$ is large (compared to, say, $1/n)$ or when $\\sigma_{*}$ is small (it is, in fact, possible for $\\sigma_{*}$ to be zero, which happens, for example, when the data rows are linearly independent). Unlike in bounds from previous work, we observe from our bounds the benefit of using shuffled SGD compared to full gradient descent, where the difference is by a factor that can be as large as $\\sqrt{n}$ ,aswe have discused in the inroduction se also Setio ). When = (m-b(o+b)g-) , the second term in our complexity bound dominates. In this case, when $b=1$ , we recover the state of the art results from [12, 30, 34], while for $b>1$ our bound provides the V/()(-n+b )-fator improvement, providing insights into benefits from the mini-batching strategy commonly used in practice. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "3.2  Extension to non-smooth convex objectives ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In non-smooth settings, we make the following standard assumption. ", "page_idx": 7}, {"type": "text", "text": "Assumption5.Each $\\ell_{i}$ isconvexand $G_{i}$ -Lipschitz. $(i\\,\\in\\,[n])$ , i.e., $|\\ell_{i}(x)-\\ell_{i}(y)|\\,\\leq\\,G_{i}|x-y|$ for any $x,y\\ \\in\\ \\mathbb{R}$ : thus $|g_{i}(x)|\\,\\leq\\,G_{i}$ where $g_{i}(x)\\ \\in\\ \\partial\\ell_{i}(x)$ .Thereexistsaminimizer $\\mathbf{\\mathcal{x}}_{*}\\in$ arg $\\scriptstyle\\operatorname*{min}_{\\substack{{\\pmb x}\\in\\mathbb{R}^{d}}}f({\\pmb x})$ ", "page_idx": 7}, {"type": "text", "text": "If Assumption 5 holds, each $\\ell_{i}(\\pmb{a}_{i}^{\\top}\\pmb{x})$ is also $G_{\\mathrm{max}}$ -Lipschitz with respect to $\\textbf{\\em x}$ ,where $G_{\\mathrm{max}}\\,=$ $\\operatorname*{max}_{i\\in[n]}G_{i}\\|\\mathbf{a}_{i}\\|_{2}$ . To state our results, we define $\\mathbf{\\dot{T}}\\ :=\\ \\operatorname{diag}({G_{1}^{2}}^{\\cdot}G_{2}^{2},\\dots,G_{n}^{2})$ and ${\\mathbf{\\nabla}}\\Gamma_{\\pi}\\;\\;=\\;\\;$ $\\mathrm{diag}\\big(G_{\\pi_{1}}^{2},G_{\\pi_{2}}^{2},\\ldots,G_{\\pi_{n}}^{2}\\big)$ given data permutation $\\pi$ f $[n]$ ", "page_idx": 7}, {"type": "text", "text": "We now extend our analysis of Algorithm 1 to convex nonsmooth Lipschitz settings, where the conjugatefunctions $\\ell_{i}^{*}(y^{i})$ are only convex. Proceeding as in Lemma 1, we obtain a bound on the primal-dual gap similar to (1), but lose two retraction terms induced by smoothness. Instead of cancelling the corresponding error terms like in the smooth case, we rely on the boundedness of the subgradients to bound these terms under a sufficiently small step size, which is common in nonsmooth Lipschitz settings. Similar to Section 2, we introduce the following quantities to obtain a tighter guarantee with respect to the data matrix and Lipschitz constants ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{G}_{\\pi}:=\\frac{1}{m n}\\big\\|\\mathbf{r}_{\\pi}^{1/2}\\big(\\sum_{j=1}^{m}I_{b(j-1)\\uparrow}A_{\\pi}A_{\\pi}^{\\top}I_{b(j-1)\\uparrow}\\big)\\mathbf{r}_{\\pi}^{1/2}\\big\\|_{2},}\\\\ &{\\tilde{G}_{\\pi}:=\\frac{1}{b}\\big\\|\\mathbf{r}_{\\pi}^{1/2}\\big(\\sum_{j=1}^{m}I_{(j)}A_{\\pi}A_{\\pi}^{\\top}I_{(j)}\\big)\\mathbf{r}_{\\pi}^{1/2}\\big\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We discuss the improvements in convergence from $\\hat{G}_{\\pi}$ and $\\tilde{G}_{\\pi}$ in Section 4, while the convergence of Algorithm 2 is described in Theorem 3, with its proof deferred to Appendix D. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3. Under Assumption 5, if $\\begin{array}{r}{H_{K}=\\sum_{k=1}^{K}\\eta_{k}}\\end{array}$ and $\\bar{G}=\\mathbb{E}_{\\pi}[\\sqrt{\\hat{G}_{\\pi}\\tilde{G}_{\\pi}}],$ theoutput $\\hat{\\pmb{x}}_{K}$ of Alg.1 with possible uniformly random shuffing satisfies ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}[H_{K}(f(\\hat{{\\pmb x}}_{K})-f({\\pmb x}_{*}))]\\leq\\frac{1}{2n}\\|{\\pmb x}_{0}-{\\pmb x}_{*}\\|_{2}^{2}+\\sum_{k=1}^{K}2\\eta_{k}^{2}n\\bar{G},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "As a result, for any $\\epsilon>0$ , there exists a step size $\\eta_{k}=\\eta$ suchthat $\\mathbb{E}[f(\\hat{\\mathbf{x}}_{K})-f(\\mathbf{x}_{*})]\\,\\le\\,\\epsilon$ after $\\mathcal{O}\\Big(\\frac{n\\bar{G}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{\\epsilon^{2}}\\Big)$ indvidual gradien gueries. ", "page_idx": 7}, {"type": "text", "text": "4 Discussion of Our New Smoothness Constants and Numerical Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To succinctly explain where our improvements come from, we now consider (PL) where $\\ell_{i}$ is 1-smooth and $b=1$ , ignoring the gains from the mini-batch estimators (for large $K$ ) and our softer guarantee that handles individual smoothness constants. For this specific case, $\\begin{array}{r}{\\tilde{L}=L_{\\mathrm{max}}=\\operatorname*{max}_{1\\leq i\\leq n}\\|\\pmb{a}_{i}\\|^{2}}\\end{array}$ and thus our results for the smooth case and the RR and SO variants match state of the art in the second term, which dominates when there are many $\\begin{array}{r}{\\langle K=\\Omega\\big(\\frac{L_{\\mathrm{max}}^{2}D^{2}n}{\\sigma_{\\ast}^{2}}\\big)\\big)}\\end{array}$ epochs. When there are $\\begin{array}{r}{K=O\\big(\\frac{L_{\\mathrm{max}}^{2}D^{2}n}{\\sigma_{*}^{2}}\\big)}\\end{array}$ epochs in the SO and RR variants or for allregimes of $K$ in the IG variant, the difference between our and state of the art bounds comes from the constant $\\hat{L}$ that replaces $L_{\\mathrm{max}}$ and our improvement is by a factor $\\sqrt{L_{\\operatorname*{max}}/\\hat{L}}$ . Note that $O\\big(\\frac{n L_{\\mathrm{max}}}{\\epsilon}\\big)$ from prior bounds, which is the dominating term in the small $K$ regime, is even worse than the complexity of full gradient descent, as the full gradient Lipschitz constant of $f$ in this case is $\\begin{array}{r}{\\frac{1}{n}\\|\\pmb{A}\\pmb{A}^{\\top}\\|_{2}^{\\bot}\\leq\\bar{L_{\\operatorname*{max}}}}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Given a worst-case permutation $\\bar{\\pi}$ , and denoting by $A_{\\bar{\\pi}}$ the data matrix $\\pmb{A}$ with its rows permuted according to $\\bar{\\pi}$ , our constant $\\hat{L}$ can be bounded above by $L_{\\mathrm{max}}$ using the following sequence of inequalities: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{L}=\\frac{1}{n^{2}}\\|\\sum_{j=1}^{n}I_{(j-1)\\uparrow}A_{\\bar{\\pi}}A_{\\bar{\\pi}}^{\\top}I_{(j-1)\\uparrow}\\|_{2}\\overset{(i)}{\\leq}\\frac{1}{n^{2}}\\sum_{j=1}^{n}\\|I_{(j-1)\\uparrow}A_{\\bar{\\pi}}A_{\\bar{\\pi}}^{\\top}I_{(j-1)\\uparrow}\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\overset{(i i)}{\\leq}\\frac{1}{n^{2}}\\sum_{j=1}^{n}\\|A_{\\bar{\\pi}}A_{\\bar{\\pi}}^{\\top}\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\overset{(i i i)}{\\leq}\\frac{1}{n}\\sum_{i=1}^{n}\\|a_{i}\\|_{2}^{2}\\leq\\operatorname*{max}_{1\\leq i\\leq n}\\|a_{i}\\|_{2}^{2}=L_{\\operatorname*{max}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $(i)$ holds by the triangle inequality, $(i i)$ holds because the operator norm of the matrix $I_{(j-1)\\uparrow}\\dot{A}_{\\pi}A_{\\pi}^{\\top}I_{(j-1)\\uparrow}$ (equal to the operator norm of the bottom right $(n-j+1)\\times(n-j+1)$ submatrix of $A_{\\pi}A_{\\pi}^{\\top}$ ) is always at most $\\lVert A_{\\pi}A_{\\pi}^{\\top}\\rVert=\\lVert A A^{\\top}\\rVert$ , for any permutation $\\pi$ , and $(i i i)$ holds by bounding above the operator norm of a symmetric matrix by its trace. Hence $\\hat{L}$ is never larger than $L_{\\mathrm{max}}$ , but can generally be much smaller, due to the sequence of inequality relaxations in (4). While each of these inequalities can be loose, we emphasize that $(i i i)$ is almost always loose, by a factor that can be as large as $n$ ", "page_idx": 8}, {"type": "text", "text": "As a specific example where $\\hat{L}$ is smaller than $L_{\\mathrm{max}}$ by a factor of $n$ , consider the example of Gaussian data, where we draw $n$ i.i.d. standard Gaussian vectors from $\\mathcal{N}(\\mathbf{0},\\mathbf{I}_{d})$ and take $d=n$ By standard concentration results, with high probability, all columns/rows of $\\mathbf{A}_{\\Bar{\\pi}}$ in this case are near-orthogonal (see, e.g., [7, Chapter]) and $\\|\\bar{\\pmb{a}}_{i}\\|_{2}^{2}\\approx d\\,\\bar{=}\\,n$ for all $i$ . As a result, the operator norm to trace inequality $(i i i)$ is loose by a factor $d=n$ , with high probability. Note that in this example all individual smoothness parameters of components $f_{i}$ are essentially the same (w.h.p.) and equal $\\|\\pmb{a}_{i}\\|_{2}^{2}$ thus the improvement of our bound on the smoothness parameter does not come from averaging but from the structure of the data. This observation is important for contrasting the results from Section 2 and Section 3. In particular, focusing solely on the finite sum structure and ignoring the structure of the data matrix would provide no improvements in the resulting convergence bounds. ", "page_idx": 8}, {"type": "text", "text": "As further evidence, we empirically evaluate $L_{\\mathrm{max}}/\\hat{L}$ on 15 large-scale machine learning datasets and demonstrate that on those datasets $L_{\\mathrm{max}}/\\hat{L}$ is of the order $n^{\\alpha}$ ,for $\\alpha\\in[0.15,0.96]$ (see Sec. 4.1 for more details), providing strong evidence of a tighter guarantee as a function of $n$ ", "page_idx": 8}, {"type": "text", "text": "For the nonsmoth setings by a simila seqguenceof ineqgulies we can showthat $\\bar{G}\\leq G_{\\mathrm{max}}^{2}$ which can be loose by a factor $1/n$ due to the operator norm to trace inequality. Thus, our bound is never worse than what would be obtained from the full subgradient method, but can match the bound of standard SGD, or even improve upon it for at least some data matrices $\\pmb{A}$ ", "page_idx": 8}, {"type": "text", "text": "4.1  Numerical results and discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we provide empirical evidence to support our claim about usefulness of the new convergence bounds obtained in our work. In particular, we conduct numerical evaluations to compare $\\hat{L}$ to the classical smoothness constant $L$ on synthetic datasets and on popular machine learning benchmarkdatasets. ", "page_idx": 8}, {"type": "text", "text": "For a more streamlined comparison and to focus on the dependence on the data matrix, we assume that the loss functions $\\ell_{i}$ all have the same smoothness constant, which leads to $L_{\\mathrm{max}}/\\hat{L}=$ $\\begin{array}{r}{(\\operatorname*{max}_{1\\leq i\\leq n}\\{\\|a_{i}\\|^{2}\\})/\\big(\\frac{1}{n^{2}}\\|\\sum_{j=1}^{n}I_{(j-1)\\uparrow}A_{\\bar{\\pi}}A_{\\bar{\\pi}}^{\\top}I_{(j-1)\\uparrow}\\|_{2}\\big)}\\end{array}$ . Since the scale of the smoothness constant of the loss functions is irrelevant for the ratio $L_{\\mathrm{max}}/\\hat{L}$ in this case, for simplicity, we take it to equal one. Note that assuming different smoothness constants over component loss functions would only make our bound better compared to related work (see Eq. (3) and the discussion following it). ", "page_idx": 8}, {"type": "text", "text": "We also compare $\\hat{L}$ and $L_{\\mathrm{max}}$ on a number of benchmarking datasets from LIBSVM [15], MNIST [17], CIFAR10 [22], and Broad Bioimage Benchmark Collection [28]. For each dataset, we generate a uniformly random permutation $\\pi$ for the data matrix $\\pmb{A}$ and compute $\\hat{L}_{\\pi}$ . We repeat this procedure 1000 times for all datasets and display the average $L_{\\mathrm{max}}/\\hat{L}_{\\pi}$ in Table 2, except for e2006train, CIFAR10, MNIST, and BBBC005 where we do 20 repetitions due to limitations of computation resources required for each calculation. We observe that among the datasets that we consider, which contain all three data matrix \u201cshapes\" $d>>n$ \uff0c $d<<n$ , and $d\\approx n$ , our novel bound dependent on $\\hat{L}$ is much tighter. For instance, for rcv1 and real-sim datasets, where $d$ and $n$ are of the same order, we observe that $L_{\\mathrm{max}}/\\hat{L}$ are approximately 111 and 194, respectively. For news20 dataset where $d>>n$ $L_{\\mathrm{max}}/\\hat{L}\\approx42.1$ . For MNIST, where $d<<n$ \uff0c $L_{\\mathrm{max}}/\\hat{L}\\approx19.1$ .Further results are provided in Appendix E. ", "page_idx": 8}, {"type": "table", "img_path": "qcPlGtzwW9/tmp/aef22007f46360de4f7827dfbfebed7d618397f3a716b89c32a5265ca51f15b2.jpg", "table_caption": ["Table 2: The following table shows the computed values of $L_{\\mathrm{max}}/\\hat{L}$ where $\\hat{L}$ is the empirical mean of $\\hat{L}_{\\pi}$ over random permutations. We note that the quantity $\\sqrt{L_{\\mathrm{max}}/\\hat{L}}$ represents the improvement provided by the bound via our novel primal-dual perspective, compared to previous work. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Finally, as a justification for using the empirical mean of $\\hat{L}_{\\pi}$ over random permutations $\\pi$ in the results displayed in Table 2, we observe in our evaluations that the values of $L_{\\mathrm{max}}/\\hat{L}_{\\pi}$ are fairly concentrated around their empirical mean values. Histogram plots showing the empirical distributions of $L_{\\mathrm{max}}/\\hat{L}_{\\pi}$ for each of the datasets are provided in Appendix E. ", "page_idx": 9}, {"type": "text", "text": "We conclude with a few additional remarks. Our results indicate that the structure of the data is important for predicting behavior of popular machine learning methods such as variants of shuffled SGD considered in our work, and thus should be incorporated in their study: as demonstrated in the Gaussian data example, considering simple finite sum structure and ignoring the dependence on the data can lead to overly pessimistic bounds. Thus it would be interesting to provide a further theoretical study of shuffled SGD that incorporates distributional assumptions for the data. Additionally, as mentioned in the previous paragraph, we empirically observed that permutation-dependent parameter $\\hat{L}_{\\pi}$ concentrates around its mean for permutations generated uniformly at random. Thus, it would be interesting to consider whether our theoretical results can be strengthened to depend on the mean valueof $\\hat{L}_{\\pi}$ (as opposed to maximum). We leave such considerations for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported in part by the U.S. Office of Naval Research under contract number N00014-22-1-2348. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Alekh Agarwal, Martin J Wainwright, Peter Bartlet, and Pradeep Ravikumar. Informationtheoretic lower bounds on the oracle complexity of convex optimization. In Proc. NeurlPS'09, 2009.   \n[2] Kwangjun Ahn, Chulhee Yun, and Suvrit Sra. SGD with shufing: optimal rates without component convexity and large epoch requirements. In Proc. NeurIPS'20, 2020.   \n[3]  Amir Beck. First-order methods in optimization. SIAM, 2017.   \n[4]  Amir Beck and Luba Tetruashvili. On the convergence of block coordinate descent type methods. SIAM Journal on Optimization, 23(4):2037-2060, 2013.   \n[5]  Yoshua Bengio. Practical recommendations for gradient-based training of deep architectures. Neural Networks: Tricks of the Trade: Second Edition, 2012.   \n[6]  Dimitri P Bertsekas and John N Tsitsiklis. Gradient convergence in gradient methods with errors. SIAM Journal on Optimization, 10(3):627-642, 2000.   \n[7] Avrim Blum, John Hopcroft, and Ravi Kannan. Foundations of Data Science. Cambridge University Press, Cambridge, 2020. ISBN 9781108485067. doi: DOI:. URL https://www.cambridge.org/core/books/foundations-of-data-science/ 6A43CE830DE83BED6CC5171E62B0AA9E.   \n[8]  Leon Bottou. Curiously fast convergence of some stochastic gradient descent algorithms. In Proc. Symposium on Learning and Data Science, Paris'09, 2009.   \n[9] Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223-311, 2018.   \n[10] Sebastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends? in Machine Learning, 2015.   \n[11]  Xufeng Cai, Chaobing Song, Stephen J Wright, and Jelena Diakonikolas. Cyclic block coordinate descent with variance reduction for composite nonconvex optimization. arXiv preprint arXiv:2212.05088, 2022.   \n[12] Jaeyoung Cha, Jaewook Lee, and Chulhee Yun. Tighter lower bounds for shuffling SGD: Random permutations and beyond. arXiv preprint arXiv:2303.07160, 2023.   \n[13]  Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems with applications to imaging. Journal of Mathematical Imaging and Vision, 40(1):120-145, 2011.   \n[14] Antonin Chambolle, Matthias J Ehrhardt, Peter Richtarik, and Carola-Bibiane Schonlieb. Stochastic primal-dual hybrid gradient algorithm with arbitrary sampling and imaging applications. SIAM Journal on Optimization, 28(4):2783-2808, 2018.   \n[15]  Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):1-27, 2011.   \n[16]  Christopher M De Sa. Random reshuffling is not always better. In Proc. NeurIPS'20, 2020.   \n[17] Li Deng. The MNIST database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141-142, 2012.   \n[18]  M Gurbuzbalaban, Asu Ozdaglar, and Pablo A Parrilo. Convergence rate of incremental gradient and incremental Newton methods. SIAM Journal on Optimization, 29(4):2542-2565, 2019.   \n[19] Mert Gurbuzbalaban, Asuman Ozdaglar, Pablo A Parrilo, and Nuri Vanli. When cyclic coordinate descent outperforms randomized coordinate descent. In Proc. NeurIPS'17, 2017.   \n[20] Mert Gurbuzbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffing beats stochastic gradient descent. Mathematical Programming, 186:49-84, 2021.   \n[21] Jef Haochen and Suvrit Sra. Random shuffing beats SGD after finite epochs. In Proc. ICML'19, 2019.   \n[22]  Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[23]  Zehua Lai and Lek-Heng Lim. Recht-re noncommutative arithmetic-geometric mean conjecture is false. In Proc. ICML'20, 2020.   \n[24]  Ching-Pei Lee and Stephen J Wright. Random permutations fix a worst case for cyclic coordinate descent. IMA Journal of Numerical Analysis, 39(3):1246-1275, 2019.   \n[25] Xiao Li, Zhihui Zhu, Anthony Man-Cho So, and Jason D Lee. Incremental methods for weakly convex optimization. arXiv preprint arXiv: 1907.11687, 2019.   \n[26]  Xingguo Li, Tuo Zhao, Raman Arora, Han Liu, and Mingyi Hong. On faster convergence of cyclic block coordinate descent-type methods for strongly convex minimization. The Journal of Machine Learning Research, 18(1):6741-6764, 2017.   \n[27]  Cheuk Yin Lin, Chaobing Song, and Jelena Diakonikolas. Accelerated cyclic coordinate dual averaging with extrapolation for composite convex optimization. arXiv preprint arXiv:2303.16279, 2023.   \n[28]  Vebjorn Ljosa, Katherine L. Sokolnicki, and Anne E Carpenter. Broad bioimage benchmark collection. https: //bbbc.broadinstitute.org/image_sets, 2012. Accessed: 2023-05- 16.   \n[29] Olvi L Mangasarian and MV Solodov. Serial and parallel backpropagation convergence via nonmonotone perturbed minimization. Technical report, University of Wisconsin-Madison Department of Computer Sciences, 1993.   \n[30] Konstantin Mishchenko, Ahmed Khaled, and Peter Richtarik. Random reshuffing: Simple analysis with vast improvements. In Proc. NeurIPS'20, 2020.   \n[31] Dheeraj Nagaraj, Prateek Jain, and Praneeth Netrapalli. SGD without replacement: Sharper rates for general smooth convex functions. In Proc. ICML'19, 2019.   \n[32]  Angelia Nedic and Dimitri P Bertsekas. Incremental subgradient methods for nondifferentiable optimization. SIAM Journal on Optimization, 12(1):109-138, 2001.   \n[33]  Yu Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341-362, 2012.   \n[34] Lam M Nguyen, Quoc Tran-Dinh, Dzung T Phan, Phuong Ha Nguyen, and Marten Van Dijk. A unified convergence analysis for shuffing-type gradient methods. The Journal of Machine Learning Research, 22(1):9397-9440, 2021.   \n[35] Shashank Rajput, Anant Gupta, and Dimitris Papailiopoulos. Closing the convergence gap of SGD without replacement. In Proc. ICML'20, 2020.   \n[36]  Benjamin Recht and Christopher R\u00e9. Toward a noncommutative arithmetic-geometric mean inequality: Conjectures, case-studies, and consequences. In Proc. COLT'12, 2012.   \n[37] Benjamin Recht and Christopher Re. Parallel stochastic gradient algorithms for large-scale matrix completion. Mathematical Programming Computation, 2013.   \n[38] Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical Statistics, pages 400-407, 1951.   \n[39] Itay Safran and Ohad Shamir. How good is SGD with random shuffing? In Proc. COLT'20, 2020.   \n[40]  Ankan Saha and Ambuj Tewari. On the nonasymptotic convergence of cyclic coordinate descent methods. SIAM Journal on Optimization, 23(1):576-601, 2013.   \n[41]  Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss minimization. Journal of Machine Learning Research, 14(1), 2013.   \n[42]  Ohad  Shamir.-  Without-replacement sampling for stochastic  gradient methods. In Proc. NeurIPS'16, 2016.   \n[43]  Chaobing Song and Jelena Diakonikolas. Fast cyclic coordinate dual averaging with extrapolation for generalized variational inequalities. arXiv preprint arXiv:2102.13244, 2021.   \n[44]  Ruoyu Sun and Yinyu Ye. Worst-case complexity of cyclic coordinate descent: $O(n^{2})$ gapwith randomized version. Mathematical Programming, 185(1):487-520, 2021.   \n[45] Trang H Tran, Lam M Nguyen, and Quoc Tran-Dinh. SMG: A shuffling gradient-based method with momentum. In Proc. ICML'21, 2021.   \n[46] Trang H Tran, Katya Scheinberg, and Lam M Nguyen. Nesterov accelerated shuffling gradient method for convex optimization. In Proc. ICML'22, 2022.   \n[47]  Stephen  Wright and Ching-pei Lee.  Analyzing random permutations for cyclic coordinate descent. Mathematics of Computation, 89(325):2217-2248, 2020.   \n[48]  Yangyang Xu and Wotao Yin. Block stochastic gradient iteration for convex and nonconvex optimization. SIAM Journal on Optimization, 25(3):1686-1716, 2015.   \n[49]  Yangyang Xu and Wotao Yin. A globally convergent algorithm for nonconvex optimization based on block coordinate update. Journal of Scientific Computing, 72(2):700-734, 2017.   \n[50] Bicheng Ying, Kun Yuan, Stefan Vlaski, and Ali H Sayed. Stochastic learning under random reshufing with constant step-sizes. IEEE Transactions on Signal Processing, 67(2):474-489, 2018.   \n[51]  Chulhee Yun, Shashank Rajput, and Suvrit Sra. Minibatch vs local SGD with shuffling: Tight convergence bounds and beyond. In Proc. ICLR'22, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Outline.  The supplementary material of the paper is organized as follows: ", "page_idx": 13}, {"type": "text", "text": "\u00b7 Section A provides a brief survey on shuffled SGD and its related work.   \n\u00b7 Section B presents the proofs related to the smooth convex setting from Section 2, where we only assume each component function $f_{i}$ to be convex and $L_{i}$ smooth.   \n\u00b7 Section C presents the proofs related to the smooth convex setting with linear predictors from Section 3.   \n\u00b7 Section D presents the proofs related to the non-smooth convex setting with linear predictors from Section3.   \n\u00b7 Section E presents the full details of the computational experiments performed in the paper. ", "page_idx": 13}, {"type": "text", "text": "A  Further Related Work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we continue with the discussion on the background of shuffed SGD from Section 1. We would like to briefly recall that shuffled SGD usually performs better in practice when compared to SGD, and is also easier and more efficient to implement. However, in terms of the theoretical analysis, sampling without replacement introduces the sampling bias at each iteration, making it difficult to approximate shuffed SGD by full gradient descent. Using empirical observations, shuffled SGD was conjectured to converge much faster than SGD with replacement, based on the noncommutative arithmetic-geometric mean inequality conjecture [36], which was later proved to be false [16, 23]. As a consequence, whether or not shuffed SGD can be faster than SGD at least in some regimes remained open [8] until a breakthrough result in [20], where it was shown that for the class of smooth strongly convex optimization problems, the convergence of the RR variant of shuffed SGD is essentially of the order- $(1\\dot{/}K^{2})$ for $K$ full passes of the data (also called epochs), which is faster than order- $\\left(1/n K\\right)$ convergence of SGD for sufficiently large $K$ . This bound for the smooth strongly convex case was later improved under various regimes and additional assumptions [2, 21, 30, 31, 34, 42], while the tightest of those bounds were matched by lower bounds in [12, 35, 39, 51]. ", "page_idx": 13}, {"type": "text", "text": "Since our results are for the general (non-strongly) convex regimes, in this section we focus on the results that apply to those (convex, smooth or nonsmooth Lipschitz) regimes. For convex nonsmooth Lipschitz problems, we are only aware of the results in [42]. These results are only useful when the number of data passes $K$ is small and the number of component functions $n$ is large, as they contain an irreducible order- $\\textstyle{\\frac{1}{\\sqrt{n}}}$ error, and are not directly comparable to our results. ", "page_idx": 13}, {"type": "text", "text": "For the IG variant of SGD without replacement (deterministic order), asymptotic convergence was established in [6, 29], with further convergence results for both smooth and nonsmooth settings provided in [18, 25, 30, 32, 34, 50]. As IG does not benefit from randomization, it is known to have a worse convergence bound than RR under the Lipschitz Hessian assumption [18, 21], which was also shown in more general settings [30]. ", "page_idx": 13}, {"type": "text", "text": "In this paper, we viewed shuffled SGD as a primal-dual method where the updates are performed on the dual side in a cyclic manner, thus we can leverage techniques from general cyclic methods. However, in contrast to randomized methods (corresponding to standard SGD), cyclic methods are usually more challenging to analyze [33], basic variants exhibit much worse worst-case complexity than even full gradient methods [4, 19, 26, 26, 40, 44, 48, 49], with more refined results being established only recently [11, 27, 43]. While the inspiration for our work came from these recent results [11, 27, 43], they are completely technically disjoint. First, all these results rely on nonstandard block Lipschitz assumptions, which are not present in our work. Second, all of them leverage proximal gradient-style cyclic updates to carry out the analysis, which is inapplicable in our case for the cyclic updates on the dual side, as otherwise the method would not correspond to (shuffed) SGD. Finally, [27, 43] utilize extrapolation steps, which would break the connection to shuffed SGD in our setting, while [11] relies on a gradient descent-type descent lemma, which is impossible to establish in our setting. ", "page_idx": 13}, {"type": "text", "text": "BOmitted Proofs From Section 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we consider the general finite-sum setting where we assume that each component function $f_{i}$ is convex and smooth, and derive the refined analysis under this setting. Here we focus on the smooth convex problems as prior work did [30, 34], since smoothness is essential to showing the advantage of shuffed SGD [31] over SGD, otherwise the rate of SGD is optimal. In particular, we study the general smooth convex finite-sum problem (P) ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{{\\pmb{x}}\\in\\mathbb{R}^{d}}\\Big\\{f({\\pmb{x}}):=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}({\\pmb{x}})\\Big\\},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where each $f_{i}$ is convex and smooth. (P) is equivalent to ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\operatorname*{max}_{y\\in\\mathbb{R}^{n d}}\\Big\\{\\mathcal{L}(x,y):=\\frac{1}{n}\\sum_{i=1}^{n}\\Big(\\left\\langle y^{i},x\\right\\rangle-f_{i}^{*}(y^{i})\\Big)=\\frac{1}{n}\\left\\langle E x,y\\right\\rangle-\\frac{1}{n}\\sum_{i=1}^{n}f_{i}^{*}(y^{i})\\Big\\},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we slightly abuse the notation in this section and use $\\pmb{y}^{i}\\in\\mathbb{R}^{d}$ to be the $i$ -th $d$ elements of the vector $\\textit{\\textbf{y}}$ such that $\\pmb{y}\\,=\\,(\\pmb{y}^{1},\\dots,\\pmb{y}^{n})^{\\top}\\,\\in\\,\\mathbb{R}^{n d}$ \uff0c $\\pmb{E}\\,=\\,[\\bar{I_{d}},\\,.\\,.\\,.\\,,I_{d}]^{\\top}\\,\\in\\,\\mathbb{R}^{n d\\times d}$ is the vertical n   \nconcatenation of $n$ identity matrices $\\pmb{I}_{d}\\in\\mathbb{R}^{d\\times d}$ and $f_{i}^{*}$ is the convex conjugate of $f_{i}$ defined by $\\begin{array}{r}{f_{i}(\\pmb{x})\\,=\\,\\operatorname*{sup}_{\\pmb{y}^{i}\\in\\mathbb{R}^{d}}\\left\\langle\\pmb{y}^{i},\\pmb{x}\\right\\rangle-f_{i}^{*}(\\pmb{y}^{i})}\\end{array}$ . In the following, we consider the mini-batch estimator of batch size $b$ , and let $\\pmb{y}^{(i)}\\ \\in\\ \\mathbb{R}^{b d}$ denote the vector comprised of the $i^{\\mathrm{th}}\\ b d$ elements of $\\textit{\\textbf{y}}$ .For simplicity and without loss of generality, we assume that $n=b m$ for some positive integer $m$ , so that $\\pmb{y}=(\\pmb{y}^{(1)},\\dots,\\pmb{y}^{(m)})^{\\top}$ . Note that if choosing $b=1$ , our setting is the same as the ones in [30, 34]. Then we have the primal-dual view of shuffled SGD scheme for general smooth convex minimization as in Alg. 1, where $\\pmb{E}_{b}^{\\top}=[\\underbrace{\\pmb{I}_{d},\\dots,\\pmb{I}_{d}}_{b}]^{\\top}\\in\\mathbb{R}^{b d\\times d}$ is the vertical concatenation of $b$ identity matrices $\\pmb{I}_{d}\\in\\mathbb{R}^{d\\times d}$ $\\pi^{(k)}=\\{\\pi_{1}^{(k)},\\pi_{2}^{(k)},\\ldots,\\pi_{n}^{(k)}\\}$ $[n]$ athe $k$   \nuse the same notation of $\\pmb{v}_{k}=(\\pmb{v}^{\\pi_{1}^{(k)}},\\cdot\\cdot\\cdot\\,,\\pmb{v}^{\\pi_{n}^{(k)}})^{\\top}\\in\\mathbb{R}^{n d}$ \uff0c $\\pmb{y}_{\\ast,k}=(\\pmb{y}_{\\ast}^{\\pi_{1}^{(k)}},\\dots,\\pmb{y}_{\\ast}^{\\pi_{n}^{(k)}})^{\\top}\\in\\mathbb{R}^{n d}$ as in previous sections except now each $\\pmb{v}^{\\pi_{i}^{(k)}},\\pmb{y}_{*}^{\\pi_{i}^{(k)}}$ are $d$ -dimensional subvectors. Further, we denote the permuted smoothness constant matrices by $\\pmb{\\Lambda}_{k}=\\mathrm{diag}(L_{\\pi_{1}^{(k)}},\\dots,L_{\\pi_{1}^{(k)}},\\dots,L_{\\pi_{n}^{(k)}},\\dots,L_{\\pi_{n}^{(k)}})\\in$ d d   \n$\\mathbb{R}^{n d\\times n d}$ , and we use $\\boldsymbol{\\mathit{I}}$ for $I_{n d}\\in\\mathbb{R}^{n d\\times n d}$ throughout this section. ", "page_idx": 14}, {"type": "text", "text": "New smoothness constants and comparisons. We first recall the new smoothness constants for anypermutation $\\pi$ of $[n]$ , defined in Eq. (2): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\hat{L}_{\\pi}^{g}:=\\frac{1}{m n}\\|\\mathbf{A}_{\\pi}^{1/2}\\big(\\sum_{i=1}^{m}I_{b d(i-1)\\uparrow}\\boldsymbol{E}\\boldsymbol{E}^{\\top}I_{b d(i-1)\\uparrow}\\big)\\mathbf{A}_{\\pi}^{1/2}\\|_{2},}&&{\\hat{L}^{g}=\\underset{\\pi}{\\operatorname*{max}}\\hat{L}_{\\pi}^{g},}\\\\ &{\\tilde{L}_{\\pi}^{g}:=\\frac{1}{b}\\|\\mathbf{A}_{\\pi}^{1/2}\\big(\\sum_{i=1}^{m}I_{(d i)}\\boldsymbol{E}\\boldsymbol{E}^{\\top}I_{(d i)}\\big)\\mathbf{A}_{\\pi}^{1/2}\\|_{2},}&&{\\tilde{L}^{g}=\\underset{\\pi}{\\operatorname*{max}}\\tilde{L}_{\\pi}^{g},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{I_{(d i)}=\\sum_{j=b d(i-1)+1}^{b d i}I_{j}}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "Tocompare $\\hat{L}_{\\pi}^{g}$ and $L:=\\operatorname*{max}_{i\\in[n]}L_{i}$ , we make use of the Kronecker product with notation $\\otimes$ defined by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pmb{A}\\otimes\\pmb{B}=\\left[\\begin{array}{c c c}{A_{11}\\pmb{B}}&{\\cdot\\cdot\\cdot}&{A_{1n}\\pmb{B}}\\\\ {\\vdots}&{\\cdot\\cdot}&{\\vdots}\\\\ {A_{m1}\\pmb{B}}&{\\cdot\\cdot\\cdot}&{A_{n n}\\pmb{B}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for two matrices $A\\in\\mathbb{R}^{m\\times n}$ and $\\b{B}\\in\\mathbb{R}^{p\\times q}$ . The following lemma states a useful fact for the Kronecker product. ", "page_idx": 14}, {"type": "text", "text": "Lemma3.Forsquarematrices $\\pmb{A}$ and $_B$ of sizes p and $q$ andwitheigenvalues $\\lambda_{i}$ $(i\\in[p])$ and $\\mu_{j}$ $(j\\in[q])$ respectively,theeigenvaluesof $A\\otimes B$ are $\\lambda_{i}\\mu_{j}$ for $i\\in[p],j\\in[q]$ ", "page_idx": 14}, {"type": "text", "text": "We now use the following chain of inequalities to compare $\\hat{L}_{\\pi}^{g}$ and $L$ for any permutation $\\pi$ of $[n]$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{L}_{\\pi}^{g}=\\frac{1}{m n}\\Big\\lVert\\mathbf{A}_{\\pi}^{1/2}\\Big(\\sum_{i=1}^{m}I_{b d(i-1)\\uparrow}\\mathbf{E}E^{\\top}I_{b d(i-1)\\uparrow}\\Big)\\mathbf{A}_{\\pi}^{1/2}\\Big\\rVert_{2}}\\\\ &{\\quad\\le\\frac{1}{n}\\Big\\lVert\\mathbf{A}^{1/2}E E^{\\top}\\mathbf{A}^{1/2}\\Big\\rVert_{2}}\\\\ &{\\quad=\\displaystyle\\frac{1}{n}\\Big\\lVert(I_{\\pi}l_{\\pi}^{\\top})\\otimes I_{d}\\Big\\rVert_{2}}\\\\ &{\\stackrel{(i)}{=}\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}L_{i}\\leq L,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we define $l_{\\pi}\\,=\\,\\bigl(\\sqrt{L_{\\pi_{1}}},\\sqrt{L_{\\pi_{2}}},\\ldots,\\sqrt{L_{\\pi_{n}}}\\bigr)^{\\top}$ . For $(i)$ , we use Lemma 3 and notice that the eigenvalues of $\\pmb{I}_{d}$ all equal 1, while the largest eigenvalue of $\\textstyle l_{\\pi}l_{\\pi}^{\\top}=\\|l\\|_{2}^{2}=\\sum_{i=1}^{n}L_{i}$ , so the operator norm of $(l_{k}l_{k}^{\\top})\\otimes I_{d}$ .s $\\textstyle\\sum_{i=1}^{n}L_{i}$ ", "page_idx": 15}, {"type": "text", "text": "To compare ${\\tilde{L}}_{\\pi}^{g}$ and $L$ , we notice that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Lambda_{\\pi}^{1/2}\\big(\\sum_{i=1}^{m}I_{(d i)}E E^{\\top}I_{(d i)}\\big)\\Lambda_{\\pi}^{1/2}=\\sum_{i=1}^{m}I_{(d i)}\\Lambda_{\\pi}^{1/2}E E^{\\top}\\Lambda_{\\pi}^{1/2}I_{(d i)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "is a block diagonal matrix whose operator norm is the maximum of the operator norms over its diagonal block submatrices, so we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{L}_{\\pi}^{g}=\\displaystyle\\frac{1}{b}\\operatorname*{max}_{i\\in[m]}\\left\\|I_{(d i)}\\Lambda_{\\pi}^{1/2}E E^{\\top}\\Lambda_{\\pi}^{1/2}I_{(d i)}\\right\\|}\\\\ &{\\quad=\\displaystyle\\frac{1}{b}\\operatorname*{max}_{i\\in[m]}\\left\\|I_{(d i)}\\left((l_{\\pi}l_{\\pi}^{\\top})\\otimes I_{d}\\right)I_{(d i)}\\right\\|}\\\\ &{\\quad\\overset{(i)}{=}\\displaystyle\\operatorname*{max}_{i\\in[m]}\\frac{1}{b}\\sum_{j=1}^{b}L_{\\pi_{b(i-1)+j}}\\leq L,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where for $(i)$ we  use  Lemma  3  for  each submatrix $(l_{\\pi}^{(i)}l_{\\pi}^{(i)\\top})\\ \\ \\otimes\\ I_{d}$ and $\\begin{array}{r l}{\\pmb{l}_{\\pi}^{(i)}}&{{}=}\\end{array}$ $(0,\\ldots,0,{\\sqrt{L_{\\pi_{b(i-1)+1}}}},\\ldots,{\\sqrt{L_{\\pi_{b i}}}},0,\\ldots,0)^{\\top}$ . Similar to the case of generalized linear models, the inequality is tight when $b=1$ but can be loose for other values of $b$ ", "page_idx": 15}, {"type": "text", "text": "Before proceeding to the omitted proofs, we first state the following standard definitions and first-order characterization of strong convexity, for completeness. ", "page_idx": 15}, {"type": "text", "text": "Definition 1. A function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is said to be $\\mu$ -strongly convex with parameter $\\mu>0$ iffor any $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$ and any $\\lambda\\in(0,1)$ ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(\\lambda x+(1-\\lambda)\\pmb{y})\\leq\\lambda f(\\pmb{x})+(1-\\lambda)f(\\pmb{y})-\\frac{\\mu}{2}\\lambda(1-\\lambda)\\|\\pmb{x}-\\pmb{y}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma 4. Let $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be a continuous $\\mu$ -strongly convex function with $\\mu>0$ Then, for any ,y E Rd: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf({\\pmb y})\\geq f({\\pmb x})+\\langle{\\pmb g}_{\\pmb x},{\\pmb y}-{\\pmb x}\\rangle+\\frac{\\mu}{2}\\|{\\pmb x}-{\\pmb y}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $g_{x}\\in\\partial f(x).$ and $\\partial f({\\pmb x})$ is the subdifferential of $f$ at $\\textbf{\\em x}$ ", "page_idx": 15}, {"type": "text", "text": "We also include the following lemma on the variance bound under without-replacement sampling.   \nwhich is useful for our proof. ", "page_idx": 15}, {"type": "text", "text": "Lemma 5. Let $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ be the set of $|\\beta|=b$ samples from $[n]$ drawn without replacement and uniformly at random. Then, $\\forall\\pmb{x}\\in\\mathbb{R}^{d}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{B}\\Big[\\big\\lVert\\frac{1}{b}\\sum_{i\\in B}\\nabla f_{i}(\\pmb{x})-\\nabla f(\\pmb{x})\\big\\rVert_{2}^{2}\\Big]=\\frac{n-b}{b(n-1)}\\mathbb{E}_{i}\\big[\\|\\nabla f_{i}(\\pmb{x})-\\nabla f(\\pmb{x})\\|_{2}^{2}\\big].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. We first expand the square on the left-hand side, as follows ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{B}\\Big[\\big\\lVert\\frac{1}{b}\\displaystyle\\sum_{i\\in B}\\nabla f_{i}(x)-\\nabla f(x)\\big\\rVert_{2}^{2}\\Big]}\\\\ &{=\\displaystyle\\frac{1}{b^{2}}\\mathbb{E}_{B}\\Big[\\displaystyle\\sum_{i,i^{\\prime}\\in B}\\langle\\nabla f_{i}(x)-\\nabla f(x),\\nabla f_{i^{\\prime}}(x)-\\nabla f(x)\\rangle\\Big]}\\\\ &{=\\displaystyle\\frac{1}{b^{2}}\\mathbb{E}_{B}\\Big[\\displaystyle\\sum_{i,i^{\\prime}\\in B,i\\neq i^{\\prime}}\\langle\\nabla f_{i}(x)-\\nabla f(x),\\nabla f_{i^{\\prime}}(x)-\\nabla f(x)\\rangle\\Big]+\\frac{1}{b}\\mathbb{E}_{i}\\big[\\|\\nabla f_{i}(x)-\\nabla f(x)\\|_{2}^{2}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since the batch $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ is sampled uniformly and without replacement from $[n]$ , the probability that any pair $(i,i^{\\prime})$ from $[n]$ with $i\\neq i^{\\prime}$ is in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ .s $\\frac{b(b\\!-\\!1)}{n(n\\!-\\!1)}$ By te linarityo expetation, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\mathbb{E}_{B}\\Big[\\displaystyle\\sum_{i,i^{\\prime}\\in B,i\\neq i^{\\prime}}\\langle\\nabla f_{i}(\\pmb{x})-\\nabla f(\\pmb{x}),\\nabla f_{i^{\\prime}}(\\pmb{x})-\\nabla f(\\pmb{x})\\rangle\\Big]}\\\\ &{=\\mathbb{E}_{B}\\Big[\\displaystyle\\sum_{i,i^{\\prime}\\in[n],i\\neq i^{\\prime}}\\mathbb{1}_{i,i^{\\prime}\\in B}\\,\\langle\\nabla f_{i}(\\pmb{x})-\\nabla f(\\pmb{x}),\\nabla f_{i^{\\prime}}(\\pmb{x})-\\nabla f(\\pmb{x})\\rangle\\Big]}\\\\ &{=\\displaystyle\\sum_{i,i^{\\prime}\\in[n],i\\neq i^{\\prime}}\\mathbb{E}_{B}\\Big[\\mathbb{1}_{i,i^{\\prime}\\in B}\\,\\langle\\nabla f_{i}(\\pmb{x})-\\nabla f(\\pmb{x}),\\nabla f_{i^{\\prime}}(\\pmb{x})-\\nabla f(\\pmb{x})\\rangle\\Big]}\\\\ &{=\\displaystyle\\frac{b(b-1)}{n(n-1)}\\sum_{i,i^{\\prime}\\in[n],i\\neq i^{\\prime}}\\langle\\nabla f_{i}(\\pmb{x})-\\nabla f(\\pmb{x}),\\nabla f_{i^{\\prime}}(\\pmb{x})-\\nabla f(\\pmb{x})\\rangle\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathbb{1}$ is the indicator function such that $\\mathbb{1}_{i,i^{\\prime}\\in B}=1$ if both $i,i^{\\prime}\\in\\mathcal{B}$ and is equal to zero otherwise. Hence, we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{B}\\Big[\\big\\|\\frac{1}{b}\\displaystyle\\sum_{i\\in B}\\nabla f_{i}(x)-\\nabla f(x)\\big\\|^{2}\\Big]}\\\\ &{=\\frac{b-1}{b n(n-1)}\\displaystyle\\sum_{i,i^{\\prime}\\in[n],i\\neq i^{\\prime}}\\langle\\nabla f_{i}(x)-\\nabla f(x),\\nabla f_{i^{\\prime}}(x)-\\nabla f(x)\\rangle+\\frac{1}{b}\\mathbb{E}_{i}\\big[\\|\\nabla f_{i}(x)-\\nabla f(x)\\|^{2}\\big]}\\\\ &{=\\frac{b-1}{b n(n-1)}\\displaystyle\\sum_{i,i^{\\prime}\\in[n]}\\langle\\nabla f_{i}(x)-\\nabla f(x),\\nabla f_{i^{\\prime}}(x)-\\nabla f(x)\\rangle+\\frac{n-b}{b(n-1)}\\mathbb{E}_{i}\\big[\\|\\nabla f_{i}(x)-\\nabla f(x)\\|^{2}}\\\\ &{\\overset{(i)}{=}\\frac{n-b}{b(n-1)}\\mathbb{E}_{i}\\big[\\|\\nabla^{j}f_{i}(x)-\\nabla^{j}f(x)\\|^{2}\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(i)$ is due to $\\textstyle f={\\frac{1}{n}}\\sum_{i=1}^{n}f_{i}$ having the finite sum structure. ", "page_idx": 16}, {"type": "text", "text": "Now we provide the omitted proofs from Section 2. ", "page_idx": 16}, {"type": "text", "text": "Lemma6. Under ssumption $^{\\,l}$ for any $k\\in[K]$ the ierates $\\{y_{k}^{(i)}\\}_{i=1}^{m}$ and $\\{\\mathbfit{x}_{k-1,i}\\}_{i=1}^{m+1}$ generated by Algorithm satisfy ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathcal{E}_{k}\\leq\\displaystyle\\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}\\bigg\\langle E_{b}^{\\top}y_{k}^{(i)},x_{k}-x_{k-1,i+1}\\bigg\\rangle+\\displaystyle\\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}\\bigg\\langle E_{b}^{\\top}\\big(v_{k}^{(i)}-y_{k}^{(i)}\\big),x_{k}-x_{k-1,i}\\bigg\\rangle}\\\\ &{}&{\\quad-\\displaystyle\\frac{\\eta_{k}}{2n}\\|y_{k}-v_{k}\\|_{\\boldsymbol{\\Lambda}_{k}^{-1}}^{2}-\\displaystyle\\frac{\\eta_{k}}{2n}\\|y_{k}-y_{*,k}\\|_{\\boldsymbol{\\Lambda}_{k}^{-1}}^{2}-\\displaystyle\\frac{b}{2n}\\sum_{i=1}^{m}\\|x_{k-1,i+1}-x_{k-1,i}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{k}:=\\eta_{k}\\mathrm{Gap}^{v}(\\pmb{x}_{k},\\pmb{y}_{*})+\\frac{b}{2n}\\|\\pmb{x}_{*}-\\pmb{x}_{k}\\|_{2}^{2}-\\frac{b}{2n}\\|\\pmb{x}_{*}-\\pmb{x}_{k-1}\\|_{2}^{2}.}\\end{array}\n$$where ", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We first note that based on Line 6 of Alg. 1, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\langle E_{b}^{\\top}y^{(i)},x_{k-1,i}\\right\\rangle-\\sum_{j=1}^{b}f_{\\pi_{b(i-1)+j}^{(k)}}^{*}(y^{j})=\\sum_{j=1}^{b}\\left(\\left\\langle y^{j},x_{k-1,i}\\right\\rangle-f_{\\pi_{b(i-1)+j}^{(k)}}^{*}(y^{j})\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since the max problem defining $\\pmb{y}_{k}$ is separable, we have for $b(i-1)+1\\leq j\\leq b i$ and $i\\in[m]$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{y}_{k}^{j}=\\arg\\operatorname*{max}_{\\pmb{y}^{j}\\in\\mathbb{R}^{d}}\\Big\\{\\left\\langle\\pmb{y}^{j},\\pmb{x}_{k-1,i}\\right\\rangle-f_{\\pi_{j}^{(k)}}^{*}(\\pmb{y}^{j})\\Big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which leads to $x_{k-1,i}\\,\\in\\,\\partial f_{\\pi_{j}^{(k)}}^{*}({\\pmb y}_{k}^{j})$ . Further, since each component function $f_{j}^{*}$ .s $\\frac{1}{L_{j}}$ -strongly convex thus for $b(i-1)+1\\leq j\\leq b i$ , we also have ", "page_idx": 17}, {"type": "equation", "text": "$$\nf_{\\pi_{j}^{(k)}}^{*}(v_{k}^{j})\\geq f_{\\pi_{j}^{(k)}}^{*}(y_{k}^{j})+\\Big\\langle x_{k-1,i},v_{k}^{j}-y_{k}^{j}\\Big\\rangle+\\frac{1}{2L_{\\pi_{j}^{(k)}}}\\|v_{k}^{j}-y_{k}^{j}\\|^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which leads to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{L}(\\boldsymbol{x}_{k},\\boldsymbol{v})}\\\\ &{=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{m}\\left(\\left\\langle E_{b}^{\\top}v_{k}^{(i)},\\boldsymbol{x}_{k-1,i}\\right\\rangle-\\displaystyle\\sum_{j=b(i-1)+1}^{b i}f_{\\pi_{j}^{(k)}}^{*}(v_{k}^{j})\\right)+\\displaystyle\\frac{1}{n}\\sum_{i=1}^{m}\\left\\langle E_{b}^{\\top}v_{k}^{(i)},\\boldsymbol{x}_{k}-\\boldsymbol{x}_{k-1,i}\\right\\rangle}\\\\ &{\\le\\displaystyle\\frac{1}{n}\\sum_{i=1}^{m}\\left(\\left\\langle E_{b}^{\\top}y_{k}^{(i)},\\boldsymbol{x}_{k-1,i}\\right\\rangle-\\displaystyle\\sum_{j=b(i-1)+1}^{b i}f_{\\pi_{j}^{(k)}}^{*}(y_{k}^{j})\\right)+\\displaystyle\\frac{1}{n}\\sum_{i=1}^{m}\\left\\langle E_{b}^{\\top}v_{k}^{(i)},\\boldsymbol{x}_{k}-\\boldsymbol{x}_{k-1,i}\\right\\rangle}\\\\ &{\\quad-\\displaystyle\\frac{1}{2n}\\lVert\\boldsymbol{y}_{k}-\\boldsymbol{v}_{k}\\rVert_{\\mathbf{A}_{k}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using the same argument, as $x_{\\ast}\\in\\partial f_{i}^{\\ast}(y_{\\ast}^{i})$ for $i\\in[n]$ ,wehave ", "page_idx": 17}, {"type": "equation", "text": "$$\nf_{\\pi_{i}^{(k)}}^{*}(y_{k}^{i})\\geq f_{\\pi_{i}^{(k)}}^{*}(y_{*,k}^{i})+\\langle x_{*},y_{k}^{i}-y_{*,k}^{i}\\rangle+\\frac{1}{2L_{\\pi_{i}^{(k)}}}\\|y_{k}^{i}-y_{*,k}^{i}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{L}(\\boldsymbol{x}_{s},\\boldsymbol{y}_{*})}\\\\ &{=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{m}\\left(\\left\\langle E_{b}^{\\top}y_{*,k}^{(i)},\\boldsymbol{x}_{*}\\right\\rangle-\\sum_{j=b(i-1)+1}^{b i}f_{\\pi_{j}^{(k)}}^{*}(y_{*,k}^{j})\\right)}\\\\ &{\\geq\\displaystyle\\frac{1}{n}\\sum_{i=1}^{m}\\left(\\left\\langle E_{b}^{\\top}y_{k}^{(i)},\\boldsymbol{x}_{*}\\right\\rangle-\\sum_{j=b(i-1)+1}^{b i}f_{\\pi_{j}^{(k)}}^{*}(y_{k}^{j})\\right)+\\frac{1}{2n}\\|y_{k}-y_{*,k}\\|_{\\boldsymbol{\\Lambda}_{k}^{-1}}^{2}}\\\\ &{=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{m}\\left(\\left\\langle E_{b}^{\\top}y_{k}^{(i)},\\boldsymbol{x}_{*}\\right\\rangle+\\frac{b}{2\\eta_{k}}\\|\\boldsymbol{x}_{*}-\\boldsymbol{x}_{k-1,i}\\|^{2}-\\frac{b}{2\\eta_{k}}\\|\\boldsymbol{x}_{*}-\\boldsymbol{x}_{k-1,i}\\|^{2}-\\sum_{j=b(i-1)+1}^{b i}f_{\\pi_{j}^{(k)}}^{*}(y_{k}^{j})\\right)}\\\\ &{\\quad+\\frac{1}{2n}\\|y_{k}-y_{*,k}\\|_{\\boldsymbol{\\Lambda}_{k}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using the updating scheme of $\\pmb{x}_{k-1,i+1}$ and noticing that $\\begin{array}{r}{\\phi_{k}^{i}(\\pmb{x})=\\left\\langle E_{b}^{\\top}\\pmb{y}_{k}^{(i)},\\pmb{x}\\right\\rangle\\!+\\!\\frac{b}{2\\eta_{k}}\\|\\pmb{x}\\!-\\!\\pmb{x}_{k-1,i}\\|^{2}}\\end{array}$ .s $\\frac{b}{\\eta_{k}}$ strongly convex and minimized at $\\pmb{x}_{k-1,i+1}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Big\\langle E_{b}^{\\top}y_{k}^{(i)},\\mathbf{x}_{*}\\Big\\rangle+\\frac{b}{2\\eta_{k}}\\|\\mathbf{x}_{*}-\\mathbf{x}_{k-1,i}\\|^{2}}\\\\ &{\\geq\\Big\\langle E_{b}^{\\top}y_{k}^{(i)},\\mathbf{x}_{k-1,i+1}\\Big\\rangle+\\frac{b}{2\\eta_{k}}\\|\\mathbf{x}_{k-1,i+1}-\\mathbf{x}_{k-1,i}\\|^{2}+\\frac{b}{2\\eta_{k}}\\|\\mathbf{x}_{k-1,i+1}-\\mathbf{x}_{*}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which leads to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\big(x_{*},y_{*}\\big)\\geq\\frac{1}{n}\\sum_{i=1}^{m}\\Big(\\Big\\langle E_{b}^{\\top}y_{k}^{(i)},x_{k-1,i+1}\\Big\\rangle+\\frac{b}{2\\eta_{k}}\\|x_{k-1,i+1}-x_{k-1,i}\\|^{2}-\\displaystyle\\sum_{j=b(i-1)+1}^{b i}f_{\\pi_{j}^{(k)}}^{*}(y_{k}^{i})\\Big)}\\qquad}&{}\\\\ &{\\quad+\\displaystyle\\frac{b}{2n\\eta_{k}}\\sum_{i=1}^{m}\\Big(\\|x_{k-1,i+1}-x_{*}\\|^{2}-\\|x_{k-1,i}-x_{*}\\|^{2}\\Big)+\\displaystyle\\frac{1}{2n}\\|y_{k}-y_{*,k}\\|_{\\mathbf{A}_{k}^{-1}}^{2}}\\\\ &{=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{m}\\Big(\\Big\\langle E_{b}^{\\top}y_{k}^{(i)},x_{k-1,i+1}\\Big\\rangle+\\frac{b}{2\\eta_{k}}\\|x_{k-1,i+1}-x_{k-1,i}\\|^{2}-\\displaystyle\\sum_{j=b(i-1)+1}^{b i}f_{\\pi_{j}^{(k)}}^{*}(y_{k}^{j})\\Big)}\\\\ &{\\quad+\\displaystyle\\frac{b}{2n\\eta_{k}}\\Big(\\|x_{k}-x_{*}\\|^{2}-\\|x_{k-1}-x_{*}\\|^{2}\\Big)+\\displaystyle\\frac{1}{2n}\\|y_{k}-y_{*,k}\\|_{\\mathbf{A}_{k}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, combining the bounds on $\\mathcal{L}(\\boldsymbol{x}_{k},\\boldsymbol{v})$ and $\\mathcal{L}(x_{*},y_{*})$ and letting ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}:=\\eta_{k}\\big(\\mathcal{L}(\\boldsymbol{x}_{k},\\boldsymbol{v})-\\mathcal{L}(\\boldsymbol{x}_{*},\\boldsymbol{y}_{*})\\big)+\\frac{b}{2n}\\|\\boldsymbol{x}_{k}-\\boldsymbol{x}_{*}\\|^{2}-\\frac{b}{2n}\\|\\boldsymbol{x}_{k-1}-\\boldsymbol{x}_{*}\\|^{2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}_{k}\\leq\\displaystyle\\frac{\\eta_{k}}{n}\\displaystyle\\sum_{i=1}^{m}\\Big\\langle E_{b}^{\\top}y_{k}^{(i)},x_{k-1,i}-x_{k-1,i+1}\\Big\\rangle+\\displaystyle\\frac{\\eta_{k}}{n}\\displaystyle\\sum_{i=1}^{m}\\Big\\langle E_{b}^{\\top}v_{k}^{(i)},x_{k}-x_{k-1,i}\\Big\\rangle}\\\\ &{\\qquad-\\displaystyle\\frac{\\eta_{k}}{2n}\\|y_{k}-v_{k}\\|_{\\boldsymbol{\\Lambda}_{k}^{-1}}^{2}-\\displaystyle\\frac{\\eta_{k}}{2n}\\|y_{k}-y_{*,k}\\|_{\\boldsymbol{\\Lambda}_{k}^{-1}}^{2}-\\displaystyle\\frac{b}{2n}\\displaystyle\\sum_{i=1}^{m}\\|x_{k-1,i+1}-x_{k-1,i}\\|^{2}}\\\\ &{=\\displaystyle\\frac{\\eta_{k}}{n}\\displaystyle\\sum_{i=1}^{m}\\Big\\langle E_{b}^{\\top}y_{k}^{(i)},x_{k}-x_{k-1,i+1}\\Big\\rangle+\\displaystyle\\frac{\\eta_{k}}{n}\\displaystyle\\sum_{i=1}^{m}\\Big\\langle E_{b}^{\\top}\\big(v_{k}^{(i)}-y_{k}^{(i)}\\big),x_{k}-x_{k-1,i}\\Big\\rangle}\\\\ &{\\qquad-\\displaystyle\\frac{\\eta_{k}}{2n}\\|y_{k}-v_{k}\\|_{\\boldsymbol{\\Lambda}_{k}^{-1}}^{2}-\\displaystyle\\frac{\\eta_{k}}{2n}\\|y_{k}-y_{*,k}\\|_{\\boldsymbol{\\Lambda}_{k}^{-1}}^{2}-\\displaystyle\\frac{b}{2n}\\displaystyle\\sum_{i=1}^{m}\\|x_{k-1,i+1}-x_{k-1,i}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "thus completing the proof. ", "page_idx": 18}, {"type": "text", "text": "We note that the frst iner product term $\\begin{array}{r}{\\mathcal{T}_{1}:=\\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}\\left\\langle E_{b}^{\\top}y_{k}^{(i)},x_{k}-x_{k-1,i+1}\\right\\rangle}\\end{array}$ in Eq. (5) can be caneld bythelas negaivetermn $\\begin{array}{r}{-\\frac{b}{2n}\\sum_{i=1}^{m}\\|\\pmb{x}_{k-1,i+1}-\\pmb{x}_{k-1,i}\\|^{2}}\\end{array}$ therein,as recisely provede in Lemma 10 of Appendix C. In the following subsections, we continue our analysis and handle the remaining terms in Eq. (5) according to different shuffling and derive the final complexity. ", "page_idx": 18}, {"type": "text", "text": "B.1   Random reshuffling/shuffle-once schemes ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We introduce the following lemma to bound the second inner product term $\\begin{array}{r l}{\\mathcal{T}_{2}}&{{}:=}\\end{array}$ $\\begin{array}{r}{\\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}\\Big\\langle E_{b}^{\\top}\\big({\\boldsymbol v}_{k}^{(i)}-{\\boldsymbol y}_{k}^{(i)}\\big),{\\boldsymbol x}_{k}-{\\boldsymbol x}_{k-1,i}\\Big\\rangle}\\end{array}$ in Lemma 6 when there are random permutations. Lemma7.UnderAssumptions $^{\\,l}$ and 2, for any $k\\in[K]$ the iterates $\\{y_{k}^{(i)}\\}_{i=1}^{m}$ and $\\{\\mathbfit{x}_{k-1,i}\\}_{i=1}^{m+1}$ generated by Algorithm withuniformlyrandomshuffing(RR/so)satisfy $\\mathbb{E}[\\mathcal{T}_{2}]\\leq\\mathbb{E}\\Big[\\frac{\\eta_{k}^{3}n\\hat{L}_{\\pi^{(k)}}^{g}\\tilde{L}_{\\pi^{(k)}}^{g}}{b^{2}}\\|y_{k}-y_{*,k}\\|_{\\Lambda_{k}^{-1}}^{2}+\\frac{\\eta_{k}}{2n}\\|v_{k}-y_{k}\\|_{\\Lambda_{k}^{-1}}^{2}\\Big]+\\frac{\\eta_{k}^{3}\\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2},$ where $\\begin{array}{r}{\\mathcal{T}_{2}:=\\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}\\Big\\langle E_{b}^{\\top}\\big(\\pmb{v}_{k}^{(i)}-\\pmb{y}_{k}^{(i)}\\big),\\pmb{x}_{k}-\\pmb{x}_{k-1,i}\\Big\\rangle.}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Proof. First note that $\\begin{array}{r l r}{{\\bf x}_{k}\\;-\\;{\\bf x}_{k-1,i}}&{\\!\\!=\\;\\;\\sum_{j=i}^{m}({\\bf x}_{k-1,j+1}\\;-\\;{\\pmb x}_{k-1,j})\\;\\;=\\;\\;-\\frac{\\eta_{k}}{b}\\sum_{j=i}^{m}E_{b}^{\\top}y_{k}^{(j)}}&{\\!\\!=\\!\\!\\!-\\!\\frac{1}{b}\\sum_{j=i}^{m}E_{b}^{\\top}y_{k}^{(j)}.}\\end{array}$ $-\\frac{\\eta_{k}}{b}E^{\\top}I_{b d(i-1)\\uparrow}y_{k}$ , so we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{\\eta_{k}}{n}\\underset{i=1}{\\overset{\\_}{\\_}{\\sum}}\\left\\langle E_{k}^{\\top}\\left(v_{k}^{(i)}-y_{k}^{(i)}\\right),x_{k}-x_{k-1,i}\\right\\rangle}\\\\ &{=\\frac{\\eta_{k}}{n}\\underset{i=1}{\\overset{\\_}{\\sum}}\\left\\langle E_{k}^{\\top}\\left(v_{k}^{(i)}-y_{k}^{(i)}\\right),\\underset{j=1}{\\overset{\\_}{\\sum}}\\left(x_{k-1,j+1}-x_{k-1,j}\\right)\\right\\rangle}\\\\ &{=-\\frac{\\eta_{k}^{2}}{n}\\underset{i=1}{\\overset{\\_}{\\sum}}\\left\\langle E^{\\top}I_{(i)}(v_{k}-y_{k}),E^{\\top}I_{\\mathrm{sf}}(\\mathrm{i}_{-1,j};y_{k})\\right\\rangle}\\\\ &{=-\\underbrace{\\frac{\\eta_{k}^{2}}{n}\\underset{i=1}{\\overset{\\_}{\\sum}}\\left\\langle E^{\\top}I_{(i)}(v_{k}-y_{k}),E^{\\top}I_{\\mathrm{sf}}(\\mathrm{i}_{-1,j};y_{k}-y_{k,i})\\right\\rangle}_{\\mathcal{I}_{1}}}\\\\ &{\\quad-\\underbrace{\\frac{\\eta_{k}^{2}}{n}\\underset{i=1}{\\overset{\\_}{\\sum}}\\left\\langle E^{\\top}I_{(i)}(v_{k}-y_{k}),E^{\\top}I_{\\mathrm{sf}}(\\mathrm{i}_{-1,j};y_{k},k)\\right\\rangle}_{\\mathcal{I}_{2}}}\\\\ &{\\quad\\quad\\underbrace{-\\frac{\\eta_{k}^{2}}{n}\\underset{i=1}{\\overset{\\_}{\\sum}}\\left\\langle E^{\\top}I_{(i)}(v_{k}-y_{k}),E^{\\top}I_{\\mathrm{sf}}(\\mathrm{i}_{-1,j};y_{k},k)\\right\\rangle}_{\\mathcal{I}_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the term ${\\mathcal{T}}_{1}$ , we use Young's inequality with $\\alpha>0$ to be set later and obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{Z}_{1}=\\,-\\,\\frac{\\eta_{k}^{2}}{b n}\\sum_{i=1}^{m}\\big\\langle\\boldsymbol{E}^{\\top}\\boldsymbol{I}_{(d i)}(\\boldsymbol{v}_{k}-\\boldsymbol{y}_{k}),\\boldsymbol{E}^{\\top}\\boldsymbol{I}_{b d(i-1)\\uparrow}(\\boldsymbol{y}_{k}-\\boldsymbol{y}_{*,k})\\big\\rangle}\\\\ {\\displaystyle\\leq\\frac{\\eta_{k}^{2}}{2b n\\alpha}\\sum_{i=1}^{m}\\|\\boldsymbol{E}^{\\top}\\boldsymbol{I}_{(d i)}(\\boldsymbol{v}_{k}-\\boldsymbol{y}_{k})\\|^{2}+\\frac{\\eta_{k}^{2}\\alpha}{2b n}\\sum_{i=1}^{m}\\|\\boldsymbol{E}^{\\top}\\boldsymbol{I}_{b d(i-1)\\uparrow}(\\boldsymbol{y}_{k}-\\boldsymbol{y}_{*,k})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Further, notice that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{\\eta_{\\mathrm{sh}}^{2}}{2m_{\\mathrm{i}}}\\biggl\\|E^{\\top}I_{\\mathbf{s}(i\\cdot1)^{\\top}}(y_{k}-y_{*,k})\\biggr\\|^{2}}\\\\ &{=\\frac{\\eta_{\\mathrm{sh}}^{2}}{2m_{\\mathrm{i}}}\\biggl[\\eta_{\\mathrm{sh}}-y_{*,k})^{\\top}I_{\\mathbf{s}(i\\cdot(1\\cdot1))^{\\top}}E E^{\\top}I_{\\mathbf{s}(i\\cdot(1\\cdot1))^{\\top}}(y_{k}-y_{*,k})}\\\\ &{=\\frac{\\eta_{\\mathrm{sh}}^{2}}{2m_{\\mathrm{i}}}(y_{k}-y_{*,k})^{\\top}\\biggl(\\underbrace{m}_{i=1}^{m}I_{\\mathbf{s}(i\\cdot1)\\top}E E^{\\top}I_{\\mathbf{s}(i\\cdot(1\\cdot1))^{\\top}}\\bigl)(y_{k}-y_{*,k})}\\\\ &{=\\frac{\\eta_{\\mathrm{sh}}^{2}}{2m_{\\mathrm{i}}}(y_{k}-y_{*,k})^{\\top}\\biggl(\\underbrace{m}_{i=1}^{m}I_{\\mathbf{s}(i\\cdot1)}\\underbrace{E^{\\top}I_{\\mathbf{s}(i\\cdot1)\\top}}_{\\mathrm{tad}}\\bigl(\\underbrace{m}_{i\\cdot1}^{m}+\\underbrace{m}_{j\\cdot k}\\bigr)}\\\\ &{=\\frac{\\eta_{\\mathrm{sh}}^{2}}{2m_{\\mathrm{i}}}(y_{k}-y_{*,k})^{\\top}\\mathbb{A}_{k}^{-1/2}\\underbrace{\\Lambda_{k}^{1/2}\\biggl(\\underbrace{m}_{i=1}^{m}I_{\\mathbf{s}(i\\cdot1)\\top}E E^{\\top}I_{\\mathbf{s}(i\\cdot1)\\top}\\bigg)\\mathbf{A}_{k}^{1/2}\\mathbf{A}_{k}^{-1/2}(y_{k}-y_{*,k})}_{\\mathrm{tad}}}\\\\ &{\\leq\\frac{\\eta_{\\mathrm{sh}}^{2}}{2m_{\\mathrm{i}}}\\biggl\\|\\mathbf{A}_{k}^{1/2}\\biggl(\\underbrace{m}_{\\mathrm{i}=1}^{m}I_{\\mathbf{s}(i\\cdot1)\\top}E\\mathbf E^{\\top}I_{\\mathbf{s}(i\\cdot1)\\top}\\biggl)\\mathbf{A}_{k}^{1/2}\\Bigl\\|\\mathbf{1}\\mathfrak{p} \n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where for the last inequality we use Cauchy-Schwarz inequality. Using the same argument, we can bound ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\eta_{k}^{2}}{2b n\\alpha}\\displaystyle\\sum_{i=1}^{m}\\|{\\cal E}^{\\top}I_{(d i)}(v_{k}-y_{k})\\|^{2}\\leq\\frac{\\eta_{k}^{2}}{2b n\\alpha}\\Big\\|\\displaystyle\\Lambda_{k}^{1/2}\\Big(\\displaystyle\\sum_{i=1}^{m}I_{(d i)}{\\cal E}{\\cal E}^{\\top}I_{(d i)}\\Big){\\bf A}_{k}^{1/2}\\Big\\|_{2}\\|v_{k}-y_{k}\\|_{\\boldsymbol\\Lambda_{k}^{-1}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{\\eta_{k}^{2}}{2n\\alpha}\\tilde{L}_{\\pi^{(k)}}^{g}\\|v_{k}-y_{k}\\|_{\\boldsymbol\\Lambda_{k}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, combining (6)-(8) and choosing $\\alpha=2\\eta_{k}\\tilde{L}_{\\pi^{(k)}}^{g}$ (k), we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{1}\\leq\\frac{\\eta_{k}^{3}m\\hat{L}_{\\pi^{(k)}}^{g}\\tilde{L}_{\\pi^{(k)}}^{g}}{b}\\|y_{k}-y_{*,k}\\|_{\\Lambda_{k}^{-1}}^{2}+\\frac{\\eta_{k}}{4n}\\|v_{k}-y_{k}\\|_{\\Lambda_{k}^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the term $\\mathcal{T}_{2}$ , we again apply Young's inequality with $\\beta>0$ to be set later and obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Z_{2}=\\displaystyle-\\frac{\\eta_{k}^{2}}{b n}\\sum_{i=1}^{m}\\big\\langle E^{\\top}I_{(d i)}(v_{k}-y_{k}),E^{\\top}I_{b d(i-1)\\uparrow}y_{*,k}\\big\\rangle}\\\\ &{\\quad\\le\\displaystyle\\frac{\\eta_{k}^{2}\\beta}{2b n}\\sum_{i=1}^{m}\\|E^{\\top}I_{b d(i-1)\\uparrow}y_{*,k}\\|^{2}+\\displaystyle\\frac{\\eta_{k}^{2}}{2b n\\beta}\\sum_{i=1}^{m}\\|E^{\\top}I_{(d i)}(v_{k}-y_{k})\\|^{2}}\\\\ &{\\quad\\le\\displaystyle\\frac{\\eta_{k}^{2}\\beta}{2b n}\\sum_{i=1}^{m}\\|E^{\\top}I_{b d(i-1)\\uparrow}y_{*,k}\\|^{2}+\\displaystyle\\frac{\\eta_{k}^{2}\\tilde{L}_{\\pi^{(k)}}^{g}}{2n\\beta}\\|v_{k}-y_{k}\\|_{\\Lambda_{k}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Choosing $\\beta=2\\eta_{k}\\tilde{L}^{g}$ and using the faethat $\\tilde{L}_{\\pi^{(k)}}^{g}\\leq\\tilde{L}^{g}$ ,we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{2}\\leq\\frac{\\eta_{k}^{3}\\tilde{L}^{g}}{b n}\\sum_{i=1}^{m}\\|E^{\\top}I_{b d(i-1)\\uparrow}y_{*,k}\\|^{2}+\\frac{\\eta_{k}}{4n}\\|v_{k}-y_{k}\\|_{\\Lambda_{k}^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, combining the above two estimates with $m=n/b$ ,wehave ", "page_idx": 20}, {"type": "equation", "text": "$$\nT_{2}\\leq\\frac{\\eta_{k}^{3}\\tilde{L}^{g}}{b n}\\sum_{i=1}^{m}\\|\\boldsymbol{E}^{\\top}\\boldsymbol{I}_{b d(i-1)\\uparrow}\\boldsymbol{y}_{*,k}\\|^{2}+\\frac{\\eta_{k}^{3}n\\hat{L}_{\\pi^{(k)}}^{g}\\tilde{L}_{\\pi^{(k)}}^{g}}{b^{2}}\\|\\boldsymbol{y}_{k}-\\boldsymbol{y}_{*,k}\\|_{\\boldsymbol{\\Lambda}_{k}^{-1}}^{2}+\\frac{\\eta_{k}}{2n}\\|\\boldsymbol{v}_{k}-\\boldsymbol{y}_{k}\\|_{\\boldsymbol{\\Lambda}_{k}^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "First, consider the RR scheme. Taking conditional expectation on both sides w.r.t. the randomness up to but not including $k$ -th epoch,wehave ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{k}[\\mathcal{T}_{2}]\\leq\\frac{\\eta_{k}^{3}\\tilde{L}^{g}}{b n}\\mathbb{E}_{k}\\Big[\\displaystyle\\sum_{i=1}^{m}\\|E^{\\top}I_{b d(i-1)\\uparrow}y_{*,k}\\|^{2}\\Big]}\\\\ &{\\qquad\\qquad+\\,\\mathbb{E}_{k}\\Big[\\frac{\\eta_{k}^{3}n\\hat{L}_{\\pi^{(k)}}^{g}\\tilde{L}_{\\pi^{(k)}}^{g}}{b^{2}}\\|y_{k}-y_{*,k}\\|_{\\Lambda_{k}^{-1}}^{2}+\\frac{\\eta_{k}}{2n}\\|v_{k}-y_{k}\\|_{\\Lambda_{k}^{-1}}^{2}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the first term, since the only randomness comes from the permutation $\\pi^{(k)}$ , we can proceed as in the proof of Lemma 11 and obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{\\eta_{k}^{3}\\tilde{L}^{\\varrho}}{b n}\\mathbb{E}_{k}\\bigg[\\displaystyle\\sum_{i=1}^{m}\\|E^{\\top}I_{b d(i-1)\\uparrow}y_{*,k}\\|^{2}\\bigg]\\stackrel{(i)}{=}\\displaystyle\\frac{\\eta_{k}^{3}\\tilde{L}^{\\varrho}}{b n}\\sum_{i=1}^{m}\\mathbb{E}_{\\pi^{(k)}}\\bigg[\\|E^{\\top}I_{b d(i-1)\\uparrow}y_{*,k}\\|^{2}\\bigg]}\\\\ {\\displaystyle}&{=\\displaystyle\\frac{\\eta_{k}^{3}\\tilde{L}^{\\varrho}}{b n}\\sum_{i=1}^{m}(n-b(i-1))^{2}\\mathbb{E}_{\\pi^{(k)}}\\bigg[\\Big\\|\\displaystyle\\frac{E^{\\top}I_{b d(i-1)\\uparrow}y_{*,k}}{n-b(i-1)}\\Big\\|^{2}\\bigg]}\\\\ {\\displaystyle}&{\\stackrel{(i i)}{\\le}\\displaystyle\\frac{\\eta_{k}^{3}\\tilde{L}^{\\varrho}}{b n}\\sum_{i=1}^{m}(n-b(i-1))^{2}\\displaystyle\\frac{b(i-1)}{(n-b(i-1))(n-1)}\\sigma_{*}^{2}}\\\\ &{=\\displaystyle\\frac{\\eta_{k}^{3}\\tilde{L}^{\\varrho}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we use the linearity of expectation for $(i)$ ,and $(i i)$ is due to Lemma 5 and the definition $\\begin{array}{r}{\\sigma_{*}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla f_{i}(\\pmb{x}_{*})\\|^{\\frac{\\sharp}{2}}=\\frac{1}{n}\\sum_{i=1}^{n}\\|\\pmb{y}_{*}^{i}\\|^{2}}\\end{array}$ Then taking expectation w.r.t.al randomness on both sides,weobtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathcal{T}_{2}]\\leq\\mathbb{E}\\Big[\\frac{\\eta_{k}^{3}n\\hat{L}_{\\pi^{(k)}}^{g}\\tilde{L}_{\\pi^{(k)}}^{g}}{b^{2}}\\|y_{k}-y_{*,k}\\|_{\\Lambda_{k}^{-1}}^{2}+\\frac{\\eta_{k}}{2n}\\|v_{k}-y_{k}\\|_{\\Lambda_{k}^{-1}}^{2}\\Big]+\\frac{\\eta_{k}^{3}\\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, we remark that the above argument  for  bounding  the  term $\\begin{array}{r}{\\frac{\\eta_{k}^{3}\\tilde{L}^{g}}{b n}\\mathbb{\\bar{E}}_{k}\\left[\\sum_{i=1}^{m}\\|\\pmb{{E}}^{\\top}\\pmb{{I}}_{b d(i-1)\\uparrow}\\pmb{{y}}_{\\ast,k}\\|^{2}\\right]}\\end{array}$ also applies to the SO scheme, in which case there is only one random permutation at the very beginning that induces the randomness. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "We state the final convergence rate and complexity in the following theorem and provide the proof for completeness. ", "page_idx": 20}, {"type": "text", "text": "Theorem 4. Under Assumptions $^{\\,l}$ and 2,if $\\begin{array}{r}{\\eta_{k}\\le\\frac{b}{n\\sqrt{2\\hat{L}_{\\pi^{(k)}}^{g}\\tilde{L}_{\\pi^{(k)}}^{g}}}}\\end{array}$ and $\\begin{array}{r}{H_{K}=\\sum_{k=1}^{K}\\eta_{k}}\\end{array}$ theoutput $\\hat{\\pmb{x}}_{K}$ ofAlgorithm $^{\\,I}$ with uniformly random (RR/SO) shuffing satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}[H_{K}(f(\\hat{{\\boldsymbol{x}}}_{K})-f({\\boldsymbol{x}}_{*}))]\\leq\\frac{b}{2n}\\|{\\boldsymbol{x}}_{0}-{\\boldsymbol{x}}_{*}\\|_{2}^{2}+\\sum_{k=1}^{K}\\frac{\\eta_{k}^{3}\\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As a consequence, for any $\\epsilon>0$ , there exists a choice of a constant step size $\\eta_{k}=\\eta$ for which $\\mathbb{E}[f(\\hat{\\mathbf{x}}_{K})\\,-\\,f(\\mathbf{x}_{*})]\\;\\le\\;\\epsilon$ afer $\\begin{array}{r}{\\mathcal{O}\\big(\\frac{n\\sqrt{\\hat{L}^{g}\\tilde{L}^{g}}\\|{\\bf{x}}_{0}-{\\bf x}_{*}\\|_{2}^{2}}{\\epsilon}+\\sqrt{\\frac{(n-b)(n+b)}{n(n-1)}}\\frac{\\sqrt{n\\tilde{L}^{g}}\\sigma_{*}\\|{\\bf{x}}_{0}-{\\bf x}_{*}\\|_{2}^{2}}{\\epsilon^{3/2}}\\big)}\\end{array}$ gradient queries. ", "page_idx": 21}, {"type": "text", "text": "Proof. Combining the bounds in Lemma 10 and 2 and plugging them into Eq. (5), we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathcal{E}_{k}]\\leq\\mathbb{E}\\Big[\\Big(\\frac{\\eta_{k}^{3}n\\hat{L}_{\\pi^{(k)}}^{g}\\tilde{L}_{\\pi^{(k)}}^{g}}{b^{2}}-\\frac{\\eta_{k}}{2n}\\Big)\\|y_{k}-y_{*,k}\\|_{\\Lambda_{k}^{-1}}^{2}\\Big]+\\frac{\\eta_{k}^{3}\\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the stepsize $\\eta_{k}$ such that $\\begin{array}{r}{\\eta_{k}\\leq\\frac{b}{n\\sqrt{2\\hat{L}_{\\pi^{(k)}}^{g}\\tilde{L}_{\\pi^{(k)}}^{g}}}}\\end{array}$ ,wehave $\\begin{array}{r}{\\frac{\\eta_{k}^{3}n\\hat{L}_{\\pi^{(k)}}^{g}\\tilde{L}_{\\pi^{(k)}}^{g}}{b^{2}}-\\frac{\\eta_{k}}{2n}\\le0}\\end{array}$ , thus ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathcal{E}_{k}]\\leq\\frac{\\eta_{k}^{3}\\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using our definition of $\\mathcal{E}_{k}$ and telescoping from $k=1$ to $K$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\sum_{k=1}^{K}\\eta_{k}\\mathrm{Gap}^{v}(\\boldsymbol{x}_{k},y_{*})\\Big]\\leq\\frac{b}{2n}\\|\\boldsymbol{x}_{*}-\\boldsymbol{x}_{0}\\|_{2}^{2}-\\frac{b}{2n}\\mathbb{E}[\\|\\boldsymbol{x}_{*}-\\boldsymbol{x}_{K}\\|_{2}^{2}]+\\sum_{k=1}^{K}\\frac{\\eta_{k}^{3}\\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Noticing   that $\\mathcal{L}(\\boldsymbol{\\mathbf{\\boldsymbol{x}}},\\boldsymbol{\\mathbf{\\boldsymbol{v}}})$ is  convex  in $\\textbf{\\em x}$ for afixed $\\pmb{v}$ \uff0cwe have $\\begin{array}{r l}{\\mathrm{Gap}^{v}(\\hat{\\mathbf{x}}_{K},y_{*})}&{{}\\leq}\\end{array}$ $\\begin{array}{r}{\\sum_{k=1}^{K}\\eta_{k}\\mathbf{Gap}^{v}(\\pmb{x}_{k},\\pmb{y}_{*})/H_{K}}\\end{array}$ , where $\\begin{array}{r}{\\hat{\\pmb{x}}_{K}\\,=\\,\\sum_{k=1}^{K}{\\eta_{k}\\pmb{x}_{k}/H_{K}}}\\end{array}$ and $\\begin{array}{r}{H_{K}\\,=\\,\\sum_{k=1}^{K}\\eta_{k}}\\end{array}$ , which leads to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[H_{K}\\mathsf{G a p}^{v}(\\hat{x}_{K},y_{*})\\Big]\\leq\\frac{b}{2n}\\|x_{0}-x_{*}\\|_{2}^{2}+\\sum_{k=1}^{K}\\frac{\\eta_{k}^{3}\\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Further choosing ${\\pmb v}={\\pmb y}_{\\hat{\\pmb x}_{K}}$ , we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}[H_{K}\\big(f(\\hat{x}_{K})-f(x_{*})\\big)]\\leq\\frac{b}{2n}\\|x_{0}-x_{*}\\|_{2}^{2}+\\sum_{k=1}^{K}\\frac{\\eta_{k}^{3}\\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To analyze the individual gradient oracle complexity, we choose constant stepsizes $\\eta\\leq\\frac{b}{n\\sqrt{2\\hat{L}^{g}\\tilde{L}^{g}}}$ then Eq. (9) will become ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(\\hat{x}_{K})-f(\\pmb{x}_{*})]\\le\\frac{b}{2n\\eta K}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}+\\frac{\\eta^{2}\\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Without loss of generality, we assume that $b\\neq n$ , otherwise the method and its analysis reduce to (full) gradient descent. We consider the following two cases: ", "page_idx": 21}, {"type": "text", "text": ".\u201cSmall $K^{\\bullet}$ caseif $\\begin{array}{r}{\\eta=\\frac{b}{n\\sqrt{2\\hat{L}^{g}\\tilde{L}^{g}}}\\le\\Big(\\frac{3b^{3}(n-1)\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{n(n-b)(n+b)\\tilde{L}^{g}K\\sigma_{*}^{2}}\\Big)^{1/3},\\mathrm{we~have}}\\end{array}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}[f(\\hat{\\mathbf{x}}_{K})-f(\\mathbf{x}_{*})]}\\\\ &{\\le\\displaystyle\\frac{b}{2n\\eta K}\\|\\mathbf{x}_{0}-\\mathbf{x}_{*}\\|_{2}^{2}+\\frac{\\eta^{2}\\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}}\\\\ &{\\le\\displaystyle\\frac{\\sqrt{\\hat{L}^{g}\\tilde{L}^{g}}}{\\sqrt{2}K}\\|\\mathbf{x}_{0}-\\mathbf{x}_{*}\\|_{2}^{2}+\\frac{1}{2}\\Big(\\frac{(n-b)(n+b)}{n^{2}(n-1)}\\Big)^{1/3}\\frac{(\\tilde{L}^{g})^{1/3}\\sigma_{*}^{2/3}\\|\\mathbf{x}_{0}-\\mathbf{x}_{*}\\|_{2}^{4/3}}{3^{1/3}K^{2/3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "\u00b7\u201cLarge $K^{\\bullet}$ case: if $\\begin{array}{r}{\\eta=\\left(\\frac{3b^{3}(n-1)\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{n(n-b)(n+b)\\tilde{L}^{g}K\\sigma_{*}^{2}}\\right)^{1/3}\\leq\\frac{b}{n\\sqrt{2\\hat{L}^{g}\\tilde{L}^{g}}}}\\end{array}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[f(\\hat{\\pmb{x}}_{K})-f(\\pmb{x}_{*})]\\leq\\frac{b}{2n\\eta K}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}+\\frac{\\eta^{2}\\tilde{L}^{g}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\Big(\\frac{(n-b)(n+b)}{n^{2}(n-1)}\\Big)^{1/3}\\frac{(\\tilde{L}^{g})^{1/3}\\sigma_{*}^{2/3}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{4/3}}{3^{1/3}K^{2/3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining these two cases by setting $\\begin{array}{r}{\\eta=\\operatorname*{min}\\Big\\{\\frac{b}{n\\sqrt{2\\hat{L}^{g}\\tilde{L}^{g}}},\\,\\Big(\\frac{3b^{3}(n-1)\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{n(n-b)(n+b)\\tilde{L}^{g}K\\sigma_{*}^{2}}\\Big)^{1/3}\\Big\\}.}\\end{array}$ we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(\\hat{x}_{K})-f(x_{*})]\\leq\\frac{\\sqrt{\\hat{L}^{g}\\tilde{L}^{g}}}{\\sqrt{2K}}\\|x_{0}-x_{*}\\|_{2}^{2}+\\Big(\\frac{(n-b)(n+b)}{n^{2}(n-1)}\\Big)^{1/3}\\frac{(\\tilde{L}^{g})^{1/3}\\sigma_{*}^{2/3}\\|x_{0}-x_{*}\\|_{2}^{4/3}}{3^{1/3}K^{2/3}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, to guarantee $\\mathbb{E}[f(\\hat{\\mathbf{x}}_{K})\\,-\\,f(\\mathbf{x}_{*})]\\,\\le\\,\\epsilon$ for $\\epsilon\\,>\\,0$ , the total number of individual gradient evaluations will be ", "page_idx": 22}, {"type": "equation", "text": "$$\nn K\\geq\\operatorname*{max}\\Big\\{\\frac{n\\sqrt{2\\hat{L}^{g}\\tilde{L}^{g}}\\|x_{0}-x_{*}\\|_{2}^{2}}{\\epsilon},\\Big(\\frac{(n-b)(n+b)}{n-1}\\Big)^{1/2}\\frac{2^{3/2}(\\tilde{L}^{g})^{1/2}\\sigma_{*}\\|x_{0}-x_{*}\\|_{2}^{2}}{3^{1/2}\\epsilon^{3/2}}\\Big\\},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "as claimed. ", "page_idx": 22}, {"type": "text", "text": "B.2  Incremental gradient descent (IG) ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this subsection, we provide the convergence results for incremental gradient descent which does not involve random permutations. We first prove the technical lemma below to bound the term $\\begin{array}{r}{\\mathcal{T}_{2}:=\\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}\\Big\\langle E_{b}^{\\top}\\big(\\pmb{v}_{k}^{(i)}-\\pmb{y}_{k}^{(i)}\\big),\\pmb{x}_{k}-\\pmb{x}_{k-1,i}\\Big\\rangle}\\end{array}$ in Eq (5) of Lemma 6. ", "page_idx": 22}, {"type": "text", "text": "Lemma 8. For any $k\\in[K]$ theiterates $\\{y_{k}^{(i)}\\}_{i=1}^{m}$ and $\\{\\pmb{x}_{k-1,i}\\}_{i=1}^{m+1}$ generatedbyAlgorinm 1 wih fixed data ordering satisfy ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{2}\\leq\\displaystyle\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\|y_{k}-y_{*}\\|_{\\mathbf{A}^{-1}}^{2}+\\displaystyle\\frac{\\eta_{k}}{2n}\\|v-y_{k}\\|_{\\mathbf{A}^{-1}}^{2}}\\\\ &{\\qquad+\\operatorname*{min}\\Big\\{\\displaystyle\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\|y_{*}\\|_{\\mathbf{A}^{-1}}^{2},\\displaystyle\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}^{g}\\sigma_{*}^{2}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Proceeding as in the proof of Lemma 7, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}\\biggl\\langle E_{b}^{\\top}\\bigl(\\mathbf{v}^{(i)}-\\mathbf{y}_{k}^{(i)}\\bigr),\\mathbf{x}_{k}-\\mathbf{x}_{k-1,i}\\biggr\\rangle}\\\\ &{=\\displaystyle\\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}\\biggl\\langle E_{b}^{\\top}\\bigl(\\mathbf{v}^{(i)}-\\mathbf{y}_{k}^{(i)}\\bigr),\\sum_{j=i}^{m}\\bigl(\\mathbf{x}_{k-1,j+1}-\\mathbf{x}_{k-1,j}\\bigr)\\biggr\\rangle}\\\\ &{=\\displaystyle-\\frac{\\eta_{k}^{2}}{b n}\\sum_{i=1}^{m}\\bigl\\langle\\mathbf{E}^{\\top}I_{(d i)}(v-y_{k}),\\mathbf{E}^{\\top}I_{b d(i-1)\\uparrow}y_{k}\\bigr\\rangle}\\\\ &{=\\displaystyle-\\frac{\\eta_{k}^{2}}{b n}\\sum_{i=1}^{m}\\bigl\\langle\\mathbf{E}^{\\top}I_{(d i)}(v-y_{k}),\\mathbf{E}^{\\top}I_{b d(i-1)\\uparrow}(y_{k}-y_{*})\\bigr\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n-\\frac{\\eta_{k}^{2}}{b n}\\sum_{i=1}^{m}\\left\\langle\\boldsymbol{E}^{\\top}\\boldsymbol{I}_{(d i)}(v-y_{k}),\\boldsymbol{E}^{\\top}\\boldsymbol{I}_{b d(i-1)\\uparrow}\\boldsymbol{y}_{\\ast}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For both terms ${\\mathcal{T}}_{1}$ and $\\mathcal{T}_{2}$ , we apply Young's inequality with $\\alpha=2\\eta_{k}\\tilde{L}_{0}^{g}$ and obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathcal{T}_{1}\\leq\\frac{\\eta_{k}^{2}\\alpha}{2b n}\\sum_{i=1}^{m}\\|{\\cal E}^{\\top}I_{b d(i-1)\\uparrow}(y_{k}-y_{*})\\|_{2}^{2}+\\frac{\\eta_{k}^{2}}{2b n\\alpha}\\sum_{i=1}^{m}\\|{\\cal E}^{\\top}I_{(d i)}(v-y_{k})\\|_{2}^{2}}}\\\\ {{\\displaystyle\\quad\\leq\\frac{\\eta_{k}^{2}n\\alpha}{2b^{2}}\\hat{L}_{0}^{g}\\|y_{k}-y_{*}\\|_{\\Lambda^{-1}}^{2}+\\frac{\\eta_{k}^{2}}{2n\\alpha}\\tilde{L}_{0}^{g}\\|v-y_{k}\\|_{\\Lambda^{-1}}^{2}}}\\\\ {{\\displaystyle\\quad=\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\|y_{k}-y_{*}\\|_{\\Lambda^{-1}}^{2}+\\frac{\\eta_{k}}{4n}\\|v-y_{k}\\|_{\\Lambda^{-1}}^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{Z}_{2}\\leq\\displaystyle\\frac{\\eta_{k}^{2}\\alpha}{2b n}\\sum_{i=1}^{m}\\|E^{\\top}I_{b d(i-1)\\uparrow}y_{*}\\|_{2}^{2}+\\displaystyle\\frac{\\eta_{k}^{2}}{2b n\\alpha}\\sum_{i=1}^{m}\\|E^{\\top}I_{(d i)}(v-y_{k})\\|_{2}^{2}}\\\\ &{\\quad\\leq\\displaystyle\\frac{\\eta_{k}^{2}\\alpha}{2b n}\\sum_{i=1}^{m}\\|E^{\\top}I_{b d(i-1)\\uparrow}y_{*}\\|_{2}^{2}+\\displaystyle\\frac{\\eta_{k}^{2}}{2n\\alpha}\\tilde{L}_{0}^{g}\\|v-y_{k}\\|_{\\Lambda^{-1}}^{2}}\\\\ &{\\quad=\\displaystyle\\frac{\\eta_{k}^{3}\\tilde{L}_{0}^{g}}{n b}\\sum_{i=1}^{m}\\|E^{\\top}I_{b d(i-1)\\uparrow}y_{*}\\|_{2}^{2}+\\displaystyle\\frac{\\eta_{k}}{4n}\\|v-y_{k}\\|_{\\Lambda^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We now show that the term m $\\begin{array}{r}{\\frac{\\eta_{k}^{3}\\tilde{L}_{0}^{g}}{n b}\\sum_{i=1}^{m}\\|\\boldsymbol{E}^{\\top}\\boldsymbol{I}_{b d(i-1)\\uparrow}\\boldsymbol{y}_{\\ast}\\|_{2}^{2}}\\end{array}$ $\\begin{array}{r}{\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\|y_{*}\\|_{\\Lambda^{-1}}^{2}}\\end{array}$ $\\frac{\\eta_{k}^{3}(n\\!-\\!b)^{2}}{b^{2}}\\tilde{L}_{0}^{g}\\sigma_{*}^{2}$ Thisis ivial hen $b=n$ $\\begin{array}{r}{E^{\\top}I_{0\\uparrow}y_{*}=\\sum_{i=1}^{n}y_{*}^{i}=\\mathbf{0}}\\end{array}$ When $b<n$ to Show the former one, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=1}^{m}\\|\\boldsymbol{E}^{\\top}\\boldsymbol{I}_{b d(i-1)\\uparrow}\\boldsymbol{y}_{*}\\|_{2}^{2}\\leq\\Big\\|\\boldsymbol{\\Lambda}^{1/2}\\Big(\\displaystyle\\sum_{i=1}^{m}\\boldsymbol{I}_{b d(i-1)\\uparrow}\\boldsymbol{E}\\boldsymbol{E}^{\\top}\\boldsymbol{I}_{b d(i-1)\\uparrow}\\Big)\\boldsymbol{\\Lambda}^{1/2}\\Big\\|_{2}\\|\\boldsymbol{y}_{*}\\|_{\\boldsymbol{\\Lambda}^{-1}}^{2}}\\\\ {\\displaystyle=m n\\hat{L}_{0}^{g}\\|\\boldsymbol{y}_{*}\\|_{\\boldsymbol{\\Lambda}^{-1}}^{2}=\\displaystyle\\frac{n^{2}}{b}\\hat{L}_{0}^{g}\\|\\boldsymbol{y}_{*}\\|_{\\boldsymbol{\\Lambda}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "To prove the latter one, we notice that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{m}\\|E^{\\top}I_{b d(i-1)\\uparrow}y_{*}\\|_{2}^{2}=\\displaystyle\\sum_{i=1}^{m}\\Big\\|\\displaystyle\\sum_{j=b(i-1)+1}^{n}y_{*}^{j}\\Big\\|_{2}^{2}=\\displaystyle\\sum_{i=0}^{m-1}\\Big\\|\\displaystyle\\sum_{j=b i+1}^{n}y_{*}^{j}\\Big\\|_{2}^{2}=\\displaystyle\\sum_{i=1}^{m-1}\\Big\\|\\displaystyle\\sum_{j=b i+1}^{n}y_{*}^{j}\\Big\\|_{2}^{2}}\\\\ &{\\displaystyle=\\displaystyle\\sum_{i=1}^{m-1}\\Big\\|\\displaystyle\\sum_{j=1}^{b i}y_{*}^{j}\\Big\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "using the fact that $\\textstyle\\sum_{i=1}^{n}y_{*}^{i}=\\mathbf{0}$ . Then using Young's inequality we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=1}^{m-1}\\left\\|\\displaystyle\\sum_{j=1}^{b i}y_{*}^{j}\\right\\|_{2}^{2}\\leq\\displaystyle\\sum_{i=1}^{m-1}b i\\displaystyle\\sum_{j=1}^{b i}\\|y_{*}^{j}\\|_{2}^{2}}\\\\ {\\displaystyle\\leq b(m-1)\\sum_{i=1}^{m-1}\\sum_{j=1}^{b i}\\|y_{*}^{j}\\|_{2}^{2}}\\\\ {\\displaystyle=b(m-1)\\sum_{i=1}^{m-1}\\sum_{j=b(i-1)+1}^{b i}(m-i)\\|y_{*}^{j}\\|_{2}^{2}}\\\\ {\\displaystyle\\leq b(m-1)^{2}\\sum_{i=1}^{b}\\|y_{*}^{j}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Further noticing that $\\begin{array}{r}{\\sum_{i=1}^{(m-1)b}\\|\\pmb{{y}}_{*}^{i}\\|_{2}^{2}\\leq\\sum_{i=1}^{n}\\|\\pmb{{y}}_{*}^{i}\\|^{2}=n\\sigma_{*}^{2}}\\end{array}$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\eta_{k}^{3}\\tilde{L}_{0}^{g}}{n b}\\sum_{i=1}^{m}\\|\\boldsymbol{E}^{\\top}\\boldsymbol{I}_{b d(i-1)\\uparrow}\\boldsymbol{y}_{*}\\|_{2}^{2}\\leq\\frac{\\eta_{k}^{3}\\tilde{L}_{0}^{g}}{n b}b(m-1)^{2}n\\sigma_{*}^{2}=\\frac{\\eta_{k}^{3}\\tilde{L}_{0}^{g}(n-b)^{2}}{b^{2}}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The same bound also captures the case $b=n$ and leads to ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{\\eta_{k}^{3}\\tilde{L}_{0}^{g}}{n b}\\sum_{i=1}^{m}\\|E^{\\top}I_{b d(i-1)\\uparrow}y_{*}\\|_{2}^{2}\\leq\\operatorname*{min}\\Big\\{\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\|y_{*}\\|_{\\Lambda^{-1}}^{2},\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}^{g}\\sigma_{*}^{2}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, combining Eq. (11)-(13), we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\mathcal{Z}_{2}\\leq\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\|y_{k}-y_{*}\\|_{\\Lambda^{-1}}^{2}+\\displaystyle\\frac{\\eta_{k}}{2n}\\|v-y_{k}\\|_{\\Lambda^{-1}}^{2}}}\\\\ {{+\\operatorname*{min}\\Big\\{\\displaystyle\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\|y_{*}\\|_{\\Lambda^{-1}}^{2},\\displaystyle\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}^{g}\\sigma_{*}^{2}\\Big\\},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which finishes the proof. ", "page_idx": 24}, {"type": "text", "text": "We are now ready to state our convergence results for IGD in the following theorem, with its proof provided for completeness. ", "page_idx": 24}, {"type": "text", "text": "Theorem 5. Under Assumptions $^{\\,l}$ and 2,ij $\\begin{array}{r}{\\dot{\\boldsymbol{\\eta}}_{k}\\leq\\frac{b}{n\\sqrt{2\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}}}}\\end{array}$ and $\\begin{array}{r}{H_{K}=\\sum_{k=1}^{K}\\eta_{k}}\\end{array}$ the output $\\hat{\\pmb{x}}_{K}$ of Algorithm $^{\\,l}$ with a fixed permutation satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\nH_{K}\\big(f(\\hat{x}_{K})-f(x_{*})\\big)\\leq\\frac{b}{2n}\\|x_{0}-x_{*}\\|_{2}^{2}+\\sum_{k=1}^{K}\\operatorname*{min}\\Big\\{\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\|y_{*}\\|_{\\Lambda^{-1}}^{2},\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}^{g}\\sigma_{*}^{2}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "As a consequence, for any $\\epsilon>0$ , there exists a choice of a constant step size $\\eta_{k}=\\eta$ such that $f(\\hat{\\mathbf{x}}_{K})-f(\\mathbf{x}_{*})\\,\\leq\\,\\epsilon$ after $\\begin{array}{r}{\\mathcal{O}\\Big(\\frac{n\\sqrt{\\widehat{L}_{0}^{g}\\widehat{L}_{0}^{g}}||x_{0}-x_{*}||_{2}^{2}}{\\epsilon}+\\frac{\\operatorname*{min}\\big\\{\\sqrt{n\\widehat{L}_{0}^{g}\\widehat{L}_{0}^{g}}||y_{*}||_{\\Lambda^{-1}},(n-b)\\sqrt{\\widehat{L}_{0}^{g}}\\sigma_{*}\\big\\}||x_{0}-x_{*}||_{2}^{2}}{\\epsilon^{3/2}}\\Big)}\\end{array}$ gradient queries. ", "page_idx": 24}, {"type": "text", "text": "Proof. Combining the bounds in Lemma 10 and 8 and plugging them into Eq. (5) in Lemma 6 without random permutations, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}\\leq\\Big(\\frac{\\eta_{k}^{3}n\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}}{b^{2}}-\\frac{\\eta_{k}}{2n}\\Big)\\|y_{k}-y_{*}\\|_{\\Lambda^{-1}}^{2}+\\operatorname*{min}\\Big\\{\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\|y_{*}\\|_{\\Lambda^{-1}}^{2},\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}^{g}\\sigma_{*}^{2}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "f $\\begin{array}{r}{\\eta_{k}\\leq\\frac{b}{n\\sqrt{2\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}}}}\\end{array}$ ,wehave $\\begin{array}{r}{\\frac{\\eta_{k}^{3}n\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}}{b^{2}}-\\frac{\\eta_{k}}{2n}\\le0}\\end{array}$ , thus ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}\\leq\\operatorname*{min}\\Big\\{\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\|y_{*}\\|_{\\Lambda^{-1}}^{2},\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}^{g}\\sigma_{*}^{2}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Using the definition of $\\mathcal{E}_{k}$ and telescoping from $k=1$ to $K$ , we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l\\ }{\\displaystyle\\sum_{k=1}^{K}\\eta_{k}\\mathrm{Gap}^{v}(\\mathbf{x}_{k},\\boldsymbol{y}_{*})\\leq\\displaystyle\\frac{b}{2n}\\|\\boldsymbol{x}_{*}-\\boldsymbol{x}_{0}\\|_{2}^{2}-\\displaystyle\\frac{b}{2n}\\|\\boldsymbol{x}_{*}-\\boldsymbol{x}_{K}\\|_{2}^{2}}&{}\\\\ {\\displaystyle\\qquad\\qquad\\qquad+\\displaystyle\\sum_{k=1}^{K}\\operatorname*{min}\\Big\\{\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\|\\boldsymbol{y}_{*}\\|_{\\boldsymbol{\\Lambda}^{-1}}^{2},\\displaystyle\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}^{g}\\sigma_{*}^{2}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Noticing that $\\mathcal{L}(\\boldsymbol{\\mathbf{\\boldsymbol{x}}},\\boldsymbol{\\mathbf{\\boldsymbol{v}}})$ is convex w.r.t. $\\textbf{\\em x}$ , we have $\\begin{array}{r}{\\mathrm{Gap}^{v}(\\hat{x}_{K},y_{*})\\leq\\sum_{k=1}^{K}\\eta_{k}\\mathrm{Gap}^{v}(x_{k},y_{*})/H_{K},}\\end{array}$ where $\\begin{array}{r}{\\hat{\\pmb{x}}_{K}=\\sum_{k=1}^{K}{\\eta_{k}\\pmb{x}_{k}/H_{K}}}\\end{array}$ and $\\begin{array}{r}{H_{K}=\\sum_{k=1}^{K}\\eta_{k}}\\end{array}$ , so we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\nH_{K}\\mathrm{Gap}^{v}(\\hat{x}_{K},y_{*})\\leq\\frac{b}{2n}\\|\\boldsymbol{x}_{0}-\\boldsymbol{x}_{*}\\|_{2}^{2}+\\sum_{k=1}^{K}\\operatorname*{min}\\Big\\{\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\|y_{*}\\|_{\\boldsymbol{\\Lambda}^{-1}}^{2},\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}^{g}\\sigma_{*}^{2}\\Big\\},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Further choosing ${\\pmb v}={\\pmb y}_{\\hat{\\pmb x}_{K}}$ , we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\nH_{K}\\big(f(\\hat{x}_{K})-f(x_{*})\\big)\\leq\\frac{b}{2n}\\|x_{0}-x_{*}\\|_{2}^{2}+\\sum_{k=1}^{K}\\operatorname*{min}\\Big\\{\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\|y_{*}\\|_{\\Lambda^{-1}}^{2},\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}^{g}\\sigma_{*}^{2}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To analyze the individual gradient oracle complexity, we choose constant stepsizes $\\begin{array}{r}{\\eta\\leq\\frac{b}{n\\sqrt{2\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}}}}\\end{array}$ and assume $b<n$ without loss of generality, then Eq. (14) becomes ", "page_idx": 25}, {"type": "equation", "text": "$$\nf(\\hat{x}_{K})-f(x_{*})\\leq\\frac{b}{2n\\eta K}\\|x_{0}-x_{*}\\|_{2}^{2}+\\operatorname*{min}\\Big\\{\\frac{\\eta^{2}n}{b^{2}}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\|y_{*}\\|_{\\mathrm{A}^{-1}}^{2},\\frac{\\eta^{2}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}^{g}\\sigma_{*}^{2}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "When $\\begin{array}{r}{\\hat{L}_{0}^{g}\\lVert y_{*}\\rVert_{{\\mathbf{A}}^{-1}}^{2}\\,\\le\\,\\frac{(n-b)^{2}}{n}\\sigma_{*}^{2}}\\end{array}$ we set $\\begin{array}{r}{\\eta=\\operatorname*{min}\\Big\\{\\frac{b}{n\\sqrt{2\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}}},\\,\\left(\\frac{b^{3}\\|{\\pmb x}_{0}-{\\pmb x}_{*}\\|_{2}^{2}}{2n^{2}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}K\\|{\\pmb y}_{*}\\|_{\\Lambda^{-1}}^{2}}\\right)^{1/3}\\Big\\}}\\end{array}$ and consider the following two possible cases: ", "page_idx": 25}, {"type": "text", "text": ".\u201cSmall $K^{\\bullet}$ case: if $\\begin{array}{r}{\\eta=\\frac{b}{n\\sqrt{2\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}}}\\le\\left(\\frac{b^{3}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{2n^{2}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}K\\|\\pmb{y}_{*}\\|_{\\Lambda^{-1}}^{2}}\\right)^{1/3}}\\end{array}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\hat{\\pmb{x}}_{K})-f(\\pmb{x}_{*})\\leq\\frac{b}{2n\\eta K}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}+\\frac{\\eta^{2}n}{b^{2}}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\|\\pmb{y}_{*}\\|_{\\mathbf{\\Lambda}^{2-1}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\sqrt{\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}}}{\\sqrt{2}K}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}+\\frac{\\left(\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\right)^{1/3}\\|\\pmb{y}_{*}\\|_{\\mathbf{\\Lambda}^{2/3}}^{2/3}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{4/3}}{2^{2/3}n^{1/3}K^{2/3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "\u00b7\u201cLarge $K^{\\bullet}$ case: if $\\begin{array}{r}{\\eta=\\left(\\frac{b^{3}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{2n^{2}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}K\\|\\pmb{y}_{*}\\|_{\\Lambda^{-1}}^{2}}\\right)^{1/3}\\leq\\frac{b}{\\sqrt{2\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}}}}\\end{array}$ Vaigig, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\hat{\\pmb{x}}_{K})-f(\\pmb{x}_{*})\\leq\\frac{b}{2n\\eta K}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}+\\frac{\\eta^{2}n}{b^{2}}\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\|\\pmb{y}_{*}\\|_{\\pmb{\\Lambda}^{-1}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{2^{1/3}\\left(\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\right)^{1/3}\\|\\pmb{y}_{*}\\|_{\\pmb{\\Lambda}^{-1}}^{2/3}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{4/3}}{n^{1/3}K^{2/3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining these two cases, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nf(\\hat{x}_{K})-f(x_{*})\\leq\\frac{\\sqrt{\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}}}{\\sqrt{2}K}\\|x_{0}-x_{*}\\|_{2}^{2}+\\frac{2^{1/3}\\big(\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\big)^{1/3}\\|y_{*}\\|_{\\Lambda^{-1}}^{2/3}\\|x_{0}-x_{*}\\|_{2}^{4/3}}{n^{1/3}K^{2/3}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hence, to guarantee $\\mathbb{E}[f(\\hat{\\mathbf{x}}_{K})\\,-\\,f(\\mathbf{x}_{*})]\\,\\le\\,\\epsilon$ for $\\epsilon\\,>\\,0$ , the total number of required individual gradient evaluations will be ", "page_idx": 25}, {"type": "equation", "text": "$$\nn K\\ge\\operatorname*{max}\\Big\\{\\frac{n\\sqrt{2\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}}\\|x_{0}-x_{*}\\|_{2}^{2}}{\\epsilon},\\frac{4n^{1/2}\\big(\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\big)^{1/2}\\|y_{*}\\|_{\\Lambda^{-1}}\\|x_{0}-x_{*}\\|_{2}^{2}}{\\epsilon^{3/2}}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "When $\\begin{array}{r}{\\frac{(n-b)^{2}}{n}\\sigma_{*}^{2}\\leq\\hat{L}_{0}^{g}\\|y_{*}\\|_{\\Lambda^{-1}}^{2}}\\end{array}$ we set $\\begin{array}{r}{\\eta=\\operatorname*{min}\\left\\{\\frac{b}{n\\sqrt{2\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}}}\\right.}\\end{array}$ $\\left(\\frac{b^{3}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{2n(n-b)^{2}\\tilde{L}_{0}^{g}K\\sigma_{*}^{2}}\\right)^{1/3}\\right\\}$ and consider the two cases as below: ", "page_idx": 25}, {"type": "text", "text": "\u00b7\u201cSmall $K^{\\bullet}$ case: if $\\begin{array}{r}{\\eta=\\frac{b}{n\\sqrt{2\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}}}\\le\\left(\\frac{b^{3}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{2n(n-b)^{2}\\tilde{L}_{0}^{g}K\\sigma_{*}^{2}}\\right)^{1/3}}\\end{array}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\hat{\\boldsymbol x}_{K})-f(\\boldsymbol x_{*})\\le\\cfrac{b}{2n\\eta K}\\|\\boldsymbol x_{0}-\\boldsymbol x_{*}\\|_{2}^{2}+\\cfrac{\\eta^{2}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}^{g}\\sigma_{*}^{2}}\\\\ &{\\qquad\\qquad\\le\\cfrac{\\sqrt{\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}}}{\\sqrt{2}K}\\|\\boldsymbol x_{0}-\\boldsymbol x_{*}\\|_{2}^{2}+\\cfrac{(n-b)^{2/3}(\\tilde{L}^{g})^{1/3}\\sigma_{*}^{2/3}\\|\\boldsymbol x_{0}-\\boldsymbol x_{*}\\|_{2}^{4/3}}{2^{2/3}n^{2/3}K^{2/3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "\u00b7\u201cLarge $K^{\\bullet}$ case: if $\\begin{array}{r}{\\eta=\\left(\\frac{b^{3}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{2n(n-b)^{2}\\tilde{L}_{0}^{g}K\\sigma_{*}^{2}}\\right)^{1/3}\\leq\\frac{b}{n\\sqrt{2\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}}}}\\end{array}$ /2igLg, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\hat{{\\boldsymbol x}}_{K})-f({\\boldsymbol x}_{*})\\le\\displaystyle\\frac{b}{2n\\eta K}\\|{\\boldsymbol x}_{0}-{\\boldsymbol x}_{*}\\|_{2}^{2}+\\displaystyle\\frac{\\eta^{2}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}^{g}\\sigma_{*}^{2}}\\\\ &{\\qquad\\qquad\\le\\displaystyle\\frac{2^{1/3}(n-b)^{2/3}(\\tilde{L}^{g})^{1/3}\\sigma_{*}^{2/3}\\|{\\boldsymbol x}_{0}-{\\boldsymbol x}_{*}\\|_{2}^{4/3}}{n^{2/3}K^{2/3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining these two cases, we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\nf(\\hat{\\boldsymbol{x}}_{K})-f(\\boldsymbol{x}_{*})\\le\\frac{\\sqrt{\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}}}{\\sqrt{2}K}\\|\\boldsymbol{x}_{0}-\\boldsymbol{x}_{*}\\|_{2}^{2}+\\frac{2^{1/3}(n-b)^{2/3}(\\tilde{L}^{g})^{1/3}\\sigma_{*}^{2/3}\\|\\boldsymbol{x}_{0}-\\boldsymbol{x}_{*}\\|_{2}^{4/3}}{n^{2/3}K^{2/3}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To guarantee $\\mathbb{E}[f(\\hat{\\mathbf{x}}_{K})-f(\\mathbf{x}_{*})]\\,\\le\\,\\epsilon$ for $\\epsilon>0$ , the total number of required individual gradient evaluations will be ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{n K\\geq\\operatorname*{max}\\Big\\{\\displaystyle\\frac{n\\sqrt{2\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}}\\|x_{0}-x_{*}\\|_{2}^{2}}{\\epsilon},\\frac{4(n-b)(\\tilde{L}_{0}^{g})^{1/2}\\sigma_{*}\\|x_{0}-x_{*}\\|_{2}^{2}}{\\epsilon^{3/2}}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combining Eq. (15) and Eq. (16), we finally have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n K\\geq\\cfrac{n\\sqrt{2\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}}\\|x_{0}-x_{*}\\|_{2}^{2}}{\\epsilon}}\\\\ &{\\qquad+\\operatorname*{min}\\Big\\{\\cfrac{4n^{1/2}\\big(\\hat{L}_{0}^{g}\\tilde{L}_{0}^{g}\\big)^{1/2}\\|y_{*}\\|_{\\Lambda^{-1}}\\|x_{0}-x_{*}\\|_{2}^{2}}{\\epsilon^{3/2}},\\cfrac{4(n-b)(\\tilde{L}_{0}^{g})^{1/2}\\sigma_{*}\\|x_{0}-x_{*}\\|_{2}^{2}}{\\epsilon^{3/2}}\\Big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "thus finishing the proof. ", "page_idx": 26}, {"type": "text", "text": "C Omitted Proofs for Smooth Convex Settting From Section 3 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Before proceeding to the omitted proofs for the smooth convex settings in finite-sum with linear predictors, we first recall its primal-dual reformulation, then state the specialized version of a primaldual shuffed SGD algorithm in Algorithm 2. Recall that (PL) admits an explicit reformulation using convexconjugatesof $\\ell_{i}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\operatorname*{max}_{y\\in\\mathbb{R}^{n}}\\left\\{\\mathcal{L}(x,y):=\\frac{1}{n}\\left\\langle A x,y\\right\\rangle-\\frac{1}{n}\\sum_{i=1}^{n}\\ell_{i}^{*}(y^{i})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(a_{i}^{\\top}x y^{i}-\\ell_{i}^{*}(y^{i})\\right)\\right\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\begin{array}{r}{\\pmb{y}_{\\pmb{x}}^{i}=\\arg\\operatorname*{max}_{\\pmb{y}^{i}\\in\\mathbb{R}}\\{\\pmb{y}^{i}\\pmb{a}_{i}^{\\top}\\pmb{x}-\\ell_{i}^{*}(\\pmb{y}^{i})\\}}\\end{array}$ (different from the general smooth convex finite-sum settings in Section 2 and Appendix B). Further, for notational convenience, we assume that the partition is ordered, in the sense that for $1\\leq j<j^{\\prime}\\leq m$ \uff0c $\\begin{array}{r}{\\operatorname*{max}_{i\\in S^{j}}i<\\operatorname*{min}_{i^{\\prime}\\in S^{j^{\\prime}}}i^{\\prime}}\\end{array}$ 2 We denote by $\\pmb{y}^{(j)}$ the subvector of $\\pmb{y}\\in\\mathbb{R}^{n}$ indexed by the elements of $S^{j}$ , and by $A^{(j)}$ the submatrix obtained from $A\\in\\mathbb{R}^{n\\times d}$ by selecting the rows indexed by $S^{j}$ ", "page_idx": 26}, {"type": "text", "text": "Based on the formulation (PL-PD), we view shuffed SGD as a primal-dual method with block coordinate updates on the dual side, as summarized in Algorithm 2, for completeness. To see the equivalence, in $i$ -th inner iteration of $k$ -th epoch, we first update the $i^{\\th}$ -th block $\\bar{\\mathbf{\\upsilon}}_{k}^{(i)}\\in\\mathbb{R}^{b}$ of the dual vector $\\!\\,y_{k-1}\\in\\mathbb{R}^{n}$ based on $\\mathbf{\\Delta}x_{k-1,i}$ as in Line 6. Since the dual update has a decomposable structure, this maximization step corresponds to computing the (sub)gradients $\\{\\ell_{\\pi_{j}^{(k)}}^{\\prime}(\\pmb{a}_{\\pi_{j}^{(k)}}^{\\top}\\pmb{x}_{k-1,i})\\}_{j=b(i-1)+1}^{b i}$ $\\mathbf{\\Delta}x_{k-1,i}$ $\\{\\pi_{j}^{(k)}\\}_{j=b(i-1)+1}^{b i}$ Thenin Line7 we pofom a minimization step using $\\pmb{y}_{k}^{(i)}$ to compute $\\pmb{x}_{k-1,i+1}$ on the primal side. Combining these two steps, we have $\\begin{array}{r}{\\pmb{x}_{k-1,i+1}=\\pmb{x}_{k-1,i}-\\frac{\\eta_{k}}{b}\\sum_{j=b(i-1)+1}^{b i}\\ell_{\\pi_{j}^{(k)}}^{\\prime}\\big(\\pmb{a}_{\\pi_{j}^{(k)}}^{\\top}\\pmb{x}_{k-1,i}\\big)\\pmb{a}_{\\pi_{j}^{(k)}}=\\pmb{x}_{k-1,i},}\\end{array}$ which is exactly the original primal shuffed SGD updating scheme. ", "page_idx": 26}, {"type": "text", "text": "1: Input: Initial point $\\pmb{x}_{0}\\in\\mathbb{R}^{d}$ , batch size $b>0$ , step size $\\{\\eta_{k}\\}>0$ , number of epochs $K>0$   \n2: for $k=1$ to $K$ do   \n3:  Generate any permutation $\\pi^{(k)}$ of $[n]$ (either deterministic or random)   \n4:  Ck-1,1 = k-1   \n5:for $i=1$ to m do   \n6: $\\begin{array}{r}{\\pmb{y}_{k}^{\\left(i\\right)}=\\arg\\operatorname*{max}_{\\pmb{y}\\in\\mathbb{R}^{b}}\\left\\{\\pmb{y}^{\\top}\\pmb{A}_{k}^{\\left(i\\right)}\\pmb{x}_{k-1,i}-\\sum_{j=1}^{b}\\ell_{\\pi_{b\\left(i-1\\right)+j}^{\\left(k\\right)}}^{*}(\\pmb{y}^{j})\\right\\}}\\\\ {\\pmb{x}_{k-1,i+1}=\\arg\\operatorname*{min}_{\\pmb{x}\\in\\mathbb{R}^{d}}\\left\\{\\pmb{y}_{k}^{\\left(i\\right)\\top}\\pmb{A}_{k}^{\\left(i\\right)}\\pmb{x}+\\frac{b}{2\\eta_{k}}\\|\\pmb{x}-\\pmb{x}_{k-1,i}\\|^{2}\\right\\}}\\end{array}$   \n7: $\\mathbf{\\Delta}\\mathbf{x}_{k}=\\mathbf{x}_{k-1,m+1}$ $\\pmb{y}_{k}=\\left(\\pmb{y}_{k}^{(1)},\\pmb{y}_{k}^{(2)},\\dots,\\pmb{y}_{k}^{(m)}\\right)^{\\top}$ $\\begin{array}{r}{\\hat{\\pmb{x}}_{K}=\\sum_{k=1}^{K}{\\eta_{k}\\pmb{x}_{k}}/\\sum_{k=1}^{K}{\\eta_{k}}}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "C.1  Omitted Proofs for the Random Reshufling/Shuffle-Once Schemes ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Lemma 9. Given $\\{y_{k}^{(i)}\\}_{i=1}^{m}$ and $\\{\\mathbfit{x}_{k-1,i}\\}_{i=1}^{m+1}$ generated by Algoritm 2 for $k\\,\\in\\,[K]$ let $\\mathcal{E}_{k}:=$ $\\begin{array}{r}{\\eta_{k}\\mathrm{Gap}^{v}(\\pmb{x}_{k},\\pmb{y}_{*})+\\frac{b}{2n}\\|\\pmb{x}_{*}-\\pmb{x}_{k}\\|_{2}^{2}-\\frac{b}{2n}\\|\\pmb{x}_{*}-\\pmb{x}_{k-1}\\|_{2}^{2}.}\\end{array}$ IfAssumtion3holds, then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathcal{E}_{k}\\leq\\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}y_{k}^{(i)\\top}A_{k}^{(i)}(\\mathbf{x}_{k}-\\mathbf{x}_{k-1,i+1})}\\quad}&{}\\\\ &{\\quad+\\ \\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}\\big(v_{k}^{(i)}-y_{k}^{(i)}\\big)^{\\top}A_{k}^{(i)}(\\mathbf{x}_{k}-\\mathbf{x}_{k-1,i})}\\\\ &{\\quad-\\ \\frac{\\eta_{k}}{2n}\\|y_{k}-v_{k}\\|_{\\mathbf{A}_{k}^{-1}}^{2}-\\frac{\\eta_{k}}{2n}\\|y_{k}-y_{*,k}\\|_{\\mathbf{A}_{k}^{-1}}^{2}}\\\\ &{\\quad-\\ \\frac{b}{2n}\\sum_{i=1}^{m}\\|\\mathbf{x}_{k-1,i}-\\mathbf{x}_{k-1,i+1}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. By Line 6 in Alg. 2, we have $\\begin{array}{r}{y_{k}^{(i)}=\\arg\\operatorname*{max}_{y\\in\\mathbb{R}^{b}}\\left\\{y^{\\top}A_{k}^{(i)}x_{k-1,i}-\\sum_{j=1}^{b}\\ell_{\\pi_{b(i-1)+j}^{(k)}}^{*}(y^{j})\\right\\}}\\end{array}$ for $i\\in[m]$ . Notice that since ", "page_idx": 27}, {"type": "equation", "text": "$$\ny^{\\top}A_{k}^{(i)}x_{k-1,i}-\\sum_{j=1}^{b}\\ell_{\\pi_{b(i-1)+j}^{(k)}}^{*}(y^{j})=\\sum_{j=1}^{b}\\left(y^{j}a_{\\pi_{b(i-1)+j}^{(k)}}^{\\top}x_{k-1,i}-\\ell_{\\pi_{b(i-1)+j}^{(k)}}^{*}(y^{j})\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "is separable, we have $\\begin{array}{r}{\\pmb{y}_{k}^{j}=\\arg\\operatorname*{max}_{\\pmb{y}\\in\\mathbb{R}}\\{\\pmb{y}\\pmb{a}_{\\pi_{j}^{(k)}}^{\\top}\\pmb{x}_{k-1,i}-\\ell_{\\pi_{j}^{(k)}}^{*}(\\pmb{y})\\}}\\end{array}$ for $b(i-1)+1\\leq j\\leq b i$ , thus ${\\pmb{a}}_{\\pi_{j}^{(k)}}^{\\top}{\\pmb{x}}_{k-1,i}\\in\\partial\\ell_{\\pi_{j}^{(k)}}^{*}({\\pmb{y}}_{k}^{j})$ Since $\\ell_{i}^{*}$ $\\frac{1}{L_{i}}$ strogly convex by Assumpton 3 the by Lemma4 we obtain for $b(i-1)+1\\leq j\\leq b i$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\ell_{\\pi_{j}^{(k)}}^{*}\\left(v_{k}^{j}\\right)\\geq\\ell_{\\pi_{j}^{(k)}}^{*}(y_{k}^{j})+a_{\\pi_{j}^{(k)}}^{\\top}x_{k-1,i}\\big(v_{k}^{j}-y_{k}^{j}\\big)+\\frac{1}{2L_{\\pi_{j}^{(k)}}}\\big(v_{k}^{j}-y_{k}^{j}\\big)^{2},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which leads to ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathcal{L}(\\boldsymbol{x}_{k},\\boldsymbol{v})=\\frac{1}{n}\\sum_{i=1}^{m}\\Big(\\boldsymbol{v}_{k}^{(i)\\top}\\boldsymbol{A}_{k}^{(i)}\\boldsymbol{x}_{k-1,i}-\\underset{j=b(i-1)+1}{\\overset{b i}{\\sum}}\\ell_{\\pi_{j}^{(k)}}^{*}(\\boldsymbol{v}_{k}^{j})\\Big)+\\frac{1}{n}\\sum_{i=1}^{m}v_{k}^{(i)\\top}\\boldsymbol{A}_{k}^{(i)}(\\boldsymbol{x}_{k}-\\boldsymbol{x}_{k-1,i})}}\\\\ &{}&{\\leq\\frac{1}{n}\\sum_{i=1}^{m}\\Big(\\boldsymbol{y}_{k}^{(i)\\top}\\boldsymbol{A}_{k}^{(i)}\\boldsymbol{x}_{k-1,i}-\\underset{j=b(i-1)+1}{\\overset{b i}{\\sum}}\\ell_{\\pi_{j}^{(k)}}^{*}(\\boldsymbol{y}_{k}^{j})\\Big)+\\frac{1}{n}\\sum_{i=1}^{m}v_{k}^{(i)\\top}\\boldsymbol{A}_{k}^{(i)}(\\boldsymbol{x}_{k}-\\boldsymbol{x}_{k-1,i})}\\\\ &{}&{\\quad-\\,\\frac{1}{2n}\\lVert\\boldsymbol{y}_{k}-\\boldsymbol{v}_{k}\\rVert_{\\boldsymbol{\\Lambda}_{k}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using the same argument for $\\mathcal{L}(x_{*},y_{*})$ as $\\pmb{a}_{j}^{\\top}\\pmb{x}_{\\ast}\\in\\partial\\ell_{j}^{\\ast}(\\pmb{y}_{\\ast}^{j})$ for $j\\in[n]$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\boldsymbol{x}_{*},\\boldsymbol{y}_{*})=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{m}\\left(y_{*,k}^{(i)\\top}A_{k}^{(i)}\\boldsymbol{x}_{*}-\\sum_{j=b(i-1)+1}^{b i}\\ell_{\\pi_{j}^{(k)}}^{*}(y_{*,k}^{j})\\right)}\\\\ &{\\qquad\\qquad\\geq\\displaystyle\\frac{1}{n}\\sum_{i=1}^{m}\\left(y_{k}^{(i)\\top}A_{k}^{(i)}\\boldsymbol{x}_{*}-\\sum_{j=b(i-1)+1}^{b i}\\ell_{\\pi_{j}^{(k)}}^{*}(y_{k}^{j})\\right)+\\frac{1}{2n}\\|\\boldsymbol{y}_{k}-\\boldsymbol{y}_{*,k}\\|_{\\boldsymbol{X}_{k}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Adding and substracting the term $\\begin{array}{r}{\\frac{b}{2n\\eta_{k}}\\sum_{i=1}^{m}\\|\\pmb{x}_{*}-\\pmb{x}_{k-1,i}\\|_{2}^{2}}\\end{array}$ on the R.H.S. of Eq. (19), we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\mathcal{L}(\\pmb{x}_{*},\\pmb{y}_{*})\\geq\\frac{1}{n}\\sum_{i=1}^{m}\\left(y_{k}^{(i)\\top}\\pmb{A}_{k}^{(i)}\\pmb{x}_{*}+\\frac{b}{2\\eta_{k}}\\|\\pmb{x}_{*}-\\pmb{x}_{k-1,i}\\|_{2}^{2}-\\sum_{j=b(i-1)+1}^{b i}\\ell_{\\pi_{j}^{(k)}}^{*}(y_{k}^{j})\\right)}\\\\ {\\displaystyle-\\,\\frac{b}{2n\\eta_{k}}\\sum_{i=1}^{m}\\|\\pmb{x}_{*}-\\pmb{x}_{k-1,i}\\|_{2}^{2}+\\frac{1}{2n}\\|y_{k}-\\pmb{y}_{*,k}\\|_{\\pmb{\\Lambda}_{k}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Line 7 of Alg.2, we have $\\begin{array}{r}{\\pmb{x}_{k-1,i+1}=\\arg\\operatorname*{min}_{\\pmb{x}\\in\\mathbb{R}^{d}}\\left\\{\\pmb{y}_{k}^{(i)\\top}\\pmb{A}_{k}^{(i)}\\pmb{x}+\\frac{b}{2\\eta_{k}}\\|\\pmb{x}-\\pmb{x}_{k-1,i}\\|_{2}^{2}\\right\\}}\\end{array}$ Further noicing that $\\begin{array}{r}{\\phi_{k}^{(i)}({\\pmb x}):={\\pmb y}_{k}^{(i)\\top}{\\pmb A}_{k}^{(i)}{\\pmb x}+\\frac{b}{2\\eta_{k}}\\|{\\pmb x}-{\\pmb x}_{k-1,i}\\|_{2}^{2}}\\end{array}$ is $\\frac{b}{\\eta_{k}}$ strongly covex w.t. $\\textbf{\\em x}$ and $\\nabla\\phi_{k}^{(i)}(\\mathbf{x}_{k-1,i+1})=\\mathbf{0}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\phi_{k}^{(i)}(\\pmb{x}_{*})\\geq\\phi_{k}^{(i)}(\\pmb{x}_{k-1,i+1})+\\frac{b}{2\\eta_{k}}\\|\\pmb{x}_{*}-\\pmb{x}_{k-1,i+1}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which leads to ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle)\\geq\\frac{1}{n}\\sum_{i=1}^{m}\\left(y_{k}^{(i)\\top}A_{k}^{(i)}x_{k-1,i+1}+\\frac{b}{2\\eta_{k}}\\|x_{k-1,i+1}-x_{k-1,i}\\|_{2}^{2}-\\sum_{j=b(i-1)+1}^{b i}\\ell_{\\pi_{j}^{(k)}}^{*}(y_{k}^{j})\\right)}\\\\ {\\displaystyle}&{\\displaystyle~~~+\\frac{b}{2n\\eta_{k}}\\sum_{i=1}^{m}\\left(\\|x_{*}-x_{k-1,i+1}\\|_{2}^{2}-\\|x_{*}-x_{k-1,i}\\|_{2}^{2}\\right)+\\frac{1}{2n}\\|y_{k}-y_{*,k}\\|_{\\boldsymbol{\\Lambda}_{k}^{-1}}^{2}}\\\\ {\\displaystyle}&{\\displaystyle\\overset{(i)}{=}\\frac{1}{n}\\sum_{i=1}^{m}\\left(y_{k}^{(i)\\top}A_{k}^{(i)}x_{k-1,i+1}+\\frac{b}{2\\eta_{k}}\\|x_{k-1,i+1}-x_{k-1,i}\\|_{2}^{2}-\\sum_{j=b(i-1)+1}^{b i}\\ell_{\\pi_{j}^{(k)}}^{*}(y_{k}^{j})\\right)}\\\\ &{\\displaystyle}&{\\displaystyle+\\frac{b}{2n\\eta_{k}}\\|x_{k}-x_{*}\\|_{2}^{2}-\\frac{b}{2n\\eta_{k}}\\|x_{k-1}-x_{*}\\|_{2}^{2}+\\frac{1}{2n}\\|y_{k}-y_{*,k}\\|_{\\boldsymbol{\\Lambda}_{k}^{-1}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we telescope from $i=1$ $m$ for the term $\\begin{array}{r}{\\sum_{i=1}^{m}\\left(\\|\\pmb{x}_{*}-\\pmb{x}_{k-1,i+1}\\|_{2}^{2}-\\|\\pmb{x}_{*}-\\pmb{x}_{k-1,i}\\|_{2}^{2}\\right)}\\end{array}$ , and use the definitions that $\\pmb{x}_{k}=\\pmb{x}_{k-1,m+1}$ and $\\mathbf{\\Delta}\\mathbf{x}_{k-1}=\\mathbf{x}_{k-1,1}$ for $(i)$ \uff1a ", "page_idx": 28}, {"type": "text", "text": "Combining the bounds from Eq. (18) and Eq. (20) and denoting ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}:=\\eta_{k}\\big(\\mathcal{L}(\\boldsymbol{x}_{k},\\boldsymbol{v})-\\mathcal{L}(\\boldsymbol{x}_{*},\\boldsymbol{y}_{*})\\big)+\\frac{b}{2n}\\|\\boldsymbol{x}_{*}-\\boldsymbol{x}_{k}\\|_{2}^{2}-\\frac{b}{2n}\\|\\boldsymbol{x}_{*}-\\boldsymbol{x}_{k-1}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}_{k}\\leq\\displaystyle\\frac{\\eta_{k}}{n}\\displaystyle\\sum_{i=1}^{m}y_{k}^{(i)\\top}A_{k}^{(i)}(x_{k-1,i}-x_{k-1,i+1})+\\displaystyle\\frac{\\eta_{k}}{n}\\displaystyle\\sum_{i=1}^{m}v_{k}^{(i)\\top}A_{k}^{(i)}(x_{k}-x_{k-1,i})}\\\\ &{\\qquad-\\displaystyle\\frac{\\eta_{k}}{2n}\\|y_{k}-v_{k}\\|_{\\mathbf{A}_{k}^{-1}}^{2}-\\displaystyle\\frac{\\eta_{k}}{2n}\\|y_{k}-y_{*,k}\\|_{\\mathbf{A}_{k}^{-1}}^{2}-\\displaystyle\\frac{b}{2n}\\displaystyle\\sum_{i=1}^{m}\\|x_{k-1,i}-x_{k-1,i+1}\\|_{2}^{2}}\\\\ &{=\\displaystyle\\frac{\\eta_{k}}{n}\\displaystyle\\sum_{i=1}^{m}y_{k}^{(i)\\top}A_{k}^{(i)}(x_{k}-x_{k-1,i+1})+\\displaystyle\\frac{\\eta_{k}}{n}\\displaystyle\\sum_{i=1}^{m}(v_{k}^{(i)}-y_{k}^{(i)})^{\\top}A_{k}^{(i)}(x_{k}-x_{k-1,i})}\\\\ &{\\qquad-\\displaystyle\\frac{\\eta_{k}}{2n}\\|y_{k}-v_{k}\\|_{\\mathbf{A}_{k}^{-1}}^{2}-\\displaystyle\\frac{\\eta_{k}}{2n}\\|y_{k}-y_{*,k}\\|_{\\mathbf{A}_{k}^{-1}}^{2}-\\displaystyle\\frac{b}{2n}\\displaystyle\\sum_{i=1}^{m}\\|x_{k-1,i}-x_{k-1,i+1}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "thus completing the proof. ", "page_idx": 28}, {"type": "text", "text": "Lemma 10. For any $k\\in[K]$ the iterates $\\{y_{k}^{(i)}\\}_{i=1}^{m}$ and $\\{\\mathbf{\\Deltax}_{k-1,i}\\}_{i=1}^{m+1}$ in Algorinm 2 satisfy ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{T}_{1}=\\frac{b}{2n}\\sum_{i=1}^{m}\\|\\pmb{x}_{k-1,i}-\\pmb{x}_{k-1,i+1}\\|_{2}^{2}-\\frac{b}{2n}\\|\\pmb{x}_{k-1}-\\pmb{x}_{k}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "ProofBy Line 7 in Alg.2, we have $\\begin{array}{r}{A_{k}^{(i)\\top}{y}_{k}^{(i)}\\,=\\,\\frac{b}{\\eta_{k}}({\\pmb x}_{k-1,i}-{\\pmb x}_{k-1,i+1})}\\end{array}$ Further noicing that $\\begin{array}{r}{\\pmb{x}_{k}-\\pmb{x}_{k-1,i+1}=-\\sum_{j=i+1}^{m}(\\pmb{x}_{k-1,j}-\\pmb{x}_{k-1,j+1})}\\end{array}$ weobtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle T_{1}:=\\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}y_{k}^{(i)\\top}A_{k}^{(i)}({\\pmb x}_{k}-{\\pmb x}_{k-1,i+1})}}\\\\ {{\\displaystyle~~~=~-\\,\\frac{b}{n}\\sum_{i=1}^{m-1}\\sum_{j=i+1}^{m}\\left\\langle{\\pmb x}_{k-1,i}-{\\pmb x}_{k-1,i+1},{\\pmb x}_{k-1,j}-{\\pmb x}_{k-1,j+1}\\right\\rangle}}\\\\ {{\\displaystyle~~~=\\frac{b}{2n}\\sum_{i=1}^{m}\\|{\\pmb x}_{k-1,i}-{\\pmb x}_{k-1,i+1}\\|^{2}-\\frac{b}{2n}\\Big\\|\\sum_{i=1}^{m}({\\pmb x}_{k-1,i}-{\\pmb x}_{k-1,i+1})\\Big\\|^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "thus completing the proof. ", "page_idx": 29}, {"type": "text", "text": "Lemma 11. UnderAssumption . for any $k\\ \\in\\ [K]$ $\\{\\pmb{y}_{k}^{(i)}\\}_{i=1}^{m}$ Cand $\\{\\mathbf{x}_{k-1,i}\\}_{i=1}^{m+1}$ generated by Algorithm 2 with uniformly random shufling satisfy ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathcal{T}_{2}]\\leq\\mathbb{E}\\Big[\\frac{\\eta_{k}^{3}n\\hat{L}_{\\pi^{(k)}}\\tilde{L}_{\\pi^{(k)}}}{b^{2}}\\|y_{k}-y_{*,k}\\|_{\\Lambda_{k}^{-1}}^{2}+\\frac{\\eta_{k}}{2n}\\|v_{k}-y_{k}\\|_{\\Lambda_{k}^{-1}}^{2}\\Big]+\\frac{\\eta_{k}^{3}\\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. By Line 7 in Alg.2. we have $\\begin{array}{r}{{\\pmb x}_{k-1,i}-{\\pmb x}_{k-1,i+1}=\\frac{\\eta_{k}}{b}A_{k}^{(i)\\top}{\\pmb y}_{k}^{(i)}}\\end{array}$ Using thedefnition of $I_{j\\uparrow}$ for $0\\leq j\\leq n-1$ as in Section 1, we obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\nx_{k}-x_{k-1,i}=-\\sum_{j=i}^{m}(x_{k-1,j}-x_{k-1,j+1})=-{\\frac{\\eta_{k}}{b}}\\sum_{j=i}^{m}A_{k}^{(j)\\top}y_{k}^{(j)}=-{\\frac{\\eta_{k}}{b}}A_{k}I_{b(i-1)\\uparrow}y_{k}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Alsoe w have AC9)T(o(a $A_{k}^{(i)\\top}(v_{k}^{(i)}\\!-\\!y_{k}^{(i)})=A_{k}I_{(i)}(v_{k}\\!-\\!y_{k})$ by the definition of $I_{(i)}$ in Section 3. Combining these two observations, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{2}:=\\displaystyle\\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}\\big(v_{k}^{(i)}-y_{k}^{(i)}\\big)^{\\top}A_{k}^{(i)}(x_{k}-x_{k-1,i})}\\\\ &{\\phantom{T_{2}}=-\\displaystyle\\frac{\\eta_{k}^{2}}{b n}\\sum_{i=1}^{m}\\langle A_{k}^{\\top}I_{b(i-1)\\uparrow}y_{k},A_{k}^{\\top}I_{(i)}(v_{k}-y_{k})\\rangle}\\\\ &{\\overset{(i)}{=}\\displaystyle-\\frac{\\eta_{k}^{2}}{b n}\\sum_{i=1}^{m}\\langle A_{k}^{\\top}I_{b(i-1)\\uparrow}(y_{k}-y_{*,k}),A_{k}^{\\top}I_{(i)}(v_{k}-y_{k})\\rangle}\\\\ &{\\phantom{T_{2}}-\\displaystyle\\frac{\\eta_{k}^{2}}{b n}\\sum_{i=1}^{m}\\langle A_{k}^{\\top}I_{b(i-1)\\uparrow}y_{*,k},A_{k}^{\\top}I_{(i)}(v_{k}-y_{k})\\rangle\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we make a decomposition w.r.t. ${\\pmb y}_{*,k}$ in $(i)$ . For the first term in Eq. (21), we use Young's inequality for $\\alpha>0$ and have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\displaystyle\\frac{\\eta_{k}^{2}}{b n}\\sum_{i=1}^{m}\\big\\langle A_{k}^{\\top}I_{b(i-1)\\uparrow}(y_{k}-y_{*,k}),A_{k}^{\\top}I_{(i)}(v_{k}-y_{k})\\big\\rangle}\\\\ &{\\leq\\displaystyle\\frac{\\eta_{k}^{2}\\alpha}{2b n}\\sum_{i=1}^{m}\\|A_{k}^{\\top}I_{b(i-1)\\uparrow}(y_{k}-y_{*,k})\\|_{2}^{2}+\\displaystyle\\frac{\\eta_{k}^{2}}{2b n\\alpha}\\sum_{i=1}^{m}\\|A_{k}^{\\top}I_{(i)}(v_{k}-y_{k})\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Expanding the squares and rearranging the terms in Eq. (23), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{\\eta_{k}^{2}\\alpha}{2b\\ln\\left(\\eta_{k}\\right)}\\sum_{i=1}^{m}\\|A_{k}^{\\top}I_{b(i-1)\\uparrow}(y_{k}-y_{*,k})\\|_{2}^{2}}\\\\ &{=\\frac{\\eta_{k}^{2}\\alpha}{2b\\ln\\left(\\frac{1}{a}\\right)}\\sum_{i=1}^{m}\\|y_{k}-y_{*,k}\\|^{7}I_{b(i-1)\\uparrow}A_{k}A_{k}^{\\top}I_{b(i-1)\\uparrow}(y_{k}-y_{*,k})}\\\\ &{=\\frac{\\eta_{k}^{2}\\alpha}{2b\\ln\\left(y_{k}-y_{*,k}\\right)^{\\top}\\left(\\sum_{i=1}^{m}I_{b(i-1)\\uparrow}A_{k}A_{k}^{\\top}I_{b(i-1)\\uparrow}\\right)\\left(y_{k}-y_{*,k}\\right)}\\\\ &{=\\frac{\\eta_{k}^{2}\\alpha}{2b\\ln\\left(y_{k}-y_{*,k}\\right)^{\\top}\\Lambda_{k}^{-1/2}\\Lambda_{k}^{1/2}}\\Big(\\underset{i=1}{\\overset{m}{\\sum}}I_{b(i-1)\\uparrow}A_{k}A_{k}^{\\top}I_{b(i-1)\\uparrow}\\Big)\\Lambda_{k}^{1/2}\\Lambda_{k}^{-1/2}(y_{k}-y_{*,k})}\\\\ &{\\overset{(i)}{\\leq}\\frac{\\eta_{k}^{2}\\alpha}{2b\\ln\\left(\\left\\Vert\\Lambda_{k}^{1/2}\\left(\\sum_{i=1}^{m}I_{b(i-1)\\uparrow}A_{k}A_{k}^{\\top}I_{b(i-1)\\uparrow}\\right)\\Lambda_{k}^{1/2}\\right\\Vert_{2}\\left\\Vert y_{k}-y_{*,k}\\right\\Vert_{\\Lambda_{k}^{-1}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we use Cauchy-Schwarz inequality for $(i)$ . Using a similar argument, we also have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\eta_{k}^{2}}{2b n\\alpha}\\sum_{i=1}^{m}\\|A_{k}^{\\top}I_{(i)}(v_{k}-y_{k})\\|_{2}^{2}\\leq\\frac{\\eta_{k}^{2}}{2b n\\alpha}\\Big\\|\\Lambda_{k}^{1/2}\\Big(\\sum_{i=1}^{m}I_{(i)}A_{k}A_{k}^{\\top}I_{(i)}\\Big)\\Lambda_{k}^{1/2}\\Big\\|_{2}\\|v_{k}-y_{k}\\|_{\\Lambda_{k}^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By the definitions of $\\hat{L}_{\\pi^{(k)}}$ and $\\tilde{L}_{\\pi^{(k)}}$ , and choosing $\\alpha=2\\eta_{k}\\tilde{L}_{\\pi^{(k)}}$ in Eq. (23), we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\displaystyle\\frac{\\eta_{k}^{2}}{b n}\\sum_{i=1}^{m}\\langle A_{k}^{\\top}I_{b(i-1)\\uparrow}(y_{k}-y_{*,k}),A_{k}^{\\top}I_{(i)}(v_{k}-y_{k})\\rangle}\\\\ &{\\leq\\displaystyle\\frac{\\eta_{k}^{3}n\\hat{L}_{\\pi^{(k)}}\\tilde{L}_{\\pi^{(k)}}}{b^{2}}\\|y_{k}-y_{*,k}\\|_{\\Lambda_{k}^{-1}}^{2}+\\displaystyle\\frac{\\eta_{k}}{4n}\\|v_{k}-y_{k}\\|_{\\Lambda_{k}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For the second term in Eq. (22), we apply Young's inequality with $\\beta>0$ and proceed as above: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\cfrac{\\eta_{k}^{2}}{b n}\\sum_{i=1}^{m}\\langle A_{k}^{\\top}I_{b(i-1)\\uparrow}y_{*,k},A_{k}^{\\top}I_{(i)}(v_{k}-y_{k})\\rangle}\\\\ &{\\leq\\cfrac{\\eta_{k}^{2}\\beta}{2b n}\\sum_{i=1}^{m}\\|A_{k}^{\\top}I_{b(i-1)\\uparrow}y_{*,k}\\|_{2}^{2}+\\cfrac{\\eta_{k}^{2}}{2b n\\beta}\\sum_{i=1}^{m}\\|A_{k}^{\\top}I_{(i)}(v_{k}-y_{k})\\|_{2}^{2}}\\\\ &{\\leq\\cfrac{\\eta_{k}^{2}\\beta}{2b n}\\sum_{i=1}^{m}\\|A_{k}^{\\top}I_{b(i-1)\\uparrow}y_{*,k}\\|_{2}^{2}+\\cfrac{\\eta_{k}^{2}}{2n\\beta}\\tilde{L}_{\\pi^{(k)}}\\|v_{k}-y_{k}\\|_{\\Lambda_{k}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Noticing that $\\tilde{L}_{\\pi^{(k)}}\\leq\\tilde{L}$ , we choose $\\beta=2\\eta_{k}\\tilde{L}$ and obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{-\\,\\displaystyle\\frac{\\eta_{k}^{2}}{b n}\\sum_{i=1}^{m}\\langle\\mathbf{A}_{k}^{\\top}I_{b(i-1)\\uparrow}y_{*,k},\\mathbf{A}_{k}^{\\top}I_{(i)}(v_{k}-y_{k})\\rangle}\\\\ &{}&{\\quad\\quad\\le\\displaystyle\\frac{\\eta_{k}^{3}\\tilde{L}}{n b}\\sum_{i=1}^{m}\\|\\mathbf{A}_{k}^{\\top}I_{b(i-1)\\uparrow}y_{*,k}\\|_{2}^{2}+\\displaystyle\\frac{\\eta_{k}}{4n}\\|v_{k}-y_{k}\\|_{\\Lambda_{k}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Combining Eq. (25) and Eq. (26), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\nT_{2}\\leq\\frac{\\eta_{k}^{3}\\tilde{L}}{n b}\\sum_{i=1}^{m}\\|A_{k}^{\\top}I_{b(i-1)\\uparrow}y_{*,k}\\|_{2}^{2}+\\frac{\\eta_{k}^{3}n\\hat{L}_{\\pi^{(k)}}\\tilde{L}_{\\pi^{(k)}}}{b^{2}}\\|y_{k}-y_{*,k}\\|_{\\mathbf{A}_{k}^{-1}}^{2}+\\frac{\\eta_{k}}{2n}\\|v_{k}-y_{k}\\|_{\\mathbf{A}_{k}^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We first assume the RR scheme. Taking conditional expectation w.r.t. the randomness up to but not including $k$ -th epoch,we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{k}[\\mathcal{T}_{2}]\\leq\\displaystyle\\frac{\\eta_{k}^{3}\\tilde{L}}{n b}\\mathbb{E}_{k}\\Big[\\sum_{i=1}^{m}\\|A_{k}^{\\top}I_{b(i-1)\\uparrow}y_{*,k}\\|_{2}^{2}\\Big]}\\\\ &{\\qquad\\qquad+\\mathbb{E}_{k}\\Big[\\displaystyle\\frac{\\eta_{k}^{3}n\\hat{L}_{\\pi^{(k)}}\\tilde{L}_{\\pi^{(k)}}}{b^{2}}\\|y_{k}-y_{*,k}\\|_{\\Lambda_{k}^{-1}}^{2}+\\displaystyle\\frac{\\eta_{k}}{2n}\\|v_{k}-y_{k}\\|_{\\Lambda_{k}^{-1}}^{2}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For the firsterm $\\begin{array}{r}{\\frac{\\eta_{k}^{3}\\tilde{L}}{n b}\\mathbb{E}_{k}\\left[\\sum_{i=1}^{m}\\|\\mathbf{A}_{k}^{\\top}I_{b(i-1)\\uparrow}\\pmb{y}_{\\ast,k}\\|_{2}^{2}\\right]}\\end{array}$ the only randomnes s fom the random permutation $\\pi^{(k)}$ . In this case, each term $A_{k}^{\\top}I_{b(i-1)\\uparrow}y_{*,k}$ can be considered as a sum of a batch sampled without replacement from $\\{y_{*}^{j}{\\pmb a}_{j}\\}_{j\\in[n]}$ , while $\\begin{array}{r}{\\sum_{j=1}^{n}y_{*}^{j}\\pmb{a}_{j}=0}\\end{array}$ as $^{x_{*}}$ is the minimizer, we then can use Lemma 5 and obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\eta_{k}^{3}\\tilde{L}}{n b}\\mathbb{E}_{k}\\bigg[\\underset{i=1}{\\overset{m}{\\sum}}\\|A_{k}^{\\top}I_{b(i-1)\\uparrow}y_{*,k}\\|_{2}^{2}\\bigg]\\overset{(i)}{=}\\frac{\\eta_{k}^{3}\\tilde{L}}{n b}\\sum_{i=1}^{m}\\mathbb{E}_{\\pi^{(k)}}[\\|A_{k}^{\\top}I_{b(i-1)\\uparrow}y_{*,k}\\|_{2}^{2}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{\\eta_{k}^{3}\\tilde{L}}{n b}\\sum_{i=1}^{m}\\big(n-b(i-1)\\big)^{2}\\mathbb{E}_{\\pi^{(k)}}\\bigg[\\big\\|\\frac{A_{k}^{\\top}I_{b(i-1)\\uparrow}y_{*,k}}{n-b(i-1)}\\bigg\\|_{2}^{2}}\\\\ &{\\overset{(i i)}{\\le}\\frac{\\eta_{k}^{3}\\tilde{L}}{n b}\\sum_{i=1}^{m}\\big(n-b(i-1)\\big)^{2}\\frac{b(i-1)}{\\big(n-b(i-1)\\big)\\big(n-1\\big)}\\sigma_{*}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{\\eta_{k}^{3}\\tilde{L}\\big(n-b\\big)\\big(n+b\\big)}{6b^{2}\\big(n-1\\big)}\\sigma_{*}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $(i)$ is due to the linearity of expectation, and we use our definition $\\begin{array}{r}{\\sigma_{*}^{2}=\\frac{1}{n}\\sum_{j=1}^{n}(\\pmb{y}_{*}^{j})^{2}\\|\\pmb{a}_{j}\\|_{2}^{2}=}\\end{array}$ $\\mathbb{E}_{j}\\left[\\|\\pmb{y}_{\\ast}^{j}\\pmb{a}_{j}\\|_{2}^{2}\\right]$ for $(i i)$ . Taking expectation w.r.t. allthe randomness on both sides and using the law of total expectation, we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathcal{T}_{2}]\\leq\\mathbb{E}\\Big[\\frac{\\eta_{k}^{3}n\\hat{L}_{\\pi^{(k)}}\\tilde{L}_{\\pi^{(k)}}}{b^{2}}\\|y_{k}-y_{*,k}\\|_{\\Lambda_{k}^{-1}}^{2}+\\frac{\\eta_{k}}{2n}\\|v_{k}-y_{k}\\|_{\\Lambda_{k}^{-1}}^{2}\\Big]+\\frac{\\eta_{k}^{3}\\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For the SO scheme, since there is only one random permutation generated at the very beginning, we can take expectation w.r.t. all the randomness on both sides of (27), and the randomness for the term $\\begin{array}{r}{\\frac{\\eta_{k}^{3}\\tilde{L}}{n b}\\mathbb{E}\\bigg[\\sum_{i=1}^{\\cdot}\\|A_{k}^{\\top}I_{b(i-1)\\uparrow}y_{*,k}\\|_{2}^{2}\\bigg]}\\end{array}$ is oly from the inialrandom permutation So the above argument still applies to this case, and we complete the proof. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "Theorem 2. Under Assumptions $^3$ and $^{4}$ if $\\begin{array}{r}{\\eta_{k}\\,\\leq\\,\\frac{b}{n\\sqrt{2\\hat{L}_{\\pi^{(k)}}\\tilde{L}_{\\pi^{(k)}}}}}\\end{array}$ and $\\begin{array}{r}{H_{K}=\\sum_{k=1}^{K}\\eta_{k}}\\end{array}$ , then the output $\\hat{\\pmb{x}}_{K}$ of Alg. 1 with uniformly random (RR/SO) shuffling satisfies ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}[H_{K}(f(\\hat{{\\boldsymbol{x}}}_{K})-f({\\boldsymbol{x}}_{*}))]\\leq\\frac{b}{2n}\\|{\\boldsymbol{x}}_{0}-{\\boldsymbol{x}}_{*}\\|_{2}^{2}+\\sum_{k=1}^{K}\\frac{\\eta_{k}^{3}\\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "As a result, given $\\epsilon>0$ , there exists a constant step size $\\eta_{k}=\\eta$ suchthat $\\mathbb{E}[f({\\hat{\\mathbf{x}}}_{K})-f(\\mathbf{x}_{*})]\\leq\\epsilon$ afer $\\begin{array}{r}{\\mathcal{O}\\big(\\frac{n\\sqrt{\\hat{L}\\tilde{L}}\\|\\mathbf{x}_{0}-\\mathbf{x}_{*}\\|_{2}^{2}}{\\epsilon}+\\sqrt{\\frac{(n-b)(n+b)}{n(n-1)}}\\frac{\\sqrt{n\\tilde{L}}\\sigma_{*}\\|\\mathbf{x}_{0}-\\mathbf{x}_{*}\\|_{2}^{2}}{\\epsilon^{3/2}}\\big)}\\end{array}$ m-b)(n+b) /nio\u221e2) individual gradient queries. ", "page_idx": 31}, {"type": "text", "text": "Proof. Combining the bounds in Lemma 10 and 11 and plugging them into Eq. (17), we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathcal{E}_{k}]\\leq\\mathbb{E}\\Big[\\Big(\\frac{\\eta_{k}^{3}n\\hat{L}_{\\pi^{(k)}}\\tilde{L}_{\\pi^{(k)}}}{b^{2}}-\\frac{\\eta_{k}}{2n}\\Big)\\|y_{k}-y_{*,k}\\|_{\\Lambda_{k}^{-1}}^{2}\\Big]+\\frac{\\eta_{k}^{3}\\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "$\\eta_{k}$ such that $\\begin{array}{r}{\\eta_{k}\\leq\\frac{b}{n\\sqrt{2\\hat{L}_{\\pi^{(k)}}\\tilde{L}_{\\pi^{(k)}}}}}\\end{array}$ we have $\\begin{array}{r}{\\frac{\\eta_{k}^{3}n\\hat{L}_{\\pi^{(k)}}\\tilde{L}_{\\pi^{(k)}}}{b^{2}}-\\frac{\\eta_{k}}{2n}\\le0}\\end{array}$ thus ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathcal{E}_{k}]\\leq\\frac{\\eta_{k}^{3}\\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Noticing that $\\begin{array}{r}{\\mathcal{E}_{k}=\\eta_{k}\\mathrm{Gap}^{v}(\\boldsymbol{x}_{k},\\boldsymbol{y}_{*})+\\frac{b}{2n}\\|\\boldsymbol{x}_{*}-\\boldsymbol{x}_{k}\\|_{2}^{2}-\\frac{b}{2n}\\|\\boldsymbol{x}_{*}-\\boldsymbol{x}_{k-1}\\|_{2}^{2}}\\end{array}$ and telescoping from $k=1$ to $K$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\sum_{k=1}^{K}\\eta_{k}\\mathrm{Gap}^{v}(x_{k},y_{*})\\Big]\\leq\\frac{b}{2n}\\|x_{*}-x_{0}\\|_{2}^{2}-\\frac{b}{2n}\\mathbb{E}[\\|x_{*}-x_{K}\\|_{2}^{2}]+\\sum_{k=1}^{K}\\frac{\\eta_{k}^{3}\\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Noticing that $\\mathcal{L}(\\boldsymbol{\\mathbf{\\boldsymbol{x}}},\\boldsymbol{\\mathbf{\\boldsymbol{v}}})$ is convex w.rt. $\\textbf{\\em x}$ we have $\\begin{array}{r}{\\mathrm{Gap}^{v}(\\hat{x}_{K},y_{*})\\leq\\sum_{k=1}^{K}\\eta_{k}\\mathrm{Gap}^{v}(x_{k},y_{*})/H_{K},}\\end{array}$ where $\\begin{array}{r}{\\hat{\\pmb{x}}_{K}=\\sum_{k=1}^{K}{\\eta_{k}\\pmb{x}_{k}/H_{K}}}\\end{array}$ and $\\begin{array}{r}{H_{K}=\\sum_{k=1}^{K}\\eta_{k}}\\end{array}$ which leads to ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[H_{K}\\mathsf{G a p}^{v}(\\hat{\\boldsymbol{x}}_{K},\\boldsymbol{y}_{*})\\Big]\\leq\\frac{b}{2n}\\|\\boldsymbol{x}_{0}-\\boldsymbol{x}_{*}\\|_{2}^{2}+\\sum_{k=1}^{K}\\frac{\\eta_{k}^{3}\\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Further choosing ${\\pmb v}={\\pmb y}_{\\hat{\\pmb x}_{K}}$ , we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}[H_{K}\\big(f(\\hat{{\\boldsymbol{x}}}_{K})-f({\\boldsymbol{x}}_{*})\\big)]\\le\\frac{b}{2n}\\|{\\boldsymbol{x}}_{0}-{\\boldsymbol{x}}_{*}\\|_{2}^{2}+\\sum_{k=1}^{K}\\frac{\\eta_{k}^{3}\\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "To analyze the individual gradient oracle complexity, we choose constant stesizes $\\begin{array}{r}{\\eta\\leq\\frac{b}{n\\sqrt{2\\hat{L}\\hat{L}}}}\\end{array}$ ,then Eq. (28) will become ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(\\hat{x}_{K})-f(\\pmb{x}_{*})]\\le\\frac{b}{2n\\eta K}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}+\\frac{\\eta^{2}\\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Without loss of generality, we assume that $b\\neq n$ , otherwise the method and its analysis reduce to (full) gradient descent. We consider the following two cases: ", "page_idx": 32}, {"type": "text", "text": ". \u201cSmall $K^{\\bullet}$ case: if $\\begin{array}{r}{\\eta=\\frac{b}{n\\sqrt{2\\hat{L}\\tilde{L}}}\\leq\\left(\\frac{3b^{3}(n-1)\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{n(n-b)(n+b)\\tilde{L}K\\sigma_{*}^{2}}\\right)^{1/3}}\\end{array}$ we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[f(\\hat{\\pmb{x}}_{K})-f(\\pmb{x}_{*})]\\leq\\frac{b}{2n\\eta K}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}+\\frac{\\eta^{2}\\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\sqrt{\\hat{L}\\tilde{L}}}{\\sqrt{2}K}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}+\\frac{1}{2}\\Big(\\frac{(n-b)(n+b)}{n^{2}(n-1)}\\Big)^{1/3}\\frac{\\tilde{L}^{1/3}\\sigma_{*}^{2/3}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{4/3}}{3^{1/3}K^{2/3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "\u00b7\u201cLarge $K^{\\bullet}$ case: if $\\begin{array}{r}{\\eta=\\left(\\frac{3b^{3}(n-1)\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{n(n-b)(n+b)\\tilde{L}K\\sigma_{*}^{2}}\\right)^{1/3}\\leq\\frac{b}{n\\sqrt{2\\hat{L}\\tilde{L}}}}\\end{array}$ we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[f(\\hat{\\pmb{x}}_{K})-f(\\pmb{x}_{*})]\\leq\\frac{b}{2n\\eta K}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}+\\frac{\\eta^{2}\\tilde{L}(n-b)(n+b)}{6b^{2}(n-1)}\\sigma_{*}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\Big(\\frac{(n-b)(n+b)}{n^{2}(n-1)}\\Big)^{1/3}\\frac{\\tilde{L}^{1/3}\\sigma_{*}^{2/3}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{4/3}}{3^{1/3}K^{2/3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Combining these two cases by setting $\\begin{array}{r}{\\eta=\\operatorname*{min}\\Big\\{\\frac{b}{n\\sqrt{2\\hat{L}\\tilde{L}}},\\,\\Big(\\frac{3b^{3}(n-1)\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{n(n-b)(n+b)\\tilde{L}K\\sigma_{*}^{2}}\\Big)^{1/3}\\Big\\}}\\end{array}$ ,we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(\\hat{x}_{K})-f(x_{*})]\\leq\\frac{\\sqrt{\\hat{L}\\tilde{L}}}{\\sqrt{2}K}\\|x_{0}-x_{*}\\|_{2}^{2}+\\Big(\\frac{(n-b)(n+b)}{n^{2}(n-1)}\\Big)^{1/3}\\frac{\\tilde{L}^{1/3}\\sigma_{*}^{2/3}\\|x_{0}-x_{*}\\|_{2}^{4/3}}{3^{1/3}K^{2/3}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Hence, to guarantee $\\mathbb{E}[f(\\hat{\\mathbf{x}}_{K})\\,-\\,f(\\mathbf{x}_{*})]\\,\\le\\,\\epsilon$ for $\\epsilon\\,>\\,0$ , the total number of individual gradient evaluations will be ", "page_idx": 32}, {"type": "equation", "text": "$$\nn K\\geq\\operatorname*{max}\\Big\\{\\frac{n\\sqrt{2\\hat{L}\\tilde{L}}\\|x_{0}-x_{*}\\|_{2}^{2}}{\\epsilon},\\Big(\\frac{(n-b)(n+b)}{n-1}\\Big)^{1/2}\\frac{2^{3/2}\\tilde{L}^{1/2}\\sigma_{*}\\|x_{0}-x_{*}\\|_{2}^{2}}{3^{1/2}\\epsilon^{3/2}}\\Big\\},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "as claimed. ", "page_idx": 32}, {"type": "text", "text": "C.2 Omitted Proofs for Incremental Gradient Descenet ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We now provide the proof for convergence of IGD in the smooth convex settings. We first prove the following technical lemma, which bounds the inner product tem $\\begin{array}{r l r}{\\mathcal{T}_{2}\\;:=}&{{}\\overline{{\\eta_{k}}}\\sum_{i=1}^{m}\\left(\\bar{\\pmb{v^{(i)}}}-\\right.}\\end{array}$ $\\pmb{y}_{k}^{(i)})^{\\top}\\pmb{A}^{(i)}(\\pmb{x}_{k}-\\pmb{x}_{k-1,i})$ Without random permutations involved. ", "page_idx": 32}, {"type": "text", "text": "Lemma 12. For any $k\\in[K]$ the iterates $\\{y_{k}^{(i)}\\}_{i=1}^{m}$ and $\\{\\pmb{x}_{k-1,i}\\}_{i=1}^{m+1}$ generated by Algorithm 2 with fixed data ordering satisfy ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{2}\\leq\\displaystyle\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}\\tilde{L}_{0}\\|y_{k}-y_{*}\\|_{\\Lambda^{-1}}^{2}+\\displaystyle\\frac{\\eta_{k}}{2n}\\|v-y_{k}\\|_{\\Lambda^{-1}}^{2}}\\\\ &{\\qquad+\\operatorname*{min}\\Big\\{\\displaystyle\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}\\tilde{L}_{0}\\|y_{*}\\|_{\\Lambda^{-1}}^{2},\\displaystyle\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}\\sigma_{*}^{2}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. Proceeding as in Lemma 11, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle T_{2}:=\\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}\\left({\\boldsymbol{v}}^{(i)}-{\\boldsymbol{y}}_{k}^{(i)}\\right)^{\\top}\\!A^{(i)}({\\boldsymbol{x}}_{k}-{\\boldsymbol{x}}_{k-1,i})}\\ ~}\\\\ {{\\mathrm{~}}}\\\\ {{\\displaystyle~~=~-\\frac{\\eta_{k}^{2}}{b n}\\sum_{i=1}^{m}\\langle A^{\\top}I_{b(i-1)\\uparrow}y_{k},A^{\\top}I_{(i)}({\\boldsymbol{v}}-{\\boldsymbol{y}}_{k})\\rangle}\\ ~}\\\\ {{\\displaystyle~~=~-\\frac{\\eta_{k}^{2}}{b n}\\sum_{i=1}^{m}\\langle A^{\\top}I_{b(i-1)\\uparrow}(y_{k}-y_{*}),A^{\\top}I_{(i)}({\\boldsymbol{v}}-{\\boldsymbol{y}}_{k})\\rangle}\\ ~}\\\\ {{\\displaystyle~~~~-\\frac{\\eta_{k}^{2}}{b n}\\sum_{i=1}^{m}\\langle A^{\\top}I_{b(i-1)\\uparrow}y_{*},A^{\\top}I_{(i)}({\\boldsymbol{v}}-{\\boldsymbol{y}}_{k})\\rangle\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For both terms in Eq. (30) and Eq. (31), we use Young's inequality for $\\alpha=2\\eta_{k}\\tilde{L}_{0}>0$ and proceed as in Eq. (24) to obtain ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\cfrac{\\eta_{k}^{2}}{b n}\\displaystyle\\sum_{i=1}^{m}\\big\\langle A^{\\top}I_{b(i-1)\\uparrow}(y_{k}-y_{*}),A^{\\top}I_{(i)}(v-y_{k})\\big\\rangle}\\\\ &{\\leq\\cfrac{\\eta_{k}^{2}\\alpha}{2b n}\\displaystyle\\sum_{i=1}^{m}\\|A^{\\top}I_{b(i-1)\\uparrow}(y_{k}-y_{*})\\|_{2}^{2}+\\cfrac{\\eta_{k}^{2}}{2b n\\alpha}\\displaystyle\\sum_{i=1}^{m}\\|A^{\\top}I_{(i)}(v-y_{k})\\|_{2}^{2}}\\\\ &{\\leq\\cfrac{\\eta_{k}^{2}n\\alpha}{2b^{2}}\\hat{L}_{0}\\|y_{k}-y_{*}\\|_{\\boldsymbol{\\Lambda}^{-1}}^{2}+\\cfrac{\\eta_{k}^{2}}{2n\\alpha}\\tilde{L}_{0}\\|v-y_{k}\\|_{\\boldsymbol{\\Lambda}^{-1}}^{2}}\\\\ &{=\\cfrac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}\\tilde{L}_{0}\\|y_{k}-y_{*}\\|_{\\boldsymbol{\\Lambda}^{-1}}^{2}+\\cfrac{\\eta_{k}}{4n}\\|v-y_{k}\\|_{\\boldsymbol{\\Lambda}^{-1}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\frac{\\eta_{k}^{2}}{b n}\\displaystyle\\sum_{i=1}^{m}\\langle A^{\\top}I_{b(i-1)\\uparrow}y_{*},A^{\\top}I_{(i)}(v-y_{k})\\rangle}\\\\ &{\\leq\\frac{\\eta_{k}^{2}\\alpha}{2b n}\\displaystyle\\sum_{i=1}^{m}\\|A^{\\top}I_{b(i-1)\\uparrow}y_{*}\\|_{2}^{2}+\\frac{\\eta_{k}^{2}}{2b n\\alpha}\\displaystyle\\sum_{i=1}^{m}\\|A^{\\top}I_{(i)}(v-y_{k})\\|_{2}^{2}}\\\\ &{\\leq\\frac{\\eta_{k}^{2}\\alpha}{2b n}\\displaystyle\\sum_{i=1}^{m}\\|A^{\\top}I_{b(i-1)\\uparrow}y_{*}\\|_{2}^{2}+\\frac{\\eta_{k}^{2}}{2n\\alpha}\\tilde{L}_{0}\\|v-y_{k}\\|_{\\Lambda^{-1}}^{2}}\\\\ &{=\\frac{\\eta_{k}^{3}\\tilde{L}_{0}}{n b}\\displaystyle\\sum_{i=1}^{m}\\|A^{\\top}I_{b(i-1)\\uparrow}y_{*}\\|_{2}^{2}+\\frac{\\eta_{k}}{4n}\\|v-y_{k}\\|_{\\Lambda^{-1}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "$\\alpha=2\\eta_{k}\\tilde{L}_{0}$ $\\begin{array}{r}{\\frac{\\eta_{k}^{3}\\tilde{L}_{0}}{n b}\\sum_{i=1}^{m}\\|A^{\\top}I_{b(i-1)\\uparrow}y_{\\ast}\\|_{2}^{2}}\\end{array}$ is no larger than the minimum of $\\begin{array}{r}{\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}\\tilde{L}_{0}\\lVert\\boldsymbol{y}_{*}\\rVert_{\\mathbf{A}^{-1}}^{2}}\\end{array}$ $\\frac{\\eta_{k}^{3}(n\\!-\\!b)^{2}}{b^{2}}\\tilde{L}_{0}\\sigma_{*}^{2}$ . Note that when $b=n$ we have $A^{\\top}I_{(0)\\uparrow}{\\pmb y}_{*}=0$ , o this term disappears. When $b<n$ , the former one can be derived as in Eq.(24), which gives ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{m}\\|A^{\\top}I_{b(i-1)\\uparrow}y_{*}\\|_{2}^{2}\\leq\\Big\\|\\mathbf{A}^{1/2}\\Big(\\displaystyle\\sum_{i=1}^{m}I_{b(i-1)\\uparrow}A A^{\\top}I_{b(i-1)\\uparrow}\\Big)\\mathbf{A}^{1/2}\\Big\\|_{2}\\|y_{*}\\|_{\\mathbf{A}^{-1}}^{2}=m n\\hat{L}_{0}\\|y_{*}\\|_{\\mathbf{A}^{-1}}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\frac{n^{2}}{b}\\hat{L}_{0}\\|y_{*}\\|_{\\mathbf{A}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For the latter one, we notice that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=1}^{m}\\|A^{\\top}I_{b(i-1)\\uparrow}y_{*}\\|_{2}^{2}=\\displaystyle\\sum_{i=1}^{m}\\Big\\|\\displaystyle\\sum_{j=b(i-1)+1}^{n}y_{*}^{j}a_{j}\\Big\\|_{2}^{2}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad=}&{\\displaystyle\\sum_{i=0}^{m-1}\\Big\\|\\displaystyle\\sum_{j=b i+1}^{n}y_{*}^{j}a_{j}\\Big\\|_{2}^{2}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad=}&{\\displaystyle\\sum_{i=1}^{m-1}\\Big\\|\\displaystyle\\sum_{j=b i+1}^{n}y_{*}^{j}a_{j}\\Big\\|_{2}^{2}=\\displaystyle\\sum_{i=1}^{m-1}\\Big\\|\\displaystyle\\sum_{j=1}^{b i}y_{*}^{j}a_{j}\\Big\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "by using the fact that $\\begin{array}{r}{\\sum_{j=1}^{n}y_{*}^{j}\\mathbf{a}_{j}=0}\\end{array}$ Using Young's inequality, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{m-1}\\left\\|\\displaystyle\\sum_{j=1}^{b i}y_{*}^{j}a_{j}\\right\\|_{2}^{2}\\leq\\displaystyle\\sum_{i=1}^{m-1}b i\\displaystyle\\sum_{j=1}^{b i}\\|y_{*}^{j}a_{j}\\|_{2}^{2}}\\\\ &{\\leq b(m-1)\\displaystyle\\sum_{i=1}^{m-1}\\sum_{j=1}^{b i}\\|y_{*}^{j}a_{j}\\|_{2}^{2}}\\\\ &{=b(m-1)\\displaystyle\\sum_{i=1}^{m-1}\\sum_{j=b(i-1)+1}^{b i}(m-i)\\|y_{*}^{j}a_{j}\\|_{2}^{2}}\\\\ &{\\leq b(m-1)^{2}\\displaystyle\\sum_{i=1}^{m-1)}\\ \\|y_{*}^{j}a_{j}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By the defintionthat $\\begin{array}{r}{\\sigma_{*}^{2}=\\frac{1}{n}\\sum_{j=1}^{n}\\|\\pmb{y}_{*}^{j}\\pmb{a}_{j}\\|_{2}^{2}}\\end{array}$ and $\\begin{array}{r}{\\sum_{i=i}^{(m-1)b}\\|\\pmb{y}_{*}^{j}\\pmb{a}_{j}\\|_{2}^{2}\\leq\\sum_{j=1}^{n}\\|\\pmb{y}_{*}^{j}\\pmb{a}_{j}\\|_{2}^{2}=n\\sigma_{*}^{2}}\\end{array}$ we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{\\eta_{k}^{3}\\tilde{L}_{0}}{n b}\\sum_{i=1}^{m}\\|A^{\\top}I_{b(i-1)\\uparrow}y_{*}\\|_{2}^{2}\\leq\\frac{\\eta_{k}^{3}\\tilde{L}_{0}}{b}b(m-1)^{2}\\sigma_{*}^{2}=\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}\\sigma_{*}^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Note that the bound in Eq. (34) equals to zero when $b=n$ , which recovers the case of full gradient descent, sowe have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{\\eta_{k}^{3}\\tilde{L}_{0}}{n b}\\sum_{i=1}^{m}\\|A^{\\top}I_{b(i-1)\\uparrow}y_{*}\\|_{2}^{2}\\leq\\operatorname*{min}\\Big\\{\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}\\tilde{L}_{0}\\|y_{*}\\|_{\\mathbf{A}^{-1}}^{2},\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}\\sigma_{*}^{2}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Combining Eq. (32)-(35), we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\nr_{2}\\leq\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}\\tilde{L}_{0}\\|y_{k}-y_{*}\\|_{\\Lambda^{-1}}^{2}+\\frac{\\eta_{k}}{2n}\\|v-y_{k}\\|_{\\Lambda^{-1}}^{2}+\\operatorname*{min}\\Big\\{\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}\\tilde{L}_{0}\\|y_{*}\\|_{\\Lambda^{-1}}^{2},\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}\\sigma_{*}^{2}\\Big\\},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "thus finishing the proof. ", "page_idx": 34}, {"type": "text", "text": "Theorem 6. Under Assumptions 3 and 4, if $\\begin{array}{r}{\\eta_{k}\\le\\frac{b}{n\\sqrt{2\\hat{L}_{0}\\tilde{L}_{0}}}}\\end{array}$ and $\\begin{array}{r}{H_{K}=\\sum_{k=1}^{K}\\eta_{k}}\\end{array}$ the output $\\hat{\\pmb{x}}_{K}$ of Alg. 2 with a fixed permutation satisfies ", "page_idx": 34}, {"type": "equation", "text": "$$\nH_{K}\\left(f(\\hat{x}_{K})-f(x_{*})\\right)\\leq\\frac{b}{2n}\\|x_{0}-x_{*}\\|_{2}^{2}+\\sum_{k=1}^{K}\\operatorname*{min}\\Big\\{\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}\\tilde{L}_{0}\\|y_{*}\\|_{\\Lambda^{-1}}^{2},\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}\\sigma_{*}^{2}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "As a consequence, given $\\epsilon\\mathrm{~\\ensuremath~{~\\leftmoon~}~}>\\mathrm{~\\ensuremath~{~\\leftmoon~}~}$ , there exists a constant step size $\\begin{array}{r l r}{\\eta_{k}}&{{}=}&{\\eta}\\end{array}$ suchthat f(mk)f(x)\u2264 afer the numberof gradient queries bounded by O(mio $\\frac{\\operatorname*{min}\\left\\{\\sqrt{n\\hat{L}_{0}\\tilde{L}_{0}}\\lVert\\mathbf{y}_{*}\\rVert_{\\mathbf{A}-1},(n{-}b)\\sqrt{\\tilde{L}_{0}}\\sigma_{*}\\right\\}\\lVert\\mathbf{x}_{0}-\\mathbf{x}_{*}\\rVert_{2}^{2}}{\\epsilon^{3/2}}\\Bigg).$ ", "page_idx": 34}, {"type": "text", "text": "Proof. Proceeding as in Lemmas 9 and 10, but without random permutations, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal E}_{k}\\leq\\displaystyle\\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}y_{k}^{(i)^{\\top}}A^{{(i)}}({\\pmb x}_{k}-{\\pmb x}_{k-1,i+1})+\\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}\\big({\\pmb v}^{(i)}-{\\pmb y}_{k}^{(i)}\\big)^{\\top}A^{{(i)}}({\\pmb x}_{k}-{\\pmb x}_{k-1,i})}\\\\ &{\\qquad-\\displaystyle\\frac{\\eta_{k}}{2n}\\|y_{k}-{\\pmb v}\\|_{{\\pmb x}^{-1}}^{2}-\\frac{\\eta_{k}}{2n}\\|y_{k}-{\\pmb y}_{*}\\|_{{\\pmb x}^{-1}}^{2}-\\frac{b}{2n}\\sum_{i=1}^{m}\\|{\\pmb x}_{k-1,i}-{\\pmb x}_{k-1,i+1}\\|_{2}^{2}}\\\\ &{\\leq\\displaystyle\\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}\\big({\\pmb v}^{(i)}-{\\pmb y}_{k}^{(i)}\\big)^{\\top}A_{k}^{(i)}({\\pmb x}_{k}-{\\pmb x}_{k-1,i})-\\frac{\\eta_{k}}{2n}\\|y_{k}-{\\pmb v}\\|_{{\\pmb x}^{-1}}^{2}-\\frac{\\eta_{k}}{2n}\\|y_{k}-{\\pmb y}_{*}\\|_{{\\pmb x}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Using the bound in Lemma 12 and applying Eq. (29) into Eq. (36), we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}\\leq\\Big(\\frac{\\eta_{k}^{3}n\\hat{L}_{0}\\tilde{L}_{0}}{b^{2}}-\\frac{\\eta_{k}}{2n}\\Big)\\|y_{k}-y_{*}\\|_{\\Lambda^{-1}}^{2}+\\operatorname*{min}\\Big\\{\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}\\tilde{L}_{0}\\|y_{*}\\|_{\\Lambda^{-1}}^{2},\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}\\sigma_{*}^{2}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "If $\\begin{array}{r}{\\eta_{k}\\leq\\frac{b}{n\\sqrt{2\\hat{L}_{0}\\tilde{L}_{0}}}}\\end{array}$ nV2ioi, we have ? $\\begin{array}{r}{\\frac{\\eta_{k}^{3}n\\hat{L}_{0}\\tilde{L}_{0}}{b^{2}}-\\frac{\\eta_{k}}{2n}\\le0}\\end{array}$ , thus ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal E_{k}\\leq\\operatorname*{min}\\Big\\{\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}\\tilde{L}_{0}\\|y_{*}\\|_{\\mathbf{A}^{-1}}^{2},\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}\\sigma_{*}^{2}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Noticing that $\\begin{array}{r}{\\mathcal{E}_{k}=\\eta_{k}\\mathrm{Gap}^{v}(\\boldsymbol{x}_{k},\\boldsymbol{y}_{*})+\\frac{b}{2n}\\|\\boldsymbol{x}_{*}-\\boldsymbol{x}_{k}\\|_{2}^{2}-\\frac{b}{2n}\\|\\boldsymbol{x}_{*}-\\boldsymbol{x}_{k-1}\\|_{2}^{2}}\\end{array}$ and telescoping from $k=1$ to $K$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{K}\\eta_{k}\\mathrm{Gap}^{\\upsilon}(x_{k},y_{*})}\\\\ &{\\displaystyle\\leq\\frac{b}{2n}\\|x_{*}-x_{0}\\|_{2}^{2}-\\frac{b}{2n}\\|x_{*}-x_{K}\\|_{2}^{2}+\\displaystyle\\sum_{k=1}^{K}\\operatorname*{min}\\Big\\{\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}\\tilde{L}_{0}\\|y_{*}\\|_{\\boldsymbol{\\Lambda}^{-1}}^{2},\\displaystyle\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}\\sigma_{*}^{2}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Noticing that $\\mathcal{L}(\\boldsymbol{\\mathbf{\\boldsymbol{x}}},\\boldsymbol{\\mathbf{\\boldsymbol{v}}})$ is convex w.t. $\\textbf{\\em x}$ we have $\\begin{array}{r}{\\mathrm{Gap}^{v}(\\hat{x}_{K},y_{*})\\leq\\sum_{k=1}^{K}\\eta_{k}\\mathrm{Gap}^{v}(x_{k},y_{*})/H_{K},}\\end{array}$ where $\\begin{array}{r}{\\hat{\\pmb{x}}_{K}=\\sum_{k=1}^{K}{\\eta_{k}\\pmb{x}_{k}/H_{K}}}\\end{array}$ and $\\begin{array}{r}{H_{K}=\\sum_{k=1}^{K}\\eta_{k}}\\end{array}$ so we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\nH_{K}\\mathrm{Gap}^{v}(\\hat{x}_{K},y_{*})\\leq\\frac{b}{2n}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}+\\sum_{k=1}^{K}\\operatorname*{min}\\Big\\{\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}\\tilde{L}_{0}\\|y_{*}\\|_{\\mathbf{A}^{-1}}^{2},\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}\\sigma_{*}^{2}\\Big\\},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Further choosing ${\\pmb v}={\\pmb y}_{\\hat{\\pmb x}_{K}}$ , we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\nH_{K}\\big(f(\\hat{x}_{K})-f(x_{*})\\big)\\leq\\frac{b}{2n}\\|x_{0}-x_{*}\\|_{2}^{2}+\\sum_{k=1}^{K}\\operatorname*{min}\\Big\\{\\frac{\\eta_{k}^{3}n}{b^{2}}\\hat{L}_{0}\\tilde{L}_{0}\\|y_{*}\\|_{\\Lambda^{-1}}^{2},\\frac{\\eta_{k}^{3}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}\\sigma_{*}^{2}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Toanalyndivia gradi raclecmleitweestant stei $\\begin{array}{r}{\\eta\\leq\\frac{b}{n\\sqrt{2\\hat{L}_{0}\\tilde{L}_{0}}}}\\end{array}$ and assume $b<n$ without loss of generality, then Eq. (37) becomes ", "page_idx": 35}, {"type": "equation", "text": "$$\nf(\\hat{\\boldsymbol x}_{K})-f(\\boldsymbol x_{*})\\le\\frac{b}{2n\\eta K}\\|\\boldsymbol x_{0}-\\boldsymbol x_{*}\\|_{2}^{2}+\\operatorname*{min}\\Big\\{\\frac{\\eta^{2}n}{b^{2}}\\hat{L}_{0}\\tilde{L}_{0}\\|\\boldsymbol y_{*}\\|_{\\boldsymbol\\Lambda^{-1}}^{2},\\frac{\\eta^{2}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}\\sigma_{*}^{2}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "When $\\begin{array}{r}{\\hat{L}_{0}\\|y_{*}\\|_{\\Lambda^{-1}}^{2}\\,\\le\\,\\frac{(n-b)^{2}}{n}\\sigma_{*}^{2}}\\end{array}$ we set $\\begin{array}{r}{\\eta\\,=\\,\\operatorname*{min}\\Big\\{\\frac{b}{n\\sqrt{2\\hat{L}_{0}\\tilde{L}_{0}}},\\,\\left(\\frac{b^{3}\\|{\\pmb x}_{0}-{\\pmb x}_{*}\\|_{2}^{2}}{2n^{2}\\hat{L}_{0}\\tilde{L}_{0}K\\|{\\pmb y}_{*}\\|_{\\Lambda^{-1}}^{2}}\\right)^{1/3}\\Big\\}}\\end{array}$ and consider the following two possible cases: ", "page_idx": 35}, {"type": "text", "text": ".\"Small $K^{\\bullet}$ case: if $\\begin{array}{r}{\\eta=\\frac{b}{n\\sqrt{2\\hat{L}_{0}\\tilde{L}_{0}}}\\le\\left(\\frac{b^{3}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{2n^{2}\\hat{L}_{0}\\tilde{L}_{0}K\\|\\pmb{y}_{*}\\|_{\\Lambda^{-1}}^{2}}\\right)^{1/3}}\\end{array}$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\hat{\\pmb{x}}_{K})-f(\\pmb{x}_{*})\\leq\\displaystyle\\frac{b}{2n\\eta K}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}+\\frac{\\eta^{2}n}{b^{2}}\\hat{L}_{0}\\tilde{L}_{0}\\|\\pmb{y}_{*}\\|_{\\pmb{\\Lambda}^{-1}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{\\sqrt{\\hat{L}_{0}\\tilde{L}_{0}}}{\\sqrt{2}K}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}+\\frac{\\hat{L}_{0}^{1/3}\\tilde{L}_{0}^{1/3}\\|\\pmb{y}_{*}\\|_{\\pmb{\\Lambda}^{-1}}^{2/3}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{4/3}}{2^{2/3}n^{1/3}K^{2/3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "\u00b7\u201cLarge $K^{\\bullet}$ case: if $\\begin{array}{r}{\\eta=\\left(\\frac{b^{3}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{2n^{2}\\hat{L}_{0}\\tilde{L}_{0}K\\|\\pmb{y}_{*}\\|_{\\pmb{\\Lambda}^{2}1}^{2}}\\right)^{1/3}\\leq\\frac{b}{\\sqrt{2\\hat{L}_{0}\\tilde{L}_{0}}}}\\end{array}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\hat{\\pmb{x}}_{K})-f(\\pmb{x}_{*})\\leq\\displaystyle\\frac{b}{2n\\eta K}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}+\\frac{\\eta^{2}n}{b^{2}}\\hat{L}_{0}\\tilde{L}_{0}\\|\\pmb{y}_{*}\\|_{\\boldsymbol{\\Lambda}^{-1}}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{2^{1/3}\\hat{L}_{0}^{1/3}\\tilde{L}_{0}^{1/3}\\|\\pmb{y}_{*}\\|_{\\boldsymbol{\\Lambda}^{-1}}^{2/3}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{4/3}}{n^{1/3}K^{2/3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Combining these two cases, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\nf(\\hat{x}_{K})-f(x_{*})\\leq\\frac{\\sqrt{\\hat{L}_{0}\\tilde{L}_{0}}}{\\sqrt{2}K}\\|x_{0}-x_{*}\\|_{2}^{2}+\\frac{2^{1/3}\\hat{L}_{0}^{1/3}\\tilde{L}_{0}^{1/3}\\|y_{*}\\|_{\\Lambda^{-1}}^{2/3}\\|x_{0}-x_{*}\\|_{2}^{4/3}}{n^{1/3}K^{2/3}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Hence, to guarantee $\\mathbb{E}[f(\\hat{\\mathbf{x}}_{K})\\,-\\,f(\\mathbf{x}_{*})]\\,\\le\\,\\epsilon$ for $\\epsilon\\,>\\,0$ , the total number of individual gradient evaluations will be ", "page_idx": 36}, {"type": "equation", "text": "$$\nn K\\ge\\operatorname*{max}\\Big\\{\\frac{n\\sqrt{2\\hat{L}_{0}\\tilde{L}_{0}}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{\\epsilon},\\frac{4n^{1/2}\\hat{L}_{0}^{1/2}\\tilde{L}_{0}^{1/2}\\|\\pmb{y}_{*}\\|_{\\mathbf{\\hat{A}}^{-1}}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{\\epsilon^{3/2}}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "When $\\begin{array}{r}{\\frac{(n-b)^{2}}{n}\\sigma_{*}^{2}\\leq\\hat{L}_{0}\\|\\pmb{\\{y}}_{*}\\|_{\\pmb{\\Lambda}^{-1}}^{2}}\\end{array}$ we set $\\begin{array}{r}{\\eta=\\operatorname*{min}\\left\\{\\frac{b}{n\\sqrt{2\\hat{L}_{0}\\tilde{L}_{0}}}\\right.}\\end{array}$ $\\begin{array}{r}{\\left(\\frac{b^{3}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{2n(n-b)^{2}\\tilde{L}_{0}K\\sigma_{*}^{2}}\\right)^{1/3}\\}}\\end{array}$ and consider the two cases as below: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\hat{{\\boldsymbol x}}_{K})-f({\\boldsymbol x}_{*})\\leq\\frac{b}{2n\\eta K}\\|{\\boldsymbol x}_{0}-{\\boldsymbol x}_{*}\\|_{2}^{2}+\\frac{\\eta^{2}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}\\sigma_{*}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\sqrt{\\hat{L}_{0}\\tilde{L}_{0}}}{\\sqrt{2}K}\\|{\\boldsymbol x}_{0}-{\\boldsymbol x}_{*}\\|_{2}^{2}+\\frac{(n-b)^{2/3}\\tilde{L}_{0}^{1/3}\\sigma_{*}^{2/3}\\|{\\boldsymbol x}_{0}-{\\boldsymbol x}_{*}\\|_{2}^{4/3}}{2^{2/3}n^{2/3}K^{2/3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(\\hat{\\pmb{x}}_{K})-f(\\pmb{x}_{*})\\leq\\frac{b}{2n\\eta K}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}+\\frac{\\eta^{2}(n-b)^{2}}{b^{2}}\\tilde{L}_{0}\\sigma_{*}^{2}}\\\\ {\\leq\\frac{2^{1/3}\\left(n-b\\right)^{2/3}\\tilde{L}_{0}^{1/3}\\sigma_{*}^{2/3}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{4/3}}{n^{2/3}K^{2/3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Combining these two cases, we obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\nf(\\hat{x}_{K})-f(x_{*})\\leq\\frac{\\sqrt{\\hat{L}_{0}\\tilde{L}_{0}}}{\\sqrt{2}K}\\|x_{0}-x_{*}\\|_{2}^{2}+\\frac{2^{1/3}(n-b)^{2/3}\\tilde{L}_{0}^{1/3}\\sigma_{*}^{2/3}\\|x_{0}-x_{*}\\|_{2}^{4/3}}{n^{2/3}K^{2/3}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "To guarantee $\\mathbb{E}[f({\\hat{\\mathbf{x}}}_{K})-f(\\mathbf{x}_{*})]\\leq\\epsilon$ for $\\epsilon>0$ , the total number of individual gradient evaluations will be ", "page_idx": 36}, {"type": "equation", "text": "$$\nn K\\geq\\operatorname*{max}\\bigg\\{\\frac{n\\sqrt{2\\hat{L}_{0}\\tilde{L}_{0}}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{\\epsilon},\\frac{4(n-b)\\tilde{L}_{0}^{1/2}\\sigma_{*}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{\\epsilon^{3/2}}\\bigg\\}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Combining Eq. (38) and Eq. (39), we finally have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n K\\geq\\frac{n\\sqrt{2}\\hat{L}_{0}\\tilde{L}_{0}^{-}\\|\\mathbf{x}_{0}-\\mathbf{x}_{*}\\|_{2}^{2}}{\\epsilon}}\\\\ &{\\qquad+\\operatorname*{min}\\Big\\{\\frac{4n^{1/2}\\hat{L}_{0}^{1/2}\\tilde{L}_{0}^{1/2}\\|\\mathbf{y}_{*}\\|_{\\mathbf{A}^{-1}}\\|\\mathbf{x}_{0}-\\mathbf{x}_{*}\\|_{2}^{2}}{\\epsilon^{3/2}},\\frac{4(n-b)\\tilde{L}_{0}^{1/2}\\sigma_{*}\\|\\mathbf{x}_{0}-\\mathbf{x}_{*}\\|_{2}^{2}}{\\epsilon^{3/2}}\\Big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "thus finishing the proof. ", "page_idx": 36}, {"type": "text", "text": "DOmitted Proofs for Non-Smooth Convex Setting From Section 3 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Before we prove Theorem 3 in convex Lipschitz settings, for completeness, we first recall the following standard first-order characterization of convexity. ", "page_idx": 37}, {"type": "text", "text": "Lemma 13. Let $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be a continuous convex function. Then, for any $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$ ", "page_idx": 37}, {"type": "equation", "text": "$$\nf({\\pmb y})\\geq f({\\pmb x})+\\left\\langle{\\pmb g}_{\\pmb x},{\\pmb y}-{\\pmb x}\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $g_{x}\\in\\partial f(x).$ and $\\partial f({\\pmb x})$ is the subdifferential of $f$ at $\\textbf{\\em x}$ ", "page_idx": 37}, {"type": "text", "text": "The following technical lemma provides a primal-dual gap bound in convex nonsmooth settings. ", "page_idx": 37}, {"type": "text", "text": "Lemma 14. For any $k\\in[K]$ the iterates $\\{y_{k}^{(i)}\\}_{i=1}^{m}$ and $\\{\\pmb{x}_{k-1,i}\\}_{i=1}^{m+1}$ generated by Algorithm 2 satisfy ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathcal{E}_{k}\\leq\\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}\\Big(y_{k}^{(i)\\top}A_{k}^{(i)}(x_{k}-x_{k-1,i+1})+({v}_{k}^{(i)}-{y}_{k}^{(i)})^{\\top}A_{k}^{(i)}(x_{k}-x_{k-1,i})\\Big)}\\\\ &{\\quad\\quad-\\displaystyle\\frac{b}{2n}\\sum_{i=1}^{m}\\|x_{k-1,i}-x_{k-1,i+1}\\|_{2}^{2},}\\\\ &{\\displaystyle\\mathcal{I}_{k}:=\\eta_{k}\\big(\\mathcal{L}(x_{k},v)-\\mathcal{L}(x_{*},y_{*})\\big)+\\frac{b}{2n}\\|x_{*}-x_{k}\\|_{2}^{2}-\\frac{b}{2n}\\|x_{*}-x_{k-1}\\|_{2}^{2}.}\\end{array}\n$$where ", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. By the same argument as in the proof for Lemma 9, we know that $\\pmb{a}_{\\pi_{j}^{(k)}}^{\\top}\\pmb{x}_{k-1,i}\\in\\partial\\ell_{\\pi_{j}^{(k)}}^{*}(\\pmb{y}_{k}^{j})$ for $b(i-1)+1\\leq j\\leq b i$ , thenbyLemma13wehave ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\ell_{\\pi_{j}^{(k)}}^{*}\\big(\\pmb{v}_{k}^{j}\\big)\\geq\\ell_{\\pi_{j}^{(k)}}^{*}(\\pmb{y}_{k}^{j})+\\pmb{a}_{\\pi_{j}^{(k)}}^{\\top}\\pmb{x}_{k-1,i}\\big(\\pmb{v}_{k}^{j}-\\pmb{y}_{k}^{j}\\big),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which leads to ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathcal{L}(\\boldsymbol{x}_{k},\\boldsymbol{v})=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{m}\\Big(v_{k}^{(i)\\top}A_{k}^{(i)}\\boldsymbol{x}_{k-1,i}-\\displaystyle\\sum_{j=b(i-1)+1}^{b i}\\ell_{\\pi_{j}^{(k)}}^{*}(v_{k}^{j})\\Big)+\\displaystyle\\frac{1}{n}\\sum_{i=1}^{m}v_{k}^{(i)\\top}A_{k}^{(i)}(\\boldsymbol{x}_{k}-\\boldsymbol{x}_{k-1,i})}\\\\ &{}&{\\le\\displaystyle\\frac{1}{n}\\sum_{i=1}^{m}\\Big(y_{k}^{(i)\\top}A_{k}^{(i)}\\boldsymbol{x}_{k-1,i}-\\displaystyle\\sum_{j=b(i-1)+1}^{b i}\\ell_{\\pi_{j}^{(k)}}^{*}(y_{k}^{j})\\Big)+\\displaystyle\\frac{1}{n}\\sum_{i=1}^{m}v_{k}^{(i)\\top}A_{k}^{(i)}(\\boldsymbol{x}_{k}-\\boldsymbol{x}_{k-1,i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Using the same argument for $\\mathcal{L}(x_{*},y_{*})$ as $\\pmb{a}_{j}^{\\top}\\pmb{x}_{\\ast}\\in\\partial\\ell_{j}^{\\ast}(\\pmb{y}_{\\ast}^{j})$ for $j\\in[n]$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\boldsymbol{x}_{*},\\boldsymbol{y}_{*})=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{m}\\Big(\\pmb{y}_{*,k}^{(i)\\top}\\pmb{A}_{k}^{(i)}\\pmb{x}_{*}-\\displaystyle\\sum_{j=b(i-1)+1}^{b i}\\ell_{\\pi_{j}^{(k)}}^{*}\\big(\\pmb{y}_{*,k}^{j}\\big)\\Big)}\\\\ {\\displaystyle\\geq\\frac{1}{n}\\sum_{i=1}^{m}\\Big(\\pmb{y}_{k}^{(i)\\top}\\pmb{A}_{k}^{(i)}\\pmb{x}_{*}-\\displaystyle\\sum_{j=b(i-1)+1}^{b i}\\ell_{\\pi_{j}^{(k)}}^{*}\\big(\\pmb{y}_{k}^{j}\\big)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Adding and substracting the term $\\begin{array}{r}{\\frac{b}{2n\\eta_{k}}\\sum_{i=1}^{m}\\|\\pmb{x}_{*}-\\pmb{x}_{k-1,i}\\|_{2}^{2}}\\end{array}$ on the R.H.S.of Eq. (42), we obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{L}(\\pmb{x}_{*},\\pmb{y}_{*})\\geq\\frac{1}{n}\\sum_{i=1}^{m}\\left(\\pmb{y}_{k}^{(i)\\top}\\pmb{A}_{k}^{(i)}\\pmb{x}_{*}+\\frac{b}{2\\eta_{k}}\\|\\pmb{x}_{*}-\\pmb{x}_{k-1,i}\\|_{2}^{2}-\\sum_{j=b(i-1)+1}^{b i}\\ell_{\\pi_{j}^{(k)}}^{*}(\\pmb{y}_{k}^{j})\\right)}\\\\ {\\displaystyle-\\,\\frac{b}{2n\\eta_{k}}\\sum_{i=1}^{m}\\|\\pmb{x}_{*}-\\pmb{x}_{k-1,i}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Denote $\\begin{array}{r}{\\phi_{k}^{(i)}({\\pmb x}):={\\pmb y}_{k}^{(i)\\top}{\\pmb A}_{k}^{(i)}{\\pmb x}+\\frac{b}{2\\eta_{k}}\\|{\\pmb x}-{\\pmb x}_{k-1,i}\\|_{2}^{2}}\\end{array}$ Whihis $\\frac{b}{\\eta_{k}}$ $\\textbf{\\em x}$ Notcing that $\\begin{array}{r}{\\pmb{x}_{k-1,i+1}=\\arg\\operatorname*{min}_{\\pmb{x}\\in\\mathbb{R}^{d}}\\left\\{\\pmb{y}_{k}^{(i)\\top}\\pmb{A}_{k}^{(i)}\\pmb{x}+\\frac{b}{2\\eta_{k}}\\|\\pmb{x}-\\pmb{x}_{k-1,i}\\|^{2}\\right\\}}\\end{array}$ by Line 7of Alg.2, we have $\\nabla\\phi_{k}^{(i)}({\\bf{x}}_{k-1,i+1})=\\mathbf{0}$ , which leads to ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\phi_{k}^{(i)}(\\pmb{x}_{*})\\geq\\phi_{k}^{(i)}(\\pmb{x}_{k-1,i+1})+\\frac{b}{2\\eta_{k}}\\|\\pmb{x}_{*}-\\pmb{x}_{k-1,i+1}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thus, we obtain ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathcal{L}({\\pmb x}_{*},{\\pmb y}_{*})\\geq\\frac{1}{n}\\sum_{i=1}^{m}\\left(y_{k}^{(i)^{\\top}}{\\pmb A}_{k}^{(i)}{\\pmb x}_{k-1,i+1}+\\frac{b}{2\\eta_{k}}\\|{\\pmb x}_{k-1,i+1}-{\\pmb x}_{k-1,i}\\|_{2}^{2}-\\sum_{j=b(i-1)+1}^{b i}\\ell_{\\pi_{j}^{(k)}}^{*}(y_{k}^{j})\\right)}\\\\ {\\displaystyle+\\frac{b}{2n\\eta_{k}}\\sum_{i=1}^{m}\\left(\\|{\\pmb x}_{*}-{\\pmb x}_{k-1,i+1}\\|_{2}^{2}-\\|{\\pmb x}_{*}-{\\pmb x}_{k-1,i}\\|_{2}^{2}\\right)}\\\\ {\\displaystyle\\overset{(i)}{=}\\frac{1}{n}\\sum_{i=1}^{m}\\left(y_{k}^{(i)^{\\top}}{\\pmb A}_{k}^{(i)}{\\pmb x}_{k-1,i+1}+\\frac{b}{2\\eta_{k}}\\|{\\pmb x}_{k-1,i+1}-{\\pmb x}_{k-1,i}\\|_{2}^{2}-\\sum_{j=b(i-1)+1}^{b i}\\ell_{\\pi_{j}^{(k)}}^{*}(y_{k}^{j})\\right)}\\\\ {\\displaystyle+\\frac{b}{2n\\eta_{k}}\\|{\\pmb x}_{k}-{\\pmb x}_{*}\\|_{2}^{2}-\\frac{b}{2n\\eta_{k}}\\|{\\pmb x}_{k-1}-{\\pmb x}_{*}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $(i)$ is by telecoping $\\begin{array}{r}{\\sum_{i=1}^{m}\\left(\\|\\pmb{x}_{*}-\\pmb{x}_{k-1,i+1}\\|_{2}^{2}-\\|\\pmb{x}_{*}-\\pmb{x}_{k-1,i}\\|_{2}^{2}\\right)}\\end{array}$ and using $\\pmb{x}_{k}=\\pmb{x}_{k-1,m+1}$ and $\\mathbf{\\Delta}\\mathbf{x}_{k-1}=\\mathbf{\\Delta}\\mathbf{x}_{k-1,1}$ , which both hold by definition. ", "page_idx": 38}, {"type": "text", "text": "Combining the bounds from Eq. (41) and Eq. (43), and denoting ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{E}_{k}:=\\eta_{k}\\big(\\mathcal{L}(\\boldsymbol{x}_{k},\\boldsymbol{v})-\\mathcal{L}(\\boldsymbol{x}_{*},\\boldsymbol{y}_{*})\\big)+\\frac{b}{2n}\\|\\boldsymbol{x}_{*}-\\boldsymbol{x}_{k}\\|_{2}^{2}-\\frac{b}{2n}\\|\\boldsymbol{x}_{*}-\\boldsymbol{x}_{k-1}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "we finally obtain ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}_{k}\\leq\\displaystyle\\frac{\\eta_{k}}{n}\\displaystyle\\sum_{i=1}^{m}y_{k}^{(i)\\top}A_{k}^{(i)}(x_{k-1,i}-x_{k-1,i+1})+\\displaystyle\\frac{\\eta_{k}}{n}\\displaystyle\\sum_{i=1}^{m}v_{k}^{(i)\\top}A_{k}^{(i)}(x_{k}-x_{k-1,i})}\\\\ &{\\qquad-\\displaystyle\\frac{b}{2n}\\displaystyle\\sum_{i=1}^{m}\\|x_{k-1,i}-x_{k-1,i+1}\\|_{2}^{2}}\\\\ &{=\\displaystyle\\frac{\\eta_{k}}{n}\\displaystyle\\sum_{i=1}^{m}y_{k}^{(i)\\top}A_{k}^{(i)}(x_{k}-x_{k-1,i+1})+\\displaystyle\\frac{\\eta_{k}}{n}\\displaystyle\\sum_{i=1}^{m}(v_{k}^{(i)}-y_{k}^{(i)})^{\\top}A_{k}^{(i)}(x_{k}-x_{k-1,i})}\\\\ &{\\qquad-\\displaystyle\\frac{b}{2n}\\displaystyle\\sum_{i=1}^{m}\\|x_{k-1,i}-x_{k-1,i+1}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "thus completing the proof. ", "page_idx": 38}, {"type": "text", "text": "Note that we can still use Lemma 10 to bound the first inner product term in Eq. (40), as we are studying the same algorithm. The following lemma provides a bound on the second inner product term $\\begin{array}{r}{\\bar{T_{2}}:=\\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}\\stackrel{\\leftarrow}{\\left(\\pmb{v}_{k}^{(i)}-\\pmb{y}_{k}^{(i)}\\right)^{\\top}}\\pmb{A}_{k}^{(i)}(\\pmb{x}_{k}^{\\top}-\\pmb{x}_{k-1,i})}\\end{array}$ in Eqg(40). ", "page_idx": 38}, {"type": "text", "text": "Lemma 15. Unde Asumption $^{5}$ for any $k\\ \\in\\ [K]$ theierales $\\{\\pmb{y}_{k}^{(i)}\\}_{i=1}^{m}$ and $\\{\\mathbfit{x}_{k-1,i}\\}_{i=1}^{m+1}$ generated by Algorithm 2 satisfy ", "page_idx": 38}, {"type": "equation", "text": "$$\nT_{2}\\leq\\frac{\\eta_{k}^{2}\\sqrt{\\hat{G}_{\\pi^{(k)}}\\tilde{G}_{\\pi^{(k)}}}}{b}\\|y_{k}\\|_{\\Gamma_{k}^{-1}}^{2}+\\frac{\\eta_{k}^{2}\\sqrt{\\hat{G}_{\\pi^{(k)}}\\tilde{G}_{\\pi^{(k)}}}}{4b}\\|v_{k}-y_{k}\\|_{\\Gamma_{k}^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. Proceeding as in Lemma 11, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\nT_{2}:=\\frac{\\eta_{k}}{n}\\sum_{i=1}^{m}\\big(\\boldsymbol{v}_{k}^{(i)}-\\boldsymbol{y}_{k}^{(i)}\\big)^{\\top}\\boldsymbol{A}_{k}^{(i)}(\\boldsymbol{x}_{k}-\\boldsymbol{x}_{k-1,i})=-\\frac{\\eta_{k}^{2}}{b n}\\sum_{i=1}^{m}\\left\\langle\\boldsymbol{A}_{k}^{\\top}\\boldsymbol{I}_{b(i-1)\\uparrow}\\boldsymbol{y}_{k},\\boldsymbol{A}_{k}^{\\top}\\boldsymbol{I}_{(i)}(\\boldsymbol{v}_{k}-\\boldsymbol{y}_{k})\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using Young's inequality for some $\\alpha>0$ and proceeding as in Eq. (24), we obtain ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\mathcal T}_{2}\\leq\\frac{\\eta_{k}^{2}\\alpha}{2b n}\\sum_{i=1}^{m}\\|{\\cal A}_{k}^{\\top}I_{b(i-1)\\uparrow}y_{k}\\|_{2}^{2}+\\frac{\\eta_{k}^{2}}{2b n\\alpha}\\sum_{i=1}^{m}\\|{\\cal A}_{k}^{\\top}I_{(i)}(v_{k}-y_{k})\\|_{2}^{2}}}\\\\ {{\\displaystyle\\quad\\leq\\frac{\\eta_{k}^{2}n\\alpha}{2b^{2}}\\hat{G}_{\\pi^{(k)}}\\|y_{k}\\|_{\\Gamma_{k}^{-1}}^{2}+\\frac{\\eta_{k}^{2}}{2n\\alpha}\\tilde{G}_{\\pi^{(k)}}\\|v_{k}-y_{k}\\|_{\\Gamma_{k}^{-1}}^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where we use our definitions that $\\begin{array}{r}{\\hat{G}_{\\pi^{(k)}}:=\\frac{1}{m n}\\big\\|\\mathbf{r}_{k}^{1/2}\\big(\\sum_{j=1}^{m}I_{b(j-1)\\uparrow}A_{k}A_{k}^{\\top}I_{b(j-1)\\uparrow}\\big)\\mathbf{r}_{k}^{1/2}\\big\\|_{2}}\\end{array}$ and $\\begin{array}{r}{\\tilde{G}_{\\pi^{(k)}}:=\\frac{1}{b}\\big\\|\\mathbf{r}_{k}^{1/2}\\big(\\sum_{j=1}^{m}I_{(j)}A_{k}A_{k}^{\\top}I_{(j)}\\big)\\mathbf{r}_{k}^{1/2}\\big\\|_{2}}\\end{array}$ $\\begin{array}{r}{\\alpha=\\frac{2b}{n}\\sqrt{\\frac{\\tilde{G}_{k}}{\\hat{G}_{k}}}}\\end{array}$ proof. \u53e3 ", "page_idx": 39}, {"type": "text", "text": "We are now ready to prove Theorem 3 for the convergence of shuffled SGD in the convex nonsmooth Lipschitz settings. ", "page_idx": 39}, {"type": "text", "text": "Theorem 3. Under Assumption $5;$ if $\\begin{array}{r}{H_{K}=\\sum_{k=1}^{K}\\eta_{k}}\\end{array}$ and $\\bar{G}=\\mathbb{E}_{\\pi}[\\sqrt{\\hat{G}_{\\pi}\\tilde{G}_{\\pi}}],$ theoutput $\\hat{\\pmb{x}}_{K}$ of Alg. 1 with possible uniformly random shuffing satisfies ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}[H_{K}(f(\\hat{\\pmb{x}}_{K})-f(\\pmb{x}_{*}))]\\leq\\frac{1}{2n}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}+\\sum_{k=1}^{K}2\\eta_{k}^{2}n\\bar{G},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "As a result for any $\\epsilon>0$ , there exists a step size $\\eta_{k}=\\eta$ such that $\\mathbb{E}[f(\\hat{\\mathbf{x}}_{K})-f(\\mathbf{x}_{*})]\\,\\le\\,\\epsilon$ after $\\mathcal{O}\\Big(\\frac{n\\bar{G}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{\\epsilon^{2}}\\Big)$ individua graientqgueries. ", "page_idx": 39}, {"type": "text", "text": "Proof. To simplify the presentation of our analysis, we first assume $\\|\\pmb{v}\\|_{\\mathbf{T}^{-1}}^{2}\\leq n$ , which will be later verified by our choice of ${\\pmb v}={\\pmb y}_{\\hat{\\pmb x}_{K}}$ and Assumption 5. ", "page_idx": 39}, {"type": "text", "text": "Combining the bounds in Lemma 10 and 15 and plugging them into Eq. (40), we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal E_{k}\\leq\\frac{\\eta_{k}^{2}\\sqrt{\\hat{G}_{\\pi^{(k)}}\\tilde{G}_{\\pi^{(k)}}}}{b}\\|y_{k}\\|_{\\mathbf{r}_{k}^{-1}}^{2}+\\frac{\\eta_{k}^{2}\\sqrt{\\hat{G}_{\\pi^{(k)}}\\tilde{G}_{\\pi^{(k)}}}}{4b}\\|v_{k}-y_{k}\\|_{\\mathbf{r}_{k}^{-1}}^{2}}\\\\ &{\\quad\\overset{(i)}{\\leq}\\frac{\\eta_{k}^{2}\\sqrt{\\hat{G}_{\\pi^{(k)}}\\tilde{G}_{\\pi^{(k)}}}}{b}\\|y_{k}\\|_{\\mathbf{r}_{k}^{-1}}^{2}+\\frac{\\eta_{k}^{2}\\sqrt{\\hat{G}_{\\pi^{(k)}}\\tilde{G}_{\\pi^{(k)}}}}{2b}(\\|v\\|_{\\mathbf{r}^{-1}}^{2}+\\|y_{k}\\|_{\\mathbf{r}_{k}^{-1}}^{2})}\\\\ &{\\quad\\overset{(i i)}{\\leq}\\frac{2\\eta_{k}^{2}n\\sqrt{\\hat{G}_{\\pi^{(k)}}\\tilde{G}_{\\pi^{(k)}}}}{b},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where we useYoung's inequality for $\\|\\pmb{v}_{k}-\\pmb{y}_{k}\\|_{\\mathbf{T}_{k}^{-1}}^{2}$ and $\\|\\pmb{v}_{k}\\|_{\\mathbf{r}_{k}^{-1}}=\\|\\pmb{v}\\|_{\\mathbf{r}^{-1}}^{2}$ $\\pmb{v}$ is a fixed vector for $(i)$ , and $(i i)$ is due to $\\|\\pmb{\\{y}}_{k}\\|_{\\mathbf{T}_{k}^{-1}}^{2}\\leq n$ by Assumption 5 and assuming that $\\|\\pmb{v}\\|_{\\mathbf{T}^{-1}}^{2}\\leq n$ . Proceeding as the proof for Theorem 2, we first assume the RR scheme and take conditional expectation w.r.t. the randomness up to but not including $k$ -th epoch, thenwe obtain ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}_{k}[\\mathcal{E}_{k}]\\leq\\frac{2\\eta_{k}^{2}n\\mathbb{E}_{k}\\big[\\sqrt{\\hat{G}_{\\pi^{(k)}}\\tilde{G}_{\\pi^{(k)}}}\\big]}{b}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Since the randomness only comes from the random permutation $\\pi^{(k)}$ ,wehave ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}_{k}[\\mathcal{E}_{k}]\\leq\\frac{2\\eta_{k}^{2}n\\mathbb{E}_{\\pi}[\\sqrt{\\hat{G}_{\\pi}\\tilde{G}_{\\pi}}]}{b}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "For notational convenience, we denote $\\bar{G}=\\mathbb{E}_{\\pi}[\\sqrt{\\hat{G}_{\\pi}\\tilde{G}_{\\pi}}]$ and further take expectation w.r.t. all the randomness on both sides and use the law of total expectation to obtain ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathcal{E}_{k}]\\leq\\frac{2\\eta_{k}^{2}n\\bar{G}}{b}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "For the SO scheme, there is one random permutation $\\pi$ generated at the very beginning such that $\\pi^{(k)}\\,=\\,\\pi$ for all $k\\,\\in\\,[K]$ . So we can directly take expectation w.r.t.all the randomness on both sides of Eq. (45), with the randomness only from $\\pi$ , which leads to the same bound as Eq. (46) with $\\mathbb{E}\\big[\\sqrt{\\hat{G}_{\\pi^{(k)}}\\tilde{G}_{\\pi^{(k)}}}\\big]=\\mathbb{E}_{\\pi}\\big[\\sqrt{\\hat{G}_{\\pi}\\tilde{G}_{\\pi}}\\big]$ Note that for incremental gradient (IG) descent, we can let $\\bar{G}=\\sqrt{\\hat{G}_{0}\\tilde{G}_{0}}$ without randomness involved, where $\\hat{G}_{0}=\\hat{G}_{\\pi^{(0)}}$ and $\\tilde{G}_{0}=\\tilde{G}_{\\pi^{(0)}}$ W.r.t. the initial, fixed permutation $\\pi^{(0)}$ of the data matrix $\\pmb{A}$ ", "page_idx": 39}, {"type": "text", "text": "Noticing that $\\begin{array}{r}{\\mathcal{E}_{k}=\\eta_{k}\\mathrm{Gap}^{v}(\\boldsymbol{x}_{k},\\boldsymbol{y}_{*})+\\frac{b}{2n}\\|\\boldsymbol{x}_{*}-\\boldsymbol{x}_{k}\\|_{2}^{2}-\\frac{b}{2n}\\|\\boldsymbol{x}_{*}-\\boldsymbol{x}_{k-1}\\|_{2}^{2}}\\end{array}$ and telescoping from $k=1$ to $K$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\sum_{k=1}^{K}\\eta_{k}\\mathrm{Gap}^{v}(\\pmb{x}_{k},\\pmb{y}_{*})\\Big]\\leq\\frac{b}{2n}\\|\\pmb{x}_{*}-\\pmb{x}_{0}\\|_{2}^{2}-\\frac{b}{2n}\\mathbb{E}[\\|\\pmb{x}_{*}-\\pmb{x}_{K}\\|_{2}^{2}]+\\sum_{k=1}^{K}\\frac{2\\eta_{k}^{2}n\\bar{G}}{b}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Noticing that $\\mathcal{L}(\\boldsymbol{\\mathbf{\\boldsymbol{x}}},\\boldsymbol{\\mathbf{\\boldsymbol{v}}})$ is convex wrt $\\textbf{\\em x}$ , we have $\\begin{array}{r}{\\mathrm{Gap}^{v}(\\hat{\\mathbf{x}}_{K},\\pmb{y}_{*})\\,\\le\\,\\sum_{k=1}^{K}\\eta_{k}\\mathrm{Gap}^{v}(\\pmb{x}_{k},\\pmb{y}_{*})/H_{K},}\\end{array}$ where $\\begin{array}{r}{\\hat{\\pmb{x}}_{K}=\\sum_{k=1}^{K}{\\eta_{k}\\pmb{x}_{k}/H_{K}}}\\end{array}$ and $\\begin{array}{r}{H_{K}=\\sum_{k=1}^{K}\\eta_{k}}\\end{array}$ , so we obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[H_{K}{\\mathrm{Gap}}^{v}(\\hat{x}_{K},y_{*})\\Big]\\leq\\frac{b}{2n}\\|x_{0}-x_{*}\\|_{2}^{2}+\\sum_{k=1}^{K}\\frac{2\\eta_{k}^{2}n\\bar{G}}{b}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Further choosing ${\\pmb v}={\\pmb y}_{\\hat{\\pmb x}_{K}}$ , which also verifies $\\|\\pmb{v}\\|_{\\Gamma^{-1}}^{2}=\\|\\pmb{y}_{\\hat{\\pmb{x}}_{K}}\\|_{\\Gamma^{-1}}^{2}\\leq n$ by Assumption 5, we obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}[H_{K}(f(\\hat{{\\pmb x}}_{K})-f({\\pmb x}_{*}))]\\leq\\frac{b}{2n}\\|{\\pmb x}_{0}-{\\pmb x}_{*}\\|_{2}^{2}+\\sum_{k=1}^{K}\\frac{2\\eta_{k}^{2}n\\bar{G}}{b}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "To analyze the individual gradient oracle complexity, we choose constant stepsize $\\eta$ Then, the above boundbecomes ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(\\hat{\\mathbf{x}}_{K})-f(\\mathbf{x}_{*})]\\le\\frac{b}{2n\\eta K}\\|\\mathbf{x}_{0}-\\mathbf{x}_{*}\\|_{2}^{2}+\\frac{2n\\eta\\bar{G}}{b}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Choosing $\\begin{array}{r}{\\eta=\\frac{b\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}}{2n\\sqrt{K\\bar{G}}}}\\end{array}$ bllxol2, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(\\hat{\\mathbf{x}}_{K})-f(\\mathbf{x}_{*})]\\leq\\frac{2\\sqrt{\\bar{G}}\\lVert\\mathbf{x}_{0}-\\mathbf{x}_{*}\\rVert_{2}}{\\sqrt{K}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Hence, given $\\epsilon\\,>\\,0$ , to ensure $\\mathbb{E}[f(\\hat{\\mathbf{x}}_{K})\\,-\\,f(\\mathbf{x}_{*})]\\,\\le\\,\\epsilon.$ , the total number of individual gradient evaluations will be ", "page_idx": 40}, {"type": "equation", "text": "$$\nn K\\geq\\frac{4n\\bar{G}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{\\epsilon^{2}},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "thus completing the proof. ", "page_idx": 40}, {"type": "text", "text": "We now briefly discuss this result. The total number of individual gradient queries is $\\mathcal{O}\\Big(\\frac{n\\bar{G}\\|\\pmb{x}_{0}-\\pmb{x}_{*}\\|_{2}^{2}}{\\epsilon^{2}}\\Big)$ \uff0c which appears independent of the batch size, but this is actually not the case, as the parameter $\\bar{G}=\\mathbb{E}_{\\pi}[\\sqrt{\\hat{G}_{\\pi}\\tilde{G}_{\\pi}}]$ depends on the block partitioning, due to Eq. (4). When $b=n$ , as a sanity check, we recover the standard guarantee of (full) subgradient descent, which is expected, as in this case shuffled SGD reduces to subgradient descent. When $b=1$ , however, the bound is worse than the corresponding bound for standard SGD, by a factor ${\\mathcal{O}}(n{\\bar{G}}/G^{2})$ . By a similar sequence of inequalities as in Eq. (4), this factor is never worse than $n$ , but it is typically much smaller, taking values as small as 1. We note that it is not known whether a better bound is possible for shuffed SGD in this setting, as the only seemingly tighter upper bound from [42] applies only for constant $K$ when $\\begin{array}{r}{n=\\Omega\\big(\\frac{1}{\\epsilon^{2}}\\big)}\\end{array}$ and under an additional boundedness assumption for the algorithm iterates. ", "page_idx": 40}, {"type": "text", "text": "E Experiment Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "We implement the computation of $\\hat{L}$ and $L_{\\mathrm{max}}$ in Julia, a high-performance scientific computation programming language, and compute matrix operator norms using the default settings in the Julia Arpack Package. However, limited by computational memory and time constraint, our selection of datasets is focused on moderately large-scale datasets of $n$ in the order of $O(10^{5})$ . We also include comparisons of small datasets such as a1a and sonar. ", "page_idx": 40}, {"type": "text", "text": "E.1  Evaluations of $L_{\\mathrm{max}}/\\tilde{L}_{\\pi}$ on Synthetic Gaussian Datasets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "We first study the gap between ${\\tilde{L}}_{\\pi}$ and $L_{\\mathrm{max}}$ for different batch sizes $b$ , as shown in Figure 2. As in Section 4.1, we focus on their dependence on the data matrix, and assume that the loss functions $\\ell_{i}$ all have the same smoothness constant. In this case, the ratio $L_{\\mathrm{max}}/\\tilde{L}_{\\pi}$ that characterizes the gap between ${\\tilde{L}}_{\\pi}$ and $L_{\\mathrm{max}}$ will become $\\begin{array}{r}{L_{\\operatorname*{max}}/\\tilde{L}_{\\pi}\\,=\\,(\\operatorname*{max}_{1\\le i\\le n}\\{\\|a_{i}\\|_{2}^{2}\\})/\\big(\\frac1b\\|\\sum_{j=1}^{m}I_{(j)}A_{\\pi}A_{\\pi}^{\\top}I_{(j)}\\|_{2}\\big)}\\end{array}$ . In particular, we run experiments on standard Gaussian data of size $(n,d)$ .We fix the dimension $d=500$ , and vary the number of samples with $n=100,500,1000,2000$ . In Figure 2, we plot the ratio $L_{\\mathrm{max}}/\\tilde{L}_{\\pi}$ versus the batch size $b$ for 100 different random permutations $\\pi$ , where the dotted lines represent the mean values and the filled regions indicate the standard deviation of permutations. We observe that the ratio $L_{\\mathrm{max}}/\\tilde{L}_{\\pi}$ is concentrated around its empirical mean and exhibits $b^{\\alpha}$ $(\\alpha\\in[0.74,0.87])$ growth as the batch size $b$ increases. In particular, if we choose $b={\\sqrt{n}}$ , the ratio can be $\\mathcal{O}(n^{0.4})$ \uff1a ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "image", "img_path": "qcPlGtzwW9/tmp/a6eabe636d49a2ed2069e10495561b50f2cd50868fa006c4f38905a0957878ad.jpg", "img_caption": ["Figure 2: llustrations of $L_{\\mathrm{max}}/\\tilde{L}_{\\pi}$ for different batch size $b$ on synthetic Gaussian data of size $(n,d)$ "], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "E.2  Distributions of $L_{\\mathrm{max}}/\\hat{L}_{\\pi}$ ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In this subsection, we include histograms in Figure 3 to illustrate the spread of $L_{\\mathrm{max}}/\\hat{L}_{\\pi}$ With respect to random permutations, for completeness. We observe that in all the examples $L_{\\mathrm{max}}/\\hat{L}_{\\pi}$ is concentrated around its empirical mean. The following plots are normalized, with y-axis representing the empirical probability density. The $\\mathbf{X}$ -axisrepresents $L_{\\mathrm{max}}/\\hat{L}_{\\pi}$ ", "page_idx": 41}, {"type": "image", "img_path": "qcPlGtzwW9/tmp/71270093c58657eb8febfa976a3b251ee7ccef562a2559a7aafe4da29fe3a5f3.jpg", "img_caption": ["Figure 3: Visualization of the empirical distributions of $L/\\hat{L}$ for 15 large-scale datasets. "], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Our abstract and introduction clearly state the scope of our work and contributions, see Section 1.2. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 43}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: Yes, see detailed discussion in the introdcution. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 43}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: We state our assumptions in sections 2 and 3, and the proofs are provided in the appendix. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 44}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We fully disclose the public datasets and tools we use for the numerical computations in Section 4.1. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 44}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: N/A Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 45}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We list the details of our experiments in Section 4.1 and Appendix E ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 45}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We use various ways, including ribbon plots and histograms, to illustrate the variance of our numerically computed values. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 46}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We list the details of all computational tools in Section 4.1 and Appendix E. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 46}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: This is a primarily theoretical work and we conform to the rules with NeurIPS Code of Ethics in every respect. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 46}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: N/A ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. \u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 46}, {"type": "text", "text": "\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 47}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] Justification: N/A ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks. ", "page_idx": 47}, {"type": "text", "text": "\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to acces the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 47}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We use public benchmarking datasets from LIBSVM [15], MNIST [17], CIFAR10 [22], and Broad Bioimage Benchmark Collection [28], and have properly cited and credited the asset's creators. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] Justification: N/A Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 48}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 48}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 48}, {"type": "text", "text": "Answer:[NA]   \nJustification: N/A   \nGuidelines: \u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 49}]