{"references": [{"fullname_first_author": "Alekh Agarwal", "paper_title": "Information-theoretic lower bounds on the oracle complexity of convex optimization", "publication_date": "2009-01-01", "reason": "This paper establishes fundamental lower bounds for convex optimization, providing a benchmark against which the efficiency of SGD methods can be measured."}, {"fullname_first_author": "L\u00e9on Bottou", "paper_title": "Curiously fast convergence of some stochastic gradient descent algorithms", "publication_date": "2009-01-01", "reason": "This paper's empirical observations on the surprisingly fast convergence of SGD with shuffled data spurred much of the subsequent theoretical work on shuffled SGD."}, {"fullname_first_author": "Konstantin Mishchenko", "paper_title": "Random reshuffling: Simple analysis with vast improvements", "publication_date": "2020-01-01", "reason": "This paper provides a simpler analysis and improved convergence bounds for shuffled SGD, advancing the state-of-the-art in understanding its performance."}, {"fullname_first_author": "Mert G\u00fcrb\u00fczbalaban", "paper_title": "Why random reshuffling beats stochastic gradient descent", "publication_date": "2021-01-01", "reason": "This paper offers a theoretical explanation for the empirical success of random reshuffling in SGD, highlighting its advantages over standard stochastic gradient descent."}, {"fullname_first_author": "Ohad Shamir", "paper_title": "Without-replacement sampling for stochastic gradient methods", "publication_date": "2016-01-01", "reason": "This paper initiated the rigorous theoretical study of SGD without replacement, laying the groundwork for subsequent analyses of shuffled SGD."}]}