[{"figure_path": "80SSl69GAz/figures/figures_0_1.jpg", "caption": "Figure 1: A schematic representation of SwitchHead. It consists of a few independent heads, each with multiple experts for value and output projections. Each head has a single attention matrix.", "description": "The figure is a schematic representation of SwitchHead, a novel Mixture-of-Experts (MoE) method for the attention layer in Transformer networks.  It shows that SwitchHead consists of multiple independent heads.  Each head comprises multiple experts responsible for the value and output projections. A key feature is that each head only requires a single attention matrix, unlike standard Transformers. The diagram visually depicts the flow of information through the selection logic, experts, and the final output. This design allows SwitchHead to reduce computational costs while maintaining performance.", "section": "1 Introduction"}, {"figure_path": "80SSl69GAz/figures/figures_8_1.jpg", "caption": "Figure 1: A schematic representation of SwitchHead. It consists of a few independent heads, each with multiple experts for value and output projections. Each head has a single attention matrix.", "description": "The figure shows a schematic of the SwitchHead architecture.  It highlights the key components: independent heads, each with multiple experts for value and output projections.  Crucially, each head only computes a single attention matrix, unlike standard Transformers.  This design is central to SwitchHead's efficiency gains.", "section": "1 Introduction"}, {"figure_path": "80SSl69GAz/figures/figures_16_1.jpg", "caption": "Figure 1: A schematic representation of SwitchHead. It consists of a few independent heads, each with multiple experts for value and output projections. Each head has a single attention matrix.", "description": "The figure shows a schematic of SwitchHead, a novel Mixture-of-Experts (MoE) method for the attention layer in Transformer networks. It highlights the key components: independent heads, multiple experts for value and output projections within each head, and a single attention matrix per head. This design enables SwitchHead to compute fewer attention matrices compared to standard Transformers, leading to reduced computational and memory requirements.", "section": "1 Introduction"}, {"figure_path": "80SSl69GAz/figures/figures_17_1.jpg", "caption": "Figure 1: A schematic representation of SwitchHead. It consists of a few independent heads, each with multiple experts for value and output projections. Each head has a single attention matrix.", "description": "The figure is a schematic illustrating the architecture of SwitchHead, a novel Mixture of Experts (MoE) method for the attention layer in Transformer networks.  It highlights the key components: independent heads, each with multiple experts for value and output projections, and a single attention matrix per head. This design allows for efficient computation by selectively activating only a subset of experts, thus reducing the number of attention matrices that need to be computed and stored.", "section": "1 Introduction"}, {"figure_path": "80SSl69GAz/figures/figures_18_1.jpg", "caption": "Figure 1: A schematic representation of SwitchHead. It consists of a few independent heads, each with multiple experts for value and output projections. Each head has a single attention matrix.", "description": "The figure illustrates the architecture of SwitchHead, a novel Mixture-of-Experts (MoE) method for the attention layer in Transformer models.  It shows that SwitchHead comprises several independent heads, each of which has multiple expert networks for processing values and generating outputs. Importantly, each head only uses one attention matrix, which is a key element in reducing the computational cost compared to standard Transformer architectures. The selection logic determines which expert networks are used based on the input data.", "section": "1 Introduction"}, {"figure_path": "80SSl69GAz/figures/figures_19_1.jpg", "caption": "Figure 1: A schematic representation of SwitchHead. It consists of a few independent heads, each with multiple experts for value and output projections. Each head has a single attention matrix.", "description": "The figure shows a simplified illustration of the SwitchHead architecture. It consists of multiple independent heads, each of which has multiple experts for value and output projections. Each head has one attention matrix, which is calculated only once per head, regardless of the number of experts.  The selection logic determines which expert is activated during the forward and backward passes, allowing for resource reduction.", "section": "1 Introduction"}, {"figure_path": "80SSl69GAz/figures/figures_19_2.jpg", "caption": "Figure 1: A schematic representation of SwitchHead. It consists of a few independent heads, each with multiple experts for value and output projections. Each head has a single attention matrix.", "description": "The figure is a schematic diagram showing the architecture of SwitchHead, a novel Mixture-of-Experts (MoE) method for accelerating Transformer self-attention layers.  It illustrates how SwitchHead uses multiple independent heads, each with several expert networks for processing value and output projections.  Crucially, it highlights that each head only requires a single attention matrix, significantly reducing computational costs compared to traditional Transformers.", "section": "1 Introduction"}, {"figure_path": "80SSl69GAz/figures/figures_19_3.jpg", "caption": "Figure 1: A schematic representation of SwitchHead. It consists of a few independent heads, each with multiple experts for value and output projections. Each head has a single attention matrix.", "description": "The figure shows a simplified diagram of the SwitchHead architecture.  It illustrates how several independent heads process information, each utilizing multiple expert networks for value and output projections.  Importantly, each head only computes a single attention matrix, rather than multiple matrices as in standard Transformer models, leading to computational savings.", "section": "1 Introduction"}, {"figure_path": "80SSl69GAz/figures/figures_19_4.jpg", "caption": "Figure 1: A schematic representation of SwitchHead. It consists of a few independent heads, each with multiple experts for value and output projections. Each head has a single attention matrix.", "description": "The figure shows a schematic of the SwitchHead architecture.  It highlights the core components: independent heads, each with multiple experts for value and output projections. A key feature is that each head uses a single attention matrix, unlike traditional Transformers which compute multiple attention matrices per head.  The experts are used to project the values and outputs, reducing the computational cost while aiming to maintain the quality of the attention mechanism.  The selection logic determines which experts are used for a given head, allowing for efficient computation of the attention mechanism.", "section": "1 Introduction"}]