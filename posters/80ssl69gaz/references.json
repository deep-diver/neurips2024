{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, the foundation upon which the SwitchHead method builds and is therefore the most foundational work referenced."}, {"fullname_first_author": "Noam Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-04-01", "reason": "This paper introduced the Mixture of Experts (MoE) layer, a crucial concept for SwitchHead's resource efficiency, making it a key influence on the proposed method."}, {"fullname_first_author": "R\u00f3bert Csord\u00e1s", "paper_title": "Approximating two-layer feedforward networks for efficient transformers", "publication_date": "2023-11-01", "reason": "This is another paper by the current authors which introduces the \u03c3-MoE method used in SwitchHead, showcasing their prior work's relevance to the current research."}, {"fullname_first_author": "Xiaofeng Zhang", "paper_title": "Mixture of attention heads: Selecting attention heads per token", "publication_date": "2022-12-01", "reason": "This work presents MoA, a closely related method to SwitchHead, providing a comparative analysis and highlighting the differences and improvements in SwitchHead."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "publication_date": "2020-07-01", "reason": "This paper introduces the C4 dataset used for training SwitchHead, a significant element of the experimental setup and results, establishing its importance."}]}