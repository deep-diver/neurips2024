{"references": [{"fullname_first_author": "Geshkovski", "paper_title": "A mathematical perspective on transformers", "publication_date": "2023-12-10", "reason": "This paper provides the foundational mathematical framework for analyzing transformers as interacting particle systems, forming the basis of the current work."}, {"fullname_first_author": "Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This seminal paper introduced the Transformer architecture, whose self-attention mechanism is the central focus of the current research."}, {"fullname_first_author": "Sander", "paper_title": "Sinkformers: Transformers with doubly stochastic attention", "publication_date": "2022-01-01", "reason": "This work offered an earlier mathematical framework to analyze the self-attention mechanism in transformers, laying groundwork for further investigation."}, {"fullname_first_author": "Koubbi", "paper_title": "Dynamic metastability in the self-attention model", "publication_date": "2024-10-01", "reason": "This paper addresses the phenomenon of meta-stable states in self-attention dynamics, a key topic also explored in the current work."}, {"fullname_first_author": "Bruno", "paper_title": "Emergence of meta-stable clustering in mean-field transformer models", "publication_date": "2024-01-01", "reason": "This paper investigates meta-stable clustering, a phenomenon that is central to the current study's analysis of causal self-attention."}]}