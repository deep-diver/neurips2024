[{"figure_path": "lOMHt16T8R/figures/figures_1_1.jpg", "caption": "Figure 1: Our framework PaCE achieves alignment goals by sparse coding and adjusting vectors in the activation space of the LLM Decoder Layer (DCL).", "description": "The figure illustrates the PaCE framework, showing how it processes a toxic input using an LLM.  The input's activation vector is decomposed into benign and undesirable components using sparse coding and a concept dictionary. The undesirable components are then removed via activation intervention, resulting in a trustworthy output. The concept dictionary maps semantic concepts (e.g., malicious, faithful, fair) to their corresponding activation vectors. This framework efficiently models the activation space's geometry to remove undesirable concepts without harming linguistic capabilities. ", "section": "1 Introduction"}, {"figure_path": "lOMHt16T8R/figures/figures_2_1.jpg", "caption": "Figure 2: To remove a concept direction \u2018red\u2019 from the latent code \u2018red apple\u2019 (left), prior works use i) orthogonal projection (middle right, (OrthoProj)), which may remove extra directions, or ii) vector addition (right, (VecAdd)), where it is hard to pick the edit strength c. Instead, PaCE explicitly models the concept dictionary in the latent space and use oblique projection (middle left).", "description": "This figure illustrates three different methods for removing a concept direction from a latent code vector.  The leftmost panel shows the goal: removing the 'red' concept from the 'red apple' vector. The middle panels illustrate existing methods. OrthoProj (orthogonal projection) directly projects onto the orthogonal complement, potentially removing unintended information, while VecAdd (vector addition) adds a scaled version of the desired concept direction, with the scaling factor (c) being hard to determine optimally. The rightmost panel depicts PaCE's approach: oblique projection.  PaCE models the entire concept dictionary in the activation space and performs an oblique projection which only removes the undesired component while preserving the desired component.", "section": "2 Basics of Latent Space Engineering"}, {"figure_path": "lOMHt16T8R/figures/figures_3_1.jpg", "caption": "Figure 3: Pipeline of PaCE has several major steps: Step 1 collects concept vectors and constructs the concept dictionary, Step 2 decomposes the activation vector of the given input by sparse coding to get concept coefficients, and Step 3 performs editing on the concepts towards reoriented response.", "description": "This figure illustrates the three main stages of the PaCE framework. First, it shows the creation of a concept dictionary from knowledge-driven concept collection. Second, it details the decomposition of the input activation vector into concept coefficients using sparse coding. Finally, it depicts the removal of undesirable components from the activation vector and the generation of an aligned response.", "section": "3 Our Method: Parsimonious Concept Engineering"}, {"figure_path": "lOMHt16T8R/figures/figures_6_1.jpg", "caption": "Figure 6: The detoxification performances for LLaMA2-13B w.r.t. the dictionary size.", "description": "This figure shows how the detoxification performance of the LLaMA2-13B model changes with different sizes of the concept dictionary used in the PaCE framework.  The x-axis represents the size of the dictionary, and the y-axis displays three metrics: fluency, safety (percentage), and time per response (seconds).  The graph illustrates a trade-off between these metrics.  Increasing the dictionary size generally improves safety, but also increases the time required for each response and may slightly decrease fluency.", "section": "4 Experimental Results"}, {"figure_path": "lOMHt16T8R/figures/figures_8_1.jpg", "caption": "Figure 8: The Representation (Activation) Space of LLaMA2-13B-Chat with the first 10000 Concepts from PaCE-1M. Appendix Figure 16 shows the zoom-in version. The visualization is the first two dimensions of UMAP of the concept vectors. We observe that concepts of similar semantics are clustered together, indicating that the activation space has semantic structures.", "description": "This figure visualizes the semantic structure of the activation space of the LLaMA2-13B-Chat language model using the first 10,000 concept vectors from the PaCE-1M dataset.  UMAP dimensionality reduction is applied to project the high-dimensional concept vectors into a 2D space for visualization. The plot shows that semantically similar concepts cluster together, demonstrating that the activation space possesses inherent semantic organization.", "section": "4.3 Representation Space Sampled by PaCE-1M"}, {"figure_path": "lOMHt16T8R/figures/figures_8_2.jpg", "caption": "Figure 10: The top 10 retrieved concepts using the similarity score in the sampled activation space. We observe close coherence between the target concept and retrieved concepts.", "description": "This figure shows the top 10 most similar concepts retrieved for the concepts \"love\" and \"angry\" using a similarity score based on their activation vectors.  The high similarity scores indicate that the activation space contains semantically coherent clusters of concepts.  The results support the claim that PaCE effectively captures and organizes semantic information within the LLM's activation space.", "section": "4.3 Representation Space Sampled by PaCE-1M"}, {"figure_path": "lOMHt16T8R/figures/figures_23_1.jpg", "caption": "Figure 5: An example of jailbreaking LLaMA2-7B-Chat and detoxification by PaCE. PaCE successfully detoxifies the response while maintaining the instruction-following capability.", "description": "This figure showcases an example of a malicious prompt (jailbreaking) given to LLaMA2-7B-Chat, along with the model's vanilla response, and the response after PaCE (Parsimonious Concept Engineering) is applied.  The vanilla response includes both an \"aligned\" and \"unaligned\" section, demonstrating the model's ability to generate both desirable and undesirable outputs. The PaCE-modified response successfully removes the undesirable content from the \"unaligned\" section while preserving the overall coherence and instruction-following of the response.", "section": "4.1 Improving Safety by Response Detoxification"}, {"figure_path": "lOMHt16T8R/figures/figures_25_1.jpg", "caption": "Figure 15: The affinity matrix learned by Elastic Net Subspace Clustering (EnSC) on the concept vectors, which gathers the concepts into 200 clusters. The rows and columns of the matrix are sorted by cluster assignment. Table 6 further shows samples of these concept clusters and their topics.", "description": "This figure shows the affinity matrix resulting from applying Elastic Net Subspace Clustering (EnSC) to the concept vectors.  The matrix visually represents the similarity between different concept vectors, revealing a block-diagonal structure indicative of successful clustering. Concepts within the same cluster exhibit high affinity, while those in different clusters show low affinity. The rows and columns are sorted by cluster assignment to highlight the block diagonal structure.  Table 6 in the paper then provides examples of concepts and their associated cluster assignments and topics for better understanding.", "section": "C PaCE-1M Dataset"}, {"figure_path": "lOMHt16T8R/figures/figures_25_2.jpg", "caption": "Figure 8: The Representation (Activation) Space of LLaMA2-13B-Chat with the first 10000 Concepts from PaCE-1M. Appendix Figure 16 shows the zoom-in version. The visualization is the first two dimensions of UMAP of the concept vectors. We observe that concepts of similar semantics are clustered together, indicating that the activation space has semantic structures.", "description": "This figure visualizes the first 10,000 concept vectors from the PaCE-1M dataset, projected onto the first two dimensions using UMAP.  It demonstrates that semantically similar concepts cluster together in the activation space of the LLaMA2-13B-Chat language model, suggesting that the activation space possesses inherent semantic structure.  Appendix Figure 16 provides a zoomed-in view for more detail.", "section": "4.3 Representation Space Sampled by PaCE-1M"}]