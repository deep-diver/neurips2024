[{"figure_path": "lOMHt16T8R/tables/tables_4_1.jpg", "caption": "Table 1: Detoxification evaluation for PaCE, representation manipulation, and training-free baselines. The best performance of each category is in bold and the second best is underlined.", "description": "This table presents the quantitative results of response detoxification experiments.  It compares the performance of PaCE against several baselines, including prompt engineering, vector addition, and orthogonal projection, across various safety categories. The safety scores are presented as percentages, with higher values indicating better performance.  It also shows linguistic capability metrics (fluency, perplexity, and MMLU scores) to assess the impact of the detoxification methods on the overall quality of the language model's output.  The best result in each category is highlighted in bold, and the second best is underlined.", "section": "4.1 Improving Safety by Response Detoxification"}, {"figure_path": "lOMHt16T8R/tables/tables_5_1.jpg", "caption": "Table 1: Detoxification evaluation for PaCE, representation manipulation, and training-free baselines. The best performance of each category is in bold and the second best is underlined.", "description": "This table presents the results of a detoxification experiment comparing PaCE against several baselines.  The experiment assesses the performance of different methods in removing toxic content from Large Language Model (LLM) responses. The table shows the performance of each method across various safety categories (Political Sensitivity, Pornography, etc.), along with metrics for linguistic capability (fluency and perplexity) and MMLU (Massive Multitask Language Understanding) scores.  The best performing method for each category is highlighted in bold, while the second-best is underlined.  This allows for a comparison of PaCE's performance in terms of both safety and maintaining linguistic capabilities against other techniques such as prompt engineering, vector addition, and orthogonal projection.", "section": "4 Experimental Results"}, {"figure_path": "lOMHt16T8R/tables/tables_6_1.jpg", "caption": "Table 1: Detoxification evaluation for PaCE, representation manipulation, and training-free baselines. The best performance of each category is in bold and the second best is underlined.", "description": "This table presents the results of a detoxification evaluation comparing PaCE against several baseline methods on the LLaMA-7B-Chat and LLaMA2-13B-Chat models.  The evaluation assesses performance across nine safety categories (Political Sensitivity, Pornography, Ethics and Morality, Illegal Activities, Mental Harm, Offensiveness, Physical Harm, Privacy and Property, and Unfairness & Bias), measuring the percentage improvement in safety scores.  It also includes metrics for linguistic capability (fluency, perplexity, and MMLU scores) to evaluate the trade-off between safety and linguistic performance.  The best performing method in each safety category is highlighted in bold, and the second-best method is underlined.", "section": "4.1 Improving Safety by Response Detoxification"}, {"figure_path": "lOMHt16T8R/tables/tables_6_2.jpg", "caption": "Table 1: Detoxification evaluation for PaCE, representation manipulation, and training-free baselines. The best performance of each category is in bold and the second best is underlined.", "description": "This table presents the results of an experiment evaluating the performance of PaCE and several baseline methods on a response detoxification task.  The task involved evaluating the safety of responses generated by different methods across nine categories (Political Sensitivity, Pornography, Ethics and Morality, Illegal Activities, Mental Harm, Offensiveness, Physical Harm, Privacy and Property, and Unfairness & Bias). The table shows the percentage improvement in safety achieved by each method compared to a vanilla LLM.  It also includes metrics assessing linguistic capability (fluency, perplexity, and MMLU scores) to ensure that the detoxification process doesn't significantly impair the quality of the generated text.", "section": "4 Experimental Results"}, {"figure_path": "lOMHt16T8R/tables/tables_7_1.jpg", "caption": "Table 2: Computation time (in seconds) evaluation for PaCE and representation manipulation baselines. We observe that, compared to OrthoProj which also projects the concept, our PaCE is more time-efficient for trustworthiness control.", "description": "This table presents the computation time, in seconds, for different methods used for response detoxification, including the proposed PaCE method and several baselines (Vanilla, VecAdd, OrthoProj).  The time is broken down into time per response and time per token.  The results show that PaCE is more time-efficient than OrthoProj, while having similar performance to other methods.", "section": "4 Experimental Results"}, {"figure_path": "lOMHt16T8R/tables/tables_7_2.jpg", "caption": "Table 4: Faithfulness and Fairness evaluation for PaCE, representation manipulation, and training-free baselines. The best performance of each category is in bold and the second best is underlined.", "description": "This table presents the results of an evaluation comparing PaCE's performance to several baseline methods on tasks related to faithfulness and fairness in language models. The evaluation considers different metrics, including fact accuracy (Fact), sentiment (Sentiment), and linguistic capability (Linguistic Capability), to assess how well the different methods preserve factual information and maintain a positive tone in their responses while maintaining good linguistic quality. For each metric, the best-performing method is indicated in bold, while the second-best is underlined.", "section": "4.2 Improving Faithfulness and Removing Negative Sentiment"}, {"figure_path": "lOMHt16T8R/tables/tables_7_3.jpg", "caption": "Table 3: Ablation study for PaCE on the detoxifying LLaMA2-7B. Starting from a small emotion dictionary and manually selected concepts for removal, each subsequent design leads to better performance.", "description": "This table presents an ablation study evaluating the impact of different design choices within the PaCE framework on the task of response detoxification using the LLaMA2-7B language model.  It shows how adding features like decomposition of concepts, clustering, concept partitioning, and removing more concepts iteratively improves the model's safety performance. The initial model used a small dictionary of manually selected emotion concepts for removal. Each row represents a different version of the PaCE model, each with increased sophistication and resulting improved safety (%).", "section": "4 Experimental Results"}, {"figure_path": "lOMHt16T8R/tables/tables_18_1.jpg", "caption": "Table 1: Detoxification evaluation for PaCE, representation manipulation, and training-free baselines. The best performance of each category is in bold and the second best is underlined.", "description": "This table presents the results of a detoxification task, comparing PaCE's performance against various baselines, including prompt engineering and other activation manipulation methods.  The evaluation metrics assess safety across nine categories (Political Sensitivity, Pornography, Ethics and Morality, Illegal Activities, Mental Harm, Offensiveness, Physical Harm, Privacy and Property, and Unfairness & Bias).  The table also includes scores representing the linguistic capability (fluency, perplexity, and MMLU) to demonstrate that PaCE's safety improvements do not come at the expense of overall language capabilities.", "section": "4.1 Improving Safety by Response Detoxification"}, {"figure_path": "lOMHt16T8R/tables/tables_20_1.jpg", "caption": "Table 5: Detoxification evaluation for PaCE, representation manipulation, and training-free baselines on AdvBench.", "description": "This table presents the results of a detoxification evaluation on the AdvBench dataset, comparing PaCE's performance against several baselines, including prompt engineering, vector addition, and orthogonal projection.  The evaluation assesses the safety scores (in percentage) of the model's responses on the AdvBench dataset for both LlaMA2-7B-Chat and LlaMA2-13B-Chat models.  Higher scores indicate better detoxification performance (i.e., fewer harmful responses).", "section": "4.1 Improving Safety by Response Detoxification"}, {"figure_path": "lOMHt16T8R/tables/tables_21_1.jpg", "caption": "Table 1: Detoxification evaluation for PaCE, representation manipulation, and training-free baselines. The best performance of each category is in bold and the second best is underlined.", "description": "This table presents the results of a detoxification evaluation comparing PaCE against several baseline methods for response detoxification.  It evaluates the performance on nine different safety categories (Political Sensitivity, Pornography, Ethics and Morality, Illegal Activities, Mental Harm, Offensiveness, Physical Harm, Privacy and Property, and Unfairness & Bias) across two different language models (LLaMA2-7B-Chat and LLaMA2-13B-Chat).  For each model and method, the table shows the percentage improvement in safety scores and also includes linguistic capability metrics such as fluency and perplexity.  The best performance in each category is highlighted in bold, indicating PaCE's superior performance for most categories.", "section": "4.1 Improving Safety by Response Detoxification"}, {"figure_path": "lOMHt16T8R/tables/tables_24_1.jpg", "caption": "Table 1: Detoxification evaluation for PaCE, representation manipulation, and training-free baselines. The best performance of each category is in bold and the second best is underlined.", "description": "This table presents the results of a detoxification task, comparing PaCE's performance against several baselines.  The metrics used assess the safety and linguistic quality of the generated text. Each row represents a different method, and each column represents a specific safety category (Political Sensitivity, Pornography, etc.). The numbers indicate the percentage improvement in safety scores compared to a vanilla LLM.", "section": "4.1 Improving Safety by Response Detoxification"}]