{"references": [{"fullname_first_author": "Naftali Tishby", "paper_title": "Deep learning and the information bottleneck principle", "publication_date": "2015-00-00", "reason": "This paper established a connection between deep learning and information theory, which is a foundational concept for this paper's approach to mutual information estimation."}, {"fullname_first_author": "Mohamed Ishmael Belghazi", "paper_title": "Mutual information neural estimation", "publication_date": "2018-07-00", "reason": "This paper introduced MINE, a neural network-based method for estimating mutual information, providing a benchmark for comparison and a basis for this paper's novel approach."}, {"fullname_first_author": "Ziv Goldfeld", "paper_title": "Estimating information flow in deep neural networks", "publication_date": "2019-00-00", "reason": "This paper delves into estimating information flow in deep neural networks, providing relevant background and context for the study of information-theoretic analysis of deep neural networks and contributing to the motivation for the proposed method."}, {"fullname_first_author": "Thomas Steinke", "paper_title": "Reasoning About Generalization via Conditional Mutual Information", "publication_date": "2020-07-09", "reason": "This paper explores the use of conditional mutual information for understanding generalization in machine learning, which relates to the theoretical underpinnings of this paper's MI estimation."}, {"fullname_first_author": "David McAllester", "paper_title": "Formal limitations on the measurement of mutual information", "publication_date": "2020-08-00", "reason": "This paper discusses the theoretical limitations inherent in measuring mutual information, which is highly relevant to this paper's efforts to address challenges in high-dimensional MI estimation."}]}