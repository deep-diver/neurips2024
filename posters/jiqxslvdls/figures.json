[{"figure_path": "JiQXsLvDls/figures/figures_0_1.jpg", "caption": "Figure 1: We propose transforming a pair of random vectors (RVs) via a Cartesian product of learnable diffeomorphisms to facilitate mutual information (MI) estimation. Ideally, we achieve tractable MI in the latent space. As diffeomorphisms preserve information, MI between latent representations equals MI between the original RVs.", "description": "This figure illustrates the core idea of the proposed method for mutual information (MI) estimation using normalizing flows.  The method involves transforming a pair of random vectors (X, Y) representing the original data into a latent space using learnable diffeomorphisms (fx and fy).  The transformation is designed to map the original data distribution into a target distribution (\u03be, \u03b7) where MI is easier to compute.  The key is that the diffeomorphisms preserve the MI, so the estimated MI in the latent space accurately reflects the MI in the original space. The figure shows two variants: a general case and a specific case with a Gaussian base distribution.", "section": "1 Introduction"}, {"figure_path": "JiQXsLvDls/figures/figures_0_2.jpg", "caption": "Figure 1: We propose transforming a pair of random vectors (RVs) via a Cartesian product of learnable diffeomorphisms to facilitate mutual information (MI) estimation. Ideally, we achieve tractable MI in the latent space. As diffeomorphisms preserve information, MI between latent representations equals MI between the original RVs.", "description": "The figure illustrates the core idea of the proposed method for mutual information estimation.  It shows two random vectors, X and Y, being transformed via learnable diffeomorphisms (fx and fy) into latent representations \u03be and \u03b7, respectively. The transformation is designed to map the original data distribution into a target distribution where MI is easier to compute (tractable).  The key is that diffeomorphisms preserve mutual information, so I(X;Y) = I(\u03be;\u03b7). The figure displays this process graphically showing the original distributions, the diffeomorphic transformations, the resulting Gaussian-like target distributions, and the final joint distribution in the latent space.", "section": "1 Introduction"}, {"figure_path": "JiQXsLvDls/figures/figures_7_1.jpg", "caption": "Figure 2: Examples of synthetic images used in the tests. Note that images are high-dimensional, but admit latent structure, which is similar to real datasets.", "description": "This figure shows examples of the synthetic images used in the paper's experiments.  The left panel displays 2D Gaussian distributions that are transformed into high-dimensional images. The right panel shows rectangles of varying sizes and orientations, also transformed into high-dimensional images.  The caption highlights that, although the images are high-dimensional, they possess a latent structure similar to that found in real-world datasets. This similarity is important because it means the results from these synthetic experiments can be generalized to real-world scenarios.", "section": "5 Experiments"}, {"figure_path": "JiQXsLvDls/figures/figures_8_1.jpg", "caption": "Figure 3: Comparison of the selected estimators. Along x axes is I(X; Y), along y axes is \u00ce(X; Y). We plot 99.9% asymptotic CIs acquired either from the MC integration standard deviation (WKL, KSG) or from the epochwise averaging (other methods, 200 last epochs). 10 \u00b7 10\u00b3 samples were used.", "description": "This figure compares the performance of several mutual information (MI) estimators, including the proposed MIENF method, against the ground truth.  The plot shows the estimated MI (\u00ce(X;Y)) versus the true MI (I(X;Y)) for four different datasets: 16x16 and 32x32 images generated from Gaussian and rectangular distributions.  99.9% asymptotic confidence intervals (CIs) are displayed to illustrate the uncertainty in the estimates. The results highlight the accuracy and robustness of MIENF across various datasets and dimensions.", "section": "5 Experiments"}, {"figure_path": "JiQXsLvDls/figures/figures_8_2.jpg", "caption": "Figure 4: Tests with incompressible multidimensional data. \u201cUniform\u201d denotes the uniformly distributed samples acquired from the correlated Gaussians via the Gaussian CDF. \u201cSmoothed uniform\u201d and \u201cStudent\u201d denote the non-Gaussian-based distributions described in Appendix B. \u201carcsinh(Student)\u201d denotes the arcsinh function applied to the \u201cStudent\u201d example (this is done to avoid numerical instabilities in the case of long-tailed distributions). We run each test 5 times and plot 99.9% asymptotic Gaussian CIs. 10 \u00b7 10\u00b3 samples were used. Note that N-MIENF and tridiag-N-MIENF yield almost the same results with similar bias.", "description": "The figure shows the comparison of different MI estimation methods on high-dimensional synthetic datasets with non-Gaussian distributions. The results demonstrate the robustness and accuracy of the proposed MI estimators (N-MIENF and tridiag-N-MIENF) compared to other methods, especially in high-dimensional settings with long-tailed distributions.", "section": "Experiments"}, {"figure_path": "JiQXsLvDls/figures/figures_20_1.jpg", "caption": "Figure 3: Comparison of the selected estimators. Along \u00e6 axes is I(X; Y), along y axes is \u00ce(X; Y). We plot 99.9% asymptotic CIs acquired either from the MC integration standard deviation (WKL, KSG) or from the epochwise averaging (other methods, 200 last epochs). 10 \u00b7 10\u00b3 samples were used.", "description": "This figure compares the performance of several mutual information (MI) estimators, including the proposed MIENF method, against ground truth values.  The x-axis represents the true MI between two random variables (I(X;Y)), while the y-axis shows the estimated MI (\u00ce(X;Y)) from each method.  The plots show the results for Gaussian and rectangular image datasets of different sizes (16x16 and 32x32 pixels). The 99.9% asymptotic confidence intervals (CIs) illustrate the uncertainty in each estimate. The CIs for the methods based on Monte Carlo (MC) integration are calculated from the standard deviation of the MC estimate, whereas for other methods the CI is calculated by averaging over the last 200 epochs of training.  A total of 10,000 samples were used for each dataset.", "section": "5 Experiments"}, {"figure_path": "JiQXsLvDls/figures/figures_21_1.jpg", "caption": "Figure 6: Point-wise mutual information plots for MINE. Correlated uniform distribution is used, with varying ground truth MI and sampling size. Note that in the case of an insufficient sampling size, MINE \u201cmemorizes\u201d the data points and \u201challucinates\u201d the relation between X and Y, which severely increases the value of the MI estimate.", "description": "This figure shows the pointwise mutual information (PMI) plots for the Mutual Information Neural Estimator (MINE) using a correlated uniform distribution with varying ground truth mutual information (MI) and sampling sizes. The left plot shows a high MI and sufficient sampling, resulting in a reasonable approximation.  The middle plot shows low MI and sufficient sampling, also resulting in a good approximation.  The right plot, however, demonstrates the effects of insufficient sampling (only 5 samples).  Here, MINE overfits to the data and incorrectly estimates a high MI even though the true MI is zero, illustrating the issue of overfitting with small sample sizes in this method.", "section": "C Overfitting and sample complexity"}, {"figure_path": "JiQXsLvDls/figures/figures_21_2.jpg", "caption": "Figure 7: Probability density function plots for tridiag-N-MIENF. Correlated uniform distribution is used, with varying ground truth MI and sampling size. Note that in the case of an insufficient sampling size, MIENF \u201cmemorizes\u201d the data points and \u201challucinates\u201d the relation between X and Y, which severely increases the value of the MI estimate.", "description": "This figure shows the probability density functions generated by the tridiag-N-MIENF model for three different scenarios: high MI, zero MI with sufficient data, and zero MI with insufficient data. The plots show how the model's performance is affected by the amount of training data, highlighting the risk of overfitting with limited data.", "section": "C Overfitting and sample complexity"}, {"figure_path": "JiQXsLvDls/figures/figures_23_1.jpg", "caption": "Figure 8: Results of an information-based nonlinear canonical correlation analysis performed on the MNIST handwritten digits dataset. The task of MI estimation between augmented (translated/rotated/...) versions of pictures is considered. Our method (the tridiagonal version) allows for simultaneous MI estimation and nonlinear independent components learning. We illustrate the semantics of the learned nonlinear components via small perturbations along the corresponding directions in the latent space. The center of each row contains an original, unperturbed picture; pictures to the left and to the right are the results of the perturbations. We also provide the values of per-component MI. Components with high MI represent the features, which are invariant to the selected augmentations.", "description": "The figure shows the results of applying the proposed method to the MNIST dataset to perform information-based nonlinear canonical correlation analysis. The method estimates mutual information (MI) between augmented versions of handwritten digits and disentangles the underlying nonlinear components.  The images illustrate how small perturbations along the axes corresponding to high and low MI values affect the reconstructed images.  High MI components represent features invariant to the augmentations (e.g., stroke thickness, digit width), while low MI components represent the augmentations themselves (e.g., translation, zoom).", "section": "D Information-theoretic disentanglement"}, {"figure_path": "JiQXsLvDls/figures/figures_23_2.jpg", "caption": "Figure 8: Results of an information-based nonlinear canonical correlation analysis performed on the MNIST handwritten digits dataset. The task of MI estimation between augmented (translated/rotated/...) versions of pictures is considered. Our method (the tridiagonal version) allows for simultaneous MI estimation and nonlinear independent components learning. We illustrate the semantics of the learned nonlinear components via small perturbations along the corresponding directions in the latent space. The center of each row contains an original, unperturbed picture; pictures to the left and to the right are the results of the perturbations. We also provide the values of per-component MI. Components with high MI represent the features, which are invariant to the selected augmentations.", "description": "This figure shows the results of applying an information-based nonlinear canonical correlation analysis to the MNIST handwritten digits dataset. The goal was to estimate the mutual information (MI) between augmented versions of images (translated, rotated, etc.). The tridiagonal version of the proposed method was used, which allowed for simultaneous MI estimation and learning of nonlinear independent components. The figure illustrates the meaning of the learned components through small perturbations along the corresponding axes in the latent space. High MI values indicate features that are invariant to the augmentations used.", "section": "D Information-theoretic disentanglement"}, {"figure_path": "JiQXsLvDls/figures/figures_23_3.jpg", "caption": "Figure 8: Results of an information-based nonlinear canonical correlation analysis performed on the MNIST handwritten digits dataset. The task of MI estimation between augmented (translated/rotated/...) versions of pictures is considered. Our method (the tridiagonal version) allows for simultaneous MI estimation and nonlinear independent components learning. We illustrate the semantics of the learned nonlinear components via small perturbations along the corresponding directions in the latent space. The center of each row contains an original, unperturbed picture; pictures to the left and to the right are the results of the perturbations. We also provide the values of per-component MI. Components with high MI represent the features, which are invariant to the selected augmentations.", "description": "This figure shows the results of applying the proposed method to the MNIST dataset to perform disentanglement.  The method estimates the mutual information between pairs of augmented images (created by applying transformations like translation, rotation, etc.).  The figure displays the resulting non-linear components, illustrating how they capture invariant features of the digits (e.g., stroke thickness, width) and those that vary with the transformations (zoom, translation). High MI values indicate components representing features less affected by augmentation.", "section": "D Information-theoretic disentanglement"}]