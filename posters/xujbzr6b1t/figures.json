[{"figure_path": "xUjBZR6b1T/figures/figures_0_1.jpg", "caption": "Figure 1: The capability of our method to locally modify video content and motion. This ability can also be easily extended to multi-area editing. The motion control is labeled in colorful lines in videos.", "description": "This figure demonstrates the ReVideo model's ability to perform localized video editing by modifying both the content and motion of specific video regions. It showcases five examples of video editing tasks: changing content and trajectory, changing content while keeping the trajectory, keeping the content while customizing trajectory, adding new object-level interactions, and editing multiple areas simultaneously.  Each row shows the original video segment, followed by the result of applying ReVideo with the indicated content and motion modifications. The motion control is visually represented by colorful lines on the video.", "section": "Abstract"}, {"figure_path": "xUjBZR6b1T/figures/figures_3_1.jpg", "caption": "Figure 2: Two potential structures to inject motion and content control. Structure A is a compact and efficient mode that integrates motion and content control via a single module. Structure B features independent control, structurally decoupling motion and content conditions, causing higher complexity.", "description": "This figure illustrates two architectural approaches for integrating motion and content control into a Stable Video Diffusion (SVD) model for video editing. Structure A uses a single trainable control module to fuse both content and motion information before feeding it into the SVD.  This approach is more compact and efficient. In contrast, Structure B employs separate control modules for motion and content, which are then integrated into the SVD. While offering more independent control, this method increases complexity.", "section": "3 Method"}, {"figure_path": "xUjBZR6b1T/figures/figures_4_1.jpg", "caption": "Figure 3: The motion control capability of two structures in Fig. 2 with different training strategies. We visualize trajectory lines in a specific area (red box) and label the editing area with a black box. Toy experiments present the coupling issue of customized motion and unedited content.", "description": "This figure presents a comparison of the motion control capabilities of two different architectures (Structure A and Structure B) from Figure 2, using various training strategies. Each row shows the results of a toy experiment, focusing on a specific area of a video. The results highlight the challenges in decoupling motion and content control during video editing, with various training approaches impacting the ability to control the motion independently from the unedited content.", "section": "3 Method"}, {"figure_path": "xUjBZR6b1T/figures/figures_5_1.jpg", "caption": "Figure 4: The data construction strategy for decoupling training and editing results from this stage.", "description": "This figure demonstrates the data augmentation strategy used in the decoupling training stage.  The goal is to decouple the learning of content and motion by presenting the model with training data where the content and motion are from separate video sources.  The figure shows the process of combining two videos (V1 and V2) using a mask (M) to create a training sample (V) where the masked region has content from V1 and motion from a trajectory, and the unmasked region has content from V2.  The result shows the successful separation of content and motion, leading to improved editing results, reducing artifacts at the boundary between edited and unedited regions. The right image shows an example of an editing result from this decoupling training strategy.", "section": "3.3 Coarse-to-fine Training Strategy"}, {"figure_path": "xUjBZR6b1T/figures/figures_6_1.jpg", "caption": "Figure 5: The architecture of our proposed spatiotemporal adaptive fusion module (left), and the visualization of fusion weight \u0393 at different timesteps (right).", "description": "This figure illustrates the Spatiotemporal Adaptive Fusion Module (SAFM) used in ReVideo. The left side shows the architecture of the SAFM, which takes content and motion encodings as inputs, uses a sigmoid layer to generate a weight map \u0393, and combines the content and motion information to generate a fused condition feature fc. The right side displays the visualization of the weight map \u0393 at different timesteps during the video generation process. The weight map shows how the contribution of content and motion is dynamically balanced during the generation process.", "section": "3.4 Spatiotemporal Adaptive Fusion Module"}, {"figure_path": "xUjBZR6b1T/figures/figures_7_1.jpg", "caption": "Figure 6: The visual comparison between InsV2V [10], AnyV2V [26], Pika [1], and our ReVideo.", "description": "This figure presents a visual comparison of video editing results from four different methods: InsV2V, AnyV2V, Pika, and the proposed ReVideo method.  Four example video editing tasks are shown, each displayed across the four methods.  The goal is to demonstrate the ability of ReVideo to accurately edit both the content and motion of specific video regions, while maintaining the quality of unedited portions. The results highlight ReVideo's superior performance in terms of visual fidelity, accurate content editing, and realistic motion control compared to the baseline methods.", "section": "4.2 Comparison"}, {"figure_path": "xUjBZR6b1T/figures/figures_8_1.jpg", "caption": "Figure 7: Ablation study of our ReVideo.", "description": "This figure presents an ablation study of the ReVideo model, showing the effects of removing or modifying different components of the model.  The top row shows the results without using the Spatiotemporal Adaptive Fusion Module (SAFM), and with SAFM but without time adaptation. The bottom row demonstrates results from tuning all control modules in stage 3, tuning only the spatial layers in stage 3, and finally the complete ReVideo model. The visual differences highlight the importance of each component in achieving accurate and high-quality video editing.", "section": "4.3 Ablation Study"}, {"figure_path": "xUjBZR6b1T/figures/figures_13_1.jpg", "caption": "Figure 8: The trajectory sampling pipeline in ReVideo training.", "description": "The figure illustrates the three-stage trajectory sampling pipeline used in the ReVideo training process.  First, the dense sampling points are sparsified using a grid. Then, a threshold is applied to filter out points with motion lengths below a certain threshold. Finally, probabilistic sampling is performed, using the normalized motion lengths as probabilities to select a subset of points for the final trajectory map.  This process ensures that the resulting trajectory map is both representative of the video's motion and sufficiently sparse for efficient training.", "section": "A.1 Details of Trajectory Sampling"}, {"figure_path": "xUjBZR6b1T/figures/figures_13_2.jpg", "caption": "Figure 9: The robustness of our ReVideo for irregular editing areas.", "description": "This figure demonstrates the robustness of the ReVideo model when handling irregular editing areas.  Despite being trained on rectangular editing regions, the model successfully edits both content and motion in an area defined by a hand-drawn, irregular mask. The results show consistent editing across multiple frames (frames 1, 7, and 14 are shown). This highlights the model's adaptability and flexibility beyond its training data.", "section": "A.2 Robustness for Irregular Editing Area"}, {"figure_path": "xUjBZR6b1T/figures/figures_14_1.jpg", "caption": "Figure 1: The capability of our method to locally modify video content and motion. This ability can also be easily extended to multi-area editing. The motion control is labeled in colorful lines in videos.", "description": "This figure demonstrates ReVideo\u2019s ability to precisely edit both content and motion within a video.  It showcases four different editing scenarios: changing content while keeping the original trajectory, changing the trajectory while preserving the original content, changing both content and trajectory, and performing multi-area editing. Each scenario highlights the precision of the editing by using colorful lines to indicate the controlled motion trajectories. The figure visually displays that the method allows localized modifications to videos, and its application can extend to multiple areas simultaneously.", "section": "Abstract"}, {"figure_path": "xUjBZR6b1T/figures/figures_14_2.jpg", "caption": "Figure 11: The editing results in some complex scenarios. The first row has dynamic background with complex lighting, and the second row has scene change.", "description": "This figure shows two example scenarios demonstrating the challenges of applying the ReVideo method to more complex videos. The top row shows a video with a dynamic background and complex lighting, illustrating how the method attempts to integrate edits within a visually busy context. The bottom row demonstrates a scene change within the video sequence, highlighting the difficulties the approach faces in maintaining consistency across significant visual shifts.", "section": "A.4 Additional Discussion in Complex Scenarios"}, {"figure_path": "xUjBZR6b1T/figures/figures_15_1.jpg", "caption": "Figure 1: The capability of our method to locally modify video content and motion. This ability can also be easily extended to multi-area editing. The motion control is labeled in colorful lines in videos.", "description": "This figure demonstrates the ReVideo model's ability to perform local modifications on video content and motion.  The top row shows the original video frames. Subsequent rows illustrate different editing scenarios: changing content while preserving the original trajectory, changing the trajectory while maintaining the original content, and simultaneously changing both content and trajectory.  Colorful lines highlight the motion control applied in each edited video.", "section": "Abstract"}, {"figure_path": "xUjBZR6b1T/figures/figures_16_1.jpg", "caption": "Figure 13: The necessity of fine-tuning key embedding and value embedding in the base model, i.e., SVD.", "description": "This figure shows the results of two ablation studies. In the first ablation study, only the key and value embeddings in the control module were fine-tuned. In the second ablation study, both the key and value embeddings in the control module and the base model were fine-tuned. The results show that fine-tuning both the control module and the base model leads to better results in terms of visual quality and consistency.  The images demonstrate that fine-tuning the base model, in addition to the control module, helps to mitigate artifacts that can occur from the mismatch between the edited and unedited regions of the video.", "section": "A.7 Necessity of Fine-tuning the Base Model"}, {"figure_path": "xUjBZR6b1T/figures/figures_16_2.jpg", "caption": "Figure 14: The ability of our ReVideo to extend the number of editing frames. The results demonstrate the performance of our ReVideo in processing a 9-second video containing 90 frames.", "description": "This figure shows the results of applying the ReVideo method to a longer video (9 seconds, 90 frames).  Instead of editing a fixed number of frames, ReVideo uses a sliding window approach. The last frame of the previous window serves as the reference image for the next window.  The figure demonstrates that ReVideo can successfully propagate edits from the initial frame across all 90 frames while maintaining motion consistency, although some error accumulation is visible towards the end, affecting the overall quality.", "section": "A.8 More Frame Editing"}, {"figure_path": "xUjBZR6b1T/figures/figures_17_1.jpg", "caption": "Figure 3: The motion control capability of two structures in Fig. 2 with different training strategies. We visualize trajectory lines in a specific area (red box) and label the editing area with a black box. Toy experiments present the coupling issue of customized motion and unedited content.", "description": "This figure compares two different model architectures (Structure A and Structure B) for integrating motion and content control in video editing, as shown in Figure 2.  It demonstrates the effects of different training strategies on the models' ability to control the motion of edited content, specifically within a target area indicated by a red box. The black boxes mark the editing areas.  Toy experiments highlight the challenges of coordinating customized motion with unchanged content in the video.", "section": "3 Method"}]