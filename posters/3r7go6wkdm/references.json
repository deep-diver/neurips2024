{"references": [{"fullname_first_author": "Preetum Nakkiran", "paper_title": "Deep double descent: Where bigger models and more data hurt", "publication_date": "2021-12-00", "reason": "This paper introduces the concept of deep double descent, a phenomenon that challenges conventional wisdom in model selection and is directly relevant to the paper's exploration of post-hoc reversal."}, {"fullname_first_author": "Pavel Izmailov", "paper_title": "Averaging weights leads to wider optima and better generalization", "publication_date": "2018-00-00", "reason": "This paper introduces Stochastic Weight Averaging (SWA), a key post-hoc transform analyzed in the paper, and its impact on generalization, which is a central theme of the paper."}, {"fullname_first_author": "Chuan Guo", "paper_title": "On calibration of modern neural networks", "publication_date": "2017-00-00", "reason": "This paper discusses temperature scaling, another critical post-hoc transform examined in the paper, which is crucial for improving the calibration and reliability of predictions."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-00-00", "reason": "This paper explores the scaling behavior of neural language models, which is relevant as the paper studies how post-hoc transformations affect different sizes of models in various contexts."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2020-00-00", "reason": "This paper introduces the Massive Multitask Language Understanding (MMLU) benchmark, used in the paper's evaluation of language models and a key component of demonstrating the approach's effectiveness on a large-scale multitask benchmark."}]}