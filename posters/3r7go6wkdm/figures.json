[{"figure_path": "3R7Go6WkDm/figures/figures_1_1.jpg", "caption": "Figure 1: An illustration of the phenomenon of post-hoc reversal on the FMoW dataset: base performance at epoch t2 is worse than at epoch t\u2081 (b2 > b\u2081), but post-hoc performance is better (p2 < p1). The current practice of naive selection considers base metrics to pick models at epoch t\u2081. Our proposed technique of post-hoc selection instead uses post-hoc metrics to pick models at epoch t2, resulting in > 2\u00d7 improvement over naive selection in both test loss and error. SWA+Ens+TS refers to the post-hoc transform obtained by composing SWA, ensemble (Ens) and temperature scaling (TS). Base curves show mean of 8 runs, models from which constitute the ensembles. Individual runs are shown in lighter colors. See Fig. 5 for more detailed curves on this dataset.", "description": "This figure illustrates the concept of post-hoc reversal using the FMoW dataset.  It shows that while the base model's performance is worse at a later epoch (t2), the performance after applying post-hoc transforms (SWA, ensembling, and temperature scaling) is better at the later epoch. This highlights the limitations of naive selection (choosing models based on initial performance) and advocates for post-hoc selection (choosing models based on performance after transforms).  The figure compares the performance curves before and after applying the post-hoc transforms.", "section": "4 Post-Hoc Reversal: Formalization and Empirical Study"}, {"figure_path": "3R7Go6WkDm/figures/figures_1_2.jpg", "caption": "Figure 2: A comparison of naive and post-hoc selection on label sets from CIFAR-10/100-N (abbr. C-10/100-N) for the SWA+TS transform. On noisy label sets, post-hoc selection is often > 2\u00d7 better.", "description": "This figure compares the performance of naive and post-hoc selection methods on CIFAR-10/100-N datasets with varying noise levels using the SWA+TS transform. The results show that post-hoc selection consistently outperforms naive selection, especially in high-noise settings, often achieving more than double the improvement.", "section": "4.2 Experiments"}, {"figure_path": "3R7Go6WkDm/figures/figures_4_1.jpg", "caption": "Figure 3: Loss and error for CIFAR-10-N Clean (approx. 0% noise), Rand1 (approx. 17% noise) and Worst (approx. 40% noise). Except for ensemble curves, mean of 8 runs is shown; individual runs are in lighter shades. Ensembles comprise models from these 8 runs. For example, observe post-hoc reversal for C-10-N Worst: (1) error plot: from epoch 5 to 50, solid red (base) curve worsens but solid orange (SWA) curve improves; (2) error plot: solid red (base) curve has a double descent but dashed red (ensemble) curve does not; (3) loss plots: solid red (base) curve has a double descent pre-TS but not post-TS; (4) error plot: best error is at approx. epoch 5 for solid red (base) curve but at approx. epoch 60 for dashed orange (SWA ensemble) curve.", "description": "The figure shows the training curves (loss and error) for different noise levels in the CIFAR-10-N dataset.  It compares the performance of base models with those that have undergone Stochastic Weight Averaging (SWA) and ensembling transformations. Key observations highlight post-hoc reversal where performance trends are reversed after applying the transformations, especially in high-noise scenarios.  The impact of these transformations on overfitting and the double descent phenomenon is also demonstrated.", "section": "4.2.1 Epoch-Wise Post-Hoc Reversal"}, {"figure_path": "3R7Go6WkDm/figures/figures_5_1.jpg", "caption": "Figure 4: C-10-N Worst test curves against model size. Best width for solid blue curves is ~ 10 but for dashed orange curves, it is ~ 50 for error and ~ 25 for post-TS loss.", "description": "This figure shows the test curves for CIFAR-10-N Worst dataset against different ResNet widths.  It demonstrates the phenomenon of post-hoc reversal with respect to model size. The base model (blue solid line) shows that a smaller ResNet width performs better early on, but after applying post-hoc transforms (SWA), a larger width is optimal.  This reversal is observed for both loss and error metrics. Note the significant performance improvement due to SWA across all model sizes. The ensemble results further highlight the impact of post-hoc transforms.", "section": "4.2 Model-Wise Post-Hoc Reversal"}, {"figure_path": "3R7Go6WkDm/figures/figures_5_2.jpg", "caption": "Figure 5: FMoW test curves for 3 LR schedules. Note that the pre-TS loss is significantly higher than the post-TS loss. For example, observe post-hoc reversal w.r.t. cosine and constant LRs at epoch 50 between: (1) solid blue (base) and dashed blue (ensemble) error curves; (2) solid blue (base) and solid orange (SWA) post-TS loss curves; (3) solid blue (base) curves for pre-TS and post-TS loss.", "description": "This figure shows the test curves for the FMoW dataset with three different learning rate schedules: exponential, cosine, and constant.  It illustrates the phenomenon of post-hoc reversal, where performance trends differ between models before and after applying post-hoc transformations (temperature scaling, ensembling, and stochastic weight averaging). The pre-TS (before temperature scaling) loss is consistently higher than the post-TS loss for all learning rate schedules.  Key observations include the reversal of performance trends between base models and models with post-hoc transformations across different epochs, highlighting the non-monotonicity between the base and post-hoc curves. This highlights how post-hoc transforms can reverse the performance trend between different models.", "section": "4.2 Experiments"}, {"figure_path": "3R7Go6WkDm/figures/figures_6_1.jpg", "caption": "Figure 6: Evolution of the fit/memorization of clean and mislabeled examples during training, for base and SWA models on C-10-N Worst. Train error drops earlier for the clean subset. In the regime of post-hoc reversal (shaded), SWA further lowers the train error on the clean subset, while raising it on the mislabeled subset.", "description": "This figure shows the training and test errors for clean and mislabeled subsets of the CIFAR-10-N Worst dataset.  The top plot shows the training error, illustrating how both the base model and the model using stochastic weight averaging (SWA) overfit. However, SWA overfits later than the base model. The shaded region highlights where post-hoc reversal occurs: the test error for SWA is lower than the base model, even though SWA's training error is higher, showing that SWA helps to correct for the detrimental influence of mislabeled examples. The bottom plot shows the test error. It indicates that post-hoc reversal is happening in the epoch range between 20 and 50. ", "section": "4.2.1 Epoch-Wise Post-Hoc Reversal"}, {"figure_path": "3R7Go6WkDm/figures/figures_8_1.jpg", "caption": "Figure 9: Perplexity and causal language modeling (CLM) error on the Guanaco test set, and MMLU accuracy (higher is better) for instruction tuning LLaMA-2-7B. Shading indicates post-hoc reversal. Base and SWA+TS curves are mean of 8 runs; SWA+Ens+TS ensembles models from these runs.", "description": "This figure shows the perplexity, causal language modeling error, and MMLU accuracy for instruction-tuning the LLaMA-2-7B model on the Guanaco dataset.  It compares the performance of the base model to models using SWA+TS and SWA+Ens+TS transforms.  The shaded regions highlight instances of post-hoc reversal, where performance trends change after the application of post-hoc transformations. The results suggest that post-hoc selection, which considers post-hoc metrics, outperforms the naive selection that only considers the metrics before the post-hoc transformations.", "section": "7.1 LLM Instruction Tuning"}, {"figure_path": "3R7Go6WkDm/figures/figures_9_1.jpg", "caption": "Figure 10: Test curves for 3 real-world noisy datasets. Note that the pre-TS loss is significantly higher than the post-TS loss. Examples of post-hoc reversal between the base curves given by the solid blue lines and the post-hoc curves given by the dashed orange lines (SWA ensemble): (1) optimal epoch is different for base and post-hoc curves for error and post-TS loss on all datasets; (2) for error on Yelp, base curve shows double descent but post-hoc curve does not; (3) for error on Income, base curve overfits catastrophically at approx. epoch 5 but post-hoc curve continues improving till approx. epoch 20; (4) for error on Reddit-12k, base curve does not show double descent but post-hoc curve does.", "description": "This figure shows the test loss and error curves for three real-world noisy datasets: Yelp, Income, and Reddit-12k.  It demonstrates post-hoc reversal, where the performance trends (loss and error) of the base models are reversed after applying post-hoc transforms (SWA ensemble). Key observations include differing optimal epochs for base and post-hoc metrics and the impact on double descent behavior.", "section": "4.2 Model-Wise Post-Hoc Reversal"}, {"figure_path": "3R7Go6WkDm/figures/figures_17_1.jpg", "caption": "Figure 1: An illustration of the phenomenon of post-hoc reversal on the FMoW dataset: base performance at epoch t2 is worse than at epoch t1 (b2 > b1), but post-hoc performance is better (p2 < p1). The current practice of naive selection considers base metrics to pick models at epoch t1. Our proposed technique of post-hoc selection instead uses post-hoc metrics to pick models at epoch t2, resulting in > 2\u00d7 improvement over naive selection in both test loss and error. SWA+Ens+TS refers to the post-hoc transform obtained by composing SWA, ensemble (Ens) and temperature scaling (TS). Base curves show mean of 8 runs, models from which constitute the ensembles. Individual runs are shown in lighter colors. See Fig. 5 for more detailed curves on this dataset.", "description": "This figure illustrates the concept of post-hoc reversal.  The naive selection method chooses a model based on its initial performance, while the post-hoc selection method incorporates post-hoc transformations (SWA, ensembling, temperature scaling) to assess performance and select a superior model based on improved metrics.  The example shows a scenario where a model performs worse initially but significantly better after transformations are applied.", "section": "4 Post-Hoc Reversal: Formalization and Empirical Study"}, {"figure_path": "3R7Go6WkDm/figures/figures_22_1.jpg", "caption": "Figure 9: Perplexity and causal language modeling (CLM) error on the Guanaco test set, and MMLU accuracy (higher is better) for instruction tuning LLaMA-2-7B. Shading indicates post-hoc reversal. Base and SWA+TS curves are mean of 8 runs; SWA+Ens+TS ensembles models from these runs.", "description": "This figure shows the perplexity, causal language modeling error, and MMLU accuracy for instruction-tuned LLaMA-2-7B model.  It compares the performance of base models with those enhanced by SWA+TS and SWA+Ens+TS post-hoc transforms across different training epochs. The shaded areas highlight instances of post-hoc reversal, where performance trends reverse after applying post-hoc transforms. Notably, the SWA+Ens+TS ensemble, which combines multiple models, demonstrates improved results overall.", "section": "7.1 LLM Instruction Tuning"}, {"figure_path": "3R7Go6WkDm/figures/figures_22_2.jpg", "caption": "Figure 16: Decision surfaces of 2 models and the ensemble (of 16 models) on a synthetic 2D dataset of spirals, at epochs 440 and 1000, between which post-hoc reversal occurs (Fig. 15).", "description": "This figure visualizes the decision boundaries learned by two individual models and their ensemble on a synthetic 2D spiral dataset at epochs 440 and 1000. The goal is to illustrate the phenomenon of post-hoc reversal. At epoch 440, the individual models show complex and spiky decision boundaries, especially around noisy data points. The ensemble, however, displays a smoother decision boundary. At epoch 1000, while the individual models' decision boundaries become even more erratic, the ensemble's boundary remains relatively consistent.  This demonstrates how post-hoc transforms like ensembling can mitigate the impact of noisy data by suppressing the influence of mislabeled examples, which leads to the reversal of performance trends observed between the base models and the ensemble.", "section": "4.2 Experiments"}]