[{"figure_path": "UGlDVc0GTU/figures/figures_1_1.jpg", "caption": "Figure 1: Zero-shot policy adaptation to contexts: In case 1, the instruction includes only the task goal. In cases 2 and 3, the instruction is supplemented by the task goal with the context. Conventional language-conditioned skill approaches struggle to generate trajectories well aligned with the contexts, and typically succeed only for instructions as in case 1. Conversely, our LLM-based policy adaptation approach effectively adapts to the contexts in a zero-shot manner across all cases.", "description": "This figure illustrates the difference between conventional language-conditioned skill approaches and the proposed LLM-based policy adaptation method (LDuS) for zero-shot policy adaptation.  In the conventional approach, the system only receives the task goal (e.g., \"Close Drawer\"), and its performance is limited.  LDuS, however, uses an LLM to process both the goal and additional context (e.g., \"with a target speed of 8 m/s\", \"with low power usage\"), enabling the generation of trajectories adapted to various contextual requirements in a zero-shot manner (without explicit training on those contexts). The figure visually represents how LDuS handles the different contexts, demonstrating superior adaptability compared to conventional methods.", "section": "Introduction"}, {"figure_path": "UGlDVc0GTU/figures/figures_8_1.jpg", "caption": "Figure 4: Skill trajectory coverage", "description": "This figure visualizes the t-SNE embeddings of h-length trajectories from the dataset and skill trajectories generated by LDuS with guidance, using different contexts and varying the guidance weight \u03b4 ranging from 0.05 to 0.4.  The embeddings are expanded via LDuS, showing that LDuS can generate novel skill trajectories that were not in the original dataset but are necessary for adapting to different contexts.  This demonstrates the versatility of LDuS in its ability to generate trajectories in response to various contexts.", "section": "5.3 Ablation study"}, {"figure_path": "UGlDVc0GTU/figures/figures_8_2.jpg", "caption": "Figure 5: Ablation on sequential in-painting", "description": "The figure shows the ablation study on sequential in-painting. It compares the performance of LDuS and Diffuser with and without sequential in-painting on multi-stage MetaWorld. The results demonstrate that sequential in-painting significantly improves both success rate and context reward, especially for LDuS, which benefits from learning common skills from the dataset.", "section": "5.3 Ablation study"}, {"figure_path": "UGlDVc0GTU/figures/figures_12_1.jpg", "caption": "Figure 1: Zero-shot policy adaptation to contexts: In case 1, the instruction includes only the task goal. In cases 2 and 3, the instruction is supplemented by the task goal with the context. Conventional language-conditioned skill approaches struggle to generate trajectories well aligned with the contexts, and typically succeed only for instructions as in case 1. Conversely, our LLM-based policy adaptation approach effectively adapts to the contexts in a zero-shot manner across all cases.", "description": "This figure illustrates the limitations of conventional language-conditioned skill approaches in adapting to various contexts and highlights the effectiveness of the proposed LLM-based policy adaptation framework (LDuS) in handling zero-shot policy adaptation to unseen contexts.", "section": "1 Introduction"}]