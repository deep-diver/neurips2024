{"references": [{"fullname_first_author": "Emmanuel Abbe", "paper_title": "The merged-staircase property: a necessary and nearly sufficient condition for SGD learning of sparse functions on two-layer neural networks", "publication_date": "2022-00-00", "reason": "This paper introduces the merged-staircase property, a crucial concept that the current paper generalizes and builds upon for its analysis of SGD learnability."}, {"fullname_first_author": "Emmanuel Abbe", "paper_title": "SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics", "publication_date": "2023-00-00", "reason": "This paper expands on the previous work by introducing leap complexity, further developing the theoretical framework used in the current study."}, {"fullname_first_author": "Song Mei", "paper_title": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit", "publication_date": "2019-00-00", "reason": "This paper provides foundational mean-field theory for two-layer neural networks, which is essential to the current paper's mean-field analysis of SGD."}, {"fullname_first_author": "Song Mei", "paper_title": "A mean field view of the landscape of two-layer neural networks", "publication_date": "2018-00-00", "reason": "This paper offers a comprehensive mean-field analysis of the loss landscape in two-layer neural networks, providing crucial context for understanding the current research's focus on loss functional decay."}, {"fullname_first_author": "Lenaic Chizat", "paper_title": "On the global convergence of gradient descent for over-parameterized models using optimal transport", "publication_date": "2018-00-00", "reason": "This paper establishes the global convergence of gradient descent for over-parameterized models, which is highly relevant to the current paper's investigation of SGD's performance in learning sparse polynomials."}]}