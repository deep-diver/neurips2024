[{"type": "text", "text": "Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ziang Chen Department of Mathematics Massachusetts Institute of Technology Cambridge, MA 02139 ziang@mit.edu ", "page_idx": 0}, {"type": "text", "text": "RongGe Department of Computer Science and Department of Mathematics Duke University Durham, NC 27708 rongge@cs.duke.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this work, we study the mean-field flow for learning subspace-sparse polynomials using stochastic gradient descent and two-layer neural networks, where the input distribution is standard Gaussian and the output only depends on the projection of the input onto a low-dimensional subspace. We establish a necessary condition for SGD-learnability, involving both the characteristics of the target function and the expressiveness of the activation function. In addition, we prove that the condition is almost sufficient, in the sense that a condition slightly stronger than the necessary condition can guarantee the exponential decay of the loss functional to zero. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural Networks (NNs) are powerful in practice to approximate mappings on certain data structures, such as Convectional Neural Networks (CNNs) for image data, Graph Neural Networks (GNNs) for graph data, and Recurrent Neural Networks (RNNs) for sequential data, stimulating numerous breakthroughs in application of machine learning in many branches of science, engineering, etc. The surprising performance of neural networks is often explained by arguing that neural networks automatically learns useful representations of the data. However, how simple training procedures such as stochastic gradient descent (SGD) extract features remains a major open problem. ", "page_idx": 0}, {"type": "text", "text": "Optimization of neural networks has received lots of attention. For simpler networks such as linear neural networks, local minima are also globally optimal [14, 15, 17]. However, this is not true for nonlinear networks even of depth 2 [23]. Neural Tangent Kernel (NTK, [4, 12, 13]) is a line of work that establishes strong convergence results for wide neural networks. However, in the NTK regime, neural network is equivalent to a kernel, which cannot learn useful features based on the target function. Such limitation prevents neural networks in NTK regime from efficiently learning even simple single index models [28]. ", "page_idx": 0}, {"type": "text", "text": "As an alternative, the behavior of SGD can also be understood via mean-field analysis, for both two-layer neural networks [9, 19, 20, 22, 24, 25] and multi-layer neural networks [5, 21, 22]. Neural networks in the mean-field regime have the potential to do feature learning. Recently, [1] showed an interesting setup where a two-layer neural network can learn representations if the target function satisfies a merged-staircase property. More precisely, [1] considers a sparse polynomial as a polynomial $f^{*}:\\mathbb{R}^{d}\\xrightarrow{}\\mathbb{R}$ defined on the hypercube $\\{-1,1\\}^{d}$ i.e.,. $f^{*}(x)=h^{*}(z)=h^{*}(x_{I})$ where $z=x_{I}=(x_{i})_{i\\in I}$ $I$ is an unknown subset of $\\{1,2,\\ldots,d\\}$ with $|I|=p$ , and $h^{\\ast}:\\{-1,1\\}^{p}\\to\\mathbb{R}$ is a function on the subset of coordinates in $I$ . They prove that a condition called the merged-staircase property is necessary and in some sense sufficient for learning such $f^{*}$ using SGD and two-layer neural networks. The merged-staircase property proposed in [1] states that all monomials of $h^{*}$ can be ordered such that each monomial contains at most one $z_{i}$ that does not appear in any previous monomial. For example, $h^{*}(z)=z_{1}+z_{1}z_{2}+z_{1}z_{2}z_{3}$ satisfies the merged-staircase property while $h^{\\ast}(z)=z_{1}+z_{1}z_{2}z_{3}$ does not. Results on similar structures can also be found in [3]. The work [2] proposes the concept of leap complexity and generalizes the results in [1] to a larger family of sparse polynomials. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we consider \u201csubspace-sparse\" polynomial that is more general. Concretely, let $f^{*}(x)\\;=\\;h^{*}(z)\\;=\\;h^{*}(x_{V})$ ,where $V$ is a subspace of $\\mathbb{R}^{d}$ with $\\mathrm{dim}(V)\\,=\\,p\\,\\ll\\,d,\\,x_{V}$ is the orthogonal projection of $x$ onto the subspace $V$ , and $h^{*}:V\\to\\mathbb{R}$ is an underlying polynomial map. In other words, the sparsity is in the sense that $f^{*}(x)$ only depends on the projection of the input $x\\in\\mathbb{R}^{d}$ in a low-dimensional subspace. Throughout this paper, the input data distribution is the standard $d$ -dimensional normal distribution, i.e., $\\boldsymbol{x}\\_{\\sim\\mathcal{N}(\\bar{0},\\bar{I}_{d})}$ , which is rotation-invariant in the sense that $O x\\sim\\mathcal{N}(0,I_{d})$ for any orthogonal matrix $O\\in\\mathbb{R}^{d\\times d}$ . Similar rotation-invariant/basis-free settings are also considered in some recent studies, including [2, 8, 10, 11]. ", "page_idx": 1}, {"type": "text", "text": "Our contribution and related works  Our first contribution is a basis-free necessary condition for SGD-learnability. More specially, we propose the reflective property of the underlying polynomial $h^{*}:V\\to\\mathbb{R}$ with respect to some subspace $S\\subset V$ , which also involves the expressiveness of the activation function. We prove that as long as the reflective property is satisfied with respect to nontrivial $S$ , the training dynamics cannot learn any information about the behavior of $h^{*}$ on $S$ (see Theorem 3.4). Therefore the loss functional will be bounded away from O during the whole training procedure. ", "page_idx": 1}, {"type": "text", "text": "One key point is that our refective property precisely characterizes the necessary expressiveness of the activation function. If the activation function is expressive enough, the refective property equivalently recovers a necessary condition characterized by isoLeap [2] that is the maximal leap complexity over all orthonormal basis and can be viewed as a basis-free generalization of the mergedstaircase property. This also indicates that our necessary condition is a bit weaker. Other related rotation-invariant conditions in the previous literature include leap exponent/index [8, 10], subspace conditioning [10] and even-symmetric directions [11]. The analysis in [10, 11] is for training the first layer for finitely many iterations with fixed second-layer, and [8] studies the joint learning dynamics where they assume that for any fixed first layer, the optimal parameters in second layer can be found efficiently and reformulate the loss as a function of the first layer. Differently and more generally, our analysis for the necessary condition does not require specific learning strategies and works for any learning rates satisfying some mild conditions. ", "page_idx": 1}, {"type": "text", "text": "Our second contribution is a sufficient condition for SGD-learnability that is also basis-free and is slightly stronger than the necessary condition. In particular, we show that if the training dynamics cannot be trapped in any proper subspace of $V$ , then one can choose the initial parameter distribution and the learning rate such that the loss functional decays to zero exponentially fast with dimensionfree rates (see Theorem 4.3). Our training strategy is inspired by [1] with the difference that we take the average of $p$ independent training trajectories, which can lift some linear independence property required for polynomials on hypercube to algebraic independence in the general polynomial setting. ", "page_idx": 1}, {"type": "text", "text": "Technical challenges  It may seem simple to leave the standard basis and generalize the results of [1, 2] to learn subspaces, because SGD itself is independent of the basis, and we can consider a symmetric Gaussian input distribution. However, there are some significant barriers that motivated our training process. The condition and the analysis in [1, 2] rely on an orthonormal basis of the input space $\\mathbb{R}^{d}$ . This is natural for polynomials on the hypercube $\\{-1,1\\}^{d}$ , but not for general polynomials on $\\mathbb{R}^{d}$ . Particularly, their theory does not work for Gaussian input data $x\\sim\\mathcal{N}(0,I_{d})$ , which is probably the most common distribution in data science, unless an orthonormal basis of $\\mathbb{R}^{d}$ is specified and $V$ is known to be spanned by $p$ elements in the basis. In this work, we consider a more general setting in which specifying a basis is not required and the space $V$ can be any $p$ -dimensional subspace of $\\mathbb{R}^{d}$ . This setting is consistent with the rotation-invariant property of $\\mathcal{N}(0,I_{d})$ and introduces more difficulties since less knowledge of $V$ is available prior to training. ", "page_idx": 1}, {"type": "text", "text": "Organization  The rest of this paper will be organized as follows. We introduce some preliminaries on mean-field dynamics in Section 2. The basis-free necessary and sufficient conditions for SGDlearnability are discussed in Section 3 and Section 4, respectively. We conclude in Section 5. ", "page_idx": 2}, {"type": "text", "text": "2   Preliminaries on Mean-Field Dynamics ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The mean-field dynamics describes the limiting behavior of the training procedure when the stepsize/learning rate converges to zero, i.e., the evolution of a neuron converges to the solution of a differential equation with continuous time, and when the number of neurons converges to infinity, i.e., the empirical distribution of all neurons converges to some limiting probability distribution. For two-layer neural networks, some quantitative results are established in [20] that characterize the distance between the SGD trajectory and the mean-field evolution flow, and these results are further improved as dimension-free in [19]. Such results suggest that analyzing the mean-field fow is sufficient for understanding the SGD trajectory in some settings. In this section, we briefly review the setup of two-layer neural networks, SGD, and their mean-field versions, following [19, 20]. ", "page_idx": 2}, {"type": "text", "text": "Two-layer neural network and SGD  The two-layer neural network is of the following form: ", "page_idx": 2}, {"type": "equation", "text": "$$\nf_{\\mathrm{NN}}(\\boldsymbol{x};\\Theta):=\\frac{1}{N}\\sum_{i=1}^{N}\\tau(\\boldsymbol{x};\\boldsymbol{\\theta}_{i})=\\frac{1}{N}\\sum_{i=1}^{N}a_{i}\\sigma(w_{i}^{\\top}\\boldsymbol{x}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $N$ is the number of neurons, $\\Theta=(\\theta_{1},\\theta_{2},\\ldots,\\theta_{N})$ with $\\theta_{i}=(a_{i},w_{i})\\in\\mathbb{R}^{d+1}$ is the set of parameters, and $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ is the activation functions with $\\tau(x;\\theta):=a\\sigma(w^{\\top}x)$ for $\\theta=(a,w)$ Then the task is to find some parameter $\\Theta$ such that the $\\ell_{2}$ -distance between $f^{*}$ and $f_{\\mathrm{NN}}$ is minimized: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Theta}\\ \\mathcal{E}_{N}(\\Theta):=\\frac{1}{2}\\mathbb{E}_{x\\sim\\mathcal{N}(0,I_{d})}\\left[|f^{*}(x)-f_{\\mathrm{NN}}(x;\\Theta)|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In practice, a widely used algorithm for solving (2.2) is the stochastic gradient descent (SGD) that iterates as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\theta_{i}^{(k+1)}=\\theta_{i}^{(k)}+\\gamma^{(k)}\\left(f^{*}(x_{k})-f_{\\mathrm{NN}}(x_{k};\\Theta^{(k)})\\right)\\nabla_{\\theta}\\tau(x_{k};\\theta_{i}^{(k)}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Where $x_{k},\\;k=1,2,\\ldots$ are the id. samples drawn from $\\mathcal{N}(0,I_{d})$ and $\\gamma^{(k)}=\\mathrm{diag}(\\gamma_{a}^{(k)},\\gamma_{w}^{(k)}I_{d})\\succeq$ O is the stepsize or the learning rate. In this paper, we only consider the one-pass model with each data point being used exactly once, following [19]. ", "page_idx": 2}, {"type": "text", "text": "Mean-field dynamics  One can generalize (2.1) to an infinite-width two-layer neural network: ", "page_idx": 2}, {"type": "equation", "text": "$$\nf_{\\mathrm{NN}}(\\boldsymbol{x};\\boldsymbol{\\rho}):=\\int\\tau(\\boldsymbol{x};\\boldsymbol{\\theta})\\rho(d\\boldsymbol{\\theta})=\\int a\\sigma(\\boldsymbol{w}^{\\top}\\boldsymbol{x})\\rho(d a,d w),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\rho\\,\\in\\,\\mathcal{P}(\\mathbb{R}^{d+1})$ is a probability measure on the parameter space $\\mathbb{R}^{d+1}$ , and generalize the loss/energy functional (2.2) to ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\mathcal E}(\\rho):=\\frac{1}{2}\\mathbb{E}_{x\\sim\\mathcal N(0,I_{d})}\\left[\\left|f^{*}(x)-f_{\\mathrm{NN}}(x;\\rho)\\right|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Wewill use ${\\mathcal{P}}(X)$ to denote the collection of probability measures on a space $X$ throughout this paper. The limiting behavior of the SGD trajectory (2.3) when $\\gamma^{(k)}~\\to~0$ and $N\\rightarrow\\infty$ can be described by the following mean-field dynamics: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\partial_{t}\\rho_{t}=\\nabla_{\\theta}\\cdot\\left(\\rho_{t}\\xi(t)\\nabla_{\\theta}\\Phi(\\theta;\\rho_{t})\\right),\\right.}\\\\ {\\left.\\left(\\rho_{t}\\right|_{t=0}=\\rho_{0},\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\xi(t)=\\mathrm{diag}(\\xi_{a}(t),\\xi_{w}(t)I_{d})\\in\\mathbb{R}^{(d+1)\\times(d+1)}$ with $\\xi_{a}(t)\\geq0$ and $\\xi_{w}(t)\\ge0$ being the learning rates and ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Phi(\\theta;\\rho)=a\\mathbb{E}_{x\\sim\\mathcal{N}(0,I_{d})}\\left[\\left(f_{\\mathrm{NN}}(x;\\rho)-f^{*}(x)\\right)\\sigma(w^{\\top}x)\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "One can also write $\\Phi(\\theta;\\rho)$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Phi(\\theta;\\rho)=V(\\theta)+\\int U(\\theta,\\theta^{\\prime})\\rho(d\\theta^{\\prime}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "$$\nV(\\theta)=-a\\mathbb{E}_{x}\\left[f^{*}(x)\\sigma(w^{\\top}x)\\right]\\quad\\mathrm{and}\\quad U(\\theta,\\theta^{\\prime})=a a^{\\prime}\\mathbb{E}_{x}\\left[\\sigma(w^{\\top}x)\\sigma((w^{\\prime})^{\\top}x)\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The PDE (2.4) is understood in the weak sense,i.e., $\\rho_{t}$ is a solution to (2.4) if and onlyif $\\left.\\rho_{t}\\right|_{t=0}=\\rho_{0}$ and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\iint\\left(-\\partial_{t}\\eta+\\nabla_{\\theta}\\eta\\cdot\\left(\\xi(t)\\nabla_{\\theta}\\Phi(\\theta;\\rho_{t})\\right)\\right)\\rho_{t}(d\\theta)d t=0,\\quad\\forall\\,\\eta\\in\\mathcal{C}_{c}^{\\infty}(\\mathbb{R}^{d+1}\\times(0,+\\infty)),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Where $\\mathcal{C}_{c}^{\\infty}(\\mathbb{R}^{d+1}\\times(0,+\\infty))$ is the collection of all smooth and compactly supported functions on $\\mathbb{R}^{d+1}\\times(0,+\\infty)$ . It can also be computed that the energy functional is non-increasing along $\\rho_{t}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\mathcal{E}(\\rho_{t})=-\\int\\nabla_{\\theta}\\Phi(\\theta;\\rho_{t})^{\\top}\\xi(t)\\nabla_{\\theta}\\Phi(\\theta;\\rho_{t})\\rho_{t}(d\\theta)\\leq0.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "There have been standard results in the existing literature that provide dimension-free bounds for the distance between the empirical distribution of the parameters generalized by (2.3) and the solution to (2.4). For the simplicity of reading, we will not present those results and the proof; interested readers are referred to [19]. In the rest of this paper, we will focus on the analysis of (2.4) and briefly discuss the sample complexity results implied by our mean-field analysis. ", "page_idx": 3}, {"type": "text", "text": "3  Necessary Condition for SGD-Learnability ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section introduces a condition that can prevent SGD from recovering all information about $f^{*}$ or in other words, prevent the loss functional $\\mathcal{E}(\\rho_{t})$ decaying to a value sufficiently close to 0. ", "page_idx": 3}, {"type": "text", "text": "3.1  Reflective Property ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Before rigorously presenting our main theorem, we state the assumptions used in this section. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.1. Assume that the followings hold: ", "page_idx": 3}, {"type": "text", "text": "(i)Theactivationfunction $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ istwicecontinuously differentiablewith $\\|\\sigma\\|_{L^{\\infty}(\\mathbb{R})}\\leq$ $K_{\\sigma}$ $\\|\\sigma^{\\prime}\\|_{L^{\\infty}(\\mathbb{R})}\\le K_{\\sigma}$ and $\\|\\sigma^{\\prime\\prime}\\|_{L^{\\infty}(\\mathbb{R})}\\le K_{\\sigma}$ for some constant $K_{\\sigma}>0$   \n(i The learning rates $\\xi_{a},\\xi_{w}:\\mathbb{R}_{\\ge0}\\rightarrow\\mathbb{R}$ satisfy that $\\|\\xi_{a}\\|_{L^{\\infty}(\\mathbb{R}_{\\geq0})}\\leq K_{\\xi}$ and $\\|\\xi_{w}\\|_{L^{\\infty}(\\mathbb{R}_{\\geq0})}\\leq$ $K_{\\xi}$ for some constant $K_{\\xi}\\,>\\,0$ Furthermore, $\\xi_{a}$ and $\\xi_{w}$ areLipschitzcontinuouswith $\\begin{array}{r}{\\int_{0}^{+\\infty}\\xi_{a}(t)d t=+\\infty}\\end{array}$ and $\\begin{array}{r}{\\int_{0}^{+\\infty}\\xi_{w}(t)d t=+\\infty.}\\end{array}$   \n(ii) The initialization is $\\rho_{0}=\\rho_{a}\\times\\rho_{w}$ such that $\\rho_{a}$ issymmetricand issupported in $[-K_{\\rho},K_{\\rho}]$ for some constant $K_{\\rho}>0$ ", "page_idx": 3}, {"type": "text", "text": "In Assumption 3.1, the Condition (i) is satisfied by some commonly used activation functions, such $\\begin{array}{r}{\\sigma(x)=\\frac{1}{1+e^{-x}}}\\end{array}$ and $\\sigma(x)=\\cos(x)$ , and isrequired for establishing the existence and uniqueness of the solution to (2.4). The Condition (ii) and (i) are also standard and easy to satisfy in practice ", "page_idx": 3}, {"type": "text", "text": "Remark 3.2.The symmetry of $\\rho_{a}$ implies that $f_{N N}(x;\\rho_{0})=0$ Therefore, the initial loss $\\mathcal{E}(\\rho_{0})=$ $\\begin{array}{r}{\\frac{1}{2}\\mathbb{E}_{x}[|f^{*}(x)|^{2}]=\\frac{1}{2}\\mathbb{\\dot{E}}_{x_{V}}[|h^{*}(x_{V}^{'})|^{2}]=\\frac{1}{2}\\mathbb{E}_{z}[|h^{*}({\\bar{z}})|^{2}]}\\end{array}$ where $x_{V}=z\\stackrel{.}{\\sim}\\mathcal{N}(0,I_{V})$ , can be viewed as a constant dependingonly on $h^{*}$ and $p_{\\mathrm{:}}$ independent of d. Noticing also the decay property (2.6), the loss at any time $t$ canbebounded as $\\begin{array}{r}{\\dot{\\boldsymbol{\\mathcal{E}}}(\\rho_{t})\\stackrel{}{\\leq}\\boldsymbol{\\mathcal{E}}(\\rho_{0})\\stackrel{}{=}\\frac{1}{2}\\mathbb{E}_{z}[|h^{*}(\\bar{z})|^{2}]}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "The main goal of this section is to generalize the merged-staircase property in a basis-free setting. for general polynomials. Without a standard basis, it is hard to talk about having a \u201cstaircase\" of monomials. Even with a fixed basis, it is still nontrial to define the merged-staircase property for general polynomials since the analysis in [1] highly depends on $z_{i}^{2}=\\bar{1}$ that is only true for polynomials on the hypercube. Instead, we use the observation that when a function does not satisfy the merged staircase property,it implies that two of the variables will behave the same in the training dynamics. Such a symmetry can be generalized to the basis-free setting for general polynomials and we summarize this as the following reflective property: ", "page_idx": 3}, {"type": "text", "text": "Definition 3.3 (Reflective property). Let $S\\subset V\\subset\\mathbb{R}^{d}$ be a subspace of $V$ .We say that the underlying polynomial $h^{*}:V\\to\\mathbb{R}$ satisfies the reflective property with respect to the subspace $S$ and the activation function $\\sigma$ if ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{z\\sim\\mathcal{N}(0,I_{V})}\\left[h^{*}(z)\\sigma^{\\prime}\\left(u+v^{\\top}z\\frac{\\perp}{S}\\right)z_{S}\\right]=0,\\quad\\forall\\,u\\in\\mathbb{R},\\,\\,v\\in V,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $z_{S}=\\mathcal{P}_{S}^{V}(z)$ and $z_{S}^{\\perp}=z-\\mathcal{P}_{S}^{V}(z)$ with $\\mathcal{P}_{S}^{V}:V\\to S$ being the orthogonal projection from $V$ onto $S$ ", "page_idx": 4}, {"type": "text", "text": "The reflective property defined above is closely related to the merged-staircase property in [1]. Let us illustrate the intuition using a simple example. Consider $V=\\mathbb{R}^{3}$ and $h^{\\ast}(z)=z_{1}+z_{1}z_{2}z_{3}$ Then $h^{*}$ does not satisfy the merged-staircase property since $z_{1}z_{2}z_{3}$ involves two new coordinates that do not appear in the first monomial $z_{1}$ . In our setting, this $h^{*}$ satisfies the reflective with respect to $S=\\operatorname{span}\\{e_{2},e_{3}\\}$ , where $e_{i}$ is the vector in $\\mathbb{R}^{3}$ with the $i$ -th entry being 1 and other entries being 0. More specifically, for $z=(z_{1},z_{2},z_{3})$ , one has that $z_{S}=(0,z_{2},z_{3})$ and $z_{S}^{\\perp}=(z_{1},0,0)$ . Thus, one has for any $u\\in\\mathbb R$ and $v\\in V$ that $\\sigma^{\\prime}\\left(u+v^{\\top}z_{S}^{\\perp}\\right)$ is independent of $z_{2},z_{3}$ and that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb E_{z_{2},z_{3}}\\left[h^{*}(z)\\sigma^{\\prime}\\left(u+v^{\\top}z_{S}^{\\perp}\\right)z_{S}\\right]=\\sigma^{\\prime}\\left(u+v^{\\top}z_{S}^{\\perp}\\right)\\mathbb E_{z_{2},z_{3}}\\left[\\left(0,z_{1}z_{2}+z_{1}z_{2}^{2}z_{3},z_{1}z_{3}+z_{1}z_{2}z_{3}^{2}\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=(0,0,0),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which leads to (3.1). One can see from this example that satisfying the refective property with respect to a nontrivial subspace $S\\subset V$ is in the same spirit as not satisfying the merged-staircase property. Furthermore, the reflective property is rotation-invariant, meaning that using a different orthonormal basis does not change the property. In this sense, our proposed condition is more general than that in [1]. We also remark that there have been other rotation-invariant conditions generalizing [1], see e.g., [2, 8, 10, 11]. ", "page_idx": 4}, {"type": "text", "text": "Another comment is that the reflective property (3.1) depends on the activation function $\\sigma$ ,While conditions in previous works [1, 2, 8, 10, 11] are all defined for the target function $f^{*}$ or $h^{*}$ itself. There does exist a variant of our reflective property that is independent of $\\sigma^{\\prime}$ ,namely, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{z_{S}\\sim\\mathcal{N}(0,I_{S})}[h^{\\ast}(z)z_{S}]=0,\\quad\\forall\\,z_{S}^{\\perp},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which actually implies (3.1). But these two conditions are different: $h^{*}(z)=z_{1}+z_{1}z_{2}+z_{1}z_{2}z_{3}$ does not satisfy (3.2) but still satisfies (3.1) if $\\sigma(\\zeta)=\\zeta$ . We use (3.1) with $\\sigma^{\\prime}$ because we want to emphasize that the SGD learnability depends on the activation function $\\sigma$ .If $\\sigma$ is less expressive, then SGD may not learn the target function even if $h^{*}$ itself satisfies the merged-staircase property. Typically people use activation functions that are expressive enough, for which (3.1) and (3.2) are similar. In addition, it can be verified that (3.2) with some nontrivial $S$ is equivalent to isoLeap $\\left(h^{*}\\right)\\geq2$ that means $h^{*}:V\\to\\mathbb{R}$ does not satisfy the merged-staircase property for some orthonormal basis of $V$ [2], and the idea of leaps is used in [8, 10]. We include the proof of equivalence in Appendix B.1. ", "page_idx": 4}, {"type": "text", "text": "Our main result in this section is that the reflective property with nontrivial $S$ would lead to a positive lower bound of $\\mathcal{E}(\\rho_{t})$ along the training dynamics, which provides a necessary condition for the SGD-learnability and is formally stated as follows. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.4. Suppose that Assumption 3.1 holds with $\\rho_{w}\\,\\sim\\textstyle{\\mathcal{N}}(0,{\\frac{1}{d}}I_{d})$ ,and that $h^{*}:V\\to\\mathbb{R}$ satisfies the refective property with respect to some subspace $S\\subset V$ and activation function $\\sigma$ .Then for any $T>0$ there exists a constant $C>0$ depending only on p, $h^{*}$ \uff0c $K_{\\sigma}$ $K_{\\xi}$ \uff0c $K_{\\rho}$ and $T$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{0\\leq t\\leq T}\\mathcal{E}(\\rho_{t})\\geq\\frac{1}{2}\\mathbb{E}_{z\\sim\\mathcal{N}(0,I_{V})}\\left[|h^{*}(z)-h_{S^{\\perp}}^{*}(z_{S}^{\\perp})|^{2}\\right]-\\frac{C}{d^{1/2}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $h_{S^{\\perp}}^{*}(z_{S}^{\\perp})=\\mathbb{E}_{z_{S}}[h^{*}(z)]$ . In particular, if $h^{*}(z)$ is not independent of $z_{S}$ then for any $T>0$ there exists $d(T)>0$ depending only on p, $h^{*}$ \uff0c $K_{\\sigma}$ $K_{\\xi}$ \uff0c $K_{\\rho}$ ,and $T$ such that for any $d>d(T)$ we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{0\\leq t\\leq T}\\mathcal{E}(\\rho_{t})\\geq\\frac{1}{4}\\mathbb{E}_{z\\sim\\mathcal{N}(0,I_{V})}\\left[|h^{*}(z)-h_{S^{\\perp}}^{*}(z_{S}^{\\perp})|^{2}\\right]>0.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It is worth remarking that in Theorem 3.4, the training time $T$ is a constant independent of the dimension $d$ .If a longer $d\\!.$ -dependent training beyond a constant time is allowed, then $\\bar{\\mathcal{E}}(\\rho_{t})$ might be reasonably small even if the necessary condition is not satisfied, see e.g. [2, 18, 26]. ", "page_idx": 4}, {"type": "text", "text": "In Appendix B.2, we include a brief discussion of the sample complexity result of SGD implied by Theorem 3.4. In particular, SGD with $O(d)$ samples cannot recover $f^{*}$ reliably if the refelctive property holds, which is consistent with observations in previous works such as [1, 2]. We also remark that our result in Theorem 3.4 is established for the mean-field dynamics corresponding to the one-pass SGD (2.3), any may not apply for other variants of SGD. In particular, some recent works [6, 11, 16] prove that multi-pass SGD with batch-reuse mechanism can learn some target functions with fewer samples than one-pass SGD. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.2 Proof Sketch for Theorem 3.4 ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To prove Theorem 3.4, the main intuition is that under some mild assumptions, if (3.1) is satisfied and the initial distribution $\\rho_{0}$ is supported in $\\{(a,w)\\in\\mathbb{R}^{d+1}:w_{S}=0\\}$ , where $w_{S}$ is the orthogonal projection of $w\\in\\mathbb{R}^{d}$ onto $S$ then $\\rho_{t}$ is supported in $\\{(a,w)\\in\\mathbb{R}^{d+1}:w_{S}=0\\}$ for all $t\\geq0$ This means that the trained neural network $f_{\\mathrm{NN}}(x;\\rho_{t})$ learns no information about $x_{S}$ , the orthogonal projection of $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ onto $S$ , and hence cannot approximate $f^{*}(x)=h^{*}(x_{V})$ with arbitrarily smaller error if $h^{*}(z)$ is dependent on $z_{S}$ . We formulate this observation in the following theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.5. Suppose that Assumption 3.1 hold and let $\\rho_{t}$ be the solution to (2.4). Let $S\\subset\\ensuremath{\\mathbb{R}}^{d}$ be $a$ subspace with the projection map $\\bar{\\mathcal{P}}_{S}:\\mathbb{R}^{d+1}\\rightarrow S$ that maps $(a,w)$ 10 $w_{S}$ $I f(\\mathcal{P}_{S})_{\\#}\\rho_{0}=\\delta_{S}$ ,where $\\delta_{S}$ isthedeltameasureon $S$ and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x}\\left[f^{*}(x)\\sigma^{\\prime}\\left(w^{\\top}x_{S}^{\\perp}\\right)x_{S}\\right]=0,\\quad\\forall\\,w\\in\\mathbb{R}^{d},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $x_{S}^{\\perp}=x-x_{S}$ , then it holds for any $t\\geq0$ that ", "page_idx": 5}, {"type": "equation", "text": "$$\n(\\mathcal{P}_{S})_{\\#}\\rho_{t}=\\delta_{S}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, the delta measure $\\delta_{S}$ on $S$ is a probability measure on $S$ such that for any continuous and compactly supported function $\\varphi:S\\rightarrow\\mathbb{R}$ , it holds that $\\begin{array}{r}{\\int_{S}\\varphi(x)\\delta_{S}(d x)=\\varphi(0)}\\end{array}$ . In Theorem 3.5, the condition (3.5) is stated in terms of $f^{*}$ . We will show later that it is closely related to and is actually implied by (3.1), via a decomposition $w^{\\top}x_{S}^{\\perp}=w^{\\top}(x-x_{V})+w^{\\top}(x_{V}\\overleftarrow{-x_{S}})$ , with $w^{\\top}(x-x_{V})$ and $w^{\\top}(x_{V}-x_{S})$ corresponding to $u$ and $v^{\\top}z_{S}^{\\top}$ in (3.1), respectively. The main idea in the proof of Theorem 3.5 is to construct a fow $\\hat{\\rho}_{t}$ in the space $\\mathcal{P}(\\mathbb{R}\\times S^{\\perp})$ , where $S^{\\perp}$ is the orthogonal complement of $S$ in $\\mathbb{R}^{d}$ , and then show that $\\rho_{t}=\\hat{\\rho}_{t}\\times\\delta_{S}$ is the solution to (2.4). More specifically, the flow $\\hat{\\rho}_{t}$ is constructed as the solution to the following evolution equation in $\\mathcal{P}(\\mathbb{R}\\times\\bar{S^{\\perp}})$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\partial_{t}\\hat{\\rho}_{t}=\\nabla_{\\hat{\\theta}}\\cdot\\Big(\\hat{\\rho}_{t}\\hat{\\xi}(t)\\nabla_{\\hat{\\theta}}\\hat{\\Phi}(\\hat{\\theta};\\hat{\\rho}_{t})\\Big)\\right.,}\\\\ {\\left.\\hat{\\rho}_{t}\\right|_{t=0}=\\hat{\\rho}_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{\\rho}_{0}\\in\\mathcal{P}(\\mathbb{R}\\times S^{\\perp})$ satisfies $\\rho_{0}=\\hat{\\rho}_{0}\\times\\delta_{S}$ \uff0c $\\hat{\\theta}=(a,w_{S}^{\\perp}),\\hat{\\xi}(t)=\\mathrm{diag}(\\xi_{a}(t),\\xi_{w}(t)I_{S^{\\perp}})$ , and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\Phi}(\\hat{\\theta};\\hat{\\rho})=a\\mathbb{E}_{x}\\left[\\left(\\hat{f}_{\\mathrm{NN}}(x_{S}^{\\perp};\\hat{\\rho})-f^{*}(x)\\right)\\sigma\\left((w_{S}^{\\perp})^{\\top}x_{S}^{\\perp}\\right)\\right]=\\hat{V}(\\hat{\\theta})+\\int\\hat{U}(\\hat{\\theta},\\hat{\\theta}^{\\prime})\\hat{\\rho}(d\\hat{\\theta}^{\\prime})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{f}_{\\mathrm{NN}}(x_{S}^{\\perp};\\hat{\\rho})=\\int a\\sigma\\left((w_{S}^{\\perp})^{\\top}x_{S}^{\\perp}\\right)\\hat{\\rho}(d a,d w_{S}^{\\perp}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and ", "page_idx": 5}, {"type": "text", "text": "$\\hat{V}(\\hat{\\theta})=-a\\mathbb{E}_{x}\\left[f^{*}(x)\\sigma\\left((w_{S}^{\\perp})^{\\top}x_{S}^{\\perp}\\right)\\right],\\quad\\hat{U}(\\theta,\\theta^{\\prime})=a a^{\\prime}\\mathbb{E}_{x}\\left[\\sigma\\left((w_{S}^{\\perp})^{\\top}x_{S}^{\\perp}\\right)\\sigma\\left(((w_{S}^{\\perp})^{\\prime})^{\\top}x_{S}^{\\perp}\\right)\\right].$ The detailed proof will be presented in Appendix A.1. ", "page_idx": 5}, {"type": "text", "text": "In practice, both $V$ and $S$ are unknown and it is nontrivial to choose an initialization $\\rho_{0}$ supported in $\\left\\{(a,w)\\;\\in\\;\\mathbb{R}^{d+1}\\;:\\;w_{S}\\;=\\;0\\right\\}$ . However, one can set $\\rho_{0}\\;=\\;\\rho_{a}\\;\\times\\;\\rho_{w}$ with $\\rho_{w}\\,\\sim\\,{\\mathcal{N}}({\\bar{0}},{\\frac{1}{d}}I_{d})$ and this can make the marginal distribution of $\\rho_{0}$ on $S$ very close to the the delta measure $\\delta_{S}$ if $d>>p=\\dim(V)\\geq\\dim({\\bar{S}})$ , which fits the setting of subspace-sparse polynomials. Rigorously, we have the following theorem stating dimension-free stability with respect to initial distribution, with the proof deferred to Appendix A.2. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.6. Suppose that Assumption 3.1 holds for both $\\rho_{0}$ and $\\tilde{\\rho}_{0}$ .Let $\\rho_{t}$ solve $\\partial_{t}\\rho_{t}=\\nabla_{\\theta}$ $(\\rho_{t}\\xi(t)\\nabla_{\\theta}\\Phi(\\theta;\\rho_{t})\\bar{)}$ and let ${\\tilde{\\rho}}_{t}$ solve $\\begin{array}{r}{\\partial_{t}\\tilde{\\rho}_{t}=\\nabla_{\\theta}\\cdot(\\tilde{\\rho_{t}}\\xi(t)\\nabla_{\\theta}\\dot{\\Phi}(\\theta;\\tilde{\\rho}_{t}))}\\end{array}$ .Then for any $T\\in(0,+\\infty)$ \uff0c there exists a constant $C_{s}>0$ depending only on p, $h^{*},\\,K_{\\sigma},\\,k$ $K_{\\xi}$ \uff0c $K_{\\rho}$ and $T$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq T}\\mathbb{E}_{x}\\left[|f_{N N}(x;\\rho_{t})-f_{N N}(x;\\tilde{\\rho}_{t})|^{2}\\right]\\leq C_{s}W_{2}^{2}(\\rho_{0},\\tilde{\\rho}_{0}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $W_{2}(\\cdot,\\cdot)$ is the 2-Wasserstein metric. ", "page_idx": 5}, {"type": "text", "text": "Based on Theorem 3.5 and Theorem 3.6, Theorem 3.4 can be proved by some straightforward computation, for which the details can be found in Appendix A.3. ", "page_idx": 5}, {"type": "text", "text": "1: Set the initial distribution as $\\rho_{0}=\\rho_{a}\\times\\delta_{\\mathbb{R}^{d}}$ , where $\\rho_{a}=\\mathcal{U}([-1,1])$ and $\\delta_{\\mathbb{R}^{d}}$ is the delta measure on $\\mathbb{R}^{d}$   \n2: Set $\\xi_{a}(t)=0$ and $\\xi_{w}(t)=1$ for $0\\leq t\\leq T$ , and train the neural network with activation function $\\sigma$ . Denote by $(a,w(a,t))$ \uff0c $0\\leq t\\leq T$ the trajectory of a single particle that starts at $(a,0)$   \n3: Repeat Step 2 for $p$ times independently and obtain $p$ copies of parameters at $T$ say $(a_{i},w(a_{i},T))$ with $a_{i}\\sim\\mathcal{U}([-1,1])$ \uff0c $i=1,2,\\dots,p$   \n4: Reset $\\rho_{T}$ as the dsribution of $\\begin{array}{r}{(0,u(a_{1},\\ldots,a_{p},T))=\\Big(0,\\frac{1}{p}\\sum_{i=1}^{p}w(a_{i},T)\\Big)}\\end{array}$ Train the neural network with $\\xi_{a}(t)\\,=\\,1$ \uff0c $\\xi_{w}(t)\\,=\\,0$ , and a new activation function ${\\hat{\\sigma}}(\\zeta)\\,=\\,(1+\\zeta)^{n}$ ,where $n=\\deg(f^{*})$ ,for $t\\geq T$ starting at $\\rho_{T}$ ", "page_idx": 6}, {"type": "text", "text": "4  Sufficient Condition for SGD-Learnability ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we propose a sufficient condition and a training strategy that can guarantee the exponentialdecayof $\\mathcal{E}(\\rho_{t})$ with constants independent of the dimension $d$ ", "page_idx": 6}, {"type": "text", "text": "4.1  Training Procedure and Convergence Guarantee ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We prove in Section 3 that if the trained parameters always stay in a proper subspace $\\{(a,w)\\in$ $\\mathbb{R}^{d+1}:w_{S}=0\\}$ , then $f_{\\mathrm{NN}}(x;\\rho_{t})$ cannot learn all information about $f^{*}$ or $h^{*}$ Ideally, one would expect the negation to be a sufficient condition for the SGD-learnability, i.e., the existence of a choice of learning rates and initial distribution that guarantees $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\mathcal{E}(\\rho_{t})}\\end{array}$ with dimension-free rate. This is almost true but we need a slightly stronger condition due to technical issues. More specifically, we need that the Taylor's expansion of some dynamics (not the dynamics itself) is not trapped in any proper subspace. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.1. Consider the following fow $\\hat{w}_{V}(t)$ in $V$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\frac{d}{d t}\\hat{w}_{V}(t)=\\mathbb{E}_{z}\\left[z h^{*}(z)\\sigma^{\\prime}(\\hat{w}_{V}(t)^{\\top}z)\\right],\\right.}\\\\ {\\left.\\hat{w}_{V}(0)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We assume that for some $s\\in\\mathbb{N}_{+}$ , the Taylor's expansion up to $s$ -thorderof $\\hat{w}_{V}(t)$ at $t=0$ is not contained in any proper subspace of $V$ ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.1 aims to state the same observation as the merged-staircase property in [1]. As a simple example, if $V=\\mathbb{R}^{p}$ and $h^{*}(z)=z_{1}+z_{1}z_{2}+z_{1}z_{2}z_{3}+\\cdot\\cdot\\cdot+z_{1}z_{2}\\cdot\\cdot\\cdot z_{p}$ which satisfies the merged-staircase property, then it can be computed that the leading order terms of the coordinates of $\\hat{w}_{V}(t)$ are given by $(\\bar{c}_{1}t,\\bar{c}_{2}t^{2},c_{3}{t^{2}}^{2},\\ldots,c_{p}t^{2^{\\bar{p}-1}})$ with nonzero constants $c_{1},c_{2},\\ldots,c_{p}$ $\\sigma\\in\\mathcal{C}^{s}(\\mathbb{R})$ with $s=2^{p-1}$ and $\\sigma^{(1)}(0),\\sigma^{(2)}(0),\\ldots,\\sigma^{(p)}(0)$ are all nonzero see Proposition 3 in [11). This is to say that Assumption 4.1 with $s\\,=\\,2^{p-1}$ is satisfied for this example. We provide further characterization of Assumption 4.1 by verifying it in a more general setting in Appendix D.1. ", "page_idx": 6}, {"type": "text", "text": "We also remark that the Taylor's expansion of the flow $\\hat{w}_{V}(t)$ that solves (4.1) depends only on the $h^{*}$ and $\\sigma^{(1)}(0),\\sigma^{(2)}(0),\\ldots,\\sigma^{(s)}(0)$ .We require some additional regularity asumption on higher-order derivatives of $\\sigma$ ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.2. Assume that $\\sigma$ satisfies $\\sigma\\,\\in\\,\\mathcal{C}^{L+1}(\\mathbb{R})$ and $\\sigma,\\sigma^{\\prime},\\sigma^{\\prime\\prime},\\sigma^{(L+1)}\\,\\in\\,L^{\\infty}(\\mathbb{R}),$ where ${\\cal L}=2s n\\binom{n+p}{p}$ with $n=d e g(f^{*})=d e g(h^{*})$ and s being the positive integer in Assumption4.1. ", "page_idx": 6}, {"type": "text", "text": "Our proposed training strategy is stated in Algorithm 1. The training strategy is inspired by the two-stage strategy proposed in [1] that trains the parameters $w$ with fixed $a$ for $t\\ \\bar{\\in}\\ [0,T]$ and then trains the parameter $a$ with fixed $w$ and a perturbed activation function for $t\\geq T$ . Several important modifications are made since we consider general polynomials, rather than polynomials on hypercubes as in [1]. In particular, ", "page_idx": 6}, {"type": "text", "text": "\u00b7 We need to repeat Step 2 (training $w$ )for $p$ times and use their average as the initialization of training $a$ , while this step only needs to be done once in [1]. The reason is that the space of polynomials on the hypercube $\\{\\pm1\\}^{p}$ is essentially a linear space with dimension $2^{p}$ . However, the space of general polynomials on $V$ is an $\\mathbb{R}$ -algebra that is also a linear space but is of infinite dimension. Therefore, to make the kernel matrix in training $a$ non-degenerate, we require some algebraic independence which can be guaranteed by $\\begin{array}{r}{u(a_{1},\\breve{\\dots},a_{p},t))\\,=\\,\\frac{1}{p}\\sum_{i=1}^{p}w(a_{i},t)}\\end{array}$ $0\\,<\\,t\\,\\leq\\,T$ , though linear independence suffices for [1]. Let us also emphasize that each run of Step 2 involves training an interacting particle system instead of training a single particle. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "\u00b7 In Step 4, we use a new activation function ${\\hat{\\sigma}}(\\zeta)\\,=\\,(1+\\zeta)^{n}$ that is a polynomial of the same degree as $f^{*}$ and $h^{*}$ . The reason is still that we work with the space general polynomials whose dimension as a linear space is infinite. Thus, we need the specific form ${\\hat{\\sigma}}(\\zeta)=(1+\\zeta)^{n}$ to guarantee the trained neural network $f_{\\mathrm{NN}}(x;\\rho_{t})$ is a polynomial with degree at most $n=\\deg(f^{*})=\\deg(h^{*})$ . As a comparison, in the setting of [1], all functions on $\\{\\pm1\\}^{p}$ can be understood as a polynomial, and no specific format of the new activation function is needed. ", "page_idx": 7}, {"type": "text", "text": "Our main theorem in this section is as follows, stating that the loss functional $\\mathcal{E}(\\rho_{t})$ can decay to O exponentially fast, with rates independent of the dimension $d$ ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.3. Suppose that Assumption 4.1 and 4.2 hold and let $\\rho_{t}$ be the flow generated by Algorithm 1. There exist constants $C_{1},C_{2}>0$ dependingon $h^{*},\\sigma,n,p,s,$ suchthat ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\rho_{t})\\leq C_{1}\\exp(-C_{2}t),\\quad\\forall\\,t\\geq0.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Let us also remark that it is possible to use the original dynamics $\\hat{w}_{V}$ defined in (4.1) when we state Assumption 4.1, which can actually imply its Taylor's expansion up to some order is not trapped in any proper subspace of $V$ if we further assume $\\hat{w}_{V}(t)$ is analytic. We choose to directly use Taylor's expansion in Assumption 4.1 since we want to avoid the additional analytic assumption and to emphasize that the constants $C_{1},C_{2}$ in Theorem 4.3 depend on the order $s$ of the Tayler's expansion satisfying Assumption 4.1. ", "page_idx": 7}, {"type": "text", "text": "Discussion about the sample complexity implied by Theorem 4.3 is included in Appendix D.2, suggestingthat $O(d)$ samples suffices for SGD to learn $f^{*}$ reliably if conditions in Theorem 4.3 are true. This is also consistent with previous works such as [1, 2]. ", "page_idx": 7}, {"type": "text", "text": "4.2Proof Sketch for Theorem 4.3 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To prove Theorem 4.3 we follow the same general strategy as [1], though some technical analysis is significantly different due to the roatation-invariant setting. The main goal here is to show before Step 4, the algorithm already learned a diverse set of features. After that, note that Step 4 in Algorithm 1 is essentially a convex/quadratic optimization problem (since we only train $a$ and set $\\xi_{w}(t)=0)$ . In addition, thanks to the new activation function ${\\hat{\\sigma}}(\\zeta)=(1+\\zeta)^{n}$ , one only needs to consider $\\mathbb{P}_{V,n}$ that is the space of of all polynomials on $V$ with degree at most $\\ddot{n^{\\prime}}=\\deg(h^{*}\\dot{)}=\\deg(f^{*})$ . The dimension $\\mathbb{P}_{V,n}$ as linear space is $\\scriptstyle{\\binom{n+p}{p}}$ Let $p_{1},p_{2},...,p_{\\left({n+p}\\right)}$ be the orthonormal basis of $\\mathbb{P}_{V,n}$ with input $z\\sim\\mathcal{N}(0,I_{V})$ and define the kernel matrix ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\boldsymbol{K}_{i_{1},i_{2}}(t)=\\mathbb{E}_{a_{1},\\ldots,a_{p}}\\left[\\mathbb{E}_{z,z^{\\prime}}\\left[p_{i_{1}}(z)\\hat{\\sigma}(u(a_{1},\\ldots,a_{p},t)^{\\top}z)\\hat{\\sigma}(u(a_{1},\\ldots,a_{p},t)^{\\top}z^{\\prime})p_{i_{2}}(z^{\\prime})\\right]\\right],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where ${\\hat{\\sigma}}(\\xi)\\,=\\,(1+\\xi)^{n}$ $\\begin{array}{r}{(a_{1},\\dotsc...,a_{p})\\sim\\mathcal{U}([-1,1]^{p}),\\,1\\,\\leq\\,i_{1},i_{2}\\,\\leq\\,\\binom{n+p}{p}}\\end{array}$ ,and $0\\,\\leq\\,t\\,\\leq\\,T$ .As long as this kernel matrix is non-degenerate, we know that the loss functional is strongly convex with respect to the parameters in the second layer when fixing the first layer, and thus, it can be computed straightforwardly that the loss decays to O exponentially fast for $t\\geq T$ , leading to the desired convergence rate in Theorem 4.3; see Appendix C.3 for details. Thus, the main part in the proof of Theorem 4.3 is to establish the non-degeneracy of the kernel matrix. ", "page_idx": 7}, {"type": "text", "text": "Proposition 4.4. Suppose that Assumption 4.1 and 4.2 hold. There exist constants $C,T\\;>\\;0$ dependingon $h^{*},\\sigma,n,p,s,$ suchthat ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(K(t))\\geq C t^{2s n\\binom{n+p}{p}},\\quad\\forall\\,0\\leq t\\leq T,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\lambda_{\\operatorname*{min}}(\\kappa(t))$ is the smallest eigenvalue of $\\kappa(t)$ ", "page_idx": 7}, {"type": "text", "text": "In the rest of this subsection, we sketch the main ideas in the proof of Proposition 4.4, with the details of the proof being deferred to Appendix $\\mathbf{C}$ Wefirst showthat $w(a_{i},t)$ and $u(a_{1},\\ldots,a_{p},t)$ canbe approximated well by $\\hat{w}(a_{i},t)$ and $\\begin{array}{r}{\\hat{u}(a_{1},\\dots,a_{p},t)=\\frac{1}{p}\\sum_{i=1}^{p}\\hat{w}(a_{i},t)}\\end{array}$ that are polynomials in $a_{i}$ and $a_{1},\\dotsc,a_{p}$ respectively. This approximation step follows [1] closely and is analyzed detailedly in Appendix C.1. Therefore, to give a positive lower bound of $\\lambda_{\\mathrm{min}}(\\boldsymbol{\\kappa}(t))$ , one only needs to show the non-degeneracy of the matrix $\\hat{M}(\\mathbf{a},t)\\in\\mathbb{R}^{\\binom{n+p}{p}\\times\\binom{n+p}{p}}$ with ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{M}_{i_{1},i_{2}}(\\mathbf{a},t)=\\mathbb{E}_{z}\\left[p_{i_{1}}(z)\\hat{\\sigma}(\\hat{u}(\\mathbf{a}_{i_{2}},t)^{\\top}z)\\right],\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathbf{a}\\;=\\;\\left(\\mathbf{a}_{1},\\mathbf{a}_{2},\\ldots,\\mathbf{a}_{\\binom{n+p}{p}}\\right)$ and a; E RP for i = 1,2,...,(m+P) . Intuitively, this nondegeneracy can be implied by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname{span}\\left\\{\\hat{\\sigma}(\\hat{u}(a_{1},\\ldots,a_{p},t)^{\\top}z):a_{1},a_{2},\\ldots,a_{p}\\in[-1,1]\\right\\}=\\mathbb{P}_{V,n},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "which is true if $\\hat{\\boldsymbol u}_{i}(a_{1},\\ldots,a_{p},t)$ \uff0c $1~\\le~i~\\le~p$ are $\\mathbb{R}$ -algebraically independent polynomials in $a_{1},a_{2},\\dotsc,a_{p}$ , where $\\hat{u}_{i}$ is the $i$ -th coefficient of $\\hat{u}$ under somebasis of $V$ , and algebraic independence can be obtained from linear independence by taking the average of independent copies. We illustrate this intuition with a bit more detail. ", "page_idx": 8}, {"type": "text", "text": "Algebraic independence of $\\hat{u}_{i}$ . With Assumption 4.1, $\\hat{w}_{1}(a,t),\\hat{w}_{2}(a,t),\\dots,\\hat{w}_{1}(a,t)$ can be proved as $\\mathbb{R}$ -linear independent polynomials in $a\\in\\mathbb{R}$ . Then one can apply the following theorem to boost the linear independence of $\\hat{w}_{i}$ , whose constant term is zero since initialization in training is set as $\\rho_{0}=\\rho_{a}\\times\\delta_{\\mathbb{R}^{d}}$ , to the algebraic independence of $\\hat{u}_{i}$ ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.5. Let $v_{1},v_{2},\\ldots,v_{p}\\in\\mathbb{R}[a]$ be $\\mathbb{R}$ -linearly independent polynomialswith the constant terms being zero. Then $\\begin{array}{r}{\\frac{1}{p}(v_{1}(a_{1})+\\cdot\\cdot\\cdot+v_{1}(a_{p})),\\ldots,\\frac{1}{p}(v_{p}(a_{1})+\\cdot\\cdot\\cdot+v_{p}(a_{p}))\\in\\mathbb{R}[a_{1},a_{2},\\ldots,a_{p}]}\\end{array}$ are $\\mathbb{R}$ -algebraically independent. ", "page_idx": 8}, {"type": "text", "text": "The proof of Theorem 4.5 is deferred to Appendix C.2 and is based on the celebrated Jacobian criterion stated as follows. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.6 (Jacobian criterion [7]). $v_{1},v_{2},\\ldots,v_{p}\\in\\mathbb{R}[a_{1},a_{2},\\ldots,a_{p}]$ are $\\mathbb{R}$ -algebraically independent if and only $i f$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{det}{\\left(\\begin{array}{c c c c}{\\frac{\\partial v_{1}}{\\partial a_{1}}}&{\\frac{\\partial v_{1}}{\\partial a_{2}}}&{\\ldots+}&{\\frac{\\partial v_{1}}{\\partial a_{p}}}\\\\ {\\frac{\\partial v_{2}}{\\partial a_{1}}}&{\\frac{\\partial v_{2}}{\\partial a_{2}}}&{\\ldots+}&{\\frac{\\partial v_{2}}{\\partial a_{p}}}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\frac{\\partial v_{p}}{\\partial a_{1}}}&{\\frac{\\partial v_{p}}{\\partial a_{2}}}&{\\ldots}&{\\frac{\\partial v_{p}}{\\partial a_{p}}}\\end{array}\\right)}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "is a nonzero polynomial in $\\mathbb{R}[a_{1},a_{2},\\ldots,a_{p}]$ ", "page_idx": 8}, {"type": "text", "text": "Non-degeneracy of $\\hat{M}({\\bf a},t)$ .With the observation that span $\\left\\{\\hat{\\sigma}(q^{\\top}z):q\\in V\\right\\}\\ =\\ \\mathbb{P}_{V,n}$ (see Lemma C.13), we defne another matrix $X(\\mathbf{q})\\in\\mathbb{R}^{{\\binom{n+p}{p}}\\times{\\binom{n+p}{p}}}$ via ", "page_idx": 8}, {"type": "equation", "text": "$$\nX_{i_{1},i_{2}}(\\mathbf{q})=\\mathbb{E}_{z}\\left[p_{i_{1}}(z)\\sigma(\\mathbf{q}_{i_{2}}^{\\top}z)\\right],\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathbf{q}=\\left(\\mathbf{q}_{1},\\mathbf{q}_{2},\\ldots,\\mathbf{q}_{\\binom{n+p}{p}}\\right)$ with $\\mathbf q_{i}\\in V$ , and prove that $\\operatorname*{det}(X(\\mathbf{q}))$ is a non-zero polynomial in $\\mathbf{q}$ of the form ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{det}(X(\\mathbf{q}))=\\sum_{i=1}^{\\binom{n+p}{p}}\\sum_{0\\leq\\|\\mathbf{j}_{i}\\|_{1}\\leq n}X_{\\mathbf{j}}\\mathbf{q}^{\\mathbf{j}}=\\sum_{i=1}^{\\binom{n+p}{p}}\\sum_{0\\leq\\|\\mathbf{j}_{i}\\|_{1}\\leq n}X_{\\mathbf{j}}\\prod_{l=1}^{\\binom{n+p}{p}}\\mathbf{q}_{l}^{\\mathbf{j}_{l}},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathbf{q}_{i}$ is understood as a (coefficient) vector in $\\mathbb{R}^{p}$ associated with a fixed orthonormal basis of $V$ and $\\mathbf{j}=\\left(\\mathbf{j}_{1},\\mathbf{j}_{2},\\ldots,\\mathbf{j}_{\\left({n+p\\atop p}\\right)}\\right)$ with $\\mathbf{j}_{i}\\in\\mathbb{N}^{p}$ . Then stting $\\mathbf{q}=\\hat{u}(\\mathbf{a}_{i_{2}},t)$ leads to ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{det}(\\hat{M}(\\mathbf{a},t))=\\sum_{i=1}^{\\binom{n+p}{p}}\\sum_{0\\leq\\|\\mathbf{j}_{i}\\|_{1}\\leq n}X_{\\mathbf{j}}\\prod_{l=1}^{\\binom{n+p}{p}}\\hat{u}(\\mathbf{a}_{l},t)^{\\mathbf{j}_{l}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "To prove that $\\operatorname*{det}(\\hat{M}({\\mathbf{a}},t))$ is a non-zero polynomial in a, i.e., $\\hat{M}({\\bf a},t)$ is non-degenerate, we use the following lemma linking algebraic independence back to linear independence. ", "page_idx": 8}, {"type": "text", "text": "Lemma 4.7. Suppose that $v_{1},v_{2},\\ldots,v_{p}\\in\\mathbb{R}[a_{1},a_{2},\\ldots,a_{p}]$ are $\\mathbb{R}$ -algebraically independent.For any $m\\geq1$ the following polynomials in $\\mathbf{a}=(\\mathbf{a}_{1},\\mathbf{a}_{2},\\dots,\\bar{\\mathbf{a}_{m}})\\in(\\mathbb{R}^{p})^{m}$ are $\\mathbb{R}$ -linearly independent ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\prod_{l=1}^{m}\\mathbf{v}(\\mathbf{a}_{l})^{\\mathbf{j}_{l}},\\quad1\\leq\\|\\mathbf{j}_{i}\\|_{1}\\leq n,\\,1\\leq i\\leq m,\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $\\mathbf{v}=(v_{1},v_{2},\\ldots,v_{p})$ ", "page_idx": 9}, {"type": "text", "text": "The proof of Lemma 4.7 and some other related analysis are deferred to Appendix C.3. ", "page_idx": 9}, {"type": "text", "text": "5  Conclusion and Discussions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we generalize the merged-staircase property in [1] to a basis-free version and establish a necessary condition for learning a subspace-sparse polynomial on Gaussian input with arbitrarily small error. Moreover, we prove the exponential decay property of the loss functional under a sufficient condition that is slightly stronger than the necessary one. The bounds and rates are all dimension-free. Our work provides some understanding of the mean-field dynamics, though its general behavior is extremely difficult to characterize due to the non-convexity of the loss functional. ", "page_idx": 9}, {"type": "text", "text": "Let us also make some comments on limitations and future directions. Firstly, there is still a gap between the necessary condition and the sufficient condition, which is basically from the fact that the sufficient condition is built on the Taylor's expansion of the flow (4.1). One future research question is whether we can fill the gap by considering the original flow (4.1) rather than its Taylor's expansion. Secondly, Algorithm 1 repeats training $w$ for $p$ times and takes the average of parameters, which is different from the usual strategy for training neural networks. This step is used to guarantee the algebraic independence. We conjecture that this step can be removed since the general algebraic independence is too strong when we have some preknowledge on the degree of $f^{*}$ or $h^{*}$ ,which deserves future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work of R. Ge is supported by NSF Award DMS-2031849 and CCF-1845171 (CAREER). We thank Joan Bruna for helpful comments and discussion. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for SGD learning of sparse functions on two-layer neural networks. In Conference on Learning Theory, pages 4782-4887. PMLR, 2022.   \n[2] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics. In The Thirty Sixth Annual Conference on Learning Theory, pages 2552-2623. PMLR, 2023.   \n[3] Emmanuel Abbe, Enric Boix-Adsera, Matthew S Brennan, Guy Bresler, and Dheeraj Nagaraj. The staircase property: How hierarchical structure can guide deep learning. Advances in Neural Information Processing Systems, 34:26989-27002, 2021.   \n[4]  Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In International conference on machine learning, pages 242-252. PMLR, 2019.   \n[5]  Dyego Araujo, Roberto I Oliveira, and Daniel Yukimura. A mean-field limit for certain deep neural networks. arXiv preprint arXiv: 1906.00193, 2019.   \n[6] Luca Arnaboldi, Yatin Dandi, Florent Krzakala, Luca Pesce, and Ludovic Stephan. Repetita iuvant: Data repetition allows sgd to learn high-dimensional multi-index functions. arXiv preprint arXiv:2405.15459, 2024.   \n[7]  Malte Beecken, Johannes Mittmann, and Nitin Saxena. Algebraic independence and blackbox identity testing. Information and Computation, 222:2-19, 2013.   \n[8]  Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. On learning Gaussian multi-index models with gradient flow. arXiv preprint arXiv:2310.19793, 2023.   \n[9] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for overparameterized models using optimal transport. Advances in neural information processing systems, 31, 2018.   \n[10] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. How two-layer neural networks learn, one (giant) step at a time. In NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning, 2023.   \n[11]  Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborova, and Florent Krzakala. The benefts of reusing batches for gradient descent in two-layer etworks: Breaking the curse of information and leap exponents. In Forty-first International Conference on Machine Learning, 2024.   \n[12] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In International Conference on Learning Representations, 2018.   \n[13]  Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.   \n[14]  Kenji Kawaguchi. Deep learning without poor local minima. Advances in neural information processing systems, 29, 2016.   \n[15]  Kenji Kawaguchi and Yoshua Bengio. Depth with nonlinearity creates no bad local minima in resnets. Neural Networks, 118:167-174, 2019.   \n[16] Jason D Lee, Kazusato Oko, Taiji Suzuki, and Denny Wu. Neural network learns lowdimensional polynomials with sgd near the information-theoretic limit. arXiv preprint arXiv:2406.01581, 2024.   \n[17] Haihao Lu and Kenji Kawaguchi.  Depth creates no bad local minima.  arXiv preprint arXiv:1702.08580, 2017.   \n[18]  Arvind Mahankali, Haochen Zhang, Kefan Dong, Margalit Glasgow, and Tengyu Ma. Beyond ntk with vanilla gradient descent: A mean-field analysis of neural networks with polynomial width, samples, and time. Advances in Neural Information Processing Systems, 36, 2024.   \n[19] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit. In Conference on Learning Theory, pages 2388-2464. PMLR, 2019.   \n[20] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665- E7671, 2018.   \n[21]  Phan-Minh Nguyen. Mean field limit of the learning dynamics of multilayer neural networks. arXiv preprint arXiv:1902.02880, 2019.   \n[22]  Grant Rotskoff and Eric Vanden-Eijnden. Trainability and accuracy of artificial neural networks: An interacting particle system approach. Communications on Pure and Applied Mathematics, 75(9):1889-1935, 2022.   \n[23]  Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer ReLU neural networks. In International conference on machine learning, pages 4433-4441. PMLR, 2018.   \n[24] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A central limit theorem. Stochastic Processes and their Applications, 130(3):1820-1852, 2020.   \n[25]  Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A law of large numbers. SIAM Journal on Applied Mathematics, 80(2):725-752, 2020.   \n[26] Taiji Suzuki, Denny Wu, Kazusato Oko, and Atsushi Nitanda. Feature learning via mean-field langevin dynamics: classifying sparse parities and beyond. Advances in Neural Information Processing Systems, 36, 2024.   \n[27]  Alain-Sol Sznitman. Topics in propagation of chaos. Lecture notes in mathematics, pages 165-251,1991.   \n[28]  Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding neural networks. Advances in Neural Information Processing Systems, 32, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proofs for Section 3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This section collects the proofs of Theorem 3.5, Theorem 3.6, and Theorem 3.4. ", "page_idx": 12}, {"type": "text", "text": "A.1Proof of Theorem 3.5 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Existence and uniqueness of solutions to (2.4)  Before proving Theorem 3.5, let us remark on the existence and uniqueness of solution to the mean-field dynamics (2.4). According to Remark 7.1 in [20] and Theorem 1.1 in [27], the PDE (2.4) admits a unique solution if Assumption 3.1 (ii) holds and both $\\nabla V(\\theta)$ and $\\nabla_{\\theta}U(\\theta,\\theta^{\\prime})$ are bounded and Lipschitz continous. Here we recall that $V$ and $U$ are defined in (2.5). With Assumption 3.1 (i) and (i), it is not hard to verify the boundedness and Lipschitzcontinuity of $\\nabla V(\\theta)$ and $\\nabla_{\\theta}U(\\theta,\\theta^{\\prime})$ by noticing that any finite-order moment of $\\mathcal{N}(0,I_{d})$ is finite. ", "page_idx": 12}, {"type": "text", "text": "Proof of Theorem 3.5. Since $(\\mathcal{P}_{S})_{\\#}\\rho_{0}=\\delta_{S}$ , the initial distribution $\\rho_{0}$ can be decomposed as $\\rho_{0}=$ $\\hat{\\rho}_{0}\\times\\delta_{S}$ ,where $\\hat{\\rho}_{0}\\in\\mathcal{P}(\\mathbb{R}\\times S^{\\perp})$ . Consider the following evolution equation (3.7) in $\\mathcal{P}(\\mathbb{R}\\times S^{\\perp})$ By the discussion at the beginning of Section A.1, we know that $\\rho_{t}$ is the unique solution to (2.4). Similar arguments also leads to the existence and uniquess of the solution to (3.7). ", "page_idx": 12}, {"type": "text", "text": "We will show that the solution to (2.4) must be of the form ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\rho_{t}=\\hat{\\rho}_{t}\\times\\delta_{S},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\hat{\\rho}_{t}$ solves (3.7), and this decomposition can immediatel imply (3.6). By the uniquess of the solution, it suffices to verify that $\\rho_{t}=\\hat{\\rho}_{t}\\times\\delta_{S}$ is a solution to (2.4). It follows directly from (A.1) that ", "page_idx": 12}, {"type": "equation", "text": "$$\nf_{\\mathrm{NN}}(x;\\rho_{t})=f_{\\mathrm{NN}}(x;\\hat{\\rho}_{t}\\times\\delta_{S})=\\hat{f}_{\\mathrm{NN}}(x_{S}^{\\perp};\\hat{\\rho}_{t}),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and hence that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\partial_{a}\\Phi(\\theta;\\rho_{t})=\\partial_{a}\\hat{\\Phi}(\\hat{\\theta};\\hat{\\rho}_{t}),\\quad\\mathrm{if~}w_{S}=0.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We also have that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{w_{S}^{\\perp}}\\Phi(\\theta;\\rho_{t})=a\\mathbb{E}_{x}\\left[\\left(f_{\\mathrm{NN}}(x;\\rho_{t})-f^{*}(x)\\right)\\sigma^{\\prime}(w^{\\top}x)x_{S}^{\\perp}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=a\\mathbb{E}_{x}\\left[\\left(\\hat{f}_{\\mathrm{NN}}(x_{S}^{\\perp};\\hat{\\rho}_{t})-f^{*}(x)\\right)\\sigma^{\\prime}\\left((w_{S}^{\\perp})^{\\top}x_{S}^{\\perp}\\right)x_{S}^{\\perp}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\nabla_{w_{S}^{\\perp}}\\hat{\\Phi}(\\hat{\\theta};\\hat{\\rho}_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "if $w_{S}=0$ . In addition, it holds also for $w_{S}=0$ that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{w s}\\Phi(\\theta;\\rho_{t})=a\\mathbb{E}_{x}\\left[\\left(f_{\\mathrm{NN}}(x;\\rho_{t})-f^{*}(x)\\right)\\sigma^{\\prime}(w^{\\top}x)x_{S}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=a\\mathbb{E}_{x}\\left[\\hat{f}_{\\mathrm{NN}}(x_{S}^{\\perp};\\hat{\\rho}_{t})\\sigma^{\\prime}\\left((w_{S}^{\\perp})^{\\top}x_{S}^{\\perp}\\right)x_{S}\\right]-a\\mathbb{E}_{x}\\left[f^{*}(x)\\sigma^{\\prime}\\left((w_{S}^{\\perp})^{\\top}x_{S}^{\\perp}\\right)x_{S}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where we used $\\mathbb{E}_{x_{S}}[x_{S}]\\;=\\;0$ and (3.5). Combining (A.2), (A.3), and (A.4), we have for any $\\eta\\in\\mathcal{C}_{c}^{\\infty}(\\mathbb{R}^{d+1}\\times(0,+\\infty))=\\mathcal{C}_{c}^{\\infty}(\\mathbb{R}\\times S^{\\perp}\\times S\\times(0,+\\infty))$ that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\iint_{{\\mathbb R}}(-\\partial_{t}\\eta+\\nabla_{\\theta}\\eta\\cdot(\\xi(t)\\nabla_{\\theta}\\Phi(\\theta;\\rho_{t})))\\,\\rho_{t}(d\\theta)d t}\\\\ &{=\\iiint\\Big(-\\partial_{t}\\eta(\\theta,t)+\\xi_{\\alpha}(t)\\partial_{\\alpha}\\eta(\\theta,t)\\cdot\\partial_{\\alpha}\\Phi(\\theta;\\rho_{t})+\\xi_{w}(t)\\nabla_{w_{\\tilde{s}}^{\\perp}}\\eta(\\theta,t)\\cdot\\nabla_{w_{\\tilde{s}}^{\\perp}}\\Phi(\\theta;\\rho_{t})}\\\\ &{\\qquad\\qquad\\qquad+\\xi_{w}(t)\\nabla_{w_{s}}\\eta(\\theta,t)\\cdot\\nabla_{w_{s}}\\Phi(\\theta;\\rho_{t})\\Big)\\hat{\\rho}_{t}(d\\theta)\\delta_{S}(d w_{s})d t}\\\\ &{=\\iint\\Big(-\\partial_{t}\\eta(\\tilde{\\theta},0,t)+\\xi_{\\alpha}(t)\\partial_{\\alpha}\\eta(\\tilde{\\theta},0,t)\\cdot\\partial_{\\alpha}\\hat{\\Phi}(\\hat{\\theta};\\hat{\\rho}_{t})}\\\\ &{\\qquad\\qquad\\qquad+\\xi_{w}(t)\\nabla_{w_{\\tilde{s}}^{\\perp}}\\eta(\\hat{\\theta},0,t)\\cdot\\nabla_{w_{\\tilde{s}}^{\\perp}}\\hat{\\Phi}(\\hat{\\theta};\\hat{\\rho}_{t})\\Big)\\hat{\\rho}_{t}(d\\hat{\\theta})d t}\\\\ &{=\\iint\\Big(-\\partial_{t}\\eta(\\hat{\\theta},0,t)+\\nabla_{\\hat{\\theta}}\\eta(\\hat{\\theta},0,t)\\cdot(\\hat{\\xi}(t)\\nabla_{\\hat{\\theta}}\\hat{\\Phi}(\\hat{\\theta};\\hat{\\rho}_{t}))\\Big)\\,\\hat{\\rho}_{t}(d\\hat{\\theta})d t}\\\\ &{\\quad-\\Omega}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the last equality holds by applying the test function $\\eta(\\cdot,0,\\cdot)\\in\\mathcal{C}_{c}^{\\infty}(\\mathbb{R}\\times S^{\\perp}\\times(0,+\\infty))$ (3.7). The proof is hence completed. ", "page_idx": 12}, {"type": "text", "text": "A.2Proof of Theorem 3.6 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The proof of Theorem 3.6 uses some ideas from the proof of Theorem 16 in [1]. Similar ideas also exist in earlier works (see e.g., [19, 20]). ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1. Suppose that Assumption 3.1 holds and let $\\rho_{t}$ solve (2.4). Then for any $t>0$ $\\rho_{t}$ is supportedin $\\begin{array}{r}{\\big\\{\\theta=(a,w)\\in\\mathbb{R}^{d+1}:|a|\\le K_{\\rho}+K_{\\xi}K_{\\sigma}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}t\\big\\}.}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "Proof. The particle dynamics for $a_{t}$ associated with (2.4) is given by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{d}{d t}a_{t}=\\xi_{a}(t)\\mathbb{E}_{x}\\left[(f_{\\mathrm{NN}}(x;\\rho_{t})-f^{*}(x))\\sigma(w_{t}^{\\top}x)\\right],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which implies that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\vert\\frac{d}{d t}a_{t}\\right\\vert\\leq K_{\\xi}K_{\\sigma}\\left\\vert\\mathbb{E}_{x}\\left[f_{\\mathrm{NN}}(x;\\rho_{t})-f^{*}(x)\\right]\\right\\vert\\leq K_{\\xi}K_{\\sigma}(2\\mathcal{E}(\\rho_{0}))^{1/2}=K_{\\xi}K_{\\sigma}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, one has $\\begin{array}{r}{|a_{t}|\\leq|a_{0}|+K_{\\xi}K_{\\sigma}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}t}\\end{array}$ which completes the proof. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.2. Suppose that Assumption 3.1 holds for both $\\rho_{0}$ and $\\tilde{\\rho}_{0}$ :Let $\\rho_{t}$ solve $\\partial_{t}\\rho_{t}\\,=\\,\\nabla_{\\theta}$ $\\left(\\rho_{t}\\xi(t)\\nabla_{\\theta}\\Phi(\\theta;\\rho_{t}\\right)\\right)$ andlet ${\\tilde{\\rho}}_{t}$ solve $\\begin{array}{r}{\\partial_{t}\\tilde{\\rho}_{t}\\;=\\;\\nabla_{\\theta}\\,\\cdot\\,\\left(\\tilde{\\rho}_{t}\\xi(t)\\nabla_{\\theta}\\Phi(\\theta;\\tilde{\\rho}_{t})\\right)}\\end{array}$ .For any coupling $\\gamma_{0}~\\in$ $\\Gamma(\\rho_{0},\\tilde{\\rho}_{0})$ ,let $\\gamma_{t}\\in\\Gamma(\\rho_{t},\\tilde{\\rho}_{t})$ be the associated coupling during the evolution and define ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Delta(t)=\\int\\!\\!\\int\\big(|a-\\tilde{a}|^{2}+||w-\\tilde{w}||^{2}\\big)\\,\\gamma_{t}(d\\theta,d\\tilde{\\theta}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then it holds for any $0\\leq t\\leq T$ that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x}\\left[|f_{N N}(x;\\rho_{t})-f_{N N}(x;\\tilde{\\rho}_{t})|^{2}\\right]\\le C_{f}\\Delta(t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\Delta(t)\\leq C_{\\Delta}\\Delta(t),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $C_{f}$ and $C_{\\Delta}$ are constants depending only on p, $h^{*},\\,K_{\\sigma},\\,K_{\\xi},\\,K_{\\rho}$ and $T$ ", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 3.6. Let $C_{f}$ and $C_{\\Delta}$ be the constants in Lemma A.2. For any $\\epsilon>0$ , there exists a coupling $\\gamma_{0}\\in\\Gamma(\\rho_{0},\\tilde{\\rho}_{0})$ such that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\iint(|a-\\tilde{a}|^{2}+\\|w-\\tilde{w}\\|^{2})\\gamma_{0}(d\\theta,d\\tilde{\\theta})\\leq W_{2}^{2}(\\rho_{0},\\tilde{\\rho}_{0})+\\epsilon.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Define $\\gamma_{t}\\in\\Gamma(\\rho_{t},\\tilde{\\rho}_{t})$ and $\\Delta(t)$ as in Lemma A.2. According (A.6) and the Gronwall's inequality, it holds that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Delta(t)\\leq\\Delta(0)e^{C_{\\Delta}t}\\leq\\left(W_{2}^{2}(\\rho_{0},\\tilde{\\rho}_{0})+\\epsilon\\right)e^{C_{\\Delta}t},\\quad\\forall\\,0\\leq t\\leq T,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which combined with (A.5) yields that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq T}\\mathbb{E}_{x}\\left[\\vert f_{\\mathrm{NN}}(x;\\rho_{t})-f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t})\\vert^{2}\\right]\\leq\\left(W_{2}^{2}(\\rho_{0},\\tilde{\\rho}_{0})+\\epsilon\\right)C_{f}e^{C_{\\Delta}T}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then we can conclude (3.8) be setting $\\epsilon\\to0$ and $C_{s}=C_{f}e^{C_{\\Delta}T}$ ", "page_idx": 13}, {"type": "text", "text": "Corollary A.3. Under the same setting as in Theorem 3.6, one has ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq T}|\\mathcal{E}(\\rho_{t})-\\mathcal{E}(\\tilde{\\rho}_{t})|\\leq\\left(C_{s}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]\\right)^{1/2}W_{2}(\\rho_{0},\\tilde{\\rho}_{0})+\\frac{1}{2}C_{s}W_{2}^{2}(\\rho_{0},\\tilde{\\rho}_{0}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. It can be computed that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}(\\rho_{t})=\\!\\frac{1}{2}\\mathbb{E}_{x}\\left[|(f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t})-f^{*}(x))+(f_{\\mathrm{NN}}(x;\\rho_{t})-f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t}))|^{2}\\right]}\\\\ &{\\quad\\quad=\\!\\mathcal{E}(\\tilde{\\rho}_{t})+\\mathbb{E}_{x}\\left[(f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t})-f^{*}(x))(f_{\\mathrm{NN}}(x;\\rho_{t})-f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t}))\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\,\\frac{1}{2}\\mathbb{E}_{x}\\left[|f_{\\mathrm{NN}}(x;\\rho_{t})-f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t})|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which implies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle}&{\\operatorname*{sup}_{0\\leq t\\leq T}\\left|\\mathcal{E}\\big(\\rho_{t}\\big)-\\mathcal{E}\\big(\\tilde{\\rho}_{t}\\big)\\right|\\leq\\mathbb{E}_{x}\\left[\\left|f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t})-f^{*}(x)\\right|^{2}\\right]^{1/2}\\mathbb{E}_{x}\\left[\\left|f_{\\mathrm{NN}}(x;\\rho_{t})-f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t})\\right|^{2}\\right]^{1/2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{1}{2}\\mathbb{E}_{x}\\left[\\left|f_{\\mathrm{NN}}(x;\\rho_{t})-f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t})\\right|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\big(C_{s}\\mathbb{E}_{x_{V}}\\left[\\left|h^{*}(x_{V})\\right|^{2}\\right]\\big)^{1/2}\\,W_{2}\\big(\\rho_{0},\\tilde{\\rho}_{0}\\big)+\\displaystyle\\frac{1}{2}C_{s}W_{2}^{2}\\big(\\rho_{0},\\tilde{\\rho}_{0}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we used Theorem 3.6 and Remark 3.2. ", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma A.2. We first prove (A.5). It can be computed that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ |f_{\\mathrm{NN}}(x;\\rho_{t})-f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t})|=\\left|\\int a\\sigma(w^{\\top}x)\\rho_{t}(d\\theta)-\\int\\tilde{a}\\sigma(\\tilde{w}^{\\top}x)\\tilde{\\rho}_{t}(d\\tilde{\\theta})\\right|}\\\\ &{\\le\\left|\\displaystyle\\iint(a-\\tilde{a})\\sigma(w^{\\top}x)\\gamma_{t}(d\\theta,d\\tilde{\\theta})\\right|+\\left|\\displaystyle\\iint\\tilde{a}\\left(\\sigma(w^{\\top}x)-\\sigma(\\tilde{w}^{\\top}x)\\right)\\gamma_{t}(d\\theta,d\\tilde{\\theta})\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and hence that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{x}\\left[\\left|f_{\\mathrm{NN}}(x;\\rho_{t})-f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t})\\right|^{2}\\right]}\\\\ &{\\le2\\mathbb{E}_{x}\\left[\\left|\\displaystyle\\int\\!\\!\\int(a-\\tilde{a})\\sigma(w^{\\top}x)\\gamma_{t}(d\\theta,d\\tilde{\\theta})\\right|^{2}\\right]+2\\mathbb{E}_{x}\\left[\\left|\\displaystyle\\int\\!\\!\\int\\tilde{a}\\left(\\sigma(w^{\\top}x)-\\sigma(\\tilde{w}^{\\top}x)\\right)\\gamma_{t}(d\\theta,d\\tilde{\\theta})\\right|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We then bound the two terms above as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x}\\left[\\left|\\int\\!\\!\\int(a-\\tilde{a})\\sigma(w^{\\top}x)\\gamma_{t}(d\\theta,d\\tilde{\\theta})\\right|^{2}\\right]\\leq K_{\\sigma}^{2}\\int\\!\\!\\int|a-\\tilde{a}|^{2}\\gamma_{t}(d\\theta,d\\tilde{\\theta})\\leq K_{\\sigma}^{2}\\Delta(t),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x}\\left[\\left|\\displaystyle\\iint\\tilde{a}\\left(\\sigma(w^{\\top}x)-\\sigma(\\tilde{w}^{\\top}x)\\right)\\gamma_{t}(d\\theta,d\\tilde{\\theta})\\right|^{2}\\right]}\\\\ &{\\le K_{\\sigma}^{2}\\left(K_{\\rho}+\\sqrt{2}K_{\\xi}K_{\\sigma}\\mathcal{E}(\\rho_{0})^{1/2}T\\right)^{2}\\mathbb{E}_{x}\\left[\\left|\\displaystyle\\iint\\left|(w-\\tilde{w})^{\\top}x|\\gamma_{t}(d\\theta,d\\tilde{\\theta})\\right|^{2}\\right]}\\\\ &{\\le K_{\\sigma}^{2}\\left(K_{\\rho}+K_{\\xi}K_{\\sigma}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}T\\right)^{2}\\displaystyle\\iint\\mathbb{E}_{x}\\left[|(w-\\tilde{w})^{\\top}x|^{2}\\right]\\gamma_{t}(d\\theta,d\\tilde{\\theta})}\\\\ &{=K_{\\sigma}^{2}\\left(K_{\\rho}+K_{\\xi}K_{\\sigma}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}T\\right)^{2}\\displaystyle\\iint\\|w-\\tilde{w}\\|^{2}\\gamma_{t}(d\\theta,d\\tilde{\\theta})}\\\\ &{\\le K_{\\sigma}^{2}\\left(K_{\\rho}+K_{\\xi}K_{\\sigma}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}T\\right)^{2}\\Delta(t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we used Lemma A.1 and $(w\\mathrm{~-~}\\tilde{w})^{\\top}x\\;\\sim\\mathcal{N}(0,\\|w\\mathrm{~-~}\\tilde{w}\\|^{2})$ if $x\\,\\sim\\mathcal{N}(0,I_{d})$ . Then we can conclude (A.5) with $C_{f}\\,=\\,K_{\\sigma}^{2}+K_{\\sigma}^{2}\\left(K_{\\rho}+K_{\\xi}K_{\\sigma}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}T\\right)^{2}$ by combining all estimations above. ", "page_idx": 14}, {"type": "text", "text": "We then head into the proof of (A.6), for which we need the particle dynamics ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\frac{d}{d t}a_{t}=\\xi_{a}(t)\\mathbb{E}_{x}\\left[(f_{\\mathrm{NN}}(x;\\rho_{t})-f^{*}(x))\\sigma(w_{t}^{\\top}x)\\right],\\right.}\\\\ {\\left.\\frac{d}{d t}\\tilde{a}_{t}=\\xi_{a}(t)\\mathbb{E}_{x}\\left[(f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t})-f^{*}(x))\\sigma(\\tilde{w}_{t}^{\\top}x)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\int\\frac{d}{d t}w_{t}=\\xi_{w}a_{t}\\mathbb{E}_{x}\\left[(f_{\\mathrm{NN}}(x;\\rho_{t})-f^{*}(x))\\sigma^{\\prime}(w_{t}^{\\top}x)x\\right],}\\\\ &{}&{\\left\\lfloor\\frac{d}{d t}\\tilde{w}_{t}=\\xi_{w}\\tilde{a}_{t}\\mathbb{E}_{x}\\left[(f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t})-f^{*}(x))\\sigma^{\\prime}(\\tilde{w}_{t}^{\\top}x)x\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The distance between the particle dynamics can be decomposed as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\cfrac{d}{d t}a_{t}-\\cfrac{d}{d t}\\tilde{a}_{t}\\right|\\leq\\!K_{\\xi}\\left|\\mathbb{E}_{x}\\left[(f_{\\mathrm{NN}}(x;\\rho_{t})-f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t}))\\sigma(w_{t}^{\\top}x)\\right]\\right|}\\\\ &{\\qquad\\qquad\\qquad+\\,K_{\\xi}\\left|\\mathbb{E}_{x}\\left[(f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t})-f^{*}(x))(\\sigma(w_{t}^{\\top}x)-\\sigma(\\tilde{w}_{t}^{\\top}x))\\right]\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\left|\\left\\langle w_{t}-\\tilde{w}_{t},\\frac{d}{d t}w_{t}-\\frac{d}{d t}\\tilde{w}_{t}\\right\\rangle\\right|}\\\\ &{\\leq\\!\\!K_{\\xi}\\left|\\left(a_{t}-\\tilde{a}_{t}\\right)\\!\\mathbb{E}_{x}\\left[\\left(f_{\\mathrm{NN}}(x;\\rho_{t})-f^{*}(x)\\right)\\!\\sigma^{\\prime}(w_{t}^{\\top}x)(w_{t}-\\tilde{w}_{t})^{\\top}x\\right]\\right|}\\\\ &{~~~~~+K_{\\xi}\\left|\\tilde{a}_{t}\\mathbb{E}_{x}\\left[\\left(f_{\\mathrm{NN}}(x;\\rho_{t})-f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t})\\right)\\!\\sigma^{\\prime}(w_{t}^{\\top}x)(w_{t}-\\tilde{w}_{t})^{\\top}x\\right]\\right|}\\\\ &{~~~~~+K_{\\xi}\\left|\\tilde{a}_{t}\\mathbb{E}_{x}\\left[\\left(f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t})-f^{*}(x)\\right)\\left(\\sigma^{\\prime}(w_{t}^{\\top}x)-\\sigma^{\\prime}(\\tilde{w}_{t}^{\\top}x)\\right)(w_{t}-\\tilde{w}_{t})^{\\top}x\\right]\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, it can be computed that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d}{d t}\\displaystyle\\int\\!\\!\\left\\{\\!\\begin{array}{l l}{-\\lambda\\!\\!\\!}&{-\\lambda\\!\\!}\\\\ {1\\!\\!}&{\\!\\!\\!}\\\\ {\\!\\!\\!}&{\\!\\!\\!}\\\\ {\\!\\!\\!\\!}&{\\!\\!\\!\\!}\\\\ {\\!\\!\\!\\!}&{\\!\\!\\!\\!}\\end{array}\\right\\}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\frac{d}{d t}\\displaystyle\\int\\!\\!\\!\\!\\left\\{\\int\\!\\!\\!\\left\\{a\\!\\!\\!\\!\\!\\:\\!\\!\\!-A\\!\\!\\!\\!\\!\\!/_{0}\\!\\!\\!\\!\\!\\:\\!\\!\\!-A\\!\\!\\!\\!\\!/_{1}\\!\\!\\!\\!\\right\\}\\!\\!\\!\\!\\right\\}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we used Remark 3.2, and that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{d}{d t}\\displaystyle\\int\\left\\lVert u-\\bar{u}\\right\\rVert^{2}\\gamma_{1}(H_{0},\\bar{u})\\,}\\\\ &{=\\frac{d}{d t}\\displaystyle\\int\\left\\lVert u_{t}-\\bar{u}_{t}\\right\\rVert^{2}\\gamma_{2}(\\bar{u}_{t}\\bar{u},d\\bar{u})}\\\\ &{=2\\int\\left\\langle\\bar{\\mathcal{T}}^{\\prime}\\left\\langle v_{t}-\\bar{u}_{t}\\right\\rangle\\bar{\\mathcal{T}}^{\\prime}\\left\\langle\\frac{d}{d t}\\bar{u},\\frac{d}{d t}\\nu\\right\\rangle\\gamma_{1}(H_{0},\\bar{u}_{\\theta})}\\\\ &{\\overset{(a)}{\\le}2K(K_{s},\\bar{f}_{s}^{\\prime}\\int\\left\\lVert u_{t}-\\bar{u}_{t}\\right\\rVert^{2}\\mathbb{E}_{\\mathbb{R}}\\left[\\left\\langle\\nabla u_{t},\\bar{\\kappa}_{\\theta}\\right\\rangle-\\bar{f}^{\\prime}(\\bar{x})\\right]\\cdot\\left\\lvert(w_{t}-\\bar{u}_{t})^{T}\\right\\rangle\\mathbb{T}_{\\mathcal{S}}\\left\\langle\\frac{d}{d t}\\bar{u}_{\\theta}\\bar{u}_{\\theta}\\right\\rangle}\\\\ &{\\qquad+2K_{K}K_{s}\\int\\left\\langle\\bar{\\mathcal{T}}^{\\prime}\\left\\lVert u_{t}\\right\\rvert\\cdot\\mathbb{E}_{\\mathbb{R}}\\left[\\left\\langle\\bar{u}_{t},\\bar{\\kappa}_{\\theta}\\right\\rangle-\\bar{f}^{\\prime}(\\bar{x})\\right]\\cdot\\left\\lvert(w_{t}-\\bar{u}_{t})^{T}\\right\\rangle\\mathbb{T}_{\\mathcal{S}}\\left\\langle\\bar{u}_{\\theta}\\bar{u}_{\\theta},\\bar{u}_{\\theta}\\right\\rangle}\\\\ &{\\qquad+2K_{K}K_{s}\\int\\left\\langle\\bar{\\mathcal{T}}^{\\prime}\\left\\lVert\\bar{u}_{t}\\right\\rvert\\cdot\\mathbb{E}_{\\mathbb{R}}\\left[\\left\\lvert\\bar{\\mathcal{X}}(\\alpha)(\\bar{\\mathcal{T}}_{\\theta},\\bar{\\kappa}_{\\theta})-\\bar{f}^{\\prime}(\\bar{x})\\right\\rvert\\cdot\\left\\lvert(w_{t}-\\bar{u}_{t})^{T}\\right\\rvert\\right]\\cdot\\left\\lvert\\bar{\\mathcal{T}}^{\\prime}\\left\\lvert\\frac{d}{d t}\\left\\langle\\bar{u}_{\\theta},\\bar{u}_{\\theta}\\right\\rangle}\\\\ &{\\le2K_{K}K_{s}\\int\\left\\langle\\bar{\\mathcal{T}}^{\\prime}\\\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\le\\,2K_{\\xi}K_{\\sigma}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}\\iint|a_{t}-\\tilde{a}_{t}|\\cdot\\|w_{t}-\\tilde{w}_{t}\\|\\gamma_{0}(d\\theta_{0},d\\tilde{\\theta}_{0})}}\\\\ &{\\le\\,2K_{\\xi}K_{\\sigma}\\left(K_{\\rho}+K_{\\xi}K_{\\sigma}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}T\\right)(C_{f}\\Delta(t))^{1/2}\\int\\int\\|w_{t}-\\tilde{w}_{t}\\|\\gamma_{0}(d\\theta_{0},d\\tilde{\\theta}_{0})}\\\\ &{+\\,2K_{\\xi}K_{\\sigma}\\left(K_{\\rho}+K_{\\xi}K_{\\sigma}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}T\\right)\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}}\\\\ &{\\qquad\\qquad\\cdot\\iint\\sqrt{3}\\|w_{t}-\\tilde{w}_{t}\\|^{2}\\gamma_{0}(d\\theta_{0},d\\tilde{\\theta}_{0})}\\\\ &{\\le\\,K_{\\xi}K_{\\sigma}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}\\Delta(t)+2K_{\\xi}K_{\\sigma}\\left(K_{\\rho}+K_{\\xi}K_{\\sigma}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}T\\right)C_{f}^{1/2}\\Delta(t)}\\\\ &{\\qquad+\\,2\\sqrt{3}K_{\\xi}K_{\\sigma}\\left(K_{\\rho}+K_{\\xi}K_{\\sigma}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}T\\right)\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}\\Delta(t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we used Remark 3.2, (A.5), Lemma A.1, and $(w-\\tilde{w})^{\\top}x\\sim\\mathcal{N}(0,\\|w-\\tilde{w}\\|^{2})$ $x\\sim\\mathcal{N}(0,I_{d})$ Therefore, we can conclude that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\Delta(t)=\\frac{d}{d t}\\iint|a-\\tilde{a}|^{2}\\gamma_{t}(d\\theta,d\\tilde{\\theta})+\\frac{d}{d t}\\iint||w-\\tilde{w}||^{2}\\gamma_{t}(d\\theta,d\\tilde{\\theta})\\leq C_{\\Delta}\\Delta(t),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{\\Delta}=\\!1+2K_{\\xi}^{2}K_{\\sigma}^{2}C_{f}+2K_{\\xi}^{2}K_{\\sigma}^{2}\\mathbb{E}_{x\\nu}[|h^{*}(x_{V})|^{2}]}\\\\ &{\\qquad+\\,K_{\\xi}K_{\\sigma}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}+2K_{\\xi}K_{\\sigma}\\left(K_{\\rho}+K_{\\xi}K_{\\sigma}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}T\\right)C_{f}^{1/2}}\\\\ &{\\qquad+\\,2\\sqrt{3}K_{\\xi}K_{\\sigma}\\left(K_{\\rho}+K_{\\xi}K_{\\sigma}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}T\\right)\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This proves (A.6). ", "page_idx": 16}, {"type": "text", "text": "A.3Proof of Theorem 3.4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The proof of Theorem 3.4 is based on Theorem 3.5 and Theorem 3.6. ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 3.4. Let $\\mathcal{P}_{S}:\\mathbb{R}^{d+1}\\rightarrow S$ be the projection in Theorem 3.5 and let ${\\mathcal P}_{S}^{\\perp}=I_{d+1}-$ $\\mathcal{P}_{S}$ . Let ${\\tilde{\\rho}}_{t}$ solve (2.4) with $\\tilde{\\rho}_{0}=\\mathcal{P}_{S}^{\\perp}\\rho_{w}\\times\\delta_{S}$ . It is clear that $(\\mathcal{P}_{S})_{\\#}\\tilde{\\rho}_{0}=\\delta_{S}$ . In addition, with the decomposition $x=x_{1}+x_{2}+x_{3}$ where $x_{1}=x_{V}^{\\perp}=x-x_{V}$ \uff0c $x_{2}=x_{V}-x_{S}$ , and $x_{3}=x_{S}$ are independent Gaussian random variables, we have for any $w\\in\\mathbb{R}^{d}$ that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x}\\left[f^{*}(x)\\sigma^{\\prime}\\left(w^{\\top}x\\underline{{\\lambda}}\\right)x S\\right]=\\mathbb{E}_{x_{1}}\\mathbb{E}_{x_{2},x_{3}}\\left[\\left[h^{*}(x_{2}+x_{3})\\sigma^{\\prime}\\left(w^{\\top}x_{1}+w^{\\top}x_{2}\\right)x_{3}\\right]\\right]=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we used (3.1). Then according to Theorem 3.5, for any $t\\geq0$ that $(\\mathcal{P}_{S})_{\\#}\\tilde{\\rho}_{t}\\,=\\,\\delta_{S}$ , which implies that $f_{\\mathrm{NN}}(x;\\rho_{t})$ is a constant function in $x_{S}$ for any fixed $x_{S}^{\\perp}$ , giving a lowerbound on its loss: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}(\\tilde{\\rho}_{t})=\\frac{1}{2}\\mathbb{E}_{x}\\left[\\|f^{*}(x)-f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t})\\|^{2}\\right]}\\\\ &{\\quad=\\frac{1}{2}\\mathbb{E}_{x_{\\delta}^{+}}\\left[\\mathbb{E}_{x_{\\delta}}\\left[\\|f^{*}(x)-f_{\\mathrm{NN}}(x;\\tilde{\\rho}_{t})\\|^{2}\\right]\\right]}\\\\ &{\\quad\\ge\\frac{1}{2}\\mathbb{E}_{x_{\\delta}^{+}}\\left[\\mathbb{E}_{x_{\\delta}}\\left[\\|f^{*}(x)-\\mathbb{E}_{x_{\\delta}}[f^{*}(x)]\\|^{2}\\right]\\right]}\\\\ &{\\quad=\\frac{1}{2}\\mathbb{E}_{z_{\\delta}^{+}}\\left[\\mathbb{E}_{z_{\\delta}}\\left[\\|h^{*}(z)-h_{S^{\\perp}}^{*}(z_{S}^{\\perp})\\|^{2}\\right]\\right]}\\\\ &{\\quad=\\frac{1}{2}\\mathbb{E}_{z}\\left[\\|h^{*}(z)-h_{S^{\\perp}}^{*}(z_{S}^{\\perp})\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $z_{S}=\\mathcal{P}_{S}^{V}z,z_{S}^{\\perp}=z-z_{S}$ , and $h_{S^{\\perp}}^{*}(z_{S}^{\\perp})=\\mathbb{E}_{z_{S}}[h^{*}(z)]$ ", "page_idx": 16}, {"type": "text", "text": "Now we show the actual flow is not very different. For $\\rho_{0}\\,=\\,\\rho_{a}\\,\\times\\,\\rho_{w}$ With $\\rho_{w}\\sim\\mathcal{N}(0,I_{d})$ and $\\tilde{\\rho}_{0}=\\mathcal{P}_{S}^{\\perp}\\rho_{w}\\times\\delta_{S}$ , it can be estimated that ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{2}^{2}(\\rho_{0},\\tilde{\\rho}_{0})\\leq\\frac{\\dim S}{d}\\leq\\frac{p}{d}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then applying Corollary A.3, we can conclude for any $T>0$ that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{inf}_{0\\leq t\\leq T}\\mathcal{E}(\\tilde{\\rho}_{t})\\geq\\operatorname*{inf}_{0\\leq t\\leq T}\\mathcal{E}(\\tilde{\\rho}_{t})-\\left(C_{s}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]\\right)^{1/2}W_{2}(\\rho_{0},\\tilde{\\rho}_{0})-\\frac{1}{2}C_{s}W_{2}^{2}(\\rho_{0},\\tilde{\\rho}_{0})}\\\\ &{\\geq\\displaystyle\\frac{1}{2}\\mathbb{E}_{z}\\left[\\|h^{*}(z)-h_{S^{\\perp}}^{*}(z_{S}^{\\perp})\\|^{2}\\right]-\\frac{\\left(p C_{s}\\mathbb{E}_{z}\\left[|h^{*}(z)|^{2}\\right]\\right)^{1/2}}{d^{1/2}}-\\frac{p C_{s}}{2d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C_{s}>0$ is the constant in Theorem 3.6 depending only on $p,h^{*},K_{\\sigma},K_{\\xi},K_{\\rho}$ and $T$ Therefore, we can obtain (3.3) and (3.4) immediately. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B  Further Discussion and Characterization of the Reflective Property and Theorem 3.4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1  Equivalence between (3.2) and isoLeap $\\left(h^{*}\\right)\\geq2$ ", "page_idx": 17}, {"type": "text", "text": "We prove the following equivalence where isoLeap $\\left(h^{*}\\right)$ is the isotropic leap complexity defined in [2, Appendix B.2]. ", "page_idx": 17}, {"type": "text", "text": "Proposition B.1. For any polynomial $h^{*}:V\\to\\mathbb{R}_{}$ . then it satisfies (3.2) with some nontrivial subspace $S\\subset V$ if and only if isoLeap $\\left(h^{*}\\right)\\geq2$ ", "page_idx": 17}, {"type": "text", "text": "Proof. Without loss of generality, we assume that $V=\\mathbb{R}^{p}$ .Suppose that isoLeap $\\left(h^{*}\\right)\\geq2$ , which means that the leap complexity of $h^{*}$ (as defined in [2, Definition 1]) is greater than one for some orthonormal basis of $V$ We can assume that the basis is $\\{e_{1},e_{2},\\ldots,e_{p}\\}$ : where $e_{j}$ is the vector in $\\mathbb{R}^{p}$ with the $j$ -th entry being 1 and other entries being 0. Denote the Hermite decomposition of $h^{*}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\nh^{\\ast}(z)=\\sum_{i=1}^{m}c_{i}\\prod_{j=1}^{p}\\operatorname{He}_{\\alpha_{i}(j)}(z_{j}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $c_{1},c_{2},\\ldots,c_{m}$ are nonzero coefficients, $\\alpha_{1},\\alpha_{2},\\ldots,\\alpha_{m}$ are pairwise distinct elements in ${\\mathbb{N}}^{p}$ with $\\alpha_{i}(j)$ being the $j$ -th entry of $\\alpha_{i}$ , and $\\mathrm{He}_{k}$ is the $k$ -th order Hermite polynomial. Since the leap complexity of $h^{*}$ is at least two, the following is true after applying some permutation on $\\{1,2,\\ldots,m\\}$ : There exists some $m_{1}\\ \\in\\ \\{1,2,\\ldots,m\\textrm{-}1\\}$ , such that it holds for any $i\\ \\in$ $\\{m_{1}+1,\\ldots,m\\}$ that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{j\\in J_{m_{1}}}\\alpha_{i}(j)\\geq2,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\nJ_{m_{1}}=\\{j\\in\\{1,2,\\ldots,p\\}:\\alpha_{i}(j)=0,\\,\\forall\\,i\\in\\{1,2,\\ldots,m_{1}\\}\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, by the orthogonality of Hermite polynomials, we have for $i\\in\\{m_{1}+1,\\ldots,m\\}$ that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{z_{j}\\sim\\mathcal{N}(0,1)}\\left[z_{j}\\prod_{j=1}^{p}\\mathsf{H e}_{\\alpha_{i}(j)}(z_{j})\\right]=0,\\quad\\forall\\,j\\in J_{m_{1}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The above also holds for $i\\in\\{1,2,\\dots,m_{1}\\}$ by the definition of $J_{m_{1}}$ . Therefore, we can conclude that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{z_{S}\\sim\\mathcal{N}(0,I_{S})}[h^{\\ast}(z)z_{S}]=0,\\quad\\forall\\,z_{S}^{\\perp},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "i.e., (3.2) holds, for $S=\\operatorname{span}\\{e_{j}:j\\in J_{m_{1}}\\}$ . Moreover, $S$ is nontrivial since (B.2) implies $J_{m_{1}}$ is not empty. ", "page_idx": 17}, {"type": "text", "text": "On the other hand, suppose that (3.2) is satisfied with some nontrivial subspace $S\\subset V$ thatcanbe assumedas $S=\\{(z_{1},\\dots,z_{p_{1}},0,\\dots,0):z_{1},\\dots,z_{p_{1}}\\in\\mathbb{R}\\}$ With $1\\leq p_{1}\\leq p$ We still consider the Hermite decomposition as in (B.1) and rewrite it as ", "page_idx": 17}, {"type": "equation", "text": "$$\nh^{*}(z)=\\sum_{i=1}^{m_{2}}c_{i}^{\\prime}\\prod_{j=1}^{p_{1}}\\mathrm{He}_{\\alpha_{i}^{\\prime}(j)}(z_{j})h_{i}(z_{p_{1}+1},\\dots,z_{p}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where c',C2, ... , Cm2 are nonzero coeficients, $\\alpha_{1}^{\\prime},\\alpha_{2}^{\\prime},\\ldots,\\alpha_{m_{2}}^{\\prime}$ are pairwise distinct elements in $\\mathbb{N}^{p_{1}}$ , and $h_{1},h_{2},\\ldots,h_{m_{2}}$ are nonzero polynomials defined on $\\mathbb{R}^{p-p^{\\prime}}$ . Then it follows from (3.2) that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{m_{2}}c_{i}^{\\prime}h_{i}(z_{p_{1}+1},\\dots,z_{p})\\mathbb{E}_{z_{j}\\sim N(0,1)}\\left[z_{j}\\mathrm{He}_{\\alpha_{i}^{\\prime}(j)}(z_{j})\\right]\\prod_{\\substack{j^{\\prime}=1,\\,j^{\\prime}\\neq j}}^{p_{1}}\\mathbb{E}_{z_{j^{\\prime}}\\sim N(0,1)}\\mathrm{He}_{\\alpha_{i}^{\\prime}(j^{\\prime})}(z_{j^{\\prime}})=0,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for any $j\\in\\{1,2,\\dots,p_{1}\\}$ and $z_{p_{1}+1},\\dotsc,z_{p}\\in\\mathbb{R}$ which implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{p_{1}}\\alpha_{i}(j)^{\\prime}\\neq1,\\quad\\forall\\:i\\in\\{1,2,\\ldots,m_{2}\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, the leap complexity of $h^{*}$ is at least 2 with respect to this basis, which leads to isoLeap $\\left(h^{*}\\right)\\geq2$ \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.2  Discretization Results Implied by Theorem 3.4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We discuss the sample complexity result of SGD implied by Theorem 3.4 in this subsection. Recall that there have been standard dimension-free results for bounding the distance between SGD and the mean-field dynamics; see e.g., [19]. So the result in this subsection is somehow a direct corollary. However, one needs to make minor modifications to guarantee that all boundedness assumptions in [19] aresatisfied. ", "page_idx": 18}, {"type": "text", "text": "Given a constant $C_{f}^{b}>0$ , define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widetilde{f}^{*}(x)=\\mathrm{sign}(f^{*}(x))\\operatorname*{min}\\{|f^{*}(x),C_{f}^{b}|\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is bounded with $|\\tilde{f}^{*}(x)|\\le C_{f}^{b}$ . One observation is that the subspace-sparse structure of $f^{*}$ implies that for any $\\delta>0$ , there exists a dimension-free constant $C_{f}^{b}$ depending on $h^{*}$ and $\\delta$ such that $\\mathbb{E}_{x\\sim\\mathcal{N}(0,I_{d})}[|f^{*}(x)-\\tilde{f}^{*}(x)|^{2}]<\\delta$ The associated mean-field dynamics is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\partial_{t}\\tilde{\\rho}_{t}=\\nabla_{\\theta}\\cdot\\Big(\\tilde{\\rho}_{t}\\xi(t)\\nabla_{\\theta}\\tilde{\\Phi}(\\theta;\\tilde{\\rho}_{t})\\Big)\\,,}\\\\ {\\tilde{\\rho}_{t}{\\big|}_{t=0}=\\rho_{0},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the learning rate $\\boldsymbol{\\xi}(t)=\\mathrm{diag}(\\xi_{a}(t),\\xi_{w}(t)I_{d})$ and the initialization $\\rho_{0}$ are shared with (2.4), and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tilde{\\Phi}(\\theta;\\rho)=a\\mathbb{E}_{x\\sim\\mathcal{N}(0,I_{d})}\\left[\\left(f_{\\mathrm{NN}}(x;\\rho)-\\tilde{f}^{*}(x)\\right)\\sigma(w^{\\top}x)\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The corresponding SGD is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{i}^{(k+1)}=\\theta_{i}^{(k)}+\\gamma^{(k)}\\left(\\tilde{f}^{*}(x_{k})-f_{\\mathrm{NN}}(x_{k};\\Theta^{(k)})\\right)\\nabla_{\\theta}\\tau(x_{k};\\theta_{i}^{(k)}),\\quad i=1,2,\\dots,N,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Where $N$ is the number of neurons and $\\gamma^{(k)}\\,=\\,\\mathrm{diag}(\\gamma_{a}^{(k)},\\gamma_{w}^{(k)}I_{d})\\,\\succeq\\,0$ is the larming rate with $\\gamma_{a}^{(k)}=\\epsilon\\xi_{a}(k\\epsilon)$ and $\\gamma_{w}^{(k)}=\\epsilon\\xi_{w}(k\\epsilon)$ ", "page_idx": 18}, {"type": "text", "text": "Suppose that assumptions made in Theorem 3.4 hold and fix $T>0$ . Using similar analysis as in Appendix A.2, one can conclude that for any $\\delta>0$ , there exists a dimension-free constant $C_{f}^{b}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq T}\\left|\\mathcal{E}(\\rho_{t})-\\mathcal{E}(\\tilde{\\rho}_{t})\\right|<\\delta.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Applying Theorem 3.4 and [19, Theorem 1], we can conclude that for any $\\mu\\in(0,1)$ , there exists dimension-free constants $N_{0},d_{0},C_{\\epsilon}$ , such that for any $N\\geq N_{0}$ and $d\\geq d_{0}$ , the following holds with probability at least \u03bc for any e \u2264 aig : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{k\\in[0,T/\\epsilon]\\cap\\mathbb{N}}\\mathcal{E}_{N}(\\Theta^{(k)})\\ge\\frac{1}{8}\\mathbb{E}_{z\\sim\\mathcal{N}(0,I_{V})}\\left[|h^{*}(z)-h_{S^{\\perp}}^{*}(z_{S}^{\\perp})|^{2}\\right]>0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If we further assume that $N={\\mathcal{O}}(e^{d})$ , this indicates that SGD as in (B.5) cannot learn the subspacesparse polynomial $f^{*}$ within finite time horizon and with $O(d)$ samples/data points. ", "page_idx": 18}, {"type": "text", "text": "C Proofs for Section 4 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1  Approximation of $w(a,t)$ by Polynomials ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This subsection follows [1] closely to approximate and analyze the behavior of $w(a,t)$ for $0\\leq t\\leq T$ with $\\xi_{a}(t)=0$ and $\\xi_{w}(t)=1$ . The dynamics of a single particle starting at $\\theta=(a,0)\\in\\mathbb{R}^{d+1}$ can be described by the following ODE: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\frac{\\partial}{\\partial t}w(a,t)=a\\mathbb{E}_{x}\\left[g(x,t)\\sigma^{\\prime}(w(a,t)^{\\top}x)x\\right],\\right.}\\\\ {\\left.w(a,0)=0,\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\ng(x,t)=f^{*}(x)-f_{\\mathrm{NN}}(x;\\rho_{t})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "is the residual. The first observation is that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x}\\left[f^{*}(x)\\sigma(w^{\\top}x_{V})x_{V}^{\\perp}\\right]=\\mathbb{E}_{x_{V}}\\left[h^{*}(x_{V})\\sigma(w^{\\top}x_{V})\\mathbb{E}_{x_{V}^{\\perp}}\\left[x_{V}^{\\perp}\\right]\\right]=0,\\quad\\forall\\,w\\in\\mathbb{R}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By Theorem 3.5, we have that $\\rho_{t}$ is supported in $\\left\\{(a,w):w_{V}^{\\perp}=0\\right\\}$ , and hence that ", "page_idx": 19}, {"type": "equation", "text": "$$\nw_{V}^{\\perp}(a,t)=0,\\quad\\forall\\,0\\leq t\\leq T.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We then analyze the behaviour of $w_{V}(a,t)$ . Let $\\{e_{1},e_{2},\\ldots,e_{p}\\}$ be an orthonormal basis of $V$ and we denote $w_{i}=w^{\\top}e_{i}$ and $x_{i}=x^{\\top}e_{i}$ for any $w,x\\in\\mathbb{R}^{d}$ and $i\\in\\{1,2,\\ldots,p\\}$ ", "page_idx": 19}, {"type": "text", "text": "By Assumption 4.2, it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sigma^{\\prime}\\left(w(a,t)^{\\top}x\\right)=m_{1}+\\sum_{l=1}^{L-1}\\frac{m_{l+1}}{l!}(w(a,t)^{\\top}x)^{l}+\\mathcal{O}\\left((w(a,t)^{\\top}x)^{L}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "With $m_{l}\\,=\\,\\sigma^{(l)}(0)$ and then an approximated solution to (C.1) (by polynomial expansion with high-order terms omitted) can be written as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tilde{w}_{i}(a,t)=\\sum_{1\\leq j\\leq L}Q_{i,j}(t)a^{j},\\quad1\\leq i\\leq p,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $Q(t)$ is given by $Q(0)=0$ and the following dynamics: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{d}{d t}Q_{i,1}(t)=\\mathbb{E}_{x}[x_{i}g(x,t)m_{1}],}\\\\ {\\displaystyle\\frac{d}{d t}Q_{i,j}(t)=\\mathbb{E}_{x}\\left[x_{i}g(x,t)\\sum_{l=1}^{L-1}\\frac{m_{l+1}}{l!}\\sum_{1\\le i_{1},\\ldots,i_{l}\\le p}\\sum_{j_{1}+\\cdots+j_{l}=j-1}\\prod_{s=1}^{l}Q_{i_{s},j_{s}}(t)x_{i_{s}}\\right],~~2\\le j\\le L.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let us remark that even if every single $\\tilde{w}_{i}(a,t)$ depends on the basis $\\{e_{1},e_{2},\\ldots,e_{p}\\}$ , the linear combination ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tilde{w}(a,t)=\\sum_{i=1}^{p}e_{i}\\tilde{w}_{i}(a,t)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "is basis-independent. To see this, let $Q_{j}(t)\\in\\mathbb{R}^{d}$ be the $j$ -th column of $Q(t)$ and it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle{\\frac{d}{d t}}Q_{1}(t)=\\mathbb{E}_{x}[x g(x,t)m_{1}],}\\\\ {\\displaystyle{\\frac{d}{d t}}Q_{j}(t)=\\mathbb{E}_{x}\\left[x g(x,t)\\sum_{l=1}^{L-1}\\frac{m_{l+1}}{l!}\\sum_{j_{1}+\\cdots+j_{l}=j-1}\\prod_{s=1}^{l}x^{\\top}Q_{j_{s}}(t)\\right],}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The distance between $w_{i}(a,t)$ and $\\tilde{w}_{i}(a,t)$ for $i\\in I$ can be bounded as follows. ", "page_idx": 19}, {"type": "text", "text": "Proposition C.1. Suppose that Assumption 4.2 holds. Then ", "page_idx": 19}, {"type": "equation", "text": "$$\n|w_{i}(a,t)-\\tilde{w}_{i}(a,t)|=\\mathcal{O}((|a|t)^{L+1}),\\quad\\forall\\:i\\in\\{1,2,\\ldots,p\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We need the following two lemmas to prove Proposition C.1. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.2. For any $i\\in\\{1,2\\dots,p\\}$ and $j\\in\\{1,2,\\ldots,L\\},$ it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\nQ_{i,j}(t)=O(t^{j}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. The non-increasing property of the energy functional implies that ", "page_idx": 20}, {"type": "equation", "text": "$$\n2\\mathcal{E}(\\rho_{t})=\\mathbb{E}_{x}[|g(x,t)|^{2}]\\leq\\mathbb{E}_{x}[|f^{*}(x)-f_{\\mathrm{NN}}(x,\\rho_{0})|^{2}]=\\mathbb{E}_{z}[|h^{*}(z)|^{2}]<+\\infty.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then (C.6) can be proved by induction. For $j=1$ , it follows from the boundedness of ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\frac{d}{d t}Q_{i,1}(t)\\right|=\\left|\\mathbb{E}_{x}[x_{i}g(x,t)m_{1}]\\right|\\le\\left|m_{1}\\right|\\left(\\mathbb{E}_{x}[x_{i}^{2}]\\cdot\\mathbb{E}_{x}[|g(x,t)|^{2}]\\right)^{1/2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "that $Q_{i,e_{l}}(t)\\,=\\,\\mathcal{O}(t)$ .  Consider any $2\\,\\le\\,j\\,\\le\\,L$ and assume that $Q_{i,j^{\\prime}}\\,=\\,\\mathcal{O}(t^{j^{\\prime}})$ holds for any $1\\leq i\\leq p$ and $1\\leq j^{\\prime}<j$ . Then one has that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\cfrac{d}{d t}Q_{i,j}(t)\\right|}\\\\ &{\\le\\left(\\mathbb{E}_{x}[|g(x,t)|^{2}]\\cdot\\mathbb{E}_{x}\\left[\\left|x_{i}\\sum_{l=1}^{L-1}\\frac{m_{l+1}}{l!}\\sum_{1\\le i_{1},\\ldots,i_{l}\\le p}\\sum_{j_{1}+\\cdots+j_{l}=j-1}\\prod_{s=1}^{l}Q_{i_{s},j_{s}}(t)x_{i_{s}}\\right|^{2}\\right]\\right)^{1/2}}\\\\ &{=\\!\\!\\mathcal{O}(t^{j-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which implies that $Q_{i,j}(t)=O(t^{j})$ ", "page_idx": 20}, {"type": "text", "text": "Lemma C.3. Suppose that Assumption 4.2 holds. We have for all $i\\in\\{1,2,\\ldots,p\\}$ that ", "page_idx": 20}, {"type": "equation", "text": "$$\nw_{i}(a,t)=O(|a|t).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. There exists a constant $C>0$ and a open subset $A\\subset\\{w\\in\\mathbb{R}^{d}:w_{V}^{\\perp}=0\\}$ containing 0, such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{x}\\left[x_{i}g(x,t)\\sigma^{\\prime}(w^{\\top}x)\\right]\\right|\\le C,\\quad\\forall\\,w\\in A,\\,t\\ge0,\\,i\\in\\{1,2,\\dots,p\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial}{\\partial t}w_{i}(a,t)\\right|\\leq|a|,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "as long as $w(a,t)$ does not leave $A$ . This implies (C.7). ", "page_idx": 20}, {"type": "text", "text": "Now we can proceed to prove Proposition C.1. ", "page_idx": 20}, {"type": "text", "text": "Proof of Proposition C.1. Set $\\tilde{w}_{V}^{\\perp}(a,t)=0$ . It can be estimated for any $i\\in\\{1,2,\\ldots,p\\}$ that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Bigg|\\Bigg|\\frac{\\partial}{\\partial t}\\bar{u_{i}}(a,t)-a\\mathbb{E}_{x}\\left[x_{i,0}(x,t)\\left(m_{1}+\\displaystyle\\sum_{l=1}^{L-1}\\frac{m_{l+1}}{l!}(\\bar{w}(a,t)\\tau_{x})^{l}\\right)\\right]\\Bigg|}\\\\ &{\\le\\Bigg|\\displaystyle{\\sum_{1\\le j\\le k}\\frac{d}{d t}Q_{i,j}(t)^{a}-a\\mathbb{E}_{x}\\left[x_{i,0}(x,t)\\left(m_{1}+\\displaystyle\\sum_{l=1}^{L-1}\\frac{m_{l+1}}{l!}\\left(\\sum_{1\\le s^{\\prime}\\le p_{1}\\le s^{\\prime}\\le L}Q_{s^{\\prime},j}(t)a^{j}x_{\\nu}\\right)^{l}\\right)\\right]}\\Bigg|}\\\\ &{\\le\\displaystyle{\\sum_{L+1\\le j\\le L}\\left[a^{j}\\cdot\\mathbb{E}_{x}\\left[x_{i,0}(x,t)\\displaystyle\\sum_{l=1}^{L-1}\\frac{m_{l+1}}{l!}\\sum_{1\\le s_{1}\\le s_{j}\\le h_{i}+\\cdots+s_{j}-1-s_{i}-1}\\displaystyle\\sum_{s=l+1}^{L}Q_{i,j}(t)x_{i,j}\\right]}\\Bigg|}\\\\ &{\\le\\displaystyle{\\sum_{L+1\\le j\\le L}\\left[a^{j}\\right]^{a}}}\\\\ &{\\qquad\\qquad\\cdot\\left(\\mathbb{E}[|g(x,t)|^{2}]\\cdot\\mathbb{E}_{x}\\left[\\left|x_{i}\\displaystyle\\sum_{l=1}^{L-1}\\frac{m_{l+1}}{l!}\\sum_{1\\le s_{l}\\le s_{j}\\le h_{i}+\\cdots+l=j-1}\\displaystyle\\sum_{s=l+1}^{L}Q_{i,j}(t)x_{i,j}\\right|^{2}\\right]\\right)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n=\\mathcal{O}(|a|^{L+1}t^{L}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which combined with (C.2) yields that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial}{\\partial t}\\displaystyle\\sum_{i=1}^{P}\\left|w_{i}(a,t)-\\widehat{w}_{i}(a,t)\\right|}\\\\ &{\\leq\\displaystyle\\sum_{i=1}^{P}\\left|\\frac{\\partial}{\\partial t}w_{i}(a,t)-\\frac{\\partial}{\\partial t}\\widehat{w}_{i}(a,t)\\right|}\\\\ &{\\leq\\displaystyle\\sum_{i=1}^{P}\\left|\\alpha\\mathrm{g}_{i}\\left[w_{i}(a,t)\\left(m_{1}+\\sum_{l=1}^{L-1}\\frac{m_{l+1}}{R_{l}^{2}}(w_{i}(a,t)^{T}x_{l})^{l}\\right)\\right]}\\\\ &{\\phantom{\\leq\\displaystyle\\sum_{i=1}^{P}\\left|\\alpha\\mathrm{g}_{i}\\left[w_{i}(a,t)\\left(m_{1}+\\sum_{l=1}^{L-1}\\frac{m_{l+1}}{R_{l}^{2}}(\\widehat{w}_{i}(a,t)^{T}x_{l})^{l}\\right)\\right]\\right|}\\\\ &{\\phantom{\\leq\\displaystyle\\sum_{i=1}^{P}\\left|\\alpha\\mathrm{g}_{i}\\left[w_{i}(a,t)\\right]\\right|\\cdot\\mathcal{O}\\left((w_{i}(a,t)^{T}x_{l})^{l}+\\mathcal{O}\\left(|a|^{L+1}t^{L}\\right)\\right)}\\\\ &{\\leq\\displaystyle\\sum_{i=1}^{P}\\left|\\alpha_{x}\\left[w_{i}(a,t)\\sum_{l=1}^{L-1}\\frac{m_{l+1}}{L}\\left((w(a,t)^{T}x_{l})^{l}-(\\widehat{w}(a,t)^{T}x_{l}^{l})^{l}\\right)\\right]+\\mathcal{O}\\left(|a|^{L+1}t^{L}\\right)}\\\\ &{\\phantom{\\leq\\displaystyle\\sum_{i=1}^{P}\\left|\\alpha\\mathrm{g}_{i}\\left[w_{i}(a,t)\\right]\\right|\\cdot\\mathcal{O}\\left(|a|^{L+1}t^{L}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then one can conclude (C.5) from Gronwall's inequality ", "page_idx": 21}, {"type": "text", "text": "Even if $\\tilde{w}(\\boldsymbol{a},t)$ approximates $w(a,t)$ using polynomial expansion, the coeficients $Q(t)$ are still very difficult to analyze. Thus, we follow [1] to consider the following dynamics that is obtained by replacing $g(x,t)=f^{*}(x)-f_{\\mathrm{NN}}(x;\\rho_{t})$ by $f^{*}(x)$ in (C.3): ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{w}_{i}(a,t)=\\sum_{1\\leq j\\leq L}\\hat{Q}_{i,j}(t)a^{j},\\quad i\\in\\{1,2,\\ldots,p\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\hat{Q}(t)$ is given by $\\hat{Q}(0)=0$ and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{d}{d t}\\hat{Q}_{i,1}(t)=\\mathbb{E}_{x}[x_{i}f^{*}(x)m_{1}],}\\\\ {\\displaystyle\\frac{d}{d t}\\hat{Q}_{i,j}(t)=\\mathbb{E}_{x}\\left[x_{i}f^{*}(x)\\sum_{l=1}^{L-1}\\frac{m_{l+1}}{l!}\\sum_{1\\le i_{1},\\ldots,i_{l}\\le p}\\sum_{j_{1}+\\cdots+j_{l}=j-1}\\prod_{s=1}^{l}\\hat{Q}_{i_{s},j_{s}}(t)x_{i_{s}}\\right],~~2\\le j\\le L.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similar to $\\tilde{w}(\\boldsymbol{a},t)$ , the linear combination $\\hat{w}(a,t)$ defined as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{w}(a,t)=\\sum_{i=1}^{p}e_{i}\\hat{w}_{i}(a,t)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "is also independent of the orthogonal basis $\\{e_{1},e_{2},\\ldots,e_{p}\\}$ $\\hat{Q}(t)$ can be understood clearly as follows. ", "page_idx": 21}, {"type": "text", "text": "Proposition C.4. For any $i\\,\\in\\,\\{1,2,\\dots,p\\}$ and $j\\,\\in\\,\\{1,2,\\dots,L\\}$ , there exists a constant $\\hat{q}_{i,j}$ depending only on $h^{*}$ and $\\sigma$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{Q}_{i,j}(t)=\\hat{q}_{i,j}t^{j}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. The proof is straightforward by induction on $j$ ", "page_idx": 21}, {"type": "text", "text": "The next proposition quantifies the distance between $\\hat{Q}(t)$ and $Q(t)$ ", "page_idx": 21}, {"type": "text", "text": "Proposition C.5. It holds for any $i\\in\\{1,2,\\ldots,p\\}$ and $j\\in\\{1,2,\\ldots,L\\}$ that ", "page_idx": 21}, {"type": "equation", "text": "$$\n|Q_{i,j}(t)-\\hat{Q}_{i,j}(t)|=\\mathcal{O}(t^{j+1}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We need the following lemma for the proof of Proposition C.5. ", "page_idx": 22}, {"type": "text", "text": "Lemma C.6. It holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}_{x}\\left[\\left|f_{N N}(x;\\rho_{t})\\right|^{2}\\right]\\right)^{1/2}=\\mathcal{O}(t).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Noticing that $f_{\\mathrm{NN}}(x;\\rho_{0})=0$ by the symmetry of $\\rho_{a}$ , one has that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{\\mathrm{NN}}(\\boldsymbol{x};\\boldsymbol{\\rho}_{t})=f_{\\mathrm{NN}}(\\boldsymbol{x};\\boldsymbol{\\rho}_{t})-f_{\\mathrm{NN}}(\\boldsymbol{x};\\boldsymbol{\\rho}_{0})}\\\\ &{\\quad\\quad\\quad\\quad=\\int a\\left(\\sigma(\\boldsymbol{w}(\\boldsymbol{a},t)^{\\top}\\boldsymbol{x})-\\sigma(0)\\right)\\rho_{a}(d a)}\\\\ &{\\quad\\quad\\quad\\quad=\\int a\\left(\\displaystyle\\sum_{l=1}^{L}\\frac{m_{l}}{l!}(\\boldsymbol{w}(\\boldsymbol{a},t)^{\\top}\\boldsymbol{x})^{l}+\\mathcal{O}\\left((\\boldsymbol{w}(\\boldsymbol{a},t)^{\\top}\\boldsymbol{x})^{L+1}\\right)\\right)\\rho_{a}(d a),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and hence by Lemma C.3 and $w_{V}^{\\perp}(a,t)=0$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x}\\left[|f_{\\mathrm{NN}}(x,\\rho_{t})|^{2}\\right]=\\mathcal{O}(t^{2}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which implies (C.12). ", "page_idx": 22}, {"type": "text", "text": "Proof of Proposition C.5. We prove (C.11) by introduction on $j$ .For $j=1$ , one has ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{d}{d t}Q_{i,1}(t)-\\frac{d}{d t}\\hat{Q}_{i,1}(t)\\bigg|=\\left|\\mathbb{E}_{x}[x_{i}f_{\\mathrm{NN}}(x,\\rho_{t})m_{1}]\\right|=\\left|m_{1}\\right|\\left(\\mathbb{E}_{x}[x_{i}^{2}]\\cdot\\mathbb{E}_{x}[|f_{\\mathrm{NN}}(x,\\rho_{t})|^{2}]\\right)^{1/2}=\\mathcal{O}(t),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which leads to $|Q_{i,e_{l}}(t)-\\hat{Q}_{i,e_{l}}(t)|\\,=\\,\\mathcal{O}(t^{2})$ . Then we consider $2\\,\\le\\,j\\,\\le\\,L$ and assume that $|Q_{i,j^{\\prime}}(t)-\\hat{Q}_{i,j^{\\prime}}(t)|=\\mathcal{O}(t^{j^{\\prime}+1})$ holds for $1\\leq j^{\\prime}<j$ . It can be estimated that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\frac{1}{n(n-1)}\\rho_{0}(n,\\vec{\\Delta}_{0},\\vec{\\Delta}_{1},\\vec{\\Delta}_{2},\\vec{\\Delta}_{3},\\vec{\\Delta}_{3},\\vec{\\Delta}_{4})\\right|}\\\\ &{=\\left|\\frac{1}{n(n-1)}\\sum_{i=1}^{n}\\frac{1}{n(n-1)}\\sum_{\\stackrel{k=1}{n},\\cdots,\\stackrel{k=n}{\\longrightarrow}\\partial_{t}\\rho_{1}(n-1)}\\sum_{\\substack{m=1}}^{n}\\prod_{l=1}^{Q}\\rho_{0,k}(\\eta_{\\mu_{k}})\\right|}\\\\ &{\\qquad-\\Xi_{k}\\left[\\tau_{k}(n)\\sum_{i=1}^{n-1}\\frac{1}{n(n-1)}\\sum_{\\substack{m=1}}^{\\infty}\\sum_{\\substack{m=1}}^{\\infty}\\sum_{n=1}^{\\infty}\\prod_{l=1}^{Q}\\rho_{0,k}(\\eta_{\\mu_{k}})\\right]}\\\\ &{=\\left|\\frac{1}{n(n)}\\left[e^{\\sum_{i=1}^{n}(n\\neq1)}\\frac{1}{n(n-1)}\\sum_{\\substack{m=1}}^{\\infty}\\sum_{\\substack{m=1}}^{\\infty}\\sum_{n=1}^{\\infty}\\frac{1}{n(n-1)}\\sum_{\\substack{m=1}}^{\\infty}\\right]\\right|}\\\\ &{\\qquad-\\Xi_{k}\\left[\\tau_{k}(n)e^{\\sum_{i=1}^{n}(n\\neq1)}\\frac{1}{n(n-1)}\\sum_{\\substack{m=1}}^{\\infty}\\sum_{\\substack{m=1}}^{\\infty}\\sum_{n=1}^{\\infty}\\left(\\prod_{l=1}^{Q}\\rho_{0,k}(\\eta_{\\mu_{k}})\\right)x_{\\mu_{k}}\\right]}\\\\ &{=\\left|\\frac{1}{n(n)}\\left[e^{\\sum_{i=1}^{n}(n\\neq1)}\\frac{1}{n(n-1)}\\sum_{\\substack{m=1}}^{\\infty}\\sum_{\\substack{m=1}}^{\\infty}\\sum_{n=1}^{\\infty}\\frac{1}{n(n-1)}\\sum_{\\substack{m=1}}^{\\infty}\\right]\\right|}\\\\ &{\\qquad-\\Xi_{k}\\left[\\tau_{k}(n)e^{\\sum_{i=1}^{n}(n\\neq1)}\\frac{1}{n(n-\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{=\\mathcal{O}(t^{j}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we used Lemma C.6. Then one can conclude (C.11) ", "page_idx": 22}, {"type": "text", "text": "C.2 From Linear Independence to Algebraic Independence ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We prove Theorem 4.5 in this subsection. ", "page_idx": 23}, {"type": "text", "text": "Definition C.7 (Algebraic independence). Let $v_{1},v_{2},\\ldots,v_{m}\\in\\mathbb{R}[a_{1},a_{2},\\ldots,a_{p}]$ bepolynomials in $a_{1},a_{2},\\dotsc,a_{p}$ mWe say that $v_{1},v_{2},\\ldots,v_{m}$ are algebraically independent if for any nonzero polynomial $F:\\mathbf{\\bar{R}}^{m}\\rightarrow\\mathbb{R}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\nF(v_{1}(a_{1},a_{2},\\ldots,a_{p}),\\ldots,v_{m}(a_{1},a_{2},\\ldots,a_{p}))\\neq0\\in\\mathbb{R}[a_{1},a_{2},\\ldots,a_{p}].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma C.8. $I f\\,v_{1},v_{2},\\ldots,v_{p}\\in\\mathbb{R}[a]$ are $\\mathbb{R}$ -linearly independent, then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{det}{\\left(\\begin{array}{c c c c}{v_{1}(a_{1})}&{v_{1}(a_{2})}&{\\cdot\\cdot\\cdot}&{v_{1}(a_{p})}\\\\ {v_{2}(a_{1})}&{v_{2}(a_{2})}&{\\cdot\\cdot\\cdot}&{v_{2}(a_{p})}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {v_{p}(a_{1})}&{v_{p}(a_{2})}&{\\cdot\\cdot\\cdot}&{v_{p}(a_{p})}\\end{array}\\right)}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "is a non-zero polynomial in $\\mathbb{R}[a_{1},a_{2},\\ldots,a_{p}]$ ", "page_idx": 23}, {"type": "text", "text": "Proof. Let $n_{i}$ be the smallest degree of nonzero monomials of $v_{i}$ and let $c_{i}$ be the accociated coefficient for $i=1,2,\\dots,p$ .Without loss of generality, we assume that $n_{0}\\,<\\,n_{1}\\,<\\,\\cdot\\,\\cdot\\,<\\,n_{p}$ (otherwise one can perform some row reductions or row permutations). The polynomial defined in (C.13) consists of monomials of degree at least $n_{1}+n_{2}+\\cdots+n_{p}$ . So it suffices to prove that the sum of monomials with degree being $n_{1}+n_{2}+\\cdots+n_{p}$ is nonzero, which is true since ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{det}{\\left(\\begin{array}{c c c c}{c_{1}a_{1}^{n_{1}}}&{c_{1}a_{2}^{n_{1}}}&{\\cdot\\cdot\\cdot}&{c_{1}a_{p}^{n_{1}}}\\\\ {c_{2}a_{1}^{n_{2}}}&{c_{2}a_{2}^{n_{2}}}&{\\cdot\\cdot\\cdot}&{c_{2}a_{p}^{n_{2}}}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {c_{p}a_{1}^{n_{p}}}&{c_{p}a_{2}^{n_{p}}}&{\\cdot\\cdot\\cdot}&{c_{p}a_{p}^{n_{p}}}\\end{array}\\right)}=c_{1}c_{2}\\cdot\\cdot c_{p}\\cdot\\operatorname*{det}{\\left(\\begin{array}{c c c c}{a_{1}^{n_{1}}}&{a_{2}^{n_{1}}}&{\\cdot\\cdot}&{a_{p}^{n_{1}}}\\\\ {a_{1}^{n_{2}}}&{a_{2}^{n_{2}}}&{\\cdot\\cdot}&{a_{p}^{n_{2}}}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {a_{1}^{n_{p}}}&{a_{2}^{n_{p}}}&{\\cdot\\cdot}&{a_{p}^{n_{p}}}\\end{array}\\right)}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "is nonzero as a generalized Vandermonde matrix. ", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 4.5. Since $v_{1},v_{2},\\ldots,v_{p}\\in\\mathbb{R}[a]$ be $\\mathbb{R}$ -linearly independent with the constant terms being zero, we can see that $v_{1}^{\\prime},v_{2}^{\\prime},\\ldots,v_{p}^{\\prime}\\in\\mathbb{R}[a]$ are also $\\mathbb{R}$ linearly independent. Noticing that ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\frac{\\partial}{\\partial a_{j}}}\\left({\\frac{1}{p}}(v_{i}(a_{1})+v_{i}(a_{2})+\\cdot\\cdot\\cdot+v_{i}(a_{p}))\\right)={\\frac{1}{p}}v_{i}^{\\prime}(a_{j}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "one can conclude that $\\begin{array}{r}{\\frac{1}{p}(v_{1}(a_{1})+\\cdot\\cdot\\cdot+v_{1}(a_{p})),\\ldots,\\frac{1}{p}(v_{p}(a_{1})+\\cdot\\cdot\\cdot+v_{p}(a_{p}))\\in\\mathbb{R}[a_{1},\\ldots,a_{p}]}\\end{array}$ are $\\mathbb{R}$ -algebraically independent by using Theorem 4.6 and Lemma C.8. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "C.3Proofs of Proposition 4.4 and Theorem 4.3 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Some ideas in this subsection are from [1], but the proofs are significantly different since we need to show the algebraic independence to obtain a non-degenerate kernel, as discussed in Section 4. ", "page_idx": 23}, {"type": "text", "text": "Lemma C.9. Suppose that Assumption 4.1 holds with $s\\in\\mathbb{N}_{+}$ .There exists some orthonormal basis $\\{e_{1},e_{2},\\ldots,e_{p}\\}$ of $V$ such that the coeficients $\\hat{q}_{i,j},1\\leq i\\leq p,1\\leq j\\leq L$ in (C.10) satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\ns_{1}<s_{2}<\\cdots<s_{p}\\leq s,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ", "page_idx": 23}, {"type": "equation", "text": "$$\ns_{i}=\\operatorname*{min}\\{j:\\hat{q}_{i,j}\\neq0\\},\\quad i=1,2,\\ldots,p.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof.Let $\\hat{w}_{V}(t)$ be the dynamics defined in (4.1). It can be seen that the Taylor's expansion of $\\hat{w}_{V}(t)$ at $t=0$ upto $s$ -th order is given by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{p}\\sum_{j=1}^{s}e_{i}\\hat{Q}_{i,j}(t)=\\sum_{i=1}^{p}\\sum_{j=1}^{s}e_{i}\\hat{q}_{i,j}t^{j},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Where $\\hat{Q}_{i,j}$ and $\\hat{q}_{i,j}$ are as in (C.9) and (C.5). According to Assumption 4.1, the matrix $(\\hat{q}_{i,j})_{1\\leq i\\leq p,1\\leq j\\leq s}$ is of full-row-rank. One can thus perform the QR decomposition, or equivalently choose some orthogonal basis, to obtain (C.14). \u53e3 ", "page_idx": 23}, {"type": "text", "text": "In the rest of this subsection, we will always denote $\\mathbf{s}=(s_{1},s_{2},\\dots,s_{P})$ and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{u(a_{1},\\ldots,a_{p},t)=\\displaystyle\\frac{1}{p}(w(a_{1},t)+w(a_{2},t)+\\cdots+w(a_{p},t)),}}\\\\ {{\\tilde{u}(a_{1},\\ldots,a_{p},t)=\\displaystyle\\frac{1}{p}(\\tilde{w}(a_{1},t)+\\tilde{w}(a_{2},t)+\\cdots+\\tilde{w}(a_{p},t)),}}\\\\ {{\\hat{u}(a_{1},\\ldots,a_{p},t)=\\displaystyle\\frac{1}{p}(\\hat{w}(a_{1},t)+\\hat{w}(a_{2},t)+\\cdots+\\hat{w}(a_{p},t)),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for $t\\in[0,T]$ , where $w(a,t),\\,\\tilde{w}(a,t)$ , and $\\hat{w}(a,t)$ are defined in (C.1), (C.4), and (C.8), respectively. Recall that $p_{1},p_{2},...,p_{\\left({}^{n+p}\\right)}$ are theorthonormal basis of $\\mathbb{P}_{V,n}$ with input $z\\sim\\mathcal{N}(0,I_{V})$ where $\\mathbb{P}_{V,n}$ is the collection of all polynomials on $V$ with degree at most $n\\,=\\,\\deg(h^{*})\\,=\\,\\deg(f^{*})$ Proposition 4.4 aims to bound from below the smallest eigenvalue of the kernel matrix (4.2) whose definition is restated as follows ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\boldsymbol{K}_{i_{1},i_{2}}(t)=\\mathbb{E}_{a_{1},\\ldots,a_{p}}\\left[\\mathbb{E}_{z,z^{\\prime}}\\left[p_{i_{1}}(z)\\hat{\\sigma}(u(a_{1},\\ldots,a_{p},t)^{\\top}z)\\hat{\\sigma}(u(a_{1},\\ldots,a_{p},t)^{\\top}z^{\\prime})p_{i_{2}}(z^{\\prime})\\right]\\right],\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where ${\\hat{\\sigma}}(\\xi)=(1+\\xi)^{n}$ \uff0c $(a_{1},\\dotsc...,a_{p})\\sim\\mathcal{U}([-1,1]^{p})$ and $1\\leq i_{1},i_{2}\\leq{\\binom{n+p}{p}}$ To do this, we definethree ${\\binom{n+p}{p}}\\,\\times\\,{\\binom{n+p}{p}}$ matrces", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{K}_{i_{1},i_{2}}(t)=\\mathbb{E}_{a_{1},\\ldots,a_{p}}\\left[\\mathbb{E}_{z,z^{\\prime}}\\left[p_{i_{1}}(z)\\hat{\\sigma}(\\tilde{u}(a_{1},\\ldots,a_{p},t)^{\\top}z)\\hat{\\sigma}(\\tilde{u}(a_{1},\\ldots,a_{p},t)^{\\top}z^{\\prime})p_{i_{2}}(z^{\\prime})\\right]\\right],\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{M}_{i_{1},i_{2}}(\\mathbf{a},t)=\\mathbb{E}_{z}\\left[p_{i_{1}}(z)\\hat{\\sigma}(\\tilde{u}(\\mathbf{a}_{i_{2}},t)^{\\top}z)\\right],}\\\\ {\\hat{M}_{i_{1},i_{2}}(\\mathbf{a},t)=\\mathbb{E}_{z}\\left[p_{i_{1}}(z)\\hat{\\sigma}(\\hat{u}(\\mathbf{a}_{i_{2}},t)^{\\top}z)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\mathbf{a}=\\left(\\mathbf{a}_{1},\\mathbf{a}_{2},\\ldots,\\mathbf{a}_{\\binom{n+p}{p}}\\right)$ and $\\mathbf{a}_{i}\\in\\mathbb{R}^{p}$ for $i=1,2,\\ldots,{\\binom{n+p}{p}}$ ", "page_idx": 24}, {"type": "text", "text": "Lemma C.10. It holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{det}(\\hat{M}({\\bf a},t))=\\sum_{i=1}^{\\binom{n+p}{p}}\\sum_{0\\leq\\|\\mathbf{j}_{i}\\|_{1}\\leq L n}\\hat{h}_{\\mathbf{j}}t^{\\|\\mathbf{j}\\|_{1}}{\\bf a}^{\\mathbf{j}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "$\\mathbf{j}=\\left(\\mathbf{j}_{1},\\mathbf{j}_{2},\\ldots,\\mathbf{j}_{\\binom{n+p}{p}}\\right)w i t h\\,\\mathbf{j}_{i}\\in\\mathbb{N}^{p}$ for $i=1,2,\\ldots,{\\binom{n+p}{p}},$ $\\hat{h}_{\\mathbf{j}}$ is a constant depending on $h^{*}$ and $\\sigma$ ,and $\\mathbf{a}^{\\mathbf{j}}$ represents the product of entrywise powers. ", "page_idx": 24}, {"type": "text", "text": "Proof. The result follows directly from Proposition C.4. ", "page_idx": 24}, {"type": "text", "text": "Lemma C.11. It holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{det}(\\tilde{M}({\\mathbf{a}},t))=\\sum_{i=1}^{\\binom{n+p}{p}}\\sum_{0\\leq\\|\\mathbf{j}_{i}\\|_{1}\\leq L n}\\tilde{h}_{\\mathbf{j}}(t){\\mathbf{a}}^{\\mathbf{j}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{h}_{\\mathbf{j}}(t)=\\hat{h}_{\\mathbf{j}}t^{||\\mathbf{j}||_{1}}+\\mathcal{O}(t^{||\\mathbf{j}||_{1}+1}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. The result follows directly from Proposition C.4 and Proposition C.5. ", "page_idx": 24}, {"type": "text", "text": "Lemma C.12. For any $m\\in\\mathbb{N}$ one has that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname{span}\\left\\{(q^{\\top}z)^{m}:q\\in V\\right\\}=\\mathbb{P}_{V,m}^{h},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\mathbb{P}_{V,m}^{h}$ is the collection of all homogeneous polynomials in $z\\in V$ with degree m. ", "page_idx": 24}, {"type": "text", "text": "Proof. Without loss of generality, we assume that $V=\\mathbb{R}^{p}$ and prove the result by induction on $p$ (C.16) is clearly true for $p=1$ . Then we consider $p\\geq2$ and assume that (C.16) holds for $p-1$ and any $m$ ", "page_idx": 24}, {"type": "text", "text": "For any $q,z\\in\\mathbb{R}^{p}$ , we denote that $\\bar{q}=(q_{2},\\ldots,q_{p})$ and $\\bar{z}=(z_{2},\\ldots,z_{p})$ . Let $t_{0},t_{1},\\ldots,t_{m}\\in\\mathbb{R}$ be distinct. Then it follows from the invertibility of $(t_{i}^{j})_{0\\leq i,j\\leq m}$ and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left((t_{0}z_{1}+\\bar{q}^{\\top}\\bar{z})^{m}\\right)}\\\\ &{\\left((t_{1}z_{1}+\\bar{q}^{\\top}\\bar{z})^{m}\\right)=\\left(\\begin{array}{c}{\\sum_{i=0}^{m}\\binom{m}{i}t_{0}^{i}z_{1}^{i}(\\bar{q}^{\\top}\\bar{z})^{m-i}}\\\\ {\\sum_{i=0}^{m}\\binom{m}{i}t_{1}^{i}z_{1}^{i}(\\bar{q}^{\\top}\\bar{z})^{m-i}}\\\\ {\\vdots}\\\\ {\\sum_{i=0}^{m}\\binom{m}{i}t_{m}^{i}z_{1}^{i}(\\bar{q}^{\\top}\\bar{z})^{m-i}}\\end{array}\\right)}\\\\ &{=\\left(\\begin{array}{c c c c}{1}&{t_{0}}&{\\cdots}&{t_{0}^{m}}\\\\ {1}&{t_{1}}&{\\cdots}&{t_{1}^{m}}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {1}&{t_{m}}&{\\cdots}&{t_{m}^{m}}\\end{array}\\right)\\left(\\begin{array}{c}{\\binom{m}{0}(\\bar{q}^{\\top}\\bar{z})^{m}}\\\\ {\\binom{m}{1}z_{1}(\\bar{q}^{\\top}\\bar{z})^{m-1}}\\\\ {\\vdots}\\\\ {\\binom{m}{m}z_{1}^{m}}\\end{array}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "that ", "page_idx": 25}, {"type": "equation", "text": "$$\nz_{1}^{i}(\\bar{q}^{\\top}\\bar{z})^{m-i}\\in\\mathrm{span}\\left\\{(r^{\\top}z)^{m}:r\\in\\mathbb{R}^{p}\\right\\},\\quad\\forall\\,\\bar{q}\\in\\mathbb{R}^{p-1},\\,i\\in\\{0,1,\\ldots,m\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then using the induction hypothesis that (C.16) is true for $p-1$ and $m-i,i=0,1,\\dotsc,m$ , one can conclude that (C.16) is also true for $p$ and $m$ \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Lemma C.13. For ${\\hat{\\sigma}}(\\xi)=(1+\\xi)^{n}$ , one has that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname{span}\\left\\{\\hat{\\sigma}(q^{\\top}z):q\\in V\\right\\}=\\mathbb{P}_{V,n}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. One can still assume that $V=\\mathbb{R}^{p}$ without loss of generality. Consider any $q\\in\\mathbb{R}^{p}$ and any distinct $t_{0},t_{1},\\ldots,t_{n}\\in\\mathbb{R}$ .Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\!\\!\\begin{array}{c}{\\!\\hat{\\sigma}((t_{0}q)^{\\top}z)\\!}\\\\ {\\!\\hat{\\sigma}((t_{1}q)^{\\top}z)\\!}\\\\ {\\!\\vdots\\!\\!}\\\\ {\\!\\hat{\\sigma}((t_{n}q)^{\\top}z)\\!}\\end{array}\\!\\right)=\\left(\\!\\!\\begin{array}{c}{\\!\\sum_{i=0}^{n}\\binom{n}{i}t_{0}^{i}(q^{\\top}z)^{i}}\\\\ {\\!\\sum_{i=0}^{n}\\binom{n}{i}t_{1}^{i}(q^{\\top}z)^{i}}\\\\ {\\!\\vdots\\!\\!}\\\\ {\\!\\sum_{i=0}^{n}\\binom{n}{i}t_{0}^{i}(q^{\\top}z)^{i}}\\end{array}\\!\\right)=\\left(\\!\\!\\begin{array}{c c c c}{\\!1}&{t_{0}}&{\\cdots}&{t_{0}^{n}}\\\\ {\\!1}&{t_{1}}&{\\cdots}&{t_{1}^{n}}\\\\ {\\!\\vdots\\!\\!}&{\\!\\vdots\\!\\!}&{\\!\\ddots}&{\\vdots\\!}\\\\ {\\!1}&{t_{n}}&{\\cdots}&{t_{n}^{n}}\\end{array}\\!\\!\\right)\\left(\\!\\!\\begin{array}{c}{\\!\\binom{n}{0}\\!\\!}\\\\ {\\!\\binom{n}{1}q^{\\top}z}\\\\ {\\!\\vdots\\!\\!}\\\\ {\\!\\binom{n}{n}\\!\\!}\\end{array}\\!\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that the matrix $(t_{i}^{j})_{0\\leq i,j\\leq n}$ is invertible when $t_{0},t_{1},\\ldots,t_{N}$ are distinct. Therefore, one can conclude that ", "page_idx": 25}, {"type": "equation", "text": "$$\n(q^{\\top}z)^{i}\\in\\mathrm{span}\\left\\{\\sigma(r^{\\top}z):r\\in\\mathbb{R}^{p}\\right\\},\\quad\\forall\\,q\\in\\mathbb{R}^{p},\\,0\\leq i\\leq n,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which combined with Lemma C.12 implies that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\mathbb P}_{V,n}\\supset\\operatorname{span}\\left\\{\\hat{\\sigma}(q^{\\top}z):q\\in{\\mathbb R}^{p}\\right\\}\\supset{\\mathbb P}_{V,0}^{h}\\oplus{\\mathbb P}_{V,1}^{h}\\oplus\\cdot\\cdot\\oplus{\\mathbb P}_{V,n}^{h}={\\mathbb P}_{V,n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which completes the proof. ", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma 4.7. We prove the result by induction. When $m=1$ , the result follows directly from the $\\mathbb{R}$ -algebraic independence of $v_{1},v_{2},\\ldots,v_{p}$ . Now we assume that the result is true for $m-1$ and consider the case of $m$ . Suppose that ", "page_idx": 25}, {"type": "equation", "text": "$$\n0=\\sum_{\\mathbf{j}_{1},\\dots,\\mathbf{j}_{m}}X_{\\mathbf{j}}\\prod_{l=1}^{m}\\mathbf{v}(\\mathbf{a}_{l})^{\\mathbf{j}_{l}}=\\sum_{\\mathbf{j}_{m}}\\left(\\sum_{\\mathbf{j}_{1},\\dots,\\mathbf{j}_{m-1}}X_{\\mathbf{j}}\\prod_{l=1}^{m-1}\\mathbf{v}(\\mathbf{a}_{l})^{\\mathbf{j}_{l}}\\right)\\mathbf{v}(\\mathbf{a}_{m})^{\\mathbf{j}_{m}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By the $\\mathbb{R}$ -algebraic independence of $v_{1},v_{2},\\ldots,v_{p}$ , one must have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{j}_{1}...,\\mathbf{j}_{m-1}}X_{\\mathbf{j}}\\prod_{l=1}^{m-1}\\mathbf{v}(\\mathbf{a}_{l})^{\\mathbf{j}_{l}}=0,\\quad\\forall\\,\\mathbf{j}_{m},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which then leads to $X_{\\mathbf{j}}=0,\\,\\forall\\,\\mathbf{j}$ by the induction hypothesis. ", "page_idx": 25}, {"type": "text", "text": "Lemma C.14. Suppose that Assumption 4.1 and 4.2 hold and let $\\hat{h}_{\\mathbf{j}}$ be the coeffcient of det $\\hat{(M}({\\mathbf{a}},t))$ in (C.15). Then therexists some $\\hat{\\mathbf{j}}=\\left(\\hat{\\mathbf{j}}_{1},\\hat{\\mathbf{j}}_{2},\\ldots,\\hat{\\mathbf{j}}_{\\binom{n+p}{p}}\\right)w i t h\\parallel\\hat{\\mathbf{j}}\\parallel_{1}\\leq s n{\\binom{n+p}{p}},$ suchthat $\\hat{h}_{\\hat{\\bf j}}\\neq0$ ", "page_idx": 25}, {"type": "text", "text": "Proof. Let $X(\\mathbf{q})\\in\\mathbb{R}^{{\\binom{n+p}{p}}\\times{\\binom{n+p}{p}}}$ be defined as ", "page_idx": 26}, {"type": "equation", "text": "$$\nX_{i_{1},i_{2}}(\\mathbf{q})=\\mathbb{E}_{z}\\left[p_{i_{1}}(z)\\sigma(\\mathbf{q}_{i_{2}}^{\\top}z)\\right],\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\mathbf{q}=\\left(\\mathbf{q}_{1},\\mathbf{q}_{2},\\ldots,\\mathbf{q}_{\\binom{n+p}{p}}\\right)$ with $\\mathbf q_{i}\\in V$ . Then $\\operatorname*{det}(X(\\mathbf{q}))$ is a polynomial in $\\mathbf{q}$ of the form ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{det}(X(\\mathbf{q}))=\\sum_{i=1}^{\\binom{n+p}{p}}\\sum_{0\\leq\\|\\mathbf{j}_{i}\\|_{1}\\leq n}X_{\\mathbf{j}}\\mathbf{q}^{\\mathbf{j}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we understand $\\mathbf{q}_{i}$ as a (coefficient) vector in $\\mathbb{R}^{p}$ associated with a fixed orthonormal basis $\\{e_{1},e_{2},\\ldots,e_{p}\\}$ $V$ $\\mathbf{q}$ $\\sigma(\\mathbf{q}_{1}^{\\top}z),\\sigma(\\mathbf{q}_{2}^{\\top}z),\\ldots,\\sigma(\\mathbf{q}_{{\\binom{n+p}{p}}}^{\\top}z)$ $\\mathbb{P}_{V,n}$ $\\operatorname*{det}(X(\\mathbf{q}))\\neq0$ q. Thus, (C.17) is a non-zero polynomial in q. Let $\\mathbf{s}=(s_{1},s_{2},\\ldots,s_{p})$ collect all indices $s_{i}$ from Lemma C.9. Denote ", "page_idx": 26}, {"type": "equation", "text": "$$\nS=\\operatorname*{min}\\left\\{\\sum_{l=1}^{\\binom{n+p}{p}}{\\mathbf s}^{\\top}{\\mathbf j}_{l}:X_{\\mathbf j}\\neq0,\\;0\\leq\\|{\\mathbf j}_{i}\\|\\leq n,\\;1\\leq i\\leq{\\binom{n+p}{p}}\\right\\},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and ", "page_idx": 26}, {"type": "equation", "text": "$$\nJ_{S}=\\left\\{\\mathbf{j}:\\sum_{l=1}^{\\binom{n+p}{p}}\\mathbf{s}^{\\top}\\mathbf{j}_{l}=S,\\;X_{\\mathbf{j}}\\neq0,\\;0\\leq\\|\\mathbf{j}_{i}\\|\\leq n,\\;1\\leq i\\leq\\binom{n+p}{p}\\right\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{det}(\\hat{M}({\\mathbf a},t))=\\operatorname*{det}\\left(X\\left(\\hat{u}({\\mathbf a}_{1},t),\\hat{u}({\\mathbf a}_{2},t),\\dots,\\hat{u}({\\mathbf a}_{{\\binom{n+p}{p}}},t)\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\sum_{i=1}^{\\binom{n+p}{p}}\\sum_{0\\leq\\|\\mathbf{j}_{i}\\|_{1}\\leq n}X_{\\mathbf{j}}\\prod_{l=1}^{\\binom{n+p}{p}}\\hat{u}({\\mathbf a}_{l},t)^{\\hat{{\\mathbf a}}_{l}}}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{\\mathbf{j}\\in J_{S}}X_{\\mathbf{j}}\\prod_{l=1}^{\\binom{n+p}{p}}\\hat{u}_{\\mathbf{s}}({\\mathbf a}_{l},t)^{\\hat{{\\mathbf a}}_{l}}+\\mathcal{O}(t^{S+1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\hat{u}_{\\mathbf{s}}(\\mathbf{a}_{l},t)=\\left(\\hat{u}_{2,s_{1}}(\\mathbf{a}_{l},t),\\dots,\\hat{u}_{p,s_{p}}(\\mathbf{a}_{l},t)\\right)$ and $\\begin{array}{r}{\\hat{u}_{i,s_{i}}(a_{1},\\dots,a_{p},t)=\\frac{1}{p}\\sum_{k=1}^{p}\\hat{q}_{i,s_{i}}t^{s_{i}}a_{k}^{s_{i}}}\\end{array}$ collects the leading order terms of $\\hat{\\boldsymbol u}_{i}(a_{1},\\ldots,a_{p},t)$ . According to Assumption C.9, Theorem 4.5, and Lemma 4.7, we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{j}\\in J_{S}}X_{\\mathbf{j}}\\prod_{l=1}^{\\binom{n+p}{p}}\\hat{u}_{\\mathbf{s}}(\\mathbf{a}_{l},t)^{\\mathbf{j}_{l}}\\neq0,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which provides at least one non-zero term in $\\operatorname*{det}(\\hat{M}({\\mathbf{a}},t))$ whose degree in $t$ ", "page_idx": 26}, {"type": "equation", "text": "$$\nS\\leq s n{\\binom{n+p}{p}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This completes the proof. ", "page_idx": 26}, {"type": "text", "text": "Proof of Proposition 4.4. According to Lemma C.14, there exist some $\\hat{\\bf j}$ with $\\|\\hat{\\mathbf{j}}\\|_{1}\\leq s n\\binom{n+p}{p}$ such that $\\hat{h}_{\\hat{\\bf j}}\\neq0$ . According to Lemma C.11 and Lemma 103 in [1], it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{a}}\\left[\\left(\\operatorname*{det}(\\tilde{M}(\\mathbf{a},t))\\right)^{2}\\right]\\ge C_{1}\\left|\\tilde{h}_{\\hat{\\mathbf{j}}}(t)\\right|^{2}=C_{1}\\left|\\hat{h}_{\\hat{\\mathbf{j}}}t^{\\Vert\\hat{\\mathbf{j}}\\Vert_{1}}+\\mathcal{O}(t^{\\Vert\\hat{\\mathbf{j}}\\Vert_{1}+1})\\right|^{2}\\ge\\frac{C_{1}}{2}\\left|\\hat{h}_{\\hat{\\mathbf{j}}}\\right|t^{2\\Vert\\hat{\\mathbf{j}}\\Vert_{1}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\mathbf{a}\\sim\\mathcal{U}\\left(([-1,1]^{p})^{\\binom{n+p}{p}}\\right)$ for some constant $C_{1}$ depending only on $n,p,s$ and for suficiently small $t$ . It can be seen that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{K}}(t)=\\frac{1}{\\binom{n+p}{p}}\\mathbb{E}_{\\mathbf{a}}\\left[\\tilde{M}(\\mathbf{a},t)\\tilde{M}(\\mathbf{a},t)^{\\top}\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By Jensen's inequality, one has that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(\\tilde{K}(t))\\geq\\frac{1}{\\binom{n+p}{p}}\\mathbb{E}_{\\mathbf{a}}\\left[\\lambda_{\\operatorname*{min}}\\left(\\tilde{M}(\\mathbf{a},t)\\tilde{M}(\\mathbf{a},t)^{\\top}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since entries of $\\tilde{M}({\\boldsymbol{a}},t)$ are all $\\mathcal{O}(1)$ by Lemma C.2, which implies the boundedness of the eigenvalues $\\lambda_{i}\\left(\\tilde{M}(\\mathbf{a},t)\\tilde{M}(\\mathbf{a},t)^{\\top}\\right),i=1,2,\\ldots,{\\binom{n+p}{p}}$ , one has that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left(\\operatorname*{det}(\\tilde{M}({\\mathbf a},t))\\right)^{2}=\\operatorname*{det}\\left(\\tilde{M}({\\mathbf a},t)\\tilde{M}({\\mathbf a},t)^{\\top}\\right)\\leq C_{2}\\lambda_{\\operatorname*{min}}\\left(\\tilde{M}({\\mathbf a},t)\\tilde{M}({\\mathbf a},t)^{\\top}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining (C.18), (C.19), and (C.20), one can conclude that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(\\tilde{K}(t))\\geq\\frac{C_{1}}{2}\\left|\\hat{h}_{\\hat{\\mathbf{j}}}(\\alpha,m)\\right|t^{2\\|\\hat{\\mathbf{j}}\\|_{1}}\\geq\\frac{C_{1}}{2}\\left|\\hat{h}_{\\hat{\\mathbf{j}}}(\\alpha,m)\\right|t^{2s n\\binom{n+p}{p}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we used $\\begin{array}{r}{\\|\\hat{\\mathbf{j}}\\|\\leq N\\binom{n+p}{p}\\operatorname*{max}_{i\\in I}s_{i}}\\end{array}$ and only considered $t\\leq1$ .By Proposition C.1, we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|\\lambda_{\\operatorname*{min}}(K(t))-\\lambda_{\\operatorname*{min}}(\\tilde{\\mathcal{K}}(t))\\right|=\\mathcal{O}(t^{L+1}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then we can obtain (4.3) as we set ${\\cal L}=2s n\\binom{n+p}{p}$ ", "page_idx": 27}, {"type": "text", "text": "Proof of Theorem 4.3. Let $C,T$ be the constants in Proposition 4.4 and consider $t>T$ . It follows from Theorem 3.5 that $w_{V}^{\\perp}(a,T)=0$ , which leads to that $u_{V}^{\\perp}(a_{1},\\ldots,a_{p},T)=0$ and hence that $f_{\\mathrm{NN}}(x;\\rho_{t})$ and $g(x,t)$ only depend on $x_{V}$ for all $t\\,\\geq\\,T$ . Define $\\mathbf{g}(t)~\\in~\\mathbb{R}^{\\binom{n+p}{p}}$ via ${\\bf g}_{i}(t)\\;=\\;$ $\\mathbb{E}_{x}[g(x,t)p_{i}(x_{V})]$ . Then according to (2.6) and (4.3), one has for $t>T$ that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{d}{d t}\\mathcal{E}(\\rho_{t})=-\\sum_{i=1}^{P}\\mathbb{E}_{a}\\mathbb{E}_{x,x^{\\prime}}\\left[g_{i}(x,t)\\sigma(w(a,T)^{\\top}x)\\sigma(w(a,T)^{\\top}x^{\\prime})g_{i}(x^{\\prime},t)\\right]}\\\\ &{\\quad\\quad\\quad=-\\;\\mathbf{g}(t)^{\\top}K(T)\\mathbf{g}(t)\\leq-\\lambda_{\\operatorname*{min}}(K(T))\\|\\mathbf{g}(t)\\|^{2}}\\\\ &{\\quad\\quad\\quad=-\\;2\\lambda_{\\operatorname*{min}}(K(T))\\mathcal{E}(\\rho_{t})\\leq-2C T^{2s n\\binom{n+p}{p}}\\mathcal{E}(\\rho_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which implies that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}(\\rho_{t})\\leq\\mathcal{E}(\\rho_{T})\\exp\\left(-2C T^{2s n\\binom{n+p}{p}}(t-T)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "by Gronwall's inequality. Then we obtain the desired exponential decay property by noticing that $\\begin{array}{r}{\\dot{\\mathcal{E}(\\rho_{T})}=\\frac{1}{2}\\mathbb{E}_{z}[\\|h^{*}\\dot{(z)}\\|^{2}]}\\end{array}$ \u53e3 ", "page_idx": 27}, {"type": "text", "text": "D  Further Discussion and Characterization of Assumption 4.1 and Theorem 4.3 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "D.1  Verification of Assumption 4.1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We provide some verification or characterization of Assumption 4.1. Without loss of generality, we fix an orthonormal basis and $V$ and view that $V=\\mathbb{R}^{p}$ . The results in this subsection are independent of the choice of the orthonormal basis. ", "page_idx": 27}, {"type": "text", "text": "It has been discussed in Section 4.1 that if $\\sigma\\in{\\mathcal{C}}^{s}(\\mathbb{R})$ with $s=2^{p-1}$ and $\\sigma^{(1)}(0),\\sigma^{(2)}(0),\\ldots,\\sigma^{(p)}(0)$ are all nonzero, then Assumption 4.1 can be verified with $s$ for $h^{\\ast}(z)=z_{1}+z_{1}z_{2}+\\cdot\\cdot\\cdot+z_{1}z_{2}\\cdot\\cdot\\cdot z_{p}$ using the calculation in [1, Proposition 33]. Similarly, the same result is also true for $h^{*}(z)=$ $c_{1}z_{1}+c_{2}z_{1}z_{2}+\\cdot\\cdot\\cdot+c_{p}z_{1}z_{2}\\cdot\\cdot\\cdot z_{p}$ if $c_{1},c_{2},\\ldots,c_{p}$ are nonzero. More generally, we have the following. ", "page_idx": 27}, {"type": "text", "text": "Proposition D.1. Let $\\{\\alpha_{1},\\alpha_{2},\\dots,\\alpha_{m}\\}$ be a set of pairwise distinct elements in ${\\mathbb{N}}^{p}$ that contains $.,0,\\ldots,0),(1,1,0,\\ldots,0),\\ldots,(1,1,\\ldots,1)$ .If $\\sigma\\,\\,\\,\\in\\,\\,\\,{\\mathcal{C}}^{s}(\\mathbb{R})$ with $\\begin{array}{r l r}{s}&{{}=}&{2^{p-1}}\\end{array}$ and $\\sigma^{(1)}(0),\\sigma^{(2)}(0),\\ldots,\\sigma^{(p)}(0)$ are all nonzero, then ", "page_idx": 27}, {"type": "equation", "text": "$$\nh^{\\ast}(z)=\\sum_{i=1}^{m}c_{i}\\prod_{j=1}^{p}\\operatorname{He}_{\\alpha_{i}(j)}(z_{j}),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "satisfies Assumption 4.1 with s unless $(c_{1},c_{2},\\ldots,c_{m})$ is in some measure-zero subset of $\\mathbb{R}^{m}$ with respect to the Lebesgue measure. ", "page_idx": 27}, {"type": "text", "text": "Proof. We assume that $\\alpha_{1}=(1,0,\\ldots,0),\\alpha_{2}=(1,1,0,\\ldots,0),\\ldots,\\alpha_{m}=(1,1,\\ldots,1)$ . As in the proof of Lemma C.9, Assumption 4. 1 is true with $s$ if and only if the matrix $\\hat{q}:=(\\hat{q}_{i,j})_{1\\le i\\le p,1\\le j\\le s}$ is of full-row-rank, i.e., $\\operatorname*{det}({\\hat{q}}{\\hat{q}}^{\\top})\\neq0$ . By (C.9), each entry in $\\hat{q}$ is a polynomial in $(c_{1},c_{2},\\ldots,c_{m})$ which implies that $\\operatorname*{det}({\\hat{q}}{\\hat{q}}^{\\top})$ is also a polynomial in $(c_{1},c_{2},\\ldots,c_{m})$ . This polynomial is nonzero since it takes nonzero value at $(1,1,...,1,0,...,0)$ with the $p$ entries being 1 and all other entries being 0, which is because that Assumption 4.1 is true for $h^{\\ast}(z)=z_{1}+z_{1}z_{2}+\\cdot\\cdot\\cdot+z_{1}z_{2}\\cdot\\cdot\\cdot z_{p}$ Finally, the conclusion of Proposition D.1 is true since the set of roots of a nonzero polynomial is of measure zero with respect to the Lebesgue measure. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "D.2Discretization Results Implied by Theorem 4.3 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The discussion in this subsection is similar to those in Appendix B.2. We slightly modified the fow $\\rho_{t}$ generated by Algorithm 1 to guarantee some boundedness conditions, and then use the standard dimension-free estimate [19] to derive a sample complexity result implied by Theorem 4.3. ", "page_idx": 28}, {"type": "text", "text": "We use the same bounded modification $\\tilde{f}^{*}$ as in (B.3) for a given constant $C_{f}^{b}~>~0$ Similarly, for any $\\delta\\:>\\:0$ and $C_{w}\\ >\\ 0$ , there exists dimension-free constant $C_{\\sigma}^{b}\\,>\\,\\bar{0}$ depending on $h^{*},\\delta,C_{w}$ , such that one can modify the activation function ${\\hat{\\sigma}}(\\zeta)=(1+\\zeta)^{\\stackrel{.}{n}}$ to $\\tilde{\\sigma}$ satisfying that $\\|\\tilde{\\sigma}\\|_{L^{\\infty}(\\mathbb{R})}\\leq C_{\\sigma}^{b},\\|\\tilde{\\sigma}^{\\prime}\\|_{L^{\\infty}(\\mathbb{R})}\\leq C_{\\sigma}^{b},\\|\\tilde{\\sigma}^{\\prime\\prime}\\|_{L^{\\infty}(\\mathbb{R})}\\leq C_{\\sigma}^{b}$ and $\\sim\\!\\!N(0,I_{d})\\,\\left[|\\hat{\\sigma}(\\stackrel{\\cdot}{w}^{\\top}\\!x)-\\tilde{\\sigma}(w^{\\top}x)|^{2}\\right]<$ $\\mathbb{E}_{x\\sim\\mathcal{N}(0,I_{d})}\\left[|\\hat{\\sigma}(\\boldsymbol{w}^{\\top}\\boldsymbol{x})-\\tilde{\\sigma}(\\boldsymbol{w}^{\\top}\\boldsymbol{x})|^{2}\\right]<\\delta$ for all $w\\in\\mathbb{R}^{d}$ with $\\|w\\|\\leq C_{w}$ ", "page_idx": 28}, {"type": "text", "text": "The associated mean-field dynamics $\\tilde{\\rho}_{t}$ , that can be viewed as a slight modification of $\\rho_{t}$ generated by Algorithm 1, is given by (B.4) for $0\\leq t\\leq T$ and follows ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\partial_{t}\\tilde{\\rho}_{t}=\\nabla_{\\theta}\\cdot\\Big(\\tilde{\\rho}_{t}\\xi(t)\\nabla_{\\theta}\\tilde{\\Phi}^{\\prime}(\\theta;\\tilde{\\rho}_{t})\\Big)\\,,}\\\\ {\\tilde{\\rho}_{t}{\\Big|}_{t=T}=\\tilde{\\rho}_{T},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\Phi}^{\\prime}(\\theta;\\rho)=a\\mathbb{E}_{x\\sim\\mathcal{N}(0,I_{d})}\\left[\\left(\\tilde{f}_{\\mathrm{NN}}(x;\\rho)-\\tilde{f}^{*}(x)\\right)\\tilde{\\sigma}(w^{\\top}x)\\right],}\\\\ &{\\tilde{f}_{\\mathrm{NN}}(x;\\rho)=\\displaystyle\\int a\\tilde{\\sigma}(w^{\\top}x)\\rho(d a,d w),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for $t\\geq T$ Here, the learning rate $\\xi(t)=\\mathrm{diag}(\\xi_{a}(t),\\xi_{w}(t)I_{d})$ is shared with Algorithm 1, namely $\\xi_{a}(t)=0,\\xi_{w}(t)=1$ for $0\\leq t\\leq T$ and $\\xi_{a}(t)=1,\\xi_{w}(t)=0$ for $t\\geq T$ ", "page_idx": 28}, {"type": "text", "text": "The SGD associated to the modified mean-field dynamics is given by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{i}^{(k+1)}=w_{i}^{(k)}+\\epsilon\\left(\\tilde{f}^{*}(x_{k})-f_{\\mathrm{NN}}(x_{k};\\Theta^{(k)})\\right)a_{i}^{(k)}\\sigma^{\\prime}\\left(\\left(w_{i}^{(k)}\\right)^{\\top}x_{k}\\right)x_{k},}\\\\ &{a_{i}^{(k+1)}=a_{i}^{(k)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for $i=1,2,\\dots,N$ and $k=0,1,\\ldots,T/\\epsilon-1$ , where $N$ is the number of neurons and $T/\\epsilon$ is assumed to be an integer, and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{i}^{(k+1)}=w_{i}^{(k)},}\\\\ &{a_{i}^{(k+1)}=a_{i}^{(k)}+\\epsilon\\left(\\tilde{f}^{*}(x_{k})-\\tilde{f}_{\\mathrm{NN}}(x_{k};\\Theta^{(k)})\\right)\\tilde{\\sigma}\\left(\\left(w_{i}^{(k)}\\right)^{\\top}x_{k}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for $i=1,2,\\dots,N$ and $k=T/\\epsilon,T/\\epsilon+1,\\dots$ ", "page_idx": 28}, {"type": "text", "text": "Suppose that assumptions made in Theorem 4.3 hold and fix $T^{\\prime}>T>0$ . Using similar analysis as in Appendix A.2, one can conclude that for any $\\delta>0$ there exist dimension-free constants $\\dot{C}_{f}^{b}$ and $C_{\\sigma}^{b}$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq T^{\\prime}}\\left|\\mathcal{E}(\\rho_{t})-\\mathcal{E}(\\tilde{\\rho}_{t})\\right|<\\delta.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Applying Theorem 4.3 and [19, Theorem 1], we can conclude that for any $\\mu\\in(0,1)$ and any $\\delta>0$ there exists dimension-free constants $N_{0},d_{0},C_{\\epsilon}$ such that for any $N\\geq N_{0}$ and $d\\geq d_{0}$ , the following holds with probabilitya least $\\mu$ for any $\\begin{array}{r}{\\epsilon\\leq\\frac{C_{\\epsilon}}{d+\\log N}}\\end{array}$ with $T/\\epsilon\\in\\mathbb{N}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{k\\in[0,T^{\\prime}/\\epsilon]\\cap\\mathbb{N}}\\mathcal{E}_{N}(\\Theta^{(k)})<\\delta.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "If we further assume that $N={\\mathcal{O}}(e^{d})$ , this indicates that SGD with (D.2) and (D.3) can learn the subspace-sparse polynomial $f^{*}$ within finite time horizon and with $O(d)$ samples/data points. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 29}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 You should answer [Yes] , [No] , or [NA] .   \n\u00b7 [NA]  means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u00b7 Please provide a short (1-2 sentence) justification right after your answer (even for NA). ", "page_idx": 29}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 29}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 29}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u00b7 Keep the checklist subsection headings, questions/answers and guidelines below. \u00b7 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The theoretical results claimed in the abstract and the introduction are established in Section 3 and Section 4. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and refect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The limitations are discussed in the last paragraph of Section 5. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The assumptions and the proof sketches are included in Section 3 and Section 4.   \nThe complete proofs are in the appendices. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper is purely theoretical and does not include experiments. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPs does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: This paper is purely theoretical and does not include experiments ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 31}, {"type": "text", "text": "\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper is purely theoretical and does not include experiments. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper is purely theoretical and does not include experiments. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper is purely theoretical and does not include experiments. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics and confirm that this paper conforms with it in every respect. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper conducts fundamental research and is purely theoretical. Thus, it has no societal impact. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper is purely theoretical and poses no such risks. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper is purely theoretical and does not use existing assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper is purely theoretical and does not release new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetis used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper is purely theoretical/mathematical and does not involve crowdsourcing or research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: This paper is purely theoretical/mathematical and does not involve crowdsourcing or research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]