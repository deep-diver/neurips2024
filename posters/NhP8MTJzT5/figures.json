[{"figure_path": "NhP8MTJzT5/figures/figures_1_1.jpg", "caption": "Figure 1: A showcase of application of SytheOcc. We enable geometric-controlled generation that conveys the user editing in 3D voxel space to generate realistic street view images. In this case, we create a rare scene that traffic cones block the way. This advancement facilitates the evaluation of autonomous systems, such as the end-to-end planner VAD [9], in simulated corner case scenes.", "description": "This figure demonstrates the application of SyntheOcc in generating geometrically controlled street view images.  The user edits the 3D occupancy (left), adding traffic cones to obstruct the road. SyntheOcc uses this modified occupancy data to generate a realistic image (middle). This generated image is then used to test the VAD (Vectorized Autonomous Driving) end-to-end planner, showcasing how SyntheOcc facilitates the creation and evaluation of rare or challenging scenarios for autonomous driving systems.", "section": "1 Introduction"}, {"figure_path": "NhP8MTJzT5/figures/figures_3_1.jpg", "caption": "Figure 2: The overall architecture of SytheOcc. We achieve 3D geometric control in image generation by utilizing our proposed 3D semantic multiplane images to encode scene occupancy. In our framework, we can edit the occupied state and semantics of every voxel in 3D space to control the image generation, thereby opening up a wide spectrum of applications as shown in the top right.", "description": "This figure illustrates the architecture of SytheOcc, a framework for 3D geometrically controlled image generation.  It takes an occupancy label as input, which can be modified by the user. This label is converted into 3D semantic multiplane images (MPIs), a novel representation encoding scene occupancy information.  The MPIs are then fed into a diffusion UNet, along with latent noise and a text prompt, to generate multi-view images.  Intra-view and cross-view/frame attention mechanisms maintain consistency between views and frames. The output consists of photorealistic multi-view images that faithfully reflect the 3D geometry of the input occupancy label, suitable for various downstream applications such as training perception models and simulation.", "section": "3 Method"}, {"figure_path": "NhP8MTJzT5/figures/figures_4_1.jpg", "caption": "Figure 3: Visualizations of geometric controlled generation. Top row: Fusion of 3D semantic MPI. Bottom row: our generation concatenated from neighboring views.", "description": "This figure showcases the results of geometrically controlled image generation using the proposed SyntheOcc model.  The top row displays the fusion of 3D semantic multi-plane images (MPIs) used as input to condition the generation process.  The bottom row shows the resulting generated images, demonstrating the model's ability to create realistic street scenes by concatenating outputs from neighboring views and precisely controlling the geometric aspects of the scene.", "section": "3 Method"}, {"figure_path": "NhP8MTJzT5/figures/figures_4_2.jpg", "caption": "Figure 4: Visualizations of the reweighing function in Eq. 6.", "description": "This figure shows curves representing the reweighing function (Equation 6 in the paper) used to enhance the importance of foreground regions during training.  Different curves are plotted for various values of parameters *m* and *n*, which control the shape and extent of the weighting. The y-axis represents the weight, and the x-axis represents the training step (or potentially depth, depending on the specific reweighing method). This function helps balance the class distribution and focus more attention on foreground objects, particularly those that may be less frequent in the training data.", "section": "3.5 Importance Reweighing"}, {"figure_path": "NhP8MTJzT5/figures/figures_5_1.jpg", "caption": "Figure 5: Visualizations of generated multi-view images. The generation conditions (occupancy labels) are from nuScenes validation set. We highlight that (i) Geometry alignment of trees in red rectangle in (b). (ii) Use text prompt to control high-level appearance in (c,d).", "description": "This figure showcases the model's ability to generate photorealistic multi-view images from occupancy labels.  The top row shows the input 3D semantic multi-plane image (MPI) and the ground truth images. The following rows demonstrate different aspects of image generation, including ordinary scenes (b), weather variations (snow and sandstorm) (c), and style control (Minecraft and Diablo styles) (d).  The red rectangle in (b) highlights the accurate geometry alignment achieved by the model, specifically for trees. The text prompts used in (c) and (d) demonstrate the high-level control the model possesses over image appearance.", "section": "4.2 Qualitative Results"}, {"figure_path": "NhP8MTJzT5/figures/figures_7_1.jpg", "caption": "Figure 6: Top row: Comparison with ControlNet. We achieve a precise alignment between conditional labels and synthesized images, while ControlNet generates objects with incorrect pose due to ambiguous 2D condition. Mid and Bottom row: Visualizations of geometry-controlled image generation. We can faithfully generate objects with the desired topology in a specific 3D position.", "description": "This figure compares the image generation results of the proposed method (SytheOcc) with ControlNet. The top row shows that SytheOcc achieves precise alignment between the conditional labels and synthesized images, unlike ControlNet, which generates objects with incorrect poses due to ambiguous 2D conditioning. The middle and bottom rows demonstrate SytheOcc's capability to generate images with faithful alignment to the desired 3D geometry and topology.", "section": "Qualitative Results"}, {"figure_path": "NhP8MTJzT5/figures/figures_12_1.jpg", "caption": "Figure 13: From top to bottom, we display images of fusion of 3D semantic MPI, synthesized images of sandstorm, snow, foggy, rainy, day night, day time, and ground truth.", "description": "This figure showcases a series of images generated using the SytheOcc model under various weather conditions. The top row displays the fusion of 3D semantic MPIs which serves as the input to the model. The subsequent rows show images generated by the model under different weather conditions: sandstorm, snow, fog, rain, night, and day.  The bottom row displays the ground truth images for comparison, illustrating the model's ability to generate photorealistic images across varied weather scenarios.", "section": "B Long-Tailed Scene Evaluation"}, {"figure_path": "NhP8MTJzT5/figures/figures_12_2.jpg", "caption": "Figure 8: Use SytheOcc to create long-tailed scenes for testing. Top: In the ordinary scene of a bus placed in front of the ego vehicle, the end-to-end planner VAD [9] predicts future waypoints without movement, thus not plotted in the image. Bottom: By harnessing the prompt-level control in our framework, we simulate a scene with the same layout but filled with fog. VAD predicts wrong waypoints that will collide with the bus.", "description": "This figure demonstrates the application of SyntheOcc in generating long-tailed scenarios for evaluating autonomous driving systems. The top part shows a normal scene where the VAD (Vectorized Autonomous Driving) system correctly predicts the waypoints. The bottom part shows a modified scene with added fog, which is a corner case not present in the training data.  Here, SyntheOcc creates a foggy scene with the same layout, and the VAD system now incorrectly predicts waypoints leading to a collision. This highlights SyntheOcc's ability to generate challenging scenarios for robust testing and evaluation.", "section": "4.2 Qualitative Results"}, {"figure_path": "NhP8MTJzT5/figures/figures_13_1.jpg", "caption": "Figure 9: Comparison with baselines.", "description": "This figure presents a qualitative comparison of the proposed SytheOcc method against three baselines: MagicDrive, ControlNet, and ControlNet+depth.  Each row shows a different scene from the nuScenes dataset, displayed as a fusion of 3D semantic MPIs, followed by the generated image from each method and the ground truth image.  The comparison highlights SytheOcc's ability to generate images with better geometric consistency and alignment to the input occupancy compared to the baselines, which often struggle with object pose accuracy, scene fidelity, and handling of occlusions.", "section": "4.2 Qualitative Results"}, {"figure_path": "NhP8MTJzT5/figures/figures_14_1.jpg", "caption": "Figure 10: We demonstrate the generalizability of SytheOcc to new camera intrinsic. We multiply factors to the focal length while keeping the resolution the same. In (b,c), focal length \u00d70.8 denotes a camera with a larger field of view similar to zoom out, focal length \u00d71.2 denotes a camera with a smaller field of view similar to zoom in.", "description": "This figure demonstrates the robustness of SytheOcc to different camera intrinsics. By modifying the focal length, it shows that SytheOcc can generate realistic images even with varying field of views, highlighting its adaptability and generalization capabilities. This experiment showcases the flexibility of SytheOcc to handle diverse camera configurations.", "section": "F Generalize to New Cameras"}, {"figure_path": "NhP8MTJzT5/figures/figures_15_1.jpg", "caption": "Figure 5: Visualizations of generated multi-view images. The generation conditions (occupancy labels) are from nuScenes validation set. We highlight that (i) Geometry alignment of trees in red rectangle in (b). (ii) Use text prompt to control high-level appearance in (c,d).", "description": "This figure showcases examples of multi-view images generated by the SytheOcc model.  The top row shows the fusion of 3D semantic multi-plane images used as input to the model. The bottom rows demonstrate various generations. Row (b) highlights precise geometric alignment (indicated by a red rectangle) of the generated trees with the ground truth. Rows (c) and (d) illustrate how text prompts can control high-level image features, such as weather conditions (snow, sandstorm) and artistic style (Minecraft, Diablo).", "section": "4.2 Qualitative Results"}, {"figure_path": "NhP8MTJzT5/figures/figures_15_2.jpg", "caption": "Figure 1: A showcase of application of SytheOcc. We enable geometric-controlled generation that conveys the user editing in 3D voxel space to generate realistic street view images. In this case, we create a rare scene that traffic cones block the way. This advancement facilitates the evaluation of autonomous systems, such as the end-to-end planner VAD [9], in simulated corner case scenes.", "description": "This figure showcases SytheOcc's ability to generate geometrically controlled street view images.  It demonstrates how user edits made in 3D voxel space (e.g., adding traffic cones) translate into realistic changes in the generated image.  This capability is valuable for evaluating autonomous driving systems in corner case scenarios.", "section": "Introduction"}, {"figure_path": "NhP8MTJzT5/figures/figures_16_1.jpg", "caption": "Figure 7: From top to bottom, we display images of fusion of 3D semantic MPI, synthesized images of sandstorm, snow, foggy, rainy, day night, day time, and ground truth.", "description": "This figure showcases the results of generating images under various weather conditions using the SytheOcc model. The top row displays the fusion of 3D semantic multi-plane images (MPIs), which serve as the input to the model. The subsequent rows show the generated images under different weather conditions: sandstorm, snow, fog, rain, day, night, and daytime. The final row depicts the ground truth images corresponding to the generated ones. This figure demonstrates the model's ability to generate realistic images under diverse weather conditions, which is a significant contribution to data augmentation for autonomous driving.", "section": "B Long-Tailed Scene Evaluation"}, {"figure_path": "NhP8MTJzT5/figures/figures_16_2.jpg", "caption": "Figure 13: From top to bottom, we display images of fusion of 3D semantic MPI, synthesized images of sandstorm, snow, foggy, rainy, day night, day time, and ground truth.", "description": "This figure showcases the results of generating images under various weather conditions using the proposed SytheOcc model. The top row displays the fusion of 3D semantic multi-plane images (MPIs) used as input to the model. The subsequent rows show the generated images for sandstorm, snow, fog, rain, day, night and day time, respectively. The final row displays the corresponding ground truth images for comparison.", "section": "B Long-Tailed Scene Evaluation"}, {"figure_path": "NhP8MTJzT5/figures/figures_17_1.jpg", "caption": "Figure 13: From top to bottom, we display images of fusion of 3D semantic MPI, synthesized images of sandstorm, snow, foggy, rainy, day night, day time, and ground truth.", "description": "This figure shows a comparison of generated images under different weather conditions (sandstorm, snow, fog, rain, day, night) with the corresponding ground truth images. The top row displays the fusion of 3D semantic multi-plane images (MPIs), which serve as the input condition for image generation.  The subsequent rows illustrate the generated images for each specified weather condition.  This visualization demonstrates the model's ability to generate images under a variety of conditions, showcasing its robustness and controllability in diverse scenarios.", "section": "B Long-Tailed Scene Evaluation"}, {"figure_path": "NhP8MTJzT5/figures/figures_18_1.jpg", "caption": "Figure 1: A showcase of application of SytheOcc. We enable geometric-controlled generation that conveys the user editing in 3D voxel space to generate realistic street view images. In this case, we create a rare scene that traffic cones block the way. This advancement facilitates the evaluation of autonomous systems, such as the end-to-end planner VAD [9], in simulated corner case scenes.", "description": "This figure demonstrates SytheOcc's application in geometrically controlled image generation.  It shows how user edits in 3D voxel space (occupancy labels) translate into realistic street view images. The example highlights generation of a rare scenario (traffic cones blocking a path), useful for evaluating autonomous driving systems like VAD in simulated corner cases.", "section": "1 Introduction"}]