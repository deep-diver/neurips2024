[{"figure_path": "g7lYP11Erv/tables/tables_1_1.jpg", "caption": "Table 1: Base-to-new class generalization comparison for representative large 3D models based on prompt learning. Each number here is the mean of three runnings. Base: base class accuracy (in %, same below). New: new class accuracy. HM: harmonic mean of base and new class accuracy. +RC demonstrates the models with our regulation constraint framework.", "description": "This table compares the base-to-new class generalization performance of several large 3D models using prompt learning, both with and without the proposed regulation constraint framework.  The results are averaged across five datasets (ModelNet40, S-PB_T50_RS, S-OBJ_BG, S-OBJ_ONLY, ShapeNetCoreV2), showing base class accuracy, new class accuracy, and the harmonic mean of both, providing a comprehensive assessment of generalization capabilities.", "section": "4.2 Base-to-new Class Generalization"}, {"figure_path": "g7lYP11Erv/tables/tables_4_1.jpg", "caption": "Table 1: Base-to-new class generalization comparison for representative large 3D models based on prompt learning. Each number here is the mean of three runnings. Base: base class accuracy (in %, same below). New: new class accuracy. HM: harmonic mean of base and new class accuracy. +RC demonstrates the models with our regulation constraint framework.", "description": "This table compares the base-to-new class generalization performance of four different large 3D models (PointCLIP, PointCLIP2, ULIP, ULIP-2) with and without the proposed regulation constraint framework (+RC).  The results are averaged over five datasets (ModelNet40, S-PB_T50_RS, S-OBJ_BG, S-OBJ_ONLY, ShapeNetCoreV2). For each model and dataset, the table shows the base class accuracy, the new class accuracy, and the harmonic mean (HM) of the two. The harmonic mean provides a balanced measure of performance across both seen and unseen classes.", "section": "4.2 Base-to-new Class Generalization"}, {"figure_path": "g7lYP11Erv/tables/tables_6_1.jpg", "caption": "Table 1: Base-to-new class generalization comparison for representative large 3D models based on prompt learning. Each number here is the mean of three runnings. Base: base class accuracy (in %, same below). New: new class accuracy. HM: harmonic mean of base and new class accuracy. +RC demonstrates the models with our regulation constraint framework.", "description": "This table presents the base-to-new class generalization performance comparison of several prominent large 3D models using prompt learning, with and without the proposed regulation constraint framework (+RC).  The results are averaged across five different datasets (ModelNet40, S-PB_T50_RS, S-OBJ_BG, S-OBJ_ONLY, ShapeNetCoreV2).  For each model and dataset, the table shows the base class accuracy, new class accuracy, and the harmonic mean of these two accuracies. The harmonic mean (HM) provides a balanced measure of performance across both seen and unseen classes.", "section": "4.2 Base-to-new Class Generalization"}, {"figure_path": "g7lYP11Erv/tables/tables_7_1.jpg", "caption": "Table 2: Comparison of OOD generalization in cross-dataset benchmark. ShapeNetV2 serves as the source domain and the other five datasets are deployed as the target domain. ShapeNetV2: 55 classes, ModelNet40: 40 classes, SONN: 15 classes, Omni3D: 216 classes. Some common categories are shared between the source and target domain. Note that Omni3D has much more new 3D object concepts than others. The last column indicates the average over five target datasets.", "description": "This table presents the results of the out-of-distribution (OOD) generalization experiment.  The source domain is ShapeNetV2, and the target domains are ModelNet40, three variants of ScanObjectNN, and Omni3D.  The table shows the performance of different methods (P-CLIP, P-CLIP2, ULIP, ULIP-2, and the proposed +RC method) on each target dataset, highlighting the impact of the proposed regulation framework on OOD generalization performance.  The average performance across all target domains is also provided.", "section": "4.3 Cross-Dataset Generalization"}, {"figure_path": "g7lYP11Erv/tables/tables_7_2.jpg", "caption": "Table 3: Comparison of corruption generalization on ModelNet-C[56] when trained on clean data. The results are reported for the corruption severity=2 in ModelNet-C.", "description": "This table presents the results of corruption generalization experiments on the ModelNet-C dataset. The experiments evaluate the robustness of different models against various types of data corruptions.  The table shows the performance (accuracy) of several methods on clean ModelNet data and corrupted ModelNet-C data with corruption severity level 2. The performance is reported for different corruption types such as adding global noise, adding local noise, dropping global data points, dropping local data points, rotation, scaling, and jittering. The average performance across all corruption types is also reported.  This experiment aims to assess how well the models generalize to noisy and corrupted data.", "section": "4.3 Cross-Dataset Generalization"}, {"figure_path": "g7lYP11Erv/tables/tables_8_1.jpg", "caption": "Table 4: Ablation study for the three regulation constraints in our framework. The results are averaged on 5 datasets.", "description": "This table presents the ablation study of the three regulation constraints (MAC, TDC, and MEC) proposed in the Point-PRC framework.  It shows the impact of each constraint on the model's performance, measured by base class accuracy, new class accuracy, and their harmonic mean (HM), averaged across five datasets. The results demonstrate the effectiveness of each constraint and their combined effect on improving generalization.", "section": "4.5 Ablation Study"}, {"figure_path": "g7lYP11Erv/tables/tables_8_2.jpg", "caption": "Table 5: Ablation studies. The results are averaged over 5 datasets in the base-to-new benchmark. (a) The distance metrics in MAC. L1: L1 norm, MSE: mean square error, Cosine: cosine distance. (b) Here GPT-3.5 is short for GPT-3.5-turbo.", "description": "This table presents ablation study results on the proposed framework.  Part (a) shows the effect of different distance metrics used in the Mutual Agreement Constraint (MAC) component on the base, new, and harmonic mean (HM) accuracies. Part (b) shows a comparison of using different text description sources (LLMs and manual descriptions) on the same metrics.", "section": "4.5 Ablation Study"}, {"figure_path": "g7lYP11Erv/tables/tables_8_3.jpg", "caption": "Table 5: Ablation studies. The results are averaged over 5 datasets in the base-to-new benchmark. (a) The distance metrics in MAC. L1: L1 norm, MSE: mean square error, Cosine: cosine distance. (b) Here GPT-3.5 is short for GPT-3.5-turbo.", "description": "This table presents the ablation study results on two aspects: the distance metrics used in the Mutual Agreement Constraint (MAC) and the source of text descriptions used in the Text Diversity Constraint (TDC).  For the MAC ablation, it compares the harmonic mean (HM) of base and new class accuracies using L1 norm, Mean Square Error (MSE), and Cosine distance. For the TDC ablation, it shows the HM using text descriptions generated by GPT-3.5, GPT-4, PointLLM, and manual descriptions. The results highlight the impact of different choices on the model's performance in terms of base and new class accuracy and the overall HM.", "section": "4.5 Ablation Study"}, {"figure_path": "g7lYP11Erv/tables/tables_16_1.jpg", "caption": "Table 1: Base-to-new class generalization comparison for representative large 3D models based on prompt learning. Each number here is the mean of three runnings. Base: base class accuracy (in %, same below). New: new class accuracy. HM: harmonic mean of base and new class accuracy. +RC demonstrates the models with our regulation constraint framework.", "description": "This table presents the base-to-new class generalization results for several large 3D models using prompt learning, both with and without the proposed regulation constraint framework (+RC).  It shows the base class accuracy, new class accuracy, and their harmonic mean (HM) across five different datasets (ModelNet40, S-PB_T50_RS, S-OBJ_BG, S-OBJ_ONLY, ShapeNetCoreV2). The results demonstrate the impact of the regulation framework on improving generalization to unseen classes.", "section": "4.2 Base-to-new Class Generalization"}, {"figure_path": "g7lYP11Erv/tables/tables_17_1.jpg", "caption": "Table 2: Comparison of OOD generalization in cross-dataset benchmark. ShapeNetV2 serves as the source domain and the other five datasets are deployed as the target domain. ShapeNetV2: 55 classes, ModelNet40: 40 classes, SONN: 15 classes, Omni3D: 216 classes. Some common categories are shared between the source and target domain. Note that Omni3D has much more new 3D object concepts than others. The last column indicates the average over five target datasets.", "description": "This table presents the results of out-of-distribution (OOD) generalization experiments on five different datasets (ModelNet40, S-OBJ_ONLY, S-OBJ_BG, S-PB_T50_RS, Omni3D), using ShapeNetCoreV2 as the source domain.  It shows the performance (accuracy) of different methods (P-CLIP, P-CLIP2, ULIP, ULIP-2, and the proposed method (+RC)) on each target dataset. The table highlights the improvement in OOD generalization achieved by the proposed method, particularly on datasets with significantly different characteristics from the source domain.", "section": "4.3 Cross-Dataset Generalization"}, {"figure_path": "g7lYP11Erv/tables/tables_17_2.jpg", "caption": "Table 3: Comparison of corruption generalization on ModelNet-C[56] when trained on clean data. The results are reported for the corruption severity=2 in ModelNet-C.", "description": "This table presents the results of corruption generalization experiments using ModelNet-C dataset with corruption severity level 2. It compares the performance of different methods (P-CLIP, P-CLIP2, ULIP, ULIP-2, and their respective versions with the proposed regulation constraint framework) across various corruption types (Add Global, Add Local, Drop Global, Drop Local, Rotate, Scale, Jitter). The table shows the average accuracy for each method across different types of corruptions. This helps in understanding the robustness and generalization capabilities of different models in the face of noisy or corrupted data.", "section": "4.3 Cross-Dataset Generalization"}, {"figure_path": "g7lYP11Erv/tables/tables_17_3.jpg", "caption": "Table 11: Comparison of cross-dataset generalization on PointDA. M: ModelNet, S: ShapeNet, S*: ScanNet. The last column is the average over 6 evaluation settings.", "description": "This table compares the cross-dataset generalization performance of different methods on the PointDA benchmark. PointDA consists of six domain adaptation settings, each involving transferring knowledge from a source dataset to a target dataset.  The table shows the accuracy of different methods (including the proposed method) across these settings, with ModelNet (M), ShapeNet (S), and ScanNet (S*) as the source and target datasets.  The results highlight the relative performance gains achieved by the proposed method compared to prior state-of-the-art methods.", "section": "4.3 Cross-Dataset Generalization"}, {"figure_path": "g7lYP11Erv/tables/tables_17_4.jpg", "caption": "Table 2: Comparison of OOD generalization in cross-dataset benchmark. ShapeNetV2 serves as the source domain and the other five datasets are deployed as the target domain. ShapeNetV2: 55 classes, ModelNet40: 40 classes, SONN: 15 classes, Omni3D: 216 classes. Some common categories are shared between the source and target domain. Note that Omni3D has much more new 3D object concepts than others. The last column indicates the average over five target datasets.", "description": "This table presents the results of the out-of-distribution (OOD) generalization experiment.  The source domain is ShapeNetV2, and five other datasets are used as target domains. The table shows the performance of several methods (P-CLIP, P-CLIP2, ULIP, ULIP-2, and the proposed method) on each target domain, highlighting the impact of the proposed regulation framework on OOD generalization.  The average performance across all target datasets is also provided.", "section": "4.3 Cross-Dataset Generalization"}, {"figure_path": "g7lYP11Erv/tables/tables_17_5.jpg", "caption": "Table 6: Statistics of the Base-to-New benchmark.", "description": "This table presents the number of classes, training samples, validation samples, and testing samples for each of the five datasets used in the Base-to-New class generalization benchmark.  The datasets are ModelNet40, three variants of ScanObjectNN (S-PB_T50_RS, S-OBJ_BG, S-OBJ_ONLY), and ShapeNetCoreV2.  The table provides a detailed breakdown of the data distribution used for evaluating the base-to-new class generalization performance of the models.", "section": "4.1 3DDG Evaluation Settings"}, {"figure_path": "g7lYP11Erv/tables/tables_18_1.jpg", "caption": "Table 1: Base-to-new class generalization comparison for representative large 3D models based on prompt learning. Each number here is the mean of three runnings. Base: base class accuracy (in %, same below). New: new class accuracy. HM: harmonic mean of base and new class accuracy. +RC demonstrates the models with our regulation constraint framework.", "description": "This table presents the results of the base-to-new class generalization experiment.  It compares the performance of several large 3D models (PointCLIP, PointCLIP2, ULIP, ULIP-2) with and without the proposed regulation constraint framework (+RC). The table shows the base class accuracy (accuracy on seen classes), new class accuracy (accuracy on unseen classes), and the harmonic mean (HM) of both, which balances performance on seen and unseen classes.  The results are averaged over five different datasets (ModelNet40, S-PB_T50_RS, S-OBJ_BG, S-OBJ_ONLY, ShapeNetCoreV2). The numbers represent the average of three runs for each model and dataset.", "section": "4.2 Base-to-new Class Generalization"}, {"figure_path": "g7lYP11Erv/tables/tables_19_1.jpg", "caption": "Table 1: Base-to-new class generalization comparison for representative large 3D models based on prompt learning. Each number here is the mean of three runnings. Base: base class accuracy (in %, same below). New: new class accuracy. HM: harmonic mean of base and new class accuracy. +RC demonstrates the models with our regulation constraint framework.", "description": "This table presents a comparison of base-to-new class generalization performance across several prominent large 3D models using prompt learning.  It shows the base class accuracy (accuracy on seen classes), new class accuracy (accuracy on unseen classes), and the harmonic mean (HM) of these two accuracies, both with and without the proposed regulation constraint framework (+RC). The results are averaged over five datasets (ModelNet40, S-PB_T50_RS, S-OBJ_BG, S-OBJ_ONLY, ShapeNetCoreV2).", "section": "4.2 Base-to-new Class Generalization"}, {"figure_path": "g7lYP11Erv/tables/tables_19_2.jpg", "caption": "Table 2: Comparison of OOD generalization in cross-dataset benchmark. ShapeNetV2 serves as the source domain and the other five datasets are deployed as the target domain. ShapeNetV2: 55 classes, ModelNet40: 40 classes, SONN: 15 classes, Omni3D: 216 classes. Some common categories are shared between the source and target domain. Note that Omni3D has much more new 3D object concepts than others. The last column indicates the average over five target datasets.", "description": "This table presents the results of the cross-dataset generalization experiment.  ShapeNetV2 is used as the source domain, and five other datasets (ModelNet40, ScanObjectNN (three variants), and Omni3D) serve as target domains. The table shows the performance (accuracy) of various methods on each target dataset, highlighting the ability of the proposed method to generalize to unseen domains. The table also notes that Omni3D contains significantly more new object categories compared to other datasets.", "section": "4.3 Cross-Dataset Generalization"}, {"figure_path": "g7lYP11Erv/tables/tables_20_1.jpg", "caption": "Table 4: Ablation study for the three regulation constraints in our framework. The results are averaged on 5 datasets. MAC: mutual agreement constraint, TDC: text diversity constraint, MEC: model ensemble constraint. HM: harmonic mean of the Base and New class accuracies.", "description": "This table presents the ablation study results for the proposed Point-PRC framework. It shows the impact of each of the three regulation constraints (Mutual Agreement Constraint, Text Diversity Constraint, and Model Ensemble Constraint) on the model's performance.  The results are averaged across five datasets and reported as Base class accuracy, New class accuracy, and Harmonic Mean (HM). The table helps determine the contribution of each constraint in improving the overall accuracy and generalization ability.", "section": "4.5 Ablation Study"}, {"figure_path": "g7lYP11Erv/tables/tables_20_2.jpg", "caption": "Table 1: Base-to-new class generalization comparison for representative large 3D models based on prompt learning. Each number here is the mean of three runnings. Base: base class accuracy (in %, same below). New: new class accuracy. HM: harmonic mean of base and new class accuracy. +RC demonstrates the models with our regulation constraint framework.", "description": "This table compares the base-to-new class generalization performance of several large 3D models using prompt learning, with and without the proposed regulation constraint framework.  It shows the base class accuracy, new class accuracy, and their harmonic mean (HM) across five different datasets (ModelNet40, S-PB_T50_RS, S-OBJ_BG, S-OBJ_ONLY, ShapeNetCoreV2). The results demonstrate the improvement in generalization ability achieved by incorporating the regulation constraint.", "section": "4.2 Base-to-new Class Generalization"}, {"figure_path": "g7lYP11Erv/tables/tables_20_3.jpg", "caption": "Table 1: Base-to-new class generalization comparison for representative large 3D models based on prompt learning. Each number here is the mean of three runnings. Base: base class accuracy (in %, same below). New: new class accuracy. HM: harmonic mean of base and new class accuracy. +RC demonstrates the models with our regulation constraint framework.", "description": "This table compares the performance of different large 3D models (PointCLIP, PointCLIP V2, ULIP, ULIP-2) on a base-to-new class generalization benchmark.  The models utilize prompt learning, and the table shows base class accuracy, new class accuracy, and their harmonic mean (HM).  It also shows the improvement achieved by adding a regulation constraint framework (+RC).  The results are averaged across five datasets.", "section": "4.2 Base-to-new Class Generalization"}]