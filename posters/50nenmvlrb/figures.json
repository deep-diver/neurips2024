[{"figure_path": "50nEnmVLRb/figures/figures_1_1.jpg", "caption": "Figure 1: A snapshot from Etsy showcases Father\u2019s Day shopping recommendations. The lack of an obvious linear search order challenges the assumptions of the cascade model. Additionally, the proximity and arrangement of items are likely to influence clicks, indicating complex interaction patterns and supporting the need for full-bandit feedback without assumptions about user interactions with recommended items.", "description": "This figure shows examples of Father's Day gift recommendations from the Etsy online marketplace. The items are displayed in a visually engaging layout rather than a simple list.  This complex layout highlights the limitations of simple cascade models that assume users examine items sequentially and stop once they find something suitable. The figure argues that more realistic bandit feedback models are necessary to capture actual user behavior in the context of complex interfaces.", "section": "1 Introduction"}, {"figure_path": "50nEnmVLRb/figures/figures_8_1.jpg", "caption": "Figure 2: Comparative evaluation of bandit algorithms: The cumulative regret RT over T rounds is shown. Lower values indicate better performance. Plots (a) and (b) represent non-contextual settings for nDCG (fndcg) and nDCG + diversity (fndcgdiv) rewards, respectively. Plots (c) and (d) show results for contextual settings for five users using the same rewards. The y-axis for (a) and (b) is on the left, and for (c) and (d) on the right.", "description": "This figure compares the cumulative regret of several bandit algorithms across different settings. The algorithms are evaluated on two reward functions (nDCG and nDCG + diversity) and in both contextual and non-contextual scenarios.  Lower regret indicates better performance. The results suggest that the GP-TopK algorithm with Kendall kernels, particularly the WCK kernel, significantly outperforms other baselines.", "section": "5 Experiments"}, {"figure_path": "50nEnmVLRb/figures/figures_8_2.jpg", "caption": "Figure 2: Comparative evaluation of bandit algorithms: The cumulative regret RT over T rounds is shown. Lower values indicate better performance. Plots (a) and (b) represent non-contextual settings for nDCG (fndcg) and nDCG + diversity (fndcgdiv) rewards, respectively. Plots (c) and (d) show results for contextual settings for five users using the same rewards. The y-axis for (a) and (b) is on the left, and for (c) and (d) on the right.", "description": "The figure compares the cumulative regret of different bandit algorithms across various settings.  It shows the performance of the proposed GP-TopK algorithm against baselines (Random, \u03b5-greedy, MAB) with and without contextual information and for different reward functions. Lower regret values indicate better performance.", "section": "5 Experiments"}, {"figure_path": "50nEnmVLRb/figures/figures_29_1.jpg", "caption": "Figure 4: Local search results for optimizing combinatorial objectives in Ik for n = 50 and k = 6. For details, see the textual description. Left (a) shows how many times out of 100 trials the local search recovers the exact maximizer, i.e., \u03c0*, and right plot (b) shows the average value of the objective for the returned maximizer. These results indicate that the local search utilized in this work is effective.", "description": "This figure shows the effectiveness of the local search algorithm used in the paper.  The left plot shows the percentage of times (out of 100 trials) that the local search algorithm successfully found the optimal top-k ranking for the three different Kendall kernels: WK, CK, and WCK. The right plot shows the average objective value achieved by the local search algorithm across these 100 trials for each kernel. The results demonstrate that the local search algorithm is effective in finding high-quality solutions.", "section": "5 Experiments"}, {"figure_path": "50nEnmVLRb/figures/figures_30_1.jpg", "caption": "Figure 2: Comparative evaluation of bandit algorithms: The cumulative regret RT over T rounds is shown. Lower values indicate better performance. Plots (a) and (b) represent non-contextual settings for nDCG (fndcg) and nDCG + diversity (fndcgdiv) rewards, respectively. Plots (c) and (d) show results for contextual settings for five users using the same rewards. The y-axis for (a) and (b) is on the left, and for (c) and (d) on the right.", "description": "This figure compares the performance of several bandit algorithms (Random, e-greedy, MAB, WK, CK, WCK) in terms of cumulative regret (RT) over time (T).  The algorithms are evaluated under different settings: (a) and (b) show non-contextual settings with nDCG and nDCG+diversity reward functions, while (c) and (d) show contextual settings (with five users) using the same reward functions.  The figure demonstrates that the GP-TopK algorithm, especially when using the WCK kernel, outperforms the baselines.", "section": "5 Experiments"}]