[{"figure_path": "iEeiZlTbts/figures/figures_2_1.jpg", "caption": "Figure 1: JaxNAV (b) brings UED, often tested on Minigrid (a), closer to the real world (c)", "description": "This figure shows a comparison of three different environments used for testing Unsupervised Environment Design (UED) methods.  (a) shows a simple grid-world environment from Minigrid, a common benchmark in reinforcement learning. (b) depicts JaxNav, a new environment designed by the authors that is more similar to real-world robotic navigation tasks. (c) shows a real-world robotics scenario (sourced from [17]), which JaxNav is intended to mimic more closely than Minigrid.", "section": "3 JaxNav"}, {"figure_path": "iEeiZlTbts/figures/figures_4_1.jpg", "caption": "Figure 2: Our analysis of UED score functions shows that they are not predictive of \"learnability\".", "description": "This figure presents an analysis of three different score functions used in Unsupervised Environment Design (UED) methods for reinforcement learning.  The functions are MaxMC (Maximum Monte Carlo), PVL (Positive Value Loss), and a novel \"Learnability\" metric proposed by the authors. The plots show the correlation between each score function and the mean success rate of an agent on different levels (tasks) in the environment.  The key finding is that the MaxMC and PVL scores do not strongly correlate with the learnability metric, which is defined as the probability of success multiplied by the probability of failure (p*(1-p)). In contrast, the learnability metric shows a clear relationship with the mean success rate, indicating levels where the agent sometimes succeeds and sometimes fails are better for learning.", "section": "4.2 Analysing Regret Approximations used by UED Methods"}, {"figure_path": "iEeiZlTbts/figures/figures_6_1.jpg", "caption": "Figure 3: Single-agent JaxNav performance on (a) a-worst levels and (b) a challenging hand-designed test set. Only Perfect (Oracle) Regret matches SFL across both metrics.", "description": "This figure shows the results of the single-agent JaxNav experiment.  The left panel (a) displays the Conditional Value at Risk (CVaR) of success at different levels (\u03b1) of risk, showing how the agent performs on its worst-performing \u03b1% of levels.  The right panel (b) presents the average return on a hand-designed test set, showing general performance.  The figure highlights that SFL (Sampling For Learnability) achieves superior robustness (CVaR) compared to other methods, with only the oracle method achieving similar performance on both metrics.", "section": "7 Results"}, {"figure_path": "iEeiZlTbts/figures/figures_7_1.jpg", "caption": "Figure 4: Single-agent JaxNav comparison results. For each figure, cell (x, y) indicates how many environments have method X solving them x% of the time and method Y solving them y% of the time. The density below the diagonal indicates that SFL is more robust than DR, ACCEL and PLR.", "description": "This figure shows a comparison of the performance of different methods on single-agent JaxNav. Each cell in the heatmap represents the number of environments where one method (X-axis) solved a certain percentage of levels, while another method (Y-axis) solved a different percentage. The diagonal represents scenarios where both methods solved the same percentage of levels, while the area below the diagonal indicates that SFL consistently outperformed other methods in solving a higher percentage of environments.", "section": "7.1 Single-Agent JaxNav"}, {"figure_path": "iEeiZlTbts/figures/figures_7_2.jpg", "caption": "Figure 3: Single-agent JaxNav performance on (a) a-worst levels and (b) a challenging hand-designed test set. Only Perfect (Oracle) Regret matches SFL across both metrics.", "description": "This figure compares the performance of different curriculum learning methods on a single-agent robot navigation task.  (a) shows the Conditional Value at Risk (CVaR) of success, measuring robustness by evaluating performance on the worst-performing \u03b1% of levels. (b) shows performance on a hand-designed test set, representing human-relevant scenarios.  SFL consistently outperforms other methods in terms of robustness (a), and nearly matches the performance of the 'Perfect Regret' oracle method (which has access to perfect regret information) in both (a) and (b).", "section": "7 Results"}, {"figure_path": "iEeiZlTbts/figures/figures_8_1.jpg", "caption": "Figure 4: Single-agent JaxNav comparison results. For each figure, cell (x, y) indicates how many environments have method X solving them x% of the time and method Y solving them y% of the time. The density below the diagonal indicates that SFL is more robust than DR, ACCEL and PLR.", "description": "This figure shows a comparison of the performance of four different methods (SFL, DR, ACCEL, and PLR) on the single-agent JaxNav environment.  Each cell in the heatmaps represents the number of environments where method X (on the x-axis) solved a certain percentage of environments and method Y (on the y-axis) solved another percentage of the same environments. The darker the color, the more environments fall into that specific cell. The heatmaps highlight the relative robustness of SFL compared to other methods.", "section": "7.1 Single-Agent JaxNav"}, {"figure_path": "iEeiZlTbts/figures/figures_8_2.jpg", "caption": "Figure 3: Single-agent JaxNav performance on (a) a-worst levels and (b) a challenging hand-designed test set. Only Perfect (Oracle) Regret matches SFL across both metrics.", "description": "This figure displays the results of a single-agent experiment on JaxNav, comparing the performance of different methods.  Subfigure (a) shows the Conditional Value at Risk (CVaR) of success at different alpha levels (a). It illustrates the robustness of each method by showing its performance on the worst-performing a% of levels. Subfigure (b) presents the average return on a hand-designed test set, evaluating performance on human-relevant levels.  The key finding is that the Sampling For Learnability (SFL) method significantly outperforms other methods in terms of robustness (a) and achieves comparable performance on human-designed levels (b).", "section": "7 Results"}, {"figure_path": "iEeiZlTbts/figures/figures_8_3.jpg", "caption": "Figure 3: Single-agent JaxNav performance on (a) a-worst levels and (b) a challenging hand-designed test set. Only Perfect (Oracle) Regret matches SFL across both metrics.", "description": "This figure compares the performance of different methods on single-agent JaxNav tasks.  (a) shows the Conditional Value at Risk (CVaR) of success, indicating robustness by evaluating performance on the worst-performing \u03b1% of levels. (b) displays mean success rate on a hand-designed test set, a more standard evaluation metric.  The results show that the proposed Sampling For Learnability (SFL) method significantly outperforms other methods in terms of robustness (a), achieving results comparable to an oracle with perfect regret information. Although performance on the hand-designed set (b) is closer across methods, SFL still demonstrates competitive performance.", "section": "7 Results"}, {"figure_path": "iEeiZlTbts/figures/figures_8_4.jpg", "caption": "Figure 3: Single-agent JaxNav performance on (a) a-worst levels and (b) a challenging hand-designed test set. Only Perfect (Oracle) Regret matches SFL across both metrics.", "description": "This figure displays the results of a single-agent experiment on the JaxNav environment.  The left plot (a) shows the Conditional Value at Risk (CVaR) of success at different \u03b1 levels (representing the average success rate on the worst \u03b1% of levels).  The right plot (b) presents the mean return on a challenging, hand-designed test set. The key finding is that SFL (Sampling for Learnability) outperforms existing methods in terms of robustness (CVaR) and performance on the hand-designed test set, with only an oracle method achieving similar results.  This highlights the effectiveness of SFL in generating robust and adaptable agents.", "section": "7 Results"}, {"figure_path": "iEeiZlTbts/figures/figures_8_5.jpg", "caption": "Figure 3: Single-agent JaxNav performance on (a) a-worst levels and (b) a challenging hand-designed test set. Only Perfect (Oracle) Regret matches SFL across both metrics.", "description": "This figure shows the performance comparison between Sampling For Learnability (SFL) and other methods on single-agent JaxNav.  The left plot (a) shows the Conditional Value at Risk (CVaR) of success at different risk levels (\u03b1). SFL outperforms others in terms of robustness by achieving high success rates even in the worst \u03b1% of levels. The right plot (b) displays the mean return on a hand-designed test set, revealing that the SFL method and the Perfect Regret method (oracle) achieve similar performance. This indicates that the success of SFL is based on its ability to prioritize levels that improve the policy.", "section": "7 Results"}, {"figure_path": "iEeiZlTbts/figures/figures_18_1.jpg", "caption": "Figure 1: JaxNAV (b) brings UED, often tested on Minigrid (a), closer to the real world (c)", "description": "This figure shows three different environments used in the paper to test the Unsupervised Environment Design (UED) methods. (a) shows a simple grid world environment from Minigrid, which is a commonly used benchmark for UED. (b) shows the JaxNav environment, a new environment introduced in the paper that is more complex and realistic, and closely inspired by a real-world robotics problem. (c) shows a real-world robot navigation scenario, demonstrating the practical application of the developed methods.", "section": "3 JaxNav"}, {"figure_path": "iEeiZlTbts/figures/figures_18_2.jpg", "caption": "Figure 9: Hand-Designed Test Set for Single Agent JaxNav Policies.", "description": "This figure shows a set of hand-designed test levels for evaluating single-agent navigation policies in the JaxNav environment. Each level presents a unique challenge for the agent in terms of obstacle layout and path planning complexity, from simple straight paths to more intricate routes that require careful maneuvering and decision-making.", "section": "D Hand Designed Test Sets"}, {"figure_path": "iEeiZlTbts/figures/figures_18_3.jpg", "caption": "Figure 11: Hand-designed Minigrid Levels [3, 22, 23].", "description": "This figure shows eight different hand-designed levels from the Minigrid environment used in the paper's experiments.  These levels vary in complexity, with some being simple and others more intricate mazes. The levels are used as a standard testbed to evaluate the performance of different curriculum learning methods, providing a consistent benchmark for comparing algorithm performance across various approaches.", "section": "3 JaxNav"}, {"figure_path": "iEeiZlTbts/figures/figures_20_1.jpg", "caption": "Figure 2: Our analysis of UED score functions shows that they are not predictive of \"learnability.\"", "description": "This figure presents an analysis of three different score functions used in Unsupervised Environment Design (UED) methods for reinforcement learning.  The functions are MaxMC, Positive Value Loss (PVL), and Learnability (a new metric proposed by the authors).  Each function's score is plotted against the mean success rate of an agent on a given level. The plots show that MaxMC and PVL have a strong correlation with success rate, which indicates that these functions are not effective at identifying the frontier of learning, where the agent can sometimes solve a level but not always. In contrast, Learnability shows a weaker correlation with success rate but a stronger correlation with the new metric, suggesting that this metric is a better indicator of level difficulty and potential learning progress.", "section": "4.2 Analysing Regret Approximations used by UED Methods"}, {"figure_path": "iEeiZlTbts/figures/figures_20_2.jpg", "caption": "Figure 2: Our analysis of UED score functions shows that they are not predictive of \u201clearnability.\u201d", "description": "This figure presents an analysis of three different scoring functions used in Unsupervised Environment Design (UED) methods for reinforcement learning.  The x-axis represents the mean success rate of an agent on different levels (environments), while the y-axis shows the score assigned to those levels by each of the three methods: MaxMC, Positive Value Loss (PVL), and Learnability (the authors' proposed method).  Scatter plots show the relationship between success rate and the scores generated by each function.  The plots demonstrate a weak or nonexistent correlation between the MaxMC and PVL scores and a novel \u201clearnability\u201d metric defined by the authors. This suggests that the existing UED scoring functions do not accurately capture the concept of \u201clearnability,\u201d which is crucial for effective curriculum design.", "section": "4.2 Analysing Regret Approximations used by UED Methods"}, {"figure_path": "iEeiZlTbts/figures/figures_21_1.jpg", "caption": "Figure 3: Single-agent JaxNav performance on (a) a-worst levels and (b) a challenging hand-designed test set. Only Perfect (Oracle) Regret matches SFL across both metrics.", "description": "This figure shows the performance comparison between SFL and other methods in single-agent JaxNav. The left subplot (a) presents the conditional value at risk (CVaR) of success at level \u03b1, which measures the robustness of the methods on their worst-performing levels.  The right subplot (b) displays the mean return on a hand-designed test set. SFL shows superior robustness across different levels of \u03b1, and its performance is comparable to a perfect regret oracle only on the hand designed test set. ", "section": "7 Results"}, {"figure_path": "iEeiZlTbts/figures/figures_21_2.jpg", "caption": "Figure 4: Single-agent JaxNav comparison results. For each figure, cell (x, y) indicates how many environments have method X solving them x% of the time and method Y solving them y% of the time. The density below the diagonal indicates that SFL is more robust than DR, ACCEL and PLR.", "description": "This figure presents a comparison of the performance of different methods (DR, ACCEL, PLR, and SFL) on single-agent JaxNav environments.  For each pair of methods, a heatmap is shown where each cell (x,y) represents the number of environments where method X solved x% of the time and method Y solved y% of the time. The heatmaps visualize the relative performance of SFL against other approaches, demonstrating its higher robustness in solving a wider range of environments compared to other methods.", "section": "7 Results"}, {"figure_path": "iEeiZlTbts/figures/figures_22_1.jpg", "caption": "Figure 3: Single-agent JaxNav performance on (a) a-worst levels and (b) a challenging hand-designed test set. Only Perfect (Oracle) Regret matches SFL across both metrics.", "description": "This figure shows the results of single-agent JaxNav experiments.  Subfigure (a) displays the Conditional Value at Risk (CVaR) of success at different levels (\u03b1) of risk, demonstrating SFL's robustness compared to other methods. Subfigure (b) shows the mean return on a hand-designed test set, where SFL again demonstrates strong performance comparable to only the 'Perfect Regret' oracle.", "section": "7 Results"}, {"figure_path": "iEeiZlTbts/figures/figures_22_2.jpg", "caption": "Figure 3: Single-agent JaxNav performance on (a) a-worst levels and (b) a challenging hand-designed test set. Only Perfect (Oracle) Regret matches SFL across both metrics.", "description": "This figure displays a comparison of the performance of different curriculum learning methods in a single-agent robotic navigation environment.  Subfigure (a) shows the Conditional Value at Risk (CVaR) of success at level \u03b1, a measure of robustness against the worst-performing \u03b1% of levels.  Subfigure (b) shows the performance on a hand-designed test set, representing human-relevant tasks.  The results indicate that Sampling For Learnability (SFL) achieves superior performance, especially in terms of robustness, compared to other methods.", "section": "7 Results"}, {"figure_path": "iEeiZlTbts/figures/figures_23_1.jpg", "caption": "Figure 18: Environment Metrics for single-agent JaxNav for SFL, PLR and DR.", "description": "This figure shows a comparison of three environment metrics (shortest path, number of walls, and solvability) for levels selected by different UED methods (SFL, PLR, and DR) in the single-agent JaxNav environment.  It visually represents how the level characteristics differ across these methods, showing that SFL tends to select levels with longer shortest paths, fewer walls, and higher solvability compared to the others. This suggests that SFL prioritizes more challenging yet solvable levels, consistent with the paper's emphasis on learning environments in the 'sweet spot' of difficulty.", "section": "G.4 Environment Metrics"}, {"figure_path": "iEeiZlTbts/figures/figures_23_2.jpg", "caption": "Figure 19: Environment Metrics for XLand-Minigrid.", "description": "The figure shows the mean number of rules for the PLR/SFL buffer rulesets in XLand-Minigrid. SFL samples well below the mean value throughout training, whereas PLR starts on par with DR before tending easier as training progresses. This result, coupled with the performance difference, illustrates how SFL's learnability score allows it to find the frontier of learning, leading to more robust agents.", "section": "G.4 Environment Metrics"}, {"figure_path": "iEeiZlTbts/figures/figures_24_1.jpg", "caption": "Figure 23: Levels in Xland-Minigrid generated by each method.", "description": "This figure shows examples of levels generated by different curriculum learning methods (ACCEL, PLR, DR, and SFL) in the XLand-Minigrid environment. Each method's level generation strategy reflects its underlying approach to curriculum design; visually, the levels differ in terms of complexity and obstacle distribution.", "section": "G.5 Level Plots"}, {"figure_path": "iEeiZlTbts/figures/figures_24_2.jpg", "caption": "Figure 23: Levels in Xland-Minigrid generated by each method.", "description": "This figure shows example levels generated by four different methods (ACCEL, PLR, DR, and SFL) in the XLand-Minigrid environment.  Each method's level generation approach is different, leading to visually distinct level designs. The image provides a visual comparison of the types of environments generated by each method in this particular domain.", "section": "G.5 Level Plots"}, {"figure_path": "iEeiZlTbts/figures/figures_24_3.jpg", "caption": "Figure 23: Levels in Xland-Minigrid generated by each method.", "description": "This figure shows example levels generated by different UED methods (ACCEL, PLR, DR, and SFL) in the XLand-Minigrid environment.  Each method's level generation strategy is reflected in the visual characteristics of the generated levels, showing differences in complexity, obstacle placement, and overall layout.", "section": "G.5 Level Plots"}, {"figure_path": "iEeiZlTbts/figures/figures_25_1.jpg", "caption": "Figure 23: Levels in Xland-Minigrid generated by each method.", "description": "This figure shows example levels generated by different curriculum learning methods (DR, PLR, and SFL) in the XLand-Minigrid environment.  Each method's level generation strategy results in visually distinct layouts and potentially different levels of complexity and difficulty for the agent to solve.", "section": "G.5 Level Plots"}, {"figure_path": "iEeiZlTbts/figures/figures_26_1.jpg", "caption": "Figure 24: Analysing the effect of hyperparameters on single-agent JaxNav. Hyperparameters not mentioned in each plot use the default configuration's values: N = 5000, T = 50, \u03c1 = 0.5, K = 1000, L = 2000.", "description": "This figure shows the ablation study on the hyperparameters of the Sampling For Learnability (SFL) method in the single-agent JaxNav environment.  It displays the impact of varying each hyperparameter (Number of Sampled Levels, Rollout Length, Buffer Size, Buffer Update Period, Sampled Environments Ratio, Buffer Sampling Strategy) on the average conditional value at risk (CVaR) of success at different alpha levels (\u03b1). Each subfigure shows how changing one hyperparameter, while keeping the others fixed at their default values, affects the performance of the algorithm. The results provide insights into the optimal settings of these hyperparameters for achieving the best robustness in the algorithm.", "section": "I.1 SFL Hyperparameters"}, {"figure_path": "iEeiZlTbts/figures/figures_27_1.jpg", "caption": "Figure 24: Analysing the effect of hyperparameters on single-agent JaxNav. Hyperparameters not mentioned in each plot use the default configuration's values: N = 5000, T = 50, \u03c1 = 0.5, K = 1000, L = 2000.", "description": "This figure shows the ablation study on the hyperparameters of the Sampling For Learnability (SFL) method in the single-agent JaxNav environment.  Each subplot presents the effect of varying one hyperparameter (Number of Sampled Levels, Rollout Length, Buffer Size, Buffer Update Period, and Sampled Environments Ratio) on the average win rate percentage on the worst-case \u03b1% levels.  The results demonstrate the impact of each hyperparameter on the robustness and performance of the SFL algorithm.  The default values used for hyperparameters that aren't being varied are specified in the caption.", "section": "I.1 SFL Hyperparameters"}, {"figure_path": "iEeiZlTbts/figures/figures_27_2.jpg", "caption": "Figure 2: Our analysis of UED score functions shows that they are not predictive of \"learnability\".", "description": "This figure presents an analysis of the correlation between three different score functions used in Unsupervised Environment Design (UED) methods and the mean success rate of an agent on those levels. The three score functions are MaxMC (Maximum Monte Carlo), PVL (Positive Value Loss), and Learnability (a novel metric proposed in the paper). The figure shows that MaxMC and PVL do not correlate well with learnability, while the proposed Learnability metric shows a strong correlation with levels that are neither too easy nor too difficult for the agent, i.e., levels at the frontier of learning.", "section": "4.2 Analysing Regret Approximations used by UED Methods"}, {"figure_path": "iEeiZlTbts/figures/figures_28_1.jpg", "caption": "Figure 3: Single-agent JaxNav performance on (a) a-worst levels and (b) a challenging hand-designed test set. Only Perfect (Oracle) Regret matches SFL across both metrics.", "description": "This figure shows a comparison of different methods for training reinforcement learning agents on a single-agent robot navigation task.  The left panel (a) displays the Conditional Value at Risk (CVaR) of success, a robustness measure, for various methods across different percentages (alpha) of worst performing levels.  It shows SFL, a new method proposed in the paper, outperforms existing methods in robustness. The right panel (b) shows the mean return on a hand-designed test set, demonstrating that SFL achieves comparable performance to other methods on this standard evaluation metric, highlighting its overall effectiveness.", "section": "7 Results"}]