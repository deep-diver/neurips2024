[{"type": "text", "text": "Two applications of Min-Max-Jump distance ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We explore two applications of Min-Max-Jump distance (MMJ distance): MMJ  \n2 based K-means and MMJ-based internal clustering evaluation index. K-means and   \n3 its variants are possibly the most popular clustering approach. A key drawback of   \n4 K-means is that it cannot deal with data sets that are not the union of well-separated,   \n5 spherical clusters. MMJ-based K-means proposed in this paper overcomes this   \n6 demerit of K-means, so that it can handle irregularly shaped clusters. Evaluation (or   \n7 \"validation\") of clustering results is fundamental to clustering and thus to machine   \n8 learning. Popular internal clustering evaluation indices like Silhouette coefficient,   \n9 Davies\u2013Bouldin index, and Calinski-Harabasz index performs poorly in evaluating   \n10 irregularly shaped clusters. MMJ-based internal clustering evaluation index uses   \n11 MMJ distance and Semantic Center of Mass (SCOM) to revise the indices, so that   \n12 it can evaluate irregularly shaped data. An experiment shows introducing MMJ   \n13 distance to internal clustering evaluation index, can systematically improve the   \n14 performance. We also devise two algorithms for calculating MMJ distance. ", "page_idx": 0}, {"type": "text", "text": "15 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "16 Distance is a numerical measurement of how far apart objects or points are. It is usually formalized   \n17 in mathematics using the notion of a metric space. A metric space is a set together with a notion of   \n18 distance between its elements, usually called points. The distance is measured by a function called   \n19 a metric or distance function. Metric spaces are the most general setting for studying many of the   \n20 concepts of mathematical analysis and geometry.   \n21 In this paper, we introduce two algorithms for calculating Min-Max-Jump distance (MMJ distance)   \n22 and explore two applications of it. Including MMJ-based K-means (MMJ-K-means) and MMJ-based   \n23 internal clustering evaluation index.   \n24 MMJ-K-means improves K-means, so that it can handle irregularly shaped clusters. We claim MMJ  \n25 CH is the SOTA (state-of-the-art) internal clustering evaluation index, which achieves an accuracy of   \n26 90/145. MMJ-CH is one of the MMJ-based internal clustering evaluation indices. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "27 2 RELATED WORK ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "28 2.1 Different distance metrics ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "29 Many distance measures have been proposed in literature, such as Euclidean distance or cosine   \n30 similarity. These distance measures often be found in algorithms like k-NN, UMAP, HDBSCAN,   \n31 etc. The most common metric is Euclidean distance. Cosine similarity is often used as a way to   \n32 counteract Euclidean distance\u2019s problem in high dimensionality. The cosine similarity is the cosine   \n33 of the angle between two vectors.   \n34 Hamming distance is the number of values that are different between two vectors. It is typically used   \n35 to compare two binary strings of equal length (1).   \n36 Manhattan distance is a geometry whose usual distance function or metric of Euclidean geometry   \n37 is replaced by a new metric in which the distance between two points is the sum of the absolute   \n38 differences of their Cartesian coordinates (2).   \n39 Chebyshev distance is defined as the greatest of difference between two vectors along any coordinate   \n40 dimension (3).   \n41 Minkowski distance or Minkowski metric is a metric in a normed vector space which can be   \n42 considered as a generalization of both the Euclidean distance and the Manhattan distance (4).   \n43 Jaccard index, also known as the Jaccard similarity coefficient, is a statistic used for gauging the   \n44 similarity and diversity of sample sets (5).   \n45 Haversine distance is the distance between two points on a sphere given their longitudes and latitudes.   \n46 It is similar to Euclidean distance in that it calculates the shortest path between two points. The main   \n47 difference is that there is no straight line, since the assumption is that the two points are on a sphere   \n48 (6). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "49 2.2 K-means ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "50 K-means (7) and its variants (8; 9; 10) are possibly the most well-liked clustering approach. K-means   \n51 divides the data into K groups, where K is a hyper-parameter to be optimized. It aims to reduce the   \n52 within-cluster dissimilarity. While popular, K-means and its variants perform poorly for data sets   \n53 that are not the union of well-separated, spherical clusters. MMJ-based K-means (MMJ-K-means)   \n54 proposed in this paper overcomes this demerit of K-means, so that it can handle irregularly shaped   \n55 clusters. ", "page_idx": 1}, {"type": "text", "text": "56 2.3 Internal clustering evaluation index ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "57 Evaluation (or \"validation\") of clustering results is as difficult as the clustering itself (11). Popular   \n58 approaches involve \"internal\" evaluation and \"external\" evaluation. In internal evaluation, a clustering   \n59 result is evaluated based on the data that was clustered itself. Popular internal evaluation indices   \n60 are Davies-Bouldin index (12), Silhouette coefficient (13), Dunn index (14), and Calinski-Harabasz   \n61 index (15) etc. In external evaluation, the clustering result is compared to an existing \"ground truth\"   \n62 classification, such as the Rand index (16). However, knowledge of the ground truth classes is almost   \n63 never available in practice.   \n64 In Section 5.2, an experiment shows introducing Min-Max-Jump (MMJ) distance to internal clustering   \n65 evaluation index, can systematically improve the performance. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "66 2.4 Path-based distances ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "67 Euclidean distances are frequently used in machine learning and clustering methods to compare   \n68 points. However, the distance is data-independent, and not tailored to the geometry of the data. Many   \n69 metrics that are data-dependent have been devised, such as diffusion distances (17) and path-based   \n70 distances (18; 19). MMJ distance is a path-based distance. ", "page_idx": 1}, {"type": "text", "text": "71 3 Definition of Min-Max-Jump ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "72 Definition 1. Min-Max-Jump distance (MMJ distance) ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "73 $\\Omega$ is a set of points (at least one). For any pair of points $p,q\\in\\Omega_{*}$ , the distance between $p$ and q is   \n74 defined by a distance function $d(p,q)$ (such as Euclidean distance). $i,j\\in\\Omega$ , $\\Psi_{(i,j,n,\\Omega)}$ is a path from   \n75 point i to point $j,$ , which has length of n points (see Table $^{\\,l}$ ). $\\Theta_{(i,j,\\Omega)}$ is the set of all paths from point   \n76 i to point $j$ . Therefore, $\\Psi_{(i,j,n,\\Omega)}\\in\\Theta_{(i,j,\\Omega)}$ . max_jump( $\\Psi_{(i,j,n,\\Omega)}$ ) is the maximum jump in path   \n77 $\\Psi_{(i,j,n,\\Omega)}$ . ", "page_idx": 1}, {"type": "text", "text": "78 The Min-Max-Jump distance between a pair of points $i,j$ , which belong to $\\Omega$ , is defined as: ", "page_idx": 1}, {"type": "table", "img_path": "2BOb4SvDFr/tmp/d064e1979ca5c3edbbdcd9759e139a88b382b663d41aa5f1f4eafa0ee75ec489.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "equation", "text": "$$\n\\Pi=\\{m a x\\_j u m p(\\epsilon)\\mid\\epsilon\\in\\Theta_{(i,j,\\Omega)}\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "equation", "text": "$$\nM M J(i,j\\mid\\Omega)=m i n(\\Pi)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "79 Where \u03f5 is a path from point i to point j, max_jump(\u03f5) is the maximum jump in path \u03f5. \u03a0 is the set   \n80 of all maximum jumps. min $(\\Pi)$ is the minimum of Set \u03a0.   \n81 Set \u2126is called the Context of the Min-Max-Jump distance. It is easy to check $M M J(i,i\\mid\\Omega)=0$ .   \n82   \n83 In summary, Min-Max-Jump distance is the minimum of maximum jumps of all path between a pair   \n84 of points, under the Context of a set of points.   \n85 Similar distances have actually been studied in many places in the literature, including the maximum   \n86 capacity path problem, the widest path problem, the bottleneck edge query problem, the minimax   \n87 path problem, the bottleneck shortest path problem, and the longest-leg path distance (LLPD)   \n88 (20; 21; 22; 23).   \n89 There is a minor difference between Min-Max-Jump distance and other similar distances: Min-Max  \n90 Jump distance stresses the context of the distance. The context is like the condition in conditional   \n91 probability. The difference becomes non-trivial when we need to calculate the pairwise MMJ distance   \n92 matrix of a set $S$ , under the context of its superset $X$ , such as in Section 6.3 of (24). A set $\\Omega$ is a   \n93 superset of another set $B$ if all elements of the set $B$ are elements of the set $\\Omega$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "2BOb4SvDFr/tmp/fe86bb3fd3cfaa37a3f6687467dcc1399fa749c3551577aa350080d2be089e44.jpg", "img_caption": ["Figure 1: An example "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "94 3.1 An example ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "95 Suppose Set $\\Omega$ is composed of the four points in Figure 1. There are five (non-looped) paths from   \n96 point $a$ to point $c$ in Figure 1:   \n97 1. $a\\to c$ , the maximum jump is 28;   \n98 2. $a\\rightarrow b\\rightarrow c$ , the maximum jump is 19;   \n99 3. $a\\rightarrow d\\rightarrow c$ , the maximum jump is 17;   \n100 4. $a\\rightarrow b\\rightarrow d\\rightarrow c$ , the maximum jump is 19;   \n101 5. $a\\rightarrow d\\rightarrow b\\rightarrow c$ , the maximum jump is 12. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "102 According to Definition 1 $,M M J(a,c\\mid\\Omega)=12$ ", "page_idx": 3}, {"type": "text", "text": "103 To understand Min-Max-Jump distance, imagine someone is traveling by jumping in $\\Omega$ . Suppose   \n104 $M M J(i,j\\mid\\Omega)=\\delta$ . If the person wants to reach $j$ from $i$ , she must have the ability of jumping at   \n105 least $\\delta$ . Otherwise, $j$ is unreachable from $i$ for her. Whether the distance to a point is \"far\" or \"near\"   \n106 is measured by how far (or how high) it requires a person to jump. If the requirement is large, then   \n107 the point is \"far\", otherwise, it is \"near.\" ", "page_idx": 3}, {"type": "text", "text": "108 3.2 Properties of MMJ distance ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "109 Theorem 1. Suppose $i,j,p,q\\in\\Omega$ , ", "text_level": 1, "page_idx": 3}, {"type": "equation", "text": "$$\nM M J(i,j\\mid\\Omega)=\\delta\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "110 ", "page_idx": 3}, {"type": "equation", "text": "$$\nd(i,p)<\\delta\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "111 ", "page_idx": 3}, {"type": "text", "text": "112 then, ", "page_idx": 3}, {"type": "equation", "text": "$$\nM M J(p,q\\mid\\Omega)=\\delta\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "113 where $\\mathrm{d}(\\mathrm{x},\\mathrm{y})$ is a distance function (Table 1). ", "page_idx": 3}, {"type": "text", "text": "114 Proof. $M M J(i,j\\mid\\Omega)=\\delta$ is equivalent to $\\exists P\\in\\Theta_{(i,j,\\Omega)}$ , such that $M(P)=\\delta$ , and $\\forall T\\in\\Theta_{(i,j,\\Omega)}$ ,   \n115 $M(T)\\geq\\delta$ , where $\\Theta_{(i,j,\\Omega)}$ is the set of all paths from point $i$ to point $j$ under context $\\Omega$ . $M(P)$ is   \n116 the maximum jump in path $P$ . We can assume $M M J(p,q\\mid\\Omega)>\\delta$ and $M M J(p,q\\mid\\Omega)<\\delta$ , then   \n117 we will arrive to a contradiction in both cases. \u53e3 ", "page_idx": 3}, {"type": "text", "text": "118 Theorem 2. Suppose $r\\in\\{1,2,\\ldots,n\\}$ , ", "page_idx": 4}, {"type": "text", "text": "119 ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{f(t)=m a x(d(\\Omega_{n+1},\\Omega_{t}),\\;M M J(\\Omega_{t},\\Omega_{r}\\mid\\Omega_{[1,n]}))}\\\\ {\\mathbb{X}=\\{f(t)\\mid t\\in\\{1,2,\\ldots,n\\}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "120 then, ", "page_idx": 4}, {"type": "equation", "text": "$$\nM M J(\\Omega_{n+1},\\Omega_{r}\\mid\\Omega_{[1,n+1]})=m i n(\\mathbb{X})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "121 For the meaning of $\\Omega_{t},\\Omega_{r},\\Omega_{[1,n]}$ , and $\\Omega_{[1,n+1]}$ , see Table 1. ", "page_idx": 4}, {"type": "text", "text": "122 Proof. There are $n$ possibilities of the MMJ path from $\\Omega_{n+1}$ to $\\Omega_{r}$ , under the context of $\\Omega_{[1,n+1]}$ ,   \n123 set $\\mathbb{X}$ enumerate them all. Each element of $\\mathbb{X}$ is the maximum jump of each possibility. Therefore,   \n124 according to the definition of MMJ distance, $M M J(\\Omega_{n+1},\\Omega_{r}\\mid\\Omega_{[1,n+1]})=m i n(\\mathbb{X})$ . \u53e3 ", "page_idx": 4}, {"type": "text", "text": "125 Corollary 1. Suppose $r\\in\\{1,2,\\ldots,N\\},p\\notin\\Omega,$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(t)=m a x(d(p,\\Omega_{t}),\\;M M J(\\Omega_{t},\\Omega_{r}\\mid\\Omega))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "126 ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{X}=\\{f(t)\\mid t\\in\\{1,2,\\dots,N\\}\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "127 then, ", "page_idx": 4}, {"type": "equation", "text": "$$\nM M J(p,\\Omega_{r}\\mid\\Omega+p)=m i n(\\mathbb{X})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "128 For the meaning of $\\Omega+p$ , see Table 1. ", "page_idx": 4}, {"type": "text", "text": "129 Proof. The proof follows the conclusion of Theorem 2. ", "page_idx": 4}, {"type": "text", "text": "130 Theorem 3. Suppose $i,j\\in\\{1,2,\\dots,n\\},$ , ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "131 ", "page_idx": 4}, {"type": "text", "text": "132 ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{x_{1}=M M J(\\Omega_{i},\\Omega_{j}\\mid\\Omega_{[1,n]})}\\\\ {t_{1}=M M J(\\Omega_{n+1},\\Omega_{i}\\mid\\Omega_{[1,n+1]})}\\\\ {t_{2}=M M J(\\Omega_{n+1},\\Omega_{j}\\mid\\Omega_{[1,n+1]})}\\\\ {x_{2}=m a x(t_{1},\\,t_{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "133 ", "page_idx": 4}, {"type": "text", "text": "134 then, ", "page_idx": 4}, {"type": "equation", "text": "$$\nM M J(\\Omega_{i},\\Omega_{j}\\mid\\Omega_{[1,n+1]})=m i n(x_{1},\\;x_{2})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "135 Proof. There are two possibilities of the MMJ path from $\\Omega_{i}$ to $\\Omega_{j}$ , under the context of $\\Omega_{[1,n+1]}$ :   \n136 $\\Omega_{n+1}$ is in the path or it is not in the path. $x_{2}$ is the min-max jump of the first possibility; $x_{1}$ is the   \n137 min-max jump of the second possibility. Therefore, according to the definition of MMJ distance,   \n138 $M M J(\\Omega_{i},\\Omega_{j}\\mid\\Omega_{[1,n+1]})=m i n(x_{1},\\;x_{2})$ . \u53e3 ", "page_idx": 4}, {"type": "text", "text": "139 4 Calculation of Min-Max-Jump distance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "140 We propose two methods to calculate the pairwise Min-Max-Jump distance matrix of a dataset. There   \n141 are other methods for calculating or estimating it, such as a modified SLINK algorithm (25), or with   \n142 Cartesian trees (26; 27), or from a sequence of nearest neighbor graphs (23), or a modified version of   \n143 the Floyd\u2013Warshall algorithm. ", "page_idx": 4}, {"type": "text", "text": "144 4.1 MMJ distance by recursion ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "145 The first method calculates $\\mathbb{M}_{\\Omega}$ by recursion. $\\mathbb{M}_{\\Omega}$ is the pairwise MMJ distance matrix of $\\Omega$ (Table   \n146 1). $\\mathbb{M}_{k,\\Omega_{[1,k]}}$ is the MMJ distance matrix of the first $k$ points of $\\Omega$ (Table 1). Note $\\mathbb{M}_{2,\\Omega_{[1,2]}}$ is simple   \n147 to calculate. $\\mathbb{M}_{\\Omega}=\\mathbb{M}_{N,\\Omega_{[1,N]}}$ . $\\mathbb{M}_{\\Omega}$ is a $N\\times N$ symmetric matrix. Rows and columns of $\\mathbb{M}_{\\Omega}$ are   \n148 indexed from 1 to N.   \n149 Step 7 of Algorithm 1 can be calculated with the conclusion of Theorem 2; Step 12 of Algorithm 1   \n150 can be calculated with the conclusion of Theorem 3. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "151 Algorithm 1 has complexity of ${\\mathcal{O}}(n^{3})$ , where $n$ is the cardinality of Set $\\Omega$ . ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 MMJ distance by recursion ", "page_idx": 5}, {"type": "text", "text": "Input: \u2126   \nOutput: $\\mathbb{M}_{\\Omega}$   \n1: function MMJ_BY_RECURSION $(\\Omega)$   \n2: $N\\gets l e n g t h(\\Omega)$   \n3: Initialize $\\mathbb{M}_{\\Omega}$ with zeros   \n4: Calculate $\\mathbb{M}_{2,\\Omega_{[1,2]}}$ , fill in $\\mathbb{M}_{\\Omega}[1,2]$ and $\\mathbb{M}_{\\Omega}[2,1]$   \n5: for $n\\leftarrow3$ to $N$ do   \n6: for $r\\gets1$ to $n-1$ do   \n7: Calculate $M M J(\\Omega_{n},\\Omega_{r}\\mid\\Omega_{[1,n]})$ , fill in $\\mathbb{M}_{\\Omega}[n,r]$ and $\\mathbb{M}_{\\Omega}[r,n]$   \n8: end for   \n9: for $i\\gets1$ to $n-1$ do   \n10: for $j\\leftarrow1$ to $n-1$ do   \n11: if $i<j$ then   \n12: Calculate $M M J(\\Omega_{i},\\Omega_{j}\\mid\\Omega_{[1,n]})$ , update $\\mathbb{M}_{\\Omega}[i,j]$ and $\\mathbb{M}_{\\Omega}[j,i]$   \n13: end if   \n14: end for   \n15: end for   \n16: end for   \n17: return $\\mathbb{M}_{\\Omega}$   \n18: end function ", "page_idx": 5}, {"type": "text", "text": "152 4.2 MMJ distance by calculation and copy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "153 According to the conclusion of Theorem 1, there are many duplicated values in $\\mathbb{M}_{\\Omega}$ . So in the second   \n154 method we can calculate the MMJ distance value in one position and copy it to other positions in   \n155 M\u2126.   \n156 A well-known fact about MMJ distance is: \"the path between any two nodes in a minimum spanning   \n157 tree (MST) is a minimax path.\" A minimax path in an undirected graph is a path between two vertices   \n158 $v,\\,w$ that minimizes the maximum weight of the edges on the path. That is to say, it is a MMJ path.   \n159 By utilizing this fact, we propose Algorithm 2. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Algorithm 2 MMJ distance by Calculation and Copy   \nInput: \u2126   \nOutput: $\\mathbb{M}_{\\Omega}$   \n1: function MMJ_CALCULATION_AND_COPY(\u2126)   \n2: Initialize $\\mathbb{M}_{\\Omega}$ with zeros   \n3: Construct a MST of $\\Omega$ , noted $T$   \n4: Sort edges of $T$ from large to small, generate a list, noted $L$   \n5: for e in $L$ do   \n6: Remove $e$ from $T$ . It will result in two connected sub-trees, $T_{1}$ and $T_{2}$ ;   \n7: Traverse $T_{1}$ and $T_{2}$ ;   \n8: For all pair of nodes $(p,q)$ , where $p\\in T_{1}$ , $q\\in T_{2}$ . Fill in $\\mathbb{M}_{\\Omega}[p,q]$ and $\\mathbb{M}_{\\Omega}[q,p]$ with the weight of $e$ .   \n9: end for   \n10: return $\\mathbb{M}_{\\Omega}$   \n11: end function ", "page_idx": 5}, {"type": "text", "text": "160 The complexity of Algorithm 2 is $O(n^{2})$ . Because the construction of a MST of a complete graph is   \n161 ${\\mathcal{O}}(n^{2})$ . During the \"for\" part (Step 5 to 9) of the algorithm, it accesses each cell of $\\mathbb{M}_{\\Omega}$ only once.   \n162 Unlike Algorithm 1, which accesses each cell of $\\mathbb{M}_{\\Omega}$ for ${\\mathcal{O}}(n)$ times. The merit of the \"Calculation   \n163 and Copy\" method is that it is easier to understand than using the Cartesian trees (26; 27). ", "page_idx": 5}, {"type": "image", "img_path": "2BOb4SvDFr/tmp/e172af86a2ed01c649472c8e2ab5675b599e4ef268485e84c66f44531879d12a.jpg", "img_caption": ["Figure 2: Standard K-means vs. MMJ-K-means "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "164 5 Applications of Min-Max-Jump distance ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "165 We explore two applications of MMJ distance, and test the applications with experiments. All the   \n166 MMJ distances in the experiments are calculated with Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "167 5.1 MMJ-based K-means ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "168 K-means clustering aims to partition $n$ observations into $k$ clusters in which each observation belongs   \n169 to the cluster with the nearest mean (cluster center or centroid), serving as a prototype of the cluster   \n170 (28). Standard K-means uses Euclidean distance. We can revise K-means to use Min-Max-Jump   \n171 distance, with the cluster centroid replaced by the Semantic Center of Mass (SCOM) (particularly,   \n172 One-SCOM) of each cluster. For the definition of SCOM, see a previous paper (29). One-SCOM is   \n173 like medoid, but has some difference from medoid. Section 6.3 of (29) compares One-SCOM and   \n174 medoid. In simple terms, the One-SCOM of a set of points, is the point which has the smallest sum   \n175 of squared distances to all points in the set.   \n176 Standard K-means usually cannot deal with non-spherical shaped data, such as the ones in Figure 2.   \n177 MMJ-based K-means (MMJ-K-means) can cluster such irregularly shaped data. Figure 2 compares   \n178 Standard K-means and MMJ-K-means, on clustering three data which come from the scikit-learn   \n179 project (30). Figure 3 are eight more samples of MMJ-K-means. The data sources corresponding to   \n180 the data IDs can be found at this URL (temporarily hidden for double blind review).   \n181 It can be seen MMJ-K-means can (almost) work properly for clustering the 11 data, which have   \n182 different kinds of shapes. The black circles are Border points (Definition 2), the red stars are the center   \n183 (One-SCOM) of each cluster. During training of MMJ-K-means, the Border points are randomly   \n184 allocated to one of its nearest centers. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "185 Definition 2. Border point ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "186 A point is defined to be a Border point if its nearest mean (center, centroid, or One-SCOM) is not   \n187 unique.   \n188 Compared with other clustering models that can handle irregularly shaped data, such as Spectral   \n189 clustering or the Density-Based Spatial Clustering of Applications with Noise (DBSCAN), the merit   \n190 of MMJ-K-means is its simplicity; the logic of MMJ-K-means is as simple as K-means. We just   \n191 replace the Euclidean distance with MMJ distance, and the centroid with the Semantic Center of   \n192 Mass (SCOM). ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "2BOb4SvDFr/tmp/5f8945f3f6d235359d1ef235ad6c42f9149ce8c0a39a11f85c792f24601e777c.jpg", "img_caption": ["Figure 3: Eight more samples of MMJ-K-means "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "2BOb4SvDFr/tmp/025b8ec2ccec080b264aa9a15ab5ea3a12cf37700ea0547973e5ce435613aee4.jpg", "table_caption": ["Table 2: Accuracy of the ten indices "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "193 5.2 MMJ-based internal clustering evaluation index ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "194 Calinski-Harabasz index, Silhouette coefficient, and Davies-Bouldin index are three of the most   \n195 popular techniques for internal clustering evaluation. They are used to calculate the goodness of a   \n196 clustering technique. ", "page_idx": 7}, {"type": "text", "text": "197 The Silhouette coefficient for a single sample is given as: ", "page_idx": 7}, {"type": "equation", "text": "$$\ns=\\frac{b-a}{m a x(a,b)}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "198 where $a$ is the mean distance between a sample and all other points in the same class. $b$ is the mean   \n199 distance between a sample and all other points in the next nearest cluster. The Silhouette coefficient   \n200 for a set of samples is given as the mean of Silhouette coefficient for each sample.   \n201 We can also revise Silhouette coefficient to use Min-Max-Jump distance, forming a new internal   \n202 clustering evaluation index called MMJ-based Silhouette coefficient (MMJ-SC). We tested the   \n203 performance of MMJ-SC with the 145 datasets mentioned in another paper(31). MMJ-SC obtained   \n204 a good performance score compared with the other seven internal clustering evaluation indices   \n205 mentioned in the paper(31). Readers can check Table 2 and compare with Table 5 of Liu\u2019s paper(31).   \n206 MMJ-based Calinski-Harabasz index (MMJ-CH) and MMJ-based Davies-Bouldin index (MMJ-DB)   \n207 were also tested. In calculation of these two indices, besides using MMJ distance, the center/centroid   \n208 of a cluster is replaced by the One-SCOM of the cluster again, as in MMJ-K-means. It can be seen   \n209 that MMJ distance systematically improves the three internal clustering evaluation indices (Table   \n210 2). The best performer is MMJ-CH, which achieves an accuracy of 90/145. The accuracy of an   \n211 index is computed by evaluating the index\u2019s ability of recognizing the best partition of a dataset from   \n212 hundreds of candidate partitions(31). ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "213 5.2.1 Using MMJ-SC in CNNI ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "214 The Clustering with Neural Network and Index (CNNI) model uses a Neural Network to cluster   \n215 data points. Training of the Neural Network mimics supervised learning, with an internal clustering   \n216 evaluation index acting as the loss function (24). CNNI with standard Silhouette coefficient as the   \n217 internal clustering evaluation index, cannot deal with non-flat geometry data, such as data B and   \n218 data C in Figure 2. MMJ-SC gives CNNI model the capability of processing non-flat geometry data.   \n219 E.g., Figure 4 is the clustering result and decision boundary of data B by CNNI using MMJ-SC.   \n220 It uses Neural Network C of the CNNI paper (24). CNNI equipped with MMJ-SC, achieves the   \n221 first inductive clustering model that can deal with non-flat geometry data (24). For the definition of   \n222 non-flat geometry data, see this1Stackexchange question. ", "page_idx": 7}, {"type": "image", "img_path": "2BOb4SvDFr/tmp/a1363892b472ce679e8a37cd9de5889cf5e5540b26d86b3a68cd2919bbedbe61.jpg", "img_caption": ["Figure 4: Clustering result and decision boundary of data B by CNNI using MMJ-SC "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "223 6 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "224 6.1 Using PAM ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "225 Since One-SCOM is like medoid, in MMJ-K-means, we can also use the Partitioning Around Medoids   \n226 (PAM) algorithm or its variants to find the One-SCOMs (32). ", "page_idx": 8}, {"type": "text", "text": "227 6.2 Multiple One-SCOMs in one cluster ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "228 There might be multiple One-SCOM points in a cluster, which have the same smallest sum of squared   \n229 distances to all the points in the cluster. Usually they are not far from each other. We can arbitrarily   \n230 choose one or keep them all. If we keep them all, then the One-SCOM of a cluster is not a point, but a   \n231 set of points. If the One-SCOM is a set, when calculating a point\u2019s MMJ distance to the One-SCOM   \n232 of a cluster, we can select the minimum of the point\u2019s MMJ distances to all the One-SCOM points. ", "page_idx": 8}, {"type": "text", "text": "233 6.3 Differentiating border points ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "234 Border points defined in Definition 2 can further be differentiated as weak and strong border points.   \n235 Definition 3. Weak Border Point (WBP)   \n236 A point is defined to be a WBP if its nearest mean (center or One-SCOM) is not unique but less than   \n237 $K$ , where $K$ is the number of clusters. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "238 Definition 4. Strong Border Point (SBP) ", "page_idx": 8}, {"type": "text", "text": "239 A point is defined to be a SBP if its nearest mean (center or One-SCOM) is not unique and equals $K$ ,   \n240 where $K$ is the number of clusters.   \n241 Then we can process different kinds of border points with different strategies. E.g., deeming the   \n242 Strong Border Points as outliers and removing them. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "243 7 Conclusion and Future Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "244 We proposed two algorithms for calculating Min-Max-Jump distance (MMJ distance), and tested   \n245 two applications of it: MMJ-based K-means and MMJ-based internal clustering evaluation index.   \n246 MMJ-K-means overcomes a big drawback of K-means, improving its ability of clustering, so that it   \n247 can handle irregularly shaped clusters. We claim MMJ-CH is the SOTA (state-of-the-art) internal   \n248 clustering evaluation index, which achieves an accuracy of 90/145. To thoroughly test the internal   \n249 clustering evaluation indices, we conducted an experiment on a set of 145 datasets. A normal   \n250 Machine Learning paper usually uses several or dozens of datasets to test their models or algorithms.   \n251 In summary, MMJ distance has good capability and potentiality in Machine Learning. Further   \n252 research may test its applications in other models, such as other clustering evaluation indices. ", "page_idx": 8}, {"type": "text", "text": "253 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "254 [1] S. Z. Li and A. Jain, Eds., Hamming Distance. Boston, MA: Springer US, 2009, pp. 668\u2013668.   \n255 [Online]. Available: https://doi.org/10.1007/978-0-387-73003-5_956   \n256 [2] D. Sinwar and R. Kaushik, \u201cStudy of euclidean and manhattan distance metrics using simple   \n257 k-means clustering,\u201d Int. J. Res. Appl. Sci. Eng. Technol, vol. 2, no. 5, pp. 270\u2013274, 2014.   \n258 [3] R. Coghetto, \u201cChebyshev distance,\u201d 2016.   \n259 [4] P. J. Groenen and K. Jajuga, \u201cFuzzy clustering with squared minkowski distances,\u201d Fuzzy Sets   \n260 and Systems, vol. 120, no. 2, pp. 227\u2013237, 2001.   \n261 [5] S. Fletcher, M. Z. Islam et al., \u201cComparing sets of patterns with the jaccard index,\u201d Australasian   \n262 Journal of Information Systems, vol. 22, 2018.   \n263 [6] N. R. Chopde and M. Nichat, \u201cLandmark based shortest path detection by using $\\mathbf{a}^{*}$ and haversine   \n264 formula,\u201d International Journal of Innovative Research in Computer and Communication   \n265 Engineering, vol. 1, no. 2, pp. 298\u2013302, 2013.   \n266 [7] T. Hastie, R. Tibshirani, J. H. Friedman, and J. H. Friedman, The elements of statistical learning:   \n267 data mining, inference, and prediction. Springer, 2009, vol. 2.   \n268 [8] R. Ostrovsky, Y. Rabani, L. J. Schulman, and C. Swamy, \u201cThe effectiveness of lloyd-type   \n269 methods for the k-means problem,\u201d Journal of the ACM (JACM), vol. 59, no. 6, pp. 1\u201322, 2013.   \n270 [9] D. Arthur and S. Vassilvitskii, \u201cK-means $^{\\star+}$ the advantages of careful seeding,\u201d in Proceedings   \n271 of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, 2007, pp. 1027\u20131035.   \n272 [10] H.-S. Park and C.-H. Jun, \u201cA simple and fast algorithm for $\\boldsymbol{\\mathrm{k}}$ -medoids clustering,\u201d Expert   \n273 systems with applications, vol. 36, no. 2, pp. 3336\u20133341, 2009.   \n274 [11] D. Pfitzner, R. Leibbrandt, and D. Powers, \u201cCharacterization and evaluation of similarity   \n275 measures for pairs of clusterings,\u201d Knowledge and Information Systems, vol. 19, no. 3, pp.   \n276 361\u2013394, 2009.   \n277 [12] S. Petrovi\u00b4c, \u201cA comparison between the silhouette index and the davies-bouldin index in   \n278 labelling ids clusters,\u201d 2006.   \n279 [13] S. Aranganayagi and K. Thangavel, \u201cClustering categorical data using silhouette coefficient as a   \n280 relocating measure,\u201d in International conference on computational intelligence and multimedia   \n281 applications (ICCIMA 2007), vol. 2. IEEE, 2007, pp. 13\u201317.   \n282 [14] J. C. Bezdek and N. R. Pal, \u201cCluster validation with generalized dunn\u2019s indices,\u201d in Proceedings   \n283 1995 second New Zealand international two-stream conference on artificial neural networks   \n284 and expert systems. IEEE Computer Society, 1995, pp. 190\u2013190.   \n285 [15] U. Maulik and S. Bandyopadhyay, \u201cPerformance evaluation of some clustering algorithms and   \n286 validity indices,\u201d IEEE Transactions on pattern analysis and machine intelligence, vol. 24,   \n287 no. 12, pp. 1650\u20131654, 2002.   \n288 [16] K. Y. Yeung and W. L. Ruzzo, \u201cDetails of the adjusted rand index and clustering algorithms,   \n289 supplement to the paper an empirical study on principal component analysis for clustering gene   \n290 expression data,\u201d Bioinformatics, vol. 17, no. 9, pp. 763\u2013774, 2001.   \n291 [17] R. R. Coifman, S. Lafon, A. B. Lee, M. Maggioni, B. Nadler, F. Warner, and S. W. Zucker,   \n292 \u201cGeometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion   \n293 maps,\u201d Proceedings of the national academy of sciences, vol. 102, no. 21, pp. 7426\u20137431, 2005.   \n294 [18] B. Fischer and J. M. Buhmann, \u201cPath-based clustering for grouping of smooth curves and texture   \n295 segmentation,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 25, no. 4,   \n296 pp. 513\u2013518, 2003.   \n297 [19] H. Chang and D.-Y. Yeung, \u201cRobust path-based spectral clustering,\u201d Pattern Recognition,   \n298 vol. 41, no. 1, pp. 191\u2013203, 2008.   \n299 [20] M. Pollack, \u201cThe maximum capacity through a network,\u201d Operations Research, vol. 8, no. 5,   \n300 pp. 733\u2013736, 1960.   \n301 [21] T. Hu, \u201cThe maximum capacity route problem,\u201d Operations Research, vol. 9, no. 6, pp. 898\u2013900,   \n302 1961.   \n303 [22] P. M. Camerini, \u201cThe min-max spanning tree problem and some extensions,\u201d Information   \n304 Processing Letters, vol. 7, no. 1, pp. 10\u201314, 1978.   \n305 [23] A. V. Little, M. Maggioni, and J. M. Murphy, \u201cPath-based spectral clustering: Guarantees,   \n306 robustness to outliers, and fast algorithms,\u201d J. Mach. Learn. Res., vol. 21, pp. 6:1\u20136:66, 2020.   \n307 [Online]. Available: http://jmlr.org/papers/v21/18-085.html   \n308 [24] G. Liu, \u201cClustering with neural network and index,\u201d arXiv preprint arXiv:2212.03853, 2022.   \n309 [25] R. Sibson, \u201cSlink: an optimally efficient algorithm for the single-link cluster method,\u201d The   \n310 computer journal, vol. 16, no. 1, pp. 30\u201334, 1973.   \n311 [26] N. Alon and B. Schieber, Optimal preprocessing for answering on-line product queries. Cite  \n312 seer, 1987.   \n313 [27] E. D. Demaine, G. M. Landau, and O. Weimann, \u201cOn cartesian trees and range minimum   \n314 queries,\u201d Algorithmica, vol. 68, pp. 610\u2013625, 2014.   \n315 [28] H.-H. Bock, \u201cClustering methods: a history of k-means algorithms,\u201d Selected contributions in   \n316 data analysis and classification, pp. 161\u2013172, 2007.   \n317 [29] G. Liu, \u201cTopic model supervised by understanding map,\u201d arXiv preprint arXiv:2110.06043,   \n318 2021.   \n319 [30] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,   \n320 P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,   \n321 M. Perrot, and E. Duchesnay, \u201cScikit-learn: Machine learning in Python,\u201d Journal of Machine   \n322 Learning Research, vol. 12, pp. 2825\u20132830, 2011.   \n323 [31] G. Liu, \u201cA new index for clustering evaluation based on density estimation,\u201d arXiv preprint   \n324 arXiv:2207.01294, 2022.   \n325 [32] E. Schubert and P. J. Rousseeuw, \u201cFast and eager k-medoids clustering: O (k) runtime improve  \n326 ment of the pam, clara, and clarans algorithms,\u201d Information Systems, vol. 101, p. 101804,   \n327 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "328 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "329 1. Claims   \n330 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n331 paper\u2019s contributions and scope? ", "page_idx": 11}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the claims made, including the contributions made in the paper. ", "page_idx": 11}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 11}, {"type": "text", "text": "345 2. Limitations ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [No] ", "page_idx": 11}, {"type": "text", "text": "Justification: We have used 145 datasets to test the models in the paper. Maybe it is not enough, we need more datasets to test the models. ", "page_idx": 11}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 11}, {"type": "text", "text": "377 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "78 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n79 a complete (and correct) proof?   \n380 Answer: [Yes]   \n381 Justification: Proofs of theoretical results have been provided in the paper.   \n382 Guidelines:   \n83 \u2022 The answer NA means that the paper does not include theoretical results.   \n84 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n85 referenced.   \n86 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n87 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n88 they appear in the supplemental material, the authors are encouraged to provide a short   \n89 proof sketch to provide intuition.   \n90 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n91 by formal proofs provided in appendix or supplemental material.   \n92 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "393 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "394 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n395 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n396 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 12}, {"type": "text", "text": "Justification: We have fully disclosed all the information needed to reproduce the main experimental results of the paper. ", "page_idx": 12}, {"type": "text", "text": "Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 12}, {"type": "text", "text": "432 5. Open access to data and code ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "433 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n434 tions to faithfully reproduce the main experimental results, as described in supplemental   \n435 material?   \n436 Answer: [Yes]   \n437 Justification: We provide an URL to data and code of the paper, to reproduce the main   \n438 experimental results.   \n439 Guidelines:   \n440 \u2022 The answer NA means that paper does not include experiments requiring code.   \n441 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/pu   \n442 blic/guides/CodeSubmissionPolicy) for more details.   \n443 \u2022 While we encourage the release of code and data, we understand that this might not be   \n444 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n445 including code, unless this is central to the contribution (e.g., for a new open-source   \n446 benchmark).   \n447 \u2022 The instructions should contain the exact command and environment needed to run to   \n448 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n449 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n450 \u2022 The authors should provide instructions on data access and preparation, including how   \n451 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n452 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n453 proposed method and baselines. If only a subset of experiments are reproducible, they   \n454 should state which ones are omitted from the script and why.   \n455 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n456 versions (if applicable).   \n457 \u2022 Providing as much information as possible in supplemental material (appended to the   \n458 paper) is recommended, but including URLs to data and code is permitted.   \n459 6. Experimental Setting/Details   \n460 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n461 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n462 results?   \n463 Answer: [Yes]   \n464 Justification: Full details are provided with the code.   \n465 Guidelines:   \n466 \u2022 The answer NA means that the paper does not include experiments.   \n467 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n468 that is necessary to appreciate the results and make sense of them.   \n469 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n470 material.   \n471 7. Experiment Statistical Significance   \n472 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n473 information about the statistical significance of the experiments?   \n474 Answer: [NA]   \n475 Justification: The paper does not contain statistical experimental results.   \n476 Guidelines:   \n477 \u2022 The answer NA means that the paper does not include experiments.   \n478 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n479 dence intervals, or statistical significance tests, at least for the experiments that support   \n480 the main claims of the paper.   \n481 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n482 example, train/test split, initialization, random drawing of some parameter, or overall   \n483 run with given experimental conditions).   \n484 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n485 call to a library function, bootstrap, etc.)   \n486 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n487 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n488 of the mean.   \n489 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n490 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n491 of Normality of errors is not verified.   \n492 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n493 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n494 error rates).   \n495 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n496 they were calculated and reference the corresponding figures or tables in the text.   \n497 8. Experiments Compute Resources   \n498 Question: For each experiment, does the paper provide sufficient information on the com  \n499 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n500 the experiments?   \n501 Answer: [No]   \n502 Justification: The paper does not discuss about the efficiency of the models, only effective  \n503 ness and time complexity of the algorithms. Because efficiency can be affected by a lot of   \n504 factors, e.g., using $C++$ to implement is much faster than using python, and some minor   \n505 optimization of the codes may drastically improve the speed.   \n506 Guidelines:   \n507 \u2022 The answer NA means that the paper does not include experiments.   \n508 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n509 or cloud provider, including relevant memory and storage.   \n510 \u2022 The paper should provide the amount of compute required for each of the individual   \n511 experimental runs as well as estimate the total compute.   \n512 \u2022 The paper should disclose whether the full research project required more compute   \n513 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n514 didn\u2019t make it into the paper).   \n515 9. Code Of Ethics   \n516 Question: Does the research conducted in the paper conform, in every respect, with the   \n517 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n518 Answer: [Yes]   \n519 Justification: The research conducted in the paper conforms with the NeurIPS Code of   \n520 Ethics.   \n521 Guidelines:   \n522 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n523 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n524 deviation from the Code of Ethics.   \n525 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n526 eration due to laws or regulations in their jurisdiction).   \n527 10. Broader Impacts   \n528 Question: Does the paper discuss both potential positive societal impacts and negative   \n529 societal impacts of the work performed?   \n530 Answer: [NA]   \n531 Justification: There is no societal impact of the work performed.   \n532 Guidelines:   \n533 \u2022 The answer NA means that there is no societal impact of the work performed.   \n534 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n535 impact or why the paper does not address societal impact.   \n536 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n537 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n538 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n539 groups), privacy considerations, and security considerations.   \n540 \u2022 The conference expects that many papers will be foundational research and not tied   \n541 to particular applications, let alone deployments. However, if there is a direct path to   \n542 any negative applications, the authors should point it out. For example, it is legitimate   \n543 to point out that an improvement in the quality of generative models could be used to   \n544 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n545 that a generic algorithm for optimizing neural networks could enable people to train   \n546 models that generate Deepfakes faster.   \n547 \u2022 The authors should consider possible harms that could arise when the technology is   \n548 being used as intended and functioning correctly, harms that could arise when the   \n549 technology is being used as intended but gives incorrect results, and harms following   \n550 from (intentional or unintentional) misuse of the technology.   \n551 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n552 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n553 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n554 feedback over time, improving the efficiency and accessibility of ML).   \n555 11. Safeguards   \n556 Question: Does the paper describe safeguards that have been put in place for responsible   \n557 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n558 image generators, or scraped datasets)?   \n559 Answer: [NA]   \n560 Justification: The paper poses no such risks.   \n561 Guidelines:   \n562 \u2022 The answer NA means that the paper poses no such risks.   \n563 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n564 necessary safeguards to allow for controlled use of the model, for example by requiring   \n565 that users adhere to usage guidelines or restrictions to access the model or implementing   \n566 safety filters.   \n567 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n568 should describe how they avoided releasing unsafe images.   \n569 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n570 not require this, but we encourage authors to take this into account and make a best   \n571 faith effort.   \n572 12. Licenses for existing assets   \n573 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n574 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n575 properly respected?   \n576 Answer: [Yes]   \n577 Justification: The creators or original owners of assets used in the paper are properly credited.   \n578 The license and terms of use are explicitly mentioned and properly respected.   \n579 Guidelines:   \n580 \u2022 The answer NA means that the paper does not use existing assets.   \n581 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n582 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n583 URL.   \n584 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n585 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n586 service of that source should be provided.   \n587 \u2022 If assets are released, the license, copyright information, and terms of use in the package   \n588 should be provided. For popular datasets, paperswithcode.com/datasets has   \n589 curated licenses for some datasets. Their licensing guide can help determine the license   \n590 of a dataset.   \n591 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n592 the derived asset (if it has changed) should be provided.   \n593 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n594 the asset\u2019s creators.   \n595 13. New Assets   \n596 Question: Are new assets introduced in the paper well documented and is the documentation   \n597 provided alongside the assets?   \n598 Answer: [NA]   \n599 Justification: The paper does not release new assets.   \n600 Guidelines:   \n601 \u2022 The answer NA means that the paper does not release new assets.   \n602 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n603 submissions via structured templates. This includes details about training, license,   \n604 limitations, etc.   \n605 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n606 asset is used.   \n607 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n608 create an anonymized URL or include an anonymized zip file.   \n609 14. Crowdsourcing and Research with Human Subjects   \n610 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n611 include the full text of instructions given to participants and screenshots, if applicable, as   \n612 well as details about compensation (if any)?   \n613 Answer: [NA]   \n614 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n615 Guidelines:   \n616 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n617 human subjects.   \n618 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n619 tion of the paper involves human subjects, then as much detail as possible should be   \n620 included in the main paper.   \n621 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n622 or other labor should be paid at least the minimum wage in the country of the data   \n623 collector.   \n624 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n625 Subjects   \n626 Question: Does the paper describe potential risks incurred by study participants, whether   \n627 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n628 approvals (or an equivalent approval/review based on the requirements of your country or   \n629 institution) were obtained?   \n630 Answer: [NA]   \n631 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n632 Guidelines:   \n633 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n634 human subjects.   \n635 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n636 may be required for any human subjects research. If you obtained IRB approval, you   \n637 should clearly state this in the paper. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 17}]