[{"figure_path": "JWLiK3kKWQ/figures/figures_1_1.jpg", "caption": "Figure 1: There naturally exists a gap between prototype and image instance embeddings, but applying the same transformation shrinks such a gap. Fig.(a) shows that there naturally exists a gap, which resembles the \u201cmodality\u201d gap in visual language models, between prototype and image instance embeddings extracted from a frozen pre-trained backbone. However, Fig.(b) shows that the gap between the representations of prototypes and image instances is shrunk after applying the same representation transformation to both the image instance and prototype embeddings.", "description": "This figure shows the results of an experiment designed to visualize the gap between prototype and image embeddings.  The left panel (a) shows the natural gap between the embeddings extracted from a frozen pre-trained backbone. Applying the same transformation to both prototype and image instance embeddings reduces this gap, as shown in the right panel (b).  The gap's existence and the transformation's effect on it are key observations in the paper.", "section": "3 Revisit the Previous Adaptation Strategy"}, {"figure_path": "JWLiK3kKWQ/figures/figures_2_1.jpg", "caption": "Figure 2: The upper subfigure shows the URL pipeline which applies the same transformation to both prototype and image instance embeddings. The bottom subfigure shows the pipeline of CoPA, which tries to adapt two different representation transformation heads respectively for prototypes and image instances in the way of CLIP via substituting text prompts with prototype embeddings.", "description": "This figure compares the architectures of URL and CoPA.  URL uses a single transformation head applied to both prototype and image embeddings. CoPA, inspired by CLIP, uses separate transformation heads for prototypes and image embeddings to address the limitations of applying a single transformation. This allows CoPA to learn distinct representations that better capture the differences between prototypes (high-level class information) and image instances (low-level instance information).", "section": "2 Preliminary"}, {"figure_path": "JWLiK3kKWQ/figures/figures_4_1.jpg", "caption": "Figure 3: (a). The global minimum validation loss is achieved when the \"modality\" gap is enlarged. Fig. (a) depicts the validation loss landscape w.r.t the changes of the \"modality\" gap between prototype and image instance embeddings. The validation loss fails to achieve the global minimum at the original gap, and the global minimum can be achieved when the gap is enlarged. (b)-(c). The shared representation transformation fails to learn compact instance representation clusters. According to the visualization results of both prototype and image instance embeddings and their representations obtained with the same representation transformation, compared to the prototype and image instance embeddings extracted from the frozen pre-trained backbone (Fig. (b)), the shared transformation fails to learn image instance representations which are well clustered (Fig. (c)).", "description": "This figure shows that enlarging the gap between prototype and image embeddings leads to a lower validation loss, indicating better generalization.  It also demonstrates that using a shared transformation for both prototypes and images hinders the learning of compact and well-clustered image representations.", "section": "3 Revisit the Previous Adaptation Strategy"}, {"figure_path": "JWLiK3kKWQ/figures/figures_5_1.jpg", "caption": "Figure 4: The change of the scale of the upper bound of URL representation gaps during the adaptation.", "description": "The figure shows the change of the scale of the upper bound of URL representation gaps during the adaptation process for eight different datasets.  The y-axis represents the scale (\u221aM ||\u0398||F),  which is a measure used in Theorem 3.2 to bound the gap between prototype and image representations.  The x-axis represents the number of steps in the adaptation.  The graph shows that for all datasets, the scale remains below 1, indicating that the shared transformation used in URL shrinks the gap between prototypes and image instances.", "section": "3.3 Further Exploration on Shared Transformation"}, {"figure_path": "JWLiK3kKWQ/figures/figures_7_1.jpg", "caption": "Figure 5: (a). The gap between prototype and image instance representations is enlarged from 0.22 to 1.38 by CoPA. Such a phenomenon is consistent with that demonstrated by Liang et al. [32]. (b). The clusters of image instance representations learned from CoPA. The more compact clusters reveal that COPA learns better instance representations. (c). The validation loss achieves its global minimum at the gap learned by CoPA, which indicates that CoPA can improve the generalization performance.", "description": "This figure shows three plots. The first plot (a) is a UMAP visualization of prototype and image embeddings showing that the gap between them is enlarged after applying CoPA. The second plot (b) visualizes the image instance representation clusters obtained with CoPA and shows that the clusters are more compact than those learned by URL. The last plot (c) shows the validation loss curve with respect to the Euclidean distance, indicating that CoPA achieves the global minimum at an enlarged gap.", "section": "5 Experiments"}, {"figure_path": "JWLiK3kKWQ/figures/figures_8_1.jpg", "caption": "Figure 3: (a). The global minimum validation loss is achieved when the \"modality\" gap is enlarged. Fig. (a) depicts the validation loss landscape w.r.t the changes of the \"modality\" gap between prototype and image instance embeddings. The validation loss fails to achieve the global minimum at the original gap, and the global minimum can be achieved when the gap is enlarged. (b)-(c). The shared representation transformation fails to learn compact instance representation clusters. According to the visualization results of both prototype and image instance embeddings and their representations obtained with the same representation transformation, compared to the prototype and image instance embeddings extracted from the frozen pre-trained backbone (Fig. (b)), the shared transformation fails to learn image instance representations which are well clustered (Fig. (c)).", "description": "This figure shows that enlarging the gap between prototype and image embeddings leads to a lower validation loss and better-formed clusters.  The shared transformation used in previous methods shrinks this gap and produces poor clusters.", "section": "3 Revisit the Previous Adaptation Strategy"}, {"figure_path": "JWLiK3kKWQ/figures/figures_16_1.jpg", "caption": "Figure 1: There naturally exists a gap between prototype and image instance embeddings, but applying the same transformation shrinks such a gap. Fig.(a) shows that there naturally exists a gap, which resembles the \u201cmodality\u201d gap in visual language models, between prototype and image instance embeddings extracted from a frozen pre-trained backbone. However, Fig.(b) shows that the gap between the representations of prototypes and image instances is shrunk after applying the same representation transformation to both the image instance and prototype embeddings.", "description": "The figure shows the results of an experiment designed to visualize the gap between prototype and image instance embeddings.  Figure 1(a) demonstrates that a gap exists between prototype and image embeddings when extracted directly from a pre-trained backbone. Figure 1(b) shows that applying the same transformation to both prototype and image embeddings reduces this gap. This reduction in the gap suggests that different transformations are needed for prototypes and image instances to maintain the distinction in their representation.", "section": "3 Revisit the Previous Adaptation Strategy"}, {"figure_path": "JWLiK3kKWQ/figures/figures_18_1.jpg", "caption": "Figure 8: Analysis results of three kinds of prototypes: \u201cMax\u201d, \u201cMax Sample\u201d and \u201cAverage\u201d prototypes on ImageNet dataset. (a). Average accuracy of URL with three kinds of prototypes on ImageNet dataset. The results are obtained by averaging 5 reproductions with different random seeds (i.e. 41-45). (b). The distribution of both positive and negative similarities with the \u201cMax\u201d prototype. (c). The distribution of both positive and negative similarities with the \u201cMax Sample\u201d prototype. (d). The distribution of both positive and negative similarities with the \u201cAverage\u201d prototype.", "description": "This figure compares three different prototype selection methods: Max, Max Sample, and Average.  It shows the average accuracy of URL (a representative adaptation strategy from prior work) using each prototype on the ImageNet dataset.  Subplots (b), (c), and (d) display the distributions of positive and negative similarities (similarity of samples to their own class prototype versus similarity to other class prototypes) for each prototype type, demonstrating the differences in their representational properties and how these properties relate to classification performance.", "section": "C Detailed Study on Prototype Selection"}, {"figure_path": "JWLiK3kKWQ/figures/figures_19_1.jpg", "caption": "Figure 1: There naturally exists a gap between prototype and image instance embeddings, but applying the same transformation shrinks such a gap. Fig.(a) shows that there naturally exists a gap, which resembles the \u201cmodality\u201d gap in visual language models, between prototype and image instance embeddings extracted from a frozen pre-trained backbone. However, Fig.(b) shows that the gap between the representations of prototypes and image instances is shrunk after applying the same representation transformation to both the image instance and prototype embeddings.", "description": "This figure shows an analysis of the gap between prototype and image embeddings. (a) shows that there is a gap between the embeddings of prototypes and image instances extracted from a frozen pre-trained backbone. This gap is similar to the modality gap observed in visual language models. (b) shows that applying the same transformation to both prototype and image embeddings shrinks this gap.  This suggests that using separate transformations for prototypes and images might be beneficial.", "section": "3 Revisit the Previous Adaptation Strategy"}, {"figure_path": "JWLiK3kKWQ/figures/figures_25_1.jpg", "caption": "Figure 3: (a). The global minimum validation loss is achieved when the \"modality\" gap is enlarged. Fig. (a) depicts the validation loss landscape w.r.t the changes of the \"modality\" gap between prototype and image instance embeddings. The validation loss fails to achieve the global minimum at the original gap, and the global minimum can be achieved when the gap is enlarged. (b)-(c). The shared representation transformation fails to learn compact instance representation clusters. According to the visualization results of both prototype and image instance embeddings and their representations obtained with the same representation transformation, compared to the prototype and image instance embeddings extracted from the frozen pre-trained backbone (Fig. (b)), the shared transformation fails to learn image instance representations which are well clustered (Fig. (c)).", "description": "This figure shows the validation loss landscape with respect to the changes in the modality gap between prototype and image embeddings. It demonstrates that the global minimum validation loss is achieved when the gap is enlarged, indicating that a larger gap is beneficial. The figure also includes visualizations of embedding clusters, showing that the shared representation transformation fails to learn compact instance representation clusters compared to the proposed method.", "section": "3 Revisit the Previous Adaptation Strategy"}, {"figure_path": "JWLiK3kKWQ/figures/figures_27_1.jpg", "caption": "Figure 10: The comparison of running time between URL and CoPA.", "description": "The figure shows the comparison of running time per iteration between the URL and CoPA methods on 13 datasets.  CoPA is generally faster than URL, with the largest differences observed on datasets like fungi and ilsvrc_2012.", "section": "F.4 Efficiency of CoPA"}, {"figure_path": "JWLiK3kKWQ/figures/figures_32_1.jpg", "caption": "Figure 8: Analysis results of three kinds of prototypes: \u201cMax\u201d, \u201cMax Sample\u201d and \u201cAverage\u201d prototypes on ImageNet dataset. (a). Average accuracy of URL with three kinds of prototypes on ImageNet dataset. The results are obtained by averaging 5 reproductions with different random seeds (i.e. 41-45). (b). The distribution of both positive and negative similarities with the \u201cMax\u201d prototype. (c). The distribution of both positive and negative similarities with the \u201cMax Sample\u201d prototype. (d). The distribution of both positive and negative similarities with the \u201cAverage\u201d prototype.", "description": "The figure shows the results of an experiment comparing three different prototype selection methods ('Max', 'Max Sample', and 'Average') for few-shot image classification using the URL baseline.  The comparison includes the average accuracy achieved by each prototype selection method on the ImageNet dataset and the distribution of positive (similarity between the prototype and samples of the same class) and negative (similarity between the prototype and samples from different classes) similarities. This helps in determining which prototype selection method produces the best prototypes for few-shot classification.", "section": "C Detailed Study on Prototype Selection"}, {"figure_path": "JWLiK3kKWQ/figures/figures_32_2.jpg", "caption": "Figure 3: (a). The global minimum validation loss is achieved when the \"modality\" gap is enlarged. Fig. (a) depicts the validation loss landscape w.r.t the changes of the \"modality\" gap between prototype and image instance embeddings. The validation loss fails to achieve the global minimum at the original gap, and the global minimum can be achieved when the gap is enlarged. (b)-(c). The shared representation transformation fails to learn compact instance representation clusters. According to the visualization results of both prototype and image instance embeddings and their representations obtained with the same representation transformation, compared to the prototype and image instance embeddings extracted from the frozen pre-trained backbone (Fig. (b)), the shared transformation fails to learn image instance representations which are well clustered (Fig. (c)).", "description": "This figure shows that the global minimum validation loss is achieved when the gap between prototype and image instance embeddings is enlarged.  The shared representation transformation fails to learn compact instance representation clusters because it shrinks the gap between prototypes and images. ", "section": "3 Revisit the Previous Adaptation Strategy"}, {"figure_path": "JWLiK3kKWQ/figures/figures_32_3.jpg", "caption": "Figure 1: There naturally exists a gap between prototype and image instance embeddings, but applying the same transformation shrinks such a gap. Fig.(a) shows that there naturally exists a gap, which resembles the \u201cmodality\u201d gap in visual language models, between prototype and image instance embeddings extracted from a frozen pre-trained backbone. However, Fig.(b) shows that the gap between the representations of prototypes and image instances is shrunk after applying the same representation transformation to both the image instance and prototype embeddings.", "description": "This figure visualizes the distributions of prototype and image instance embeddings.  Figure 1(a) shows a clear gap between prototype and image embeddings extracted from a pre-trained backbone, similar to the modality gap observed in visual language models.  Figure 1(b) demonstrates that applying the same transformation to both prototype and image embeddings shrinks this gap, highlighting the difference in information captured by prototype and image embeddings.", "section": "3 Revisit the Previous Adaptation Strategy"}, {"figure_path": "JWLiK3kKWQ/figures/figures_32_4.jpg", "caption": "Figure 3: (a). The global minimum validation loss is achieved when the \\\"modality\\\" gap is enlarged. Fig. (a) depicts the validation loss landscape w.r.t the changes of the \\\"modality\\\" gap between prototype and image instance embeddings. The validation loss fails to achieve the global minimum at the original gap, and the global minimum can be achieved when the gap is enlarged. (b)-(c). The shared representation transformation fails to learn compact instance representation clusters. According to the visualization results of both prototype and image instance embeddings and their representations obtained with the same representation transformation, compared to the prototype and image instance embeddings extracted from the frozen pre-trained backbone (Fig. (b)), the shared transformation fails to learn image instance representations which are well clustered (Fig. (c)).", "description": "The figure shows that enlarging the gap between prototype and image embeddings leads to a lower validation loss, suggesting that the shared transformation used in previous methods constrains learning and prevents the formation of well-defined clusters.  The optimal performance is obtained with an enlarged gap between the embeddings.", "section": "3 Revisit the Previous Adaptation Strategy"}, {"figure_path": "JWLiK3kKWQ/figures/figures_32_5.jpg", "caption": "Figure 8: Analysis results of three kinds of prototypes: \u201cMax\u201d, \u201cMax Sample\u201d and \u201cAverage\u201d prototypes on ImageNet dataset. (a). Average accuracy of URL with three kinds of prototypes on ImageNet dataset. The results are obtained by averaging 5 reproductions with different random seeds (i.e. 41-45). (b). The distribution of both positive and negative similarities with the \u201cMax\u201d prototype. (c). The distribution of both positive and negative similarities with the \u201cMax Sample\u201d prototype. (d). The distribution of both positive and negative similarities with the \u201cAverage\u201d prototype.", "description": "This figure compares three different prototype selection methods: Max, Max Sample, and Average.  It shows the average accuracy of the URL method using each prototype type on the ImageNet dataset.  Subplots (b), (c), and (d) display the distributions of positive and negative similarities for each prototype type, illustrating their effectiveness in distinguishing between samples within and outside of their assigned classes.", "section": "C Detailed Study on Prototype Selection"}, {"figure_path": "JWLiK3kKWQ/figures/figures_33_1.jpg", "caption": "Figure 8: Analysis results of three kinds of prototypes: \u201cMax\u201d, \u201cMax Sample\u201d and \u201cAverage\u201d prototypes on ImageNet dataset. (a). Average accuracy of URL with three kinds of prototypes on ImageNet dataset. The results are obtained by averaging 5 reproductions with different random seeds (i.e. 41-45). (b). The distribution of both positive and negative similarities with the \u201cMax\u201d prototype. (c). The distribution of both positive and negative similarities with the \u201cMax Sample\u201d prototype. (d). The distribution of both positive and negative similarities with the \u201cAverage\u201d prototype.", "description": "This figure analyzes three different prototype selection methods for few-shot classification: Max, Max Sample, and Average.  It shows the average accuracy of the URL method using each prototype type on the ImageNet dataset.  Additionally, it presents the distributions of positive (similarity between a sample and its own class prototype) and negative (similarity between a sample and prototypes from other classes) similarities for each prototype type, providing insights into their discriminative power. ", "section": "C Detailed Study on Prototype Selection"}, {"figure_path": "JWLiK3kKWQ/figures/figures_33_2.jpg", "caption": "Figure 3: (a). The global minimum validation loss is achieved when the \"modality\" gap is enlarged. Fig. (a) depicts the validation loss landscape w.r.t the changes of the \"modality\" gap between prototype and image instance embeddings. The validation loss fails to achieve the global minimum at the original gap, and the global minimum can be achieved when the gap is enlarged. (b)-(c). The shared representation transformation fails to learn compact instance representation clusters. According to the visualization results of both prototype and image instance embeddings and their representations obtained with the same representation transformation, compared to the prototype and image instance embeddings extracted from the frozen pre-trained backbone (Fig. (b)), the shared transformation fails to learn image instance representations which are well clustered (Fig. (c)).", "description": "This figure shows that enlarging the gap between prototype and image embeddings leads to a lower validation loss.  It also shows that using the same transformation for both prototypes and images results in poorly formed clusters, while separate transformations lead to better-formed clusters.", "section": "3 Revisit the Previous Adaptation Strategy"}, {"figure_path": "JWLiK3kKWQ/figures/figures_33_3.jpg", "caption": "Figure 8: Analysis results of three kinds of prototypes: \u201cMax\u201d, \u201cMax Sample\u201d and \u201cAverage\u201d prototypes on ImageNet dataset. (a). Average accuracy of URL with three kinds of prototypes on ImageNet dataset. The results are obtained by averaging 5 reproductions with different random seeds (i.e. 41-45). (b). The distribution of both positive and negative similarities with the \u201cMax\u201d prototype. (c). The distribution of both positive and negative similarities with the \u201cMax Sample\u201d prototype. (d). The distribution of both positive and negative similarities with the \u201cAverage\u201d prototype.", "description": "This figure compares three different prototype selection methods for few-shot classification on ImageNet: Max, Max Sample, and Average.  It shows the average accuracy of the URL method using each prototype type, along with the distributions of positive and negative similarities for each.  Positive similarity refers to the similarity between a sample and its own class prototype, while negative similarity measures similarity with prototypes from other classes. The results indicate that the \"Average\" prototype generally outperforms the others in terms of accuracy and similarity distributions, suggesting it is the most effective prototype type in this context.", "section": "C Detailed Study on Prototype Selection"}, {"figure_path": "JWLiK3kKWQ/figures/figures_34_1.jpg", "caption": "Figure 1: There naturally exists a gap between prototype and image instance embeddings, but applying the same transformation shrinks such a gap. Fig.(a) shows that there naturally exists a gap, which resembles the \u201cmodality\u201d gap in visual language models, between prototype and image instance embeddings extracted from a frozen pre-trained backbone. However, Fig.(b) shows that the gap between the representations of prototypes and image instances is shrunk after applying the same representation transformation to both the image instance and prototype embeddings.", "description": "This figure shows the results of an experiment designed to visualize the gap between prototype and image instance embeddings.  Panel (a) shows the natural gap between prototypes and image embeddings extracted from a pre-trained backbone using UMAP visualization.  Panel (b) demonstrates that applying the same transformation to both prototype and image embeddings reduces this gap. This observation highlights a key problem addressed in the paper, specifically that assuming prototypes and image embeddings share the same transformation constrains optimal representation learning and limits the effectiveness of cross-domain few-shot classification.", "section": "3 Revisit the Previous Adaptation Strategy"}, {"figure_path": "JWLiK3kKWQ/figures/figures_34_2.jpg", "caption": "Figure 3: (a). The global minimum validation loss is achieved when the \"modality\" gap is enlarged. Fig. (a) depicts the validation loss landscape w.r.t the changes of the \"modality\" gap between prototype and image instance embeddings. The validation loss fails to achieve the global minimum at the original gap, and the global minimum can be achieved when the gap is enlarged. (b)-(c). The shared representation transformation fails to learn compact instance representation clusters. According to the visualization results of both prototype and image instance embeddings and their representations obtained with the same representation transformation, compared to the prototype and image instance embeddings extracted from the frozen pre-trained backbone (Fig. (b)), the shared transformation fails to learn image instance representations which are well clustered (Fig. (c)).", "description": "The figure shows an analysis of the effect of the gap between prototype and image embeddings on validation loss and representation cluster quality.  The leftmost subplot shows that validation loss is minimized when the gap is enlarged. The two rightmost subplots compare representation clusters generated with and without a shared transformation, revealing the shared transformation impedes the formation of compact clusters.", "section": "3 Revisit the Previous Adaptation Strategy"}, {"figure_path": "JWLiK3kKWQ/figures/figures_34_3.jpg", "caption": "Figure 3: (a). The global minimum validation loss is achieved when the \\\"modality\\\" gap is enlarged. Fig. (a) depicts the validation loss landscape w.r.t the changes of the \\\"modality\\\" gap between prototype and image instance embeddings. The validation loss fails to achieve the global minimum at the original gap, and the global minimum can be achieved when the gap is enlarged. (b)-(c). The shared representation transformation fails to learn compact instance representation clusters. According to the visualization results of both prototype and image instance embeddings and their representations obtained with the same representation transformation, compared to the prototype and image instance embeddings extracted from the frozen pre-trained backbone (Fig. (b)), the shared transformation fails to learn image instance representations which are well clustered (Fig. (c)).", "description": "This figure shows the validation loss landscape as a function of the gap between prototype and image embeddings.  The global minimum loss is obtained when this gap is enlarged, illustrating that the shared transformation used in previous methods is suboptimal.  The figure also includes visualizations showing the improved clustering of image representations when this gap is increased. ", "section": "3 Revisit the Previous Adaptation Strategy"}, {"figure_path": "JWLiK3kKWQ/figures/figures_35_1.jpg", "caption": "Figure 20: Embedding and representation clusters on Omniglot dataset.", "description": "This figure visualizes the embedding and representation clusters of the Omniglot dataset using three different methods: (a) Extracted embeddings, (b) URL representation, and (c) CoPA representation.  It demonstrates that CoPA learns more compact and clearly separated representation clusters compared to the other methods.  The visualization helps understand how different representation learning strategies impact the quality of cluster formations, which is critical for few-shot classification.", "section": "5.2 Discussion: Why CoPA Performs Better?"}, {"figure_path": "JWLiK3kKWQ/figures/figures_35_2.jpg", "caption": "Figure 3: (a). The global minimum validation loss is achieved when the \"modality\" gap is enlarged. Fig. (a) depicts the validation loss landscape w.r.t the changes of the \"modality\" gap between prototype and image instance embeddings. The validation loss fails to achieve the global minimum at the original gap, and the global minimum can be achieved when the gap is enlarged. (b)-(c). The shared representation transformation fails to learn compact instance representation clusters. According to the visualization results of both prototype and image instance embeddings and their representations obtained with the same representation transformation, compared to the prototype and image instance embeddings extracted from the frozen pre-trained backbone (Fig. (b)), the shared transformation fails to learn image instance representations which are well clustered (Fig. (c)).", "description": "The figure shows three subfigures. (a) shows the validation loss landscape with respect to the changes in the modality gap between prototype and image instance embeddings. (b) shows the embedding clusters of prototype and image instances from the frozen backbone. (c) shows the embedding clusters of prototype and image instances with the shared transformation. The global minimum validation loss occurs when the modality gap is enlarged. The shared transformation fails to generate compact representation clusters.", "section": "3 Revisit the Previous Adaptation Strategy"}, {"figure_path": "JWLiK3kKWQ/figures/figures_35_3.jpg", "caption": "Figure 3: (a). The global minimum validation loss is achieved when the \\\"modality\\\" gap is enlarged. Fig. (a) depicts the validation loss landscape w.r.t the changes of the \\\"modality\\\" gap between prototype and image instance embeddings. The validation loss fails to achieve the global minimum at the original gap, and the global minimum can be achieved when the gap is enlarged. (b)-(c). The shared representation transformation fails to learn compact instance representation clusters. According to the visualization results of both prototype and image instance embeddings and their representations obtained with the same representation transformation, compared to the prototype and image instance embeddings extracted from the frozen pre-trained backbone (Fig. (b)), the shared transformation fails to learn image instance representations which are well clustered (Fig. (c)).", "description": "This figure shows the relationship between the validation loss and the distance between prototype and image embeddings.  The global minimum validation loss is not at the original distance.  When the distance between the prototypes and image embeddings is enlarged, the validation loss decreases.  Furthermore, the figure shows the effect of a shared representation transformation on the clustering of embeddings.  A shared transformation results in less well-defined clusters compared to when separate transformations are used for prototypes and images.", "section": "3 Revisit the Previous Adaptation Strategy"}, {"figure_path": "JWLiK3kKWQ/figures/figures_35_4.jpg", "caption": "Figure 3: (a). The global minimum validation loss is achieved when the \"modality\" gap is enlarged. Fig. (a) depicts the validation loss landscape w.r.t the changes of the \"modality\" gap between prototype and image instance embeddings. The validation loss fails to achieve the global minimum at the original gap, and the global minimum can be achieved when the gap is enlarged. (b)-(c). The shared representation transformation fails to learn compact instance representation clusters. According to the visualization results of both prototype and image instance embeddings and their representations obtained with the same representation transformation, compared to the prototype and image instance embeddings extracted from the frozen pre-trained backbone (Fig. (b)), the shared transformation fails to learn image instance representations which are well clustered (Fig. (c)).", "description": "This figure shows that enlarging the gap between prototype and image embeddings leads to a lower validation loss and better clustering of image representations.  The shared transformation used in previous methods fails to achieve this optimal gap and representation clustering.", "section": "3 Revisit the Previous Adaptation Strategy"}, {"figure_path": "JWLiK3kKWQ/figures/figures_35_5.jpg", "caption": "Figure 3: (a). The global minimum validation loss is achieved when the \"modality\" gap is enlarged. Fig. (a) depicts the validation loss landscape w.r.t the changes of the \"modality\" gap between prototype and image instance embeddings. The validation loss fails to achieve the global minimum at the original gap, and the global minimum can be achieved when the gap is enlarged. (b)-(c). The shared representation transformation fails to learn compact instance representation clusters. According to the visualization results of both prototype and image instance embeddings and their representations obtained with the same representation transformation, compared to the prototype and image instance embeddings extracted from the frozen pre-trained backbone (Fig. (b)), the shared transformation fails to learn image instance representations which are well clustered (Fig. (c)).", "description": "The figure shows that the global minimum validation loss is achieved when the gap between prototype and image instance embeddings is enlarged.  The shared representation transformation fails to learn compact instance representation clusters. Applying the same transformation shrinks the gap and hinders the learning of clear clusters, whereas enlarging the gap improves the model's generalization performance.", "section": "3 Revisit the Previous Adaptation Strategy"}, {"figure_path": "JWLiK3kKWQ/figures/figures_36_1.jpg", "caption": "Figure 1: There naturally exists a gap between prototype and image instance embeddings, but applying the same transformation shrinks such a gap. Fig.(a) shows that there naturally exists a gap, which resembles the \u201cmodality\u201d gap in visual language models, between prototype and image instance embeddings extracted from a frozen pre-trained backbone. However, Fig.(b) shows that the gap between the representations of prototypes and image instances is shrunk after applying the same representation transformation to both the image instance and prototype embeddings.", "description": "This figure shows that there is a gap between prototype and image embeddings before applying a transformation. However, when the same transformation is applied to both, the gap shrinks. This supports the paper's claim that applying separate transformations for prototypes and images is beneficial.", "section": "3 Revisit the Previous Adaptation Strategy"}]