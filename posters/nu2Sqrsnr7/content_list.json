[{"type": "text", "text": "Compute-Optimal Solutions for Acoustic Wave Equation Using Hard-Constraint PINNs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 This paper explores the optimal imposition of hard constraints, strategic sampling   \n2 of PDEs, and computational domain scaling for solving the acoustic wave equation   \n3 within a specified computational budget. First, we derive a formula to systemat  \n4 ically enforce hard boundary and initial conditions in Physics-Informed Neural   \n5 Networks (PINNs), employing continuous functions within the PINN ansatz to   \n6 ensure that these conditions are satisfied. We demonstrate that optimally selecting   \n7 these functions significantly enhances the convergence of the solution. Secondly,   \n8 we introduce a Dynamic Amplitude-Focused Sampling (DAFS) method that opti  \n9 mizes the efficiency of hard-constraint PINNs under a fixed number of sampling   \n10 points. Leveraging these strategies, we develop an algorithm to determine the opti  \n11 mal computational domain size, given a computational budget. Our approach offers   \n12 a practical framework for domain decomposition in large-scale implementation of   \n13 acoustic wave equation systems. ", "page_idx": 0}, {"type": "text", "text": "14 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "15 The concept of using artificial neural networks to solve differential equations was first explored in the   \n16 1990s by Lagaris et al. [1998]. In the work of Lagaris et al. [1998], they developed an ansatz solution   \n17 that inherently satisfies the boundary conditions (BC) and the initial conditions (IC) of differential   \n18 equations. More recently, the advent of physics-informed neural networks (PINNs) was marked by   \n19 the influential study of Raissi et al. [2019]. This work leverages modern deep neural networks to solve   \n20 forward and inverse problems involving nonlinear partial differential equations (PDEs), incorporating   \n21 BCs and ICs through soft constraints in loss functions.   \n22 Subsequent research has introduced various modifications to PINNs to enhance their accuracy,   \n23 efficiency, and scalability [Lu et al., 2021a]. There are a couple of drawbacks for many PINNs with   \n24 soft constraints for BCs and ICs. The selection of weights and samples for BCs and ICs cannot   \n25 certainly be determined and requires many trial-and-error tests. Even when the loss function is   \n26 minimized, the BCs and ICs are not strictly satisfied. To target the scaling problems of general PDEs   \n27 and take advantage of parallel computing, XPINNs and FBPINNs have been developed based on   \n28 domain decomposition methods [Jagtap and Karniadakis, 2020, Shukla et al., 2021, Moseley et al.,   \n29 2023].   \n30 There are a few key points that these previous reseearches missed. First, how to formulate ansatz   \n31 solutions satisfying BCs and ICs, specifically the function multiplier of NN. Second, if BC and IC   \n32 are inheriently satisfied by constructing the ansatz solution, how to optimally sample the PDEs in the   \n33 training process. Furthermore, for the existing PINNs handling scaling problems, how to decompose   \n34 the domain to save the overall compute budget.   \n35 In this paper, we set up a 1D wave equation problem and investigate the optimal sampling and   \n36 constraint imposing method given a compute budget. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "37 The contributions of this paper are as follows. ", "page_idx": 1}, {"type": "text", "text": "38 \u2022 We systematically derived the implementation of hard BC and IC constraints in PINNs to   \n39 solve acoustic wave equations. We give a strategy to select basic functions in the PINN   \n40 ansatz solution that guarantee the satisfaction of BCs and ICs. We find that optimal selection   \n41 of the basic function in the PINN ansatz can improve the convergence of PINNs.   \n42 \u2022 We developed a Dynamic Amplitude-Focused Sampling (DAFS) algorithm to improve the   \n43 convergence of hard-constraint PINNs for wave equations given a fixed number of sampling   \n44 points.   \n45 \u2022 With the hard constraint and importance sampling strategies, we propose an algorithm   \n46 to find the optimal size of the computational given a compute budget. This domain size   \n47 optimization algorithm can help the domain decomposition-based PINNs for large-scale   \n48 problems save computational cost. ", "page_idx": 1}, {"type": "text", "text": "49 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "50 Hard constraint Hard constraint PINNs can guarantee the satisfaction of BCs, ICs, symmetries,   \n51 and/or conservation laws. There are comprehensive studies of embedding BCs in PINNs. Lu   \n52 et al. [2021b] demontrated various ansatz equations to strictly meet Dirichlet and periodic BCs,   \n53 and proposed the penalty method and the augamented Lagrangian method to impose inequality   \n54 constraints as hard constraints. Liu et al. [2022] developed a unified ansatz formula to enforce the   \n55 Dirichlet, Neumann, and Robin boundary conditions for high-dimensional and geometrically complex   \n56 domains. Moseley et al. [2023] implemented the hard Dirichlet in the subdomain using a $\\operatorname{tanh}^{2}\\bar{(}\\omega x)$   \n57 function as the multiplier function of the neural networks in their FBPINN ansatz solution. However,   \n58 studies on how to impose both hard BC and IC constraints in PINNs for acoustic wave equations   \n59 that have a second-order time dirivative term are still limited. Alkhadhr and Almekkawy [2023]   \n60 compared the accuracy and performance of PINNs with a combination of hard-BC/soft-BC and   \n61 hard-IC/soft-IC for solving a 1D wave equation with a time-dependent point source function. This   \n62 implementation of the hard-IC only considers the satisfaction of the wavefield values at the initial   \n63 time $u(x,t=0)$ , but neglects the hard constraint of the first-order time derivative of the wavefield   \n64 $u(x,t)$ , i.e., $\\partial_{t}u(x,t=0)$ . Brecht et al. [2023] proposed improved physics-informed DeepONets   \n65 with hard constraints, and presented a numerical example of a 1D standing wave equation with   \n66 Dirichlet BCs. The DeepONet framework used in the paper has an inherent satisfaction of the initial   \n67 wavefield, but $\\partial_{t}u(x,t=0)$ is also neglected. This neglection does not affect the numerical results   \n68 for the 1D standing wave equation in their paper, since they simply assume $\\partial_{t}u(x,t=0)=0$ .   \n69 Strategic Sampling Many sampling algorithms have been developed to improve the training effi  \n70 ciency, mitigating failure modes of PINNs. [Wu et al., 2023] provided a comprehensive comparison of   \n71 ten sampling methods, including non-adaptive and residual-based adaptive methods. Daw et al. [2023]   \n72 proposed a Retain-Resample-Release (R3) Sampling algorithm to mitigate the failure propagation   \n73 during the training processes of PINNs. [Gao et al., 2023a,b] developed failure informed adamptive   \n74 sampling for PINNs, with the extentions of combining re-sampling and subset simulation. Yang et al.   \n75 [2023] introduced a Dynamic Mesh-Based Importance Sampling (DMIS) method to enhance the   \n76 training of PINNs. Additionally, [Zhang et al., 2024] proposed an annealed adaptive importance   \n77 sampling method for solving high-dimensional partial differential equations using PINNs.   \n78 Domain Scaling Computational domain scaling is a key issue to apply PINNs to real-world large   \n79 spatial-temporal scale applications. [Jagtap and Karniadakis, 2020] proposed a generalized space  \n80 time domain decomposition framework for PINNs, named extended PINNs (XPINNs), which can   \n81 handle nonlinear PDEs on complex-geometry domains. XPINNs provide large representation and   \n82 parallelization capacity by deploying multiple neural networks in smaller subdomains, offering both   \n83 space and time parallelization to reduce training costs effectively. Shukla et al. [2021] developed   \n84 a distributed framework for PINNs based on two extensions: conservative PINNs (cPINNs) and   \n85 XPINNs. These methods employ domain decomposition in space and time-space, respectively,   \n86 enhancing the parallelization capacity, representation capacity, and efficient hyperparameter tuning of   \n87 PINNs. The framework allows for optimizing all hyperparameters of each neural network separately   \n88 in each subdomain, providing significant advantages for multi-scale and multi-physics problems. They   \n89 demonstrated the efficiency of cPINNs and XPINNs through various forward problems, highlighting   \n90 that cPINNs are more communication-efficient while XPINNs offer greater flexibility for handling   \n91 complex subdomains. Moseley et al. [2023] addressed the limitations of PINNs in solving large   \n92 domains and multi-scale solutions by proposing Finite Basis PINNs (FBPINNs). FBPINNs use neural   \n93 networks to learn basis functions defined over small, overlapping subdomains, inspired by classical   \n94 finite element methods. This approach mitigates the spectral bias of neural networks and reduces the   \n95 complexity of the optimization problem by using smaller neural networks in a parallel, divide-and  \n96 conquer approach. Their experiments showed that FBPINNs outperform standard PINNs in accuracy   \n97 and computational efficiency for both small and large, multi-scale problems. Chalapathi et al. [2024]   \n98 introduced a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE)   \n99 in neural network architectures. This method imposes constraints over smaller decomposed domains,   \n100 with each domain solved by an expert through differentiable optimization. The independence of each   \n101 expert allows for parallelization across multiple GPUs, improving accuracy, training stability, and   \n102 computational efficiency for predicting the dynamics of complex nonlinear systems. The optimal   \n103 decomposition of subdomains is critical to the effectiveness of these scaling methods, given a fixed   \n104 compute budget. Our work focuses on finding the maximum subdomain size that even a 64x2 small   \n105 PINN can handle within a compute budget. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "106 3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "107 In this section, we outline our approach to effectively implement hard constraints, strategically   \n108 sampling partial differential equations (PDEs), and optimizing the scaling of computational domains.   \n109 These methods are utilized to solve the acoustic wave equation within a specified computational   \n110 budget. ", "page_idx": 2}, {"type": "text", "text": "111 We focus on an acoustic wave equation defined by: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathcal{D}[{\\bf u}({\\bf x},t);c({\\bf x})]=f({\\bf x},t),\\qquad{\\bf x}\\in\\Omega,\\quad t\\in[t_{0},T],}\\\\ &{}&{\\mathcal{B}_{i}[{\\bf u}({\\bf x},t)]=U_{i}({\\bf x},t),\\;\\;\\;{\\bf x}\\in\\partial\\Omega_{i},\\;\\;\\;t\\in[t_{0},T],}\\\\ &{}&{\\mathcal{T}_{j}[{\\bf u}({\\bf x},t_{0})]=V_{j}({\\bf x}),\\;\\;\\;\\;\\;\\;\\;{\\bf x}\\in\\Omega,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "112 where: ", "page_idx": 2}, {"type": "text", "text": "113 \u2022 $\\mathcal{D}$ represents the differential operator. For a simplified one-dimensional acoustic wave   \n114 equation, $\\begin{array}{r}{\\mathcal{D}=\\partial_{t t}-c^{2}(\\mathbf{x})\\nabla^{2}}\\end{array}$ , indicating the second temporal derivative minus the spatial   \n115 derivative scaled by the square of the local speed of sound, $c(\\mathbf{x})$ .   \n116 \u2022 $B_{i}$ denotes the boundary condition operator applied at $\\mathbf{x}\\in\\partial\\Omega_{i}$ .   \n117 \u2022 $\\mathcal{T}_{j}$ signifies the initial condition operator, defining the state of the system at $t=t_{0}$ across   \n118 the domain $\\Omega$ . ", "page_idx": 2}, {"type": "text", "text": "119 3.1 Hard constraint imposing ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "120 A prevalent ansatz employed in prior studies on hard-constraint PINNs for 1D wave equations is   \n121 expressed as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nu(x,t)=\\tau(t)\\tilde{u}(x,t)+(1-\\tau(t))u(x,0),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "122 where $\\tilde{u}(x,t)$ represents the neural network output with inputs $x$ and $t$ , and $\\tau(t)$ is a function that   \n123 satisfies $\\tau(0)=0$ . This design ensures that the initial condition $u(x,0)$ is met precisely when $t=0$ . ", "page_idx": 2}, {"type": "text", "text": "124 To accommodate boundary conditions (BCs) at $x=0$ and $x=L$ , the ansatz is often modified to: ", "page_idx": 2}, {"type": "equation", "text": "$$\nu(x,t)=x(L-x)\\tilde{u}(x,t)+U_{i}(x,t),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "125 ensuring that $u(x_{i},t)=U_{i}(x_{i},t)$ for $x\\in\\partial\\Omega_{i}$ . ", "page_idx": 2}, {"type": "text", "text": "126 A more comprehensive form, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{u(x,t)=x(L-x)\\tau(t)\\tilde{u}(x,t)+(1-\\tau(t))u(x,0)}}\\\\ {{\\displaystyle\\qquad+\\frac{L-x}{L}(u(0,t)-(1-\\tau(t))u(0,0))}}\\\\ {{\\displaystyle\\qquad+\\frac{x}{L}(u(L,t)-(1-\\tau(t))u(L,0)),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "127 can ensure both Dirichlet BCs and the initial condition $u(x,t)|_{t=0}=u(x,0)$ . However, this ansatz   \n128 does not account for $\\partial_{t}u(x,t)|_{t=0}$ , unless it is assumed to be zero. ", "page_idx": 3}, {"type": "text", "text": "129 We propose a more general hard constraint imposition formula: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{u(x,t)=x(L-x)\\tau(t)\\tilde{u}(x,t)+((1-\\tau(t))+t\\partial_{t})u(x,0)}}}\\\\ {{\\displaystyle\\;\\;\\;\\;\\;\\;\\;\\;+\\frac{L-x}{L}(u(0,t)-((1-\\tau(t))+t\\partial_{t})u(0,0))}}\\\\ {{\\displaystyle\\;\\;\\;\\;\\;\\;\\;\\;+\\frac{x}{L}(u(L,t)-((1-\\tau(t))+t\\partial_{t})u(L,0)),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "130 which guarantees satisfaction of the conditions: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{u(x,t)=U_{i}(x,t),}&{{}}&{x\\in\\partial\\Omega_{i},}\\\\ {u(x,t)|_{t=0}=V_{j}(x),}&{{}}&{x\\in\\Omega,}\\\\ {\\partial_{t}u(x,t)|_{t=0}=W_{j}(x),}&{{}}&{x\\in\\Omega,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "131 where $U_{i}(x,t),\\,V_{j}(x),\\,W_{j}(x)$ are the specified functions in BCs and $\\mathbf{ICs}$ , and $\\tau(t)$ is an arbitrary   \n132 function satisfying $\\tau(0)\\stackrel{\\cdot}{=}d_{t}\\tau(0)=0$ .   \n133 It is straightforward to demonstrate that the proposed ansatz correctly imposes all BCs and ICs as   \n134 required: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\boldsymbol{u}(\\boldsymbol{x},t)|_{\\boldsymbol{x}=0}}&{=\\boldsymbol{u}(0,t),}\\\\ {\\boldsymbol{u}(\\boldsymbol{x},t)|_{\\boldsymbol{x}=L}}&{=\\boldsymbol{u}(L,t),}\\\\ {\\boldsymbol{u}(\\boldsymbol{x},t)|_{t=0}}&{=\\boldsymbol{u}(\\boldsymbol{x},0),}\\\\ {\\partial_{t}\\boldsymbol{u}(\\boldsymbol{x},t)|_{t=0}}&{=\\partial_{t}\\boldsymbol{u}(\\boldsymbol{x},0).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "135 In Section 4.2, we will explore numerical tests to optimize the selection of $\\tau(t)$ by evaluating   \n136 convergence rates and mean absolute errors (MAE).   \n137 The primary advantage of employing hard constraints in our model is the elimination of the need to   \n138 fine-tune the weights of PDE, BC, and IC loss terms typically required in soft-constraint PINNs. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "139 3.2 Sampling strategy ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "140 Sampling is crucial for efficient training of PINNs, ensuring rapid convergence and mitigating   \n141 potential failure modes. To enhance the computational efficiency of our hard-constraint PINNs,   \n142 we introduce the Dynamic Amplitude-Focused Sampling (DAFS) method. This strategy optimally   \n143 selects the number of points, $N_{p d e}$ , used in the training.   \n144 Initially, we segmented the computational domain to identify regions with high-amplitude acoustic   \n145 wave fields, based on low-resolution finite difference (FD) simulations. These high-amplitude regions   \n146 are defined by a threshold $\\delta$ , which determines the intensity level above which areas are considered   \n147 to be of high amplitude. Within these identified regions, we uniformly sampled $\\alpha N_{p d e}$ points. This   \n148 was supplemented by uniformly sampling $(1-\\alpha)N_{p d e}$ points in the remaining areas of the domain.   \n149 Both and $\\alpha$ are parameters crucial to the sampling process and are optimally chosen to balance the   \n150 computational budget and the accuracy of the simulations. By adjusting these parameters, we can   \n151 tailor the distribution of sample points to areas that are most influential in the wave dynamics, thereby   \n152 improving the efficiency of our PINN training. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "153 The pseudocode for the DAFS algorithm is provided in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "154 This sampling strategy, characterized by its focus on dynamically identified regions of interest based   \n155 on wave amplitude, significantly optimizes the efficiency of the computation during the PINN training   \n156 phase. The numerical tests for DAFS are in Section 4.3. ", "page_idx": 3}, {"type": "text", "text": "157 4 Experiments ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "158 4.1 Problem setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "159 We applied our method to three numerical examples for three different types of 1D acoustic wave   \n160 equations \u2014 standing waves, string waves, and traveling waves. The ground truth wavefields are   \n161 shown in Figure 1. ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Dynamic Amplitude-Focused Sampling (DAFS) ", "page_idx": 4}, {"type": "text", "text": "Require: $N_{\\mathrm{pde},\\,\\alpha}$ , domain, FD results (low-resolution Finite Difference results indicating amplitude)   \nEnsure: Sampled points for training   \n1: Initialize points $\\gets[]$   \n2: Identify high-amplitude regions from FD results   \n3: $N_{\\mathrm{high}}\\leftarrow\\alpha N_{\\mathrm{pde}}$ \u25b7Number of points in high-amplitude regions   \n4: $N_{\\mathrm{low}}\\leftarrow(1\\-\\dot{-}\\alpha)N_{\\mathrm{pde}}$ \u25b7Number of points in low-amplitude regions   \n5: Uniformly sample $N_{\\mathrm{high}}$ points in high-amplitude regions and add to points   \n6: Uniformly sample $N_{\\mathrm{low}}$ points in the remaining areas of the domain and add to points return points ", "page_idx": 4}, {"type": "image", "img_path": "nu2Sqrsnr7/tmp/f1696d71997dc071e54fba02d38d2c9b5e70cd3f5f672bad84aae82b6b1cafb0.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 1: Ground truth wavefields for (a) standing waves, (b) string waves, and (c) traveling waves with $k=1,2,3$ . ", "page_idx": 4}, {"type": "text", "text": "162 Standing waves for Dirichlet BCs Our first numerical example is a standing wave solution for the   \n163 following 1D wave equation with Dirichlet BCs: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial^{2}u(x,t)}{\\partial t^{2}}-c^{2}\\frac{\\partial^{2}u}{\\partial x^{2}}=0,\\;x\\in(0,L)}\\\\ &{\\mathrm{\\bf~B.C.}\\colon u(0,t)=u(L,t)=0,}\\\\ &{\\mathrm{\\bf~I.C.}\\colon u(x,0)=U(x),\\frac{\\partial u}{\\partial t}(x,0)=V(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "164 The analytical solution $\\boldsymbol{u}(\\boldsymbol{x},t)$ for Equation 8 is ", "page_idx": 4}, {"type": "equation", "text": "$$\nu(x,t)=\\sum_{n=1}^{\\infty}A_{n}\\sin\\left({\\frac{n\\pi x}{L}}\\right)\\cos\\left({\\frac{n\\pi c t}{L}}\\right)+B_{n}\\sin\\left({\\frac{n\\pi x}{L}}\\right)\\sin\\left({\\frac{n\\pi c t}{L}}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "165 A standing wave solution ", "page_idx": 4}, {"type": "equation", "text": "$$\nu(x,t)=\\sin\\left(\\frac{k\\pi x}{L}\\right)\\cos\\left(\\frac{k\\pi c t}{L}\\right),k\\in\\mathbb{Z}^{+}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "166 can be achieved if we assume $\\begin{array}{r}{U(x)=\\sin\\left(\\frac{k\\pi x}{L}\\right)}\\end{array}$ and $V(x)=0$ . We show the solutions for $k=1,2,3$   \n167 in Figure 1(a).   \n168 String waves for time-dependent BCs Our third example is a string wave solution for time  \n169 dependent BCs shown in Equation 11. The ground truth solutions in Figuer 1(b) are achieved by   \n170 finite different simulation. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\displaystyle\\frac{\\partial^{2}u(x,t)}{\\partial t^{2}}-c^{2}\\frac{\\partial^{2}u}{\\partial x^{2}}=0,\\ x\\in(0,L)}\\\\ &{\\qquad\\mathrm{3.C.}\\colon u(0,t)=u(L,t)=\\sin(2\\pi t),}\\\\ &{\\qquad\\mathrm{I.C.}\\colon u(x,0)=0,\\,\\displaystyle\\frac{\\partial u}{\\partial t}(x,0)=2\\pi\\cos\\left(\\frac{2k\\pi x}{L}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "171 Traveling waves for Gaussian source time functions Our third example is a traveling wave   \n172 solution for initial conditions of Gaussian source time functions shown in Equation 12. The ground   \n173 truth solutions in Figuer 1(c) are computed by finite different simulation. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\displaystyle\\frac{\\partial^{2}u(x,t)}{\\partial t^{2}}-c^{2}\\frac{\\partial^{2}u}{\\partial x^{2}}=0,\\ x\\in(0,L)}\\\\ &{{\\bf B}.{\\bf C}.:u(0,t)=u(L,t)=0,}\\\\ &{{\\bf I}.{\\bf C}.:u(x,0)=\\displaystyle\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right),\\ \\frac{\\partial u}{\\partial t}(x,0)=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "174 4.2 Optimal $\\tau(t)$ selection for hard constraints ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "175 We selected six candidate functions for $\\tau(t)$ to construct PINNs with a network configuration of only   \n176 $64\\mathrm{x}2$ neurons. Figures 2 through 4 illustrate the $L^{2}$ loss and $L^{1}$ error as functions of training epochs.   \n117778 fOaiulru rfein dminogdse ss.u Igng egset ntehraat $\\tau(t)$ $t^{2}$ $\\scriptstyle{\\frac{2t^{2}}{1+t^{2}}}$ fpicearnftolry misn fblueettnecr eisn  bgoethn etrhael ,c eosnpveecrigaellnyc ef orra the iagnhde rt hme oedmees $k=2,3$   \n179 We show a few training dynmaics in Appendix C.   \n180 Our analysis indicates that the frequency characteristics of $\\tau(t)$ and the corresponding wavefields may   \n181 be critical for selecting an appropriate $\\tau(t)$ . Matching these characteristics can potentially enhance   \n182 the model\u2019s efficiency by aligning $\\tau(t)$ \u2019s influence on the neural network\u2019s learning dynamics with   \n183 the physical properties of the wave phenomena being modeled. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "nu2Sqrsnr7/tmp/8e2291767385c70e55d599e1e3582f7c8b2738375116debc2b130313bab82a1c.jpg", "img_caption": ["Figure 2: $L^{2}$ loss and $L^{1}$ error for standing waves with PINNs constructed using six canditate $\\tau(t)$ functions. ", "(b) $L^{1}$ error "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "nu2Sqrsnr7/tmp/d8e472b620c9474564f1bf3b9836e1acb7af4d042531d17be633ec457796bf6a.jpg", "img_caption": ["Figure 3: $L^{2}$ loss and $L^{1}$ error for string waves with PINNs constructed using six canditate $\\tau(t)$ functions. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "nu2Sqrsnr7/tmp/27f3b77a0342abd26af34acbd69f80d28397fc6b1e8862b4fc15e16e9cd03677.jpg", "img_caption": ["Figure 4: $L^{2}$ loss and $L^{1}$ error for travelling Gaussian waves with PINNs constructed using six canditate $\\tau(t)$ functions. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "184 4.3 Dynamic Amplitude-Focused Sampling ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "185 We demonstrate the efficacy of our proposed Dynamic Amplitude-Focused Sampling (DAFS) in   \n186 enhancing both the convergence and accuracy of Physics-Informed Neural Networks (PINNs).   \n187 Experiments varying $\\alpha$ from 0 to 0.5 to 1 indicate that optimal results are typically achieved when $\\alpha$   \n188 is around 0.5.   \n189 This suggests a balanced sampling strategy, where a significant portion of the samples is concentrated   \n190 in regions of higher amplitude. However, exclusively focusing on these high-amplitude areas can   \n191 hinder information transfer from boundary conditions to the interior of the domain, potentially leading   \n192 to failure modes. Figures 5 and 6 illustrate these dynamics, showing the $L^{2}$ loss and $L^{1}$ error across   \n193 different values of $\\alpha$ , and the impact on the predicted wavefield and its accuracy. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "nu2Sqrsnr7/tmp/817ba2a1bc51118dcb838c1bac42d10e189236c51b9773405b4ca9ebcb604d7d.jpg", "img_caption": ["Figure 5: $L^{2}$ loss and $L^{1}$ error with varied $\\alpha$ from 0 to 1. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "nu2Sqrsnr7/tmp/32e053e01b17dac5c00e808426c6e4a4de3fab8cb3e5abdfad29a3f539d80f44.jpg", "img_caption": ["Figure 6: Visualizations for $\\alpha=0.00$ , 0.50, and 1.00 (top to bottom): Left - Predicted wavefield, Middle - Difference between the prediction and ground truth, Right - Sampling distribution. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "194 4.4 Optimal subdomain ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "195 We then propose an optimal subdomain selection method shown in a flow chart in Figure 7. This   \n196 method will automatically determine the optimal $k$ our 64x2 small PINNs can handle, given a   \n197 compute budget. ", "page_idx": 7}, {"type": "text", "text": "198 5 Limitations and Training Dynamics ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "199 While our proposed methods significantly enhance the functionality and efficiency of PINNs, the   \n200 determination of the optimal function $\\tau(t)$ presents certain limitations. The choice of $\\tau(t)$ is crucial   \n201 as it directly affects the model\u2019s ability to satisfy boundary and initial conditions rigidly. However,   \n202 finding an ideal $\\tau(t)$ that adapts across different problems and boundary conditions without extensive   \n203 trial and error remains challenging. The training dynamics are also sensitive to the form of $\\tau(t)$ , where   \n204 inappropriate selections can lead to slower convergence or even divergence in some cases. These   \n205 issues underscore the need for a more automated, perhaps adaptive, approach to selecting $\\tau(t)$ that   \n206 can dynamically adjust based on the evolving training characteristics and the specific requirements of   \n207 the PDE being solved. ", "page_idx": 7}, {"type": "image", "img_path": "nu2Sqrsnr7/tmp/c2d7d889dcdeddeb37dfeb29a1cf6dfab0006075be73b148ecf0c03f0e7f7a2a.jpg", "img_caption": ["Figure 7: The flow chart of optimal subdomain determination. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "208 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "209 This work presented a comprehensive approach to improving the effectiveness and efficiency of   \n210 Physics-Informed Neural Networks (PINNs) for solving acoustic wave equations. By integrating   \n211 a well-formulated hard constraint imposition strategy and the novel Dynamic Amplitude-Focused   \n212 Sampling (DAFS) method, we have significantly enhanced both the accuracy and convergence of   \n213 PINNs. ", "page_idx": 8}, {"type": "text", "text": "214 Our methodological innovations include: ", "page_idx": 8}, {"type": "text", "text": "215 \u2022 A systematic derivation of hard boundary and initial conditions in PINNs that ensures these   \n216 constraints are inherently satisfied, leading to better convergence and stability of the solution.   \n217 \u2022 The introduction of DAFS, which optimally allocates computational resources by focus  \n218 ing sampling in regions of high amplitude while ensuring adequate coverage across the   \n219 computational domain to prevent information isolation.   \n220 \u2022 Development of a domain size optimization algorithm that assists in domain decompo  \n221 sition, enabling efficient scaling of PINNs for large-scale applications while managing   \n222 computational costs.   \n223 These contributions mark a significant step forward in the practical deployment of PINNs, especially   \n224 in fields requiring the simulation of complex physical phenomena over large scales. Future work will   \n225 focus on extending these strategies to other types of partial differential equations and exploring the   \n226 integration of our methods with other deep learning frameworks to further enhance the adaptability   \n227 and efficiency of PINNs in diverse applications, for example, we will explore the integration of our   \n228 methods with existing PINNs frameworks that employ domain decomposition techniques, such as   \n229 XPINNs and FBPINNs, to further enhance their scalability and adaptability. We aim to make PINNs   \n230 more adaptable and efficient for a broader range of applications, particularly in complex systems   \n231 where traditional numerical methods struggle. By advancing these strategies, we can significantly   \n232 contribute to the deployment of PINNs in real-world scenarios, tackling large-scale and multi-scale   \n233 challenges effectively. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "234 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "235 Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary and   \n236 partial differential equations. IEEE transactions on neural networks, 9(5):987\u20131000, 1998.   \n237 Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning   \n238 framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal   \n239 of Computational physics, 378:686\u2013707, 2019.   \n240 Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. Deepxde: A deep learning library for solving   \n241 differential equations. SIAM review, 63(1):208\u2013228, 2021a.   \n242 Ameya D Jagtap and George Em Karniadakis. Extended physics-informed neural networks (xpinns): A   \n243 generalized space-time domain decomposition based deep learning framework for nonlinear partial differential   \n244 equations. Communications in Computational Physics, 28(5):2002\u20132041, 2020.   \n245 Khemraj Shukla, Ameya D Jagtap, and George Em Karniadakis. Parallel physics-informed neural networks via   \n246 domain decomposition. Journal of Computational Physics, 447:110683, 2021.   \n247 Ben Moseley, Andrew Markham, and Tarje Nissen-Meyer. Finite basis physics-informed neural networks   \n248 (fbpinns): a scalable domain decomposition approach for solving differential equations. Advances in   \n249 Computational Mathematics, 49(4):62, 2023.   \n250 Lu Lu, Rapha\u00ebl Pestourie, Wenjie Yao, Zhicheng Wang, Francesc Verdugo, and Steven G. Johnson. Physics  \n251 Informed Neural Networks with Hard Constraints for Inverse Design. SIAM Journal on Scientific Computing,   \n252 43(6):B1105\u2013B1132, January 2021b. ISSN 1064-8275, 1095-7197. doi: 10.1137/21M1397908. URL   \n253 https://epubs.siam.org/doi/10.1137/21M1397908.   \n254 Songming Liu, Zhongkai Hao, Chengyang Ying, Hang Su, Jun Zhu, and Ze Cheng. A Unified Hard-Constraint   \n255 Framework for Solving Geometrically Complex PDEs. Advances in Neural Information Processing Systems,   \n256 35:20287\u201320299, 2022.   \n257 Shaikhah Alkhadhr and Mohamed Almekkawy. Wave Equation Modeling via Physics-Informed Neural Networks:   \n258 Models of Soft and Hard Constraints for Initial and Boundary Conditions. Sensors, 23(5):2792, March 2023.   \n259 ISSN 1424-8220. doi: 10.3390/s23052792. URL https://www.mdpi.com/1424-8220/23/5/2792.   \n260 R\u00fcdiger Brecht, Dmytro R. Popovych, Alex Bihlo, and Roman O. Popovych. Improving physics  \n261 informed DeepONets with hard constraints, September 2023. URL http://arxiv.org/abs/2309.07899.   \n262 arXiv:2309.07899 [physics].   \n263 Chenxi Wu, Min Zhu, Qinyang Tan, Yadhu Kartha, and Lu Lu. A comprehensive study of non-adaptive and   \n264 residual-based adaptive sampling for physics-informed neural networks. Computer Methods in Applied   \n265 Mechanics and Engineering, 403:115671, 2023.   \n266 Arka Daw, Jie Bu, Sifan Wang, Paris Perdikaris, and Anuj Karpatne. Mitigating propagation failures in physics  \n267 informed neural networks using retain-resample-release (r3) sampling. In International Conference on   \n268 Machine Learning, pages 7264\u20137302. PMLR, 2023.   \n269 Zhiwei Gao, Liang Yan, and Tao Zhou. Failure-informed adaptive sampling for pinns. SIAM Journal on Scientific   \n270 Computing, 45(4):A1971\u2013A1994, 2023a.   \n271 Zhiwei Gao, Tao Tang, Liang Yan, and Tao Zhou. Failure-informed adaptive sampling for pinns, part ii: combin  \n272 ing with re-sampling and subset simulation. Communications on Applied Mathematics and Computation,   \n273 pages 1\u201322, 2023b.   \n274 Zijiang Yang, Zhongwei Qiu, and Dongmei Fu. Dmis: Dynamic mesh-based importance sampling for train  \n275 ing physics-informed neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence,   \n276 volume 37, pages 5375\u20135383, 2023.   \n277 Zhengqi Zhang, Jing Li, and Bin Liu. Annealed adaptive importance sampling method in pinns for solving high   \n278 dimensional partial differential equations. arXiv preprint arXiv:2405.03433, 2024.   \n279 Nithin Chalapathi, Yiheng Du, and Aditi Krishnapriyan. Scaling physics-informed hard constraints with   \n280 mixture-of-experts. arXiv preprint arXiv:2402.13412, 2024. ", "page_idx": 9}, {"type": "text", "text": "281 A Phase diagrams of loss weights ", "text_level": 1, "page_idx": 10}, {"type": "image", "img_path": "nu2Sqrsnr7/tmp/c2fb74765caabfddc7c42d2dfac42bb3825ba6320c333048681d65436dc959aa.jpg", "img_caption": ["Figure 8: Phase diagrams "], "img_footnote": [], "page_idx": 10}, {"type": "text", "text": "282 B Seed ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "283 C Training dynmaics ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "284 mono:   \n285 string: increase Npde to $10^{4}$ , we have converged solution(each $10^{4}$ steps): ", "page_idx": 10}, {"type": "image", "img_path": "nu2Sqrsnr7/tmp/474b9392777e7c20b8a299333bcb18e3f6dd5f83eecc37aad56024ed67bb71b0.jpg", "img_caption": ["(c) Gaussian traveling waves "], "img_footnote": [], "page_idx": 11}, {"type": "image", "img_path": "nu2Sqrsnr7/tmp/6bcf255cd795b406bba7ec3f5169bec2b299bfdfdb286311b7f465faf4bcec2e.jpg", "img_caption": ["Figure 10: 0, 1000, 2000, and the last(converged) "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "286 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "287 The checklist is designed to encourage best practices for responsible machine learning research, addressing   \n288 issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The   \n289 papers not including the checklist will be desk rejected. The checklist should follow the references and follow   \n290 the (optional) supplemental material. The checklist does NOT count towards the page limit.   \n291 Please read the checklist guidelines carefully for information on how to answer these questions. For each   \n292 question in the checklist: ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 12}, {"type": "text", "text": "\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 12}, {"type": "text", "text": "297 The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area   \n298 chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions)   \n299 with the final version of your paper, and its final version will be published with the paper.   \n300 The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While   \n301 \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper   \n302 justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or   \n303 \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not   \n304 grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is   \n305 often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting   \n306 evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer   \n307 [Yes] to a question, in the justification please point to the section(s) where related material for the question can   \n308 be found. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "nu2Sqrsnr7/tmp/68a0039c362530509027dd1f6fce67e70a0e64222ed7e8b47298d93f27c57dfb.jpg", "img_caption": ["Figure 11: 0, 10000, 20000, and the last(converged) "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "309 IMPORTANT, please: ", "page_idx": 13}, {"type": "text", "text": "310   \n311   \n312   \n313   \n314   \n315   \n316   \n317   \n318   \n319   \n320   \n321   \n322   \n323   \n324   \n325   \n326   \n327 ", "page_idx": 13}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\",   \n\u2022 Keep the checklist subsection headings, questions/answers and guidelines below.   \n\u2022 Do not modify the questions and only use the provided macros for your answers.   \n1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? Answer: [Yes] Justification: NA   \n2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: NA   \n3. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [TODO] Justification: [TODO]   \n4. Experimental Result Reproducibility ", "page_idx": 13}, {"type": "text", "text": "328 Question: Does the paper fully disclose all the information needed to reproduce the main experimental   \n329 results of the paper to the extent that it affects the main claims and/or conclusions of the paper   \n330 (regardless of whether the code and data are provided or not)?   \n331 Answer: [TODO]   \n332 Justification: [TODO]   \n333 5. Open access to data and code   \n334 Question: Does the paper provide open access to the data and code, with sufficient instructions to   \n335 faithfully reproduce the main experimental results, as described in supplemental material?   \n336 Answer: [TODO]   \n337 Justification: [TODO]   \n338 6. Experimental Setting/Details   \n339 Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,   \n340 how they were chosen, type of optimizer, etc.) necessary to understand the results?   \n341 Answer: [TODO]   \n342 Justification: [TODO]   \n343 7. Experiment Statistical Significance   \n344 Question: Does the paper report error bars suitably and correctly defined or other appropriate informa  \n345 tion about the statistical significance of the experiments?   \n346 Answer: [TODO]   \n347 Justification: [TODO]   \n348 8. Experiments Compute Resources   \n349 Question: For each experiment, does the paper provide sufficient information on the computer   \n350 resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?   \n351 Answer: [TODO]   \n352 Justification: [TODO]   \n353 9. Code Of Ethics   \n354 Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code   \n355 of Ethics https://neurips.cc/public/EthicsGuidelines?   \n356 Answer: [TODO]   \n357 Justification: [TODO]   \n358 10. Broader Impacts   \n359 Question: Does the paper discuss both potential positive societal impacts and negative societal impacts   \n360 of the work performed?   \n361 Answer: [TODO]   \n362 Justification: [TODO]   \n363 11. Safeguards   \n364 Question: Does the paper describe safeguards that have been put in place for responsible release of   \n365 data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or   \n366 scraped datasets)?   \n367 Answer: [TODO]   \n368 Justification: [TODO]   \n369 12. Licenses for existing assets   \n370 Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper,   \n371 properly credited and are the license and terms of use explicitly mentioned and properly respected?   \n372 Answer: [Yes]   \n373 Justification: NA   \n374 13. New Assets   \n375 Question: Are new assets introduced in the paper well documented and is the documentation provided   \n376 alongside the assets?   \n377 Answer: [No]   \n378 Justification: NA   \n379 14. Crowdsourcing and Research with Human Subjects   \n380 Question: For crowdsourcing experiments and research with human subjects, does the paper include   \n381 the full text of instructions given to participants and screenshots, if applicable, as well as details about   \n382 compensation (if any)?   \n383 Answer: [No]   \n384 Justification: NA.   \n385 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects   \n386 Question: Does the paper describe potential risks incurred by study participants, whether such   \n387 risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an   \n388 equivalent approval/review based on the requirements of your country or institution) were obtained?   \n389 Answer: [No]   \n390 Justification: NA ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}]