[{"figure_path": "xL7Ve14AHA/figures/figures_22_1.jpg", "caption": "Figure 1: Group sparsity level and validation prediction performance v.s epochs. In the plot for Transformer-XL, one step processes ten batches, and for our batch size of 64, one epoch consists of 8,401 batches.", "description": "This figure visualizes the training process of various models, illustrating the relationship between the weighted structured sparsity and validation performance (accuracy or perplexity, depending on the specific model) over epochs. Each subplot represents a different experiment, such as ResNet50 on ImageNet, Transformer-XL on WikiText-103, and others. The plots demonstrate how the weighted structured sparsity evolves during the training process and how it relates to the validation performance, helping to evaluate the effectiveness of the RAMDA algorithm compared to other methods. The figure also shows the number of batches processed in each step for the Transformer-XL model.", "section": "6 Experiments"}, {"figure_path": "xL7Ve14AHA/figures/figures_23_1.jpg", "caption": "Figure 1: Group sparsity level and validation prediction performance v.s epochs. In the plot for Transformer-XL, one step processes ten batches, and for our batch size of 64, one epoch consists of 8,401 batches.", "description": "This figure displays the trends of weighted structured sparsity and validation metrics (accuracy, perplexity, or loss) across various epochs for multiple algorithms and datasets. The algorithms are compared on different neural network architectures for image classification (ResNet50 on ImageNet, VGG19 on CIFAR10, ResNet50 on CIFAR10), language modeling (Transformer-XL on WikiText-103), and speech synthesis (Tacotron2 on LJSpeech). The plot for Transformer-XL shows a different scale because one step in this model processes 10 batches of data, resulting in 8401 batches per epoch.", "section": "Experiments"}, {"figure_path": "xL7Ve14AHA/figures/figures_23_2.jpg", "caption": "Figure 1: Group sparsity level and validation prediction performance v.s epochs. In the plot for Transformer-XL, one step processes ten batches, and for our batch size of 64, one epoch consists of 8,401 batches.", "description": "This figure shows the training performance of different algorithms over epochs for various tasks. The plots show the weighted structured sparsity and validation metrics (accuracy, perplexity, or loss) for each algorithm.  The x-axis represents the training epochs, while the y-axis represents the weighted structured sparsity and validation metrics.  The figure highlights how RAMDA maintains a high level of structured sparsity while achieving competitive validation performance. Note that the scale of the x-axis differs for each subplot, and for Transformer-XL, one step represents processing 10 batches (each with batch size of 64), so one epoch contains 8401 steps.", "section": "6 Experiments"}, {"figure_path": "xL7Ve14AHA/figures/figures_23_3.jpg", "caption": "Figure 1: Group sparsity level and validation prediction performance v.s epochs. In the plot for Transformer-XL, one step processes ten batches, and for our batch size of 64, one epoch consists of 8,401 batches.", "description": "This figure shows the group sparsity level and validation performance (accuracy or perplexity or loss) over epochs for various models and datasets.  It compares the performance of RAMDA with other methods, highlighting the stability of RAMDA's sparsity level and its competitive performance.  The different subplots represent results from various experimental settings, including different network architectures (ResNet50, Transformer-XL, Tacotron2), datasets (ImageNet, WikiText-103, LJSpeech, MNIST, CIFAR10, CIFAR100), and regularization techniques.", "section": "Experiments"}, {"figure_path": "xL7Ve14AHA/figures/figures_23_4.jpg", "caption": "Figure 1: Group sparsity level and validation prediction performance v.s epochs. In the plot for Transformer-XL, one step processes ten batches, and for our batch size of 64, one epoch consists of 8,401 batches.", "description": "This figure displays the weighted group sparsity and validation accuracy/perplexity/loss for several methods (ProxSGD, ProxSSI, ProxGen, RMDA, RAMDA) across different datasets and model architectures. Each plot shows how these metrics evolve over the training epochs.  The Transformer-XL plot uses a different step size than the other plots. The plots show how the performance and sparsity of various methods converge over time and indicate the relative effectiveness of each algorithm.", "section": "Experiments"}, {"figure_path": "xL7Ve14AHA/figures/figures_23_5.jpg", "caption": "Figure 1: Group sparsity level and validation prediction performance v.s epochs. In the plot for Transformer-XL, one step processes ten batches, and for our batch size of 64, one epoch consists of 8,401 batches.", "description": "This figure shows the change in weighted group sparsity and validation performance (accuracy or perplexity) for various algorithms (ProxSGD, ProxSSI, ProxGen, RMDA, and RAMDA) across different datasets and model architectures over the training epochs. The plot highlights the stability of RAMDA's sparsity level in comparison to other methods, particularly towards the end of training.  The difference in x-axis scales across subplots is because of varying batch sizes and epoch lengths used for training different models.", "section": "D Plots of Sparsity Level and Validation Accuracy over Epochs"}, {"figure_path": "xL7Ve14AHA/figures/figures_23_6.jpg", "caption": "Figure 2: Group sparsity level at the last epochs.", "description": "The figure shows the weighted group sparsity level at the last epochs for different algorithms (ProxSGD, ProxSSI, ProxGen, RMDA, and RAMDA) across various experiments: ResNet50 on ImageNet, Transformer-XL on WikiText-103, Tacotron2 on LJSpeech, Logistic Regression on MNIST, VGG19 on CIFAR10, VGG19 on CIFAR100, ResNet50 on CIFAR10, and ResNet50 on CIFAR100.  It illustrates the stability of RAMDA's sparsity level compared to the fluctuations observed in other methods.", "section": "D Plots of Sparsity Level and Validation Accuracy over Epochs"}, {"figure_path": "xL7Ve14AHA/figures/figures_23_7.jpg", "caption": "Figure 1: Group sparsity level and validation prediction performance v.s epochs. In the plot for Transformer-XL, one step processes ten batches, and for our batch size of 64, one epoch consists of 8,401 batches.", "description": "This figure shows the change of group sparsity and validation accuracy over epochs for different algorithms on various datasets. The datasets include ImageNet, WikiText-103, LJSpeech, MNIST, and CIFAR10/100.  The algorithms compared are MSGD, ProxSGD, ProxSSI, ProxGen, RMDA, and RAMDA. The plots illustrate the performance of each algorithm in terms of achieving structured sparsity and maintaining prediction accuracy.  The Transformer-XL plot has a different x-axis scale due to the larger batch size.", "section": "D Plots of Sparsity Level and Validation Accuracy over Epochs"}, {"figure_path": "xL7Ve14AHA/figures/figures_23_8.jpg", "caption": "Figure 2: Group sparsity level at the last epochs.", "description": "The figure shows the weighted structured sparsity for different algorithms (ProxSGD, ProxSSI, ProxGen, RMDA, and RAMDA) over epochs for various experiments: ResNet50 on ImageNet, Transformer-XL on WikiText-103, Tacotron2 on LJSpeech, Logistic Regression on MNIST, VGG19 on CIFAR10, VGG19 on CIFAR100, ResNet50 on CIFAR10, and ResNet50 on CIFAR100.  It demonstrates the stability of RAMDA's sparsity level compared to the fluctuating behavior of other algorithms.", "section": "D Plots of Sparsity Level and Validation Accuracy over Epochs"}]