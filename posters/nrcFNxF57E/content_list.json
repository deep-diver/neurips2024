[{"type": "text", "text": "Partial Gromov Wasserstein Metric ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Gromov-Wasserstein (GW) distance has gained increasing interest in the   \n2 machine learning community in recent years, as it allows for the comparison   \n3 of measures in different metric spaces. To overcome the limitations imposed   \n4 by the equal mass requirements of the classical GW problem, researchers have   \n5 begun exploring its application in unbalanced settings. However, Unbalanced GW   \n6 (UGW) can only be regarded as a discrepancy rather than a rigorous metric/distance   \n7 between two metric measure spaces (mm-spaces). In this paper, we propose a   \n8 particular case of the UGW problem, termed Partial Gromov-Wasserstein (PGW).   \n9 We establish that PGW is a well-defined metric between mm-spaces and discuss its   \n10 theoretical properties, including the existence of a minimizer for the PGW problem   \n11 and the relationship between PGW and GW, among others. We then propose two   \n12 variants of the Frank-Wolfe algorithm for solving the PGW problem and show   \n13 that they are mathematically and computationally equivalent. Moreover, based   \n14 on our PGW metric, we introduce the analogous concept of barycenters for mm  \n15 spaces. Finally, we validate the effectiveness of our PGW metric and related solvers   \n16 in applications such as shape matching, shape retrieval, and shape interpolation,   \n17 comparing them against existing baselines. ", "page_idx": 0}, {"type": "text", "text": "18 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "19 The classical optimal transport (OT) problem [1] seeks to match two probability measures while   \n20 minimizing the expected transportation cost. At the heart of classical OT theory lies the principle of   \n21 mass conservation, which aims to optimize the transfer between two probability measures, assuming   \n22 they have the same total mass and strictly preserving it. Statistical distances that arise from OT,   \n23 such as Wasserstein distances, have been widely applied across various machine learning domains,   \n24 ranging from generative modeling [2, 3] to domain adaptation [4] and representation learning [5].   \n25 Recent advancements have extended the OT problem to address certain limitations within machine   \n26 learning applications. These advancements include: 1) facilitating the comparison of non-negative   \n27 measures that possess different total masses via unbalanced [6] and partial OT [7], and 2) enabling   \n28 the comparison of probability measures across distinct metric spaces through Gromov-Wasserstein   \n29 distances [8], with applications spanning from quantum chemistry [9] to natural language processing   \n30 [10].   \n31 Regarding the first aspect, many applications in machine learning involve comparing non-negative   \n32 measures (often empirical measures) with varying total amounts of mass, e.g., domain adaptation   \n33 [11]. Moreover, OT distances (or dissimilarity measures) are often not robust against outliers and   \n34 noise, resulting in potentially high transportation costs for outliers. Many recent publications have   \n35 focused on variants of the OT problem that allow for comparing non-negative measures with unequal   \n36 mass. For instance, the optimal partial transport problem [7, 12, 13, 14], Kantorovich\u2013Rubinstein   \n37 norm [15, 16, 17], and the Hellinger\u2013Kantorovich distance [18, 19]. These methods fall under the   \n38 broad category of \u201cunbalanced optimal transport\u201d. In this regard, we also highlight [20, 21, 22],   \n39 which enhance OT\u2019s robustness in the presence of outliers.   \n40 Regarding the second aspect, comparing probability measures across different metric spaces is   \n41 essential in many machine learning applications, ranging from computer graphics, where shapes and   \n42 surfaces are compared [23, 24], to graph partitioning and matching problems [25]. Source and target   \n43 distributions often arise from varied conditions, such as different times, contexts, or measurement   \n44 techniques, creating substantial differences in intrinsic distances among data points. The conventional   \n45 OT framework necessitates a meaningful distance across diverse domains, a requirement that is not   \n46 always achievable. To circumvent this issue, the Gromov-Wasserstein (GW) distances were proposed   \n47 in [8, 24] as an adaptation of the Gromov-Hausdorff distance, which measures the discrepancy   \n48 between two metric spaces [26, 27, 28, 29]. The GW distance [8, 30] extends OT-based distances to   \n49 metric measure spaces (mm-spaces) up to isometries. Its invariance across isomorphic mm-spaces   \n50 makes the GW distance particularly valuable for applications like shape comparison and matching,   \n51 where invariance to rigid motion transformations is crucial.   \n52 The main computational challenge of the GW metric is the non-convexity of its formulation [8]. The   \n53 conventional computational approach relies on the Frank-Wolfe (FW) algorithm [31, 32]. Optimal   \n54 transport (OT) computational methods [15, 33, 34, 35, 36, 37, 38, 39, 40], such as the Sinkhorn   \n55 algorithm, can be incorporated into FW iterations, which yields the classical GW solvers [41, 42, 43].   \n56 Given that the GW distance is limited to the comparison of probability mm-spaces, recent works   \n57 have introduced unbalanced and partial variations [44, 45, 46]. These variations have been applied in   \n58 diverse contexts, including partial graph matching for social network analysis [47] and the alignment   \n59 of brain images [48]. Although solving these unbalanced variants of the GW problem yields notions   \n60 of discrepancies between mm-spaces, their metric properties remain unclear in the literature.   \n61 Motivated by the emerging applications of the GW problem in unbalanced settings, this paper focuses   \n62 on developing a metric between general (not necessarily probability) mm-spaces and providing   \n63 efficient solvers for its computation. Our proposed metric arises from formulating a variant of the GW   \n64 problem for unbalanced contexts, rooted in the framework provided by [44], which we named the   \n65 Partial Gromov-Wasserstein (PGW) problem. In contrast to [44], which introduces a KL-divergence   \n66 penalty and a Sinkhorn solver, we employ a total variation penalty, demonstrate the resulting metric   \n67 properties, and provide novel, efficient solvers for this problem. To the best of our knowledge, this   \n68 paper presents the first metric for non-probability mm-spaces based on the GW distance. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "69 Contributions. Our specific contributions in this paper are: ", "page_idx": 1}, {"type": "text", "text": "70 \u2022 GW metric in unbalanced settings. We propose the Partial Gromov-Wasserstein (PGW)   \n71 problem and prove that it gives rise to a metric between arbitrary mm-spaces.   \n72 \u2022 PGW solver.Analogous to the technique presented in [12], we show that the PGW problem   \n73 can be turned into a variant of the GW problem. Based on this relation, we propose two   \n74 mathematically equivalent, but distinct in numerical implementation, Frank-Wolfe solvers   \n75 for the discrete PGW problem. Inspired by the results of [32], we prove that similar to the   \n76 Frank-Wolfe solver presented in [45], our proposed solvers for the PGW problem converge   \n77 linearly to a stationary point.   \n78 \u2022 Numerical experiments. We demonstrate the performance of our proposed algorithms in   \n79 terms of computation time and efficacy on a series of tasks: shape-matching with outliers   \n80 between 2D and 3D objects, shape retrieval between 2D shapes, and shape interpolation   \n81 using the concept of PGW barycenters. We compare the performance of our proposed   \n82 algorithms against existing baselines for each task. ", "page_idx": 1}, {"type": "text", "text": "83 2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "84 In this section, we review the basics of OT theory, one of its variants in unbalanced contexts called   \n85 Partial OT (POT), and their connection as established in [12]. We then introduce the GW distance. ", "page_idx": 1}, {"type": "text", "text": "86 2.1 Optimal Transport and Partial Optimal Transport ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "87 Let $\\Omega\\subseteq\\mathbb{R}^{d}$ be, for simplicity, a compact subset of $\\mathbb{R}^{d}$ , and ${\\mathcal{P}}(\\Omega)$ be the space of probability measures   \n88 defined on the Borel $\\sigma$ -algebra of $\\Omega$ .   \n89 The Optimal Transport (OT) problem for $\\mu,\\nu\\in\\mathscr{P}(\\Omega)$ , with transportation cost $c(x,y):\\Omega\\times\\Omega\\rightarrow$   \n90 $\\mathbb{R}_{+}$ being a lower-semi continuous function, is defined as: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\nO T(\\mu,\\nu):=\\operatorname*{min}_{\\gamma\\in\\Gamma(\\mu,\\nu)}\\gamma(c),\\qquad{\\mathrm{where}}\\quad\\gamma(c):=\\int_{\\Omega^{2}}c(x,y)\\,d\\gamma(x,y)\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "91 and where $\\Gamma(\\mu,\\nu)$ denotes the set of all joint probability measures on $\\Omega^{2}:=\\Omega\\times\\Omega$ with marginals   \n92 $\\mu,\\nu$ , i.e., $\\gamma_{1}:=\\pi_{1\\#}\\gamma=\\mu,\\gamma_{2}:=\\pi_{2\\#}\\gamma=\\nu$ , where $\\pi_{1}\\overline{{{,}}}\\,\\pi_{2}:\\Omega^{2}\\to\\Omega$ are the canonical projections   \n93 $\\pi_{1}(x,y):=x,\\pi_{2}(x,y):=y$ . A minimizer for (1) always exists [1, 49] and when $c(x,y)=\\|{\\bar{x}}-y\\|^{p}$ ,   \n94 for $p\\geq1$ , it defines a metric on ${\\mathcal{P}}(\\Omega)$ , which is referred to as the ${}^{\\bullet\\bullet}p$ -Wasserstein distance\u201d: ", "page_idx": 2}, {"type": "equation", "text": "$$\nW_{p}^{p}(\\mu,\\nu):=\\operatorname*{min}_{\\gamma\\in\\Gamma(\\mu,\\nu)}\\int_{\\Omega^{2}}\\|x-y\\|^{p}d\\gamma(x,y).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "95 The Partial Optimal Transport (POT) problem [6, 13, 50] extends the OT problem to the set of   \n96 Radon measures $\\mathcal{M}_{+}(\\Omega)$ , i.e., non-negative and finite measures. For $\\lambda>0$ and $\\mu,\\nu\\in\\mathcal{M}_{+}(\\Omega)$ , the   \n97 POT problem is defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nP O T(\\mu,\\nu;\\lambda):=\\operatorname*{inf}_{\\gamma\\in\\mathcal{M}_{+}(\\Omega^{2})}\\gamma(c)+\\lambda(|\\mu-\\gamma_{1}|+|\\nu-\\gamma_{2}|),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where, in general, $|\\sigma|$ denotes the total variation norm of a measure $\\sigma$ , i.e., $|\\sigma|\\,:=\\,\\sigma(\\Omega)$ . The constraint $\\gamma\\in\\mathcal{M}_{+}(\\Omega^{2})$ in (3) can be further restricted to $\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu)$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Gamma_{\\leq}(\\mu,\\nu):=\\{\\gamma\\in\\mathcal{M}_{+}(\\Omega^{2}):\\gamma_{1}\\leq\\mu,\\gamma_{2}\\leq\\nu\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "98 denoting $\\gamma_{1}\\leq\\mu$ if for any Borel set $B\\subseteq\\Omega$ , $\\gamma_{1}(B)\\leq\\mu(B)$ (respectively, for $\\gamma_{2}\\leq\\nu$ ) [7]. Roughly   \n99 speaking, the linear penalization indicates that if the classical transportation cost exceeds $2\\lambda$ , it is   \n100 better to create/destroy\u2019 mass (see [40] for further details).   \n101 The relationship between POT and OT. By using the techniques in [12], the POT problem can be   \n102 transferred into an OT problem, and thus, OT solvers (e.g., network simplex) can be employed to   \n103 solve the POT problem.   \n104 Proposition 2.1. $I I2$ , 40] Given $\\mu,\\nu\\in\\mathcal{M}_{+}(\\Omega)$ , construct the following measures on $\\hat{\\Omega}:=\\Omega\\cup\\{\\hat{\\infty}\\}$ ,   \n105 for an auxiliary point $\\hat{\\infty}$ : ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\mu}=\\mu+|\\nu|\\delta_{\\hat{\\infty}}\\ \\ \\ \\ a n d\\ \\ \\ \\ \\hat{\\nu}=\\nu+|\\mu|\\delta_{\\hat{\\infty}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "106 Consider the following OT problem ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\cal O T}(\\hat{\\mu},\\hat{\\nu})=\\operatorname*{min}_{\\hat{\\gamma}\\in\\Gamma(\\hat{\\mu},\\hat{\\nu})}\\hat{\\gamma}(\\hat{c}),\\qquad w h e r e\\quad\\hat{c}(x,y):=\\left\\{\\!\\!\\begin{array}{l l}{c(x,y)-2\\lambda}&{\\mathrm{~}i f x,y\\in\\Omega,}\\\\ {0}&{e l s e w h e r e.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "107 Then, there exists a bijection ${\\cal F}:\\Gamma_{\\le}(\\mu,\\nu)\\to\\Gamma(\\hat{\\mu},\\hat{\\nu})$ given by ", "page_idx": 2}, {"type": "equation", "text": "$$\nF(\\gamma):=\\gamma+(\\mu-\\gamma_{1})\\otimes\\delta_{\\hat{\\infty}}+\\delta_{\\hat{\\infty}}\\otimes(\\nu-\\gamma_{2})+|\\gamma|\\delta_{\\hat{\\infty},\\hat{\\infty}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "108 such that $\\gamma$ is optimal for the POT problem (3) if and only if $F(\\gamma)$ is optimal for the OT problem (5). ", "page_idx": 2}, {"type": "text", "text": "109 It is worth noting that instead of considering the same underlying space $\\Omega$ for both measures $\\mu$ and $\\nu$ ,   \n110 the OT and POT problems can be formulated in the scenario where $\\mu$ and $\\nu$ are defined on different   \n111 metric spaces $X$ and $Y$ , respectively. In this setting, one needs a cost function $c:X\\times Y\\rightarrow\\mathbb{R}_{+}$ to   \n112 formulate the OT and POT problems. However, in practice it is usually difficult to define reasonable   \n113 \u2018distance\u2019 or ground cost $c(\\cdot,\\cdot)$ between the two spaces $X$ and $Y$ . In particular, the $p$ -Wasserstein   \n114 distance cannot be adopted if $\\mu,\\nu$ are defined on different spaces. To relax this requirement, in the   \n115 next section, we will review the fundamentals of the Gromov-Wasserstein problem [8]. ", "page_idx": 2}, {"type": "text", "text": "116 2.2 The Gromov-Wasserstein (GW) Problem ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "117 A metric measure space (mm-space) consists of a set $X$ endowed with a metric structure, that is, a   \n118 notion of distance $d_{X}$ between its elements, and equipped with a Borel measure $\\mu$ . As in [8, Ch.   \n119 5], we will assume that $X$ is compact and that $\\operatorname{supp}(\\mu)=X$ . Given two probability mm-spaces   \n120 $\\mathbb{X}\\,=\\,(X,d_{X},\\mu)$ , $\\mathbb{Y}\\,=\\,(Y,d_{Y},\\nu)$ , with $\\mu\\ \\in\\ {\\mathcal{P}}(X)$ and $\\nu\\,\\in\\,\\mathcal{P}(Y)$ , and a non-negative lower   \n121 semi-continuous cost function $L:\\mathbb{R}^{2}\\,\\rightarrow\\,\\mathbb{R}_{+}$ (e.g., the Euclidean distance or the KL-loss), the   \n122 Gromov-Wasserstein (GW) matching problem is defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nG W^{L}(\\mathbb{X},\\mathbb{Y}):=\\operatorname*{inf}_{\\gamma\\in\\Gamma(\\mu,\\nu)}\\gamma^{\\otimes2}(L(d_{X}(\\cdot,\\cdot),d_{Y}(\\cdot,\\cdot))),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "123 where, for brevity, we employ the notation $\\gamma^{\\otimes2}$ for the product measure $d\\gamma^{\\otimes2}((x,y),(x^{\\prime},y^{\\prime}))=$   \n124 $d\\gamma(x,y)d\\gamma(x^{\\prime},y^{\\prime})$ . If $L(a,b)=|a\\!-\\!b|^{p}$ , for $1\\leq p<\\infty$ , we denote $G W^{L}(\\cdot,\\cdot)$ simply by $G W^{p}(\\cdot,\\cdot)$ .   \n125 In this case, the expression (7) defines an equivalence relation $\\sim$ among probability mm-spaces, i.e.,   \n126 $\\mathbb{X}\\sim\\mathbb{Y}$ if and only if $G W^{p}(\\mathbb{X},\\mathbb{Y})=0^{1}$ . A minimizer of the GW problem (7) always exists, and thus,   \n127 we can replace inf by min. Moreover, similar to OT, the above GW problem defines a distance for   \n128 probability mm-spaces after taking the quotient under $\\sim$ . For details, we refer to [8, Ch. 5 and 10]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "129 3 The Partial Gromov-Wasserstein (PGW) Problem ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "130 The Unbalanced Gromov-Wasserstein (UGW) problem for general (compact) mm-spaces $\\mathbb{X}=$   \n131 $(X,d_{X},\\mu)$ , $\\mathbb{Y}=(Y,d_{Y},\\nu)$ , with $\\mu\\in\\mathcal{M}_{+}(X),\\nu\\in\\mathcal{M}_{+}(Y)$ , studied in [44] is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nU G W_{\\lambda}^{L}(\\mathbb{X},\\mathbb{Y}):=\\operatorname*{inf}_{\\gamma\\in{\\mathcal{M}}_{+}(X\\times Y)}\\gamma^{\\otimes2}(L(d_{X},d_{Y}))+\\lambda(D_{\\phi}(\\gamma_{1}^{\\otimes2}\\parallel\\mu^{\\otimes2})+D_{\\phi}(\\gamma_{2}^{\\otimes2}\\parallel\\nu^{\\otimes2})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "132 where $\\lambda>0$ is a fixed linear penalization parameter, and $D_{\\phi}$ is a Csisz\u00e1r or $\\phi$ -divergence. The above   \n133 formulation extends the classical GW problem (7) into the unbalanced setting $\\lvert\\mu$ and $\\nu$ are no longer   \n134 necessarily probability measures but general Radon measures).   \n135 We underline two points: First, as discussed in [44], while the above quantity allows us to \u2018compare\u2019   \n136 the mm-spaces $\\mathbb{X}$ and $\\mathbb{Y}$ , its metric property is unclear. Secondly, when $D_{\\phi}$ is the KL divergence, a   \n137 Sinkhorn solver has been proposed in [44]. However, a solver for general $\\phi$ -divergences has not yet   \n138 been proposed.   \n139 In this paper, we will analyze the case when $D_{\\phi}$ is the total variation norm. Specifically, for $q\\geq1$ ,   \n140 we consider the following problem, which we refer to as the Partial Gromov-Wasserstein (PGW)   \n141 problem: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{P G W}_{\\lambda,q}^{L}(\\mathbb{X},\\mathbb{Y}):=\\operatorname*{inf}_{\\gamma\\in\\mathcal{M}_{+}(X\\times Y)}\\gamma^{\\otimes2}(L(d_{X}^{q},d_{Y}^{q}))+\\lambda(|\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2}|+|\\nu^{\\otimes2}-\\gamma_{2}^{\\otimes2}|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "142 Remark 3.1. Given $\\gamma\\in\\Gamma\\leq(\\mu,\\nu)$ , the above cost functional can be rewritten as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\gamma^{\\otimes2}(L(d_{X}^{q},d_{Y}^{q}))+\\lambda(|\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2}|+|\\nu^{\\otimes2}-\\gamma_{2}^{\\otimes2}|)=\\gamma^{\\otimes2}\\left(L(d_{X}^{q},d_{Y}^{q})-2\\lambda\\right)+\\lambda\\left(|\\mu|^{2}+|\\nu|^{2}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "143 Proposition 3.2. Given mm-spaces $\\mathbb{X}=(X,d_{X},\\mu)$ , $\\mathbb{Y}=(Y,d_{Y},\\nu)$ , the minimization problem (9)   \n144 can be restricted to the set $\\Gamma_{\\leq}(\\mu,\\nu)=\\{\\gamma\\in\\mathcal{M}_{+}(X\\times Y):\\gamma_{1}\\leq\\mu,\\gamma_{2}\\leq\\nu\\}$ . That is, ", "page_idx": 3}, {"type": "equation", "text": "$$\nP G W_{\\lambda,q}^{L}(\\mathbb{X},\\mathbb{Y})=\\operatorname*{inf}_{\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu)}\\gamma^{\\otimes2}\\left(L(d_{X}^{q},d_{Y}^{q})-2\\lambda\\right)+\\lambda(|\\mu|^{2}+|\\nu|^{2}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "145 For the proof, inspired by [50], we direct the reader to Appendix B. ", "page_idx": 3}, {"type": "text", "text": "146 We notice that a similar Partial Gromov-Wasserstein problem (and its solver) has been studied [45].   \n147 Indeed, in [45], the $\\lambda$ -penalization in the optimization problem (10) is avoided, but the constraint set   \n148 is replaced by the subset of all $\\gamma\\in\\Gamma_{\\leq}(\\mu,\\bar{\\nu})$ such that $|\\gamma|=\\rho$ for a fixed $\\rho\\in[0,\\operatorname*{min}\\{|\\mu|,|\\nu|\\}]$ . We   \n149 will call this formulation the Mass-Constrained Partial Gromov-Wasserstein (MPGW) problem. In   \n150 Appendix L, we explore the relations between PGW and MPGW, and in Section 5 and Appendices N,   \n151 O, P, we analyze the performance of the different solvers through different experiments.   \n152 Proposition 3.3. If $L(r_{1},r_{2})=|r_{1}-r_{2}|^{p}$ , for $p\\in[1,\\infty)$ , we use $P G W_{\\lambda,q}^{p}$ to denote $P G W_{\\lambda,q}^{L}$ . $I n$   \n153 this case, (9) and (10) admit a minimizer.   \n154 The proof is given in Appendix C: Its idea extends results from [8] from probability mm-spaces to   \n155 arbitrary mm-spaces.   \n156 Next, we state one of our main results: The PGW problem gives rise to a metric between mm-spaces.   \n157 The rigorous statement as well as its proof is given in Appendix D.   \n158 Proposition 3.4. Let $\\lambda>0$ , $1\\leq q,p<\\infty$ and $L(r_{1},r_{2})=|r_{1}-r_{2}|^{p}$ . Then $(P G W_{\\lambda,q}^{p}(\\cdot,\\cdot))^{1/p}$   \n159 defines a metric between mm-spaces.   \n160 Finally, for consistency, we provide the following result when the penalization tends to infinity. Its   \n161 proof is given in Appendix E.   \n162 Proposition 3.5. Consider probability mm-spaces $\\mathbb{X}=(X,d_{X},\\mu),\\,\\mathbb{Y}=(Y,d_{Y},\\nu)$ , that is, $|\\mu|=$   \n163 $\\left|\\underline{{\\boldsymbol\\nu}}\\right|=1$ . Assume that $L$ is a continuous funtion. Then $\\begin{array}{r}{\\operatorname*{lim}_{\\lambda\\rightarrow\\infty}P\\dot{G}W_{\\lambda,1}^{L}(\\mathbb{X},\\mathbb{Y})=G W^{L}(\\mathbb{X},\\mathbb{Y})}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "164 4 Computation of the Partial GW Distance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "165 In the discrete setting, consider mm-s $\\begin{array}{r}{\\mathrm{)aces\\,\\mathbb{X}}=(X,d_{X},\\sum_{i=1}^{n}p_{i}^{X}\\delta_{x_{i}}),\\mathbb{Y}=(Y,d_{Y},\\sum_{j=1}^{m}q_{j}^{Y}\\delta_{y_{j}}).}\\end{array}$   \n166 where $X=\\{x_{1},\\ldots,x_{n}\\}$ , $Y=\\{y_{1},\\dots,y_{m}\\}$ , the weights $p_{i}^{X},\\,q_{j}^{Y}$ are non-negative numbers, and   \n167 the distances $d_{X},d_{Y}$ are determined by the matrices $C^{X}\\in\\mathbb{R}^{n\\times n}$ , $C^{Y}\\in\\mathbb{R}^{m\\times m}$ defined by ", "page_idx": 4}, {"type": "equation", "text": "$$\nC_{i,i^{\\prime}}^{X}:=d_{X}^{q}(x_{i},x_{i^{\\prime}})\\quad\\forall i,i^{\\prime}\\in[1:n]\\quad\\mathrm{and}\\quad C_{j,j^{\\prime}}^{Y}:=d_{Y}^{q}(y_{j},y_{j^{\\prime}})\\quad\\forall j,j^{\\prime}\\in[1:m].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "168 Let $\\mathrm{~p~}:=[q_{1}^{X},\\dots,q_{n}^{X}]^{\\top}$ and $\\mathfrak{q}:=[q_{1}^{Y},\\dots,q_{m}^{Y}]^{\\top}$ denote the weight vectors corresponding to the   \n169 given discrete measures. We view the sets of transportation plans $\\Gamma(\\mathrm{p},\\mathrm{q})$ and $\\Gamma_{\\le}(\\mathrm{p},\\mathrm{{q}})$ for the GW   \n170 and PGW problems, respectively, as the subsets of $n\\times m$ matrices ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Gamma(\\mathbf{p},\\mathbf{q}):=\\{\\gamma\\in\\mathbb{R}_{+}^{n\\times m}:\\gamma\\mathbf{1}_{m}=\\mathbf{p},\\gamma^{\\top}\\mathbf{1}_{n}=\\mathbf{q}\\},\\quad\\mathrm{if}\\ |\\mathbf{p}|=\\sum_{i=1}^{n}p_{i}^{X}=\\mathbf{1}=\\sum_{j=1}^{m}q_{j}^{Y}=|\\mathbf{q}|;\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "171 ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{q}):=\\{\\gamma\\in\\mathbb{R}_{+}^{n\\times m}:\\gamma1_{m}\\leq\\mathrm{p},\\gamma^{\\top}1_{n}\\leq\\mathrm{q}\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "172 for any pair of non-negative vectors $\\mathrm{~p~}\\!\\in\\mathbb{R}_{+}^{n}$ , $\\mathrm{q}\\in\\mathbb{R}_{+}^{m}$ , where $1_{n}$ is the vector with all ones in $\\mathbb{R}^{n}$   \n173 (resp. $1_{m.}$ ), and $\\gamma1_{m}\\leq\\mathrm{p}$ means that component-wise the $\\leq$ relation holds.   \n174 Given by a non-negative function $L:\\mathbb{R}^{n\\times n}\\times\\mathbb{R}^{m\\times m}\\rightarrow\\mathbb{R}_{+}$ , he transportation cost $M$ and the   \n175 \u2018partial\u2019 transportation con $\\tilde{M}$ are represented by the $n\\times m\\times n\\times m$ tensors: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nM_{i,j,i^{\\prime},j^{\\prime}}=L(C_{i,i^{\\prime}}^{X},C_{j,j^{\\prime}}^{Y})\\qquad\\mathrm{and}\\qquad\\tilde{M}:=M-2\\lambda:=M-2\\lambda1_{n,m,n,m},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $_{1_{n,m,n,m}}$ is the tensor with ones in all its entries. For each $n\\times m\\times n\\times m$ tensor $M$ and each $n\\times m$ matrix $\\gamma$ , we define tensor-matrix multiplication $M\\circ\\gamma\\in\\mathbb{R}^{n\\times m}$ by ", "page_idx": 4}, {"type": "equation", "text": "$$\n(M\\circ\\gamma)_{i j}=\\sum_{i^{\\prime},j^{\\prime}}(M_{i,j,i^{\\prime},j^{\\prime}})\\gamma_{i^{\\prime},j^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "176 Then, the Partial GW problem in (10) can be written as ", "page_idx": 4}, {"type": "equation", "text": "$$\nP G W_{\\lambda}^{L}(\\mathbb{X},\\mathbb{Y})=\\operatorname*{min}_{\\gamma\\in\\Gamma_{\\leq}(\\mathrm{p},\\mathbf{q})}\\mathcal{L}_{\\tilde{M}}(\\gamma)+\\lambda(|\\mathrm{p}|^{2}+|\\mathbf{q}|^{2}),\\quad\\mathrm{~where~}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "177 ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\tilde{M}}(\\gamma):=\\tilde{M}\\gamma^{\\otimes2}:=\\sum_{i,j,i^{\\prime},j^{\\prime}}\\tilde{M}_{i,j,i^{\\prime},j^{\\prime}}\\gamma_{i,j}\\gamma_{i^{\\prime},j^{\\prime}}=\\sum_{i j}(\\tilde{M}\\circ\\gamma)_{i j}\\gamma_{i j}=:\\langle\\tilde{M}\\circ\\gamma,\\gamma\\rangle_{F},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "178 and $\\langle\\cdot,\\cdot\\rangle_{F}$ stands for the Frobenius dot product. The constant term $\\lambda(|\\mathrm{p}|^{2}+|\\mathrm{q}|^{2})$ will be ignored in   \n179 the rest of this paper since it does not depend on $\\gamma$ . ", "page_idx": 4}, {"type": "text", "text": "180 4.1 Frank-Wolfe for the PGW Problem \u2013 Solver 1 ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "181 In this section, we discuss the Frank-Wolfe (FW) algorithm for the PGW problem (15). A second   \n182 variant of the FW solver is provided in the Appendix G.   \n183 As a summary, in our proposed method, we address the discrete PGW problem (15), highlighting   \n184 that the direction-finding subproblem in the Frank-Wolfe (FW) algorithm is a POT problem for (15).   \n185 Specifically, (15) is treated as a discrete POT problem in our Solver 1, where we apply Proposition   \n186 2.1 to solve a discrete OT problem. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "187 For each iteration $k$ , the procedure is summarized in three steps detailed below. ", "page_idx": 4}, {"type": "text", "text": "188 The convergence analysis, detailed in Appendix K, applies the results from \u221a[32] to our context,   \n189 showing that the FW algorithm achieves a stationary point at a rate of ${\\mathcal{O}}(1/{\\sqrt{k}})$ for non-convex   \n190 objectives with a Lipschitz continuous gradient in a convex and compact domain. ", "page_idx": 4}, {"type": "text", "text": "191 Step 1. Computation of gradient and optimal direction. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "192 It is straightforward to verify that the gradient of the objective function (16) in (15) is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{L}_{\\tilde{M}}(\\gamma)=2\\tilde{M}\\circ\\gamma.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "193 The classical method to compute $M\\circ\\gamma$ is the following: First, convert $M$ into an $(n\\times m)\\times(n\\times m)$   \n194 matrix, denoted as $v(M)$ , and convert $\\gamma$ into an $(n\\times m)\\times1$ vector $v(\\gamma)$ . Then, the computation   \n195 of $M\\circ\\gamma$ is equivalent to the matrix multiplication $v(M)v(\\gamma)$ . The computational cost and the ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1: Frank-Wolfe Algorithm for PGW, ver 1 ", "page_idx": 5}, {"type": "text", "text": "Input: $\\begin{array}{r}{\\mu=\\sum_{i=1}^{n}p_{i}^{X}\\delta_{x_{i}},\\nu=\\sum_{j=1}^{m}q_{j}^{Y}\\delta_{y_{j}},\\gamma^{(1)}}\\end{array}$   \nOutput: \u03b3(final)   \nCompute $\\boldsymbol{C}^{X},\\boldsymbol{C}^{Y}$   \nfor $k=1,2,\\dots$ do $G^{(k)}\\gets2\\tilde{M}\\circ\\gamma^{(k)}\\,/$ / Compute gradient $\\begin{array}{r}{\\gamma^{(k)^{\\prime}}\\gets\\arg\\operatorname*{min}_{\\gamma\\in\\Gamma_{\\leq}(\\mathbf{p},\\mathbf{q})}\\langle G^{(k)},\\gamma\\rangle_{F}}\\end{array}$ // Solve the POT problem. Compute $\\alpha^{(k)}\\in[0,1]$ via (18) // Line search $\\gamma^{(k+1)}\\gets(1-\\alpha^{(k)})\\gamma^{(k)}+\\alpha^{(k)}\\gamma^{(k)^{\\prime}}//$ Update $\\gamma$ if convergence, break   \nend for   \n$\\gamma^{(f i n a l)}\\leftarrow\\gamma^{(k)}$ ", "page_idx": 5}, {"type": "text", "text": "196 required storage space are $O(n^{2}m^{2})$ . In certain conditions, the above computation can be reduced to   \n197 $O(n^{2}+m^{2})$ . We refer to Appendices $\\boldsymbol{\\mathrm F}$ and $\\mathrm{H}$ for details. ", "page_idx": 5}, {"type": "text", "text": "198 Next, we aim to solve the following problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\gamma^{(k)^{\\prime}}\\gets\\mathrm{arg}\\operatorname*{min}_{\\gamma\\in\\Gamma_{\\le}(\\mathrm{p},\\mathrm{q})}\\langle\\nabla\\mathcal{L}_{\\tilde{M}}(\\gamma^{(k)}),\\gamma\\rangle_{F},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which is a discrete POT problem since it is equivalent to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\gamma\\in\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{q})}\\langle2M\\circ\\gamma^{(k)},\\gamma\\rangle_{F}+\\lambda|\\gamma^{(k)}|(|\\mathrm{p}|+|\\mathrm{q}|-2|\\gamma|).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "199 The solver can be obtained by firstly converting the POT problem into an OT problem via Proposition   \n200 2.1 and then solving the proposed OT problem. ", "page_idx": 5}, {"type": "text", "text": "201 Step 2: Line search method. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this step, at the $k$ -th iteration, we need to determine the optimal step size: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha^{(k)}=\\arg\\operatorname*{min}_{\\alpha\\in[0,1]}\\{\\mathcal{L}_{\\tilde{M}}((1-\\alpha)\\gamma^{(k)}+\\alpha\\gamma^{(k)^{\\prime}})\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "202 The optimal $\\alpha^{(k)}$ takes the following values (see Appendix I for details): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Let}\\,\\alpha^{(k)}=\\left\\{\\begin{array}{l l}{0}&{\\mathrm{if~}a\\leq0,a+b>0,}\\\\ {1}&{\\mathrm{if~}a\\leq0,a+b\\leq0,\\mathrm{~where~}\\left\\{\\begin{array}{l l}{\\delta\\gamma^{(k)}=\\gamma^{(k)^{\\prime}}-\\gamma^{(k)},}\\\\ {a=\\langle\\tilde{M}\\circ\\delta\\gamma^{(k)},\\delta\\gamma^{(k)}\\rangle_{F}\\ ,}\\\\ {b=2\\langle\\tilde{M}\\circ\\gamma^{(k)},\\delta\\gamma^{(k)}\\rangle_{F}.}\\end{array}\\right.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "203 and $\\begin{array}{r}{\\mathrm{clip}(\\frac{-b}{2a},[0,1])=\\operatorname*{min}\\{\\operatorname*{max}\\{-\\frac{b}{2a},0\\},1\\}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "205 4.2 Numerical Implementation Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "206 The initial guess, $\\gamma^{(1)}$ . In the GW problem, the initial guess is simply set to $\\gamma^{(1)}=\\mathrm{pq}^{\\top}$ if there   \n207 is no prior knowledge. In PGW, however, as $\\mu,\\nu$ may not necessarily be probability measures   \n208 (i.e., $\\begin{array}{r}{\\sum_{i}p_{i}^{X},\\sum_{j}q_{j}^{Y}\\neq1}\\end{array}$ in general), we set $\\begin{array}{r}{\\gamma^{(1)}=\\frac{\\mathrm{p}\\mathbf{q}^{\\top}}{\\operatorname*{max}(|\\mathbf{p}|,|\\mathbf{q}|)}.}\\end{array}$ maxp(|qp|,|q|). It is straightforward to verify that   \n209 $\\gamma^{(1)}\\in\\Gamma_{\\le}(\\mathrm{p},\\mathrm{q})$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\gamma^{(1)}1_{m}=\\frac{|\\mathbf{q}|\\mathrm{p}}{\\operatorname*{max}(|\\mathbf{p}|,|\\mathbf{q}|)}\\leq\\mathrm{p},\\,\\,\\,\\gamma^{(1)\\top}1_{n}=\\frac{|\\mathbf{p}|\\mathrm{q}}{\\operatorname*{max}(|\\mathbf{p}|,|\\mathbf{q}|)}\\leq\\mathbf{q}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "210 Column/Row-Reduction. According to the interpretation of the penalty weight parameter in the   \n211 Partial OT problem (e.g. see Lemma 3.2 in [40]), during the POT solving step, for each $i\\in[1:n]$   \n212 (or $j\\in[1:m])$ , if the $i^{t h}$ row $(j^{t h}$ column) of $\\tilde{M}\\circ\\gamma^{(k)}$ contains a non-negative entry, all the mass   \n213 of $\\bar{p}_{i}^{X}\\bar{(q_{j}^{Y})}$ will be destroyed (created). Thus, we can remove the corresponding row (column) to   \n214 improve the computational efficiency. ", "page_idx": 5}, {"type": "text", "text": "215 5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "216 In addition to the three experiments detailed here, we also perform a wall-clock time comparison   \n217 of our proposed PGW solvers in Appendix O and a positive-unlabeled (PU) learning experiment in   \n218 Appendix P. ", "page_idx": 6}, {"type": "text", "text": "219 5.1 Toy Example: Shape Matching with Outliers ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "220 We use the moon dataset and synthetic 2D/3D spherical data in this experiment. Let $\\{x_{i}\\}_{i=1}^{n},\\{y_{j}\\}_{j=1}^{n}$   \n221 denote the source and target point clouds. In addition, we add $\\eta n$ (where $\\eta=20\\%$ ) outliers to the   \n222 target point cloud. See Figure 1 for visualization.   \n223 We visualize the transportation plans given by the GW [8], MPGW [45], UGW [44], and our proposed   \n224 PGW problems. For MPGW, UGW, and PGW, we set the mass to be 1 for each point in the source   \n225 and target point clouds. For GW, we normalize the mass of these points so that the source and target   \n226 have the same total mass. From Figure 1, we observe that PGW and MPGW induce a one-by-one   \n227 relation in both cases and no outlier points are matched to the source point cloud. Meanwhile, GW   \n228 matches all of the outliers. For UGW, as it applies the Sinkhorn algorithm, we observe mass-splitting   \n229 transportation plans in both cases. Moreover, we observe that some mass from the outliers has been   \n230 matched, which is not desired. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "nrcFNxF57E/tmp/ab0fb85d9a6ef4308dc47111f28d4c781aab6d2f72a9d7aae5c645d910dab7a3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 1: The set of red points comprises the source point cloud. The union of the dark blue (outliers) and light blue points comprises the target point cloud. For UGW, MPGW, and PGW, we set the mass for each point to be the same. For GW, we normalize the mass for the balanced mass constraint setting. ", "page_idx": 6}, {"type": "text", "text": "231 5.2 Shape Retrieval ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "232 Experiment setup. We now employ the PGW distance to distinguish between 2D shapes, as done   \n233 in [51], and use GW, MPGW, and UGW as baselines for comparison. Given a series of 2D shapes,   \n234 we represent the shapes as mm-spaces Xi = (R2, \u2225\u00b7 \u22252, \u00b5i), where \u00b5i = kni= 1 \u03b1i\u03b4xi. For the GW   \n235 method, we normalize the mass for the balanced mass constraint setting (i.e. $\\begin{array}{r}{\\alpha^{i}=\\frac{1}{n^{i}};}\\end{array}$ ), and for the   \n236 remaining methods we let $\\alpha^{i}=\\alpha$ for all the shapes, where $\\alpha>0$ is a fixed constant. In this manner,   \n237 we compute the pairwise distances between the shapes.   \n238 We then use the computed distances for nearest neighbor classification. We do this by choosing a   \n239 representative at random from each class in the dataset and then classifying each shape according to   \n240 its nearest representative. This is repeated over 10,000 iterations, and we generate a confusion matrix   \n241 for each distance used. Finally, using the approach given by [51, 52], we combine each distance with   \n242 a support vector machine (SVM), applying stratified 10-fold cross validation. In each iteration of   \n243 cross validation, we train an SVM using $\\exp(-\\sigma D)$ as the kernel, where $D$ is the matrix of pairwise   \n244 distances (w.r.t. one of the considered distances) restricted to 9 folds, and compute the accuracy of   \n245 the model on the remaining fold. We report the accuracy averaged over all 10 folds for each model.   \n246 Dataset setup. We test two datasets in this experiment, which we refer to as Dataset I and Dataset II.   \n247 We construct Dataset I by adapting the 2D shape dataset given in [51], consisting of 20 shapes in   \n248 each of the classes bone, goblet, star, and horseshoe. For each class, we augment the dataset with an   \n249 additional class by selecting either a subset of points from each shape of that class (rectangle/bone,   \n250 trapezoid/goblet, disk/star) or adding additional points to each shape of that class (annulus/horseshoe).   \n251 Hence, the final dataset consists of 160 shapes across 8 total classes. This dataset is visualized in   \n252 Figure 6a.   \n253 For Dataset II, we generate 20 shapes for each of the classes rectangle, house, arrow, double arrow,   \n254 semicircle, and circle. These shapes were generated in pairs, such that each shape of class rectangle   \n255 is a subset of the corresponding shape of class house, and similarly for arrow/double arrow and   \n256 semicircle/circle. This dataset is visualized in Figure 6b.   \n257 Performance analysis. We refer to Appendix N for full numerical details, parameter settings, and   \n258 the visualization of the resulting confusion matrices. We visualize the two considered datasets and   \n259 the resulting pairwise distance matrices in Figure 2. For the SVM experiments, GW achieves the   \n260 highest accuracy on Dataset I, $98.13\\%$ , while the second best method is PGW, $96.25\\%$ . For Dataset   \n261 II, PGW achieves the highest accuracy, correctly classifying $100\\%$ of the samples. The complete set   \n262 of accuracies for all considered distances on each dataset is reported in Table 1a.   \n263 In addition, we report the wall-clock time required to compute all pairwise distances for each distance   \n264 in Table 1b. We observe that GW, MPGW, and PGW have similar wall-clock times across both   \n265 experiments (30-50 seconds for Dataset I, 80-140 seconds for Dataset II), with PGW admitting   \n266 a slightly faster runtime in both cases. Meanwhile, UGW requires almost 1500 seconds on the   \n267 experiment with Dataset I and over 500 seconds on the experiment with Dataset II. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "nrcFNxF57E/tmp/3367bef25cf2da7c7646a61e03a3b705347bde04314f628c56899292d4abba65.jpg", "img_caption": ["Figure 2: In each row, the first figure visualizes an example shape from each class, and the second figure visualizes the resulting pairwise distance matrices. The first row corresponds to Dataset I and the second corresponds to Dataset II. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "268 5.3 Partial Gromov-Wasserstein Barycenter and Shape Interpolation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "269 By [41], Gromov-Wasserstein can be applied to interpolate two shapes via the concept of Gromov  \n270 Wasserstein Barycenters. In this paper, we introduce Partial Gromov-Wasserstein Barycenters by   \n271 extending the GW Barycenter to the setting of PGW as follows.   \n272 Consider the discrete mm-spaces $\\mathbb{X}^{1},\\ldots,\\mathbb{X}^{K}$ , where $\\begin{array}{r}{\\mathbb{X}^{k}=(X^{k},\\|\\cdot\\|_{\\mathbb{R}^{d_{k}}},\\textstyle\\sum_{i=1}^{n_{k}}p_{i}^{k}\\delta_{x_{i}^{k}})}\\end{array}$ , with $X^{k}=$   \n273 $\\{x_{i}^{k}\\}_{i=1}^{n_{k}}\\subset\\mathbb{R}^{d_{k}}$ . We denote $C^{k}=[\\|x_{i}^{k}-x_{i^{\\prime}}^{k}\\|^{2}]_{i,i^{\\prime}}$ and $\\mathrm{p}^{k}=[p_{1}^{k},\\dots,p_{n_{k}}^{k}]$ . Given positive constants   \n274 $\\lambda_{1},\\dots,\\lambda_{K}>0$ , the PGW Barycenter is defined by: ", "page_idx": 7}, {"type": "table", "img_path": "nrcFNxF57E/tmp/84da7b4378a64ca2a7a2808c4a6ddb3301852dcae6cd6f20d43e9f7f4042407a.jpg", "table_caption": [], "table_footnote": ["(a) Mean accuracy of SVM using each distance in kernel. "], "page_idx": 7}, {"type": "table", "img_path": "nrcFNxF57E/tmp/436bbb459d47448ca30072fad0350defe28c9925b9da7ecd54378179076107fe.jpg", "table_caption": ["(b) Wall-clock time comparison. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "nrcFNxF57E/tmp/e2487a91205d162c1dde5df8df157a16d805669f0fc01537b41f3dc7a9bd5a17.jpg", "img_caption": ["Figure 3: In the first column, the first and second figures are the source and target point clouds in the first experiment $(\\eta=5\\%)$ ; the third and fourth figures are the source and target point clouds in the second experiment $(\\eta=10\\%)$ ). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{C,\\gamma_{k}}\\sum_{k}\\xi_{k}\\langle M(C,C^{k})\\circ\\gamma^{k},\\gamma^{k}\\rangle-2\\lambda_{k}|\\gamma^{k}|^{2}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "275 where each $\\gamma^{k}\\in\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{p}^{k})$ . We refer to Appendix $\\mathbf{M}$ for the solver of (19) and details. ", "page_idx": 8}, {"type": "text", "text": "276 Experiment setup. We apply the PGW barycenter to the following problem: Given two shapes   \n277 $\\bar{X}\\,\\,{\\stackrel{\\bullet}{=}}\\,\\{x_{i}\\}_{i=1}^{n}\\subset\\bar{\\mathbb{R}}^{d_{1}}$ and $\\dot{Y}^{-}\\dot{=}\\,\\{y_{i}\\}_{i=1}^{m}\\subset\\mathbb{R}^{d_{2}}$ , modeled as mm-spaces $\\stackrel{\\cdot}{\\mathbb{X}}=(X,\\|\\cdot\\|_{\\mathbb{R}^{d_{1}}},\\textstyle\\sum_{i=1}^{n}{\\bar{\\delta}}_{x_{i}})$   \n278 and $\\mathbb{Y}\\,=\\,(Y,\\|\\,\\cdot\\,\\|_{\\mathbb{R}^{d_{2}}},\\textstyle\\sum_{i=1}^{m}\\delta_{y_{i}})$ , we wish to find interpolations between them. In addition, we   \n279 assume $\\mathbb{Y}$ is corrupted by noise, i.e., $\\mathbb{Y}$ is redefined as $\\begin{array}{r}{\\mathbb{Y}\\,=\\,(\\tilde{Y},\\,\\|\\,\\cdot\\,\\|_{\\mathbb{R}^{d_{2}}},\\sum_{i=1}^{m}\\delta_{y_{i}}\\,+\\sum_{i=1}^{m\\eta}\\delta_{\\tilde{y}_{i}})}\\end{array}$   \n280 with $\\tilde{Y}=Y\\cup\\{\\tilde{y}_{i}\\}_{i=1}^{m}$ , where $\\eta\\in[0,1]$ is the noise level and each $\\tilde{y}_{i}$ is randomly selected from a   \n281 particular region $\\mathcal{R}\\subset\\mathbb{R}^{d_{2}}$ .   \n282 Dataset setup. We adapt the dataset given in [41]. See Appendix M.1 for further details on the   \n283 dataset. In this experiment, we test $\\eta=5\\%$ , $10\\%$ . We visualize the barycenter interpolation from   \n284 $t=0/7$ to $t=7/7$ , where $(1-t),t$ are the weight of the source $\\mathbb{X}$ and the target $\\mathbb{Y}$ , respectively,   \n285 in the barycenter (19). The visualization given in Figure 3 is obtained by applying SMACOF MDS   \n286 (multidimensional scaling) of the minimizer $C$ .   \n287 Performance analysis. From Figure 3, we observe that in this two scenarios, the interpolation   \n288 derived from GW is clearly disturbed by the noise data points. For example, in rows 1, 3, columns   \n289 $t=1/7,2/7,3/7$ , we see that the point clouds reconstructed by MDS have significantly different   \n290 width-height ratios from those of the source and target point clouds. In contrast, PGW is significantly   \n291 less disturbed, and the interpolation is more natural. The width-height ratio of the point clouds   \n292 generated by the PGW barycenter is consistent with that of the source/target point clouds. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "293 6 Summary ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "294 In this paper, we propose the Partial Gromov-Wasserstein (PGW) problem and introduce two Frank  \n295 Wolfe solvers for it. As a byproduct, we provide pertinent theoretical results, including the relation   \n296 between PGW and GW, the metric property of PGW, and the PGW barycenter. Furthermore, we   \n297 demonstrate the efficacy of the PGW solver in solving shape-matching, shape retrieval, and shape   \n298 interpolation tasks. For the shape retrieval experiment, we observe that due to the metric property,   \n299 PGW and GW have similar accuracy and outperform the other methods evaluated. In the shape   \n300 matching and point cloud interpolation experiments, we demonstrate PGW admits a more robust   \n301 result when the data are corrupted by outliers/noisy data. ", "page_idx": 8}, {"type": "text", "text": "302 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "303 [1] Cedric Villani. Optimal transport: old and new. Springer, 2009.   \n304 [2] Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial   \n305 networks. In International conference on machine learning, pages 214\u2013223. PMLR, 2017.   \n306 [3] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.   \n307 Improved training of wasserstein gans. Advances in neural information processing systems, 30,   \n308 2017.   \n309 [4] Nicolas Courty, R\u00e9mi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution   \n310 optimal transportation for domain adaptation. Advances in neural information processing   \n311 systems, 30, 2017.   \n312 [5] Soheil Kolouri, Navid Naderializadeh, Gustavo K Rohde, and Heiko Hoffmann. Wasserstein   \n313 embedding for graph learning. In International Conference on Learning Representations, 2020.   \n314 [6] Lenaic Chizat, Gabriel Peyr\u00e9, Bernhard Schmitzer, and Fran\u00e7ois-Xavier Vialard. Unbalanced   \n315 optimal transport: Dynamic and Kantorovich formulations. Journal of Functional Analysis,   \n316 274(11):3090\u20133123, 2018.   \n317 [7] Alessio Figalli. The optimal partial transport problem. Archive for rational mechanics and   \n318 analysis, 195(2):533\u2013560, 2010.   \n319 [8] Facundo M\u00e9moli. Gromov\u2013wasserstein distances and the metric approach to object matching.   \n320 Foundations of computational mathematics, 11:417\u2013487, 2011.   \n321 [9] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural   \n322 message passing for quantum chemistry. In International conference on machine learning,   \n323 pages 1263\u20131272. PMLR, 2017.   \n324 [10] David Alvarez-Melis and Tommi Jaakkola. Gromov-wasserstein alignment of word embedding   \n325 spaces. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language   \n326 Processing, pages 1881\u20131890, 2018.   \n327 [11] Kilian Fatras, Thibault S\u00e9journ\u00e9, R\u00e9mi Flamary, and Nicolas Courty. Unbalanced minibatch   \n328 optimal transport; applications to domain adaptation. In International Conference on Machine   \n329 Learning, pages 3186\u20133197. PMLR, 2021.   \n330 [12] Luis A Caffarelli and Robert J McCann. Free boundaries in optimal transport and monge-ampere   \n331 obstacle problems. Annals of mathematics, pages 673\u2013730, 2010.   \n332 [13] Alessio Figalli and Nicola Gigli. A new transportation distance between non-negative mea  \n333 sures, with applications to gradients flows with dirichlet boundary conditions. Journal de   \n334 math\u00e9matiques pures et appliqu\u00e9es, 94(2):107\u2013130, 2010.   \n335 [14] Anh Duc Nguyen, Tuan Dung Nguyen, Quang Nguyen, Hoang Nguyen, Lam M. Nguyen, and   \n336 Kim-Chuan Toh. On partial optimal transport: Revised sinkhorn and efficient gradient methods.   \n337 In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, 2024.   \n338 [15] Kevin Guittet. Extended Kantorovich norms: a tool for optimization. PhD thesis, INRIA, 2002.   \n339 [16] Florian Heinemann, Marcel Klatt, and Axel Munk. Kantorovich\u2013rubinstein distance and   \n340 barycenter for finitely supported measures: Foundations and algorithms. Applied Mathematics   \n341 & Optimization, 87(1):4, 2023.   \n342 [17] Jan Lellmann, Dirk A Lorenz, Carola Schonlieb, and Tuomo Valkonen. Imaging with   \n343 kantorovich\u2013rubinstein discrepancy. SIAM Journal on Imaging Sciences, 7(4):2833\u20132859,   \n344 2014.   \n345 [18] Lenaic Chizat, Gabriel Peyr\u00e9, Bernhard Schmitzer, and Fran\u00e7ois-Xavier Vialard. An interpolat  \n346 ing distance between optimal transport and Fisher\u2013Rao metrics. Foundations of Computational   \n347 Mathematics, 18(1):1\u201344, 2018.   \n348 [19] Matthias Liero, Alexander Mielke, and Giuseppe Savare. Optimal entropy-transport problems   \n349 and a new Hellinger\u2013Kantorovich distance between positive measures. Inventiones mathemati  \n350 cae, 211(3):969\u20131117, 2018.   \n351 [20] Yogesh Balaji, Rama Chellappa, and Soheil Feizi. Robust optimal transport with applications   \n352 in generative modeling and domain adaptation. Advances in Neural Information Processing   \n353 Systems, 33:12934\u201312944, 2020.   \n354 [21] Quang Minh Nguyen, Hoang H Nguyen, Yi Zhou, and Lam M Nguyen. On unbalanced   \n355 optimal transport: Gradient methods, sparsity and approximation error. The Journal of Machine   \n356 Learning Research, 2023.   \n357 [22] Khang Le, Huy Nguyen, Quang M Nguyen, Tung Pham, Hung Bui, and Nhat Ho. On robust   \n358 optimal transport: Computational complexity and barycenter computation. Advances in Neural   \n359 Information Processing Systems, 34:21947\u201321959, 2021.   \n360 [23] Alexander M Bronstein, Michael M Bronstein, and Ron Kimmel. Generalized multidimensional   \n361 scaling: a framework for isometry-invariant partial surface matching. Proceedings of the   \n362 National Academy of Sciences, 103(5):1168\u20131172, 2006.   \n363 [24] Facundo M\u00e9moli. Spectral gromov-wasserstein distances for shape matching. In 2009 IEEE 12th   \n364 International Conference on Computer Vision Workshops, ICCV Workshops, pages 256\u2013263.   \n365 IEEE, 2009.   \n366 [25] Hongteng Xu, Dixin Luo, and Lawrence Carin. Scalable gromov-wasserstein learning for graph   \n367 partitioning and matching. Advances in neural information processing systems, 32, 2019.   \n368 [26] David A Edwards. The structure of superspace. In Studies in topology, pages 121\u2013133. Elsevier,   \n369 1975.   \n370 [27] Mikhael Gromov. Structures m\u00e9triques pour les vari\u00e9t\u00e9s riemanniennes. Textes Math., 1, 1981.   \n371 [28] Michael Gromov. Groups of polynomial growth and expanding maps (with an appendix by   \n372 jacques tits). Publications Math\u00e9matiques de l\u2019IH\u00c9S, 53:53\u201378, 1981.   \n373 [29] Dmitri Burago, Yuri Burago, Sergei Ivanov, et al. A course in metric geometry, volume 33.   \n374 American Mathematical Society Providence, 2001.   \n375 [30] Karl-Theodor Sturm. The space of spaces: curvature bounds and gradient flows on the space of   \n376 metric measure spaces, volume 290. American Mathematical Society, 2023.   \n377 [31] Marguerite Frank, Philip Wolfe, et al. An algorithm for quadratic programming. Naval research   \n378 logistics quarterly, 3(1-2):95\u2013110, 1956.   \n379 [32] Simon Lacoste-Julien. Convergence rate of frank-wolfe for non-convex objectives. arXiv   \n380 preprint arXiv:1607.00345, 2016.   \n381 [33] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in   \n382 neural information processing systems, 26, 2013.   \n383 [34] Nicolas Papadakis, Gabriel Peyr\u00e9, and Edouard Oudet. Optimal transport with proximal splitting.   \n384 SIAM Journal on Imaging Sciences, 7(1):212\u2013238, 2014.   \n385 [35] Jean-David Benamou, Brittany D Froese, and Adam M Oberman. Numerical solution of the   \n386 optimal transportation problem using the monge\u2013amp\u00e8re equation. Journal of Computational   \n387 Physics, 260:107\u2013126, 2014.   \n388 [36] Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyr\u00e9. Itera  \n389 tive bregman projections for regularized transportation problems. SIAM Journal on Scientific   \n390 Computing, 37(2):A1111\u2013A1138, 2015.   \n391 [37] Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport: With applications to data   \n392 science. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355\u2013607, 2019.   \n393 [38] Lenaic Chizat, Gabriel Peyr\u00e9, Bernhard Schmitzer, and Fran\u00e7ois-Xavier Vialard. Scaling algo  \n394 rithms for unbalanced optimal transport problems. Mathematics of Computation, 87(314):2563\u2013   \n395 2609, 2018.   \n396 [39] Nicolas Bonneel and David Coeurjolly. SPOT: sliced partial optimal transport. ACM Transac  \n397 tions on Graphics, 38(4):1\u201313, 2019.   \n398 [40] Yikun Bai, Bernhard Schmitzer, Matthew Thorpe, and Soheil Kolouri. Sliced optimal partial   \n399 transport. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   \n400 Recognition, pages 13681\u201313690, 2023.   \n401 [41] Gabriel Peyr\u00e9, Marco Cuturi, and Justin Solomon. Gromov-wasserstein averaging of kernel and   \n402 distance matrices. In International conference on machine learning, pages 2664\u20132672. PMLR,   \n403 2016.   \n404 [42] Hongteng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin Duke. Gromov-wasserstein   \n405 learning for graph matching and node embedding. In International conference on machine   \n406 learning, pages 6932\u20136941. PMLR, 2019.   \n407 [43] Vayer Titouan, Nicolas Courty, Romain Tavenard, and R\u00e9mi Flamary. Optimal transport for   \n408 structured data with application on graphs. In International Conference on Machine Learning,   \n409 pages 6275\u20136284. PMLR, 2019.   \n410 [44] Thibault S\u00e9journ\u00e9, Fran\u00e7ois-Xavier Vialard, and Gabriel Peyr\u00e9. The unbalanced gromov   \n411 wasserstein distance: Conic formulation and relaxation. Advances in Neural Information   \n412 Processing Systems, 34:8766\u20138779, 2021.   \n413 [45] Laetitia Chapel, Mokhtar Z Alaya, and Gilles Gasso. Partial optimal tranport with applications   \n414 on positive-unlabeled learning. Advances in Neural Information Processing Systems, 33:2903\u2013   \n415 2913, 2020.   \n416 [46] Nicol\u00f2 De Ponti and Andrea Mondino. Entropy-transport distances between unbalanced metric   \n417 measure spaces. Probability Theory and Related Fields, 184(1-2):159\u2013208, 2022.   \n418 [47] Weijie Liu, Chao Zhang, Jiahao Xie, Zebang Shen, Hui Qian, and Nenggan Zheng. Partial   \n419 gromov-wasserstein learning for partial graph matching. arXiv preprint arXiv:2012.01252,   \n420 2020.   \n421 [48] Alexis Thual, Quang Huy Tran, Tatiana Zemskova, Nicolas Courty, R\u00e9mi Flamary, Stanislas   \n422 Dehaene, and Bertrand Thirion. Aligning individual brains with fused unbalanced gromov   \n423 wasserstein. Advances in Neural Information Processing Systems, 35:21792\u201321804, 2022.   \n424 [49] C\u00e9dric Villani. Topics in optimal transportation, volume 58. American Mathematical Soc.,   \n425 2021.   \n426 [50] Benedetto Piccoli and Francesco Rossi. Generalized wasserstein distance and its application to   \n427 transport equations with source. Archive for Rational Mechanics and Analysis, 211(1):335\u2013358,   \n428 2014.   \n429 [51] Florian Beier, Robert Beinert, and Gabriele Steidl. On a linear gromov\u2013wasserstein distance.   \n430 IEEE Transactions on Image Processing, 31:7292\u20137305, 2022.   \n431 [52] Vayer Titouan, Nicolas Courty, Romain Tavenard, Chapel Laetitia, and R\u00e9mi Flamary. Optimal   \n432 transport for structured data with application on graphs. In Kamalika Chaudhuri and Ruslan   \n433 Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning,   \n434 volume 97 of Proceedings of Machine Learning Research, pages 6275\u20136284, Long Beach,   \n435 California, USA, 09\u201315 Jun 2019. PMLR.   \n436 [53] Xinran Liu, Yikun Bai, Huy Tran, Zhanqi Zhu, Matthew Thorpe, and Soheil Kolouri. Ptlp:   \n437 Partial transport $l^{p}$ distances. In NeurIPS 2023 Workshop Optimal Transport and Machine   \n438 Learning, 2023.   \n439 [54] Filippo Santambrogio. Optimal transport for applied mathematicians. Birk\u00e4user, NY, 55(58-   \n440 63):94, 2015.   \n441 [55] R\u00e9mi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aur\u00e9lie Boisbunon,   \n442 Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, L\u00e9o   \n443 Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko,   \n444 Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander   \n445 Tong, and Titouan Vayer. Pot: Python optimal transport. Journal of Machine Learning Research,   \n446 22(78):1\u20138, 2021.   \n447 [56] Jessa Bekker and Jesse Davis. Learning from positive and unlabeled data: A survey. Machine   \n448 Learning, 109:719\u2013760, 2020.   \n449 [57] Charles Elkan and Keith Noto. Learning classifiers from only positive and unlabeled data. In   \n450 Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and   \n451 data mining, pages 213\u2013220, 2008.   \n452 [58] Masahiro Kato, Takeshi Teshima, and Junya Honda. Learning from positive and unlabeled data   \n453 with a selection bias. In International conference on learning representations, 2018.   \n454 [59] Yu-Guan Hsieh, Gang Niu, and Masashi Sugiyama. Classification from positive, unlabeled   \n455 and biased negative data. In International Conference on Machine Learning, pages 2820\u20132829.   \n456 PMLR, 2019.   \n457 [60] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to   \n458 new domains. In Computer Vision\u2013ECCV 2010: 11th European Conference on Computer Vision,   \n459 Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11, pages 213\u2013226.   \n460 Springer, 2010.   \n461 [61] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor   \n462 Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In   \n463 International conference on machine learning, pages 647\u2013655. PMLR, 2014. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "464 A Notation and Abbreviations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "465 \u2022 OT: Optimal Transport.   \n466 \u2022 POT: Partial Optimal Transport.   \n467 \u2022 GW: Gromov-Wasserstein.   \n468 \u2022 PGW: Partial Gromov-Wasserstein.   \n469 \u2022 FW: Frank-Wolfe.   \n470 \u2022 MPGW: Mass-Constrained Partial Gromov-Wasserstein.   \n471 \u2022 $\\|\\cdot\\|$ : Euclidean norm.   \n472 \u2022 $X^{2}=X\\times X$ .   \n473 \u2022 $\\mathcal{M}_{+}(X)$ : set of all positive (non-negative) Randon (finite) measures defined on $X$ .   \n474 \u2022 ${\\mathcal{P}}_{2}(X)$ : set of all probability measures defined on $X$ , whose second moment is finite.   \n475 \u2022 $\\mathbb{R}_{+}$ : set of all non-negative real numbers.   \n476 \u2022 $\\mathbb{R}^{n\\times m}$ : set of all $n\\times m$ matrices with real coefficients.   \n477 \u2022 $\\mathbb{R}_{+}^{n\\times m}$ (resp. $\\mathbb{R}_{+}^{n})$ ): set of all $n\\times m$ matrices (resp., $n$ -vectors) with non-negative coefficients.   \n478 \u2022 $\\mathbb{R}^{n\\times m\\times n\\times m}$ : set of all $n\\times m\\times n\\times m$ tensors with real coefficients.   \n479 \u2022 $1_{n},1_{n\\times m},1_{n\\times m\\times n\\times m}$ : vector, matrix, and tensor of all ones.   \n480 \u2022 $\\mathbb{1}_{E}$ : characteristic function of a measurable set $E$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{1}_{E}(z)={\\binom{1}{0}}{\\begin{array}{r l}{{\\mathrm{~if~}}z\\in E,}\\\\ {{\\mathrm{~otherwise.~}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "\u2022 X, Y: metric measure spaces (mm-spaces): $\\mathbb{X}=(X,d_{X},\\mu)$ , $\\mathbb{Y}=(Y,d_{Y},\\nu)$ .   \n\u2022 $C^{X}$ : given a discrete mm-space $\\mathbb{X}=(X,d_{X},\\mu)$ , where $X=\\{x_{1},\\ldots,x_{n}\\}$ , the symmetric matrix $C^{X}\\in\\mathbb{R}^{n\\times n}$ is defined as $C_{i,i^{\\prime}}^{X}=d_{X}^{q}(x_{i},x_{i}^{\\prime})$ .   \n\u2022 $\\mu^{\\otimes2}$ : product measure $\\mu\\otimes\\mu$ .   \n\u2022 $T_{\\#}\\sigma$ : $T:X\\rightarrow Y$ is a measurable function and $\\sigma$ is a measure on $X$ . $T_{\\#}\\sigma$ is the pushforward measure of $\\sigma$ , i.e., its is the measure on $Y$ such that for all Borel set $A\\subset Y$ , $T_{\\#}\\sigma(A)=\\sigma(T^{-1}(A))$ .   \n\u2022 $\\gamma,\\gamma_{1},\\gamma_{2}\\colon\\gamma$ is a joint measure defined in a product space having $\\gamma_{1},\\gamma_{2}$ as its first and second marginals, respectively. In the discrete setting, they are viewed as matrices and vectors, i.e., $\\gamma\\in\\mathbb{R}_{+}^{n\\times m}$ , and $\\gamma_{1}=\\gamma1_{m}\\in\\mathbb{R}_{+}^{n}$ , $\\gamma_{2}=\\gamma^{\\top}\\bar{1_{n}}\\in\\dot{\\mathbb{R}}_{+}^{m}$ .   \n\u2022 $\\pi_{1}:X\\times Y\\to X$ , canonical projection mapping, with $(x,y)\\mapsto x$ . Similarly, $\\pi_{2}:X\\times Y\\rightarrow$ $Y$ is canonical projection mapping, with $(x,y)\\mapsto y$ .   \n\u2022 $\\pi_{1,2}:S\\times X\\times Y\\to X\\times Y$ , canonical projection mapping, with $(s,x,y)\\,\\to\\,(x,y)$ . Similarly, $\\pi_{0,1}$ maps $(s,x,y)$ to $(s,x)$ ; $\\pi_{0,2}$ maps $(s,x,y)$ to $(s,y)$ .   \n\u2022 $\\Gamma(\\mu,\\nu)$ , where $\\mu\\in\\mathcal{P}_{2}(X),\\nu\\in\\mathcal{P}_{2}(Y)$ (where $X,Y$ may not necessarily be the same set): it is the set of all the couplings (transportation plans) between $\\mu$ and $\\nu$ , i.e., $\\Gamma(\\mu,\\nu):=\\left\\{\\gamma\\in\\right.$ ${\\mathcal{P}}_{2}(X\\times Y):\\,\\gamma_{1}=\\mu,\\gamma_{2}=\\nu\\}$ .   \n\u2022 $\\Gamma({\\mathrm{p}},{\\mathrm{q}})$ : set of all the couplings between the discrete probability measures $\\textstyle\\mu=\\sum_{i=1}^{n}p_{i}^{X}\\delta_{x_{i}}$ and $\\begin{array}{r}{\\nu=\\sum_{j=1}^{m}q_{j}^{Y}\\delta_{y_{j}}}\\end{array}$ with weight vectors ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{p}=[p_{1}^{X},\\dots,p_{n}^{X}]^{\\top}\\qquad\\mathrm{and}\\qquad\\mathrm{q}=[q_{1}^{Y},\\dots,q_{m}^{Y}]^{\\top}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "That is, $\\Gamma({\\mathrm{p}},{\\mathrm{q}})$ coincides with $\\Gamma(\\mu,\\nu)$ , but it is viewed as a subset of $n\\times m$ matrices defined in (12). \u2022 $p,q$ : real numbers $1\\leq p,q<\\infty$ . \u2022 p, q: vectors of weights as in (20). \u2022 $\\mathrm{p}=[p_{1},\\ldots,p_{n}]\\leq\\mathrm{p}^{\\prime}=[p_{1}^{\\prime},\\ldots,p_{n}^{\\prime}]$ if $p_{j}\\le p_{j}^{\\prime}$ for all $1\\le j\\le n$ . \u2022 $\\textstyle|\\mathrm{p}|=\\sum_{i=1}^{n}p_{i}$ for $\\mathrm{p}=[p_{1},...\\,,p_{n}]$ . ", "page_idx": 13}, {"type": "text", "text": "\u2022 $c(x,y):X\\times Y\\to\\mathbb{R}_{+}$ denotes the cost function used for classical and partial optimal transport problems. lower-semi continuous function.   \n\u2022 $O T(\\mu,\\nu)$ : it is the classical optimal transport (OT) problem between the probability measures $\\mu$ and $\\nu$ defined in (1).   \n\u2022 $W_{p}(\\mu,\\nu)$ : it is the $p$ -Wasserstein distance between the probability measures $\\mu$ and $\\nu$ defined in (2), for $1\\leq p<\\infty$ .   \n\u2022 $P O T(\\mu,\\nu;\\lambda)$ : the Partial Optimal Transport (OPT) problem defined in (3).   \n\u2022 $|\\mu|$ : total variation norm of the positive Randon (finite) measure $\\mu$ defined on a measurable space $X$ , i.e., $|\\mu|=\\mu(X)$ .   \n\u2022 $\\mu\\le\\sigma$ : denotes that for all Borel set $B\\subseteq X$ we have that the measures $\\mu,\\sigma\\in\\mathcal{M}_{+}(X)$ satisfy $\\mu(B)\\leq\\sigma(B)$ .   \n\u2022 $\\Gamma_{\\le}(\\mu,\\nu)$ , where $\\mu\\in\\mathcal{M}_{+}(X),\\nu\\in\\mathcal{M}_{+}(Y)$ : set of all \u201cpartial transportation plans\u201d ", "page_idx": 14}, {"type": "text", "text": "517   \n518   \n519   \n520   \n521   \n522   \n523   \n524   \n525   \n526   \n527   \n528   \n529   \n530   \n531   \n532   \n533   \n534   \n535   \n536   \n537   \n538   \n539   \n540   \n541   \n542   \n543 ", "page_idx": 14}, {"type": "text", "text": "$\\Gamma_{\\leq}(\\mu,\\nu):=\\{\\gamma\\in\\mathcal{M}_{+}(X\\times Y):\\gamma_{1}\\leq\\mu,\\gamma_{2}\\leq\\nu\\}.$ \u2022 $\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{q})$ : set of all the \u201cpartial transportation plans\u201d between the discrete probability measures $\\textstyle\\mu=\\sum_{i=1}^{n}p_{i}^{X}\\delta_{x_{i}}$ and $\\begin{array}{r}{\\nu=\\sum_{j=1}^{m}q_{j}^{Y}\\delta_{y_{j}}}\\end{array}$ with weight vectors $\\mathbf{p}=[p_{1}^{X},\\cdot\\cdot\\cdot,p_{n}^{X}]$ and $\\mathbf{q}\\,=\\,[q_{1}^{Y},\\ldots,q_{m}^{Y}]$ . That is, $\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{q})$ coincides with $\\Gamma_{\\le}(\\mu,\\nu)$ , but it is viewed as a subset of $n\\times m$ matrices defined in (13). \u2022 $\\lambda>0$ : positive real number. \u2022 $\\hat{\\infty}$ : auxiliary point. \u2022 ${\\hat{X}}=X\\cup\\{{\\hat{\\infty}}\\}$ . \u2022 $\\hat{\\mu},\\hat{\\nu}$ : given in (4). $\\bullet\\,{\\hat{\\mathrm{\\boldmath~\\p}}},{\\hat{\\mathrm{q}}}$ : given in (53). \u2022 $\\hat{\\gamma}$ : given in (6). \u2022 $\\hat{c}(\\cdot,\\cdot):\\hat{X}\\times\\hat{Y}\\rightarrow\\mathbb{R}_{+}$ : cost as in (5). \u2022 $L:\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}$ : cost function for the GW problems. \u2022 $D:\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}$ : generic distance on $\\mathbb{R}$ used for GW problems. \u2022 $G W^{L}(\\cdot,\\cdot)$ : GW optimization problem given in (7). \u2022 $G W^{p}(\\cdot,\\cdot)$ : GW optimization problem given in (7) when $L(a,b)=|a-b|^{p}$ . \u2022 $G W_{q}^{L}(\\cdot,\\cdot)$ : general GW optimization problem for $g\\geq1$ given in (33). \u2022 $G W_{q}^{p}(\\cdot,\\cdot)$ : general GW optimization problem for $q\\geq1$ and $L(a,b)=|a-b|^{p}$ given in (34). \u2022 $G W_{\\lambda,q}^{p}(\\cdot,\\cdot)$ : generalized GW problem given in (39). \u2022 $\\widehat{G W}$ : GW-variant problem given in (51) for the general case, and in (55) for the discrete setting. \u2022 $\\hat{L}$ : cost given in (16) for the GW-variant problem. \u2022 $d:\\hat{X}\\times\\hat{X}\\to\\mathbb{R}_{+}\\cup\\{\\infty\\}$ : \u201cgeneralized\u201d metric given in (50) for $\\hat{X}$ . \u2022 $\\mathbb{X}\\sim\\mathbb{Y}$ : equivalence relation in for mm-spaces, $\\mathbb{X}\\sim\\mathbb{Y}$ if and only if they have the same total mass and $G W_{q}^{p}(\\mathbb{X},\\mathbb{Y})=0$ . \u2022 $P G W_{\\lambda,q}^{L}(\\cdot,\\cdot)$ : partial GW optimization problem given in (9) or, equivalently, in (10). \u2022 $P G W_{\\lambda,q}^{p}(\\cdot,\\cdot)$ : partial GW optimization problem given in (10) when $L(a,b)=|a-b|^{p}$ . 44 \u2022 $P G W_{\\lambda}(\\cdot,\\cdot)$ : is is the PGW problem $P G W_{\\lambda,q}^{p}(\\cdot,\\cdot)$ for the case when $p=2=q$ . \u2022 $\\mu(\\phi)$ : given a measure $\\mu$ and a function $\\phi$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mu(\\phi):=\\int\\phi(x)d\\mu(x).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "\u2022 $C(\\gamma;\\lambda,\\mu,\\nu)$ : the transportation cost induced by transportation plan $\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu)$ in the Partial GW problem 10, ", "page_idx": 15}, {"type": "equation", "text": "$$\nC(\\gamma;\\lambda,\\mu,\\nu):=\\gamma^{\\otimes2}(L(d_{X}^{q},d_{Y}^{q}))+\\lambda(|\\mu|^{2}+|\\nu|^{2}-2|\\gamma|^{2}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "545 \u2022 $\\mathcal{L}$ : functional for the optimization problem $P G W_{\\lambda}(\\cdot,\\cdot)$ .   \n546 \u2022 $M,{\\tilde{M}}$ , and $\\hat{M}$ : see (14), and (54). Notice that, $(M-2\\lambda)_{i,i^{\\prime},j,j^{\\prime}}:=M_{i,i^{\\prime},j,j^{\\prime}}-2\\lambda$ .   \n547 \u2022 $\\langle\\cdot,\\cdot\\rangle_{F}$ : Frobenius inner product for matrices, i.e., $\\langle A,B\\rangle_{F}\\;\\;=\\;\\;\\mathrm{trace}(A^{\\top}B)\\;\\;\\;=\\;\\;$   \n548 in,,jmAi,jBi,j for all A, B \u2208Rn\u00d7m.   \n549 \u2022 $M\\circ\\gamma$ : product between the tensor $M$ and the matrix $\\gamma$ .   \n550 \u2022 $\\nabla$ : gradient.   \n551 \u2022 $[1:n]=\\{1,\\ldots,n\\}.$ .   \n552 \u2022 $\\alpha$ : step size based on the line search method.   \n553 \u2022 $\\gamma^{(1)}$ : initialization of the algorithm.   \n554 \u2022 $\\gamma^{(k)}$ , $\\gamma^{(k)^{\\prime}}$ : previous and new transportation plans before and after step 1 in the $k$ \u2212th   \n555 iteration of version 1 of our proposed FW algorithm.   \n556 \u2022 ${\\hat{\\gamma}}^{(k)},\\,\\,{\\hat{\\gamma}}^{(k)^{\\prime}}$ : previous and new transportation plans before and after step 1 in the $k$ \u2212th   \n557 iteration of version 2 of our proposed FW algorithm.   \n558 \u2022 $G=2\\tilde{M}\\circ\\gamma$ , $\\hat{G}=2\\hat{M}\\circ\\hat{\\gamma}$ : Gradient of the objective function in version 1 and version 2,   \n559 respectively, of our proposed FW algorithm for solving the discrete version of partial GW   \n560 problem.   \n561 \u2022 $(\\delta\\gamma,a,b)$ and $(\\delta\\hat{\\gamma},a,b)$ : given in (18) and (56) for versions 1 and 2 of the algorithm,   \n562 respectively.   \n563 \u2022 $C^{1}$ -function: continuous and with continuous derivatives.   \n564 \u2022 $M P G W_{\\rho}(\\cdot,\\cdot)$ : Mass-Constrained Partial Gromov-Wasserstein defined in (73)   \n565 \u2022 $\\Gamma_{\\le}^{\\rho}(\\mu,\\nu)$ : set transport plans defined in (74) for the Mass-Constrained Partial Gromov  \n566 Wasserstein problem.   \n567 \u2022 $\\Gamma_{P U,\\pi}(\\mathrm{p},\\mathrm{q})$ : defined in (87). ", "page_idx": 15}, {"type": "text", "text": "568 B Proof of Proposition 3.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "569 The idea of the proof is inspired by the proof of Proposition 1 in [50]. ", "page_idx": 15}, {"type": "text", "text": "570 The goal is to verify that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P G W_{\\lambda,q}^{L}(\\mathbb{X},\\mathbb{Y})}\\\\ &{\\quad:=\\underset{\\gamma\\in{\\cal M}_{+}(X,Y)}{\\operatorname*{inf}}\\underbrace{\\int_{(X\\times Y)^{2}}L(d_{X}^{q}(x,x^{\\prime}),d_{Y}^{q}(y,y^{\\prime}))d\\gamma^{\\otimes2}}_{\\mathrm{transport}\\:\\mathrm{GW}\\:\\mathrm{cost}}+\\underset{\\mathrm{mass}\\:\\mathrm{penalty}}{\\underbrace{\\sum_{1}^{\\otimes2}-\\gamma_{1}^{\\otimes2}}}|+|\\nu^{\\otimes2}-\\gamma_{2}^{\\otimes2}|)}\\\\ &{\\quad=\\underset{\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu)}{\\operatorname*{inf}}\\int_{(X\\times Y)^{2}}L(d_{X}^{q}(x,x^{\\prime}),d_{Y}^{q}(y,y^{\\prime}))d\\gamma^{\\otimes2}+\\lambda\\left(|\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2}|+|\\nu^{\\otimes2}-\\gamma_{2}^{\\otimes2}|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consider $\\gamma\\,\\in\\,{\\mathcal{M}}_{+}(X\\times Y)$ such that $\\gamma_{1}\\leq\\mu$ does not hold. Then we can write the Lebesgue decomposition of $\\gamma_{1}$ with respect to $\\mu$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\gamma_{1}=f\\mu+\\mu^{\\perp},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $f\\geq0$ is the Radon-Nikodym derivative of $\\gamma_{1}$ with respect to $\\mu$ , and $\\mu^{\\perp}$ , $\\mu$ are mutually singular, that is, there exist measurable sets $A,B$ such that $A\\cap B=\\emptyset$ , $X=A\\cup B$ and $\\dot{\\mu}^{\\perp}(A)=0,\\dot{\\mu}(B)\\dot{=}\\,0$ . Without loss of generality, we can assume that the support of $f$ lies on $A$ , since ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\gamma_{1}(E)=\\int_{E\\cap A}f(x)\\,d\\mu(x)+\\mu^{\\perp}(E\\cap B)\\qquad\\forall E\\subseteq X{\\mathrm{~measurable}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Define $A_{1}=\\{x\\in A:\\,f(x)>1\\},A_{2}=\\{x\\in A:\\,f(x)\\leq1\\}$ (both are measurable, since $f$ is measurable), and define $\\bar{\\mu}=\\operatorname*{min}\\{f,1\\}\\mu$ . Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\bar{\\mu}\\leq\\mu\\qquad\\mathrm{and}\\qquad\\bar{\\mu}\\leq f\\mu\\leq f\\mu+\\mu^{\\perp}=\\gamma_{1}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "There exists a $\\bar{\\gamma}\\in\\mathcal{M}_{+}(X\\times Y)$ such that $\\bar{\\gamma}_{1}=\\bar{\\mu},\\bar{\\gamma}\\leq\\gamma$ , and $\\bar{\\gamma}_{2}\\leq\\gamma_{2}$ . Indeed, we can construct $\\bar{\\gamma}$ in the following way: First, let $\\{\\gamma^{x}\\}_{x\\in X}$ be the set of conditional measures (disintegration) such that for every measurable (test) function $\\psi:X\\times Y\\rightarrow\\mathbb{R}$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int\\psi(x,y)\\,d\\gamma(x,y)=\\int_{X}\\int_{Y}\\psi(x,y)\\,d\\gamma^{x}(y)\\,d\\gamma_{1}(x).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, define $\\bar{\\gamma}$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\bar{\\gamma}}(U):=\\int_{X}\\int_{Y}\\mathbb{1}_{U}(x,y)\\,d\\gamma^{x}(y)\\,d{\\bar{\\mu}}(x)\\qquad\\forall U\\subseteq X\\times Y\\,\\,{\\mathrm{Borel.}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "571 Then, $\\bar{\\gamma}$ verifies that $\\bar{\\gamma}_{1}=\\bar{\\mu}$ , and since $\\bar{\\mu}\\leq\\gamma_{1}$ , we also have that $\\bar{\\gamma}\\le\\gamma$ , which implies $\\bar{\\gamma}_{2}\\leq\\gamma_{2}$ .   \n572 Since $|\\gamma_{1}|=|\\gamma_{2}|$ and $|\\bar{\\gamma}_{1}|=|\\bar{\\gamma}_{2}|$ , then we have $|\\gamma_{1}^{\\otimes2}-\\bar{\\gamma}_{1}^{\\otimes2}|=|\\gamma_{2}^{\\otimes2}-\\bar{\\gamma}_{2}^{\\otimes2}|$ . ", "page_idx": 16}, {"type": "text", "text": "573 We claim that ", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2}|\\geq|\\mu^{\\otimes2}-\\bar{\\gamma}_{1}^{\\otimes2}|+|\\gamma_{1}^{\\otimes2}-\\bar{\\gamma}_{1}^{\\otimes2}|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "574   \n575 ", "page_idx": 16}, {"type": "text", "text": "\u2022 Left-hand side of (22): Since $\\{A,B\\}$ is a partition of $X$ , we first spit the left-hand side of (22) as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big|\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2}\\big|=\\underbrace{\\big(\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2}\\big)(A\\times A)}_{(I)}+\\underbrace{\\big(\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2}\\big)(A\\times B)+\\big(\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2}\\big)(B\\times A)}_{(I I)}}\\\\ &{\\qquad\\qquad\\qquad+\\underbrace{\\big(\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2}\\big)(B\\times B)}_{(I I I)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "576 ", "page_idx": 16}, {"type": "text", "text": "Then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(I I I)=(\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2})(B\\times B)=\\mu^{\\perp}\\otimes\\mu^{\\perp}(B\\times B)=|\\mu^{\\perp}|^{2},}\\\\ &{(I I)=(\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2})(A\\times B)+(\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2})(B\\times A)=2|\\mu^{\\perp}|(\\mu-\\gamma_{1})(A).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "577 ", "page_idx": 16}, {"type": "text", "text": "Since $\\gamma_{1}=f\\mu$ in $A$ , then $\\bar{\\gamma}_{1}=\\gamma_{1}$ in $A_{2}$ and $\\bar{\\gamma}_{1}=\\mu$ in $A_{1}$ , so we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\mu-\\gamma_{1})(A)=(\\mu-\\gamma_{1})(A_{1})+(\\mu-\\gamma_{1})(A_{2})=(\\gamma_{1}-\\bar{\\gamma}_{1})(A_{1})+(\\mu-\\bar{\\gamma}_{1})(A_{2})}\\\\ {=(\\gamma_{1}-\\bar{\\gamma}_{1})(A)+(\\mu-\\bar{\\gamma}_{1})(A).\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "578 ", "page_idx": 16}, {"type": "text", "text": "Thus, ", "page_idx": 16}, {"type": "equation", "text": "$$\n(I I)=2|\\mu^{\\perp}|((\\gamma_{1}-\\bar{\\gamma}_{1})(A)+(\\mu-\\bar{\\gamma}_{1})(A)),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "579 ", "page_idx": 16}, {"type": "text", "text": "and we also get that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(I)=(\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2})(A\\times A)}\\\\ &{\\quad=(\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2})(A_{1}\\times A_{1})+(\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2})(A_{2}\\times A_{2})+(\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2})(A_{1}\\times A_{2})}\\\\ &{\\quad\\quad\\quad+(\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2})(A_{2}\\times A_{1})}\\\\ &{\\quad=(\\gamma_{1}^{\\otimes2}-\\tilde{\\gamma}_{1}^{\\otimes2})(A_{1}\\times A_{1})+(\\mu^{\\otimes2}-\\tilde{\\gamma}_{1}^{\\otimes2})(A_{2}\\times A_{2})+}\\\\ &{\\quad\\quad\\quad+|\\tilde{\\gamma}_{1}\\otimes\\mu-\\gamma_{1}\\otimes\\tilde{\\gamma}_{1}|(A_{1}\\times A_{2})+|\\mu\\otimes\\tilde{\\gamma}_{1}-\\tilde{\\gamma}_{1}\\otimes\\gamma_{1}|(A_{2}\\times A_{1})}\\\\ &{\\quad\\quad=(\\gamma_{1}^{\\otimes2}-\\tilde{\\gamma}_{1}^{\\otimes2})(A_{1}\\times A_{1})+(\\mu^{\\otimes2}-\\tilde{\\gamma}_{1}^{\\otimes2})(A_{2}\\times A_{2})+2(\\tilde{\\gamma}_{1}-\\gamma_{1})(A_{1})(\\mu-\\tilde{\\gamma}_{1})(A_{2})}\\\\ &{\\quad=(\\gamma_{1}^{\\otimes2}-\\tilde{\\gamma}_{1}^{\\otimes2})(A\\times A)+(\\mu^{\\otimes2}-\\tilde{\\gamma}_{1}^{\\otimes2})(A\\times A)+\\underbrace{2(\\tilde{\\gamma}_{1}-\\gamma_{1})(A_{1})(\\mu-\\tilde{\\gamma}_{1})(A_{2})}_{\\ge0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "\u2022 Right-hand side of (22): First notice that ", "page_idx": 16}, {"type": "equation", "text": "$$\n(\\gamma_{1}-\\bar{\\gamma}_{1})(B)=(\\gamma_{1}-\\bar{\\gamma}_{1})(B)\\leq\\gamma_{1}(B)=|\\mu^{\\perp}|,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n(\\mu-\\bar{\\gamma}_{1})(B)=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "582 Then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mu^{\\otimes2}-\\bar{\\gamma}_{1}^{\\otimes2}|+|\\gamma_{1}^{\\otimes2}-\\bar{\\gamma}_{1}^{\\otimes2}|=}\\\\ &{=(\\mu^{\\otimes2}-\\bar{\\gamma}_{1}^{\\otimes2})(A\\times A)+(\\gamma_{1}^{\\otimes2}-\\bar{\\gamma}_{1}^{\\otimes2})(A\\times A)+(\\mu^{\\otimes2}-\\bar{\\gamma}_{1}^{\\otimes2})(B\\times B)}\\\\ &{\\quad+\\,(\\gamma_{1}^{\\otimes2}-\\bar{\\gamma}_{1}^{\\otimes2})(B\\times B)+(\\mu^{\\otimes2}-\\bar{\\gamma}_{1}^{\\otimes2})(A\\times B)+(\\gamma_{1}^{\\otimes2}-\\bar{\\gamma}_{1}^{\\otimes2})(A\\times B)}\\\\ &{\\quad+\\,(\\mu^{\\otimes2}-\\bar{\\gamma}_{1}^{\\otimes2})(B\\times A)+(\\gamma_{1}^{\\otimes2}-\\bar{\\gamma}_{1}^{\\otimes2})(B\\times A)}\\\\ &{\\leq\\underbrace{(\\mu^{\\otimes2}-\\bar{\\gamma}_{1}^{\\otimes2})(A\\times A)+(\\gamma_{1}^{\\otimes2}-\\bar{\\gamma}_{1}^{\\otimes2})(A\\times A)}_{\\leq(I)}+\\underbrace{|\\mu^{\\bot}|^{2}}_{=(I I I)}+\\underbrace{2|\\mu^{\\bot}|(\\gamma_{1}-\\bar{\\gamma}_{1})(A)}_{=(I I I)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "583 Thus, (22) holds. ", "page_idx": 17}, {"type": "text", "text": "584 We finish the proof of the proposition by noting that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mu^{\\otimes2}-\\bar{\\gamma}_{1}^{\\otimes2}|+|\\nu^{\\otimes2}-\\bar{\\gamma}_{2}^{\\otimes2}|\\leq|\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2}|-|\\gamma_{1}^{\\otimes2}-\\bar{\\gamma}_{1}^{\\otimes2}|+|\\nu^{\\otimes2}-\\bar{\\gamma}_{2}^{\\otimes2}|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=|\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2}|-|\\gamma_{2}^{\\otimes2}-\\bar{\\gamma}_{2}^{\\otimes2}|+|\\nu^{\\otimes2}-\\bar{\\gamma}_{2}^{\\otimes2}|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq|\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2}|+|\\nu^{\\otimes2}-\\gamma_{2}^{\\otimes2}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "585 where the first inequality follows from (22), and the second inequality holds from the fact the total   \n586 variation norm $|\\cdot|$ satisfies triangular inequality. Therefore $\\bar{\\gamma}$ induces a smaller transport GW cost   \n587 than $\\gamma$ (since $\\bar{\\gamma}\\leq\\gamma$ ), and also $\\bar{\\gamma}$ decreases the mass penalty in comparison that corresponding to   \n588 $\\gamma$ . Thus, $\\bar{\\gamma}$ is a better GW transportation plan, which satisfies $\\bar{\\gamma}_{1}\\leq\\mu$ . Similarly, we can further   \n589 construct $\\bar{\\gamma}^{\\prime}$ based on $\\bar{\\gamma}$ such that ${\\bar{\\gamma}}_{1}^{\\prime}\\leq\\mu,{\\bar{\\gamma}}_{2}^{\\prime}\\leq\\nu$ . Therefore, we can restrict the minimization in (9)   \n590 from $\\mathcal{M}_{+}(X\\times Y)$ to $\\Gamma_{\\leq}(\\mu,\\nu)$ . Thus, the equality (21) is satisfied.   \n591 Proof of Remark 3.1. Given $\\gamma\\,\\in\\,\\Gamma_{\\le}(\\mu,\\nu)$ , since $\\gamma_{1}\\,\\leq\\,\\mu,\\,\\gamma_{2}\\,\\leq\\,\\nu$ , and $\\gamma_{1}(X)\\,=\\,|\\gamma_{1}|\\,=\\,|\\gamma|\\,=$   \n592 $|\\gamma_{2}|=\\gamma_{2}(Y)$ , we have ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2}|+|\\nu^{\\otimes2}-\\gamma_{2}^{\\otimes2}|=\\mu^{\\otimes2}(X^{2})-\\gamma_{1}^{\\otimes2}(X^{2})+\\nu^{\\otimes2}(Y^{2})-\\gamma_{2}^{\\otimes2}(Y^{2})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=|\\mu|^{2}+|\\nu|^{2}-2|\\gamma|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "593 and so the transportation cost in partial GW problem (10) becomes ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C(\\gamma;\\lambda,\\mu,\\nu)}\\\\ &{\\quad:=\\displaystyle\\int_{(X\\times Y)^{2}}L(d_{X}^{q}(x,x^{\\prime}),d_{Y}^{q}(y,y^{\\prime}))\\,d\\gamma(x,y)d\\gamma(x^{\\prime},y^{\\prime})+\\lambda\\left(|\\mu^{\\otimes2}-\\gamma_{1}^{\\otimes2}|+|\\nu^{\\otimes2}-\\gamma_{2}^{\\otimes2}|\\right)}\\\\ &{\\quad=\\displaystyle\\int_{(X\\times Y)^{2}}L(d_{X}^{q}(x,x^{\\prime}),d_{Y}^{q}(y,y^{\\prime}))\\,d\\gamma(x,y)d\\gamma(x^{\\prime},y^{\\prime})+\\lambda\\left(|\\mu|^{2}+|\\nu|^{2}-2|\\gamma|^{2}\\right)}\\\\ &{\\quad=\\displaystyle\\int_{(X\\times Y)^{2}}\\left(L(d_{X}^{q}(x,x^{\\prime}),d_{Y}^{q}(y,y^{\\prime})-2\\lambda)\\,\\,d\\gamma(x,y)d\\gamma(x^{\\prime},y^{\\prime})+\\underbrace{\\lambda\\left(|\\mu|^{2}+|\\nu|^{2}\\right)}_{:=:\\displaystyle\\sum_{i=1}^{\\infty}:\\quad\\quad\\mathcal{O}(\\mathbb{R}^{|\\mathcal{I}|})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "594 ", "page_idx": 17}, {"type": "text", "text": "595 C Proof of Proposition 3.3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "596 In this section, we discuss the minimizer of the Partial GW problem (9). Trivially, $\\Gamma_{\\leq}(\\mu,\\nu)\\ \\subseteq$   \n597 $\\mathcal{M}_{+}(X\\times Y)$ and by using Proposition 3.2 it is enough to show that a minimizer for problem (10)   \n598 exists. ", "page_idx": 17}, {"type": "text", "text": "599 We refer the reader to [8, Chapters 5 and 10] for similar ideas. ", "page_idx": 17}, {"type": "text", "text": "600 C.1 Formal Statement of Proposition 3.3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Suppose $X,Y$ are compact sets, then exists compact set $[0,\\beta]\\subset\\mathbb{R},$ , such that ", "page_idx": 18}, {"type": "equation", "text": "$$\nd(x,x^{\\prime}),\\,d(y,y^{\\prime})\\in[0,\\beta],\\qquad\\forall x,x^{\\prime}\\in X,y,\\,y^{\\prime}\\in Y\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "601 Let $A=[0,\\beta^{q}]$ . Let $L_{A^{2}}$ denote the restriction of $L$ on $A^{2}$ , i.e. $L_{A^{2}}:A^{2}\\to\\mathbb{R}$ with $L_{A^{2}}(r_{1},r_{2})=$   \n602 $L(r_{1},r_{2}),\\,\\forall r_{1},r_{2}\\in A$ . Suppose $L$ satisfies the following: there exists $0<K<\\infty$ such that for   \n603 every $r_{1},r_{1}^{\\prime},r_{2},r_{2}^{\\prime}\\in A$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|L_{A^{2}}(r_{1},r_{2})-L_{A^{2}}(r_{1}^{\\prime},r_{2})|\\leq K|r_{1}-r_{1}^{\\prime}|,\\ \\ |L_{A^{2}}(r_{1},r_{2})-L_{A^{2}}(r_{1},r_{2}^{\\prime})|\\leq K|r_{2}-r_{2}^{\\prime}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "604 (i.e., $L_{A^{2}}$ is Lipschitz on each variable). Then $P G W_{\\lambda}^{L}(\\cdot,\\cdot)$ admits a minimizer. ", "page_idx": 18}, {"type": "text", "text": "605 Note, the condition (24) contains the case $L(r_{1},r_{2})=|r_{1}-r_{2}|^{p}$ as a special case: ", "page_idx": 18}, {"type": "text", "text": "606 Lemma C.1. If $L(r_{1},r_{2})=|r_{1}-r_{2}|^{p},$ , for $1\\leq p<\\infty$ , then $L$ satisfies the condition (24). ", "page_idx": 18}, {"type": "text", "text": "607 Proof. Assume that $L$ is defined on an interval of the form $[0,M]$ , for some $M\\,>\\,0$ . Consider   \n608 $r_{1},r_{1}^{\\prime},r_{2},r_{2}^{\\prime}\\in[0,M]$ . If $p=1$ , by triangle inequality we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n|L(r_{1},r_{2})-L(r_{1}^{\\prime},r_{2})|=||r_{1}-r_{2}|-|r_{1}^{\\prime}-r_{2}||\\leq|r_{1}-r_{1}^{\\prime}|\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "609 and similarly, ", "page_idx": 18}, {"type": "equation", "text": "$$\n|L(r_{1},r_{2})-L(r_{1},r_{2}^{\\prime})|\\leq|r_{2}-r_{2}^{\\prime}|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "610 From [8, page 473], since for $1\\leq p<\\infty$ , the function $t\\mapsto t^{p}$ , for $t\\in[0,M]$ , is Lipschitz with   \n611 constant bounded by $p M^{p-1}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n|L(r_{1},r_{2})-L(r_{1}^{\\prime},r_{2})|\\leq p M^{p-1}|r_{1}-r_{1}^{\\prime}|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "612 and similarly, ", "page_idx": 18}, {"type": "equation", "text": "$$\n|L(r_{1},r_{2})-L(r_{1},r_{2}^{\\prime})|\\leq p M^{p-1}|r_{2}-r_{2}^{\\prime}|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "613 ", "page_idx": 18}, {"type": "text", "text": "614 Lemma C.2. Given $q\\geq1$ , consider $\\beta>0$ . Then $[0,\\beta]\\ni c\\mapsto c^{q}\\in[0,\\beta^{q}]$ is a Lipschitz function. ", "page_idx": 18}, {"type": "text", "text": "615 Proof. Given $c_{1},c_{2}\\in[0,\\beta]$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n|c_{1}^{q}-c_{2}^{q}|\\leq q\\beta^{q-1}|c_{1}-c_{2}|\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "616 Thus, $c\\mapsto c^{q}$ is a Lipschitz function. ", "page_idx": 18}, {"type": "text", "text": "617 C.2 Convergence Auxiliary Result ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "618 If a sequence $\\{\\gamma^{n}\\}$ converges weakly to $\\gamma$ , we write $\\gamma^{n}\\xrightarrow{w}\\gamma$ . In this setting, if $\\gamma^{n}\\stackrel{w}{\\to}\\gamma$ , it does not   \n619 imply that $(\\gamma^{n})^{\\otimes2}\\ {\\overset{w}{\\rightharpoondown}}\\ \\gamma^{\\otimes2}$ . Thus, the technique used in classical OT for proving the existence of a   \n620 minimizer for the optimal transport optimization problem as a consequence of the Stone-Weierstrass   \n621 theorem does not apply directly in the Gromov-Wasserstein context. ", "page_idx": 18}, {"type": "text", "text": "622 Inspired by [8], we introduce the following lemma. ", "page_idx": 18}, {"type": "text", "text": "Lemma C.3. Given metric space $(Z,d_{Z})$ , suppose $\\phi:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ is a Lipschitz continuous function with respect to $(Z^{2},d_{Z}^{+})$ , where ", "page_idx": 18}, {"type": "equation", "text": "$$\nd_{Z}^{+}((z_{1},z_{2}),(z_{1}^{\\prime},z_{2}^{\\prime})):=d_{Z}(z_{1},z_{1}^{\\prime})+d_{Z}(z_{2},z_{2}^{\\prime}),\\qquad\\forall(z_{1},z_{2}),(z_{1}^{\\prime},z_{2}^{\\prime})\\in Z^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Given $\\gamma\\in\\mathcal{M}_{+}(Z)$ , and a sequence $\\{\\gamma^{n}\\}_{n\\geq1}\\in\\mathcal{M}_{+}(Z)$ such that converges weakly to $\\gamma,$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\gamma^{n}\\stackrel{w}{\\rightarrow}\\gamma\\qquad(n\\rightarrow\\infty).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, consider the mapping ", "page_idx": 18}, {"type": "equation", "text": "$$\nZ\\ni z\\mapsto\\gamma(\\phi(z,\\cdot)):=\\int_{Z}\\phi(z,z^{\\prime})d\\gamma(z^{\\prime})\\in\\mathbb{R}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "623 Then we have the following results: ", "page_idx": 18}, {"type": "text", "text": "(1) $\\gamma^{n}(\\phi(z,\\cdot))\\rightarrow\\gamma(\\phi(z,\\cdot))$ uniformly (when $n\\rightarrow\\infty,$ ). ", "page_idx": 19}, {"type": "text", "text": "(3) If ${\\mathcal{M}}\\subset{\\mathcal{M}}_{+}(Z)$ is compact for the weak convergence, then $\\operatorname*{inf}_{\\gamma\\in{\\mathcal{M}}}\\gamma^{\\otimes2}(\\phi(\\cdot,\\cdot))$ admits a minimizer. ", "page_idx": 19}, {"type": "text", "text": "628 Proof. The main idea of the proof is similar to [8, Lemma 10.3]: we extend it from $\\mathcal{P}_{+}(Z)$ to   \n629 $\\mathcal{M}_{+}(Z)$ . ", "page_idx": 19}, {"type": "text", "text": "(1) Since $\\gamma^{n}\\xrightarrow{w}\\gamma$ , and $Z$ is compact, we have $|\\gamma^{n}|\\rightarrow|\\gamma|$ . Then, given $\\epsilon>0$ , for $n$ sufficiently large we have $|\\gamma^{n}|\\leq|\\gamma|+\\epsilon$ . ", "page_idx": 19}, {"type": "text", "text": "Let us denote by $\\|\\phi\\|_{L i p}$ the Lipschitz constant of $\\phi$ . For any $z_{1},z_{2}\\in Z$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\vert\\gamma^{n}(\\phi(z_{1},\\cdot))-\\gamma^{n}(\\phi(z_{2},\\cdot))\\vert\\le\\displaystyle\\int_{Z}\\vert\\phi(z_{1},z)-\\phi(z_{2},z)\\vert\\gamma^{n}(z)}&{}\\\\ {\\le\\displaystyle\\operatorname*{max}_{z\\in Z}\\vert\\phi(z_{1},z)-\\phi(z_{2},z)\\vert(\\vert\\gamma\\vert+\\epsilon)}&{}\\\\ {\\le(\\vert\\gamma\\vert+\\epsilon)\\Vert\\phi\\Vert_{L i p}\\,d_{Z}(z_{1},z_{2})=K d_{Z}(z_{1},z_{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $K=(|\\gamma|+\\epsilon)\\|\\phi\\|_{L i p}$ is a finite positive value. Note that the above inequality also holds if we replace $\\gamma^{n}$ by $\\gamma$ . ", "page_idx": 19}, {"type": "text", "text": "Since $(Z,d_{Z})$ is compact, $\\smash{Z\\,=\\,\\bigcup_{i=1}^{N}B(z_{i},\\epsilon/K)}$ for some $z_{1},\\dots,z_{N}\\ \\in\\ Z$ , where $B(z_{i},\\epsilon/3K)\\;=\\;\\{z\\;\\in\\;Z\\;:\\;d_{Z}(z,\\bar{z_{i}}\\}\\;\\stackrel{\\cdot}{\\leq}\\;\\epsilon/3K\\}$ is the closed ball centered at $z_{i}$ , with radius $\\epsilon/K$ . By definition of weak convergence, when $n$ is sufficiently large, ", "page_idx": 19}, {"type": "equation", "text": "$$\n|\\gamma^{n}(\\phi(z_{i},\\cdot))-\\gamma(\\phi(z_{i},\\cdot))|<\\epsilon/3,\\qquad\\mathrm{for}\\;\\mathrm{each}\\;i\\in[1:N].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "635 ", "page_idx": 19}, {"type": "text", "text": "Given $z\\in Z$ , then $z\\in B(z_{i})$ for some $z_{i}$ . For sufficiently large $n$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\gamma^{n}(\\phi(z,\\cdot))-\\gamma(\\phi(z,\\cdot))|}\\\\ &{\\leq|\\gamma^{n}(\\phi(z,\\cdot))-\\gamma^{n}(\\phi(z_{i},\\cdot))|+|\\gamma^{n}(\\phi(z_{i},\\cdot))-\\gamma(\\phi(z_{i},\\cdot))|+|\\gamma(\\phi(z_{i},\\cdot))-\\gamma(\\phi(z,\\cdot))|}\\\\ &{\\leq K d(z,z_{i})+\\epsilon/3+K d(z,z_{i})=\\epsilon/3+\\epsilon/3+\\epsilon/3=\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "636 ", "page_idx": 19}, {"type": "text", "text": "Thus we prove the first statement. ", "page_idx": 19}, {"type": "text", "text": "637 ", "page_idx": 19}, {"type": "text", "text": "(2) We recall that we do not have $(\\gamma^{n})^{\\otimes2}\\ {\\overset{w}{\\rightharpoondown}}\\ \\gamma^{\\otimes2}$ . ", "page_idx": 19}, {"type": "text", "text": "638 ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\leq\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}\\operatorname*{sup}}\\,|(\\gamma^{n})^{\\otimes2}(\\phi)-(\\gamma)^{\\otimes2}(\\phi)|}\\\\ &{\\ \\ \\leq\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}\\operatorname*{sup}}\\underbrace{|(\\gamma^{n}\\otimes\\gamma^{n})(\\phi)-(\\gamma\\otimes\\gamma^{n})(\\phi)|}_{A_{n}}+\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}\\operatorname*{sup}}\\underbrace{|(\\gamma^{n}\\otimes\\gamma)(\\phi)-(\\gamma\\otimes\\gamma)(\\phi)|}_{B_{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the first term, when $n$ is sufficiently large, by statement (1), we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{n}=\\displaystyle\\int\\big(\\gamma^{n}(\\phi(z,\\cdot))-\\gamma(\\phi(z,\\cdot))\\,d\\gamma^{n}(z)}\\\\ &{\\quad\\leq\\displaystyle\\operatorname*{max}_{z}|\\gamma^{n}(\\phi(z,\\cdot))-\\gamma(\\phi(z,\\cdot)||\\gamma^{n}|}\\\\ &{\\quad\\leq\\epsilon(|\\gamma|+\\epsilon)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "640 ", "page_idx": 19}, {"type": "text", "text": "Thus, $\\textstyle\\operatorname*{lim}\\operatorname*{sup}_{n}A=\\operatorname*{lim}_{n}A=0$ . ", "page_idx": 19}, {"type": "text", "text": "641 ", "page_idx": 19}, {"type": "text", "text": "Similarly, for the second term, when $n$ is sufficiently large, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nB_{n}:=\\int(\\gamma^{n}(\\phi(z,\\cdot))-\\gamma(\\phi(z,\\cdot)))d\\gamma(z)\\leq\\epsilon|\\gamma|.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "642 ", "page_idx": 19}, {"type": "text", "text": "Thus, $\\operatorname*{lim}\\operatorname*{sup}_{n}B_{n}=\\operatorname*{lim}_{n}B_{n}=0$ . ", "page_idx": 19}, {"type": "text", "text": "643 ", "page_idx": 19}, {"type": "text", "text": "Therefore, from (27), (28) and (29), we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}|(\\gamma^{n})^{\\otimes2}(\\phi)-(\\gamma)^{\\otimes2}(\\phi)|=\\operatorname*{lim}_{n\\to\\infty}|(\\gamma^{n})^{\\otimes2}(\\phi)-(\\gamma)^{\\otimes2}(\\phi)|=0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "(3) Let $\\gamma^{n}\\in{\\mathcal{M}}$ be a sequence such that $(\\gamma^{n})^{\\otimes2}(\\phi)$ (weakly) converges to $\\operatorname{inf}_{\\gamma\\in\\mathcal{M}}\\gamma^{\\otimes2}(\\phi)$ . Since $\\mathcal{M}$ is compact, there exists a sub-sequence $\\gamma^{n_{k}}~\\overset{w}{\\to}\\gamma$ for some $\\gamma\\in\\mathcal{M}$ . Then, by statement (2), we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\gamma^{\\otimes2}(\\phi)=\\operatorname*{lim}_{k}(\\gamma^{n_{k}})^{\\otimes2}(\\phi)=\\operatorname*{inf}_{\\gamma\\in\\mathcal{M}}\\gamma^{\\otimes2}(\\phi),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "644 ", "page_idx": 20}, {"type": "text", "text": "and we complete the proof. ", "page_idx": 20}, {"type": "text", "text": "645 ", "page_idx": 20}, {"type": "text", "text": "646 C.3 Proof of the Formal Statement for Proposition 3.3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "647 The proof follows the ideas of [8, Corollary 10.1]. ", "page_idx": 20}, {"type": "text", "text": "648 Define $(Z,d_{Z})$ as $Z:=X\\times Y$ , with $d_{Z}((x,y),(x^{\\prime},y^{\\prime})):=d_{X}(x,x^{\\prime})+d_{Y}(y,y^{\\prime}).$ ", "page_idx": 20}, {"type": "text", "text": "649 We claim that the following mapping ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(X\\times Y)^{2}=Z^{2}\\to\\mathbb{R}}\\\\ &{\\mathsf{\\Gamma}((x,y),(x^{\\prime},y^{\\prime}))\\mapsto\\phi((x,y),(x^{\\prime},y^{\\prime})):=L(d_{X}^{q}(x,x^{\\prime}),d_{Y}^{q}(y,y^{\\prime}))-2\\lambda}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "650 is a Lipschitz function with respect to $d_{Z}^{+}$ , where $L$ satisfies (24). Indeed, given   \n651 $((x_{1},y_{1}),(x_{1}^{\\prime},y_{1}^{\\prime})),((x_{2},y_{2}),(x_{2}^{\\prime},y_{2}^{\\prime}))\\in Z^{2}$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi((x_{1},y_{1}),(x_{1}^{\\prime},y_{1}^{\\prime}))-\\phi((x_{2},y_{2}),(x_{2}^{\\prime},y_{2}^{\\prime}))|}\\\\ &{=|L(d x(x_{1},x_{1}^{\\prime}],d_{Y}(y_{1},y_{1}^{\\prime}))-L(d_{X}(x_{2},x_{2}^{\\prime}),d_{Y}(y_{2},y_{2}^{\\prime}))|}\\\\ &{\\leq|L(d_{X}(x_{1},x_{1}^{\\prime}),d_{Y}(y_{1},y_{1}^{\\prime}))-L(d_{X}(x_{2},x_{2}^{\\prime}),d_{Y}(y_{1},y_{1}^{\\prime}))|}\\\\ &{\\quad+|L(d_{X}(x_{2},x_{2}^{\\prime}),d_{Y}(y_{1},y_{1}^{\\prime}))-L(d_{X}(x_{2},x_{2}^{\\prime}),d_{Y}(y_{2},y_{2}^{\\prime}))|}\\\\ &{\\leq K|d_{X}^{\\prime}(x_{1},x_{1}^{\\prime})-d_{X}^{\\prime}(x_{2},x_{2}^{\\prime})|+K|d_{Y}^{\\prime}(y_{1},y_{1}^{\\prime})-d_{Y}^{\\prime}(y_{2},y_{2}^{\\prime})|}\\\\ &{\\leq K^{\\prime}|d_{X}(x_{1},x_{1}^{\\prime})-d_{X}(x_{2},x_{2}^{\\prime})|+K^{\\prime}|d_{Y}(y_{1},y_{1}^{\\prime})-d_{Y}(y_{2},y_{2}^{\\prime})|}\\\\ &{\\leq K^{\\prime}(d_{X}(x_{1},x_{2}^{\\prime})+d_{X}(x_{1}^{\\prime},x_{2}^{\\prime}))+K^{\\prime}(d_{Y}(y_{1},y_{2})+d_{Y}(y_{1}^{\\prime},y_{2}^{\\prime}))}\\\\ &{=K^{\\prime}|(d(x(x_{1},x_{2})+d_{Y}(y_{1},y_{2}))+((d_{X}(x_{1}^{\\prime},x_{2\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "652 where in (31), $K^{\\prime}=q\\beta^{q-1}K$ ; the inequality holds by lemma C.2; The inequality (32) follows from   \n653 the triangle inequality: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{X}(x_{1},x_{1}^{\\prime})-d_{X}(x_{2},x_{2}^{\\prime})\\leq d_{X}(x_{1},x_{2})+d_{X}(x_{2},x_{2}^{\\prime})+d_{X}(x_{2}^{\\prime},x_{1}^{\\prime})-d_{X}(x_{2},x_{2}^{\\prime})}\\\\ {=d_{X}(x_{1},x_{2})+d_{X}(x_{1}^{\\prime},x_{2}^{\\prime}),\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and similarly, ", "page_idx": 20}, {"type": "equation", "text": "$$\nd_{X}(x_{2},x_{2}^{\\prime})-d_{X}(x_{1},x_{1}^{\\prime})\\leq d_{X}(x_{1},x_{2})+d_{X}(x_{1}^{\\prime},x_{2}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "654 Let $\\mathcal{M}=\\Gamma_{\\leq}(\\mu,\\nu)$ . From [53, Proposition B.1], we have that $\\Gamma_{\\le}(\\mu,\\nu)$ is a compact set with respect   \n655 to the weak convergence topology. ", "page_idx": 20}, {"type": "text", "text": "656 By Lemma (C.3) part (3), we have the PGW problem, which can be written as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu)}\\gamma^{\\otimes2}(\\phi)+\\lambda(|\\mu|^{2}+|\\nu|^{2})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "657 admits a solution, i.e., a minimizer $\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu)$ . Therefore, we end the proof of Proposition 3.3. ", "page_idx": 20}, {"type": "text", "text": "658 D Proof of Proposition 3.4: Metric Property of Partial GW ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "659 Let $L(r_{1},r_{2})=D^{p}(r_{1},r_{2})$ for a metric $D$ on $\\mathbb{R}$ , and since all the metrics in $\\mathbb{R}$ are equivalent, for   \n660 simplicity, consider $D(r_{1},r_{2})=|r_{1}-r_{2}|$ . (Notice that this satisfies the hypothesis of Proposition   \n661 H.1 used in the experiments). ", "page_idx": 20}, {"type": "text", "text": "662 Consider the GW problem, for $q\\geq1$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nG W_{q}^{L}(\\mathbb{X},\\mathbb{Y}):=\\operatorname*{inf}_{\\gamma\\in\\Gamma(\\mu,\\nu)}\\int_{(X\\times Y)^{2}}L(d_{X}^{q}(x,x^{\\prime}),d_{Y}^{q}(y,y^{\\prime}))\\,d\\gamma^{\\otimes2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "663 or, in particular, ", "page_idx": 21}, {"type": "equation", "text": "$$\nG W_{q}^{p}(\\mathbb{X},\\mathbb{Y}):=\\operatorname*{inf}_{\\gamma\\in\\Gamma(\\mu,\\nu)}\\int_{(X\\times Y)^{2}}|d_{X}^{q}(x,x^{\\prime})-d_{Y}^{q}(y,y^{\\prime})|^{p}\\,d\\gamma^{\\otimes2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "664 For probability mm-spaces we have the equivalence relation $\\mathbb{X}\\sim\\mathbb{Y}$ if and only if $G W_{q}^{p}(\\mathbb{X},\\mathbb{Y})=0$ .   \n665 By [8, Chapter 5], $\\mathbb{X}\\sim\\mathbb{Y}$ is equivalent to the following: there exists a bijective isometry mapping   \n666 $\\phi:X\\rightarrow Y$ , such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{d_{X}(x,x^{\\prime})-d_{Y}(\\phi(x),\\phi(x^{\\prime}))=0,\\;\\;\\;\\;\\mu^{\\otimes2}-a.s.}}\\\\ {{\\phi_{\\#}\\mu=\\nu.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "667 Remark D.1. In the literature, the case where $q=1$ is the most frequently considered problem. In   \n668 particular, in $I8J$ it is stated the equivalence relation $\\mathbb{X}\\sim\\mathbb{Y}$ if and only if there exists $\\phi:X\\rightarrow Y$   \n669 such that $\\phi_{\\#}\\mu=\\nu$ and $d_{X}(x,x^{\\prime}\\bar{)}=d_{Y}(\\phi(x),\\phi(x^{\\prime}))\\;\\mu^{\\otimes2}-a.s.\\;i f\\,c$ and only if $G W_{1}^{p}(\\mathbb{X},\\mathbb{Y})=0$ .   \n670 Thus, $\\mathbb{X}\\sim\\mathbb{Y}$ is also equivalent to have $\\phi:X\\rightarrow Y$ such that $\\phi_{\\#}\\mu=\\nu$ and $d_{X}(x,x^{\\prime})=d_{Y}(y,y^{\\prime})$   \n671 $\\gamma^{\\otimes2}-a.s$ . where $\\gamma$ is a minimizer for $G W_{1}^{p}(\\mathbb{X},\\mathbb{Y})$ . So, in this situation we also have $d_{X}^{q}(x,x^{\\prime})=$   \n672 $d_{Y}^{q}(y,y^{\\prime})\\;\\gamma^{\\otimes2}-a.s.$ . for any given $q\\geq1$ . Therefore, $\\mathbb{X}\\sim\\mathbb{Y}$ if and only if $G W_{q}^{p}(\\mathbb{X},\\mathbb{Y})=0$ . ", "page_idx": 21}, {"type": "text", "text": "673 D.1 Formal Statement of Proposition 3.4 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "674 We first introduce the formal statement of Proposition 3.4. To do so, we extend the equivalence relation   \n675 $\\sim$ to all mm-spaces (not only probability mm-spaces): Given arbitrary mm-spaces $\\mathbb{X}=(X,d_{X},\\mu)$ ,   \n676 $\\mathbb{Y}=(Y,d_{Y},\\bar{\\nu})$ , where $X,Y$ are compact and $\\bar{\\mu}\\in\\mathcal{M}_{+}(X)$ , $\\nu\\in\\mathcal{M}_{+}(Y)$ , we write $\\mathbb{X}\\sim\\mathbb{Y}$ if and   \n677 only if they have the same total mass (i.e., $|\\mu|=\\mu(X)=\\nu(Y)=|\\nu|)$ and $G W_{q}^{p}(\\mathbb{X},\\mathbb{Y})=0$ .   \n678 Formal statement of Proposition 3.4: Given $\\lambda>0$ , $1\\leq p,q<\\infty,$ , then $(P G W_{\\lambda,q}^{p}(\\cdot,\\cdot))^{1/p}$ defines   \n679 a metric among mm-spaces under taking quotient with respect to the equivalence relation $\\sim$ . ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "680 Next, we discuss its proof. ", "page_idx": 21}, {"type": "text", "text": "681 D.2 Non-Negativity and Symmetry Properties ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "682 It is straightforward to verify $P G W_{\\lambda,q}^{p}(\\mathbb{X},\\mathbb{Y})\\geq0$ , and that $P G W_{\\lambda,q}^{p}(\\mathbb{X},\\mathbb{Y})=P G W_{\\lambda,q}^{p}(\\mathbb{Y},\\mathbb{X})$ . In   \n683 what follows, we will concentrate on proving $P G W_{\\lambda,q}^{p}(\\mathbb{X},\\mathbb{Y})=0$ if and only if $\\mathbb{X}\\sim\\mathbb{Y}$ : ", "page_idx": 21}, {"type": "text", "text": "If $\\mathbb{X}\\sim\\mathbb{Y}$ , then $|\\mu|=|\\nu|$ , and we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n0\\leq P G W_{\\lambda,q}^{p}(\\mathbb{X},\\mathbb{Y})\\leq G W_{q}^{p}(\\mathbb{X},\\mathbb{Y})=0,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "684 where the inequality follows from the fact $\\Gamma(\\mu,\\nu)\\subseteq\\Gamma_{\\leq}(\\mu,\\nu)$ . Thus, $P G W_{\\lambda,q}^{p}(\\mathbb{X},\\mathbb{Y})=0.$ . ", "page_idx": 21}, {"type": "text", "text": "685 For the other direction, suppose that $P G W_{\\lambda,q}^{p}(\\mathbb{X},\\mathbb{Y})=0$ . We claim that $|\\mu|=|\\nu|$ and that there exist   \n686 an optimal plan $\\gamma$ for $P G W_{\\lambda,q}^{p}(\\mathbb{X},\\mathbb{Y})$ such that $|\\mu|=|\\gamma|=|\\nu|$ . Let us prove this by contradiction.   \n687 Assume $|\\mu|<|\\nu|$ . For convenience, suppose $|\\mu|^{2}\\,\\leq\\,|\\nu|^{2}\\,-\\epsilon$ , for some $\\epsilon\\,>\\,0$ . Then, for each   \n688 $\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu)$ , we have $|\\gamma^{\\otimes2}|\\leq|\\mu|^{2}\\leq|\\dot{\\nu}|^{2}-\\epsilon$ , and so ", "page_idx": 21}, {"type": "equation", "text": "$$\nP G W_{\\lambda,q}^{p}(\\mathbb{X},\\mathbb{Y})\\geq\\lambda(|\\mu|^{2}+|\\nu|^{2}-2|\\gamma|^{2})\\geq\\lambda(|\\nu^{2}|-|\\gamma|^{2})\\geq\\lambda\\epsilon>0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "689 Thus, $P G W_{\\lambda,q}^{p}(\\mathbb{X},\\mathbb{Y})>0$ , which is a contradiction. So, $|\\mu|=|\\nu|$ . In addition, if $\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu)$   \n690 is optimal for $P G W_{\\lambda,q}^{p}(\\mathbb{X},\\mathbb{Y})$ , we have $|\\gamma|\\;=\\;|\\mu|\\;=\\;|\\nu|$ , thus $\\gamma\\,\\in\\,\\Gamma(\\mu,\\nu)$ . Therefore, since   \n691 $P G W_{\\lambda,q}^{p}(\\mathbb{X},\\mathbb{Y})=0$ , and for such optimal $\\gamma$ we have $|\\gamma|=|\\mu|=|\\nu|$ , we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{(X\\times Y)^{2}}|d_{X}^{q}(x,x^{\\prime})-d_{Y}^{q}(y,y^{\\prime})|^{p}d\\gamma^{\\otimes2}=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "692 As a result, $d_{X}^{q}(x,x^{\\prime})=d_{Y}^{q}(y,y^{\\prime})\\;\\gamma^{\\otimes2}-a.s.$ , which implies that $G W_{q}^{p}(\\mathbb{X},\\mathbb{Y})=0$ , and so $\\mathbb{X}\\sim\\mathbb{Y}$ . ", "page_idx": 21}, {"type": "text", "text": "693 D.3 Triangle Inequality \u2013 Strategy: Convert the PGW Problem into a GW Problem ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "694 Consider three arbitrary mm-spaces $\\mathbb{S}=(S,d_{S},\\sigma)$ , $\\mathbb{X}=\\left(X,d_{X},\\mu\\right)$ , $\\mathbb{Y}=(Y,d_{Y},\\nu)$ . We define   \n695 $\\hat{\\mathbb{S}}=(\\hat{S},d_{\\hat{S}},\\hat{\\sigma}),\\hat{\\mathbb{X}}=(\\hat{\\hat{X}},d_{\\hat{X}},\\bar{\\mu}),\\hat{\\mathbb{Y}}=(\\hat{Y},d_{\\hat{Y}},\\hat{\\nu})$ in a similar way to that of Proposition G.1 but now   \n696 aiming to have new spaces with equal total mass: ", "page_idx": 22}, {"type": "text", "text": "697 First, introduce auxiliary points $\\hat{\\infty}_{0},\\hat{\\infty}_{1},\\hat{\\infty}_{2}$ and set ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\hat{S}\\right.\\ =S\\cup\\{\\hat{\\infty}_{0},\\hat{\\infty}_{1},\\hat{\\infty}_{2}\\},}\\\\ {\\left.\\hat{X}\\right.\\ =X\\cup\\{\\hat{\\infty}_{0},\\hat{\\infty}_{1},\\hat{\\infty}_{2}\\},}\\\\ {\\left.\\hat{Y}\\right.\\ =Y\\cup\\{\\hat{\\infty}_{0},\\hat{\\infty}_{1},\\hat{\\infty}_{2}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "698 Define $\\hat{\\sigma},\\hat{\\mu},\\hat{\\nu}$ as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\hat{\\sigma}\\right.}&{=\\sigma+|\\mu|\\delta_{\\hat{\\infty}_{1}}+|\\nu|\\delta_{\\hat{\\infty}_{2}},}\\\\ {\\hat{\\mu}}&{=\\mu+|\\sigma|\\delta_{\\hat{\\infty}_{0}}+|\\nu|\\delta_{\\hat{\\infty}_{2}},}\\\\ {\\hat{\\nu}}&{=\\nu+|\\sigma|\\delta_{\\hat{\\infty}_{0}}+|\\mu|\\delta_{\\hat{\\infty}_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "699 Note that $\\hat{\\sigma}$ is not supported on point $\\hat{\\infty}_{0}$ , similarly, $\\hat{\\mu}$ is not supported on $\\hat{\\infty}_{1}$ , $\\hat{\\nu}$ is not supported   \n700 on $\\hat{\\infty}_{2}$ . In addition, we have $|\\hat{\\mu}|\\,=\\,|\\hat{\\nu}|\\,=\\,|\\hat{\\sigma}|\\,=\\,|\\mu|\\,+\\,|\\nu|\\,+\\,|\\sigma|$ . (For a similar idea in classical   \n701 unbalanced optimal transport see, for example, [16].) ", "page_idx": 22}, {"type": "text", "text": "702 Finally, define $d_{\\hat{S}}:\\hat{S}^{2}\\rightarrow\\mathbb{R}\\cup\\{\\infty\\}$ as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\nd_{\\hat{S}}(s,s^{\\prime})=\\left\\{{d_{S}(s,s^{\\prime})\\quad\\mathrm{if~}(s,s^{\\prime})\\in S^{2}},\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "703 Note, $d_{\\hat{S}}(\\cdot,\\cdot)$ is not a rigorous metric in $\\hat{S}$ since we allow $d_{\\hat{S}}=\\infty$ . Similarly, define $d_{\\hat{X}},d_{\\hat{Y}}$ . As a   \n704 result, we have constructed new spaces ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\mathbb{S}}=(\\hat{S},d_{\\hat{S}},\\hat{\\sigma}),\\quad\\hat{\\mathbb{X}}=(\\hat{X},d_{\\hat{X}},\\hat{\\mu}),\\quad\\hat{\\mathbb{Y}}=(\\hat{Y},d_{\\hat{Y}},\\hat{\\nu}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "705 We define the following mapping $D_{\\lambda}:(\\mathbb{R}\\cup\\{\\infty\\})\\times(\\mathbb{R}\\cup\\{\\infty\\})\\rightarrow\\mathbb{R}_{+}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\nD_{\\lambda}^{p}(r_{1},r_{2})={\\left\\{\\begin{array}{l l}{|r_{1}-r_{2}|^{p}}&{{\\mathrm{if}}\\ r_{1},r_{2}<\\infty,}\\\\ {\\lambda}&{{\\mathrm{if}}\\ r_{1}=\\infty,r_{2}<\\infty\\ {\\mathrm{or}}\\ {\\mathrm{vice}}\\ {\\mathrm{versa}},}\\\\ {0}&{{\\mathrm{if}}\\ r_{1}=r_{2}=\\infty.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "706 Note that $D_{\\lambda}$ is not a rigorous metric since it may sometimes violate triangle inequality. See the   \n707 following lemma for a detailed and precise explanation.   \n708 Lemma D.2. Let $D_{\\lambda}(\\cdot,\\cdot)$ denote the function defined in (38). For any $r_{0},r_{1},r_{2}\\in\\mathbb{R}\\cup\\{\\infty\\}$ , we   \n709 have the following: ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "710 ", "page_idx": 22}, {"type": "text", "text": "711 ", "page_idx": 22}, {"type": "text", "text": "\u2022 $D_{\\lambda}(r_{1},r_{2})\\;\\geq\\;0.$ . $D_{\\lambda}(r_{1},r_{2})\\,=\\,0$ if and only if $r_{1}\\,=\\,r_{2}$ , where $r_{1}\\,=\\,r_{2}$ denotes that $r_{1}=r_{2}\\in\\mathbb{R}$ or $r_{1}=r_{2}=\\infty$ . ", "page_idx": 22}, {"type": "text", "text": "\u2022 Except the case $r_{1},r_{2}\\in\\mathbb{R},r_{0}=\\infty,$ , for all other cases, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nD_{\\lambda}(r_{1},r_{2})\\leq D_{\\lambda}(r_{1},r_{0})+D_{\\lambda}(r_{2},r_{0}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "712 Proof of Lemma $D.2$ . It is straightforward to verify $D_{\\lambda}(\\cdot,\\cdot)\\geq0$ . ", "page_idx": 22}, {"type": "text", "text": "713 Now, consider $r_{0},r_{1},r_{2}\\in\\mathbb{R}\\cup\\{\\infty\\}$ . If $r_{1}=r_{2}\\in\\mathbb{R}$ or $r_{1}=r_{2}=\\infty$ , we have $D_{\\lambda}(r_{1},r_{2})=0$ .   \n714 Otherwise, $D_{\\lambda}(r_{1},r_{2})>0$ . So, $D_{\\lambda}(r_{1},r_{2})=0$ if and only if $r_{1}=r_{2}$ . ", "page_idx": 22}, {"type": "text", "text": "715 For the second item, we have the following cases: ", "page_idx": 22}, {"type": "text", "text": "716 Case 1: $r_{1},r_{2},r_{0}\\in\\mathbb{R}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{D_{\\lambda}(r_{1},r_{2})=\\vert r_{1}-r_{2}\\vert}}\\\\ {{\\leq\\vert r_{1}-r_{2}\\vert+\\vert r_{2}-r_{0}\\vert}}\\\\ {{=D_{\\lambda}(r_{0},r_{1})+D_{\\lambda}(r_{0},r_{2})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "717 Case 2: $r_{1},r_{2}\\in\\mathbb{R},r_{0}=\\infty$ . We do not need to verify the inequality in this case. ", "page_idx": 22}, {"type": "text", "text": "718 Case 3: $r_{1}\\in\\mathbb{R},r_{2},r_{0}=\\infty$ , or $r_{1}=\\infty,r_{2}\\in\\mathbb{R},r_{0}=\\infty$ . In this case, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nD_{\\lambda}(r_{1},r_{2})=D_{\\lambda}(r_{1},r_{0})=\\sqrt{\\lambda},D_{\\lambda}(r_{2},r_{0})=0\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "719 and it is straightforward to verify the inequality. ", "page_idx": 23}, {"type": "text", "text": "720 Case 4: $r_{1},r_{2}=\\infty,r_{3}\\in\\mathbb{R}$ . In this case, we have $D_{\\lambda}(r_{1},r_{2})=0\\leq D_{\\lambda}(r_{0},r_{1})+D_{\\lambda}(r_{0},r_{2}).$   \n721 Case 5: $r_{1},r_{2},r_{0}=\\infty$ . In this case, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nD_{\\lambda}(r_{1},r_{2})=D_{\\lambda}(r_{1},r_{0})=D_{\\lambda}(r_{2},r_{0})=0\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "722 and it is straightforward to verify the inequality. ", "page_idx": 23}, {"type": "text", "text": "723 We construct the following generalized GW problem: ", "page_idx": 23}, {"type": "equation", "text": "$$\nG W_{\\lambda,q}^{p}(\\hat{\\mathbb{X}},\\hat{\\mathbb{Y}}):=\\operatorname*{inf}_{\\hat{\\gamma}\\in\\Gamma(\\hat{\\mu},\\hat{\\nu})}\\underbrace{\\int_{(\\hat{X}\\times\\hat{Y})^{2}}D_{\\lambda}^{p}(d_{\\hat{X}}^{q}(x,x^{\\prime}),d_{\\hat{Y}}^{q}(y,y^{\\prime}))\\,d\\hat{\\gamma}^{\\otimes2}}_{\\hat{C}(\\hat{\\gamma};\\lambda,\\hat{\\mu},\\hat{\\nu})}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "724 Similarly, we define $G W_{\\lambda,q}^{p}(\\hat{\\mathbb{X}},\\hat{\\mathbb{S}})$ , and $G W_{\\lambda,q}^{p}(\\hat{\\mathbb{S}},\\hat{\\mathbb{Y}})$ . ", "page_idx": 23}, {"type": "text", "text": "725 The mapping (6) is modified as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Gamma_{\\le}(\\sigma,\\mu)\\ni\\gamma^{01}\\mapsto\\hat{\\gamma}^{01}\\in\\Gamma(\\hat{\\sigma},\\hat{\\mu}),}\\\\ &{\\hat{\\gamma}^{01}:=\\gamma^{01}+(\\sigma-\\gamma_{1}^{01})\\otimes\\delta_{\\hat{\\infty}_{0}}+\\delta_{\\hat{\\infty}_{1}}\\otimes(\\mu-\\gamma_{2}^{01})+|\\gamma|\\delta_{\\hat{\\infty}_{1},\\hat{\\infty}_{0}}+|\\nu|\\delta_{\\hat{\\infty}_{2},\\hat{\\infty}_{2}};}\\\\ &{\\Gamma_{\\le}(\\sigma,\\nu)\\ni\\gamma^{02}\\mapsto\\hat{\\gamma}^{02}\\in\\Gamma(\\hat{\\sigma},\\hat{\\nu}),}\\\\ &{\\hat{\\gamma}^{02}:=\\gamma^{02}+(\\sigma-\\gamma_{1}^{02})\\otimes\\delta_{\\hat{\\infty}_{0}}+\\delta_{\\hat{\\infty}_{2}}\\otimes(\\nu-\\gamma_{2}^{02})+|\\gamma|\\delta_{\\hat{\\infty}_{2},\\hat{\\infty}_{0}}+|\\mu|\\delta_{\\hat{\\infty}_{1},\\hat{\\infty}_{1}};}\\\\ &{\\Gamma_{\\le}(\\mu,\\nu)\\ni\\gamma^{12}\\mapsto\\hat{\\gamma}^{12}\\in\\Gamma(\\hat{\\mu},\\hat{\\nu}),}\\\\ &{\\hat{\\gamma}^{12}:=\\gamma^{12}+(\\mu-\\gamma_{1}^{12})\\otimes\\delta_{\\hat{\\infty}_{1}}+\\delta_{\\hat{\\infty}_{2}}\\otimes(\\nu-\\gamma_{2}^{12})+|\\gamma|\\delta_{\\hat{\\infty}_{2},\\hat{\\infty}_{1}}+|\\mu|\\delta_{\\hat{\\infty}_{0},\\hat{\\infty}_{0}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "726 It is straightforward to verify the above mappings are well-defined. In addition, we can observe that,   \n727 for each $\\tilde{\\gamma}^{01}\\in\\Gamma_{\\leq}(\\sigma,\\mu),\\gamma^{\\bar{0}2}\\in\\Gamma_{\\leq}(\\sigma,\\nu),\\dot{\\gamma}^{\\mathrm{12}}\\not\\in\\Gamma_{\\leq}(\\mu,\\nu).$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\gamma}^{01}\\big(\\{\\hat{\\infty}_{2}\\}\\times X\\big)=\\hat{\\gamma}^{01}(S\\times\\{\\hat{\\infty}_{2}\\})=0,}\\\\ &{\\hat{\\gamma}^{02}\\big(\\{\\hat{\\infty}_{1}\\}\\times Y\\big)=\\hat{\\gamma}^{02}(S\\times\\{\\hat{\\infty}_{1}\\})=0,}\\\\ &{\\hat{\\gamma}^{12}\\big(\\{\\hat{\\infty}_{0}\\}\\times Y\\big)=\\hat{\\gamma}^{12}(X\\times\\{\\hat{\\infty}_{0}\\})=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proposition D.3. If $\\ '\\gamma^{12}\\in\\Gamma_{\\leq}(\\mu,\\nu)$ is optimal in PGW problem $P G W_{\\lambda,q}^{p}(\\mathbb{X},\\mathbb{Y})$ , then $\\hat{\\gamma}^{12}$ defined in (40) is optimal in generalized GW problem $G W_{\\lambda,q}^{p}(\\hat{\\mathbb{X}},\\hat{\\mathbb{Y}})$ . Furthermore, $\\hat{C}(\\hat{\\gamma}^{12};\\lambda,\\hat{\\mu},\\hat{\\nu})\\;=$ $C(\\gamma^{12};\\lambda,\\mu,\\nu)$ , and thus, ", "page_idx": 23}, {"type": "equation", "text": "$$\nP G W_{\\lambda,q}^{p}(\\mathbb{X},\\mathbb{Y})=G W_{\\lambda,q}^{p}(\\hat{\\mathbb{X}},\\hat{\\mathbb{Y}}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "728 Proof of Proposition D.3. For each $\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu)$ , define $\\hat{\\gamma}$ by (40). ", "page_idx": 23}, {"type": "text", "text": "Note that if we merge the points $\\hat{\\infty}_{1},\\hat{\\infty}_{2},\\hat{\\infty}_{3}$ as $\\hat{\\infty}$ , i.e. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\infty}=\\hat{\\infty}_{1}=\\hat{\\infty}_{2}=\\hat{\\infty}_{3},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "729 the value $\\hat{C}(\\hat{\\gamma};\\lambda,\\hat{\\mu},\\hat{\\nu})$ will not change. Thus, we merge these three auxiliary points. ", "page_idx": 23}, {"type": "text", "text": "730 We have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C(\\gamma;\\lambda,\\beta,i,\\delta)=\\int_{(\\Lambda\\times\\tilde{\\gamma})^{2}}D_{x}^{i}(d\\tilde{x},(x,x^{\\prime}),d\\tilde{\\gamma}(x,x^{\\prime}))d\\tilde{\\gamma}^{(2)}}\\\\ &{=\\int_{(\\Lambda\\times\\tilde{\\gamma})^{2}}|d\\tilde{x}(x,x^{\\prime})-d\\tilde{\\gamma}^{(1)}(\\mu,\\tilde{\\gamma})|^{p}d\\tilde{\\mu}^{(2)}+\\int_{((\\Lambda\\times\\tilde{\\gamma})\\times\\tilde{\\gamma}^{2})}\\lambda d\\tilde{\\mu}^{(2)}+\\int_{(\\Lambda\\times(\\tilde{\\gamma}))^{2}}\\lambda\\tilde{\\gamma}^{(0)}}\\\\ &{\\quad+2\\int_{((\\Lambda\\times\\tilde{\\gamma})\\times\\Lambda^{\\prime})}M^{i\\beta}\\mathfrak{L}^{\\otimes2}+2\\int_{(\\Lambda\\times(\\tilde{\\gamma})\\times\\tilde{\\gamma}^{(1)})\\times(X\\times Y)}M^{i\\beta}\\mathfrak{L}^{\\otimes2}+\\int_{((\\Lambda\\times\\tilde{\\gamma})\\times(\\tilde{\\gamma}))^{2}}D_{x}^{i}(\\alpha,\\alpha)d\\tilde{\\gamma}^{(0)}}\\\\ &{\\quad+2\\int_{((\\Lambda\\times\\tilde{\\gamma})\\times\\Lambda^{\\prime})(X\\times(\\tilde{\\gamma}))}D_{x}^{i}(x,\\alpha)d\\tilde{\\gamma}^{(0)}+2\\int_{((\\Lambda\\times\\tilde{\\gamma})\\times(\\tilde{\\gamma}))\\times(X\\times Y)}D_{x}^{i}(x,\\alpha)d\\tilde{\\gamma}^{(0)}}\\\\ &{\\quad+2\\int_{((\\Lambda\\times\\tilde{\\gamma})\\times(X\\times\\tilde{\\gamma}))^{2}}D_{x}^{i}(x,\\infty)d\\tilde{\\mu}^{(2)}+2\\int_{(\\Lambda\\times(\\tilde{\\gamma})\\times\\tilde{\\gamma}^{(1)})\\times(X\\times Y)}D_{x}^{i}(x,\\alpha)d\\tilde{\\gamma}^{(0)}}\\\\ &{=\\int_{(\\Lambda\\times\\tilde{\\gamma})^{2}}|d\\tilde{x}(x,x^{\\prime})-d\\tilde{\\gamma}^{(0)}|^{p}d\\tilde{\\mu}^{(2)}}\\\\ &{\\quad+2\\lambda(|x|-|\\tilde{\\gamma}|)|\\gamma+\\lambda(|x|-|\\tilde{\\gamma}|)^{2}+2\\lambda(|\\mu|-|\\tilde{\\gamma}|)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "As we merged the points $\\hat{\\infty}_{1},\\hat{\\infty}_{2},\\hat{\\infty}_{3}$ , by [40, Proposition B.1.], the mapping $\\gamma\\mapsto\\hat{\\gamma}$ defined in (40) is a bijection. Then, if $\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu)$ is optimal for the PGW problem $P G W_{\\lambda,q}^{p}(\\mathbb{X},\\mathbb{Y})$ (defined in (10)), $\\hat{\\gamma}\\in\\Gamma(\\hat{\\mu},\\hat{\\nu})$ is optimal for generalized GW problem $G W_{\\lambda,q}^{p}(\\hat{\\mathbb{X}},\\hat{\\mathbb{Y}})$ (defined in (39)). Therefore, ", "page_idx": 24}, {"type": "equation", "text": "$$\nG W_{\\lambda,q}^{p}(\\hat{\\mathbb{X}},\\hat{\\mathbb{Y}})=P G W_{\\lambda,q}^{p}(\\mathbb{X},\\mathbb{Y}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "731 ", "page_idx": 24}, {"type": "text", "text": "Proposition D.4 (Triangle inequality for $G W_{\\lambda,q}^{p}(\\cdot,\\cdot))$ . Consider the generalized GW problem (39). Then, for any $p\\in[1,\\infty)$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nG W_{\\lambda,q}^{p}(\\hat{\\mathbb{X}},\\hat{\\mathbb{Y}})\\leq G W_{\\lambda,q}^{p}(\\hat{\\mathbb{S}},\\hat{\\mathbb{X}})+G W_{\\lambda,q}^{p}(\\hat{\\mathbb{S}},\\hat{\\mathbb{Y}}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "732 Proof of Proposition D.4. We prove the case $p=2$ . For general $p\\geq1$ , it can be proved similarly. ", "page_idx": 24}, {"type": "text", "text": "733 Choose an optimal $\\gamma^{12}~\\in~\\Gamma_{\\leq}(\\mu,\\nu)$ for $P G W_{\\lambda,q}^{2}(\\mathbb{X},\\mathbb{Y})$ , an optimal $\\gamma^{01}~\\in~\\Gamma_{\\leq}(\\sigma,\\mu)$ for   \n734 $P G W_{\\lambda,q}^{2}(\\mathbb{S},\\mathbb{X})$ , and an optimal $\\gamma^{02}\\,\\in\\,\\Gamma_{\\le}(\\sigma,\\nu)$ for $P G W_{\\lambda,q}^{2}(\\mathbb{S},\\mathbb{Y})$ . Construct $\\hat{\\gamma}^{12},\\hat{\\gamma}^{01},\\hat{\\gamma}^{02}$ by   \n735 (40).   \n736 By Proposition D.3, we have that $\\hat{\\gamma}^{12},\\ \\hat{\\gamma}^{01},\\ \\hat{\\gamma}^{02}$ are optimal for $G W_{\\lambda,q}^{2}(\\hat{\\mathbb{X}},\\hat{\\mathbb{Y}}),~G W_{\\lambda,q}^{2}(\\hat{\\mathbb{S}},\\hat{\\mathbb{X}})$ ,   \n737 $G W_{\\lambda,q}^{2}(\\hat{\\mathbb{S}},\\hat{\\mathbb{Y}})$ , respectively. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "738 Define canonical projection mapping ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\pi_{0,1}:(\\hat{S}\\times\\hat{X}\\times\\hat{Y})\\to(\\hat{S}\\times\\hat{X})}}\\\\ {{(s,x,y)\\mapsto(s,x).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "739 Similarly, we define $\\pi_{0,2},\\pi_{1,2}$ . ", "page_idx": 24}, {"type": "text", "text": "740 By gluing lemma (see Lemma 5.5 [54]), there exists $\\hat{\\gamma}\\in\\mathcal{M}_{+}(\\hat{S}\\times\\hat{X}\\times\\hat{Y})$ , such that $(\\pi_{0,1})_{\\#}\\hat{\\gamma}=$   \n741 $\\hat{\\gamma}^{01},(\\pi_{0,2})_{\\#}\\hat{\\gamma}=\\hat{\\gamma}^{02}$ . Thus, $(\\pi_{1,2})_{\\#}\\hat{\\gamma}$ is a coupling between $\\hat{\\mu},\\hat{\\nu}$ . We have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{G W_{\\lambda,q}^{2}(\\mathbb{X},\\mathbb{Y})=\\displaystyle\\int_{(\\hat{X}\\times\\hat{Y})^{2}}D_{\\lambda}^{2}(d_{\\hat{X}}^{q}(x,x^{\\prime}),d_{\\hat{Y}}^{q}(y,y^{\\prime}))d(\\hat{\\gamma}^{12})^{\\otimes2}}\\\\ {\\le\\displaystyle\\int_{(\\hat{S}\\times\\hat{X}\\times\\hat{Y})^{2}}D_{\\lambda}^{2}(d_{\\hat{X}}^{q}(x,x^{\\prime}),d_{\\hat{Y}}^{q}(y,y^{\\prime}))d\\hat{\\gamma}^{\\otimes2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "742 The inequality holds since $(\\pi_{1,2})_{\\#}\\hat{\\gamma},\\hat{\\gamma}^{12}\\in\\Gamma(\\hat{\\mu},\\hat{\\nu})$ , and $\\hat{\\gamma}^{12}$ is optimal. ", "page_idx": 24}, {"type": "text", "text": "743 Next, we will show that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{(\\hat{S}\\times\\hat{X}\\times\\hat{Y})^{2}}D_{\\lambda}^{2}(d_{\\hat{X}}^{q}(x,x^{\\prime}),d_{\\hat{Y}}^{q}(y,y^{\\prime}))d{\\hat{\\gamma}}^{\\otimes2}}\\\\ &{\\leq\\displaystyle\\int_{(\\hat{S}\\times\\hat{X}\\times\\hat{Y})^{2}}(D_{\\lambda}(d_{\\hat{S}}^{q}(s,s^{\\prime}),d_{\\hat{X}}^{q}(x,x^{\\prime}))+D_{\\lambda}(d_{\\hat{S}}^{q}(s,s^{\\prime}),d_{\\hat{Y}}^{q}(y,y^{\\prime})))^{2}d{\\hat{\\gamma}}^{\\otimes2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "744 Let $((s,x,y),(s^{\\prime},x^{\\prime},y^{\\prime}))\\in(\\hat{S},\\hat{X},\\hat{Y})^{2}$ , and assume that ", "page_idx": 25}, {"type": "equation", "text": "$$\nD_{\\lambda}(d_{\\hat{X}}^{2}(x,x^{\\prime}),d_{\\hat{Y}}^{2}(y,y^{\\prime}))>D_{\\lambda}(d_{\\hat{S}}^{2}(s,s^{\\prime}),d_{\\hat{X}}^{2}(x,x^{\\prime}))+D_{\\lambda}(d_{\\hat{S}}^{2}(s,s^{\\prime}),d_{\\hat{Y}}^{2}(y,y^{\\prime})).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "745 By Lemma D.2, (44) implies $d_{\\hat{X}}(x,x^{\\prime}),d_{\\hat{Y}}(y,y^{\\prime})\\in\\mathbb{R},d_{\\hat{S}}(s,s^{\\prime})=\\infty$ . Thus, by definition (36), it   \n746 also implies ", "page_idx": 25}, {"type": "equation", "text": "$$\n(x,x^{\\prime})\\in X^{2},(y,y^{\\prime})\\in Y^{2},(s,s^{\\prime})\\in\\hat{S}^{2}\\setminus S^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "747 Define the following sets: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{\\alpha}=\\hat{S}\\times X\\times Y,}\\\\ &{A_{0}=\\{\\hat{\\infty}_{0}\\}\\times X\\times Y,}\\\\ &{A_{1}=\\{\\hat{\\infty}_{1}\\}\\times X\\times Y,}\\\\ &{A_{2}=\\{\\hat{\\infty}_{2}\\}\\times X\\times Y.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "748 Notice that, $(44)\\Longrightarrow(45)$ is equivalent to ", "page_idx": 25}, {"type": "equation", "text": "$$\n(44)\\Longrightarrow((s,x,y),(s,x^{\\prime},y^{\\prime}))\\in A:=\\bigcup_{i=0}^{2}(A_{i}\\times A_{\\alpha})\\cup\\bigcup_{i=0}^{2}(A_{\\alpha}\\times A_{i}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "749 Next, we will show $\\hat{\\gamma}^{\\otimes2}(A)=0$ . Indeed, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\gamma}(A_{0})\\leq\\hat{\\gamma}(\\{\\infty_{0}\\}\\times\\hat{X}\\times\\hat{Y})=\\hat{\\sigma}(\\{\\infty_{0}\\})=0\\qquad\\mathrm{by~definition~}(35)\\mathrm{~of~}\\hat{\\sigma}\\,,}\\\\ &{\\hat{\\gamma}(A_{1})\\leq\\hat{\\gamma}(\\{\\infty_{1}\\}\\times\\hat{X}\\times Y)=\\hat{\\gamma}^{02}(\\{\\hat{\\infty}_{1}\\times Y\\})=0\\qquad\\mathrm{by~}(42),}\\\\ &{\\hat{\\gamma}(A_{2})\\leq\\hat{\\gamma}(\\{\\infty_{2}\\}\\times X\\times\\hat{Y})=\\hat{\\gamma}^{01}(\\{\\hat{\\infty}_{2}\\times X\\})=0\\qquad\\mathrm{by~}(41).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "750 Thus, $\\hat{\\gamma}^{\\otimes2}(A)=0$ . By considering $B=(\\hat{S}\\times\\hat{X}\\times Y)^{2}\\setminus A$ , we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{(\\bar{S}\\times\\hat{X}\\times\\hat{Y})^{2}}D_{\\lambda}^{2}(d_{\\hat{X}}^{q}(x,x^{\\prime}),d_{\\hat{Y}}^{q}(y,y^{\\prime}))d\\gamma^{\\otimes2}}\\\\ &{=\\displaystyle\\int_{B}D_{\\lambda}^{2}(d_{\\hat{X}}^{q}(x,x^{\\prime}),d_{\\hat{Y}}^{q}(y,y^{\\prime}))d\\gamma^{\\otimes2}\\qquad\\mathrm{since~}\\gamma^{\\otimes2}(A)=0}\\\\ &{\\le\\displaystyle\\int_{B}\\left(D_{\\lambda}(d_{\\hat{S}}^{q}(s,s^{\\prime}),d_{\\hat{X}}^{q}(x,x^{\\prime})+D_{\\lambda}(d_{\\hat{S}}^{q}(s,s^{\\prime}),d_{\\hat{Y}}^{q}(y,y^{\\prime}))\\right)^{2}d\\gamma^{\\otimes2}\\qquad\\mathrm{by~}(a)}\\\\ &{\\le\\displaystyle\\int_{(\\hat{S}\\times\\hat{X}\\times\\hat{Y})^{2}}\\left(D_{\\lambda}(d_{\\hat{S}}^{q}(s,s^{\\prime}),d_{\\hat{X}}^{q}(x,x^{\\prime})+D_{\\lambda}(d_{\\hat{S}}^{q}(s,s^{\\prime}),d_{\\hat{Y}}^{q}(y,y^{\\prime}))\\right)^{2}d\\gamma^{\\otimes2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "751 Following (43) and (47), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G W_{\\lambda,q}^{\\lambda}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{r}})\\leq\\Bigg(\\int_{(\\delta\\times\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})\\geq1}D_{\\lambda}^{2}(d_{\\tilde{\\mathbf{x}}}^{x}(x,x^{\\prime}),d_{\\tilde{\\mathbf{r}}}^{x}(y,y))d_{\\tilde{\\mathbf{r}}}^{x\\geq}\\Bigg)^{1/2}}\\\\ &{\\leq\\Bigg(\\int_{(\\delta\\times\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})\\geq1}\\left(D_{\\lambda}(\\tilde{\\mathbf{x}}_{0}^{\\mathcal{X}}(s,x^{\\prime}),d_{\\tilde{\\mathbf{x}}}^{x}(x,x^{\\prime}))+D_{\\lambda}(\\tilde{\\mathbf{x}}_{0}^{\\mathcal{X}}(s,x^{\\prime}),d_{\\tilde{\\mathbf{r}}}^{x}(y,y))\\right)^{2}d_{\\tilde{\\mathbf{r}}}^{x\\geq}\\Bigg)^{1/2}}\\\\ &{\\leq\\Bigg(\\int_{(\\delta\\times\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})\\geq1}D_{\\lambda}^{2}(d_{\\tilde{\\mathbf{x}}}^{x}(s,x^{\\prime}),d_{\\tilde{\\mathbf{x}}}^{x}(x,x^{\\prime}))d_{\\tilde{\\mathbf{r}}}^{x\\geq}\\Bigg)^{1/2}}\\\\ &{\\qquad+\\left(\\int_{(\\delta\\times\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})\\geq1}D_{\\lambda}^{2}(d_{\\tilde{\\mathbf{x}}}^{x}(s,\\tilde{\\mathbf{x}}),d_{\\tilde{\\mathbf{r}}}^{x}(y,y))d_{\\tilde{\\mathbf{r}}}^{x\\geq}\\right)^{1/2}}\\\\ &{=\\Bigg(\\int_{(\\delta\\times\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})\\geq1}D_{\\lambda}^{2}(d_{\\tilde{\\mathbf{x}}}^{x}(s,\\tilde{\\mathbf{x}}^{\\prime}),d_{\\tilde{\\mathbf{x}}}^{x}(x,x^{\\prime}))d_{\\tilde{\\mathbf{r}}}^{x\\geq}\\Bigg)^{1/2}}\\\\ &{\\qquad+\\Bigg(\\int_{(\\delta\\times\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})\\geq1}D_{\\lambda}^{2}(d_{\\tilde{\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "752 where in the third inequality (48) we used the Minkowski inequality in $L^{2}((\\hat{S}\\times\\hat{X}\\times\\hat{Y})^{2},\\hat{\\gamma}^{\\otimes2})$ . ", "page_idx": 26}, {"type": "text", "text": "Now, we can complete the proof of Proposition 3.4: By the Propositions D.3, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nP G W_{\\lambda,q}^{p}(\\mathbb{X},\\mathbb{Y})=G W_{\\lambda,q}^{p}(\\hat{\\mathbb{X}},\\hat{\\mathbb{Y}})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "753 and similarly for $P G W_{\\lambda,q}^{p}$ and $(\\mathbb{S},\\mathbb{X}),P G W_{\\lambda,q}^{p}(\\mathbb{S},\\mathbb{Y})$ . By the Proposition D.4, $G W_{\\lambda,q}^{p}(\\cdot,\\cdot)$ satisfies   \n754 the triangle inequality, thus we complete the proof: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P G W_{\\lambda,q}^{p}(\\mathbb{X},\\mathbb{Y})=G W_{\\lambda,q}^{p}(\\hat{\\mathbb{X}},\\hat{\\mathbb{Y}})}\\\\ &{\\qquad\\qquad\\qquad\\leq G W_{\\lambda,q}^{p}(\\hat{\\mathbb{S}},\\hat{\\mathbb{X}})+G W_{\\lambda,q}^{p}(\\hat{\\mathbb{S}},\\hat{\\mathbb{Y}})}\\\\ &{\\qquad\\qquad\\qquad=P G W_{\\lambda,q}^{p}(\\mathbb{S},\\mathbb{X})+P G W_{\\lambda,q}^{p}(\\mathbb{S},\\mathbb{Y}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "755 E Proof of Proposition 3.5: PGW converges to GW as $\\lambda\\rightarrow\\infty$ . ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "756 In the main text, we set $\\lambda\\in\\mathbb R$ . In this section, we discuss the limit case that when $\\lambda\\rightarrow\\infty$ ", "page_idx": 26}, {"type": "text", "text": "757 Lemma E.1. Suppose $|\\mu|\\leq|\\nu|,$ , for each $\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu).$ , there exists $\\gamma^{\\prime}\\in\\Gamma_{\\le}(\\mu,\\nu)$ such that $\\gamma\\le\\gamma^{\\prime}$   \n758 and $(\\pi_{1})_{\\#}\\gamma^{\\prime}=\\mu$ . ", "page_idx": 26}, {"type": "text", "text": "759 Proof. Let $\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu)$ . ", "page_idx": 26}, {"type": "text", "text": "760 If $|\\gamma|=|\\mu|$ , then we have $(\\pi_{1})_{\\#}\\gamma=\\mu$ . ", "page_idx": 26}, {"type": "text", "text": "761 If $|\\gamma|<|\\mu|$ , let $\\mu^{r}=\\mu-(\\pi_{1})_{\\#}\\gamma,\\nu^{r}=\\nu-(\\pi_{2})_{\\#}\\gamma.$ . We have that $\\mu^{r},\\nu^{r}$ are non-negative measures,   \n762 with $|\\mu^{r}|=|\\mu|-|\\gamma|>0$ . If we define ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\gamma^{\\prime}:=\\gamma+\\frac{1}{|\\nu|-|\\gamma|}\\mu^{r}\\otimes\\nu^{r},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "763 we obtain $\\gamma\\le\\gamma^{\\prime}$ . In addition, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{(\\pi_{1})_{\\#}\\gamma^{\\prime}=(\\pi_{1})_{\\#}\\gamma+\\mu^{r}\\displaystyle\\frac{|\\nu^{r}|}{|\\nu|-|\\gamma|}=(\\pi_{1})_{\\#}\\gamma+\\mu^{r}=\\mu,}}\\\\ {{(\\pi_{2})_{\\#}\\gamma^{\\prime}=(\\pi_{2})_{\\#}\\gamma+\\nu^{r}\\displaystyle\\frac{|\\mu^{r}|}{|\\nu|-|\\gamma|}\\leq(\\pi_{2})_{\\#}\\gamma+\\nu^{r}\\displaystyle\\frac{|\\nu^{r}|}{|\\nu|-|\\gamma|}=\\nu.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "764 Thus, $\\gamma^{\\prime}\\in\\Gamma_{\\le}(\\mu,\\nu)$ and $(\\pi_{1})_{\\#}\\gamma^{\\prime}=\\mu$ . ", "page_idx": 26}, {"type": "text", "text": "765 Lemma E.2. Given general mm-spaces $\\mathbb{X}=(X,d_{X},\\mu)$ , $\\mathbb{Y}=(Y,d_{Y},\\nu)$ , where $\\mu,\\nu$ are supported   \n766 on bounded sets (in general, it is assumed that $X$ and $Y$ are compact, and that $s u p p(\\mu)\\,=\\,X$ ,   \n767 $s u p p(\\nu)\\:=\\:Y)$ , consider the problem the problem $P G W_{\\lambda,q}^{L}(\\mathbb{X},\\mathbb{Y})$ with $L(r_{1},r_{2})$ a continuous   \n768 functions. If $\\lambda$ is sufficiently large, for all optimal $\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu)$ we have $|\\gamma|=\\operatorname*{min}(|\\mu|,|\\nu|)$ . ", "page_idx": 27}, {"type": "text", "text": "769 Proof. We prove it for $q=1$ , for a general $q\\geq1$ , it can be proved similarly. ", "page_idx": 27}, {"type": "text", "text": "770 Without loss of generality, suppose $|\\mu|\\le|\\nu|$ . ", "page_idx": 27}, {"type": "text", "text": "771 Since $\\mu,\\nu$ are supported on bounded sets, there exists $A=[0,M]$ such that $d_{X}(x,x^{\\prime}),d_{Y}(y,y^{\\prime})\\in A$   \n772 for all $x,x^{\\prime}\\in{\\mathrm{supp}}(\\mu),y,y^{\\prime}\\in{\\mathrm{supp}}(\\nu)$ . ", "page_idx": 27}, {"type": "text", "text": "Thus, the restriction of $L$ on $A^{2}$ , denoted as $L_{A^{2}}$ , is continuous on $A^{2}$ , and thus it is bounded. So, consider ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{m}:=\\operatorname*{max}_{r_{1},r_{2}\\in A}(L(r_{1},r_{2}))\\geq L(d_{X}(x,x^{\\prime}),d_{Y}(y,y^{\\prime})),\\quad\\forall x,x^{\\prime}\\in\\operatorname{supp}(\\mu),y,y^{\\prime}\\in\\operatorname{supp}(\\nu).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "773 Suppose $2\\lambda\\geq\\mathrm{m}+1$ , and assume that there exists a optimal $\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu)$ such that $|\\gamma|<|\\mu|$ . By   \n774 Lemma E.1, there exists $\\gamma^{\\prime}$ such that $\\gamma\\leq\\gamma^{\\prime},(\\pi_{1})_{\\#}\\gamma^{\\prime}=\\mu$ . Thus, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C(\\gamma^{\\prime};\\lambda,\\mu,\\nu)-C(\\gamma;\\lambda,\\mu,\\nu)=\\displaystyle\\int_{(X\\times Y)}L(d_{X}(x,x^{\\prime}),d_{Y}(y,y^{\\prime}))-2\\lambda\\,d((\\gamma^{\\prime})^{\\otimes2}-(\\gamma)^{\\otimes2})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\int_{(X\\times Y)}\\operatorname*{m}-2\\lambda\\,d((\\gamma^{\\prime})^{\\otimes2}-(\\gamma)^{\\otimes2})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=-(|\\gamma^{\\prime}|^{2}-|\\gamma|^{2})=-(|\\mu|^{2}-|\\gamma|^{2})<0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "775 which is contradiction since $\\gamma$ is optimal, and so we have completed the proof. ", "page_idx": 27}, {"type": "text", "text": "776 Lemma E.3. Consider probability mm-spaces $\\mathbb{X}\\,=\\,(X,d_{X},\\mu)$ , $\\mathbb{Y}\\,=\\,(Y,d_{Y},\\nu)$ , that is, with   \n777 $|\\mu|=|\\nu|=1$ . Then, for each $\\lambda>0$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\nP G W_{\\lambda,q}^{L}(\\mathbb{X},\\mathbb{Y})\\leq G W_{q}^{L}(\\mathbb{X},\\mathbb{Y}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "778 Proof. In this setting, we have $\\Gamma(\\mu,\\nu)\\subset\\Gamma_{\\leq}(\\mu,\\nu)$ , and thus ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P G W_{\\lambda,q}^{L}(\\mathbb{X},\\mathbb{Y})}\\\\ &{=\\underset{\\mathrm{\\tiny~F\\in\\Gamma_{\\leq}(\\mu,\\nu)}}{\\operatorname*{inf}}\\int_{(X\\times Y)^{2}}L(d_{X}^{q}(x,x^{\\prime}),d_{Y}^{q}(y,y^{\\prime}))d\\gamma^{\\otimes2}+\\lambda(|\\mu|^{2}+|\\nu|^{2}-2|\\gamma|^{2})}\\\\ &{\\leq\\underset{\\gamma\\in\\Gamma(\\mu,\\nu)}{\\operatorname*{inf}}\\int_{(X\\times Y)^{2}}L(d_{X}^{q}(x,x^{\\prime}),d_{Y}^{q}(y,y^{\\prime}))+\\lambda(|\\mu|^{2}+|\\nu|^{2}-2|\\gamma|^{2})d\\gamma^{\\otimes2}}\\\\ &{=\\underset{\\gamma\\in\\Gamma(\\mu,\\nu)}{\\operatorname*{inf}}\\int_{(X\\times Y)^{2}}L(d_{X}^{q}(x,x^{\\prime}),d_{Y}^{q}(y,y^{\\prime}))d\\gamma^{\\otimes2}}\\\\ &{=G W_{q}^{L}(\\mathbb{X},\\mathbb{Y}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "779 ", "page_idx": 27}, {"type": "text", "text": "780 Based on the above properties, we can now prove Proposition 3.5: ", "page_idx": 27}, {"type": "text", "text": "Proposition E.4 (Generalization of Proposition 3.5). Consider general probability mm-spaces $\\mathbb{X}=(X,d_{X},\\mu),\\,\\mathbb{Y}=(Y,d_{Y},\\nu),$ , that is, with $|\\mu|=|\\nu|=1$ , where $X,Y$ are bounded. Assume that $L$ is continuous. Then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\lambda\\to\\infty}P G W_{\\lambda,q}^{L}(\\mathbb{X},\\mathbb{Y})=G W_{q}^{L}(\\mathbb{X},\\mathbb{Y}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "781 Proof. When $\\lambda$ is sufficiently large, by Lemma E.2, for each optimal $\\gamma_{\\lambda}\\in\\Gamma_{\\leq}(\\mu,\\nu)$ of the minimiza  \n782 tion problem $P G W_{\\lambda,q}^{L}(\\mathbb{X},\\mathbb{Y})$ , we have $|\\gamma_{\\lambda}|=\\operatorname*{min}(|\\mu|,|\\nu|)=1$ . That is, $\\gamma_{\\lambda}\\in\\Gamma(\\mu,\\nu)$ . Plugging ", "page_idx": 27}, {"type": "text", "text": "783 $\\gamma_{\\lambda}$ into $C(\\gamma_{\\lambda};\\lambda,\\mu,\\nu)$ , we obtain: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P G W_{\\lambda,q}^{L}(\\mathbb{X},\\mathbb{Y})=\\displaystyle\\int_{(X\\times Y)^{2}}L(d_{X}^{q}(x,x^{\\prime}),d_{Y}^{q}(y,y^{\\prime}))d\\gamma_{\\lambda}^{\\otimes2}+\\lambda(1^{2}+1^{2}-2\\cdot1^{2})}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\int_{(X\\times Y)^{2}}L(d_{X}^{q}(x,x^{\\prime}),d_{Y}^{q}(y,y^{\\prime}))d\\gamma_{\\lambda}^{\\otimes2}\\geq G W(\\mathbb{X},\\mathbb{Y}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "784 By Lemma E.3, we also have $P G W_{\\lambda,q}^{L}(\\mathbb{X},\\mathbb{Y})\\leq G W_{q}^{L}(\\mathbb{X},\\mathbb{Y})$ and we complete the proof. ", "page_idx": 28}, {"type": "text", "text": "785 F Tensor Product Computation ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "786 Lemma F.1. Given a tensor $M\\,\\in\\,\\mathbb{R}^{n\\times m\\times n\\times n}$ and $\\gamma,\\gamma^{\\prime}\\,\\in\\,\\mathbb{R}^{n\\times m}$ , the tensor product operator   \n787 $M\\circ\\gamma$ satisfies the following: ", "page_idx": 28}, {"type": "text", "text": "788 (i) The mapping $\\gamma\\mapsto M\\circ\\gamma$ is linear with respect to $\\gamma$ . ", "page_idx": 28}, {"type": "text", "text": "(ii) If M is symmetric, in particular, $M_{i,j,i^{\\prime},j^{\\prime}}=M_{i^{\\prime},j^{\\prime},i,j},\\forall i,i^{\\prime}\\in[1:n],j,j^{\\prime}\\in[1:m]$ , then $\\langle M\\circ\\gamma,\\gamma^{\\prime}\\rangle_{F}=\\langle M\\circ\\gamma^{\\prime},\\gamma\\rangle_{F}$ . ", "page_idx": 28}, {"type": "text", "text": "789 Proof. ", "page_idx": 28}, {"type": "text", "text": "790 (i) For the first part, consider $\\gamma,\\gamma^{\\prime}\\in\\mathbb{R}^{n\\times m}$ and $k\\in\\mathbb{R}$ . For each $i,j\\in[1:n]\\times[1:m]$ , we   \n791 have we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle(M\\circ(\\gamma+\\gamma^{\\prime}))_{i j}=\\sum_{\\stackrel{i^{\\prime},j^{\\prime}}{i^{\\prime},j^{\\prime}}}M_{i,j^{\\prime},j^{\\prime}}(\\gamma+\\gamma^{\\prime})_{i^{\\prime}j^{\\prime}}}&{}\\\\ {=\\displaystyle\\sum_{\\stackrel{i^{\\prime},j^{\\prime}}{i^{\\prime},j^{\\prime}}}M_{i,j^{\\prime},j^{\\prime}}\\gamma_{i^{\\prime}j^{\\prime}}+\\displaystyle\\sum_{\\stackrel{i^{\\prime},j^{\\prime}}{i^{\\prime},j^{\\prime}}}M_{i,j^{\\prime},j^{\\prime}}\\gamma_{i^{\\prime}j^{\\prime}}^{\\prime}}&{}\\\\ {=(M\\circ\\gamma)_{i j}+(M\\circ\\gamma)_{i^{\\prime}j^{\\prime}},}&{}\\\\ {\\displaystyle(M\\circ(k\\gamma))_{i j}=\\sum_{\\stackrel{i^{\\prime},j^{\\prime}}{i^{\\prime},j^{\\prime}}}M_{i,j^{\\prime},j^{\\prime}}(k\\gamma)_{i j}}&{}\\\\ {=k\\displaystyle\\sum_{\\stackrel{i^{\\prime},j^{\\prime}}{i^{\\prime},j^{\\prime}}}M_{i,j^{\\prime},j^{\\prime}}\\gamma_{i j}}&{}\\\\ {=k(M\\circ\\gamma)_{i j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, $M\\circ(\\gamma+\\gamma^{\\prime})=M\\circ\\gamma+M\\circ\\gamma^{\\prime}$ and $M\\circ(k\\gamma)=k M\\circ\\gamma$ . Therefore, $\\gamma\\mapsto M\\circ\\gamma$ is linear. ", "page_idx": 28}, {"type": "text", "text": "794 (ii) For the second part, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle M\\circ\\gamma,\\gamma^{\\prime}\\rangle_{F}=\\displaystyle\\sum_{i j i^{\\prime}j^{\\prime}}M_{i,j,i^{\\prime},j^{\\prime},}\\gamma_{i j}\\gamma_{i^{\\prime}j^{\\prime}}^{\\prime}}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{i,j,i^{\\prime},j^{\\prime}}M_{i^{\\prime},j^{\\prime},i,j}\\gamma_{i^{\\prime},j^{\\prime}}\\gamma_{i,j}}\\\\ &{\\qquad\\qquad=\\langle M\\gamma^{\\prime},\\gamma\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where (49) follows from the fact that $M$ is symmetric. ", "page_idx": 28}, {"type": "text", "text": "796 ", "page_idx": 28}, {"type": "text", "text": "797 G Another Algorithm for Computing PGW Distance \u2013 Solver 2 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "798 Our Algorithm 2 for solving the proposed PGW problem is based on a theoretical result that relates   \n799 GW and PGW. The details of our computational method, as well as the proof of Proposition G.1 stated   \n800 below, are provided in Appendix G.1. Based on such proposition, we extend the PGW problem to a   \n801 discrete GW-variant problem (55), leading to a solution for the original PGW problem by truncating   \n802 the GW-variant solution.   \n803 Proposition G.1. Let $\\mathbb{X}\\,=\\,(X,d_{X},\\mu)$ be a mm-space. Consider an auxiliary point $\\hat{\\infty}$ and let   \n804 $\\hat{\\mathbb X}=(\\hat{X},d_{\\hat{X}},\\hat{\\mu})$ , where ${\\hat{X}}=X\\cup\\{{\\hat{\\infty}}\\}$ , $\\hat{\\mu}$ is constructed by (4), and considering $\\infty$ as an auxiliary   \n805 point to $\\mathbb{R}$ such that $x\\le\\infty$ for every $x\\in\\mathbb{R},$ , we extend $d_{X}$ into $d_{\\hat{X}}:{\\hat{X}}^{2}\\to\\mathbb{R}\\cup\\{\\infty\\}$ and define   \n806 $L_{\\lambda}:\\mathbb{R}\\cup\\{\\infty\\}\\rightarrow\\mathbb{R}$ as follows: ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "equation", "text": "$$\nd_{\\hat{X}}(\\boldsymbol{x},\\boldsymbol{x}^{\\prime})=\\left\\{\\!\\!\\begin{array}{l l}{d_{X}(\\boldsymbol{x},\\boldsymbol{x}^{\\prime})}&{i f\\,\\boldsymbol{x},\\boldsymbol{x}^{\\prime}\\in X}\\\\ {\\infty}&{o t h e r w i s e}\\end{array}\\!,L_{\\lambda}(r_{1},r_{2}):=\\left\\{\\!\\!\\begin{array}{l l}{L(r_{1},r_{2})-2\\lambda}&{i f\\,r_{1},r_{2}\\in\\mathbb{R}}\\\\ {0}&{e l s e w h e r e}\\end{array}\\!\\right..\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "807 Consider the following GW-variant2 problem: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\widehat{G W}^{L_{\\lambda}}(\\hat{\\mathbb{X}},\\hat{\\mathbb{Y}})=\\operatorname*{inf}_{\\hat{\\gamma}\\in\\Gamma(\\hat{\\mu},\\hat{\\nu})}\\hat{\\gamma}^{\\otimes2}\\big(L_{\\lambda}(d_{\\hat{X}}^{q},d_{\\hat{Y}}^{q})\\big)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "808 Then, when considering the bijection $\\gamma\\mapsto\\hat{\\gamma}$ defined in (6) we have that $\\gamma$ is optimal for PGW   \n809 problem (10) if and only if $\\hat{\\gamma}$ is optimal for the GW-variant problem (51). ", "page_idx": 29}, {"type": "text", "text": "810 Proof. The mapping $F$ defined by (6) well-defined bijection, as shown in[40, 12]. ", "page_idx": 29}, {"type": "text", "text": "811 Given $\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu)$ , we have $\\hat{\\gamma}=F(\\gamma)\\in\\Gamma(\\hat{\\mu},\\hat{\\nu})$ . Let $\\hat{C}(\\hat{\\gamma};\\mu,\\nu)$ denote the transportation cost in   \n812 the GW-variant problem (51), that is, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\hat{C}(\\hat{\\gamma};\\mu,\\nu):=\\int_{(\\hat{X}\\times\\hat{Y})^{2}}L_{\\lambda}(d_{\\hat{X}}^{q}(x,x^{\\prime}),d_{\\hat{Y}}^{q}(y,y^{\\prime}))\\,d\\hat{\\gamma}(x,y)d\\hat{\\gamma}(x^{\\prime},y^{\\prime})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "813 Then, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C(\\gamma;\\lambda,\\mu,\\nu)}\\\\ &{\\quad=\\int_{(X\\times Y)^{2}}(L(a_{X}^{q}(x,x^{\\prime}),a_{Y}^{q}(y,y^{\\prime}))-2\\lambda)\\,d\\gamma^{\\otimes2}+\\underbrace{\\lambda(|\\mu|+|\\nu|)}_{\\mathrm{dess~not~dept~\\alpha}_{0}}}\\\\ &{\\quad=\\int_{(X\\times Y)^{2}}(L(a_{X}^{q}(x,x^{\\prime}),a_{Y}^{q}(y,y^{\\prime}))-2\\lambda)\\,d\\gamma^{\\otimes2}+\\lambda(|\\mu|+|\\nu|)\\quad\\ \\ (\\mathrm{since}\\ \\hat{\\gamma}|_{X\\times Y}=\\gamma)}\\\\ &{\\quad=\\int_{(X\\times Y)^{2}}(L(a_{X}^{q}(x,x^{\\prime}),a_{Y}^{q}(y,y^{\\prime}))-2\\lambda)\\,d\\gamma^{\\otimes2}+\\lambda(|\\mu|+|\\nu|)\\quad\\ (\\mathrm{as}\\ d_{X}|x_{X}x=\\alpha_{X},\\ d_{Y}|_{Y\\times Y}=\\hat{\\gamma}|_{X})}\\\\ &{\\quad=\\int_{(X\\times Y)^{2}}L_{\\lambda}(a_{X}^{q}(x,x^{\\prime}),a_{Y}^{q}(y,y^{\\prime}))\\,d\\gamma^{\\otimes2}+\\lambda(|\\mu|+|\\nu|)\\quad\\mathrm{(since}\\ \\hat{L}|_{\\mathbb{R}\\times\\mathbb{R}}(\\cdot,\\cdot)=(L(\\cdot,\\cdot)-2\\lambda))}\\\\ &{\\quad=\\int_{(\\hat{X}\\times\\hat{\\mathcal{X}})^{2}}L_{\\lambda}(a_{\\hat{X}}^{q}(x,x^{\\prime}),a_{Y}^{q}(y,y^{\\prime}))\\,d\\hat{\\gamma}^{\\otimes2}+\\ \\underbrace{\\lambda(|\\mu|+|\\nu|)}_{\\mathrm{dess~since~dence}\\ \\hat{L}}_{\\mathrm{dessigns~0}}\\quad\\mathrm{(since~\\hat{L}~a s s i g n s~0\\omega\\hat{\\omega})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "814 Combining this with the fact that $F:\\gamma\\mapsto{\\hat{\\gamma}}$ is a bijection, we have that $\\gamma$ is optimal for (10) if   \n815 and only if $\\hat{\\gamma}$ is optimal for (51). Under the assumptions of Proposition 3.3, there exists an optimal   \n816 $\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu)$ for the PGW problem exists, and so we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\hat{\\gamma}\\in\\Gamma(\\hat{\\mu},\\hat{\\nu})}\\hat{C}(\\hat{\\gamma};\\mu,\\nu)=\\arg\\operatorname*{min}_{\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu)}C(\\gamma;\\lambda,\\mu,\\nu).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "817 ", "page_idx": 29}, {"type": "text", "text": "818 Remark G.2. Both algorithms (Algorithm $^{\\,l}$ , and 2) are mathematically and computationally   \n819 equivalent, owing to the equivalence between the POT problem in Solver 1 and the OT problem in   \n820 Solver 2. ", "page_idx": 29}, {"type": "text", "text": "821 G.1 Frank-Wolfe for the PGW Problem \u2013 Solver 2 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "822 Similarly to the discrete PGW problem (15), consider the discrete version of (4): ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{p}}=[\\mathbf{p};|\\mathbf{q}|]\\in\\mathbb{R}^{n+1},\\quad\\hat{\\mathbf{q}}=[\\mathbf{q};|\\mathbf{p}|]\\in\\mathbb{R}^{m+1},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Algorithm 2: Frank-Wolfe Algorithm for partial GW, ver 2 ", "page_idx": 30}, {"type": "text", "text": "Input: $\\begin{array}{r}{\\mu=\\sum_{i=1}^{n}p_{i}^{X}\\delta_{x_{i}},\\nu=\\sum_{j=1}^{m}q_{j}^{Y}\\delta_{y_{j}},\\gamma^{(1)}}\\end{array}$   \nOutput: \u03b3(final)   \nCompute $\\mathcal{C}^{X},C^{Y},\\hat{\\mathrm{p}},\\hat{\\mathrm{q}},\\hat{\\gamma}^{(1)}$   \nfor $k=1,2,\\dots$ do $\\hat{G}^{(k)}\\gets2\\hat{M}\\circ\\hat{\\gamma}^{(k)}$ // Compute gradient $\\hat{\\gamma}^{(k)^{\\prime}}\\gets\\arg\\operatorname*{min}_{\\hat{\\gamma}\\in\\Gamma(\\hat{\\mathfrak{p}},\\hat{\\mathfrak{q}})}\\langle\\hat{G}^{(k)},\\hat{\\gamma}\\rangle_{F}$ // Solve the OT problem Compute $\\alpha^{(k)}\\in[0,1]$ via (56), (18) // Line search $\\hat{\\gamma}^{(k+1)}\\gets(1-\\alpha^{(k)})\\hat{\\gamma}^{(k)^{\\prime}}+\\alpha\\hat{\\gamma}^{(k)}//\\prime$ Update $\\hat{\\gamma}$ if convergence, break   \nend for   \n$\\gamma^{(f i n a l)}\\leftarrow\\hat{\\gamma}^{(k)}[1:n,1:m]$ ", "page_idx": 30}, {"type": "text", "text": "823 and, in a similar fashion, we define $\\hat{M}\\in\\mathbb{R}^{(n+1)\\times(m+1)\\times(n+1)\\times(m+1)}$ as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\hat{M}_{i,j,i^{\\prime},j^{\\prime}}=\\binom{\\tilde{M}_{i,j,i^{\\prime},j^{\\prime}}}{0}\\quad\\mathrm{if~}i,i^{\\prime}\\in[1:n],j,j^{\\prime}\\in[1:m],\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "824 Then, the GW-variant problem (51) can be written as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\widehat{G W}(\\hat{\\mathbb{X}},\\hat{\\mathbb{Y}})=\\operatorname*{min}_{\\hat{\\gamma}\\in\\Gamma(\\hat{\\mathbb{p}},\\hat{\\mathfrak{q}})}\\mathcal{L}_{\\hat{M}}(\\hat{\\gamma}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "825 Based on Proposition G.1 (which relates $P G W_{\\lambda}^{L}(\\cdot,\\cdot)$ with $\\widehat{G W}(\\cdot,\\cdot))$ , we propose two versions of   \n826 the Frank-Wolfe algorithm [31] that can solve the PGW problem (15). Apart from Algorithm 1 in   \n827 [45], which solves a different formulation of partial GW, and Algorithm 1 in [44], which applies the   \n828 Sinkhorn algorithm to solve an entropic regularized version of (8), to the best of our knowledge, a   \n829 precise computational method for the discrete PGW problem (15) has not been studied.   \n830 Here, we discuss another version of the FW Algorithm for solving the PGW problem (15). The main   \n831 idea relies on solving first the GW-variant problem (51), and, at the end of the iterations, by using   \n832 Proposition G.1, convert the solution of the GW-variant problem to a solution for the original partial   \n833 GW problem (15).   \n834 First, construct $\\hat{\\mathrm{p}},\\hat{\\mathrm{q}},\\hat{\\mathrm{M}}$ as described in Proposition G.1. Then, for each iteration $k$ , perform the   \n835 following three steps. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "836 Step 1: Computation of gradient and optimal direction. Solve the OT problem: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\hat{\\gamma}^{(k)^{\\prime}}\\gets\\mathrm{arg}\\operatorname*{min}_{\\hat{\\gamma}\\in\\Gamma(\\hat{\\mathfrak{p}},\\hat{\\mathfrak{q}})}\\langle\\mathcal{L}_{\\hat{M}}(\\hat{\\gamma}^{(k)}),\\hat{\\gamma}\\rangle_{F}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "837 The gradient $\\mathcal{L}_{\\hat{M}}(\\gamma^{(k)})$ can be computed in a similar way as described in Lemma H.2. We refer to   \n838 Section $\\mathrm{H}$ for details. ", "page_idx": 30}, {"type": "text", "text": "Step 2: Line search method. Find optimal step size $\\alpha^{(k)}$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\alpha^{(k)}=\\arg\\operatorname*{min}_{\\alpha\\in[0,1]}\\{\\mathcal{L}_{\\hat{M}}((1-\\alpha)\\hat{\\gamma}^{(k)}+\\alpha\\hat{\\gamma}^{(k)^{\\prime}})\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "839 Similar to Solver 1, let ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\,\\,\\delta\\hat{\\gamma}^{(k)}=\\hat{\\gamma}^{(k)^{\\prime}}-\\hat{\\gamma}^{(k)},}\\\\ {\\,\\,\\boldsymbol{a}=\\langle\\hat{M}\\circ\\delta\\hat{\\gamma}^{(k)},\\delta\\hat{\\gamma}^{(k)}\\rangle_{F},}\\\\ {\\,\\,\\boldsymbol{b}=2\\langle\\hat{M}\\circ\\delta\\hat{\\gamma}^{(k)},\\hat{\\gamma}^{(k)}\\rangle_{F}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "840 Then the optimal $\\alpha^{(k)}$ is given by formula (18). See Appendix $\\textbf{J}$ for a detailed discussion. ", "page_idx": 30}, {"type": "text", "text": "841 Step 3. Update $\\hat{\\gamma}^{(k+1)}\\gets(1-\\alpha^{(k)})\\hat{\\gamma}^{(k)}+\\alpha^{(k)}\\hat{\\gamma}^{(k)^{\\prime}}.$ . ", "page_idx": 30}, {"type": "text", "text": "842 H Gradient Computation in Algorithms 1 and 2 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "843 In this section, we discuss the computation of Gradient $\\nabla\\mathcal{L}_{\\tilde{M}}(\\gamma)$ in Algorithm 1 and $\\nabla\\mathcal{L}_{\\hat{M}}(\\hat{\\gamma})$ in   \n844 Algorithm 2. ", "page_idx": 31}, {"type": "text", "text": "845 Proposition H.1 (Proposition 1 [41]). If the cost function can be written as ", "page_idx": 31}, {"type": "equation", "text": "$$\nL(r_{1},r_{2})=f_{1}(r_{1})+f_{2}(r_{2})-h_{1}(r_{1})h_{2}(r_{2})\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "846 then ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{M\\circ\\gamma=u(C^{X},C^{Y},\\gamma)-h_{1}(C^{X})\\gamma h_{2}(C^{Y})^{\\top},}\\\\ {,\\gamma):=f_{1}(C^{X})\\gamma_{1}\\boldsymbol{1}_{m}^{\\top}+\\boldsymbol{1}_{n}\\gamma_{2}^{\\top}f_{2}(C^{Y}).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "847 where ", "page_idx": 31}, {"type": "text", "text": "848 Additionally, the following lemma builds the connection between $\\tilde{M}\\circ\\gamma$ and $M\\circ\\gamma$ . ", "page_idx": 31}, {"type": "text", "text": "849 Lemma H.2. For any $\\gamma\\in\\mathbb{R}^{n\\times m}$ , we have: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\tilde{M}\\circ\\gamma=M\\circ\\gamma-2\\lambda|\\gamma|1_{n,m}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "850 Proof. For any $\\gamma\\in\\mathbb{R}^{n\\times m}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\tilde{M}\\circ\\gamma=(M1_{n,n,m,m}-2\\lambda)\\circ\\gamma}}\\\\ &{=(M-2\\lambda1_{n,n,m,m})\\circ\\gamma}\\\\ &{=M\\circ\\gamma-2\\lambda1_{n,m,n,m}\\circ\\gamma}\\\\ &{=M\\circ\\gamma-2(\\langle1_{n,m},\\gamma\\rangle_{F})1_{n,m}}\\\\ &{=M\\circ\\gamma-2\\lambda|\\gamma|1_{n,m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "851 where the second equality follows from Lemma F.1. ", "page_idx": 31}, {"type": "text", "text": "852 Next, in the setting of Algorithm 2, for any $\\hat{\\gamma}\\in\\mathbb{R}^{(n+1)\\times(m+1)}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{L}_{\\hat{M}}(\\hat{\\gamma})=2\\hat{M}\\circ\\hat{\\gamma}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "853 and $\\hat{M}\\circ\\hat{\\gamma}$ can be computed by the following lemma. ", "page_idx": 31}, {"type": "text", "text": "854 Lemma H.3. For each $\\hat{\\gamma}\\in\\mathbb{R}^{(n+1)\\times(m+1)}$ , we have $\\hat{M}\\circ\\hat{\\gamma}\\in\\mathbb{R}^{(n+1)\\times(m+1)}$ with the following: ", "page_idx": 31}, {"type": "equation", "text": "$$\n(\\hat{M}\\circ\\hat{\\gamma})_{i j}=\\left\\{\\binom{(\\tilde{M}\\circ\\hat{\\gamma}[1:n,1:m])_{i j}}{0}}\\right.\\ \\ i f i\\in[1:n],j\\in[1:m]}\\\\ {\\left.e l s e w h e r e\\right.\\,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "855 Proof. Recall the definition of $\\hat{M}$ is given by (54), choose $i\\in[1:n],j\\in[1:m]$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle(\\hat{M}\\circ\\hat{\\gamma})_{i j}=\\sum_{i^{\\prime}=1}^{n}\\sum_{j^{\\prime}=1}^{m}\\hat{M}_{i,j,i^{\\prime},j^{\\prime}}\\hat{\\gamma}_{i^{\\prime},j^{\\prime}}+\\sum_{j^{\\prime}=1}^{m}\\hat{M}_{i,j,n+1,j}\\hat{\\gamma}_{n+1,j^{\\prime}}+\\sum_{i^{\\prime}=1}^{n}\\hat{M}_{i,j,i^{\\prime},m+1}\\hat{\\gamma}_{i,m+1}}}\\\\ {{\\displaystyle\\qquad\\qquad+\\hat{M}_{i,j,n+1,m+1}\\hat{\\gamma}_{n+1,m+1}}}\\\\ {{\\displaystyle=\\sum_{i^{\\prime}=1}^{n}\\sum_{j^{\\prime}=1}^{m}\\hat{M}_{i,j,i^{\\prime},j^{\\prime}}\\hat{\\gamma}_{i^{\\prime},j^{\\prime}}+0+0+0=\\sum_{i^{\\prime}=1}^{n}\\sum_{j^{\\prime}=1}^{m}\\tilde{M}_{i,j,i^{\\prime},j^{\\prime}}\\hat{\\gamma}_{i^{\\prime},j^{\\prime}}}}\\\\ {{\\displaystyle=(\\tilde{M}\\circ(\\hat{\\gamma}[1:n,1:m]))_{i j}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "856 If $i=n+1$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n(\\hat{M}\\circ\\hat{\\gamma})_{n+1,j}=\\sum_{i^{\\prime}=1}^{n+1}\\sum_{j^{\\prime}=1}^{m+1}\\hat{M}_{n+1,j,i^{\\prime},j^{\\prime}}\\hat{\\gamma}_{i^{\\prime},j^{\\prime}}=0\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "857 Similarly, $(\\hat{M}\\circ\\hat{\\gamma})_{i,m+1}=0$ . Thus, we complete the proof. ", "page_idx": 31}, {"type": "text", "text": "858 I Line Search in Algorithm 1 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "859 In this section, we discuss the derivation of the line search algorithm. ", "page_idx": 32}, {"type": "text", "text": "860 We observe that in the partial GW setting, for each $\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu)$ , the marginals of $\\gamma$ are not fixed.   \n861 Thus, we can not directly apply the classical algorithm (e.g. [43]).   \n862 In iteration $k$ , let $\\gamma^{(k)},\\gamma^{(k)^{\\prime}}$ be the previous and new transportation plans from step 1 of the algorithm.   \n863 For convenience, we denote them as $\\gamma,\\gamma^{\\prime}$ , respectively. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "864 The goal is to solve the following problem: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\alpha\\in[0,1]}\\mathcal{L}(\\tilde{M},(1-\\alpha)\\gamma+\\alpha\\gamma^{\\prime})\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\mathcal{L}(\\tilde{M},\\gamma)=\\langle\\tilde{M}\\circ\\gamma,\\gamma\\rangle_{F}$ . By denoting $\\delta\\gamma=\\gamma^{\\prime}-\\gamma$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\tilde{M},(1-\\alpha)\\gamma+\\alpha\\gamma^{\\prime})=\\mathcal{L}(\\tilde{M},\\gamma+\\alpha\\delta\\gamma).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "865 Then, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\tilde{M}\\circ(\\gamma+\\alpha\\delta\\gamma),(\\gamma+\\alpha\\delta\\gamma)\\rangle_{F}}\\\\ &{=\\langle\\tilde{M}\\circ\\gamma,\\gamma\\rangle_{F}+\\alpha\\left(\\langle\\tilde{M}\\circ\\gamma,\\delta\\gamma\\rangle_{F}+\\langle\\tilde{M}\\circ\\delta\\gamma,\\gamma\\rangle_{F}\\right)+\\alpha^{2}\\langle\\tilde{M}\\circ\\delta\\gamma,\\delta\\gamma\\rangle_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "866 Let ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a=\\langle\\tilde{M}\\circ\\delta\\gamma,\\delta\\gamma\\rangle_{F},}\\\\ &{b=\\langle\\tilde{M}\\circ\\gamma,\\delta\\gamma\\rangle_{F}+\\langle\\tilde{M}\\circ\\delta\\gamma,\\gamma\\rangle_{F}=2\\langle\\tilde{M}\\circ\\gamma,\\delta\\gamma\\rangle_{F},}\\\\ &{c=\\langle\\tilde{M}\\circ\\gamma,\\gamma\\rangle_{F},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "867 where the second identity in (63) follows from Lemma F.1 and the fact that $\\tilde{M}=M1_{n,n,m,m}\\textrm{--}$   \n868 $2\\lambda1_{n,m,n,m}$ is symmetric. ", "page_idx": 32}, {"type": "text", "text": "Therefore, the above problem (62) becomes ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\alpha\\in[0,1]}a\\alpha^{2}+b\\alpha+c.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "869 The solution is the following: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\alpha^{*}=\\left\\{\\!\\!\\begin{array}{l l}{1}&{\\mathrm{if}\\;a\\leq0,a+b\\leq0,}\\\\ {0}&{\\mathrm{if}\\;a\\leq0,a+b>0,}\\\\ {\\mathrm{clip}(\\frac{-b}{2a},[0,1])}&{\\mathrm{if}\\;a>0,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathrm{clip}(\\frac{-b}{2a},[0,1])=\\mathrm{min}\\left\\{1,\\mathrm{max}\\{0,\\frac{-b}{2a}\\}\\right\\}=\\left\\{\\begin{array}{l l}{\\frac{-b}{2a}}&{\\mathrm{if~}\\frac{-b}{2a}\\in[0,1],}\\\\ {0}&{\\mathrm{if~}\\frac{-b}{2a}<0,}\\\\ {1}&{\\mathrm{if~}\\frac{-b}{2a}>1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "870 We can further discuss the difference in computation of $a$ and $b$ in PGW setting and the classical GW   \n871 setting. If the assumption in Proposition H.1 holds, by (58) and (59), we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a=\\langle\\tilde{M}\\circ\\delta\\gamma,\\delta\\gamma\\rangle_{F}}\\\\ &{\\quad=\\langle(M\\circ\\delta\\gamma-2\\lambda|\\delta\\gamma|I_{n,m}),\\delta\\gamma\\rangle_{F}}\\\\ &{\\quad=\\langle M\\circ\\delta\\gamma,\\delta\\gamma\\rangle_{F}-2\\lambda|\\delta\\gamma|^{2}}\\\\ &{\\quad=\\langle u(C^{X},C^{Y},\\delta\\gamma)-h_{1}(C^{X})\\delta\\gamma h_{2}(C^{Y})^{\\top},\\delta\\gamma\\rangle_{F}-2\\lambda|\\delta\\gamma|^{2},}\\\\ &{b=2\\langle\\tilde{M}\\circ\\gamma,\\delta\\gamma\\rangle_{F}}\\\\ &{\\quad=2\\langle M\\circ\\gamma-2\\lambda|\\gamma|I_{n,m},\\delta\\gamma\\rangle}\\\\ &{\\quad=2(\\langle M\\circ\\gamma,\\delta\\gamma\\rangle_{F}-2\\lambda|\\delta\\gamma||\\gamma|)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "872 Note that in the classical GW setting [43], the term $u(C^{X},C^{Y},\\delta\\gamma)=0_{n\\times m}$ and $|\\delta\\gamma|=0$ . Therefore,   \n873 in such line search algorithm (Algorithm 2 in [43]), the terms $u(C^{X},C^{Y},\\delta\\gamma),2\\lambda|\\delta\\gamma|1_{n\\times m}$ are not   \n874 required. In addition, in equation (66), $M\\circ\\gamma,2\\lambda|\\gamma|$ have been computed in the gradient computation   \n875 step, thus these two terms can be directly applied in this step. ", "page_idx": 32}, {"type": "text", "text": "876 J Line Search in Algorithm 2 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "877 Similar to the previous section, in iteration $k$ , let $\\hat{\\gamma}^{(k)},\\hat{\\gamma}^{(k)^{\\prime}}$ denote the previous transportation plan   \n878 and the updated transportation plan. For convenience, we denote them as $\\hat{\\gamma},\\hat{\\gamma}^{\\prime}$ , respectively. ", "page_idx": 33}, {"type": "text", "text": "879 Let $\\delta\\hat{\\gamma}=\\hat{\\gamma}-\\hat{\\gamma}^{\\prime}$ . ", "page_idx": 33}, {"type": "text", "text": "880 The goal is to find the following optimal $\\alpha$ : ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\alpha=\\arg\\operatorname*{min}_{\\alpha\\in[0,1]}\\mathcal{L}(\\hat{M},(1-\\alpha)\\hat{\\gamma},\\alpha\\hat{\\gamma}^{\\prime})=\\arg\\operatorname*{min}_{\\alpha\\in[0,1]}\\mathcal{L}(\\hat{M},\\alpha\\delta\\hat{\\gamma}+\\hat{\\gamma}),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "881 where $\\hat{M}\\ \\in\\ \\mathbb{R}^{(n+1)\\times(m+1)\\times(n+1)\\times(m+1)}$ , with $\\hat{M}[1\\,:\\,n,1\\,:\\,m,1\\,:\\,n,1\\,:\\,m]\\,=\\,\\tilde{M}\\,=\\,M\\,-$   \n882 $2\\lambda1_{n\\times m\\times n\\times m}$ . ", "page_idx": 33}, {"type": "text", "text": "883 Similar to the previous section, let ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a=\\langle\\hat{M}\\circ\\delta\\hat{\\gamma},\\delta\\hat{\\gamma}\\rangle_{F},}\\\\ &{b=\\langle\\hat{M}\\circ\\delta\\hat{\\gamma},\\hat{\\gamma}\\rangle_{F}+\\langle\\hat{M}\\circ\\hat{\\gamma},\\delta\\hat{\\gamma}\\rangle_{F}=2\\langle\\hat{M}\\circ\\delta\\hat{\\gamma},\\hat{\\gamma}\\rangle_{F},}\\\\ &{c=\\langle\\hat{M}\\circ\\hat{\\gamma},\\hat{\\gamma}\\rangle_{F},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "884 where (68) holds since $\\hat{M}$ is symmetric. Then, the optimal $\\alpha$ is given by (64). ", "page_idx": 33}, {"type": "text", "text": "885 It remains to discuss the computation. By Lemma F.1, we set $\\gamma=\\hat{\\gamma}[1:n,1:m],\\delta\\gamma=\\delta\\hat{\\gamma}[1:n,1:$   \n886 $m]$ . Then, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a=\\langle(\\hat{M}\\circ\\delta\\hat{\\gamma})[1:n,1:m],\\delta\\gamma\\rangle_{F}=\\langle(\\tilde{M}\\circ\\delta\\gamma,\\delta\\gamma\\rangle_{F},}\\\\ &{b=\\langle(\\hat{M}\\circ\\delta\\hat{\\gamma})[1:n,1:m],\\gamma\\rangle_{F}=\\langle(\\tilde{M}\\circ\\delta\\gamma,\\gamma)_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "887 Thus, we can apply (65), (66) to compute $a,b$ in this setting by plugging in $\\gamma=\\hat{\\gamma}[1:n,1:m]$ and   \n888 $\\delta\\gamma=\\delta\\hat{\\gamma}[1:n,\\bar{1}:\\stackrel{}{m}]$ . ", "page_idx": 33}, {"type": "text", "text": "889 K Convergence ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "890 As in [45] we will use the results from [32] on the convergence of the Frank-Wolfe algorithm for   \n891 non-convex objective functions. ", "page_idx": 33}, {"type": "text", "text": "892 Consider the minimization problems ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\gamma\\in\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{q})}\\mathcal{L}_{\\tilde{M}}(\\gamma)\\qquad\\mathrm{and}\\qquad\\operatorname*{min}_{\\hat{\\gamma}\\in\\Gamma(\\hat{\\mathbb{P}},\\hat{\\mathrm{q}})}\\mathcal{L}_{\\hat{M}}(\\hat{\\gamma})\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "893 that corresponds to the discrete partial GW problem, and the discrete GW-variant problem (used in   \n894 version 2), respectively. The objective functions $\\gamma\\mapsto{\\mathcal{L}}_{\\hat{M}}(\\gamma)={\\tilde{M}}\\gamma^{\\otimes2}$ (where $\\tilde{M}=M-2\\lambda1_{n,m}$   \n895 for a fixed matrix $M\\,\\in\\,\\mathbb{R}^{n\\times m}$ and $\\lambda\\,>\\,0$ ), and $\\hat{\\gamma}\\,\\mapsto\\,{\\mathcal L}_{\\hat{M}}(\\hat{\\gamma})\\,=\\,\\hat{M}\\hat{\\gamma}^{\\otimes2}$ (where $\\hat{M}$ is given by   \n896 (54)) are non-convex in general (for $\\lambda\\,>\\,0$ , the matrices $\\tilde{M}$ and $\\hat{M}$ symmetric but not positive   \n897 semi-definite), but the constraint sets $\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{q})$ and $\\Gamma(\\hat{\\mathrm{p}},\\hat{\\mathrm{q}})$ are convex and compact on $\\mathbb{R}^{n\\times m}$ (see   \n898 Proposition B.2 [53]) and on $\\mathbb{R}^{(n+1)\\times(m\\!+\\!1)}$ , respectively.   \n899 From now on we will concentrate on the first minimization problem in (69) and the convergence   \n900 analysis for the second one will be analogous. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "901 Consider the Frank-Wolfe gap of $\\mathcal{L}_{\\tilde{M}}$ at the approximation $\\gamma^{(k)}$ of the optimal plan $\\gamma$ : ", "page_idx": 33}, {"type": "equation", "text": "$$\ng_{k}=\\operatorname*{min}_{\\gamma\\in\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{q})}\\langle\\nabla\\mathcal{L}_{\\tilde{M}}(\\gamma^{(k)}),\\gamma^{(k)}-\\gamma\\rangle_{F}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "902 It provided a good criterion to measure the distance to a stationary point at iteration $k$ . Indeed, a plan   \n903 $\\gamma^{(k)}$ is a stationary transportation plan for the corresponding constrained optimization problem in   \n904 (69) if and only if $g_{k}=0$ . Moreover, $g_{k}$ is always non-negative $(g_{k}\\ge0)$ ).   \n905 From Theorem 1 in [32], after $K$ iterations we have the following upper bound for the minimal   \n906 Frank-Wolf gap: ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "equation", "text": "$$\n\\tilde{g}_{K}:=\\operatorname*{min}_{1\\leq k\\leq K}g_{k}\\leq\\frac{\\operatorname*{max}\\{2L_{1},D_{L}\\}}{\\sqrt{K}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where ", "page_idx": 34}, {"type": "equation", "text": "$$\nL_{1}:=\\mathcal{L}_{\\tilde{M}}(\\gamma^{(1)})-\\operatorname*{min}_{\\gamma\\in\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{q})}\\mathcal{L}_{\\tilde{M}}(\\gamma)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "907 is the initial global suboptimal bound for the initialization $\\gamma^{(1)}$ of the algorithm, and $D_{L}:=\\mathrm{Lip}\\;\\cdot$   \n908 $(\\mathrm{diam}(\\Gamma_{\\le}(\\mathrm{p,\\bar{q})}))^{2}$ , where Lip is the Lipschitz constant of $\\nabla{\\mathcal{L}}_{\\tilde{M}}$ and diam $(\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{q}))$ is the $\\|\\cdot\\|_{F}$   \n909 diameter of $\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{q})$ in $\\mathbb{R}^{n\\times m}$ .   \n910 The important thing to notice is that the constant $\\operatorname*{max}\\{2L_{1},D_{L}\\}$ does\u221a not depend on the iteration   \n911 step $k$ . Thus, according to Theorem 1 in [32], the rate on $\\tilde{g}_{K}$ is $\\mathcal{O}(1/\\sqrt{K})$ . That is, the algorithm   \n912 takes at most ${\\mathcal{O}}(1/\\varepsilon^{2})$ iterations to find an approximate stationary point with a gap smaller than $\\varepsilon$ .   \n913 Finally, we adapt Lemma 1 in Appendix B.2 in [45] to our case characterizing the convergence   \n914 guarantee, precisely, determining such a constant $\\operatorname*{max}\\{2L_{1},D_{L}\\}$ in (71). Essentially, we will   \n915 estimate upper bounds for the Lipschitz constant Lip and for the diameter diam $(\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{q}))$ . ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "916 ", "page_idx": 34}, {"type": "text", "text": "917 ", "page_idx": 34}, {"type": "text", "text": "\u2022 Let us start by considering the diameter of the couplings of $\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{q})$ with respect to the Frobenious norm $\\|\\cdot\\|_{F}$ . By definition, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{diam}(\\Gamma_{\\le}(\\mathrm{p},\\mathrm{q})):=\\operatorname*{sup}_{\\gamma,\\gamma^{\\prime}\\in\\Gamma_{\\le}(\\mathrm{p},\\mathrm{q})}\\|\\gamma-\\gamma^{\\prime}\\|_{F}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For any $\\gamma\\in\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{q})$ , since $\\gamma_{1}\\leq\\mathrm{p}$ and $\\gamma_{2}\\leq\\mathrm{q}$ , we obtain that, in particular, $|\\gamma_{1}|\\le|{\\mathrm{p}}|$ and $|\\gamma_{2}|\\leq|\\mathbf{q}|$ . Thus, since $|\\gamma_{1}|=|\\gamma|=|\\gamma_{2}|$ (recall that $\\gamma_{1}=\\pi_{1\\#}\\gamma$ and $\\gamma_{2}=\\pi_{2\\#}\\gamma)$ we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left|\\gamma\\right|\\leq\\operatorname*{min}\\{|\\mathrm{p}|,|\\mathrm{q}|\\}=:\\sqrt{s}\\qquad\\forall\\gamma\\in\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{q}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "918 ", "page_idx": 34}, {"type": "text", "text": "Thus, given $\\gamma,\\gamma^{\\prime}\\in\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{q})$ , we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\gamma-\\gamma^{\\prime}\\|_{F}^{2}\\leq2\\|\\gamma\\|_{F}^{2}+2\\|\\gamma^{\\prime}\\|_{F}^{2}=2\\displaystyle\\sum_{i,j}(\\gamma_{i,j})^{2}+2\\displaystyle\\sum_{i,j}(\\gamma_{i,j}^{\\prime})^{2}}\\\\ {\\displaystyle\\leq2\\left(\\displaystyle\\sum_{i,j}|\\gamma_{i,j}|\\right)^{2}+2\\left(\\displaystyle\\sum_{i,j}|\\gamma_{i,j}^{\\prime}|\\right)^{2}=2|\\gamma|^{2}+2|\\gamma^{\\prime}|^{2}\\leq4s}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "919 ", "page_idx": 34}, {"type": "text", "text": "920 ", "page_idx": 34}, {"type": "text", "text": "(essentially, we used that $\\|\\cdot\\|_{F}$ is the 2-norm for matrices viewed as vectors, that $\\big|\\cdot\\big|$ is the 1-norm for matrices viewed as vectors, and the fact that $\\|\\cdot\\|_{2}\\leq\\|\\cdot\\|_{1})$ . As a result, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{diam}(\\Gamma_{\\le}(\\mathrm{p},\\mathrm{q}))\\le2\\sqrt{s},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "921   \n922   \n923 ", "page_idx": 34}, {"type": "text", "text": "where $s$ only depends on $\\mathrm{p}$ and q that are fixed weight vectors in $\\mathbb{R}_{+}^{n}$ and $\\mathbb{R}_{+}^{m}$ , respectively. \u2022 Now, let us analyze the Lipschitz constant of $\\nabla\\mathcal{L}_{\\hat{M}}$ with respect to $\\|\\cdot\\|_{F}$ . For any $\\gamma,\\gamma^{\\prime}\\in$ $\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{q})$ we have, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla C_{\\widehat{\\mathbf{A}}}(\\gamma)-\\nabla C_{\\widehat{\\mathbf{A}}}(\\gamma)\\|_{2}^{2}}\\\\ &{=\\|\\widetilde{\\mathbf{A}}\\gamma\\sigma_{\\gamma}-\\widehat{\\mathbf{A}}\\gamma\\sigma_{\\gamma}\\gamma\\|_{2}^{2}}\\\\ &{=\\|\\widetilde{\\mathbf{A}}\\left(-2\\lambda\\right)\\sigma_{\\gamma}\\left(\\gamma-\\gamma\\right)\\|_{2}^{2}}\\\\ &{=\\big(\\|\\overline{{M}}-2\\lambda\\big)\\sigma_{\\gamma}\\left(\\gamma-\\gamma\\right)\\big[\\lambda\\big]-2\\lambda\\big)\\sigma\\left(\\gamma-\\gamma\\right)\\big[\\gamma\\sigma_{\\gamma}\\big]}\\\\ &{=\\displaystyle\\sum_{k=0}^{T}\\Big(\\big(M-2\\lambda\\big)\\sigma\\left(\\gamma-\\gamma\\right)\\big)_{\\gamma,k}\\sigma^{2}}\\\\ &{=\\displaystyle\\sum_{k=0}^{T}\\Bigg(\\sum_{\\gamma=0}^{T}\\Big(M_{\\gamma,k,\\sigma,\\gamma}-2\\lambda\\big)(\\gamma\\sigma_{\\gamma,k}-\\gamma\\sigma_{\\gamma,k}^{\\prime}\\Big)\\Bigg)^{2}}\\\\ &{\\leq\\left(\\frac{\\gamma\\sigma_{\\gamma,k}}{\\delta\\gamma\\sigma_{\\gamma,k}}\\big(M_{\\gamma,k,\\sigma,\\gamma}-2\\lambda\\big)\\right)^{2}\\left(\\sum_{k=0}^{T}\\Big(\\sum_{\\gamma=\\gamma}^{T}\\big(\\gamma\\sigma_{\\gamma,k}-\\gamma\\sigma_{\\gamma,k}^{\\prime}\\big)\\sigma\\Big)\\right)^{2}}\\\\ &{=\\operatorname*{max}(M)-2\\lambda\\gamma^{2}\\left(\\sum_{k=0}^{T}\\|\\gamma\\sigma_{\\gamma}-\\gamma\\|_{2}^{2}\\right)}\\\\ &{\\leq m m\\operatorname{(max)}(M)-2\\lambda\\gamma^{2}\\left(\\sum_{k=0}^{T}\\gamma\\sigma_{\\gamma,k}^{\\prime}\\right)\\|_{2}\\gamma-\\gamma\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "924 Hence, the Lipschitz constant of the gradient of $\\mathcal{L}_{\\tilde{M}}$ is by ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathrm{Lip}\\leq\\sqrt{n m}\\left|\\operatorname*{max}_{i,j,i^{\\prime},j^{\\prime}}\\{M_{i,j,i^{\\prime},j^{\\prime}}\\}-2\\lambda\\right|.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "925 In the particular case where $L(r_{1},r_{2})=|r_{1}-r_{2}|^{2}$ we have $M_{i,j,i^{\\prime},j^{\\prime}}=|C_{i,i^{\\prime}}^{X}-C_{j,j^{\\prime}}^{Y}|^{2}$ (as in (14))   \n926 where $C^{X}$ , $C^{Y}$ are given $n\\times n$ and $m\\times m$ non-negative symmetric matrices defined in (11), that   \n927 depend on the given discrete mm-spaces $\\mathbb{X}$ and $\\mathbb{Y}$ . Here, we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i,j,i^{\\prime},j^{\\prime}}\\{M_{i,j,i^{\\prime},j^{\\prime}}\\}=\\operatorname*{max}_{i,j,i^{\\prime},j^{\\prime}}\\{|C_{i,i^{\\prime}}^{X}-C_{j,j^{\\prime}}^{Y}|^{2}\\}\\leq\\left((\\operatorname*{max}_{i,i^{\\prime}}\\{C_{i,i^{\\prime}}^{X}\\})^{2}+(\\operatorname*{max}_{j,j^{\\prime}}\\{C_{j,j^{\\prime}}^{Y}\\})^{2}\\right)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "928 and so the Lipschitz constant verifies ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathrm{Lip}\\leq\\sqrt{n m}\\left|((\\operatorname*{max}(C^{X})^{2}+\\operatorname*{max}(C^{Y})^{2})-2\\lambda\\right|\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "929 Combining all together, we obtain that after $K$ iterations, the minimal Frank-Wolf gap verifies ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{g}_{K}=\\underset{1\\leq k\\leq K}{\\operatorname*{min}}g_{k}\\leq\\frac{\\operatorname*{max}\\{2L_{1},4s\\sqrt{n m}\\,\\vert\\operatorname*{max}_{i,j,i^{\\prime},j^{\\prime}}\\{M_{i,j,i^{\\prime},j^{\\prime}}\\}-2\\lambda\\vert\\}}{\\sqrt{K}}~~~~~~~~~~~~~~~~~~}\\\\ {\\leq2\\frac{\\operatorname*{max}\\{L_{1},2s\\sqrt{n m}\\,\\vert(\\operatorname*{max}(C^{X})^{2}+\\operatorname*{max}(C^{Y})^{2})-2\\lambda\\vert\\}}{\\sqrt{K}}~~~~~~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "930 where $L_{1}$ dependents on the initialization of the algorithm. ", "page_idx": 35}, {"type": "text", "text": "931 Finally, we mention that there is a dependence in the constant $\\operatorname*{max}\\{2L_{1},D_{L}\\}$ on the number of   \n932 points $\\acute{n}$ and $m$ ) of our discrete spaces $X=\\{x_{1},\\ldots x_{n}\\}$ and $Y=\\{y_{1},\\ldots,y_{m}\\}$ which was not   \n933 pointed out in [45]. ", "page_idx": 35}, {"type": "text", "text": "934 L Related Work: Mass-Constrained Partial Gromov-Wasserstein ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "935 Partial Gromov-Wasserstein is first introduced in [45]. To distinguish the PGW problem in [45] and   \n936 the PGW problem in this paper, we call the former one the Mass-Constrained Gromov-Wasserstein   \n937 problem (MPGW): ", "page_idx": 35}, {"type": "equation", "text": "$$\nM P G W_{\\rho}(\\mathbb{X},\\mathbb{Y}):=\\operatorname*{inf}_{\\gamma\\in\\Gamma_{\\leq}^{\\rho}(\\mu,\\nu)}\\gamma^{\\otimes2}(L(d_{X}^{q},d_{Y}^{q})),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "938 where $\\rho\\in[0,\\operatorname*{min}\\{|\\mu|,|\\nu|\\}]$ , and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\Gamma_{\\leq}^{\\rho}(\\mu,\\nu):=\\{\\gamma\\in\\mathcal{M}_{+}(X\\times Y):\\gamma_{1}\\leq\\mu,\\,\\gamma_{2}\\leq\\nu,\\,\\,|\\gamma|=\\rho\\}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "939 Unlike the relation between Partial OT and OT, it is not rigorous to say that the PGW and the MPGW   \n940 problems are equivalent, since the objective function ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\gamma\\mapsto\\int_{(X\\times Y)^{2}}L(d_{X}^{2}(x,x^{\\prime}),d_{Y}^{2}(y,y^{\\prime}))d\\gamma^{\\otimes2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "941 is not a convex function even if $(r_{1},r_{2})\\mapsto L(r_{1},r_{2})$ is convex [37]: (If the problems were convex,   \n942 MPGW, as the $^{\\ast}L$ agrangian formulation\u2019 of PGW\u2014adding the constraint of PGW in the functional   \n943 \u00e0 la Lagrange Multipliers\u2014 would be equivalent to PGW. However, since these problems are not   \n944 convex, we cannot claim that they are equivalent in principle.)   \n945 We can still investigate their relation by the following lemma, based on which we design the wall-clock   \n946 time experiment in Section O.   \n947 Proposition L.1. Suppose $\\gamma\\in\\Gamma_{\\leq}(\\mu,\\nu)$ is optimal for $P G W_{\\lambda}(\\mathbb{X},\\mathbb{Y})$ . Let $\\rho=|\\gamma|$ , we have $\\gamma$ is   \n948 also optimal in $M P G W_{\\rho}(\\mathbb{X},\\mathbb{Y})$ . ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "949 Proof. Pick $\\gamma^{\\prime}\\in\\Gamma_{\\leq}^{\\rho}(\\mu,\\nu)\\subset\\Gamma_{\\leq}(\\mu,\\nu)$ , since $\\gamma$ is optimal in $P G W_{\\lambda}(\\mu,\\nu)$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{0\\leq C(\\gamma;\\lambda,\\mu,\\nu)-C(\\gamma^{\\prime};\\lambda,\\mu,\\nu)}}\\\\ {{\\mathrm{}}}\\\\ {{\\mathrm{}}=\\displaystyle\\int_{(X\\times Y)^{2}}L(d_{X}^{2}(x,x^{\\prime}),d_{Y}^{2}(y,y^{\\prime}))d(\\gamma^{\\otimes2}-\\gamma^{\\prime\\otimes2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "950 Thus, $\\gamma$ is optimal in $\\Gamma_{\\le}^{\\rho}(\\mu,\\nu)$ for $M P G W_{\\rho}(\\mathbb{X},\\mathbb{Y})$ and we complete the proof. ", "page_idx": 35}, {"type": "text", "text": "951 At first glance, the formulations of the MPGW (73) and the PGW (10) problems could be thought to   \n952 be equivalent since tuning the hyper-parameter $\\lambda$ for controlling the total mass in the PGW problem   \n953 is quite similar in spirit to the approach in [45] (MPGW) which instead constrains the total mass of $\\gamma$   \n954 by the hyper-parameter $\\rho$ . However, since classical GW and its variants (e.g. UPGW, PGW, MPGW)   \n955 are not convex problems, mathematically this equivalence relation is not verified.   \n956 We first notice that the \"Lagrangian form\" of the MPGW problem (73) is our PGW formulation   \n957 (10) by considering $2\\lambda$ be the \"Lagrange variable\" of constraint $-|\\gamma|^{2}+\\rho^{2}\\le0$ . However, as said   \n958 before, the equivalence is not direct as the cost functional (75) is not convex. In fact, he MPGW   \n959 problem does not give rise to a metric, while our PGW formulation gives rise to a metric as shown in   \n960 Proposition 3.4. We will show this through the following example. In fact, we will see that by using   \n961 the MPGW formulation we cannot distinguish different mm-spaces, while with our PGW we can   \n962 discriminate different mm-spaces. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "Example: Consider the following three mm-spaces ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{X}_{1}=(\\mathbb{R}^{3},\\lVert\\cdot\\rVert,\\sum_{i=1}^{1000}\\alpha\\delta_{x_{i}}),\\quad\\mathbb{X}_{2}=(\\mathbb{R}^{3},\\lVert\\cdot\\rVert,\\sum_{i=1}^{800}\\alpha\\delta_{x_{i}}),\\quad\\mathbb{X}_{3}=(\\mathbb{R}^{3},\\lVert\\cdot\\rVert,\\sum_{i=1}^{400}\\alpha\\delta_{x_{i}}),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "963 where $\\alpha>0$ is the mass of each point. For numerical stability reasons, we set $\\alpha=1/1000$ . On the   \n964 one hand, if we compute MPGW, the mass is fixed to be a value $\\rho\\in[0,0.4]$ , since the total mass in   \n965 $\\mathbb{X}_{3}$ is 0.4. For our experiment, we set $\\rho=0.4$ , and we observe: ", "page_idx": 36}, {"type": "equation", "text": "$$\nM P G W_{\\rho}(\\mathbb{X}_{1},\\mathbb{X}_{2};\\rho=0.4)=M P G W_{\\rho}(\\mathbb{X}_{2},\\mathbb{X}_{3};\\rho=0.4)=M P G W_{\\rho}(\\mathbb{X}_{1},\\mathbb{X}_{3};\\rho=0.4)=0\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "966 On the other hand, if we compute our PGW, considering any $\\lambda>0$ , (in particular, we set $\\lambda=10$ ),   \n967 we obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P G W_{\\lambda}(\\mathbb{X}_{1},\\mathbb{X}_{2};\\lambda=10)=3.6}\\\\ {P G W_{\\lambda}(\\mathbb{X}_{2},\\mathbb{X}_{3};\\lambda=10)=4.8}\\\\ {P G W_{\\lambda}(\\mathbb{X}_{1},\\mathbb{X}_{3};\\lambda=10)=8.4}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "968 In particular, one can verify the triangular inequality. ", "page_idx": 36}, {"type": "text", "text": "969 As a conclusion, in this example, MPGW can not describe the dissimilarity of any two datasets taken   \n970 from $\\{{\\mathbb X}_{1},{\\mathbb X}_{2},{\\mathbb X}_{3}\\}$ . They are three distinct datasets, but MPGW returns zero for each pair. On the   \n971 contrary, our PGW can measure dissimilarity.   \n972 In addition, the discrepancy provided by our PGW formulation is consistent with the follow  \n973 ing intuitive observation: One expects the dissimilarity between $\\mathbb{X}_{1}$ and $\\mathbb{X}_{3}$ to be larger than   \n974 the difference $\\mathbb{X}_{1}$ and $\\mathbb{X}_{2}$ , and than the difference between $\\mathbb{X}_{1}$ and $\\mathbb{X}_{2}$ . This is because we   \n975 are considering discrete measures, with the same mass at each point concentrated on the sets   \n976 $\\{x_{1},\\ldots,x_{400}\\}^{-}\\subset\\{x_{1},\\ldots,x_{400},\\ldots,x_{800}\\}\\subset\\{x_{1},\\ldots,x_{400},\\ldots,\\bar{x_{800}},\\ldots,x_{1000}\\}$ for the datasets   \n977 $\\mathbb{X}_{3},\\mathbb{X}_{2},\\mathbb{X}_{1}$ , respectively. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "978 M Partial Gromov-Wasserstein Barycenter ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "979 We first introduce the classical Gromov-Wasserstein problem [41]: Consider finite discrete probability   \n980 measures \u00b51, . . . , \u00b5K, where \u00b5k =  in=k1 pik \u03b4xik and each xik \u2208 Rdk for some dk \u2208 N. Let   \n981 $C^{k}=[\\|x_{i}^{k}-x_{i^{\\prime}}^{k}\\|^{2}]_{i,i^{\\prime}\\in[1:n_{k}]}$ and $\\mathrm{p}^{k}=[p_{1}^{k},\\cdot\\cdot\\cdot,\\bar{p_{n_{k}}^{k}}]^{\\top}$ . Given $\\mathrm{p}\\in\\mathbb{R}_{+}^{n}$ with $|\\mathrm{p}|=1$ for some $n\\in\\mathbb N$   \n982 and $\\xi_{1},\\dots,\\xi_{K}\\ge0$ with $\\begin{array}{r}{\\sum_{k=1}^{K}\\xi_{k}=1}\\end{array}$ , the GW barycenter problem is defined by: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{C,\\gamma^{k}}\\sum_{k=1}^{K}\\xi_{k}\\langle L(C,C^{k})\\circ\\gamma^{k},\\gamma^{k}\\rangle,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "983 where the minimization is over all matrices $C\\in\\mathbb{R}^{n\\times n},\\gamma^{k}\\in\\Gamma(\\mathrm{p},\\mathrm{p}^{k}),\\forall k\\in[1:K].$ ", "page_idx": 36}, {"type": "text", "text": "984 Similarly, we can extend the above definition into PGW setting. In particular, we relax the assumptions   \n985 $|\\mathrm{p}|=1$ and $|\\mathrm{p}^{k}|=1$ for each $k\\in[1:K]$ . Given $\\lambda_{1},\\dots,\\lambda_{K}>0$ , the PGW barycenter is the follow   \n986 problem: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{C,\\gamma_{k}}\\sum_{k}\\xi_{k}\\langle M(C,C^{k})\\circ\\gamma^{k},\\gamma^{k}\\rangle-2\\lambda_{k}|\\gamma^{k}|^{2}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "987 where each $\\gamma^{k}\\in\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{p}^{k})$ . ", "page_idx": 37}, {"type": "text", "text": "988 The problem (77) can be solved iterative by two steps: ", "page_idx": 37}, {"type": "text", "text": "Minimization with respect to $C$ : For each $k$ , we solve the PGW problem ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\gamma^{k}\\in\\Gamma_{\\leq}(p,p^{k})}\\langle M(C,C^{k})\\circ\\gamma^{k},\\gamma^{k}\\rangle-2\\lambda_{k}|\\gamma^{k}|^{2}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "989 via solver 1 or 2. ", "page_idx": 37}, {"type": "text", "text": "990 Minimization with respect to $\\{\\gamma^{k}\\}_{k}$ : ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{C}\\sum_{k}\\xi_{k}\\langle M(C,C^{k})\\circ\\gamma^{k},\\gamma^{k}\\rangle\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "991 Note, we can ignore the $-2\\lambda_{k}|\\gamma^{k}|^{2}$ terms as $\\gamma^{k}$ is fixed in this case. ", "page_idx": 37}, {"type": "text", "text": "992 It has closed form solution due to the following lemma and proposition: Lemma M.1. Given matrices $A\\in\\mathbb{R}^{n,m},B\\in\\mathbb{R}^{m,l},C\\in\\mathbb{R}^{n,l},$ , let ", "page_idx": 37}, {"type": "equation", "text": "$$\n{\\mathcal{L}}=\\langle A B,C\\rangle,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "993 then $\\begin{array}{r}{{\\frac{d{\\mathcal{L}}}{d A}}=C B^{\\top}}\\end{array}$ ", "page_idx": 37}, {"type": "text", "text": "994 Proof. For any $i\\in[1:n],j\\in[1:m]$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{d{\\mathcal{L}}}{d A_{i j}}:=\\sum_{i^{\\prime},j^{\\prime}}\\frac{d}{d A_{i j}}C_{i^{\\prime},j^{\\prime}}(A B)_{i^{\\prime},j^{\\prime}}}}\\\\ {{\\displaystyle=\\sum_{i^{\\prime},j^{\\prime}}C_{i^{\\prime},j^{\\prime}}\\frac{d(\\sum_{k}A_{i^{\\prime},k}B_{k,j^{\\prime}})}{d A_{i j}}}}\\\\ {{\\displaystyle=\\sum_{j^{\\prime}}C_{i,j^{\\prime}}B_{k,j^{\\prime}}=(C B^{\\top})_{i j}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "995 ", "page_idx": 37}, {"type": "text", "text": "996 Proposition M.2. If $L$ satisfies (57), and $f_{1}^{\\prime}/h_{1}^{\\prime}$ is invertible, then (78) can be solved by ", "page_idx": 37}, {"type": "equation", "text": "$$\nC=\\left(\\frac{f_{1}^{\\prime}}{h_{1}^{\\prime}}\\right)^{-1}\\left(\\frac{\\sum_{k}\\xi_{k}\\gamma^{k}h_{2}(C^{k})(\\gamma_{k})^{\\top}}{\\sum_{k}\\xi_{k}\\gamma_{1}^{k}(\\gamma_{1}^{k})^{\\top}}\\right),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where ", "page_idx": 37}, {"type": "equation", "text": "$$\n{\\frac{A}{B}}=\\left[{\\frac{A_{i j}}{B_{i j}}}\\right]_{i j},w i t h\\;c o n\\nu e n t i o n\\;{\\frac{0}{0}}=0.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "997 Special case: $i f|\\mathrm{p}|\\leq|\\mathrm{p}^{k}|,\\forall k,$ , when $\\lambda$ is sufficiently large, (79) and [41, Proposition 3] coincide. ", "page_idx": 37}, {"type": "text", "text": "998 Proof. From Proposition H.1, the objective in (78) becomes ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{L}=\\sum_{k}\\xi_{k}\\langle f_{1}(C)\\gamma_{1}^{1}1_{n_{k}}^{\\top}+1_{n}(\\gamma_{2}^{k})^{\\top}f_{2}(C^{k})-h_{1}(C)\\gamma^{k}h_{2}(C^{k})^{\\top},\\gamma^{k}\\rangle}\\\\ {\\displaystyle\\qquad=\\sum_{k}\\xi_{k}\\langle f_{1}(C)\\gamma_{1}^{1}1_{n_{k}}^{\\top},\\gamma^{k}\\rangle+\\sum_{k}\\xi_{k}\\langle1_{n}(\\gamma_{2}^{k})^{\\top}f_{2}(C^{k}),\\gamma^{k}\\rangle-\\sum_{k}\\xi_{k}\\langle h_{1}(C)\\gamma^{k}h_{2}(C^{k})^{\\top},\\gamma^{k}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "999 We set $\\begin{array}{r}{\\frac{d\\mathcal{L}}{d C}=0}\\end{array}$ . From Lemma M.1, we have: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{0=\\displaystyle\\frac{d{\\mathcal{L}}}{d C}}\\\\ {\\quad=\\displaystyle\\sum_{k}\\xi_{k}f_{1}^{\\prime}(C)\\odot\\gamma^{k}1_{n_{k}}(\\gamma_{1}^{k})^{\\top}-\\displaystyle\\sum_{k}\\xi_{k}h_{1}^{\\prime}(C)\\odot\\gamma^{k}h_{2}(C^{k})(\\gamma^{k})^{\\top}}\\\\ {\\quad=f_{1}^{\\prime}(C)\\odot\\displaystyle\\sum_{k}\\xi_{k}\\gamma^{k}1_{n_{k}}(\\gamma_{1}^{k})^{\\top}-h_{1}^{\\prime}(C)\\odot\\displaystyle\\sum_{k}\\xi_{k}\\gamma^{k}h_{2}(C^{k})(\\gamma^{k})^{\\top}}\\\\ {\\quad=f_{1}^{\\prime}(C)\\odot\\displaystyle\\sum_{k}\\xi_{k}\\gamma_{1}^{k}(\\gamma_{1}^{k})^{\\top}-h_{1}^{\\prime}(C)\\odot\\displaystyle\\sum_{k}\\xi_{k}\\gamma^{k}h_{2}(C^{k})(\\gamma^{k})^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "1000 We claim $\\textstyle{\\frac{A}{B}}$ is well-defined, i.e., if $B_{i j}=0$ , then $A_{i j}=0$ . ", "page_idx": 38}, {"type": "text", "text": "1001 For each $i,j\\in[1:n]$ , if $B_{i j}=0$ , we have two cases:   \n1002 Case $1\\colon\\forall k\\in[1:K]$ , we have $\\gamma_{1}^{k}[i]=0$ .   \n1003 Thus, $\\gamma^{k}[i,:]=0_{n_{k}}^{\\top}$ . So $A[i,:]=(\\gamma^{k}h_{2}(C^{k})(\\gamma^{k})^{\\top})[i,:]=0_{n_{k}}^{\\top}.$   \n1004 Case $\\geq:\\forall k\\in[1:K]$ , we have $\\gamma_{1}^{k}[j]=0$ .   \n1005 It implies $(\\gamma^{k})^{\\perp}[:,j]=0_{n}$ , thus $A[:,j]=(\\gamma^{k}h_{2}(C^{k}))(\\gamma^{k})^{\\top}[:,j]=0_{n_{k}}$ . Therefore, $A_{i j}=0$ .   \n1006 Thus $\\textstyle{\\frac{A}{B}}$ is well-defined.   \n1007 In addition, in these two cases, if we change the value $C_{i j}^{k}$ , $\\mathcal{L}$ will not change.   \n1008 From (80), we have: ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left(\\frac{f_{1}^{\\prime}}{h_{1}^{\\prime}}(C)\\right)_{i j}=\\frac{\\left(\\sum_{k}\\xi_{k}\\gamma^{k}h_{2}(C^{k})(\\gamma^{k})^{\\top}\\right)_{i j}}{\\left(\\sum_{k}\\xi_{k}\\gamma_{1}^{k}(\\gamma_{1}^{k})^{\\top}\\right)_{i j}}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "1009 if $B_{i j}>0$ . In addition, if $B_{i j}=0$ , there is no constraint for $C_{i j}$ . ", "page_idx": 38}, {"type": "text", "text": "1010 Combining it with the fact that if $B_{i,j}=0$ , then $C_{i,j}$ has no effect on $\\mathcal{L}$ . Thus, we have the following is a solution: ", "page_idx": 38}, {"type": "equation", "text": "$$\nC=\\left(\\frac{f_{1}^{\\prime}}{h_{1}^{\\prime}}\\right)^{-1}\\left(\\frac{\\sum_{k}\\xi_{k}\\gamma^{k}h_{2}(C^{k})(\\gamma^{k})^{\\top}}{\\sum_{k}\\xi_{k}\\gamma_{1}^{k}(\\gamma_{1}^{k})^{\\top}}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "1011 In particular case: $|\\mathrm{p}|\\leq|\\mathrm{p}^{k}|,\\forall k$ , suppose $\\lambda>\\operatorname*{max}\\{c^{2}:c\\in\\bigcup_{k}C^{k}\\cup C\\}$ , by lemma E.1, we have   \n1012 for each $k,|\\gamma^{k}|=\\operatorname*{min}(|\\mathrm{p}|,|\\mathrm{p}|^{k})=|\\mathrm{p}|$ , that is $\\gamma_{1}^{k}=\\mathrm{p}$ . ", "page_idx": 38}, {"type": "text", "text": "1013 Thus, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{k}\\xi_{k}\\gamma_{1}^{k}(\\gamma_{1}^{1})^{\\top}=\\sum_{k}\\xi_{k}\\gamma_{1}^{k}(\\gamma_{1}^{k})^{\\top}=\\sum_{k}\\xi_{k}\\mathrm{pp}^{\\top}=\\mathrm{pp}^{\\top}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "1014 Thus, $\\begin{array}{r}{C=\\left(\\frac{f_{1}^{\\prime}}{h_{1}^{\\prime}}\\right)^{-1}\\left(\\frac{\\sum_{k}\\xi_{k}\\gamma^{k}h_{2}(C^{k})(\\gamma^{k})^{\\top}}{\\mathrm{pp}^{\\top}}\\right)}\\end{array}$ ", "page_idx": 38}, {"type": "text", "text": "1015 Remark M.3. In $l^{2}$ loss case, i.e. $L(r_{1},r_{2})=|r_{1}-r_{2}|^{2}$ , (79) becomes ", "page_idx": 38}, {"type": "equation", "text": "$$\nC=\\frac{\\sum_{k}\\xi_{k}\\gamma^{k}C^{k}(\\gamma^{k})^{\\top}}{\\sum_{k}\\xi_{k}\\gamma_{1}^{k}(\\gamma_{1}^{k})^{\\top}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Since in this case, we can set ", "page_idx": 38}, {"type": "equation", "text": "$$\nf_{1}(x)=x^{2},f_{2}(y)=y^{2},h_{1}(x)=2x,h_{2}(y)=y.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "1016 Thus $\\begin{array}{r}{\\frac{f_{1}^{\\prime}}{h_{1}^{\\prime}}(x)=\\frac{2x}{2}=x}\\end{array}$ and $\\left(\\frac{f_{1}^{\\prime}}{h_{1}^{\\prime}}\\right)^{-1}(x)=x$ . Therefore, (79) becomes (81). ", "page_idx": 38}, {"type": "text", "text": "Algorithm 3: Partial Gromov-Wasserstein Barycenter ", "page_idx": 39}, {"type": "text", "text": "Input: $\\{C^{k},\\mathrm{p}^{k},\\lambda_{k}\\}_{k=1}^{K},\\mathrm{p}$   \nOutput: $C$   \nInitialize $C$ .   \nfor $i=1,2,\\dots$ do compute $\\begin{array}{r}{\\gamma^{k}\\leftarrow\\arg\\operatorname*{min}_{\\gamma\\in\\Gamma_{\\leq}(\\mathbf{p},\\mathbf{p}^{k})}\\langle\\boldsymbol{Z}(C,C^{k})-2\\lambda_{k},\\gamma\\rangle,\\forall k\\in[1:K].}\\end{array}$ Update $C$ by (79). if convergence, break   \nend for ", "page_idx": 39}, {"type": "text", "text": "Algorithm 4: Mass-Constrained Partial Gromov-Wasserstein Barycenter Input: $\\{C^{k},\\mathrm{p}^{k},\\lambda_{k}\\}_{k=1}^{K},\\mathrm{p}$ Output: $C$ Initialize $C$ . for $i=1,2,\\dots$ do compute $\\begin{array}{r}{\\gamma^{k}\\leftarrow\\arg\\operatorname*{min}_{\\gamma\\in\\Gamma_{\\leq}^{\\rho_{k}}(\\mathbf{p},\\mathbf{p}^{k})}\\langle\\mathcal{L}(C,C^{k}),\\gamma\\rangle,\\forall k\\in[1:K].}\\end{array}$ Update $C$ by (79). if convergence, break end for ", "page_idx": 39}, {"type": "text", "text": "Similarly, we can also extend the above PGW Barycenter into the MPGW setting: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{C,\\gamma^{k}}\\sum_{k=1}^{K}\\xi_{k}\\langle L(C,C^{k})\\circ\\gamma^{k},\\gamma^{k}\\rangle,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "1017 where, for each $k\\,\\in\\,[1\\,:\\,K]$ , $\\rho_{k}\\,\\in\\,[0,\\operatorname*{min}(|\\mathbf{p}|,|\\mathbf{p}^{k}|)]$ , and the optimization is over $C\\,\\in\\,\\mathbb{R}^{n}$ and   \n1018 $\\gamma_{k}\\in\\Gamma_{\\leq}^{\\rho_{k}}(\\mathrm{p},\\mathrm{p}^{k})$ for $k\\in[1:K]$ . ", "page_idx": 39}, {"type": "text", "text": "1019 It can be solved by the following algorithm 4. ", "page_idx": 39}, {"type": "image", "img_path": "nrcFNxF57E/tmp/224655ab2b63eebb3933698750dec28bace0fb9c410c878fda38905072034c78.jpg", "img_caption": [], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "Figure 4: We visualize the dataset in point cloud interpolation. The first row is the original images in Link. The second row is the point clouds obtained by the k-mean method, where $k=1024$ . ", "page_idx": 39}, {"type": "image", "img_path": "nrcFNxF57E/tmp/452d6705ded3ca09c697667ae0578a10c2e251d7bf6b0e9035cd8991733376d1.jpg", "img_caption": ["Figure 5: We test interpolation tasks in 3 scenarios: source data is clean, target data is selected from three cases as described in section dataset and data processing. In each scenario, we test $\\eta=$ $5\\%$ , $10\\%$ respectively. In the first column, we present the source and target point cloud visualization in each task. In columns 2-9, we present GW, PGW barycenter for $t=0/7,1/7,\\ldots,7/7$ . "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "1020 M.1 Details of Point Cloud Interpolation Experiment ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "1021 Dataset and data processing. We apply the dataset in [41] with download link. The original data are   \n1022 images, which we convert into a point cloud using the $\\mathbf{k}$ -mean algorithm, where $k=1024$ (see the   \n1023 second row of Figure 4).   \n1024 Suppose $\\mathcal{D}\\subset\\mathbb{R}^{2}$ is a region that contains these point clouds. Let $\\mathcal{R}\\subset\\mathbb{R}^{2}$ denote another region. In   \n1025 $\\mathcal{R}$ , we randomly select and add $n\\eta$ noise points to these point clouds. In particular, we consider noise   \n1026 corruption in the following three cases: ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "1027 Case $1\\colon\\mathcal{R}$ is a rectangle region which is disjoint to $\\mathcal{D}$ . See the third row in Figure 4. ", "page_idx": 41}, {"type": "text", "text": "1028 Case 2: $\\mathcal{R}=\\mathcal{R}_{1}\\cup\\mathcal{R}_{2}$ , where $\\mathcal{R}_{1},\\mathcal{R}_{2}$ are rectangles which are disjoint to $\\mathcal{D}$ . See the fourth row in   \n1029 Figure 4. ", "page_idx": 41}, {"type": "text", "text": "1030 Case $3;\\,\\mathcal{R}$ contains $\\mathcal{D}$ . See the fifth row in Figure 4. ", "page_idx": 41}, {"type": "text", "text": "1031 GW Barycenter and PGW Barycenter methods. We select $t_{1},\\ldots,t_{K}$ with $0=t_{1}<t_{2}<...<$   \n1032 $t_{K}=1$ . For each $t\\in\\{t_{1},\\ldots,t_{K}\\}$ , we compute the GW Barycenter ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{C,\\gamma^{1},\\gamma^{2}}(1-t)\\langle L(C,C^{1})\\circ\\gamma^{1},\\gamma^{1}\\rangle+t\\langle L(C,C^{2})\\circ\\gamma^{2},\\gamma^{2}\\rangle,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "1033 where $\\gamma_{1}~\\in~\\Gamma(\\mathrm{p},\\mathrm{p}^{1}),\\gamma_{2}~\\in~\\Gamma(\\mathrm{p},\\mathrm{p}^{2})$ . Apply Smacof-MDS to the minimizer $C$ , the resulting   \n1034 embedding, denoted as $X_{t}\\in\\mathbb{R}^{n\\times2}$ (where $n=1024_{\\mathrm{~\\rightmoon~}}$ ) is the GW-based interpolation. ", "page_idx": 41}, {"type": "text", "text": "1035 Replacing the GW Barycenter with the PGW Barycenter ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{C,\\gamma^{1},\\gamma^{2}}(1-t)(\\langle L(C,C^{1})\\circ\\gamma^{1},\\gamma^{1}\\rangle+\\lambda_{1}|\\gamma^{1}|^{2})+t(\\langle L(C,C^{2})\\circ\\gamma^{2},\\gamma^{2}\\rangle+\\lambda_{2}|\\gamma^{2}|),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "1036 where $\\lambda_{1},\\lambda_{2}>0,\\gamma^{1}\\in\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{p}^{1}),\\gamma^{2}\\in\\Gamma_{\\leq}(\\mathrm{p},\\mathrm{p}^{2})$ . Then we obtain PGW-based interpolation. ", "page_idx": 41}, {"type": "text", "text": "1037 Problem setup. We select one point cloud from the clean dataset denoted as $X=\\{x_{i}\\}_{i=1}^{n}$ (source   \n1038 point cloud), $n=1024$ .   \n1039 Next, we select one noise-corrupted point cloud, as described in Case 1, Case 2, and Case 3,   \n1040 respectively. In these three scenarios, we test $\\eta=0.5\\%$ and $\\eta=10\\%$ where $\\eta$ is the noise level.   \n1041 Therefore, we test $3*2=6$ different interpolation tasks for these two methods. The size of the target   \n1042 point cloud is then $m=n+n\\eta$ . See Figure 5 for details. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "Numerical details. In the GW-barycenter method, because of the balanced mass setting, we set ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathrm{p}^{1}=\\frac{1}{n}\\boldsymbol{1}_{n},\\mathrm{p}^{2}=\\frac{1}{m}\\boldsymbol{1}_{m},\\mathrm{p}=\\frac{1}{n}\\boldsymbol{1}_{n}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "In PGW-barycenter, we set ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathrm{p}^{1}=\\frac{1}{n}1_{n},\\mathrm{p}^{2}=\\frac{1}{n}1_{m},\\mathrm{p}=\\frac{1}{n}1_{n}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "1043 In addition, we set $\\lambda_{1},\\lambda_{2}$ such that $2\\lambda_{1},2\\lambda_{2}\\geq\\operatorname*{max}(\\operatorname*{max}(C_{1})^{2},\\operatorname*{max}(C_{2})^{2})$ . We compute GW/PGW   \n1044 barycenter for $t=0/7,1/7,\\ldots,7/7$ .   \n1045 In both GW and PGW barycenter algorithms, we set the largest number of iterations to be 100. The   \n1046 threshold for convergence is set to be 1e-5.   \n1047 Performance analysis. Each interpolation task is essentially unbalanced: the source point cloud   \n1048 contains clean data, while the target point cloud contains clean and noise points. We observe that in   \n1049 the first two scenarios, the interpolation derived from GW is clearly disturbed by the noise data points.   \n1050 For example, in rows $1,3,5,7$ , columns $t=1/7,2/7,3/7$ , we see that the point clouds reconstructed   \n1051 by MDS have significantly different width-height ratios from those of the source and target point   \n1052 clouds.   \n1053 In contrast, PGW is significantly less disturbed, and the interpolation is more natural. The width  \n1054 height ratio of the point clouds generated by the PGW barycenter is consistent with that of the   \n1055 source/target point clouds.   \n1056 In the third scenario, the noise data is uniformly selected from a large region that contains the domain   \n1057 of all clean point clouds. In this case, we observe that the GW and PGW barycenters perform similarly.   \n1058 However, at $t=1/7,2/7,4/7$ , GW-barycenters present more noise points than PGW-barycenters in   \n1059 the same truncated region.   \n1060 Limitations and future work. The main issue of the above GW/PGW techniques arises from the   \n1061 MDS method:   \n1062 Given minimizer $C\\,\\in\\,\\mathbb{R}^{n\\times n}$ of GW/PGW barycenter problem (82) (or (83)), MDS studies the   \n1063 following problem: ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{X\\in\\mathbb{R}^{n\\times d}}\\sum_{i,i^{\\prime}=1}^{n}\\left|C_{i,i^{\\prime}}^{1/2}-\\|X_{i}-X_{i^{\\prime}}\\|\\right|^{2}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "1064 Let $O(n)$ denote the set of all $n\\times n$ orthonormal matrices. Suppose $X^{*}$ is a minimizer, then $R X^{*}$ is   \n1065 also a minimizer for the above problem for all $R\\in O(n)$ .   \n1066 In practice, this means manually setting suitable rotation and flipping matrices for each method at   \n1067 each step, especially for the GW method.   \n1068 However, we understand that this issue stems from the inherent properties of the GW/PGW method.   \n1069 GW can be seen as a tool that describes the similarity between two graphs, which are rotation-invariant   \n1070 and flipping-invariant. Therefore, the GW/PGW barycenter essentially describes the interpolation   \n1071 between two graphs rather than two point clouds. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "1072 M.2 Details of Point Cloud Matching ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "1073 Dataset setup. In the Moon dataset (see link), we apply $n=200$ and set Gaussian variance to be 0.2.   \n1074 The outliers are sampled from region $[[-2,-1.5]\\times[-3.5,-3]]$ . ", "page_idx": 42}, {"type": "text", "text": "In the second experiment, the circle data is uniformly sampled from 2D circle ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{S}^{1}=\\{s\\in\\mathbb{R}^{2}:\\|s\\|^{2}=1\\}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and spherical data is uniformly sampled from 3D sphere ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{S}^{2}=\\{s+[0,0,4]\\in\\mathbb{R}^{2}:\\|s\\|^{2}=1\\},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "1075 where the shift $[0,0,4]$ is applied for visualization. ", "page_idx": 42}, {"type": "text", "text": "1076 We set sample size $n=200$ for both 2D and 3D samples.   \n1077 In both experiment, the number of outliers is $\\eta n=0.2n=40$ . ", "page_idx": 42}, {"type": "text", "text": "Numerical details. In GW, we normalize the two point clouds as ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{X}=(X,d_{X},\\sum_{i=1}^{n}\\frac{1}{n}\\delta_{x_{i}}),\\mathbb{Y}=(Y,d_{Y},\\sum_{j=1}^{n+n\\eta}\\frac{1}{n+n\\eta}\\delta_{y_{j}}).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "1078 In PGW, MPGW, UGW, we define the point clouds as ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{X}=(X,d_{X},\\sum_{i=1}^{n}\\frac{1}{n}\\delta_{x_{i}}),\\mathbb{Y}=(Y,d_{Y},\\sum_{j=1}^{n+n\\eta}\\frac{1}{n}\\delta_{y_{j}}).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "1079 In PGW, we choose $\\lambda$ such that $\\lambda\\geq\\operatorname*{max}(\\operatorname*{max}((C^{X})^{2}),\\operatorname*{max}((C^{Y})^{2}))$ , in particular, $\\lambda=10.0$ . ", "page_idx": 42}, {"type": "text", "text": "1080 In MPGW, we set $\\rho=1.0$ . ", "page_idx": 42}, {"type": "text", "text": "1081 In UGW, we set $\\rho_{1}=\\rho_{2}=10.0$ , $\\epsilon=0.05$ . ", "page_idx": 42}, {"type": "text", "text": "1082 N Details of Shape Retrieval Experiment ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "1083 Dataset details. We test two datasets in this experiment, which we refer to as Dataset I and Dataset   \n1084 II. We visualize Dataset I in Figure 6a and Dataset $\\mathrm{II}$ in Figure $^\\mathrm{6b}$ . The complete datasets can be   \n1085 accessed from the supplementary materials.   \n1086 Numerical details. We represent the shapes in each dataset as mm-spaces $\\mathbb{X}^{i}\\quad=$   \n1087 $\\begin{array}{r}{\\left(\\mathbb{R}^{2},\\|\\cdot\\|_{2},\\mu^{i}=\\sum_{k=1}^{n^{i}}\\alpha^{i}\\delta_{x_{k}^{i}}\\right)}\\end{array}$ . We use $\\textstyle\\alpha^{i}\\,=\\,{\\frac{1}{n^{i}}}$ to compute the GW distances for the balanced   \n1088 mass constraint setting. For the remaining distances, we set $\\begin{array}{r}{\\alpha=\\frac{1}{N}}\\end{array}$ = N1 , where N is the median number   \n1089 of points across all shapes in the dataset. For the SVM experiments, we use $\\exp(-\\sigma D)$ as the kernel   \n1090 for the SVM model, and we set $\\sigma=10$ for all distances. Moreover, we normalize the matrix $D$ to   \n1091 facilitate a fair comparison of each distance used, since the considered distance may have different   \n1092 scales. We note that the resulting kernel matrix is not necessarily positive semidefinite.   \n1093 In computing the pairwise distances, for the PGW method, we set $\\lambda$ such that $\\lambda\\,\\leq\\,\\lambda_{m a x}\\,=$   \n1094 $\\mathrm{max}_{i}$ $(|\\dot{C}^{i}|^{2})$ . In particular, we compute $\\lambda_{m a x}$ for each dataset and use $\\lambda\\ =\\ \\frac{1}{5}\\lambda_{m a x}$ for each   \n1095 experiment. For UGW, we use $\\varepsilon=10^{-1}$ and $\\rho_{1}=\\rho_{2}=1$ for both experiments. Finally, for MPGW,   \n1096 we set the mass-constrained term to be $\\rho=\\operatorname*{min}(|\\mu^{i}|,|\\mu^{j}|)$ when computing the similarity between   \n1097 shape $\\mathbb{X}^{i}$ and $\\mathbb{X}^{j}$ .   \n1098 Performance analysis. The pairwise distance matrices are visualized for each dataset in Figure 7, and   \n1099 the confusion matrices computed with each dataset are given in Figure 8. Finally, the classification   \n1100 accuracy with the SVM experiments is reported in Table 1a. The results indicate that the PGW   \n1101 distance is able to consistently obtain high performance across both datasets.   \n1102 In addition, from Figure 7, we observe that PGW qualitatively admits a more reasonable similarity   \n1103 measure compared to other methods. For example, in Dataset I, class \u201cbone\u201d and \u201crectangle\u201d should   \n1104 have relatively smaller distance than \u201cbone\u201d and \u201cannulus\u201d. Ideally, a reasonable distance should   \n1105 satisfy the following: ", "page_idx": 42}, {"type": "image", "img_path": "nrcFNxF57E/tmp/8222236cf82286df630e82964d6c0451b5dbf49f1a18dbc006f99ee386cc2d47.jpg", "img_caption": ["Figure 6: Visualization of a representative shape from each class of the two datasets. "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 43}, {"type": "equation", "text": "$$\n0<d(\\mathrm{bone,rectangle})<d(\\mathrm{bone,anulus}).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "1106 However, we do not observe this relation in GW and $\\mathrm{UGW}^{3}$ , and for the MPGW method,   \n1107 MPGW(bone, rectangle) $\\approx\\,0$ , which is also undesirable. For PGW, however, we do observe   \n1108 this relation. Additionally, we report the wall-clock time comparison in Table 1b.   \n1110 In this section, we present the wall-clock time comparison between our method Algorithms 1, 2,   \n1111 the Frank-Wolf algorithm proposed in [45], and its Sinkhorn version [41, 45]. Note that these two   \n1112 baselines solve a mass constraint version of the PGW problem, which we refer to as the \u201cMPGW\u201d   \n1113 problem. The proposed PGW formulation in this paper can be regarded as a \u201cLagrangian formulation\u201d   \n1114 of $\\mathrm{MPGW^{4}}$ formulation to the PGW problem defined in (10). In this paper, we call these two baselines   \n1115 as \u201cMPGW algorithm\u201d and \u201cSinkhorn PGW algorithm\u201d.   \n1116 Numerical details. The data is generated as follows: let $\\mu\\ \\ =\\ \\,\\mathrm{Unif}([0,2]^{2})$ and $\\nu\\quad=$   \n1117 $\\mathrm{Unif}([0,2]^{3})$ , we select i.i.d. samples $\\{x_{i}\\;\\sim\\;\\mu\\}_{i=1}^{n},\\{y_{j}\\;\\sim\\;\\nu\\}_{j=1}^{m}$ , where $n$ is selected from   \n1118 $[10,50,100,150,...,10000]$ and $m\\:=\\:n\\:+\\:100$ , $\\mathrm{~p~}=\\,1_{n}/m,\\mathrm{q~}\\dot{=}\\,1_{m}/m$ . For each $n$ , we set   \n1119 $\\dot{\\lambda}=0.2,1.0,10.0$ . The mass constraint parameter for the algorithm in [45], and Sinkhorn is com  \n1120 puted by the mass of the transportation plan obtained by Algorithm 1 or 2. The runtime results are   \n1121 shown in Figure 9.   \n1122 Regarding the acceleration technique, for the POT problem in step 1, our algorithms and the MPGW   \n1123 algorithm apply the linear programming solver provided by Python OT package [55], which is written   \n1124 in $C++$ . The Sinkhorn algorithm from Python OT does not have an acceleration technique. Thus, we   \n1125 only test its wall-clock time for $n\\leq2000$ . The data type is 64-bit float number.   \n1126 From Figure 9, we can observe the Algorithms 1, 2 and MPGW algorithm have a similar order of   \n1127 time complexity. However, using the column/row-reduction technique for the POT computation   \n1128 discussed in previous sections, and the fact the convergence behaviors of Algorithms 1 and 2 are   \n1129 similar to the MPGW algorithm, we observe that the proposed algorithms 1, 2 admits a slightly faster   \n1130 speed than MPGW solver. ", "page_idx": 43}, {"type": "image", "img_path": "nrcFNxF57E/tmp/7f107ff6144784d94cc0dfdffecb88618b88effcca90a3441cd8639f097df8f9.jpg", "img_caption": ["Figure 7: Pairwise distance matrices computed for each dataset. "], "img_footnote": [], "page_idx": 44}, {"type": "image", "img_path": "nrcFNxF57E/tmp/02bce8086f6f6117d3dd12c0effd7f9b20a3ebcfc86773dbc47c34a2e597723d.jpg", "img_caption": ["Figure 8: Confusion matrices computed from nearest neighbor classification experiments. "], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 45}, {"type": "image", "img_path": "nrcFNxF57E/tmp/aa33c7cf8b1fe3aa48fb6381ca13c4aaba9c6815688e0b4603f0c50a7bcb359a.jpg", "img_caption": ["Figure 9: We test the wall-clock time of our Algorithm 1 and Algorithm 2, the MPGW solver (Algorithm 1 in [45]) , and the Sinkhorn algorithm [41]. We denote these methods as v1, v2, m, s respectively. The linear programming solver applied in the first three methods is from POT [55], which is written in $C++$ . The maximum number of iterations for all the methods is set to be 1000. The maximum iteration for OT/OPT solvers is set to be $300n$ . The maximum Sinkhorn iteration is set to be 1000. The convergence tolerance for the Frank-Wolfe algorithm and the Sinkhorn algorithm are set to be $1e-5$ . To achieve their best performance, the number of dummy points is set to be 1 for MPGW and PGW. "], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "1131 P Positive Unlabeled Learning Problem ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "1132 P.1 Problem setup. ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "1133 Positive unlabeled (PU) learning [56, 57, 58] is a semi-supervised binary classification problem for   \n1134 which the training set only contains positive samples. In particular, suppose there exists a fixed   \n1135 unknown overall distribution over triples $(x,o,l)$ , where $x$ is data, $l\\in\\{0,1\\}$ is the label of $x$ ,   \n1136 $o\\in\\{0,1\\}$ where $o=1$ , $o=0$ denote that $l$ is observed or not, respectively. In the PU task, the   \n1137 assumption is that only positive samples\u2019 labels can be observed, i.e., ${\\mathrm{Prob}}(o=1|x,l=0)=0$ .   \n1138 Consider training labeled data $X^{p u}\\,^{\\cdot}=\\,\\{(x_{i}^{p u},l)\\}_{i=1}^{n}\\,\\subset\\,\\{x\\,:\\,o\\,=\\,1\\}$ and testing data $X^{u n}\\;=$   \n1139 $\\{x_{j}^{u n}\\}_{j=1\\_{}}^{m}\\subset\\;\\{x\\,:\\,o\\,=\\,0\\}$ , where $x_{i}p_{i}^{X}\\,\\in\\,\\mathbb{R}^{d_{1}},x_{j}^{u}\\,\\in\\,\\mathbb{R}^{d_{2}}$ . In the classical PU learning setting,   \n1140 $d_{2}=\\check{d}_{1}$ . However, in [44] this assumption is relaxed. The goal is to leverage $X^{p}$ to design a classifier   \n1141 ${\\hat{l}}:x^{u}\\to\\{0,1\\}$ to predict $l(x^{u})$ for all $x^{u}\\in X^{u}$ .5   \n1142 Following [57, 45, 44], in this experiment, we assume that the \u201cselect completely at random\u201d (SCAR)   \n1143 assumption holds: ${\\mathrm{Prob}}(o=1|{\\bar{x}},l=1)={\\mathrm{Prob}}(o=1|l=1)$ . In addition, we use $\\pi=\\operatorname{Prob}(l=$   \n1144 $1)\\in[0,1]$ to denote the ratio of positive samples in testing $\\mathrm{set}^{6}$ . Following the PU learning setting in   \n1145 [58, 59, 45, 44], we assume $\\pi$ is known. In all the PU learning experiments, we fix $\\pi=0.2$ . ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 46}, {"type": "text", "text": "1146 P.2 Our method. ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "1147 Similar to [45] our method is designed as follows: We set $\\mathrm{~p~}\\in\\mathbb{R}^{n},\\mathrm{q}\\in\\mathbb{R}^{m}$ as $\\begin{array}{r}{p_{i}^{X}=\\frac{\\pi}{n},i\\in[1:}\\end{array}$   \n1148 $n]$ ; $\\begin{array}{r}{q_{j}^{Y}=\\frac{1}{m},j\\in[1:m]}\\end{array}$ . Let $\\begin{array}{r}{\\mathbb{X}^{p}=(X^{p},\\lVert\\cdot\\rVert_{d_{1}},\\sum_{i=1}^{n}p_{i}^{X}\\delta_{x_{i}}),\\mathbb{X}^{u}=(X^{u},\\lVert\\cdot\\rVert_{d_{2}},\\sum_{j=1}^{n}q_{j}^{Y}\\delta_{y_{j}})}\\end{array}$ .   \n1149 We solve the partial GW problem $P G W_{\\lambda}(\\mathbb{X}^{p},\\mathbb{X}^{u})$ and suppose $\\gamma$ is a solution. Let $\\gamma_{2}=\\gamma^{\\top}1_{n}$ . The   \n1150 classifier $\\hat{l}$ is defined by the indicator function ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\hat{l}_{\\gamma}(x^{u})=\\mathbb{1}_{\\left\\{x^{u}:\\gamma_{2}(x^{u})\\geq\\mathrm{quantile}\\right\\}},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "1151 where quantile is the quantile value of $\\gamma_{2}$ according to $1-\\pi$ . ", "page_idx": 46}, {"type": "text", "text": "1152 Regarding the initial guess $\\gamma^{(1)}$ , [45] proposed a POT-based approach when $X$ and $Y$ are sampled   \n1153 from the same domain, i.e., $d_{1}=d_{2}$ , which we refer to as \u201cPOT initialization.\u201d   \n1154 When $X,Y$ are sampled from different spaces, that is, $d_{1}\\neq d_{2}$ , the above technique (86) is not   \n1155 well-defined. Inspired by [8, 44], we propose the following \u201cfirst lower bound-partial OT\u201d (FLB-POT)   \n1156 initialization: ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 46}, {"type": "equation", "text": "$$\n\\gamma^{(1)}=\\arg\\operatorname*{min}_{\\gamma\\in\\Gamma_{\\leq}(\\mathbf{p},\\mathbf{q})}\\int_{X\\times Y}|s_{X,2}(x)-s_{Y,2}(y)|^{2}d\\gamma(x,y)+\\lambda(|\\mathbf{p}-\\gamma_{1}|+|\\mathbf{q}-\\gamma_{2}|),\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "1157 where $\\begin{array}{r}{s_{X,2}(x)=\\int_{X}|x-x^{\\prime}|^{2}d\\mu(x)}\\end{array}$ and $^{S}Y,^{2}$ is defined similarly. The above formula is analog to   \n1158 Eq. (7) in [44], which is designed for the unbalanced GW setting. To distinguish them, in this paper   \n1159 we call the Eq. (7) in [44] as \u201cFLB-UOT initilization\u201d. ", "page_idx": 46}, {"type": "text", "text": "1160 P.3 Dataset. ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "1161 The datasets include MNIST, EMNIST, and the following three domains of Caltech Office: Amazon   \n1162 (A), Webcam (W), and DSLR (D) [60]. For each domain, we select the SURF features [60] and   \n1163 DECAF features [61]. For MNIST and EMNIST, we train an auto-encoder, respectively, and the   \n1164 embedding space dimension is 4 and 6, respectively. See Figure 10 for the TSNE visualization of   \n1165 these datasets. ", "page_idx": 46}, {"type": "text", "text": "1166 P.4 Initial methods. ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "1167 In this experiment, we employ three distinct initial methods: \u201cPOT\u201d, \u201cFLB-UOT\u201d, \u201cFLB-POT\u201d. ", "page_idx": 46}, {"type": "image", "img_path": "nrcFNxF57E/tmp/dfbd5b4a2320834d7b3a195cc8cd5c5d1db87811d979f1664ac88ecf358a972b.jpg", "img_caption": ["Figure 10: TSNE visulization for datasets MNIST,EMNIST,Caltech Office. "], "img_footnote": [], "page_idx": 47}, {"type": "text", "text": "1168 \u201cPOT initialization\u201d is firstly introduced in [45].When $X_{1},X_{2}$ are in the same dimensional space,   \n1169 i.e. $d_{1}=d_{2}$ . The initial guess, $\\gamma^{(1)}$ is given by the following partial OT variant problem: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\gamma^{(1)}=\\arg\\operatorname*{min}_{\\gamma\\in\\Gamma_{P U,\\pi}^{}(\\mathrm{p},\\mathrm{q})}\\langle L(X,Y),\\gamma\\rangle_{F},\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "1170 where $L(X,Y)\\in\\mathbb{R}^{n\\times m}$ , $(L(X,Y))_{i j}=\\|x_{i}-y_{j}\\|^{2}$ and ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Gamma_{P U,\\pi}(\\mathbf{p},\\mathbf{q}):=\\{\\gamma\\in\\mathbb{R}_{+}^{n\\times m}:(\\gamma^{\\top}1_{n})_{j}\\in\\{q_{j}^{Y},0\\},\\forall j;\\gamma1_{m}\\leq\\mathbf{p},|\\gamma|=\\pi\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "1171 The above problem can be solved by a Lasso ( $\\mathcal{L}^{1}$ norm) regularized OT solver. ", "page_idx": 48}, {"type": "text", "text": "1172 When $d_{1}\\neq d_{2}$ , the above technique can not be applied since the problem (86) (in particular $L(X,Y))$ )   \n1173 is not well-defined. ", "page_idx": 48}, {"type": "text", "text": "1174 The second method \u201cFLB-UOT\u201d is induced in [44]: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\gamma^{(1)}=\\arg\\operatorname*{min}_{\\gamma\\in\\Gamma_{\\le}(\\gamma,\\mathbf{q})}\\int_{X\\times Y}|s_{X,2}(x)-s_{Y,2}(y)|^{2}d\\gamma(x,y)+\\lambda(D_{K L}(\\gamma_{1},\\mathbf{p})+D_{K L}(\\gamma_{2},\\mathbf{q})),\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "1175 where $\\begin{array}{r}{s_{X,2}(x)\\,=\\,\\int_{X}|x-x^{\\prime}|^{2}d\\mu(x)}\\end{array}$ and $s_{Y,2}$ is defined similarly. The problem (88) is called   \n1176 Hellinger Kantorovich, which is a classical unbalanced optimal transport problem. It can be solved   \n1177 by the Sinkhorn solver [38].   \n1178 Analog to the above method, we propose the third method, called \u201cFLB-POT\u201d (first lower bound  \n1179 partial optimal transport) ", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 48}, {"type": "equation", "text": "$$\n\\gamma^{(1)}=\\arg\\operatorname*{min}_{\\gamma\\in\\Gamma_{\\leq}(\\mathbf{p},\\mathbf{q})}\\int_{X\\times Y}|s_{X,2}(x)-s_{Y,2}(y)|^{2}d\\gamma(x,y)+\\lambda(|\\mathbf{p}-\\gamma_{1}|+|\\mathbf{q}-\\gamma_{2}|).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "1180 The above problem is a partial OT problem and can be solved by classical linear programming [12]. ", "page_idx": 48}, {"type": "text", "text": "1181 P.5 Numerical details and performance. ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "1182 Accuracy Comparison. In Table 2 and 4, we present the accuracy results for the MPGW, UGW, and   \n1183 the proposed PGW methods when using three different initialization methods: POT, FLB-UOT, and   \n1184 FLB-POT.   \n1185 Following [45], in the MPGW and PGW methods, we incorporate the prior knowledge $\\pi$ into the   \n1186 definition of $p$ and $q$ . Thus it is sufficient to set $m a s s\\,=\\,\\pi$ for MPGW and choose a sufficiently   \n1187 large value for $\\lambda$ in the PGW method. This configuration ensures that the mass matched in the target   \n1188 domain $\\boldsymbol{\\wp}$ is exactly equal to $\\pi$ . However, in the UGW method [44], the setting is $\\textstyle p={\\frac{1}{n}}\\boldsymbol{1}_{n}$ and   \n1189 $\\textstyle q={\\frac{1}{m}}1_{m}$ . Therefore, in each experiment, we test different parameters $(\\rho,\\rho_{2},\\epsilon)$ and select the ones   \n1190 that result in transported mass close to $\\pi$ .   \n1191 Overall, all methods show improved performance in MNIST and EMNIST datasets. One possible   \n1192 reason for this could be the better separability of the embeddings in MNIST and EMNIST, as   \n1193 illustrated in Figure 10. Additionally, since MPGW and PGW incorporate information from $r$ into   \n1194 their formulations, they exhibit slightly better accuracy in many experiments.   \n1195 Numerical details. In this experiment, to prevent unexpected convergence to local minima in the   \n1196 Frank-Wolf algorithms, we manually set $\\alpha=1$ during the line search step for both MPGW and PGW   \n1197 methods.   \n1198 For the convergence criteria, we set the tolerance term for Frank-Wolfe convergence and the main   \n1199 loop in the UGW algorithm to be $1e-5$ . Additionally, the tolerance for Sinkhorn convergence in   \n1200 UGW was set to $1e-6$ . The maximum number of iterations for the POT solver in PGW and MPGW   \n1201 was set to $500n$ . In addition, for MPGW, we set mass $=0.2$ and for PGW method, based on lemma   \n1202 E.2, we set $\\lambda$ to be constant such that $2\\lambda\\geq(\\operatorname*{max}(|C^{X}|)^{2}+\\operatorname*{max}(|C^{Y}|)^{2})$ .   \n1203 Regarding data types, we used 64-bit floating-point numbers for MPGW and PGW, and 32-bit   \n1204 floating-point numbers for UGW.   \n1205 For the MNIST and EMNIST datasets, we set $n=1000$ and $m=5000$ . In the $\\operatorname{Surf}(\\mathbf{A})$ and Decaf(A)   \n1206 datasets, each class contained an average of 100 samples. To ensure the SCAR assumption, we set   \n1207 $n=1/2*100=50$ and $m=250$ . Similarly, for the Surf(D) and Decaf(D) datasets, we set $n=15$   \n1208 and $m=75$ . Finally, for $\\operatorname{Surf}(\\operatorname{W})$ and Decaf(W), we used $n=20$ and $m=100$ .   \n1209 Wall-clock time In Table 3, we provide a comparison of wall-clock times for the MNIST and   \n1210 EMNIST datasets. ", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 48}, {"type": "table", "img_path": "nrcFNxF57E/tmp/4f9490b1b22f94b6d92f167905125d8a92aa1ec54df87fd370b2b578a0df60ef.jpg", "table_caption": [], "table_footnote": ["Table 2: Accuracy comparison of the MPGW, UGW, and the proposed PGW method on PU learning. Here, \u2018M\u2019 denotes MNIST, and \u2018EM\u2019 denotes EMNIST. "], "page_idx": 48}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 49}, {"type": "table", "img_path": "nrcFNxF57E/tmp/344af50bfae77286d62e675d5c3ea34dedc84d36197c819ca3c13d61b1c1f674.jpg", "table_caption": [], "table_footnote": ["Table 3: In this table, we present the wall-clock time for the MPGW, UGW, and the proposed PGW method, as well as three different initialization methods (POT, FLB-UOT, FLB-POT). In the \u201cSource\u201d (or \u201cTarget\u201d) columm, M (or EM) denotes the MNIST (or EMNIST) dataset, the value 1000 (or 5000) denotes the sample size of $X$ (or $Y$ ). The units of all reported wall-clock times is seconds. "], "page_idx": 49}, {"type": "table", "img_path": "nrcFNxF57E/tmp/c502f11b3296a578df403792009a6ede77aab928dc1abf870db64bae53161687.jpg", "table_caption": [], "table_footnote": ["Table 4: In this table, we present the accuracy comparison of the MPGW, UGW, and the proposed PGW method. We report the initialization method and its accuracy, followed by the accuracy of each of the methods MPGW, UGW, and PGW. The prior distribution $\\pi=p(l=1)$ is set to be 0.2 in all experiments. To guarantee the SCAR assumption, for $\\operatorname{Surf}(\\mathbf{A})$ and Decaf(A), we set $n=50$ , which is the half of the total number of data in one single class. $m$ is set to be 250. Similarly, we set suitable $n,m$ for Surf(D), Decaf(D), Surf(W), Decaf(W). "], "page_idx": 50}, {"type": "table", "img_path": "nrcFNxF57E/tmp/fc8fd7ce234b33cea77cb939db880bd111e39620052f489c28f9ebabefbd47cc.jpg", "table_caption": [], "table_footnote": ["Table 5: In this table, we present the wall-clock time comparison of the MPGW, UGW, and the proposed PGW method. We report the initialization method and its wall-clock time, followed by the wall-clock time of each of the methods MPGW, UGW, and PGW. The units of all reported wall-clock times is seconds. The prior distribution $\\pi=p(l=1)$ is set to be 0.2 in all experiments. To guarantee the SCAR assumption, for $\\operatorname{Surf}(\\mathbf{A})$ and Decaf(A), we set $n=50$ , which is the half of the total number of data in one single class. $m$ is set to be 250. Similarly, we set suitable $n,m$ for Surf(D), Decaf(D), Surf(W), Decaf(W). "], "page_idx": 51}, {"type": "text", "text": "1211 Q Limitations ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "1212 Compatibility Between Linear Search and Frank-Wolf Solver ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "1213 In practice, we have found that in some experiments, the linear search algorithm (see Sections I, J)   \n1214 may cause the Frank Wolfe algorithms (1, 2) to stop running earlier than expected. This may hurt the   \n1215 performance observed in the PU learning experiments (see Appendix P). As such, we disable line   \n1216 search in these experiments.   \n1217 However, in other experiments, for example PGW barycenter (Appendix M.1), we do not find a   \n1218 significant effect of the linear search algorithm on the results. ", "page_idx": 52}, {"type": "text", "text": "", "page_idx": 52}, {"type": "text", "text": "1219 MDS in Point Cloud Interpolation Experiment ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "1220 In the point cloud interpolation experiment (see Appendix M), for the classical GW barycenter method   \n1221 [41] or our PGW barycenter method, the last step is the same: applying MDS on the barycenter   \n1222 minimizer $C$ to construct interpolation point cloud $X_{t}$ . However, such construction is not unique.   \n1223 As a consequence, for each constructed $X_{t}$ , we need to manually set up the rotation and flipping   \n1224 matrices.   \n1225 This problem follows from the fact that the GW and PGW formulations cannot distinguish the data   \n1226 from its rotated (and flipped) version. We refer to Section M.1 for details. ", "page_idx": 52}, {"type": "text", "text": "", "page_idx": 52}, {"type": "text", "text": "1227 R Compute Resources ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "1228 All experiments presented in this paper are conducted on a computational machine with an AMD   \n1229 EPYC 7713 64-Core Processor, $8\\times32\\mathrm{GB}$ DIMM DDR4, 3200 MHz, and a NVIDIA RTX A6000   \n1230 GPU. ", "page_idx": 52}, {"type": "text", "text": "1231 S Impact Statement ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "1232 The work presented in this paper aims to advance the field of machine learning, particularly the   \n1233 supplementary theoretical developments and explorations of computational optimal transport. There   \n1234 are many potential societal consequences of our work, none of which we feel must be specifically   \n1235 highlighted here. ", "page_idx": 52}, {"type": "text", "text": "1236 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "38 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n39 paper\u2019s contributions and scope?   \n40 Answer: [Yes]   \n41 Justification: In the Abstract, we briefly introduce our main contributions, and in the   \n42 Introduction (Section 1) we explain our main contributions in detail. These contributions   \n43 are reflected by the theoretical and experimental results provided in the remainder of the   \n44 main text and appendices.   \n45 Guidelines:   \n46 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n47 made in the paper.   \n48 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n49 contributions made in the paper and important assumptions and limitations. A No or   \n50 NA answer to this question will not be perceived well by the reviewers.   \n251 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n52 much the results can be expected to generalize to other settings.   \n253 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n54 are not attained by the paper. ", "page_idx": 53}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: We explain the limitations in Appendix Q. ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 53}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "7 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n88 a complete (and correct) proof? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes]   \nJustification: In each theorem, we clearly specify the details of conditions and assumptions along with complete proof.   \n\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 54}, {"type": "text", "text": "", "page_idx": 54}, {"type": "text", "text": "1303 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "04 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n05 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n06 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justifications: In Sections M.1,M.2,N, subsection \u201cnumerical details\u201d, we explain the detailed parameter settings for each method in order to reproduce our results. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 54}, {"type": "text", "text": "1342 5. Open access to data and code ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "tions to faithfully reproduce the main experimental results, as described in supplemental   \nmaterial?   \nAnswer: [Yes]   \nJustification: We provide the data and code as supplementary material.   \nGuidelines: \u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). \u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. \u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. \u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 55}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: We refer to the subsections \u201cexperiment setup\u201d in Sections 5, M.1, M.2, N, P. Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 55}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: We calculate accuracy in experiments N, P, which are the only statistics reported in this paper. These values are classification accuracies for each tested dataset. Thus, error bar/variance are not involved in this work. ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). ", "page_idx": 55}, {"type": "text", "text": "1395 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n1396 call to a library function, bootstrap, etc.)   \n1397 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n1398 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n1399 of the mean.   \n1400 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n1401 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n1402 of Normality of errors is not verified.   \n1403 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n1404 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n1405 error rates).   \n1406 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n1407 they were calculated and reference the corresponding figures or tables in the text.   \n1408 8. Experiments Compute Resources   \n1409 Question: For each experiment, does the paper provide sufficient information on the com  \n1410 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n1411 the experiments?   \n1412 Answer: [Yes]   \n1413 Justification: See Appendix R.   \n1414 Guidelines:   \n1415 \u2022 The answer NA means that the paper does not include experiments.   \n1416 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n1417 or cloud provider, including relevant memory and storage.   \n1418 \u2022 The paper should provide the amount of compute required for each of the individual   \n1419 experimental runs as well as estimate the total compute.   \n1420 \u2022 The paper should disclose whether the full research project required more compute   \n1421 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n1422 didn\u2019t make it into the paper).   \n1423 9. Code Of Ethics   \n1424 Question: Does the research conducted in the paper conform, in every respect, with the   \n1425 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n1426 Answer: [Yes]   \n1427 Justification: The authors have reviewed the NeurIPS Code of Ethics and all the imported   \n1428 code has been properly cited.   \n1429 Guidelines:   \n1430 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n1431 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n1432 deviation from the Code of Ethics.   \n1433 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n1434 eration due to laws or regulations in their jurisdiction).   \n1435 10. Broader Impacts   \n1436 Question: Does the paper discuss both potential positive societal impacts and negative   \n1437 societal impacts of the work performed?   \n1438 Answer: [Yes]   \n1439 Justification: See Appendix S.   \n1440 Guidelines:   \n1441 \u2022 The answer NA means that there is no societal impact of the work performed.   \n1442 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n1443 impact or why the paper does not address societal impact.   \n1444 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n1445 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n1446 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n1447 groups), privacy considerations, and security considerations.   \n1448 \u2022 The conference expects that many papers will be foundational research and not tied   \n1449 to particular applications, let alone deployments. However, if there is a direct path to   \n1450 any negative applications, the authors should point it out. For example, it is legitimate   \n1451 to point out that an improvement in the quality of generative models could be used to   \n1452 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n1453 that a generic algorithm for optimizing neural networks could enable people to train   \n1454 models that generate Deepfakes faster.   \n1455 \u2022 The authors should consider possible harms that could arise when the technology is   \n1456 being used as intended and functioning correctly, harms that could arise when the   \n1457 technology is being used as intended but gives incorrect results, and harms following   \n1458 from (intentional or unintentional) misuse of the technology.   \n1459 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n1460 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1461 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1462 feedback over time, improving the efficiency and accessibility of ML).   \n1463 11. Safeguards   \n1464 Question: Does the paper describe safeguards that have been put in place for responsible   \n1465 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1466 image generators, or scraped datasets)?   \n1467 Answer: [NA]   \n1468 Justification: This paper does not pose such risks.   \n1469 Guidelines:   \n1470 \u2022 The answer NA means that the paper poses no such risks.   \n1471 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1472 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1473 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1474 safety filters.   \n1475 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1476 should describe how they avoided releasing unsafe images.   \n1477 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1478 not require this, but we encourage authors to take this into account and make a best   \n1479 faith effort.   \n1480 12. Licenses for existing assets   \n1481 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1482 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1483 properly respected?   \n1484 Answer: [Yes]   \n1485 Justification: In Sections M.1, M.2, N, P, subsection \u201cdataset\u201d, we provide the citations of   \n1486 all datasets from other literature. We also cite all code adapted from other sources.   \n1487 Guidelines:   \n1488 \u2022 The answer NA means that the paper does not use existing assets.   \n1489 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1490 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1491 URL.   \n1492 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1493 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1494 service of that source should be provided.   \n95 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n96 package should be provided. For popular datasets, paperswithcode.com/datasets   \n97 has curated licenses for some datasets. Their licensing guide can help determine the   \n98 license of a dataset.   \n99 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n00 the derived asset (if it has changed) should be provided.   \n01 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n02 the asset\u2019s creators. ", "page_idx": 56}, {"type": "text", "text": "", "page_idx": 57}, {"type": "text", "text": "", "page_idx": 58}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 58}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 58}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 58}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 58}, {"type": "text", "text": "34 Question: Does the paper describe potential risks incurred by study participants, whether   \n35 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n36 approvals (or an equivalent approval/review based on the requirements of your country or   \n37 institution) were obtained? ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 58}, {"type": "text", "text": "", "page_idx": 59}]