[{"figure_path": "k6m3y6qnSj/figures/figures_0_1.jpg", "caption": "Figure 2: Overview. Given a set of images I and camera poses \u03c0 in (a), we run NeRF to extract the 3D geometry as in (b). Based on this geometry and a target light shown in (c), we create radiance cues for each given input view as in (d). Next, we independently relight each input image using a single-image Relighting Diffusion Model illustrated in (e) and sample S possible solutions for each given view displayed in (f). Finally, we distill the relit set of images into a 3D representation through a Latent NeRF optimization as in (g) and (h).", "description": "This figure provides a visual overview of the IllumiNeRF method.  It shows the process from input images and poses (a) through NeRF geometry extraction (b), radiance cue creation (d) based on the target lighting (c), image relighting using a diffusion model (e), and finally, the generation of a consistent 3D representation (g, h) from the relit images (f) using a latent NeRF.  The final 3D model can then be used to render novel views under the target lighting.", "section": "3.2 Model Overview"}, {"figure_path": "k6m3y6qnSj/figures/figures_3_1.jpg", "caption": "Figure 2: Overview. Given a set of images I and camera poses \u03c0 in (a), we run NeRF to extract the 3D geometry as in (b). Based on this geometry and a target light shown in (c), we create radiance cues for each given input view as in (d). Next, we independently relight each input image using a single-image Relighting Diffusion Model illustrated in (e) and sample S possible solutions for each given view displayed in (f). Finally, we distill the relit set of images into a 3D representation through a Latent NeRF optimization as in (g) and (h).", "description": "This figure shows the overview of the IllumiNeRF model. It takes as input a set of images and camera poses. It first uses NeRF to reconstruct the 3D geometry of the scene. Then, it generates radiance cues based on the geometry and target lighting. These cues are used to condition a Relighting Diffusion Model, which produces multiple relit images for each input view. Finally, these relit images are used to train a Latent NeRF, which can be used to render novel views under the target lighting.", "section": "3.2 Model Overview"}, {"figure_path": "k6m3y6qnSj/figures/figures_4_1.jpg", "caption": "Figure 3: Relit samples vs. latent NeRF. (a) Samples of the Relighting Diffusion Model (Sec. 3.4) for the same target environment map, and (b) renderings from the optimized Latent NeRF (Sec. 3.3) for a fixed value of the latent. The diffusion samples correspond to different latent explanations of the scene and our latent NeRF optimization is able to effectively optimize these latent variables along with the NeRF model's parameters to produce consistent renderings for each latent explanation.", "description": "This figure compares the results of using the Relighting Diffusion Model and the Latent NeRF model to relight images. The left side shows various samples generated by the diffusion model for the same target lighting condition, illustrating the diversity of possible explanations for the scene's appearance. The right side displays renderings produced by the optimized Latent NeRF for a fixed latent code, demonstrating how the model effectively combines the different latent interpretations into consistent and high-quality results. This highlights the effectiveness of the Latent NeRF in reconciling multiple plausible explanations generated by the diffusion model into a single coherent 3D representation.", "section": "3.3 Latent NeRF Model"}, {"figure_path": "k6m3y6qnSj/figures/figures_5_1.jpg", "caption": "Figure 4: Example radiance cues for a view of the 'hotdog' scene.", "description": "This figure displays four images illustrating the radiance cues used in the IllumiNeRF model.  The images show a rendered view of a hot dog on a plate under different material properties.  The first image shows a diffuse material, while the remaining three images show specular materials with varying roughness values (0.34, 0.13, and 0.05). These cues help to encode lighting information and are used as conditioning input for the single-image relighting diffusion model within the IllumiNeRF pipeline. They provide information about the effects of specularities, shadows, and global illumination without requiring the diffusion network to learn these effects from scratch.", "section": "4 Experiments"}, {"figure_path": "k6m3y6qnSj/figures/figures_6_1.jpg", "caption": "Figure 5: Qualitative results on TensoIR. Renderings from all approaches have been rescaled with respect to the ground-truth as mentioned in Eq. (4.1). Unlike TensoIR, our method faithfully recovers specular highlights and colors as indicated in red.", "description": "This figure shows a qualitative comparison of relighting results on the TensoIR benchmark dataset.  It includes ground truth images alongside images generated by the proposed IllumiNeRF method and the TensoIR baseline method.  The red boxes highlight areas where IllumiNeRF excels in recovering specular highlights and accurate color reproduction, showcasing its superior performance compared to the baseline. The overall aim is to demonstrate the superior quality of relighting achieved by the proposed method.", "section": "4.1 Experimental Setup"}, {"figure_path": "k6m3y6qnSj/figures/figures_7_1.jpg", "caption": "Figure 6: Qualitative results on Stanford-ORB. Renderings from all approaches have been rescaled with respect to the ground-truth as mentioned in Sec. 4.1. Areas where our approach performs well are highlighted. Our approach produces high-quality renderings with plausible specular reflections.", "description": "This figure shows a qualitative comparison of different relighting methods on the Stanford-ORB dataset.  Each row represents a different object and lighting condition.  The first column shows the ground truth image; the subsequent columns show relighting results generated by various methods, including the proposed IllumiNeRF. The red boxes highlight regions where IllumiNeRF demonstrates superior performance, especially in capturing specular highlights and realistic material appearances.", "section": "4 Experiments"}, {"figure_path": "k6m3y6qnSj/figures/figures_8_1.jpg", "caption": "Figure 5: Qualitative results on TensoIR. Renderings from all approaches have been rescaled with respect to the ground-truth as mentioned in Eq. (4.1). Unlike TensoIR, our method faithfully recovers specular highlights and colors as indicated in red.", "description": "This figure displays qualitative results of the TensoIR benchmark. It compares renderings from various methods with ground truth images.  The red highlights in the image emphasize areas where the proposed method (IllumiNeRF) stands out by accurately capturing specular highlights and colors.", "section": "4.1 Experimental Setup"}, {"figure_path": "k6m3y6qnSj/figures/figures_9_1.jpg", "caption": "Figure 8: Using a standard NeRF instead of a latent NeRF model is unable to reconcile training samples with different underlying latent explanations. Using a latent NeRF model significantly increases the accuracy of rendered specular appearance, and increasing the number of samples S from the RDM used to train the latent NeRF model further increases the quality of the output renderings.", "description": "This figure compares the results of using a standard NeRF versus a latent NeRF, and varying the number of samples (S) used from the Relighting Diffusion Model (RDM) for training the NeRF.  It demonstrates that using a latent NeRF, and increasing the number of samples, significantly improves the quality of the rendered images, especially in terms of the accuracy of specular highlights.", "section": "4.3 Ablations"}, {"figure_path": "k6m3y6qnSj/figures/figures_14_1.jpg", "caption": "Figure S1: Effects of shading normal smoothing function.", "description": "This figure shows a comparison of radiance cues rendered with and without the shading normal smoothing function enabled in Blender. The leftmost image in each row shows a diffuse material, while the remaining images show progressively rougher specular materials.  The top row (a) shows the results without smoothness enabled, while the bottom row (b) shows the results with smoothness enabled.  The comparison highlights that while smoothness helps to create more realistic-looking specular highlights, over-smoothness can negatively impact the photorealism of shadows.", "section": "A.2 Radiance Cues"}, {"figure_path": "k6m3y6qnSj/figures/figures_14_2.jpg", "caption": "Figure 2: Overview. Given a set of images I and camera poses \u03c0 in (a), we run NeRF to extract the 3D geometry as in (b). Based on this geometry and a target light shown in (c), we create radiance cues for each given input view as in (d). Next, we independently relight each input image using a single-image Relighting Diffusion Model illustrated in (e) and sample S possible solutions for each given view displayed in (f). Finally, we distill the relit set of images into a 3D representation through a Latent NeRF optimization as in (g) and (h).", "description": "This figure illustrates the overall pipeline of the IllumiNeRF method. Starting with input images and camera poses (a), it first reconstructs a 3D representation using NeRF (b). Then, it generates radiance cues based on the 3D geometry and target lighting (c, d).  Next, it uses a single-image relighting diffusion model to relight each input image, producing multiple plausible relit images (e, f). Finally, it trains a latent NeRF on these relit images to produce a consistent 3D model that can be rendered under the target lighting (g, h).", "section": "3.2 Model Overview"}, {"figure_path": "k6m3y6qnSj/figures/figures_16_1.jpg", "caption": "Figure 2: Overview. Given a set of images I and camera poses \u03c0 in (a), we run NeRF to extract the 3D geometry as in (b). Based on this geometry and a target light shown in (c), we create radiance cues for each given input view as in (d). Next, we independently relight each input image using a single-image Relighting Diffusion Model illustrated in (e) and sample S possible solutions for each given view displayed in (f). Finally, we distill the relit set of images into a 3D representation through a Latent NeRF optimization as in (g) and (h).", "description": "This figure provides a visual overview of the IllumiNeRF pipeline. It shows how input images and poses are used to first extract a 3D geometry using NeRF. Then, radiance cues are generated based on the geometry and target lighting, which are then used to relight input images using a single-image relighting diffusion model. Finally, the relit images are used to train a latent NeRF to create a 3D representation that can be used for novel view synthesis under the target lighting.", "section": "3.2 Model Overview"}, {"figure_path": "k6m3y6qnSj/figures/figures_17_1.jpg", "caption": "Figure 3: Relit samples vs. latent NeRF. (a) Samples of the Relighting Diffusion Model (Sec. 3.4) for the same target environment map, and (b) renderings from the optimized Latent NeRF (Sec. 3.3) for a fixed value of the latent. The diffusion samples correspond to different latent explanations of the scene and our latent NeRF optimization is able to effectively optimize these latent variables along with the NeRF model's parameters to produce consistent renderings for each latent explanation.", "description": "This figure compares samples from the Relighting Diffusion Model with renderings from the optimized Latent NeRF for the same target environment map.  The diffusion model produces samples representing different latent explanations of the scene (different interpretations of material, geometry, lighting).  The latent NeRF optimization combines these diverse interpretations to produce consistent and coherent 3D renderings.", "section": "3.3 Latent NeRF Model"}]