[{"type": "text", "text": "Physics-Informed Regularization for Domain-Agnostic Dynamical System Modeling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zijie Huang1\u2217 Wanjia Zhao2\u2217\u2020 Jingdong Gao1 Ziniu $\\mathbf{H}\\mathbf{u}^{3}$ Xiao Luo1 Yadi Cao1 Yuanzhou Chen1 Yizhou Sun1 Wei Wang1 ", "page_idx": 0}, {"type": "text", "text": "1University of California Los Angeles, 2Stanford University 3California Institute of Technology https://treat-ode.github.io/ ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning complex physical dynamics purely from data is challenging due to the intrinsic properties of systems to be satisfied. Incorporating physics-informed priors, such as in Hamiltonian Neural Networks (HNNs), achieves high-precision modeling for energy-conservative systems. However, real-world systems often deviate from strict energy conservation and follow different physical priors. To address this, we present a framework that achieves high-precision modeling for a wide range of dynamical systems from the numerical aspect, by enforcing Time-Reversal Symmetry (TRS) via a novel regularization term. It helps preserve energies for conservative systems while serving as a strong inductive bias for non-conservative, reversible systems. While TRS is a domain-specific physical prior, we present the first theoretical proof that TRS loss can universally improve modeling accuracy by minimizing higher-order Taylor terms in ODE integration, which is numerically beneficial to various systems regardless of their properties, even for irreversible systems. By integrating the TRS loss within neural ordinary differential equation models, the proposed model TREAT demonstrates superior performance on diverse physical systems. It achieves a significant $11.5\\%$ MSE improvement in a challenging chaotic triple-pendulum scenario, underscoring TREAT\u2019s broad applicability and effectiveness. Code and further details are available at here. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dynamical systems, spanning applications from physical simulations (Kipf et al., 2018; Wang et al., 2020; Lu et al., 2022; Huang et al., 2023; Luo et al., 2023a; Xu et al., 2024; Luo et al., 2024) to robotic control (Li et al., 2022; Ni and Qureshi, 2022), are challenging to model due to intricate dynamic patterns and potential interactions under multi-agent settings. Traditional numerical simulators require extensive domain knowledge for design, which is sometimes unknown (Sanchez-Gonzalez et al., 2020), and can consume significant computational resources (Wang et al., 2024). Therefore, directly learning dynamics from the observational data becomes an attractive alternative. ", "page_idx": 0}, {"type": "text", "text": "Existing deep learning approaches (Sanchez-Gonzalez et al., 2020; Pfaff et al., 2021; Han et al., 2022a) usually learn a fixed-step transition function to predict system dynamics from timestamp $t$ to timestamp $t+1$ and rollout trajectories recursively. The transition function can have different inductive biases, such as Graph Neural Networks (GNNs) (Pfaff et al., 2020; Martinkus et al., 2021; Lam et al., 2023; Cao et al., 2023) for capturing pair-wise interactions among agents through message passing. Most recently, neural ordinary differential equations (Neural ODEs) (Chen et al., ", "page_idx": 0}, {"type": "image", "img_path": "iWlqbNE8P7/tmp/9db10a6bb4e726445fa6041ac461a2e372b188b5beec1ab2b238a6c46ab47da5.jpg", "img_caption": ["Figure 1: (a) High-precision modeling for dynamical systems; (b.1) Classification of classical mechanical systems based on (Tolman, 1938; Lamb and Roberts, 1998);(b.2) Tim-Reversal Symmetry illustration;(b.3) Error accumulation in numerical solvers. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "2018; Rubanova et al., 2019) have emerged as a potent solution for modeling system dynamics in a continuous manner, which offer superior prediction accuracy over discrete models in the long-range, and can handle systems with partial observations. In particular, GraphODEs (Huang et al., 2020; Luo et al., 2023b; Zang and Wang, 2020; Jiang et al., 2023; Luo et al., 2023c) extend NeuralODEs to model interacting (multi-agent) dynamical systems, where agents co-evolve and form trajectories jointly. ", "page_idx": 1}, {"type": "text", "text": "However, the complexity of dynamical systems necessitates large amounts of data. Models trained on limited data risk violating fundamental physical principles such as energy conservation. A promising strategy to improve modeling accuracy involves incorporating physical inductive biases (Raissi et al., 2019; Cranmer et al., 2020). Existing models like Hamiltonian Neural Networks (HNNs) (Greydanus et al., 2019; Sanchez-Gonzalez et al., 2019) strictly enforce energy conservation, yielding more accurate predictions for energy-conservative systems. However, not all real-world systems strictly adhere to energy conservation, and they may adhere to various physical priors. Other methods that model both energy-conserving and dissipative systems, as well as reversible systems, offer more flexibility (Zhong et al., 2020; Gruber et al., 2024). Nevertheless, they often rely on prior knowledge of the system and are also limited to systems with corresponding physical priors. Such system diversity largely limits the usage of existing models which are designed for specific physical prior. ", "page_idx": 1}, {"type": "text", "text": "To address this, we present a framework that achieves high-precision modeling for a wide range of dynamical systems from the numerical aspect, by enforcing Time-Reversal Symmetry (TRS) via a novel regularization term. Specifically, TRS posits that a system\u2019s dynamics should remain invariant when time is reversed (Lamb and Roberts, 1998). To incorporate TRS, we propose a simple-yet-effective self-supervised regularization term that acts as a soft constraint. This term aligns forward and backward trajectories predicted by a neural network and we use GraphODE as the backbone. We theoretically prove that the TRS loss effectively minimizes higher-order Taylor expansion terms during ODE integration, offering a general numerical advantage for improving modeling accuracy across a wide array of systems, regardless of their physical properties. It forces the model to capture fine-grained physical properties such as jerk (the derivatives of accelerations) and provides more regularization for long-term prediction. We also justify our TRS design choice, showing case its superior performance both analytically and empirically. We name the model as TREAT (Time-Reversal Symmetry ODE). ", "page_idx": 1}, {"type": "text", "text": "Note that TRS itself is a physical prior, that is broader than energy conservation as depicted in Figure 1(b.1). It covers classical energy-conservative systems such as Newtonian mechanics, and also non-conservative, reversible systems like Stokes flow (Pozrikidis, 2001), commonly encountered in microfluidics (Kim and Karrila, 2013; Cao and Li, 2018; Cao et al., 2019). Therefore, TRS loss achieves high-precision modeling from both the physical aspect, and the numerical aspect as shown in Figure 1(a), making it domain-agnostic and widely applicable to various dynamical systems. We systematically conduct experiments across 9 diverse datasets spanning across 1.) single-agent, multi-agent systems; 2.) simulated and real-world systems; and 3.) systems with different physical priors. TREAT consistently outperforms state-of-the-art baselines, affirming its effectiveness and versatility across various dynamic scenarios. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Our primary contributions can be summarized as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We introduce TREAT, a powerful framework that achieves high-precision modeling for a wide range of systems from the numerical aspect, by enforcing Time-Reversal Symmetry (TRS) via a regularization term. \u2022 We establish the first theoretical proof that the time-reversal symmetry loss could in general help learn more fine-grained and long-context system dynamics from the numerical aspect, regardless of systems\u2019 physical properties (even irreversible systems). This bridges the specific physical implication and the general numerical beneftis of the physical prior -TRS. \u2022 We present empirical evidence of TREAT\u2019s state-of-the-art performance in a variety of systems over 9 datasets, including real-world & simulated systems, etc. It yields a significant MSE improvement of $11.5\\%$ on the challenging chaotic triple-pendulum system. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries and Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We represent a dynamical system as a graph $\\mathcal{G}\\;=\\;(\\mathcal{V},\\mathcal{E})$ , where $\\nu$ denotes the node set of $N$ agents3 and $\\mathcal{E}$ denotes the set of edges representing their physical interactions. For simplicity, we assumed $\\mathcal{G}$ to be static over time. Single-agent dynamical system is a special case where the graph only has one node. In the following, we use the multi-agent setting by default to illustrate our model. We denote $\\pmb{X}(t)\\,\\in\\,\\mathbb{R}^{N\\times d}$ as the feature matrix at timestamp $t$ for all agents, with $d$ as the feature dimension. Model input consists of trajectories of feature matrices over $M$ historical timestamps $X(t_{-M:-1})=\\{X(t_{-M}),\\dots,X(t_{-1})\\}$ and $\\mathcal{G}$ . The timestamps $t_{-1},\\cdot\\cdot\\cdot\\,,t_{-M}<0$ can have non-uniform intervals and take any continuous values. Our goal is to learn a neural simulator $f_{\\theta}(\\cdot):\\left[X(t_{-M:-1}),\\mathcal{G}\\right]\\rightarrow Y(t_{0:K})$ , which predicts node dynamics $\\mathbf{}Y(t)$ in the future on timestamps $0=t_{0}<\\cdot\\cdot\\cdot<t_{K}=T$ sampled within $[0,T]$ . We use ${\\pmb y}_{i}(t)$ to denote the targeted dynamic vector of agent $i$ at time $t$ . In some cases when we are only predicting system feature trajectories, $\\mathbf{Y}(\\cdot)\\equiv X(\\cdot)$ . ", "page_idx": 2}, {"type": "text", "text": "2.1 NeuralODE for Dynamical Systems ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "NeuralODEs (Chen et al., 2018; Rubanova et al., 2019) are a family of continuous models that define the evolution of dynamical systems by ordinary differential equations (ODEs). The state evolution can be described as: z\u02d9i(t) := dzdit(t) $\\begin{array}{r}{\\dot{z}_{i}(\\dot{t})\\;:=\\;\\frac{d z_{i}(t)}{d t}\\;=\\;g\\left(z_{1}(t),z_{2}(t)\\cdot\\cdot\\cdot z_{N}(t)\\right)}\\end{array}$ , where $\\boldsymbol{z}_{i}(t)\\,\\in\\,\\mathbb{R}^{d}$ denotes the latent state variable for agent $i$ at timestamp $t$ . The ODE function $g$ is parameterized by a neural network such as Multi-Layer Perception (MLP), which is automatically learned from data. GraphODEs (Poli et al., 2019; Huang et al., 2020; Luo et al., 2023b; Wen et al., 2022; Huang et al., 2024) are special cases of NeuralODEs, where $g$ is a Graph Neural Network (GNN) to capture the continuous interaction among agents. ", "page_idx": 2}, {"type": "text", "text": "GraphODEs have been shown to achieve superior performance, especially in long-range predictions and can handle data irregularity issues. They usually follow the encoder-processor-decoder architecture, where an encoder first computes the latent initial states $z_{1}(t_{0}),\\bar{\\cdot}\\cdot\\cdot z_{N}(t_{0})$ for all agents simultaneously based on their historical observations as in Eqn 1. ", "page_idx": 2}, {"type": "equation", "text": "$$\nz_{1}(t_{0}),z_{2}(t_{0}),...,z_{N}(t_{0})=f_{\\mathrm{ENC}}\\big(X(t_{-M:-1}),\\mathcal{G}\\big)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then the GNN-based ODE predicts the latent trajectories starting from the learned initial states. The latent state $z_{i}(t)$ can be computed at any desired time using a numerical solver such as RungeKuttais (Schober et al., 2019) as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nz_{i}(t)=\\mathrm{ODE-Solver}\\big(g,[z_{1}(t_{0}),\\ldots z_{N}(t_{0})],t\\big)=z_{i}(t_{0})+\\int_{t_{0}}^{t}g\\left(z_{1}(t),z_{2}(t)\\cdot\\cdot\\cdot z_{N}(t)\\right)d t.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Finally, a decoder extracts the predicted dynamics $\\hat{\\pmb y}_{i}(t)$ based on the latent states $z_{i}(t)$ for any timestamp $t$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\pmb{y}}_{i}(t)=f_{\\mathrm{DEC}}(\\pmb{z}_{i}(t)).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "However, vanilla GraphODEs can violate physical properties of a system, resulting in unrealistic predictions. We therefore propose to inject physics-informed regularization term to make more accurate predictions. ", "page_idx": 3}, {"type": "text", "text": "2.2 Time-Reversal Symmetry (TRS) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider a dynamical system described in the form of dxd(tt) $\\begin{array}{r l}{\\frac{d\\pmb{x}(t)}{d t}}&{{}=}\\end{array}$ $F({\\pmb x}(t))$ , where $\\pmb{x}(t)\\,\\in\\,\\Omega$ is the observed states such as positions. The system is said to follow the TimeReversal Symmetry if there exists a reversing operator $R:\\Omega\\mapsto\\Omega$ such that (Lamb and Roberts, 1998): ", "page_idx": 3}, {"type": "image", "img_path": "iWlqbNE8P7/tmp/495900674a1ee03e928060ce1b7ca931292222fe47fc7e6db3bc9b5785d081c6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{d\\big(R\\circ{\\pmb x}(t)\\big)}{d t}=-F\\big(R\\circ{\\pmb x}(t)\\big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\circ$ denote the action of functional $R$ on the function $\\textbf{\\em x}$ . ", "page_idx": 3}, {"type": "text", "text": "Figure 2: Illustration of time-reversal symmetry based on Lemma 2.1.The total length of the trajectory is $t_{K}-t_{0}=T$ . $t_{k}^{\\prime}$ is the time index in the reverse trajectory, which points to the same time as $t_{K-k}$ in the forward trajectory. ", "page_idx": 3}, {"type": "text", "text": "Intuitively, we can assume ${\\mathbf{}}x(t)$ is the position of a flying ball and the con", "page_idx": 3}, {"type": "text", "text": "ventional reversing operator is defined as $R:{\\pmb x}\\mapsto R\\circ{\\pmb x},R\\circ{\\pmb x}(t)={\\pmb x}(-t)$ . This implies when ${\\mathbf{}}x(t)$ is a forward trajectory position with initial position ${\\pmb x}(0)$ , $\\pmb{x}(-t)$ is then a position in the timereversal trajectory, where $\\pmb{x}(-t)$ is calculated using the same function $F$ , but with the integration time reversed, i.e. $d t\\mapsto d(-t)$ . Eqn 4 shows how to create the reverse trajectory of a flying ball: at each position, the velocity (i.e., the derivative of position with respect to time) should be the opposite. In neural networks, we usually model trajectories in the latent space via $_{\\textit{z}}$ (Sanchez-Gonzalez et al., 2020), which can be decoded back to real observation state i.e. positions. Therefore, we apply the reversal operator for $_{z}$ . ", "page_idx": 3}, {"type": "text", "text": "Now we introduce a time evolution operator $\\phi_{\\tau}$ such that $\\phi_{\\tau}\\circ z(t)=z(t+\\tau)$ for arbitrary $t,\\tau\\in\\mathbb{R}$ . It satisfies $\\phi_{\\tau_{1}}\\circ\\phi_{\\tau_{2}}=\\phi_{\\tau_{1}+\\tau_{2}}$ , where $\\circ$ denotes composition. The time evolution operator helps us to move forward (when $\\tau>0$ ) or backward (when $\\tau<0$ ) through time, thus forming a trajectory. Based on (Lamb and Roberts, 1998), in terms of the evolution operator, Eqn 4 implies: ", "page_idx": 3}, {"type": "equation", "text": "$$\nR\\circ\\phi_{t}=\\phi_{-t}\\circ R=\\phi_{t}^{-1}\\circ R,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which means that moving forward $t$ steps and then turning to the opposite direction is equivalent to firstly turning to the opposite direction and then moving backwards $t$ steps4. Eqn 5 has been widely used to describe time-reversal symmetry in existing literature (Huh et al., 2020; Valperga et al., 2022). Nevertheless, we propose the following lemma, which is more intuitive to understand and straightforward to guide the design of our time-reversal regularizer. ", "page_idx": 3}, {"type": "text", "text": "Lemma 2.1. Eqn 5 is equivalent to $R\\circ\\phi_{t}\\circ R\\circ\\phi_{t}=I_{\\mathrm{~\\,~}}$ , where $I$ denotes identity mapping. ", "page_idx": 3}, {"type": "text", "text": "Lemma 2.1 means if we move $t$ steps forward, then turn to the opposite direction, and then move forward for $t$ more steps, it shall restore back to the same state. This is illustrated in Figure 2 where the reverse trajectory should be the same as the forward trajectory.5 It can be understood as rewinding a video to the very beginning. The proof of Lemma 2.1 is in Appendix A.2. ", "page_idx": 3}, {"type": "text", "text": "3 Method: TREAT ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We present a novel framework TREAT that achieves high-precision modeling for a wide range of systems from the numerical aspect, by enforcing Time-Reversal Symmetry (TRS) via a regularization term. It improves modeling accuracy regardless of systems\u2019 physical properties. We first introduce our architecture design, followed by theoretical analysis to explain its numerical benefits. ", "page_idx": 3}, {"type": "image", "img_path": "iWlqbNE8P7/tmp/2ac14e3288326f81b67bea52730333e2f292e9621bf53f9951df0440398c9e7f.jpg", "img_caption": ["Figure 3: Overall framework of TREAT. $O_{1},O_{2},O_{3}$ are connected agents. It follows the encoderprocessor-decoder architecture introduced in Sec 2.1. A novel TRS loss is incorporated to improve modeling accuracy across systems from the numerical aspect, regardless of their physical properties. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "TREAT uses GraphODE (Huang et al., 2020) as the backbone and flexibly incorporates TRS as a regularization term based on Lemma 2.1. This term aligns model forward and reverse trajectories. In practice, our model predicts the forward trajectories at a series of timestamps $\\{t_{k}\\}_{k=0}^{K}$ as ground truth ssoaabtsmieserf vysaietnriigoe $K$ $0=t_{0}<t_{1}<\\cdot\\cdot\\cdot<t_{K}=T$ .t aeTr tdhh eoa rtn eetv,h eerw svheia tclruha ejsew ceot fod rteiheneso  ttaeir mea sea $\\{t_{k}^{\\prime}\\}_{k=0}^{K}$ $0=t_{0}^{\\prime}<t_{1}^{\\prime}<\\cdot\\cdot<t_{K}^{\\prime}=T$   \n$t_{k}^{\\prime}$ in the reverse trajectories do not represent real time, but serve as indexes of reverse trajectories. This leads to the relation $t_{K-k}^{\\prime}=T-t_{k}$ , which means the reverse trajectories at timestamp $t_{K-k}^{\\prime}$ correspond to the forward trajectories at time $t_{k}$ . For example, $t_{0}^{\\prime}=T-t_{K}=0$ . It indicates $t_{0}^{\\prime}$ and $t_{K}$ are both pointing to the same real time $T$ , which is the ending point of the forward trajectory as shown in Figure 3. Based on Lemma 2.1, the difference of the two trajectories at any observed time should be small, i.e. $z^{\\mathrm{fwd}}(t_{k})\\approx z^{\\mathrm{rev}}(t_{K-k}^{\\prime})$ . This serves as the guideline for our regularizer design. The weight of the regularizer is also adjustable to adapt different systems. The overall framework is depicted in Figure 3. ", "page_idx": 4}, {"type": "text", "text": "3.1 Time-Reversal Symmetry Loss and Training ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Forward Trajectory Prediction and Reconstruction Loss. For multi-agent systems, we utilize the GNN operator described in (Kipf et al., 2018) as our ODE function $g(\\cdot)$ , which drives the system to move forward and output the forward trajectories for latent states $z_{i}^{\\mathrm{fwd}}(t)$ at each continuous time $t\\in[0,T]$ and each agent $i$ .We then employ a Multilayer Perceptron (MLP) as a decoder to predict output trajectories $\\hat{y}_{i}^{\\mathrm{fwd}}(t)$ based on the latent states. We summarize the whole procedure as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{z}_{i}^{\\mathrm{fwd}}(t):=\\frac{d z_{i}^{\\mathrm{fwd}}(t)}{d t}=g(z_{1}^{\\mathrm{fwd}}(t),z_{2}^{\\mathrm{fwd}}(t),\\cdot\\cdot\\cdot z_{N}^{\\mathrm{fwd}}(t)),}\\\\ &{z_{i}^{\\mathrm{fwd}}(t_{0})=f_{\\mathrm{ENC}}(X(t_{-M:-1}),\\mathcal{G}),\\quad\\hat{y}_{i}^{\\mathrm{fwd}}(t)=f_{\\mathrm{DEC}}(z_{i}^{\\mathrm{fwd}}(t)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To train the model, we use the reconstruction loss that minimizes the L2 distance between predicted forward trajectories $\\{\\hat{\\pmb{y}}_{i}^{\\mathrm{fwd}}(t_{k})\\}_{k=0}^{K}$ and the ground truth trajectories $\\{y_{i}(t_{k})\\}_{k=0}^{K}$ as : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{p r e d}=\\sum_{i=1}^{N}\\sum_{k=0}^{K}\\Big\\|y_{i}(t_{k})-\\hat{y}_{i}^{\\mathrm{fwd}}(t_{k})\\Big\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Reverse Trajectory Prediction and Regularization Loss. We design a novel time-reversal symmetry loss as a soft constraint to flexibly regulate systems\u2019 behavior based on Lemma 2.1. Specifically, we first compute the latent reverse trajectories $z^{\\mathrm{rev}}(t)$ by starting from the ending state of the forward one, traversed back over time. We then employ the decoder to output dynamic trajectories $\\pmb{y}^{\\mathrm{rev}}(t)$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{z}_{i}^{\\mathrm{rev}}(t):=\\frac{d z_{i}^{\\mathrm{rev}}(t)}{d t}=-g(z_{1}^{\\mathrm{rev}}(t),z_{2}^{\\mathrm{rev}}(t),\\cdot\\cdot\\cdot z_{N}^{\\mathrm{rev}}(t)),}\\\\ &{z_{i}^{\\mathrm{rev}}(t_{0}^{\\prime})=z_{i}^{\\mathrm{fwd}}(t_{K}),\\quad\\hat{y}_{i}^{\\mathrm{rev}}(t)=f_{\\mathrm{DEC}}(z_{i}^{\\mathrm{rev}}(t)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Next, based on Lemma 2.1, if the system follows Time-Reversal Symmetry, the forward and backward trajectories shall be exactly overlap. We thus design the reversal loss by minimizing the L2 distances between model forward and backward trajectories decoded from the latent trajectories: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{r e v e r s e}=\\sum_{i=1}^{N}\\sum_{k=0}^{K}\\Big\\|\\hat{y}_{i}^{\\mathrm{fwd}}(t_{k})-\\hat{y}_{i}^{\\mathrm{rev}}(t_{K-k}^{\\prime})\\Big\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Finally, we jointly train TREAT as a weighted combination of the two losses: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{p r e d}+\\alpha\\mathcal{L}_{r e v e r s e}=\\sum_{i=1}^{N}\\sum_{k=0}^{K}\\left\\|y_{i}(t_{k})-\\hat{y}_{i}^{\\mathrm{{twd}}}(t_{k})\\right\\|_{2}^{2}+\\alpha\\sum_{i=1}^{N}\\sum_{k=0}^{K}\\left\\|\\hat{y}_{i}^{\\mathrm{{fwd}}}(t_{k})-\\hat{y}_{i}^{\\mathrm{{rev}}}(t_{K-k}^{\\prime})\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\alpha$ is a positive coefficient to balance the two losses based on different targeted systems. ", "page_idx": 5}, {"type": "text", "text": "Remark. The computational time of $\\mathcal{L}_{r e v e r s e}$ is of the same scale as the reconstruction loss $\\mathcal{L}_{p r e d}$ As the computation process of the reversal loss is to first use the ODE solver to generate the reverse trajectories, which has the same computational overhead as computing the forward trajectories, and then compute the L2 distances. ", "page_idx": 5}, {"type": "text", "text": "3.2 Theoretical Analysis of Time-Reversal Symmetry Loss ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We next theoretically show that the time-reversal symmetry loss numerically helps to improve prediction accuracy in general, regardless of systems\u2019 physical properties. Specifically, we show that it minimizes higher-order Taylor expansion terms during the ODE integration steps. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1. Let $\\Delta t$ denote the integration step size in an ODE solver and $T$ be the prediction length. The reconstruction loss $\\mathcal{L}_{p r e d}$ defined in Eqn 7 is $\\mathcal{O}(T^{3}\\Delta t^{2})$ . The time-reversal loss $\\mathcal{L}_{r}$ everse defined in Eqn 9 is $\\mathcal{O}(T^{5}\\Delta t^{4})$ . ", "page_idx": 5}, {"type": "text", "text": "We prove Theorem 3.1 in Appendix A.3. From Theorem 3.1, we can see two nice properties of our proposed time-reversal loss: 1) Regarding the relationship to $\\Delta t$ , $\\mathcal{L}_{r e v e r s e}$ is optimizing a highorder term $\\Delta t^{4}$ , which forces the model to predict fine-grained physical properties such as jerk (the derivatives of accelerations). In comparison, the reconstruction loss optimizes $\\Delta t^{2}$ , which mainly guides the model to predict the locations/velocities accurately. Therefore, the combined loss enables our model to be more noise-tolerable; 2) Regarding the relationship to $T$ , $\\mathcal{L}_{r e v e r s e}$ is more sensitive to total sequence length $(T^{5})$ , thus it provides more regularization for long-context prediction, a key challenge for dynamic modeling. ", "page_idx": 5}, {"type": "text", "text": "TRS Loss Design Choice. We define $\\mathcal{L}_{r e v e r s e}$ as the distance between model forward trajectories and backward trajectories. Based on the definition of TRS in Sec. 2.2, there are other implementation choices. One prior work TRS-ODE (Huh et al., 2020) designed a TRS loss based on Eqn 5, where a reverse trajectory shares the same starting point as the forward one. However, we show that our implementation based on Lemma 2.1 to approximate time-reversal symmetry has a lower maximum error compared to their implementation below, supported by empirical experiments in Sec. 4.2. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.2. Let $\\mathcal{L}_{r e v e r s e}$ be the TRS implementation of TREAT based on Lemma 2.1, Lreverse2 be the one in (Huh et al., 2020) based on Eqn 5. When the reconstruction loss defined in Eqn 7 of both methods are equal, and the two TRS losses are equal, i.e. $\\mathcal{L}_{r e v e r s e}=\\mathcal{L}_{r e v e r s e2}$ , the maximum error between the reversal and ground truth trajectory for each agent, i.e. $M a x E r r o r_{g t\\_r e v}\\,=$ $\\mathrm{max}_{k\\in[K]}\\,\\|y_{i}(t_{k})-\\hat{y}_{i}^{\\mathrm{rev}}(t_{K-k}^{\\prime})\\|_{2}$ for $i=1,2\\cdot\\cdot\\cdot N_{i}$ , made by TREAT is smaller. ", "page_idx": 6}, {"type": "text", "text": "We prove Lemma 3.2 in Appendix A.4. Another implementation is to minimize the distances between model backward trajectories and ground truth trajectories. When both forward and backward trajectories are close to ground-truth, they are implicitly symmetric. The major drawback is that at the early stage of learning when the forward is far away from ground truth $(\\mathcal{L}_{p r e d})$ , such implicit regularization does not force time-reversal symmetry, but introduces more noise. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We conduct systematic evaluations over five multi-agent systems including three 5-body spring systems (Kipf et al., 2018), a complex chaotic pendulum system and a real-world motion capture dataset (Carnegie Mellon University, 2003); and four single-agent systems including three spring systems (with only one node) and a chaotic strange attractors system (Huh et al., 2020). ", "page_idx": 6}, {"type": "text", "text": "The settings of spring systems include: 1) conservative, i.e. no interactions with the environments, we call it Simple Spring; 2) non-conservative with frictions, we call it Damped Spring; 3) nonconservative with periodic external forces, we call it Forced Spring. The Pendulum system contains three connected sticks in a 2D plane. It is highly sensitive to initial states, with minor disturbances leading to significantly different trajectories (Shinbrot et al., 1992; Awrejcewicz et al., 2008). The realworld motion capture dataset (Carnegie Mellon University, 2003) describes the walking trajectories of a person, each tracking a single joint. We call it Human Motion. The strange attractor consists of symmetric attractor/repellor force pairs and is chaotic (Sprott, 2015). It is also highly sensitive to the initial states (Koppe et al., 2019). We call it Attractor. ", "page_idx": 6}, {"type": "text", "text": "Towards physical properties, Simple Spring and Pendulum are conservative and reversible; Force Spring and Attractor are reversible but non-conservative; Damped Spring are irreversible and nonconservative. For Human Motion, it does not adhere to specific physical laws since it is a real-world dataset. Details of the datasets and generation pipelines can be found inAppendix C. ", "page_idx": 6}, {"type": "text", "text": "Task Setup. We conduct evaluation by splitting trajectories into two halves: $[t_{1},t_{M}]$ , $[t_{M+1},t_{K}]$ where timestamps can be irregular. We condition the first half of observations to make predictions for the second half as in (Rubanova et al., 2019). For spring datasets and Pendulum, we generate irregular-sampled trajectories and set the training samples to be 20,000 and testing samples to be 5,000 respectively. For Attractor, We generate 1,000 and 50 trajectories for training and testing respectively following Huh et al. (2020). $10\\%$ of training samples are used as validation sets and the maximum trajectory prediction length is 60. Details can be found in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We compare TREAT against three baseline types: 1) pure data-driven approaches including LG-ODE (Huang et al., 2020) and LatentODE (Rubanova et al., 2019), where the first one is a multiagent approach considering pair-wise interactions, and the second one is a single-agent approach that predicts each trajectory independently; 2) energy-preserving HODEN (Greydanus et al., 2019); and 3) time-reversal TRS-ODEN (Huh et al., 2020). ", "page_idx": 6}, {"type": "text", "text": "The latter two are single-agent approaches and require initial states as given input. To handle missing initial states in our dataset, we approximate the initial states for the two methods via linear spline interpolation (Endre S\u00fcli, 2003). In addition, we substitute the ODE network in TRS-ODEN with a GNN (Kipf et al., 2018) as TRS-ODENGNN, which serves as a new multi-agent approach for fair comparison. HODEN cannot be easily extended to the multi-agent setting as replacing the ODE function with a GNN can violate energy conservation of the original HODEN. For running LGODE and TREAT on single-agent datasets, we only include self-loop edges in the graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ , which makes the ODE function $g$ a simple MLP. Implementation details can be found in Appendix D.2. ", "page_idx": 6}, {"type": "table", "img_path": "iWlqbNE8P7/tmp/ee264be52d5fd457b47d3bca2cc2a796601a3b7b6c4aae70f919cade958a2127.jpg", "table_caption": ["Table 1: Evaluation results on MSE $(10^{-2})$ . Best results are in bold numbers and second-best results are in underline numbers. Human Motion is a real-world dataset and all others are simulated datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.1 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 1 shows the prediction performance on both multi-agent systems and single-agent systems measured by mean squared error (MSE). We can see that TREAT consistently surpasses other models, highlighting its generalizability and the efficacy of the proposed TRS loss. ", "page_idx": 7}, {"type": "text", "text": "For multi-agent systems, approaches that consider interactions among agents (LG-ODE, TRSODENGNN, TREAT) consistently outperform single-agent baselines (LatentODE, HODEN, TRSODEN), and TREAT achieves the best performance across datasets. ", "page_idx": 7}, {"type": "text", "text": "The chaotic nature of the Pendulum system and the Attractor system, with their sensitivity to initial states 6, poses extreme challenges for dynamic modeling. This leads to highly unstable predictions for models like HODEN and TRS-ODEN, as they estimate initial states via inaccurate linear spline interpolation (Endre S\u00fcli, 2003). In contrast, LatentODE, LG-ODE, and TREAT employ advanced encoders that infer latent states from observed data and demonstrate superior accuracy. Among them, TREAT achieves the most accurate predictions, further showing its robust generalization capabilities. ", "page_idx": 7}, {"type": "text", "text": "We observe that misapplied inductive biases can degrade results, which limits the usage of physicsinformed methods that are designed for individual physical prior such as HODEN. HODEN only excels on energy-conservative systems, such as Simple Spring compared with LatentODE and TRSODEN in the multi-agent setting. Its performance drop dramatically on Force Spring, Damped Spring, and Attractor. Note that HODEN naively forces each agent to be energy-conservative, instead of the whole system. Therefore, it performs poorly than LG-ODE, TREAT in the multi-agent settings. ", "page_idx": 7}, {"type": "text", "text": "For the Human Motion dataset, characterized by its dynamic ambiguity as it does not adhere to specific physical laws, we cannot directly determine whether it is conservative or time-reversal. For such a system with an unknown nature, TREAT outperforms other purely data-driven methods significantly, showcasing its strong numerical benefits in improving prediction accuracy across diverse system types. This is also shown by its superior performance on Damped Spring, which is irreversible. ", "page_idx": 7}, {"type": "image", "img_path": "iWlqbNE8P7/tmp/cb757cb23a248a8045893cda4a0c1e26613de7dceb0cdf96cd85d5d84c0834be.jpg", "img_caption": ["Figure 4: Varying prediction lengths across multi-agent datasets (Pendulum MSE is in log values). "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "iWlqbNE8P7/tmp/7d6860f4ee83bf7ef769e15388c9582a0df2e4e7702cce7e7858f2c10411134b.jpg", "img_caption": ["Figure 5: Varying $\\alpha$ values across multi-agent datasets. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.2 Ablation and Sensitivity Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Ablation on implementation of $\\mathcal{L}_{r e v e r s e}$ . We conduct two ablation by changing the implementation of $\\mathcal{L}_{r e v e r s e}$ discussed in Sec. 3.2: 1) TREA $\\Gamma_{C_{r e v}=\\mathrm{gt-rev}}$ , which computes the reversal loss as the L2 distance between ground truth trajectories to model backward trajectories; 2) $\\mathrm{TREAT}_{\\mathcal{L}_{r e v}=\\mathrm{rev}2}$ , which implements the TRS loss based on Eqn 5 as in TRS-ODEN (Huh et al., 2020). From the last block of Table 1, we can clearly see that our implementation achieves the best performance against the two. ", "page_idx": 8}, {"type": "text", "text": "Evaluation across prediction lengths. We vary the maximum prediction lengths from 20 to 60 and report model performance as shown in Figure 4. As the prediction step increases, TREAT consistently maintains optimal prediction performance, while other baselines exhibit significant error accumulations. The performance gap between TREAT and baselines widens when making long-range predictions, highlighting the superior predictive capability of TREAT. ", "page_idx": 8}, {"type": "text", "text": "Evaluation across different $\\alpha$ . We vary the values of the coefficient $\\alpha$ defined in Eqn 10, which balances the reconstruction loss and the TRS loss. Figure 5 demonstrates that the optimal $\\alpha$ values being neither too high nor too low. This is because when $\\alpha$ is too small, the model tends to neglect the TRS physical bias, resulting in error accumulations. Conversely, when $\\alpha$ becomes too large, the model can emphasize TRS at the cost of accuracy. Nonetheless, across different $\\alpha$ values, TREAT consistently surpasses the purely data-driven LG-ODE, showcasing its superiority and flexibility in modeling diverse dynamical systems. ", "page_idx": 8}, {"type": "text", "text": "We study TREAT\u2019s sensitivity towards solver choice and observation ratios in Appendix E.1 and Appendix E.2 respectively. ", "page_idx": 8}, {"type": "image", "img_path": "iWlqbNE8P7/tmp/ab6af8e1580903ca9db4f1766055b78e8db979dfee8b7b75436cabbe7b15149c.jpg", "img_caption": ["Figure 6: Visualization for 5-body spring systems (trajectory starts from light to dark colors). "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "iWlqbNE8P7/tmp/7945c22434b5b6f958d162eb6f717aeac67f2a7a63151a58d2892a5ee478a8b2.jpg", "img_caption": ["Figure 7: TRS loss visualization across multi-agent datasets (scales of two y-axes are different). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.3 Visualizations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Trajectory Visualizations. Model predictions and ground truth are visualized in Figure 6. As HODEN is a single-agent baseline that individually forces every agent\u2019s energy to be constant over time which is not valid, the predicted trajectories is having the largest errors and systems\u2019 total energy is not conserved for all datasets. The purely data-driven LG-ODE exhibits unrealistic energy patterns, as seen in the energy spikes in Simple Spring and Force Spring. In contrast, TREAT, incorporating reversal loss, generates realistic energy trends, and consistently produces trajectories closest to the ground truth, showing its superior performance. ", "page_idx": 9}, {"type": "text", "text": "Reversal Loss Visualizations To illustrate the issue of energy explosion from the purely data-driven LG-ODE, we visualize the TRS loss over training epochs from LG-ODE7 and TREAT in Figure 7. As results suggest, LG-ODE has increased TRS loss over training epochs, meaning it is violating the time-reversal symmetry sharply, in contrast to TREAT which has decreased reversal loss over epochs. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose TREAT, a deep learningframework that achieves high-precision modeling for a wide range of dynamical systems by injecting time-reversal symmetry as an inductive bias. TREAT features a novel regularization term to softly enforce time-reversal symmetry by aligning predicted forward and reverse trajectories from a GraphODE model. Notably, we theoretically prove that the regularization term effectively minimizes higher-order Taylor expansion terms during the ODE integration, which serves as a general numerical benefit widely applicable to various systems (even irreversible systems) regardless of their physical properties. Empirical evaluations on different kinds of datasets illustrate TREAT\u2019s superior efficacy in accurately capturing real-world system dynamics. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Currently, TREAT only incorporates inductive bias from the temporal aspect, while there are many important properties in the spatial aspect such as translation and rotation equivariance (Satorras et al., 2021; Han et al., 2022b; Xu et al., 2022). Future endeavors that combine biases from both temporal and spatial dimensions could unveil a new frontier in dynamical systems modeling. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially supported by NSF 2200274, NSF 2106859, NSF 2312501, NSF 2211557, NSF 1937599, NSF 2119643, NSF 2303037, NSF 2312501, DARPA HR00112290103/HR0011260656, HR00112490370, NIH U54HG012517, NIH U24DK097771, NASA, SRC JUMP 2.0 Center, Amazon Research Awards, and Snapchat Gifts. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "J. Awrejcewicz, G. Kudra, and G. Wasilewski. 2008. Chaotic zones in triple pendulum dynamics observed experimentally and numerically. Applied Mechanics and Materials (2008), 1\u201317. ", "page_idx": 9}, {"type": "text", "text": "Y. Cao, M. Chai, M. Li, and C. Jiang. 2023. Efficient learning of mesh-based physical simulation with bi-stride multi-scale graph neural network. In International Conference on Machine Learning. PMLR, 3541\u20133558.   \nY. Cao, X. Gao, and R. Li. 2019. A liquid plug moving in an annular pipe\u2013Heat transfer analysis. International Journal of Heat and Mass Transfer 139 (2019), 1065\u20131076.   \nY. Cao and R. Li. 2018. A liquid plug moving in an annular pipe\u2014Flow analysis. Physics of Fluids 30, 9 (2018).   \nCarnegie Mellon University. 2003. Carnegie-Mellon Motion Capture Database. http://mocap. cs.cmu.edu Online database.   \nB. Chang, L. Meng, E. Haber, L. Ruthotto, D. Begert, and E. Holtham. 2018. Reversible architectures for arbitrarily deep residual neural networks. In Proceedings of the AAAI conference on artificial intelligence, Vol. 32.   \nR. T. Q. Chen, B. Amos, and M. Nickel. 2021. Learning Neural Event Functions for Ordinary Differential Equations. International Conference on Learning Representations (2021).   \nT. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud. 2018. Neural Ordinary Differential Equations. In Advances in Neural Information Processing Systems.   \nM. Cranmer, S. Greydanus, S. Hoyer, P. Battaglia, D. Spergel, and S. Ho. 2020. Lagrangian neural networks. arXiv preprint arXiv:2003.04630 (2020).   \nD. F. M. Endre S\u00fcli. 2003. An Introduction to Numerical Analysis. Cambridge University Press. 293 pages.   \nS. Greydanus, M. Dzamba, and J. Yosinski. 2019. Hamiltonian neural networks. Advances in Neural Information Processing Systems (2019).   \nAnthony Gruber, Kookjin Lee, and Nathaniel Trask. 2024. Reversible and irreversible bracket-based dynamics for deep graph neural networks. Advances in Neural Information Processing Systems 36 (2024).   \nJiaqi Han, Wenbing Huang, Hengbo Ma, Jiachen Li, Joshua B. Tenenbaum, and Chuang Gan. 2022a. Learning Physical Dynamics with Subequivariant Graph Neural Networks. In Advances in Neural Information Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.). https://openreview.net/forum?id $=$ siG_S8mUWxf   \nJiaqi Han, Wenbing Huang, Tingyang Xu, and Yu Rong. 2022b. Equivariant graph hierarchy-based neural networks. Advances in Neural Information Processing Systems 35 (2022), 9176\u20139187.   \nZ. Hu, Y. Dong, K. Wang, and Y. Sun. 2020. Heterogeneous Graph Transformer. In Proceedings of the 2020 World Wide Web Conference.   \nZijie Huang, Jeehyun Hwang, Junkai Zhang, Jinwoo Baik, Weitong Zhang, Dominik Wodarz, Yizhou Sun, Quanquan Gu, and Wei Wang. 2024. Causal Graph ODE: Continuous Treatment Effect Modeling in Multi-agent Dynamical Systems. In Proceedings of the ACM Web Conference 2024 (Singapore, Singapore) (WWW \u201924). 4607\u20134617.   \nZ. Huang, Y. Sun, and W. Wang. 2020. Learning Continuous System Dynamics from IrregularlySampled Partial Observations. In Advances in Neural Information Processing Systems.   \nZ. Huang, Y. Sun, and W. Wang. 2021. Coupled Graph ODE for Learning Interacting System Dynamics. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.   \nZ. Huang, Y. Sun, and W. Wang. 2023. Generalizing Graph ODE for Learning Complex System Dynamics across Environments. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD \u201923). 798\u2013809.   \nI. Huh, E. Yang, S. J. Hwang, and J. Shin. 2020. Time-Reversal Symmetric ODE Network. In Advances in Neural Information Processing Systems.   \nS. Jiang, Z. Huang, X. Luo, and Y. Sun. 2023. CF-GODE: Continuous-Time Causal Inference for Multi-Agent Dynamical Systems. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.   \nS. Kim and S. J. Karrila. 2013. Microhydrodynamics: principles and selected applications. Courier Corporation.   \nT. Kipf, E. Fetaya, K. Wang, M. Welling, and R. Zemel. 2018. Neural Relational Inference for Interacting Systems. arXiv preprint arXiv:1802.04687 (2018).   \nG. Koppe, H. Toutounji, P. Kirsch, S. Lis, and D. Durstewitz. 2019. Identifying nonlinear dynamical systems via generative recurrent neural networks with applications to fMRI. PLoS computational biology 15, 8 (2019), e1007263.   \nR. Lam, A. Sanchez-Gonzalez, M. Willson, P. Wirnsberger, M. Fortunato, F. Alet, S. Ravuri, T. Ewalds, Z. Eaton-Rosen, W. Hu, A. Merose, S. Hoyer, G. Holland, O. Vinyals, J. Stott, A. Pritzel, S. Mohamed, and P. Battaglia. 2023. Learning skillful medium-range global weather forecasting. Science 382, 6677 (2023), 1416\u20131421.   \nJ. S. Lamb and J. A. Roberts. 1998. Time-reversal symmetry in dynamical systems: a survey. Physica D: Nonlinear Phenomena (1998), 1\u201339.   \nC. Li, F. Xia, R. Mart\u00edn-Mart\u00edn, M. Lingelbach, S. Srivastava, B. Shen, K. E. Vainio, C. Gokmen, G. Dharan, T. Jain, A. Kurenkov, K. Liu, H. Gweon, J. Wu, L. Fei-Fei, and S. Savarese. 2022. iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks. In Proceedings of the 5th Conference on Robot Learning.   \nJ. Liu, A. Kumar, J. Ba, J. Kiros, and K. Swersky. 2019. Graph normalizing flows. Advances in Neural Information Processing Systems 32 (2019).   \nI. Loshchilov and F. Hutter. 2019. Decoupled weight decay regularization. In The International Conference on Learning Representations.   \nY. Lu, S. Lin, G. Chen, and J. Pan. 2022. ModLaNets: Learning Generalisable Dynamics via Modularity and Physical Inductive Bias. In Proceedings of the 39th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (Eds.). PMLR, 14384\u2013 14397.   \nXiao Luo, Yiyang Gu, Huiyu Jiang, Hang Zhou, Jinsheng Huang, Wei Ju, Zhiping Xiao, Ming Zhang, and Yizhou Sun. 2024. PGODE: Towards High-quality System Dynamics Modeling. In Forty-first International Conference on Machine Learning.   \nXiao Luo, Haixin Wang, Zijie Huang, Huiyu Jiang, Abhijeet Sadashiv Gangan, Song Jiang, and Yizhou Sun. 2023a. CARE: Modeling Interacting Dynamics Under Temporal Environmental Variation. In Thirty-seventh Conference on Neural Information Processing Systems. https: //openreview.net/forum?id $=$ lwg3ohkFRv   \nX. Luo, J. Yuan, Z. Huang, H. Jiang, Y. Qin, W. Ju, M. Zhang, and Y. Sun. 2023b. HOPE: Highorder Graph ODE For Modeling Interacting Dynamics. In Proceedings of the 40th International Conference on Machine Learning.   \nXiao Luo, Jingyang Yuan, Zijie Huang, Huiyu Jiang, Yifang Qin, Wei Ju, Ming Zhang, and Yizhou Sun. 2023c. Hope: High-order graph ode for modeling interacting dynamics. In International Conference on Machine Learning. PMLR, 23124\u201323139.   \nKarolis Martinkus, Aurelien Lucchi, and Nathana\u00ebl Perraudin. 2021. Scalable graph networks for particle simulations. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 8912\u20138920.   \nR. Ni and A. H. Qureshi. 2022. Ntfields: Neural time fields for physics-informed robot motion planning. arXiv preprint arXiv:2210.00120 (2022).   \nJ. North. 2021. Formulations of classical mechanics. Forthcoming in A companion to the philosophy of physics. Routledge (2021).   \nT. Pfaff, M. Fortunato, A. Sanchez-Gonzalez, and P. Battaglia. 2021. Learning Mesh-Based Simulation with Graph Networks. In International Conference on Learning Representations.   \nTobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W Battaglia. 2020. Learning mesh-based simulation with graph networks. arXiv preprint arXiv:2010.03409 (2020).   \nM. Poli, S. Massaroli, J. Park, A. Yamashita, H. Asama, and J. Park. 2019. Graph neural ordinary differential equations. arXiv preprint arXiv:1911.07532 (2019).   \nC. Pozrikidis. 2001. Interfacial dynamics for Stokes flow. J. Comput. Phys. 169, 2 (2001), 250\u2013301.   \nM. Raissi, P. Perdikaris, and G. E. Karniadakis. 2019. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics 378 (2019), 686\u2013707.   \nY. Rubanova, R. T. Chen, and D. K. Duvenaud. 2019. Latent ordinary differential equations for irregularly-sampled time series. In Advances in Neural Information Processing Systems.   \nA. Sanchez-Gonzalez, V. Bapst, K. Cranmer, and P. Battaglia. 2019. Hamiltonian Graph Networks with ODE Integrators. In Advances in Neural Information Processing Systems.   \nA. Sanchez-Gonzalez, J. Godwin, T. Pfaff, R. Ying, J. Leskovec, and P. W. Battaglia. 2020. Learning to Simulate Complex Physics with Graph Networks. In Proceedings of the 37th International Conference on Machine Learning.   \nV. G. Satorras, E. Hoogeboom, and M. Welling. 2021. E (n) equivariant graph neural networks. In International conference on machine learning. PMLR, 9323\u20139332.   \nM. Schober, S. S\u00e4rkk\u00e4, and P. Hennig. 2019. A probabilistic model for the numerical solution of initial value problems. In Statistics and Computing. 99\u2013122.   \nH. Sepp and S. J\u00fcrgen. 1997. Long Short-term Memory. Neural computation (1997).   \nT. Shinbrot, C. Grebogi, J. Wisdom, and J. A. Yorke. 1992. Chaos in a double pendulum. American Journal of Physics 6 (1992), 491\u2013499.   \nJ. C. Sprott. 2015. Symmetric time-reversible flows with a strange attractor. International Journal of Bifurcation and Chaos 25, 05 (2015), 1550078.   \nT. Stachowiak and T. Okada. 2006. A numerical analysis of chaos in the double pendulum. Chaos, Solitons & Fractals 2 (2006), 417\u2013422.   \nE. C. Tolman. 1938. The Determiners of Behavior at a Choice Point. Psychological Review 45, 1 (1938), 1\u201341.   \nR. Valperga, K. Webster, D. Turaev, V. Klein, and J. Lamb. 2022. Learning Reversible Symplectic Dynamics. In Proceedings of The 4th Annual Learning for Dynamics and Control Conference.   \nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u02d9 U. Kaiser, and I. Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems.   \nHaixin Wang, Yadi Cao, Zijie Huang, Yuxuan Liu, Peiyan Hu, Xiao Luo, Zezheng Song, Wanjia Zhao, Jilin Liu, Jinan Sun, et al. 2024. Recent Advances on Machine Learning for Computational Fluid Dynamics: A Survey. arXiv preprint arXiv:2408.12171 (2024).   \nR. Wang, K. Kashinath, M. Mustafa, A. Albert, and R. Yu. 2020. Towards physics-informed deep learning for turbulent flow prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.   \nS. Wen, H. Wang, and D. Metaxas. 2022. Social ODE: Multi-agent Trajectory Forecasting with Neural Ordinary Differential Equations. In European Conference on Computer Vision.   \nMinkai Xu, Jiaqi Han, Aaron Lou, Jean Kossaif,i Arvind Ramanathan, Kamyar Azizzadenesheli, Jure Leskovec, Stefano Ermon, and Anima Anandkumar. 2024. Equivariant Graph Neural Operator for Modeling 3D Dynamics. In Forty-first International Conference on Machine Learning. https: //openreview.net/forum?id $\\cdot$ dccRCYmL5x   \nMinkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. 2022. Geodiff: A geometric diffusion model for molecular conformation generation. arXiv preprint arXiv:2203.02923 (2022).   \nC. Zang and F. Wang. 2020. Neural dynamics on complex networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.   \nYaofeng Desmond Zhong, Biswadip Dey, and Amit Chakraborty. 2020. Dissipative symoden: Encoding hamiltonian dynamics with dissipation and control into deep learning. arXiv preprint arXiv:2002.08860 (2020). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Theoretical Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Implementation of the Time-Reversal Symmetry Loss ", "page_idx": 14}, {"type": "text", "text": "Algorithm 1 The implementation of $\\mathcal{L}_{r e v e r s e}$   \nRequire: latent initial states $z_{i}^{\\mathrm{fwd}}(t_{0})$ ; the ODE function $g(\\cdot)$ ; number of agents $N$ :   \n1: for each $i\\in N$ do   \n2: Compute the latent forward trajectory at timestamps $\\{t_{k}\\}_{k=0}^{K}$ : $z_{i}^{\\mathrm{fwd}}(t_{k})=\\mathrm{ODE-Solver}\\big(g,[z_{1}^{\\mathrm{fwd}}(t_{0}),z_{2}^{\\mathrm{fwd}}(t_{0})...z_{N}^{\\mathrm{fwd}}(t_{0})],t_{k}\\big)$ Reach the final state $z_{i}^{\\mathrm{fwd}}(t_{K})$ .   \n3: The initial state of the reverse trajectory is defined as $z_{i}^{\\mathrm{rev}}(t_{0}^{\\prime})=z_{i}^{\\mathrm{fwd}}(t_{K})$ , and the dynamics of the system which is the ODE function $g(\\cdot)$ is also reversed as $-g(\\cdot)$ .   \n4: Compute the latent reverse trajectory at timestamps $\\{t_{k}^{\\prime}\\}_{k=0}^{K}$ , $z_{i}^{\\mathrm{rev}}(t_{k}^{\\prime})=\\mathrm{ODE-Solver}\\big(g,[z_{1}^{\\mathrm{rev}}(t_{0}^{\\prime}),z_{2}^{\\mathrm{rev}}(t_{0}^{\\prime})...z_{N}^{\\mathrm{rev}}(t_{0}^{\\prime})],t_{k}^{\\prime}\\big)$ .   \n5: $\\hat{y}_{i}^{\\mathrm{fwd}}(t_{k})=f_{\\mathrm{DEC}}(z_{i}^{\\mathrm{fwd}}(t_{k}))\\,,\\hat{y}_{i}^{\\mathrm{rev}}(t_{k}^{\\prime})=f_{\\mathrm{DEC}}(z_{i}^{\\mathrm{rev}}(t_{k}^{\\prime}))$   \n6: end for   \n7: $\\begin{array}{r}{\\stackrel{\\mathrm{suss~sva~son}}{\\mathcal{L}_{r e v e r s e}}=\\sum_{i=1}^{N}\\sum_{k=0}^{K}\\left\\|\\hat{\\pmb{y}}_{i}^{\\mathrm{fwd}}(t_{k})-\\hat{\\pmb{y}}_{i}^{\\mathrm{rev}}(t_{K-k}^{\\prime})\\right\\|_{2}^{2}}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "A.2 Proof of Lemma 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. The definition of time-reversal symmetry is given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\nR\\circ\\phi_{t}=\\phi_{-t}\\circ R=\\phi_{t}^{-1}\\circ R\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, $R$ is an involution operator, which means $R\\circ R=\\operatorname{I}$ . ", "page_idx": 14}, {"type": "text", "text": "First, we apply the time evolution operator $\\phi_{t}$ to both sides of Eqn 11: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\phi_{t}\\circ R\\circ\\phi_{t}=\\phi_{t}\\circ\\phi_{t}^{-1}\\circ R\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Simplifying, we obtain: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\phi_{t}\\circ R\\circ\\phi_{t}=R\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Next, we apply the involution operator $R$ to both sides of the equation: ", "page_idx": 14}, {"type": "equation", "text": "$$\nR\\circ\\phi_{t}\\circ R\\circ\\phi_{t}=R\\circ R\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $R\\circ R=\\operatorname{I}.$ , we finally arrive at: ", "page_idx": 14}, {"type": "equation", "text": "$$\nR\\circ\\phi_{t}\\circ R\\circ\\phi_{t}=\\operatorname{I}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which means the trajectories can overlap when evolving backward from the final state. ", "page_idx": 14}, {"type": "text", "text": "A.3 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let $\\Delta t$ denote the integration step size in an ODE solver and $T$ be the prediction length. The time stamps of the ODE solver are $\\{\\dot{t}_{j}\\}_{j=0}^{T}$ , where $t_{j+1}-t_{j}\\,=\\,\\Delta t$ for $j\\,=\\,0,\\cdot\\cdot\\cdot\\,,T(\\bar{T}\\,>\\,1)$ . Next suppose during the forward evolution, the updates go through states $z^{\\mathrm{fwd}}(t_{j})=(q^{\\mathrm{fwd}}(t_{j}),p^{\\mathrm{fwd}}(t_{j}))$ for $j\\,=\\,0,\\cdot\\cdot\\cdot\\,,T$ , where $\\pmb q^{\\mathrm{fwd}}(t_{j})$ is position, $\\pmb{p}^{\\mathrm{fwd}}(t_{j})$ is momentum, while during the reverse evolution they go through states $\\begin{array}{r}{\\bar{z}^{\\mathrm{rev}}(t_{j})=(\\pmb{q}^{\\mathrm{rev}}(t_{j}),\\bar{p}^{\\mathrm{rev}}(t_{j}))}\\end{array}$ for $j=0,\\cdot\\cdot\\cdot,T$ , in reverse order. The ground truth trajectory is $z^{\\mathrm{gt}}(t_{j})=(\\pmb{q}^{\\mathrm{gt}}(t_{j}),\\pmb{p}^{\\mathrm{gt}}(t_{j}))$ for $j=0,\\cdot\\cdot\\cdot,T$ . ", "page_idx": 14}, {"type": "text", "text": "For the sake of brevity in the ensuing proof, we denote $\\boldsymbol{z}^{\\mathrm{gt}}(t_{j})$ by $z_{j}^{\\mathrm{gt}}$ , $z^{\\mathrm{fwd}}(t_{j})$ by $z_{j}^{\\mathrm{fwd}}$ and $z^{\\mathrm{rev}}(t_{j})$ by $z_{j}^{\\mathrm{rev}}$ , and we will use Mathematical Induction to prove the theorem. ", "page_idx": 14}, {"type": "text", "text": "A.3.1 Reconstruction Loss $(\\mathcal{L}_{p r e d})$ Analysis. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "First, we bound the forward loss $\\begin{array}{r}{\\sum_{j=0}^{T}\\|z_{j}^{\\mathrm{fwd}}-z_{j}^{\\mathrm{gt}}\\|_{2}^{2}}\\end{array}$ . Since our method models the momentum and position of the system, we can write the following Taylor expansion of the forward process, where ", "page_idx": 14}, {"type": "text", "text": "for any $0\\le j<T$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{q_{j+1}^{\\mathrm{fwd}}=\\pmb{q}_{j}^{\\mathrm{fwd}}+(\\pmb{p}_{j}^{\\mathrm{fwd}}/m)\\Delta t+(\\dot{p}_{j}^{\\mathrm{fwd}}/2m)\\Delta t^{2}+\\mathcal{O}(\\Delta t^{3}),}\\\\ {p_{j+1}^{\\mathrm{fwd}}=\\pmb{p}_{j}^{\\mathrm{fwd}}+\\dot{p}_{j}^{\\mathrm{fwd}}\\Delta t+\\mathcal{O}(\\Delta t^{2}),}\\\\ {\\dot{p}_{j+1}^{\\mathrm{fwd}}=\\dot{p}_{j}^{\\mathrm{fwd}}+\\mathcal{O}(\\Delta t),}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and for the ground truth process, we also have from Taylor expansion that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{q_{j+1}^{\\mathrm{gt}}=q_{j}^{\\mathrm{gt}}+(p_{j}^{\\mathrm{gt}}/m)\\Delta t+(\\dot{p}_{j}^{\\mathrm{gt}}/2m)\\Delta t^{2}+\\mathcal{O}(\\Delta t^{3}),}\\\\ {p_{j+1}^{\\mathrm{gt}}=p_{j}^{\\mathrm{gt}}+\\dot{p}_{j}^{\\mathrm{gt}}\\Delta t+\\mathcal{O}(\\Delta t^{2}),}\\\\ {\\dot{p}_{j+1}^{\\mathrm{gt}}=\\dot{p}_{j}^{\\mathrm{gt}}+\\mathcal{O}(\\Delta t).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "With these, we aim to prove that for any $k=0,1,\\cdots\\,,T$ , the following hold : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\|q_{k}^{\\mathrm{fwd}}-q_{k}^{\\mathrm{gt}}\\|_{2}\\leq C_{2}^{\\mathrm{fwd}}k^{2}\\Delta t^{2},\\right.}\\\\ {\\|p_{k}^{\\mathrm{fwd}}-p_{k}^{\\mathrm{gt}}\\|_{2}\\leq C_{1}^{\\mathrm{fwd}}k\\Delta t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where Cf1wd and Cfwd are constants. ", "page_idx": 15}, {"type": "text", "text": "Base Case $k\\,=\\,0$ : Based on the initialization rules, it is obvious that $\\left\\lVert\\pmb{q}_{0}^{\\mathrm{fwd}}-\\pmb{q}_{0}^{\\mathrm{gt}}\\right\\rVert_{2}\\,=\\,0$ and $\\left\\lVert\\pmb{p}_{0}^{\\mathrm{fwd}}-\\pmb{p}_{0}^{\\mathrm{gt}}\\right\\rVert_{2}=0$ , thus (18a) and (18b) both hold for $k=0$ . ", "page_idx": 15}, {"type": "text", "text": "Inductive Hypothesis: Assume (18a) and (18b) hold for $k=j$ , which means: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\|q_{j}^{\\mathrm{fwd}}-q_{j}^{\\mathrm{gt}}\\|_{2}\\leq C_{2}^{\\mathrm{fwd}}j^{2}\\Delta t^{2},\\right.}\\\\ {\\left.\\|p_{j}^{\\mathrm{fwd}}-p_{j}^{\\mathrm{gt}}\\|_{2}\\leq C_{1}^{\\mathrm{fwd}}j\\Delta t,\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Inductive Proof: We need to prove (18a) and (18b) hold for $k=j+1$ . ", "page_idx": 15}, {"type": "text", "text": "First, using (16c) and (17c), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|{\\dot{p}}_{j+1}^{\\mathrm{fwd}}-{\\dot{p}}_{j+1}^{\\mathrm{gt}}\\right\\|_{2}=\\left\\|{\\dot{p}}_{j}^{\\mathrm{fwd}}-{\\dot{p}}_{j}^{\\mathrm{gt}}\\right\\|_{2}+{\\mathcal{O}}(\\Delta t)=\\left\\|{\\dot{p}}_{0}^{\\mathrm{fwd}}-{\\dot{p}}_{0}^{\\mathrm{gt}}\\right\\|_{2}+{\\mathcal{O}}\\big((j+1)\\Delta t\\big)={\\mathcal{O}}(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we iterate through $j,j-1,\\cdot\\cdot\\cdot\\:,0$ in the second equality. Then using (17b) and (16b), we get for $j+1$ that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\big\\|\\boldsymbol{p}_{j+1}^{\\mathrm{fwd}}-\\boldsymbol{p}_{j+1}^{\\mathrm{gt}}\\big\\|_{2}=\\big\\|\\big(\\boldsymbol{p}_{j}^{\\mathrm{fwd}}+\\dot{\\boldsymbol{p}}_{j}^{\\mathrm{fwd}}\\Delta t\\big)-\\big(\\boldsymbol{p}_{j}^{\\mathrm{gt}}+\\dot{\\boldsymbol{p}}_{j}^{\\mathrm{gt}}\\Delta t\\big)+\\mathcal{O}(\\Delta t^{2})\\|_{2}}}\\\\ &{\\leq\\big\\|\\boldsymbol{p}_{j}^{\\mathrm{fwd}}-\\boldsymbol{p}_{j}^{\\mathrm{gt}}\\big\\|_{2}+\\big\\|\\dot{\\boldsymbol{p}}_{j}^{\\mathrm{fwd}}-\\dot{\\boldsymbol{p}}_{j}^{\\mathrm{gt}}\\big\\|_{2}\\Delta t+\\mathcal{O}(\\Delta t^{2})}\\\\ &{\\leq\\big[C_{1}^{\\mathrm{fwd}}j+\\mathcal{O}(1)\\big]\\Delta t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the first inequality uses the triangle inequality, and in the second inequality we use (19b) as well as (20). We can see there exists $C_{1}^{\\mathrm{fwd}}$ such that the final expression above is upper bounded by $C_{1}^{\\mathrm{fwd}}(j+1)\\Delta t$ , with which the claim holds for $j+1$ . ", "page_idx": 15}, {"type": "text", "text": "Next for (18a), using (17a) and (16a), we get for any $j$ that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{j+1}^{\\mathrm{twd}}-q_{j+1}^{\\mathrm{gt}}\\|_{2}=\\big\\|\\big(q_{j}^{\\mathrm{fwd}}+(p_{j}^{\\mathrm{twd}}/m)\\Delta t+(\\dot{p}_{j}^{\\mathrm{fwd}}/2m)\\Delta t^{2}\\big)-\\big(q_{j}^{\\mathrm{gt}}+(p_{j}^{\\mathrm{gt}}/m)\\Delta t+(\\dot{p}_{j}^{\\mathrm{gt}}/2m)\\Delta t^{2}\\big)+\\mathcal{O}(\\Delta t^{3})\\big\\|_{2}}\\\\ &{\\qquad\\qquad\\leq\\big\\|{q}_{j}^{\\mathrm{twd}}-q_{j}^{\\mathrm{gt}}\\big\\|_{2}+\\frac{1}{m}\\big\\|{p}_{j}^{\\mathrm{fwd}}-p_{j}^{\\mathrm{gt}}\\big\\|_{2}\\Delta t+\\frac{1}{2m}\\big\\|\\dot{p}_{j}^{\\mathrm{fwd}}-\\dot{p}_{j}^{\\mathrm{gt}}\\big\\|_{2}\\Delta t^{2}+\\mathcal{O}(\\Delta t^{3})}\\\\ &{\\qquad\\qquad\\leq\\bigg[C_{j}^{\\mathrm{fwd}}j^{2}+\\frac{C_{1}^{\\mathrm{fwd}}}{m}j+\\mathcal{O}(1)\\bigg]\\Delta t^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the first inequality uses the triangle inequality, and in the second inequality we use (19a) and (19b) as well as (20). Thus with an appropriate $C_{2}^{\\mathrm{fwd}}$ , we have the final expression above is upper bounded by $C_{2}^{\\mathrm{fwd}}(j+1)^{2}\\Delta t^{2}$ , and so the claim holds for $j+1$ . ", "page_idx": 15}, {"type": "text", "text": "Since both the base case and the inductive step have been proven, by the principle of mathematical induction, (18a) and (18b) holds for all $k=0,1,\\cdots\\,,T$ . ", "page_idx": 15}, {"type": "text", "text": "With this, we finish the forward proof by plugging (18a) and (18b) into the loss function: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=0}^{T}\\|\\boldsymbol{z}_{j}^{\\mathrm{fwd}}-\\boldsymbol{z}_{j}^{\\mathrm{gt}}\\|_{2}^{2}=\\displaystyle\\sum_{j=0}^{T}\\|\\boldsymbol{p}_{j}^{\\mathrm{fwd}}-\\boldsymbol{p}_{j}^{\\mathrm{gt}}\\|_{2}^{2}+\\displaystyle\\sum_{j=0}^{T}\\|\\boldsymbol{q}_{j}^{\\mathrm{fwd}}-\\boldsymbol{q}_{j}^{\\mathrm{gt}}\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\left(C_{1}^{\\mathrm{fwd}}\\right)^{2}\\displaystyle\\sum_{j=0}^{T}j^{2}\\Delta t^{2}+\\left(C_{2}^{\\mathrm{fwd}}\\right)^{2}\\displaystyle\\sum_{j=0}^{T}j^{4}\\Delta t^{4}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathcal{O}(T^{3}\\Delta t^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.3.2 Reversal Loss $(\\mathcal{L}_{r e v e r s e})$ Analysis. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Next we analyze the reversal loss $\\begin{array}{r}{\\sum_{j=0}^{T}\\|R(z_{j}^{\\mathrm{rev}})-z_{j}^{\\mathrm{fwd}}\\|_{2}^{2}}\\end{array}$ . For this, we need to refine the Taylor expansion residual terms for a more in-depth analysis. ", "page_idx": 16}, {"type": "text", "text": "First reconsider the forward process. Since the process is generated from the learned network, we may assume that for some constants $c_{1},c_{2}$ , and $c_{3}$ , the states satisfy the following for any $0\\le j<T$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\!\\!\\!\\left\\{\\begin{array}{l}{q_{j}^{\\mathrm{fwd}}=q_{j+1}^{\\mathrm{fwd}}-(p_{j+1}^{\\mathrm{fwd}}/m)\\Delta t+(\\dot{p}_{j+1}^{\\mathrm{fwd}}/2m)\\Delta t^{2}+\\mathbf{rem}_{j}^{\\mathrm{fwd,3}},}\\\\ {p_{j}^{\\mathrm{fwd}}=p_{j+1}^{\\mathrm{fwd}}-\\dot{p}_{j+1}^{\\mathrm{fwd}}\\Delta t+\\mathbf{rem}_{j}^{\\mathrm{fwd,2}},}\\\\ {\\dot{p}_{j}^{\\mathrm{fwd}}=\\dot{p}_{j+1}^{\\mathrm{fwd}}+\\mathbf{rem}_{j}^{\\mathrm{fwd,1}},}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Twahyelroer  tehxep arensmioainnsi fnogr  ttehrem rse $\\left\\|\\mathbf{rem}_{j}^{\\mathrm{{fwd}},i}\\right\\|_{2}\\leq c_{i}\\Delta t^{i}$ for $i=1,2,3$ . Similarly, we have approximate ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\!\\!\\!\\!\\left\\{\\begin{array}{l}{q_{j}^{\\mathrm{rev}}=q_{j+1}^{\\mathrm{rev}}+(p_{j+1}^{\\mathrm{rev}}/m)\\Delta t+(\\dot{p}_{j+1}^{\\mathrm{rev}}/2m)\\Delta t^{2}+\\mathrm{rem}_{j}^{\\mathrm{rev,3}},}\\\\ {p_{j}^{\\mathrm{rev}}=p_{j+1}^{\\mathrm{rev}}+\\dot{p}_{j+1}^{\\mathrm{rev}}\\Delta t+\\mathrm{rem}_{j}^{\\mathrm{rev,2}},}\\\\ {\\dot{p}_{j}^{\\mathrm{rev}}=\\dot{p}_{j+1}^{\\mathrm{rev}}+\\mathrm{rem}_{j}^{\\mathrm{rev,1}},}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\big\\|\\mathbf{rem}_{j}^{\\mathrm{rev},i}\\big\\|_{2}\\leq c_{i}\\Delta t^{i}$ for $i={1,2,3}$ . ", "page_idx": 16}, {"type": "text", "text": "We will prove via induction that for $k=T,T-1,\\cdots\\,,0,$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\|R(q_{k}^{\\mathrm{rev}})-q_{k}^{\\mathrm{fwd}}\\|_{2}\\leq C_{3}^{\\mathrm{rev}}(T-k)^{3}\\Delta t^{3},}\\\\ {\\|R(p_{k}^{\\mathrm{rev}})-p_{k}^{\\mathrm{fwd}}\\|_{2}\\leq C_{2}^{\\mathrm{rev}}(T-k)^{2}\\Delta t^{2},}\\\\ {\\|R(\\dot{p}_{k}^{\\mathrm{rev}})-\\dot{p}_{k}^{\\mathrm{fwd}}\\|_{2}\\leq C_{1}^{\\mathrm{rev}}(T-k)\\Delta t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $C_{1}^{\\mathrm{rev}},C_{2}^{\\mathrm{rev}}$ and Crev are constants. ", "page_idx": 16}, {"type": "text", "text": "The entire proof process is analogous to the previous analysis of Reconstruction Loss. ", "page_idx": 16}, {"type": "text", "text": "Base Case $k=T$ : Since the reverse process is initialized by the forward process variables at $k=T$ , it is obvious that $\\left\\|q_{T}^{\\mathrm{fwd}}-q_{T}^{e v}\\right\\|_{2}=\\left\\|p_{T}^{\\mathrm{fwd}}-p_{T}^{\\mathrm{rev}}\\right\\|_{2}=\\left\\|\\dot{p}_{T}^{\\mathrm{fwd}}-\\dot{p}_{T}^{\\mathrm{rev}}\\right\\|_{2}\\stackrel{.}{=}0$ . Thus (23a), (23b) and $k=0$ . ", "page_idx": 16}, {"type": "text", "text": "Inductive Hypothesis: Assume the inequalities (23b), (23a) and (23c) hold for $k=j+1$ , which means: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\|R(\\pmb{q}_{j+1}^{\\mathrm{rev}})-\\pmb{q}_{j+1}^{\\mathrm{twd}}\\|_{2}\\leq C_{3}^{\\mathrm{rev}}(T-(j+1))^{3}\\Delta t^{3},}\\\\ {\\|R(\\pmb{p}_{j+1}^{\\mathrm{rev}})-\\pmb{p}_{j+1}^{\\mathrm{twd}}\\|_{2}\\leq C_{2}^{\\mathrm{rev}}(T-(j+1))^{2}\\Delta t^{2},}\\\\ {\\|R(\\pmb{\\dot{p}}_{j+1}^{\\mathrm{rev}})-\\pmb{\\dot{p}}_{j+1}^{\\mathrm{twd}}\\|_{2}\\leq C_{1}^{\\mathrm{rev}}(T-(j+1))\\Delta t,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Inductive Proof: We need to prove (23b) (23a) and (23c) holds for $k=j$ . ", "page_idx": 16}, {"type": "text", "text": "First, for (23c), using (21c) and (22c), we get for any $j$ that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|R(\\dot{p}_{j}^{\\mathrm{rev}})-\\dot{p}_{j}^{\\mathrm{fwd}}\\right\\|_{2}}\\\\ &{=\\left\\|(\\dot{p}_{j+1}^{\\mathrm{rev}}+\\mathbf{rem}_{j}^{\\mathrm{rev},1})-(\\dot{p}_{j+1}^{\\mathrm{fwd}}+\\mathbf{rem}_{j}^{\\mathrm{fwd},1})\\right\\|_{2}}\\\\ &{\\leq\\left\\|R(\\dot{p}_{j+1}^{\\mathrm{rev}})-\\dot{p}_{j+1}^{\\mathrm{fwd}}\\right\\|_{2}+\\left\\|\\mathbf{rem}_{j}^{\\mathrm{rev},1}\\right\\|_{2}+\\left\\|\\mathbf{rem}_{j}^{\\mathrm{fwd},1}\\right\\|_{2}}\\\\ &{\\leq C_{1}^{\\mathrm{rev}}(T-j-1)\\Delta t+2c_{1}\\Delta t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first inequality uses the triangle inequality, and the second inequality plugs in (24c). Thus taking $C_{1}^{\\mathrm{rev}}=2c_{1}$ , the above is upped bounded by $\\overbrace{C_{1}^{\\mathrm{rev}}(T-j)}\\Delta t$ , and (23b) holds for $j$ . ", "page_idx": 17}, {"type": "text", "text": "Second, for (24b), using (21b) and (22b), we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big\\|R(p_{j}^{\\mathrm{rev}})-p_{j}^{\\mathrm{fwd}}\\big\\|_{2}=\\big\\|-\\big(p_{j+1}^{\\mathrm{rev}}+p_{j+1}^{\\mathrm{rev}}\\Delta t+\\mathbf{rem}_{j}^{\\mathrm{rev},2}\\big)-\\big(p_{j+1}^{\\mathrm{fwd}}-p_{j+1}^{\\mathrm{fwd}}\\Delta t+\\mathbf{rem}_{j}^{\\mathrm{fwd},2}\\big)\\big\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\big\\|R(p_{j+1}^{\\mathrm{rev}})-p_{j+1}^{\\mathrm{fwd}}\\big\\|_{2}+\\big\\|R(\\dot{p}_{j+1}^{\\mathrm{rev}})-\\dot{p}_{j+1}^{\\mathrm{fwd}}\\big\\|_{2}\\Delta t+\\big\\|\\mathbf{rem}_{j}^{\\mathrm{rev},2}\\big\\|_{2}+\\|\\mathbf{rem}_{j}^{\\mathrm{fwd},2}\\big\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\big[C_{2}^{\\mathrm{rev}}(T-j-1)^{2}+C_{1}^{\\mathrm{rev}}(T-j-1)+2c_{2}\\big]\\Delta t^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first inequality uses the triangle inequality, and in the second inequality we use (24a) and (24b). Thus taking $C_{2}^{\\mathrm{rev}}=\\operatorname*{max}\\{C_{1}^{\\mathrm{rev}}/\\bar{2},2c_{2}\\}$ , we have the final expression above is upper bounded by $C_{2}^{\\mathrm{rev}}(T-j)^{2}\\bar{\\Delta}t^{2}$ , and so the claim holds for $j$ . ", "page_idx": 17}, {"type": "text", "text": "Finally, for (24a), we use (21a) and (22a) to get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|R(q_{j}^{\\mathrm{rev}})-q_{j}^{\\mathrm{rel}}\\|_{2}}\\\\ &{=\\!\\!\\left\\|\\big(q_{j+1}^{\\mathrm{rev}}+(p_{j+1}^{\\mathrm{rev}}/m)\\Delta t+({p_{j+1}^{\\mathrm{rev}}}/{2m})\\Delta t^{2}+\\mathbf{rem}_{j}^{\\mathrm{rev},3}\\big)-\\big(q_{j+1}^{\\mathrm{red}}-(p_{j+1}^{\\mathrm{red}}/m)\\Delta t+(p_{j+1}^{\\mathrm{fwd}}/{2m})\\Delta t^{2}}\\\\ &{\\leq\\!\\left\\|R(q_{j+1}^{\\mathrm{rev}})-q_{j+1}^{\\mathrm{red}}\\right\\|_{2}+\\frac{1}{m}\\|R(p_{j+1}^{\\mathrm{rev}})-p_{j+1}^{\\mathrm{red}}\\|_{2}\\Delta t+\\frac{1}{2m}\\|R(\\dot{p}_{j+1}^{\\mathrm{rev}})-\\dot{p}_{j+1}^{\\mathrm{red}}\\|_{2}\\Delta t^{2}+\\|\\mathbf{rem}_{j}^{\\mathrm{rev},3}\\|_{2}+}\\\\ &{\\leq\\!\\left[C_{3}^{\\mathrm{rev}}(T-j-1)^{3}+\\frac{C_{2}^{\\mathrm{rev}}}{m}(T-j-1)^{2}+\\frac{C_{1}^{\\mathrm{rev}}}{2m}(T-j-1)+2c_{3}\\right]\\Delta t^{3},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first inequality uses the triangle inequality, and in the second inequality we use (24a), (24b) and (24c). Thus taking $C_{3}^{\\mathrm{rev}}=\\operatorname*{max}\\{C_{2}^{\\mathrm{rev}}/3m,C_{1}^{\\mathrm{rev}}/6m,2c_{3}\\}$ , we have the final expression above is upper bounded by $C_{3}^{\\mathrm{rev}}(\\bar{T}-j)^{3}\\Delta t^{3}$ , and so the claim holds for $j$ . ", "page_idx": 17}, {"type": "text", "text": "Since both the base case and the inductive step have been proven, by the principle of mathematical induction, (23b), (23a) and (23c) hold for all $k=T,T-1,\\cdots\\,,0$ . ", "page_idx": 17}, {"type": "text", "text": "With this we finish the proof by plugging (23b) and (23a) into the loss function: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=0}^{T}\\|R(z_{j}^{\\mathrm{rev}})-z_{j}^{\\mathrm{fwd}}\\|_{2}^{2}=\\displaystyle\\sum_{j=0}^{T}\\|R(p_{j}^{\\mathrm{rev}})-p_{j}^{\\mathrm{fwd}}\\|_{2}^{2}+\\displaystyle\\sum_{j=0}^{T}\\|R(q_{j}^{\\mathrm{rev}})-q_{j}^{\\mathrm{fwd}}\\|_{2}^{2}}\\\\ &{\\leq\\big(C_{2}^{\\mathrm{rev}}\\big)^{2}\\displaystyle\\sum_{j=0}^{T}(T-j)^{4}\\Delta t^{4}+\\big(C_{3}^{\\mathrm{rev}}\\big)^{2}\\displaystyle\\sum_{j=0}^{T}(T-j)^{6}\\Delta t^{6}}\\\\ &{=\\mathcal{O}(T^{5}\\Delta t^{4}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.4 Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "iWlqbNE8P7/tmp/aaac58d22305607ced83ded81ec7162672c786e6aeac4fddc92f0f983590b5a1.jpg", "img_caption": ["Figure 8: Comparison between two reversal loss implementation "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "We expect an ideal model to align both the predicted forward and reverse trajectories with the ground truth. As shown in Figure 8, we integrate one step from the initial state $\\hat{y}_{i}^{\\mathrm{fwd}}(0)$ (which is the same as $y_{i}(0))$ and reach the state $\\hat{y}_{i}^{\\mathrm{fwd}}(1)$ . ", "page_idx": 18}, {"type": "text", "text": "The first reverse loss implementation (ours) follows Lemma 2.1 as $R\\circ\\Phi_{t}\\circ R\\circ\\Phi_{t}=\\operatorname{I}$ , which means when we evolve forward and reach the state $\\hat{y}_{i}^{\\mathrm{fwd}}(1)$ we reverse it into $\\hat{\\pmb{y}}_{i}^{\\mathrm{rev}}(-1)=R(\\hat{\\pmb{y}}_{i}^{\\mathrm{fwd}}(1))$ and go back to reach $\\hat{y}_{i}^{\\mathrm{rev}}(0)$ , then reverse it to get $R(\\hat{y}_{i}^{\\mathrm{rev}}(0))$ , which ideally should be the same as $\\hat{\\pmb{y}}_{i}^{\\mathrm{fwd}}(0)$ . ", "page_idx": 18}, {"type": "text", "text": "The second reverse loss implementation follows Eqn 5as $R\\circ\\Phi_{t}=\\Phi_{-t}\\circ R$ , which means we first reverse the initial state as $\\bar{\\pmb{y}}_{i}^{\\mathrm{rev2}}(0)=R(\\pmb{y}_{i}(0))$ , then evolve the reverse trajectory in the opposite direction to reach $\\hat{y}_{i}^{\\mathrm{rev2}}(-1)$ , and then perform a symmetric operation to reach $\\hat{y}_{i}^{\\mathrm{rev2}}(1)$ , aligning it with the forward trajectory. ", "page_idx": 18}, {"type": "text", "text": "We assume the two reconstruction losses $\\mathcal{L}_{p r e d}=||\\hat{\\pmb{y}}_{i}^{\\mathrm{fwd}}(1)-\\pmb{y}_{i}(1)||_{2}^{2}:=a$ are the same. For the time-reversal losses, we also assume they have reached the same value $b$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{r e v e r s e}=\\|R(\\hat{y}_{i}^{\\mathrm{rev}}(0))-\\hat{y}_{i}^{\\mathrm{twd}}(0)\\|_{2}^{2}+\\|R(\\hat{y}_{i}^{\\mathrm{rev}}(-1))-\\hat{y}_{i}^{\\mathrm{twd}}(1)\\|_{2}^{2}=\\|R(\\hat{y}_{i}^{\\mathrm{rev}}(0))-\\hat{y}_{i}^{\\mathrm{twd}}(0)\\|_{2}^{2}:=b}\\\\ &{\\dot{\\cdot}_{r e v e r s e2}=\\|\\hat{y}_{i}^{\\mathrm{rev}2}(0)-\\hat{y}_{i}^{\\mathrm{red}}(0)\\|_{2}^{2}+\\|\\hat{y}_{i}^{\\mathrm{rev}2}(1)-\\hat{y}_{i}^{\\mathrm{twd}}(1)\\|_{2}^{2}=\\|\\hat{y}_{i}^{\\mathrm{rev}2}(1)-\\hat{y}_{i}^{\\mathrm{twd}}(1)\\|_{2}^{2}:=b,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As shown in Figure 8 where we illustrate the worst case scenario $\\begin{array}{r l}{M a x E r r o r_{g t\\_r e v}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\operatorname*{max}_{k\\in[K]}\\|y_{i}(t_{k})-\\hat{y}_{i}^{\\mathrm{rev}}(t_{K-k}^{\\prime})\\|_{2}}\\end{array}$ of TREAT and TRS-ODEN, we can see that in our implementation the worst error is the maximum of two loss, while the TRS-ODEN\u2019s implementation has the risk of accumulating the error together, making the worst error being the sum of both: $\\begin{array}{r l}&{\\ \\ \\ \\ M a x E r r o r_{\\mathrm{TREM}}=\\operatorname*{max}\\left\\{\\left\\|R(\\hat{y}_{i}^{\\mathrm{rev}}(0))-y_{i}(0)\\right\\|_{2},\\left\\|R(\\hat{y}_{i}^{\\mathrm{rev}}(-1))-y_{i}(1)\\right\\|_{2}\\right\\}=m a x\\{a,b\\},}\\\\ &{\\ M a x E r r o r_{\\mathrm{TRS-ODEN}}=\\operatorname*{max}\\left\\{\\left\\|\\hat{y}_{i}^{\\mathrm{rev}}(0)-y_{i}(0)\\right\\|_{2},\\left\\|\\hat{y}_{i}^{\\mathrm{reg}}(1)-y_{i}(1)\\right\\|_{2}\\right\\}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\operatorname*{max}\\left\\{0,\\left\\|R(\\hat{y}_{i}^{\\mathrm{rev}}(-1))-y_{i}(1)\\right\\|_{2}\\right\\}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\left\\|\\hat{y}_{i}^{\\mathrm{rev}2}(1)-\\hat{y}_{i}^{\\mathrm{red}}(1)\\right\\|_{2}+\\left\\|\\hat{y}^{\\mathrm{red}}(1)-y(1)\\right\\|_{2}=a+b,}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "So it is obvious that MaxErrorTREAT made by TREAT is smaller., which means our model achieves a smaller error of the maximum distance between the reversal and ground truth trajectory. ", "page_idx": 18}, {"type": "text", "text": "B Example of varying dynamical systems ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We illustrate the energy conservation and time reversal of the three n-body spring systems used in our experiments. We use the Hamiltonian formalism of systems under classical mechanics to describe their dynamics and verify their energy conservation and time-reversibility characteristics. ", "page_idx": 18}, {"type": "text", "text": "The scalar function that describes a system\u2019s motion is called the Hamiltonian, $\\mathcal{H}$ , and is typically equal to the total energy of the system, that is, the potential energy plus the kinetic energy (North, 2021). It describes the phase space equations of motion by following two first-order ODEs called Hamilton\u2019s equations: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{d\\mathbf{q}}{d t}=\\frac{\\partial\\mathcal{H}(\\mathbf{q},\\mathbf{p})}{\\partial\\mathbf{p}},\\frac{d\\mathbf{p}}{d t}=-\\frac{\\partial\\mathcal{H}(\\mathbf{q},\\mathbf{p})}{\\partial\\mathbf{q}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{q}\\in\\mathbb{R}^{n},\\mathbf{p}\\in\\mathbb{R}^{n}$ , and $\\mathcal{H}:\\mathbb{R}^{2n}\\mapsto\\mathbb{R}$ are positions, momenta, and Hamiltonian of the system. ", "page_idx": 18}, {"type": "text", "text": "Under this formalism, energy conservative is defined by $d\\mathcal{H}/d t=0$ , and the time-reversal symmetry is defined by $\\mathcal{H}(\\boldsymbol{q},\\boldsymbol{p},t)=\\mathcal{H}(\\boldsymbol{q},-\\boldsymbol{p},-t)$ (Lamb and Roberts, 1998). ", "page_idx": 18}, {"type": "text", "text": "B.1 Conservative and reversible systems. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A simple example is the isolated $\\mathbf{n}$ -body spring system, which can be described by : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d{\\bf q_{i}}}{d t}=\\frac{{\\bf p_{i}}}{m}}\\\\ {\\displaystyle\\frac{d{\\bf p_{i}}}{d t}=\\sum_{j\\in N_{i}}-k({\\bf q_{i}}-{\\bf q_{j}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{q}=(\\mathbf{q_{1}},\\mathbf{q_{2}},\\cdots,\\mathbf{q_{N}})$ is a set of positions of each object , $\\ensuremath{\\mathbf{p}}=(\\ensuremath{\\mathbf{p_{1}}},\\ensuremath{\\mathbf{p_{2}}},\\cdots,\\ensuremath{\\mathbf{p_{N}}})$ is a set of momenta of each object, $m_{i}$ is mass of each object, $k$ is spring constant. ", "page_idx": 18}, {"type": "text", "text": "The Hamilton\u2019s equations are: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial\\mathcal{H}(\\mathbf{q},\\mathbf{p})}{\\partial\\mathbf{p}_{\\mathbf{i}}}=\\frac{d\\mathbf{q}_{\\mathbf{i}}}{d t}=\\frac{\\mathbf{p}_{\\mathbf{i}}}{m}}\\\\ &{\\displaystyle\\frac{\\partial\\mathcal{H}(\\mathbf{q},\\mathbf{p})}{\\partial\\mathbf{q}_{\\mathbf{i}}}=-\\frac{d\\mathbf{p}_{\\mathbf{i}}}{d t}=\\sum_{j\\in N_{i}}k(\\mathbf{q}_{\\mathbf{i}}-\\mathbf{q}_{\\mathbf{j}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, we can obtain the Hamiltonian through the integration of the above equation. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{H}(\\ensuremath{\\mathbf{q}},\\ensuremath{\\mathbf{p}})=\\sum_{i=1}^{N}\\frac{\\ensuremath{\\mathbf{p_{i}}}^{2}}{2m_{i}}+\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j\\in N_{i}}^{N}\\frac{1}{2}k(\\ensuremath{\\mathbf{q}}_{\\mathrm{i}}-\\ensuremath{\\mathbf{q}}_{\\mathrm{j}})^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Verify the systems\u2019 energy conservation ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\n{\\frac{d{\\mathcal{H}}\\left(\\mathbf{q},\\mathbf{p}\\right)}{d t}}={\\frac{1}{d t}}(\\sum_{i=1}^{N}{\\frac{\\mathbf{p_{i}}^{2}}{2m_{i}}})+{\\frac{1}{d t}}{\\bigl(}{\\frac{1}{2}}\\sum_{i=1}^{N}\\sum_{j\\in N_{i}}^{N}{\\frac{1}{2}}k(\\mathbf{q_{i}}-\\mathbf{q_{j}})^{2}{\\bigr)}=0,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "So it is conservative. ", "page_idx": 19}, {"type": "text", "text": "Verify the systems\u2019 time-reversal symmetry We do the transformation $R:(\\mathbf{q},\\mathbf{p},t)\\mapsto(\\mathbf{q},-\\mathbf{p},-t)$ . ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathcal{H}({\\bf q},{\\bf p})=\\sum_{i=1}^{N}\\frac{{\\bf p_{i}}^{2}}{2m_{i}}+\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j\\in N_{i}}^{N}\\frac{1}{2}k({\\bf q_{i}}-{\\bf q_{j}})^{2}},}\\\\ {{\\displaystyle\\mathcal{H}({\\bf q},-{\\bf p})=\\sum_{i=1}^{N}\\frac{\\left(-{\\bf p_{i}}\\right)^{2}}{2m_{i}}+\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j\\in N_{i}}^{N}\\frac{1}{2}k({\\bf q_{i}}-{\\bf q_{j}})^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "It is obvious $\\mathcal{H}(\\ensuremath{\\mathbf{q}},\\ensuremath{\\mathbf{p}})=\\mathcal{H}(\\ensuremath{\\mathbf{q}},-\\ensuremath{\\mathbf{p}})$ , so it is reversible ", "page_idx": 19}, {"type": "text", "text": "B.2 Non-conservative and reversible systems. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "A simple example is a n-body spring system with periodical external force, which can be described by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d{\\bf q_{i}}}{d t}=\\frac{{\\bf p_{i}}}{m}}\\\\ {\\displaystyle\\frac{d{\\bf p_{i}}}{d t}=\\sum_{j\\in N_{i}}^{N}-k({\\bf q_{i}}-{\\bf q_{j}})-k_{1}\\cos\\omega t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The Hamilton\u2019s equations are: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial\\mathcal{H}(\\mathbf{q},\\mathbf{p})}{\\partial\\mathbf{p_{i}}}=\\frac{d\\mathbf{q_{i}}}{d t}=\\frac{\\mathbf{p_{i}}}{m}}\\\\ &{\\displaystyle\\frac{\\partial\\mathcal{H}(\\mathbf{q},\\mathbf{p})}{\\partial\\mathbf{q_{i}}}=-\\frac{d\\mathbf{p_{i}}}{d t}=\\sum_{j\\in N_{i}}k(\\mathbf{q_{i}}-\\mathbf{q_{j}})+k_{1}\\cos\\omega t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, we can obtain the Hamiltonian through the integration of the above equation: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{H}(\\mathbf{q},\\mathbf{p})=\\sum_{i=1}^{N}\\frac{\\mathbf{p_{i}}^{2}}{2m_{i}}+\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j\\in N_{i}}^{N}\\frac{1}{2}k(\\mathbf{q_{i}}-\\mathbf{q_{j}})^{2}+\\sum_{i=1}^{N}q_{i}*k_{1}\\cos\\omega t,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Verify the systems\u2019 energy conservation ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{{\\cfrac{d{\\mathcal{H}}\\left(\\mathbf{q},\\mathbf{p}\\right)}{d t}}={\\cfrac{1}{d t}}(\\sum_{i=1}^{N}{\\cfrac{\\mathbf{p_{i}}^{2}}{2m_{i}}})+{\\cfrac{1}{d t}}\\left({\\cfrac{1}{2}}\\sum_{i=1}^{N}\\sum_{j\\in N_{i}}^{N}{\\cfrac{1}{2}}k(\\mathbf{q}_{i}-\\mathbf{q}_{j})^{2}\\right)+{\\cfrac{1}{d t}}\\left(\\sum_{i=1}^{N}q_{i}*k_{1}\\cos\\omega t\\right)}\\\\ {\\qquad\\qquad=0+{\\cfrac{1}{d t}}\\left(\\sum_{i=1}^{N}q_{i}k_{1}\\cos\\omega t\\right)}\\\\ {\\qquad\\qquad=\\left(\\sum_{i=1}^{N}-\\omega q_{i}k_{1}\\sin\\omega t\\right)\\neq0}\\end{array}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "So it is non-conservative. ", "page_idx": 20}, {"type": "text", "text": "Verify the systems\u2019 time-reversal symmetry We do the transformation $R:(\\mathbf{q},\\mathbf{p},t)\\mapsto(\\mathbf{q},-\\mathbf{p},-t)$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{H}(\\mathbf{q},\\mathbf{p})=\\sum_{i=1}^{N}\\frac{\\mathbf{p_{i}}^{2}}{2m_{i}}+\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j\\in N_{i}}^{N}\\frac{1}{2}k(\\mathbf{q}_{i}-\\mathbf{q}_{j})^{2}+\\displaystyle\\sum_{i=1}^{N}q_{i}*k_{1}\\cos\\omega t,}\\\\ {\\displaystyle\\mathcal{H}(\\mathbf{q},\\mathbf{-p})=\\sum_{i=1}^{N}\\frac{(-\\mathbf{p_{i}})^{2}}{2m_{i}}+\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j\\in N_{i}}^{N}\\frac{1}{2}k(\\mathbf{q}_{i}-\\mathbf{q}_{j})^{2}+\\displaystyle\\sum_{i=1}^{N}q_{i}*k_{1}\\cos\\omega(-t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It is obvious $\\mathcal{H}(\\mathbf{q},\\mathbf{p},t)=\\mathcal{H}(\\mathbf{q},-\\mathbf{p},t)$ , so it is reversible ", "page_idx": 20}, {"type": "text", "text": "B.3 Non-conservative and irreversible systems. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "A simple example is an $\\mathbf{n}$ -body spring system with frictions proportional to its velocity, $\\gamma$ is the coefficient of friction, which can be described by: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d\\mathbf{q}_{i}}{d t}=\\frac{\\mathbf{p}_{i}}{m}}\\\\ &{\\frac{d\\mathbf{p}_{i}}{d t}=-k_{0}\\mathbf{q}_{i}-\\gamma\\frac{\\mathbf{p}_{i}}{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The Hamilton\u2019s equations are: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial\\mathcal{H}(\\mathbf{q},\\mathbf{p})}{\\partial\\mathbf{p}_{\\mathbf{i}}}=\\frac{d\\mathbf{q}_{\\mathbf{i}}}{d t}=\\frac{\\mathbf{p}_{\\mathbf{i}}}{m}}\\\\ &{\\displaystyle\\frac{\\partial\\mathcal{H}(\\mathbf{q},\\mathbf{p})}{\\partial\\mathbf{q}_{\\mathbf{i}}}=-\\frac{d\\mathbf{p}_{\\mathbf{i}}}{d t}=\\sum_{j\\in N_{i}}k(\\mathbf{q}_{\\mathbf{i}}-\\mathbf{q}_{\\mathbf{j}})+\\gamma\\frac{\\mathbf{p}_{i}}{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, we can obtain the Hamiltonian through the integration of the above equation: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{H}(\\mathbf{q},\\mathbf{p})=\\sum_{i=1}^{N}\\frac{\\mathbf{p_{i}}^{2}}{2m_{i}}+\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j\\in N_{i}}^{N}\\frac{1}{2}k(\\mathbf{q_{i}}-\\mathbf{q_{j}})^{2}+\\sum_{i=1}^{N}\\frac{\\gamma}{m}\\int_{0}^{t}\\frac{\\mathbf{p_{i}}^{2}}{m}d t,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Verify the systems\u2019 energy conservation ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{{\\cfrac{d{\\mathcal{H}}\\left(\\mathbf{q},\\mathbf{p}\\right)}{d t}}={\\cfrac{1}{d t}}(\\sum_{i=1}^{N}{\\cfrac{\\mathbf{p_{i}}^{2}}{2m_{i}}})+{\\cfrac{1}{d t}}\\left({\\cfrac{1}{2}}\\sum_{i=1}^{N}\\sum_{j\\in N_{i}}^{N}{\\cfrac{1}{2}}k(\\mathbf{q}_{i}-\\mathbf{q}_{j})^{2}\\right)+{\\cfrac{1}{d t}}\\left(\\sum_{i=1}^{N}{\\cfrac{\\gamma}{m}}\\int_{0}^{t}{\\frac{\\mathbf{p_{i}}^{2}}{m}}d t\\right)}\\\\ {\\qquad=0+{\\cfrac{1}{d t}}(\\sum_{i=1}^{N}{\\cfrac{\\gamma}{m}}\\int_{0}^{t}{\\frac{\\mathbf{p_{i}}^{2}}{m}}d t)}\\\\ {\\qquad=\\left(\\sum_{i=1}^{N}{\\cfrac{\\gamma}{m}}{\\frac{\\mathbf{p_{i}}^{2}}{m}}\\right)\\neq0}\\end{array}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "So it is non-conservative. ", "page_idx": 20}, {"type": "text", "text": "Verify the systems\u2019 time-reversal symmetry We do the transformation $R:(\\mathbf{q},\\mathbf{p},t)\\mapsto(\\mathbf{q},-\\mathbf{p},-t)$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{H}({\\bf q},{\\bf p})=\\sum_{i=1}^{N}\\frac{{\\bf p_{i}}^{2}}{2m_{i}}+\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j\\in N_{i}}^{N}\\frac{1}{2}k({\\bf q}_{i}-{\\bf q}_{j})^{2}+\\sum_{i=1}^{N}\\frac{\\gamma}{m}\\int_{0}^{t}\\frac{{\\bf p_{i}}^{2}}{m}d t,}\\\\ {\\displaystyle\\mathcal{H}({\\bf q},-{\\bf p})=\\sum_{i=1}^{N}\\frac{\\left(-{\\bf p_{i}}\\right)^{2}}{2m_{i}}+\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j\\in N_{i}}^{N}\\frac{1}{2}k({\\bf q}_{i}-{\\bf q_{j}})^{2}+\\sum_{i=1}^{N}\\frac{\\gamma}{m}\\int_{0}^{(-t)}\\frac{{\\bf p_{i}}^{2}}{m}d(-t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It is obvious $\\mathcal{H}(\\mathbf{q},\\mathbf{p},t)\\neq\\mathcal{H}(\\mathbf{q},-\\mathbf{p},t)$ , so it is irreversible ", "page_idx": 20}, {"type": "text", "text": "In our experiments, all datasets are synthesized from ground-truth physical law via sumulation. We generate five simulated datasets: three $n$ -body spring systems under damping, periodic, or no external force, one chaotic tripe pendulum dataset with three sequentially connected stiff sticks that form and a chaotic strange attractor. We name the first three as Sipmle Spring, Forced Spring, and Damped Spring respectively. For multi-agent systems, all $n$ -body spring systems contain 5 interacting balls, with varying connectivities. Each Pendulum system contains 3 connected stiff sticks. For single-agent systems, all spring systems contain only one ball. For the chaotic single Attractor, we follow the setting of (Huh et al., 2020). ", "page_idx": 21}, {"type": "text", "text": "For the $n$ -body spring system, we randomly sample whether a pair of objects are connected, and model their interaction via forces defined by Hooke\u2019s law. In the Damped spring, the objects have an additional friction force that is opposite to their moving direction and whose magnitude is proportional to their speed. In the Forced spring, all objects have the same external force that changes direction periodically. We show in Figure 1(a), the energy variation in both of the Damped spring and Forced spring is significant. For the chaotic triple Pendulum , the equations governing the motion are inherently nonlinear. Although this system is deterministic, it is also highly sensitive to the initial condition and numerical errors (Shinbrot et al., 1992; Awrejcewicz et al., 2008; Stachowiak and Okada, 2006). This property is often referred to as the \"butterfly effect\", as depicted in Figure 9. Unlike for $n$ -body spring systems, where the forces and equations of motion can be easily articulated, for the Pendulum, the explicit forces cannot be directly defined, and the motion of objects can only be described through Lagrangian formulations (North, 2021), making the modeling highly complex and raising challenges for accurate learning. We simulate the trajectories by using Euler\u2019s method for $n$ -body spring systems and using the 4th order Runge-Kutta (RK4) method for the Pendulum and Attractor . For all spring systems and Pendulum, We integrate with a fixed step size and subsample every 100 steps. For training, we use a total of 6000 forward steps. To generate irregularly sampled partial observations, we follow (Huang et al., 2020) and sample the number of observations n from a uniform distribution U(40, 52) and draw the n observations uniformly for each object. For testing, we additionally sample 40 observations following the same procedure from PDE steps [6000, 12000], besides generating observations from steps [1, 6000]. The above sampling procedure is conducted independently for each object. We generate 20k training samples and 5k testing samples for each dataset. For Attractor, we integrate a total of 600 forward steps for training and subsample every 10 steps. For testing, we additionally sample 40 observations from step [600,1200].The irregularly sampled partial observations generation is the same as above. We generate 1000 training samples and 50 testing samples following (Huh et al., 2020). Therefore, for all datasets, condition length is ", "page_idx": 21}, {"type": "image", "img_path": "iWlqbNE8P7/tmp/2875bcdfd96732fed9a6168abc2756ec112c20f2bcd6f2e4b42e87bf551d93f8.jpg", "img_caption": ["Figure 9: Illustration to show the pendulum is highly-sensitive to initial states "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "60 steps and prediction length is 40s steps. The features (position/velocity) are normalized to the maximum absolute value of 1 across training and testing datasets. ", "page_idx": 22}, {"type": "text", "text": "We also compute the Maximum Lyapunov Exponent (MLE) to assess the chaos level of the systems, using the formula: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lambda=m a x_{t\\rightarrow\\mathrm{inf}}(\\frac{1}{t}\\ln\\frac{||\\delta(t)||}{||\\delta(0)||}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We set fixed initial values for each dataset and generate 10 trajectories by perturbing the initial values with random noise (0, 0.0001). We calculate the Maximum Lyapunov Exponent (MLE) between any two trajectories. Finally, we compute the average and std of MLE from all pairs to gauge the chaotic behavior of each dataset. The data is presented in the table below: ", "page_idx": 22}, {"type": "table", "img_path": "iWlqbNE8P7/tmp/f7bd180f2db4b7e8bab5b22562dd9392a891ce16b7935ceea95ef15fa5b7b5ca.jpg", "table_caption": ["Table 2: MLE of different Multi-agent Systems "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "From the table, it\u2019s evident that the order of MLE values is: Pendulum $\\gg$ three Spring datasets. This observation is consistent with the evaluation results based on MSE presented in our previous responses in Table 3 which indicates that as the prediction length(steps\\*step size) increases, there is a more significant performance degradation of all models on Pendulum dataset. ", "page_idx": 22}, {"type": "text", "text": "In the following subsections, we show the dynamical equations of each dataset in detail. ", "page_idx": 22}, {"type": "text", "text": "C.1 Spring Systems ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "C.1.1 Simple Spring ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The dynamical equations of simple spring are as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\frac{d{\\bf q_{i}}}{d t}}=\\frac{{\\bf p_{i}}}{m}}}\\\\ {{\\displaystyle{\\frac{d{\\bf p_{i}}}{d t}}=\\sum_{j\\in{\\cal N}_{i}}^{N}-k({\\bf q_{i}}-{\\bf q_{j}})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where where $\\mathbf{q}=(\\mathbf{q_{1}},\\mathbf{q_{2}},\\cdots,\\mathbf{q_{N}})$ is a set of positions of each object , $\\ensuremath{\\mathbf{p}}=(\\ensuremath{\\mathbf{p_{1}}},\\ensuremath{\\mathbf{p_{2}}},\\cdots,\\ensuremath{\\mathbf{p_{N}}})$ is a set of momenta of each object. We set the mass of each object $m=1$ , the spring constant $k=0.1$ . ", "page_idx": 22}, {"type": "text", "text": "C.1.2 Damped Spring ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The dynamical equations of damped spring are as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d\\mathbf{q_{i}}}{d t}=\\frac{\\mathbf{p_{i}}}{m}}\\\\ {\\displaystyle\\frac{d\\mathbf{p_{i}}}{d t}=\\sum_{j\\in N_{i}}-k(\\mathbf{q_{i}}-\\mathbf{q_{j}})-\\gamma\\frac{\\mathbf{p}_{i}}{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where where $\\mathbf{q}=(\\mathbf{q_{1}},\\mathbf{q_{2}},\\cdots,\\mathbf{q_{N}})$ is a set of positions of each object, $\\ensuremath{\\mathbf{p}}=(\\ensuremath{\\mathbf{p_{1}}},\\ensuremath{\\mathbf{p_{2}}},\\cdots,\\ensuremath{\\mathbf{p_{N}}})$ is a set of momenta of each object, We set the mass of each object $m=1$ , the spring constan $\\mathcal{k}=0.1$ , the coefficient of friction $\\gamma=10$ . ", "page_idx": 22}, {"type": "text", "text": "C.1.3 Forced Spring ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The dynamical equations of forced spring system are as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d{\\bf q_{i}}}{d t}=\\frac{{\\bf p_{i}}}{m}}\\\\ {\\displaystyle\\frac{d{\\bf p_{i}}}{d t}=\\sum_{j\\in N_{i}}^{N}-k({\\bf q_{i}}-{\\bf q_{j}})-k_{1}\\cos\\omega t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where where $\\mathbf{q}=(\\mathbf{q_{1}},\\mathbf{q_{2}},\\cdots,\\mathbf{q_{N}})$ is a set of positions of each object , $\\ensuremath{\\mathbf{p}}=(\\ensuremath{\\mathbf{p_{1}}},\\ensuremath{\\mathbf{p_{2}}},\\cdots,\\ensuremath{\\mathbf{p_{N}}})$ is a set of momenta of each object. We set the mass of each object $m=1$ , the spring constant $k=0.1$ , the external strength $k_{1}=10$ and the frequency of variation $\\omega=1$ ", "page_idx": 23}, {"type": "text", "text": "We simulate the positions and momentums of three spring systems by using Euler methods as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{q_{i}}(t+1)=\\mathbf{q_{i}}(t)+\\displaystyle\\frac{d\\mathbf{q_{i}}}{d t}\\Delta t}\\\\ {\\mathbf{p_{i}}(t+1)=\\mathbf{p_{i}}(t)+\\displaystyle\\frac{d\\mathbf{p_{i}}}{d t}\\Delta t}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\frac{d\\mathbf{q}_{\\mathrm{i}}}{d t}$ and $\\frac{d\\mathbf{p_{i}}}{d t}$ were defined as above for each datasets, and $\\Delta t=0.001$ is the integration steps. ", "page_idx": 23}, {"type": "text", "text": "C.2 Chaotic Pendulum ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we demonstrate how to derive the dynamics equations for a chaotic triple pendulum using the Lagrangian formalism. ", "page_idx": 23}, {"type": "text", "text": "The moment of inertia of each stick about the centroid is ", "page_idx": 23}, {"type": "equation", "text": "$$\nI={\\frac{1}{12}}m l^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The position of the center of gravity of each stick is as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle x_{1}=\\frac{l}{2}\\sin\\theta_{1},\\quad y_{1}=-\\frac{l}{2}\\cos\\theta_{1}}}\\\\ {{\\displaystyle x_{2}=l(\\sin\\theta_{1}+\\frac{1}{2}\\sin\\theta_{2}),\\quad y_{2}=-l(\\cos\\theta_{1}+\\frac{1}{2}\\cos\\theta_{2})}}\\\\ {{\\displaystyle x_{3}=l(\\sin\\theta_{1}+\\sin\\theta_{2}+\\frac{1}{2}\\sin\\theta_{3}),\\quad y_{3}=-l(\\cos\\theta_{1}+\\cos\\theta_{2}+\\frac{1}{2}\\cos\\theta_{3})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The change in the center of gravity of each stick is: ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{{\\dot{x}}_{1}={\\cfrac{l}{2}}\\cos\\theta_{1}\\cdot{\\dot{\\theta_{1}}},\\quad{\\dot{y}}_{1}={\\cfrac{l}{2}}\\sin\\theta_{1}\\cdot{\\dot{\\theta_{1}}}}\\\\ {{\\dot{x}}_{2}=l(\\cos\\theta_{1}\\cdot{\\dot{\\theta_{1}}}+{\\cfrac{1}{2}}\\cos\\theta_{2}\\cdot{\\dot{\\theta_{2}}}),\\quad{\\dot{y}}_{2}=l(\\sin\\theta_{1}\\cdot{\\dot{\\theta_{1}}}+{\\cfrac{1}{2}}\\sin\\theta_{2}\\cdot{\\dot{\\theta_{2}}})}\\\\ {{\\dot{x}}_{3}=l(\\cos\\theta_{1}\\cdot{\\dot{\\theta_{1}}}+\\cos\\theta_{2}\\cdot{\\dot{\\theta_{2}}}+{\\cfrac{1}{2}}\\cos\\theta_{3}\\cdot{\\dot{\\theta_{3}}}),\\quad{\\dot{y}}_{3}=l(\\sin\\theta_{1}\\cdot{\\dot{\\theta_{1}}}+\\sin\\theta_{2}\\cdot{\\dot{\\theta_{2}}}+{\\cfrac{1}{2}}\\sin\\theta_{3}\\cdot{\\dot{\\theta_{3}}})}\\end{array}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The Lagrangian $\\mathrm{L}$ of this triple pendulum system is: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle=\\frac{1}{2}m\\big(\\dot{x_{1}}^{2}+\\dot{x_{2}}^{2}+\\dot{x_{3}}^{2}+\\dot{y_{1}}^{2}+\\dot{y_{2}}^{2}+\\dot{y_{3}}^{2}\\big)+\\frac{1}{2}I\\big(\\dot{\\theta_{1}}^{2}+\\dot{\\theta_{2}}^{2}+\\dot{\\theta_{3}}^{2}\\big)-m g\\big(y_{1}+y_{2}+y_{3}\\big)}\\\\ &{\\displaystyle=\\frac{1}{6}m l\\big(9\\dot{\\theta_{2}}\\dot{\\theta_{1}}l\\cos\\(\\theta_{1}-\\theta_{2})+3\\dot{\\theta_{3}}\\dot{\\theta_{1}}l\\cos\\big(\\theta_{1}-\\theta_{3}\\big)+3\\dot{\\theta_{2}}\\dot{\\theta_{3}}l\\cos\\big(\\theta_{2}-\\theta_{3}\\big)+7\\dot{\\theta_{1}}^{2}l+4\\dot{\\theta_{2}}^{2}l+\\dot{\\theta_{3}}^{2}\\dot{\\theta_{1}}}\\\\ &{\\quad+\\,15g\\cos\\big(\\theta_{1}\\big)+9g\\cos\\big(\\theta_{2}\\big)+3g\\cos\\big(\\theta_{3}\\big)\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The Lagrangian equation is defined as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\frac{d}{d t}}{\\frac{\\partial{\\mathcal{L}}}{\\partial{\\dot{\\theta}}}}-{\\frac{\\partial{\\mathcal{L}}}{\\partial\\theta}}=\\mathbf{0}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and we also have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\frac{\\partial{\\mathcal{L}}}{\\partial{\\dot{\\theta}}}}={\\frac{\\partial T}{\\partial{\\dot{\\theta}}}}=p}}\\\\ {{\\displaystyle{\\dot{p}}={\\frac{d}{d t}}{\\frac{\\partial{\\mathcal{L}}}{\\partial{\\dot{\\theta}}}}={\\frac{\\partial{\\mathcal{L}}}{\\partial\\theta}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\mathfrak{p}$ is the Angular Momentum. ", "page_idx": 23}, {"type": "text", "text": "We can list the equations for each of the three sticks separately: ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{p_{1}={\\cfrac{\\partial{\\mathcal{L}}}{\\partial{\\dot{\\theta_{1}}}}}}&{{\\dot{p_{1}}}={\\cfrac{\\partial{\\mathcal{L}}}{\\partial\\theta_{1}}}}\\\\ {p_{2}={\\cfrac{\\partial{\\mathcal{L}}}{\\partial{\\dot{\\theta_{2}}}}}}&{{\\dot{p_{2}}}={\\cfrac{\\partial{\\mathcal{L}}}{\\partial\\theta_{2}}}}\\\\ {p_{3}={\\cfrac{\\partial{\\mathcal{L}}}{\\partial{\\dot{\\theta_{3}}}}}}&{{\\dot{p_{3}}}={\\cfrac{\\partial{\\mathcal{L}}}{\\partial\\theta_{3}}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally, we have : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\dot{\\theta_{1}}=\\frac{6(9p_{1}\\cos(2(\\theta_{2}-\\theta_{3}))+27p_{2}\\cos(6_{1}-\\theta_{2})-9p_{2}\\cos(6_{1}+\\theta_{2}-2\\theta_{3})+21p_{3}\\cos(\\theta_{1}-\\theta_{3})-27p_{3}\\cos(\\theta_{1}-2\\theta_{2}+\\theta_{3})-23\\theta_{1}\\sin(\\theta_{1}-2\\theta_{2}+\\theta_{3})-23\\theta_{1}\\sin(\\theta_{1}-\\theta_{3})}{m!2(81-\\theta\\cos(2(\\theta_{1}-\\theta_{2}))-9\\cos(2(\\theta_{1}-\\theta_{3}))+45\\cos(2(\\theta_{2}-\\theta_{3}))-169)}}\\\\ {\\dot{\\theta_{2}}=\\frac{6(27p_{1}\\cos(\\theta_{1}-\\theta_{2})-9p_{1}\\cos(\\theta_{1}+\\theta_{2}-2\\theta_{3})+9p_{2}\\cos(2(\\theta_{1}-\\theta_{3}))-27p_{3}\\cos(2(\\theta_{1}-\\theta_{2}-\\theta_{3})+57p_{3}\\cos(\\theta_{2}-\\theta_{3})-47p_{3}\\sin(\\theta_{1}-\\theta_{3})+7\\theta_{1}\\sin(\\theta_{1}-\\theta_{2})}{m!2(81-\\theta_{2})-9\\cos(2(\\theta_{1}-\\theta_{3}))+45\\cos(2(\\theta_{2}-\\theta_{3}))-169}}\\\\ {\\dot{\\theta_{3}}=\\frac{6(21p_{1}\\cos(\\theta_{1}-\\theta_{3})-27p_{1}\\cos(\\theta_{1}-2\\theta_{2}+\\theta_{3})-27p_{2}\\cos(2\\theta_{1}-\\theta_{2}-\\theta_{3})+57p_{2}\\cos(\\theta_{2}-\\theta_{3})+81p_{3}\\cos(2(\\theta_{1}-\\theta_{2}))-16\\theta_{1}\\sin(\\theta_{1}-\\theta_{3})}{m!2(81-\\theta_{2})-9\\cos(2(\\theta_{1}-\\theta_{3}))+45\\cos(2(\\theta_{2}-\\theta_ \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We simulate the angular of the three sticks by using the Runge-Kutta 4th Order Method as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\Delta\\theta^{1}(t)=\\dot{\\theta}(t,\\theta(t))\\cdot\\Delta t}}\\\\ {{\\Delta\\theta^{2}(t)=\\dot{\\theta}(t+\\frac{\\Delta t}{2},\\theta(t)+\\frac{\\Delta\\theta^{1}(t)}{2})\\cdot\\Delta t}}\\\\ {{\\Delta\\theta^{3}(t)=\\dot{\\theta}(t+\\frac{\\Delta t}{2},\\theta(t)+\\frac{\\Delta\\theta^{2}(t)}{2})\\cdot\\Delta t}}\\\\ {{\\Delta\\theta^{4}(t)=\\dot{\\theta}(t+\\Delta t,\\theta(t)+\\Delta\\theta^{3}(t))\\cdot\\Delta t}}\\\\ {{\\Delta\\theta(t)=\\frac{1}{6}(\\Delta\\theta^{1}(t)+\\Delta\\theta^{2}(t)+\\Delta\\theta^{3}(t)+\\Delta\\theta^{4}(t))}}\\\\ {{\\theta(t+1)=\\theta(t)+\\Delta\\theta(t)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\dot{\\theta}$ was defined as above , and $\\Delta t=0.0001$ is the integration steps. ", "page_idx": 24}, {"type": "text", "text": "C.3 Chaotic Strange Attractor ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The dynamical equations of this reversible strange attractor are as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d x}{d t}=1+y z,}\\\\ {\\displaystyle\\frac{d y}{d t}=-x z,}\\\\ {\\displaystyle\\frac{d z}{d t}=y^{2}+2y z,}\\\\ {\\displaystyle x,y,x\\in\\mathbb{R}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The above equations can be presented as $(\\dot{x}(t),\\dot{y}(t),\\dot{z}(t))=D y n a m i c(x(t),y(t),z(t))$ . We simulate $K(t)=(x(t),y(t),z(t))$ by using the Runge-Kutta 4th Order Method as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\Delta K_{1}(t)={D y n a m i c}(K(t))*\\Delta t}}\\\\ {{\\Delta K_{2}(t)={D y n a m i c}(K(t)+\\frac{\\Delta K_{1}(t)}{2})*\\Delta t}}\\\\ {{\\Delta K_{3}(t)={D y n a m i c}(K(t)+\\frac{\\Delta K_{2}(t)}{2})*\\Delta t}}\\\\ {{\\Delta K_{4}(t)={D y n a m i c}(K(t)+\\Delta K_{3}(t))*\\Delta t}}\\\\ {{\\Delta K(t)=\\frac{1}{6}(\\Delta K_{1}(t)+\\Delta K_{2}(t)+\\Delta K_{3}(t)+\\Delta K_{4}(t))}}\\\\ {{K(t+1)=K(t)+\\Delta K(t)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We sampling $z(t_{0})$ randomly from uniform distribution [1, 3] while fixing $x(t_{0})=y(t_{0})=0$ . We set the trajectory lengths of both training and test dataset to 600, with regular time-step size $\\Delta t=0.03$ and the sample frequency of 10. We add Gaussian noise $0.05n$ , $n\\sim\\bar{\\mathcal{N}}(0,1)$ to training trajectories. ", "page_idx": 24}, {"type": "text", "text": "C.4 Human Motion ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For the real-world motion capture dataset(Carnegie Mellon University, 2003), we focus on the walking sequences of subject 35. Each sample in this dataset is represented by 31 trajectories, each corresponding to the movement of a single joint. For each joint, we first randomly sample the number of observations from a uniform distribution $\\mathcal{U}(30,42)$ and then sample uniformly from the first 50 frames for training and validation trajectories. For testing, we additionally sampled 40 observations from frames [51, 99].We split different walking sequences into training (15 trials) and test sets (7 trials). For each walking sequence, we further split it into several non-overlapping small sequences with maximum length 50 for training, and maximum length 100 for testing. In this way, we generate total 120 training samples and 27 testing samples. We normalize all features (position/velocity) to maximum absolute value of 1 across training and testing datasets. ", "page_idx": 25}, {"type": "text", "text": "D Model Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In the following we introduce in details how we implement our model and each baseline. ", "page_idx": 25}, {"type": "text", "text": "D.1 Initial State Encoder ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For multi-agent systems, the initial state encoder computes the latent node initial states $z_{i}(t)$ for all agents simultaneously considering their mutual interaction. Specifically, it first fuses all observations into a temporal graph and conducts dynamic node representation through a spatial-temporal GNN as in (Huang et al., 2020): ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{j(t)}^{l+1}=h_{j(t)}^{l}+\\sigma\\left(\\displaystyle\\sum_{i(t^{\\prime})\\in\\mathcal{N}_{j(t)}}\\alpha_{i(t^{\\prime})\\to j(t)}^{l}\\times W_{v}\\hat{h}_{i(t^{\\prime})}^{l-1}\\right)}\\\\ &{\\alpha_{i(t^{\\prime})\\to j(t)}^{l}=\\left(W_{k}\\hat{h}_{i(t^{\\prime})}^{l-1}\\right)^{T}\\left(W_{q}h_{j(t)}^{l-1}\\right)\\cdot\\displaystyle\\frac{1}{\\sqrt{d}},\\quad\\hat{h}_{i(t^{\\prime})}^{l-1}=h_{i(t^{\\prime})}^{l-1}+\\mathrm{TE}(t^{\\prime}-t)}\\\\ &{\\mathrm{TE}(\\Delta t)_{2i}=\\sin\\left(\\frac{\\Delta t}{10000^{2i/d}}\\right),~\\mathrm{TE}(\\Delta t)_{2i+1}=\\cos\\left(\\frac{\\Delta t}{10000^{2i/d}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\bigstar\\bigstar||\\bigstar$ denotes concatenation; $\\sigma(\\cdot)$ is a non-linear activation function; $d$ is the dimension of node embeddings. The node representation is computed as a weighted summation over its neighbors plus residual connection where the attention score is a transformer-based (Vaswani et al., 2017) dot-product of node representations by the use of value, key, query projection matrices $W_{v},W_{k},W_{q}$ . Here $h_{j(t)}^{l}$ is the representation of agent $j$ at time $t$ in the $l_{\\cdot}$ -th layer. $i(t^{\\prime})$ is the general index for neighbors connected by temporal edges (where $t^{\\prime}\\neq t$ ) and spatial edges (where $t=t^{\\prime}$ and $i\\neq j$ ). The temporal encoding (Hu et al., 2020) is added to a neighborhood node representation in order to distinguish its message delivered via spatial and temporal edges. Then, we stack $L$ layers to get the final representation for each observation node: $h_{i}^{t}\\doteq h_{i(t)}^{L}$ . Finally, we employ a self-attention mechanism to generate the sequence representation $\\pmb{u}_{i}$ for each agent as their latent initial states: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\pmb{u}_{i}=\\frac{1}{K}\\sum_{t}\\sigma\\left(\\pmb{u}_{i}^{T}\\hat{h}_{i}^{t}\\hat{h}_{i}^{t}\\right),\\;\\pmb{a}_{i}=\\operatorname{tanh}\\left(\\left(\\frac{1}{K}\\sum_{t}\\hat{h}_{i}^{t}\\right)\\pmb{W}_{\\b{a}}\\right),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\mathbf{}a_{i}$ is the average of observation representations with a nonlinear transformation $W_{a}$ and $\\hat{\\pmb{h}}_{i}^{t}={\\pmb{h}}_{i}^{t}+\\mathrm{TE}(t)$ . $K$ is the number of observations for each trajectory. Compared with recurrent models such as RNN, LSTM (Sepp and J\u00fcrgen, 1997), it offers better parallelization for accelerating training speed and in the meanwhile alleviates the vanishing/exploding gradient problem brought by long sequences. For single-agent Systems, there only left the self-attention mechanism component. ", "page_idx": 25}, {"type": "text", "text": "Given the latent initial states, the dynamics of the whole system are determined by the ODE function $g$ which we parametrize as a GNN as in (Huang et al., 2020) for Multi-Agent Systems to capture the continuous interaction among agents. For single-agent systems, we only include self-loop edges in the graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ , which makes the ODE function $g$ a simple MLP. ", "page_idx": 25}, {"type": "text", "text": "We then employ Multilayer Perceptron (MLP) as a decoder to predict the trajectories $\\hat{\\pmb y}_{i}(t)$ from the latent states $z_{i}(t)$ . ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{z_{1}(t),z_{2}(t),z_{3}(t)\\cdots z_{N}(t)=\\mathrm{ODEsolver}(g,[z_{1}(t_{0}),z_{2}(t_{0})\\cdots z_{N}(t_{0})],(t_{0},t_{1}\\cdots t_{K}))}\\\\ &{}&{\\hat{y}_{i}(t)=f_{d e c}(z_{i}(t))}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "D.2 Implementation Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "TREAT ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For multi-agent systems, our implementation of TREAT follows GraphODE pipeline. We implement the initial state encoder using a 2-layer GNN with a hidden dimension of 64 across all datasets. We use ReLU for nonlinear activation. For the sequence self-attention module, we set the output dimension to 128. The encoder\u2019s output dimension is set to 16, and we add 64 additional dimensions initialized with all zeros to the latent states $z_{i}(t)$ to stabilize the training processes as in (Huang et al., 2021). The GNN ODE function is implemented with a single-layer GNN from (Kipf et al., 2018) with hidden dimension 128. For single-agent systems, we only include self-loop edges in the graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ , which makes the ODE function $g$ a simple MLP. To compute trajectories, we use the Runge-Kutta method from torchdiffeq python package s(Chen et al., 2021) as the ODE solver and a one-layer MLP as the decoder. ", "page_idx": 26}, {"type": "text", "text": "We implement our model in pytorch. Encoder, generative model, and the decoder parameters are jointly optimized with AdamW optimizer (Loshchilov and Hutter, 2019) using a learning rate of 0.0001 for spring datasets and 0.00001 for Pendulum. The batch size for all datasets is set to 512. ", "page_idx": 26}, {"type": "text", "text": "$\\mathrm{TREAT}_{\\mathcal{L}_{r e v}=\\mathrm{gt-rev}}$ and $\\mathrm{TREAT}_{\\mathcal{L}_{\\tau e v}=\\mathrm{rev}2}$ share the same architecture and hyparameters as TREAT, with different implementations of the loss function. In TREA $\\mathrm{T}_{\\mathcal{L}_{r e v}=\\mathrm{gt-rev}}$ , instead of comparing forward and reverse trajectories, we look at the L2 distance between the ground truth and reverse trajectories when computing the reversal loss. ", "page_idx": 26}, {"type": "text", "text": "For $\\mathrm{TREAT}_{\\mathcal{L}_{r e v}=\\mathrm{rev}2}$ , we implement the reversal loss following (Huh et al., 2020) with one difference: we do not apply the reverse operation to the momentum portion of the initial state to the ODE function. This is because the initial hidden state is an output of the encoder that mixes position and momentum information. Note that we also remove the additional dimensions to the latent state that TREAT has. To reproduce our model\u2019s results, we provide our code implementation link here. ", "page_idx": 26}, {"type": "text", "text": "LatentODE ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We implement the Latent ODE sequence to sequence model as specified in (Rubanova et al., 2019). We use a 4-layer ODE function in the recognition ODE, and a 2-layer ODE function in the generative ODE. The recognition and generative ODEs use Euler and Dopri5 as solvers (Chen et al., 2021), respectively. The number of units per layer is 1000 in the ODE functions and 50 in GRU update networks. The dimension of the recognition model is set to 100. The model is trained with a learning rate of 0.001 with an exponential decay rate of 0.999 across different experiments. Note that since latentODE is a single-agent model, we compute the trajectory of each object independently when applying it to multi-agent systems. ", "page_idx": 26}, {"type": "text", "text": "HODEN ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "To adapt HODEN, which requires full initial states of all objects, to systems with partial observations, we compute each object\u2019s initial state via linear spline interpolation if it is missing. Following the setup in (Huh et al., 2020), we have two 2-layer linear networks with Tanh activation in between as ODE functions, in order to model both positions and momenta. Each network has a 1000-unit layer followed by a single-unit layer. The model is trained with a learning rate of 0.00001 using a cosine scheduler.HODEN is a single-agent model, we compute the trajectory of each object independently when applying it to multi-agent systems. ", "page_idx": 26}, {"type": "text", "text": "TRS-ODEN ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Similar to HODEN, we compute each object\u2019s initial state via linear spline interpolation if it is missing. As in (Huh et al., 2020), we use a 2-layer linear network with Tanh activation in between as the ODE functions, and the Leapfrog method for solving ODEs. The network has 1000 hidden units and is trained with a learning rate of 0.00001 using a cosine scheduler. TRS-ODEN is a single-agent model, we compute the trajectory of each object independently when applying it to multi-agent systems. ", "page_idx": 26}, {"type": "text", "text": "TRS-ODENGNN ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "For TRSODENGNN, we substitute the ODE function in TRS-ODEN with a GraphODE network. The GraphODE generative model is implemented with a single-layer GNN with hidden dimension 128. As in HODEN and TRS-ODEN, we compute each object\u2019s missing initial state via linear spline interpolation and the Leapfrog method for solving ODE. For all datasets, we use 0.5 as the coefficient for the reversal loss in (Huh et al., 2020), and 0.0002 as the learning rate under cosine scheduling. ", "page_idx": 27}, {"type": "text", "text": "LGODE ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Our implementation follows (Huang et al., 2020) except we remove the Variational Autoencoder (VAE) from the initial state encoder. Instead of using the output from the encoder GNN as the mean and std of the VAE, we directly use it as the latent initial state. That is, the initial states are deterministic instead of being sampled from a distribution. We use the same architecture as in TREAT and train the model using an AdamW optimizer with a learning rate of 0.0001 across all datasets. ", "page_idx": 27}, {"type": "text", "text": "E Additional Experiments ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "E.1 Comparison of different solvers ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We next show our model\u2019s sensitivity regarding solvers with different precisions. Specifically, we compare against Euler and Runge-Kutta (RK4) where the latter is a higher-precision solver. We show the comparison against LGODE and TREAT in Table 3. ", "page_idx": 27}, {"type": "text", "text": "We can firstly observe that TREAT consistently outperforms LGODE, which is our strongest baseline across different solvers and datasets, indicating the effectiveness of the proposed time-reversal symmetry loss. Secondly, we compute the improvement ratio as $\\begin{array}{r}{\\frac{L G O D E-T^{\\mathbf{\\lambda}}\\!R E^{\\mathbf{\\lambda}}\\!A T}{L G O D E}}\\end{array}$ . We can see that the improvement ratios get larger when using RK4 over Euler. This can be understood as our reversal loss is minimizing higher-order Tayler expansion terms (Theoreom 3.1) thus compensating numerical errors brought by ODE solvers. ", "page_idx": 27}, {"type": "table", "img_path": "iWlqbNE8P7/tmp/8339cb646f43890048d84f790e2b7b7176f6b1db839faa4ecaf1524f99759409.jpg", "table_caption": ["Table 3: Evaluation results on MSE $(10^{-2})$ over different solvers for multi-agent systems. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "E.2 Evaluation across observation ratios. ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "For LG-ODE and TREAT, the encoder computes the initial states from observed trajectories. To show models\u2019 sensitivity towards data sparsity, we randomly mask out $40\\%$ and $80\\%$ historical observations and compare model performance. As shown in Table 4, when changing the ratios from $80\\%$ to $40\\%$ , we observe that TREAT has a smaller performance drop compared with LG-ODE, especially on the more complex Pendulum dataset (LG-ODE decreases $2\\bar{2}.04\\%$ while TREAT decreases $1.62\\dot{\\%}$ ). This indicates that TREAT is less sensitive toward data sparsity. ", "page_idx": 27}, {"type": "table", "img_path": "iWlqbNE8P7/tmp/9c2c4bcc6607e628cd27d50367d6cd50f5db6cfc23cd3580992085fe45c41513.jpg", "table_caption": ["Table 4: Results of varying observation ratios on MSE $(10^{-2})$ of multi-agent datasets. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "F Discussion about Reversible Neural Networks ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In literature, there is another line of research about building reversible neural networks (NNs). For example, (Chang et al., 2018) formulates three architectures for reversible neural networks to address the stability issue and achieve arbitrary deep lengths, motivated by dynamical system modeling. (Liu et al., 2019) employs normalizing flow to create a generative model of graph structures. They all propose novel architectures to construct reversible NN where intermediate states across layer depths do not need to be stored, thus improving memory efficiency. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "However, we\u2019d like to clarify that reversible NNs (RevNet) do not resolve the time-reversal symmetry problem that we\u2019re studying. The core of RevNet is that input can be recovered from output via a reversible operation (which is another operator), similar as any linear operator $W(\\cdot)$ have a reversed projector $\\mathring{W}^{-1}(\\cdot)$ . In the contrary, what we want to study is that the same operator can be used for both forward and backward prediction over time, and keep the trajectory the same. That being said, to generate the forward and backward trajectories, we are using the same $g(\\cdot)$ , instead of $g(\\cdot),\\bar{g}^{-1}(\\cdot)$ respectively. ", "page_idx": 28}, {"type": "text", "text": "In summary, though both reversible NN and time-reversal symmetry share similar insights and intuition, they\u2019re talking about different things: reversible NNs make every operator $g(\\cdot)$ having a $g^{-1}(\\cdot)$ , while time-reversible assume the trajectory get from $\\hat{z}^{f w d}=g(z)$ and $\\hat{z}^{b w d}=-g(z)$ to be closer. Making $g$ to be reversible cannot make the system to be time-reversible. ", "page_idx": 28}, {"type": "text", "text": "G Impact Statement ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "This paper presents work whose goal is to advance the field of Machine Learning. TREAT is trained upon physical simulation data (e.g., , spring and pendulum) and implemented by public libraries in PyTorch. During the modeling, we neither introduces any social/ethical bias nor amplify any bias in the data. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 29}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 29}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 29}, {"type": "text", "text": "\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 29}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 29}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 29}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 29}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: [TODO] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Limitations are discussed in Appendix 6 ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: Proofs are in Appendix A.3, A.2and A.4. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The Datasets, Task Setup, Baselines describtion are in Sec. 4. Pseudo code for the implementation of the Time-Reversal Symmetry Loss is in Appendix A.1. More Model Details and Implementation Details are in Appendix D. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 30}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The Datasets, Task Setup, Baselines description are in Sec. 4. Pseudo code for the implementation of the Time-Reversal Symmetry Loss is in Appendix A.1. More Dataset descriptions are in Appendix C. More Model Details and Implementation Details are in Appendix D. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 31}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The Datasets, Task Setup, Baselines description are in Sec. 4. More Implementation Details are in Appendix D.2. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: In Appendix ?? ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 32}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: In Appendix G. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: [TODO] ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: [TODO] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: [TODO] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]