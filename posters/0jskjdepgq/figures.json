[{"figure_path": "0JSKjdePGq/figures/figures_3_1.jpg", "caption": "Figure 1: Experiment on the Pendulum environment for the average cost and a bounded number of switches setting.", "description": "This figure shows the results of an experiment conducted on a pendulum environment using two different settings: average cost and bounded number of switches.  The average cost setting penalizes each interaction with a constant cost, leading to fewer interactions overall and a smoother control trajectory. The bounded number of switches setting limits the total number of interactions, resulting in a policy that strategically chooses the timing of those interactions to maximize reward. The plots show state trajectories, control actions, reward, and time steps.", "section": "4 TACOS with Model-free RL Algorithms"}, {"figure_path": "0JSKjdePGq/figures/figures_3_2.jpg", "caption": "Figure 1: Experiment on the Pendulum environment for the average cost and a bounded number of switches setting.", "description": "This figure shows the results of experiments conducted on a pendulum swing-up task using two different settings: average cost and bounded number of switches.  The plots illustrate the state (cos(\u03b8), sin(\u03b8)), action, running reward, and time for action, over time for each setting. The average cost setting shows a significant reduction in the number of interactions, while the bounded number of switches setting demonstrates that the task can still be solved with a limited number of interactions.", "section": "4 TACOS with Model-free RL Algorithms"}, {"figure_path": "0JSKjdePGq/figures/figures_4_1.jpg", "caption": "Figure 2: We study the effects of the bound on interactions K on the performance of the agent. TACOS performs significantly better than equidistant discretization, especially for small values of K.", "description": "This figure compares the performance of TACOS against an equidistant time discretization baseline for three different tasks: Greenhouse Temperature Tracking, Pendulum Swing-up, and Pendulum Swing-down.  The x-axis represents the number of interactions, and the y-axis shows the episode reward.  The solid lines represent TACOS, and the dashed lines represent the equidistant baseline.  The shaded regions indicate the standard deviation across multiple runs. The results clearly show that TACOS achieves significantly higher rewards than the baseline, especially when the number of interactions (K) is small, highlighting its efficiency in reducing interactions while maintaining or improving performance.", "section": "How does the bound on the number of interactions K affect TACOS?"}, {"figure_path": "0JSKjdePGq/figures/figures_4_2.jpg", "caption": "Figure 3: Effect of interaction cost (first row) and environment stochasticity (second row) on the number of interactions and episode reward for the Pendulum and Greenhouse tasks.", "description": "This figure presents the results of experiments evaluating the effect of interaction cost and environment stochasticity on the performance of the TACOS algorithm. The first row shows the impact of increasing interaction cost (C) on both the episode reward and the number of interactions for the Pendulum Swing-up and Greenhouse Temperature Tracking tasks.  The second row displays the influence of increasing environment stochasticity (magnitude of g*) on the same metrics, revealing how the algorithm adapts to more challenging scenarios.  The shaded regions represent the standard deviation across multiple experimental runs.", "section": "How does the interaction cost magnitude influence TACOS?"}, {"figure_path": "0JSKjdePGq/figures/figures_5_1.jpg", "caption": "Figure 4: We compare the performance of TACOS in combination with SAC and PPO with the standard SAC algorithm and SAC with more compute (SAC-MC) over a range of values for tmin (first row). In the second row, we plot the episode reward versus the physical time in seconds spent in the environment for SAC-TACOS, SAC, and SAC-MC for a specific evaluation frequency 1/teval. We exclude PPO-TACOS in this plot as it, being on-policy, requires significantly more samples than the off-policy methods. While all methods perform equally well for standard discretization (denoted with 1/t*), our method is robust to interaction frequency and does not suffer a performance drop when we decrease tmin.", "description": "The figure compares the performance of different RL algorithms, including SAC-TACOS, on various robotic tasks across different interaction frequencies. The top row shows how the performance changes with respect to the minimum time between interactions (tmin), while the bottom row visualizes the trade-off between episode reward and physical time.", "section": "4 TACOS with Model-free RL Algorithms"}, {"figure_path": "0JSKjdePGq/figures/figures_7_1.jpg", "caption": "Figure 5: We run OTACOS on the pendulum and RC car environment. We report the achieved rewards averaged over five different seeds with one standard error.", "description": "This figure compares the performance of the OTACOS algorithm against other model-based reinforcement learning methods (PETS-TACOS and MEAN-TACOS) and a model-free method (SAC-TACOS) on two robotic control tasks: pendulum swing-up and RC car driving.  The results show the episodic rewards (averaged over five runs) over the number of episodes.  Shaded areas indicate the standard error of the mean.  OTACOS demonstrates superior sample efficiency compared to baselines, reaching higher rewards more quickly.", "section": "5 Efficient Exploration for TACOS via Model-Based RL"}, {"figure_path": "0JSKjdePGq/figures/figures_23_1.jpg", "caption": "Figure 1: Experiment on the Pendulum environment for the average cost and a bounded number of switches setting.", "description": "This figure shows the results of experiments conducted on a pendulum swing-up task using two different settings: one with a constant cost for each interaction, and another with a fixed, limited number of interactions.  The plots illustrate the state (cosine and sine of the angle), action (torque), running reward, and time for each action for both settings.  The figure demonstrates that the proposed method effectively reduces the number of interactions needed to achieve successful control. (a) shows significant reduction in interaction numbers by adding a constant interaction cost, demonstrating effective control with minimal intervention. (b) shows the ability to successfully solve the task even with a drastically reduced number of interactions (K=5).", "section": "4 TACOS with Model-free RL Algorithms"}, {"figure_path": "0JSKjdePGq/figures/figures_24_1.jpg", "caption": "Figure 1: Experiment on the Pendulum environment for the average cost and a bounded number of switches setting.", "description": "This figure presents the results of an experiment on a pendulum swing-up task using two different settings: average cost and bounded number of switches.  The top row (a) shows the results for the average cost setting, where the policy learns to reduce the number of interactions from 200 to 24 by applying maximal torque for longer durations initially and switching the controller more frequently near the equilibrium position. The bottom row (b) illustrates the bounded number of interactions setting (K=5), where the policy successfully solves the task with a limited number of interactions, demonstrating adaptability of the approach.", "section": "4 TACOS with Model-free RL Algorithms"}]