[{"type": "text", "text": "When to Sense and Control? A Time-adaptive Approach for Continuous-Time RL ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lenart Treven,\u2217 Bhavya Sukhija, Yarden As, Florian D\u00f6rfler, Andreas Krause ETH Zurich, Switzerland ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) excels in optimizing policies for discrete-time Markov decision processes (MDP). However, various systems are inherently continuous in time, making discrete-time MDPs an inexact modeling choice. In many applications, such as greenhouse control or medical treatments, each interaction (measurement or switching of action) involves manual intervention and thus is inherently costly. Therefore, we generally prefer a time-adaptive approach with fewer interactions with the system. In this work, we formalize an RL framework, Time-adaptive Control & Sensing (TACOS), that tackles this challenge by optimizing over policies that besides control predict the duration of its application. Our formulation results in an extended MDP that any standard RL algorithm can solve. We demonstrate that state-of-the-art RL algorithms trained on TACOS drastically reduce the interaction amount over their discrete-time counterpart while retaining the same or improved performance, and exhibiting robustness over discretization frequency. Finally, we propose OTACOS, an efficient model-based algorithm for our setting. We show that OTACOS enjoys sublinear regret for systems with sufficiently smooth dynamics and empirically results in further sample-efficiency gains. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nearly all state-of-the-art RL algorithms (Schulman et al., 2017; Haarnoja et al., 2018; Lillicrap et al., 2015; Schulman et al., 2015) were developed for discrete-time MDPs. Nevertheless, continuoustime systems are ubiquitous in nature, ranging from robotics, biology, medicine, environment and sustainability etc. (cf. Spong et al., 2006; Jones et al., 2009; Lenhart and Workman, 2007; Panetta and Fister, 2003; Turchetta et al., 2022). Such systems can be naturally modeled with stochastic differential equations (SDEs), but computational approaches necessitate discretization. Furthermore, in many applications, obtaining measurements and switching actions is expensive. For instance, consider a greenhouse of fruits or medical treatment recommendations. In both cases, each measurement (crop inspection, medical exam) or switching of actions (climate control, treatment adjustment) typically involves costly human intervention. Hence, minimizing such interactions with the underlying system is desirable. This underlying challenge is rarely addressed in the RL literature. ", "page_idx": 0}, {"type": "text", "text": "In practice, a time-equidistant discretization frequency is set, often manually, adjusted to the underlying system\u2019s characteristic time scale. This is challenging, however, especially for unknown/uncertain systems, and systems with multiple dominant time scales (Engquist et al., 2007). Therefore, for many real-world applications having a global frequency of control is inadequate and wasteful. For example, in medicine, patient monitoring often requires higher frequency interaction during the onset of illness and lower frequency interactions as the patient recovers (Kaandorp and Koole, 2007). ", "page_idx": 0}, {"type": "text", "text": "In this work, we address this limitation of standard RL methods and propose a novel RL framework, Time-adaptive Control & Sensing (TACOS). TACOS reduces a general continuous-time RL problem with underlying SDE dynamics to an equivalent discrete-time MDP, that can be solved with any ", "page_idx": 0}, {"type": "text", "text": "RL algorithm, including standard policy gradient methods like PPO and SAC (Schulman et al., 2017;   \nHaarnoja et al., 2018). We summarize our contributions below. ", "page_idx": 1}, {"type": "text", "text": "Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "1. We reformulate the problem of time-adaptive continuous time RL to an equaivalent discrete-time MDP that can be solved with standard RL algorithms.   \n2. Using our formulation, we extend standard policy gradient techniques (Haarnoja et al. (2018) and Schulman et al. (2017)) to the time-adaptive setting. Our empirical results on standard RL benchmarks (Freeman et al., 2021) show that TACOS outperforms its discrete-time counterpart in terms of policy performance, computational cost, and sample efficiency.   \n3. To further improve sample efficiency, we propose a model-based RL algorithm, OTACOS. OTACOS uses well-calibrated probabilistic models to capture epistemic uncertainty and, similar to Curi et al. (2020) and Treven et al. (2023), leverages the principle of optimism in the face of uncertainty to guide exploration during learning. We theoretically prove that OTACOS suffers no regret and empirically demonstrate its sample efficiency. ", "page_idx": 1}, {"type": "text", "text": "2 Problem statement ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider a general nonlinear continuous time dynamical system with continuous state $\\mathcal{X}\\subset\\mathbb{R}^{d_{\\mathbf{x}}}$ and action $\\mathcal{U}\\subset\\mathbb{R}^{d_{\\mathbf{u}}}$ space. The underlying dynamics are governed by a (controllable) SDE: ", "page_idx": 1}, {"type": "equation", "text": "$$\nd\\pmb{x}_{t}=\\pmb{f}^{*}(\\pmb{x}_{t},\\pmb{u}_{t})d t+\\pmb{g}^{*}(\\pmb{x}_{t},\\pmb{u}_{t})d\\pmb{B}_{t}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here $\\mathbf{\\boldsymbol{x}}_{t}\\in\\mathcal{X}$ is the state at time $t$ , $\\boldsymbol{u}_{t}\\in\\mathcal{U}$ the control input, $\\boldsymbol{f}^{*},\\boldsymbol{g}^{*}$ are unknown drift and diffusion functions and $B_{t}$ is a standard Brownian motion in $\\mathbb{R}^{d\\hat{\\boldsymbol{B}}}$ . Our goal is to find a control policy $\\pi_{U}:$ $\\mathcal{X}\\times\\mathcal{T}\\rightarrow\\mathcal{U}$ which maximizes an unknown reward $b^{*}(\\pmb{x}_{t},\\pmb{u}_{t})$ over a fixed horizon $\\tau\\,_{\\overline{{=}}}^{\\mathrm{def}}\\,[0,T]$ , i.e., ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi\\in\\Pi}\\mathbb{E}\\left[\\int_{t\\in\\mathcal{T}}b^{*}(\\pmb{x}_{t},\\pmb{\\pi}_{\\mathcal{U}}(\\pmb{x}_{t},t))d t\\right],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where the expectation is taken w.r.t. the policy and stochastic dynamics and $\\Pi$ is the class of policies2 over which we search. ", "page_idx": 1}, {"type": "text", "text": "In practice, we can only measure the system state and execute control policies in discrete points in time. In this work, we focus on problems where state measurement and control are synchronized in time. We refer to these synchronized time points as interactions in the following parts of this paper. Synchronizing state measurement and control contrasts standard time-adaptive approaches such as event-triggered control (Heemels et al., 2021), where the state is measured arbitrarily high frequency and control inputs are changed only so often to ensure stability. It is also in contrast to the complementary setting, where control inputs are changing at an arbitrarily high frequency but measurements are collected adaptively in time (Treven et al., 2023). An adaptive control approach as Heemels et al. (2021) is very important for many real-world applications but similarly, an adaptive measurement strategy is crucial for efficient learning in RL (Treven et al., 2023). Our approach treats both of these requirements jointly. ", "page_idx": 1}, {"type": "text", "text": "We consider two different scenarios for continuous-time control: (i) Penalizing interactions with some cost, $(i i)$ bounded number of interactions, i.e., hard constraint on control/measurement steps. ", "page_idx": 1}, {"type": "text", "text": "Interaction cost We consider the setting where every interaction we take has an inherent cost $c(\\mathbf{\\boldsymbol{x}}_{t},\\mathbf{\\boldsymbol{u}}_{t})>0$ . Note that we consider this cost structure for its simplicity and TACOS works for more general cost functions that depend on the duration of application for the action $\\pmb{u}_{t}$ or the previous action $\\mathbf{\\delta}u_{t-1}$ and thus captures many practical real-world settings. We define this task more formally below ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\pi\\in\\Pi,\\pi_{T}}\\mathbb{E}\\left[\\sum_{i=0}^{K-1}\\int_{t_{i-1}}^{t_{i}}b^{*}({\\pmb x}_{t},{\\pi}_{\\mathcal{U}}({\\pmb x}_{t_{i-1}},t_{i-1}))d t-c({\\pmb x}_{t_{i-1}},{\\pi}_{\\mathcal{U}}({\\pmb x}_{t_{i-1}},t_{i-1}))\\right],}\\\\ &{t_{i}={\\pi}_{\\mathcal{T}}({\\pmb x}_{t_{i-1}},t_{i-1})+t_{i-1},\\;t_{0}=0,t_{K}=T,\\;\\forall({\\pmb x},t)\\in\\mathcal{X}\\times T;{\\pi}_{\\mathcal{T}}({\\pmb x},t)\\in[t_{\\operatorname*{min}},t_{\\operatorname*{max}}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here $t_{\\mathrm{min}}>0$ is the minimal duration for which we have to apply the control, $t_{\\operatorname*{max}}\\in[t_{\\operatorname*{min}},T]$ the maximum duration, and $\\pi_{T}$ is a policy that predicts the duration of applying the action. ", "page_idx": 1}, {"type": "text", "text": "Bounded number of interactions In this setting, the number of interactions with the system is limited by a known amount $K$ . Intuitively, this represents a scenario where we have a finite budget for the inputs that we can apply and have to decide on the best strategy to space these $K$ inputs over the full horizon. A formal definition of this task is given below ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\pi\\in\\Pi,\\pi_{T}}\\mathbb{E}\\left[\\sum_{i=0}^{K-1}\\int_{t_{i-1}}^{t_{i}}b^{*}({\\pmb x}_{t},{\\pi}_{\\mathcal{U}}({\\pmb x}_{t_{i-1}},t_{i-1},i-1))d t\\right],}\\\\ &{t_{i}=\\pi_{T}({\\pmb x}_{t_{i-1}},t_{i-1},i-1)+t_{i-1},\\;t_{0}=0,t_{K}=T,\\forall({\\pmb x},t,i):\\pi_{T}({\\pmb x},t,i)\\in[t_{\\operatorname*{min}},t_{\\operatorname*{max}}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In the absence of the transition costs or the bound on the number of interactions, intuitively the policy would propose to interact with the system as frequently as possible, i.e., every $t_{\\mathrm{min}}$ seconds. The additional costs/constraints ensure that we do not converge to this trivial (but unrealistic) solution. ", "page_idx": 2}, {"type": "text", "text": "3 TACOS: Time Adaptive Control or Sensing ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the following, we reformulate the continuous-time problem as an equivalent discrete-time MDP. We first denote the state and running reward flows of Equation (1). The state flow by applying action $\\pmb{u}_{k}$ for $t_{k}$ time reads: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle x_{k+1}=\\Xi({\\boldsymbol x}_{k},{\\boldsymbol u}_{k},t_{k}),}\\\\ {\\Xi({\\boldsymbol x},{\\boldsymbol u},t)\\overset{\\mathrm{def}}{=}{\\boldsymbol x}+\\displaystyle\\int_{0}^{t}f^{*}({\\boldsymbol x}_{s},{\\boldsymbol u})d s+\\displaystyle\\int_{0}^{t}g^{*}({\\boldsymbol x}_{s},{\\boldsymbol u})d B_{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We assume that every time we interact with the system, we also obtain the integrated reward and define the reward flow as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Xi_{b^{\\ast}}(\\pmb{x},\\pmb{u},t)\\stackrel{\\mathrm{def}}{=}\\int_{0}^{t}b^{\\ast}\\,(\\Xi(\\pmb{x},\\pmb{u},s),\\pmb{u})\\,d s.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Due to the stochasticity of $(B_{t})_{t\\in{\\mathcal{T}}}$ , the state flow $\\Xi(x,u,t)$ and the reward flow $\\Xi_{b^{*}}(\\pmb{x},\\pmb{u},t)$ are stochastic. For ease of notation, we denote ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Phi_{f^{*}}(x_{k},u_{k},t_{k})\\stackrel{\\mathrm{def}}{=}\\mathbb{E}\\left[\\Xi({\\boldsymbol x}_{k},u_{k},t_{k})\\right],\\quad\\Phi_{b^{*}}(x_{k},u_{k},t_{k})\\stackrel{\\mathrm{def}}{=}\\mathbb{E}\\left[\\Xi_{b^{*}}({\\boldsymbol x}_{k},u_{k},t_{k})\\right]}\\\\ {w_{k}^{x}\\stackrel{\\mathrm{def}}{=}\\Xi({\\boldsymbol x}_{k},u_{k},t_{k})-\\Phi({\\boldsymbol x}_{k},u_{k},t_{k}),\\quad w_{k}^{b^{*}}\\stackrel{\\mathrm{def}}{=}\\Xi_{b^{*}}({\\boldsymbol x}_{k},u_{k},t_{k})-\\Phi_{b^{*}}({\\boldsymbol x}_{k},u_{k},t_{k}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and the concatenated state and reward flow function, and noise as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Phi^{*}({\\boldsymbol x}_{k},{\\boldsymbol u}_{k},t_{k})=\\left(\\Phi_{f^{*}}({\\boldsymbol x}_{k},{\\boldsymbol u}_{k},t_{k})\\right),\\quad{\\boldsymbol w}_{k}=\\left({\\boldsymbol w}_{k}^{x}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In this work, we search for policies that return the next control we apply and also the time for how long to apply the control. ", "page_idx": 2}, {"type": "text", "text": "3.1 Reforumlation of Interaction Cost setting to Discrete-time MDPs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We convert the problem with interaction costs to a standard MDP which any RL algorithm for continuous state-action spaces can solve. To this end, we restrict ourselves to a policy class: ", "page_idx": 2}, {"type": "equation", "text": "$\\Pi_{I C}=\\{\\pi:\\mathcal{X}\\times\\mathcal{T}\\to\\mathcal{U}\\times\\mathcal{T}\\ |\\ \\pi_{T}(\\cdot,t)\\in[t_{\\operatorname*{min}},t_{\\operatorname*{max}}],\\pi\\ \\mathrm{is}\\ L_{\\pi}-\\frac{1}{2}\\},$ ", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For simplicity, we denote by $\\pi_{T}$ the component of the policy that predicts the duration of applying the action and with $\\pi_{\\mathcal{U}}$ the component that predicts the action value. The policies we consider map state $\\textbf{\\em x}$ and time-to-go $t$ to control $\\textbf{\\em u}$ and the time $\\tau$ for how long we apply the action $\\textbf{\\em u}$ . We define the augmented state $\\pmb{s}=(\\pmb{x},b,t)$ , where $\\textbf{\\em x}$ is the state, $b$ integrated reward and $t$ time-to-go. With the introduced notation we arrive at a discrete-time MDP problem formulation ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\pi\\in\\Pi_{I C}}V_{\\pi,\\Phi^{*}}(x_{0},T)=\\operatorname*{max}_{\\pi\\in\\Pi_{I C}}\\,\\mathbb{E}\\left[\\sum_{k=0}^{K-1}r(s_{k},\\pi(s_{k}))\\right]}\\\\ &{\\displaystyle\\mathrm{s.t.}\\quad s_{k+1}=\\Psi_{\\Phi^{*}}(s_{k},\\pi(s_{k}),w_{k}),\\ s_{0}=(x_{0},0,T),\\quad\\sum_{k=0}^{K-1}\\pi_{T}(\\pmb{x}_{k},t_{k})=T,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where we have ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Psi_{\\Phi^{*}}(s_{k},\\pi(s_{k}),w_{k})=(\\Phi^{*}(x_{k},\\pi(x_{k},t_{k}))+w_{k},t_{k}-\\pi_{{\\mathcal{T}}}(x_{k},t_{k}))}\\\\ {r(s_{k},\\pi(s_{k}))=\\Xi_{b^{*}}(x_{k},\\pi(x_{k},t_{k}))-c({\\pmb x}_{k},\\pi_{\\mathcal{U}}({\\pmb x}_{k},t_{k})).\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "0JSKjdePGq/tmp/bc0c3bcbd72af18e8eec1c303dba0951a0e716f4bfc759ce3284085ed75de0a8.jpg", "img_caption": ["Pendulum Swing Up Task [Duration $_{1=10.05}$ , $c(\\pmb{x},\\pmb{u},t)=0.1]$ "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "(a) We add a constant switch cost of 0.1 and significantly reduce the number of interactions from 200 to 24. Initially, the policy applies maximal bang-bang torque for longer times, until the pendulum reaches the top. On the top, we measure and change the controller at a higher frequency in order to keep the pendulum stable, at the position with the highest reward. ", "page_idx": 3}, {"type": "image", "img_path": "0JSKjdePGq/tmp/569e596e31e8e2bac715ff44403fecb881b2d72ca24a45e5c941e0985737ee82.jpg", "img_caption": ["(b) We set a tight bound of $K=5$ for the number of interactions and observe that we can still solve the task. ", "Pendulum Swing Up Task [Duration $=\\!5.05$ , $\\mathsf K=5]$ "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: Experiment on the Pendulum environment for the average cost and a bounded number of switches setting. ", "page_idx": 3}, {"type": "text", "text": "3.2 Reformulation of Bounded Number of Interactions to Discrete-time MDPs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The second setting is similar to the one studied by $\\mathrm{Ni}$ and Jang (2022). In this case, we consider the following class of policies: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Pi_{B I}=\\left\\{\\pi:\\mathcal{X}\\times\\mathcal{T}\\times\\mathbb{N}\\to\\mathcal{U}\\times\\mathcal{T}\\mid\\forall k\\in[K]:\\pi(\\cdot,\\cdot,k){\\mathrm{~is~}}L_{\\pi}-\\mathrm{Lipschitz}\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For an augmented state $\\pmb{s}\\,=\\,(\\pmb{x},b,t,k)$ , our policies map states $\\textbf{\\em x}$ , time-to-go $t$ , number of past interactions $k$ to a controller $\\textbf{\\em u}$ and the time duration $\\tau$ for applying the action. Here the optimal control problem reads ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\pi\\in\\Pi_{B I}}V_{\\pi,\\Phi^{*}}(\\pmb{x}_{0},T)=\\displaystyle\\operatorname*{max}_{\\pi\\in\\Pi_{B I}}\\mathbb{E}\\left[\\sum_{k=0}^{K-1}r(\\pmb{s}_{k},\\pmb{\\pi}(\\pmb{s}_{k}))\\right]}\\\\ &{\\mathrm{s.t.}\\quad\\pmb{s}_{k+1}=\\pmb{\\Psi}_{\\Phi^{*}}(\\pmb{s}_{k},\\pmb{\\pi}(\\pmb{s}_{k}),\\pmb{w}_{k}),\\ \\pmb{s}_{0}=(\\pmb{x}_{0},0,T,0),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Psi_{\\Phi^{*}}(s_{k},\\pi(s_{k}),w_{k})=(\\Phi^{*}(x_{k},\\pi(x_{k},t_{k},k))+w_{k},t_{k}-\\pi_{\\mathcal{T}}(x_{k},t_{k},k),k+1)}\\\\ {r(s_{k},\\pi(s_{k}))=\\Xi_{b^{*}}(x_{k},\\pi(x_{k},t_{k},k)).\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the following, we provide a simple proposition which shows that our reformulated problem is equivalent to its continuous-time counterpart from Section 2. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. The problem in Equation (2) and 3 are equivalent to Equation (6) and 7, respectively. ", "page_idx": 3}, {"type": "text", "text": "Figure 1 depicts the influence of interaction cost and $K$ on the controller\u2019s performance for the pendulum environment. ", "page_idx": 3}, {"type": "text", "text": "4 TACOS with Model-free RL Algorithms ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now illustrate the performance of TACOS on several well-studied robotic RL tasks. We consider the RC car (Kabzan et al., 2020), Greenhouse (Tap, 2000), Pendulum, Reacher, Halfcheetah and Humanoid environments from Brax (Freeman et al., 2021). Thus our experiments range from environments necessitating time-adaptive control like the Greenhouse, a realistic and highly dynamic race car simulation, and a very high dimensional RL task like the Humanoid.3 ", "page_idx": 3}, {"type": "text", "text": "We investigate both the bounded number of interactions and interaction cost settings in our experiments. In particular, we study how the bound $K$ affects the performance of TACOS and compare it to the standard equidistant baseline. We further study the interplay between the stochasticity of the environments (magnitude of $g^{*}$ ) and interaction costs and the influence of $t_{\\mathrm{min}}\\mathrm{on}$ TACOS. For all experiments in this section, we combine SAC with TACOS (SAC-TACOS). ", "page_idx": 4}, {"type": "image", "img_path": "0JSKjdePGq/tmp/69fa7ab923460154c11e8e8f19011433f7e7f3207c6a2ab0d9a0c4afa366600b.jpg", "img_caption": ["Figure 2: We study the effects of the bound on interactions $K$ on the performance of the agent. TACOS performs significantly better than equidistant discretization, especially for small values of $K$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "How does the bound on the number of interactions $K$ affect TACOS? We analyze the bounded number of interactions setting (cf. Section 3.2) of TACOS, studying the relationship between the number of interactions and the achieved episode reward. We compare our algorithm with the standard equidistant time discretization approach which splits the whole horizon $T$ into $T/K$ discrete time steps at which an interaction takes place. We evaluate the two methods in the greenhouse and pendulum environments. For the pendulum, we consider the swing-up and swing-down tasks. The results are reported in Figure 2. The time-adaptive approach performs significantly better than the standard equidistant time discretization. This is particularly the case for the greenhouse and pendulum swing-down tasks. Both tasks involve driving the system to a stable equilibrium and thus, while high-frequency interaction might be necessary at the initial stages, a fairly low interaction frequency can be maintained when the system has reached the equilibrium state. This demonstrates the practical benefits of time-adaptive control. ", "page_idx": 4}, {"type": "image", "img_path": "0JSKjdePGq/tmp/f3e85f8c14e37760bf73d9cf2c699f6f327e5929243b7160b25598194c73a6d5.jpg", "img_caption": ["Figure 3: Effect of interaction cost (first row) and environment stochasticity (second row) on the number of interactions and episode reward for the Pendulum and Greenhouse tasks. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "How does the interaction cost magnitude influence TACOS? We investigate the setting from Section 3.1 with interaction costs. In our experiments, we always pick a constant cost, i.e., $c(\\pmb{x},\\pmb{u})=$ $C$ . We study the influence of $C$ on the episode reward and on the number of interactions that the policy has with the system within an episode. We again evaluate this on the greenhouse and pendulum environment. For the pendulum, we consider the swing-up task. The results are presented in the first row of Figure 3. Noticeably, increasing $C$ reduces the number of interactions. The decrease is drastic for the greenhouse environment since it can be controlled with considerably fewer interactions without having any effect on the performance. Generally, we observe that decreasing the number of interactions, that is, increasing $C$ , also results in a slight decline in episode reward. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "How does environment stochasticity influence the number of interactions? We analyze the influence of the environment\u2019s stochasticity, i.e., the magnitude of the diffusion term $g^{*}$ , on the episode reward and number of interactions on TACOS. Intuitively, the more stochastic the environment, the more interactions we would require to stabilize the system. We again evaluate our method on the greenhouse and pendulum swing-up tasks. The results are reported in the second row of Figure 3. The results verify our intuition that more stochasticity in the environment generally leads to more interactions. However, we observe that the policy is still able to achieve high rewards for a wide range of magnitude of $g^{*}$ . This showcases the robustness and adaptability of TACOS to stochastic environments. ", "page_idx": 5}, {"type": "image", "img_path": "0JSKjdePGq/tmp/a514fa7dfeb42f63ddf6c45dee6fc4ac3afca0fef92207579c397d337a7866af.jpg", "img_caption": ["Figure 4: We compare the performance of TACOS in combination with SAC and PPO with the standard SAC algorithm and SAC with more compute (SAC-MC) over a range of values for $t_{\\mathrm{min}}$ (first row). In the second row, we plot the episode reward versus the physical time in seconds spent in the environment for SAC-TACOS, SAC, and SAC-MC for a specific evaluation frequency $^1\\!/\\!t_{\\mathrm{cval}}$ . We exclude PPO-TACOS in this plot as it, being on-policy, requires significantly more samples than the off-policy methods. While all methods perform equally well for standard discretization (denoted with $1/t^{*}$ ), our method is robust to interaction frequency and does not suffer a performance drop when we decrease $t_{m i n}$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "How does $t_{\\mathrm{min}}$ influence TACOS? As highlighted in Section 1, picking the right discretization for interactions is a challenging task. We show that TACOS can naturally alleviate this issue and adaptively pick the frequency of interaction while also being more computationally and data-efficient. Moreover, we show that TACOS is robust to the choice of $t_{\\mathrm{min}}$ , which represents the minimal duration an action has to be applied, i.e., its inverse is the highest frequency at which we can control the system. In this experiment, we consider SAC-TACOS and compare it to the standard SAC algorithm. TACOS adaptively picks the number of interactions and therefore during an episode of time $T$ , it effectively collects less data than the standard discrete-time RL algorithm.4 This makes comparison to the discrete-time setting challenging since environment interactions and physical time on the environment are not linearly related for TACOS as opposed to the standard discrete-time setting. Nevertheless, to be fair to the discrete-time method, we give SAC more physical time on the system for all environments, effectively resulting in the collection of more data for learning. Since the standard SAC algorithm updates the policy relative to the data amount, we consider a version of SAC, SAC-MC (SAC more compute), which leverages the additional data it collects to perform more gradient updates. This version essentially performs more policy updates than SAC-TACOS and thus is computationally more expensive. Furthermore, to demonstrate the generality of our framework, we also combine TACOS with PPO (PPO-TACOS). ", "page_idx": 5}, {"type": "text", "text": "We report the performance after convergence across different $t_{\\mathrm{min}}$ in the first row of Figure 4. From our experiment, we conclude that SAC-TACOS and PPO-TACOS are robust to the choice of $t_{\\mathrm{min}}$ and perform equally well when $t_{\\mathrm{min}}$ is decreased, i.e., frequency is increased. This is in contrast to the standard RL methods, which have a significant drop in performance at high frequencies. This observation is also made in prior work (Hafner et al., 2019). Crucially, this highlights the sensitivity of the standard RL methods to the frequency of interaction. In the second row of Figure 4 we show the learning curve of the methods for a specific frequency $^1\\!/\\!t_{\\mathrm{eval}}$ . From the curve, we conclude that SAC-TACOS achieves higher rewards with significantly less physical time on the environment. We believe this is because our method explores more efficiently (akin to Dabney et al., 2020; Eberhard et al., 2022), and also learns a much stronger/continuous-time representation of the underlying MDP. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Interestingly, at the default frequency used in the benchmarks $^1/t^{*}$ , all methods perform similarly. However, slightly decreasing the frequency already leads to a drastic drop in performance for all methods. Intuitively, decreasing the frequency prevents us from performing the necessary fine-grained control and obtaining the highest performance. ", "page_idx": 6}, {"type": "text", "text": "While we have access to the optimal frequency $^1/t^{*}$ for these benchmarks, for a general and unknown system it is very difficult to estimate this frequency. Furthermore, as we observe in our experiments, picking a very high frequency is also not an option when using standard RL algorithms. We believe this is where TACOS excels as it adaptively picks the frequency of interaction, thereby relieving the problem designer of this decision. ", "page_idx": 6}, {"type": "text", "text": "5 Efficient Exploration for TACOS via Model-Based RL ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we propose a novel model-based RL algorithm for TACOS called Optimistic TACOS (OTACOS). We analyze the episodic setting, where we interact with the system in episodes ${n=1,\\ldots,N}$ . In episode $n$ , we execute the policy $\\pi_{n}$ , collect measurements and integrated rewards $({\\pmb x}_{n,0},b_{n,0}),\\dots,({\\pmb x}_{n,k_{n}},b_{n,k_{n}})$ , and prepare the data $\\mathcal{D}_{n}~=~\\{(z_{n,1},y_{n,1}),\\ldots,(\\bar{z}_{n,k_{n}},y_{n,k_{n}})\\}$ , where $\\boldsymbol{z}_{n,i}=(\\pmb{x}_{n,i-1},\\pmb{u}_{n,i-1},t_{n,i-1})$ and ${\\pmb y}_{n,i}\\,=\\,({\\pmb x}_{n,i},b_{n,i})$ . From the dataset ${\\mathcal{D}}_{1:n}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\cup_{i\\leq n}{\\mathcal{D}}_{i}$ we build a model $\\mathcal{M}_{n}$ for the unknown function $\\Phi^{*}$ such that it is well-calibrated in the sense of the following definition. ", "page_idx": 6}, {"type": "text", "text": "Definition 1 (Well-calibrated statistical model of $\\Phi^{*}$ , Rothfuss et al. (2023)). Let $\\mathcal{Z}\\overset{d e f}{=}\\mathcal{X}\\times\\mathcal{U}\\times\\mathcal{T}$ . We assume $\\Phi^{*}\\in\\bigcap_{n\\geq0}{\\mathcal{M}}_{n}$ with probability at least $1-\\delta$ , where statistical model $\\mathcal{M}_{n}$ is defined as $\\begin{array}{r}{{\\cal M}_{n}\\stackrel{d e f}{=}\\left\\{f:\\mathcal{Z}\\to\\mathbb{R}^{d_{x}+1}\\;|\\;\\forall z\\in\\mathcal{Z},\\forall j\\in\\{1,\\dots,d_{x}+1\\}:|\\mu_{n,j}(z)-f_{j}(z)|\\leq\\beta_{n}(\\delta)\\sigma_{n,j}(z)\\right\\},}\\end{array}$ Here, $\\mu_{n,j}$ and $\\sigma_{n,j}$ denote the $j$ -th element in the vector-valued mean and standard deviation functions $\\pmb{\\mu}_{n}$ and $\\sigma_{n}$ respectively, and $\\beta_{n}(\\delta)\\ \\in\\ \\mathbb{R}_{\\geq0}$ is a scalar function that depends on the confidence level $\\delta\\in(0,1]$ and which is monotonically increasing in $n$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Similar to model-based RL algorithms for the discrete-time setting (Kakade et al., 2020; Curi et al., 2020; Sukhija et al., 2024), we follow the principle of optimism in the face of uncertainty and select the policy $\\pi_{n}$ for both settings of TACOS (cf. Sections 3.1 and 3.2) by solving: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pmb{\\pi}_{n}\\stackrel{\\mathrm{def}}{=}\\operatorname*{argmax}_{\\pmb{\\pi}\\in\\Pi_{\\square}}\\operatorname*{max}_{\\pmb{\\Phi}\\in\\mathcal{M}_{n-1}}V_{\\pmb{\\pi},\\Phi}(\\pmb{x}_{0},T),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\square\\;\\in\\;\\{I C,B I\\}$ is the appropriate policy class from Section 3. Running OTACOS for $N$ episodes, we measure the performance via the regret: ", "page_idx": 6}, {"type": "equation", "text": "$$\nR_{N}=\\sum_{n=1}^{N}\\bigl(V_{\\pi^{*},\\Phi^{*}}(x_{0},T)-V_{\\pi_{n},\\Phi^{*}}(x_{0},T)\\bigr).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here $\\pi^{*}$ is the optimal policy from the class of policies we optimize over. Any kind of regret bound requires certain assumptions on the regularity of the underlying dynamics (1). ", "page_idx": 6}, {"type": "text", "text": "Assumption 1 (Dynamics model). Given any norm $\\left\\Vert\\cdot\\right\\Vert$ , we assume that the drift $f^{*}$ , and diffusion $g^{*}$ are $L_{f^{*}}$ and $L_{g^{*}}$ -Lipschitz continuous, respectively, with respect to the induced metric. We further assume $\\operatorname*{sup}_{z\\in\\mathcal{Z}}\\|g^{*}(z)\\|_{F}\\leq A$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 1 ensures the existence of the SDE (1) solution under policy $\\pi_{n}$ . To provide bounds on the performance of OTACOS for settings Sections 3.1 and 3.2 we also need some assumptions on the noise and reward model. ", "page_idx": 6}, {"type": "text", "text": "Assumption 2 (Reward and noise model for Section 3.1 Setting). Given any norm $\\lVert\\cdot\\rVert$ , we assume that running reward $b$ is $L_{b}$ -Lipschitz continuous, with respect to the induced metric. We further assume boundedness of the reward $0\\leq b^{*}(\\pmb{x},\\pmb{u})\\leq B$ , and interaction cost $0\\leq c(\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{u}})\\leq C$ . The dynamics noise is independent and follows: $\\pmb{w}_{k}^{x}\\sim\\mathcal{N}\\left(0,\\sigma^{2}(\\pmb{x}_{k},\\pmb{u}_{k},t_{k})I_{d_{x}}\\right)$ . ", "page_idx": 7}, {"type": "text", "text": "Assumption 3 (Reward and noise model for Section 3.2 Setting). Given any norm $\\lVert\\cdot\\rVert$ , we assume that the running reward $b$ is $L_{b}$ -Lipschitz continuous, w.r.t. to the induced metric. ", "page_idx": 7}, {"type": "text", "text": "Finally, we assume that we learn a well-calibrated model of the unknown flow $\\Phi^{*}$ . ", "page_idx": 7}, {"type": "text", "text": "Assumption 4 (Well calibration assumption). Our learned model is an all-time-calibrated statistical model of $\\Phi^{*}$ , i.e., there exists an increasing sequence of $(\\beta_{n}(\\delta))_{n\\geq0}$ such that our model satisfies the well-calibration condition, cf., Definition $^{\\,I}$ . ", "page_idx": 7}, {"type": "text", "text": "Analogous assumptions are made for model-based RL algorithms in the discrete-time setting (Curi et al., 2020; Sukhija et al., 2024). This calibration assumption is satisfied if $\\Phi^{*}$ can be represented with Gaussian Process (GP) (Williams and Rasmussen, 2006; Kirschner and Krause, 2018) models. ", "page_idx": 7}, {"type": "text", "text": "Theorem 2. Consider the setting from Section 3.1 and let Assumption 1, 2, and Assumption 4 hold. Then we have with probability at least $1-\\delta$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\nR_{N}\\leq\\mathcal{O}\\left(\\beta_{N-1}T^{3/2}\\sqrt{N\\mathcal{Z}_{N}}\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Now consider, the setting with a bounded number of switches $K$ , and let Assumption 1, 3, and Assumption $^{4}$ hold. Then, we get with probability at least $1-\\delta$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n{\\cal R}_{N}\\le\\mathcal{O}\\left(\\beta_{N-1}^{K}K e^{D(L_{f^{*}}+L_{g^{*}}^{2})(1+L_{\\pi})T K}\\sqrt{N\\mathcal{Z}_{N}}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $D$ is a constant. Here, with $\\mathcal{T}_{N}$ we denote the model-complexity after observing $N$ points (Curi et al., 2020), which quantifies the difficulty of learning $\\Phi^{*}$ . For $G P s$ , it behaves similar to the maximum information gain $\\gamma_{N}$ (Srinivas et al., 2009), i.e., implying sublinear regret for several common kernels (Vakili et al., 2021). ", "page_idx": 7}, {"type": "text", "text": "As a proof of concept, we evaluate OTACOS on the pendulum and RC car environment for the interaction cost setting. 5 As baselines, we adapt common model-based RL methods such as PETS (Chua et al., 2018) and planning with the mean to TACOS. We call them PETS-TACOS and MEAN-TACOS, respectively. The result is reported in Figure 5. From the figure, we conclude that OTACOS is more sample efficient than other model-based baselines and SAC-TACOS (SAC-TACOS requires circa 6000 episodes for the pendulum and 2000 for the RC car). ", "page_idx": 7}, {"type": "image", "img_path": "0JSKjdePGq/tmp/bdcadadad1f7def9ab7dac3e46a07eab5782e83edabac31bb54af3a915ccca54.jpg", "img_caption": ["Figure 5: We run OTACOS on the pendulum and RC car environment. We report the achieved reward averaged over five different seeds with one standard error. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Similar to this work, Holt et al. (2023); Ni and Jang (2022); Karimi (2023) consider continuoustime deterministic dynamical systems where the measurements or control input changes can only happen at discrete time steps. Moreover, Holt et al. (2023) proposes a similar problem as ours from Section 3.1, where they specify a cost on the number of interactions. However, their solution is based on a heuristic, where a measurement is taken when the variance of the potential reward surpasses a prespecified threshold. On the contrary, we directly tackle this problem at hand and propose a general framework for time-adaptive control that does not rely on any heuristics. Karimi (2023) adapt SAC (Haarnoja et al., 2018) to include a regularization term, which effectively adds a cost for every discrete interaction. Ni and Jang (2022) induce a soft-constraint on the duration $\\tau$ of each action in the environment. However, all the aforementioned works propose heuristic techniques to minimize interactions, whereas we formalize the problem systematically for the more general case of SDEs and show that it has an underlying MDP structure that any RL algorithm can leverage. In addition, we propose a no-regret model-based RL algorithm for this setting and analyze its sample complexity. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Temporal abstractions are considered also in the framework of options (Sutton et al., 1999; Mankowitz et al., 2014; Mann and Mannor, 2014; Harb et al., 2018). However, a key difference to TACOS is that in the options framework, the agent measures the state even between the controller switches. ", "page_idx": 8}, {"type": "text", "text": "Learning to repeat actions Several works observe that repeating actions in the discrete-time MDPs problems such as Atari (Mnih et al., 2013; Braylan et al., 2015) or Cartpole (Hafner et al., 2019) significantly increase the speed of learning. However, the action repeat is fixed through the entire rollout and treated as a hyperparameter. Durugkar et al. (2016); Vezhnevets et al. (2016); Srinivas et al. (2017); Sharma et al. (2017); Lee et al. (2020); Grigsby et al. (2021); Chen et al. (2021); Nam et al. (2021); Yu et al. (2021); Biedenkapp et al. (2021); Krale et al. (2023) automate the selection of action repeat, and show superior performance over the fixed number setting. Dabney et al. (2020) empirically show that repeating the actions helps with the exploration, effectively having a similar effect that colored noise exploration has over the standard white noise exploration (Eberhard et al., 2022). ", "page_idx": 8}, {"type": "text", "text": "Continuous-time RL Following the seminal work of Doya (2000) and the advances in Neural ODEs of Chen et al. (2018), continuous-time RL has regained interest (Cranmer et al., 2020; Greydanus et al., 2019; Yildiz et al., 2021; Lutter et al., 2021). Moreover, modeling in continuous-time is found to be particularly useful when learning from different data sources where each source is collected at a different frequency (Burns et al., 2023; Zheng et al., 2023). An important line of work exists for modeling continuous dynamics for the case when states and actions are discrete, called Markov Jump Processes (Kallianpur and Sundar, 2014; Berger, 1993; Huang et al., 2019; Seifner and Sanchez, 2023). Another line of work that is close to ours is event and self-Triggered Control (Astrom and Bernhardsson, 2002; Anta and Tabuada, 2010; Heemels et al., 2012, 2021), where they model continuous-time control systems by implementing changes to the input only when stability is at risk, ensuring efficient and timely interventions. Treven et al. (2023) propose a no-regret continuous-time model-based RL algorithm, which akin to OTACOS, performs optimistic exploration. They study the problem where controls can be executed continuously in time and propose adaptive measurement selection strategies. Similarly, we propose a novel model-based RL algorithm, OTACOS, based on the principle of optimism in the face of uncertainty. We show that OTACOS has no regret for sufficiently smooth dynamics and has considerable sample-efficiency gains over its model-free counterpart. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion and discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We study the problem of time-adaptive RL for continuous-time systems with continuous state and action spaces. We investigate two practical settings where each interaction has an inherent cost and where we have a hard constraint on the number of interactions. We propose a novel RL framework, TACOS, and show that both of these settings result in extended MDPs which can be solved with standard RL algorithms. In our experiments, we show that combining standard RL algorithms with TACOS results in a significant reduction in the number of interactions without having any effect on the performance for the interaction cost setting. Furthermore, for the second setting, TACOS achieves considerably better control performance despite having a small budget for the number of interactions. Moreover, we show that TACOS improves robustness to a large range of interaction frequencies, and generally improves sample complexity of learning. Finally, we propose, OTACOS, a no-regret model-based RL algorithm for TACOS and show that it has further sample efficiency gains. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This project has received funding from the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545, the Microsoft Swiss Joint Research Center, grant of the Hasler foundation (grant no. 21039) and the SNSF Postdoc Mobility Fellowship 211086. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Anta, A. and Tabuada, P. (2010). To sample or not to sample: Self-triggered control for nonlinear systems. IEEE Transactions on automatic control, 55(9):2030\u20132042.   \nAstrom, K. J. and Bernhardsson, B. M. (2002). Comparison of riemann and lebesgue sampling for first order stochastic systems. In Proceedings of the 41st IEEE Conference on Decision and Control, 2002., volume 2, pages 2011\u20132016. IEEE.   \nBerger, M. A. (1993). Markov Jump Processes, pages 121\u2013138. Springer New York, New York, NY.   \nBiedenkapp, A., Rajan, R., Hutter, F., and Lindauer, M. (2021). Temporl: Learning when to act. In International Conference on Machine Learning, pages 914\u2013924. PMLR.   \nBobkov, S. G. and G\u00f6tze, F. (1999). Exponential integrability and transportation cost related to logarithmic sobolev inequalities. Journal of Functional Analysis, 163(1):1\u201328.   \nBraylan, A., Hollenbeck, M., Meyerson, E., and Miikkulainen, R. (2015). Frame skip is a powerful parameter for learning to play atari. In Workshops at the twenty-ninth AAAI conference on artificial intelligence.   \nBurns, K., Yu, T., Finn, C., and Hausman, K. (2023). Offline reinforcement learning at multiple frequencies. In Conference on Robot Learning, pages 2041\u20132051. PMLR.   \nChen, C., Tang, H., Hao, J., Liu, W., and Meng, Z. (2021). Addressing action oscillations through learning policy inertia. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 7020\u20137027.   \nChen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. (2018). Neural ordinary differential equations. Advances in neural information processing systems, 31.   \nChua, K., Calandra, R., McAllister, R., and Levine, S. (2018). Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In NeurIPS.   \nCranmer, M., Greydanus, S., Hoyer, S., Battaglia, P., Spergel, D., and Ho, S. (2020). Lagrangian neural networks. arXiv preprint arXiv:2003.04630.   \nCuri, S., Berkenkamp, F., and Krause, A. (2020). Efficient model-based reinforcement learning through optimistic policy search and planning. Advances in Neural Information Processing Systems, 33:14156\u201314170.   \nDabney, W., Ostrovski, G., and Barreto, A. (2020). Temporally-extended {\\epsilon}-greedy exploration. arXiv preprint arXiv:2006.01782.   \nDjellout, H., Guillin, A., and Wu, L. (2004). Transportation cost-information inequalities and applications to random dynamical systems and diffusions. The Annals of Probability, 32(3):2702\u2013 2732.   \nDoya, K. (2000). Reinforcement learning in continuous time and space. Neural computation, 12(1):219\u2013245.   \nDurugkar, I. P., Rosenbaum, C., Dernbach, S., and Mahadevan, S. (2016). Deep reinforcement learning with macro-actions. arXiv preprint arXiv:1606.04615.   \nEberhard, O., Hollenstein, J., Pinneri, C., and Martius, G. (2022). Pink noise is all you need: Colored noise exploration in deep reinforcement learning. In The Eleventh International Conference on Learning Representations.   \nEngquist, B., Li, X., Ren, W., Vanden-Eijnden, E., et al. (2007). Heterogeneous multiscale methods: a review. Communications in Computational Physics, 2(3):367\u2013450.   \nFreeman, C. D., Frey, E., Raichuk, A., Girgin, S., Mordatch, I., and Bachem, O. (2021). Brax - a differentiable physics engine for large scale rigid body simulation.   \nGreydanus, S., Dzamba, M., and Yosinski, J. (2019). Hamiltonian neural networks. Advances in neural information processing systems, 32.   \nGrigsby, J., Yoo, J. Y., and Qi, Y. (2021). Towards automatic actor-critic solutions to continuous control. arXiv preprint arXiv:2106.08918.   \nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR.   \nHafner, D., Lillicrap, T., Ba, J., and Norouzi, M. (2019). Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603.   \nHarb, J., Bacon, P.-L., Klissarov, M., and Precup, D. (2018). When waiting is not an option: Learning options with a deliberation cost. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.   \nHeemels, W., Johansson, K. H., and Tabuada, P. (2021). Event-triggered and self-triggered control. In Encyclopedia of Systems and Control, pages 724\u2013730. Springer.   \nHeemels, W. P., Johansson, K. H., and Tabuada, P. (2012). An introduction to event-triggered and self-triggered control. In 2012 ieee 51st ieee conference on decision and control (cdc), pages 3270\u20133285. IEEE.   \nHolt, S., H\u00fcy\u00fck, A., and van der Schaar, M. (2023). Active observing in continuous-time control. In Thirty-seventh Conference on Neural Information Processing Systems.   \nHuang, Y., Kavitha, V., and Zhu, Q. (2019). Continuous-time markov decision processes with controlled observations. In 2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 32\u201339. IEEE.   \nJones, D. S., Plank, M., and Sleeman, B. D. (2009). Differential equations and mathematical biology. CRC press.   \nKaandorp, G. C. and Koole, G. (2007). Optimal outpatient appointment scheduling. Health care management science, 10:217\u2013229.   \nKabzan, J., Valls, M. I., Reijgwart, V. J., Hendrikx, H. F., Ehmke, C., Prajapat, M., B\u00fchler, A., Gosala, N., Gupta, M., Sivanesan, R., et al. (2020). Amz driverless: The full autonomous racing system. Journal of Field Robotics, 37(7):1267\u20131294.   \nKakade, S., Krishnamurthy, A., Lowrey, K., Ohnishi, M., and Sun, W. (2020). Information theoretic regret bounds for online nonlinear control. NeurIPS, 33:15312\u201315325.   \nKallianpur, G. and Sundar, P. (2014). 266Jump Markov Processes. In Stochastic Analysis and Diffusion Processes. Oxford University Press.   \nKarimi, A. (2023). Decision frequency adaptation in reinforcement learning using continuous options with open-loop policies.   \nKirschner, J. and Krause, A. (2018). Information directed sampling and bandits with heteroscedastic noise. In Conference On Learning Theory, pages 358\u2013384. PMLR.   \nKrale, M., Sim\u00e3o, T. D., and Jansen, N. (2023). Act-then-measure: reinforcement learning for partially observable environments with active measuring. In Proceedings of the International Conference on Automated Planning and Scheduling, volume 33, pages 212\u2013220.   \nLee, J., Lee, B.-J., and Kim, K.-E. (2020). Reinforcement learning for control with multiple frequencies. Advances in Neural Information Processing Systems, 33:3254\u20133264. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Lenhart, S. and Workman, J. T. (2007). Optimal control applied to biological models. CRC press. ", "page_idx": 11}, {"type": "text", "text": "Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. (2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.   \nLutter, M., Mannor, S., Peters, J., Fox, D., and Garg, A. (2021). Value iteration in continuous actions, states and time. arXiv preprint arXiv:2105.04682.   \nMankowitz, D. J., Mann, T. A., and Mannor, S. (2014). Time regularized interrupting options. In Internation Conference on Machine Learning.   \nMann, T. and Mannor, S. (2014). Scaling up approximate value iteration with options: Better policies with fewer iterations. In International conference on machine learning, pages 127\u2013135. PMLR.   \nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.   \nNam, H. A., Fleming, S., and Brunskill, E. (2021). Reinforcement learning with state observation costs in action-contingent noiselessly observable markov decision processes. Advances in Neural Information Processing Systems, 34:15650\u201315666.   \nNi, T. and Jang, E. (2022). Continuous control on time. In ICLR 2022 Workshop on Generalizable Policy Learning in Physical World.   \nPanetta, J. C. and Fister, K. R. (2003). Optimal control applied to competing chemotherapeutic cell-kill strategies. SIAM Journal on Applied Mathematics, 63(6):1954\u20131971.   \nRothfuss, J., Sukhija, B., Birchler, T., Kassraie, P., and Krause, A. (2023). Hallucinated adversarial control for conservative offilne policy evaluation. In Uncertainty in Artificial Intelligence, pages 1774\u20131784. PMLR.   \nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust region policy optimization. In International conference on machine learning, pages 1889\u20131897. PMLR.   \nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.   \nSeifner, P. and Sanchez, R. J. (2023). Neural markov jump processes. arXiv preprint arXiv:2305.19744.   \nSharma, S., Srinivas, A., and Ravindran, B. (2017). Learning to repeat: Fine grained action repetition for deep reinforcement learning. arXiv preprint arXiv:1702.06054.   \nSpong, M. W., Hutchinson, S., Vidyasagar, M., et al. (2006). Robot modeling and control, volume 3. Wiley New York.   \nSrinivas, A., Sharma, S., and Ravindran, B. (2017). Dynamic action repetition for deep reinforcement learning. In Proc. AAAI.   \nSrinivas, N., Krause, A., Kakade, S. M., and Seeger, M. (2009). Gaussian process optimization in the bandit setting: No regret and experimental design. arXiv preprint arXiv:0912.3995.   \nSukhija, B., Treven, L., Sancaktar, C., Blaes, S., Coros, S., and Krause, A. (2024). Optimistic active exploration of dynamical systems. NeurIPS.   \nSutton, R. S., Precup, D., and Singh, S. (1999). Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181\u2013211.   \nTap, F. (2000). Economics-based optimal control of greenhouse tomato crop production. Wageningen University and Research.   \nTreven, L., H\u00fcbotter, J., Sukhija, B., D\u00f6rfler, F., and Krause, A. (2023). Efficient exploration in continuous-time model-based reinforcement learning.   \nTurchetta, M., Corinzia, L., Sussex, S., Burton, A., Herrera, J., Athanasiadis, I., Buhmann, J. M., and Krause, A. (2022). Learning long-term crop management strategies with cyclesgym. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, Advances in Neural Information Processing Systems, volume 35, pages 11396\u201311409. Curran Associates, Inc.   \nVakili, S., Khezeli, K., and Picheny, V. (2021). On information gain and regret bounds in gaussian process bandits. In AISTATS.   \nVezhnevets, A., Mnih, V., Osindero, S., Graves, A., Vinyals, O., Agapiou, J., and kavukcuoglu, k. (2016). Strategic attentive writer for learning macro-actions. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc.   \nWilliams, C. K. and Rasmussen, C. E. (2006). Gaussian processes for machine learning, volume 2. MIT press Cambridge, MA.   \nYildiz, C., Heinonen, M., and L\u00e4hdesm\u00e4ki, H. (2021). Continuous-time model-based reinforcement learning. In International Conference on Machine Learning, pages 12009\u201312018. PMLR.   \nYu, H., Xu, W., and Zhang, H. (2021). Taac: Temporally abstract actor-critic for continuous control. Advances in Neural Information Processing Systems, 34:29021\u201329033.   \nZheng, Q., Henaff, M., Amos, B., and Grover, A. (2023). Semi-supervised offline reinforcement learning with action-free trajectories. In International conference on machine learning, pages 42339\u201342362. PMLR. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Contents of Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Extended Theory 15 ", "page_idx": 13}, {"type": "text", "text": "A.1 Transition Cost setting 15   \nA.2 Bounded number of transition 18 ", "page_idx": 13}, {"type": "text", "text": "B Additional Experiments ", "page_idx": 13}, {"type": "text", "text": "24 ", "page_idx": 13}, {"type": "text", "text": "A Extended Theory", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we prove Theorem 2 for OTACOS. We separate the section into two parts; proof for the transaction cost setting (Appendix A.1) and the proof for the bounded number of switches setting (Appendix A.2). ", "page_idx": 14}, {"type": "text", "text": "We start with the definitions of model complexity and sub-Gaussian random vector that we will use extensively in this section. ", "page_idx": 14}, {"type": "text", "text": "Definition 2 (Model Complexity). We define the model complexity as is defined by Curi et al. (2020). ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{N}:=\\operatorname*{max}_{\\mathcal{D}_{1},\\ldots,\\mathcal{D}_{N}}\\sum_{n=1}^{N}\\sum_{(\\pmb{x},\\pmb{u},t)\\in\\mathcal{D}_{n}}\\|\\pmb{\\sigma}_{n}(\\pmb{x},\\pmb{u},t)\\|_{2}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Definition 3. A random variable $x\\in\\mathbb R$ is said to be sub-Gaussian with variance proxy $\\sigma^{2}\\,i f\\mathbb{E}[x]=0$ and we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}[e^{t x}]\\leq e^{{\\frac{\\sigma^{2}t^{2}}{2}}},\\quad\\forall t\\in\\mathbb{R}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A random vector $\\pmb{x}\\in\\mathbb{R}^{d}$ is said to be sub Gaussian with variance proxy $\\sigma^{2}$ if for any $e\\in\\mathbb{R}^{d},\\|e\\|_{2}=$ 1 the random variable $x^{\\top}e$ is $\\sigma^{2}$ sub Gaussian. We write $\\mathbf{\\boldsymbol{x}}\\sim s u b\\boldsymbol{G}\\left(\\sigma^{2}\\right)$ . ", "page_idx": 14}, {"type": "text", "text": "In the following, we will be distinguishing between the state of the augmented MDP $\\pmb{s}$ and the true state of the dynamical system $\\textbf{\\em x}$ . The augmented state at time step $k$ includes the true state of the system, $\\pmb{x}_{k}$ , the integrated reward $b_{k}$ between $k-1$ and $k$ , and the time to left to go $t_{k}$ , i.e., $\\pmb{s}_{k}=[\\pmb{x}_{k}^{\\top},b_{k},t_{k}]^{\\top}$ . ", "page_idx": 14}, {"type": "text", "text": "A.1 Transition Cost setting ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We prove our regret bound for the transition cost case in the following. We start with the difference lemma which adapts Sukhija et al. (2024, Lemma 2) to our setting. ", "page_idx": 14}, {"type": "text", "text": "Lemma 3 (Difference lemma). Define $V_{\\pi_{n},\\Phi}(x,\\tau)$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi,\\Phi}\\left[\\sum_{k\\geq0}^{K(\\tau)-1}r(s_{k},\\pi(s_{k}))\\Big|x_{0}=x\\right];\\;w h e r e\\sum_{k=0}^{K(\\tau)-1}\\pi_{T}(x_{k},t_{k})=\\tau\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "that is the total reward starting with time to go $\\tau$ and state $\\textbf{\\em x}$ for the policy $\\pi$ and dynamics $\\Phi$ . Here the expectation w.r.t. $\\pi,\\Phi$ represents the expectation of the underlying trajectory induced by the policy $\\pi$ on the dynamics $\\Phi$ . Then we have for all $\\pi,\\,\\Phi^{\\prime},\\,\\Phi^{*},\\,x_{0},\\,T<0$ ; ", "page_idx": 14}, {"type": "equation", "text": "$$\nV_{\\pi,\\Phi^{\\prime}}(x_{0},T)-V_{\\pi,\\Phi^{*}}(x_{0},T)=\\mathbb{E}_{\\pi,\\Phi^{*}}\\left[\\sum_{k\\geq0}V_{\\pi,\\Phi^{\\prime}}(\\widehat{x}_{k+1},t_{k+1})-V_{\\pi,\\Phi^{\\prime}}(x_{k+1},t_{k+1})\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\widehat{\\boldsymbol{x}}_{k+1}$ is the state of $\\widehat{\\pmb{s}}_{k+1}\\;\\;=\\;\\;\\Psi_{\\Phi^{\\prime}}(\\pmb{s}_{k},\\pi(\\pmb{s}_{k}),\\pmb{w}_{k})$ and $\\pmb{x}_{k+1}$ is the state of $\\begin{array}{r l}{\\mathbf{\\mathcal{S}}_{k+1}}&{{}=}\\end{array}$ $\\Psi_{\\Phi^{*}}(s_{k},\\pi(s_{k}),w_{k})$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\pi,\\Phi^{*}}(x_{0},T)=\\mathbb{E}_{\\pi,\\Phi^{*}}\\left[\\underset{k\\geq0}{\\sum}r(s_{k},\\pi(s_{k}))\\right]}\\\\ &{\\phantom{\\sum_{\\pi,\\Phi^{*}}(x_{0},T)}=\\mathbb{E}_{\\pi,\\Phi^{*}}\\left[r(s_{0},\\pi(s_{0}))+\\sum_{k\\geq1}r(s_{k},\\pi(s_{k}))\\right]}\\\\ &{\\phantom{\\sum_{\\pi,\\Phi^{*}}(x_{0},T)}=\\mathbb{E}_{\\pi,\\Phi^{*}}\\left[r(s_{k},\\pi(s_{0}))+V_{\\pi,\\Phi^{*}}(x_{1},t_{1})\\right]}\\\\ &{=\\mathbb{E}_{\\pi,\\Phi^{*}}\\left[r(s_{k},\\pi(s_{0}))+V_{\\pi,\\Phi^{*}}(\\hat{x}_{1},t_{1})-V_{\\pi,\\Phi^{*}}(x_{0},T)\\right]+}\\\\ &{\\phantom{\\sum_{\\pi,\\Phi^{*}}\\left[V_{\\pi,\\Phi^{*}}(x_{0},T)-V_{\\pi,\\Phi^{*}}(\\hat{x}_{1},t_{1})+V_{\\pi,\\Phi^{*}}(x_{1},t_{1})\\right]}}\\\\ &{=V_{\\pi,\\Phi^{*}}(x_{0},T)+\\mathbb{E}_{\\pi,\\Phi^{*}}\\left[V_{\\pi,\\Phi}(x_{1},t_{1})-V_{\\pi,\\Phi^{*}}(\\hat{x}_{1},t_{1})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n+\\mathbb{E}_{\\pmb{\\pi},\\Phi^{*}}\\left[V_{\\pmb{\\pi},\\Phi^{*}}(\\pmb{x}_{1},t_{1})-V_{\\pmb{\\pi},\\Phi^{\\prime}}(\\pmb{x}_{1},t_{1})\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\pi,\\Phi^{*}}(x_{0},T)-V_{\\pi,\\Phi^{\\prime}}(x_{0},T)=}\\\\ &{\\ =\\mathbb{E}_{\\pi,\\Phi^{*}}\\left[V_{\\pi,\\Phi^{\\prime}}(x_{1},t_{1})-V_{\\pi,\\Phi^{\\prime}}(\\widehat{x}_{1},t_{1})\\right]+\\mathbb{E}_{\\pi,\\Phi^{*}}\\left[V_{\\pi,\\Phi^{*}}(x_{1},t_{1})-V_{\\pi,\\Phi^{\\prime}}(x_{1},t_{1})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By repeating the step inductively the result follows. ", "page_idx": 15}, {"type": "text", "text": "In the following, we leverage the result above to bound the regret of our optimistic planner w.r.t. the difference in value functions. ", "page_idx": 15}, {"type": "text", "text": "Lemma 4 (Per episode regret bound). Let Assumption 4 hold, then we have with probability at least $1-\\delta$ for all $n\\geq0$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n/_{\\pi_{n},\\Phi^{\\star}}(x_{0},T)-V_{\\pi^{\\star},\\Phi^{\\star}}(x_{0},T)\\leq\\mathbb{E}_{\\pi_{n},\\Phi^{\\star}}\\left[\\sum_{k\\geq0}V_{\\pi_{n},\\Phi_{n}}(\\widehat{x}_{n,k+1},t_{n,k+1})-V_{\\pi_{n},\\Phi_{n}}({\\pmb x}_{n,k+1},t_{n,k+1})\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Since we choose the policy optimistically, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\nV_{\\pi^{*},\\Phi^{*}}(x_{0},T)-V_{\\pi_{n},\\Phi^{*}}(x_{0},T)\\leq V_{\\pi_{n},\\Phi_{n}}(x_{0},T)-V_{\\pi_{n},\\Phi^{*}}(x_{0},T).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Applying Lemma 3 the result follows. ", "page_idx": 15}, {"type": "text", "text": "Now we derive an upper and lower bound on our value function. ", "page_idx": 15}, {"type": "text", "text": "Lemma 5 (Objective upper bound). Let $\\pi$ be any policy from the class $\\Pi_{T C}$ and consider any $T>0$ , then we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n-\\frac{C}{t_{\\mathrm{min}}}T\\leq V_{\\pi,\\Psi^{*}}(x_{0},T)\\leq B T.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Since running reward is bounded $0\\leq b^{*}(\\pmb{x},\\pmb{u})\\leq B$ , the number of steps $K$ we can do in an episode is bounded with $\\begin{array}{r}{0\\le K\\le\\frac{T}{t_{\\mathrm{min}}}}\\end{array}$ , and switch cost is bounded $0\\leq c(\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{u}})\\leq C$ we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n-\\frac{C}{t_{\\mathrm{min}}}T\\leq V_{\\pi,\\Psi^{*}}(x_{0},T)\\leq B T.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A key lemma we use to bound the difference in value functions is the following from Kakade et al.   \n(2020). ", "page_idx": 15}, {"type": "text", "text": "Lemma 6 (Absolute expectation Difference Under Two Gaussians (Lemma C.2. Kakade et al. (2020))). Let $z_{1}\\sim\\mathcal{N}(\\bar{\\mu}_{1},\\sigma^{2}I)$ and $z_{2}\\sim\\mathcal{N}(\\pmb{\\mu}_{2},\\pmb{\\sigma}^{2}\\pmb{I})$ , and for any (appropriately measurable) positive function $g$ , it holds that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}[g(z_{1})]-\\mathbb{E}[g(z_{2})]\\leq\\operatorname*{min}\\left\\{{\\frac{\\|\\mu_{1}-\\mu_{2}\\|}{\\sigma^{2}}},1\\right\\}{\\sqrt{\\mathbb{E}[g^{2}(z_{1})]}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Furthermore, due to Assumption 4 we can also bound the distance between the next state prediction by the true system $\\Phi^{*}$ and the optimistic system $\\Phi_{n}$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 7. Let Assumption 4 hold, then we have the following for all $n\\geq0$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\pmb{x}_{n,k+1}-\\widehat{\\pmb{x}}_{n,k+1}\\|\\leq2\\sqrt{d_{\\pmb{x}}}\\beta_{n-1}\\,\\|\\pmb{\\sigma}_{n-1}(\\pmb{x}_{n,k},\\pmb{\\pi}_{n}(\\pmb{x}_{n,k},t_{n,k}))\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|{\\pmb x}_{n,k+1}-\\widehat{\\pmb x}_{n,k+1}\\|=\\|{\\pmb\\Phi}^{*}({\\pmb x}_{k},{\\pmb\\pi}_{n}({\\pmb x}_{n,k},t_{n,k}))+{\\pmb w}_{n,k}-\\left(\\Phi_{n}({\\pmb x}_{n,k},{\\pmb\\pi}_{n}({\\pmb x}_{n,k},t_{n,k})+{\\pmb w}_{n,k})\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\|{\\pmb\\Phi}^{*}({\\pmb x}_{k},{\\pmb\\pi}_{n}({\\pmb x}_{n,k},t_{n,k}))-\\Phi_{n}({\\pmb x}_{n,k},{\\pmb\\pi}_{n}({\\pmb x}_{n,k},t_{n,k})\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\sqrt{d_{x}}{\\beta}_{n-1}\\left\\|{\\pmb\\sigma}_{n-1}({\\pmb x}_{n,k},{\\pmb\\pi}_{n}({\\pmb x}_{n,k},t_{n,k}))\\right\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last inequality follows from the fact that $\\Phi^{*},\\Phi_{n}\\in\\mathcal{M}_{n-1}$ ", "page_idx": 15}, {"type": "text", "text": "Next, we relate the regret at each episode to the model epistemic uncertainty using Lemma 3 and Lemma 7. ", "page_idx": 16}, {"type": "text", "text": "Corollary 8. Let Assumption 1 \u2013 2 and Assumption 4 hold, then we have for all $n\\geq0$ with probability at least $1-\\delta$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{\\gamma}_{\\pi_{n},\\Phi^{*}}(x_{0},T)-V_{\\pi^{*},\\Phi^{*}}(x_{0},T)\\leq\\frac{2\\sqrt{d_{x}}\\beta_{n-1}T}{\\sigma}\\left(B+\\frac{C}{t_{\\mathrm{min}}}\\right)\\mathbb{E}\\left[\\sum_{k\\geq0}\\|\\sigma_{n-1}(x_{n,k},\\pi_{n}(x_{n,k},t_{n,k}))\\|_{2}\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. From Lemma 4 we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\gamma_{\\pi_{n},\\Phi^{*}}(x_{0},T)-V_{\\pi^{*},\\Phi^{*}}(x_{0},T)\\leq\\mathbb{E}\\left[\\sum_{k\\geq0}V_{\\pi_{n},\\Phi_{n}}(x_{n,k+1},t_{n,k+1})-V_{\\pi_{n},\\Phi_{n}}(\\widehat{x}_{n,k+1},t_{n,k+1})\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma 6 can be applied to positive function $g$ . We hence make a transformation and apply it to $\\begin{array}{r}{g(\\cdot)=V_{\\pi_{n},\\Phi_{n}}(\\cdot,\\dot{t_{n,k+1}}^{\\dot{\\mathrm{~\\,~}}})+\\frac{\\dot{\\mathrm{~\\,~}}C}{t_{\\mathrm{min}}}T}\\end{array}$ , which is positive due to Lemma 5. Moreover, $\\forall{\\pmb x}\\in{\\mathcal{X}}$ ; ", "page_idx": 16}, {"type": "equation", "text": "$$\ng(\\cdot)=V_{\\pi_{n},\\Phi_{n}}(\\cdot,t_{n,k+1})+\\frac{C}{t_{\\operatorname*{min}}}T\\leq B t_{n,k+1}+\\frac{C}{t_{\\operatorname*{min}}}T\\leq T(B+\\frac{C}{t_{\\operatorname*{min}}}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Applying Lemma 6 we obtain: ", "page_idx": 16}, {"type": "equation", "text": "$$\nV_{\\pi_{n},\\Phi_{n}}(x_{n,k+1},t_{n,k+1})-V_{\\pi_{n},\\Phi_{n}}(\\widehat{x}_{n,k+1},t_{n,k+1})\\leq\\frac{T}{\\sigma}\\left(B+\\frac{C}{t_{\\operatorname*{min}}}\\right)\\mathbb{E}\\left[\\left\\|x_{n,k+1}-\\widehat{x}_{n,k+1}\\right\\|\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, applying Lemma 7 we arrive at: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{\\gamma}_{\\pi_{n},\\Phi^{*}}(x_{0},T)-V_{\\pi^{*},\\Phi^{*}}(x_{0},T)\\leq\\frac{2\\sqrt{d_{x}}\\beta_{n-1}T}{\\sigma}\\left(B+\\frac{C}{t_{\\mathrm{min}}}\\right)\\mathbb{E}\\left[\\sum_{k\\geq0}\\|\\sigma_{n-1}(x_{n,k},\\pi_{n}(x_{n,k},t_{n,k}))\\|_{2}\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we can prove our regret bound for the transition cost case. ", "page_idx": 16}, {"type": "text", "text": "Theorem 9. Let Assumption $I-2$ and Assumption 4 hold, then we have for all $n\\geq0$ with probability at least $1-\\delta$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle R_{N}=\\sum_{n=1}^{N}V_{\\pi_{n},\\Phi^{*}}(x_{0},T)-V_{\\pi^{*},\\Phi^{*}}(x_{0},T)}}\\\\ {{\\displaystyle\\qquad\\leq\\frac{2\\sqrt{d_{x}}\\beta_{N-1}T^{3/2}}{\\sigma^{2}t_{\\mathrm{min}}}\\left(B+\\frac{C}{t_{\\mathrm{min}}}\\right)\\sqrt{N\\mathbb{Z}_{N}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We compute: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{R_{N}}=\\displaystyle\\sum_{n=1}^{N}{{V_{\\pi}}_{n}},\\boldsymbol{\\#}\\left(\\boldsymbol{x_{0}},T\\right)-{V_{\\pi^{*}}},\\boldsymbol{\\#}\\left(\\boldsymbol{x_{0}},T\\right)}\\\\ &{\\quad\\le\\displaystyle\\frac{2\\sqrt{d_{\\alpha}}T}{\\sigma}\\left(B+\\frac{C}{{t_{\\operatorname*{min}}}}\\right)\\sum_{n=1}^{N}{\\beta_{n-1}}\\mathbb{E}\\left[\\sum_{k\\ge0}^{N}{\\|\\sigma_{n-1}({x_{n,k}},{\\pi_{n}}({x_{n,k}},t_{n,k}))\\|}\\right]}\\\\ &{\\quad\\le\\displaystyle\\frac{2\\sqrt{d_{\\alpha}}{\\beta_{N-1}}T}{\\sigma}\\left(B+\\frac{C}{{t_{\\operatorname*{min}}}}\\right)\\mathbb{E}\\left[\\sum_{n=1}^{N}{\\|\\sigma_{n-1}({x_{n,k}},{\\pi_{n}}({x_{n,k}},t_{n,k}))\\|}\\right]}\\\\ &{\\quad\\le\\displaystyle\\frac{2\\sqrt{d_{\\alpha}}{\\beta_{N-1}}T}{\\sigma}\\left(B+\\frac{C}{{t_{\\operatorname*{min}}}}\\right)\\sqrt{\\frac{T N}{t_{\\operatorname*{min}}}}\\mathbb{E}\\left[\\sqrt{\\sum_{n=1}^{N}\\sum_{k\\ge0}^{N}{\\|\\sigma_{n-1}({x_{n,k}},{\\pi_{n}}({x_{n,k}},t_{n,k}))\\|^{2}}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\leq\\frac{2\\sqrt{d_{x}}\\beta_{N-1}T^{3/2}}{\\sigma\\sqrt{t_{\\mathrm{min}}}}\\left(B+\\frac{C}{t_{\\mathrm{min}}}\\right)\\sqrt{N\\mathbb{Z}_{N}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here the first inequality follows because of Corollary 8, the second inequality follows due to the monotonicity of sequence $(\\beta_{n})_{n\\geq0}$ , the third inequality follows by Cauchy\u2013Schwarz and the last one by maximizing the term in expectation. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Our regret $R_{N}$ is sublinear if $\\beta_{N-1}\\sqrt{N\\mathbb{Z}_{N}}$ is sublinear. For general well-calibrated models this is tough to verify. However, for Gaussian process dynamics, $\\mathcal{T}_{N}$ is equal to (up to constant factors) the maximum information gain $\\gamma_{N}$ (Srinivas et al., 2009) (c.f., Curi et al. (2020, Lemma 17)). The maximum information gain is sublinear for a rich class of kernels (Vakili et al., 2021), i.e., yielding sublinear regret for OTACOS (see Sukhija et al. (2024, Theorem 2) for more detail). ", "page_idx": 17}, {"type": "text", "text": "A.2 Bounded number of transition ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We overload the notation in this section and add number of switches to the value function, such that we have $V_{\\pi_{n},\\Phi^{*}}(x_{0},T,0)=V_{\\pi_{n},\\Phi^{*}}(x_{0},T)$ ", "page_idx": 17}, {"type": "text", "text": "Lemma 10 (Per episode regret bound). We have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{V_{\\pi_{n},\\Phi^{*}}(x_{0},T,0)-V_{\\pi^{*},\\Phi^{*}}(x_{0},T,0)\\le}}\\\\ {{\\displaystyle\\le\\mathbb{E}\\left[\\sum_{k=0}^{K-1}V_{\\pi_{n},\\Phi_{n}}(x_{n,k+1},t_{n,k+1},k+1)-V_{\\pi_{n},\\Phi_{n}}(\\widehat{x}_{n,k+1},t_{n,k+1},k+1)\\right],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\widehat{\\pmb{x}}_{n,k+1}$ is the state of one step hallucinated component $\\widehat{\\pmb{s}}_{n,k+1}=\\Psi_{\\Phi_{n}}(\\pmb{s}_{n,k},\\pmb{\\pi}_{n}(\\pmb{s}_{n,k}),\\pmb{w}_{n,k})$ and $\\pmb{x}_{n,k+1}$ is the state of $\\mathbf{s}_{n,k+1}=\\Psi_{\\Phi^{*}}(s_{n,k},\\pi_{n}(s_{n,k}),w_{n,k})$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{l}_{\\pi_{n},\\Phi^{*}}(x_{0},T,0)=\\mathbb{E}\\left[\\sum_{k\\geq0}r(s_{n,k},\\pi_{n}(s_{n,k}))\\right]=\\mathbb{E}\\left[r(s_{n,0},\\pi_{n}(s_{n,0}))+\\sum_{k\\geq1}r(s_{n,k},\\pi_{n}(s_{n,k}))\\right]}\\\\ {\\displaystyle=\\mathbb{E}\\left[r(s_{n,k},\\pi_{n}(s_{n,0}))+V_{\\pi_{n},\\Phi^{*}}(x_{n,1},t_{n,1},1)\\right]}\\\\ {\\displaystyle=\\mathbb{E}\\left[r(s_{n,k},\\pi_{n}(s_{n,0}))+V_{\\pi_{n},\\Phi_{n}}(x_{n,1},t_{n,1},1)-V_{\\pi_{n},\\Phi_{n}}(x_{0},T,0)\\right]+}\\\\ {\\displaystyle+\\mathbb{E}\\left[\\nu_{\\pi_{n},\\Phi_{n}}(x_{0},T,0)-V_{\\pi_{n},\\Phi_{n}}(x_{n,1},t_{n,1},1)+V_{\\pi_{n},\\Phi^{*}}(x_{n,1},t_{n,1},1)\\right]}\\\\ {\\displaystyle=V_{\\pi_{n},\\Phi_{n}}(x_{0},T,0)+\\mathbb{E}\\left[V_{\\pi_{n},\\Phi_{n}}(\\hat{x}_{n,1},t_{n,1},1)-V_{\\pi_{n},\\Phi_{n}}(x_{n,1},t_{n,1},1)\\right]}\\\\ {\\displaystyle+\\mathbb{E}\\left[V_{\\pi_{n},\\Phi^{*}}(x_{n,1},t_{n,1},1)-V_{\\pi_{n},\\Phi_{n}}(x_{n,1},t_{n,1},1)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{I}_{\\pi_{n},\\Phi^{\\star}}(x_{0},T,0)-V_{\\pi_{n},\\Phi_{n}}(x_{0},T,0)=}\\\\ &{=\\mathbb{E}\\left[V_{\\pi_{n},\\Phi_{n}}(\\widehat{x}_{n,1},t_{n,1},1)-V_{\\pi_{n},\\Phi_{n}}(x_{n,1},t_{n,1},1)\\right]+\\mathbb{E}\\left[V_{\\pi_{n},\\Phi^{\\star}}(x_{n,1},t_{n,1},1)-V_{\\pi_{n},\\Phi_{n}}(x_{n,1},t_{n,1},1)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Repeating the step inductively the result follows and using $V_{\\pi_{n},\\Phi^{*}}({\\bf x}_{n,K},t_{n,K},K)=0$ we prove the lemma. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "A.2.1 Subgaussianity of the noise ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In principle, we could assume that the noise $\\pmb{w}_{k}$ is Gaussian and then with the same analysis obtain the regret bound. However, stochastic flows are in many cases not exactly Gaussian but only subGaussian. For such noise we need can not apply Lemma 6 and need to escort to different analysis. First we show that under mild assumptions on the SDE dynamics functions $f^{*}$ and $g^{\\ast}$ the resulting noise $\\pmb{w}_{k}$ is sub-Gaussian. ", "page_idx": 17}, {"type": "text", "text": "To derive this result we will follow the work of Djellout et al. (2004). We present the results in quite informal way, for more rigorous statements we refer the reader to Djellout et al. (2004). ", "page_idx": 17}, {"type": "text", "text": "Definition 4 (Wasserstein distance). Let $(\\mathcal{E},d_{\\mathcal{E}})$ be a metric space and let $\\mu,\\nu$ be two probability measures on $\\mathcal{E}$ . We define: ", "page_idx": 17}, {"type": "equation", "text": "$$\nW_{p}(\\mu,\\nu)=\\operatorname*{inf}_{\\gamma\\in\\Gamma(\\mu,\\nu)}\\mathbb{E}_{(x,y)\\sim\\gamma}\\left[d(x,y)^{p}\\right]^{\\frac{1}{p}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Definition 5 (Kullback\u2013Leibler divergence). Let $(\\mathcal{E},d\\varepsilon)$ be a metric space and let $\\mu,\\nu$ be two probability measures on $\\mathcal{E}$ . We define: ", "page_idx": 18}, {"type": "equation", "text": "$$\nH(\\nu||\\mu)=\\left\\{\\!\\!\\!\\begin{array}{l l}{\\mathbb{E}_{x\\sim\\nu}\\left[\\log\\left(\\frac{d\\nu(x)}{d\\mu(x)}\\right)\\right],}&{i f\\nu\\ll\\mu}\\\\ {+\\infty,}&{e l s e}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Definition 6 ( $L^{p}$ -transportation cost information inequality). Let $(\\mathcal{E},d\\varepsilon)$ be a metric space and let $\\mu$ be a probability measure on $\\mathcal{E}$ . We say that $\\mu$ satisfy the $L^{p}$ -transportation cost information inequality, and for short write $\\mu\\in T_{p}(C)$ , if there exists a constant $C$ such that for any measure $\\nu$ on $\\mathcal{E}$ we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W_{p}(\\mu,\\nu)\\leq\\sqrt{2C H(\\nu||\\mu)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We now state an important theroem of Bobkov and G\u00f6tze (1999) that we will use later. ", "page_idx": 18}, {"type": "text", "text": "Theorem 11 (From Bobkov and G\u00f6tze (1999)). Let $(\\mathcal{E},d\\varepsilon)$ be a metric space and let $\\mu$ be $a$ probability measure on $\\mathcal{E}$ . We have that $\\mu\\,\\in\\,T_{1}(C)$ if and only if for any $\\mu$ -integrable and $L_{F}$ - Lipschitz function $F:(\\mathcal{E},d_{\\mathcal{E}})\\rightarrow\\mathbb{R}$ and for any $\\lambda\\in\\mathbb R$ we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x\\sim\\mu}\\left[e^{\\lambda(F(x)-\\mathbb{E}_{x\\sim\\mu}[F(x)])}\\right]\\le e^{\\frac{\\lambda^{2}}{2}C L_{F}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, we provide a condition under which $\\Xi(x,u,t)$ is sub-Gaussian random variable for any $t\\in\\mathcal T$ . Corollary 12 (Adjusted Corollary 4.1 of Djellout et al. (2004)). Assume ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in\\mathbb{R}^{d_{x}}}\\|g^{*}(x,u)\\|_{F}\\leq A,\\quad\\|f^{*}(x,u)-f^{*}(\\widehat{x},\\widehat{u})\\|\\leq L_{f^{*}}\\left\\|(x,u)-(\\widehat{x},\\widehat{u})\\right\\|,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and denote the law of $(\\Xi(x,u,t))_{t\\in\\mathcal{T}}$ on the space $C(\\mathcal{T},\\mathbb{R}^{d_{\\mathbf{x}}})$ (space of continuous functions from $\\tau$ to $\\mathbb{R}^{d_{\\mathbf{x}}}$ ) by $\\mathbb{P}_{x}$ . Then, there exist a constant $C=C(A,L_{f^{*}},T)$ such that $\\mathbb{P}_{\\mathbf{x}}\\,\\in\\,T_{1}(C)$ on the space $C(\\mathcal{T},\\mathbb{R}^{d_{\\mathbf{x}}})$ equipped with the metric: ", "page_idx": 18}, {"type": "equation", "text": "$$\nd(\\gamma_{1},\\gamma_{2})=\\operatorname*{sup}_{t\\in[0,T]}\\|\\gamma_{1}(t)-\\gamma_{2}(t)\\|\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lets $^e$ be $\\mathsf{a}(\\mathsf{n y})$ unit vector in $\\mathbb{R}^{d_{x}}$ and define: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{e,t}:C(\\mathcal{T},\\mathbb{R}^{d_{\\mathbf{x}}})\\to\\mathbb{R}}\\\\ &{F_{e,t}:\\gamma\\mapsto\\gamma(t)^{\\top}e}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|F_{e,t}(\\gamma_{1})-F_{e,t}(\\gamma_{2})|=\\big|(\\gamma_{1}(t)-\\gamma_{2}(t))^{\\top}e\\big|}\\\\ {\\displaystyle\\leq\\|\\gamma_{1}(t)-\\gamma_{2}(t)\\|\\,\\|e\\|=\\|\\gamma_{1}(t)-\\gamma_{2}(t)\\|}\\\\ {\\displaystyle\\leq\\operatorname*{sup}_{t\\in\\mathcal{T}}\\|\\gamma_{1}(t)-\\gamma_{2}(t)\\|=d(\\gamma_{1},\\gamma_{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore for any $e,t$ the function $F_{e,t}$ is 1\u2013Lipschitz. Since we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[|F_{e,t}(\\gamma)|]=\\int_{C(\\mathcal{T},\\mathbb{R}^{d_{\\mathbf{x}}})}|\\gamma(t)|\\,d\\mathbb{P}_{x}(\\gamma)=\\mathbb{E}[|\\Xi({\\boldsymbol{x}},{\\boldsymbol{u}},t)^{\\top}{\\boldsymbol{e}}|]<\\infty\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "the function $F_{e,t}$ is also $\\mathbb{P}_{x}$ -integrable. Combining the latter observation with the Theorem 11 we obtain that for any $e\\in\\mathbb{R}^{d_{\\mathbf{x}}}$ and any $t\\in\\mathcal T$ we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\Xi(x,u,t)}\\left[e^{\\lambda(\\Xi(x,u,t)^{\\top}e-\\mathbb{E}[\\Xi(x,u,t)^{\\top}e])}\\right]=\\mathbb{E}_{\\gamma\\sim\\mathbb{P}_{x}}\\left[e^{\\lambda(F_{e,t}(\\gamma)-\\mathbb{E}_{\\gamma\\sim\\mathbb{P}_{x}}[F_{e,t}(\\gamma)])}\\right]\\le e^{\\frac{\\lambda^{2}}{2}C}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence under the assumption of Theorem 2 for Bounded number of switches setting we have that for any $t\\in\\mathcal T$ the random variable $\\Xi(x,u,t)-\\mathbb{E}\\left[\\Xi(x,u,t)\\right]\\sim\\mathrm{subG}\\left(C\\right)$ . The variance proxy $C$ depends on $A,L_{f^{*}},T$ . ", "page_idx": 18}, {"type": "text", "text": "A.2.2 Lipschitness of the expected flow $\\Phi^{*}$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To apply analysis for the case when noise $\\pmb{w}_{k}$ is any sub-Gaussian we also need to show that the dynamics function $\\Phi^{*}$ is Lipschitz. We first start with some general results. ", "page_idx": 19}, {"type": "text", "text": "Lemma 13. Let $\\pmb{f}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$ , $A\\subset[n]$ and denote $B=A^{C}$ . If we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bullet\\ \\|f(\\pmb{x}_{A},\\pmb{x}_{B})-f(\\widehat{\\pmb{x}}_{A},\\pmb{x}_{B})\\|_{2}\\leq L_{A}\\,\\|\\pmb{x}_{A}-\\widehat{\\pmb{x}}_{A}\\|_{2},}\\\\ {\\bullet\\ \\|f(\\pmb{x}_{A},\\pmb{x}_{B})-f(\\pmb{x}_{A},\\widehat{\\pmb{x}}_{B})\\|_{2}\\leq L_{B}\\,\\|\\pmb{x}_{B}-\\widehat{\\pmb{x}}_{B}\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "then $\\boldsymbol{\\textbf{\\textit{f}}}$ is $2(L_{A}+L_{B})$ Lipschitz. ", "page_idx": 19}, {"type": "text", "text": "Proof. We have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|f(x)-f(\\widehat{x})\\|_{2}=\\|f(x_{A},x_{B})-f(\\widehat{x}_{A},\\widehat{x}_{B})\\|_{2}}&{}\\\\ {=\\|f(x_{A},x_{B})-f(\\widehat{x}_{A},x_{B})+f(\\widehat{x}_{A},x_{B})-f(\\widehat{x}_{A},\\widehat{x}_{B})\\|_{2}}&{}\\\\ {\\le L_{A}\\,\\|x_{A}-\\widehat{x}_{A}\\|_{2}+L_{B}\\,\\|x_{B}-\\widehat{x}_{B}\\|_{2}}&{}\\\\ {\\le\\left(L_{A}+L_{B}\\right)\\left(\\|x_{A}-\\widehat{x}_{A}\\|_{2}+\\|x_{B}-\\widehat{x}_{B}\\|_{2}\\right)}&{}\\\\ {\\le2(L_{A}+L_{B})\\,\\bigg\\|\\bigg(\\frac{x_{A}-\\widehat{x}_{A}}{x_{B}-\\widehat{x}_{B}}\\bigg)\\bigg\\|_{2}}&{}\\\\ {=2(L_{A}+L_{B})\\,\\|x-\\widehat{x}\\|_{2}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma 14 (Lipschitzness of $\\Phi_{f^{*}}$ ). There exists a positive constant $L_{\\Phi_{f}}$ such that the flow $\\Phi_{f^{*}}$ is $L_{\\Phi_{f}}$ \u2013Lipschitz. ", "page_idx": 19}, {"type": "text", "text": "Proof. We will first prove coordinate-wise Lipschitzness. We observe: ", "page_idx": 19}, {"type": "text", "text": "1. Lipschitness in time: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|\\Phi_{f^{*}}(x,u,t)-\\Phi_{f^{*}}(x,u,\\hat{t})\\|=\\left\\|\\int_{0}^{t}\\mathbb{E}[f^{*}(x_{s},u)]d s-\\int_{0}^{\\hat{t}}\\mathbb{E}[f^{*}(x_{s},u)]d s\\right\\|}}\\\\ &{\\leq\\int_{\\hat{t}}^{t}\\mathbb{E}\\left[\\|f^{*}(x_{s},u)\\|\\right]d s\\leq F\\left|t-\\hat{t}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "2. Lipschitness in state $\\textbf{\\em x}$ : To prove this, consider the $\\delta{\\pmb x}_{t}=\\Xi({\\pmb x},{\\pmb u},t)-\\Xi(\\widehat{\\pmb x},{\\pmb u},t)$ , then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d\\delta x_{t}=(f^{*}(x_{t},u)-f^{*}(\\widehat{x}_{t},u))d t+(g^{*}(x_{t},u)-f^{*}(\\widehat{x}_{t},u))d B_{t}}\\\\ &{\\qquad=\\delta f_{t}^{*}d t+\\delta g_{t}^{*}d B_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that $\\lVert\\delta\\pmb{f}_{t}^{*}\\rVert\\leq L_{f^{*}}\\lVert\\delta\\pmb{x}_{t}\\rVert$ and $\\lVert\\delta\\pmb{g}_{t}^{*}\\rVert\\leq L_{\\pmb{g}^{*}}\\left\\lVert\\delta\\pmb{x}_{t}\\right\\rVert$ since both functions are Lipschitz. Define $\\pmb{y}_{t}=\\delta\\pmb{x}_{t}^{\\top}\\delta\\pmb{x}_{t}$ and use Ito\u2019s Lemma to get ", "page_idx": 19}, {"type": "equation", "text": "$$\nd\\pmb{y}_{t}=2\\delta\\pmb{x}_{t}^{\\top}(\\delta\\pmb{f}_{t}^{*}d t+\\delta\\pmb{g}_{t}^{*}d B_{t})+\\mathrm{tr}(\\delta\\pmb{g}_{t}^{*}(\\delta\\pmb{g}_{t}^{*})^{\\top})d t\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[y_{t}]=\\int_{0}^{t}2\\mathbb{E}[\\delta x_{s}^{\\top}\\delta f_{s}^{*}]+\\mathbb{E}[\\mathrm{tr}(\\delta g_{s}^{*}(\\delta g_{s}^{*})^{\\top})]d s}\\\\ &{\\qquad\\le\\int_{0}^{t}2\\mathbb{E}\\left[\\|\\delta x_{s}\\|\\,\\|\\delta f_{s}^{*}\\|\\right]+\\mathbb{E}[\\|\\delta g_{s}^{*}\\|^{2}]d s}\\\\ &{\\qquad\\le\\int_{0}^{t}(2L_{f^{*}}+L_{g^{*}}^{2})\\mathbb{E}[\\|\\delta x_{s}\\|^{2}]d s}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that $y_{t}=\\lVert\\delta\\mathbf{x}_{t}\\rVert^{2}$ , so we can apply Gr\u00f6nwall\u2019s inequality to get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\delta\\pmb{x}_{t}\\right\\Vert^{2}\\right]\\leq\\left\\Vert\\delta\\pmb{x}_{0}\\right\\Vert^{2}e^{(2L_{f^{\\ast}}+L_{g^{\\ast}}^{2})t}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mathbb{E}[\\delta x_{t}]\\right\\|\\leq\\sqrt{\\mathbb{E}\\left[\\left\\|\\delta x_{t}\\right\\|^{2}\\right]}\\leq\\left\\|\\delta x_{0}\\right\\|e^{\\frac{2L_{f^{*}}+L_{g^{*}}^{2}}{2}t}\\leq\\left\\|\\delta x_{0}\\right\\|e^{\\frac{2L_{f^{*}}+L_{g^{*}}^{2}}{2}T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\lVert\\Phi_{f^{*}}(x,u,t)-\\Phi_{f^{*}}(\\widehat{x},u,t)\\rVert\\leq\\lVert x-\\widehat{x}\\rVert\\,e^{\\frac{2L_{f^{*}}+L_{g^{*}}^{2}}{2}T}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "3. Lipschitness in action $\\textbf{\\em u}$ : We denote $\\delta{\\pmb x}_{t}\\,=\\,{\\Xi}({\\pmb x},{\\pmb u},t)\\,-\\,{\\Xi}({\\pmb x},\\widehat{{\\pmb u}},t)$ and $\\delta\\pmb{u}\\,=\\,\\pmb{u}\\,-\\,\\widehat{\\pmb{u}}$ Following the same steps as in the proof of Lipschitzness in state  w e arrive at: ", "page_idx": 20}, {"type": "equation", "text": "$$\nd\\pmb{y}_{t}=2\\delta\\pmb{x}_{t}^{\\top}(\\delta\\pmb{f}_{t}^{*}d t+\\delta\\pmb{g}_{t}^{*}d B_{t})+\\mathrm{tr}(\\delta\\pmb{g}_{t}^{*}(\\delta\\pmb{g}_{t}^{*})^{\\top})d t\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Integration yields: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[y_{t}]=\\int_{0}^{t}2\\mathbb{E}[\\delta x_{s}^{\\top}\\delta f_{s}^{*}]+\\mathbb{E}[\\mathrm{tr}(\\delta g_{s}^{*}(\\delta g_{s}^{*})^{\\top})]d s}\\\\ &{\\qquad\\leq\\int_{0}^{t}2\\mathbb{E}\\left[\\|\\delta x_{s}\\|\\,\\|\\delta f_{s}^{*}\\|\\right]+\\mathbb{E}[\\|\\delta g_{s}^{*}\\|^{2}]d s}\\\\ &{\\qquad\\leq\\int_{0}^{t}2\\mathbb{E}\\left[L_{f^{*}}\\,\\|\\delta x_{s}\\|\\,(\\|\\delta x_{s}\\|+\\|\\delta u\\|)\\right]+\\mathbb{E}\\left[2L_{g^{*}}^{2}\\left(\\|\\delta x_{s}\\|^{2}+\\|\\delta u\\|^{2}\\right)\\right]d s}\\\\ &{\\qquad\\leq\\int_{0}^{t}(3L_{f^{*}}+2L_{g^{*}}^{2})\\mathbb{E}\\left[y_{s}\\right]+(L_{f^{*}}+2L_{g^{*}}^{2})\\left\\|\\delta u\\right\\|d s,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we used $(a+b)^{2}\\leq2a^{2}+2b^{2}$ and $a b\\leq{\\frac{1}{2}}(a^{2}+b^{2})$ . Applying Gr\u00f6nwall\u2019s inequality results in: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\delta\\pmb{x}_{t}\\right\\|^{2}\\right]\\leq\\left\\|\\delta\\pmb{u}\\right\\|^{2}(L_{\\pmb{f}^{*}}+2L_{\\pmb{g}^{*}}^{2})e^{(3L_{\\pmb{f}^{*}}+2L_{\\pmb{g}^{*}}^{2})t}}\\\\ {\\leq\\left\\|\\delta\\pmb{u}\\right\\|^{2}(L_{\\pmb{f}^{*}}+2L_{\\pmb{g}^{*}}^{2})e^{(3L_{\\pmb{f}^{*}}+2L_{\\pmb{g}^{*}}^{2})T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Applying Lemma 13 on 2. and 3. we have that $\\Phi_{f^{*}}(\\cdot,\\cdot,t)$ is $2\\left(e^{\\frac{2L_{f^{*}}+L_{g^{*}}^{2}}{2}T}+\\sqrt{L_{f^{*}}+2L_{g^{*}}^{2}}e^{\\frac{3L_{f^{*}}+2L_{g^{*}}^{2}}{2}T}\\right)\\!.$ \u2013Lipschitz. Applying Lemma 13 on and \u03a6f \u2217(,, t) and bounding $2\\:\\:\\leq\\:\\:4$ we finally obtain that $\\Phi_{f^{*}}$ is $4\\bigg(e^{\\frac{2L_{f^{*}}+L_{g^{*}}^{2}}{2}T}+\\sqrt{L_{f^{*}}+2L_{g^{*}}^{2}}e^{\\frac{3L_{f^{*}}+2L_{g^{*}}^{2}}{2}T}+F\\bigg),$ \u2013Lipschitz. ", "page_idx": 20}, {"type": "text", "text": "Corollary 15 (Lipschitzness of the $\\Phi_{b^{*}}$ ). The cost flow $\\Phi_{b^{*}}$ is $\\mathcal{O}\\left(e^{C_{1}\\left(L_{f^{*}}+L_{g^{*}}^{2}\\right)T}\\right).$ \u2013Lipschitz, where $C_{1}$ is a constant. ", "page_idx": 20}, {"type": "text", "text": "Proof. Same as in the proof of Lemma 14 we first show coordinate-wise Lipschitzness. ", "page_idx": 20}, {"type": "text", "text": ". We first show Lipschitness in time: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\Phi_{b^{*}}(x,\\pmb{u},t)-\\Phi_{b^{*}}(\\pmb{x},\\pmb{u},\\widehat{t})\\right|=\\left|\\mathbb{E}\\left[\\int_{\\widehat{t}}^{t}b^{*}(\\pmb{x}_{s},\\pmb{u})d s\\right]\\right|}&{}\\\\ {\\qquad\\qquad\\leq\\mathbb{E}\\left[\\int_{\\widehat{t}}^{t}|b^{*}(\\pmb{x}_{s},\\pmb{u})|\\,d s\\right]}&{}\\\\ {\\qquad\\qquad\\leq\\mathbb{E}\\left[B(t-\\widehat{t})\\right]=B(t-\\widehat{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "2. To obtain Lipschitzness in state observe: ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\Phi_{b^{*}}(x,u,t)-\\Phi_{b^{*}}(\\widehat{x},u,t)|=\\left|\\mathbb{E}\\left[\\int_{0}^{t}b^{*}(\\Xi(x,u,s),u)-b^{*}(\\Xi(\\widehat{x},u,s),u)d s\\right]\\right|\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\mathbb{E}\\left[\\int_{0}^{t}\\left|b^{*}\\left(\\Xi(x,u,s),u\\right)-b^{*}\\left(\\Xi(\\widehat{x},u,s),u\\right)\\right|d s\\right]}\\\\ &{\\leq L_{b^{*}}\\mathbb{E}\\left[\\int_{0}^{t}\\left|\\mathbb{E}(x,u,s)-\\Xi(\\widehat{x},u,s)\\right|d s\\right]}\\\\ &{\\leq L_{b^{*}}\\int_{0}^{t}\\sqrt{\\mathbb{E}\\left[\\|\\Xi(x,u,s)-\\Xi(\\widehat{x},u,s)\\|^{2}\\right]}d s}\\\\ &{\\leq L_{b^{*}}\\left\\|x-\\widehat{x}\\|\\int_{0}^{t}e^{\\frac{2L_{f^{*}}+L_{g^{*}}^{2}}{2}s}d s}\\\\ &{=\\frac{2L_{b^{*}}}{2L_{f^{*}}+L_{g^{*}}^{2}}\\left(e^{\\frac{2L_{f^{*}}+L_{g^{*}}^{2}}{2}t}-1\\right)\\left\\|x-\\widehat{x}\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "3. Finally, for Lipschitzness in action observe: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi_{t}\\langle x,u,t\\rangle-\\Phi_{t}\\langle x,\\bar{u},t\\rangle=\\left|\\mathbb{E}\\left[\\int_{0}^{t}\\hat{\\nu}(\\Xi(x,u,s),u)-b^{*}(\\Xi(x,\\bar{u},s),u)d s\\right]\\right|}\\\\ &{\\leq\\mathbb{E}\\left[\\int_{0}^{t}|\\hat{\\nu}^{*}(\\Xi(x,u,s),u)-b^{*}(\\Xi(x,\\bar{u},s),u)|d s\\right]}\\\\ &{\\leq L_{\\mathrm{s}}\\mathbb{E}\\left[\\int_{0}^{t}\\left|\\mathbb{E}\\left[(x,u,s)-\\Xi(x,\\bar{u},s)\\right]+\\mathbb{I}(u-\\bar{u}|d s)\\right]}\\\\ &{\\leq L_{\\mathrm{s}}t\\cdot\\left[\\|u-\\bar{u}\\|+L_{x}\\int_{0}^{t}\\sqrt{\\mathbb{E}\\left[\\left|\\Xi(x,u,s)-\\Xi(x,\\bar{u},s)\\right|\\right]^{2}}\\right]d s}\\\\ &{\\leq L_{\\mathrm{s}}t\\left[u-\\bar{u}\\right]+L_{\\mathrm{s}}\\cdot\\left\\|u-\\bar{u}\\right\\|\\sqrt{\\int_{t}^{t}+2L_{x}^{2}}\\int_{0}^{t}e^{-\\frac{\\bar{u}(x,\\bar{u},s)^{2}}{2}s_{x}}d s}\\\\ &{=L_{\\mathrm{s}}\\cdot\\left(t+\\frac{2}{3L_{x}+2}\\sum_{l\\geq-1}^{L_{y}-2}\\left(e^{\\frac{u_{x}+s+2L_{x}^{2}}{2}t}-1\\right)\\right)\\left\\|u-\\bar{u}\\right\\|}\\\\ &{\\leq L_{\\mathrm{s}}\\cdot\\left(T+\\frac{2\\sqrt{L_{y}+2L_{y}^{2}}}{3L_{y}+2L_{y}^{2}}\\left(e^{\\frac{u_{x}+s+2L_{x}^{2}}{2}t}-1\\right)\\right)\\left\\|u-\\bar{u}\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Applying Lemma 13 result follows. ", "page_idx": 21}, {"type": "text", "text": "Corollary 16 (Lipschitzness of $\\Phi^{*}$ ). The unknown function $\\Phi^{*}$ is ${\\cal L}_{\\Phi}~=~{\\cal L}_{\\Phi_{f}}\\,+\\,{\\cal L}_{\\Phi_{b}}~=~$ $\\mathcal{O}\\Big(e^{D(L_{f^{*}}+L_{g^{*}}^{2})T}\\Big).$ \u2013Lipschitz, where $D$ is constant. ", "page_idx": 21}, {"type": "text", "text": "A.2.3 Regret bound ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma 17 (Per episode regret bound (general sub-Gaussian noise)). Consider the setting with a bounded number of switches $K$ , and let Assumption 1, 3, and Assumption 4 hold. Then, we get with probability at least $1-\\delta$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle V_{\\pi_{n},\\Phi^{*}}(x_{0},T,K)-V_{\\pi^{*},\\Phi^{*}}(x_{0},T,K)\\leq}\\\\ {\\le\\mathcal{O}\\left(L_{\\sigma}^{K-1}\\beta_{n-1}^{K}e^{C(L_{f^{*}}+L_{g^{*}}^{2})(1+L_{\\pi})T K}\\mathbb{E}\\left[\\displaystyle\\sum_{k=0}^{K}\\|\\sigma_{n-1}(x_{n,k},\\pi_{n}(x_{n,k},t_{n,k},k))\\|_{2}\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Applying Lemma 5 of Curi et al. (2020) the result follows. ", "page_idx": 21}, {"type": "text", "text": "Theorem 18. Consider the setting with a bounded number of switches $K$ , and let Assumption $I,\\,3$ , and Assumption 4 hold. Then, we get with probability at least $1-\\delta$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\nR_{N}=\\sum_{n=1}^{N}V_{\\pmb{\\pi}_{n},\\pmb{\\Phi}^{*}}(\\pmb{x}_{0},T,K)-V_{\\pmb{\\pi}^{*},\\pmb{\\Phi}^{*}}(\\pmb{x}_{0},T,K)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\leq\\mathcal{O}\\left(L_{\\sigma}^{K-1}\\beta_{N-1}^{K}\\sqrt{K}e^{C(L_{f^{*}}+L_{g^{*}}^{2})(1+L_{\\pi})T K}\\sqrt{N\\mathbb{Z}_{N}}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We apply Lemma 17 and Cauchy-Schwarz: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{R_{N}}=\\displaystyle\\sum_{n=1}^{N}{V_{\\pi_{n},\\Phi^{*}}(x_{0},T,K)-V_{\\pi^{*},\\Phi^{*}}(x_{0},T,K)}}\\\\ &{\\quad\\le\\displaystyle\\sum_{n=1}^{N}\\mathcal{O}\\left(L_{\\sigma}^{K-1}\\beta_{n-1}^{K}e^{C(L_{f^{*}}+L_{g^{*}}^{2})(1+L_{\\pi})T K}\\mathbb{E}\\left[\\sum_{k=0}^{K}\\|\\sigma_{n-1}(x_{n,k},\\pi_{n}(x_{n,k},t_{n,k},k))\\|_{2}\\right]\\right)}\\\\ &{\\quad\\le\\mathcal{O}\\left(L_{\\sigma}^{K-1}\\beta_{N-1}^{K}e^{C(L_{f^{*}}+L_{g^{*}}^{2})(1+L_{\\pi})T K}\\right)\\mathbb{E}\\left[\\sum_{n=1}^{N}\\sum_{k=0}^{K}\\|\\sigma_{n-1}(x_{n,k},\\pi_{n}(x_{n,k},t_{n,k},k))\\|_{2}\\right]}\\\\ &{\\quad\\le\\mathcal{O}\\left(L_{\\sigma}^{K-1}\\beta_{N-1}^{K}e^{C(L_{f^{*}}+L_{g^{*}}^{2})(1+L_{\\pi})T K}\\right)\\sqrt{K}\\sqrt{N T_{N}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here we first applied Lemma 17. Then we used the monotonicity of $(\\beta_{n})_{n\\geq0}$ sequence. In the last step we first applied maximum over the collected data, then Cauchy-Schwarz inequality and finally the definition of model complexity. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "B Additional Experiments ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "0JSKjdePGq/tmp/54313b2889b2bb4f122325b5a7c4f7dd29744e2ea8528c963c85eced2e1f3b56.jpg", "img_caption": ["Figure 6: Pendulum swing-down task. Row 1: 4 interactions, optimized interaction times, Row 2: 5 interactions, optimized interaction times, Row 3: 4 equidistant interactions, Row 4: 4 equidistant interactions. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "0JSKjdePGq/tmp/90256e14abe9fee49dcc63b0e83611e36c5c4d4f0769c1928263646b3f3a02e4.jpg", "img_caption": ["Figure 7: When stochasticity of the environments increases, we need more interactions at the unstable equilibrium (Pendulum on top). The stochasticity scale goes from 0.1 to 0.5 to 1.0 from top to bottom row respectively. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We highlight the problem setting, algorithm, and theoretical and empirical results in the abstract and introduction. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: In Section 5 we highlight the assumptions of our work, which also correspond to the limitations of our theoretical analysis and also the setting for which our algorithm yields theoretical guarantees. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All theoretical results are accompanied by the relevant assumptions that are listed in Section 5 and we provide all proofs in Appendix A. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide the code with all hyperparameters we used to run the experiments. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide the code with all the hyperparameters to run the experiments. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide all the experimental details either, minor part in the main paper, and the major part in the accompanying code. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: All experiments are run with 5 seeds and mean performance with standard error is reported in all our plots. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide the details of compute in the readme.txt flie as part of the enclosed code. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our research conforms, in every respect, to the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper proposes a method to improve exploration in reinforcement learning in the nonepisodic setting, and is not tied to specific applications. As such, it shares the many potential societal consequences that are associated with reinforcement learning and automation as a whole, spanning from environmental impact to concerns on ethics and alignment. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: We do not release high-risk data or models. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We cite all creators whose code we used in our experiments in Section 4. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The code is documented where applicable. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}]