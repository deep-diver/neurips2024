[{"figure_path": "QXkFC7D6p4/figures/figures_18_1.jpg", "caption": "Figure 1: Visualization of Convergence Results. We use CIFAR10 \u2192 SVHN with K = 100 as an example. The top two plots correspond to a fraction of 10% of participating clients, while the bottom two plots correspond to 100% participation. We report the training and test accuracy along with finetuned epochs for both settings. The grey dashed lines represent FedAVG, where the coefficient for the regularizer term is set to 0. Other lines represent FedGTST with tuned coefficients.", "description": "This figure visualizes the convergence behavior of FedAVG and FedGTST on the CIFAR10 to SVHN transfer task.  It shows training and testing accuracy curves over finetuning epochs for different participation rates (10% and 100% of clients) and regularization coefficients (\u03be). The grey dashed lines represent the baseline FedAVG, while colored lines show FedGTST with various tuned hyperparameters.  The results illustrate FedGTST's faster convergence and improved accuracy compared to FedAVG.", "section": "6 Experiments"}, {"figure_path": "QXkFC7D6p4/figures/figures_19_1.jpg", "caption": "Figure 2: Cross-client statistics tuning via FedGTST. We use CIFAR10\u2192SVHN with K = 100 as an example. The left plot reports the global Jacobian (gradient) norm versus the index of the federated round. The grey dashed line represents FedAVG, while other lines correspond to FedGTST with different coefficients. We truncate the plot to only capture the results of the first 100 rounds, since at the end of training the gradient norm should drop to a value close to 0 due to convergence, and we are only interested in observing the behaviour of Jacobian norms during relative early pretraining stages. We select the best-performing setup from the left plot (the red line with coefficient 1e \u2013 3), and then in the right plot, compare its variance during a federated round with that of FedAVG. The blue line represents FedAVG and the yellow line corresponds to FedGTST. The yellow line terminated earlier since all experiments are averaged over 3 runs and aligned with the run that converges the fastest.", "description": "This figure shows the cross-client averaged Jacobian norm and variance during the federated training process for FedAVG and FedGTST with different coefficients. The left plot demonstrates that FedGTST significantly increases the Jacobian norm compared to FedAVG, while the right plot illustrates that FedGTST effectively reduces the Jacobian variance.", "section": "Results"}]