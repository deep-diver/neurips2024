[{"figure_path": "QXkFC7D6p4/tables/tables_8_1.jpg", "caption": "Table 1: Target accuracy (%) of the finetuned model pretrained on a small number of clients (K = 10). FedGTST outperforms other methods across both tasks and both backbones; for the example MNIST to MNIST-M with a LeNet backbone, FedGTST outperforms FedIIR and FedSR by around 4%.", "description": "This table shows the target accuracy achieved by different federated transfer learning methods on two benchmark tasks (MNIST to MNIST-M and CIFAR-10 to SVHN) using two different backbones (LeNet and ResNet).  The results demonstrate that FedGTST significantly outperforms existing methods, particularly when using a smaller number of clients (K=10). The improvement is consistently observed across different tasks and model architectures.", "section": "6.2 Results"}, {"figure_path": "QXkFC7D6p4/tables/tables_9_1.jpg", "caption": "Table 2: Target accuracy (%) of the finetuned model pretrained on a medium number of clients (K = 50). FedGSTS outperforms other methods across both tasks and both backbones; for both the examples of MNIST\u2192MNIST-M and CIFAR10\u2192SVHN with a ResNet18 backbone, FedGTST outperforms FedIIR and FedSR by around 5%.", "description": "This table presents the target accuracy achieved by fine-tuning a model pretrained on a medium-sized federated learning setup (50 clients).  It compares the performance of FedGTST against baseline methods (FedAVG, FedSR, and FedIIR) across two transfer learning tasks (MNIST to MNIST-M and CIFAR10 to SVHN) and two different model backbones (LeNet and ResNet18).  FedGTST demonstrates a notable improvement in accuracy compared to the baselines.", "section": "6.2 Results"}, {"figure_path": "QXkFC7D6p4/tables/tables_9_2.jpg", "caption": "Table 3: Target accuracy (%) of the finetuned model pretrained on a large number of clients (K = 100). FedGTST outperforms other methods across both tasks and for both backbones; for CIFAR10\u2192SVHN with a ResNet18 and LeNet backbone, FedGTST outperforms FedIIR and FedSR by 7%.", "description": "This table shows the target accuracy of a model fine-tuned on a target domain after being pre-trained using federated learning on a large number of clients (K=100). The results are compared across two tasks (MNIST to MNIST-M and CIFAR10 to SVHN) and two backbone architectures (LeNet and ResNet18).  FedGTST consistently outperforms other methods (FedAVG, FedSR, FedIIR). The improvement is particularly significant for the CIFAR10 to SVHN task, where FedGTST surpasses FedIIR and FedSR by approximately 7%.", "section": "6.2 Results"}, {"figure_path": "QXkFC7D6p4/tables/tables_17_1.jpg", "caption": "Table 4: Transferability versus the fraction of participating clients in each round. We report the results for CIFAR10 \u2192 SVHN on the LeNet backbone, with K = 100.", "description": "This table shows how the transferability (measured by the target accuracy) changes when varying the percentage of clients participating in each round of federated learning.  The experiment uses the CIFAR-10 to SVHN transfer task with the LeNet backbone and a total of 100 clients (K=100).  The results indicate that increasing client participation generally improves transferability, although even with only 10% participation, performance is relatively close to that of 100% participation.", "section": "6.2 Results"}, {"figure_path": "QXkFC7D6p4/tables/tables_17_2.jpg", "caption": "Table 5: Transferability v.s. local number of epochs. We report results for CIFAR10\u2192SVHN with the LeNet backbone, for 10% of participating clients and K = 100.", "description": "This table compares the transferability performance (target accuracy) of FedAVG and FedGTST with different coefficient values when using 1 local epoch versus 10 local epochs for the CIFAR10 to SVHN transfer task.  The results are based on the LeNet backbone and with 10% of clients participating in each round. The table shows how the number of local epochs and the coefficient value impact the transferability of the models. ", "section": "6.2 Results"}, {"figure_path": "QXkFC7D6p4/tables/tables_18_1.jpg", "caption": "Table 6: Comparison of different federated models on intra-domain transfer tasks of DomainNet. FedGTST consistently outperforms the other methods.", "description": "This table presents the results of the leave-one-out domain transfer experiments on the DomainNet dataset.  The performance of three federated learning methods (FedAVG, FedSR, and FedGTST) is compared across six distinct domains (C, I, P, Q, R, and S).  Each row represents a different method, while each column represents a target domain.  The \"Average\" column shows the average performance across all six target domains.  The table highlights that FedGTST consistently outperforms both FedAVG and FedSR in terms of accuracy.", "section": "6 Experiments"}, {"figure_path": "QXkFC7D6p4/tables/tables_18_2.jpg", "caption": "Table 1: Target accuracy (%) of the finetuned model pretrained on a small number of clients (K = 10). FedGTST outperforms other methods across both tasks and both backbones; for the example MNIST to MNIST-M with a LeNet backbone, FedGTST outperforms FedIIR and FedSR by around 4%.", "description": "This table shows the target accuracy of a model fine-tuned on a target dataset after being pre-trained using federated learning with 10 clients. The results are presented for two different datasets (MNIST to MNIST-M and CIFAR10 to SVHN) and two different model backbones (LeNet and ResNet).  The table highlights that the proposed FedGTST method outperforms existing methods (FedAVG, FedSR, and FedIIR).", "section": "6.2 Results"}, {"figure_path": "QXkFC7D6p4/tables/tables_19_1.jpg", "caption": "Table 3: Target accuracy (%) of the finetuned model pretrained on a large number of clients (K = 100). FedGTST outperforms other methods across both tasks and for both backbones; for CIFAR10\u2192SVHN with a ResNet18 and LeNet backbone, FedGTST outperforms FedIIR and FedSR by 7%.", "description": "This table presents the target accuracy results for the finetuned model pretrained with different methods using 100 clients.  FedGTST shows superior performance compared to other methods (FedAVG, FedSR, FedIIR) across both MNIST to MNIST-M and CIFAR10 to SVHN transfer tasks, with both LeNet and ResNet18 backbones.", "section": "6.2 Results"}, {"figure_path": "QXkFC7D6p4/tables/tables_20_1.jpg", "caption": "Table 1: Target accuracy (%) of the finetuned model pretrained on a small number of clients (K = 10). FedGTST outperforms other methods across both tasks and both backbones; for the example MNIST to MNIST-M with a LeNet backbone, FedGTST outperforms FedIIR and FedSR by around 4%.", "description": "This table presents the target accuracy achieved by four different federated transfer learning methods (FedAVG, FedSR, FedIIR, and FedGTST) when pretrained on a small number of clients (K=10).  The results are shown for two different transfer tasks (MNIST to MNIST-M and CIFAR10 to SVHN) and two different backbones (LeNet and ResNet).  The table highlights that FedGTST consistently outperforms the other methods across all scenarios.", "section": "6.2 Results"}, {"figure_path": "QXkFC7D6p4/tables/tables_20_2.jpg", "caption": "Table 3: Target accuracy (%) of the finetuned model pretrained on a large number of clients (K = 100). FedGTST outperforms other methods across both tasks and for both backbones; for CIFAR10\u2192SVHN with a ResNet18 and LeNet backbone, FedGTST outperforms FedIIR and FedSR by 7%.", "description": "This table presents the target accuracy results achieved by four different federated learning methods (FedAVG, FedSR, FedIIR, and FedGTST) when the number of clients is large (K=100). The results are broken down by the transfer task (MNIST to MNIST-M and CIFAR10 to SVHN) and the backbone architecture (LeNet and ResNet18) used for the model. The table shows that FedGTST outperforms other baselines across all settings, particularly in the CIFAR10 to SVHN transfer tasks.", "section": "6.2 Results"}, {"figure_path": "QXkFC7D6p4/tables/tables_21_1.jpg", "caption": "Table 1: Target accuracy (%) of the finetuned model pretrained on a small number of clients (K = 10). FedGTST outperforms other methods across both tasks and both backbones; for the example MNIST to MNIST-M with a LeNet backbone, FedGTST outperforms FedIIR and FedSR by around 4%.", "description": "This table presents the target accuracy achieved by different federated learning methods (FedAVG, FedSR, FedIIR, and FedGTST) when fine-tuning a model pretrained on a small number of clients (K=10) on two different transfer tasks: MNIST to MNIST-M and CIFAR-10 to SVHN.  Two different backbones, LeNet and ResNet, were used. The results show that FedGTST consistently outperforms the baselines across both tasks and backbones, with a noticeable improvement over FedIIR and FedSR on the MNIST to MNIST-M task with the LeNet backbone.", "section": "6.2 Results"}]