[{"figure_path": "cjM2bhLOiC/tables/tables_5_1.jpg", "caption": "Table 1: WRN-28-10/ResNet-56 on CIFAR-10/100.", "description": "This table presents the classification accuracy results on CIFAR-10 and CIFAR-100 datasets using WideResNet-28-10 and ResNet-56 models.  It compares the performance of standard SGD and SAM optimizers with their IRE-enhanced counterparts (SGD-IRE and SAM-IRE).  The values show the improvement in accuracy achieved by IRE for each model and dataset.", "section": "4.1.2 IRE can consistently improve generalization"}, {"figure_path": "cjM2bhLOiC/tables/tables_5_2.jpg", "caption": "Table 2. ResNet-50 on ImageNet.", "description": "This table presents the results of training a ResNet-50 model on the ImageNet dataset using different optimization methods.  The table compares the Top-1 and Top-5 accuracy achieved by standard SGD and SAM optimizers, against their enhanced versions using the Implicit Regularization Enhancement (IRE) framework proposed in the paper.  The numbers in parentheses show the improvement in accuracy resulting from the use of IRE.  The results demonstrate that IRE consistently leads to improvements in accuracy for both SGD and SAM. ", "section": "4.1 Image classification"}, {"figure_path": "cjM2bhLOiC/tables/tables_5_3.jpg", "caption": "Table 3: ViT-T/S on CIFAR-100.", "description": "The table presents the results of image classification experiments using Vision Transformers (ViT-T and ViT-S) on the CIFAR-100 dataset.  The performance of AdamW, AdmIRE (AdamW with IRE), SAM (Sharpness-Aware Minimization), and SAM-IRE (SAM with IRE) are compared, showcasing the improvement in accuracy achieved by incorporating the IRE framework.  The numbers in parentheses indicate the increase in accuracy compared to the baseline optimizer (AdamW or SAM).", "section": "4.1 Image classification"}, {"figure_path": "cjM2bhLOiC/tables/tables_6_1.jpg", "caption": "Table 4: ViT-S on ImageNet.", "description": "This table presents the results of training a Vision Transformer (ViT-S) model on the ImageNet dataset using AdamW and AdmIRE (a variant of AdamW that incorporates the Implicit Regularization Enhancement (IRE) framework). The table shows that AdmIRE achieves a higher top-1 and top-5 accuracy compared to AdamW, demonstrating the effectiveness of the IRE framework in improving the generalization performance of the model.", "section": "4.1 Image classification"}, {"figure_path": "cjM2bhLOiC/tables/tables_6_2.jpg", "caption": "Table 5: Wall-clock time on 1 A800.", "description": "This table presents the wall-clock time per step for both AdamW and AdmIRE on a single A800 GPU.  It demonstrates the computational efficiency of AdmIRE, showing that its per-step time is only slightly higher than AdamW's.", "section": "4.2.1 Computational efficiency and hyperparameter robustness"}, {"figure_path": "cjM2bhLOiC/tables/tables_7_1.jpg", "caption": "Table 6: Comparison of the sharpness of the solutions found by AdamW/AdmIRE.", "description": "This table compares the sharpness of the solutions obtained using AdamW and AdmIRE after training a Llama (60M) model on the Wikitext-103 dataset.  Sharpness is measured by the trace of the Hessian (Tr(\u2207\u00b2L(\u03b8))).  The results show that AdmIRE achieves a comparable final loss in half the number of training steps, and the resulting solution exhibits significantly lower sharpness.", "section": "4.2 Large language model pre-training"}, {"figure_path": "cjM2bhLOiC/tables/tables_8_1.jpg", "caption": "Table 7: Comparison of the implicit regularization strength of SAMs w/o IRE.", "description": "This table compares the effective learning rate (LR) of different algorithms in minimizing the trace of the Hessian, a measure of sharpness.  It shows that IRE (Implicit Regularization Enhancement) significantly increases the effective LR for both standard and average SAM (Sharpness-Aware Minimization), thereby accelerating the convergence towards flatter minima, which improves generalization.", "section": "5 Theoretical Guarantees for IRE on SAMS"}, {"figure_path": "cjM2bhLOiC/tables/tables_18_1.jpg", "caption": "Table 1: WRN-28-10/ResNet-56 on CIFAR-10/100.", "description": "This table shows the classification accuracy results on CIFAR-10 and CIFAR-100 datasets using two different Convolutional Neural Networks (CNNs): WideResNet-28-10 and ResNet-56.  The results are presented for four different training methods: SGD (standard stochastic gradient descent), SGD-IRE (SGD with Implicit Regularization Enhancement), SAM (Sharpness-Aware Minimization), and SAM-IRE (SAM with Implicit Regularization Enhancement). The numbers in parentheses indicate the improvement in accuracy achieved by IRE compared to the baseline methods (SGD and SAM).", "section": "4.1.2 IRE can consistently improve generalization"}]