[{"figure_path": "cjM2bhLOiC/figures/figures_3_1.jpg", "caption": "Figure 1: A 2-d example of (1): L(u, v) = (1 + u\u00b2)v\u00b2/2. The gray arrows denote to the minima manifold M = {(u, v) : v = 0}, where the smaller the u, the flatter the minimizer. The red marker highlights the flattest minimizer (0, 0). (a) The dynamics of GD (\u03b7 = 1), which moves slowly towards flatter minima as it converges. (b) The dynamics of GD (\u03b7 = 2), which diverges due to the excessively large \u03b7. (c) The behavior of our IRE approach with varying \u03ba'\u03c2 v.s. GD (\u03b7 = 1). Is is shown that IRE can significantly accelerate the ut's dynamics, almost reaching the flattest minimum (0,0) when taking a very large \u03ba.", "description": "This figure shows a 2D illustration of the sharpness reduction process.  Panel (a) demonstrates the slow convergence of Gradient Descent (GD) with a small learning rate (\u03b7=1) towards a flat minimum. Panel (b) shows that a larger learning rate (\u03b7=2) leads to divergence. Panel (c) illustrates the proposed Implicit Regularization Enhancement (IRE) method, which significantly accelerates the convergence to the flat minimum by selectively increasing the learning rate along flat directions while maintaining stability along sharp directions.", "section": "2 An Illustrative Example Motivating IRE"}, {"figure_path": "cjM2bhLOiC/figures/figures_4_1.jpg", "caption": "Figure 1: A 2-d example of (1): L(u, v) = (1 + u\u00b2)v\u00b2/2. The gray arrows denote to the minima manifold M = {(u, v) : v = 0}, where the smaller the u, the flatter the minimizer. The red marker highlights the flattest minimizer (0, 0). (a) The dynamics of GD (\u03b7 = 1), which moves slowly towards flatter minima as it converges. (b) The dynamics of GD (\u03b7 = 2), which diverges due to the excessively large n. (c) The behavior of our IRE approach with varying \u03ba's v.s. GD (\u03b7 = 1). Is is shown that IRE can significantly accelerate the ut's dynamics, almost reaching the flattest minimum (0,0) when taking a very large \u03ba.", "description": "This figure shows a 2D illustration of the loss landscape with flat and sharp directions.  Panel (a) demonstrates the slow convergence of Gradient Descent (GD) with a small learning rate towards a flat minimum.  Panel (b) shows that increasing the learning rate too much causes divergence. Panel (c) illustrates the proposed Implicit Regularization Enhancement (IRE) method, which selectively accelerates convergence along flat directions without impacting stability along sharp directions, leading to significantly faster convergence to the flattest minimum.", "section": "2 An Illustrative Example Motivating IRE"}, {"figure_path": "cjM2bhLOiC/figures/figures_5_1.jpg", "caption": "Figure 2: Training WRN-16-8 on CIFAR-10 by SAM-IRE with varying \u03b3, \u03ba. Particularly, the case of \u03ba = 0 corresponds to the standard SAM.", "description": "This figure presents the results of training a WideResNet-16-8 model on the CIFAR-10 dataset using the Sharpness-Aware Minimization with Implicit Regularization Enhancement (SAM-IRE) method.  It shows how varying the hyperparameters  \u03ba (enhancement strength) and \u03b3 (proportion of flat directions considered) affects the sharpness (trace of the Hessian), training loss, and test accuracy.  The heatmaps visually represent the performance across different combinations of \u03ba and \u03b3, with constant and decayed learning rates.  The results demonstrate that SAM-IRE consistently achieves flatter minima (lower sharpness), lower training loss, and higher test accuracy compared to the standard SAM (\u03ba = 0).", "section": "4.1 Image classification"}, {"figure_path": "cjM2bhLOiC/figures/figures_6_1.jpg", "caption": "Figure 3: Transformer on wikitext-2.", "description": "This figure shows the training loss curves for AdamW and AdmIRE with various hyperparameter settings (\u03ba and \u03b3) when training a 2-layer decoder-only transformer model on the Wikitext-2 dataset.  The x-axis represents the number of training steps, and the y-axis represents the training loss. The plot demonstrates that AdmIRE consistently achieves a faster convergence rate than AdamW, with the best configuration resulting in a 5.4x speedup. The different colored lines represent different AdmIRE configurations.", "section": "4.2.1 Computational efficiency and hyperparameter robustness"}, {"figure_path": "cjM2bhLOiC/figures/figures_7_1.jpg", "caption": "Figure 4: AdmIRE outperforms AdamW in the pre-training of Llama models.", "description": "This figure shows the validation loss curves for training three different sizes of Llama language models (60M, 119M, and 229M parameters) on three different datasets (wikitext-103, minipile, and openwebtext).  The figure compares the performance of AdamW (a widely used optimizer for large language models) against AdmIRE (the proposed Implicit Regularization Enhancement framework combined with AdamW).  For all three model sizes and datasets, AdmIRE demonstrates faster convergence towards lower validation loss than AdamW, achieving approximately a 2x speedup in terms of the number of training steps required. The figure highlights the consistent improvement of AdmIRE over AdamW across different model sizes and datasets.", "section": "4.2 Experiments on Llama models"}, {"figure_path": "cjM2bhLOiC/figures/figures_8_1.jpg", "caption": "Figure 1: A 2-d example of (1): L(u, v) = (1 + u\u00b2)v\u00b2/2. The gray arrows denote to the minima manifold M = {(u, v) : v = 0}, where the smaller the u, the flatter the minimizer. The red marker highlights the flattest minimizer (0, 0). (a) The dynamics of GD (\u03b7 = 1), which moves slowly towards flatter minima as it converges. (b) The dynamics of GD (\u03b7 = 2), which diverges due to the excessively large \u03b7. (c) The behavior of our IRE approach with varying \u03ba's v.s. GD (\u03b7 = 1). Is is shown that IRE can significantly accelerate the ut's dynamics, almost reaching the flattest minimum (0,0) when taking a very large \u03ba.", "description": "This figure shows a 2D illustration of the optimization problem (1) where the goal is to minimize L(u,v) = (1+u^2)v^2/2.  The gray arrows indicate the minima manifold M = {(u,v):v=0}, with flatter minima at smaller values of u. The red dot represents the flattest minimum at (0,0). Subfigure (a) demonstrates the slow convergence of gradient descent (GD) with a learning rate \u03b7=1 towards flatter minima. Subfigure (b) shows that a larger learning rate (\u03b7=2) leads to divergence. Subfigure (c) illustrates how the proposed Implicit Regularization Enhancement (IRE) method significantly accelerates convergence towards the flattest minimum by boosting the dynamics along flat directions, while maintaining stability in sharp directions.", "section": "An Illustrative Example Motivating IRE"}, {"figure_path": "cjM2bhLOiC/figures/figures_19_1.jpg", "caption": "Figure 5: The results for tuning lr_max in AdamW.", "description": "This figure presents the results of tuning the maximum learning rate (lr_max) in the AdamW optimizer.  Two sub-figures are shown. The left sub-figure displays the training loss curves for different lr_max values on a 2-layer Transformer model trained on the wikitext-2 dataset. The right sub-figure shows the validation loss curves for varying lr_max values on a Llama (60M) model trained on the wikitext-103 dataset. These plots illustrate how different learning rates affect the convergence and performance during the training process, aiding in the selection of an optimal lr_max for both model types.", "section": "4.2.1 Computational efficiency and hyperparameter robustness"}]