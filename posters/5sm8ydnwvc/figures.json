[{"figure_path": "5sm8YDnWvC/figures/figures_1_1.jpg", "caption": "Figure 1: Emerging patterns when attending both ways. (a) Input image. (b) depicts the areas of the image that 4 different latents attend to, while (c) inversely shows which image regions attend to these latents (transformed into the same coordinate system for ease of interpretation). (d) displays which areas & latents are symmetrically attended to using our proposed bi-directional cross-attention.", "description": "The figure shows how different attention mechanisms focus on different parts of an image.  (a) shows the input image. (b) demonstrates the areas that four different latent vectors attend to, while (c) shows which image areas attend to each of these latents. Finally, (d) presents the symmetric attention pattern resulting from the authors' proposed bidirectional cross-attention. This illustrates the more efficient attention that arises by considering a more balanced approach between the latents and input tokens.", "section": "1 Introduction"}, {"figure_path": "5sm8YDnWvC/figures/figures_3_1.jpg", "caption": "Figure 2: BiXT architecture. (left) Input data passing through one layer of our Bi-Directional Cross-Attention Transformer. (right) Internal structure of proposed efficient bi-directional cross-attention.", "description": "This figure illustrates the BiXT architecture, showing how input data is processed through a layer of the Bi-Directional Cross-Attention Transformer. The left panel displays the overall architecture, showing the flow of data through the latent self-attention, bi-directional cross-attention, and optional token refinement modules.  The right panel provides a detailed view of the efficient bi-directional cross-attention module, highlighting how latent vectors and input tokens attend to each other simultaneously, resulting in a reduction in computational cost.", "section": "2.3 BiXT - Bi-Directional Cross-Attention Transformers"}, {"figure_path": "5sm8YDnWvC/figures/figures_8_1.jpg", "caption": "Figure 3: Scaling trends. Ablating the influence of embedding dimension, varying numbers of latents and sequence lengths for ImageNet1K classification. All models trained with shorter schedule (only 300 epochs) to save computational resources, and comparisons should therefore be performed relative to each other. Red star-markers correspond to BiXT-Ti/16 (Acc. 80.1) from Table 2. Validation accuracy represented through solid lines, while dashed lines indicate the computational resources.", "description": "This figure shows how the validation accuracy of the BiXT model changes when varying the embedding dimension and the number of latents.  It also shows the effect of changing the input sequence length on both accuracy and computational resources (GFLOPs).  The results indicate that increasing embedding dimension and the number of latents leads to higher accuracy but also higher computational cost.  The relationship between input sequence length and computational resources is linear.", "section": "Scaling Trends \u2013 Number of Latents & Dimensions"}, {"figure_path": "5sm8YDnWvC/figures/figures_14_1.jpg", "caption": "Figure 2: BiXT architecture. (left) Input data passing through one layer of our Bi-Directional Cross-Attention Transformer. (right) Internal structure of proposed efficient bi-directional cross-attention.", "description": "This figure shows the BiXT architecture, illustrating how input data is processed through a layer of the Bi-Directional Cross-Attention Transformer.  The left side displays the overall architecture, showing how latent vectors and input tokens are processed through self-attention and cross-attention modules. The right side details the internal workings of the efficient bi-directional cross-attention module, illustrating how latent vectors and tokens simultaneously attend to and refine each other, enabling efficient information exchange.  The approximate symmetry in the attention patterns between latents and tokens allows computation of the attention matrix only once. ", "section": "2.3 BiXT - Bi-Directional Cross-Attention Transformers"}, {"figure_path": "5sm8YDnWvC/figures/figures_15_1.jpg", "caption": "Figure 2: BiXT architecture. (left) Input data passing through one layer of our Bi-Directional Cross-Attention Transformer. (right) Internal structure of proposed efficient bi-directional cross-attention.", "description": "This figure illustrates the BiXT architecture, showing how input data (images, point clouds, or text) is processed through a layer of the Bi-Directional Cross-Attention Transformer.  The left panel shows the overall architecture:  Input data is tokenized, then passed through a bi-directional cross-attention module where latent vectors and input tokens interact simultaneously. The latents are further refined through self-attention.  Optionally, the tokens can also undergo additional refinement. The right panel provides a detailed view of the efficient bi-directional cross-attention module, illustrating how computations are performed to refine both latents and tokens efficiently using a shared attention matrix.", "section": "2.3 BiXT - Bi-Directional Cross-Attention Transformers"}, {"figure_path": "5sm8YDnWvC/figures/figures_16_1.jpg", "caption": "Figure 2: BiXT architecture. (left) Input data passing through one layer of our Bi-Directional Cross-Attention Transformer. (right) Internal structure of proposed efficient bi-directional cross-attention.", "description": "The figure illustrates the BiXT architecture, showing how input data is processed through a layer of the bi-directional cross-attention transformer.  The left side shows the overall architecture, while the right side zooms in on the efficient bi-directional cross-attention module. This module is designed to enable simultaneous refinement of both input tokens ('where') and latent vectors ('what') by computing a single attention matrix, leveraging the naturally emerging symmetry between them. This improves efficiency compared to methods using sequential, one-sided cross-attention.", "section": "2.3 BiXT - Bi-Directional Cross-Attention Transformers"}, {"figure_path": "5sm8YDnWvC/figures/figures_24_1.jpg", "caption": "Figure 1: Emerging patterns when attending both ways. (a) Input image. (b) depicts the areas of the image that 4 different latents attend to, while (c) inversely shows which image regions attend to these latents (transformed into the same coordinate system for ease of interpretation). (d) displays which areas & latents are symmetrically attended to using our proposed bi-directional cross-attention.", "description": "This figure shows how the attention mechanism works in different scenarios. (a) shows the input image. (b) and (c) demonstrate the attention patterns in a uni-directional setting, where latent variables attend to input tokens and vice versa, respectively. (d) presents the attention mechanism in the proposed bi-directional model. The symmetry between latent variables and input tokens is highlighted, demonstrating the efficiency of the model.", "section": "1 Introduction"}, {"figure_path": "5sm8YDnWvC/figures/figures_25_1.jpg", "caption": "Figure 1: Emerging patterns when attending both ways. (a) Input image. (b) depicts the areas of the image that 4 different latents attend to, while (c) inversely shows which image regions attend to these latents (transformed into the same coordinate system for ease of interpretation). (d) displays which areas & latents are symmetrically attended to using our proposed bi-directional cross-attention.", "description": "This figure shows the attention patterns when using different attention mechanisms in the proposed architecture. The input image is shown in (a). (b) and (c) show the attention patterns using a uni-directional attention mechanism, while (d) shows the attention patterns using the proposed bi-directional cross-attention mechanism. The bi-directional cross-attention mechanism is more efficient and achieves better performance than the uni-directional attention mechanism. The symmetric attention patterns naturally emerge between latents and tokens. ", "section": "1 Introduction"}, {"figure_path": "5sm8YDnWvC/figures/figures_26_1.jpg", "caption": "Figure 1: Emerging patterns when attending both ways. (a) Input image. (b) depicts the areas of the image that 4 different latents attend to, while (c) inversely shows which image regions attend to these latents (transformed into the same coordinate system for ease of interpretation). (d) displays which areas & latents are symmetrically attended to using our proposed bi-directional cross-attention.", "description": "This figure shows how the attention mechanism works in different approaches. (a) shows the input image. (b) and (c) demonstrate a unidirectional approach where the latent variables attend to specific image areas, and vice versa. (d) illustrates the proposed bidirectional cross-attention, highlighting the symmetrical attention patterns between latents and image areas.", "section": "1 Introduction"}]