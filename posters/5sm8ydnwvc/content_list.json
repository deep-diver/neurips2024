[{"type": "text", "text": "Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Markus Hiller, Krista A. Ehinger, and Tom Drummond School of Computing and Information Systems The University of Melbourne, Australia m.hiller@unimelb.edu.au ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We present a novel bi-directional Transformer architecture (BiXT) which scales linearly with input size in terms of computational cost and memory consumption, but does not suffer the drop in performance or limitation to only one input modality seen with other efficient Transformer-based approaches. BiXT is inspired by the Perceiver architectures but replaces iterative attention with an efficient bidirectional cross-attention module in which input tokens and latent variables attend to each other simultaneously, leveraging a naturally emerging attention-symmetry between the two. This approach unlocks a key bottleneck experienced by Perceiverlike architectures and enables the processing and interpretation of both semantics (\u2018what\u2019) and location (\u2018where\u2019) to develop alongside each other over multiple layers \u2013 allowing its direct application to dense and instance-based tasks alike. By combining efficiency with the generality and performance of a full Transformer architecture, BiXT can process longer sequences like point clouds, text or images at higher feature resolutions and achieves competitive performance across a range of tasks like point cloud part segmentation, semantic image segmentation, image classification, hierarchical sequence modeling and document retrieval. Our experiments demonstrate that BiXT models outperform larger competitors by leveraging longer sequences more efficiently on vision tasks like classification and segmentation, and perform on par with full Transformer variants on sequence modeling and document retrieval \u2013 but require $28\\%$ fewer FLOPs and are up to $8.4\\times$ faster. 1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Much of the data we obtain when perceiving our environment can be interpreted via a division into \u2018what\u2019 and \u2018where\u2019. If we consider for example the image pictured in Figure 1 on the left, we can easily describe its content by \u2018what\u2019 we see \u2013 the building, sky and a flag. If we were to draw conclusions on a more fine-grained level though, we would likely include more specific descriptions like \u201clower left corner\u201d referring to their positions within the image \u2013 the \u2018where\u2019. In other words, \u2018where\u2019 denotes the actual geometric location of the individual elements (e.g. pixels) and \u2018what\u2019 the semantic entities (e.g. objects) that collectively describe the data as a whole. Note that this similarly applies to many other modalities, like point clouds or even language where we form words via letters that together have a certain meaning. ", "page_idx": 0}, {"type": "text", "text": "Thanks to the few structural constraints placed on the input data paired with high performance, Transformers [44] have shown great capabilities in extracting both \u2019what\u2019 and \u2019where\u2019 for a range of input modalities, giving rise to significant advances across various fields such as Natural Language Processing [9] and Computer Vision [10, 41, 42]. However, their success comes at the high cost of scaling quadratically in memory and time with the input length, practically prohibiting their use on larger input data like point clouds, long documents, or high-resolution images when computational resources are limited. ", "page_idx": 0}, {"type": "image", "img_path": "5sm8YDnWvC/tmp/e2d830b51975e4c8c233b2bdfa17aedf4127710d51a35c596579c8b4db4f264c.jpg", "img_caption": ["Figure 1: Emerging patterns when attending both ways. (a) Input image. (b) depicts the areas of the image that 4 different latents attend to, while (c) inversely shows which image regions attend to these latents (transformed into the same coordinate system for ease of interpretation). (d) displays which areas & latents are symmetrically attended to using our proposed bi-directional cross-attention. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Several approaches have since been proposed to increase their efficiency, either by changing how the computationally expensive self-attention operation is realized [37, 45] or by exploiting the domainspecific structure of their data input [17, 29, 34, 43]. However, all these either face a reduction in the Transformer\u2019s performance or limit its application to only one specific type of input [11]. ", "page_idx": 1}, {"type": "text", "text": "In an attempt to preserve the generality by not imposing additional constraints on the input data, Jaegle et al. [18] employ a small set of latent vectors as a bottleneck to extract the \u2018what\u2019 via one-sided (iterative) cross-attention \u2013 and require an additional decoder to draw conclusions about \u2018where\u2019 [19]. While achieving linear complexity w.r.t. the input length, these \u2018Perceiver\u2019 architectures require between 360 - 707 GFLOPs to achieve around $\\bar{7}8\\%$ accuracy on ImageNet1K \u2013 results that recent Transformer variants like ViT [41, 42] are able to obtain at a fraction of the compute. One possible explanation for this discrepancy is that the effective working memory of Perceiver architectures is strictly limited to the latents which therefore need to compensate via increased computation, whereas conventional Transformers like ViTs leverage the (larger) number of tokens across several layers. This raises an important question: Are the appealing individual properties of these two methods mutually exclusive, or can we in fact have the best of both worlds? ", "page_idx": 1}, {"type": "text", "text": "In this paper, we set out to affirm the latter. We demonstrate that a small set of latent vectors appropriately combined with layerwise simultaneous refinement of both input tokens and latents makes it possible to pair the high performance and architectural simplicity of Transformers with the linear scaling of Perceivers \u2013 outperforming both in settings where compute is limited. We start off by investigating a na\u00efve approach: sequentially applying cross-attention to refine \u2018what\u2019 and \u2018where\u2019, one after the other. We discover that approximately symmetric attention patterns naturally emerge between latents and tokens even when both are provided with complete flexibility. In other words, for most latents (\u2018what\u2019) that pay attention to particular tokens (\u2018where\u2019), these tokens in turn pay attention to exactly these latents (see Figure 1 and Section 3.1). Not only does this intuitively make sense \u2013 objects need to know \u2018where\u2019 they are located in the image, and image locations need to know \u2018what\u2019 objects are located there \u2013 it more importantly offers us a unique opportunity to save FLOPs, memory and parameters. ", "page_idx": 1}, {"type": "text", "text": "As we will demonstrate in Section 2, this approximate symmetry means we only need to compute the attention matrix once, reducing the involved parameters by $\\sim1/3$ to facilitate an efficient bidirectional information exchange via our proposed bi-directional cross-attention. Integrated into our bi-directional cross-attention Transformer architecture (BiXT), this forms a flexible and highperforming yet efficient way to process different input modalities like images, point clouds or text on a variety of instance-based (e.g. classification) or dense tasks (e.g. segmentation) \u2013 all while scaling linearly w.r.t. the input length. ", "page_idx": 1}, {"type": "text", "text": "In summary, our main contributions include the following: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "1. We introduce a novel bi-directional cross-attention Transformer architecture $(B i X T)$ that scales linearly with the input size in terms of computational cost and memory consumption, allowing us to process longer sequences like point clouds, text or images at higher resolution. ", "page_idx": 1}, {"type": "text", "text": "2. We propose bi-directional cross-attention as an efficient way to establish information exchange that requires computation of the attention matrix only once and reduces the involved parameters by $\\sim\\!1/3$ , motivated by a naturally emerging symmetry in cross-attention and showing significant improvements over uni-directional iterative methods like Perceiver.   \n3. We analyze BiXT\u2019s advantage of processing longer sequences across a number of tasks using different input modalities and output structures in settings with limited computational resources \u2013 with our tiny 15M parameter model achieving accuracies up to $8\\bar{3}.1\\%$ for classification on ImageNet1K without any modality-specific internal components, performing competitively for semantic image and point cloud part segmentation even among modality-specific approaches, and being up to $28\\%$ more efficient and $8.4\\times$ faster on LRA.   \n4. We further provide insights into BiXT\u2019s extendibility: Thanks to its simple and flexible design, modality-specific components can easily be incorporated in a plug-and-play fashion should the need arise \u2013 further improving results while trading off generality. ", "page_idx": 2}, {"type": "text", "text": "2 Perceiving via Bi-Directional Cross-Attention ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We start this section by briefly revisiting the concept of attention before moving on to presenting our proposed bi-directional cross-attention methodology, followed by its use within our BiXT architecture (Figure 2). Please note that we define the concepts using single-head attention for brevity instead of the actually employed multi-head attention (MHA), and all methods directly generalize to MHA. ", "page_idx": 2}, {"type": "text", "text": "2.1 Background: The Attention Mechanism ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "While self-attention has recently gained great popularity through its use in the Transformer architecture [44], we will start from a slightly more general point of view: Given a source sequence $\\mathcal{S}\\in\\mathbb{R}^{\\bar{N}\\times D_{\\mathcal{S}}}$ and a target sequence $\\mathcal{T}\\overset{\\bullet}{\\in}\\mathbb{R}^{M\\times\\breve{D_{\\mathcal{T}}}}$ , attention aims to refine $\\tau$ by exhaustively discovering pairwise correlations between all elements of both sequences and integrating information from the source components of interest into the target. ", "page_idx": 2}, {"type": "text", "text": "Formally, $\\boldsymbol{S}$ is linearly projected into two $D$ -dimensional representations using learnable matrices \u2013 yielding a key KS \u2208RN\u00d7D and value $V_{S}\\!\\in\\!\\mathbb{R}^{N\\times D}$ \u2013 while $\\tau$ is projected into one $D$ -dimensional representation to obtain the query $Q_{\\mathcal{T}}\\in\\mathbb{R}^{M\\times D}.$ These representations are then used to compute the attention-based target refinement as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Delta_{T}^{\\mathrm{att}}\\!=\\!\\arctan{(Q_{\\mathcal{T}},K_{S},V_{S})}\\!=\\!\\mathrm{softmax}\\!\\left(\\frac{Q_{\\mathcal{T}}K_{S}^{\\top}}{\\sqrt{D}}\\right)\\cdot V_{S},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with the scaled dot product $\\bar{\\pmb{A}}_{\\mathcal{T},\\mathcal{S}}\\,=\\,1/\\sqrt{D}\\left(\\pmb{Q}_{\\mathcal{T}}\\pmb{K}_{\\mathcal{S}}^{\\top}\\right)\\,\\in\\,\\mathbb{R}^{M\\times N}$ representing the scaled pairwise similarity between target and source elements. This concept is commonly referred to as crossattention (CA) between target $\\tau$ and source $\\boldsymbol{S}$ . If a representation itself is to be refined given the context within, i.e. source and target are identical $(S\\!=\\!\\mathcal T)$ , Equation (1) reduces to the well-known self-attention where the triplet key, query and value are all generated as a function of the same sequence elements. ", "page_idx": 2}, {"type": "text", "text": "Note that computing the similarity matrix $\\bar{A}_{\\mathcal{T},\\mathcal{S}}$ has computational complexity $\\mathcal{O}(N M)$ . For selfattention used in Transformers where $\\tau\\!=\\!s$ and hence $M=N$ , this yields quadratic complexity $\\mathcal{O}(N^{2})$ w.r.t. the input sequence length $N$ , prohibiting its use on longer sequences when computational resources are limited. On the other hand, if cross-attention is employed with a fixed sequence length $M\\!=\\!\\mathrm{const}\\ll N$ , the complexity becomes linear $\\mathcal{O}(N)$ . ", "page_idx": 2}, {"type": "text", "text": "2.2 Bi-Directional Cross-Attention ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Reducing the complexity of attention from quadratic to linear without impairing performance or adding constraints w.r.t. input modalities is one of the main aspects of this work. We build our approach on the previously introduced notion that most data can be interpreted as \u2018what\u2019 and \u2018where\u2019 \u2013 and both need to pay attention to the other for optimal information exchange. We represent the \u2018what\u2019 via a small set of $M$ learnable latent vectors and the \u2018where\u2019 via an input-dependent sequence of $N$ tokens, respectively denoted via the subscripts $_\\mathrm{lat}$ and $\\mathrm{tok}$ in the following and with $M\\ll N$ . Na\u00efvely, one could simply apply two individual cross-attention operations sequentially \u2013 first querying information from one side and then the other by creating two query-key-value triplets. However, our analyses in Section 3.1 show that symmetric tendencies in the attention patterns between latents and tokens naturally emerge during training, offering a chance to further reduce the computational requirements and to increase efficiency via our $b i$ -directional cross-attention as follows. ", "page_idx": 2}, {"type": "image", "img_path": "5sm8YDnWvC/tmp/4df10675e60602285564227f1adde24636d689fc0894c8751a4012c7d9b32d51.jpg", "img_caption": ["Figure 2: BiXT architecture. (left) Input data passing through one layer of our Bi-Directional CrossAttention Transformer. (right) Internal structure of proposed efficient bi-directional cross-attention. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We start by creating reference-value pairs $R_{\\mathrm{lat}}\\in\\mathbb{R}^{M\\times D}$ , $V_{\\mathrm{lat}}\\in\\mathbb{R}^{M\\times D}$ and $R_{\\mathrm{tok}}\\in\\mathbb{R}^{N\\times D}$ , $V_{\\mathrm{tok}}\\in$ $\\mathbb{R}^{N\\times D}$ via learnable linear projection from the latent vectors and tokens, respectively. Leveraging symmetry to create bi-directional information exchange, pairwise similarities between latents and tokens are then computed via a scaled dot product as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{\\boldsymbol{A}}_{\\mathrm{lat,tok}}=\\left(\\frac{\\boldsymbol{R}_{\\mathrm{lat}}\\boldsymbol{R}_{\\mathrm{tok}}^{\\mathsf{T}}}{\\sqrt{D}}\\right)=\\bar{\\boldsymbol{A}}_{\\mathrm{tok,lat}}^{\\mathsf{T}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is in turn used to obtain the attention-based refinement for both, the latents and tokens, via ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{\\mathrm{lat}}^{\\mathrm{atu}}=\\mathrm{softmax}\\big(\\bar{A}_{\\mathrm{lat,tok}}\\big)\\cdot V_{\\mathrm{tok}}\\qquad\\mathrm{and}\\qquad\\Delta_{\\mathrm{tok}}^{\\mathrm{atu}}=\\mathrm{softmax}\\big(\\bar{A}_{\\mathrm{tok,lat}}\\big)\\cdot V_{\\mathrm{lat}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that in addition to providing linear scaling w.r.t. to the input length $N$ , Equation (2) requires evaluating the most computationally-expensive operation, namely the similarity matrix $(\\mathcal{O}(M N))$ , only once and allows simultaneous refinement of latents and tokens as defined in Equation (3). The implicit reuse of the references as both query and key further reduces the parameter count of the linear projection matrices by $^{1}\\!/\\!3$ compared to na\u00efve sequential cross-attention. ", "page_idx": 3}, {"type": "text", "text": "2.3 BiXT \u2013 Bi-Directional Cross-Attention Transformers ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Figure 2 (left) illustrates the individual components that make up our BiXT architecture. BiXT is designed in a simple symmetric, ladder-like structure allowing \u2018what\u2019 (latent vectors) and \u2018where\u2019 (tokens) to simultaneously attend to and develop alongside each other \u2013 making it equally-well suited for instance-based tasks like classification and dense tasks like semantic segmentation on a variety of input modalities. We start this section with a brief overview, followed by more detailed descriptions of the individual components. ", "page_idx": 3}, {"type": "text", "text": "General overview. The raw input data is first passed through a tokenization module which projects the data into an embedding sequence of length $N$ and optionally adds positional encodings, depending on the input modality and data structure. These tokens together with a fixed set of $M$ learnable latent vectors are then passed to the first layer\u2019s bi-directional cross-attention module for efficient refinement (details depicted in Figure 2 (right) and explained below). The latents are then further refined via latent self-attention, while the tokens are either directly passed on to the next layer (default) or optionally refined by a token refinement module which could include modality-specific components. The simultaneous ladder-like refinement of \u2018what\u2019 and \u2018where\u2019 is repeated for $L$ layers, before the result is passed to task-specific output head(s). For instance-based tasks like classification, we simply average the set of latent vectors and attach a classification head to the output, while for tasks like segmentation that require outputs resembling the input data structure, the refined tokens are used. ", "page_idx": 3}, {"type": "text", "text": "Efficient bi-directional information exchange. We use bi-directional cross-attention introduced in Section 2.2 to enable $M$ latents and $N$ tokens to simultaneously attend to each other in a time and memory efficient way, provided $M\\ll N$ . The detailed internal structure of our module is depicted in ", "page_idx": 3}, {"type": "text", "text": "Figure 2 (right) and defined via Equations (2) and (3). Apart from the efficient bi-directional attention computation, it follows the common Transformer-style multi-head attention in terms of normalization, activations and processing via feed-forward networks (FFN) introduced by Vaswani et al. [44] and can thus be easily implemented in modern deep learning frameworks. ", "page_idx": 4}, {"type": "text", "text": "Three aspects are particularly worth noting here: 1) While our bi-directional attention imposes a \u2018hard\u2019 structural constraint of symmetry on the pair-wise similarity matrix between tokens and latents as defined in Equation (2), the actual information exchange is less strict: applying the row-wise and column-wise softmax operations to obtain the actual attention maps offers a certain degree of flexibility, since adding a constant to each element in a row keeps the resulting (latent) attention map unchanged while modulating the column-wise (token) one, and vice versa. More specifically, bi-directional CA between $M$ latents and $N$ tokens has in total $M N-1$ degrees of freedom (dof), only $(M\\!-\\!1)\\!\\cdot\\!(N\\!-\\!1)$ of which are shared \u2013 leaving $M\\!+\\!N\\!-\\!2$ dof that can be used by the network for the modulation of the (non-strictly-symmetric) information exchange (see Appendix A.3 for detailed discussion). 2) Even if the latents and tokens symmetrically attend to each other, the actual information that is transferred is created via individual value projection matrices and thus offers flexibility in terms of content. 3) While tokens cannot directly communicate with each other as is possible when using computationally expensive self-attention, this communication can still take place over two layers in our structure by using a latent vector as temporary storage in a token-latent-token sequence. Since the total number of latents is usually larger than the semantic concepts required to describe one data sample, we can expect this to be possible without impairing performance. ", "page_idx": 4}, {"type": "text", "text": "Latent vector refinement. After gathering information from the tokens, we use one multi-head self-attention operation [44] to further refine the information stored in the latents and provide direct information exchange with a global receptive field across latents. Note that since the number of latents $M$ is fixed and significantly smaller than the input sequence, this operation is input-length independent and not particularly resource intensive. This step is similar to Perceiver [18, 19], but we only use one instead of several self-attention operations at each layer. ", "page_idx": 4}, {"type": "text", "text": "Optional token refinement. In the majority of experiments presented in this paper, we simply pass the tokens returned by the bi-directional cross-attention to the next layer. However, our architectural structure also allows to easily include additional (e.g. data-specific) modules for further refinement in a plug-n-play manner. We demonstrate examples of this in Section 3, where we add a local refinement component exploiting grid-shaped data for semantic segmentation and a data-specific hierarchical grouping module for point cloud shape classification. ", "page_idx": 4}, {"type": "text", "text": "Positional encodings. We use additive sinusoidal positional encodings [44] to represent the structure of input data, which is more efficient than learnt encodings for variable input size. For simplicity, we follow previous works [11] and create the encodings in 32 dimensions per input axis followed by a linear projection into the model\u2019s token dimension $D$ . This method is applicable independent of the raw data\u2019s dimensions and thus easily handles data ranging from 2D images to 3D or 6D point clouds. ", "page_idx": 4}, {"type": "text", "text": "Input tokenization. Tokenization can be performed in various ways and is the only input modalityspecific component in our architecture, akin to Perceiver-IO\u2019s input adapters [19]. For image-based experiments, we follow common practice and use simple linear projection as our default tokenizer to embed image patches. For point cloud data, we simply encode the 3D or 6D points directly into embedding space using our sinusoidal positional encoder. We adhere to the guidelines of Tay et al. [40] for text-based hierarchical sequence modelling and document retrieval experiments on LRA. ", "page_idx": 4}, {"type": "text", "text": "3 Experimental Evaluation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The purpose of our investigations presented in the following is twofold: 1) To provide qualitative and quantitative insights into our proposed bi-directional cross-attention and the underlying intuition of symmetry, and 2) to demonstrate how BiXT\u2019s ability to efficiently and effectively process longer sequences positively affects various tasks. We focus the majority of our experiments around efficient architectures in the low FLOP, memory and parameter regime, and unless otherwise stated, we use BiXT- $\\cdot\\!\\,T{\\i}$ with 64 latent vectors, embedding dimension 192 and 6 heads for all attention modules. ", "page_idx": 4}, {"type": "text", "text": "3.1 Symmetric Tendencies Emerge when Attending Both Ways ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We start by investigating the intuition underlying our work: When describing data like an image by asking \u2018what\u2019 is in it and \u2018where\u2019 things are, it intuitively makes sense that these two components are tightly interconnected, and that they will inform aka pay attention to each other. To this end, we set ", "page_idx": 4}, {"type": "text", "text": "Table 1: Bi-directional vs. iterative attention. (a) Classification accuracy on ImageNet1K. All architectures use 64 latent vectors and have been trained for 120 epochs with hyperparameters individually optimized. Architectural configurations noted in brackets. \u2020indicates sharing of all, \u2021of all but the 1st layer\u2019s cross-attention parameters. Results reported as mean and (unbiased) std-dev over 3 randomly seeded training runs (see appendix for complete results). (b) Point cloud shape classification on ModelNet40. BiXT without (na\u00efve) and with modality-specific components. ", "page_idx": 5}, {"type": "table", "img_path": "5sm8YDnWvC/tmp/26810826daf80d42bdbbbc49bf6cf8f04f31bad09735800a77100f0ac649a712.jpg", "table_caption": ["(a) ImageNet1K $@$ 120epochs. ", "(b) ModelNet40."], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "up a na\u00efve architecture where latent vectors first query the tokens via cross-attention (CA), followed by the tokens querying the latents (i.e. using independent query-key-value triplets), before a further refinement step of the latent information via one self-attention operation \u2013 repeated over multiple layers and trained on ImageNet1K [36]. When looking at the resulting attention patterns depicted in Figure 1, we discover that most latents pay attention to parts of the image representing one specific \u2018entity\u2019 like a building ((b), top-left), a flag ((b), top-right) or parts of the sky ((b), lower-right) \u2013 supporting the notion that latent vectors represent \u2018things\u2019. More interestingly however, we discover in (c) that most of these image regions (tokens) are in turn also paying attention to exactly these latent vectors \u2013 showing a roughly symmetric information exchange and providing a qualitative indication that our idea of leveraging symmetry via our bi-directional architecture might be well justified. We additionally visualize the attention patterns after replacing the na\u00efve sequential CA through our efficient bi-directional one in (d), and the results look surprisingly similar \u2013 clearly indicating that our symmetrically constrained approach can achieve similar information exchange while being significantly more efficient. ", "page_idx": 5}, {"type": "text", "text": "3.2 Attention \u2013 Iterative, Sequential or Bi-directional? ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We aim to provide conclusive insights about the two major advantages of our proposed bi-directional attention compared to Perceiver\u2019s iterative attention: 1) Higher performance for comparable numbers of FLOPs, and 2) Ability to optionally extend the architecture via modality-specific components. We therefore choose two tasks that have also been investigated in the Perceiver paper: Image classification on ImageNet1K [36] and point cloud shape classification on ModelNet40 [49]. ", "page_idx": 5}, {"type": "text", "text": "ImageNet classification. To provide a fair basis for comparison, we create a range of architectural configurations with iterative attention based on the insights reported by Jaegle et al. [18]. Targeting a similar FLOP count as our BiXT tiny, we experiment with different numbers of layers, varying numbers of self-attention operations per block and with sharing all CA parameters as well as all but the first layer\u2019s (for details, see Perceiver paper and our appendix) \u2013 yielding a total of 10 architectures based on Perceiver\u2019s iterative attention. Having optimized the hyperparameters (learning rate and schedule) for each individually, we run 3 randomly seeded training runs for the best 5 configurations and report their results after training for 120 epochs in Table 1 (a) together with BiXT and the na\u00efve sequential CA variant. It is apparent that removing the bottleneck of iterative attention significantly boosts the performance, with both BiXT and sequential CA outperforming all iterative variants by a significant margin at comparable FLOP counts. Interestingly, we find the configuration with 8 blocks and 6 self-attention layers per block (sa6-d8) to achieve best performance among the iterative variants, which aligns with the \u2018best\u2019 configuration reported by Jaegle et al. [18]. ", "page_idx": 5}, {"type": "text", "text": "Contrasting the two CA-based approaches with identical numbers of layers $(\\sqrt{a}I2^{\\circ})$ demonstrates the clear advantage of our proposed bi-directional $C A$ , requiring ${\\sim}7\\%$ fewer FLOPs, ${\\sim}15\\%$ less memory and $5\\%$ fewer parameters to achieve similar results as the sequential variant. This allows BiXT to use one additional layer at matching FLOP count, consistently outperforming the na\u00efve approach across all our experiments while being still $7\\!-\\!8\\%$ more memory efficient. ", "page_idx": 5}, {"type": "text", "text": "Point cloud shape classification. To gain further quantitative insights how bi-directional attention affects processing of other modalities, we evaluate our approach on the ModelNet40 dataset [49]. BiXT again clearly outperforms Perceiver in terms of overall accuracy (OA) and is even competitive to other point-based methods like the seminal PointNet [32] (Figure 2 (b)). In contrast to Perceiver\u2019s iterative attention that gathers information exclusively in the latents, BiXT\u2019s simultaneous refinement of latents and tokens allows us to easily integrate data-specific modules for token refinement. To gauge the effect, we add the \u2018affine grouping\u2019 module from PointMLP [25] without and with hierarchical structure (i.e. point reduction). While BiXT is still outperformed by point cloud specific PointMLP, these optional modules help to boost the accuracy by up to $3.9\\%$ while trading off generality. ", "page_idx": 6}, {"type": "text", "text": "3.3 Image Classification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Comparison to SOTA. Note that we focus here on efficient Transformer models in the low FLOP and/or parameter regime, with results reported in Table 2. BiXT performs favourably with default and convolutional tokenizer against the other \u2018vanilla\u2019 Transformers, outperforming both versions of DeiT by a significant margin $(6.2\\,{-}\\,11.8\\%)$ while being $\\sim\\!200\\times$ more efficient than Perceiver (IO). These results are highly competitive even when compared to specialized visiononly architectures that leverage complex pyramidal multi-scale techniques, with BiXT outperforming all but one very recent method (which however requires $29\\%$ more FLOPs than our BiXT). ", "page_idx": 6}, {"type": "text", "text": "Increasing feature resolution and input size. We keep the patch size fixed to $16^{2}$ while reducing the stride of our linear patch projector to increase feature resolution (see appendix for ablation on patch sizes vs. stride). Note that our BiXT/4 model can easily process 3,136 tokens per $224^{2}$ image thanks to linear scaling, boosting the top-1 accuracy to $82.7\\%$ . Linear scaling also lets us process larger input images more efficiently \u2013 which we investigate by fine-tuning on $384^{2}$ for 30 epochs to reduce the required computational resources. Increasing the input size further notably improves the accuracy across architectures by up to $2.1\\%$ , however at the expense of higher FLOP counts. Nevertheless, BiXT shows that it is possible to achieve $83.1\\%$ on ImageNet with only 15M parameters and no vision-specific internals. ", "page_idx": 6}, {"type": "text", "text": "Longer sequence beats model size. Most importantly, BiXT is able to efficiently leverage longer sequences to outperform larger competitors at fewer $F L O P s$ : The most-recent DeiT3-S achieves $81.4\\%$ (4.6G FLOPs, 22M param), while BiXT obtains $81.8\\%$ at only 3.6G FLOPs & 15M parameters \u2013 see Appendix B.1 for further details. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Classification on ImageNet1K using \u2018few-FLOP\u2019 Transformers. Note that we focus here on efficient models in the low FLOP and/or parameter regime. Perceiver architectures are included as contrast to our bi-directional attention. All methods have been trained on input resolutions of $224^{2}$ , and $\\uparrow384$ further fine-tuned on $384^{2}$ . Note that different models may have received a different optimization effort. \u2217result reproduced as not reported in original work. $\\ ^{\\bullet}(c o n\\nu)^{\\ast}$ indicates the use of a convolutional tokenizer (see appendix for details). ", "page_idx": 6}, {"type": "table", "img_path": "5sm8YDnWvC/tmp/66fc354be62254a81c2e937846a1f870d7809b70940bb3ac0ada174b0fc7b062.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "3.4 Dense Tasks \u2013 Semantic Image Segmentation & Point Cloud Part Segmentation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Semantic Segmentation. We investigate the transferability of our methods onto semantic image segmentation on ADE20K [56]. We follow common practice and first integrate BiXT pretrained on ImageNet1K together with SemFPN [21] as decoder. Our vanilla BiXT performs competitively against other methods with similar FLOP counts, while the more vision-specific variant $\\mathrm{BiXT+LPI}$ with local token refinement is on par with even the improved pyramidal PvTv2 and outperforms the other models of comparable complexity (Table 3). Please refer to Appendix C for more details. ", "page_idx": 6}, {"type": "text", "text": "Table 3: Semantic Segmentation on ADE20K. We again focus here on efficient models in the low FLOP and/or parameter regime. All methods trained on $512^{2}$ images, and FLOPs are computed on $512^{2}$ images as well. ", "page_idx": 7}, {"type": "table", "img_path": "5sm8YDnWvC/tmp/4d07887256b13155d2afcc2dc51654682ec97ec46437ac261459f9a48ef57a9b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "However, decoders like SemFPN were originally introduced for multi-scale CNN-like architectures and take feature maps at multiple resolutions as input. Non-hierarchical Transformers like BiXT therefore need to down- and upsample their feature maps at various stages \u2013 raising the question how this affects performance and to what extent results are caused by backbone, decoder, and their compatibility. To provide insights unaffected by these potential influences, we take inspiration from DINOv2 [27] and simply use a linear layer to directly predict a segmentation map at feature resolution from the last layer\u2019s tokens, which is then upsampled using bilinear interpolation. Interestingly, our na\u00efve approach is on par with the SemFPN variants but requires $80\\%$ fewer FLOPs, and outperforms by ${\\sim}1.6\\%$ at higher resolution while still being $32\\%$ more efficient (Table 3) \u2013 indicating that more research into the suitability of such decoders with non-hierarchical architectures might be needed. ", "page_idx": 7}, {"type": "text", "text": "Point Cloud Part Segmentation. Since BiXT provides a similar generality as Perceiver regarding its input data structure but additionally allows the use of the dense, local token information, we determine its suitability for the segmentation of parts of a point cloud on ShapeNetPart [52]. The na\u00efve application of BiXT with a linear classifier directly applied to the last layer\u2019s tokens achieves a competitive class mIoU of $83.5\\%$ and outperforms other \u2018simple\u2019 methods like seminal PointNet [32] (class mIoU of $80.4\\%$ ), but lags slightly behind recent more complex encoder-decoder methods like PointMLP [25] (class mIoU of $84.6\\%$ ). Including a modality-specific token-refinement module and decoder however closes the gap and lets BiXT obtain a highly competitive class mIoU of $84.7\\%$ \u2013 as always trading off performance and generality. Please refer to Appendix D for more detailed results. ", "page_idx": 7}, {"type": "text", "text": "3.5 Beyond Visual Perception: Hierarchical Sequence Modeling and Document Retrieval ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 4: Hierarchical Sequence Modeling and Document Retrieval using the LRA benchmark [40]. Samples per second indicate empirical throughput at inference time for varying specified batch sizes \u2018bs\u2019 (using one NVIDIA A100). ", "page_idx": 7}, {"type": "table", "img_path": "5sm8YDnWvC/tmp/97cc09a48657c23082f4717726de3a531b11d1ff837dd530784d6f9d61168e5a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Up to this point, we have demonstrated BiXT\u2019s advantages on perception tasks centered around visual and 3D-structural reasoning. We now go one step further and investigate whether our claim of \u2018BiXT performing at the same level as a full Transformer while being more efficient\u2019 holds on tasks that are proven to require modeling of and reasoning over very long and often complex sequences. We evaluate the two tasks from the LRA benchmark with the \u2019longest required attention span\u2019 [40]: hierarchical sequence modeling us", "page_idx": 7}, {"type": "text", "text": "ing Long-ListOps [26], and byte-level document retrieval using AAN [35]. Long-ListOps tests the ability to reason hierarchically over complex sequences composed of numbers, mathematical operators and brackets \u2013 requiring models to access all tokens and model the logical structure of inputs. \u2018Retrieval\u2019 evaluates the ability to encode and compress sequences of $4\\mathbf{k}$ length for matching and retrieval, requiring reasoning over 8k tokens in total. To allow fair comparison, we follow the setup in [50], and train both a full Transformer model and our BiXT variant for 5 random seeds each. While both models are on par in terms of accuracy, BiXT requires up to $28\\%$ fewer FLOPs and is up to $8.4\\times$ faster \u2013 clearly supporting our claim of significantly improving the efficiency for processing long sequences (Table 4). For additional details, please refer to the discussion in Appendix E. ", "page_idx": 7}, {"type": "image", "img_path": "5sm8YDnWvC/tmp/a1cc8521c529868c654044cb611f95f4ca6f4ab18e48095d9a946aef7477789f.jpg", "img_caption": ["Figure 3: Scaling trends. Ablating the influence of embedding dimension, varying numbers of latents and sequence lengths for ImageNet1K classification. All models trained with shorter schedule (only 300 epochs) to save computational resources, and comparisons should therefore be performed relative to each other. Red star-markers correspond to BiXT-Ti/16 (Acc. 80.1) from Table 2. Validation accuracy represented through solid lines, while dashed lines indicate the computational resources. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "3.6 Scaling Trends \u2013 Number of Latents & Dimensions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The majority of this paper is concerned with tiny efficient models; however, it is interesting to see whether our models follow previous Transformers in terms of scaling behavior. BiXT offers an additional degree of freedom in the number of latents. We therefore provide some insights into BiXT\u2019s ImageNet1K performance changes for 32, 64 and 128 latents as well as various embedding dimensions (Figure 3). As expected, accuracy increases with both larger embedding dimension and number of latents \u2013 and it is worth noting that increasing the number of latents scales quadratically in FLOPs due to the self-attention-based latent refinement while increasing the sequence length scales linearly. Note that we use shorter training schedules for this ablation, and results are intended to be interpreted relative to each other. While we chose not to run excessive hyperparameter optimization and refrain from translating to very large architectures due to the large computational requirements involved, we did not observe any signs why BiXT should not behave like other Transformer architectures in terms of scaling and performance. We therefore anticipate to see similar tendencies as reported for related attention-based architectures, but leave this to future work. ", "page_idx": 8}, {"type": "text", "text": "3.7 Limitations & Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our results obtained from the investigation of iterative vs. bi-directional attention as well as our experiments across multiple tasks and modalities clearly show that bi-directional attention offers advantages in a number of settings, both in terms of performance and efficiency. However, it is worth noting that by simultaneously refining the tokens alongside the latents, BiXT does not decouple the model\u2019s depth from the input, unlike Perceiver models [18]. Therefore, very deep BiXT variants might potentially face difficulties in settings of extremely long sequences paired with limited compute and memory. However, we suspect most such scenarios to benefti from some form of preprocessing via a modality-specific input tokenizer, similar to the input-adapter-based concept used in Perceiver-IO [19] \u2013 shifting most applications again into regions where BiXT performs effectively and efficiently. ", "page_idx": 8}, {"type": "text", "text": "Given the current popularity of natural language processing tasks, we would further like to note that BiXT in its current form is an encoder-based architecture (similar to BERT-like models), and we expect it to perform well on tasks that require understanding and modeling of entire sequences \u2013 which is what our results obtained in Section $3.5\\,/$ Table 4 on the LRA tasks indicate. However, as BiXT circumvents the expensive token self-attention of Transformers via our proposed bi-directional cross-attention, causal masking as commonly used in decoder-only methods for generative language tasks is not directly applicable to BiXT\u2019s current attention mechanism, as information from later tokens would be able to \u2018leak\u2019 to earlier ones via the latent refinement. One possibility to establish causality in this setup could be to assign groups of tokens to specific latents by masking the bidirectional cross-attention and latent refinement accordingly (while trading off some processing resolution at training time), but we expect there to be numerous potential ways and leave this as an interesting area for future follow-up research. ", "page_idx": 8}, {"type": "text", "text": "4 Related work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The introduction of Transformers [44] has helped self-attention to significantly gain in popularity, despite its caveat of scaling quadratically in computational time and memory with input length. Their flexibility regarding input modality and success in Natural Language Processing (NLP) [9] and Computer Vision (CV) [10, 41, 42] prompted a series of works targeting more efficient versions. ", "page_idx": 9}, {"type": "text", "text": "Approximating the attention matrix via low-rank factorization has been employed across NLP [20, 45, 39], CV [6, 58, 23] and others [7], essentially avoiding the explicit computation through associativity, estimating a set of bases or using sampling \u2013 usually at the expense of performance. Others proposed to use tensor formulations [24, 3] or exploit the input data structure [29, 17, 34, 11] under the umbrella of sparsity, however limiting their use to only one specific input modality. ", "page_idx": 9}, {"type": "text", "text": "The line of work closest related to ours are \u2018memory-based approaches\u2019 which employ some form of global memory to allow indirect interaction between local tokens. [4] propose to compose various local windowed patterns (sliding, dilated) with global attention on few \u2018pre-selected\u2019 and task-specific input locations for NLP tasks, while its vision derivative [55] provides global memory as tokens within a vision-pyramid architecture and employs four different pairwise attention operations combined with several sets of global tokens that are discarded at certain stages, introducing rather high architectural complexity. [1] additionally investigate the encoding of structured NLP inputs, whereas [54] propose a hand-crafted mix of random, window and global attention to sparsify and thus reduce attention complexity. [57] route information between selected tokens in a directed graph to achieve sparsity and skip computation in regions deemed irrelevant, whereas [5] split the input sequence and introduce dedicated latents for each chunk. [51] in turn use cross-attention-based dual-blocks for efficiency but combine these with merging-blocks that cast attention over the entire concatenated token sequence, introducing a shared representation space and preventing linear scaling. While these ideas of indirect local token communication via a shared global memory align with ours, BiXT realizes this goal in a much simpler and modality-independent manner when compared to the mix of highly modalityspecific components, attention patterns and strategies involved in these works. Preserving generality w.r.t. the input, [22] use a set of learnable \u2018inducing points\u2019 via cross-attention to query input data, while the recent Perceiver architectures [18, 19] similarly use a fixed set of latents to query input data \u2013 yet none offers the efficient simultaneous refinement of latents and tokens realized in our BiXT. Please see Appendix A.5 for some further in-detail discussion and a wider scope of related work. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we presented a novel bi-directional cross-attention Transformer architecture (BiXT) for which computational cost and memory consumption scale linearly with input size, motivated by a naturally emerging symmetry in two-way cross-attention that aligns with common intuition and has been empirically demonstrated in this work. By allowing the \u2018what\u2019 (latent variables) and \u2018where\u2019 (input tokens) to attend to each other simultaneously and develop alongside throughout the architectural stages, BiXT combines Perceiver\u2019s linear scaling with full Transformer architectures\u2019 high performance in a best-of-both-worlds approach. The ability to efficiently process longer sequences paired with the ease to integrate further domain-specific token refinement modules helps BiXT to outperform larger models on ImageNet1K, be up to $80\\%$ more efficient in semantic image segmentation, competitive across two point-cloud tasks, and on par with full Transformers in sequence modeling and document retrieval while requiring up to $28\\%$ less compute and being up to $8.4\\times$ faster. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported by the Australian Research Council (ARC) through grant DP230102775, The University of Melbourne\u2019s Research Computing Services and the Petascale Campus Initiative. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Ainslie, J., Ontanon, S., Alberti, C., Cvicek, V., Fisher, Z., Pham, P., Ravula, A., Sanghai, S., Wang, Q., and Yang, L. Etc: Encoding long and structured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 268\u2013284, 2020. ", "page_idx": 9}, {"type": "text", "text": "[2] Amos, I., Berant, J., and Gupta, A. Never train from scratch: Fair comparison of long-sequence models requires data-driven priors. In The Twelfth International Conference on Learning Representations, 2024.   \n[3] Babiloni, F., Marras, I., Slabaugh, G., and Zafeiriou, S. Tesa: Tensor element self-attention via matricization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13945\u2013 13954, 2020.   \n[4] Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.   \n[5] Chen, T. and Li, L. Fit: Far-reaching interleaved transformers. arXiv preprint arXiv:2305.12689, 2023.   \n[6] Chen, Y., Kalantidis, Y., Li, J., Yan, S., and Feng, J. $\\mathrm{A^{2}}$ -nets: Double attention networks. Advances in Neural Information Processing Systems, 31, 2018.   \n[7] Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. In International Conference on Learning Representations, 2021.   \n[8] Contributors, M. MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. https: //github.com/open-mmlab/mmsegmentation, 2020.   \n[9] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, 2019.   \n[10] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.   \n[11] El-Nouby, A., Touvron, H., Caron, M., Bojanowski, P., Douze, M., Joulin, A., Laptev, I., Neverova, N., Synnaeve, G., Verbeek, J., and Jegou, H. XCiT: Cross-covariance image transformers. In Advances in Neural Information Processing Systems, 2021.   \n[12] Gu, A., Goel, K., Gupta, A., and R\u00e9, C. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35:35971\u201335983, 2022.   \n[13] Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022.   \n[14] Gupta, A., Gu, A., and Berant, J. Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35:22982\u201322994, 2022.   \n[15] He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770\u2013778, 2016.   \n[16] Heo, B., Yun, S., Han, D., Chun, S., Choe, J., and Oh, S. J. Rethinking spatial dimensions of vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 11936\u2013 11945, 2021.   \n[17] Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180, 2019.   \n[18] Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General perception with iterative attention. In International Conference on Machine Learning, pp. 4651\u20134664. PMLR, 2021.   \n[19] Jaegle, A., Borgeaud, S., Alayrac, J.-B., Doersch, C., Ionescu, C., Ding, D., Koppula, S., Zoran, D., Brock, A., Shelhamer, E., et al. Perceiver io: A general architecture for structured inputs & outputs. In International Conference on Learning Representations, 2022.   \n[20] Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156\u20135165. PMLR, 2020.   \n[21] Kirillov, A., Girshick, R., He, K., and Doll\u00e1r, P. Panoptic feature pyramid networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6399\u20136408, 2019.   \n[22] Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh, Y. W. Set transformer: A framework for attention-based permutation-invariant neural networks. In International Conference on Machine Learning, pp. 3744\u20133753. PMLR, 2019.   \n[23] Li, X., Zhong, Z., Wu, J., Yang, Y., Lin, Z., and Liu, H. Expectation-maximization attention networks for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9167\u20139176, 2019.   \n[24] Ma, X., Zhang, P., Zhang, S., Duan, N., Hou, Y., Zhou, M., and Song, D. A tensorized transformer for language modeling. Advances in Neural Information Processing Systems, 32, 2019.   \n[25] Ma, X., Qin, C., You, H., Ran, H., and Fu, Y. Rethinking network design and local geometry in point cloud: A simple residual mlp framework. In International Conference on Learning Representations, 2022.   \n[26] Nangia, N. and Bowman, S. Listops: A diagnostic dataset for latent tree learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pp. 92\u201399, 2018.   \n[27] Oquab, M., Darcet, T., Moutakanni, T., Vo, H. V., Szafraniec, M., Khalidov, V., Fernandez, P., HAZIZA, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang, P.-Y., Li, S.-W., Misra, I., Rabbat, M., Sharma, V., Synnaeve, G., Xu, H., Jegou, H., Mairal, J., Labatut, P., Joulin, A., and Bojanowski, P. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024.   \n[28] Orvieto, A., Smith, S. L., Gu, A., Fernando, A., Gulcehre, C., Pascanu, R., and De, S. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pp. 26670\u201326698. PMLR, 2023.   \n[29] Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., and Tran, D. Image transformer. In International Conference on Machine Learning, pp. 4055\u20134064. PMLR, 2018.   \n[30] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, pp. 8024\u20138035. 2019.   \n[31] Peng, B., Alcaide, E., Anthony, Q. G., Albalak, A., Arcadinho, S., Biderman, S., Cao, H., Cheng, X., Chung, M. N., Derczynski, L., Du, X., Grella, M., GV, K. K., He, X., Hou, H., Kazienko, P., Kocon, J., Kong, J., Koptyra, B., Lau, H., Lin, J., Mantri, K. S. I., Mom, F., Saito, A., Song, G., Tang, X., Wind, J. S., Woz\u00b4niak, S., Zhang, Z., Zhou, Q., Zhu, J., and Zhu, R.-J. RWKV: Reinventing RNNs for the transformer era. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.   \n[32] Qi, C. R., Su, H., Mo, K., and Guibas, L. J. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 652\u2013660, 2017.   \n[33] Qi, C. R., Yi, L., Su, H., and Guibas, L. J. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in Neural Information Processing Systems, 30, 2017.   \n[34] Qiu, J., Ma, H., Levy, O., Yih, W.-t., Wang, S., and Tang, J. Blockwise self-attention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 2555\u20132565, 2020.   \n[35] Radev, D. R., Muthukrishnan, P., Qazvinian, V., and Abu-Jbara, A. The acl anthology network corpus. Language Resources and Evaluation, 47:919\u2013944, 2013.   \n[36] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015.   \n[37] Shen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 3531\u20133539, 2021.   \n[38] Smith, J. T., Warrington, A., and Linderman, S. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023.   \n[39] Song, K., Jung, Y., Kim, D., and Moon, I.-C. Implicit kernel attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 9713\u20139721, 2021.   \n[40] Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021.   \n[41] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J\u00e9gou, H. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pp. 10347\u201310357. PMLR, 2021.   \n[42] Touvron, H., Cord, M., and J\u00e9gou, H. Deit iii: Revenge of the vit. In European Conference on Computer Vision, pp. 516\u2013533. Springer, 2022.   \n[43] Tu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., and Li, Y. Maxvit: Multi-axis vision transformer. In European Conference on Computer Vision, pp. 459\u2013479, 2022.   \n[44] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.   \n[45] Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.   \n[46] Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 568\u2013578, 2021.   \n[47] Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media, 8(3):415\u2013424, 2022.   \n[48] Wightman, R., Touvron, H., and J\u00e9gou, H. Resnet strikes back: An improved training procedure in timm. arXiv preprint arXiv:2110.00476, 2021.   \n[49] Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., and Xiao, J. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1912\u20131920, 2015.   \n[50] Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., and Singh, V. Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention. 2021.   \n[51] Yao, T., Li, Y., Pan, Y., Wang, Y., Zhang, X.-P., and Mei, T. Dual vision transformer. IEEE transactions on pattern analysis and machine intelligence, 2023.   \n[52] Yi, L., Kim, V. G., Ceylan, D., Shen, I.-C., Yan, M., Su, H., Lu, C., Huang, Q., Sheffer, A., and Guibas, L. A scalable active framework for region annotation in 3d shape collections. ACM Transactions on Graphics (ToG), 35(6):1\u201312, 2016.   \n[53] You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X., Demmel, J., Keutzer, K., and Hsieh, C.-J. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2020.   \n[54] Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33:17283\u201317297, 2020.   \n[55] Zhang, P., Dai, X., Yang, J., Xiao, B., Yuan, L., Zhang, L., and Gao, J. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. arXiv preprint arXiv:2103.15358, 2021.   \n[56] Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., and Torralba, A. Scene parsing through ade20k dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 633\u2013641, 2017.   \n[57] Zhu, L., Wang, X., Ke, Z., Zhang, W., and Lau, R. W. Biformer: Vision transformer with bi-level routing attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10323\u201310333, June 2023.   \n[58] Zhu, Z., Xu, M., Bai, S., Huang, T., and Bai, X. Asymmetric non-local neural networks for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 593\u2013602, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Impact Statement ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This paper presents work whose goal is to advance the field of machine learning and in particular to increase the efficiency of Transformer models to allow higher accuracy without increasing FLOPs. There are many potential societal and ethical consequences of large-scale machine learning and its applications, but these are applicable to the entire field and not specific to our proposed architecture. Our approach aims to reduce the computational cost of Transformer models, which makes these models more accessible to users with lower-end computing systems; this democratization of AI can have positive or negative social consequences. Reducing the computational costs of Transformer models reduces their energy consumption and therefore their impact on the environment; however, these benefits may be offset if users take advantage of the increased efficiency of our approach to implement more or larger models. ", "page_idx": 13}, {"type": "text", "text": "A BiXT \u2013 General Aspects and Insights ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Code and Reproducibility ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We implemented our models in PyTorch [30] using the timm library, and will release all code and pretrained models. We further made use of the mmsegmentation library [8] for the semantic segmentation experiments. Point cloud experiments were built on the publicly released code base from Ma et al. [25]. ", "page_idx": 13}, {"type": "text", "text": "A.2 Complexity Analysis ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The complexity of BiXT is dominated by the bi-directional cross-attention, in particular by a) the matrix multiplication to compute the similarity matrix and b) the two matrix multiplications to compute the refined outputs. Using the previously specified embedding dimension $D$ , $N$ tokens and $M$ latent vectors, multiplication a) involves matrices of shape $M\\times D,D\\times N$ with result $M\\times N$ , and the two multiplications b) involve matrices of shape $M\\times N$ , $N\\times D$ with result $M\\times D$ and $N\\!\\times\\!M,M\\!\\times\\!D$ with result $N\\!\\times\\!D$ . The overall complexity per layer is thus $\\mathcal{O}(M N D)=\\mathcal{O}(N)$ and linear in the size of the input $N$ . ", "page_idx": 13}, {"type": "text", "text": "A.3 Bi-Directional Attention and Degrees of Freedom ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we discuss the degrees of freedom (dof) inherent to our bi-directional cross-attention and provide some further insights into why the information exchange between latents and tokens is less restricted than might at first be expected. It is worth noting that there might be cases where the approximate symmetry that motivates our approach does not clearly emerge when using a na\u00efve sequential method. Even in these cases, we however found our method to still consistently provide a net benefit across all experiments. We conjecture that multiple aspects contribute to this effect, one of which is that even though a \u2018hard\u2019 structural symmetry constraint is imposed on the pairwise similarity matrix, the actual attention matrices obtained after row- and column-wise softmax have additional non-shared degrees of freedom which can be used to modulate the information exchange. We discuss this in the following in more detail. (Another helpful aspect could be that having an additional layer due to BiXT\u2019s higher efficiency can compensate for additionally required non-symmetric processing, and information exchange can also be realized across multiple layers via e.g. a token-latent-token sequence.) ", "page_idx": 13}, {"type": "text", "text": "TLDR: Bi-directional cross-attention between $M$ latents and $N$ tokens has in total $M N-1$ dof, only $(M{-}1){\\cdot}(N{-}1)$ of which are shared \u2013 leaving $M\\!+\\!N\\!-\\!2$ dof that can be used by the network for the modulation of the (non-strictly-symmetric) information exchange. ", "page_idx": 13}, {"type": "text", "text": "Gentle introduction. For ease of understanding, we start from a vector $\\bar{\\pmb{v}}\\in\\mathbb{R}^{N}$ and apply the softmax operation to obtain $\\pmb{v}=\\mathrm{softmax}(\\bar{\\pmb{v}})$ . Given that all entries $v_{i}$ of this vector have to sum to 1 due to the applied softmax operation, $\\pmb{v}$ has $N\\!-\\!1$ dof. This can also be interpreted as \u201cadding a constant to all elements of $\\bar{\\pmb v}$ doesn\u2019t change $\\pmb{v}^{\\circ}$ . ", "page_idx": 13}, {"type": "text", "text": "Uni-directional cross-attention. Let us now consider the pair-wise similarity matrix $\\bar{A}_{\\mathcal{T},\\mathcal{S}}$ between target $\\tau$ and source $\\boldsymbol{S}$ as introduced in Section 2.1. Casting uni-directional attention between $M$ latents and $N$ tokens to refine the latents, we obtain $\\begin{array}{r}{A_{\\mathrm{lat,tok}}=\\mathrm{softmax}(\\bar{A}_{\\mathrm{lat,tok}})\\in\\mathbb{R}^{M\\times N}}\\end{array}$ with the softmax applied row-wise \u2013 resulting in $M\\!\\cdot\\!(N\\!-\\!1)$ dof as visualized in Figure A1 a). Likewise, computing the attention matrix $A_{\\mathrm{tok,lat}}\\in\\mathbb{R}^{N\\times M}$ between tokens and latents using a different set of key and query vectors yields $N{\\cdot}(M{-}1)$ dof, which is visualized in its transposed form in Figure A1 b). $\\rightarrow$ Therefore, sequentially applying two uni-directional cross-attention operations on two individual pair-wise similarity matrices provides a total of $2M N\\!-\\!M\\!-\\!N$ dof. ", "page_idx": 13}, {"type": "image", "img_path": "5sm8YDnWvC/tmp/95544781cf849e02441b2ef91fde060d4ab64e9fbb4145b97d6830df2e2480bd.jpg", "img_caption": ["Figure A1: Degrees of Freedom. (a) Row-wise softmax for uni-directional cross-attention, based on matrix $\\in\\mathbb{R}^{M\\times N}$ with $M\\cdot(N{-}1)$ degrees of freedom. (b) Column-wise softmax for uni-directional cross-attention, based on matrix $\\in\\mathbb{R}^{M\\times N}$ with $N\\!\\cdot(M\\!-\\!1)$ degrees of freedom. (c) Row- and columnwise softmax for our proposed bi-directional cross-attention, using the same matrix $\\in\\mathbb{R}^{M\\times N}$ with $M N{-}1$ degrees of freedom. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Bi-directional cross-attention. Unlike the sequential approach, our proposed bi-directional crossattention uses the same pair-wise similarity matrix and obtains the attention matrices via row- and column-wise softmax. This can be interpreted as overlaying both operations and their respective degrees of freedom, and is visualized in Figure A1 c). As demonstrated by the shaded area, both softmax operations \u2018share\u2019 a total of $(M-1)\\cdot(N-1)$ dofs. With the row-wise softmax yielding $M\\!\\cdot\\!(N\\!-\\!1)$ dof and the column-wise softmax $N\\!\\cdot\\!(M\\!-\\!1)$ dof, this results in a total of $M N{-}1$ dof \u2013 where the \u20181\u2019 can be interpreted as \u201cadding the same constant to all elements pre-softmax doesn\u2019t change the result\u201d. Note however that while adding the same constant to all elements of a row (pre-softmax) does not affect the results after the row-wise softmax, it does change the column-wise one. Therefore, the non-overlapping areas in Figure A1 c) can be interpreted as the dof that are unique to the attention maps obtained via row- or column-wise softmax, and can be used to modulate the resulting information flow to better accommodate potential deviations from symmetry. ", "page_idx": 14}, {"type": "text", "text": "$\\rightarrow\\mathrm{Bi}$ -directional cross-attention uses the same pairwise similarity matrix to obtain both attention maps and therefore has a total of $M N{-}1$ dof, $(\\bar{M}\\!-\\!1)\\!\\cdot\\!(N\\!-\\!1)$ of which are shared and $M\\!+\\!N\\!-\\!2$ are unique. ", "page_idx": 14}, {"type": "text", "text": "A.4 Types of Attention \u2013 Additional Results, Visualizations and Further Explanations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "An extended list of the results stated in Section 3.2 are presented in Table A1. Note that we performed an individual sweep over a set of learning rates for each individual architecture \u2013 usually starting at $4e^{-3}$ and lowering until stable training occurred. We then used these results to pick the best 5 architectural variants and training schemes, and ran them for an additional 2 random seeds. Note that all architectural variants, including BiXT and the sequential one have only been run in this setup for a total of maximum 3 runs, and no cherry-picking of results occurred for any of the architectures. Note that we have also tried stepped schedulers with the schedule proposed in the original Perceiver paper [18], but resorted back to using the cosine since it showed equal or superior results. ", "page_idx": 14}, {"type": "text", "text": "To contrast the sequential attention to our default BiXT with 12 layers (d12) on a matching FLOP level, the sequential version uses only $_{l l}$ layers (d11) due to its higher complexity per layer. This is due to the fact that our bi-directional cross-attention only requires 4 instead of 6 projection matrices $(2\\times[R,V]$ vs. $2\\times[Q,K,V])$ and only computes the attention matrix once (instead of twice). The hereby saved FLOPs (as well as parameters and memory) can then be spent on additional layers, further improving results. Architectures with one more layer each show the same trend. ", "page_idx": 14}, {"type": "text", "text": "In other words, by holding FLOP and/or memory requirements constant, we consistently observe a net benefit with our bi-directional attention in terms of accuracy throughout our experiments. We empirically found that it additionally improved robustness/consistency across different parameter initializations (seeds), which can be seen by the slightly smaller standard deviations of the bidirectional variants. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Table A1: Architectural variants using iterative attention & cross-attention parameter sharing. Classification accuracy on the ImageNet1K dataset for varying types of attention. All architectures use 64 latent vectors and have been trained for 120 epochs with hyperparameters individually optimized. Cross-attention parameter sharing schemes: \u2020indicates sharing of all, $^{\\ddagger}$ of all but the $1^{\\mathrm{st}}$ layer\u2019s crossattention parameters. Architectural configurations noted in brackets. Three randomly seeded runs were performed for the \u2018best\u2019 architectures (judged by their performance on seed $=42$ ), and mean and (unbiased) standard deviation are reported. One randomly seeded run reported for all other architectures. ", "page_idx": 15}, {"type": "table", "img_path": "5sm8YDnWvC/tmp/f4aa935d6b3faabd702a490a196d303f1776e7aaa73410fd6e873e3cdda70d52.jpg", "table_caption": ["Visualizing the three types of attention. To further ease understanding and provide a clearer overview of the differences between the various investigated types of attention, we visualize the conceptual changes in the architectural layout when transitioning from \u2018iterative\u2019 over \u2018sequential\u2019 to our proposed efficient \u2018bi-directional\u2019 attention and their respective differences in Figure A2. "], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "5sm8YDnWvC/tmp/47f70976ce9ad7fb43be4a60f6bf74967d28018487caf309a4f7f71651f6c2a7.jpg", "img_caption": ["Figure A2: Transitioning from iterative to bi-directional attention. (a) Perceiver-like iterative attention, creating a bottleneck and small effective working memory; (b) Na\u00efve sequential attention \u2018unblocking\u2019 the bottleneck and extending working memory, but still markedly less efficient than: (c) Bi-directional cross-attention used in BiXT, combining efficient linear scaling with competitive performance across various tasks. Note that iterative attention attends to the (unrefined) input at every layer, while sequential and bi-directional attend to variants of the input refined by the previous layer. The Perceiver-like setup additionally uses multiple self-attention layers to refine between each iterative cross-attention $(\\times B)$ in each architectural layer, whereas sequential and bi-directional variants only use one self-attention operation per architectural layer. Architectures are then built by stacking $L$ layers. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "5sm8YDnWvC/tmp/cfbc22484b7b4bee849781bfaebba480ee7f128326dfd8c16c009f3107714909.jpg", "img_caption": ["Figure A3: Detailed structure of attention blocks. (a) Perceiver-like iterative attention, creating a bottleneck and small effective working memory; (b) Bi-directional cross-attention used in BiXT. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure A3 shows the internal difference between the Perceiver-like iterative attention and our proposed bi-directional cross-attention in more detail. ", "page_idx": 16}, {"type": "text", "text": "A.5 More Detailed Discussion of Most-Recent Related Work ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In the following, we provide some additional and more in-depth discussion of methods we see related to our proposed BiXT architecture. We start by taking a look at three methods mainly targeting the image space, and follow up with a more general discussion of related methods across modalities that focus on the long-sequence aspect \u2013 including recently proposed Structured State Space Sequence Models. ", "page_idx": 16}, {"type": "text", "text": "Methods mainly targeting the image domain. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u00bb DualViT [51]. DualViT\u2019s dual block used in the early layers of their architecture does to some extent show similarity to the na\u00efve solution of sequential cross-attention, but is distinctly different from our bi-directional approach as it does not leverage any symmetry. Importantly, their multi-stage pyramidal vision-only architecture uses a large number of \u2018merging blocks/layers\u2019 (between 9 - 24) which cast full self-attention over the concatenated sequence of latents and tokens. This prevents linear scaling and also introduces a shared embedding space of latent vectors and tokens through the use of the same key-query-value projection matrices \u2013 whereas our architecture keeps those separate (aligned with the presented \u2019what\u2019 and \u2019where\u2019 analogy and the level of information they represent) and scales linearly with respect to the input length. ", "page_idx": 16}, {"type": "text", "text": "$\\gg$ BiFormer [57]. BiFormer follows the common trend for vision-only approaches and employs a pyramidal structure. In contrast to previous work, the authors reduce the computational complexity through routing information between selected tokens via a directed graph, thus achieving sparsity to skip computation of certain regions that are deemed \u2018irrelevant\u2019. While this is a very neat way of dynamically reducing complexity, it is distinctly different from our approach and does not achieve true linear scaling. ", "page_idx": 16}, {"type": "text", "text": "$\\gg$ FiT [5]. FiT explicitly divides a token sequence into subgroups of tokens to cast quadratic local/windowed self-attention within, and assigns a small set of latents to each group. Exchange between these latents is accomplished via one-way cross-attention within each group, followed by global information routing via multiple self-attention operations cast across the latents of all groups. The exact architectural structure in terms of composition varies between architectural variants (number of local and global layers per block $^+$ number of blocks). Our BiXT in contrast achieves its entire information exchange via our proposed efficient bi-directional cross-attention between latents and tokens, followed by one self-attention operation among latents. This significantly simplifies the architecture in terms of complexity, only requires one set of latents that efficiently interacts with the entire sequence and does not require any manual grouping of the input sequence. ", "page_idx": 16}, {"type": "text", "text": "While our approach markedly differs from FiT in various aspects, their experimental setups are quite interesting and it is great to see that the research community is following similar directions in terms of decomposing and routing information among global latents and local sequence tokens. ", "page_idx": 16}, {"type": "text", "text": "Beyond Transformers \u2013 Recent developments in recurrent methods. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As we were focusing mainly on Transformer-based approaches and perception-based tasks in the main paper, we kept this as the primary focus of the literature review of the main manuscript. Here, we provide some additional recent methods relevant in the context of long sequence processing (especially beyond perception-based data) that warrant closer discussion. ", "page_idx": 17}, {"type": "text", "text": "While Transformer-based architectures have steadily gained in popularity over the last years, recurrent methods have recently enjoyed increased attention and have been both revisited and further improved across several works \u2013 e.g. by \u2019reinventing RNNs for the Transformer era\u2019 [31] with the goal of combining Transformer-style efficient training with the fast inference speed of RNNs. An alternative to the well-known recursive methods like RNNs are the recently introduced structured state-space sequence (S4) models [13], which are based on a new way of parameterizing SSMs that makes their application to long sequence modelling tasks computationally feasible and training much more efficient. Multiple works have since proposed simplifications to the S4 model ([12, 14, 38]) \u2013 while others have used the gained insights to further improve well-known models like RNNs [28]. ", "page_idx": 17}, {"type": "text", "text": "B ImageNet1K Experiments \u2013 Further Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "This section outlines further details and additional insights regarding our image classification experiments conducted on the ImageNet1K dataset [36]. ", "page_idx": 17}, {"type": "text", "text": "B.1 Longer Sequences Help to Beat Larger Models \u2013 Further Discussion and Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As reported in the main paper in Section 3.3, BiXT\u2019s ability to efficiently leverage longer sequences helps it to outperform larger models \u2013 and often at fewer FLOPs. ", "page_idx": 17}, {"type": "text", "text": "In the following, we contrast BiXT to different \u2018evolutions\u2019 of the ViT/DeiT family [10, 41, 42] with approximately matching parameter and/or FLOP counts. We start with our tiny BiXT and contrast it with the next larger Vision Transformer models \u2013 DeiT-S & DeiT3-S \u2013 in addition to the results shown in Table 2. This allows a much closer comparison in terms of FLOPs and parameters. Both DeiT-3 with $79.8\\%$ and the most-recent DeiT3-S with $81.4\\%$ use 22M parameters & 4.6GFLOPs. This is surpassed by both of our closest BiXT variants with fewer or similar FLOP counts (Table A2): ", "page_idx": 17}, {"type": "text", "text": "\u2022 BiXT-Ti/16 $\\uparrow384$ achieves $81.8\\%$ accuracy with 15M param & 3.6GFLOPs, and \u2022 BiXT-Ti/8 achieves $81.9\\%$ accuracy with 15M param & 4.6GFLOPs ", "page_idx": 17}, {"type": "text", "text": "Note that the use of longer sequences, either via $384{\\times}384$ images or through a patch size of 8, cannot be efficiently leveraged by DeiT variants as it would significantly increase their FLOP count due to the inherent quadratic scaling of their attention ( $_{\\sim15.5}$ GFLOPs for DeiT-S\u2191384). ", "page_idx": 17}, {"type": "text", "text": "In addition to matching DeiT3-S\u2019s performance via longer sequence length, we have run some additional experiments for BiXT with increased embedding dimension 256 (given limited available resources). This approximately matches DeiT-S in terms of parameters (BiXT-d256 27M vs. DeiT-S 22M), with results included in Table A2: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Our \u2018small\u2019 BiXT-d256/16 achieves $81.7\\%$ and already outperforms the original ViT-B $(77.91\\%)$ and recent DeiT3-S $(81.4\\%)$ , and is on par with DeiT-B $(81.8\\%)$ at a fraction of the FLOP count (2.9G vs. 17.5G).   \n\u2022 Our longer-sequence model BiXT-d256/8\u2191384 is on par even with the newest (mostoptimized) DeiT3-B while showing much higher parameter efficiency (26.7M vs 86.6M, albeit requiring slightly more FLOPs). ", "page_idx": 17}, {"type": "text", "text": "$\\gg$ A Note Regarding Larger Models and Actual Complexity of Training \u00ab ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "While it would indeed be very interesting to analyze larger models, we would like to note that this requires a substantial number of additional large experiments. Even though such models might at first appear to require moderate compute, the actually required computational budget not only encompasses the training runs but also the hyperparameter search. The importance of well-chosen hyperparameters and augmentation strategies grows significantly with model size, as can be seen in the literature (e.g. in the transition from ViT $[10]\\rightarrow\\mathrm{DeiT}$ [41] $\\rightarrow$ DeiT3 [42] or ResNet [15] $\\rightarrow$ ResNet strikes back [48]). This makes an appropriate exploration of this vast search space essential but computationally very expensive, and we (have to) leave this as an opportunity for future work. ", "page_idx": 17}, {"type": "table", "img_path": "5sm8YDnWvC/tmp/ecd9c0aa0e4951aaabbe3507cb0844f8f33266e3fbd45f6d8d6e5a316dfe261f.jpg", "table_caption": ["Table A2: Matching FLOP and parameter counts of Transformer models. Comparing evolutions of ViTs to variants of BiXT for image classification on ImageNet1K [36]. Note that different models might have received different levels of optimization effort, especially the ViT/DeiT variants across their multiple evolutions. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "B.2 Computational Complexity, Sequence Length and Empirical Throughput aka \u2018Latency\u2019 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The benefit of modeling input data at a higher resolution (e.g. smaller patches and larger images in vision) has been demonstrated across most works like ViT/DeiT. For example, increasing the input image size from 224 to 384 for DeiT3-S yields a boost of $2\\%$ in accuracy, but requires $3\\times$ as many FLOPs due to quadratic scaling of the attention with input sequence length. Reducing the patch size from $16{\\times}16$ to $4{\\times}4$ incurs $15.5\\times$ as many operations (Table A3). ", "page_idx": 18}, {"type": "text", "text": "One of the main advantages of our BiXTin contrast to vanilla Transformers is its linear scaling with the input sequence length while maintaining competitive performance. Increasing the input size from 224 to 384 only incurs $2.2\\times$ as many FLOPs, and patch-size reduction to $4{\\times}4$ less than $10\\times\\mathrm{~-~}\\mathbf{a}$ decrease by $26\\%$ and $35\\%$ , respectively. ", "page_idx": 18}, {"type": "text", "text": "This allows BiXT to essentially process and model longer sequences much more efficiently than na\u00efve Transformer models, boost results (see main paper) and extend its processing capabilities to regions where Transformer-like methods with full self-attention become infeasible. In our image segmentation experiments for example, BiXT processes sequences of up to 16,384 tokens during training \u2013 and up to 65,536 at inference time for $512\\times2048$ images. ", "page_idx": 18}, {"type": "text", "text": "Note that this aligns well with our obtained insights that BiXT is able to efficiently leverage a longer sequence to outperform a \u2018larger\u2019 DeiT model at fewer FLOPs (Section 3.3), as well as with the results obtained on the LRA benchmark in Section 3.5. ", "page_idx": 18}, {"type": "text", "text": "Table A3 shows common sequence lengths encountered during image processing (classification on ImageNet [36], semantic segmentation on ADE20K [56]) and demonstrates the scaling differences for ViT/DeiT variants [10, 41, 42] and BiXT. ", "page_idx": 18}, {"type": "text", "text": "While latency is closely linked to the FLOP counts, we additionally provide empirical data on the throughput $\\mathrm{(img/s)}$ in this section. Note that these numbers are obtained with a batch size of 256 on a single A100 GPU with float32 precision (no amp) \u2013 and that given its popularity and maturity, DeiT might have received more optimization effort than our BiXT. ", "page_idx": 18}, {"type": "text", "text": "As can be seen in Table A4, while the tiny version of DeiT3 [42] in its default configuration (patch 16) is faster than BiXT, our method significantly outperforms DeiT3 methods across all higher sequence lengths (i.e. larger images, smaller patches) \u2013 e.g. with BiXT-Ti384/4 (160img/s) being $6.4\\times$ faster than DeiT3-Ti384/4 (25img/s). ", "page_idx": 18}, {"type": "table", "img_path": "5sm8YDnWvC/tmp/ab6b5444618da31e6d220f93db90f65f15a78d7a87a6b9285ec23da8000cabbb.jpg", "table_caption": ["Table A3: Scaling of computational complexity. Relative increase in FLOPs and Activations (memory) over sequence length (w.r.t. baseline 224 / p16). "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "5sm8YDnWvC/tmp/c6a4ca60835b4650245b93fc3a8a36991ffd1ce8e45a5bb47cb5348c54857a32.jpg", "table_caption": ["Table A4: Throughput. Empirical latency for different variants of DeiT3 and BiXT. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "B.3 Model Configurations and Training Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Hyperparameter choice for the default ImageNet experiments: BiXT with 64 latents, 12 layers, embedding dimension for latents and tokens 192 paired with 6 heads (head dimension 32) \u2013 learning rate $2.5e^{-3}$ , weight decay 0.05 and lambc optimizer, as well as cosine learning rate scheduler with linear warmup; stochastic dropout on self-attention and cross-attention 0.1 for all tiny models. Apart from these, we directly apply the augmentation and training proposed by Touvron et al. [42]. Our models have been trained between 300 (ablations) and 800 epochs on one or several A100 GPUs. Note that we did not conduct an extensive hyperparameter search, and we expect results to potentially improve if done so. ", "page_idx": 19}, {"type": "text", "text": "Finetuning on images of size $384{\\times}384$ was performed for 30 epochs using a batch size of 512 and an initial learning rate of $2.5e^{-5}$ with cosine decline, starting from the model trained on $224{\\times}224$ images. We found empirically that increasing the stochastic dropout during finetuning to 0.2 can help to improve the results, and we hence use this as default value for our finetuning experiments. ", "page_idx": 19}, {"type": "text", "text": "B.4 Ablating Patch Size for Fixed Sequence Lengths in Image Classification ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we investigate whether lowering the patch size to increase the resolution of the resulting feature maps is actually the most-suited way \u2013 or whether simply reducing the stride and thus creating tokens that originate from overlapping patches yield better results. Our experiments on image classification using the ImageNet1k [36] dataset with models using varying patch sizes and strides to keep the sequence lengths fixed show that the originally introduced and commonly used patch size of $16\\times16$ pixels seems to be a good fit when using no overlapping patches (Table A5). Interestingly, we find that even when we increase the feature resolution and thus choose smaller strides, a patch size of $16\\times16$ still yields best results across our experiments. One potential reason is that patch boundaries are randomly chosen and objects in images do naturally not match these boundaries, so that information has to be exchanged \u2013 whereas slight overlaps might ease this to some extent. Another potential reason for this behaviour is that significantly decreasing the patch size reduces the input information per patch, with an $8^{2}\\,\\,\\mathrm{RGB}$ patch having a total of 192 channels, exactly matching the tiny embedding dimension. Smaller patches however would create a significant null space, which might be an additional reason for better performance when using larger patches. ", "page_idx": 19}, {"type": "table", "img_path": "5sm8YDnWvC/tmp/8fa9547c83826e866f37ad40f7a311c6aa1e7c22b2fb35f200ff6afa5841224a.jpg", "table_caption": ["Table A5: Varying patch sizes for fixed sequence lengths. ImageNet1k classification results for varying patch sizes are presented for three fixed sequence lengths (realised via stride). All models have been trained for 300 epochs using the same (default) hyperparameters and input images of size $224\\times224$ . Best results for each sequence length is highlighted in bold. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "B.5 Convolutional Tokenizer ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In addition to our default linearly-projecting tokenizer, we report results using a convolutional tokenizer as BiXT-Ti/16 (conv) in Table 2. This tokenizer follows El-Nouby et al. [11] and consists of a stack of four {conv - Batch Norm - GeLU} groups, using $3\\times3$ convs with stride 1 and sequentially encoding the input channels into the specified embedding dimension $D$ (via $D/8,D/4,D/2,\\bar{D})$ . ", "page_idx": 20}, {"type": "text", "text": "B.6 Token Refinement via Local Patch Interaction (XCiT) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We integrate a slightly modified version of the \u2018LPI\u2019 module from El-Nouby et al. [11] together with their convolutional tokenizer for our vision-specific image segmentation experiments. Our LPI module consists of two depth-wise convolutional layers (3x3) with Layer Normalization (instead of the original Batch Normalization) and a GELU non-linearity in between. For further details, please refer to the original paper. ", "page_idx": 20}, {"type": "text", "text": "C Semantic Image Segmentation Experiments \u2013 Further Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We investigate the transferability of our methods onto semantic image segmentation on the ADE20K dataset [56]. We follow common practice and integrate BiXT pretrained on ImageNet1K together with SemanticFPN [21] as decoder, train for $80\\mathrm{k}$ iterations with learning rate $6e^{-5}$ and weight decay 0.01 following El-Nouby et al. [11] and others. We choose a batch size of 32 due to the efficiency of our model on the $512^{2}$ images, and train on a single A100 GPU. Our vanilla BiXT performs competitively against other methods with similar FLOP counts, while the more vision-specific version $\\mathrm{BiXT+LPI}$ with local token refinement is on par with even the improved pyramidal PvTv2 and outperforms the others (Table A6). ", "page_idx": 20}, {"type": "text", "text": "Criticism on decoders $\\&$ a potential alternative. Decoders like SemFPN were originally introduced for CNN-like architectures and use feature maps at multiple resolutions. Non-hierarchical Transformer architectures like BiXT thus need to downsample and up-convolve their feature maps at various stages \u2013 raising the question how this affects performance and to which extent results are caused by backbone, decoder and the compatibility of the two. To provide insights unaffected by these potential influences, we take inspiration from the recently published DINOv2 [27] and simply use a linear layer to directly predict a segmentation map at feature resolution from the last layer\u2019s tokens, which we then upsample using bilinear interpolation. Interestingly, our naive approach clearly outperforms our SemFPN variants with $80\\%$ fewer FLOPs (6.4G vs 31.8G). Increasing the sequence length via smaller stride improves results further, with BiXT-Ti/8 (conv) clearly outperforming other methods while still requiring $\\sim32\\%$ fewer FLOPs. ", "page_idx": 20}, {"type": "text", "text": "These insights are somewhat surprising and clearly indicate that more research into the suitability of these decoders with non-hierarchical architectures might be needed. ", "page_idx": 20}, {"type": "text", "text": "Table A6: Semantic Segmentation on ADE20K. We again focus here on efficient models in the low FLOP and/or parameter regime. All methods trained on $512^{2}$ images, and FLOPs are computed on $512^{2}$ images as well. ", "page_idx": 21}, {"type": "table", "img_path": "5sm8YDnWvC/tmp/1c7f80190afacbc5e78721bf55fea28fa198b2ce3097aa1e9006fd2ac8a852ec.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "D Point Cloud Experiments \u2013 Further Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "D.1 Training and Evaluation Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Note that we do not use any voting strategy or other multi-scale augmentation and simply follow the training regime of PointMLP [25] for most of our experiments. We use a standard BiXT architecture for the \u2018na\u00efve\u2019 point cloud experiments as well as the ones using simple grouping \u2013 and reduce our architecture to 4 layers when using the decoder for part segmentation and the hierarchical approach for shape classification \u2013 paired with 32 and 24 neighbours, respectively (which are the default values used in other works like PointMLP). We train our models using a single A100 GPU (80Gb). ", "page_idx": 21}, {"type": "text", "text": "D.2 Detailed Results for Point Cloud Part Segmentation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Since BiXT provides a similar generality as Perceiver regarding its input data structure but additionally allows the use of the dense, local token information, we run experiments to determine its suitability regarding the segmentation of sub-parts of a point cloud \u2013 commonly referred to as point cloud part segmentation \u2013 on the ShapeNetPart dataset [52]. ", "page_idx": 21}, {"type": "text", "text": "The detailed results of our experiments are reported in the form of class intersection over union (IoU) and instance IoU in Table A7, together with the individual results for all object classes. The na\u00efve application of BiXT with a linear classifier directly applied to the last layer\u2019s tokens achieves a competitive class mIoU of $83.5\\%$ (instance mIoU of $85.2\\%$ ) and outperforms other simple methods like seminal PointNet [32](class mIoU of $80.4\\%$ ), but lacks slightly behind recent more complex encoder-decoder methods like PointMLP [25] (class mIoU of $84.6\\%$ ). Note, however, that methods in this space are usually highly specialized encoder-decoder structures. Including a modality-specific token-refinement (\u2019geometric affine grouping\u2019) and passing the encoded information to PointMLP\u2019s decoder [25] however closes the gap and lets BiXT obtain a highly competitive class mIoU of $84.7\\%$ (instance mIoU $86.0\\%$ ) \u2013 as always trading off performance and generality. ", "page_idx": 21}, {"type": "table", "img_path": "5sm8YDnWvC/tmp/34be4bcb473b0525df0dd270d06a321483e1262898f8d2dc260afbf5e55d6984.jpg", "table_caption": ["Table A7: Point cloud part segmentation on ShapeNetPart [52]. Reported are the class IoU and instance IoU for BiXT and PointMLP [25]. Note that we only compare here to PointMLP due to investigating the use of their grouping module and decoder within BiXT. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "E Hierarchical Sequence Modeling and Document Retrieval \u2013 Further Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "As detailed in the main paper\u2019s body in Section 3.5, we investigate BiXT\u2019s capabilities in modeling long sequences by using the Long Range Arena (LRA) benchmark proposed by Tay et al. [40]. We provide more details in the following. ", "page_idx": 22}, {"type": "text", "text": "E.1 Training and Evaluation Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For our experiments, we follow the setup proposed by Xiong et al. [50] and use models with 2 layers. The embedding dimension is set to 64, and we employ a hidden dimension of 128 (i.e. mlp-ratio of 2), as well as 2 attention heads. This applies to both the Transformer and our BiXT architecture. BiXT employs 32 latents for both experiments. ", "page_idx": 22}, {"type": "text", "text": "For the hierarchical sequence modeling experiments on Long ListOps [26], we use a vocabulary size of 32, and train for 40 epochs using a batch size of 32, learning rate of 2.5e-4, path-dropout rate of 0.02, the lamb optimizer [53] and a cosine scheduler with 1 epoch linear warm-up. ", "page_idx": 22}, {"type": "text", "text": "For the byte-level document retrieval task on AAN [35], we use a vocabulary size of 128, and train for 20 epochs using a batch size of 32, learning rate of 2.5e-5, the lamb optimizer [53] and a cosine scheduler with 1 epoch linear warm-up. ", "page_idx": 22}, {"type": "text", "text": "Models for both tasks are trained using a single A100 GPU. ", "page_idx": 22}, {"type": "text", "text": "E.2 Detailed Results and Additional Discussion ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To investigate our claim of \u2018BiXT performing at the same level as a full Transformer while being more efficient\u2019 in the context of tasks that are proven to require modeling of and reasoning over very long and often complex sequences, we evaluate the two tasks from the Long Range Arena (LRA) benchmark with the \u2019longest required attention span\u2019 [40]: hierarchical sequence modeling using Long-ListOps [26], and byte-level document retrieval using AAN [35]. ", "page_idx": 22}, {"type": "text", "text": "Note that the LRA benchmark has been specifically designed to evaluate the capabilities of Transformer-like models in very long-context scenarios in a systematic and unified manner [40]. ", "page_idx": 22}, {"type": "text", "text": "Long-ListOps tests the ability to reason hierarchically over complex sequences (length 2048) composed of numbers, mathematical operators and delimiters (brackets). To successfully solve this task, models are required to access all tokens and model the logical structure of the inputs while handling long contexts in order to make a prediction \u2013 a task considered to be \u201cconsiderably challenging\u201d [40]. For more information, we refer the interested reader to the original ListOps work [26] and the LRA benchmark [40], both of which provide more detail including a visualization of a shortened example sequence. ", "page_idx": 22}, {"type": "text", "text": "The \u2018retrieval\u2019 task on the other hand is designed to evaluate the ability of models to encode and compress sequences of $4\\mathbf{k}$ length into representations that are useful for matching and retrieval. With each individual document being 4k bytes/characters in length, this requires reasoning over 8k tokens in total. ", "page_idx": 22}, {"type": "text", "text": "To allow fair comparison, we follow the setup in [50] as detailed above in terms of model size and most hyperparameters. We train a full Transformer model and our BiXT variant for 5 random seeds each. We pick the best model based on validation accuracy, and report the mean and (unbiased) standard deviation across these models evaluated on the withheld test set in Table A8. ", "page_idx": 22}, {"type": "text", "text": "While both models are on par in terms of accuracy, BiXT requires up to $28\\%$ fewer FLOPs and is up to $8.4\\times$ faster \u2013 outlining BiXT\u2019s advantage in efficiently modeling long sequences. ", "page_idx": 22}, {"type": "text", "text": "E.3 Alternative Setups Found in Related Works on LRA ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Note that we follow the \u2018classic\u2019 2-layer setup as related works like [50], and run our architecture in direct comparison to a full Transformer [44] under the same conditions for fair comparison. ", "page_idx": 22}, {"type": "table", "img_path": "5sm8YDnWvC/tmp/be327d1b993fc938be4fa3cc75e665b2d4cc9a037552a1923e1a5c1fb15e7fa7.jpg", "table_caption": ["Table A8: Hierarchical Sequence Modeling and Document Retrieval using the LRA benchmark. Samples per second indicate empirical throughput at inference time. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Some recent approaches by Gu et al. [12, 13], Gupta et al. [14], and others have moved to target the alternate \u2018free-for-all\u2019 setting of LRA with often extensive task-specific hyperparameter and model optimization, e.g. see Table 11 (appendix) in the work by Smith et al. [38], where a specific architecture (layers, blocks, dimensions, initialization) is created for each task, paired with its own unique optimization configuration \u2013 requiring extensive search across possible configurations. ", "page_idx": 23}, {"type": "text", "text": "Given that our goal of evaluating BiXT on the LRA benchmark is to support our claim of \u2018being as performant as a full Transformer while being significantly more efficient\u2019, we deem it more appropriate instead provide the side-by-side evaluations as previously described to reduce compute requirements and allow faster training. ", "page_idx": 23}, {"type": "text", "text": "Note that recent work by Amos et al. [2] sheds new light on the comparability of methods under this \u2018free-for-all\u2019 setting and outlines significant changes in performance depending on a variety of factors like model initialization \u2013 further supporting our side-by-side model comparison using the same setup (including initialization method). ", "page_idx": 23}, {"type": "text", "text": "F Visualization of Latent-Token Attention ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To provide some additional qualitative insights into the bi-directional attention that is cast within BiXT, we provide three sets of attention maps overlaid onto the input image: ", "page_idx": 24}, {"type": "text", "text": "\u2022 Figure A4: The attention maps of the four latent vectors presented in Figure 1(d) for all layers throughout the BiXT tiny architecture (layer 1, top-left to layer 12, bottom-right).   \n\u2022 Figure A5 The attention maps of all latent vectors (64 in this case) for the final layer of our BiXT tiny architecture.   \n\u2022 Figure A6 The attention maps of all latent vectors (64 in this case) for the second-last layer of our BiXT tiny architecture. ", "page_idx": 24}, {"type": "image", "img_path": "5sm8YDnWvC/tmp/05207a43dd1d61c0c6908d56f162388cec96fed2b7a7994517ada31b55a6b2b2.jpg", "img_caption": ["Figure A4: Attention across layers. Bi-directional attention maps for the four selected tokens presented in Figure 1(d) across all layers: Starting with first layer on top left, ending with last layer (layer 12) on the bottom right. Displayed are the mean attention maps averaged across the heads of BiXT tiny with 64 latents. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "5sm8YDnWvC/tmp/46309faaca66a21f73f2a72c14a0bd4e3b869a093b667420780a5acaa20174cc.jpg", "img_caption": ["Figure A5: Attention maps of final layer. Bi-directional cross-attention maps of all 64 latent vectors of the final layer (layer 12) of our BiXT tiny architecture. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "5sm8YDnWvC/tmp/229fabb9cbe9f82d7028b90ca6c618fcb56ab1e75ee56f842ce5bb6d668b4d20.jpg", "img_caption": ["Figure A6: Attention maps of penultimate layer. Bi-directional cross-attention maps of all 64 latent vectors of the second-last layer (layer 11) of our BiXT tiny architecture. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All claims are backed up via thorough experiments presented in Section 3, and further complemented by a range of additional details, results and insights reported in the appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have added a dedicated section discussing the limitations of our work, see Section 3.7; We further discuss some additional limitations and difficulties within the individual experimental subsections, where appropriate. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA]   \nJustification: We do not introduce new theorems or lemmas that require proofs or explicit assumptions.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We detail all components of our introduced architecture throughout Section 2.3. We provide additional information on the components as well as the used settings including hyperparameter choices for all experiments in the Appendices A.1,B.3,B.4,B.5,B.6,C,D, and E. Our code including pretrained models will be made available upon acceptance. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our code including pretrained models will soon be made available at https://github.com/mrkshllr/BiXT. In the meantime, we detail all components of our introduced architecture throughout Section 2.3. We provide additional information on the components as well as the used settings including hyperparameter choices to reproduce all experiments in the Appendices A.1,B.3,B.4,B.5,B.6,C,D, and E. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We provide all information on the components as well as the used settings including hyperparameter choices required to reproduce all experiments in the Appendices A.1,B.3,B.4,B.5,B.6,C,D, and E. We also detail all components of our introduced architecture throughout Section 2.3. In addition, our code demonstrating the use of all hyperparameters in context will be made available upon acceptance. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide mean and (unbiased) standard deviation either across 3 or 5 randomly seeded training runs for the main experimental results supporting our claims (assuming normally distributed errors), see e.g. Table 1 (a) and Table 4. For the larger experiments, we (have to) refrain from doing so due to computational resource limitations. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide information on the required computational resources in the respective sections of the appendix (one or several A100 GPUs w/ 80Gb), and further report empirical throughput for the image classification, hierarchical sequence modeling and document retrieval experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our research conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We have added an individual section discussing the potential broader societal impacts of our work at the beginning of the appendix. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: As discussed in the societal impact statement, we do not see such immediate risks of our work, but highly encourage responsible use of any research, including ours. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: All used existing datasets and libraries have been appropriately cited, therefore linking to the appropriate places where individual licences can be found. We do not \u2018rerelease\u2019 any existing assets with our current work. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation   \nprovided alongside the assets?   \nAnswer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have not yet released any new assets with our work \u2013 but will add the respective documentation to the paper upon release of our code and models. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA]   \nJustification: Not applicable to our research. Guidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA]   \nJustification: Not applicable to our research. Guidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}]