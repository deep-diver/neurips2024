{"references": [{"fullname_first_author": "Vaswani, A.", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is the foundation of the BiXT architecture discussed in the main paper."}, {"fullname_first_author": "Dosovitskiy, A.", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-01-01", "reason": "This paper demonstrated the effectiveness of applying Transformers to computer vision tasks, significantly influencing the development of BiXT for image processing."}, {"fullname_first_author": "Jaegle, A.", "paper_title": "Perceiver: General perception with iterative attention", "publication_date": "2021-07-01", "reason": "The Perceiver architecture, introduced in this paper, inspired the design of BiXT, particularly its use of latent vectors for efficient processing of long sequences."}, {"fullname_first_author": "Tay, Y.", "paper_title": "Long range arena: A benchmark for efficient transformers", "publication_date": "2021-01-01", "reason": "This paper introduced the Long Range Arena (LRA) benchmark, which was used to evaluate the efficiency and performance of BiXT on long sequence tasks."}, {"fullname_first_author": "He, K.", "paper_title": "Deep residual learning for image recognition", "publication_date": "2016-01-01", "reason": "The ResNet architecture, introduced in this paper, is used as a backbone for BiXT's convolutional tokenizer, highlighting the integration of traditional CNNs with Transformer architectures."}]}