[{"figure_path": "1zVinhehks/figures/figures_2_1.jpg", "caption": "Figure 1: The GRDL framework. Classification involves using a GNN fg to encode a graph's information into a node embedding distribution. The similarities between the node embeddings and K reference distributions are calculated by the reference module fD. The graph is assigned the label of the reference that exhibits the highest similarity.", "description": "The figure illustrates the Graph Reference Distribution Learning (GRDL) framework.  A graph G is first encoded by a Graph Neural Network (GNN) into a node embedding matrix H. This matrix is treated as a discrete distribution. Then, a reference module fD compares this distribution with K learned reference distributions (D1...Dk), one for each class. The graph is classified into the class of the reference distribution that has the highest similarity to the graph's embedding distribution.", "section": "2 Proposed Approach"}, {"figure_path": "1zVinhehks/figures/figures_9_1.jpg", "caption": "Figure 2: Average training time per epoch. GRDL is 10 times faster than OT-GNN and TFGW.", "description": "This figure compares the average training time per epoch of the proposed GRDL model against two other models that utilize optimal transport distances: OT-GNN and TFGW.  The bar chart visually represents the training time for each model across multiple graph datasets.  The key takeaway is that GRDL demonstrates significantly faster training times, approximately 10 times faster than both OT-GNN and TFGW. This highlights the efficiency advantage of the GRDL approach.", "section": "5.2 Time Cost Comparison"}, {"figure_path": "1zVinhehks/figures/figures_9_2.jpg", "caption": "Figure 3: T-SNE visualization of MUTAG embeddings and reference distributions given by GRDL. Each dot denotes a graph and each square denotes a reference distribution.", "description": "This figure visualizes the results of applying t-SNE dimensionality reduction to the node embeddings generated by the GRDL model for the MUTAG dataset.  The plot shows the distribution of graph node embeddings in a 3D space, where each point represents a graph. The two classes of graphs are shown in light blue and orange.  Two squares represent the learned reference distributions, one for each class. The proximity of graph embeddings to the reference distributions indicates how well GRDL classifies those graphs.  The clustering of the graphs around their respective reference distributions indicates that GRDL is effectively separating the two classes in the MUTAG dataset.", "section": "5.3 Graph Visualization"}, {"figure_path": "1zVinhehks/figures/figures_18_1.jpg", "caption": "Figure 4: Training data misclassification rate on MUTAG (left) and IMDB-BINARY (right) with different numbers of references for each class (P). The effect of P on the training misclassification rates is not obvious.", "description": "This figure shows the training misclassification rate for two datasets, MUTAG and IMDB-BINARY, while varying the number of reference distributions (P) for each class.  The plots demonstrate the training misclassification rate across 100 epochs for three different values of P: 1, 2, and 3.  The results show that the impact of changing P on the training misclassification rate is minimal.  This observation supports a conclusion made in the paper that using a single reference distribution per class (P=1) is optimal for the GRDL model.", "section": "More Experiments"}, {"figure_path": "1zVinhehks/figures/figures_22_1.jpg", "caption": "Figure 2: Average training time per epoch. GRDL is 10 times faster than OT-GNN and TFGW.", "description": "This figure shows the average training time per epoch for three different graph classification methods: GRDL, OT-GNN, and TFGW.  The results demonstrate that GRDL is significantly faster than both OT-GNN and TFGW, achieving a speedup of at least 10 times. This highlights one of the key advantages of GRDL: its efficiency in training.  The figure supports the claim made by the authors regarding GRDL's superior efficiency compared to other state-of-the-art methods.", "section": "5.2 Time Cost Comparison"}, {"figure_path": "1zVinhehks/figures/figures_23_1.jpg", "caption": "Figure 6: Misclassification rate of GRDL on MUTAG (Figure 6a) and PROTEINS (Figure 6b) using different reference sizes m. The figures show that our model performs the best when a moderate m is used.", "description": "This figure displays the misclassification rate achieved by the GRDL model on the MUTAG and PROTEINS datasets when varying the size (m) of the reference distributions.  The plots show that selecting a moderate size for the reference distributions leads to the best classification performance, avoiding both overly simplistic and overly complex representations.", "section": "D.7 Experiments on The Generalization Error Bound"}]