[{"figure_path": "L8ifDX5XNq/tables/tables_4_1.jpg", "caption": "Table 1: The chart illustrates peak GPU memory consumption for various model architectures and configurations, highlighting differences across models. The LISA configuration is specifically labeled in the table: \u201cE\u201d denotes the embedding layer, \u201cH\u201d represents the language modeling head layer, and \u201c2L\u201d indicates two additional intermediate layers. *: Model parallelism is applied for the 70B model.", "description": "This table compares the peak GPU memory usage of several large language models (LLMs) under different fine-tuning methods: Vanilla (no fine-tuning), LoRA (low-rank adaptation), and LISA (layerwise importance sampling).  It shows how memory usage varies with different LLM sizes and the number of layers activated during training. The LISA configurations show the number of layers used during training; \u2018E\u2019 denotes the embedding layer, \u2018H\u2019 the language model head, and \u20182L\u2019 and \u20184L\u2019 indicate two and four additional intermediate layers activated, respectively.  The table highlights that LISA can significantly reduce memory usage, making it possible to train large models on less powerful hardware.", "section": "4 Experimental Results"}, {"figure_path": "L8ifDX5XNq/tables/tables_5_1.jpg", "caption": "Table 2: Results of different methods on MMLU, AGIEval, and WinoGrande, measured by accuracy.", "description": "This table presents the accuracy results of various fine-tuning methods (Vanilla, LoRA, GaLORE, LISA, and Full Fine-tuning) on three different benchmark datasets: MMLU (multitask language understanding), AGIEval (general abilities), and WinoGrande (commonsense reasoning).  It demonstrates the performance of LISA compared to other methods across various tasks and model architectures.", "section": "4.2 Moderate Scale Fine-Tuning"}, {"figure_path": "L8ifDX5XNq/tables/tables_6_1.jpg", "caption": "Table 3: Different methods on MT-Bench.", "description": "This table presents the results of different fine-tuning methods (Vanilla, LoRA, GaLore, LISA, and Full Parameter Training) on the MT-Bench benchmark for three different language models (TinyLlama, Mistral-7B, and LLaMA-2-7B).  The MT-Bench score, which is a metric for evaluating the overall performance of a language model across a range of tasks, is reported for each method and model. The results show the effectiveness of LISA in outperforming other methods in terms of achieving higher MT-Bench scores.", "section": "4.3 Moderate Scale Fine-Tuning"}, {"figure_path": "L8ifDX5XNq/tables/tables_6_2.jpg", "caption": "Table 4: Comparison of Moderate Scale Model Continual Pre-training on OpenWebMath Dataset.", "description": "This table presents the results of continual pre-training experiments on the GSM8K dataset using different methods for two LLMs: TinyLlama and LLaMA-2-7B.  The methods compared are Vanilla (no fine-tuning), LISA (the proposed method), and Full Fine-tuning (FT). The table shows the GSM8K accuracy scores and the peak GPU memory consumption (MEM.) for each model and method.  The results highlight LISA's ability to achieve competitive or even better performance than FT with significantly less memory usage.", "section": "4.3 Moderate Scale Continual Pre-training"}, {"figure_path": "L8ifDX5XNq/tables/tables_7_1.jpg", "caption": "Table 5: Different methods on MT-Bench, GSM8K, and PubMedQA score for LLaMA-2-70B.", "description": "This table presents the performance comparison of four different fine-tuning methods (Vanilla, LoRA, LISA, and Full Parameter Training) on three benchmark datasets: MT-Bench, GSM8K, and PubMedQA.  The results are presented for the LLaMA-2-70B model, showing the improvement achieved by LISA in terms of MT-Bench, GSM8K, and PubMedQA scores compared to the other methods.  The scores represent the model's accuracy or performance on each specific task.", "section": "4.4 Large Scale Fine-Tuning"}, {"figure_path": "L8ifDX5XNq/tables/tables_7_2.jpg", "caption": "Table 6: Different LISA hyperparameters combinations. All settings adopt learning rate  = 10<sup>-5</sup>. Here \u03b3 stands for sampling layers, K stands for sampling period.", "description": "This table presents the results of an ablation study on the LISA algorithm, investigating the effect of different hyperparameter combinations on the MT-Bench score.  The study varied the number of sampling layers (\u03b3) and the sampling period (K), while keeping the learning rate constant at 10<sup>-5</sup>.  The results show how these hyperparameters influence the model's performance.  Specifically, it shows the MT-Bench score achieved for each combination across two different LLMs: TinyLlama and LLaMA-2-7B.", "section": "4.5 Ablation Studies"}, {"figure_path": "L8ifDX5XNq/tables/tables_7_3.jpg", "caption": "Table 7: The MT-Bench scores derived from varying random seeds for layer selection.", "description": "This table presents the results of the MT-Bench evaluation metric for three different random seeds used in the layer selection process.  It demonstrates the robustness of the LISA method by showing consistent performance across different random seed initializations, indicating that the performance is not overly sensitive to the specific random seed used.", "section": "4.5 Ablation Studies"}, {"figure_path": "L8ifDX5XNq/tables/tables_17_1.jpg", "caption": "Table 8: Comparison of Language Model Fine-Tuning Methods on the MT-Bench score.", "description": "This table presents a comprehensive comparison of the performance of four different fine-tuning methods (Full Parameter Fine-Tuning (FT), Low-Rank Adaptation (LoRA), Gradient Low-Rank Projection (GaLore), and Layerwise Importance Sampling AdamW (LISA)) across a diverse range of tasks within the MT-Bench benchmark.  The tasks cover various aspects of language understanding, including writing, roleplaying, reasoning, coding, math, extraction, STEM, and humanities.  The table shows the average score for each method across these tasks, allowing for a direct comparison of their relative effectiveness in different domains.", "section": "A.2 Instruction Following Fine-tuning"}, {"figure_path": "L8ifDX5XNq/tables/tables_18_1.jpg", "caption": "Table 9: Mean score of three fine-tuning methods over three seeds for LLaMA-2-70B on the MT-Bench.", "description": "This table presents the average MT-Bench scores across three different random seeds for the LLaMA-2-70B model.  It compares the performance of three fine-tuning methods: Vanilla (baseline), LoRA, and LISA. The scores are broken down by task category (Writing, Roleplay, Reasoning, Code, Math, Extraction, STEM, Humanities) to allow for a more granular analysis of the model's strengths and weaknesses under each fine-tuning method.  The average score across all tasks is also provided for each method.", "section": "4.4 Large Scale Fine-Tuning"}, {"figure_path": "L8ifDX5XNq/tables/tables_20_1.jpg", "caption": "Table 2: Results of different methods on MMLU, AGIEval, and WinoGrande, measured by accuracy.", "description": "This table presents the performance comparison of various fine-tuning methods (Vanilla, LoRA, GaLORE, LISA, and Full Parameter Training) across three different benchmarks: MMLU, AGIEval, and Winogrande.  Each benchmark assesses different aspects of language model capabilities, and the results are reported as accuracy scores with standard deviations. This allows for a comprehensive evaluation of the methods' effectiveness across various tasks and provides insights into their relative strengths and weaknesses.", "section": "4.2 Moderate Scale Fine-Tuning"}, {"figure_path": "L8ifDX5XNq/tables/tables_20_2.jpg", "caption": "Table 1: The chart illustrates peak GPU memory consumption for various model architectures and configurations, highlighting differences across models. The LISA configuration is specifically labeled in the table: \"E\" denotes the embedding layer, \u201cH\u201d represents the language modeling head layer, and \"2L\" indicates two additional intermediate layers. *: Model parallelism is applied for the 70B model.", "description": "This table compares the peak GPU memory usage of different large language models (LLMs) under various fine-tuning methods.  The models include GPT2-Small, TinyLlama, Mistral-7B, LLaMA-2-7B, and LLaMA-2-70B.  Fine-tuning methods include vanilla training, LoRA with different rank sizes, and LISA with varying numbers of activated layers.  The LISA configurations are denoted with \"E\" (embedding layer), \"H\" (head layer), and \"2L\" (two additional layers). The table highlights how LISA achieves memory efficiency compared to other methods, especially for larger models. Model parallelism was used for the 70B model.", "section": "4 Experimental Results"}, {"figure_path": "L8ifDX5XNq/tables/tables_21_1.jpg", "caption": "Table 12: Compare LISA with fixed layers on LLaMA-2-7B, evaluate on MT-Bench.", "description": "The table presents the results of an ablation study on the LLaMA-2-7B model to evaluate the impact of fixing the randomly selected layers during training. It shows that randomly selecting layers generally outperforms the fixed-layer approach.  The results are presented with three different seeds for the random layer selection.", "section": "A.4.3 Sensitiveness to Randomness"}, {"figure_path": "L8ifDX5XNq/tables/tables_22_1.jpg", "caption": "Table 13: GSM8K Scores for LLaMA-2-7B when LISA meets the early exiting strategy DoLa.", "description": "This table presents the GSM8K accuracy improvement percentage (%) for four different training methods on the LLaMA-2-7B model.  These methods are: Vanilla (baseline), Vanilla with the early exiting strategy DoLA, Full Parameter Fine-Tuning (FT) with DoLA, and LISA with DoLA. The results show that incorporating LISA with DoLA yields the highest improvement in performance.", "section": "A.5 Performance with Early Exiting"}, {"figure_path": "L8ifDX5XNq/tables/tables_23_1.jpg", "caption": "Table 1: The chart illustrates peak GPU memory consumption for various model architectures and configurations, highlighting differences across models. The LISA configuration is specifically labeled in the table: \"E\" denotes the embedding layer, \u201cH\u201d represents the language modeling head layer, and \"2L\" indicates two additional intermediate layers. *: Model parallelism is applied for the 70B model.", "description": "This table compares the peak GPU memory usage of different LLMs (GPT2-Small, TinyLlama, Mistral-7B, LLaMA-2-7B, and LLaMA-2-70B) under various fine-tuning methods: Vanilla (full parameter training), LoRA with different ranks, and LISA with different numbers of activated layers (embedding, head, and additional intermediate layers).  It highlights how LISA can significantly reduce memory consumption compared to full parameter training and LoRA, especially for larger models.", "section": "4 Experimental Results"}, {"figure_path": "L8ifDX5XNq/tables/tables_23_2.jpg", "caption": "Table 1: The chart illustrates peak GPU memory consumption for various model architectures and configurations, highlighting differences across models. The LISA configuration is specifically labeled in the table: \u201cE\u201d denotes the embedding layer, \u201cH\u201d represents the language modeling head layer, and \u201c2L\u201d indicates two additional intermediate layers. *: Model parallelism is applied for the 70B model.", "description": "This table compares the peak GPU memory usage for different large language models (LLMs) under various fine-tuning configurations.  The models include GPT2-Small, TinyLlama, Mistral-7B, LLaMA-2-7B, and LLaMA-2-70B. Each model is evaluated under several configurations: Vanilla (full parameter training), LoRA (Low-Rank Adaptation) with different ranks (128, 256, 512), and LISA (Layerwise Importance Sampled AdamW) with varying numbers of activated layers (embedding and head only, embedding, head and 2 intermediate layers, embedding, head and 4 intermediate layers).  The table shows that LISA generally requires less memory than LoRA and, in some cases, even less than full parameter training, demonstrating its memory efficiency.", "section": "4 Experimental Results"}, {"figure_path": "L8ifDX5XNq/tables/tables_24_1.jpg", "caption": "Table 16: The hyperparameter search identified optimal settings for each method: FP (Full Parameter Training), LoRA, GaLore, and LISA.", "description": "This table presents the hyperparameters used for each of the four fine-tuning methods (Full Parameter Training, LoRA, GaLore, and LISA) across five different language models (GPT2-Small, TinyLlama, Mistral-7B, LLaMA-2-7B, and LLaMA-2-70B).  For each model and method, the table shows the learning rate (lr) used, as well as the rank (for LoRA) and the number of sampling layers (\u03b3) and sampling period (K) (for LISA).  These hyperparameters were determined through a hyperparameter search to identify the optimal settings for each method and model.", "section": "B Training Setup and Hyperparameters"}]