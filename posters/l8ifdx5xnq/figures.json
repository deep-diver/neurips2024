[{"figure_path": "L8ifDX5XNq/figures/figures_1_1.jpg", "caption": "Figure 6: Loss curves for LoRA, LISA, and full-parameter training on the Alpaca-GPT4 dataset across different models.", "description": "This figure compares the training loss curves of four different methods: LoRA, LISA, full parameter fine-tuning (FT), and GaLore, across four different language models (TinyLlama, GPT2-Small, Mistral-7B, and LLaMA-2-70B).  The x-axis represents the training step, and the y-axis represents the loss. The figure aims to show the convergence behavior and performance of each method relative to each other and full fine-tuning.  By comparing the loss curves across these different models, the figure provides a visual representation of the relative efficiency and effectiveness of LISA compared to other parameter-efficient fine-tuning techniques and full fine-tuning in different model settings.", "section": "A Additional Experiments"}, {"figure_path": "L8ifDX5XNq/figures/figures_3_1.jpg", "caption": "Figure 2: Layer-wise weight norms during training of GPT2 and LLaMA-2-7B Model with LoRA and Full Parameters training.", "description": "This figure shows the mean weight norms of each layer during training with LoRA and full parameter training for GPT-2 and LLaMA-2-7B models.  The x-axis represents the layer, from the embedding layer to the final layer. The y-axis shows the mean weight norm of each layer.  The figure highlights a key observation from the paper:  LoRA training shows a skewed distribution of weight norms, with the embedding and output layers having significantly larger norms than the intermediate layers. This is in contrast to full parameter training, where the weight norm distribution is more even across layers.", "section": "3 Method"}, {"figure_path": "L8ifDX5XNq/figures/figures_4_1.jpg", "caption": "Figure 3: GPU memory consumption of LLaMA-2-7B with different methods and batch size 1.", "description": "This figure compares the GPU memory consumption of different optimization methods for training the LLaMA-2-7B model with a batch size of 1.  The methods compared are the baseline (full parameter training), LoRA, and the proposed LISA method.  The chart is a stacked bar chart showing the breakdown of memory usage into four categories: Weight Memory, Activation Memory, Gradient Memory, and Optimizer Memory for each method.  A horizontal dashed line indicates the 24GB memory capacity of a single GPU. The figure visually demonstrates that LISA achieves comparable memory efficiency to LoRA and significantly less memory usage compared to the baseline.", "section": "4.1 Memory Efficiency"}, {"figure_path": "L8ifDX5XNq/figures/figures_5_1.jpg", "caption": "Figure 4: Single-iteration time cost of LLaMA-2-7B with different methods and batch size 1.", "description": "This figure shows a bar chart comparing the single-iteration time cost for three different optimization methods: Baseline (full parameter training), LoRA, and LISA.  The chart is broken down into forward and backward pass times.  LISA shows a significant reduction in the total iteration time (2.9 times faster than baseline), mostly due to less backward pass time, demonstrating its superior efficiency. The LoRA method also shows some speed improvement compared to the baseline method.", "section": "4.2 Moderate Scale Fine-Tuning"}, {"figure_path": "L8ifDX5XNq/figures/figures_18_1.jpg", "caption": "Figure 5: Generated images using LoRA (left) and LISA (right) on Stable Diffusion v2.1 model and Stable Diffusion v1.5. First row: number of inference step = 2. Second row: number of inference step = 10.", "description": "This figure compares image generation results using two different methods, LoRA and LISA, on two versions of the Stable Diffusion model.  The top row shows images generated after 2 inference steps, while the bottom row shows images generated after 10 inference steps. The comparison highlights the difference in image quality and detail between LoRA and LISA, showcasing LISA's ability to generate higher-quality images with more intricate details and sharper clarity, particularly evident in facial features and environmental textures, even with fewer inference steps.", "section": "A.1 Image Generation"}, {"figure_path": "L8ifDX5XNq/figures/figures_19_1.jpg", "caption": "Figure 6: Loss curves for LoRA, LISA, and full-parameter training on the Alpaca-GPT4 dataset across different models.", "description": "This figure displays the training loss curves for four different language models (TinyLlama, GPT2-Small, Mistral-7B, and LLaMA-2-70B) using three different fine-tuning methods: LoRA, LISA, and full parameter fine-tuning. Each curve represents the average loss over multiple training steps, showing how the loss changes over time during training.  The plots allow a comparison of the convergence speed and final loss achieved by each method for each model, highlighting the relative performance of LISA compared to LoRA and full parameter fine-tuning.", "section": "A Additional Experiments"}, {"figure_path": "L8ifDX5XNq/figures/figures_19_2.jpg", "caption": "Figure 7: The comparison of full parameter (FT) training and LISA with different sampling layers under continual pre-training scenario. The accuracy is the test set of GSM8K.", "description": "This figure shows the results of continual pre-training experiments using the LLaMA-2-7B model.  The accuracy on the GSM8K test set is plotted against the number of activated layers in the LISA method. The results are compared to the accuracy achieved with full parameter training (FT).  The graph demonstrates how increasing the number of activated layers in LISA generally improves accuracy, eventually surpassing the performance of full parameter training.", "section": "A.3 Continual Pre-training"}, {"figure_path": "L8ifDX5XNq/figures/figures_20_1.jpg", "caption": "Figure 8: Comparison of loss curves for the \u03b3 ablation experiment.", "description": "The figure shows the impact of varying the number of sampling layers (\u03b3) on the training loss during the fine-tuning of a language model. Three different values of \u03b3 (2, 4, and 8) are compared, with each corresponding to a different line on the plot. The plot shows that a higher value of \u03b3 generally leads to a lower loss, indicating that more sampling layers lead to better model performance. However, the differences in loss are relatively small.", "section": "A.4 Ablation Experiments"}, {"figure_path": "L8ifDX5XNq/figures/figures_21_1.jpg", "caption": "Figure 9: Comparison of loss curves for the sampling period K ablation experiment.", "description": "This figure shows the effects of varying the sampling period K (K=122, K=25, K=13) on the training loss for a 7B-sized model using the 52K-entry Alpaca-GPT4 dataset.  The loss curves for different sampling periods are plotted against the number of training steps (0-122).  While the loss curves vary initially, they show similar convergence behavior, suggesting that the optimal sampling period K for this model and dataset might be around 13.", "section": "A.4.2 Sampling Period K"}, {"figure_path": "L8ifDX5XNq/figures/figures_21_2.jpg", "caption": "Figure 10: Comparison of loss curves for random variance ablation experiment, indicating the loss metric over steps.", "description": "This figure shows the results of an ablation study on the impact of randomness in layer selection during the training process using LISA. Three separate training runs were performed with different random seeds for selecting the layers to be updated.  The plot displays the loss values over a series of training steps for each of the three runs.  The purpose is to demonstrate the robustness and consistency of LISA's performance even with different random layer selections.", "section": "A.4.3 Sensitiveness to Randomness"}, {"figure_path": "L8ifDX5XNq/figures/figures_22_1.jpg", "caption": "Figure 11: Validation loss comparison on the Alpaca-GPT4 dataset for LLaMA-2-7B, showing LISA, GaLore, LoRA, and FT strategies, with arrows indicating specific observations in the loss trends.", "description": "This figure compares the validation loss curves of four different fine-tuning methods: Full Parameter Fine-Tuning (FT), LoRA, GaLore, and the proposed LISA method, for the LLaMA-2-7B model trained on the Alpaca-GPT4 dataset. The x-axis represents the training steps, and the y-axis shows the validation loss.  The arrows highlight key points in the curves, illustrating the relative performance and convergence behavior of each method. LISA demonstrates comparable or even better performance compared to other methods, with a potentially more stable convergence.", "section": "A.6 Comparison of Evaluation Loss"}, {"figure_path": "L8ifDX5XNq/figures/figures_23_1.jpg", "caption": "Figure 12: Layer-wise weight norms during training of Mistral-7B with LoRA and Full Parameters training.", "description": "This figure shows the weight norms of different layers in the Mistral-7B model during training with LoRA and full parameter training.  The x-axis represents the layer, starting from the embedding layer and ending at the linear layer. The y-axis represents the weight norm of that layer.  The figure demonstrates a similar trend observed in other models (like Llama-2):  with full parameter training the weight norms for most layers show similar values. With LoRA training however, the weight norms are significantly higher for the embedding layer and the last layer (lm_head).  The observation that the weight norms are concentrated in the embedding and output layers when using LoRA is a key observation that motivates the LISA algorithm.", "section": "A.7 Additional Observations of Layerwise Skewness"}]