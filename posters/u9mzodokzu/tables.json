[{"figure_path": "U9MzoDOKZu/tables/tables_6_1.jpg", "caption": "Table 1: Few-shot test returns of Meta-DT against baselines using Medium datasets.", "description": "This table presents the few-shot test results comparing Meta-DT's performance against several baseline methods on various MuJoCo and Meta-World benchmark environments.  The results are averaged over multiple test tasks, using medium-quality datasets collected with a medium-performing policy.  The table highlights the superior performance of Meta-DT in terms of the average return achieved.  Lower values represent worse performance in these tasks.", "section": "5 Experiments"}, {"figure_path": "U9MzoDOKZu/tables/tables_7_1.jpg", "caption": "Table 2: Zero-shot test returns of Meta-DT against baselines using Medium datasets. The \u2193 denotes the performance drop compared to the few-shot setting.", "description": "This table presents the results of a zero-shot generalization experiment on various benchmark tasks using Medium datasets.  It compares the performance of Meta-DT against four other offline meta-RL algorithms (Prompt-DT, Generalized DT, CORRO, and CSRO). The table shows the average return for each algorithm on each task, and also indicates the percentage decrease in performance compared to the results obtained in the few-shot setting (shown in Table 1).  The zero-shot setting means that no expert demonstrations or domain knowledge are available during test time.  The results illustrate the zero-shot generalization capacity of Meta-DT, showcasing its ability to adapt to new tasks with minimal information.", "section": "5.2 Zero-shot Generalization Performance"}, {"figure_path": "U9MzoDOKZu/tables/tables_8_1.jpg", "caption": "Table 3: Test returns of Meta-DT ablations using Medium datasets. w/o_context removes task representation, w/o_com removes the complementary way, and w/o_prompt removes the prompt.", "description": "This table presents the results of an ablation study conducted on the Meta-DT model.  It shows the impact of removing different components of the model (context, complementary prompt generation method, and the prompt itself) on performance across three different environments: Point-Robot, Cheetah-Dir, and Ant-Dir. The results are reported as mean \u00b1 standard deviation of the reward obtained in each environment, with the Medium dataset used for evaluation.", "section": "5.3 Ablation Study"}, {"figure_path": "U9MzoDOKZu/tables/tables_9_1.jpg", "caption": "Table 1: Few-shot test returns of Meta-DT against baselines using Medium datasets.", "description": "This table presents the few-shot test results of Meta-DT and four baseline algorithms across six different environments, using medium-quality datasets.  For each environment, the mean test return and standard deviation are reported for each algorithm.  This allows for a quantitative comparison of Meta-DT's performance relative to the state-of-the-art in offline meta-reinforcement learning in a few-shot setting.  The use of medium-quality datasets emphasizes the practical applicability of the proposed method. ", "section": "5.1 Few-shot Generalization Performance"}, {"figure_path": "U9MzoDOKZu/tables/tables_17_1.jpg", "caption": "Table 5: Hyperparameters of SAC used to collect multi-task datasets.", "description": "This table lists the hyperparameters used for training the Soft Actor-Critic (SAC) agents to collect the offline datasets for each environment.  The hyperparameters include training steps, warmup steps, save frequency, learning rate, soft update rate, discount factor, and entropy ratio.  The table shows the specific hyperparameters for each of the environments used in the paper.", "section": "Appendix B. The Details of Environments and Dataset Construction"}, {"figure_path": "U9MzoDOKZu/tables/tables_20_1.jpg", "caption": "Table 6: The network configurations used for Meta-DT.", "description": "This table details the hyperparameters used in the Meta-DT model architecture. It shows the dimensions of various components, such as GRU hidden layers, task representation, decoder hidden layers, and embedding dimensions, along with the activation function used (ReLU).  The table is divided into two sections: World Model and Causal Transformer, reflecting the two main components of the model.", "section": "Appendix D. Implementation Details of Meta-DT"}, {"figure_path": "U9MzoDOKZu/tables/tables_20_2.jpg", "caption": "Table 7: Hyperparameters of Meta-DT on Point-Robot and MuJoCo domains with various datasets.", "description": "This table shows the hyperparameters used for training the Meta-DT model on the Point-Robot and MuJoCo environments. The hyperparameters are separated for three types of datasets (Medium, Expert, and Mixed). Each dataset type has different training steps, sequence length, context horizon, learning rate, and prompt length, indicating variations in the training configurations tailored to different data characteristics.", "section": "5. Experiments"}, {"figure_path": "U9MzoDOKZu/tables/tables_20_3.jpg", "caption": "Table 8: Hyperparameters of Meta-DT trained on Meta-World domains with Medium datasets.", "description": "This table lists the hyperparameters used for training the Meta-DT model on the Meta-World benchmark using Medium datasets.  It shows the specific settings for each of the three environments: Reach, Sweep, and Door-Lock. The hyperparameters include training steps, sequence length, context horizon, learning rate, and prompt length.", "section": "5. Experiments"}, {"figure_path": "U9MzoDOKZu/tables/tables_21_1.jpg", "caption": "Table 9: Few-shot test returns averaged over test tasks of Meta-DT with different values of context horizon h using Medium datasets.", "description": "This table presents the few-shot test returns of the Meta-DT model with different context horizon values (h=4, 6, 8) using medium datasets. The results are averaged over multiple test tasks for each environment (Point-Robot, Cheetah-Dir, and Ant-Dir). The table shows how the performance of the model varies with the context horizon. A larger context horizon might capture more context information for better generalization, but it could also lead to overfitting. The goal is to find a context horizon value that gives the best performance in different environments.", "section": "5.3 Ablation Study"}, {"figure_path": "U9MzoDOKZu/tables/tables_21_2.jpg", "caption": "Table 10: Few-shot test returns of Meta-DT with different prompt length k using Medium datasets.", "description": "This table presents the few-shot test returns achieved by the Meta-DT model across three different environments (Point-Robot, Cheetah-Dir, Ant-Dir) using Medium datasets. The results are presented for three different prompt lengths (k=3, 5, 7). The table shows how the model's performance varies with changes in prompt length and across different environments, demonstrating the impact of this hyperparameter on generalization capacity.", "section": "5.3 Ablation Study"}, {"figure_path": "U9MzoDOKZu/tables/tables_22_1.jpg", "caption": "Table 1: Few-shot test returns of Meta-DT against baselines using Medium datasets.", "description": "This table presents the few-shot test results comparing Meta-DT's performance against other baselines across various tasks.  The results are averaged over multiple test tasks using medium difficulty datasets and indicate the mean return achieved along with the standard deviation.  The tasks encompass diverse control challenges including locomotion and manipulation.", "section": "5.1 Few-shot Generalization Performance"}, {"figure_path": "U9MzoDOKZu/tables/tables_23_1.jpg", "caption": "Table 12: Few-shot test returns of Meta-DT and baselines on Meta-World using Medium datasets.", "description": "This table presents the few-shot test results of Meta-DT and four baseline algorithms (Prompt-DT, Generalized DT, CORRO, and CSRO) on three Meta-World benchmark tasks (Reach, Sweep, and Door-Lock).  The results are presented as the mean \u00b1 standard deviation of the episodic return obtained using Medium datasets.  The table highlights the superior performance of Meta-DT compared to the baselines across all three tasks. ", "section": "5. Experiments"}, {"figure_path": "U9MzoDOKZu/tables/tables_23_2.jpg", "caption": "Table 13: Few-shot returns of OOD test tasks using Medium datasets from Ant-Dir.", "description": "This table presents the few-shot test returns achieved by different offline meta-RL methods on out-of-distribution (OOD) tasks from the Ant-Dir environment within the Meta-World benchmark. The results showcase the generalization performance of each method when the test tasks are outside the distribution of the training tasks, highlighting the ability to extrapolate knowledge to unseen scenarios.", "section": "5.2 Zero-shot Generalization Performance"}]