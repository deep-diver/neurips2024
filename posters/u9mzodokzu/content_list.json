[{"type": "text", "text": "Meta-DT: Offline Meta-RL as Conditional Sequence Modeling with World Model Disentanglement ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhi Wang1 Li Zhang1 Wenhao $\\mathbf{W}\\mathbf{u}^{1}$ Yuanheng Zhu2 Dongbin Zhao2 Chunlin Chen1\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1Nanjing University 2Institution of Automation, Chinese Academy of Sciences {zhiwang, clchen}@nju.edu.cn {lizhang, whao_wu}@smail.nju.edu.cn {yuanheng.zhu, dongbin.zhao}@ia.ac.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A longstanding goal of artificial general intelligence is highly capable generalists that can learn from diverse experiences and generalize to unseen tasks. The language and vision communities have seen remarkable progress toward this trend by scaling up transformer-based models trained on massive datasets, while reinforcement learning (RL) agents still suffer from poor generalization capacity under such paradigms. To tackle this challenge, we propose Meta Decision Transformer (Meta-DT), which leverages the sequential modeling ability of the transformer architecture and robust task representation learning via world model disentanglement to achieve efficient generalization in offilne meta-RL. We pretrain a context-aware world model to learn a compact task representation, and inject it as a contextual condition to the causal transformer to guide task-oriented sequence generation. Then, we subtly utilize history trajectories generated by the meta-policy as a self-guided prompt to exploit the architectural inductive bias. We select the trajectory segment that yields the largest prediction error on the pretrained world model to construct the prompt, aiming to encode task-specific information complementary to the world model maximally. Notably, the proposed framework eliminates the requirement of any expert demonstration or domain knowledge at test time. Experimental results on MuJoCo and Meta-World benchmarks across various dataset types show that Meta-DT exhibits superior few and zero-shot generalization capacity compared to strong baselines while being more practical with fewer prerequisites. Our code is available at https://github.com/NJU-RL/Meta-DT. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Building generalist models that solve various tasks by training on vast task-agnostic datasets has emerged as a dominant paradigm in the language [1] and vision [2] communities. Offline reinforcement learning (RL) [3] allows learning an optimal policy from trajectories collected by some behavior policies without access to the environments, which holds tremendous promise for turning massive datasets into powerful generic decision-making engines [4, 5, 6], akin to the rise of large language models (LLMs) like GPT [7]. A performant example is the decision transformer (DT) [8] that leverages the transformer\u2019s strong sequence modeling capability for trajectory data and achieves promising results on offline RL [9, 10, 11, 12]. ", "page_idx": 0}, {"type": "text", "text": "Transformer-based large models have shown remarkable scaling properties and high transferability across various domains, including language modeling [13, 7], computer vision [14, 15], and image generation [16, 17]. However, their counterparts in RL are usually specialized to a narrowly defined task and struggle with poor generalization to unseen tasks, due to distribution shift and the lack of self-supervised pretraining techniques [18, 19, 20]. To promote generalization, Prompt-DT [21] uses expert demonstrations as a high-quality prompt to encode task-specific information. Generalized DT [22] conditions on some hindsight information, e.g., statistics of the reward distribution, to match task-specific feature distributions. These works usually rely on domain knowledge at test time, e.g., expert demonstrations or hindsight statistics, which is expensive and even infeasible to acquire in advance for unseen tasks [23]. The aforementioned limitations raise a key question: Can we design an offilne meta-RL framework to achieve efficient generalization to unseen tasks while drawing upon advances in the sequence modeling paradigm with the scalable transformer architecture? ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In offilne RL, the collected dataset depends on the task and behavior policy. When behavior policies are highly correlated with tasks in the training dataset, the agent tends to memorize the features of behavior policies and produce biased task inference at test time due to the change of behavior policies [24, 25]. One major challenge for generalization is how to accurately encode task-relevant information for extrapolating knowledge across tasks, while minimizing the requirement on the distribution of pre-collected data and behavior policies. RL agents typically learn through active interaction with the environment, and the transition dynamics $\\bar{p(r,s^{\\prime}|\\bar{s},\\bar{a})}$ completely describes the characteristics of the underlying environment [26]. The environment dynamics, also called world model [27, 28], is intrinsically invariant to behavior policies or collected datasets, thus presenting a promising alternative for robustly encoding task beliefs. By capturing compact representations of the environment, world models carry the potential for substantial transfer between tasks [29], continual learning [30], and generalization from offline datasets [31, 32]. ", "page_idx": 1}, {"type": "text", "text": "Inspired by this, we propose a novel framework for offilne meta-RL, named Meta-DT that leverages robust task representation learning via world model disentanglement to conduct task-oriented sequence generation for efficient generalization. First, we pretrain a context-aware world model to capture task-relevant information from the multi-task offilne dataset. The world model contains an encoder that abstracts dynamics-specific information into a compact task representation, and a decoder that predicts the reward and state transition functions conditioned on that representation. Second, we inject the task representation as a contextual label to the transformer to guide task-conditioned sequence generation. In this way, the autoregressive model learns to estimate the conditional output of multi-task distributions and achieve desired returns based on the task-oriented context. Finally, we leverage the past trajectories generated by the meta-policy as a self-guided prompt to exploit the architecture inductive bias, akin to Prompt-DT [21]. We feed available trajectory segments to the pretrained world model and choose the segment with the largest prediction error to construct the prompt, aiming to encode task-specific information complementary to the world model maximally. ", "page_idx": 1}, {"type": "text", "text": "In summary, our main contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Generalization Ability. We propose Meta-DT to leverage the conditional sequence modeling paradigm with the scalable transformer architecture to achieve efficient generalization across unseen tasks without any expert demonstration or domain knowledge at test time. \u2022 Context-Aware World Model. We introduce a context-aware world model to learn a compact task representation capable of generalizing across a distribution of varying environment dynamics. \u2022 Complementary Prompt. We design a self-guided prompt that encodes task-specific information complementary to the world model maximally, harnessing the architectural inductive bias. \u2022 Superior Performance. Experiments on various benchmarks show that Meta-DT exhibits higher few and zero-shot generalization capacity, while being more practical with fewer prerequisites. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Offilne Meta-RL. Offilne RL [33] allows learning optimal policies from pre-collected offilne datasets without online interactions with the environment [34, 35]. Offilne meta-RL learns to generalize to new tasks via training on a distribution of such offilne tasks [25, 36]. Optimization-based meta-learning methods [37, 38] seek policy parameter initialization that requires only a few adaptation steps to new tasks. MACAW [39] introduces this architecture into value-based RL subroutines, and uses simple supervised regression objectives for sample-efficient offline meta-training. On the other hand, context-based meta-learning methods learn a context encoder to perform approximate inference on task representations, and condition the meta-policy on the approximate belief for generalization, such as PEARL [40], VariBAD [41], FOCAL [42], CORRO [24], CSRO [25], and UNICORN [43]. These works extended from the online meta-RL setting are mainly trained by temporal difference learning, the dominant paradigm in RL [9, 44]. This paradigm might be prone to instabilities due to function approximation, off-policy learning, and bootstrapping, together known as the deadly triad [26]. Moreover, many of these works rely on hand-designed heuristics to keep the policy within the offline dataset distribution [45]. This motivates us to turn to harness the lens of conditional sequence modeling with the transformer architecture to scale existing offline meta-RL algorithms. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "RL as Sequence Modeling. The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws with the transformer architecture, such as GPT [7], ViT [14], and DiT [46]. Decision transformer [8] first introduces the transformer\u2019s sequence modeling capacity to solving offilne RL without temporal difference learning, which autoregressively outputs optimal actions conditioned on desired returns. As a concurrent study, trajectory transformer [47] directly models distributions over sequences of states, actions, and rewards, followed by beam search as a planning algorithm in a model-based manner. Extending the LLM-like structure to RL, this paradigm activates a new pathway toward scaling powerful RL engines with large-scale compute and data [48]. ", "page_idx": 2}, {"type": "text", "text": "For multi-task learning, Gato [49] trains a multi-modal generalist policy in the GPT style with a decoder-only transformer. Multi-game DT [4] trains a suite of up to 46 Atari games simultaneously, and controls policy generation at inference time with a pretrained classifier that indicates the expert level of behaviors. For generalization to unseen tasks, Prompt-DT [21] exploits a prefix prompt architecture and Generalized DT [22] designs a hindsight information matching structure to encoder task-specific information. These works usually require domain knowledge at test time, such as expert demonstrations or hindsight statistics. $\\mathrm{T^{3}}$ GDT [50] conditions action generation on three-tier guidance of global transitions, local relevant experiences, and vectorized time embedding. CMT [18] provides a pretrain and prompt-tuning paradigm where a task prompt is extracted from offline trajectories with an encoder-only transformer to realize few-shot generalization. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Offline Meta Reinforcement Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "RL is generally formulated as a Markov decision process (MDP) $M{=}\\langle S,\\mathcal{A},T,R,\\gamma\\rangle$ , where ${\\mathcal{S}}/A$ is the state/action space, $T/R$ is the state transition/reward function, and $\\gamma$ is the discount. The objective is to find a policy $\\pi(a|s)$ to maximize the expected return as $\\begin{array}{r}{\\operatorname*{max}_{\\pi}J_{M}(\\pi)\\!=\\!\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R(\\mathring{s}_{t},a_{t})\\right]}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": ". In offilne meta-RL, we assume that the task follows a distribution $M_{i}=\\langle S,A,T_{i},R_{i},\\gamma\\rangle\\sim P(M),$ , where tasks share the same state-action spaces while vary in the reward and state transition functions, i.e., environment dynamics. For each task $i$ from a total of $N$ training tasks $\\{M_{i}\\}_{i=1}^{N}$ , an offilne dataset $\\mathcal{D}_{i}\\!=\\!\\{(s_{i,j},a_{i,j},r_{i,j},s_{i,j}^{\\prime})\\}_{j=1}^{J}$ is collected by an arbitrary behavior policy $\\pi_{\\beta}^{i}$ . The agent can only access the offilne datasets $\\{\\mathcal{D}_{i}\\}_{i=1}^{N}$ to train a meta-policy $\\pi_{\\mathrm{meta}}$ . At test time with the few-shot setting, given an unseen task $M\\sim P(M)$ , the agent can access a small dataset $\\mathcal D=\\{(\\underline{{s_{j}}},a_{j},r_{j},s_{j}^{\\prime})\\}_{j=1}^{J^{\\prime}}$ to construct the task prompt or infer the task belief before policy evaluation. For the zero-shot setting, the trained meta-policy is directly evaluated on the unseen task by interacting with the test environment to estimate the expected return. The objective is to learn a meta-policy to maximize the expected episodic return over test tasks as $J(\\pi_{\\mathrm{meta}})\\overset{\\cdot}{=}\\mathbb{E}_{M\\sim P(M)}\\left[J_{M}(\\pi_{\\mathrm{meta}})\\right]$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Decision Transformer ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "By leveraging the superiority of the attention mechanism [13] in extracting dependencies between sequences, transformers have gained substantial popularity in the language and vision communities [7, 14]. Decision transformer [8] introduces associated advances to solve RL problems and re-frames offilne RL as a return-conditioned sequential modeling problem, inheriting the transformer\u2019s efficiency and scalability when modeling long trajectory data. DT models the probability of the next sequence token $x_{t}$ conditioned on all tokens prior to it: $P_{\\theta}(x_{t}|x_{<t})$ , similar to contemporary decoder-only sequence models [51]. The sequence we consider has the form: $\\boldsymbol{x}=(\\cdots,\\hat{R}_{t},s_{t},a_{t},\\cdot\\cdot\\cdot)$ , where $\\hat{R}_{t}$ is the agent\u2019s target return for the rest of the trajectory. Such a sequence order respects the causal structure of the decision process. When training with offilne data, $\\bar{\\hat{R}}_{t}\\!=\\!\\sum_{i=t}^{T}r_{i}$ , and during testing, $\\textstyle\\hat{R}_{t}\\!=\\!G^{*}\\!-\\!\\sum_{i=0}^{t}r_{i}$ , where where $G^{*}$ is the target return for an entire episode. The same timestep embedding is concatenated to the embeddings of $\\hat{R}_{t},\\,s_{t}$ , and $a_{t}$ , and the head corresponding to the state token is trained to predict the action by minimizing its error from the ground truth. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overbrace{\\left(\\underbrace{\\int_{i\\overrightarrow{S}-\\overrightarrow{S}-\\overrightarrow{r}\\cdot\\overrightarrow{t}}_{i}\\cdot\\Gamma(\\mathrm{ask}\\mathrm{~pata}\\mathrm{~pata})}_{\\overbrace{i,j},\\ldots,\\ldots,\\ldots,\\ldots,\\ldots,i\\ldots}\\right)}^{\\,\\,\\,a\\ast}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Figure 1: The overview of Meta-DT. We pretrain a context-aware world model to accurately disentangle task-specific information. It contains a context encoder $E_{\\psi}$ that abstracts recent $h$ -step history $\\mu_{t}^{i}$ into a compact task representation $\\ensuremath{\\boldsymbol{z}}_{t}^{i}$ , and the generalized decoders $(R_{\\phi},T_{\\varphi})$ that predict the reward and next state conditioned on $z_{t}^{i}$ . Then, the inferred task representation is injected as a contextual condition to the causal transformer to guide task-oriented sequence generation. Finally, we design a self-guided prompt from history trajectories generated by the meta-policy at test time. We select the trajectory segment that yields the largest prediction error on the pretrained world model, aiming to encode task-relevant information complementary to the world model maximally. ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present Meta Decision Transformer (Meta-DT), a novel offilne meta-RL framework that draws upon advances in conditional sequence modeling with robust task representation learning and a complementary prompt design for efficient generalization across unseen tasks. Fig. 1 illustrates the method overview, and the following subsections present the detailed implementations. ", "page_idx": 3}, {"type": "text", "text": "4.1 Context-Aware World Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The key to efficient generalization is how to accurately encode task-relevant information for extrapolating knowledge across tasks, which proves more challenging in RL. A supervised or unsupervised learner is passively presented with a fixed dataset and aims to recognize the pattern within that dataset. In contrast, RL typically learns from active interactions with the environment, and aims at an optimal policy that receives the maximal return in the environment, rather than only recognizing the pattern within the pre-collected dataset. Since the offilne dataset depends on both the task and the behavior policy, the task information could be entangled with the features of behavior policies, thus producing biased task inference at test time due to the shift in behavior policies. Taking 2D navigation as an example, tasks differ in goals and the behavior policy is going towards the goal for each task. The algorithm can easily distinguish tasks based on state-action distributions rather than reward functions, leading to extrapolating errors when the behavior policy shifts to random exploration during testing. ", "page_idx": 3}, {"type": "text", "text": "To address this intrinsic challenge, we ought to disentangle task-relevant information from behavior policies. The transition dynamics, i.e., the reward and state transition functions $p(s^{\\prime},r|s,a)$ , completely describes the characteristics of the underlying environment. Naturally, this environment dynamics, also called world model, keeps invariant to behavior policies or collected datasets, and could be a promising alternative for accurately inferring task beliefs. Therefore, we introduce a context-aware world model to learn robust task representations that can generalize to unseen environments with varying transition dynamics. ", "page_idx": 3}, {"type": "text", "text": "In the typical meta-learning setting, the reward and state transition functions that are unique to each MDP are unknown, but also share some common structure across the task distribution $\\bar{P(M)}$ . There exists a true variable that represents either a task description or task identity, but we do not have access to this information. Hence, we use a latent representation $z$ to approximate that variable. For a given task $M_{i}$ , the reward and state transition functions can be approximated by a generalized context-aware world model $W$ that is shared across tasks as ", "page_idx": 3}, {"type": "equation", "text": "$$\nW_{i}(r_{t},s_{t+1}|s_{t},a_{t})\\approx W(r_{t},s_{t+1}|s_{t},a_{t};z_{t}^{i}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Since we do not have access to the true task description or identity, we need to infer task representation $z_{t}^{i}$ given the agent\u2019s $h$ -step experience up to timestep $t$ collected in task $M_{i}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mu_{t}^{i}=(s_{t-h},a_{t-h},r_{t-h},...,s_{t-1},a_{t-1},r_{t-1},s_{t}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The intuition is that the true task belief of the underlying MDP can be abstracted from the agent\u2019s recent experiences, analogous to recent studies in meta-RL [41, 36]. ", "page_idx": 4}, {"type": "text", "text": "We separate the reasoning about the world model into two parts: i) encoding the dynamics-specific information into a latent task representation, and ii) decoding the environment dynamics conditioned on that representation. First, we use a simple yet effective context encoder $E_{\\psi}$ to embed recent experiences into a task representation as $z_{t}^{i}\\!=\\!E_{\\psi}(\\mu_{t}^{i})$ . Second, the decoder contains a generalized reward model $R_{\\phi}$ and state transition model $T_{\\varphi}$ . The task representation is augmented into the input of both models to predict the instant reward $\\hat{r}_{t+1}\\!=\\!R_{\\phi}\\big(s_{t},a_{t};z_{t}^{i}\\big)$ and next state $\\hat{s}_{t+1}\\!=\\!T_{\\varphi}\\big(s_{t},a_{t};z_{t}^{i}\\big)$ . Under the assumption that tasks with similar contexts will behave similarily [52, 53], the proposed world model can extrapolate the meta-level knowledge across tasks by accurately capturing taskrelevant information from training environments. The context encoder is jointly trained by minimizing the reward and state transition prediction error conditioned on the task representation as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\psi,\\phi,\\varphi)=\\mathbb{E}_{\\mu_{t}^{i}\\sim\\mathcal{D}_{i}}\\Big[\\mathbb{E}_{z_{t}^{i}\\sim E_{\\psi}(\\mu_{t}^{i})}\\Big[\\big(r_{t+1}-R_{\\phi}(s_{t},a_{t};z_{t}^{i})\\big)^{2}+\\big(s_{t+1}-T_{\\varphi}(s_{t},a_{t};z_{t}^{i})\\big)^{2}\\Big]\\Big].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4.2 Complementary Prompt ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Recent works suggest the prompt framework as an effective paradigm for pretraining transformerbased large models on massive datasets and adapting them to new scenarios with few or no labeled data [54]. Prompt-DT [21] adopts that paradigm to RL problems by prepending a task prompt to the DT\u2019s input. At test time, the agent is assumed to access a handful of expert demonstrations to construct the prompt and perform few-shot generalization to new tasks. However, in real-world scenarios, it is expensive and even infeasible to acquire such domain knowledge as expert demonstrations in advance for unseen tasks. Especially in RL, agents typically learn from interacting with an initially unknown environment. Recent studies [36, 23] also suggest that the quality of demonstrations must be high enough to act as a well-constructed prompt. Otherwise, the performance of Prompt-DT may degrade since some medium or random data can easily disrupt the abstraction of task-specific information. ", "page_idx": 4}, {"type": "text", "text": "Here, we design a self-guided prompt to achieve more realistic generalization at test time. In the ideal case where the testing policy is optimal, past experiences during evaluation can act as high-quality demonstrations to construct the prompt. Motivated by this, we leverage history trajectories generated by the meta-policy during evaluation as a self-guided prompt to enjoy the power of architecture inductive bias, while eliminating the dependency on expensive domain knowledge. Though, the meta-policy may generate medium or even inferior data during initial training. As meta-learning proceeds, the policy can exhibit increasing generalization capacity via the context-aware world model, thus generating trajectories that gradually approach expert demonstrations. ", "page_idx": 4}, {"type": "text", "text": "Both the world model (algorithmic perspective) and the self-guided prompt (architecture perspective) aim to extract task-specific information to guide policy generation across tasks. To facilitate collaboration between these two parts, we force the prompt to maximally complement the world model towards the same goal. We feed all available segments selected from history trajectories to the pretrained world model, and use the segment with the largest prediction error to construct the prompt. In this way, the complementary prompt attempts to encode the portion of task-relevant information that the world model struggles to capture effectively. ", "page_idx": 4}, {"type": "text", "text": "Formally, the complementary prompt is a sequence segment containing multiple tuples, $(\\hat{R}^{*},s^{*},a^{*})$ , which are sampled from the agent\u2019s history trajectories. Each element with superscript $^*$ is associated with the prompt. For a given task $M_{i}$ , we obtain a $k$ -step prompt $\\tau_{i}^{*}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau_{i}^{*}=\\left(\\hat{R}_{j}^{*},s_{j}^{*},a_{j}^{*},\\hat{R}_{j+1}^{*},s_{j+1}^{*},a_{j+1}^{*},...,\\hat{R}_{j+k}^{*},s_{j+k}^{*},a_{j+k}^{*}\\right),}\\\\ &{j=\\operatorname*{max}_{j}\\sum_{t=j}^{j+k}\\left[\\left(r_{t+1}-R_{\\phi}\\big(s_{t},a_{t};z_{t}^{i}\\big)\\right)^{2}+\\big(s_{t+1}-T_{\\varphi}\\big(s_{t},a_{t};z_{t}^{i}\\big)\\big)^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Following Prompt-DT, we choose a much smaller value of $k$ than the task horizon, ensuring that the prompt only contains the information needed to help identify the task but insufficient information for the agent to imitate. Compared to the world model that explicitly learns the reward and transition functions, the complementary prompt can store partial information to implicitly capture task dynamics. ", "page_idx": 4}, {"type": "text", "text": "4.3 Meta-DT Architecture ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our Meta-DT architecture is built on decision transformer [8] and solves the offilne meta-RL problem through the lens of conditional sequence modeling with robust task representation learning. We first pretrain the context-aware world model and keep it fixed to produce compact task representations for the downstream meta-training process. For each task $M_{i}$ , we use the context encoder from the pretrained world model to infer the contextual information for each timestep as $z_{t}^{i}=E_{\\psi}(\\mu_{t}^{i})$ . Then, the input of Meta-DT consists of two parts: i) the $k$ -step complementary prompt $\\tau_{i}^{*}$ derived from (4), and ii) the most recent $K$ -step history $\\tau_{i}^{+}$ augmented by the learned task representations as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tau_{i}^{+}=(z_{t-K+1}^{i},\\hat{R}_{t-K+1},s_{t-K+1},a_{t-K+1},...,z_{t}^{i},\\hat{R}_{t},s_{t},a_{t})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The input sequence $(\\tau_{i}^{*},\\tau_{i}^{+})$ corresponds to $3k+4K$ tokens in the transformer, and Meta-DT autoregressively outputs $k+K$ actions at heads corresponding to state tokens in the input sequence. During training, we construct the prompt from the top few trajectories that obtain the highest returns in the offilne dataset. Meta-DT minimizes errors between the predicted and real actions in the data for the $K$ -step history. At test time with a few-shot setting, the meta-policy is allowed to interact with the environment for a few episodes, and Meta-DT utilizes these history interactions to construct the self-guided prompt. For the zero-shot setting, we ablate the prompt component and directly evaluate Meta-DT on test tasks. Corresponding algorithm pseudocodes are given in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We comprehensively evaluate the generalization capacity of Meta-DT on popular benchmarking domains across various dataset types. In general, we aim to answer the following questions: ", "page_idx": 5}, {"type": "text", "text": "\u2022 Can Meta-DT achieve consistent performance gain on the few and zero-shot generalization to unseen tasks compared with other strong baselines? (Secs. 5.1 and 5.2) \u2022 How do the context-aware world model, the self-guided prompt design, and the complementary prompt construction affect the generalization performance, respectively? (Sec. 5.3) \u2022 Is Meta-DT robust to the quality of offline datasets? (Sec. 5.4) ", "page_idx": 5}, {"type": "text", "text": "Environments. We evaluate all tested methods on three classical benchmarks in meta-RL: i) the 2D navigation environment Point-Robot [25]; ii) the multi-task MuJoCo control [55, 36], containing Cheetah-Vel, Cheetah-Dir, Ant-Dir, Hopper-Param, and Walker-Param; and iii) the MetaWorld manipulation platform [56], including Reach, Sweep, and Door-Lock. For each environment, we randomly sample a distribution of tasks and divide them into the training set $\\mathcal{T}^{\\mathrm{train}}$ and test set $\\mathcal{T}^{\\mathrm{test}}$ . On each training task, we use SAC [57] to train a single-task policy independently for collecting datasets. We consider three ways to construct offline datasets: Medium, Mixed, and Expert. More details about environments and datasets are given in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Baselines. We compare Meta-DT to four competitive baselines that cover two distinctive paradigms in offline meta-RL: the DT-based 1) Prompt-DT [21], 2) Generalized DT [22], and the temporal difference-based 3) CORRO [24], 4) CSRO [25]. More details about baselines are given in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "All evaluated methods are carried out with 5 different random seeds, and the mean of the received return is plotted with $95\\%$ bootstrapped confidence intervals of the mean (shaded). The standard errors are presented for numerical results. Appendix D gives implementation details of Meta-DT. Appendix E presents hyperparameter analysis on the context horizon $h$ , the prompt length $k$ , and the number of training tasks. Appendix F shows experimental results on Meta-World domains. ", "page_idx": 5}, {"type": "text", "text": "5.1 Few-shot Generalization Performance ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Note that these baselines work under the few-shot setting, since they require expert demonstrations as task prompts or some warm-start data to infer the task representation. We first compare MetaDT to them under an aligned few-shot setting, where each method can leverage a fixed number of trajectories for task inference. Fig. 2 and Table 1 present the testing curves and converged performance of Meta-DT and baselines on various domains using Medium datasets under an aligned few-shot setting. In these environments with varying reward functions or state transition dynamics, Meta-DT consistently obtains superior performance regarding data efficiency and final asymptotic results. A noteworthy point is that Meta-DT outperforms baselines by a larger margin in relatively complex environments like Ant-Dir, which demonstrates the high generalization capacity of Meta-DT when tackling challenging problems. Moreover, Meta-DT generally exhibits a lower variance during learning, signifying not only superior learning efficiency but also enhanced training stability. ", "page_idx": 5}, {"type": "image", "img_path": "U9MzoDOKZu/tmp/b1d12af771fd6236922ddb113f880a34c13ddfa3ceba830afa2deafeadbac439.jpg", "img_caption": ["Figure 2: The received return curves averaged over test tasks of Meta-DT and baselines using Medium datasets under an aligned few-shot setting. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "U9MzoDOKZu/tmp/e904118e0e40a9ceb58f9061879eb6da926e5e0058c8d7a72eac547780821170.jpg", "table_caption": ["Table 1: Few-shot test returns of Meta-DT against baselines using Medium datasets. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5.2 Zero-shot Generalization Performance ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Since Meta-DT can derive real-time task representations $z_{t}$ from its $h$ -step experience $\\mu_{t}$ via the pretrained context encoder, it can achieve zero-shot policy adaptation to unseen tasks. To demonstrate its zero-shot generalization ability, we ablate the prompt component and directly evaluate Meta-DT on test tasks. For a fair comparison, we modify the baselines to an aligned zero-shot setting, where prompt or warm-start data is inaccessible before policy evaluation. All methods can only use samples generated by the trained meta-policy during evaluation to construct task prompts (Prompt-DT), calculate hindsight statistics (Generalized DT), or infer task representations (CORRO and CSRO). ", "page_idx": 6}, {"type": "text", "text": "Fig. 3 and Table 2 present the testing curves and converged performance of Meta-DT and baselines on various domains using Medium datasets under an aligned zero-shot setting. Unsurprisingly, most baselines exhibit a collapsed performance (about $20\\%.35\\%$ drop on average) compared to their fewshot counterparts. The main reason is that they usually require expert demonstrations as task prompts or some high-quality warmup data to accurately capture task information. In contrast, Meta-DT can abstract compact task presentations via the context-aware world model, and its performance in zero-shot scenarios drops merely a little (about $5\\%$ on average) compared to the few-shot counterpart. The superiority of Meta-DT over all baselines is more pronounced when deployed to zero-shot adaptation scenarios. In addition to its enhanced generalization capacity, this result also demonstrates the superior practicability of Meta-DT with fewer prerequisites in real-world applications. ", "page_idx": 6}, {"type": "image", "img_path": "U9MzoDOKZu/tmp/88caa24731b234e67c582122a4d75579856a60ea3e05d17283a424bf88f38c79.jpg", "img_caption": ["Figure 3: The received return curves averaged over test tasks of Meta-DT and baselines using Medium datasets under an aligned zero-shot setting. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: Zero-shot test returns of Meta-DT against baselines using Medium datasets. The $\\downarrow$ denotes the performance drop compared to the few-shot setting. ", "page_idx": 7}, {"type": "table", "img_path": "U9MzoDOKZu/tmp/2615ab6b1df64c0544ac4adbac054a40cb1717d4235a57c194e0dddddd8cbe8b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To test the respective contributions of each component, we compare Meta-DT to four ablations: i) w/o_context, it only removes the task representation $\\ensuremath{\\boldsymbol{z}}_{t}^{i}$ from the input of the casual transformer; ii) $\\mathtt{w}/0\\_\\mathtt{c o m}$ , it randomly chooses a segment from a candidate trajectory to construct the prompt, i.e., removing the complementary way for constructing the prompt; iii) $_{\\mathtt{w/o_{-}p r o m p t}}$ , it directly removes the prompt component; iv) DT, it removes all components and degrades to the original DT. For ablations, all other structural modules are kept consistent strictly with the full Meta-DT. ", "page_idx": 7}, {"type": "text", "text": "Fig. 4 and Table 3 show ablation results on representative domains using Medium datasets. First, Meta-DT obtains a decreased performance when any component is removed, which demonstrates that all components are essential for Meta-DT\u2019s capability and they complement each other. Second, ablating the context incurs a more significant performance drop than ablating the complementary way or the whole prompt. It indicates that task representation learning via the world model plays a more vital role in capturing task-relevant information. Third, the performance order of w/o_prompt $<\\mathrm{w}/\\mathrm{o\\_com}<$ Meta-DT shows that using the self-guided prompt can improve the performance, and leveraging the complementary way to construct the prompt can further achieve a performance gain. Another interesting point is that the performance gain achieved by the self-guided prompt and the complementary way is more significant in complex environments like Ant-Dir than in simpler ones like Point-Robot. It again verifies our superiority in tackling challenging tasks. ", "page_idx": 7}, {"type": "text", "text": "5.4 Robustness to the Quality of Offline Datasets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "A good offline algorithm ought to be robust to different types of datasets that involve a wide range of behavioral policies [58]. To test how Meta-DT performs with data of different qualities, we also conduct experiments on the Mixed and Expert datasets. Figs. 5-6 present test return curves of ", "page_idx": 7}, {"type": "image", "img_path": "U9MzoDOKZu/tmp/e0d884d86ba8f51f75133bbb71334b0eff150e4cc00af1a88edeea94957d5171.jpg", "img_caption": ["Figure 4: Test return curves of Meta-DT ablations using Medium datasets. w/o_context removes task representation, w/o_com removes the complementary way, and w/o_prompt removes the prompt. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "U9MzoDOKZu/tmp/bcda5615d67415f78d981867faa7b3e94dbdd626a9886ea0ee1f7f77c1940e0f.jpg", "table_caption": ["Table 3: Test returns of Meta-DT ablations using Medium datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "U9MzoDOKZu/tmp/058a2692ec35a31f99b0573a0c4771ca33afdef50428c8e7db0feb0c832d6be8.jpg", "img_caption": ["Figure 5: Few-shot test curves of Meta-DT and baselines using Mixed datasets. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Meta-DT and baselines with a few-shot setting, and Table 4 shows corresponding converged results. In Mixed datasets, Meta-DT still outperforms all baselines by a large margin, especially in complex environments like Ant-Dir and Walker-Param. ", "page_idx": 8}, {"type": "text", "text": "In Expert datasets, Meta-DT exhibits superior generalization capacity than the three baselines of Generalized DT, CORRO, and CSRO. Compared to Prompt-DT, Meta-DT obtains significantly better performance in Point-Robot, Cheetah-Vel, and Ant-Dir, and obtains comparable performance in the other three environments. This phenomenon is because Prompt-DT is sensitive to the quality of prompt demonstrations. The performance can drop a lot when provided with prompts from Medium or Mixed datasets, also mentioned in the original paper [21] and subsequent studies [36]. Hence, Prompt-DT can achieve satisfactory performance only when expert demonstrations are available at test time. When tackling offilne tasks with lower data qualities, Prompt-DT would induce a significant performance gap compared to Meta-DT. In summary, the above results verify that our method is robust to the dataset quality and is more practical with fewer prerequisites in real-world scenarios. ", "page_idx": 8}, {"type": "image", "img_path": "U9MzoDOKZu/tmp/a96e9fc09abe9a901def37f65c5be0a75a389112c358442e7003ddfc2cf46ade.jpg", "img_caption": ["Figure 6: Few-shot test curves of Meta-DT and baselines using Expert datasets. "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "U9MzoDOKZu/tmp/05d2ab9e91941e2be2596c1813521ff2694ffaa68f06cb34f9a20389113306bb.jpg", "table_caption": ["Table 4: Few-shot test returns of Meta-DT against baselines using Mixed and Expert datasets. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusions, Limitations, and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In the paper, we tackle the offilne meta-RL challenge via leveraging advances in sequence modeling with scalable transformers, marking a step toward developing highly capable generalists akin to those in the language and vision communities. Improvements in few and zero-shot generalization capacities highlight the potential impact of our robust task representation learning and self-guided complementary prompt design. Without requiring any expert demonstration or domain knowledge at test time, our method exhibits superior practicability with fewer prerequisites in real-world scenarios. ", "page_idx": 9}, {"type": "text", "text": "Though, our method requires a two-phase process of pre-training the world model and training the causal transformer. A promising improvement is to develop a unified framework that simultaneously abstracts the task representation and learns the meta-policy, akin to in-context learning [59]. Also, our generalist model is trained on relatively lightweight datasets compared to popular large models. A crucial future step is to deploy our model on significantly larger datasets, unlocking the scaling law with the transformer architecture. This also aligns with the urgent trend that RL practitioners are striving to break through. Another interesting line is to leverage self-supervised learning [15, 60] to facilitate task representation learning at scale. We leave these as future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Zican Hu and Zhenhong Sun for their helpful discussions. This work was supported by the National Natural Science Foundation of China (Nos. 62376122 & 62073160), the Nanjing University Integrated Research Platform of the Ministry of Education-Top Talents Program, the Beijing Natural Science Foundation under Grant 4232056, the Beijing Nova Program under Grant 20240484514, and the Youth Innovation Promotion Association CAS under Grant 2021132. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the North American Chapter of the Association for Computational Linguistics, pages 4171\u20134186, 2019.   \n[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u02c7ci\u00b4c, and Cordelia Schmid. ViViT: A video vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6836\u20136846, 2021.   \n[3] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In Proceedings of International Conference on Machine Learning, pages 2052\u20132062, 2019.   \n[4] Kuang-Huei Lee, Ofir Nachum, Mengjiao Sherry Yang, Lisa Lee, Daniel Freeman, Sergio Guadarrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk Michalewski, et al. Multi-game decision transformers. In Advances in Neural Information Processing Systems, volume 35, pages 27921\u201327936, 2022.   \n[5] Thomas Schmied, Markus Hofmarcher, Fabian Paischer, Razvan Pascanu, and Sepp Hochreiter. Learning to modulate pre-trained models in rl. In Advances in Neural Information Processing Systems, 2023.   \n[6] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: A multi-task transformer for robotic manipulation. In Proceedings of Conference on Robot Learning, pages 785\u2013799, 2023.   \n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901, 2020.   \n[8] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In Advances in Neural Information Processing Systems, volume 34, pages 15084\u201315097, 2021.   \n[9] David Brandfonbrener, Alberto Bietti, Jacob Buckman, Romain Laroche, and Joan Bruna. When does return-conditioned supervised learning work for offline reinforcement learning? In Advances in Neural Information Processing Systems, volume 35, pages 1542\u20131553, 2022.   \n[10] Yueh-Hua Wu, Xiaolong Wang, and Masashi Hamaya. Elastic decision transformer. In Advances in Neural Information Processing Systems, 2023.   \n[11] Anirudhan Badrinath, Yannis Flet-Berliac, Allen Nie, and Emma Brunskill. Waypoint transformer: Reinforcement learning via supervised learning with intermediate targets. In Advances in Neural Information Processing Systems, volume 36, 2023.   \n[12] Prajjwal Bhargava, Rohan Chitnis, Alborz Geramifard, Shagun Sodhani, and Amy Zhang. When should we prefer decision transformers for offline reinforcement learning? In Proceedings of International Conference on Learning Representations, 2024.   \n[13] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998\u20136008, 2017.   \n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Proceedings of International Conference on Learning Representations, 2021.   \n[15] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000\u201316009, 2022.   \n[16] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Proceedings of International Conference on Machine Learning, pages 8821\u20138831, 2021.   \n[17] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, et al. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research, 2022.   \n[18] Runji Lin, Ye Li, Xidong Feng, Zhaowei Zhang, Xian Hong Wu Fung, Haifeng Zhang, Jun Wang, Yali Du, and Yaodong Yang. Contextual transformer for offline meta reinforcement learning. In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022.   \n[19] Shengchao Hu, Li Shen, Ya Zhang, and Dacheng Tao. Prompt-tuning decision transformer with preference ranking. arXiv preprint arXiv:2305.09648, 2023.   \n[20] Zhihui Xie, Zichuan Lin, Deheng Ye, Qiang Fu, Yang Wei, and Shuai Li. Future-conditioned unsupervised pretraining for decision transformer. In Proceedings of International Conference on Machine Learning, pages 38187\u201338203, 2023.   \n[21] Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and Chuang Gan. Prompting decision transformer for few-shot policy generalization. In Proceedings of International Conference on Machine Learning, volume 162, pages 24631\u201324645, 2022.   \n[22] Hiroki Furuta, Yutaka Matsuo, and Shixiang Shane Gu. Generalized decision transformer for offilne hindsight information matching. In Proceedings of International Conference on Learning Representations, 2022.   \n[23] Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang, Dong Wang, Bin Zhao, and Xuelong Li. Diffusion model is an effective planner and data synthesizer for multi-task reinforcement learning. In Advances in Neural Information Processing Systems, 2023.   \n[24] Haoqi Yuan and Zongqing Lu. Robust task representations for offline meta-reinforcement learning via contrastive learning. In Proceedings of International Conference on Machine Learning, pages 25747\u201325759, 2022.   \n[25] Yunkai Gao, Rui Zhang, Jiaming Guo, Fan Wu, Qi Yi, Shaohui Peng, Siming Lan, Ruizhi Chen, Zidong Du, Xing Hu, et al. Context shift reduction for offilne meta-reinforcement learning. In Advances in Neural Information Processing Systems, volume 36, 2023.   \n[26] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018.   \n[27] David Ha and J\u00fcrgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.   \n[28] Tim Brooks, Bill Peebles, Connor Homes, Will DePue, Yufei Guo, et al. Video generation models as world simulators. 2024.   \n[29] Arunkumar Byravan, Jost Tobias Springenberg, Abbas Abdolmaleki, Roland Hafner, Michael Neunert, Thomas Lampe, Noah Siegel, Nicolas Heess, and Martin Riedmiller. Imagined value gradients: Model-based policy optimization with tranferable latent dynamics models. In Proceedings of the Conference on Robot Learning, volume 100, pages 566\u2013589, 2020.   \n[30] Samuel Kessler, Mateusz Ostaszewski, Micha\u0142 Pawe\u0142Bortkiewicz, Mateusz \u02d9Zarski, Maciej Wolczyk, Jack Parker-Holder, Stephen J. Roberts, and Piotr Mi\u00b4s. The effectiveness of world models for continual reinforcement learning. In Proceedings of Conference on Lifelong Learning Agents, volume 232, pages 184\u2013204, 2023.   \n[31] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. In Advances in Neural Information Processing Systems, volume 34, pages 28954\u201328967, 2021.   \n[32] Zohar Rimon, Tom Jurgenson, Orr Krupnik, Gilad Adler, and Aviv Tamar. MAMBA: an effective world model approach for meta-reinforcement learning. In Proceedings of International Conference on Learning Representations, 2024.   \n[33] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.   \n[34] Alexander Nikulin, Vladislav Kurenkov, Denis Tarasov, and Sergey Kolesnikov. Antiexploration by random network distillation. In Proceedings of International Conference on Machine Learning, 2023.   \n[35] Yuhang Ran, Yi-Chen Li, Fuxiang Zhang, Zongzhang Zhang, and Yang Yu. Policy regularization with dataset constraint for offline reinforcement learning. In Proceedings of International Conference on Machine Learning, 2023.   \n[36] Fei Ni, Jianye Hao, Yao Mu, Yifu Yuan, Yan Zheng, Bin Wang, and Zhixuan Liang. MetaDiffuser: Diffusion model as conditional planner for offilne meta-rl. In Proceedings of International Conference on Machine Learning, 2023.   \n[37] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of International conference on machine learning, pages 1126\u20131135, 2017.   \n[38] Zhongwen Xu, Hado P van Hasselt, and David Silver. Meta-gradient reinforcement learning. In Advances in Neural Information Processing Systems, volume 31, 2018.   \n[39] Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. Offline metareinforcement learning with advantage weighting. In Proceedings of International Conference on Machine Learning, pages 7780\u20137791, 2021.   \n[40] Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy meta-reinforcement learning via probabilistic context variables. In Proceedings of International Conference on Machine Learning, pages 5331\u20135340, 2019.   \n[41] Luisa Zintgraf, Sebastian Schulze, Cong Lu, Leo Feng, Maximilian Igl, Kyriacos Shiarlis, Yarin Gal, Katja Hofmann, and Shimon Whiteson. VariBAD: Variational Bayes-adaptive deep RL via meta-learning. The Journal of Machine Learning Research, 22(1):13198\u201313236, 2021.   \n[42] Lanqing Li, Rui Yang, and Dijun Luo. FOCAL: Efficient fully-offline meta-reinforcement learning via distance metric learning and behavior regularization. In Proceedings of International Conference on Learning Representations, 2021.   \n[43] Lanqing Li, Hai Zhang, Xinyu Zhang, Shatong Zhu, Junqiao Zhao, and Pheng-Ann Heng. Towards an information theoretic framework of context-based offline meta-reinforcement learning. arXiv preprint arXiv:2402.02429, 2024.   \n[44] Zhi Wang, Chunlin Chen, and Daoyi Dong. Lifelong incremental reinforcement learning with online Bayesian inference. IEEE Transactions on Neural Networks and Learning Systems, 33(8):4003\u20134016, 2022.   \n[45] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision-making? In Proceedings of International Conference on Learning Representations, 2023.   \n[46] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023.   \n[47] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. In Advances in Neural Information Processing Systems, volume 34, pages 1273\u20131286, 2021.   \n[48] Linghui Meng, Muning Wen, Chenyang Le, Xiyun Li, Dengpeng Xing, Weinan Zhang, Ying Wen, Haifeng Zhang, Jun Wang, Yaodong Yang, and Bo Xu. Offline pre-trained multi-agent decision transformer. Machine Intelligence Research, 20(2):233\u2013248, 2023.   \n[49] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio G\u00f3mez Colmenarejo, Alexander Novikov, Gabriel Barth-maron, Mai Gim\u00e9nez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. Transactions on Machine Learning Research, 2022.   \n[50] Zhe Wang, Haozhu Wang, and Yanjun (Jane) Qi. T3GDT: Three-tier tokens to guide decision transformer for offline meta reinforcement learning. In NeurIPS 2023 Workshop on Robot Learning, 2023.   \n[51] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, et al. PaLM: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1\u2013113, 2023.   \n[52] Aditya Modi, Nan Jiang, Satinder Singh, and Ambuj Tewari. Markov decision processes with continuous side information. In Proceedings of Algorithmic Learning Theory, pages 597\u2013618, 2018.   \n[53] Kimin Lee, Younggyo Seo, Seunghyun Lee, Honglak Lee, and Jinwoo Shin. Context-aware dynamics model for generalization in model-based reinforcement learning. In Proceedings of International Conference on Machine Learning, pages 5757\u20135766, 2020.   \n[54] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023.   \n[55] Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based control. In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033, 2012.   \n[56] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-World: A benchmark and evaluation for multi-task and meta reinforcement learning. In Proceedings of Conference on Robot Learning, pages 1094\u20131100, 2020.   \n[57] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of International Conference on Machine Learning, pages 1861\u20131870, 2018.   \n[58] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.   \n[59] Jake Grigsby, Linxi Fan, and Yuke Zhu. AMAGO: Scalable in-context reinforcement learning for adaptive agents. In Proceedings of International Conference on Learning Representations, 2024.   \n[60] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix A. Algorithm Pesudocodes ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Based on the implementations in Sec. 4, this section gives the brief procedures of our method. First, Algorithm 1 presents the pretraining of the context-aware world model. Then, Algorithm 2 shows the pipeline of training Meta-DT, where the sub-procedure of generating the complementary prompt is given in Algorithm 3. Finally, Algorithm 4 and Algorithm 5 show the few-shot and zero-shot evaluations on test tasks, respectively. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 1: Pretraining the Context-Aware World Model ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Input: Training tasks $\\mathcal{T}^{\\mathrm{train}}$ and corresponding offline datasets $\\mathcal{D}^{\\mathrm{train}}$ Context encoder $E_{\\psi}$ ; Reward decoder $R_{\\phi}$ ; Transition decoder $T_{\\varphi}$ Context horizon $h$ ; Batch size $B$   \n1 for each iteration do   \n2 for $b=1,...,B$ do   \n3 Sample a task $M_{i}\\sim\\mathcal{T}^{\\mathrm{train}}$ and obtain the corresponding dataset $\\mathcal{D}_{i}$ from $\\mathcal{D}^{\\mathrm{train}}$   \n4 Sample a whole trajectory $\\left(s_{0}^{i},a_{0}^{i},r_{0}^{i},s_{1}^{i},a_{1}^{i},r_{1}^{i},\\ldots\\right)$ from $\\mathcal{D}_{i}$   \n5 Sample a transition tuple $(s_{t},a_{t},r_{t},s_{t+1})$ with randomly selected $t$   \n6 Obtain its $h$ -step history $\\mu_{t}^{i}=\\left(s_{t-h},a_{t-h},r_{t-h},...,s_{t-1},a_{t-1},r_{t-1},s_{t}\\right)$   \n7 Compute the context $z_{t}^{i}=E_{\\psi}(\\mu_{t}^{i})$   \n8 Compute the predicted reward $\\hat{r}_{t}=R_{\\phi}(s_{t},a_{t};z_{t}^{i})$ and next state $\\hat{s}_{t+1}=T_{\\varphi}(s_{t},a_{t};z_{t}^{i})$   \n9 end   \n10 Update $E_{\\psi}$ , $R_{\\phi}$ , and $T_{\\varphi}$ using the loss as $\\begin{array}{r}{\\mathcal{L}(\\psi,\\phi,\\varphi)=\\frac{1}{B}\\sum\\left[\\left(r_{t+1}-R_{\\phi}(s_{t},a_{t};z_{t}^{i})\\right)^{2}+\\left(s_{t+1}-T_{\\varphi}(s_{t},a_{t};z_{t}^{i})\\right)^{2}\\right]}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "Algorithm 2: Training Meta Decision Transformer ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Input: Training tasks $\\mathcal{T}^{\\mathrm{train}}$ and corresponding offline datasets $\\mathcal{D}^{\\mathrm{train}}$   \nCausal transformer $F_{\\theta}$ ; Pretrained context encoder $E_{\\psi}$ and decoders $R_{\\phi},T_{\\varphi}$   \nContext horizon $h$ ; Trajectory length $K$ ; Batch size $B$   \n1 for each task $M_{i}\\in\\mathcal{T}^{t r a i n}$ do   \n2 Construct the demo dataset $\\mathcal{D}_{i}^{\\mathrm{demo}}$ using the top few trajectories with the highest returns in $\\mathcal{D}_{i}$   \n3 end   \n4 for each iteration do   \n5 for $b=1,2,...,B$ do   \n6 Sample a task $M_{i}\\sim\\mathcal{T}^{\\mathrm{train}}$ and obtain the corresponding dataset $\\mathcal{D}_{i}$ from $\\mathcal{D}^{\\mathrm{train}}$   \n7 Sample a trajectory $\\tau_{i}$ of length $K$ from $\\mathcal{D}_{i}$ , $\\tau_{i}=\\{(\\hat{R},s,a)\\}_{t-K+1}^{t}$   \n8 Infer the context of each timestep $t$ from its $h$ -step history using the pretrained context   \nencoder as $z_{t}=E_{\\psi}(s_{t-h},a_{t-h},r_{t-h},...,s_{t-1},a_{t-1},r_{t-1},s_{t})$   \n9 Augment the trajectory $\\tau_{i}$ with per-step context as $\\tau_{i}^{+}=\\{(z,\\hat{R},s,a)\\}_{t-K+1}^{t}$   \n10 Sample a prompt \u03c4 i\u2217 = GetPrompt $\\left(\\mathcal{D}_{i}^{\\mathrm{demo}},E_{\\psi},R_{\\phi},T_{\\varphi}\\right)$ as in Algorithm 3   \n11 Get input \u03c4 iin $\\tau_{i}^{\\mathrm{input}}=(\\tau_{i}^{*},\\tau_{i}^{+})$   \n2 end   \n3 $a^{\\mathrm{pred}}=F_{\\theta}(\\tau_{i}^{\\mathrm{input}})$ $\\boldsymbol{B}=\\{\\tau_{i}^{\\mathrm{input}}\\}_{b=1}^{B}$ \u03c4 iinput\u2208B   \n5 $\\begin{array}{r}{\\mathcal L(\\theta)=\\frac{1}{B}\\sum_{\\tau_{i}^{\\mathrm{input}}\\in\\mathcal B}(a-a^{\\mathrm{pred}})^{2}}\\end{array}$   \n6 Few or Zero-Shot Evaluation along with training   \n17 end ", "page_idx": 14}, {"type": "text", "text": "Algorithm 3: Complementary Prompt Generation (GetPrompt) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Input: Demo Dataset $\\mathcal{D}_{i}^{\\mathrm{demo}}$ ; Prompt length $k$ Pretrained context encoder $E_{\\psi}$ and decoders $R_{\\phi},T_{\\varphi}$ 1 Sample a trajectory $\\left(s_{0},a_{0},r_{0},...,s_{t},a_{t},r_{t},...\\right)$ from $\\mathcal{D}_{i}^{\\mathrm{demo}}$ 2 Get the segment with the largest prediction error on the pretrained world model as $\\begin{array}{r l}&{j=\\operatorname*{max}_{j}\\sum_{t=j}^{j+k}\\left[\\left(r_{t+1}-R_{\\phi}(s_{t},a_{t};z_{t}^{i})\\right)^{2}+\\left(s_{t+1}-T_{\\varphi}(s_{t},a_{t};z_{t}^{i})\\right)^{2}\\right]\\mathrm{,where~}z_{t}^{i}=}\\\\ &{\\mathrm{2btain~the~}k\\mathrm{-step~prompt~}\\tau_{i}^{*}=\\left(\\hat{R}_{j}^{*},s_{j}^{*},a_{j}^{*},\\hat{R}_{j+1}^{*},s_{j+1}^{*},a_{j+1}^{*},...,\\hat{R}_{j+k}^{*},s_{j+k}^{*},a_{j+k}^{*}\\right)}\\end{array}$ $z_{t}^{i}=E_{\\psi}(\\tau_{t}^{i})$ 3 ", "page_idx": 15}, {"type": "text", "text": "Algorithm 4: Meta-DT Few-Shot Evaluation on Test Tasks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Input: Test tasks $\\mathcal{T}^{\\mathrm{test}}$ ; Target return $G^{*}$ ; Evaluation episodes $N$ Context horizon $h$ ; Trajectory length $K$ ; Prompt length $k$ Trained meta-DT $F_{\\theta}$ ; Pretrained context encoder $E_{\\psi}$ and decoders $R_{\\phi},T_{\\varphi}$ 1 for each task $M_{i}\\in\\mathcal{T}^{t e s t}$ do 2 Initialize the demo dataset $\\mathcal{D}_{i}^{\\mathrm{demo}}=\\mathcal{D}$ 3 for $n=1,...,N$ do 4 Initialize desired return $\\hat{R}=G_{i}^{*}$ 5 for each timestep $t$ do 6 Infer the context from $h$ -step history using $E_{\\psi}$ as $z_{t}=E_{\\psi}(s_{t-H+1},a_{t-H+1},r_{t-H+1},...,s_{t-1},a_{t-1},r_{t-1},s_{t})$ 7 Augment the trajectory $\\tau_{i}$ with context as $\\tau_{i}^{+}=\\{(z,\\hat{R},s,a)\\}_{t-K+1}^{t}$ 8 Sample a prompt \u03c4 i\u2217 = GetPrompt $\\left(\\mathcal{D}_{i}^{\\mathrm{demo}},E_{\\psi},R_{\\phi},T_{\\varphi}\\right)$ as in Algorithm 3 9 Get action $a=F_{\\theta}\\big((\\tau_{i}^{+},\\tau_{i}^{*})\\big)$ 10 Step env. and get feedback $s,a,r,\\hat{R}\\gets\\hat{R}-r$ 11 Append $({\\hat{R}},s,a)$ to recent history $\\tau_{i}$ 12 end 13 Append the whole trajectory $\\tau_{i}$ to the demo dataset as $\\mathcal{D}_{i}^{\\mathrm{demo}}\\leftarrow\\mathcal{D}_{i}^{\\mathrm{demo}}\\cup\\tau_{i}$ 14 end 15 end ", "page_idx": 15}, {"type": "text", "text": "Algorithm 5: Meta-DT Zero-Shot Evaluation on Test Tasks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Input: Test tasks $\\mathcal{T}^{\\mathrm{test}}$ ; Target return $G^{*}$ Context horizon $h$ ; Trajectory length $K$ Trained meta-DT $F_{\\theta}$ ; Pretrained context encoder $E_{\\psi}$ 1 for each task $M_{i}\\in\\mathcal{T}^{t e s t}$ do 2 Initialize desired return ${\\hat{R}}=G_{i}^{*}$ 3 for each timestep $t$ do 4 Infer the context from $h$ -step history using $E_{\\psi}$ as zt = E\u03c8(st\u2212H+1, at\u2212H+1, rt\u2212H+1, ..., st\u22121, at\u22121, rt\u22121, st) 5 Augment the trajectory $\\tau_{i}$ with context as $\\tau_{i}^{+}=\\{(z,\\hat{R},s,a)\\}_{t-K+1}^{t}$ 6 Get action $a=F_{\\theta}(\\tau_{i}^{+})$ 7 Step env. and get feedback $s,a,r,\\hat{R}\\gets\\hat{R}-r$ 8 Append $({\\hat{R}},s,a)$ to recent history $\\tau_{i}$ 9 end 10 end ", "page_idx": 15}, {"type": "text", "text": "Appendix B. The Details of Environments and Dataset Construction ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we show details of evaluation environments over a variety of testbeds, as well as the offline dataset collection process conducted on these environments. ", "page_idx": 16}, {"type": "text", "text": "B.1. The Details of Environments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Following the mainstream studies in offilne meta-RL [39, 24], we adopt three classical benchmarks: the 2D navigation [25], the multi-task MuJoCo control [55, 36], and the Meta-World [56]. We evaluate all tested methods on the following environments as ", "page_idx": 16}, {"type": "text", "text": "\u2022 Point-Robot: a problem of a point agent navigating to a given goal position in the 2D space. The observation is the 2D coordinate of the robot and the goal is not included in the observation. The action space is $[-0.1,0.1]^{2}$ with each dimension corresponding to the moving distance in the horizontal and vertical directions. The reward function is defined as the negative distance between the point agent and the goal location. Each learning episode always starts from the fixed origin and terminates at the horizon of 20. Tasks differ in goal positions that are uniformly distributed in a unit square, resulting in the variation of the reward functions. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Cheetah-Vel, Cheetah-Dir, Ant-Dir: multi-task MuJoCo continuous control benchmarks in which the reward functions differ across tasks. Cheetah-Vel requires a planar cheetah robot to run at a particular velocity in the positive $x$ -direction, and the reward function is negatively correlated with the absolute value between the current velocity of the agent and a goal. Cheetah-Dir/Ant-Dir is to control a 2D cheetah/3D quadruped ant robot to move in a specific direction, and the reward function is the cosine product of the agent\u2019s velocity and the goal direction. The goal velocity and direction are uniformly sampled from the distribution $U[0.075,3.0]$ for Cheetah-Vel and $U[0,2\\pi]$ for Ant-Dir. In Cheetah-Dir, the goal directions are limited to forward and backward. The maximal episode step is set to 200. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Hopper-Param, Walker-Param: multi-task MuJoCo benchmarks where tasks differ in state transition dynamics. The two benchmarks control a one-legged hopper and a two-legged walker robot to run as fast as possible. The reward function is proportional to the running velocity in the positive $x$ -direction, which remains consistent for different tasks. In both domains, the physical parameters of body mass, inertia, damping, and friction are randomized across tasks. The agent needs to move forward with varying environment dynamics to accomplish the task. The maximal episode step is set to 200 for both. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Reach, Sweep, Door-Lock: three typical environments from the robotic manipulation benchmark Meta-World. Reach, Sweep, and Door-Lock control a robotic arm to reach a goal location in 3D space, to sweep a puck off the table, and to lock the door by rotating the lock clockwise, respectively. Tasks are assigned with random goal object positions for Reach, with random puck positions for Sweep, and with random door positions for Door-Lock. The variations in goal/puck/door positions are not provided in the observation, forcing meta-RL algorithms to adapt to the goal through trial-and-error. The maximal episode step is set to 500. ", "page_idx": 16}, {"type": "text", "text": "For the Point-Robot and MuJoCo environments, we sample 45 tasks for training and another 5 held-out tasks for testing. For Meta-World environments, we sample 15 tasks for training and 5 held-out tasks for testing. ", "page_idx": 16}, {"type": "text", "text": "B.2. The Details of Datasets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We employ the soft actor-critic (SAC) [57] algorithm to train a policy independently for each task. Table 5 shows the detailed hyperparameters for training the SAC agents in each environment. Figs. 7-8 show the learning curves of independently training the SAC agents for each training and testing task in all evaluation domains. During training, we periodically save the policy checkpoints to generate various types of offline datasets as ", "page_idx": 16}, {"type": "text", "text": "\u2022 Medium: using a medium policy that achieves a one-third to one-second score of an expert policy. All trajectories are generated by this medium policy. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Expert: using an expert policy that obtains the optimal return in the environment. In practice, we load the last policy checkpoint after the training has converged, and use this expert policy to generate all trajectories. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Mixed: a mixed dataset of the Medium and Expert types. We use a combination of the medium $(70\\%)$ and expert $(30\\%)$ datasets collected as above to generate the mixed dataset. ", "page_idx": 17}, {"type": "text", "text": "Specifically, we generate 100 trajectories for the Point-Robot and MuJoCo environments, and 300 trajectories for the Meta-World environments, using the saved checkpoint policies to construct corresponding offline datasets. ", "page_idx": 17}, {"type": "text", "text": "B.3. Further discussions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The above environments are relatively homogenous in nature. Our generalist model is trained on relatively lightweight datasets compared to popular large models. The urgent trend that RL practitioners are striving to break through is to deploy on significantly large datasets with more diversity, unlocking the scaling law with the transformer architecture. It would be interesting to see how Meta-DT generalizes across worlds and tasks with more diversity, like training on $K$ -levels of an Atari game and generalizing to the remaining $N-K$ levels of well-suited Atari games. ", "page_idx": 17}, {"type": "text", "text": "The challenges in extending to such paradigms might include: i) how to collect large-scale datasets with sufficient diversity and high quality, ii) how to tackle the heterogenous task diversity in hard cases, and iii) how to scale RL models to very large network architectures like in large language or visual models. To tackle these potential challenges, we conjecture several promising solutions including: i) leveraging self-supervised learning to facilitate task representation learning at scale, ii) enabling efficient prompt design or prompt tuning to facilitate in-context RL, and iii) incorporating effective network architectures like mixture-of-experts to handle heterogenous domain diversity. ", "page_idx": 17}, {"type": "table", "img_path": "U9MzoDOKZu/tmp/63fb6f7096834bc9a0424960a179a0c94c2843fcf4fb216d8ee703044a3a81f8.jpg", "table_caption": ["Table 5: Hyperparameters of SAC used to collect multi-task datasets. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Appendix C. The Details of Baselines ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "This section gives the details of the representative five baselines, including three DT-based approaches, one temporal difference-based approach, and one diffusion-based approach. These baselines are thoughtfully selected to encompass the three distinctive categories of offilne meta-RL methods. Also, since our method Meta-DT belongs to the DT-based category, we adopt more approaches from this kind as our baselines. The baselines are introduced as follows: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Prompt-DT [21], is a DT-based method that leverages the sequential modeling ability of the Transformer architecture and the prompt framework to achieve few-shot adaptation in offilne RL. It designs the trajectory prompt, which contains segments of the few-shot demonstrations, and encodes task-specific information to guide policy generation. At test time, it assumes that the agent can assess a handful of expert demonstrations to construct the prompt. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Generalized DT [22], is a DT-based method that formulates multi-task learning as a hindsight information matching (HIM) problem, i.e., training policies that can output the rest of the trajectory to match some statistics of future information. It defines offline multi-task statemarginal matching and imitation learning as two generic HIM problems to evaluate the proposed Categorical DT and Bi-directional DT. ", "page_idx": 17}, {"type": "image", "img_path": "U9MzoDOKZu/tmp/388a66b0e98785aea0f775e2aac15f2599b2cfab00bcc190d92f141051ac03b2.jpg", "img_caption": ["Figure 7: Learning curves of training independent SAC agents in Point-Robot and MuJoCo domains. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "U9MzoDOKZu/tmp/84dcf57e4cee231be9fadb5611b8e2a5702d09bb18081d80b2f35b99895c7004.jpg", "img_caption": ["Figure 8: Learning curves of independently training the SAC agents for each task in Meta-World. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "\u2022 CORRO [24], is a temporal difference-based method with a contrastive learning framework to learn task representations that are robust to the distribution mismatch of behavior policies in training and testing. It formalizes the learning objective as mutual information maximization between the representation and task, to maximally eliminate the influence of behavior policies from task representations. At test time, it needs to collect a context trajectory with an arbitrary policy to infer the task representation. ", "page_idx": 19}, {"type": "text", "text": "\u2022 CSRO [25], is a temporal difference-based method that proposes a max-min mutual information representation learning framework to mitigate the distribution discrepancy between the contexts used for training (from the behavior policy) and testing (for the exploration policy). At test time, it explores the new task with a non-prior strategy and collects few-shot context data to infer the task representation. It demonstrates superior performance than prior context-based meta-RL methods. ", "page_idx": 19}, {"type": "text", "text": "Note that these baselines can only work under the few-shot setting, since they require several expert demonstrations as the task prompt or some warm-start data to infer the task representation. We first compare Meta-DT to them under an aligned few-shot setting, where each method can leverage a fixed number of trajectories for task inference. ", "page_idx": 19}, {"type": "text", "text": "Further, we demonstrate the zero-shot generalization capability of Meta-DT and baselines by directly evaluating the meta-policy on the unseen task without collecting any data in advance. For Meta-DT, we ablate the prompt component and derive real-time task representations from the history trajectory via the pre-trained context encoder. For a fair comparison, we modify the baselines to an aligned zero-shot setting, where prompt demonstrations or warm-start data are inaccessible before policy evaluation. All these baselines can only use samples generated by the trained meta-policy during evaluation to construct task prompts (Prompt-DT), calculate hindsight statistics (Generalized DT), or infer task representations (CORRO and CSRO). ", "page_idx": 19}, {"type": "text", "text": "Appendix D. Implementation Details of Meta-DT ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Network Architecture. In this paper, we use simple network structures for the context-aware world model, including the context encoder, the reward decoder, and the state transition decoder. Specifically, the context encoder contains a fully-connected multi-layer perceptron (MLP) and a GRU network with ReLU as the activation function. The GRU cell encodes the agent\u2019s $h$ -step history $\\left(s_{t-h},a_{t-h},r_{t-h},...,s_{t-1},a_{t-1},r_{t-1},s_{t}\\right)$ into a 128-dimensional vector, and the MLP transforms this vector into a 16-dimensional embedding, i.e., the task representation $z$ . The reward decoder is an MLP that takes as input the $(s,a,s^{\\prime},z)$ tuple and predicts the reward $r$ through two 128-dimensional hidden layers. Similarly, the state transition decoder is an MLP that takes as input the $(s,a,r,z)$ tuple and predicts the next state $s^{\\prime}$ through two 128-dimensional hidden layers. ", "page_idx": 19}, {"type": "text", "text": "We implement the proposed Meta-DT based on the official codebase released by DT [8] (https://github.com/kzl/decision-transformer). We follow most of the hyperparameters as they did. Specifically, task representations $z$ , returns R\u02c6, states $s$ , and actions $a$ are fed into modality-specific linear embeddings, and the same positional episodic timestep encoding is added to tokens corresponding to the same timestep. Tokens are fed into a GPT architecture which predicts actions autoregressively using a causal self-attention mask. In summary, Table 6 shows the details of network structures. ", "page_idx": 19}, {"type": "text", "text": "Algorithm Hyperparameters. We evaluate the proposed Meta-DT algorithm on various testbeds with different types of offilne datasets. Each report unit involves running on one environment (PointRobot, Cheetah-Vel, Cheetah-Dir, Ant-Dir, Hopper-Param, Walker-Param, Reach) with one dataset (Medium, Expert, Mixed). Some common hyperparameters across all report units are set as: optimizer Adam, weight decay 1e-4, linear warmup steps for learning rate decay 10000, gradient norm clip 0.25, dropout 0.1, and batch size 128. Table 7 presents the detailed hyperparameters of Meta-DT trained on the Point-Robot and MuJoCo domains with the Medium, Expert, and Mixed datasets. Table 8 presents the detailed hyperparameters of Meta-DT trained on Meta-World environments with the Medium datasets. ", "page_idx": 19}, {"type": "text", "text": "Compute. We train our models on one Nvidia RTX4080 GPU with the Intel Core i9-10900X CPU and 256G RAM. Pretraining the context-aware world model and the causal transformer costs about 0.5-6 hours and 1-8 hours, respectively, depending on the complexity of the environment. ", "page_idx": 19}, {"type": "table", "img_path": "U9MzoDOKZu/tmp/89cb4a528a1d5a4ad0ac6f90685a5040d82ce320a9a19fdf6e5c34ccb6fbffc2.jpg", "table_caption": ["Table 6: The network configurations used for Meta-DT. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "U9MzoDOKZu/tmp/1f5a5f5d6410b6e11a75f776b6c89c8815fe4922f72b84bba5b09a5c19aa0184.jpg", "table_caption": ["Table 7: Hyperparameters of Meta-DT on Point-Robot and MuJoCo domains with various datasets. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "U9MzoDOKZu/tmp/9e8a777c1021f85869edd8dd0c22b8f9bdf51da5893b3073eee6245c0b62ffcc.jpg", "table_caption": ["Table 8: Hyperparameters of Meta-DT trained on Meta-World domains with Medium datasets. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Appendix E. Hyperparameter Analysis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Context horizon $h$ . As shown by the ablation studies in Sec. 5.3, task representation learning via the world model disentanglement plays a more vital role in capturing task-relevant information in Meta-DT. The task representation $\\left\\vert z_{t}^{i}\\right\\vert$ is abstracted from the agent\u2019s $h$ -step experience $\\mu_{t}^{i}\\,=$ $\\left(s_{t-h},a_{t-h},r_{t-h},...,s_{t-1},a_{t-1},r_{t-1},s_{t}\\right)$ . The context horizon $h$ is a key hyperparameter in learning effective task representations. Hence, we conduct additional experiments to analyze the influence of context horizon $h$ on Meta-DT\u2019s performance. ", "page_idx": 20}, {"type": "text", "text": "Fig. 9 and Table 9 present the testing curves and converged performance of Meta-DT with different values of context horizon $h$ on representative environments. Generally, the performance of Meta-DT is not sensitive to the pre-defined value of the context horizon. In simple environments like PointRobot, increasing the context horizon can degrade the final performance a little bit. The most likely reason for this phenomenon is that the context-aware world model might be overfitted with a large value of context horizon. In more complex environments like Cheetah-Dir and Ant-Dir, adopting different values of context horizon leads to very close performance. Hence, throughout the paper, we choose the context horizon as 4 for our method to exploit a lightweight network design while not sacrificing performance. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "image", "img_path": "U9MzoDOKZu/tmp/820852f28224c6437a3dc848345a80eb96284b11f3c01e87cbb80a317b97c04a.jpg", "img_caption": ["Figure 9: The received return curves averaged over test tasks of Meta-DT with different values of context horizon $h$ using Medium datasets under an aligned few-shot setting. "], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "U9MzoDOKZu/tmp/c580ba8a44a281512d46dea414f4659d691deb0cf80e33cdb0e87fc2cda78f03.jpg", "table_caption": ["Table 9: Few-shot test returns averaged over test tasks of Meta-DT with different values of context horizon $h$ using Medium datasets. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Prompt length $k$ . Analogously, we also investigate the influence of prompt length $k$ on Meta-DT\u2019s performance. Fig. 10 and Table 10 present the testing curves and converged performance of Meta-DT with different values of prompt length $k$ on representative environments. Generally, the performance of Meta-DT is not sensitive to the pre-defined value of prompt length. ", "page_idx": 21}, {"type": "image", "img_path": "U9MzoDOKZu/tmp/f9053a4059cdb321b880357e80e9c490bbb3b7fe54189cbcf89b31fe8d33a8e1.jpg", "img_caption": ["Figure 10: Few-shot return curves of Meta-DT with different prompt length $k$ using Medium datasets. "], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "U9MzoDOKZu/tmp/b1ef1fafb1d3808624e5defe07cf61e0f5d82be46a1832bc5023192c8e4f9113.jpg", "table_caption": ["Table 10: Few-shot test returns of Meta-DT with different prompt length $k$ using Medium datasets. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "The number of training tasks. We evaluate Meta-DT with a different number of training tasks. Fig. 11 and Table 11 present the testing curves and converged performance of Meta-DT with different numbers of training tasks on representative environments using Medium datasets. Generally, increasing the number of training tasks can usually improve the generalization ability to test tasks, especially in harder tasks like Ant-Dir. ", "page_idx": 22}, {"type": "text", "text": "The result also matches the intuition of function approximation in machine learning. The \u201cgeneralization\u201d refers to the question: How can experience with a limited subset of the data space be usefully generalized to produce a good approximation over a much larger subset? In the single-task setting, increasing the valid sample points (before being overfitted) can generally improve the function approximation of the sample space and the generalization to unseen samples at testing. In the meta-learning setting, increasing the valid task points (before being overftited) can usually boost the function approximation of the task space and the generalization to unseen tasks at testing. ", "page_idx": 22}, {"type": "image", "img_path": "U9MzoDOKZu/tmp/f6ee00360a033b30e783af63099f1ebe88e28924dbe062ee22c3007b33d8e8f9.jpg", "img_caption": ["Figure 11: Few-shot return curves of Meta-DT with different numbers of training tasks (Medium). "], "img_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "U9MzoDOKZu/tmp/87f27304006dd47e31752d4853d7be4e3da2ed458b20ede940b6a83e737affd3.jpg", "table_caption": ["Table 11: Few-shot test returns of Meta-DT with different numbers of training tasks (Medium). "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Appendix F. Experimental Results on Meta-World ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here, we present experimental results on the robotic manipulation benchmark Meta-World [56], including three environments of Reach, Sweep, and Door-Lock. Fig. 12 and Table 12 show the testing curves and converged performance of Meta-DT and baselines using Medium datasets under an aligned few-shot setting. The results show that Meta-DT still performs better than the baselines in these challenging and realistic Meta-World environments. ", "page_idx": 22}, {"type": "image", "img_path": "U9MzoDOKZu/tmp/8bf1647f1eeff62d9ebb065e56cc3c07b4c271b0c14e594d78cef2d8edaf28a1.jpg", "img_caption": ["Figure 12: Few-shot return curves of Meta-DT and baselines on Meta-World using Medium datasets. "], "img_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "U9MzoDOKZu/tmp/9341ee4d82b926b46f9ad850a912489cf98bd834528ca504ec97365906a6bca5.jpg", "table_caption": ["Table 12: Few-shot test returns of Meta-DT and baselines on Meta-World using Medium datasets. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Appendix G. Generalization to Out-of-Distribution Tasks ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Most meta-RL studies follow the experimental setting as training in a large distribution of tasks and evaluating in a few held-out tasks. The held-out tasks have goals (target position or direction) within the goal range calibrated by training tasks. We desire to test whether Meta-DT enables knowledge extrapolation when handling tasks with goals out of the training range, i.e., the generalization ability in out-of-distribution (OOD) tasks. Hence, we conduct experiments on Ant-Dir to evaluate Meta-DT\u2019s generalization ability to OOD tasks. In this case, we sample training tasks with a goal direction of $U[0,1.5\\pi]$ and then test on tasks of $[1.5\\pi,2\\pi]$ . Fig. 13 and Table 13 show the few-shot return curves and converged performance on OOD test tasks using Medium datasets from Ant-Dir. ", "page_idx": 23}, {"type": "text", "text": "Obviously, Meta-DT can still obtain better performance than baselines on OOD test tasks, which again verifies our superiority. Our method extrapolates the meta-level knowledge across tasks by the extrapolation ability of the world model, which is more accurate and robust since the world model is intrinsically invariant to behavior policies or collected datasets. For tasks like Ant-Dir, the world model shares some common structure across the task distribution (even for OOD tasks), e.g., the kinematics principle or locomotion skills. Hence, the extrapolation of the world model also works for OOD test tasks in this case. ", "page_idx": 23}, {"type": "image", "img_path": "U9MzoDOKZu/tmp/558cd67164ded708a503ec9926958dee6f0ca642c8163332f322c924ee442c28.jpg", "img_caption": ["Figure 13: Few-shot return curves of OOD test tasks using Medium datasets from Ant-Dir. "], "img_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "U9MzoDOKZu/tmp/2647b8e5368e80c09159bc96b829edfa66a6f6671c3964916cb3123a8df8b635.jpg", "table_caption": ["Table 13: Few-shot returns of OOD test tasks using Medium datasets from Ant-Dir. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please refer to the Abstract and Section 1: Introduction. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please refer to Section 6. Conclusions, Limitations, and Future Work. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please refer to Appendix A for our algorithm pseudocodes, Appendix B for the dataset construction, and Appendix D for the implementation details of our method. We have also provided our source code at github for reliable reproduction. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have provided the source code with sufficient instructions at github: https://github.com/NJU-RL/Meta-DT. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please refer to Section 5: Experiments, Appendix B, Appendix D, and the source code at github. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please refer to Section 5: Experiments. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please refer to Appendix D. ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Please refer to our submitted source code at github. ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Please refer to Section 6: Conclusions, Limitations, and Future Work. ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}]