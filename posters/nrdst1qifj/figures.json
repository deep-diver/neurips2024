[{"figure_path": "nRdST1qifJ/figures/figures_1_1.jpg", "caption": "Figure 1: The pipeline of our proposed Prompt Adversarial Tuning (PAT) at the inference stage. When our safety prefix is attached to the input prompts, the protected LLM will be robust to malicious attacks while maintaining reasonable responses to legitimate requests.", "description": "This figure illustrates how the proposed Prompt Adversarial Tuning (PAT) method works.  A safety prefix is added to user prompts before they reach the language model.  This prefix acts as a safeguard, preventing the model from generating harmful responses to malicious prompts (jailbreaking attempts) while still allowing for normal, safe interactions for legitimate requests. The figure uses a simple example to show how a malicious prompt is blocked by the safety prefix while a benign prompt is processed normally.", "section": "1 Introduction"}, {"figure_path": "nRdST1qifJ/figures/figures_2_1.jpg", "caption": "Figure 1: The pipeline of our proposed Prompt Adversarial Tuning (PAT) at the inference stage. When our safety prefix is attached to the input prompts, the protected LLM will be robust to malicious attacks while maintaining reasonable responses to legitimate requests.", "description": "The figure illustrates how Prompt Adversarial Tuning (PAT) enhances the robustness of Large Language Models (LLMs) against malicious prompts.  A \"safety prefix\" (defense control) is prepended to user inputs before they reach the LLM.  This prefix is designed to neutralize harmful intentions within the prompt, preventing the LLM from generating unsafe responses.  The example shows how a malicious prompt asking for instructions on exploiting a software system is blocked by the safety prefix, while a benign request is still handled normally.", "section": "3 The Proposed Prompt Adversarial Tuning"}, {"figure_path": "nRdST1qifJ/figures/figures_6_1.jpg", "caption": "Figure 2: Transferability of PAT across models. PAT can acquire low ASR when it transfers the prefix across different model architectures.", "description": "This figure shows the results of applying the PAT model across different language models.  The heatmap shows the Attack Success Rate (ASR) for each combination of a source and a target model, indicating how well the defense mechanism trained on one model generalizes to others. Lower ASR values represent better transferability of the defense mechanism, showing its effectiveness even when applied to models it wasn't originally trained on. The three subfigures show the ASR using different attack methods: GCG, ICA, and PAIR.", "section": "4.3 Transferability of PAT across Open-source Models"}, {"figure_path": "nRdST1qifJ/figures/figures_7_1.jpg", "caption": "Figure 1: The pipeline of our proposed Prompt Adversarial Tuning (PAT) at the inference stage. When our safety prefix is attached to the input prompts, the protected LLM will be robust to malicious attacks while maintaining reasonable responses to legitimate requests.", "description": "This figure illustrates how PAT works.  A safety prefix (a guard) is added to user prompts before they reach the LLM. This prefix, trained using PAT, helps the model resist malicious prompts designed to elicit unsafe responses (jailbreaks), while still allowing the model to function normally with legitimate inputs.  The figure compares the response of an unprotected LLM to a protected LLM using PAT in response to the same prompt, showcasing the effectiveness of the method.", "section": "3 The Proposed Prompt Adversarial Tuning"}, {"figure_path": "nRdST1qifJ/figures/figures_8_1.jpg", "caption": "Figure 4: Ablation Studies for PAT. We investigate the influence of different factors, including (a) the length of the defense control |Idef| (b) the trade-off factor \u03b1", "description": "This figure presents the ablation study results for the proposed Prompt Adversarial Tuning (PAT) method.  Two subfigures show how the model's performance is affected by changing different hyperparameters. (a) shows the impact of varying the length of the defense control prefix, denoted as |Idef|.  It demonstrates how the attack success rate (ASR) and MMLU scores change with different prefix lengths for four different attack methods (GCG, ICA, PAIR, and a multi-turn benchmark). (b) shows the effect of altering the trade-off parameter \u03b1 (alpha) which balances the emphasis between robustness against adversarial attacks and maintenance of model usability for regular benign prompts.", "section": "4 Experiments"}]