[{"figure_path": "3dn1hINA6o/figures/figures_1_1.jpg", "caption": "Figure 1: Existing offline model-based RL methods fail if the accuracy of the dynamics model is increased (with all else kept the same). Results shown are for MOPO [36], but note that this failure indicates the failure of all existing uncertainty-based methods since each of their specific penalty terms disappear under the true dynamics as 'uncertainty' is zero. By contrast, our method is much more robust to changes in dynamics model. The x-axis shows linearly interpolating next states and rewards of the learned model with the true model (center\u2192right) and random model (center left), with results on the D4RL W2d-medexp benchmark (min/max over 4 seeds). The full set of results and experimental setup are provided in Table 1 and Appendix C.2 respectively.", "description": "This figure shows the performance of existing offline model-based reinforcement learning methods and the proposed RAVL method as the accuracy of the dynamics model is increased. The x-axis represents the interpolation between a random model, a learned model, and the true dynamics model. The y-axis represents the D4RL normalized score. The figure demonstrates that existing methods fail when the dynamics model is perfect (true dynamics), while RAVL remains robust.", "section": "3 The Edge-of-Reach Problem"}, {"figure_path": "3dn1hINA6o/figures/figures_3_1.jpg", "caption": "Figure 2: The previously unnoticed edge-of-reach problem. Left illustrates the base procedure used in offline model-based RL, whereby synthetic data is sampled as k-step trajectories \"rollouts\" starting from a state in the original offline dataset. Edge-of-reach states are those that can be reached in k-steps, but which cannot (under any policy) be reached in less than k-steps. We depict the data collected with two rollouts, one ending in Sk = D, and the other with sk = C. Right then shows this data arranged into a dataset of transitions as used in Q-updates. State D is edge-of-reach and hence appears in the dataset as s' but never as s. Bellman updates therefore bootstrap from D, but never update the value at D (see Equation (1)). (For comparison consider state C: C is also sampled at sk, but unlike D it is not edge-of-reach, and hence is also sampled at si<k meaning it is updated and hence does not cause issues.)", "description": "This figure illustrates the edge-of-reach problem in offline model-based reinforcement learning. The left panel shows how k-step rollouts from a fixed dataset can lead to states (edge-of-reach states) that are only reachable in the final step of a rollout and are never updated during training. The right panel shows how this affects the Bellman update, leading to value overestimation and performance collapse.  The figure highlights the difference between states that are updated (because they are reachable in less than k steps) and states that are used for targets but never updated (edge-of-reach states).", "section": "The Edge-of-Reach Problem"}, {"figure_path": "3dn1hINA6o/figures/figures_5_1.jpg", "caption": "Figure 3: The base procedure results in poor performance (left) with exponential increase in Q-values (right) on the D4RL benchmark. Approx Q* indicates the Q-value for a normalized score of 100 (with \u03b3 = 0.99). Results are shown for Walker2d-medexp (6 seeds), but we note similar trends across other D4RL datasets.", "description": "This figure shows the results of the base offline model-based RL algorithm on the D4RL Walker2d-medexp benchmark.  The left panel shows the normalized score over training epochs, indicating that the algorithm fails to learn a good policy. The right panel shows the Q-values over training epochs, illustrating a dramatic, exponential increase that signifies pathological value overestimation.  This comparison highlights the significant performance degradation and instability of the base method, which serves as a key motivation for introducing the proposed RAVL algorithm.", "section": "3 The Edge-of-Reach Problem"}, {"figure_path": "3dn1hINA6o/figures/figures_6_1.jpg", "caption": "Figure 4: Experiments on the simple environment, illustrating the edge-of-reach problem and potential solutions. (a) Reward function, (b) final (failed) policy with na\u00efve application of the base procedure (see Algorithm 1), (c) final (successful) policy with patching in oracle Q-values for edge-of-reach states, (d) final (successful) policy with RAVL, (e) returns evaluated over training, (f) mean Q-values evaluated over training.", "description": "This figure shows the results of experiments conducted on a simple environment designed to isolate the edge-of-reach problem.  It compares four different approaches: the base model-based offline RL method (which fails), a version of the base method where oracle Q-values are patched into edge-of-reach states (which succeeds), and RAVL (the proposed solution, which also succeeds).  The visualization includes reward functions, final policies, training returns, and Q-value evolution over training. This illustrates that the edge-of-reach problem is causing the failures of base methods, and that RAVL successfully addresses the problem by preventing overestimation of values in the edge-of-reach states.", "section": "4 Analysis with a Simple Environment"}, {"figure_path": "3dn1hINA6o/figures/figures_7_1.jpg", "caption": "Figure 5: RAVL's effective penalty of Q-ensemble variance on the environment in Section 4, showing that - as intended - edge-of-reach states have significantly higher penalty than within-reach states.", "description": "This figure shows the distribution of the effective RAVL penalty for within-reach and edge-of-reach states in the simple environment described in Section 4.  The box plot summarizes the distribution, while the histogram provides a more detailed view. As intended, the penalty is significantly higher for edge-of-reach states than for within-reach states, demonstrating that RAVL successfully identifies and penalizes these critical states.", "section": "Empirical evidence on the D4RL benchmark"}, {"figure_path": "3dn1hINA6o/figures/figures_18_1.jpg", "caption": "Figure 6: We find that the dynamics uncertainty-based penalty used in MOPO [36] is positively correlated with the variance of the value ensemble of RAVL, suggesting prior methods may unintentionally address the edge-of-reach problem. Pearson correlation coefficients are 0.49, 0.43, and 0.27 for Hopper-mixed, Walker2d-medexp, and Halfcheetah-medium respectively.", "description": "This figure shows the positive correlation between the dynamics uncertainty penalty used in MOPO and the variance of the value ensemble in RAVL.  This suggests that existing methods, which focus on dynamics uncertainty, might indirectly address the edge-of-reach problem, even without explicitly acknowledging it.  The correlation coefficients provided quantify the strength of this relationship across three different D4RL benchmark environments.", "section": "Additional Visualizations"}, {"figure_path": "3dn1hINA6o/figures/figures_19_1.jpg", "caption": "Figure 7: A visualization of the rollouts sampled over training on the simple environment in Section 4. We note the pathological behavior of the baseline, and then the success of the ideal intervention Base-OraclePatch, and our practically realizable method RAVL.", "description": "This figure visualizes the rollouts sampled during training in a simple environment for three different methods: the base method, Base-OraclePatch (which uses oracle Q-values to correct for edge-of-reach states), and RAVL.  It shows how the base method's rollouts are increasingly concentrated in edge-of-reach states (red), leading to its failure. Base-OraclePatch successfully avoids this issue by using oracle values. RAVL mimics the behavior of Base-OraclePatch, indicating its effectiveness in addressing the edge-of-reach problem.", "section": "E.2 Behaviour of Rollouts Over Training"}]