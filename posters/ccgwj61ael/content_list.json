[{"type": "text", "text": "Optimal Design for Human Preference Elicitation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Subhojyoti Mukherjee Anusha Lalitha Kousha Kalantari University of Wisconsin-Madison\u2217 AWS AI Labs AWS AI Labs smukherjee27@wisc.edu Aniket Deshmukh Ge Liu Yifei Ma Branislav Kveton AWS AI Labs UIUC\u2217 AWS AI Labs Adobe Research\u2217 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning of preference models from human feedback has been central to recent advances in artificial intelligence. Motivated by the cost of obtaining high-quality human annotations, we study efficient human preference elicitation for learning preference models. The key idea in our work is to generalize optimal designs, a methodology for computing optimal information-gathering policies, to questions with multiple answers, represented as lists of items. The policy is a distribution over lists and we elicit preferences from the list proportionally to its probability. To show the generality of our ideas, we study both absolute and ranking feedback models on items in the list. We design efficient algorithms for both and analyze them. Finally, we demonstrate that our algorithms are practical by evaluating them on existing question-answering problems. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning from human feedback (RLHF) has been effective in aligning and fine-tuning large language models (LLMs) [68, 36, 15, 77, 39]. The main difference from classic reinforcement learning (RL) [81] is that the agent learns from human feedback, which is expressed as preferences for different potential choices [2, 49, 71, 10, 90]. The human feedback allows LLMs to be adapted beyond the distribution of data that was used for their pre-training and generate answers that are more preferred by humans [15]. The feedback can be incorporated by learning a preference model. When the human decides between two choices, the Bradley-Terry-Luce (BTL) model [12] can be used. For multiple choices, the Plackett-Luce $(P L)$ model [65, 54] can be adopted. A good preference model should correctly rank answers to many potential questions. Therefore, learning of a good preference model can be viewed as learning to rank, and we adopt this view in this work. Learning to rank has been studied extensively in both offline [14] and online [67, 43, 83, 80, 46] settings. ", "page_idx": 0}, {"type": "text", "text": "To effectively learn preference models, we study efficient methods for human preference elicitation. We formalize this problem as follows. We have a set of $L$ lists representing questions, each with $K$ items representing answers. The objective of the agent is to learn to rank all items in all lists. The agent can query humans for feedback. Each query is a question with $K$ answers represented as a list. The human provides feedback on it. We study two feedback models: absolute and ranking. In the absolute feedback model, a human provides noisy feedback for each item in the list. This setting is motivated by how annotators assign relevance judgments in search [30, 57]. The ranking feedback is motivated by learning reward models in RLHF [68, 36, 15, 77, 39]. In this model, a human ranks all items in the list according to their preferences. While $K=2$ is arguably the most common case, we study $K\\ge2$ for the sake of generality and allowing a higher-capacity communication channel with the human [102]. The agent has a budget for the number of queries. To learn efficiently within the budget, it needs to elicit preferences from the most informative lists, which allows it to learn to rank all other lists. Our main contribution is an efficient algorithm for computing the distribution of the most informative lists. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our work touches on many topics. Learning of reward models from human feedback is at the center of RLHF [62] and its recent popularity has led to major theory developments, including analyses of regret minimization in RLHF [16, 87, 90, 91, 61, 75]. These works propose and analyze adaptive algorithms that interact with the environment to learn highly-rewarding policies. Such policies are usually hard to deploy in practice because they may harm user experience due to over-exploration [22, 82]. Therefore, Zhu et al. [102] studied RLHF from ranking feedback in the offilne setting with a fixed dataset. We study how to collect an informative dataset for offilne learning to rank with both absolute and ranking feedback. We approach this problem as an optimal design, a methodology for computing optimal information-gathering policies [66, 24]. The policies are non-adaptive and thus can be precomputed, which is one of their advantages. The main technical contribution of this work is a matrix generalization of the Kiefer-Wolfowitz theorem [41], which allows us to formulate optimal designs for ranked lists and solve them efficiently. Optimal designs have become a standard tool in exploration [45, 37, 38, 58, 33] and adaptive algorithms can be obtained by combining them with elimination. Therefore, optimal designs are also a natural stepping stone to other solutions. ", "page_idx": 1}, {"type": "text", "text": "We make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "1. We develop a novel approach for human preference elicitation. The key idea is to generalize the Kiefer-Wolfowitz theorem [41] to matrices (Section 3), which then allows us to compute information-gathering policies for ranked lists.   \n2. We propose an algorithm that uses an optimal design to collect absolute human feedback (Section 4.1), where a human provides noisy feedback for each item in the queried list. A least-squares estimator is then used to learn a preference model. The resulting algorithm is both computationally and statistically efficient. We bound its prediction error (Section 4.2) and ranking loss (Section 4.3), and show that both decrease with the sample size.   \n3. We propose an algorithm that uses an optimal design to collect ranking human feedback (Section 5.1), where a human ranks all items in the list according to their preferences. An estimator of Zhu et al. [102] is then used to learn a preference model. Our approach is both computationally and statistically efficient, and we bound its prediction error (Section 5.2) and ranking loss (Section 5.3). These results mimic the absolute feedback setting and show the generality of our framework.   \n4. We compare our algorithms to multiple baselines in several experiments. We observe that the algorithms achieve a lower ranking loss than the baselines. ", "page_idx": 1}, {"type": "text", "text": "2 Setting ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Notation: Let $[K]=\\{1,\\ldots,K\\}$ . Let $\\Delta^{L}$ be the probability simplex over $[L]$ . For any distribution $\\pi\\in\\Delta^{L}$ , we get $\\begin{array}{r}{\\sum_{i=1}^{L}\\pi(i)=1}\\end{array}$ . Let $\\Pi_{2}(K)=\\{(j,k)\\in[K]^{2}:j<k\\}$ be the set of all pairs over $[K]$ where the first entry is lower than the second one. Let $\\|\\mathbf{x}\\|_{\\mathbf{A}}^{2}=\\mathbf{x}^{\\top}\\mathbf{Ax}$ for any positive-definite $\\mathbf{A}\\in\\mathbb{R}^{d\\times d}$ and $\\mathbf{x}\\in\\mathbb{R}^{d}$ . We use $\\tilde{O}$ for the big-O notation up to logarithmic factors. Specifically, for any function $f$ , we write $\\tilde{O}(f(n))$ if it is $O(f(n)\\log^{k}f(n))$ for some $k>0$ . Let $\\operatorname{supp}\\left(\\pi\\right)$ be the support of distribution $\\pi$ or a random variable. ", "page_idx": 1}, {"type": "text", "text": "Setup: We learn to rank $L$ lists, each with $K$ items. An item $k\\in[K]$ in list $i\\in[L]$ is represented by a feature vector $\\mathbf{x}_{i,k}\\in\\mathcal{X}$ , where $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ is the set of feature vectors. The relevance of items is given by their mean rewards. The mean reward of item $k$ in list $i$ is $\\mathbf{x}_{i,k}^{\\top}\\pmb{\\theta}_{*}$ , where $\\pmb{\\theta}_{\\ast}\\in\\mathbb{R}^{d}$ is an unknown parameter. Without loss of generality, we assume that the original order of the items is optimal, $\\dot{\\mathbf{x}_{i,j}^{\\top}}\\pmb{\\theta}_{*}>\\mathbf{x}_{i,k}^{\\top}\\pmb{\\theta}_{*}$ for any $j<k$ and list $i$ . The agent does not know it. The agent interacts with humans for $n$ rounds. At round $t$ , it selects a list $I_{t}$ and the human provides stochastic feedback on it. Our goal is to design a policy for selecting the lists such that the agent learns the optimal order of all items in all lists after $n$ rounds. ", "page_idx": 1}, {"type": "text", "text": "Feedback model: We study two models of human feedback, absolute and ranking: ", "page_idx": 1}, {"type": "text", "text": "(1) In the absolute feedback model, the human provides a reward for each item in list $I_{t}$ chosen by the agent. Specifically, the agent observes noisy rewards ", "page_idx": 2}, {"type": "equation", "text": "$$\ny_{t,k}=\\mathbf{x}_{I_{t},k}^{\\top}\\pmb{\\theta}_{*}+\\eta_{t,k}\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for all $k\\in[K]$ in list $I_{t}$ , where $\\eta_{t,k}$ is independent zero-mean 1-sub-Gaussian noise. This feedback is stochastic and similar to that in the document-based click model [19]. ", "page_idx": 2}, {"type": "text", "text": "(2) In the ranking feedback model, the human orders all $K$ items in list $I_{t}$ selected by the agent. The feedback is a permutation $\\sigma_{t}:[K]\\to[K]$ , where $\\sigma_{t}(k)$ is the index of the $k$ -th ranked item. The probability that this permutation is generated is ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\sigma_{t})=\\prod_{k=1}^{K}\\frac{\\exp[{\\bf x}_{I_{t},\\sigma_{t}(k)}^{\\top}\\pmb\\theta_{*}]}{\\sum_{j=k}^{K}\\exp[{\\bf x}_{I_{t},\\sigma_{t}(j)}^{\\top}\\pmb\\theta_{*}]}\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Simply put, items with higher mean rewards are more preferred by humans and hence more likely to be ranked higher. This feedback model is known as the Plackett-Luce $(P L)$ model [65, 54, 102], and it is a standard assumption when learning values of individual choices from relative feedback. Since the feedback at round $t$ is with independent noise, in both (1) and (2), any list can be observed multiple times and we do need to assume that $n\\leq L$ . ", "page_idx": 2}, {"type": "text", "text": "Objective: At the end of round $n$ , the agent outputs a permutation $\\hat{\\sigma}_{n,i}:[K]\\to[K]$ for all lists $i\\in[L]$ , where $\\hat{\\sigma}_{n,i}(k)$ is the item placed at position $k$ in list $i$ . Our evaluation metric is the ranking loss after $n$ rounds, which we define as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{R}_{n}=\\sum_{i=1}^{L}\\sum_{j=1}^{K}\\sum_{k=j+1}^{K}\\mathbb{1}\\{\\hat{\\sigma}_{n,i}(j)>\\hat{\\sigma}_{n,i}(k)\\}\\ .\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The loss is the number of incorrectly ordered pairs of items in permutation $\\hat{\\sigma}_{n,i}$ , summed over all lists $i\\in[L]$ . It can also be viewed as the Kendall tau rank distance [40] between the optimal order of items in all lists and that according to $\\hat{\\sigma}_{n,i}$ . We note that other ranking metrics exist, such as the normalized discounted cumulative gain (NDCG) [86] and mean reciprocal rank (MRR) [85]. Our work can be extended to them and we leave this for future work. ", "page_idx": 2}, {"type": "text", "text": "The two closest related works are Mehta et al. [56] and Das et al. [20]. They proposed algorithms for learning to rank $L$ pairs of items from pairwise feedback. Their optimized metric is the maximum gap over the $L$ pairs. We learn to rank $L$ lists of $K$ items from $K$ -way ranking feedback. We bound the maximum prediction error, which is a similar metric to the prior works, and the ranking loss in (3), which is novel. Our setting is related to other bandit settings as follows. Due to the budget $n$ , it is similar to fixed-budget best arm identification (BAI) [13, 5, 6, 95]. The main difference is that we do not want to identify the best arm. We want to sort $L$ lists of $K$ items. Online learning to rank has also been studied extensively [67, 43, 105, 53, 44]. We do not minimize cumulative regret or try to identify the best arm. A more detailed comparison is in Appendix D. ", "page_idx": 2}, {"type": "text", "text": "We introduce optimal designs [66, 24] next. This allows us to minimize the expected ranking loss within a budget of $n$ rounds efficiently. ", "page_idx": 2}, {"type": "text", "text": "3 Optimal Design and Matrix Kiefer-Wolfowitz ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section introduces a unified approach to human preference elicitation from both absolute and ranking feedback. First, we note that to learn the optimal order of items in all lists, the agent has to estimate the unknown model parameter $\\pmb{\\theta}_{*}$ well. In this work, the agent uses a maximum-likelihood estimator $(M L E)$ to obtain an estimate $\\widehat{\\pmb{\\theta}}_{n}$ of $\\pmb{\\theta}_{*}$ . After that, it orders the items in all lists according to their estimated mean rewards $\\mathbf{x}_{i,k}^{\\top}\\hat{\\pmb{\\theta}}_{n}$ in descending order to get the permutation $\\hat{\\sigma}_{n,i}$ . If $\\widehat{\\pmb{\\theta}}_{n}$ would minimize the prediction error $(\\mathbf{x}_{i,k}^{\\top}(\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{\\ast}))^{2}$ over all items $k\\in[K]$ in list $i$ , the permutation $\\hat{\\sigma}_{n,i}$ would be closer to the optimal order. Moreover, if $\\widehat{\\pmb{\\theta}}_{n}$ minimized the maximum error over all lists, all permutations would be closer and the ranking loss in (3) would be minimized. This is why we focus on minimizing the maximum prediction error ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[L]}\\sum_{\\mathbf{a}\\in\\mathbf{A}_{i}}(\\mathbf{a}^{\\top}(\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*}))^{2}=\\operatorname*{max}_{i\\in[L]}\\mathrm{tr}(\\mathbf{A}_{i}^{\\top}(\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*})(\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*})^{\\top}\\mathbf{A}_{i})\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{A}_{i}$ is a matrix representing list $i$ and $\\mathbf{a}\\in\\mathbf{A}_{i}$ is a column in it. In the absolute feedback model, the columns of ${\\bf A}_{i}$ are feature vectors of items in list $i$ (Section 4.1). In the ranking feedback model, the columns of $\\mathbf{A}_{i}$ are the differences of feature vectors of items in list $i$ (Section 5.1). Therefore, ${\\bf A}_{i}$ depends on the type of human feedback. In fact, as we show later, it is dictated by the covariance of $\\widehat{\\pmb{\\theta}}_{n}^{\\phantom{\\dagger}}$ in the corresponding human feedback model. We note that the objective in (4) is worst-case over lists and that other alternatives, such as $\\begin{array}{r}{\\frac{1}{L}\\sum_{i=1}^{L}\\sum_{\\mathbf{a}\\in\\mathbf{A}_{i}}(\\mathbf{a}^{\\top}(\\hat{\\pmb{\\theta}}_{n}-\\bar{\\pmb{\\theta}_{*}}))^{2}}\\end{array}$ , may be possible. We leave this for future work. ", "page_idx": 3}, {"type": "text", "text": "We prove in Sections 4 and 5 that the agent can minimize the maximum prediction error in (4) and the ranking loss in (3) by sampling from a fixed distribution $\\pi_{*}\\in\\Delta^{L}$ . That is, the probability of selecting list $i$ at round $t$ is $\\mathbb{P}\\left(\\bar{I}_{t}=\\bar{\\boldsymbol{\\imath}}\\right)=\\pi_{*}(i)$ . The distribution $\\pi_{*}$ is a minimizer of ", "page_idx": 3}, {"type": "equation", "text": "$$\ng(\\pi)=\\operatorname*{max}_{i\\in[L]}\\operatorname{tr}(\\mathbf{A}_{i}^{\\top}\\mathbf{V}_{\\pi}^{-1}\\mathbf{A}_{i})\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{{\\bf V}_{\\pi}=\\sum_{i=1}^{L}{\\pi}(i){\\bf A}_{i}{\\bf A}_{i}^{\\top}}\\end{array}$ is a design matrix. The optimal design aims to find the distribution $\\pi_{*}$ . Since (5)  does not depend on the received feedback, our algorithms are not adaptive. ", "page_idx": 3}, {"type": "text", "text": "The problem of finding $\\pi_{*}$ that minimizes (5) is called the $G$ -optimal design [45]. The minimum of (5) and the support of $\\pi_{*}$ are characterized by the Kiefer-Wolfowitz theorem [41, 45]. The original theorem is for least-squares regression, where $\\mathbf{A}_{i}$ are feature vectors. At a high level, it says that the smallest ellipsoid that covers all feature vectors has the minimum volume, and in this way relates the minimization of (5) to maximizing $\\log\\operatorname*{det}(\\mathbf{V}_{\\pi})$ . We generalize this claim to lists, where $\\mathbf{A}_{i}$ is a matrix of feature vectors representing list $i$ . This generalization allows us to go from a design over feature vectors to a design over lists represented by matrices. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (Matrix Kiefer-Wolfowitz). Let $M\\geq1$ be an integer and $\\mathbf{A}_{1},\\dots,\\mathbf{A}_{L}\\in\\mathbb{R}^{d\\times M}$ be $L$ matrices whose column space spans $\\mathbb{R}^{d}$ . Then the following claims are equivalent: ", "page_idx": 3}, {"type": "text", "text": "(a) $\\pi_{*}$ is a minimizer of $g(\\pi)$ defined in (5). ", "page_idx": 3}, {"type": "text", "text": "Furthermore, there exists a minimizer $\\pi_{*}$ of $g(\\pi)$ such that $|\\mathrm{supp}\\left(\\pi_{*}\\right)|\\leq d(d+1)/2$ . ", "page_idx": 3}, {"type": "text", "text": "Proof. We generalize the proof of the Kiefer-Wolfowitz theorem in Lattimore and Szepesvari [45]. The key observation is that even if ${\\bf A}_{i}$ is a matrix and not a vector, the design matrix $\\mathbf{V}_{\\pi}$ is positive definite. Using this, we establish three key facts that are used in the original proof. First, we show that $f(\\pi)$ is concave in $\\pi$ and that $(\\nabla f(\\pi))_{i}=\\operatorname{tr}(\\mathbf{A}_{i}^{\\top}\\mathbf{V}_{\\pi}^{-1}\\mathbf{A}_{i})$ is its gradient with respect to $\\pi(i)$ . Second, $\\begin{array}{r}{\\sum_{i=1}^{L}\\pi(i)\\operatorname{tr}(\\mathbf{A}_{i}^{\\top}\\mathbf{V}_{\\pi}^{-1}\\mathbf{A}_{i})=d}\\end{array}$ . Finally, we prove that $\\begin{array}{r}{g(\\pi)\\geq\\sum_{i=1}^{L}\\pi(i)\\operatorname{tr}(\\mathbf{A}_{i}^{\\top}\\mathbf{V}_{\\pi}^{-1}\\mathbf{A}_{i})}\\end{array}$ The com plete proof is in Appendix A.1. \u53e3 ", "page_idx": 3}, {"type": "text", "text": "From the equivalence in Theorem 1, it follows that the agent should solve the optimal design ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{*}=\\underset{\\pi\\in\\Delta^{L}}{\\arg\\operatorname*{max}}\\,f(\\pi)=\\underset{\\pi\\in\\Delta^{L}}{\\arg\\operatorname*{max}}\\log\\operatorname*{det}(\\mathbf{V}_{\\pi})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and sample according to $\\pi_{*}$ to minimize the maximum prediction error in (4). Note that the optimal design over lists in (6) is different from the one over features [45]. As an example, suppose that we have 4 feature vectors $\\{\\mathbf{x}_{i}\\}_{i\\in[4]}$ and two lists: $\\mathbf{A}_{1}=\\left(\\mathbf{x}_{1},\\mathbf{x}_{2}\\right)$ and $\\mathbf{A}_{2}=(\\mathbf{x}_{3},\\mathbf{x}_{4})$ . The list design is over 2 variables (lists) while the feature-vector design is over 4 variables (feature vectors). The list design can also be viewed as a constrained feature-vector design, where $(\\mathbf{x}_{1},\\mathbf{x}_{2})$ and $(\\mathbf{x}_{3},\\mathbf{x}_{4})$ are observed together with the same probability. ", "page_idx": 3}, {"type": "text", "text": "The optimization problem in (6) is convex and thus easy to solve. When the number of lists is large, the Frank-Wolfe algorithm [59, 32] can be applied, which solves convex optimization problems with linear constraints as a sequence of linear programs. We use CVXPY [21] to compute the optimal design. We report its computation time, as a function of the number of lists $L$ , in Appendix E. The computation time scales roughly linearly with the number of lists $L$ . In the following sections, we utilize Theorem 1 to bound the maximum prediction error and ranking loss for both absolute and ranking feedback. ", "page_idx": 3}, {"type": "table", "img_path": "cCGWj61Ael/tmp/9c5b1a29437e05da24c9c50ca67fc9e0882b1f826ab0b255fafa8ddd9871b678.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 Learning with Absolute Feedback ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section is organized as follows. In Section 4.1, we present an algorithm for human preference elicitation under absolute feedback. We bound its prediction error in Section 4.2 and its ranking loss in Section 4.3. ", "page_idx": 4}, {"type": "text", "text": "4.1 Algorithm Dope ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Now we present our algorithm for absolute feedback called $\\mathbf{D}$ -optimal preference elicitation (Dope). The algorithm has four main parts. First, we solve the optimal design problem in (6) to get a data logging policy $\\pi_{*}$ . The matrix for list $i$ is $\\mathbf{A}_{i}=[\\mathbf{x}_{i,k}]_{k\\in[K]}\\in\\mathbb{R}^{d\\times K}$ , where ${\\bf x}_{i,k}$ is the feature vector of item $k$ in list $i$ . Second, we collect human feedback for $n$ rounds. At round $t\\in[n]$ , we sample a list $I_{t}\\sim\\pi_{*}$ and then observe $y_{t,k}$ for all $k\\in[K]$ , as defined in (1). Third, we estimate the model parameter as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\pmb{\\theta}}_{n}=\\bar{\\\\pmb{\\Sigma}}_{n}^{-1}\\sum_{t=1}^{n}\\sum_{k=1}^{K}\\mathbf{x}_{I_{t},k}y_{t,k}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The normalized and unnormalized covariance matrices corresponding to the estimate are ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{\\Sigma}_{n}=\\frac{1}{n}\\bar{\\pmb{\\Sigma}}_{n}\\,,\\quad\\bar{\\pmb{\\Sigma}}_{n}=\\sum_{t=1}^{n}\\sum_{k=1}^{K}\\mathbf{x}_{I_{t},k}\\mathbf{x}_{I_{t},k}^{\\top}\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "respectively. Finally, we sort the items in all lists $i$ according to their estimated mean rewards $\\mathbf{x}_{i,k}^{\\top}\\hat{\\pmb{\\theta}}_{n}$ in descending order, to obtain the permutation $\\hat{\\sigma}_{n,i}$ . The pseudo-code of Dope is in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "The estimator (7) is the same as in ordinary least squares $(O L S)$ , because each observed list can be treated as $K$ independent observations. The matrix for list $i$ , $\\mathbf{A}_{i}$ , can be related to the inner sum in (8) through tr(AiAi\u22a4 ) =  kK=1 xi,kxi\u22a4,k. Therefore, our optimal design for absolute feedback logs data for a least-squares estimator by optimizing its covariance [45, 33]. ", "page_idx": 4}, {"type": "text", "text": "4.2 Maximum Prediction Error Under Absolute Feedback ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we bound the maximum prediction error of Dope under absolute feedback. We start with a lemma that uses the optimal design $\\pi_{*}$ to bound $\\begin{array}{r}{\\operatorname*{max}_{i\\in[L]}\\sum_{\\mathbf{a}\\in\\mathbf{A}_{i}}\\|\\mathbf{a}\\|_{\\bar{\\Sigma}_{n}^{-1}}^{2}}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 2. Let $\\pi_{*}$ be the optimal design in (6). Fix budget $n$ and let each allocation $n\\pi_{*}(i)$ be an integer. Then $\\begin{array}{r}{\\operatorname*{max}_{i\\in[L]}\\sum_{\\mathbf{a}\\in\\mathbf{A}_{i}}\\|\\mathbf{a}\\|_{\\bar{\\Sigma}_{n}^{-1}}^{2}=d/n.}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "The lemma is proved in Appendix A.2. Since all $n\\pi_{*}(i)$ are integers, we note that $\\bar{\\Sigma}_{n}$ is full rank and thus invertible. The condition of the lemma, that each $n\\pi_{*}(i)$ is an integer, does not require $n\\geq L$ . This is because $\\pi_{*}(i)$ has at most $d(d+1)/2$ non-zero entries (Theorem 1). This is independent of the number of lists $L$ , which could also be infinite (Chapter 21.1 in Lattimore and Szepesvari [45]). The integer condition can be also relaxed by rounding non-zero entries of $n\\pi_{*}(i)$ up to the closest integer. This clearly yields an integer allocation of size at most $n+d(d+1)/2$ . All claims in our work would hold for any $\\pi_{*}$ and this allocation. With Lemma 2 in hand, the maximum prediction error is bounded as follows. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3 (Maximum prediction error). With probability at least $1-\\delta$ , the maximum prediction error after n rounds is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[L]}\\operatorname{tr}(\\mathbf{A}_{i}^{\\top}(\\hat{\\theta}_{n}-\\theta_{*})(\\hat{\\theta}_{n}-\\theta_{*})^{\\top}\\mathbf{A}_{i})=O\\left(\\frac{d^{2}+d\\log(1/\\delta)}{n}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The theorem is proved in Appendix A.3. As in Lemma 2, we assume that each allocation $n\\pi_{*}(i)$ is an integer. If the allocations were not integers, rounding errors would arise and need to be bounded [66, 25, 37]. At a high level, our bound would be multiplied by $1+\\beta$ for some $\\beta>0$ (Chapter 21 in Lattimore and Szepesvari [45]). We omit this factor in our proofs to simplify them. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3 says that the maximum prediction error is ${\\tilde{O}}(d^{2}/n)$ . Note that this rate cannot be attained trivially, for instance by uniform sampling. To see this, consider the following example. Take $K=2$ Let $\\mathbf{x}_{i,1}=(1,0,0)$ for $i\\in[L-1]$ and $\\mathbf{x}_{L,1}=(0,1,0)$ , and $\\mathbf{x}_{i,2}=(0,0,1)$ for all $i\\in[L]$ . In this case, the minimum eigenvalue of $\\dot{\\Sigma}_{n}$ is $n/L$ in expectation, because only one item in list $L$ provides information about the second feature, $\\mathbf{x}_{L,1}=(0,1,0)$ . Following the same steps as in Theorem 3, we would get a rate of $\\tilde{O}(d L/n)$ . Prior works on optimal designs also made similar observations [78]. ", "page_idx": 5}, {"type": "text", "text": "The rate in Theorem 3 is the same as in linear models. More specifically, by the Cauchy-Schwarz inequality, we would get ", "page_idx": 5}, {"type": "equation", "text": "$$\n(\\mathbf{x}^{\\top}(\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*}))^{2}\\leq\\lVert\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*}\\rVert_{\\bar{\\Sigma}_{n}}^{2}\\lVert\\mathbf{x}\\rVert_{\\bar{\\Sigma}_{n}^{-1}}^{2}=\\tilde{O}(d)\\,\\tilde{O}(d/n)=\\tilde{O}(d^{2}/n)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with a high probability, where $\\theta_{*},\\hat{\\theta}_{n}$ , and $\\bar{\\Sigma}_{n}$ are the analogous linear model quantities. This bound holds for infinitely many feature vectors. It can be tightened to ${\\tilde{O}}(d/n)$ for a finite number of feature vectors, where $\\tilde{O}$ hides the logarithm of the number of feature vectors. This can be proved using a union bound over (20.3) in Chapter 20 of Lattimore and Szepesvari [45]. ", "page_idx": 5}, {"type": "text", "text": "4.3 Ranking Loss Under Absolute Feedback ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we bound the expected ranking loss under absolute feedback. Recall from Section 2 that the original order of items in each list is optimal. With this in mind, the gap between the mean rewards of items $j$ and $k$ in list $i$ is $\\Delta_{i,j,k}=(\\dot{\\mathbf{x}}_{i,j}-\\mathbf{x}_{i,k})^{\\top}\\pmb{\\theta}_{\\ast}$ , for any $i\\in[L]$ and $(j,k)\\in\\Pi_{2}(K)$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 4 (Ranking loss). The expected ranking loss after $n$ rounds is bounded as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathrm{R}_{n}]\\leq2\\sum_{i=1}^{L}\\sum_{j=1}^{K}\\sum_{k=j+1}^{K}\\exp\\left[{-\\frac{\\Delta_{i,j,k}^{2}n}{4d}}\\right]\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof. From the definition of the ranking loss, we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{R}_{n}]=\\sum_{i=1}^{L}\\sum_{j=1}^{K}\\sum_{k=j+1}^{K}\\mathbb{E}[\\mathbb{I}\\{\\hat{\\sigma}_{n,i}(j)>\\hat{\\sigma}_{n,i}(k)\\}]=\\sum_{i=1}^{L}\\sum_{j=1}^{K}\\sum_{k=j+1}^{K}\\mathbb{P}\\left(\\mathbf{x}_{i,j}^{\\top}\\hat{\\pmb{\\theta}}_{n}<\\mathbf{x}_{i,k}^{\\top}\\hat{\\pmb{\\theta}}_{n}\\right)\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbb{P}\\left(\\mathbf{x}_{i,j}^{\\top}\\hat{\\pmb{\\theta}}_{n}<\\mathbf{x}_{i,k}^{\\top}\\hat{\\pmb{\\theta}}_{n}\\right)$ is the probability of predicting a sub-optimal item $k$ above item $j$ in list $i$ . We bound this probability from above by bounding the sum of $\\begin{array}{r}{\\mathbb{P}\\left(\\mathbf{x}_{i,k}^{\\top}(\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*})>\\frac{\\Delta_{i,j,k}}{2}\\right)}\\end{array}$ and $\\begin{array}{r}{\\mathbb{P}\\left(\\mathbf{x}_{i,j}^{\\top}(\\pmb{\\theta}_{*}-\\hat{\\pmb{\\theta}}_{n})>\\frac{\\Delta_{i,j,k}}{2}\\right)}\\end{array}$ . Each of these probabilities is bounded from above by $\\exp\\left[-\\frac{\\Delta_{i,j,k}^{2}n}{4d}\\right],$ using a concentration inequality in Lemma 8. The full proof is in Appendix A.4. ", "page_idx": 5}, {"type": "text", "text": "Each term in Theorem 4 can be bounded from above by $\\exp\\left[-{\\frac{\\Delta_{\\operatorname*{min}}^{2}n}{4d}}\\right]$ , where $n$ is the sample size, $d$ is the number of features, and $\\Delta_{\\mathrm{min}}$ denotes the minimum gap. Therefore, the bound decreases exponentially with budget $n$ and gaps, and increases with $d$ . This dependence is similar to that in Theorem 1 of Azizi et al. [6] for fixed-budget best-arm identification in linear models. Yang and Tan [95] derived a similar bound and a matching lower bound. The gaps $\\Delta_{i,j,k}$ reflect the hardness of sorting list $i$ , which depends on the differences of the mean rewards of items $j$ and $k$ in it. ", "page_idx": 6}, {"type": "text", "text": "Finally, we wanted to note that our optimal designs may not be optimal for ranking. We have not focused solely on ranking because we see value in both prediction error (Theorem 3) and ranking loss (Theorem 4) bounds. The fact that we provide both shows the versatility of our approach. ", "page_idx": 6}, {"type": "text", "text": "5 Learning with Ranking Feedback ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section is organized similarly to Section 4. In Section 5.1, we present an algorithm for human preference elicitation under ranking feedback. We bound its prediction error in Section 5.2 and its ranking loss in Section 5.3. Our algorithm design and analysis are under the following assumption, which we borrow from Zhu et al. [102]. ", "page_idx": 6}, {"type": "text", "text": "Assumption 1. We assume that the model parameter satisfies $\\mathbf{\\theta}_{*}\\in\\mathbf{\\Theta}_{*}$ , where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta=\\left\\{\\pmb{\\theta}\\in\\mathbb{R}^{d}:\\pmb{\\theta}^{\\top}\\mathbf{1}_{d}=0,\\|\\pmb{\\theta}\\|_{2}\\leq1\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We also assume that $\\begin{array}{r}{\\operatorname*{max}_{i\\in[L],\\,k\\in[K]}\\|\\mathbf{x}_{i,k}\\|_{2}\\leq1.}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "The assumption of bounded model parameter and feature vectors is common in bandits [1, 45]. The additional assumption of $\\mathbf{\\nabla}\\theta^{\\top}\\mathbf{1}_{d}=\\mathbf{\\dot{0}}$ is from Zhu et al. [102], from which we borrow the estimator and concentration bound. ", "page_idx": 6}, {"type": "text", "text": "5.1 Algorithm Dope ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We present Dope for ranking feedback next. The algorithm is similar to Dope in Section 4 and has four main parts. First, we solve the optimal design problem in (6) to get a data logging policy $\\pi_{*}$ . The matrix for list $i$ is $\\mathbf{A}_{i}=[\\mathbf{z}_{i,j,k}]_{(j,k)\\in\\Pi_{2}(K)}\\in\\bar{\\mathbb{R}}^{d\\times K(K-1)/2}$ , where $\\mathbf{z}_{i,j,k}=\\mathbf{x}_{i,j}-\\mathbf{x}_{i,k}$ denotes the difference of feature vectors of items $j$ and $k$ in list $i$ . Second, we collect human feedback for $n$ rounds. At round $t\\in[n]$ , we sample a list $I_{t}\\sim\\pi_{*}$ and then observe $\\sigma_{t}$ drawn from the PL model, as defined in (2). Third, we estimate the model parameter as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{n}=\\underset{\\theta\\in\\Theta}{\\arg\\operatorname*{min}}\\,\\ell_{n}(\\theta)\\,,\\quad\\ell_{n}(\\theta)=-\\frac{1}{n}\\sum_{t=1}^{n}\\sum_{k=1}^{K}\\log\\left(\\frac{\\exp[\\mathbf{x}_{I_{t},\\sigma_{t}(k)}^{\\top}\\theta]}{\\sum_{j=k}^{K}\\exp[\\mathbf{x}_{I_{t},\\sigma_{t}(j)}^{\\top}\\theta]}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Theta$ is defined in Assumption 1. We solve this estimation problem using iteratively reweighted least squares (IRLS) [89], a popular method for fitting the parameters of generalized linear models $(G L M s)$ . Finally, we sort the items in all lists $i$ according to their estimated mean rewards $\\mathbf{x}_{i,k}^{\\top}\\hat{\\pmb{\\theta}}_{n}$ in descending order, to obtain the permutation $\\hat{\\sigma}_{n,i}$ . The pseudo-code of Dope is in Algorithm 2. ", "page_idx": 6}, {"type": "text", "text": "The optimal design for (10) is derived as follows. First, we derive the Hessian of $\\ell_{n}(\\pmb\\theta),\\nabla^{2}\\ell_{n}(\\pmb\\theta)$ , in Lemma 9. The optimal design with $\\nabla^{2}\\ell_{n}(\\pmb{\\theta})$ cannot be solved exactly because $\\nabla^{2}\\ell_{n}(\\pmb{\\theta})$ depends on an unknown model parameter $\\pmb{\\theta}$ . To get around this, we eliminate $\\pmb{\\theta}$ -dependent terms by bounding them from below. Many prior works on decision making under uncertainty with GLMs took this approach [26, 52, 102, 20, 99]. We derive normalized and unnormalized covariance matrices ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pmb{\\Sigma}_{n}=\\frac{2}{K(K-1)n}\\bar{\\pmb{\\Sigma}}_{n}\\,,\\quad\\bar{\\pmb{\\Sigma}}_{n}=\\sum_{t=1}^{n}\\sum_{j=1}^{K}\\sum_{k=j+1}^{K}\\mathbf{z}_{I_{t},j,k}\\mathbf{z}_{I_{t},j,k}^{\\top}\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and prove that $\\nabla^{2}\\ell_{n}(\\pmb{\\theta})\\succeq\\gamma\\pmb{\\Sigma}_{n}$ for some $\\gamma>0$ . Therefore, we can maximize $\\log\\operatorname*{det}(\\nabla^{2}\\ell_{n}(\\pmb{\\theta}))$ , for any $\\theta\\in\\Theta$ , by maximizing $\\log\\operatorname*{det}(\\pmb{\\Sigma}_{n})$ . The matrix for list $i$ , $\\mathbf{A}_{i}$ , can be related to the inner sum in (11) through tr(AiAi\u22a4 ) =  jK=1 kK=j+1 zi,j,kzi\u22a4,j,k. ", "page_idx": 6}, {"type": "text", "text": "The cost for our approximation is a constant factor of $C>0$ in our bounds (Theorems 5 and 6). In Appendix C, we discuss a more adaptive design and also compare to it empirically. We conclude that it would be harder to implement and analyze, and we do not observe empirical benefits at $K=2$ . ", "page_idx": 6}, {"type": "text", "text": "5.2 Maximum Prediction Error Under Ranking Feedback ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we bound the maximum prediction error of Dope under ranking feedback. Similarly to the proof of Theorem 3, we decompose the error into two parts, which capture the efficiency of the optimal design and the uncertainty in the MLE $\\widehat{\\pmb{\\theta}}_{n}$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 5 (Maximum prediction error). With probability at least $1-\\delta$ , the maximum prediction error after n rounds is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[L]}\\mathrm{tr}(\\mathbf{A}_{i}^{\\top}(\\hat{\\theta}_{n}-\\theta_{*})(\\hat{\\theta}_{n}-\\theta_{*})^{\\top}\\mathbf{A}_{i})=O\\left(\\frac{K^{6}(d^{2}+d\\log(1/\\delta))}{n}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This theorem is proved in Appendix A.5. We build on a self-normalizing bound of Zhu et al. [102], \u2225\u03b8\u02c6n \u2212\u03b8\u2217\u22252\u03a3 \u2264O K4(d+lnog(1/\u03b4)) , which may not be tight in $K$ . If the bound could be improved by a multiplicative $c>0$ , we would get a multiplicative $c$ improvement in Theorem 5. We remind the reader that if the allocations $n\\pi_{*}(i)$ are not integers, a rounding procedure is needed [66, 25, 37]. This would result in a multiplicative $1+\\beta$ factor in our bound, for some $\\beta>0$ . For simplicity, we omit this factor in our derivations. ", "page_idx": 7}, {"type": "text", "text": "5.3 Ranking Loss Under Ranking Feedback ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we bound the expected ranking loss under ranking feedback. Similarly to Section 4.3, we define the gap between the mean rewards of items $j$ and $k$ in list $i$ as $\\Delta_{i,j,k}=\\mathbf{\\dot{z}}_{i,j,k}^{\\top}\\pmb{\\theta}_{*}$ , where $\\mathbf{z}_{i,j,k}=\\mathbf{x}_{i,j}-\\mathbf{x}_{i,k}$ is the difference of feature vectors of items $j$ and $k$ in list $i$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 6 (Ranking loss). The expected ranking loss after $n$ rounds is bounded as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathrm{R}_{n}]\\leq\\sum_{i=1}^{L}\\sum_{j=1}^{K}\\sum_{k=j+1}^{K}\\exp\\left[-\\frac{\\Delta_{i,j,k}^{2}n}{C K^{4}d}+d\\right]\\,,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $C>0$ is a constant. ", "page_idx": 7}, {"type": "text", "text": "Proof. The proof is similar to Theorem 4. At the end of round $n$ , we bound the probability that a sub-optimal item $k$ is ranked above item $j$ . The proof has two parts. First, for any list $i\\in[L]$ and items $(j,k)\\in\\Pi_{2}(K)$ , we show that $\\mathbb{P}\\left(\\mathbf{x}_{i,j}^{\\top}\\hat{\\pmb{\\theta}}_{n}<\\mathbf{x}_{i,k}^{\\top}\\hat{\\pmb{\\theta}}_{n}\\right)=\\mathbb{P}\\left(\\mathbf{z}_{i,j,k}^{\\top}(\\pmb{\\theta}_{*}-\\hat{\\pmb{\\theta}}_{n})>\\Delta_{i,j,k}\\right)$ . Then we bound this quantity by $\\exp\\left[-\\frac{\\Delta_{i,j,k}^{2}n}{C K^{4}d}+d\\right]$ . The full proof is in Appendix A.6. \u53e3 ", "page_idx": 7}, {"type": "text", "text": "The bound in Theorem 6 is similar to that in Theorem 4, with the exception of multiplicative $K^{-4}$ and additive $d$ . The leading term inside the sum can be bounded by e $\\left.\\mathrm{xp}\\left[-\\frac{\\Delta_{\\mathrm{min}}^{2}n}{C K^{4}d}\\right]\\right.$ , where $n$ is the sample size, $d$ is the number of features, and $\\Delta_{\\mathrm{min}}$ is the minimum gap. Therefore, similarly to Theorem 4, the bound decreases exponentially with budget $n$ and gaps; and increases with $d$ . This dependence is similar to Theorem 2 of Azizi et al. [6] for fixed-budget best-arm identification in GLMs. Our bound does not involve the extra factor of $\\kappa>0$ because we assume that all vectors lie in a unit ball (Assumption 1). ", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The goal of our experiments is to evaluate Dope empirically and compare it to baselines. All methods estimate $\\widehat{\\pmb{\\theta}}_{n}$ using (7) or (10), depending on the feedback. To guarantee that these problems are well defined, even when the sample covariance matrix is not full rank, we regularize both objectives with $\\gamma\\|\\pmb{\\theta}\\|_{2}^{2}$ , for a small $\\gamma>0$ . This mostly impacts small sample sizes. Specifically, since the optimal design leads to policies that collect diverse feature vectors, the sample covariance matrix is likely to be full rank when the sample size is large. After $\\widehat{\\pmb{\\theta}}_{n}$ is estimated, each method ranks items in all lists based on their estimated mean rewards $\\mathbf{x}_{i,k}^{\\top}\\hat{\\pmb{\\theta}}_{n}$ . The performance of all methods is measured by their ranking loss in (3) divided by $L$ . All experiments are averaged over 100 independent runs, and we report results in Figure 1. We compare the following algorithms: ", "page_idx": 7}, {"type": "image", "img_path": "cCGWj61Ael/tmp/6c356fea56339d90f06074eb28421112b37d5250a57bd9f84dc29e8ccdf9f380.jpg", "img_caption": ["Figure 1: Ranking loss of all compared methods plotted as a function of the number of rounds. The error bars are one standard error of the estimates. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "(1) Dope: This is our method. We solve the optimal design problem in (6) and then sample lists $I_{t}$ according to $\\pi_{*}$ . ", "page_idx": 8}, {"type": "text", "text": "(2) Unif: This baseline chooses lists $I_{t}$ uniformly at random from $[L]$ . While simple, it is known to be competitive in real-world problems where feature vectors may cover the feature space close to uniformly [4, 96, 3, 70]. ", "page_idx": 8}, {"type": "text", "text": "(3) Avg-Design: The exploration policy is an optimal design over feature vectors. The feature vector of list $i$ is the mean of the feature vectors of all items in it, $\\begin{array}{r}{\\bar{\\mathbf{x}}_{i}=\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{x}_{i,k}}\\end{array}$ . After the design is computed, we sample lists according to it. The rest is the same as in Dope. This baseline shows that our list representation with multiple feature vectors can outperform more naive choices. ", "page_idx": 8}, {"type": "text", "text": "(4) Clustered-Design: This approach uses the same representation as Avg-Design. The difference is that we cluster the lists using $k$ -medoids. Then we sample lists $I_{t}$ uniformly at random from the cluster centroids. The rest is the same as in Avg-Design. This baseline shows that Dope outperforms other notions of diversity, such as obtained by clustering. We tune $k$ ( $k=10$ in the Nectar dataset and $k=6$ otherwise) and report only the best results. ", "page_idx": 8}, {"type": "text", "text": "(5) APO: This method was proposed in Das et al. [20] and is the closest related work. APO greedily minimizes the maximum error in pairwise ranking of $L$ lists of length $K=2$ . We extend it to $K>2$ as follows. First, we turn $L$ lists of length $K$ into ${\\binom{K}{2}}L$ lists of length 2, one for each pair of items in the original lists. Then we apply APO to these ${\\binom{K}{2}}L$ lists of length 2. ", "page_idx": 8}, {"type": "text", "text": "Pure exploration algorithms are often compared to cumulative regret baselines [13, 5]. Since our problem is a form of learning to rank, online learning to rank (OLTR) baselines [67, 43, 105] seem natural. We do not compare to them for the following reason. The problem of an optimal design over lists is to design a distribution over queries. All OLTR algorithms solve a different problem, return a ranked list of items conditioned on a query chosen by the environment. Since they do not choose the queries, they cannot solve our problem. ", "page_idx": 8}, {"type": "text", "text": "Synthetic experiment 1 (absolute feedback): We have $L=400$ questions and represent them by random vectors $\\mathbf{q}_{i}\\in[-1,1]^{6}$ . Each question has $K=4$ answers. For each question, we generate $K$ random answers $\\mathbf{a}_{i,k}\\in[-1,1]^{6}$ . Both the question and answer vectors are normalized to unit length. ", "page_idx": 8}, {"type": "text", "text": "For each question-answer pair $(i,k)$ , the feature vector is $\\mathbf{x}_{i,k}=\\mathrm{vec}(\\mathbf{q}_{i}\\mathbf{a}_{i,k}^{\\top})$ and has length $d=36$ The outer product captures cross-interaction terms of the question and answer representations. A similar technique has been used for feature preprocessing of the Yahoo! Front Page Today Module User Click Log Dataset [50, 51, 103, 7]. We choose a random $\\pmb{\\theta}_{\\ast}\\in[0,1]^{d}$ . The absolute feedback is generated as in (1). Our results are reported in Figure 1(a). We note that the ranking loss of Dope decreases the fastest among all methods, with Unif, Avg-Design, and APO being close second. ", "page_idx": 9}, {"type": "text", "text": "Synthetic experiment 2 (ranking feedback): This experiment is similar to the first experiment, except that the feedback is generated by the PL model in (2). Our results are reported in Figure 1(b) and we observe again that the ranking loss of Dope decreases the fastest. The closest baselines are Unif, Avg-Design, and APO. Their lowest ranking loss $n=100)$ ) is attained by Dope at $n=60$ , which is nearly a two-fold reduction in sample size. In Appendix E, we conduct additional studies on this problem. We vary the number of lists $L$ and items $K$ , and report the computation time and ranking loss. ", "page_idx": 9}, {"type": "text", "text": "Experiment 3 (Nectar dataset): The Nectar dataset [101] is a dataset of 183k questions, each with 7 answers. We take a subset of this dataset: $L=2\\,000$ questions and $K=5$ answers. The answers are generated by GPT-4, GPT-4-0613, GPT-3.5-turbo, GPT-3.5-turbo-instruct, and Anthropic models. We embed the questions and answers in 768 dimensions using Instructor embeddings [79]. Then we project them to $\\mathbb{R}^{10}$ using a random projection matrix. The feature vector for answer $k$ to question $i$ is $\\mathbf{x}_{i,k}=\\mathrm{vec}(\\mathbf{q}_{i}\\mathbf{a}_{i,k}^{\\top})$ , where $\\mathbf{q}_{i}$ and ${\\bf a}_{i,k}$ are the projected embeddings of question $i$ and answer $k$ , respectively. Hence $d=100$ . The ranking feedback is simulated using the PL model in (2). We estimate its parameter $\\pmb{\\theta}_{\\ast}\\in\\mathbb{R}^{d}$ from the ranking feedback in the dataset using the MLE in (10). Our results are reported in Figure 1(c). We observe that the ranking loss of Dope is the lowest. The closest baseline is APO. Its lowest ranking loss $n=500)$ ) is attained by Dope at $n=150$ , which is more than a three-fold reduction in sample size. ", "page_idx": 9}, {"type": "text", "text": "Experiment 4 (Anthropic dataset): The Anthropic dataset [8] is a dataset of 161k questions with two answers per question. We take a subset of $L=2\\,000$ questions. We embed the questions and answers in 768 dimensions using Instructor embeddings [79]. Then we project them to $\\mathbb{R}^{6}$ using a random projection matrix. The feature vector for answer $k$ to question $i$ is $\\mathbf{x}_{i,k}=\\mathrm{vec}(\\mathbf{q}_{i}\\mathbf{a}_{i,k}^{\\top})$ , where $\\mathbf{q}_{i}$ and ${\\bf a}_{i,k}$ are the projected embeddings of question $i$ and answer $k$ , respectively. Hence $d=36$ . The ranking feedback is simulated using the PL model in (2). We estimate its parameter $\\pmb{\\theta}_{\\ast}\\in\\mathbb{R}^{d}$ from the ranking preference feedback in the dataset using the MLE in (10). Our results are reported in Figure 1(d). We note again that the ranking loss of Dope is the lowest. The closest baselines are Unif, Avg-Design, and APO. Their lowest ranking loss ( $n=1\\,000)$ ) is attained by Dope at $n=300$ , which is more than a three-fold reduction in sample size. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We study the problem of optimal human preference elicitation for learning preference models. The problem is formalized as learning to rank $K$ answers to $L$ questions under a budget on the number of asked questions. We consider two feedback models: absolute and ranking. The absolute feedback is motivated by how humans assign relevance judgments in search [30, 57]. The ranking feedback is motivated by learning reward models in RLHF [39, 68, 36, 15, 77, 17]. We address both settings in a unified way. The key idea in our work is to generalize optimal designs [41, 45], a methodology for computing optimal information-gathering policies, to ranked lists. After the human feedback is collected, we learn preference models using existing estimators. Our method is statistically efficient, computationally efficient, and can be analyzed. We bound its prediction errors and ranking losses, in both absolute and ranking feedback models, and evaluate it empirically to show that it is practical. ", "page_idx": 9}, {"type": "text", "text": "Our work can be extended in several directions. First, we study only two models of human feedback: absolute and ranking. However, many feedback models exist [34]. One common property of these models is that learning of human preferences can be formulated as likelihood maximization. In such cases, an optimal design exists and can be used for human preference elicitation, exactly as in our work. Second, while we bound the prediction errors and ranking losses of Dope, we do not derive matching lower bounds. Therefore, although we believe that Dope is near optimal, we do not prove it. Third, we want to extend our methodology to the fixed-confidence setting. Finally, we want to apply our approach to learning a reward model in the LLM and evaluate it. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yasin Abbasi-Yadkori, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri. Improved algorithms for linear stochastic bandits. Advances in neural information processing systems, 24, 2011.   \n[2] Riad Akrour, Marc Schoenauer, and Mich\u00e8le Sebag. April: Active preference learningbased reinforcement learning. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2012, Bristol, UK, September 24-28, 2012. Proceedings, Part II 23, pages 116\u2013131. Springer, 2012.   \n[3] Jordan Ash, Surbhi Goel, Akshay Krishnamurthy, and Sham Kakade. Gone fishing: Neural active learning with fisher embeddings. Advances in Neural Information Processing Systems, 34, 2021.   \n[4] Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch active learning by diverse, uncertain gradient lower bounds. arXiv preprint arXiv:1906.03671, 2019.   \n[5] Jean-Yves Audibert, Sebastien Bubeck, and Remi Munos. Best arm identification in multiarmed bandits. In Proceedings of the 23rd Annual Conference on Learning Theory, pages 41\u201353, 2010.   \n[6] Mohammad Javad Azizi, Branislav Kveton, and Mohammad Ghavamzadeh. Fixed-budget best-arm identification in structured bandits. In Proceedings of the 31st International Joint Conference on Artificial Intelligence, 2022.   \n[7] Jackie Baek and Vivek Farias. Ts-ucb: Improving on thompson sampling with little to no additional computation. In International Conference on Artificial Intelligence and Statistics, pages 11132\u201311148. PMLR, 2023.   \n[8] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.   \n[9] Viktor Bengs, R\u00f3bert Busa-Fekete, Adil El Mesaoudi-Paul, and Eyke H\u00fcllermeier. Preferencebased online learning with dueling bandits: A survey. The Journal of Machine Learning Research, 22(1):278\u2013385, 2021.   \n[10] Erdem B\u0131y\u0131k, Nicolas Huynh, Mykel J Kochenderfer, and Dorsa Sadigh. Active preferencebased gaussian process regression for reward learning. arXiv preprint arXiv:2005.02575, 2020.   \n[11] Stephane Boucheron, Gabor Lugosi, and Pascal Massart. Concentration Inequalities: A Nonasymptotic Theory of Independence. Oxford University Press, 2013.   \n[12] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3-4):324\u2013345, 1952.   \n[13] Sebastien Bubeck, Remi Munos, and Gilles Stoltz. Pure exploration in multi-armed bandits problems. In Proceedings of the 20th International Conference on Algorithmic Learning Theory, pages 23\u201337, 2009.   \n[14] Christopher Burges. From RankNet to LambdaRank to LambdaMART: An overview. Technical Report MSR-TR-2010-82, Microsoft Research, 2010.   \n[15] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J\u00e9r\u00e9my Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217, 2023.   \n[16] Xiaoyu Chen, Han Zhong, Zhuoran Yang, Zhaoran Wang, and Liwei Wang. Human-inthe-loop: Provably efficient preference-based reinforcement learning with general function approximation. In International Conference on Machine Learning, pages 3773\u20133793. PMLR, 2022.   \n[17] Zhuotong Chen, Yifei Ma, Branislav Kveton, and Anoop Deoras. Active learning with crowd sourcing improves information retrieval. In ICML 2023 Workshop on Interactive Learning with Implicit Human Feedback, 2023.   \n[18] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.   \n[19] Aleksandr Chuklin, Ilya Markov, and Maarten De Rijke. Click models for web search. Springer Nature, 2022.   \n[20] Nirjhar Das, Souradip Chakraborty, Aldo Pacchiano, and Sayak Ray Chowdhury. Active preference optimization for sample efficient RLHF. CoRR, abs/2402.10500, 2024. URL https://arxiv.org/abs/2402.10500.   \n[21] Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex optimization. Journal of Machine Learning Research, 17(83):1\u20135, 2016.   \n[22] Miroslav Dudik, Dumitru Erhan, John Langford, and Lihong Li. Doubly robust policy evaluation and optimization. Statistical Science, 29(4):485\u2013511, 2014.   \n[23] Beyza Ermis, Patrick Ernst, Yannik Stein, and Giovanni Zappella. Learning to rank in the position based model with bandit feedback. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pages 2405\u20132412, 2020.   \n[24] Valerii Vadimovich Fedorov. Theory of optimal experiments. Elsevier, 2013.   \n[25] Tanner Fiez, Lalit Jain, Kevin G Jamieson, and Lillian Ratliff. Sequential experimental design for transductive linear bandits. Advances in neural information processing systems, 32, 2019.   \n[26] Sarah Filippi, Olivier Cappe, Aurelien Garivier, and Csaba Szepesvari. Parametric bandits: The generalized linear case. In Advances in Neural Information Processing Systems 23, pages 586\u2013594, 2010.   \n[27] Johannes F\u00fcrnkranz and Eyke H\u00fcllermeier. Pairwise preference learning and ranking. In European conference on machine learning, pages 145\u2013156. Springer, 2003.   \n[28] Alyssa Glass. Explaining preference learning, 2006.   \n[29] Joey Hejna and Dorsa Sadigh. Inverse preference learning: Preference-based rl without a reward function. arXiv preprint arXiv:2305.15363, 2023.   \n[30] Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. Fidelity, soundness, and efficiency of interleaved comparison methods. ACM Transactions on Information Systems, 31(4):1\u201343, 2013.   \n[31] Neil Houlsby, Ferenc Husz\u00e1r, Zoubin Ghahramani, and M\u00e1t\u00e9 Lengyel. Bayesian active learning for classification and preference learning. arXiv preprint arXiv:1112.5745, 2011.   \n[32] Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In International conference on machine learning, pages 427\u2013435. PMLR, 2013.   \n[33] Kevin Jamieson and Lalit Jain. Interactive machine learning. 2022.   \n[34] Hong Jun Jeon, Smitha Milli, and Anca Dragan. Reward-rational (implicit) choice: A unifying formalism for reward learning. In Advances in Neural Information Processing Systems 33, 2020.   \n[35] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offilne rl? In International Conference on Machine Learning, pages 5084\u20135096. PMLR, 2021.   \n[36] Yachen Kang, Diyuan Shi, Jinxin Liu, Li He, and Donglin Wang. Beyond reward: Offline preference-guided policy optimization, 2023.   \n[37] Julian Katz-Samuels, Lalit Jain, Kevin G Jamieson, et al. An empirical process approach to the union bound: Practical algorithms for combinatorial and linear bandits. Advances in Neural Information Processing Systems, 33:10371\u201310382, 2020.   \n[38] Julian Katz-Samuels, Jifan Zhang, Lalit Jain, and Kevin Jamieson. Improved algorithms for agnostic pool-based active classification. In International Conference on Machine Learning, pages 5334\u20135344. PMLR, 2021.   \n[39] Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke H\u00fcllermeier. A survey of reinforcement learning from human feedback, 2024.   \n[40] Maurice George Kendall. Rank correlation methods. 1948.   \n[41] Jack Kiefer and Jacob Wolfowitz. The equivalence of two extremum problems. Canadian Journal of Mathematics, 12:363\u2013366, 1960.   \n[42] Johannes Kirschner and Andreas Krause. Bias-robust Bayesian optimization via dueling bandits. In Proceedings of the 38th International Conference on Machine Learning, 2021.   \n[43] Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan. Cascading bandits: Learning to rank in the cascade model. In Proceedings of the 32nd International Conference on Machine Learning, 2015.   \n[44] Paul Lagree, Claire Vernade, and Olivier Cappe. Multiple-play bandits in the position-based model. In Advances in Neural Information Processing Systems 29, pages 1597\u20131605, 2016.   \n[45] Tor Lattimore and Csaba Szepesvari. Bandit Algorithms. Cambridge University Press, 2019.   \n[46] Tor Lattimore, Branislav Kveton, Shuai Li, and Csaba Szepesvari. TopRank: A practical algorithm for online stochastic ranking. In Advances in Neural Information Processing Systems 31, pages 3949\u20133958, 2018.   \n[47] Kimin Lee, Laura Smith, Anca Dragan, and Pieter Abbeel. B-pref: Benchmarking preferencebased reinforcement learning. arXiv preprint arXiv:2111.03026, 2021.   \n[48] Tyler Lekang and Andrew Lamperski. Simple algorithms for dueling bandits. arXiv preprint arXiv:1906.07611, 2019.   \n[49] John R Lepird, Michael P Owen, and Mykel J Kochenderfer. Bayesian preference elicitation for multiobjective engineering design optimization. Journal of Aerospace Information Systems, 12(10):634\u2013645, 2015.   \n[50] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661\u2013670, 2010.   \n[51] Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 297\u2013306, 2011.   \n[52] Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual bandits. In Proceedings of the 34th International Conference on Machine Learning, pages 2071\u20132080, 2017.   \n[53] Shuai Li, Baoxiang Wang, Shengyu Zhang, and Wei Chen. Contextual combinatorial cascading bandits. In Proceedings of the 33rd International Conference on Machine Learning, pages 1245\u20131253, 2016.   \n[54] Robert Duncan Luce. Individual Choice Behavior: A Theoretical Analysis. Dover Publications, 2005.   \n[55] Blake Mason, Kwang-Sung Jun, and Lalit Jain. An experimental design approach for regret minimization in logistic bandits. In Proceedings of the AAAI Conference on Artificial Intelligence, 2022.   \n[56] Viraj Mehta, Vikramjeet Das, Ojash Neopane, Yijia Dai, Ilija Bogunovic, Jeff Schneider, and Willie Neiswanger. Sample efficient reinforcement learning from human feedback via active exploration. CoRR, abs/2312.00267, 2023. URL https://arxiv.org/abs/2312.00267.   \n[57] MS MARCO. MS MARCO Dataset. https://microsoft.github.io/msmarco/, 2016.   \n[58] Subhojyoti Mukherjee, Ardhendu S Tripathy, and Robert Nowak. Chernoff sampling for active testing and extension to active regression. In International Conference on Artificial Intelligence and Statistics, pages 7384\u20137432. PMLR, 2022.   \n[59] Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 1999.   \n[60] Ellen Novoseller, Yibing Wei, Yanan Sui, Yisong Yue, and Joel Burdick. Dueling posterior sampling for preference-based reinforcement learning. In Conference on Uncertainty in Artificial Intelligence, pages 1029\u20131038. PMLR, 2020.   \n[61] Kweku A Opoku-Agyemang. Randomized controlled trials via reinforcement learning from human feedback. 2023.   \n[62] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.   \n[63] Barna Pasztor, Parnian Kassraie, and Andreas Krause. Bandits with preference feedback: A stackelberg game perspective. CoRR, abs/2406.16745, 2024. URL https://arxiv.org/abs/2406. 16745.   \n[64] Kaare Petersen and Michael Pedersen. The matrix cookbook. http://www2.compute.dtu.dk/ pubdb/pubs/3274-full.html, 2012.   \n[65] Robin Lewis Plackett. The analysis of permutations. Journal of the Royal Statistical Society: Series C (Applied Statistics), 24(2):193\u2013202, 1975.   \n[66] Friedrich Pukelsheim. Optimal Design of Experiments. Society for Industrial and Applied Mathematics, 2006.   \n[67] Filip Radlinski, Robert Kleinberg, and Thorsten Joachims. Learning diverse rankings with multi-armed bandits. In Proceedings of the 25th International Conference on Machine Learning, pages 784\u2013791, 2008.   \n[68] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=HPuSIXJaa9.   \n[69] Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offilne reinforcement learning and imitation learning: A tale of pessimism. Advances in Neural Information Processing Systems, 34:11702\u201311716, 2021.   \n[70] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B Gupta, Xiaojiang Chen, and Xin Wang. A survey of deep active learning. ACM Computing Surveys (CSUR), 54 (9):1\u201340, 2021.   \n[71] Dorsa Sadigh, Anca Dragan, Shankar Sastry, and Sanjit Seshia. Active preference-based learning of reward functions. 2017.   \n[72] Aadirupa Saha. Optimal algorithms for stochastic contextual preference bandits. In Advances in Neural Information Processing Systems 34, 2021.   \n[73] Aadirupa Saha and Pierre Gaillard. Versatile dueling bandits: Best-of-both-world analyses for online learning from preferences. arXiv preprint arXiv:2202.06694, 2022.   \n[74] Aadirupa Saha and Akshay Krishnamurthy. Efficient and optimal algorithms for contextual dueling bandits under realizability. In Proceedings of the 33rd International Conference on Algorithmic Learning Theory, 2022.   \n[75] Aadirupa Saha, Aldo Pacchiano, and Jonathan Lee. Dueling rl: Reinforcement learning with trajectory preferences. In International Conference on Artificial Intelligence and Statistics, pages 6263\u20136289. PMLR, 2023.   \n[76] Ayush Sekhari, Karthik Sridharan, Wen Sun, and Runzhe Wu. Contextual bandits and imitation learning with preference-based active queries. Advances in Neural Information Processing Systems, 36, 2024.   \n[77] Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, and Deyi Xiong. Large language model alignment: A survey. arXiv preprint arXiv:2309.15025, 2023.   \n[78] Marta Soare, Alessandro Lazaric, and Remi Munos. Best-arm identification in linear bandits. In Advances in Neural Information Processing Systems 27, pages 828\u2013836, 2014.   \n[79] Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned text embeddings. 2022. URL https://arxiv.org/abs/2212.09741.   \n[80] Yanan Sui, Masrour Zoghi, Katja Hofmann, and Yisong Yue. Advancements in dueling bandits. In IJCAI, pages 5502\u20135510, 2018.   \n[81] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[82] Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged bandit feedback. In Proceedings of the 32nd International Conference on Machine Learning, pages 814\u2013823, 2015.   \n[83] Bal\u00e1zs Sz\u00f6r\u00e9nyi, R\u00f3bert Busa-Fekete, Adil Paul, and Eyke H\u00fcllermeier. Online rank elicitation for plackett-luce: A dueling bandits approach. Advances in neural information processing systems, 28, 2015.   \n[84] Shion Takeno, Masahiro Nomura, and Masayuki Karasuyama. Towards practical preferential Bayesian optimization with skew Gaussian processes. In Proceedings of the 40th International Conference on Machine Learning, 2023.   \n[85] EM Voorhees. Proceedings of the 8th text retrieval conference. TREC-8 Question Answering Track Report, pages 77\u201382, 1999.   \n[86] Yining Wang, Liwei Wang, Yuanzhi Li, Di He, and Tie-Yan Liu. A theoretical analysis of ndcg type ranking measures. In Conference on learning theory, pages 25\u201354. PMLR, 2013.   \n[87] Yuanhao Wang, Qinghua Liu, and Chi Jin. Is rlhf more difficult than standard rl? a theoretical perspective. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[88] Christian Wirth, Riad Akrour, Gerhard Neumann, Johannes F\u00fcrnkranz, et al. A survey of preference-based reinforcement learning methods. Journal of Machine Learning Research, 18 (136):1\u201346, 2017.   \n[89] R. Wolke and H. Schwetlick. Iteratively reweighted least squares: Algorithms, convergence analysis, and numerical comparisons. SIAM Journal on Scientific and Statistical Computing, 9 (5):907\u2013921, 1988.   \n[90] Runzhe Wu and Wen Sun. Making rl with preference-based feedback efficient via randomization. arXiv preprint arXiv:2310.14554, 2023.   \n[91] Wei Xiong, Hanze Dong, Chenlu Ye, Han Zhong, Nan Jiang, and Tong Zhang. Gibbs sampling from human feedback: A provable kl-constrained framework for rlhf. arXiv preprint arXiv:2312.11456, 2023.   \n[92] Wenjie Xu, Wenbin Wang, Yuning Jiang, Bratislav Svetozarevic, and Colin Jones. Principled preferential Bayesian optimization. In Proceedings of the 41th International Conference on Machine Learning, 2024.   \n[93] Yichong Xu, Aparna Joshi, Aarti Singh, and Artur Dubrawski. Zeroth order non-convex optimization with dueling-choice bandits. In Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence, 2020.   \n[94] Yichong Xu, Ruosong Wang, Lin Yang, Aarti Singh, and Artur Dubrawski. Preference-based reinforcement learning with finite-time guarantees. Advances in Neural Information Processing Systems, 33:18784\u201318794, 2020.   \n[95] Junwen Yang and Vincent Tan. Minimax optimal fixed-budget best arm identification in linear bandits. In Advances in Neural Information Processing Systems 35, 2022.   \n[96] Michelle Yuan, Hsuan-Tien Lin, and Jordan Boyd-Graber. Cold-start active learning through self-supervised language modeling. arXiv preprint arXiv:2010.09535, 2020.   \n[97] Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The $\\mathbf{k}$ -armed dueling bandits problem. Journal of Computer and System Sciences, 78(5):1538\u20131556, 2012.   \n[98] Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic methods for offilne reinforcement learning. Advances in neural information processing systems, 34:13626\u201313640, 2021.   \n[99] Wenhao Zhan, Masatoshi Uehara, Wen Sun, and Jason Lee. Provable reward-agnostic preference-based reinforcement learning. In Proceedings of the 12th International Conference on Learning Representations, 2024.   \n[100] Tianchen Zhou, Jia Liu, Yang Jiao, Chaosheng Dong, Yetian Chen, Yan Gao, and Yi Sun. Bandit learning to rank with position-based click models: Personalized and equal treatments. arXiv preprint arXiv:2311.04528, 2023.   \n[101] Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao. Starling-7b: Improving llm helpfulness & harmlessness with rlaif, November 2023.   \n[102] Banghua Zhu, Jiantao Jiao, and Michael Jordan. Principled reinforcement learning with human feedback from pairwise or $K$ -wise comparisons. CoRR, abs/2301.11270, 2023. URL https://arxiv.org/abs/2301.11270.   \n[103] Yinglun Zhu, Dongruo Zhou, Ruoxi Jiang, Quanquan Gu, Rebecca Willett, and Robert Nowak. Pure exploration in kernel and neural bandits. Advances in neural information processing systems, 34:11618\u201311630, 2021.   \n[104] Masrour Zoghi, Tomas Tunys, Mohammad Ghavamzadeh, Branislav Kveton, Csaba Szepesvari, and Zheng Wen. Online learning to rank in stochastic click models. In Proceedings of the 34th International Conference on Machine Learning, 2017.   \n[105] Shi Zong, Hao Ni, Kenny Sung, Nan Rosemary Ke, Zheng Wen, and Branislav Kveton. Cascading bandits for large-scale recommendation problems. In Proceedings of the 32nd Conference on Uncertainty in Artificial Intelligence, 2016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This section contains proofs of our main claims. ", "page_idx": 16}, {"type": "text", "text": "A.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We follow the sketch of the proof in Section 21.1 of Lattimore and Szepesvari [45] and adapt it to matrices. Before we start, we prove several helpful claims. ", "page_idx": 16}, {"type": "text", "text": "First, using (43) in Petersen and Pedersen [64], we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial\\pi(i)}f(\\pi)=\\frac{\\partial}{\\partial\\pi(i)}\\log\\operatorname*{det}(\\mathbf{V}_{\\pi})=\\operatorname{tr}\\left(\\mathbf{V}_{\\pi}^{-1}\\frac{\\partial}{\\partial\\pi(i)}\\mathbf{V}_{\\pi}\\right)=\\operatorname{tr}(\\mathbf{V}_{\\pi}^{-1}\\mathbf{A}_{i}\\mathbf{A}_{i}^{\\top})=\\operatorname{tr}(\\mathbf{A}_{i}^{\\top}\\mathbf{V}_{\\pi}^{-1}\\mathbf{A}_{i})\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In the last step, we use the cyclic property of the trace. We define the gradient of $f(\\pi)$ with respect to $\\pi$ as $\\nabla f(\\pi)=(\\operatorname{tr}(\\mathbf{A}_{i}^{\\top}\\mathbf{V}_{\\pi}^{-1}\\mathbf{A}_{i}))_{i=1}^{L}$ . Second, using basic properties of the trace, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{i=1}^{L}\\pi(i)\\operatorname{tr}({\\bf A}_{i}^{\\top}{\\bf V}_{\\pi}^{-1}{\\bf A}_{i})=\\sum_{i=1}^{L}\\pi(i)\\operatorname{tr}({\\bf V}_{\\pi}^{-1}{\\bf A}_{i}{\\bf A}_{i}^{\\top})=\\operatorname{tr}\\left(\\sum_{i=1}^{L}\\pi(i){\\bf V}_{\\pi}^{-1}{\\bf A}_{i}{\\bf A}_{i}^{\\top}\\right)}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~=\\operatorname{tr}\\left({\\bf V}_{\\pi}^{-1}\\sum_{i=1}^{L}\\pi(i){\\bf A}_{i}{\\bf A}_{i}^{\\top}\\right)=\\operatorname{tr}(I_{d})=d\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, for any distribution $\\pi$ , (12) implies ", "page_idx": 16}, {"type": "equation", "text": "$$\ng(\\pi)=\\operatorname*{max}_{i\\in[L]}\\operatorname{tr}(\\mathbf{A}_{i}^{\\top}\\mathbf{V}_{\\pi}^{-1}\\mathbf{A}_{i})\\geq\\sum_{i=1}^{L}\\pi(i)\\operatorname{tr}(\\mathbf{A}_{i}^{\\top}\\mathbf{V}_{\\pi}^{-1}\\mathbf{A}_{i})=d\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we are ready to start the proof. ", "page_idx": 16}, {"type": "text", "text": "$(b)\\Rightarrow(a)$ : Let $\\pi_{*}$ be a maximizer of $f(\\pi)$ . By first-order optimality conditions, for any distribution $\\pi$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle0\\geq\\langle\\nabla f(\\pi_{*}),\\pi-\\pi_{*}\\rangle=\\displaystyle\\sum_{i=1}^{L}\\pi(i)\\,\\mathrm{tr}(\\mathbf{A}_{i}^{\\top}\\mathbf{V}_{\\pi_{*}}^{-1}\\mathbf{A}_{i})-\\displaystyle\\sum_{i=1}^{L}\\pi_{*}(i)\\,\\mathrm{tr}(\\mathbf{A}_{i}^{\\top}\\mathbf{V}_{\\pi_{*}}^{-1}\\mathbf{A}_{i})}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\sum_{i=1}^{L}\\pi(i)\\,\\mathrm{tr}(\\mathbf{A}_{i}^{\\top}\\mathbf{V}_{\\pi_{*}}^{-1}\\mathbf{A}_{i})-d\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In the last step, we use (12). Since this inequality holds for any distribution $\\pi$ , including Dirac at $i$ for any $i\\in[L]$ , we have that $\\begin{array}{r}{d\\geq\\operatorname*{max}_{i\\in[L]}\\mathrm{\\dot{tr}}(\\mathbf{A}_{i}^{\\top}\\mathbf{V}_{\\pi_{*}}^{-1}\\mathbf{A}_{i})=\\dot{g}(\\pi_{*})}\\end{array}$ . Finally, (13) says that for any distribution $\\pi$ $\\cdot,g(\\pi)\\geq d$ . Therefore, $\\pi_{*}$ must be a minimizer of $g(\\pi)$ and $g(\\pi_{*})=d$ . ", "page_idx": 16}, {"type": "text", "text": "$(c)\\Rightarrow(b)$ : Note that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle\\nabla f(\\pi_{*}),\\pi-\\pi_{*}\\rangle=\\sum_{i=1}^{L}\\pi(i)\\operatorname{tr}(\\mathbf{A}_{i}^{\\top}\\mathbf{V}_{\\pi_{*}}^{-1}\\mathbf{A}_{i})-d\\leq\\operatorname*{max}_{i\\in[L]}\\operatorname{tr}(\\mathbf{A}_{i}^{\\top}\\mathbf{V}_{\\pi_{*}}^{-1}\\mathbf{A}_{i})-d=g(\\pi_{*})-d\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "holds for any distributions $\\pi$ and $\\pi_{*}$ . Since $g(\\pi_{*})=d$ , we have $\\langle\\nabla f(\\pi_{*}),\\pi-\\pi_{*}\\rangle\\leq0$ . Therefore, by first-order optimality conditions, $\\pi_{*}$ is a maximizer of $f(\\pi)$ . ", "page_idx": 16}, {"type": "text", "text": "$(a)\\Rightarrow(c)$ : This follows from the same argument as in $(b)\\Rightarrow(a)$ . In particular, we show there that the maximizer $\\pi_{*}$ of $f(\\pi)$ is the minimizer of $g(\\pi)$ , and that $g(\\pi_{*})=d$ . ", "page_idx": 16}, {"type": "text", "text": "To prove that $|\\mathrm{supp}\\left(\\pi_{*}\\right)|\\leq d(d+1)/2$ , we argue that $\\pi_{*}$ can be substituted for a distribution with a lower support whenever $|\\mathrm{supp}\\left(\\pi_{*}\\right)|>d(d+1)/2$ . The claim then follows by induction. ", "page_idx": 16}, {"type": "text", "text": "Let $S=\\operatorname{supp}\\left(\\pi_{*}\\right)$ and suppose that $|S|>d(d+1)/2$ . We start with designing a suitable family of optimal solutions. Since the space of $d\\times d$ symmetric matrices has $d({\\bar{d}}\\,{\\bar{+}}\\,1)\\bar{/}2$ dimensions, there must exist an $L$ -dimensional vector $\\eta$ such that supp $(\\eta)\\subseteq S$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i\\in S}\\eta(i)\\mathbf{A}_{i}\\mathbf{A}_{i}^{\\top}=\\mathbf{0}_{d,d}\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathbf{0}_{d,d}$ is a $d\\times d$ zero matrix. Let $\\pi_{t}=\\pi_{*}+t\\eta$ for $t\\geq0$ . An important property of $\\pi_{t}$ is that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\log\\operatorname*{det}(\\mathbf{V}_{\\pi_{t}})=\\log\\operatorname*{det}\\left(\\mathbf{V}_{\\pi_{*}}+t\\sum_{i\\in S}\\eta(i)\\mathbf{A}_{i}\\mathbf{A}_{i}^{\\top}\\right)=\\log\\operatorname*{det}(\\mathbf{V}_{\\pi_{*}})\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, $\\pi_{t}$ is also an optimal solution. However, it may not be a distribution. ", "page_idx": 17}, {"type": "text", "text": "We prove that $\\pi_{t}\\in\\Delta^{L}$ , for some $t>0$ , as follows. First, note that $\\mathrm{tr}(\\mathbf{A}_{i}^{\\top}\\mathbf{V}_{\\pi_{*}}^{-1}\\mathbf{A}_{i})=d$ holds for any $i\\in S$ . Otherwise $\\pi_{*}$ could be improved. Using this observation, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle d\\sum_{i\\in S}\\boldsymbol{\\eta}(i)=\\sum_{i\\in S}\\boldsymbol{\\eta}(i)\\,\\mathrm{tr}(\\mathbf{A}_{i}^{\\top}\\mathbf{V}_{\\pi_{*}}^{-1}\\mathbf{A}_{i})=\\sum_{i\\in S}\\boldsymbol{\\eta}(i)\\,\\mathrm{tr}(\\mathbf{V}_{\\pi_{*}}^{-1}\\mathbf{A}_{i}\\mathbf{A}_{i}^{\\top})}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~=\\mathrm{tr}\\left(\\mathbf{V}_{\\pi_{*}}^{-1}\\sum_{i\\in S}\\boldsymbol{\\eta}(i)\\mathbf{A}_{i}\\mathbf{A}_{i}^{\\top}\\right)=0\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last equality follows from (14). This further implies that $\\textstyle\\sum_{i\\in S}\\eta(i)=0$ , and in turn that $\\pi_{t}\\in\\Delta^{L}$ , for as long as $\\pi_{t}\\geq\\mathbf{0}_{L}$ . ", "page_idx": 17}, {"type": "text", "text": "Finally, we take the largest feasible $t$ , $\\tau=\\operatorname*{max}\\{t>0:\\pi_{t}\\in\\Delta^{L}\\}$ , and note that $\\pi_{\\tau}$ has at least one more non-zero entry than $\\pi_{*}$ while having the same value. This concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "A.2 Proof of Lemma 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We start by noting that for any list $i\\in[L]$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\mathbf{a}\\in\\mathbf{A}_{i}}\\|\\mathbf{a}\\|_{\\bar{\\mathbf{\\Sigma}}_{n}^{-1}}^{2}=\\mathrm{tr}(\\mathbf{A}_{i}^{\\top}\\bar{\\mathbf{\\Sigma}}_{n}^{-1}\\mathbf{A}_{i})=\\mathrm{tr}\\left(\\mathbf{A}_{i}^{\\top}\\left(\\displaystyle\\sum_{t=1}^{n}\\sum_{k=1}^{K}\\mathbf{x}_{I_{t},k}\\mathbf{x}_{I_{t},k}^{\\top}\\right)^{-1}\\mathbf{A}_{i}\\right)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad=\\frac{1}{n}\\,\\mathrm{tr}\\left(\\mathbf{A}_{i}^{\\top}\\left(\\displaystyle\\sum_{i=1}^{L}\\pi_{*}(i)\\displaystyle\\sum_{k=1}^{K}\\mathbf{x}_{i,k}\\mathbf{x}_{i,k}^{\\top}\\right)^{-1}\\mathbf{A}_{i}\\right)=\\frac{1}{n}\\,\\mathrm{tr}(\\mathbf{A}_{i}^{\\top}\\mathbf{V}_{\\pi_{*}}^{-1}\\mathbf{A}_{i})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The third equality holds because all $n\\pi_{*}(i)$ are integers and $n>0$ . In this case, the optimal design is exact and $\\bar{\\pmb{\\Sigma}}_{n}$ invertible, because all of its eigenvalues are positive. Now we use the definition of $g(\\pi_{*})$ , apply Theorem 1, and get that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[L]}\\mathrm{tr}(\\mathbf{A}_{i}^{\\top}\\mathbf{V}_{\\pi_{*}}^{-1}\\mathbf{A}_{i})=g(\\pi_{*})=d\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "A.3 Proof of Theorem 3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For any list $i\\in[L]$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{tr}(\\mathbf{A}_{i}^{\\top}(\\hat{\\theta}_{n}-\\theta_{*})(\\hat{\\theta}_{n}-\\theta_{*})^{\\top}\\mathbf{A}_{i})=\\displaystyle\\sum_{\\mathbf{a}\\in\\mathbf{A}_{i}}(\\mathbf{a}^{\\top}(\\hat{\\theta}_{n}-\\theta_{*}))^{2}=\\displaystyle\\sum_{\\mathbf{a}\\in\\mathbf{A}_{i}}(\\mathbf{a}^{\\top}\\bar{\\Sigma}_{n}^{-1/2}\\bar{\\Sigma}_{n}^{1/2}(\\hat{\\theta}_{n}-\\theta_{*}))^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{\\mathbf{a}\\in\\mathbf{A}_{i}}\\|\\mathbf{a}\\|_{\\bar{\\Sigma}_{n}^{-1}}^{2}\\|\\hat{\\theta}_{n}-\\theta_{*}\\|_{\\bar{\\Sigma}_{n}}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last step follows from the Cauchy-Schwarz inequality. It follows that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{i\\in[L]}{\\operatorname*{max}}\\mathrm{tr}(\\mathbf A_{i}^{\\top}(\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*})(\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*})^{\\top}\\mathbf A_{i})\\leq\\underset{i\\in[L]}{\\operatorname*{max}}\\sum_{\\mathbf a\\in\\mathbf A_{i}}\\|\\mathbf a\\|_{\\bar{\\Sigma}_{n}^{-1}}^{2}\\|\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*}\\|_{\\bar{\\Sigma}_{n}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\underset{i\\in[L]}{\\operatorname*{max}}\\sum_{\\mathbf a\\in\\mathbf A_{i}}\\|\\mathbf a\\|_{\\bar{\\Sigma}_{n}^{-1}}^{2}\\underset{\\mathrm{Part}}{\\underbrace{\\|\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*}\\|_{\\bar{\\Sigma}_{n}}^{2}}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we use $\\bar{\\Sigma}_{n}=n\\Sigma_{n}$ in the last step. ", "page_idx": 17}, {"type": "text", "text": "Part I captures the efficiency of data collection and depends on the optimal design. By Lemma 2, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[L]}\\sum_{{\\bf a}\\in{\\bf A}_{i}}\\|{\\bf a}\\|_{\\bar{\\bf{\\Sigma}}_{n}^{-1}}^{2}=\\frac{d}{n}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Part II measures how the estimated model parameter $\\widehat{\\pmb{\\theta}}_{n}$ differs from the true model parameter $\\pmb{\\theta}_{*}$ , under the empirical covariance matrix $\\Sigma_{n}$ . For Part II, we use Lemma 7 and get that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Vert\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*}\\Vert_{\\pmb{\\Sigma}_{n}}^{2}\\leq\\frac{16d+8\\log(1/\\delta)}{n}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "holds with probability at least $1-\\delta$ . The main claim follows from combining the upper bounds on Parts I and $\\mathrm{II}$ . ", "page_idx": 18}, {"type": "text", "text": "A.4 Proof of Theorem 4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "From the definition of ranking loss, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{R}_{n}]=\\sum_{i=1}^{L}\\sum_{j=1}^{K}\\sum_{k=j+1}^{K}\\mathbb{E}[\\mathbb{I}\\{\\hat{\\sigma}_{n,i}(j)>\\hat{\\sigma}_{n,i}(k)\\}]=\\sum_{i=1}^{L}\\sum_{j=1}^{K}\\sum_{k=j+1}^{K}\\mathbb{P}\\left(\\mathbf{x}_{i,j}^{\\top}\\hat{\\pmb{\\theta}}_{n}<\\mathbf{x}_{i,k}^{\\top}\\hat{\\pmb{\\theta}}_{n}\\right)\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In the rest of the proof, we bound each term separately. Specifically, for any list $i\\in[L]$ and items $(j,k)\\in\\Pi_{2}(K)$ in it, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left(\\mathbf{x}_{i,j}^{\\top}\\hat{\\boldsymbol{\\theta}}_{n}<\\mathbf{x}_{i,k}^{\\top}\\hat{\\boldsymbol{\\theta}}_{n}\\right)=\\mathbb{P}\\left(\\mathbf{x}_{i,k}^{\\top}\\hat{\\boldsymbol{\\theta}}_{n}-\\mathbf{x}_{i,j}^{\\top}\\hat{\\boldsymbol{\\theta}}_{n}>0\\right)}&{}\\\\ {=\\mathbb{P}\\left(\\mathbf{x}_{i,k}^{\\top}\\hat{\\boldsymbol{\\theta}}_{n}-\\mathbf{x}_{i,j}^{\\top}\\hat{\\boldsymbol{\\theta}}_{n}+\\Delta_{i,j,k}>\\Delta_{i,j,k}\\right)}&{}\\\\ {=\\mathbb{P}\\left(\\mathbf{x}_{i,k}^{\\top}\\hat{\\boldsymbol{\\theta}}_{n}-\\mathbf{x}_{i,j}^{\\top}\\hat{\\boldsymbol{\\theta}}_{n}+\\mathbf{x}_{i,j}^{\\top}\\theta_{*}-\\mathbf{x}_{i,k}^{\\top}\\theta_{*}>\\Delta_{i,j,k}\\right)}&{}\\\\ {=\\mathbb{P}\\left(\\mathbf{x}_{i,k}^{\\top}(\\hat{\\boldsymbol{\\theta}}_{n}-\\theta_{*})+\\mathbf{x}_{i,j}^{\\top}(\\theta_{*}-\\hat{\\boldsymbol{\\theta}}_{n})>\\Delta_{i,j,k}\\right)}&{}\\\\ {\\ }&{\\leq\\mathbb{P}\\left(\\mathbf{x}_{i,k}^{\\top}(\\hat{\\boldsymbol{\\theta}}_{n}-\\theta_{*})>\\frac{\\Delta_{i,j,k}}{2}\\right)+\\mathbb{P}\\left(\\mathbf{x}_{i,j}^{\\top}(\\theta_{*}-\\hat{\\boldsymbol{\\theta}}_{n})>\\frac{\\Delta_{i,j,k}}{2}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In the third equality, we use that $\\Delta_{i,j,k}=(\\mathbf{x}_{i,j}-\\mathbf{x}_{i,k})^{\\top}\\pmb{\\theta}_{\\ast}$ . The last step follows from the fact that event $A+B>c$ occurs only if $A>c/2$ or $B>c/2$ . ", "page_idx": 18}, {"type": "text", "text": "Now we bound $\\mathbb{P}\\left(\\mathbf{x}_{i,k}^{\\top}(\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*})>\\Delta_{i,j,k}/2\\right)$ and note that the other term can be bounded analogously. Specifically, we apply Lemmas 2 and 8, and get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathbf{x}_{i,k}^{\\top}(\\widehat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*})>\\frac{\\Delta_{i,j,k}}{2}\\right)\\leq\\exp\\left[-\\frac{\\Delta_{i,j,k}^{2}}{4\\|\\mathbf{x}_{i,k}\\|_{\\widehat{\\pmb{\\Sigma}}_{n}^{-1}}^{2}}\\right]\\leq\\exp\\left[-\\frac{\\Delta_{i,j,k}^{2}n}{4d}\\right]\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 18}, {"type": "text", "text": "The above approach relies on the concentration of $\\mathbf{x}_{i,k}^{\\top}(\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{\\ast})$ , which is proved in Lemma 8. An alternative way of proving is a similar result is through the Cauchy-Schwarz inequality. This is useful when a high-probability bound on $\\lVert\\hat{{\\boldsymbol{\\theta}}}_{n}-{\\boldsymbol{\\theta}}_{\\ast}\\rVert_{\\pmb{\\Sigma}_{n}}^{2}$ is available, as in Appendix A.6. Specifically, by the Cauchy-Schwarz inequality ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\mathbf{x}_{i,k}^{\\top}(\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*})>\\frac{\\Delta_{i,j,k}}{2}\\right)\\leq\\mathbb{P}\\left(\\|\\mathbf{x}_{i,k}\\|_{\\Sigma_{n}^{-1}}\\|\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*}\\|_{\\Sigma_{n}}>\\frac{\\Delta_{i,j,k}}{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{P}\\left(\\|\\mathbf{x}_{i,k}\\|_{\\Sigma_{n}^{-1}}^{2}\\|\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*}\\|_{\\Sigma_{n}}^{2}>\\frac{\\Delta_{i,j,k}^{2}}{4}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\mathbb{P}\\left(\\|\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*}\\|_{\\Sigma_{n}}^{2}>\\frac{\\Delta_{i,j,k}^{2}}{4d}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In the second inequality, we use that $\\|\\mathbf{x}_{i,k}\\|_{\\Sigma_{n}^{-1}}^{2}=n\\|\\mathbf{x}_{i,k}\\|_{\\bar{\\Sigma}_{n}^{-1}}^{2}\\leq d$ , which follows from Lemma 2. Finally, Lemma 7 says that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\|\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*}\\|_{\\pmb{\\Sigma}_{n}}^{2}\\geq\\frac{16d+8\\log(1/\\delta)}{n}\\right)\\leq\\delta\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "holds for any $\\delta>0$ . To apply this bound, we let ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{16d+8\\log(1/\\delta)}{n}=\\frac{\\Delta_{i,j,k}^{2}}{4d}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and express $\\delta$ . This leads to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\Vert\\hat{\\theta}_{n}-\\theta_{*}\\Vert_{\\Sigma_{n}}^{2}>\\frac{\\Delta_{i,j,k}^{2}}{4d}\\right)\\leq\\delta=\\exp\\left[-\\frac{\\Delta_{i,j,k}^{2}n}{32d}+2d\\right]\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which concludes the alternative proof. ", "page_idx": 19}, {"type": "text", "text": "A.5 Proof of Theorem 5 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Following the same steps as in Appendix A.3, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[L]}\\mathrm{tr}(\\mathbf{A}_{i}^{\\top}(\\hat{\\theta}_{n}-\\theta_{*})(\\hat{\\theta}_{n}-\\theta_{*})^{\\top}\\mathbf{A}_{i})\\leq\\underbrace{\\operatorname*{max}_{i\\in[L]}\\sum_{\\mathbf{a}\\in\\mathbf{A}_{i}}\\Vert\\mathbf{a}\\Vert_{\\bar{\\Sigma}_{n}^{-1}}^{2}}_{\\mathbf{partI}}\\underbrace{\\frac{K(K-1)n}{2}\\Vert\\hat{\\theta}_{n}-\\theta_{*}\\Vert_{\\Sigma_{n}}^{2}}_{\\mathrm{PartII}}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Part I captures the efficiency of data collection and depends on the optimal design. By Lemma 2, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[L]}\\sum_{{\\bf a}\\in{\\bf A}_{i}}\\|{\\bf a}\\|_{\\bar{\\bf{\\Sigma}}_{n}^{-1}}^{2}=\\frac{d}{n}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Part II measures how the estimated model parameter $\\widehat{\\pmb{\\theta}}_{n}$ differs from the true model parameter $\\pmb{\\theta}_{*}$ , under the empirical covariance matrix $\\pmb{\\Sigma}_{n}$ . For Part II, we use Lemma 9 (restatement of Theorem 4.1 in Zhu et al. [102]) and get that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*}\\|_{\\pmb{\\Sigma}_{n}}^{2}\\leq\\frac{C K^{4}(d+\\log(1/\\delta))}{n}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "holds with probability at least $1-\\delta$ , where $C>0$ is some constant. The main claim follows from combining the upper bounds on Parts I and $\\mathrm{II}$ . ", "page_idx": 19}, {"type": "text", "text": "A.6 Proof of Theorem 6 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Following the same steps as in Appendix A.4, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left({\\mathbf x}_{i,j}^{\\top}\\hat{\\boldsymbol\\theta}_{n}<{\\mathbf x}_{i,k}^{\\top}\\hat{\\boldsymbol\\theta}_{n}\\right)=\\mathbb{P}\\left({\\mathbf x}_{i,k}^{\\top}(\\hat{\\boldsymbol\\theta}_{n}-{\\boldsymbol\\theta}_{*})+{\\mathbf x}_{i,j}^{\\top}(\\theta_{*}-\\hat{\\boldsymbol\\theta}_{n})>\\Delta_{i,j,k}\\right)}&{}\\\\ {=\\mathbb{P}\\left({\\mathbf z}_{i,j,k}^{\\top}(\\theta_{*}-\\hat{\\boldsymbol\\theta}_{n})>\\Delta_{i,j,k}\\right)}&{}\\\\ {\\ }&{\\leq\\mathbb{P}\\left(\\|{\\mathbf z}_{i,j,k}\\|_{\\mathbf{z}_{n}^{-1}}\\|\\hat{\\boldsymbol\\theta}_{n}-{\\boldsymbol\\theta}_{*}\\|_{\\mathbf{z}_{n}}>\\Delta_{i,j,k}\\right)}\\\\ {=\\mathbb{P}\\left(\\|{\\mathbf z}_{i,j,k}\\|_{\\mathbf{z}_{n}^{-1}}\\|\\hat{\\boldsymbol\\theta}_{n}-{\\boldsymbol\\theta}_{*}\\|_{\\mathbf{z}_{n}}^{2}>\\Delta_{i,j,k}^{2}\\right)}\\\\ {\\ }&{\\leq\\mathbb{P}\\left(\\|\\hat{\\boldsymbol\\theta}_{n}-{\\boldsymbol\\theta}_{*}\\|_{\\mathbf{z}_{n}}^{2}>\\frac{\\Delta_{i,j,k}^{2}}{d}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In the first inequality, we use the Cauchy-Schwarz inequality. In the second inequality, we use that $\\|\\mathbf{z}_{i,j,k}\\|_{\\pmb{\\Sigma}_{n}^{-1}}^{2}=\\dot{n}\\|\\pmb{\\mathbf{z}}_{i,j,k}\\|_{\\bar{\\pmb{\\Sigma}}_{n}^{-1}}^{2}\\leq d$ , which follows from Lemma 2. Finally, Lemma 9 says that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\|\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*}\\|_{\\pmb{\\Sigma}_{n}}^{2}\\geq\\frac{C K^{4}(d+\\log(1/\\delta))}{n}\\right)\\leq\\delta\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "holds for any $\\delta>0$ . To apply this bound, we let ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{C K^{4}(d+\\log(1/\\delta))}{n}=\\frac{\\Delta_{i,j,k}^{2}}{d}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and express $\\delta$ . This leads to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\|\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*}\\|_{\\pmb{\\Sigma}_{n}}^{2}>\\frac{\\Delta_{i,j,k}^{2}}{d}\\right)\\le\\delta=\\exp\\left[-\\frac{\\Delta_{i,j,k}^{2}n}{C K^{4}d}+d\\right]\\,,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 20}, {"type": "text", "text": "B Supporting Lemmas ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This section contains our supporting lemmas and their proofs. ", "page_idx": 20}, {"type": "text", "text": "Lemma 7. Consider the absolute feedback model in Section 4. Fix $\\delta\\in(0,1)$ . Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Vert\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*}\\Vert_{\\pmb{\\Sigma}_{n}}^{2}\\leq\\frac{16d+8\\log(1/\\delta)}{n}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "holds with probability at least $1-\\delta$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Let $\\mathbf{X}\\in\\mathbb{R}^{K n\\times d}$ be a matrix of $K n$ feature vectors in (7) and $\\mathbf{y}\\in\\mathbb{R}^{K n}$ be a vector of the corresponding $K n$ observations. Under 1-sub-Gaussian noise in (1), we can rewrite $\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{\\ast}$ as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*}=(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}(\\mathbf{y}-\\mathbf{X}\\pmb{\\theta}_{*})=(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\boldsymbol{\\eta}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\eta\\in\\mathbb{R}^{K n}$ is a vector of independent 1-sub-Gaussian noise. Now note that $\\mathbf{a}^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}$ is a fixed vector of length $K n$ for any fixed $\\mathbf{a}\\in\\mathbb{R}^{d}$ . Therefore, ${\\bf a}^{\\top}(\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{\\ast})$ is a sub-Gaussian random variable with a variance proxy ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{a}^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{X}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{a}=\\mathbf{a}^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{a}=\\left\\|\\mathbf{a}\\right\\|_{(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}}^{2}=\\left\\|\\mathbf{a}\\right\\|_{\\Sigma_{n}^{-1}}^{2}/n\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "From standard concentration inequalities for sub-Gaussian random variables [11], ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathbf{a}^{\\top}(\\widehat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*})\\geq\\sqrt{\\frac{2\\|\\mathbf{a}\\|_{\\mathbf{\\widehat{Z}}_{n}^{-1}}^{2}\\log(1/\\delta)}{n}}\\right)\\leq\\delta\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "holds for any fixed $\\mathbf{a}\\in\\mathbb{R}^{d}$ . To bound $\\lVert\\hat{{\\pmb\\theta}}_{n}-{\\pmb\\theta}_{\\ast}\\rVert_{{\\pmb\\Sigma}_{n}}$ , we take advantage of the fact that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Vert\\widehat{\\theta}_{n}-\\theta_{*}\\Vert_{\\Sigma_{n}}=\\langle\\widehat{\\theta}_{n}-\\theta_{*},\\Sigma_{n}^{1/2}A\\rangle\\,,\\quad A=\\frac{\\Sigma_{n}^{1/2}(\\widehat{\\theta}_{n}-\\theta_{*})}{\\Vert\\widehat{\\theta}_{n}-\\theta_{*}\\Vert_{\\Sigma_{n}}}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "While $A\\in\\ensuremath{\\mathbb{R}}^{d}$ is random, it has two important properties. First, its length is 1. Second, if it was fixed, we could apply (15) and would get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\langle\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*},\\Sigma_{n}^{1/2}A\\rangle\\geq\\sqrt{\\frac{2\\log(1/\\delta)}{n}}\\right)\\leq\\delta\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To eliminate the randomness in $A$ , we use a coverage argument. ", "page_idx": 20}, {"type": "text", "text": "Let $S=\\{\\mathbf{a}\\in\\mathbb{R}^{d}:\\|a\\|_{2}=1\\}$ be a unit sphere. Lemma 20.1 in Lattimore and Szepesvari [45] says that there exists an $\\varepsilon$ -cover $\\mathcal{C}_{\\varepsilon}\\subset\\mathbb{R}^{d}$ of $\\boldsymbol{S}$ that has at most $|\\mathcal{C}_{\\varepsilon}|\\leq(3/\\varepsilon)^{d}$ points. Specifically, for any $\\mathbf a\\in S$ , there exists $\\mathbf{y}\\in{\\mathcal{C}}_{\\varepsilon}$ such that $\\|\\mathbf{a}-\\mathbf{y}\\|_{2}\\leq\\varepsilon$ . By a union bound applied to all points in $\\mathcal{C}_{\\varepsilon}$ , we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\exists\\,\\mathbf{y}\\in\\mathcal{C}_{\\varepsilon}:\\langle\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*},\\pmb{\\Sigma}_{n}^{1/2}\\mathbf{y}\\rangle\\ge\\sqrt{\\frac{2\\log(|\\mathcal{C}_{\\varepsilon}|/\\delta)}{n}}\\right)\\le\\delta\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now we are ready to complete the proof. Specifically, note that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\hat{\\theta}_{n}-\\theta_{*}\\|_{\\Sigma_{n}}\\overset{(\\mathrm{a})}{=}\\operatorname*{max}\\langle\\hat{\\theta}_{n}-\\theta_{*},\\Sigma_{n}^{1/2}\\mathbf{a}\\rangle}&{}\\\\ {=\\underset{\\mathrm{a\\in\\mathcal{S}}}{\\mathrm{max}}\\,\\underset{\\mathbf{y}\\in\\mathcal{C}_{\\varepsilon}}{\\mathrm{min}}\\left[\\langle\\hat{\\theta}_{n}-\\theta_{*},\\Sigma_{n}^{1/2}(\\mathbf{a}-\\mathbf{y})\\rangle+\\langle\\hat{\\theta}_{n}-\\theta_{*},\\Sigma_{n}^{1/2}\\mathbf{y}\\rangle\\right]}&{}\\\\ {\\overset{(\\mathrm{b})}{\\le}\\underset{\\mathrm{a\\in\\mathcal{S}}}{\\mathrm{max}}\\,\\underset{\\mathbf{y}\\in\\mathcal{C}_{\\varepsilon}}{\\mathrm{min}}\\left[\\|\\hat{\\theta}_{n}-\\theta_{*}\\|_{\\Sigma_{n}}\\|\\mathbf{a}-\\mathbf{y}\\|_{2}+\\sqrt{\\frac{2\\log(|\\mathcal{C}_{\\varepsilon}|/\\delta)}{n}}\\right]}&{}\\\\ {\\overset{(\\mathrm{c})}{\\le}\\varepsilon\\|\\hat{\\theta}_{n}-\\theta_{*}\\|_{\\Sigma_{n}}+\\sqrt{\\frac{2\\log(|\\mathcal{C}_{\\varepsilon}|/\\delta)}{n}}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "holds with probability at least $1-\\delta$ . In this derivation, (a) follows from (16), (b) follows from the Cauchy-Schwarz inequality and (17), and (c) follows from the definition of $\\varepsilon$ -cover $\\mathcal{C}_{\\varepsilon}$ . Finally, we rearrange the terms, choose $\\varepsilon=1/2$ , and get that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\hat{\\theta}_{n}-\\theta_{*}\\|_{\\Sigma_{n}}\\le2\\sqrt{\\frac{2\\log(|\\mathcal{C}_{\\varepsilon}|/\\delta)}{n}}\\le2\\sqrt{\\frac{(2\\log6)d+2\\log(1/\\delta)}{n}}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 21}, {"type": "text", "text": "Lemma 8. Consider the absolute feedback model in Section 4. Fix $\\mathbf{x}\\in\\mathbb{R}^{d}$ and $\\Delta>0$ . Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left({\\bf x}^{\\top}(\\hat{\\pmb{\\theta}}_{n}-{\\pmb{\\theta}}_{*})>\\Delta\\right)\\leq\\exp\\left[-\\frac{\\Delta^{2}}{2\\|\\bf x\\|_{\\bar{\\pmb{\\Sigma}}_{n}^{-1}}^{2}}\\right]\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. The proof is from Section 2.2 in Jamieson and Jain [33]. Let $\\mathbf{X}\\in\\mathbb{R}^{K n\\times d}$ be a matrix of $K n$ feature vectors in (7) and $\\mathbf{y}\\in\\mathbb{R}^{K n}$ be a vector of the corresponding $K n$ observations. Under 1-sub-Gaussian noise in (1), we can rewrite $\\mathbf{x}^{\\top}(\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{\\ast})$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{x}^{\\top}(\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*})=\\underbrace{\\mathbf{x}^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}}_{\\mathbf{w}}\\eta=\\mathbf{w}^{\\top}\\eta=\\sum_{t=1}^{K n}\\mathbf{w}_{t}\\eta_{t}\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mathbf{w}\\in\\mathbb{R}^{K n}$ is a fixed vector and $\\eta\\in\\mathbb{R}^{K n}$ is a vector of independent sub-Gaussian noise. Then, for any $\\Delta>0$ and $\\lambda>0$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\mathbf{x}^{\\top}(\\hat{\\theta}_{n}-\\theta_{*})>\\Delta\\right)=\\mathbb{P}\\left(\\mathbf{w}^{\\top}\\boldsymbol{\\eta}>\\Delta\\right)=\\mathbb{P}\\left(\\mathbf{exp}[\\mathbf{\\bar{x}w}^{\\top}\\boldsymbol{\\eta}]>\\exp[\\lambda\\Delta]\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\stackrel{(a)}{\\le}\\exp[-\\lambda\\Delta]\\mathbb{E}\\left[\\exp\\left[\\lambda\\displaystyle\\sum_{t=1}^{K_{n}}\\mathbf{w}_{t}\\boldsymbol{\\eta}_{t}\\right]\\right]\\overset{(b)}{\\le}\\exp[-\\lambda\\Delta]\\displaystyle\\prod_{t=1}^{K_{n}}\\mathbb{E}[\\exp[\\lambda\\mathbf{w}_{t}\\boldsymbol{\\eta}_{t}]}\\\\ &{\\qquad\\qquad\\qquad\\quad\\stackrel{(c)}{\\le}\\exp[-\\lambda\\Delta]\\displaystyle\\prod_{t=1}^{K_{n}}\\exp[\\lambda^{2}\\mathbf{w}_{t}^{2}/2]=\\exp[-\\lambda\\Delta+\\lambda^{2}]|\\mathbf{w}||_{2}^{2}/2]}\\\\ &{\\qquad\\qquad\\quad\\stackrel{(d)}{\\le}\\exp\\left[-\\displaystyle\\frac{\\Delta^{2}}{2|\\mathbf{\\bar{l}}\\mathbf{w}|_{2}^{2}}\\right]\\overset{(c)}{=}\\exp\\left[-\\displaystyle\\frac{\\Delta^{2}}{2\\mathbf{x}^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}}\\right]}\\\\ &{\\qquad\\qquad\\quad=\\exp\\left[-\\displaystyle\\frac{\\Delta^{2}}{2|\\mathbf{\\bar{l}}\\mathbf{x}|_{2}^{2}-1}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In the above derivation, (a) follows from Markov\u2019s inequality, (b) follows from independent noise, (c) follows from sub-Gaussianity, (d) follows from setting $\\lambda=\\Delta/||\\mathbf{w}||_{2}^{2}$ , and $(e)$ follows from ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\mathbf{w}\\|_{2}^{2}=\\mathbf{x}^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{X}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}=\\mathbf{x}^{\\top}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{x}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 21}, {"type": "text", "text": "Lemma 9. Consider the ranking feedback model in Section 5. Fix $\\delta\\in(0,1)$ . Then there exists $a$ constant $C>0$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*}\\|_{\\pmb{\\Sigma}_{n}}^{2}\\leq\\frac{C K^{4}(d+\\log(1/\\delta))}{n}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "holds with probability at least $1-\\delta$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. The proof has two main steps. ", "page_idx": 22}, {"type": "text", "text": "Step 1: We first prove that $\\ell_{n}(\\pmb\\theta)$ is strongly convex with respect to the norm $\\|\\cdot\\|\\pmb{\\Sigma}_{n}$ at $\\pmb{\\theta}_{*}$ . This means that there exists $\\gamma>0$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ell_{n}(\\pmb{\\theta}_{*}+\\Delta)\\geq\\ell_{n}(\\pmb{\\theta}_{*})+\\langle\\nabla\\ell_{n}(\\pmb{\\theta}_{*}),\\Delta\\rangle+\\gamma\\|\\Delta\\|_{\\pmb{\\Sigma}_{n}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all perturbations $\\Delta$ such that $\\pmb{\\theta}_{*}+\\Delta\\in\\pmb{\\Theta}$ . To show this, we derive the Hessian of $\\ell_{n}(\\pmb\\theta)$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla^{2}\\ell_{n}(\\theta)=\\frac{1}{n}\\sum_{t=1}^{n}\\sum_{j=1}^{K}\\sum_{k=j}^{K}\\sum_{k^{\\prime}=j}^{K}\\frac{\\exp[\\mathbf{x}_{I_{t},\\sigma_{t}(k)}^{\\top}\\theta+\\mathbf{x}_{I_{t},\\sigma_{t}(k^{\\prime})}^{\\top}\\theta]}{2\\left(\\sum_{\\ell=j}^{K}\\exp[\\mathbf{x}_{I_{t},\\sigma_{t}(\\ell)}^{\\top}\\theta]\\right)^{2}}\\,\\mathbf{z}_{I_{t},\\sigma_{t}(k),\\sigma_{t}(k^{\\prime})}\\mathbf{z}_{I_{t},\\sigma_{t}(k),\\sigma_{t}(k^{\\prime})}^{\\top}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $\\|\\mathbf{x}\\|\\leq1$ and $\\|\\pmb\\theta\\|\\leq1$ , we have $\\exp[\\mathbf{x}^{\\top}\\theta]\\in[e^{-1},e]$ , and thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\exp[\\mathbf{x}_{I_{t},\\sigma_{t}(k)}^{\\top}\\theta+\\mathbf{x}_{I_{t},\\sigma_{t}(k^{\\prime})}^{\\top}\\theta]}{2\\left(\\sum_{\\ell=j}^{K}\\exp[\\mathbf{x}_{I_{t},\\sigma_{t}(\\ell)}^{\\top}\\theta]\\right)^{2}}\\geq\\frac{e^{-4}}{2(K-j)^{2}}\\geq\\frac{e^{-4}}{2K(K-1)}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We further have for any $t\\in[n]$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=1}^{K}\\sum_{k=j}^{K}\\sum_{k^{\\prime}=j}^{K}\\mathbf{z}_{I_{t},\\sigma_{t}(k),\\sigma_{t}(k^{\\prime})}\\mathbf{z}_{I_{t},\\sigma_{t}(k),\\sigma_{t}(k^{\\prime})}^{\\top}\\succeq\\sum_{k=1}^{K}\\sum_{k^{\\prime}=1}^{K}\\mathbf{z}_{I_{t},\\sigma_{t}(k),\\sigma_{t}(k^{\\prime})}\\mathbf{z}_{I_{t},\\sigma_{t}(k),\\sigma_{t}(k^{\\prime})}^{\\top}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\succeq2\\sum_{k=1}^{K}\\sum_{k^{\\prime}=k+1}^{K}\\mathbf{z}_{I_{t},k,k^{\\prime}}\\mathbf{z}_{I_{t},k,k^{\\prime}}^{\\top}\\cdot}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The last step follows from the fact that $\\sigma_{t}$ is a permutation. Simply put, we replace the sum of $K^{2}$ outer products by all but the ones between the same vectors. Now we combine all claims and get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla^{2}\\ell_{n}(\\pmb{\\theta})\\succeq\\frac{e^{-4}}{K(K-1)n}\\sum_{t=1}^{n}\\sum_{j=1}^{K}\\sum_{k=j+1}^{K}\\mathbf{z}_{I_{t},j,k}\\mathbf{z}_{I_{t},j,k}^{\\top}=\\gamma\\Sigma_{n}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for $\\gamma=e^{-4}/2$ . Therefore, $\\ell_{n}(\\pmb\\theta)$ is strongly convex at $\\pmb{\\theta}_{*}$ with respect to the norm $\\|\\cdot\\|\\pmb{\\Sigma}_{n}$ ", "page_idx": 22}, {"type": "text", "text": "Step 2: Now we rearrange the strong convexity inequality and get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma\\|\\Delta\\|_{\\mathbf{\\Sigma}_{n}}^{2}\\leq\\ell_{n}(\\pmb{\\theta}_{*}+\\Delta)-\\ell_{n}(\\pmb{\\theta}_{*})-\\langle\\nabla\\ell_{n}(\\pmb{\\theta}_{*}),\\Delta\\rangle\\leq\\langle\\nabla\\ell_{n}(\\pmb{\\theta}_{*}),\\Delta\\rangle}\\\\ &{\\quad\\quad\\quad\\leq\\|\\nabla\\ell_{n}(\\pmb{\\theta}_{*})\\|_{\\mathbf{\\Sigma}_{n}^{-1}}\\|\\Delta\\|_{\\Sigma_{n}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In the second inequality, we use that $\\hat{\\pmb\\theta}$ minimizes $\\ell_{n}$ , and hence $\\ell_{n}(\\pmb{\\theta}_{*}+\\Delta)-\\ell_{n}(\\pmb{\\theta}_{*})\\leq0$ . In the last inequality, we use the Cauchy-Schwarz inequality. ", "page_idx": 22}, {"type": "text", "text": "Next we write the gradient of the loss function ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla\\ell_{n}(\\pmb{\\theta})=-\\frac{1}{n}\\sum_{t=1}^{n}\\sum_{j=1}^{K}\\sum_{k=j}^{K}\\frac{\\exp[\\mathbf{x}_{I_{t},\\sigma_{t}(k)}^{\\top}\\pmb{\\theta}]}{\\sum_{\\ell=j}^{K}\\exp[\\mathbf{x}_{I_{t},\\sigma_{t}(\\ell)}^{\\top}\\pmb{\\theta}]}\\mathbf{z}_{I_{t},\\sigma_{t}(j),\\sigma_{t}(k)}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Zhu et al. [102] note that is a sub-Gaussian random variable and prove that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\nabla\\ell_{n}(\\pmb{\\theta}_{*})\\|_{\\pmb{\\Sigma}_{n}^{-1}}^{2}\\leq\\frac{C K^{4}(d+\\log(1/\\delta))}{n}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "holds with probability at least $1-\\delta$ , where $C>0$ is a constant. Finally, we plug the above bound into (18) and get that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\gamma\\|\\boldsymbol{\\Delta}\\|_{\\Sigma_{n}}^{2}\\leq\\sqrt{\\frac{C K^{4}(d+\\log(1/\\delta))}{n}}\\|\\boldsymbol{\\Delta}\\|_{\\Sigma_{n}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "holds with probability at least $1-\\delta$ . We rearrange the inequality and since $\\gamma$ is a constant, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\hat{\\pmb{\\theta}}_{n}-\\pmb{\\theta}_{*}\\|_{\\pmb{\\Sigma}_{n}}^{2}=\\|\\Delta\\|_{\\pmb{\\Sigma}_{n}}^{2}\\leq\\frac{C K^{4}(d+\\log(1/\\delta))}{n}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "holds with probability at least $1-\\delta$ for some $C>0$ . This concludes the proof. ", "page_idx": 22}, {"type": "table", "img_path": "cCGWj61Ael/tmp/6f2b1cd1f765dc9e8db7bf2f528abd91055fcf6b2cbe8d07f89a512d136e78b0.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison of Dope with plug-in designs Plug-in and optimal solution Optimal. "], "page_idx": 23}, {"type": "text", "text": "C Optimal Design for Ranking Feedback ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The optimal design for (10) is derived as follows. First, we compute the Hessian of $\\ell_{n}(\\pmb\\theta)$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla^{2}\\ell_{n}(\\boldsymbol{\\theta})=\\frac{1}{n}\\sum_{t=1}^{n}\\sum_{j=1}^{K}\\sum_{k=j}^{K}\\sum_{k^{\\prime}=j}^{K}\\frac{\\exp[\\mathbf{x}_{I_{t},\\sigma_{t}(k)}^{\\top}\\boldsymbol{\\theta}+\\mathbf{x}_{I_{t},\\sigma_{t}(k^{\\prime})}^{\\top}\\boldsymbol{\\theta}]}{2\\left(\\sum_{\\ell=j}^{K}\\exp[\\mathbf{x}_{I_{t},\\sigma_{t}(\\ell)}^{\\top}\\boldsymbol{\\theta}]\\right)^{2}}\\,\\mathbf{z}_{I_{t},\\sigma_{t}(k),\\sigma_{t}(k^{\\prime})}\\mathbf{z}_{I_{t},\\sigma_{t}(k),\\sigma_{t}(k^{\\prime})}^{\\top}\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In this work, we maximize the log determinant of relaxed $\\nabla^{2}\\ell_{n}(\\pmb{\\theta}_{\\ast})$ . Note that the exact optimization is impossible because $\\theta_{*}$ is unknown. This can be addressed in two ways. ", "page_idx": 23}, {"type": "text", "text": "Worst-case design: Solve an approximation where $\\theta_{*}$ -dependent terms are replaced with a lower bound. We take this approach. Specifically, we show in the proof of Lemma 9 that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla^{2}\\ell_{n}(\\pmb{\\theta})\\succeq\\frac{e^{-4}}{K(K-1)n}\\sum_{t=1}^{n}\\sum_{j=1}^{K}\\sum_{k=j+1}^{K}\\mathbf{z}_{I_{t},j,k}\\mathbf{z}_{I_{t},j,k}^{\\top}=\\gamma\\Sigma_{n}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for $\\gamma=e^{-4}/2$ . Then we maximize the log determinant of a relaxed problem. This solution is sound and justified, because we maximize a lower bound on the original objective. ", "page_idx": 23}, {"type": "text", "text": "Plug-in design: Solve an approximation where $\\theta_{*}$ is replaced with a plug-in estimate. ", "page_idx": 23}, {"type": "text", "text": "We discuss the pluses and minuses of both approaches next. ", "page_idx": 23}, {"type": "text", "text": "Prior works: Mason et al. [55] used a plug-in estimate to design a cumulative regret minimization algorithm for logistic bandits. Recent works on preference-based learning [102, 20, 99], which are the closet related works, used worst-case designs. Interestingly, Das et al. [20] analyzed an algorithm with a plug-in estimate but empirically evaluated a worst-case design. This indicates that their plug-in design is not practical. ", "page_idx": 23}, {"type": "text", "text": "Ease of implementation: Worst-case designs are easier to implement. This is because the plug-in estimate does not need to be estimated. Note that this requires solving an exploration problem with additional hyper-parameters, such as the number of exploration rounds for the plug-in estimation. ", "page_idx": 23}, {"type": "text", "text": "Theory: Worst-case designs can be analyzed similarly to linear models. Plug-in designs require an analysis of how the plug-in estimate concentrates. The logging policy for the plug-in estimate can be non-trivial as well. For instance, the plug-in estimate exploration in Mason et al. [55] is over $\\tilde{O}(d)$ special arms, simply to get pessimistic per-arm estimates. Their exploration budget is reported in Table 1. The lowest one, for a 3-dimensional problem, is 6 536 rounds. This is an order of magnitude more than in our Figure 1(b) for a larger 36-dimensional problem. ", "page_idx": 23}, {"type": "text", "text": "Based on the above discussion, we believe that worst-case designs strike a good balance between practicality and a theory support. Nevertheless, plug-in designs are intriguing because they may perform well with a decent plug-in estimate. To investigate if this happens in our experiments, we repeat Experiment 2 in Section 6 with $K=2$ (logistic regression). We compare three methods: ", "page_idx": 23}, {"type": "text", "text": "(1) Dope: This is our method. The exploration policy is $\\pi_{*}$ in (6). ", "page_idx": 23}, {"type": "text", "text": "(2) Plug-in $(m)$ : This is a plug-in baseline. For the first $m$ rounds, it explores using policy $\\pi_{*}$ in (6). After that, we compute the plug-in estimate of $\\pmb{\\theta}_{*}$ using (10) and solve the D-optimal design with it. The resulting policy is used for exploration for the remaining $n-m$ rounds. Finally, $\\pmb{\\theta}_{*}$ is estimated from all feedback using (10). ", "page_idx": 23}, {"type": "text", "text": "(3) Optimal: The exploration policy $\\pi_{*}$ is computed using the D-optimal design with the unknown $\\pmb{\\theta}_{*}$ . This validates our implementation and also shows the gap from the optimal solution. ", "page_idx": 24}, {"type": "text", "text": "We report both the prediction errors and ranking losses at $n=500$ rounds in Table 1. The results are averaged over 100 runs. We observe that the prediction error of Dope is always smaller than that of Plug-in (6 times at $m=100$ ). Optimal outperforms Dope but cannot be implemented in practice. The gap between Optimal and Plug-in shows that an optimal design with a plug-in estimate of $\\pmb{\\theta}_{*}$ can be much worse than that with $\\theta_{*}$ . Dope has a comparable ranking loss to Plug-in at $m=300$ and $m=400$ . Plug-in has a higher ranking loss otherwise. ", "page_idx": 24}, {"type": "text", "text": "Based on our discussion and experiments, we do not see any strong evidence to adopt plug-in designs. They would be more complex than worst-case designs, harder to analyze, and we do not see beneftis in our experiments. This also follows the principle of Occam\u2019s razor, which tells us to design with a minimal needed complexity. ", "page_idx": 24}, {"type": "text", "text": "D Related Work ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The two closest related works are Mehta et al. [56] and Das et al. [20]. They proposed algorithms for learning to rank $L$ pairs of items from pairwise feedback. Their optimized metric is the maximum gap over $L$ pairs. We learn to rank $L$ lists of $K$ items from $K$ -way ranking feedback. We bound the maximum prediction error, which is a similar metric to these related works, and the ranking loss in (3), which is novel. Algorithm APO in Das et al. [20] is the closest related algorithmic design. APO greedily minimizes the maximum error in pairwise ranking of $L$ lists of length $K=2$ . Therefore, Dope with ranking feedback (Section 5.1) can be viewed as a generalization of Das et al. [20] to lists of length $K\\ge2$ . APO can be compared to Dope by applying it to all possible ${\\binom{K}{2}}L$ lists of length 2 created from our lists of length $K$ . We do that in APO in Section 6. The last difference from Das et al. [20] is that they proposed two variants of APO: analyzed and practical. We propose a single algorithm, which is both analyzable and practical. ", "page_idx": 24}, {"type": "text", "text": "Our problem can be generally viewed as learning preferences from human feedback [27, 28, 31]. The two most common forms of feedback are pairwise, where the agent observes a preference over two items [12]; and ranking, where the agent observes a ranking of the items [65, 54]. Online learning from human feedback has been studied extensively. In online learning to rank [67, 104], the agent selects a list of $K$ items and the human provides absolute feedback, in the form of clicks, on all recommended items or their subset. Two popular feedback models are cascading [43, 105, 53] and position-based [44, 23, 100] models. The main difference in Section 4 is that we do not minimize cumulative regret or try to identify the best list of $K$ items. We learn to rank $L$ lists of $K$ items within a budget of $n$ observations. Due to the budget, our work is related to fixed-budget BAI [13, 5, 6, 95]. The main difference is that we do not aim to identify the best arm. ", "page_idx": 24}, {"type": "text", "text": "Online learning from preferential feedback has been studied extensively [9] and is often formulated as a dueling bandit [97, 48, 93, 42, 63, 72, 74, 73, 75, 84, 92]. Our work on ranking feedback (Section 5) differs from these works in three main aspects. First, most dueling bandit papers consider pairwise feedback ( $K=2$ ) while we study a more general setting of $K\\geq2$ . Second, a classic objective in dueling bandits is to minimize regret with respect to the best arm, sometimes in context; either in a cumulative or simple regret setting. We do not minimize cumulative or simple regret. We learn to rank $L$ lists of $K$ items. Finally, the acquisition function in dueling bandits is adaptive and updated in each round. Dope is a static design where the exploration policy is precomputed. ", "page_idx": 24}, {"type": "text", "text": "Preference-based learning has also been studied in a more general setting of reinforcement learning [88, 60, 94, 29]. Preference-based RL differs from classic RL by learning human preferences through non-numerical rewards [18, 47, 16]. Our work can be also viewed as collecting human feedback for learning policies offilne [35, 69, 98, 76]. One of the main challenges of offilne learning is potentially insufficient data coverage. We address this by collecting diverse data, using optimal designs [66, 24]. ", "page_idx": 24}, {"type": "text", "text": "Finally, we wanted to compare the ranking loss in (3) to other objectives. There is no reduction to dueling bandits. A classic objective in dueling bandits is to minimize regret with respect to the best arm from dueling feedback. Our goal is to rank $L$ lists. One could think that our problem can be solved as a contextual dueling bandit, where each list is represented as a context. This is not possible because the context is controlled by the environment. In our setting, the agent controls the chosen list, similarly to APO in Das et al. [20]. Our objective also cannot be reduced to fixed-budget BAI. Our comparisons to Azizi et al. [6] (Sections 4.3 and 5.3) focus on similarities in high-probability bounds. The dependence on $n$ and $d$ is expected to be similar because the probability of making a mistake in Azizi et al. [6] and a ranking error in our work depend on how well the generalization model is estimated, which is the same in both works. ", "page_idx": 24}, {"type": "table", "img_path": "cCGWj61Ael/tmp/c18701aee4a7d63370152b55e489611149d349742290765a8f4fd74afe7cdf48.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "cCGWj61Ael/tmp/c8ecd1b8a3abeb87bc774e8d23a4bf2ac186fb3a9e1265a6f6a3760794d4c6eb.jpg", "table_caption": ["Table 2: Computation time of policy $\\pi_{*}$ in (6) as a function of the number of lists $L$ . ", "Table 3: The ranking loss of Dope as a function of the number of lists $L$ and items $K$ "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "E Ablation Studies ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We conduct two ablation studies on Experiment 2 in Section 6. ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In Table 2, we report the computation time of policy $\\pi_{*}$ in (6). We vary the number of lists $L$ and use CVXPY [21] to solve (6). For $L=100$ , the computation takes 4 seconds; and for $L=800$ , it takes 50. Therefore, it scales roughly linearly with the number of lists $L$ . ", "page_idx": 25}, {"type": "text", "text": "In Table 3, we vary the number of lists $L$ and items $K$ , and report the ranking loss of Dope. We observe that the problems get harder as $L$ increases (more lists to rank) and easier as $K$ increases (longer lists with more feedback). ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We clearly state all contributions and point to them from Section 1. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Limitations of our work, such as non-adaptive designs and the lack of lower bounds, are discussed throughout the paper. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All claims are proved in Appendices A and B. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The pseudo-code of the proposed algorithm Dope is given. Experiments are described in a sufficient detail to be reproducible. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: We did not get a permission to release the code. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All details are provided in Section 6 and Appendix E. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Standard errors are reported in all plots. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The computation time is discussed in Section 3 and Appendix E. ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We follow the ethics guidelines. ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This work is algorithmic and not tied to any particular application. ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This is not applicable to this paper. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All used assets are stated and cited. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: This is not applicable to this paper. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This is not applicable to this paper. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This is not applicable to this paper. ", "page_idx": 27}]