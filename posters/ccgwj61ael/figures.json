[{"figure_path": "cCGWj61Ael/figures/figures_8_1.jpg", "caption": "Figure 1: Ranking loss of all compared methods plotted as a function of the number of rounds. The error bars are one standard error of the estimates.", "description": "This figure compares the performance of five different methods for learning to rank items using human feedback, across four different datasets.  The x-axis represents the number of human feedback interactions. The y-axis represents the ranking loss, a metric indicating how well the learned ranking matches the true, optimal ranking.  The five methods are: Unif (uniformly random sampling), Dope (the authors' proposed method), Avg Design (average design), Clustered Design (clustered design), and APO (a baseline algorithm from related work).  The plots show that Dope generally outperforms the other methods in terms of lower ranking loss across all datasets, demonstrating its efficiency in learning to rank from human feedback. Error bars indicate the standard error of the mean ranking loss.", "section": "6 Experiments"}]