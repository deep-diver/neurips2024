[{"type": "text", "text": "Local Superior Soups: A Catalyst for Model Merging in Cross-Silo Federated Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Minghui Chen12 Meirui Jiang3 Xin Zhang4 Qi Dou3 Zehua Wang1 Xiaoxiao ${\\bf L i}^{12*}$ 1University of British Columbia 2Vector Institute 3Chinese University of Hong Kong 4Meta ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning (FL) is a learning paradigm that enables collaborative training of models using decentralized data. Recently, the utilization of pre-trained weight initialization in FL has been demonstrated to effectively improve model performance. However, the evolving complexity of current pre-trained models, characterized by a substantial increase in parameters, markedly intensifies the challenges associated with communication rounds required for their adaptation to FL. To address these communication cost issues and increase the performance of pre-trained model adaptation in FL, we propose an innovative model interpolation-based local training technique called \u201cLocal Superior Soups.\u201d Our method enhances local training across different clients, encouraging the exploration of a connected low-loss basin within a few communication rounds through regularized model interpolation. This approach acts as a catalyst for the seamless adaptation of pre-trained models in in FL. We demonstrated its effectiveness and efficiency across diverse widely-used FL datasets. Our code is available at https://github.com/ubc-tea/Local-Superior-Soups. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning (FL) [35] has emerged as a promising methodology for leveraging the power of private data without the need for centralized data governance. However, data heterogeneity in FL poses significant challenges to the design of efficient training for global convergence. With the emergence of the pre-training and fine-tuning paradigm in various applications [15, 19], recent studies [37, 2] have attempted to address the problem of FL under data heterogeneity with pre-trained initialization. Although pre-trained federated learning can speed up convergence compared to random initialization, it still requires a significant number of communication rounds between the server and clients, often amounting to hundreds of rounds [37]. Existing pre-trained models [41, 47] often have an enormous parameter scale, and following the neural scaling law [24], there is a continuous trend toward increasing model parameters. Deploying models with such a large parameter size in FL introduces significant communication overhead. This greatly hampers the flexibility and scalability of model updates. Reducing FL communication overhead can be approached by reducing the scale of model parameters involved in distributed training [59] or reducing communication rounds [35]. Comparing with reducing model parameters, reducing communication rounds typically leads to a more efficient reduction of network congestion [17], decreased energy consumption on client devices [33], and a lower risk of privacy breaches [61]. In this paper, we focus on reducing communication rounds in FL with pre-trained model as initialization. ", "page_idx": 0}, {"type": "text", "text": "Typically, increasing the number of local training steps can effectively reduce communication rounds. However, there is an upper limit to the extent of local training step increments. This limitation arises due to the presence of data heterogeneity, where the consistency of optimization among different clients deteriorates with the increasing number of local steps [35]. This optimization inconsistency leads to a discrepancy between local and global models and decelerates the convergence rate of FL. The discrepancy is often called client drift [25]. Previously, some FL methods [25, 45] attempted to introduce proximal terms to regularize local training, with the aim of reducing local overfitting and minimizing the problem of client drift. While these methods can accelerate convergence, they restrict the progress of each local training steps towards the optimal solution, impeding the attainment of FL with more aggressive communication round reductions. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "While these client drift mitigation methods can reduce local overftiting to some extent, they cannot ensure strong performance of the global aggregated models, particularly in scenarios with limited communication rounds. This situation arises when individual local clients become trapped in isolated low-loss valleys. More specifically, as illustrated in Figure 1, two models from clients \u2018A\u2019 and \u2018B\u2019, even if their optimal model distance is small, still result in a poorly performing aggregated global model. Moreover, ", "page_idx": 1}, {"type": "image", "img_path": "0LfgE6kvKZ/tmp/80930f4c1d333cb2bc3b16a514f0c6bf7a1b1df483ecb1368bf24e6fd11fbff8.jpg", "img_caption": ["Figure 1: Illustration on isolated (left) and connected low-loss valley with larger regions in dark red (right). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "the preceding FL methods aimed at minimizing communication rounds exclusively address scenarios involving random initialization, lacking a customized approach tailored to pre-trained models. Recent proposed centralized fine-tuning methods (e.g., model soups [52] and DiWA [43] \u2013 a greedy model selection version of model soups) based on model interpolation (averages of a large number of model weights) are effective approaches to seek large connected low-loss region, which are promising for applying in FL to reduce communication rounds. These methods can prevent individual clients from being trapped in isolated low-loss valleys by positioning the global model centrally within a larger low-loss region by overlapping the low-loss regions among clients, as shown in Fig. 1 (right). However, their training efficiency is exceedingly low, requiring complete retraining of numerous models, leading to significant computational overhead on clients and intolerable communication costs when applied in FL, due to two aspects: First, they involve a time-consuming model selection phase within the model pool, which consists of all candidate models available for weight interpolation. Secondly, model soups entail an extensive number of model training iterations, lacking prior guidance and relying on brute-force, random, and often redundant training. Many of the trained models end up unused. ", "page_idx": 1}, {"type": "text", "text": "To enjoy the connected low-loss valley benefits of model soup-based methods [52, 43] without burdening local training, we propose an efficient and local model interpolation-based method, called Local Superior Soups (LSS). To address the first issue, we propose a sequential random model interpolation method during training. This eliminates the need for subsequent model selection steps and ensures that the models slated for interpolation reside within the same low-loss valley during training (Sec. 3.3.1). For the second issue, we introduce two quantifiable indicators of candidate model quality, inspired by data augmentation quality quantification [32, 4]: diversity (Sec. 3.3.2) and affinity (Sec. 3.3.3). Specifically, the diversity indicator quantifies the diversity among models in the model pool with their pairwise model distance, where larger distances denote higher diversity, signifying better model quality for interpolation. ", "page_idx": 1}, {"type": "text", "text": "As illustrated in Figure 2 (left), a low-loss region, supported by models with low diversity, can be effectively covered with only a few candidate models positioned near its periphery. Thus, we propose incorporating the diversity metric as a regularization term during training to maximize the expansion of low-loss regions, thereby increasing the utilization of trained models. The affinity indicator measures the affinity of each candidate model in the model pool to the initial model. Smaller distances indicate greater affinity, indicating better model quality for interpolation. This affinity is also incorporated as ", "page_idx": 1}, {"type": "image", "img_path": "0LfgE6kvKZ/tmp/f5a59f44d234e4e1948f39f0a8bb1b887f5c5575361285a23fdc128905c5ab39.jpg", "img_caption": ["Figure 2: Illustration on diversity (left) and affinity (right) regularization. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "a regularization term during training to prevent the expansion of low-loss regions from deviating too far from the shared initialization point, thus increasing the likelihood of overlapping connected regions (as depicted on the right side of Fig. 2). These two indicators facilitate the efficient inclusion of models into the model pool, preventing wasteful training of models that may ultimately go unused. ", "page_idx": 1}, {"type": "text", "text": "In experiments, we found that our proposed method greatly reduces communication rounds, and we achieved the performance of models fused after multiple rounds of communication in other FL methods with only a few rounds of communication. ", "page_idx": 2}, {"type": "text", "text": "In summary, our contributions are as follows. ", "page_idx": 2}, {"type": "text", "text": "(1) We reveal the importance of regularizing local client models in the connected low-loss valleys for reducing communication rounds when initializing FL with pre-trained models. (2) We introduce an innovative and efficient model soups-based method for FL, called Local Superior Soups (LSS) that eliminates the need for time-consuming model selection and redundant model training in the existing soups-based approaches, while expanding connected low-loss valleys of client models for faster convergence. (3) In experimental evaluations, LSS demonstrates a significant reduction in communication rounds, achieving superior performance with only a few rounds of communication, exceeding baseline FL methods significantly in four datasets and two types of distribution shifts. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Heterogeneous Federated Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "FL struggles with Non-IID data, leading to various proposed algorithms. FedProx [28] uses proximal term to regularize local training, preventing client divergence. Scaffold [25] adds variance reduction to combat \"clients-drift.\" MOON [27] employs mode-level contrastive learning to stabilize local training. Personalized FL [46] targets high local performance on Non-IID data. FedBN [30] applies local batch normalization to mitigate feature shift before model averaging. Recent one-shot and few-round FL methods use parallel server-side techniques like prediction ensembles [14], data generation [56, 18], or meta-learning [39] to improve aggregated model performance. ", "page_idx": 2}, {"type": "text", "text": "2.2 Federated Fine-tuning and Model Interpolation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Fine-tuning leverages pre-trained models to enhance task performance [7]. FedFTG [57] proposes knowledge distillation for global model fine-tuning in FL. Personalized FL employs fine-tuning to adapt global models to local ones, e.g., FedBABU [38], FTFA, and RTFA [5]. However, this focus on local performance neglects global generalization. Inspired by linear mode connectivity [36, 12], Model Soups [52] combines runs with varied hyper-parameters to improving fine-tuning performance in the centralized setting. DiWA [43] and other Soups-based methods [52, 43, 3] extends this concept, emphasizing the importance of model diversity. Some methods induce diversity through high learning rates [34], cosine similarity minimization [51], tempered posteriors [20], or auxiliary dataset-trained model soups [42]. We depict the difference of different model ensemble-based methods in our appendix 7. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The structure of this Section is as follows: firstly, we provide the problem definition and corresponding notions to be used (Sec. 3.1); secondly, we reveal the dilemma for existing federated learning methods on reducing communication rounds (Sec. 3.2); finally, we propose a regularized model interpolationbased method as a solution, provide corresponding analysis (Sec. 3.3), and present the overall algorithm flow. ", "page_idx": 2}, {"type": "text", "text": "3.1 Notions and Problem Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notions. Let $\\mathcal{X}$ be the input space of data, $\\boldsymbol{\\wp}$ be the label space. Consider a $\\mathrm{FL}$ setting with $M$ clients, \u03c4 local steps and R communication rounds. Let D := {Di}iM=1 be a set of M domain, each of which is a distribution over the input space $\\mathcal{X}$ . For each client, we have access to $n$ training data points imn  the form of $(\\boldsymbol{\\chi}_{i},\\boldsymbol{\\mathcal{V}}_{i})=\\{(\\dot{x_{j}^{i}},y_{j}^{i})\\}_{j=1}^{n}$ , where $y_{j}^{i}$ denotmes  the target label for input $\\boldsymbol{x}_{j}^{i}$ . Let represents the parameter for the global model, denotes the local objective function at client $i$ , and $\\mathcal{P}$ denotes a distribution on the entire set of clients. We provide a notation table in Appendix A to clarify the meanings of the corresponding notations. ", "page_idx": 2}, {"type": "text", "text": "Problem definition. We aim to address the challenge of optimizing the global performance on $\\mathcal{D}$ of aggregated models fine-tuned from different clients with data heterogeneity, while minimizing communication rounds between the clients and the server in data Non-IID setting. In terms of global performance, we perform empirical risk minimization (ERM) on the sampled data $\\mathcal{D}_{i}$ for $i\\in[M]$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}(f)=\\sum_{i=1}^{M}p_{i}\\mathcal{L}_{i}(f),\\quad\\mathrm{where~}\\mathcal{L}_{i}(f)=\\frac{1}{\\left|\\mathcal{D}_{i}\\right|}\\sum_{\\xi\\in\\mathcal{D}_{i}}\\ell_{i}(f,\\xi)\\mathrm{~and~}\\sum_{i=1}^{M}p_{i}=1.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 Effect of Regularization and Local Steps on FL Convergence under Data Heterogeneity ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present a theoretical analysis to understand how communication rounds, local steps, and our introduced regularization terms affect the convergence bound in federated learning. Formally, we present the error term and posit the following assumptions for the purpose of analysis. Our analysis builds on the assumptions and convergence bound in [50] with formal statements in Appendix B.1. We first present a theorem providing a convergence guarantee when the proposed regularization terms are applied. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 (Convergence Rate for Convex Local Functions with Affinity and Diversity Constraint). Under Convexity and Smoothness Assumption on $\\beta$ -smooth loss function, Bounded Variance of Stochastic Gradient and Bounded Variance of Local and Global Gradient assumptions, when the client learning rate is chosen properly as, \u03b7 = min{ 41\u03b2 ,\u03c412M R2 21 d,\u03c3,\u03c423 Rd31 3\u03b231 \u03c323 ,\u03c4R13 \u03b2d31 3(\u03b6+c)32 } we define $\\begin{array}{r}{\\epsilon=\\mathbb{E}\\left[\\frac{1}{\\tau R}\\sum_{r=0}^{R-1}\\sum_{k=1}^{\\tau}\\beta(\\overline{{f}}^{(r,k)})-\\beta(f^{\\star})\\right]}\\end{array}$ , and have $\\epsilon\\le\\frac{2\\beta R^{2}}{\\tau R}+\\frac{2\\sigma d}{\\sqrt{M\\tau R}}+\\frac{5\\beta^{\\frac{1}{3}}\\sigma^{\\frac{2}{3}}d^{\\frac{4}{3}}}{\\tau^{\\frac{1}{3}}R^{\\frac{2}{3}}}+\\frac{15\\beta^{\\frac{1}{3}}(\\zeta+c)^{\\frac{2}{3}}d^{\\frac{4}{3}}}{R^{\\frac{2}{3}}}$ (2) ", "page_idx": 3}, {"type": "text", "text": "Here, the update rule of the $t$ iteration with the affinity and diversity term is defined as $\\theta(t+1)=$ $\\theta(t)-\\eta g(\\bar{t})-q(t,\\mu_{t},\\mu_{a})$ , and the extra term satisfies $q(t,\\mu_{t},\\mu_{a})\\leq c$ . The hyper-parameters $\\mu_{t}$ and $\\mu_{a}$ represent the co-efficient of tuning affinity and diversity respectively. ", "page_idx": 3}, {"type": "text", "text": "Besides, $d:=\\lVert f^{(0,0)}-f^{\\star}\\rVert$ refers to the distance between initialization $f^{(0,0)}$ and the global optimum $f^{\\star}$ , $\\sigma$ bounds variance of stochastic gradient by $\\mathbb{E}[\\|g_{i}(f^{(r,k)})-\\nabla\\mathcal{L}_{i}(f^{(r,k)})\\|^{2}|f^{(r,k)}]\\le\\sigma^{2}$ , and $\\zeta$ bounds variance of local and global gradient by $\\begin{array}{r}{\\operatorname*{max}_{i}\\operatorname*{sup}_{f}\\left\\|\\nabla\\mathcal{L}_{i}\\big(f^{(r,k)}\\big)-\\nabla\\mathcal{L}\\big(f^{(r,k)}\\big)\\right\\|\\leq\\zeta.}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "How to reduce communication rounds under data heterogeneity? Increasing local fine-tuning steps seems to be a straightforward technique to reduce communication costs. Nevertheless, this approach cannot reduce the an error term in the convergence rate (see the 4th term of the RHS of Eq. 2), which remains unaltered by increasing local steps. Moreover, increasing local update steps in the presence of Non-IID client data exacerbates the inconsistency in local objectives, further magnifying this error term. Here, we provide a more detailed explanation, specifically identifying the conditions under which increasing iteration steps can effectively reduce communication rounds. ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.2. Under the data heterogeneity setting, when the total number of gradient computations across all clients $'K=M\\tau R_{}$ ) is fixed and the local steps $\\tau$ satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tau\\leq\\frac{\\sigma}{\\zeta+c}\\sqrt{\\frac{\\sigma}{d\\beta}\\frac{K^{\\frac{1}{2}}}{M^{2}}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "the error upper bound Eq.2 will be dominated by the second term $\\mathcal{O}(1/\\sqrt{K})$ . ", "page_idx": 3}, {"type": "text", "text": "We provide the proof for Proposition 3.2 in Appendix B.2. Accordingly, increasing the bound in Eq. 2 and meeting the aforementioned condition for local steps allows us to reduce communication rounds. From the above in-equation, we can observe that although increasing the number of local training steps can reduce communication rounds, there is a limit to the number of steps that can be added. This limit is primarily determined by the error term introduced by local updates. ", "page_idx": 3}, {"type": "text", "text": "Why connecting low-loss valley in local training with pre-trained initialization can achieve extreme communication rounds reduction? Our analysis indicates that for substantial communication efficiency in federated learning, it is not enough to just increase local training steps. The focus should be on minimizing the error term from local updates, particularly the last term in Formula 2. This term, influenced by gradient dissimilarity $\\left(\\zeta\\right)$ , distance to optimal weights $(d)$ , and our proposed regularization update bound $c$ , remains significant even as training steps increase. ", "page_idx": 4}, {"type": "text", "text": "Prior research suggests [37] that pre-training initialization reduces $\\zeta$ by aligning client updates, and overparameterization ensures that the optimal parameters are typically close to the initialization [22, 6, 31], decreasing $d$ . It is important to note that the regularization related term $c$ is influenced by a combination of model diversity and affinity, and can be reduced by adjusting the parameters $\\mu_{t}$ and $\\mu_{a}$ . In the absence of a common pre-trained initialization, ensuring model affinity within the model pool is often challenging (i.e., models from different clients tend to diverge significantly from the initialization values), resulting in a larger value of $c$ , which in turn affects the effectiveness of our method. Therefore, our approach is more suitable when combined with pre-trained models. Consequently, a combination of pre-training and our proposed connectivity preserving local training can effectively lower error terms from local updates, increasing the limit of local training steps and thus reducing communication rounds. More experimental support see our Appendix. ", "page_idx": 4}, {"type": "text", "text": "3.3 Our Solution: LSS Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this part, we first present the shortcomings of the previous model soups method applied in FL. Secondly, we propose our three targeted improvements, i.e. random model interpolation (Sec. 3.3.1), diversity term (Sec. 3.3.2), and affinity regularization term (Sec. 3.3.3). Finally, we present the complete algorithm process and detailed implementation in local client training. ", "page_idx": 4}, {"type": "text", "text": "Limitation of previous model soups methods. Previous model soups methods [52] can induce a trained model located in a connected low-loss valley, but their training efficiency is exceedingly low, due to two aspects: Time-Consuming model selection phase: Firstly, these methods involve a time-consuming model selection phase, which consists of all candidate models available for weight interpolation [3, 52]. This phase aims to choose models that reside within the same low-loss valleys. During this selection process, significant computational resources are consumed to identify suitable models for interpolation, adding to the overall training time and complexity. Extensive and redundant model training: Secondly, model soups entail an extensive number of model training iterations, lacking prior guidance and relying on brute-force, random, and often redundant training [29, 52]. Many of the trained models end up unused, further exacerbating the computational inefficiency. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 LSS (Local Training) Pseudo-code   \nRequire: $f_{p}$ pre-trained model $\\;R=1\\;\\;$ ) or global aggregated model $(R>1)$ ); $\\mathcal{L}$ loss function; $\\mathcal{D}$ dataset; dist distance function; $\\tau$ iteration steps; $\\eta$ learning rate; $\\lambda_{a}$ affinity coefficient; $\\lambda_{d}$ diversity coefficient; $n$ number of averaged models.   \n1: LSS Local Training :   \n2: $\\overline{{\\mathcal{M}\\leftarrow\\{f_{p}\\}}}$   \n3: for $p_{i}=1$ to $N$ do   \n4: $f_{p_{i}}\\gets A v e r a g i n g(\\mathcal{M})$   \n5: $\\bar{\\mathcal{M}}\\leftarrow\\mathcal{M}\\cup\\{f_{p_{i}}\\}$ {sequential training with newly added model}   \n6: for $t=1$ to $\\tau$ do   \n7: $f_{s}\\gets$ RandomInterpolation $(\\mathcal{M})$ {connectivity preserving}   \n8: $\\begin{array}{r l r}{{\\mathcal L}_{r e q}(f_{p_{i}})}&{{}=}&{{\\mathcal L}(f_{s},{\\mathcal D})\\;\\;+\\;\\;\\lambda_{a}}\\end{array}$ $d i s i(f_{p_{i}},f_{p})-\\lambda_{d}\\cdot d i s t(f_{p_{i}},\\mathcal{M})$   \n9: $f_{p_{i}}\\gets f_{p_{i}}-\\eta\\nabla_{f_{p_{i}}}\\mathcal{L}_{r e g}(f_{p_{i}})$   \n10: end for   \n11: end for   \n12: Inference:   \n13: $f\\leftarrow A v e r a g i n g(\\mathcal{M})$ ", "page_idx": 4}, {"type": "text", "text": "3.3.1 Random interpolation conserving connected low-loss region. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To address the time-consuming model selection issue of the previous soups-based method, we ", "page_idx": 4}, {"type": "text", "text": "propose a sequential random model interpolation method during training. This innovative approach streamlines the training process by eliminating the need for subsequent model selection steps within the model pool (i.e., local models to be interpolated), which traditionally consumes a considerable amount of computational resources and time. Let $\\mathcal{M}=\\{f_{p_{1}},f_{p_{2}},...\\,,\\dot{f_{p_{N}}}\\}$ be a pool of $N$ models, where $f_{p_{i}}$ represents the weights of the $i$ -th model. We define the interpolated model $f_{\\mathrm{interp}}$ as a weighted combination of the models in $\\mathcal{M}$ . The interpolation coefficients ${\\boldsymbol{\\alpha}}=(\\alpha_{1},\\alpha_{2},\\ldots,\\alpha_{N}^{\\stackrel{\\textstyle.}{}})$ are sampled using a uniform distribution and normalization strategy. The interpolated model $f_{\\mathrm{interp}}$ is then computed as: $\\begin{array}{r}{f_{\\mathrm{interp}}=\\sum_{i=1}^{N}\\alpha_{i}f_{p_{i}}}\\end{array}$ . Here, $\\alpha_{i}$ represents the weight assigned to the $i$ -th model in the pool. The uniform di stribution ensures that the coefficients $\\alpha_{i}$ are non-negative and sum to 1, providing a simple and effective way to combine the model weights from the pool $\\mathcal{M}$ . Forward and backward propagation are performed using the interpolated model, updating the weights of the currently active model (i.e., the newly added model ) (corresponding to Algorithm 1 Line 7), while previously added model weights remain fixed. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.3.2 Diversity term. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The diversity term is proposed to address the redundant model training issue of the previous soupsbased methods by encouraging low-loss region expansion. In particular, the diversity indicator assesses the variability among models within the model pool by summing the distances between pairs of models. Greater distances between models indicate a higher degree of diversity, which correlates with enhanced model quality. This diversity metric is integrated into the FL local training process as a regularization term to facilitate the extensive enlargement of low-loss regions, consequently maximizing the effectiveness of trained models. The diversity term (in Algorithm 1 Line 8) measures the distance between the current training model and other models that will be averaged, and we hope that this distance to be large. The diversity loss can be defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{diversity}}=d i s t(f,\\mathcal{M})=\\frac{1}{N}\\sum_{n=1}^{N}d i s t(f,f_{n}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $f_{n}$ belongs to local interpolated model pool $\\mathcal{M}$ and $N$ is the number of local candidate models. The candidate models (i.e., model soups ingredients) are models to be interpolated in local training, and the model pool is the set of local candidate models (see Algorithm 1 Line 5). We use the $\\ell_{2}$ norm to measure the distance between model weights. ", "page_idx": 5}, {"type": "text", "text": "3.3.3 Affinity term. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The affinity term is proposed to control the expansion of low-loss regions and prevent local candidate model training divergence. The affinity indicator assesses the level of alignment between each candidate model within the model pool and the initial global model by calculating the cumulative distances between each candidate model and the initialization model. Smaller distances between models signify a stronger affinity, indicating higher model quality. To ensure the controlled expansion of low-loss regions and reduce the probability of overlapping connected regions, this affinity metric is integrated into the training process as a regularization term. The affinity term (in Algorithm 1 Line 8) measures the distance between the candidate model and the initial model weights, with the aim of minimizing this dissimilarity (maximize this loss term) to ensure that the distance remains relatively small. The affinity loss can be defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{affinity}}=d i s t(f,f_{p}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $f_{p}$ is a pre-trained model in the first communication round $\\langle R=1$ ). Moreover, it encourages each local model to lie in a close zone in the parameter space, which is beneficial for subsequent server aggregation, especially under data heterogeneity. We use $l_{2}$ distance for the $d i s t(,)$ metric for both Eq. 4 and Eq. 5. ", "page_idx": 5}, {"type": "text", "text": "3.3.4 Overall pipeline. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We outline LSS as follows: We begin with the initialization of the client\u2019s local model with the pretrained global model. Then we will refine the local model using affinity and diversity loss. This step is performed for a few local update steps. Finally, after updating local model, we aggregate them in the server following the common averaging operation in FedAvg [35]. The flow of $L S S$ for local updating (Step 2 described in Sec 3.1) can be found in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "In conclusion, our method aims to minimize the distance between the local fine-tuned model and the pre-trained initialized global model while maximizing the distance between the model soups ingredients (i.e., the models to be averaged). Our fine-tuned models find large low-loss regions on their respective local datasets while ensuring parameters close to the pre-trained initialization. It is intuitive that the parameters of our fine-tuned models can be more easily aligned with those of models fine-tuned on similar datasets, thereby improving communication efficiency. ", "page_idx": 5}, {"type": "table", "img_path": "0LfgE6kvKZ/tmp/eac3d9820be8f0207714f82656851a291bf5fb272a08562d3c3f966bdaefdf5c.jpg", "table_caption": ["Table 1: Label shift test accuracy after $R=1$ and $R=3$ communication rounds. We primarily compared two categories of methods: conventional FL methods and state-of-the-art local weight averaging-based fine-tuning methods that enhance domain generalization. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "0LfgE6kvKZ/tmp/6053eb67c3d5b68cbea498e159c22fd6e4fc86784c263f1b0fd18bb05bfa6ea1.jpg", "table_caption": ["Table 2: Feature shift test accuracy after $R\\,=\\,1$ and $R\\,=\\,3$ communication rounds. LSS consistently outperforms other methods on both datasets across under feature shift settings. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset. Our experimental section considers two scenarios of Non-IID settings, namely label shift and feature shift. The label shift scenario investigates datasets such as FMNIST [53] and CIFAR10 [26], while feature shift involves Digit5 and DomainNet. Further information on the specific datasets can be found in the appendix. In the label shift scenario, we partitioned the dataset into five clients and the data for each client are sampled following Dirichlet distributions with coefficient $\\alpha=1.0$ , yielding imbalanced label distributions. In the feature shift scenario, we utilized five clients for Digit5 [13, 30] and five clients for DomainNet [40]. Additional results on an extended number of clients are presented in the appendix. ", "page_idx": 6}, {"type": "text", "text": "Model. In terms of models, we used the ImageNet pre-trained ResNet50 [16] as the base model for the DomainNet dataset, while for other datasets, we used the pre-trained ResNet-18 trained on ImageNet [9]. We also present the experimental results based on the vision transformer (ViT) model [11] with parameter-efficient fine-tuning. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We compare LSS against the vanilla FL method - FedAvg [35] and several advanced FL algorithms designed for Non-IID settings, including FedProx [28], MOON [27], FedBN [30], FedFomo [58], FedRep [8] and FedBABU [38]. Additionally, we make comparisons with topperforming weight/model-averaging-based domain generalization methods including SWA [21], SWAD [1], Soups [52] and DiWA [43] by adapting them to FL. In particular, the specific approach is to modify the local client training in the FedAvg framework to a corresponding fine-tuning approach. For more details, please refer to the appendix. ", "page_idx": 6}, {"type": "text", "text": "Evaluation and implementation details. Unless otherwise specified, the model performance in the experiments below refers to the global model performance after aggregation on the server side. Our training optimizer uses the Adam optimizer with a learning rate of 5e\u22124 and a training batch size of ", "page_idx": 6}, {"type": "text", "text": "64. For commonly used FL methods, due to the significant increase in local update steps that leads to worse convergence, we set their local update steps to 8. For SWA, SWAD, and our method, we take more local update steps, with each model being averaged trained 8 steps, and the default number of models to be averaged is 4. For the Model Soups method and DiWA, we train 32 models with 8 steps. Additional details of experiment implementations are included in the Appendix. ", "page_idx": 7}, {"type": "image", "img_path": "0LfgE6kvKZ/tmp/230ab49f455bf35cbf0804640f1f8efa3b84e7e75ebe8fa6dc59adcbe7cd1bbe.jpg", "img_caption": ["Figure 3: Convergence comparison of our proposed LSS with FedAvg. LSS achieves high accuracy much earlier (around 6 to 8 rounds) than FedAvg, which takes hundreds of communication rounds. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Performance Comparison ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Results on label shift. To demonstrate the effectiveness of $L S S$ on label shift scenario, we conduct comparison experiments on FMNIST and CIFAR-10 datasets. We consider fine-tuning with an extremely limited number of communication rounds (i.e., $R=1$ and $R=3$ ). Table 1 reports the test accuracy with the format of mean (std) for all compared algorithms. All experiments are repeated 3 runs with different random seeds. In Table 1, $L S S$ achieves the best accuracy on all settings of both datasets, which validates that $L S S$ is efficient and effective in fine-tuning FL for label shift Non-IID. Notably, with just one round of communication, $L S S$ can double the accuracy of the best Non-IID FL baseline method. Surprisingly, the simple extension of model-averaging-based domain generalization methods onto FedAvg [35] (the 2nd big row in Table 1) perform very well, especially when the number communication round is small. The superior performance using local weight averaging-based fine-tuning is likely because it significantly reduces the gradient variance of local and global variance (see 3.2). We further provide results on different levels of label shift in the supplementary material. ", "page_idx": 7}, {"type": "text", "text": "Results on feature shift. Table 2 evaluates on feature shift scenario using Digits-5 and DomainNet datasets. Similar to the previous experiment setting for Table 1, we repeat all the algorithms with 3 random seeds. Consistent with the observation in Table 2, LSS is the top-performing method under all the settings for both datasets. We also observe better performance achieved by adapting model-averaging-based domain generalization methods (the 2nd big row in Table 2) in FL than the existing Non-IID FL methods (the 1st big row in Table 2), which further verifies the effectiveness of model averaging to obtain better global model while improving communication efficiency. ", "page_idx": 7}, {"type": "text", "text": "Convergence plots. We also evaluate the strength of faster convergence using the proposed LSS compared with FedAvg [35] on CIFAR-10 (label shift) and Digtis-5 (feature shift). Fig. 3 depicts the testing accuracies at early and late phases regarding the number of communication rounds to reach convergence. First, by looking at the final testing accuracies on Fig. 3 (b) and (d), LSS achieves better performance. Second, Fig. 3 (a) and (c) show that $L S S$ almost meets the targeted performance at the very early stage (i.e.around 6 to 8 rounds), whereas FedAvg requests over hundreds of communication rounds. ", "page_idx": 7}, {"type": "text", "text": "Parameter-Efficient Tuning with ViT. We also deployed the Vision Transformer (ViT) [11] in FL learning. On Digits-5 dataset, we evaluate the ViT model with a resolution of 224 and a patch size of 16, which was pretrained on the ImageNet-21k dataset. Due to the large number of parameters in ViT, we used a parameter-efficient fine-tuning method called LoRA [19] to train it for all the methods. For more details about our ViT architecture and LoRA training, please refer to the appendix. It can be observed in Fig. 4 that our method is applicable to pre-trained ViT models, demonstrating that our approach can be combined with parameter-efficient fine-tuning methods to further enhance the communication efficiency of FL. ", "page_idx": 7}, {"type": "image", "img_path": "0LfgE6kvKZ/tmp/e36eb064a44c46dbc3ecf42b94c26dab4d2399475e50e9b30adf7950a5f66020.jpg", "img_caption": ["Figure 4: Evaluation on ViT fine-tuned with LoRA (Digit5 dataset). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conducted ablation experiments on the main components (i.e., affinity, diversity term and averaged model quantity) of our proposed method and evaluated their performance on the CIFAR dataset, with the performance metric being the global model performance at communication round $R=1$ . ", "page_idx": 8}, {"type": "text", "text": "Investigation on regularization losses. In order to examine the importance of affinity loss and diversity loss, as well as the influence of their corresponding coefficients, we adjust one coefficient within a range of 0 to 4 while maintaining the other at a constant value. By comparing the performance with and without loss term, we observe that adding affinity and diversity terms can enhance the model\u2019s performance. Additionally, we observe that the two terms complement each other, and selecting appropriate coefficients can achieve significant performance improvement (e.g., adjusting the affinity coefficient to 3 as shown in Fig. 5 (a) and diversity coefficient to 3 as shown in Fig. 5 (b)). ", "page_idx": 8}, {"type": "image", "img_path": "0LfgE6kvKZ/tmp/adb4dad0e6da7523ebabdcbc83bd3ce06ea83127d7d84af8b62ae2b798864691.jpg", "img_caption": ["Figure 5: Ablation on the affinity & diversity. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Investigation on the number of averaged models. ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To investigate the impact of the averaged model quantity on enhancing communication efficiency and reducing gradient variance between local and global, we experiment with varied model quantities and evaluate their influence on global model performance, averaged local model performance2, and worst outof-distribution (OOD) generalization performance on the other clients. Fig. 6 shows that increasing the number of averaged models can improve the model\u2019s OOD generalization ability and enhance the performance of the aggregated model. This similar upward trend confirms the validity of our analysis linking ", "page_idx": 8}, {"type": "image", "img_path": "0LfgE6kvKZ/tmp/be571284de6ca6d061c4266633b3f1f593ff2b60dff8cc5f05bc3a66bcf75a18.jpg", "img_caption": ["Figure 6: Ablation studies on the impact of the number of averaged models on communication efficiency and performance variance. We evaluated the influence of varied model quantities on global and averaged local model performance, as well as generalization on the worst client. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "OOD generalization and local-global variance. We provide a more detailed analysis on connecting our proposed LSS and OOD generalization in appendix C. Additionally, we can observe that increasing the number of models in our method can improve both pre-aggregation and post-aggregation model performance. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Limitations and Broader Impact. Our method reduces communication rounds but trades off training memory and performance. Future work should explore more memory-efficient deployments. While focused on vision tasks, extending to language and multimodal scenarios is promising. Balancing performance and communication in healthcare FL is promising, but excessive reduction can impair critical medical decisions. Careful trade-off consideration is essential for reliable FL applications in sensitive areas. ", "page_idx": 8}, {"type": "text", "text": "Conclusion. We propose an efficient method, Local Superior Soups (LSS), to reduce communication rounds in FL with pre-trained initialization, addressing the challenge of data heterogeneity. By employing sequential model interpolation, connectivity preservation, and two regularization terms (diversity and affinity), the method allows for an increase in local training steps and a reduction in communication rounds while avoiding client drift. This approach, tailored for pre-trained model adaptation in FL, offers training and inference efficiency, making it suitable for practical deployment in edge computing scenarios. As the first step towards understanding and developing model soupsbased methods in pre-trained models in FL, this study conducts experiments on benchmark datasets. Our method attain superior performance with a only few rounds of communication and surpasses the performance of standard FL methods significantly across four datasets and under two distribution shift scenarios. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgement. M. Chen, Z. Wang and X. Li are grateful for the support of the Natural Science and Engineering Research Council of Canada (NSERC). M. Chen and X. Li are supported by the Canada CIFAR AI Chairs program, MITACS-CIFAR Catalyst Grant Program, NVIDIA Hardware Awards, the Digital Research Alliance of Canada, and Canada Foundation for Innovation (CFI). M. Jiang and Q. Dou are supported by the Research Grants Council of the Hong Kong Special Administrative Region (Project No. T45- 401/22-N). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. SWAD: domain generalization by seeking flat minima. In NeurIPS, pages 22405\u201322418, 2021.   \n[2] Hong-You Chen, Cheng-Hao Tu, Ziwei Li, Han-Wei Shen, and Wei-Lun Chao. On the importance and applicability of pre-training for federated learning, 2022.   \n[3] Minghui Chen, Meirui Jiang, Qi Dou, Zehua Wang, and Xiaoxiao Li. Fedsoup: Improving generalization and personalization in federated learning via selective model interpolation. In MICCAI (2), volume 14221 of Lecture Notes in Computer Science, pages 318\u2013328. Springer, 2023.   \n[4] Minghui Chen, Cheng Wen, Feng Zheng, Fengxiang He, and Ling Shao. VITA: A multi-source vicinal transfer augmentation method for out-of-distribution generalization. In AAAI, pages 321\u2013329. AAAI Press, 2022.   \n[5] Gary Cheng, Karan N. Chadha, and John C. Duchi. Fine-tuning is fine in federated learning. CoRR, abs/2108.07313, 2021.   \n[6] L\u00e9na\u00efc Chizat, Edouard Oyallon, and Francis R. Bach. On lazy training in differentiable programming. In NeurIPS, pages 2933\u20132943, 2019.   \n[7] Leshem Choshen, Elad Venezian, Shachar Don-Yehiya, Noam Slonim, and Yoav Katz. Where to start? analyzing the potential value of intermediate models. CoRR, abs/2211.00107, 2022.   \n[8] Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for personalized federated learning. In ICML, volume 139 of Proceedings of Machine Learning Research, pages 2089\u20132099. PMLR, 2021.   \n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248\u2013255. IEEE Computer Society, 2009.   \n[10] Li Deng. The MNIST database of handwritten digit images for machine learning research [best of the web]. IEEE Signal Process. Mag., 29(6):141\u2013142, 2012.   \n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR. OpenReview.net, 2021.   \n[12] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In ICML, volume 119 of Proceedings of Machine Learning Research, pages 3259\u20133269. PMLR, 2020.   \n[13] Yaroslav Ganin and Victor S. Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, volume 37 of JMLR Workshop and Conference Proceedings, pages 1180\u20131189. JMLR.org, 2015.   \n[14] Neel Guha, Ameet Talwalkar, and Virginia Smith. One-shot federated learning. CoRR, abs/1902.11175, 2019.   \n[15] Kaiming He, Ross B. Girshick, and Piotr Doll\u00e1r. Rethinking imagenet pre-training. In ICCV, pages 4917\u20134926. IEEE, 2019.   \n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778. IEEE Computer Society, 2016.   \n[17] Parikshit Hegde, Gustavo de Veciana, and Aryan Mokhtari. Network adaptive federated learning: Congestion and lossy compression. In INFOCOM, pages 1\u201310. IEEE, 2023.   \n[18] Clare Elizabeth Heinbaugh, Emilio Luz-Ricca, and Huajie Shao. Data-free one-shot federated learning under very high statistical heterogeneity. In The Eleventh International Conference on Learning Representations, 2023.   \n[19] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR. OpenReview.net, 2022.   \n[20] Pavel Izmailov, Wesley J. Maddox, Polina Kirichenko, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. Subspace inference for bayesian deep learning. In UAI, volume 115 of Proceedings of Machine Learning Research, pages 1169\u20131179. AUAI Press, 2019.   \n[21] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In UAI, pages 876\u2013885. AUAI Press, 2018.   \n[22] Arthur Jacot, Cl\u00e9ment Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and generalization in neural networks. In NeurIPS, pages 8580\u20138589, 2018.   \n[23] Jean Kaddour, Linqing Liu, Ricardo Silva, and Matt J. Kusner. Questions for flat-minima optimization of modern neural networks. CoRR, abs/2202.00661, 2022.   \n[24] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020.   \n[25] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich, and Ananda Theertha Suresh. SCAFFOLD: stochastic controlled averaging for federated learning. In ICML, volume 119 of Proceedings of Machine Learning Research, pages 5132\u2013 5143. PMLR, 2020.   \n[26] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. corr, 2009.   \n[27] Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In CVPR, pages 10713\u201310722. Computer Vision Foundation / IEEE, 2021.   \n[28] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. In MLSys. mlsys.org, 2020.   \n[29] Weishi Li, Yong Peng, Miao Zhang, Liang Ding, Han Hu, and Li Shen. Deep model fusion: A survey. CoRR, abs/2309.15698, 2023.   \n[30] Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning on non-iid features via local batch normalization. In ICLR. OpenReview.net, 2021.   \n[31] Xinyan Li and Arindam Banerjee. Experiments with rich regime training for deep learning. CoRR, abs/2102.13522, 2021.   \n[32] Raphael Gontijo Lopes, Sylvia J. Smullin, Ekin D. Cubuk, and Ethan Dyer. Affinity and diversity: Quantifying mechanisms of data augmentation. CoRR, abs/2002.08973, 2020.   \n[33] Bing Luo, Xiang Li, Shiqiang Wang, Jianwei Huang, and Leandros Tassiulas. Cost-effective federated learning in mobile edge networks. IEEE J. Sel. Areas Commun., 39(12):3606\u20133621, 2021.   \n[34] Wesley J. Maddox, Pavel Izmailov, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. A simple baseline for bayesian uncertainty in deep learning. In NeurIPS, pages 13132\u201313143, 2019.   \n[35] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag\u00fcera y Arcas. Communication-efficient learning of deep networks from decentralized data. In AISTATS, volume 54 of Proceedings of Machine Learning Research, pages 1273\u20131282. PMLR, 2017.   \n[36] Vaishnavh Nagarajan and J. Zico Kolter. Uniform convergence may be unable to explain generalization in deep learning. In NeurIPS, pages 11611\u201311622, 2019.   \n[37] John Nguyen, Jianyu Wang, Kshitiz Malik, Maziar Sanjabi, and Michael Rabbat. Where to begin? on the impact of pre-training and initialization in federated learning. CoRR, abs/2210.08090, 2022.   \n[38] Jaehoon Oh, Sangmook Kim, and Se-Young Yun. Fedbabu: Towards enhanced representation for federated image classification. In ICLR. OpenReview.net, 2022.   \n[39] Younghyun Park, Dong-Jun Han, Do-Yeon Kim, Jun Seo, and Jaekyun Moon. Few-round learning for federated learning. In NeurIPS, pages 28612\u201328622, 2021.   \n[40] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, pages 1406\u20131415. IEEE, 2019.   \n[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, volume 139 of Proceedings of Machine Learning Research, pages 8748\u20138763. PMLR, 2021.   \n[42] Alexandre Ram\u00e9, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, L\u00e9on Bottou, and David LopezPaz. Recycling diverse models for out-of-distribution generalization. CoRR, abs/2212.10445, 2022.   \n[43] Alexandre Ram\u00e9, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, Patrick Gallinari, and Matthieu Cord. Diverse weight averaging for out-of-distribution generalization. In NeurIPS, 2022.   \n[44] Shivalika Singh, Freddie Vargus, Daniel D\u2019souza, B\u00f6rje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura O\u2019Mahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzeminski, Hakimeh Fadaei, Irem Erg\u00fcn, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Minh Chien Vu, Sebastian Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet \u00dcst\u00fcn, Marzieh Fadaee, and Sara Hooker. Aya dataset: An open-access collection for multilingual instruction tuning. CoRR, abs/2402.06619, 2024.   \n[45] Yan Sun, Li Shen, Tiansheng Huang, Liang Ding, and Dacheng Tao. Fedspeed: Larger local interval, less communication round, and higher generalization accuracy. In ICLR. OpenReview.net, 2023.   \n[46] Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. Towards personalized federated learning. CoRR, abs/2103.00710, 2021.   \n[47] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023.   \n[48] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aur\u00e9lien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. ", "page_idx": 12}, {"type": "text", "text": "[49] Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan AlShedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021. ", "page_idx": 12}, {"type": "text", "text": "[50] Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H. Brendan McMahan, Blaise Ag\u00fcera y Arcas, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, Suhas N. Diggavi, Hubert Eichner, Advait Gadhikar, Zachary Garrett, Antonious M. Girgis, Filip Hanzely, Andrew Hard, Chaoyang He, Samuel Horv\u00e1th, Zhouyuan Huo, Alex Ingerman, Martin Jaggi, Tara Javidi, Peter Kairouz, Satyen Kale, Sai Praneeth Karimireddy, Jakub Konec\u02c7n\u00fd, Sanmi Koyejo, Tian Li, Luyang Liu, Mehryar Mohri, Hang Qi, Sashank J. Reddi, Peter Richt\u00e1rik, Karan Singhal, Virginia Smith, Mahdi Soltanolkotabi, Weikang Song, Ananda Theertha Suresh, Sebastian U. Stich, Ameet Talwalkar, Hongyi Wang, Blake E. Woodworth, Shanshan Wu, Felix X. Yu, Honglin Yuan, Manzil Zaheer, Mi Zhang, Tong Zhang, Chunxiang Zheng, Chen Zhu, and Wennan Zhu. A field guide to federated optimization. CoRR, abs/2107.06917, 2021.   \n[51] Mitchell Wortsman, Maxwell Horton, Carlos Guestrin, Ali Farhadi, and Mohammad Rastegari. Learning neural network subspaces. In ICML, volume 139 of Proceedings of Machine Learning Research, pages 11217\u201311227. PMLR, 2021.   \n[52] Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In ICML, volume 162 of Proceedings of Machine Learning Research, pages 23965\u201323998. PMLR, 2022.   \n[53] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. CoRR, abs/1708.07747, 2017.   \n[54] Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W. Mahoney. Pyhessian: Neural networks through the lens of the hessian. In IEEE BigData, pages 581\u2013590. IEEE, 2020.   \n[55] Rui Ye, Rui Ge, Xinyu Zhu, Jingyi Chai, Yaxin Du, Yang Liu, Yanfeng Wang, and Siheng Chen. Fedllm-bench: Realistic benchmarks for federated learning of large language models. CoRR, abs/2406.04845, 2024.   \n[56] Jie Zhang, Chen Chen, Bo Li, Lingjuan Lyu, Shuang Wu, Shouhong Ding, Chunhua Shen, and Chao Wu. DENSE: data-free one-shot federated learning. In NeurIPS, 2022.   \n[57] Lin Zhang, Li Shen, Liang Ding, Dacheng Tao, and Ling-Yu Duan. Fine-tuning global model via data-free knowledge distillation for non-iid federated learning. In CVPR, pages 10164\u201310173. IEEE, 2022.   \n[58] Michael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, and Jose M. Alvarez. Personalized federated learning with first order model optimization. In ICLR. OpenReview.net, 2021.   \n[59] Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu, Lizhen Qu, and Zenglin Xu. Fedpetuning: When federated learning meets the parameter-efficient tuning methods of pretrained language models. In ACL (Findings), pages 9963\u20139977. Association for Computational Linguistics, 2023.   \n[60] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS, 2023.   \n[61] Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In NeurIPS, pages 14747\u201314756, 2019. ", "page_idx": 12}, {"type": "text", "text": "Roadmap of Appendix The appendix is organized as follows. We list the notations table in Section A. We provide the theoretical proof of the convergence analysis in Section B. We present the theoretical intuition of our proposed two loss term C. Next, we provide more detailed related work in Sec. D We present more experiment details and results in Sec. E. ", "page_idx": 13}, {"type": "text", "text": "A Notation Table ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "0LfgE6kvKZ/tmp/ac4afa926692b272c7298950629129c2b49bef50db8c4d238172b5c5fc68c56b.jpg", "table_caption": ["Table 3: Important notations used in the paper. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Convergence Analysis ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Formal Restatement of Convergence Theorem ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Standard FL [35] employs a server to coordinate the following iterative distributed training: ", "page_idx": 13}, {"type": "text", "text": "Step $^{\\,I}$ In each global round of training $r\\,\\in\\,[R]$ , the server broadcasts its current global model weight f g(r\u22121)to all the clients;   \nStep 2 The selected client $c$ copies the current server model weight $f_{c}^{r,0}\\gets f_{g}$ , performs $\\tau$ local step updates, then sends $f_{c}^{r,\\tau}-f_{g}^{(r-1)}$ f g(r\u22121)back to the server;   \nStep 3 The server aggregates the updates from all clients $\\{f_{c}^{r,\\tau}-f_{g}^{(r-1)}\\}_{c=1}^{C}$ to form the new server model using the weighted averaging in Eq 1: ", "page_idx": 13}, {"type": "text", "text": "Note that the initialization $f^{(0,0)}$ ,with the subscription indicating model at $0{-}t h$ communication round and $0{-}t h$ local step, is a pre-trained model (e.g. using public datasets) in our problem. This work focus on improving Step 2 to explore a larger low-loss region in local clients. ", "page_idx": 13}, {"type": "text", "text": "Formally, we present the convergence results (Theorem 3.1) and specify the following formal assumptions: 1) Convexity and Smoothness Assumption on $\\beta$ -smooth loss function, 2) Bounded Variance of Stochastic Gradient Assumption and 3) Bounded Variance of Local and Global Gradient Assumption). ", "page_idx": 13}, {"type": "text", "text": "Assumption B.1. (Convexity and Smoothness). $\\mathcal{L}_{i}$ is convex and $\\beta$ -smooth for all $i\\in[M]$ , i.e., ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla\\mathcal{L}_{i}(w)-\\nabla\\mathcal{L}_{i}(v)\\|\\leq\\beta\\|w-v\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for all $w,v$ in its domain and $i\\in[M]$ . ", "page_idx": 13}, {"type": "text", "text": "Assumption B.2. (Bounded variance of stochastic gradient). Each client can achieve an unbiased stochastic gradient with $\\sigma^{2}$ -uniformly bounded variance for all $k\\in[0,\\tau)$ , namely ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[g_{i}(f_{i}^{(r,k)})|f_{i}^{(r,k)}]=\\nabla\\mathcal{L}_{i}(f_{i}^{(r,k)}),\\quad\\mathbb{E}[\\|g_{i}(f_{i}^{(r,k)})-\\nabla\\mathcal{L}_{i}(f_{i}^{(r,k)})\\|^{2}|f_{i}^{(r,k)}]\\le\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Assumption B.3. (Bounded variance of local and global gradient). The difference of local gradient $\\nabla\\mathcal{L}_{i}(\\bar{f})$ and the global gradient $\\nabla{\\mathcal{L}}(f)$ is bounded in $\\ell_{2}$ norm, namely ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i}\\operatorname*{sup}_{f}\\left\\|\\nabla\\mathcal{L}_{i}(f_{i}^{(r,k)})-\\nabla\\mathcal{L}(f_{i}^{(r,k)})\\right\\|\\leq\\zeta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Assumption B.4. (Bounded regularization update). The update introduced by the affinity and diversity regularization term $q(t,\\mu_{t},\\mu_{a})$ in the update rule $\\theta(t+1)=\\theta(t)-\\eta g(t)-q(t,\\mu_{t},\\mu_{a})$ , is bounded by a constant $c$ , namely ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(t,\\mu_{t},\\mu_{a})\\leq c.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We have the main theorem on convergence rate, which similar to [49] except for the introduced regularization terms. ", "page_idx": 14}, {"type": "text", "text": "Theorem 3.1: Convergence Rate for Convex Local Functions with Affinity and Diversity Constraint Under Convexity and Smoothness Assumption on $\\beta.$ -smooth loss function, Bounded Variance of Stochastic Gradient and Bounded Variance of Local and Global Gradient assumptions, when the client learning rate is chosen properly as, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\eta=\\operatorname*{min}\\{{\\frac{1}{4\\beta}},{\\frac{M^{\\frac{1}{2}}d}{\\tau^{{\\frac{1}{2}}}R^{{\\frac{1}{2}}},\\sigma}},{\\frac{d^{\\frac{2}{3}}}{\\tau^{{\\frac{2}{3}}}R^{{\\frac{1}{3}}}\\beta^{{\\frac{1}{3}}}\\sigma^{\\frac{2}{3}}}},{\\frac{d^{\\frac{2}{3}}}{\\tau R^{{\\frac{1}{3}}}\\beta^{{\\frac{1}{3}}}(\\zeta+c)^{\\frac{2}{3}}}}\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "we define $\\begin{array}{r}{\\epsilon=\\mathbb{E}\\left[\\frac{1}{\\tau R}\\sum_{r=0}^{R-1}\\sum_{k=1}^{\\tau}\\beta(\\overline{{f}}^{(r,k)})-\\beta(f^{\\star})\\right]}\\end{array}$ , and have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\epsilon\\le\\frac{2\\beta R^{2}}{\\tau R}+\\frac{2\\sigma d}{\\sqrt{M\\tau R}}+\\frac{5\\beta^{\\frac{1}{3}}\\sigma^{\\frac{2}{3}}d^{\\frac{4}{3}}}{\\tau^{\\frac{1}{3}}R^{\\frac{2}{3}}}+\\frac{15\\beta^{\\frac{1}{3}}(\\zeta+c)^{\\frac{2}{3}}d^{\\frac{4}{3}}}{R^{\\frac{2}{3}}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, the update rule of the $t$ iteration with the affinity and diversity term is defined as $\\theta(t+1)=$ $\\theta(t)-\\eta g(\\bar{t})-q(t,\\mu_{t},\\mu_{a})$ , and the extra term satisfies $q(t,\\mu_{t},\\mu_{a})\\leq c$ . The hyper-parameters $\\mu_{t}$ and $\\mu_{a}$ represent the co-efficient of tuning affinity and diversity respectively. ", "page_idx": 14}, {"type": "text", "text": "Besides, $d:=\\lVert f^{(0,0)}-f^{\\star}\\rVert$ refers to the distance between initialization $f^{(0,0)}$ and the global optimum $f^{\\star}$ , $\\sigma$ bounds variance of stochastic gradient by $\\mathbb{E}[\\|g_{i}(f^{(r,k)})-\\nabla\\mathcal{L}_{i}(f^{(r,k)})\\|^{2}|f^{(r,k)}]\\le\\sigma^{2}$ , and $\\zeta$ bounds variance of local and global gradient by $\\begin{array}{r}{\\operatorname*{max}_{i}\\operatorname*{sup}_{f}\\left\\|\\nabla\\mathcal{L}_{i}\\big(f^{(r,k)}\\big)-\\nabla\\mathcal{L}\\big(f^{(r,k)}\\big)\\right\\|\\leq\\zeta.}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "The regularization update bound is reasonable since $q(\\mu_{t},\\mu_{a})=\\mu_{t}*(\\theta-\\theta_{m})-\\mu_{a}*(\\theta-\\theta_{m})$ . Here, $\\theta_{m}$ is the averaged parameter of all the parameter in the model pool for interpolation. As the inherent trade-off effect of diversity and affinity term, $q(\\mu_{t},\\mu_{a})$ will not diverge too much in practice. And the the bounded value $c$ can be effectively controlled by tuning hyper-parameter $\\mu_{a}$ and $\\mu_{t}$ ", "page_idx": 14}, {"type": "text", "text": "The distinguishing factor in our convergence rate, as compared to that in [49], stems from the unique inter-client update bound facilitated by our proposed regularization term. We have the inter-client (e.g., for client index 1 and 2) update bound as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla\\mathcal{L}_{1}(f)-\\nabla\\mathcal{L}_{2}(f)-q_{1}(\\mu_{t},\\mu_{a})+q_{2}(\\mu_{t},\\mu_{a})\\|\\leq4(\\zeta^{2}+c^{2}+2\\zeta c)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. To find a tight bound for $\\|\\nabla\\mathcal{L}_{1}(f)-\\nabla\\mathcal{L}_{2}(f)-q_{1}(\\mu_{t},\\mu_{a})+q_{2}(\\mu_{t},\\mu_{a})\\|$ , we will use the given inequalities: $\\left\\|\\nabla\\mathcal{L}_{i}(f^{(r,k)})-\\nabla\\mathcal{L}(f^{(r,k)})\\right\\|\\leq\\zeta$ , i.e., ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla\\mathcal{L}_{1}(f)-\\nabla\\mathcal{L}(f))\\|\\leq\\zeta,\\,\\|\\nabla\\mathcal{L}_{2}(f)-\\nabla\\mathcal{L}(f))\\|\\leq\\zeta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By breaking down the expression with inserting global gradient $\\nabla\\mathcal{L}$ and then apply the triangle inequality of absolute value, and our given inequalities, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla\\mathcal{L}_{1}(f)-\\nabla\\mathcal{L}_{2}(f))-q_{1}(\\mu_{t},\\mu_{a})+q_{2}(\\mu_{t},\\mu_{a})\\|\\leq(2\\zeta+2c)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combined our derived inter-client update bound with the Equation (30) in Appendix D.2 in [49], we can easily obtain a different bounded client update drift bound $\\epsilon_{c}$ as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\epsilon_{c}\\leq4\\tau\\eta^{2}\\sigma^{2}+14\\tau^{2}\\eta^{2}(\\zeta+c)^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Leveraging the above in-equation and choosing the learning rate $\\eta$ properly, we can get our Theorem 3.1. ", "page_idx": 14}, {"type": "text", "text": "B.2 Proof of Proposition 3.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proposition 3.2 Under the data heterogeneity setting, when the total number of gradient computations across all clients $\\langle K=M\\tau R\\rangle$ ) is fixed and the local steps $\\tau$ satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tau\\leq\\frac{\\sigma}{\\zeta+c}\\sqrt{\\frac{\\sigma}{d\\beta}\\frac{K^{\\frac{1}{2}}}{M^{2}}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "the error upper bound Eq.equation 15 will be dominated by the second term $\\mathcal{O}(1/\\sqrt{K})$ . ", "page_idx": 15}, {"type": "text", "text": "Taking local steps can save total communication rounds compared to synchronous SGD. To be more specific, as suggested in [49], when the total number of gradient evaluations/computations across all clients $\\langle K=M\\tau R$ ) is fixed and the local steps $\\tau$ satisfies: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tau\\leq\\operatorname*{min}\\left\\{\\frac{\\sigma}{d\\beta}\\frac{K^{\\frac{1}{2}}}{M^{2}},\\frac{\\sigma}{\\zeta+c}\\sqrt{\\frac{\\sigma}{d\\beta}\\frac{K^{\\frac{1}{2}}}{M^{2}}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When the upper bound of local steps (Eq.(3)) becomes larger, there will be more communication savings. Therefore, the quantity in Eq.(3) represents the largest savings in communication rounds. Next, we show the error upper bound under the data heterogeneity setting. ", "page_idx": 15}, {"type": "text", "text": "Proof. Under high data heterogeneity, we have $\\zeta+c\\geq\\sigma$ , and: ", "page_idx": 15}, {"type": "equation", "text": "$$\n1\\leq\\frac{\\sigma}{\\zeta+c}\\sqrt{\\frac{\\sigma}{d\\beta}\\frac{K^{\\frac{1}{2}}}{M^{2}}}\\leq\\sqrt{\\frac{\\sigma}{d\\beta}\\frac{K^{\\frac{1}{2}}}{M^{2}}}\\leq\\frac{\\sigma}{d\\beta}\\frac{K^{\\frac{1}{2}}}{M^{2}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, we have Proposition 3.2: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tau\\leq\\frac{\\sigma}{\\zeta+c}\\sqrt{\\frac{\\sigma}{d\\beta}\\frac{K^{\\frac{1}{2}}}{M^{2}}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This Proposition 3.2 indicates that when client data are Non-IID, the side effects of the error term in the Theorem 1 will be further exacerbated, therefore, increasing the local iteration steps effectively reduces the communication rounds. ", "page_idx": 15}, {"type": "text", "text": "C Theoretical Intuitions. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Decomposition of Generalization Bound ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Connecting $\\zeta$ with out-of-distribution error. Ensemble is a category of the promising method that ensembles trained models to improve generalizability as demonstrated in centralized settings via reducing model discrepancy [21]. To reduce the variance $\\zeta$ of local and global gradients that is resulted by data heterogeneity, we aim to adapt ensemble to FL. Intuitively, local client training that can reduce the error on the worst domain (client) in $\\mathrm{FL}$ will reduce the variance $\\zeta$ . ", "page_idx": 15}, {"type": "text", "text": "In the following, we detail how to reduce $\\zeta$ with OOD error with a bias-variance-covariance-locality (BVCL) decomposition analysis. ensemble can be defined as: $\\begin{array}{r}{f_{\\mathrm{WA}}\\,\\triangleq\\,1/N\\sum_{n=1}^{N}f_{n}}\\end{array}$ . We have the following decomposition of ensemble\u2019s expected test error. Bias-variance -covariance-locality decomposition. The expected generalization error on domain $T$ of $f_{\\mathrm{WA}}$ over the joint distribution $(L_{S}^{N}\\triangleq\\{l_{S}^{(N)}\\}_{N=1}^{N})$ of $N$ learning procedure on domain $S$ is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{L_{S}^{N}}\\mathcal{E}_{T}(f_{\\mathrm{WA}}(L_{S}^{N}))=\\mathbb{E}_{(x,y)\\sim p_{T}}\\Big[\\mathrm{bias}^{2}(x,y)+\\frac{1}{N}\\mathrm{var}(x)+\\frac{N-1}{N}\\mathrm{cov}(x)\\Big]+O(\\bar{\\Delta}^{2}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, cov refers to the covariance of predictions made by two member models. The first component is the same bias as that of each individual member. The variance of ensemble is split into two parts: ", "page_idx": 15}, {"type": "text", "text": "the variance of each member divided by the number of members $(N)$ and a covariance term. The last locality term enforces constraints on the weights to ensure the functional ensembling approximation remains valid. In summary, combining $N$ models reduces variance by a factor of $N$ , but introduces the covariance and locality terms which must be controlled to ensure low OOD error. ", "page_idx": 16}, {"type": "text", "text": "In the analysis presented in [43], the authors proposed a BVCL decomposition based on the approximation of functional ensembling (i.e., averaged prediction instead of parameter) by WA. The expected generalization error on domain $T$ of $f_{\\mathrm{WA}}$ over the joint distribution $(L_{S}^{N}\\triangleq\\{l_{S}^{(N)}\\}_{N=1}^{N})$ of $N$ learning procedure on domain $S$ is: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{L_{S}^{N}}\\mathcal{E}_{T}(f_{\\mathrm{WA}}(L_{S}^{N}))=\\mathbb{E}_{(x,y)\\sim p_{T}}\\Big[\\mathrm{bias}^{2}(x,y)+\\frac{1}{N}\\mathrm{var}(x)+\\frac{N-1}{N}\\mathrm{cov}(x)\\Big]+O(\\bar{\\Delta}^{2}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Definition C.1 (Bias). For $x\\in X$ and $y\\in Y$ , we define the bias of OOD prediction as, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{bias}(x,y)=y-\\mathbb{E}_{l_{S}}[f(x,l_{S})].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Definition C.2 (Variance). For $x\\in X$ , we define the variance of prediction as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{var}(x)=\\mathbb{E}_{f_{S}}\\left[\\left(f(x,l_{S})-\\mathbb{E}_{l_{S}}\\left[f(x,l_{S})\\right]\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Definition C.3 (Covariance). For $x\\in X$ , we define the covariance of prediction produced by two different learning procedures $l_{S}$ and $l_{S}^{\\prime}$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{cov}(x)=\\mathbb{E}_{l_{S},l_{S}^{\\prime}}\\left[\\left(f(x,l_{S})-\\mathbb{E}_{l_{S}}\\left[f(x,l_{S})\\right]\\right)\\left(f(x,l_{S}^{\\prime})-\\mathbb{E}_{l_{S}}\\left[f(x,l_{S})\\right]\\right)\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Definition C.4 (Locality). For any averaged models $f_{i}$ (for $i\\in[N])$ , $i$ is the index of an averaged model, $N$ is the total number of averaged models, we define the locality of all averaged models as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\bar{\\Delta}^{2}=\\mathbb{E}_{L_{S}^{N}}\\Delta_{L_{S}^{N}}^{2}\\;\\mathrm{with}\\;\\Delta_{L_{S}^{N}}=\\operatorname*{max}_{i=1}^{N}\\left\\|f_{i}-f_{\\mathrm{WA}}\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Following the definitions of the terms in the BCVL generalization bound, we discuss the insights of reducing the bound via the proposed strategy. Our method is based on WAFT, which enjoys the benefti of reducing prediction variance by averaging the predictions of multiple models. The diversity term in our proposed method reduces the covariance term by encouraging functional diversity in the parameter space. The affinity term in our proposed method reduces the locality term to ensure the approximation of weight averaging (WA) to prediction ensembling. ", "page_idx": 16}, {"type": "text", "text": "Analysis on variance. One can see that an increase in the number of averaged models can directly lead to a reduction in variance. The straightforward averaging $M$ models, as seen in the vanilla WAFT method, diminishes variance by a factor of $M$ . However, this approach also introduces covariance and locality terms, which necessitate meticulous management on adding new averaged models to guarantee minimal out-of-distribution (OOD) error. ", "page_idx": 16}, {"type": "text", "text": "Analysis on covariance. The covariance term represents the predictive covariance between two member models whose weights are averaged. It increases when the predictions of different averaged models are highly correlated. In the worst-case scenario where all predictions are identical, the covariance is equal to the variance, rendering the benefits of weight averaging ineffective [43]. Conversely, when the covariance is lower, the advantages of weight averaging over individual models become more pronounced. Therefore, it is crucial to address covariance by promoting functional diversity among the averaged models. Our proposed method incorporates a diversity term that aims to reduce this covariance. ", "page_idx": 16}, {"type": "text", "text": "Analysis on locality. The locality term, which represents the expected squared maximum distance between weights and their average, constrains the weights to be close and ensures the approximation. The affinity term in our proposed method encourages the reduction of this locality term. ", "page_idx": 16}, {"type": "text", "text": "Overall, to reduce WA\u2019s error in OOD, we need to seek a good trade-off between diversity and locality. Our solution achieves this balance through two optimizable loss terms, the diversity term, and the affinity term. Besides, the direct combination of $M$ models, as in the vanilla WAFT method, reduces variance by a factor of $M$ but introduces covariance and locality terms that need to be carefully managed in order to ensure low OOD error. ", "page_idx": 16}, {"type": "image", "img_path": "0LfgE6kvKZ/tmp/5a3285dedff7196ab86aa4702285d640cf9858d5155d9a4cf47e6fccf340f739.jpg", "img_caption": ["Figure 7: Comparison on model ensemble, model soups, and superior soups. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "It is worth noting that, from an implementation perspective, unlike the model soups method (see Fig. 7 middle), which requires retraining a large number of candidate models for model selection and interpolation, our method only selects a few models (typically 3 to 5) for sequential random interpolation training in order to maintain connectivity. This significantly reduces the time cost of local training. Furthermore, unlike model ensembles (see Fig. 7) that require storing multiple model weights and integrating predictions during inference, our method only needs to retain an averaged weight during the final inference stage. This greatly reduces the memory footprint and enhances the inference speed on the client side. ", "page_idx": 17}, {"type": "text", "text": "D More Related Work ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Heterogeneous Federated Learning ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "FL performance downgrading on Non-IID data is a critical challenge. A variety of FL algorithms have been proposed to handle this heterogeneous issue. From an optimization perspective: FedProx [28] adds $L_{2}$ norm to the client model and the previous server model to regularize them. This helps to prevent the client models from diverging too far from the server model. Scaffold [25] adds a variance reduction term to mitigate the \u201cclients-drift.\u201d MOON [27] uses mode-level contrastive learning to stabilize local training by making the client models more robust to changes in the data distribution. In addition, personalized FL [46] is another approach to achieving high local testing performance on Non-IID data. For aggregation perspective: FedBN [30] uses local batch normalization to alleviate the feature shift before averaging models. For extreme communication efficient: In recent years, there have been some FL methods based on one-shot communication rounds. These methods typically use additional techniques on the server-side, such as using prediction ensembles [14] instead of weight ensembles or generating data [56, 18] from local models for centralized training, to improve the performance of the aggregated model. These methods are orthogonal to our client training-based approach. There are also works on few-round communication rounds in FL based on meta-learning frameworks [39], but the data partition used in the experimental setup may not be suitable for practical FL scenarios. ", "page_idx": 17}, {"type": "text", "text": "D.2 Federated Fine-tuning and Model Interpolation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Fine-tuning aims to achieve improved performance on the given task by leveraging the learned knowledge of the pre-trained model. [7] empirically study the impact of fine-tuning from a pretrained model in FL and unsurprisingly find that starting from a pre-trained model reduces the training time required to reach a target error rate and enables the training of more accurate models than starting from random initialization. [57] propose a knowledge distillation approach for fine-tuning the global model, called FedFTG. In addition, fine-tuning in FL has been widely used in personalized FL to address Non-IID problems by having each user adapt the global model to personalized local models using their own data. For example, FedBABU [38] splits the model into body and head, then fine-tuning the head part for personalization. [5] propose FTFA and RTFA that start with a pre-trained model and then fine-tunes a small subset of model parameters using the FedAvg [35] algorithm. However, this line of work focuses on optimizing local performance and ignores the generalization of global data. This can lead to a performance drop when we further update the global model from the updated local models. Weight averaging and model recycling are not only efficient ways to aggregate machine learning models but also present promising benefits of improving model generalizability. Inspired by the linear mode connectivity property of neural networks trained with stochastic gradient descent (SGD) [36, 12], Model Soups [52] proposes to combine many independent runs with varied hyper-parameter configurations. Similarly, DiWA [43] utilizes this idea of Model Soups while theoretically analyzing the importance of training different models with diverse hyper-parameters within mild ranges. Soups-based methods [52, 43] rely on aggregating diverse models to improve model generalizability. To induce greater diversity, some methods such as [34] using a high constant learning rate, [51] minimizing cosine similarity between weights, [20] using a tempered posterior and model Ratatouille [42] averages diverse model trained from auxiliary datasets. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "E Experiment Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "E.1 Experimental Setup Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Dataset. We validate the effectiveness of our proposed method with four datasets, FMNIST [53], CIFAR-10 [26], Digit-5 [13, 30], and DomainNet [40]. The Fashion-MNIST (FMNIST) dataset is a dataset of Zalando\u2019s article images consisting of a training set of 60, 000 examples and a test set of 10, 000 examples. Each example is a $28\\times28$ grayscale image of a piece of clothing. The dataset is divided into 10 classes: t-shirt/top, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, and ankle boot. The CIFAR-10 dataset is a popular dataset for machine learning research. It consists of $60,000\\;32\\times32$ color images divided into 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. The dataset is split into 50, 000 training images and 10, 000 test images. The Digit-5 dataset is a collection of five popular digit datasets, MNIST [10] (55000 samples), MNIST-M (55000 samples), Synthetic Digits [13] (25000 samples), SVHN (73257 samples), and USPS (7438 samples). Each digit dataset includes a different style of 0-9 digit images. The DomainNet dataset is a large-scale dataset of images collected from six different domains: clipart, infograph, painting, quickdraw, real, and sketch. The dataset contains 600, 000 images, each labeled with one of 345 object categories. The images in the DomainNet dataset are of high quality and are diverse in terms of their content and style. ", "page_idx": 18}, {"type": "text", "text": "Model. We used the pre-trained models from the timm repo 1, which are a collection of state-of-theart deep learning models for computer vision tasks. For our proposed $L S S$ , we use Adam optimizer with a learning rate of $5\\mathrm{e-4}$ , momentum 0.9, and weight decay 5e\u22124. The default number of averaged models is 4. Each model updates 8 epoch then aggregates with the others. The default affinity term coefficient is 3 and diversity term coefficient is 3. We set the batch size to 64 by default. For vision transformer (ViT) [11] model, we adopt ViT base model with $224\\times224$ image size and $16\\times16$ input patch size. The ViT is a neural network architecture for image classification that uses a self-attention mechanism to learn the relationships between pixels in an image. ViT has been shown to achieve state-of-the-art results on a variety of image classification benchmarks, including ImageNet and CIFAR-10. ", "page_idx": 18}, {"type": "text", "text": "Training Details. We implement all the methods in PyTorch, and we run all the experiments on an NVIDIA Tesla V100 GPU. Unless otherwise specified, the model performance in the experiments below refers to the global model performance after aggregation on the server side. For commonly used FL methods, due to the significant increase in local update steps that leads to worse convergence, we set their local update steps to 8. ", "page_idx": 18}, {"type": "text", "text": "Applying WAFT to FL Local Update. For SWA [21], SWAD [1], and our method LSS, we take more local update steps, with each model being averaged trained 8 steps, and the default number of models to be averaged is 4. For the Model Soups [52] method and DiWA [43], we trained 32 models and each model trained 8 steps. The hyper-parameter configuration for model selection includes learning rate $([1{\\mathrm{e}}{-}4,5{\\mathrm{e}}{-}4,1{\\mathrm{e}}{-}5])$ , batch size ([32, 64, 128]), dropout rate ([0.0, 0.1, 0.3]), and weight decay $[5\\mathrm{e}{-}4,5\\mathrm{e}{-}5,5\\mathrm{e}{-}6].$ Each run randomly select one of the hyper-parameter options. From each run of WAFT method, we take the weights of the epoch with maximum accuracy on the validation dataset, which follows the training distribution. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "E.2 Extended Experiment Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Arbitrarily increasing local steps cannot reduce communication rounds. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "From Table 4, we can see that simply increasing local steps does not always lead to improved model performance. For FedAvg on the CIFAR10 dataset, increasing local steps beyond 8 actually results in a decrease in model performance. ", "page_idx": 19}, {"type": "text", "text": "Table 4: FedAvg with different local steps: Label shift test accuracy after $R=1$ communication rounds (CIFAR-10 with 5 Clients). ", "page_idx": 19}, {"type": "table", "img_path": "0LfgE6kvKZ/tmp/2db0674a1abc6db644c3b1873dc81b0bbb1e5820c02543f4a0d444b6c17ad0a8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Computational and memory costs comparison. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Table 5, we provide detailed information on computational overhead and memory usage for various methods. Since the computational overhead and memory usage of FedAvg and other used FL methods are nearly identical, we only present the data for FedAvg here. Similarly, as the computational overhead and memory usage for SWA and SWAD, as well as for Soups and DiWA, are also nearly the same, we only show the data for SWA and Soups methods. It can be observed that our method requires more memory compared to other soups-based methods. However, the overall computational time for a single client\u2019s communication round is faster in our approach. This is because other soups-based methods require training a large number of models repeatedly to achieve good model performance. For instance, Soups needs to train 32 models, whereas our method only requires training 4 models. If the number of models trained by Soups is reduced to just 4, it only brings about a $5\\%$ improvement compared to FedAvg with a communication round of 1. ", "page_idx": 19}, {"type": "table", "img_path": "0LfgE6kvKZ/tmp/5b93ae5b3592c4601e3696ce2dba93a250eb085fa580e7a3a7e8f6ab61929367.jpg", "table_caption": ["Table 5: Computational and memory costs of different types of method (ResNet-18). "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "LSS encourages smoothness (reducing $\\beta$ ). In Table 6, we provide the performance degradation of trained models evaluating under varying levels of random noise. Generally, a smaller performance degradation indicates a more robust model, which to some extent reflects the smoothness of the trained model. We can observe that our method exhibits greater robustness to noise perturbation. ", "page_idx": 19}, {"type": "text", "text": "Table 6: Smoothness of the trained model. Evaluated trained model performance drop on a testset with added $\\ell_{0}$ norm random noise. CIFAR-10 dataset Dirichlet distribution $\\alpha=1.0$ and $\\alpha=0.1$ : Label shift test accuracy after $R=1$ ", "page_idx": 19}, {"type": "table", "img_path": "0LfgE6kvKZ/tmp/9f26d75a23981fe473e03652ce918ca1ba6465f4bf913c1fe3f400996f990705.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "LSS improves flatness of loss landscape. The sharpness measure utilized in the Table 7 computes the median of the dominant Hessian eigenvalue across all training set batches through the Power Iteration algorithm [54]. This metric signifies the maximum curvature of the loss landscape, commonly employed in the literature on flat minima [23] to indicate sharpness. As demonstrated in the presented table, it is clear that our proposed method results in flatter minima compared to FedAvg. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "table", "img_path": "0LfgE6kvKZ/tmp/1e4cad9903416666255ffbad0e864e72e9157c40e6616bbde227486fbdf338c4.jpg", "table_caption": ["Table 7: Loss landscape flatness quantification with Hessian eigenvalue. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Evaluation with more clients. To assess the effectiveness of our method in larger-scale client scenarios, we conducted an expanded experiment involving 50 clients. From the Table 8, we can observe that our proposed method maintains a significant advantage across different client scales, particularly when the number of communication rounds is small $\\;R=1\\;\\;$ ). ", "page_idx": 20}, {"type": "table", "img_path": "0LfgE6kvKZ/tmp/af66dbf9da4992d38658f911142acededa05f75f519b2d215d44583f6dd2049f.jpg", "table_caption": ["Table 8: Different client numbers (5 Clients and 50 Clients): Label shift test accuracy after $R=1$ and $R=3$ communication rounds. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "0LfgE6kvKZ/tmp/12d78661fe20c28efbe6f325b0614d3b6dc0644bd88fa21a267d2e7802410911.jpg", "table_caption": ["Table 9: Different Network Architecture (ResNet-18 and ViT): Label shift test accuracy after $R=1$ and $R=3$ communication rounds. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Evaluation with ViT. To validate the effectiveness of our method across different network architectures, we conducted an expanded experiment using the Vision Transformer (ViT) model based on the Transformer architecture. Upon observing the Table 9, it is evident that our method consistently enhances the communication efficiency of federated learning with ViT model architectures. ", "page_idx": 20}, {"type": "text", "text": "Evaluation with different Non-IID level. To further comprehensively validate the effectiveness of our method under different levels of data heterogeneity, we conducted experiments on the CIFAR-10 dataset by adjusting the coefficients $\\alpha$ of the Dirichlet distribution. We examined the performance of our method in scenarios with greater distribution variations. Based on the Table 10, it is evident that our method maintains a significant advantage in scenarios with larger data heterogeneity. ", "page_idx": 20}, {"type": "text", "text": "Evaluation with different Initialized Models. To compare the performance of our method under different types of parameter initialization, we conducted experiments on the CIFAR-10 dataset using both pretrained and random initialization. Table 11 shows that our method still maintains a significant advantage with random initialization, but it does not achieve the near-optimal performance seen with pre-trained initialization. ", "page_idx": 20}, {"type": "image", "img_path": "0LfgE6kvKZ/tmp/a1ee449b5a0b80992e8cbf2bcc5cf442817a98e4cc10805753b970b4ca6bfc85.jpg", "img_caption": ["Figure 8: FedAya Evaluation Comparison with FedAvg and LSS. Our method, LSS, when applied to large language models for instruction tuning, achieves higher scores than the common FedAvg. This suggests that LSS is a promising approach for improving performance and convergence in federated learning settings for large language models, in addition to its success in image classification. Exploring the use of our method in a diverse set of complex LLM tasks is an interesting direction for future research. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "table", "img_path": "0LfgE6kvKZ/tmp/4ee4c6772e723790e3fd3b9590afabbc5f7ada6c12075257ba6f277fa0e0645f.jpg", "table_caption": ["Table 10: Different Non-IID level (Dirichlet distribution $\\alpha=1.0$ and $\\alpha=0.1)$ ): Label shift test accuracy after $R=1$ and $R=3$ communication rounds. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Comparison of Convergence Speed Between FedProx and LSS. Fig. 9 shows the accuracy of the testing during the early and late phases in terms of the number of communication rounds required to reach convergence. These results demonstrate that our method outperforms FedProx in both the early and late phases of federated learning. ", "page_idx": 21}, {"type": "text", "text": "Evaluation of Large Language Models for Multilingual Instruction Tuning ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Setup. We follow the setup of Fed-Aya [55], which involves four iterative steps: server-to-client model downloading, local model training, client-to-server model uploading, and global model aggregation. For instruction tuning, we use the parameter-efficient fine-tuning technique, LoRA [19], applied to the Llama-7b model. ", "page_idx": 21}, {"type": "text", "text": "Dataset. We use the Aya dataset [44], a multilingual instruction tuning dataset with annotations from contributors worldwide. Our experiments include 6 high-resource languages (English, Spanish, French, Russian, Portuguese, Chinese) and 2 low-resource languages (standard Arabic, Telugu). The dataset is filtered to include contributors with at least 100 annotations, resulting in 38 clients with a total of $25\\mathrm{k}$ data samples. ", "page_idx": 21}, {"type": "text", "text": "Model. The model used for our experiments is the Llama2-7b [48], fine-tuned using the LoRA technique. We evaluate the effectiveness of the training methods using an in-domain evaluation metric termed Ref-GPT4 [60], where GPT-4o rates the generated responses against ground-truth responses. The score given by GPT-Ref ranges from 0 to 10. We adopt the same prompt template used in FedLLM-Bench [55]. The implementation of applying our method to LoRA is the same as that used in the ViT experiments (see Fig. 4) described earlier. ", "page_idx": 21}, {"type": "text", "text": "Result. Our method, LSS, when applied to large language models for instruction tuning, achieves higher scores than the common FedAvg. This suggests that LSS is a promising approach for improving performance and convergence in federated learning settings for large language models, in addition to its success in image classification. Exploring the use of our method in a diverse set of complex LLM tasks is an interesting direction for future research. ", "page_idx": 21}, {"type": "table", "img_path": "0LfgE6kvKZ/tmp/5b982fccbead2c070d7598b409ad7dae149a71941f03a94453b334dc57949df6.jpg", "table_caption": ["Table 11: Different model initialization (Pre-trained v.s. Random): Label shift test accuracy after $R\\,=\\,1$ and $R\\,=\\,3$ communication rounds. Result: It shows that our method still maintains a significant advantage with random initialization, but it does not achieve the near-optimal performance seen with pre-trained initialization. "], "table_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "0LfgE6kvKZ/tmp/5e9eaf577b7fad7d5c9a7a0e315d54871e18692863b95bd536266b1dd22b57bf.jpg", "img_caption": ["Figure 9: Convergence comparison of our proposed LSS with FedProx. $L S S$ also achieves high accuracy much earlier (around 6 to 8 rounds) than FedProx, which takes hundreds of communication rounds. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. The concepts of federated learning settings are clearly introduced and defined in Sec. 1. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The paper acknowledges several limitations and areas for future research in Sec. 5. Firstly, it highlights a trade-off between training memory and performance when reducing communication rounds in model merging. This indicates an awareness of the potential drawbacks of their method and suggests that further work is needed to make the method more training-memory efficient. Additionally, the paper notes that it focuses exclusively on vision-related tasks, suggesting that extending the findings to language tasks or multimodal scenarios would be a promising direction for future research. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 22}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, the paper meticulously outlines all relevant assumptions and presents comprehensive, logically sound proofs, ensuring the validity and reliability of each theoretical result. These elements are included in Sec. 3 and Appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper fully discloses all the information needed to reproduce the main experimental results. This information is provided in Sec. 4, with additional detailed information available in the appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes, the paper specifies all the training and test details necessary to understand the results, including data splits, hyperparameters, their selection process, and the type of optimizer used. This comprehensive detailing ensures that the experimental setup and outcomes are transparent and reproducible in Section 4. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper specifies all the training and test details necessary to understand the results, including data splits, hyperparameters, their selection process, and the type of optimizer used. These details are comprehensively documented in Sec. 4 to ensure clarity and reproducibility. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, the paper reports error bars and other appropriate information about the statistical significance of the experiments in Sec. 4. These details are suitably and correctly defined to ensure the reliability and validity of the reported results. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, the paper provides sufficient information on the computer resources needed to reproduce the experiments in Sec. 4. This includes details on the type of compute workers, memory requirements, and execution time, ensuring that others can accurately replicate the experimental setup and results. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, the research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. The paper adheres to the guidelines on responsible release and publication strategy, ensuring that all necessary safeguards are in place for controlled use of the model (see Sec. 4). ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: es, the paper discusses both potential positive and negative societal impacts of the work performed. The discussion includes an analysis of how the research can benefit society and also addresses possible adverse effects, ensuring a balanced and comprehensive evaluation of the societal implications in Sec. 5. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not describe safeguards for the responsible release of data or models that have a high risk for misuse, such as pretrained language models, image generators, or scraped datasets. Because we does not focus the release of such assets, we focus model training techniques. (see Sec. 4). ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not utilize external assets such as code, data, or models from other creators or original owners. Therefore, there are no specific credits, licenses, or terms of use that need to be mentioned or respected. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not introduce any new assets such as code, data, or models.   \nTherefore, there is no documentation provided alongside new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing experiments or research with human subjects. Therefore, it does not include instructions given to participants, screenshots, or details about compensation. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve any study participants, crowdsourcing experiments, or research with human subjects. Therefore, it does not describe potential risks incurred by study participants, disclose such risks to subjects, or obtain Institutional Review Board (IRB) approvals. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]