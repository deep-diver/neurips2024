{"importance": "This paper is crucial for researchers in federated learning, particularly those working with large pre-trained models.  It offers a novel, efficient solution to reduce communication costs, a major bottleneck in FL. The proposed method, combined with the theoretical analysis, opens up new avenues for improving the scalability and efficiency of FL across diverse datasets and settings. This work significantly contributes to the advancement of FL, making it more practical for real-world applications. ", "summary": "Local Superior Soups (LSS) significantly accelerates federated learning by efficiently merging pre-trained models, drastically cutting communication rounds without sacrificing accuracy.", "takeaways": ["Local Superior Soups (LSS) dramatically reduces communication rounds in federated learning by using a novel model interpolation technique.", "LSS addresses the communication cost challenges associated with large pre-trained models in federated learning.", "Theoretical analysis supports LSS's efficiency and effectiveness across various datasets and non-IID data distributions."], "tldr": "Federated learning (FL) faces significant communication overhead, especially with large pre-trained models.  Existing techniques struggle to balance local training steps and communication efficiency, leading to slow convergence and suboptimal performance due to client drift.  The main challenge lies in finding a balance between local training optimization on heterogeneous data and effective model merging across distributed clients, often leading to isolated low-loss valleys instead of a connected low-loss basin. \nThe proposed solution, Local Superior Soups (LSS), employs a regularized model interpolation technique during local training. This approach uses two novel metrics, **diversity and affinity**, to guide model selection and interpolation. Diversity promotes exploration of a large connected low-loss basin, while affinity keeps model updates close to the initial pre-trained model, preventing isolated valleys. This efficient local training method ensures seamless and fast model merging with fewer communication rounds, resulting in faster convergence and improved global model performance.  Results show that LSS achieves the accuracy of models trained with multiple communication rounds, with only a few communication rounds. ", "affiliation": "University of British Columbia", "categories": {"main_category": "Machine Learning", "sub_category": "Federated Learning"}, "podcast_path": "0LfgE6kvKZ/podcast.wav"}