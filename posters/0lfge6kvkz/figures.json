[{"figure_path": "0LfgE6kvKZ/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration on isolated (left) and connected low-loss valley with larger regions in dark red (right).", "description": "This figure illustrates the concept of isolated vs. connected low-loss valleys in the context of model training.  On the left, two models (from clients A and B) are trained independently and end up in separate low-loss regions. The resulting global model (an average of the two client models) falls into a high-loss region because the individual models' low-loss valleys are isolated. On the right, training is modified to allow the models to explore overlapping low-loss regions. This results in a connected low-loss region and a global model that performs well, as its parameters reside centrally within this connected region. This highlights the importance of encouraging exploration of connected low-loss regions for better global model performance in federated learning.", "section": "3.2 Effect of Regularization and Local Steps on FL Convergence under Data Heterogeneity"}, {"figure_path": "0LfgE6kvKZ/figures/figures_1_2.jpg", "caption": "Figure 2: Illustration on diversity (left) and affinity (right) regularization.", "description": "This figure illustrates the concepts of diversity and affinity regularization used in the Local Superior Soups (LSS) method.  The left panel shows how high diversity among models (larger pairwise distances) leads to a larger covered low-loss region, meaning fewer models are needed to span the region. Conversely, low diversity (models clustered together) results in a smaller covered region. The right panel shows how high affinity (models closer to the initial model) leads to larger overlapping regions between low-loss areas of different clients, while low affinity (models far from the initial model) results in smaller overlapping regions.  These visualizations explain how the diversity and affinity metrics guide the model selection and interpolation process within LSS to efficiently explore connected low-loss regions during local training.", "section": "3.3 Our Solution: LSS Algorithm"}, {"figure_path": "0LfgE6kvKZ/figures/figures_7_1.jpg", "caption": "Figure 3: Convergence comparison of our proposed LSS with FedAvg. LSS achieves high accuracy much earlier (around 6 to 8 rounds) than FedAvg, which takes hundreds of communication rounds.", "description": "This figure compares the convergence speed of the proposed Local Superior Soups (LSS) method and the standard FedAvg method. The plots show the training accuracy over communication rounds for both methods on CIFAR-10 and Digit-5 datasets, with separate plots for early and late phases of training.  LSS consistently reaches high accuracy within a small number of communication rounds (6-8), significantly outperforming FedAvg, which requires hundreds of rounds to converge.", "section": "4.2 Performance Comparison"}, {"figure_path": "0LfgE6kvKZ/figures/figures_7_2.jpg", "caption": "Figure 3: Convergence comparison of our proposed LSS with FedAvg. LSS achieves high accuracy much earlier (around 6 to 8 rounds) than FedAvg, which takes hundreds of communication rounds.", "description": "This figure compares the convergence speed of the proposed Local Superior Soups (LSS) method and the standard Federated Averaging (FedAvg) method.  The plots show the accuracy achieved over communication rounds for both methods on four different datasets, namely CIFAR-10 early phase, CIFAR-10 late phase, Digit-5 early phase, and Digit-5 late phase.  The results clearly demonstrate that LSS achieves significantly higher accuracy in a substantially smaller number of communication rounds compared to FedAvg, showcasing the efficiency of the proposed method.", "section": "4.2 Performance Comparison"}, {"figure_path": "0LfgE6kvKZ/figures/figures_8_1.jpg", "caption": "Figure 5: Ablation on the affinity & diversity.", "description": "This figure shows the ablation study on the affinity and diversity terms in the proposed LSS method. The left subplot (a) shows the effect of varying the affinity coefficient while keeping the diversity coefficient fixed at 0 and 3. The right subplot (b) shows the effect of varying the diversity coefficient while keeping the affinity coefficient fixed at 0 and 3. The results demonstrate the importance of both terms in achieving optimal performance and also highlight the complementary nature of affinity and diversity.", "section": "3.3 Our Solution: LSS Algorithm"}, {"figure_path": "0LfgE6kvKZ/figures/figures_8_2.jpg", "caption": "Figure 6: Ablation studies on the impact of the number of averaged models on communication efficiency and performance variance. We evaluated the influence of varied model quantities on global and averaged local model performance, as well as generalization on the worst client.", "description": "This figure presents the results of ablation studies conducted to investigate the impact of the number of averaged models on various performance metrics. The figure displays three subplots: (a) shows the after-aggregation global performance, (b) shows the before-aggregation global performance, and (c) shows the before-aggregation worst performance. Each subplot illustrates how the performance metric changes as the number of averaged models increases from 2 to 5. This analysis helps understand the optimal number of models for balancing communication efficiency and performance variance in federated learning.", "section": "4.3 Ablation Studies"}, {"figure_path": "0LfgE6kvKZ/figures/figures_17_1.jpg", "caption": "Figure 7: Comparison on model ensemble, model soups, and superior soups.", "description": "This figure illustrates the differences between three model aggregation methods: Model Ensemble, Model Soups, and Superior Soups.  Model Ensemble shows multiple individual models, implying that many models are independently trained and then combined. Model Soups depicts a large pot with numerous ingredients, representing the training of many models, where the final model is obtained from averaging the weights of many models. Superior Soups refines this further by showing a selection of 'Curated Ingredients', indicating that only a subset of carefully selected models are averaged, leading to a single, final model.  This highlights the efficiency gain of Superior Soups, which avoids the redundancy of training many less effective models.", "section": "3.3 Our Solution: LSS Algorithm"}, {"figure_path": "0LfgE6kvKZ/figures/figures_20_1.jpg", "caption": "Figure 8: FedAya Evaluation Comparison with FedAvg and LSS. Our method, LSS, when applied to large language models for instruction tuning, achieves higher scores than the common FedAvg. This suggests that LSS is a promising approach for improving performance and convergence in federated learning settings for large language models, in addition to its success in image classification. Exploring the use of our method in a diverse set of complex LLM tasks is an interesting direction for future research.", "description": "This figure compares the performance of FedAvg and LSS on a multilingual instruction tuning task using the Llama2-7b language model.  The y-axis represents performance (%), and the x-axis shows different methods and numbers of communication rounds (R=1 and R=3). The results demonstrate that LSS consistently outperforms FedAvg, achieving higher scores with fewer communication rounds, suggesting its effectiveness in federated learning for LLMs.", "section": "4.2 Performance Comparison"}, {"figure_path": "0LfgE6kvKZ/figures/figures_21_1.jpg", "caption": "Figure 3: Convergence comparison of our proposed LSS with FedAvg. LSS achieves high accuracy much earlier (around 6 to 8 rounds) than FedAvg, which takes hundreds of communication rounds.", "description": "This figure compares the convergence speed of the proposed Local Superior Soups (LSS) method and the standard Federated Averaging (FedAvg) method on four different scenarios: CIFAR-10 early phase, CIFAR-10 late phase, Digit-5 early phase, and Digit-5 late phase.  The x-axis represents the number of communication rounds, and the y-axis represents the accuracy.  The plots demonstrate that LSS achieves significantly higher accuracy with far fewer communication rounds than FedAvg. This highlights the efficiency of LSS in reducing communication overhead during federated learning.", "section": "4.2 Performance Comparison"}]