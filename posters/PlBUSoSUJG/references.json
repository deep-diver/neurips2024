{"references": [{"fullname_first_author": "A. Agarwal", "paper_title": "On the theory of policy gradient methods: Optimality, approximation, and distribution shift", "publication_date": "2021-MM-DD", "reason": "This paper provides a theoretical foundation for policy gradient methods, which is highly relevant to the core topic of the current paper."}, {"fullname_first_author": "M. G. Bellemare", "paper_title": "The arcade learning environment: An evaluation platform for general agents", "publication_date": "2013-MM-DD", "reason": "This paper introduces the Arcade Learning Environment (ALE), a benchmark widely used in reinforcement learning research and directly relevant to the experimental evaluation in the current paper."}, {"fullname_first_author": "S. Bhatnagar", "paper_title": "Natural actor-critic algorithms", "publication_date": "2009-MM-DD", "reason": "This paper presents natural actor-critic algorithms, which are closely related to the policy gradient methods discussed in the current paper."}, {"fullname_first_author": "J. Mei", "paper_title": "On the global convergence rates of softmax policy gradient methods", "publication_date": "2020-MM-DD", "reason": "This paper analyzes the convergence rates of softmax policy gradient methods, which are directly related to the analysis of SoftTreeMax in the current paper."}, {"fullname_first_author": "R. S. Sutton", "paper_title": "Policy gradient methods for reinforcement learning with function approximation", "publication_date": "1999-MM-DD", "reason": "This seminal paper introduces policy gradient methods, which form the basis for the current paper's approach."}]}