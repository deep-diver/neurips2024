{"references": [{"fullname_first_author": "J. Ainslie", "paper_title": "GQA: Training generalized multi-query transformer models from multi-head checkpoints", "publication_date": "2023-12-01", "reason": "This paper introduces Grouped-Query Attention (GQA), a generalization of Multi-Query Attention (MQA), which is a key concept related to the paper's core contribution, Cross-Layer Attention (CLA)."}, {"fullname_first_author": "N. Shazeer", "paper_title": "Fast transformer decoding: One write-head is all you need", "publication_date": "2019-05-02", "reason": "This paper introduces Multi-Query Attention (MQA), a significant precursor to CLA, which shares core concepts and provides a foundational approach for reducing the KV cache size."}, {"fullname_first_author": "Z. Dai", "paper_title": "Transformer-XL: Attentive language models beyond a fixed-length context", "publication_date": "2019-07-01", "reason": "This paper introduced Transformer-XL, a method for handling longer sequences in transformers, directly addressing a limitation of traditional transformers that CLA aims to improve."}, {"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This foundational paper introduced the original transformer architecture, which CLA modifies to improve efficiency and memory usage; therefore, understanding this architecture is crucial to understanding CLA."}, {"fullname_first_author": "A. Chowdhery", "paper_title": "Palm: Scaling language modeling with pathways", "publication_date": "2022-04-01", "reason": "This paper discusses memory limitations in large language models, which is the core problem that CLA aims to solve; therefore, it provides important background and context for this work."}]}