[{"type": "text", "text": "Low-Rank Optimal Transport through Factor Relaxation with Latent Coupling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Peter Halmos1,\\*, Xinhao Liu1,\\*, Julian $\\mathbf{Gold}^{2,*}$ , and Benjamin J. Raphael1 ", "page_idx": 0}, {"type": "text", "text": "1Department of Computer Science, Princeton University 2Center for Statistics and Machine Learning, Princeton University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Optimal transport (OT) is a general framework for finding a minimum-cost transport plan, or coupling, between probability distributions, and has many applications in machine learning. A key challenge in applying OT to massive datasets is the quadratic scaling of the coupling matrix with the size of the dataset. Forrow et al. (2019) introduced a factored coupling for the $k$ -Wasserstein barycenter problem, which Scetbon et al. (2021) adapted to solve the primal low-rank OT problem. We derive an alternative parameterization of the low-rank problem based on the latent coupling (LC) factorization previously introduced by Lin et al. (2021) generalizing Forrow et al. (2019). The LC factorization has multiple advantages for low-rank OT including decoupling the problem into three OT problems and greater flexibility and interpretability. We leverage these advantages to derive a new algorithm Factor Relaxation with Latent Coupling (FRLC), which uses coordinate mirror descent to compute the LC factorization. FRLC handles multiple OT objectives (Wasserstein, Gromov-Wasserstein, Fused Gromov-Wasserstein), and marginal constraints (balanced, unbalanced, and semi-relaxed) with linear space complexity. We provide theoretical results on FRLC, and demonstrate superior performance on diverse applications \u2013 including graph clustering and spatial transcriptomics \u2013 while demonstrating its interpretability. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Optimal transport (OT) is a powerful geometric framework for comparing probability distributions. OT problems seek a transport plan $_{P}$ efficiently transforming one distribution $(a)$ into another $(b)$ , subject to a ground cost $_{C}$ . The minimum cost yields a distance between $\\textbf{\\em a}$ and $^{b}$ , while the optimal transport plan reveals key structural similarities between the distributions. Owing to its versatility \u2013 different ground costs result in different ways to compare data \u2013 OT has found many applications in machine learning and beyond: from self-attention Tay et al. (2020); Sander et al. (2022); Geshkovski et al. (2023) and domain adaptation Courty et al. (2014); Solomon et al. (2015) to computational biology Schiebinger et al. (2019); Yang et al. (2020); Bunne et al. (2023); Liu et al. (2023). ", "page_idx": 0}, {"type": "text", "text": "This versatility is compounded by several variants using different forms of the objective function and/or constraints on the transport plan $_{P}$ . Wasserstein (W) OT Kantorovich (1942) compares distributions over the same space through the expected work of $_{P}$ , while Gromov-Wasserstein (GW) OT M\u00e9moli (2011) compares distributions supported on distinct geometries through the expected metric distortion of $_{P}$ . Fused Gromov-Wasserstein (FGW) Vayer et al. (2020) OT is suited to structured data, taking a convex combination of the former two objectives. Independently, one can relax constraints on the marginals of $_{P}$ : in computational applications, $_{P}$ is a matrix whose row-sum ${\\cal P}\\mathbf{1}_{m}$ and column-sum $P^{\\mathrm{T}}\\bar{\\mathbf{1}}_{n}$ are called its left and right marginals. Balanced OT requires ${\\cal P}{\\bf1}_{m}={\\bf a}$ and $P^{\\mathrm{T}}\\mathbf{1}_{n}=b$ . Unbalanced OT Frogner et al. (2015) replaces these constraints with penalties in the transport cost, and is more robust to outliers. Semi-relaxed OT can be used to understand how one dataset embeds into another by imposing one hard constraint on either the left or right marginal, used for feature transfer Dong et al. (2023), and alignment of spatiotemporal data Halmos et al. (2024). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "An important consideration in applying OT is the quadratic space of the transport plan. To address both the quadratic complexity and to provide robustness under sampling noise, Forrow et al. (2019) introduced another variant of OT, optimizing a $k$ -Wasserstein Barycenter proxy for the rank-constrained Wasserstein objective. Their approach factors the transport plan through a small set of anchor points called hubs. Generalizing this approach, Scetbon et al. (2021) introduce the factorization $\\mathbf{\\dot{\\calP}}=Q\\operatorname{diag}{(\\underline{{{1}}}/\\mathbf{g})}R^{\\mathrm{T}}$ comprised of sub-coupling matrices $Q$ and $\\boldsymbol{R}$ sharing an inner marginal $\\textbf{\\textit{g}}$ , meaning $\\overline{{Q^{\\mathrm{T}}\\bar{\\mathbf{1}_{n}}}}\\,=\\,R^{\\mathrm{T}}\\bar{\\mathbf{1}_{m}}\\,=\\,g$ . Building on this, Scetbon et al. (2021, 2022, 2023) derived algorithms to compute low-rank optimal transport plans for the primal OT problem with general costs, extending low-rank OT to GW and unbalanced problems using factored couplings. ", "page_idx": 1}, {"type": "text", "text": "Interestingly, a different factorization of $_{P}$ was proposed by Lin et al. (2021) in the context of $k$ -Wasserstein barycenters. We call their factorization a latent coupling (LC) factorization, given by $P=Q\\mathrm{diag}(1/g_{Q})T\\mathrm{diag}(1/g_{R})R^{\\mathrm{T}}$ , with $t w o$ inner marginals $\\bar{\\pmb{g}_{Q}}=\\pmb{Q}^{\\mathrm{T}}\\mathbf{1}_{n}$ and ${\\pmb g}_{R}=R^{\\tilde{\\mathrm{T}}}{\\bf1}_{n}$ and a general coupling $\\textbf{\\emph{T}}$ . Lin et al. (2021) constrain the transport between $\\textbf{\\em a}$ and $^{b}$ through two sets of learned anchor points, where the factorization is defined by three transport plans computed from three cost matrices between the points and their anchors. This objective differs from that of Forrow et al. (2019); Scetbon et al. (2021), who seek a minimal rank coupling with respect to a single, fixed cost $_{C}$ . We observe that factored couplings of Forrow et al. (2019) correspond to LC factorizations with diagonal $\\textbf{\\emph{T}}$ , suggesting the LC factorization of Lin et al. (2021) may provide an alternative parameterization of transport plans for the low-rank OT problem considered in Forrow et al. (2019); Scetbon et al. (2021). To our knowledge, this idea has not yet been explored. ", "page_idx": 1}, {"type": "text", "text": "Contributions. We present a new algorithm, Factor Relaxation with Latent Coupling (FRLC, with the informal mnemonic \u201cfrolic\u201d), to compute a minimum cost low-rank transport plan using the LC factorization. Parameterizing low-rank transport plans with the LC factorization has a number of advantages. First, optimization of the low-rank OT objective decouples into three OT sub-problems on the LC factors $Q,R,T$ , leading to a simpler optimization algorithm. Second, this decoupling provides straightfoward extensions of FRLC to low-rank unbalanced and semi-relaxed OT; similar extensions for factored couplings required additional work Scetbon et al. (2023) beyond the balanced case. Third, the latent coupling $\\textbf{\\emph{T}}$ in the LC factorization provides additional flexibility to model transport between datasets with different numbers of clusters, and to model mass-splitting between these clusters, providing a high-level and interpretable description of $_{P}$ that differs from the factored couplings of Forrow et al. (2019). FRLC computes the LC factorization using a novel coordinate mirror descent scheme, alternating descent steps on variables $(Q,R)$ and $_T$ , inspired by the mirror descent approach of Scetbon et al. (2021). We call the descent step on $(Q,R)$ factor relaxation, as the factors $Q$ and $\\boldsymbol{R}$ have relaxed inner marginals, allowing FRLC to be solved by OT sub-problems. FRLC handles multiple OT objectives (Wasserstein, Gromov-Wasserstein, Fused Gromov-Wasserstein), and marginal constraints (balanced, unbalanced, and semi-relaxed). We show FRLC performs better than existing state-of-the-art low-rank methods on a range of synthetic and real datasets, retaining the interpretability of Lin et al. (2021), and inheriting the broad applicability of Scetbon et al. (2021); Scetbon & Cuturi (2022); Scetbon et al. (2022, 2023). ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Wasserstein OT. Let $\\{x_{1},\\ldots,x_{n}\\}$ and $\\{y_{1},\\ldots,y_{m}\\}$ be datasets in a metric space $\\mathcal{X}$ , and let $\\Delta_{d}$ be the probability simplex of size $d$ . Through probability vectors $\\pmb{a}\\in\\Delta_{n}$ and $b\\in\\Delta_{m}$ , each dataset is encoded as a probability measure: $\\textstyle\\mu={\\bar{\\sum}}_{i=1}^{n^{\\star}}\\,\\mathbf{a}_{i}\\delta_{x_{i}}$ and $\\begin{array}{r}{\\nu=\\sum_{j=1}^{m}b_{j}\\delta_{y_{j}}}\\end{array}$ . Let ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Pi_{a,\\mathbf{\\lambda}}:=\\{P\\in\\mathbb{R}_{+}^{n\\times m}:P\\mathbf{1}_{m}=a\\},\\ \\Pi_{\\mathbf{\\lambda},b}:=\\{P\\in\\mathbb{R}_{+}^{n\\times m}:P^{\\Pi}\\mathbf{1}_{n}=b\\},\\ \\Pi_{a,b}:=\\Pi_{a,\\mathbf{\\lambda}}\\cap\\Pi_{\\mathbf{\\lambda},b}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Thus, $\\Pi_{a,b}$ is the set of transport plans (probabilistic coupling matrices) with marginals $\\textbf{\\em a}$ and $^{b}$ . Given a cost function $c:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}_{+}$ , define the cost matrix $C\\in\\mathbb{R}_{+}^{n\\times m}$ via $C_{i j}=c(x_{i},y_{j})$ . The Kantorovich formulation Kantorovich (1942) of discrete OT, also called the Wasserstein problem, seeks a transport plan $_{P}$ of minimal cost : ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname{W}(\\mu,\\nu):=\\operatorname*{min}_{P\\in\\Pi_{a,b}}\\langle C,P\\rangle_{F}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Gromov-Wasserstein OT. In many applications, one wishes to compare datasets $\\{x_{1},\\ldots,x_{n}\\}\\subset$ $\\mathcal{X}$ and $\\left\\{y_{1},\\ldots,y_{m}\\right\\}\\subset\\mathcal{Y}$ across distinct metric spaces $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ . The Gromov-Wasserstein (GW) objective M\u00e9moli (2007, 2011) addresses the absence of a common metric or coordinate system through intra-domain cost functions $c_{1}\\,:\\,\\mathcal{X}\\,\\times\\,\\mathcal{X}\\,\\to\\,\\mathbb{R}_{+}$ and $c_{2}\\,:\\,\\mathcal{V}\\,\\times\\,\\mathcal{V}\\,\\to\\,\\mathbb{R}_{+}$ , leading to intra-domain cost matrices $A_{i k}\\,=\\,c_{1}(x_{i},x_{k})$ and $B_{j l}\\,=\\,c_{2}(y_{j},y_{l})$ . The GW objective function $\\begin{array}{r}{\\mathcal{Q}_{A,B}(P):=\\sum_{i,j,k,l}(A_{i k}\\!-\\!B_{j l})^{2}P_{i j}P_{k l}}\\end{array}$ quantifies the expected metric distortion under $_{P}$ , leading to the optimization problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{GW}(\\mu,\\nu):=\\operatorname*{min}_{P\\in\\Pi_{a,b}}\\mathcal{Q}_{A,B}(P).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The Fused Gromov-Wasserstein (FGW) objective function Vayer et al. (2020) is a convex combination of the W and GW objectives, given as $\\alpha\\langle\\pmb{C},\\pmb{P}\\rangle_{F}+(1-\\alpha)\\mathcal{Q}_{A,B}(\\pmb{P})$ , for hyperparameter $\\alpha\\in(0,1)$ . ", "page_idx": 2}, {"type": "text", "text": "Relaxed marginal constraints. Balanced OT (1) constrains $_{P}$ to lie in $\\Pi_{a,b}$ . Unbalanced OT relaxes constraints $P\\mathbf{1}_{m}\\,=\\,\\mathbf{a}$ and $P^{\\mathrm{T}}\\mathbf{1}_{n}\\,=\\,b$ , replacing them with penalties in the form of KL divergences (or other divergences, see Chizat et al. (2018)): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{U-W}(\\mu,\\nu):=\\operatorname*{min}_{{P\\in\\mathbb{R}_{+}^{n\\times m}}}\\langle{C,P}\\rangle_{F}+\\tau_{L}\\mathrm{KL}({P\\mathbf{1}}_{m}\\|{\\boldsymbol{a}})+\\tau_{R}\\mathrm{KL}({P^{\\mathrm{T}}\\mathbf{1}}_{n}\\|{\\boldsymbol{b}}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\tau_{L},\\tau_{R}>0$ control the strength of each penalty. Semi-relaxed optimal transport relaxes exactly one of the hard constraints $P\\mathbf{1}_{m}=\\pmb{a}$ and $P^{\\mathrm{T}}\\mathbf{1}_{n}=\\mathbf{\\dot{\\boldsymbol{b}}}$ in the same manner. The semi-relaxed version of (1) obtained by relaxing only the \u201cright\u201d marginal constraint on $^{b}$ is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname{SR}^{\\mathrm{R}}.\\operatorname{W}(\\mu,\\nu):=\\operatorname*{min}_{P\\in\\Pi_{a,\\cdot}}\\langle C,P\\rangle+\\tau\\mathrm{KL}(P^{\\mathrm{T}}\\mathbf{1}_{n}\\|b),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "while it\u2019s \u201cleft\u201d marginal counterpart $\\mathrm{SR}^{\\mathrm{L}}{-}\\mathrm{W}(\\mu,\\nu)$ is defined analogously over $P\\in\\Pi_{\\cdot,b}$ , using penalty $\\tau\\mathrm{KL}(P\\mathbf{1}_{m}\\|\\bar{\\mathbf{a}})$ . Likewise, one can form semi-relaxed or unbalanced GW and FGW problems. ", "page_idx": 2}, {"type": "text", "text": "Entropy regularization. The seminal work Cuturi (2013b) introduced the Sinkhorn algorithm to solve an entropy regularized version of (1), $\\mathrm{W}_{\\epsilon}(\\mu,\\nu):=\\operatorname*{min}_{P\\in\\Pi_{a,b}}\\langle C,P\\rangle_{F}-\\epsilon H(P)$ , massively improving the $O(n^{3}\\log{n})$ time complexity of classical techniques Orlin (1997); Tarjan (1997). Above, $H$ is the entropy, $\\begin{array}{r}{\\dot{H}(P)=-\\sum_{i j}\\dot{P_{i j}}(\\log P_{i j}-1)}\\end{array}$ , and $\\epsilon>0$ is the regularization strength. ", "page_idx": 2}, {"type": "text", "text": "Low-rank regularization. The nonnegative rank $\\mathrm{rk}_{+}(M)$ of matrix $_M$ is the least number of nonnegative rank-one matrices summing to $_M$ . For $r\\geq1$ , define ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Pi_{a,\\cdot}(r)=\\{P\\in\\Pi_{a,\\cdot}:\\mathrm{rk}_{+}(P)\\leq r\\},\\quad\\Pi_{\\cdot,b}(r)=\\{P\\in\\Pi_{\\cdot,b}:\\mathrm{rk}_{+}(P)\\leq r\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and let $\\Pi_{\\pmb{a},\\pmb{b}}(r)=\\Pi_{\\pmb{a},\\cdot}(r)\\cap\\Pi_{\\cdot,\\pmb{b}}(r)$ . To estimate Wasserstein distances with greater stability and accuracy under sampling noise, Forrow et al. (2019) proposed a low-rank regularization on the coupling matrix, factoring the transport through a small set of anchor points. More explicitly, Scetbon et al. (2021) parameterized the set as $\\Pi_{a,b}(r)$ through the set $\\mathsf{F C}_{a,b}(r)$ of factored couplings, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathsf{F}\\mathsf{C}_{a,b}(r):=\\{(Q,\\pmb{R},\\pmb{g})\\in\\mathbb{R}_{+}^{n\\times r}\\times\\mathbb{R}_{+}^{m\\times r}\\times(\\mathbb{R}_{+}^{*})^{r}:Q\\in\\Pi_{a,\\pmb{g}},\\pmb{R}\\in\\Pi_{b,\\pmb{g}}\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The set $\\mathsf{F C}_{a,b}(r)$ parameterizes $\\Pi_{a,b}(r)$ through $(Q,R,g)\\mapsto Q\\mathrm{diag}(1/g)R^{\\mathrm{T}}$ , as shown by Cohen & Rothblum (1993). ", "page_idx": 2}, {"type": "text", "text": "Scetbon et al. (2021) apply this factorization to solve the Wasserstein problem subject to $P\\in\\Pi_{a,b}(r)$ for general cost matrices: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{W}_{r}(\\mu,\\nu):=\\operatorname*{min}_{P\\in\\Pi_{a,b}(r)}\\langle C,P\\rangle_{F}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "GW, unbalanced and semi-relaxed low-rank OT problems are defined as in (2), (3) and (4), replacing $\\mathbb{R}_{+}^{n\\times m}$ , $\\Pi_{a,\\cdot}$ , or $\\Pi_{\\cdot,b}$ with rank-constrained counterparts (5). Scetbon & Cuturi (2022); Scetbon et al. (2022, 2023) developed a robust framework for solving all of these problems. ", "page_idx": 2}, {"type": "text", "text": "3 Factor Relaxation with Latent Coupling (FRLC) algorithm ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Latent Coupling Factorization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We parameterize low-rank coupling matrices $P\\in\\Pi_{a,b}(r)$ using a factorization introduced in Lin et al. (2021), which we call the latent coupling $(L C)$ factorization (Fig. 1). The key property of this factorization is the presence of a coupling matrix $\\textbf{\\emph{T}}$ linking two distinct inner marginals. For simplicity we describe this factorization using an $r$ -dimensional latent space, but we also extend to non-square matrices linking two latent spaces of different dimensions, as demonstrated in the results. Definition 3.1 (Inner marginals). Given a factorization $\\boldsymbol{P}=\\boldsymbol{Q}\\boldsymbol{X}\\boldsymbol{R}^{\\mathrm{T}}$ of a coupling matrix $_P\\in$ $\\Pi_{a,b}(r)$ , the inner marginals of $Q$ and $\\boldsymbol{R}$ are $g_{Q}:=Q^{\\mathrm{T}}\\mathbf{1}_{n}$ and $g_{R}:=R^{\\mathrm{T}}\\mathbf{1}_{m}$ , respectively, where gQ, gR \u2208\u2206r. ", "page_idx": 2}, {"type": "image", "img_path": "hGgkdFF2hR/tmp/2194168c7a50a0134cbb6429fa200535c2bf8a1193efa35526ad4420df876763.jpg", "img_caption": ["Figure 1: (Left) The LC factorization $P=Q\\mathrm{diag}(1/g_{Q})T\\mathrm{diag}(1/g_{R})R^{\\mathrm{T}}$ of coupling matrix $_{P}$ with outer marginals $a,b$ , inner marginals $\\pmb{g}_{Q},\\pmb{g}_{R}$ , factors $Q,R$ , and latent coupling $\\textbf{\\emph{T}}$ . (Right) Full-rank coupling matrix $_{P}$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "To distinguish the different marginals, we refer to $\\textbf{\\em a}$ and $^{b}$ as outer marginals. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2 (LC factorization). Given a coupling matrix $P\\in\\Pi_{a,b}(r)$ , a latent coupling $(L C)$ factorization of $_{P}$ is $P=Q\\mathrm{diag}(1/g_{Q})\\mathbf{{\\calT}}\\mathrm{diag}(1/g_{R})\\mathbf{{\\calR}}^{\\mathrm{T}}$ , where $_{g Q}$ and $\\scriptstyle g_{R}$ are the inner marginals of $Q$ and $\\boldsymbol{R}$ $;{\\cal Q}\\in\\Pi_{a,}.$ , $R\\in\\Pi_{b,}.$ \u00b7, and $\\pmb{T}\\in\\Pi_{\\pmb{g}_{Q},\\pmb{g}_{R}}$ . ", "page_idx": 3}, {"type": "text", "text": "We call the factors $Q,R,T$ in an LC factorization sub-couplings. Let $\\mathcal{R}_{+}:=\\mathbb{R}_{+}^{n\\times r}\\times\\mathbb{R}_{+}^{m\\times r}\\times\\mathbb{R}_{+}^{r\\times r}$ . Given probability vectors $\\pmb{a}\\in\\Delta_{n}$ , $b\\in\\Delta_{m}$ and a positive integer rank $r$ , let ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{L}\\mathsf{C}_{a,b}(r):=\\{(Q,R,T)\\in\\mathcal{R}_{+}:Q\\in\\Pi_{a,\\cdot},R\\in\\Pi_{b,\\cdot},T\\in\\Pi_{g Q,g_{R}}\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "be the set of admissible sub-couplings for the LC factorization. Definition 3.2 gives the following map from $\\mathsf{L C}_{a,b}(r)$ to $\\Pi_{a,b}(r)$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n(Q,R,T)\\mapsto Q\\mathrm{diag}(1/g_{Q})T\\mathrm{diag}(1/g_{R})R^{\\mathrm{T}}=:P_{(Q,R,T)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Since this map is surjective, the set $\\mathsf{L C}_{a,b}(r)$ parameterizes $\\Pi_{a,b}(r)$ . Surjectivity follows from the fact that $\\mathsf{F C}_{a,b}(r)$ maps injectively into $\\mathsf{L C}_{a,b}(r)$ , through $(Q,R,g)\\mapsto(Q,R,\\mathrm{diag}(g))$ , and $\\mathsf{F C}_{a,b}(r)$ maps surjectively onto $\\Pi_{a,b}(r)$ via $(Q,R,g)\\,\\mapsto\\,Q\\mathrm{diag}(1/g)R^{\\mathrm{T}}$ . Definition 3.1 and aDree frinelitaixoend  3s.u2c ahr teh raet $\\b{Q}\\in\\mathbb{R}_{+}^{n\\times m}$ eodr $R\\in\\mathbb{R}_{+}^{n\\times m}$ ,t iownhsil: et hmea icnatsaei nwinhge nt hteh ec oonustterra imnta trhgiatn $T\\in\\Pi_{g_{Q},g_{R}}$ as well as the case of non-square . ", "page_idx": 3}, {"type": "text", "text": "3.2 The Balanced FRLC Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We introduce an algorithm Factor Relaxation with Latent Coupling (FRLC), to compute a LC factorization of minimum cost. We first describe the FRLC algorithm for the balanced Wasserstein problem. Extensions to other and marginal constraints are discussed later. The FRLC objective function, for low-rank, balanced Wasserstein OT, is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{LC}}(Q,R,T):=\\langle C,P_{(Q,R,T)}\\rangle_{F},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $P_{(Q,R,T)}$ is defined by (7). Since $\\mathsf{L C}_{a,b}(r)$ parameterizes $\\Pi_{a,b}(r)$ , problem (8) is equivalent to low rank problem (6). The FRLC algorithm is built from projections onto convex sets, described by constraints on the outer marginals alone for $(Q,R)$ and by the inner marginals alone for $_T$ . Given $(\\dot{Q},R,T)\\in\\mathsf{L C}_{a,b}(r).$ , sub-couplings $Q$ and $\\boldsymbol{R}$ are constrained by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{C}_{1}(a):=\\big\\{(Q,R,T)\\in\\mathcal{R}_{+}:Q\\mathbf{1}_{r}=a\\big\\},\\quad\\mathcal{C}_{1}(b):=\\big\\{(Q,R,T)\\in\\mathcal{R}_{+}:R\\mathbf{1}_{r}=b\\big\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The convex sets constraining the latent coupling matrix $_T$ are ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{C}_{2}(g_{Q}):=\\{(Q,R,T)\\in\\mathcal{R}_{+}:T\\mathbf{1}_{r}=g_{Q}\\},\\quad\\mathcal{C}_{2}(g_{R}):=\\{(Q,R,T)\\in\\mathcal{R}_{+}:T^{\\Pi}\\mathbf{1}_{r}=g_{R}\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $g_{Q}\\,=\\,Q^{\\mathrm{T}}\\mathbf{1}_{n}$ and $g_{R}\\,=\\,R^{\\mathrm{T}}{\\bf1}_{m}$ as per Definition 3.1. Writing $\\mathcal{C}_{1}\\,=\\,\\mathcal{C}_{1}({\\pmb a})\\cap\\mathcal{C}_{1}({\\pmb b})$ and $\\mathcal{C}_{2}=\\mathcal{C}_{2}(\\mathbf{\\dot{g}}_{Q})\\cap\\mathcal{C}_{2}(\\mathbf{g}_{R})$ , one has $\\mathsf{L C}_{a,b}(r)=\\mathcal{C}_{1}\\cap\\mathcal{C}_{2}$ . ", "page_idx": 4}, {"type": "text", "text": "We use coordinate mirror descent to optimize (8), building on the mirror descent (MD) approach of Scetbon et al. (2021); Scetbon & Cuturi (2022); Scetbon et al. (2022, 2023) for the low-rank problem. First we take a descent step in the variables $(Q,R)$ for a fixed $_T$ , using KL penalties on their inner marginals. These \u201csoft\u201d constraints allow the joint optimization in $(Q,R)$ to decouple into two semi-relaxed OT problems, one for each variable. We call this step factor relaxation as this allows $(Q,R)$ to have relaxed inner marginals $_{g Q}$ and $\\scriptstyle g_{R}$ . Next we take a descent step in the latent coupling variable $_T$ , fixing the $Q$ and $\\boldsymbol{R}$ , equivalent to solving a balanced $\\mathrm{OT}$ problem. Thus, solving both coordinate descent steps corresponds to solving three OT problems. ", "page_idx": 4}, {"type": "text", "text": "We now provide further details on these coordinate descent steps, with the full algorithm given in Algorithm 1. Let $(\\gamma_{k})_{k=1}^{N}$ be a sequence of step sizes. As in Scetbon $\\&$ Cuturi (2022), we choose $\\ell^{\\infty}$ -normalization for the step-sizes. Our coordinate mirror descent in the factor relaxation step is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(Q_{k+1},R_{k+1})\\gets\\underset{(Q,R):(Q,R,T_{k})\\in\\mathcal{C}_{1}}{\\arg\\operatorname*{min}}\\langle(Q,R),\\nabla_{(Q,R)}\\mathcal{L}_{\\mathrm{LC}}\\rangle+\\frac{1}{\\gamma_{k}}\\mathrm{KL}((Q,R)||(Q_{k},R_{k}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\tau\\mathrm{KL}((Q^{\\mathrm{T}}\\mathbf{1}_{n},R^{\\mathrm{T}}\\mathbf{1}_{m})||(Q_{k}^{\\mathrm{T}}\\mathbf{1}_{n},R_{k}^{\\mathrm{T}}\\mathbf{1}_{m}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The Sinkhorn kernels for the semi-relaxed OT problems arising from the factor relaxation step are: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K_{Q}^{(k)}:=Q_{k}\\odot\\exp(-\\gamma_{k}(C R_{k}X_{k}^{\\mathrm{T}}-\\mathbf1_{n}\\mathrm{diag}^{-1}((C R_{k}X_{k}^{\\mathrm{T}})^{\\mathrm{T}}Q_{k}\\mathrm{diag}(1/g_{Q_{k}}))^{\\mathrm{T}}))}\\\\ &{K_{R}^{(k)}:=R_{k}\\odot\\exp(-\\gamma_{k}(C^{\\mathrm{T}}Q_{k}X_{k}-\\mathbf1_{m}\\mathrm{diag}^{-1}(\\mathrm{diag}(1/g_{R_{k}})R_{k}^{\\mathrm{T}}C^{\\mathrm{T}}Q_{k}X_{k})^{\\mathrm{T}})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "introducing the shorthand ${\\pmb X}=\\operatorname{diag}\\left(1/{\\pmb g}_{Q}\\right){\\pmb T}\\operatorname{diag}\\left(1/{\\pmb g}_{R}\\right)$ and where $\\mathrm{diag}^{-1}(\\cdot):\\mathbb{R}^{r\\times r}\\rightarrow\\mathbb{R}^{r}$ denotes the matrix-to-vector extraction of the diagonal. This $\\tau$ -dependent regularization also allows us to show smoothness of the objective in Proposition E.5, from which the convergence guarantee Proposition 3.3 follows. We derive the semi-relaxed projection Algorithm 2 of the sub-couplings $Q$ and $\\boldsymbol{R}$ in Appendix $\\mathrm{G}$ for completeness. We also show in Lemma A.1 that $_{g Q}$ and $\\scriptstyle g_{R}$ induced by the semi-relaxed projection are both feasible and locally optimal, not requiring separate optimization. ", "page_idx": 4}, {"type": "text", "text": "As $(Q_{k+1},R_{k+1},T)\\in\\mathcal{C}_{2}$ if and only if $\\pmb{T}\\in\\Pi_{g_{Q_{k+1}},\\pmb{g}_{R_{k+1}}}$ , after the factor relaxation step, we next take a coordinate MD step on the latent coupling $\\textbf{\\emph{T}}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nT_{k+1}\\gets\\operatorname*{arg\\,min}_{\\pmb{T}:\\,(\\pmb{Q}_{k+1},\\pmb{R}_{k+1},\\pmb{T})\\in\\mathcal{C}_{2}}\\langle\\pmb{T},\\nabla_{\\pmb{T}}\\mathcal{L}_{\\mathrm{LC}}\\rangle+\\frac{1}{\\gamma_{k}}\\mathrm{KL}(\\pmb{T}\\|\\pmb{T}_{k}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This is equivalent to applying Sinkhorn (Algorithm 5) to $\\textbf{\\emph{T}}$ given $_{g Q}$ and $\\scriptstyle g_{R}$ with the kernel: ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\pmb K}_{T}^{(k)}:={\\bf T}_{k}\\odot\\exp(-\\gamma_{k}\\mathrm{diag}(1/g_{Q_{k+1}}){\\pmb Q}_{k+1}^{\\mathrm{T}}C R_{k+1}\\mathrm{diag}(1/g_{R_{k+1}})\\,).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "After the final iteration of the coordinate-MD scheme, $X=\\operatorname{diag}\\left(1/g_{Q}\\right)T\\operatorname{diag}\\left(1/g_{R}\\right)$ satisfies $X g_{R}=\\mathbf{1}_{r}$ and $X^{\\mathrm{T}}g_{Q}=\\mathbf{1}_{r}$ as $\\textbf{\\emph{T}}$ is a coupling between $_{g Q}$ and $\\scriptstyle g_{R}$ . Thus $P_{r}=Q X R^{\\mathrm{T}}\\in\\Pi_{a,b}$ and the iterates $(Q_{k},\\dot{T_{k}},R_{k})$ remain in the intersection of the constraint sets. Thus, in contrast to other approaches Scetbon et al. (2021); Lin et al. (2021); Forrow et al. (2019), we do not require Dykstra projections back into the intersection to maintain feasability. We note that our implementation of FRLC allows for a non-square latent coupling $\\textbf{\\emph{T}}$ , providing greater interpretability in problemspecific applications. Above, we presented FRLC in the simplest case that $\\textbf{\\emph{T}}$ is square. ", "page_idx": 4}, {"type": "text", "text": "3.3 Initialization, convergence, and FRLC extensions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Full-rank random initializations of the sub-coupling matrices. We propose a new initialization of the sub-couplings $(Q,R,T)$ for the LC-factorization in Algorithm 6.This generates a full-rank initialization (Proposition F.1) in the set of rank- $^r$ couplings $\\Pi_{a,b}(r)$ and is accomplished by applying Sinkhorn to random matrices. Our approach differs from Scetbon et al. (2021); Scetbon & Cuturi (2022) who use initializations for the diagonal factorization of Forrow et al. (2019), and are not applicable to a latent coupling that is non-diagonal, non-square, or with two distinct inner marginals. ", "page_idx": 4}, {"type": "text", "text": "Input $C,r,a,b,\\tau,\\gamma,\\delta,\\varepsilon$   \nInitialize $\\begin{array}{r}{g_{Q},g_{R}=\\frac{1}{r}\\mathbf{1}_{r}}\\end{array}$   \n$Q_{0},R_{0},T_{0}\\gets I n i t i a l i z e-C o u p l i n g s(a,b,g_{Q},g_{R})$ # Alg. 6   \n$X_{0}\\gets\\mathrm{diag}(1/Q_{0}^{\\mathrm{T}}\\mathbf{1}_{n})T_{0}\\,\\mathrm{diag}(1/R_{0}^{\\mathrm{T}}\\mathbf{1}_{m})$   \nwhile $\\Delta((Q_{k},R_{k},T_{k}),(Q_{k-1},R_{k-1},T_{k-1}))>\\varepsilon$ do # $\\Delta$ as in (10) $\\begin{array}{r}{\\overline{{\\cup}}_{Q}\\leftarrow C R_{k}X_{k}^{\\mathrm{T}}-\\mathbf{1}_{n}\\mathrm{diag}^{-1}((C R_{k}X_{k}^{\\mathrm{T}})^{\\mathrm{T}}Q_{k}\\mathrm{diag}(1/g_{Q}))^{\\mathrm{T}}}\\end{array}$ $\\nabla_{R}\\gets C^{\\mathrm{T}}Q_{k}\\mathbf{X}_{k}-\\mathbf{1}_{m}\\mathrm{diag}^{-1}(\\mathrm{diag}(1/g_{R})R_{k}^{\\mathrm{T}}C^{\\mathrm{T}}Q_{k}\\mathbf{X}_{k})^{\\mathrm{T}}$ $\\gamma_{k}\\gets\\gamma/\\operatorname*{max}\\{\\|\\nabla_{Q}\\|_{\\infty},\\|\\nabla_{R}\\|_{\\infty}\\}\\quad\\#\\ \\ell^{\\infty}$ -normalization of Scetbon & Cuturi (2022) $K_{Q}^{(k)},K_{R}^{(k)}\\gets Q_{k}\\odot\\exp(-\\gamma_{k}\\nabla_{Q}\\,),R_{k}\\odot\\exp(-\\gamma_{k}\\nabla_{R}\\,)$ $\\mathcal{Q}_{k}\\gets\\mathrm{{SR}}^{\\mathrm{{R}}}.\\mathrm{{}}p r o j e c t i o n(K_{Q}^{(k)},\\gamma_{k},\\tau,a,Q_{k-1}^{\\mathrm{{T}}}\\mathbf{1}_{n},\\delta)$ ) # Semi-relaxed OT, Alg. 2 K(Rk ), \u03b3k, \u03c4, b, RkT\u221211m, \u03b4) # Semi-relaxed OT gQ, gR\u2190QT 1n, RT 1m \u2207T = diag(1/gQ)QkT CRk diag(1/gR) \u03b3T = \u03b3/ T \u221e # \u2113\u221e-normalization K(Tk)\u2190Tk \u2299 exp( \u2212\u03b3T \u2207T ) Tk \u2190Sinkhorn(K(Tk ), gR, gQ, \u03b4) # Balanced OT, Alg. 5 $X_{k}\\leftarrow\\mathrm{diag}(1/g_{Q})\\bar{T}T\\,\\mathrm{diag}(1/g_{R})$   \nend while   \nReturn $\\boldsymbol{P_{r}}=\\boldsymbol{Q}\\boldsymbol{X}\\boldsymbol{R}^{\\mathrm{T}}$ ", "page_idx": 5}, {"type": "text", "text": "Convergence analysis of FRLC. As objective (8) is non-convex, it is important to have convergence guarantees. Our convergence criterion $\\bar{\\Delta(\\cdot,\\cdot)}$ is defined in (10). To prove convergence we require a lower bound on the entries of $\\scriptstyle g_{Q}$ and $\\scriptstyle g_{R}$ . Previous works introduce a lower-bound vector $\\alpha\\leq g$ enforced element-wise for stability and smoothness Scetbon et al. (2021). In FRLC the use of semi-relaxed projections naturally enforces a lower-bound. In Appendix E.5, we show that for any $\\delta\\in(0,\\frac{1}{r})$ , the FRLC algorithm\u2019s $\\tau$ -weighted regularization on the inner marginals can guarantee a uniform lower-bound of $\\delta$ on the entries: for sufficiently large $\\tau$ and $\\tilde{O}(m^{2}/\\epsilon)$ iterations for the sub-coupling Pham et al. (2020), one guarantees a lower bound of $\\delta$ on $\\scriptstyle g_{R}$ and $_{g_{Q}}$ . This allows us to show objective smoothness in Proposition E.5. Previous work on low-rank optimal transport Scetbon et al. (2021) use the non-asymptotic convergence criterion of Ghadimi et al. (2014). Following existing works Dang & Lan (2015) establishing convergence rates of coordinate mirror-descent for smooth objectives, we show in Proposition 3.3 this criterion may be extended to coordinate-MD by adapting the block-descent lemma of Beck & Tetruashvili (2013). ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.3. Suppose one has $f\\,\\in\\,C^{1}(\\mathcal{X},\\mathbb{R})$ with block-coordinate Lipschitz gradient and block smoothness constants $(L_{i})_{i=1}^{p}$ , and a function $h\\in C(\\mathcal{X},\\mathbb{R})$ which is $\\alpha$ -strongly convex. For $\\Phi=f+h,$ , suppose one performs coordinate mirror descent on $\\Phi$ minimized over a product of closed convex sets $\\begin{array}{r}{\\mathcal{X}=\\prod_{i=1}^{p}\\mathcal{X}_{i}}\\end{array}$ . Let the sub-iterates with respect to the $i$ -th block update be $\\{\\mathbf{x}_{k}^{i}\\}_{i=0}^{p}$ where $\\mathbf{x}_{k}:=\\mathbf{x}_{k}^{0}$ for $k\\in[N]$ outer iterations. Then one has: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k}\\Delta(\\mathbf{x}_{k},\\mathbf{x}_{k-1})\\leq\\frac{D^{2}L}{N(\\alpha^{2}/2L)}=\\frac{2D^{2}L^{2}}{N\\alpha^{2}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $D$ is (36), $L$ is the global smoothness constant, stepsizes $\\gamma_{k,i}\\,:=\\,\\alpha/L$ , and convergence criterion $\\Delta({\\bf x}_{k},{\\bf x}_{k-1})$ is given in (35). ", "page_idx": 5}, {"type": "text", "text": "Specialized to the LC-parametrization, the criterion $\\Delta_{k}({\\boldsymbol{x}}_{k},{\\boldsymbol{x}}_{k+1})$ is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta_{k}(x_{k},x_{k+1}):=\\frac{1}{\\gamma_{k}^{2}}\\left[\\|Q_{k+1}-Q_{k}\\|_{F}^{2}+\\|R_{k+1}-R_{k}\\|_{F}^{2}+\\|T_{k+1}-T_{k}\\|_{F}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for $\\pmb{x}_{k}=(Q_{k},R_{k},T_{k})$ . We show through Propositions 3.3, E.5 the following result: ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.4. The FRLC algorithm with step-sizes $\\gamma_{k}=\\alpha/L$ and iterates $\\pmb{x}_{k}=(Q_{k},R_{k},T_{k})$ has non-asymptotic stationary convergence in the criterion $\\Delta(\\cdot,\\cdot)$ with: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in1,\\ldots,N-1}\\Delta_{k}(\\pmb{x}_{k},\\pmb{x}_{k+1})\\leq2D^{2}L^{2}/N\\alpha^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "image", "img_path": "hGgkdFF2hR/tmp/c5dec77d4d7a24ea715ecf679030f5e39bfdbdf1e963603335a509d6859a7012.jpg", "img_caption": ["Figure 2: (a) Simulated dataset containing points from two moons (orange) and eight Gaussians (blue). (b) Transport cost $\\langle C,P\\rangle_{F}$ achieved by FRLC and LOT Scetbon et al. (2021) for the balanced Wasserstein problem on the dataset in (a) for different ranks and initializations. FLRC full rank (blue curve) is average over 10 random initializations. (c) Results on the 10D mixture of Gaussians dataset. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Where $N$ is the number of iterations, $D$ the optimality-gap as in (36), and $L=\\operatorname*{max}_{i\\in\\{1,..,3\\}}(L_{i})$ the global smoothness for $L_{i}=\\mathrm{poly}(\\Vert C\\Vert_{F},n,m,r,\\delta)$ the block-wise smoothness constants. ", "page_idx": 6}, {"type": "text", "text": "The proof of Proposition 3.4 follows directly from our extension of the non-asymptotic criterion with the block-descent result Proposition 3.3 and the proof that this lemma holds in FRLC Proposition E.5. We also mention two improvements to other low rank approximation results in literature. In Proposition F.2 we show that one can analytically solve for the block-optimal $\\textbf{\\textit{g}}$ for the factorization of Scetbon et al. (2021), and we improve the bound on the low-rank approximation error in Proposition E.7. ", "page_idx": 6}, {"type": "text", "text": "FRLC for other marginal constraints and objectives. The balanced FRLC algorithm can be extended simply to other marginal constraints owing to the decoupling of the coordinate MD scheme. In particular, by using either the semi-relaxed projections (Algorithm 2) or fully-relaxed (unbalanced) projections (Algorithm 3) on sub-couplings $Q$ and $\\boldsymbol{R}$ , one can solve the balanced problem, the problem with the left or right marginal relaxed, or the unbalanced problem. As such, all variants of marginal constraints can be handled by a single algorithm, given in Algorithm 4. ", "page_idx": 6}, {"type": "text", "text": "We also extend the FRLC algorithm to the Gromov-Wasserstein problem. This consists of computing a GW-specific gradient with the appropriate marginal constraints applied to simplify their form, and re-computing Sinkhorn kernels as exponentiations of these gradients. The matrix form of the quadratic GW objective is $\\mathbf{1}_{m}^{\\mathrm{T}}P^{\\mathrm{T}}A^{\\odot2}P\\dot{\\mathbf{1}}_{m}+\\mathbf{1}_{n}^{\\mathrm{T}}P B^{\\odot2}P^{\\mathrm{T}}\\mathbf{1}_{n}\\overset{\\smile}{-}2\\langle A P B,P\\rangle$ , where $\\odot$ denotes the Hadamard (entrywise) product. Then the GW-specific Sinkhorn kernels are ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K_{Q}^{(k)}\\leftarrow\\exp\\left(2\\gamma_{k}(2A Q X R^{\\mathrm{T}}B R X^{\\mathrm{T}}-A^{\\odot2}Q\\mathbf{1}_{r}\\mathbf{1}_{r}^{\\mathrm{T}})\\right),}\\\\ &{K_{R}^{(k)}\\leftarrow\\exp\\left(2\\gamma_{k}(2B R X^{\\mathrm{T}}Q^{\\mathrm{T}}A Q X-B^{\\odot2}R\\mathbf{1}_{r}\\mathbf{1}_{r}^{\\mathrm{T}})\\right),}\\\\ &{K_{T}^{(k)}\\leftarrow\\exp(4\\gamma_{k}\\operatorname{diag}(g_{Q}^{-1})Q^{\\mathrm{T}}A Q X R^{\\mathrm{T}}B R\\operatorname{diag}(g_{R}^{-1})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In Algorithm 4, one can solve the GW-problem by using the kernels above. Here, we present the kernels omitting a rank-1 perturbation, which is given in Appendix D. From the Wasserstein and GW gradients, the FGW gradient is easily taken as a convex combination of the two. In this work, we primarily focus on the LC-factorization for the rank $r$ Wasserstein problem (6). ", "page_idx": 6}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare FRLC to existing low-rank and full-rank optimal transport algorithms on several datasets: simulated datasets previously used in Tong et al. (2023) and Scetbon et al. (2021); a massive spatialtranscriptomics dataset Chen et al. (2022); and a graph partitioning task Chowdhury & Needham (2021). Further details of each experiment (e.g. pre-processing, validation) are in Appendices K, L, and M. In the section below, LOT refers to the works of Scetbon et al. (2021, 2023, 2022) and Latent OT refers to Lin et al. (2021). ", "page_idx": 6}, {"type": "image", "img_path": "hGgkdFF2hR/tmp/f0a5f8896233aabd7ecad6371a0887430ad5f2d41c93c22b04e63bd420e77a0c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: LC-projections of couplings of Gaussians centered on the $5^{\\mathrm{th}}$ -roots of unity (green) and $10^{\\mathrm{th}}$ roots of unity (yellow). (a) Ground-truth full-rank coupling. (b) Non-square rank-5 latent-coupling of FRLC (c) LC-projection barycenters aligned with rank-5 diagonal coupling of LOT Scetbon et al. (2021). (d) Square rank-10 latent coupling of FRLC. (e) Rank-10 diagonal coupling of LOT . ", "page_idx": 7}, {"type": "text", "text": "4.1 Evaluation of Low-rank Approximations for Balanced OT on Synthetic Data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We first compare the balanced OT version of FRLC with the the low-rank balanced OT algorithm LOT of Scetbon et al. (2021) on a synthetic dataset following Tong et al. (2023). The dataset consists of $m=1000$ points from two moons and $n=1000$ points sampled from eight 2D Gaussian densities (Fig. 2a). We solve the Wasserstein problem (1) with cost matrix $_{C}$ computed using the Euclidean distance. The full-rank coupling matrix $_{P}$ has rank 1000, and we compute both FRLC and LOT solutions with rank between 20 and 200. For each rank, we initialize FRLC adapting the deterministic rank-2 initialization proposed in Scetbon et al. (2021) and the random initialization of Alg. 6. We initialize LOT using the rank-2 initialization and two other options in ott-jax Cuturi et al. (2022). ", "page_idx": 7}, {"type": "text", "text": "We find that FRLC obtains lower transport cost $\\langle C,P\\rangle_{F}$ with increasing rank (Fig. 2b) and consistently achieves lower transport cost than LOT across all ranks and all initializations. Specifically, starting both methods at the same rank-2 initialization, FRLC consistently achieves a lower cost than LOT for all ranks. Additionally, we observe smooth convergence of FRLC for both rank-2 initialization and the full-rank random initialization of Alg. 6 (Fig. 5). ", "page_idx": 7}, {"type": "text", "text": "We also evaluate FRLC and LOT on two datasets of Gaussian mixtures, one in 2-dimensions and one in 10-dimensions, each with $n=m=5,000$ points from two mixtures of Gaussians, following Scetbon et al. (2021), with further details in Appendix K. We observe the same trend as the previous simulation for both datasets (Fig. 2c, Fig. 7), with FRLC achieving lower transport costs than LOT across all ranks and all initializations. In addition FRLC has half the runtime of LOT (CPU) \u2013 including the setup time of FRLC but excluding the setup time of LOT in ott-jax \u2013 on datasets of $n=m=1000$ points from all three datasets with rank $r=100$ (Table 2). At the same time FRLC achieves lower primal cost $\\langle C,P\\rangle_{F}$ with tighter marginals $\\|\\pmb{P1}_{n}-\\pmb{a}\\|_{2}$ and $\\|P^{T}\\mathbf{1}_{m}-b\\|_{2}$ . Lin et al. (2021) only solves a proxy for the rank-constrained Wasserstein problem, and thus is not the focus of our comparisons. Nevertheless, we verify that on all synthetic experiments that FRLC achieves significantly lower primal OT cost than Latent OT (Table 5). ", "page_idx": 7}, {"type": "text", "text": "4.2 Interpretation of the Latent Coupling and LC-Projection ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We demonstrate the intepretability of the latent coupling $_T$ in the LC factorization. In both the LC factorization and factored couplings, the sub-couplings $Q$ and $\\boldsymbol{R}$ each have associated barycentric projection operators which coarse-grain input datasets ${\\bf Z}^{(1)},{\\bf Z}^{(2)}$ . In particular, the LC projection is defined from the LC factorization as follows. ", "page_idx": 7}, {"type": "text", "text": "Definition 4.1 (LC-Projection). Let $Q\\,\\mathrm{diag}(1/g_{Q})\\pmb{T}\\,\\mathrm{diag}(1/\\pmb{g}_{R})\\pmb{R}^{\\mathrm{T}}$ be an LC factorization of of a coupling matrix $P\\,\\in\\,\\Pi_{a,b}(r)$ computed from datasets $\\mathbf{Z}^{(1)}\\,\\in\\,\\mathbb{R}^{n\\times d},\\mathbf{Z}^{(2)}\\,\\in\\,\\mathbb{R}^{m\\times d}$ , with $\\pmb{T}\\in\\mathbb{R}_{+}^{r_{1}\\times r_{2}}$ . The $L C$ -projections ${\\bf Y}^{(1)}$ and $\\mathbf{Y}^{(2)}$ of $\\mathbf{Z}^{(1)}$ and $\\mathbf{Z}^{(2)}$ are $\\mathbf{Y}^{(1)}:=\\mathrm{diag}(1/\\pmb{g}_{Q})\\pmb{Q}^{\\mathrm{T}}\\mathbf{Z}^{(1)}$ and $\\mathbf{Y}^{(\\overset{.}{2})}:=\\mathrm{diag}(1/\\pmb{g}_{R})\\pmb{R}^{\\mathrm{T}}\\mathbf{Z}^{(2)}$ . ", "page_idx": 7}, {"type": "text", "text": "By interpreting any factored coupling $(Q,R,g)$ as an LC factorization $(Q,R,\\operatorname{diag}(g))$ , Definition 4.1 describes the barycentric projections for both factorizations. We compare the projections of the coupling computed by FRLC to those of LOT Scetbon et al. (2021) on a dataset containing 1000 samples from 2D-Gaussians centered at the $5^{\\mathrm{th}}$ -roots of unity and 1000 samples from 2D Gaussians centered at the $10^{\\mathrm{th}}$ -roots of unity (the latter scaled by a factor of two, Fig. 3a). In both cases, the latent coupling $\\textbf{\\emph{T}}$ or $\\operatorname{diag}(g)$ is visualized as a transport between barycenters. We run FRLC and LOT with ranks $r=5$ and $r=10$ to match the number of target and source clusters. In the rank-5 case, FRLC uses a non-square latent coupling $\\pmb{T}\\in\\mathbb{R}_{+}^{10\\times5}$ which correctly captures the coupling between clusters (Fig. 3(b)), while the LOT rank-5 projection computes barycenters that are outside of the clusters (Fig. 3c). A similar result is observed for square rank-10 latent couplings computed by FRLC (Fig. 3d) and LOT (Fig. 3e) demonstrating that the LOT barycenters in Fig. 3b) are not an artifact of using the lowest rank. We observe similar results on other simulated datasets (Fig. 11). ", "page_idx": 7}, {"type": "image", "img_path": "hGgkdFF2hR/tmp/920ea9e0a4045d463c61f166aef4fc3f2e516e54ed40e183b3be537fe3e40046.jpg", "img_caption": ["Figure 4: (a) Brain marker gene $T u b b2b$ expression and FRLC prediction. (b) Comparison of the low-rank unbalanced (LOT-U) algorithm of Scetbon et al. (2023) and FRLC on aligning spatial transcriptomics data. Bold indicates top performing method for each metric on each objective. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.3 Evaluation on Spatial Transcriptomics Alignment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We compare FRLC and the algorithm (LOT-U) of Scetbon et al. (2023) (which solves unbalanced low-rank Wasserstein, GW, and FGW problems) on the task of computing an alignment between cells from different time points during mouse embryonic development. Specifically, we compute an alignment between a spatial transcriptomics (ST) dataset of an E11.5 stage mouse embryo and an E12.5 stage mouse embryo Chen et al. (2022). Optimal transport is a popular approach to align single-cell Schiebinger et al. (2019) and spatial trancriptomics datasets Zeira et al. (2022); Liu et al. (2023); Klein et al. (2023). In single-cell transcriptomics, one measures a gene expression vector for each cell, and in spatial transcriptomics one additionally measures the 2D location of each cell. The cost matrix $_{C}$ describes the difference between gene expression vectors and intra-domain cost matrices $\\pmb{A}$ and $_B$ are derived from the 2D coordinates within each slice. Therefore, OT problems of W, GW, and FGW objectives can be solved and the coupling matrix represents the cell-cell alignment (Appendix M). However, computation of a full-rank OT solution is not feasible in our large-scale dataset: the E11.5 slice has about 30,000 cells while the E12.5 slice has about 50,000 cells. ", "page_idx": 8}, {"type": "text", "text": "We evaluate the alignments by assessing performance on two prediction tasks from Scetbon et al. (2023): (1) a gene expression prediction task where we predict the expression of a gene in E12.5 from expression of the gene in E11.5 using the alignment; (2) a cell type prediction task where we predict the cell types of E12.5 from the cell type clustering of E11.5 (Appendix M). We evaluate the accuracy of the gene expression prediction task through the Spearman correlation $\\rho$ between the predicted expression and the ground truth expression of 10 test marker genes. We evaluate the accuracy of the cell type prediction task by computing the Adjusted Rand Index (ARI) and Adjusted Mutual Information (AMI) between the predicted cell types and the cell types derived in the original publication Chen et al. (2022). Being a comparison between different objectives, this relies on downstream metrics. For completeness, we validate the efficacy of FRLC on directly minimizing the balanced Wasserstein cost $\\langle C,P\\rangle_{F}$ against Scetbon et al. (2021) in Figure 8. ", "page_idx": 8}, {"type": "text", "text": "For a direct comparison, we use FRLC to solve the same unbalanced problems (denoted FRLC-U). We perform an extensive grid search (Appendix M.3) to pick the best hyperparameters (including rank $\\ll30,000;$ ) for all algorithms. Scetbon et al. (2023) previously showed that unbalanced FGW algorithm has the best performance on ST alignment. We find that unbalanced FRLC achieves comparable or better results than the previous state-of-the-art unbalanced low-rank method on all three objectives (Table 4). We also solve a semi-relaxed version of each problem motivated by the observation that all cells from E12.5 have an ancestor, but not all cells from E11.5 have the same number of descendants due to cell growth and death. Thus the former marginal is tight, and the latter relaxed Halmos et al. (2024). We run both semi-relaxed FRLC (FRLC-SR) and a setting of LOT-U that recovers the semi-relaxed problem (LOT-SR). Semi-relaxed FRLC achieves the best results on all three metrics by a large margin (Table 4). As one example, the expression of $T u b b2b$ , a mouse brain marker gene, agreeing with the expression predicted from the semi-relaxed alignment of FRLC (Fig. 4a). ", "page_idx": 8}, {"type": "table", "img_path": "hGgkdFF2hR/tmp/3b8581b8304e8b6f323f0f67175dde4001f698dc9234da8fb6b0f3003d371a1f.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparing aspects of low-rank OT methods. Factorization indicates the structure of the inner matrix. "], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "4.4 Additional Experiments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We evaluate FRLC on an unsupervised graph partitioning problem Chowdhury & Needham (2021) on four real-world graph datasets Yang & Leskovec (2012); Yin et al. (2017); Banerjee et al. (2013). We benchmark the performance of the semi-relaxed and GW settings of FRLC against (1) GWL Xu et al. (2019), solving a balanced GW problem; (2) SpecGWL Chowdhury & Needham (2021) using the heat kernel on the graph Laplacian as the cost matrix. We find FRLC achieves the better clustering performance than GWL and SpecGWL on 9/12 and 11/12 of the datasets (Table 3 and Appendix L). ", "page_idx": 9}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We provide comparison of existing low-rank solvers in Table 1. The FRLC algorithm has a number of advantages, including (1) coarsening a full-rank plan $_{P}$ to non-diagonal latent coupling $_T$ ; (2) minimizing the primal OT problem for general cost $_{C}$ rather than a barycenteric problem; (3) optimizing only sub-couplings; and (4) using Sinkhorn alone as the sub-routine for low-rank OT. While we argue these are substantial advantages, FRLC has limitations which warrant follow-up work. In particular, three key limitations of our work, common to the existing low-rank OT algorithms, are: (1) selecting values of the latent coupling ranks; (2) strengthening the convergence criterion; (3) addressing sensitivity to the initialization from non-convexity of the objective. A limitation specific to our work is the selection of the $\\tau$ hyperparameter controlling the smoothness of the trajectory. These and other limitations are discussed in Section $_\\mathrm{N}$ of the Appendix. Another direction for further investigation is to better understand what structure LC factorizations capture when the optimal plan is known to have full rank, e.g. when the Monge map exists, as has been explored by Liu et al. (2021). ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce FRLC, an algorithm to compute low-rank optimal transport plan from the latent coupling (LC) factorization. FRLC handles different OT objective costs and relaxations of the marginal constraints. Moreover, the LC factorization provides an interpretable coarse-graining of the full transport plan and its marginals through the mapping $(P,a,\\bar{\\boldsymbol{b}})\\,\\rightarrow\\,(T,g_{\\mathcal{Q}},\\bar{g_{R}})$ . We demonstrate the superior performance of FRLC compared to state-of-the-art low-rank methods on real and synthetic datasets. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported by NCI grant U24CA248453 to B.J.R. J.G. gratefully acknowledges support from the Schmidt DataX Fund at Princeton University made possible through a major gift from the Schmidt Futures Foundation. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Bakshi, A. and Woodruff, D. Sublinear Time Low-Rank Approximation of Distance Matrices. Advances in Neural Information Processing Systems, 31, 2018.   \nBanerjee, A., Chandrasekhar, A. G., Duflo, E., and Jackson, M. O. The Diffusion of Microfinance. Science, 341(6144):1236498, 2013.   \nBauschke, H. H. and Lewis, A. S. Dykstras algorithm with Bregman projections: A convergence proof. Optimization, 48(4):409\u2013427, January 2000. ISSN 1029-4945. doi: 10.1080/02331930008844513. URL http://dx.doi.org/10.1080/02331930008844513.   \nBeck, A. and Tetruashvili, L. On the Convergence of Block Coordinate Descent Type Methods. SIAM J. Optim., 23:2037\u20132060, 2013. URL https://api.semanticscholar.org/CorpusID: 6866704.   \nBenamou, J.-D., Carlier, G., Cuturi, M., Nenna, L., and Peyr\u00e9, G. Iterative Bregman Projections for Regularized Transportation Problems. SIAM Journal on Scientific Computing, 37 (2):A1111\u2013A1138, January 2015. ISSN 1095-7197. doi: 10.1137/141000439. URL http: //dx.doi.org/10.1137/141000439.   \nBregman, L. M. The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. USSR computational mathematics and mathematical physics, 7(3):200\u2013217, 1967.   \nBunne, C., Stark, S. G., Gut, G., del Castillo, J. S., Levesque, M., Lehmann, K.-V., Pelkmans, L., Krause, A., and R\u00e4tsch, G. Learning single-cell perturbation responses using neural optimal transport. Nature Methods, 20(11):1759\u20131768, September 2023. ISSN 1548-7105. doi: 10.1038/ s41592-023-01969-x. URL http://dx.doi.org/10.1038/s41592-023-01969-x.   \nCharikar, M., Chen, B., R\u00e9, C., and Waingarten, E. Fast Algorithms for a New Relaxation of Optimal Transport. In Neu, G. and Rosasco, L. (eds.), Proceedings of Thirty Sixth Conference on Learning Theory, volume 195 of Proceedings of Machine Learning Research, pp. 4831\u20134862. PMLR, 12\u201315 Jul 2023. URL https://proceedings.mlr.press/v195/charikar23a.html.   \nChen, A., Liao, S., Cheng, M., Ma, K., Wu, L., Lai, Y., Qiu, X., Yang, J., Xu, J., Hao, S., et al. Spatiotemporal transcriptomic atlas of mouse organogenesis using DNA nanoball-patterned arrays. Cell, 185(10):1777\u20131792, 2022.   \nChen, X. and Price, E. Condition number-free query and active learning of linear families. 2017.   \nChizat, L., Peyr\u00e9, G., Schmitzer, B., and Vialard, F.-X. Unbalanced Optimal Transport: Dynamic and Kantorovich Formulations. Journal of Functional Analysis, 274(11):3090\u20133123, June 2018. ISSN 0022-1236. doi: 10.1016/j.jfa.2018.03.008. URL http://dx.doi.org/10.1016/j.jfa.2018. 03.008.   \nChowdhury, S. and Needham, T. Generalized Spectral Clustering via Gromov-Wasserstein Learning. In International Conference on Artificial Intelligence and Statistics, pp. 712\u2013720. PMLR, 2021.   \nChung, F. Laplacians and the Cheeger Inequality for Directed Graphs. Annals of Combinatorics, 9: 1\u201319, 2005.   \nCohen, J. E. and Rothblum, U. G. Nonnegative Ranks, Decompositions, and Factorizations of Nonnegative Matrices. Linear Algebra and its Applications, 190:149\u2013168, 1993.   \nCourty, N., Flamary, R., and Tuia, D. Domain adaptation with regularized optimal transport. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part I 14, pp. 274\u2013289. Springer, 2014.   \nCuturi, M. Sinkhorn Distances: Lightspeed Computation of Optimal Transport. Advances in neural information processing systems, 26, 2013a.   \nCuturi, M. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in Neural Information Processing Systems, pp. 2292\u20132300, 2013b. URL https://proceedings.neurips. cc/paper/2013/hash/af21d0c97db2e27e13572cbf59eb343d-Abstract.html.   \nCuturi, M., Meng-Papaxanthos, L., Tian, Y., Bunne, C., Davis, G., and Teboul, O. Optimal Transport Tools (OTT): A JAX Toolbox for all things Wasserstein. arXiv preprint arXiv:2201.12324, 2022.   \nDang, C. D. and Lan, G. Stochastic Block Mirror Descent Methods for Nonsmooth and Stochastic Optimization. SIAM J. Optim., 25(2):856\u2013881, January 2015.   \nDong, S., Pan, Z., Fu, Y., Xu, D., Shi, K., Yang, Q., Shi, Y., and Zhuo, C. Partial Unbalanced Feature Transport for Cross-Modality Cardiac Image Segmentation. IEEE Transactions on Medical Imaging, 2023.   \nDykstra, R. L. An Algorithm for Restricted Least Squares Regression. Journal of the American Statistical Association, 78(384):837\u2013842, December 1983. ISSN 1537-274X. doi: 10.1080/ 01621459.1983.10477029. URL http://dx.doi.org/10.1080/01621459.1983.10477029.   \nForrow, A., H\u00fctter, J.-C., Nitzan, M., Rigollet, P., Schiebinger, G., and Weed, J. Statistical Optimal Transport via Factored Couplings. In Chaudhuri, K. and Sugiyama, M. (eds.), Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pp. 2454\u20132465. PMLR, 16\u201318 Apr 2019. URL https://proceedings.mlr.press/v89/forrow19a.html.   \nFrieze, A., Kannan, R., and Vempala, S. Fast Monte-Carlo Algorithms for Finding Low-rank Approximations. J. ACM, 51(6):1025\u20131041, nov 2004. ISSN 0004-5411. doi: 10.1145/1039488. 1039494. URL https://doi.org/10.1145/1039488.1039494.   \nFrogner, C., Zhang, C., Mobahi, H., Araya, M., and Poggio, T. A. Learning with a Wasserstein Loss. Advances in neural information processing systems, 28, 2015.   \nGeshkovski, B., Letrouit, C., Polyanskiy, Y., and Rigollet, P. A mathematical perspective on Transformers. arXiv preprint arXiv:2312.10794, 2023.   \nGhadimi, S., Lan, G., and Zhang, H. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1\u20132):267\u2013305, December 2014. ISSN 1436-4646. doi: 10.1007/s10107-014-0846-1. URL http://dx.doi.org/10. 1007/s10107-014-0846-1.   \nHalmos, P., Liu, X., Gold, J., Chen, F., Ding, L., and Raphael, B. J. DeST-OT: Alignment of Spatiotemporal Transcriptomics Data. In International Conference on Research in Computational Molecular Biology, pp. 434\u2013437. Springer, 2024.   \nIndyk, P., Vakilian, A., Wagner, T., and Woodruff, D. P. Sample-optimal low-rank approximation of distance matrices. In Beygelzimer, A. and Hsu, D. (eds.), Proceedings of the ThirtySecond Conference on Learning Theory, volume 99 of Proceedings of Machine Learning Research, pp. 1723\u20131751. PMLR, 25\u201328 Jun 2019. URL https://proceedings.mlr.press/ v99/indyk19a.html.   \nKantorovich, L. On the Translocation of Masses: Doklady akademii nauk ussr. 1942.   \nKlein, D., Palla, G., Lange, M., Klein, M., Piran, Z., Gander, M., Meng-Papaxanthos, L., Sterr, M., Bastidas-Ponce, A., Tarquis-Medina, M., et al. Mapping cells through time and space with moscot. bioRxiv, pp. 2023\u201305, 2023.   \nLin, C.-H., Azabou, M., and Dyer, E. L. Making transport more robust and interpretable by moving data through a small number of anchor points. Proceedings of machine learning research, 139: 6631, 2021.   \nLiu, W., Zhang, C., Zheng, N., and Qian, H. Approximating optimal transport via low-rank and sparse factorization. CoRR, abs/2111.06546, 2021. URL https://arxiv.org/abs/2111.06546. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Liu, X., Zeira, R., and Raphael, B. J. Partial alignment of multislice spatially resolved transcriptomics data. Genome Research, 33(7):1124\u20131132, 2023. ", "page_idx": 12}, {"type": "text", "text": "M\u00e9moli, F. On the use of Gromov-Hausdorff Distances for Shape Comparison. 2007. ", "page_idx": 12}, {"type": "text", "text": "M\u00e9moli, F. Gromov\u2013Wasserstein Distances and the Metric Approach to Object Matching. Foundations of computational mathematics, 11:417\u2013487, 2011.   \nNesterov, Y. Efficiency of Coordinate Descent Methods on Huge-Scale Optimization Problems. SIAM Journal on Optimization, 22(2):341\u2013362, 2012.   \nOrlin, J. B. A polynomial time primal network simplex algorithm for minimum cost flows. Mathematical Programming, 78(2):109\u2013129, Aug 1997. ISSN 1436-4646. doi: 10.1007/BF02614365. URL https://link.springer.com/content/pdf/10.1007/BF02614365.pdf.   \nPham, K., Le, K., Ho, N., Pham, T., and Bui, H. H. On Unbalanced Optimal Transport: An Analysis of Sinkhorn Algorithm. In International Conference on Machine Learning, 2020. URL https://api.semanticscholar.org/CorpusID:211068892.   \nSander, M. E., Ablin, P., Blondel, M., and Peyr\u00e9, G. Sinkformers: Transformers with Doubly Stochastic Attention. In Camps-Valls, G., Ruiz, F. J. R., and Valera, I. (eds.), Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pp. 3515\u20133530. PMLR, 28\u201330 Mar 2022. URL https://proceedings.mlr.press/v151/sander22a.html.   \nScetbon, M. and Cuturi, M. Low-rank Optimal Transport: Approximation, Statistics and Debiasing. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=4btNeXKFAQ.   \nScetbon, M., Cuturi, M., and Peyr\u00e9, G. Low-Rank Sinkhorn Factorization. In International Conference on Machine Learning, 2021. URL https://api.semanticscholar.org/CorpusID: 232147563.   \nScetbon, M., Peyr\u00e9, G., and Cuturi, M. Linear-time Gromov Wasserstein Distances using Low Rank Couplings and Costs. In International Conference on Machine Learning, pp. 19347\u201319365. PMLR, 2022.   \nScetbon, M., Klein, M., Palla, G., and Cuturi, M. Unbalanced Low-rank Optimal Transport Solvers, 2023.   \nSchiebinger, G., Shu, J., Tabaka, M., Cleary, B., Subramanian, V., Solomon, A., Gould, J., Liu, S., Lin, S., Berube, P., et al. Optimal-Transport Analysis of Single-Cell Gene Expression Identifies Developmental Trajectories in Reprogramming. Cell, 176(4):928\u2013943, 2019.   \nSolomon, J., De Goes, F., Peyr\u00e9, G., Cuturi, M., Butscher, A., Nguyen, A., Du, T., and Guibas, L. Convolutional Wasserstein Distances: Efficient Optimal Transportation on Geometric Domains. ACM Transactions on Graphics (ToG), 34(4):1\u201311, 2015.   \nSt\u00e5hl, P. L., Salm\u00e9n, F., Vickovic, S., Lundmark, A., Navarro, J. F., Magnusson, J., Giacomello, S., Asp, M., Westholm, J. O., Huss, M., et al. Visualization and analysis of gene expression in tissue sections by spatial transcriptomics. Science, 353(6294):78\u201382, 2016.   \nTarjan, R. E. Dynamic trees as search trees via Euler tours, applied to the network simplex algorithm. Mathematical Programming, 78(2):169\u2013177, Aug 1997. ISSN 1436-4646. doi: 10.1007/BF02614369. URL https://link.springer.com/content/pdf/10.1007/ BF02614369.pdf.   \nTay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C. Sparse Sinkhorn Attention. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 9438\u20139447. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/tay20a.html.   \nTong, A., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks, J., Fatras, K., Wolf, G., and Bengio, Y. Improving and generalizing flow-based generative models with minibatch optimal transport. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems, 2023.   \nVayer, T., Chapel, L., Flamary, R., Tavenard, R., and Courty, N. Fused Gromov-Wasserstein distance for structured objects. Algorithms, 13(9):212, August 2020. ISSN 1999-4893. doi: 10.3390/a13090212. URL http://dx.doi.org/10.3390/a13090212.   \nVincent-Cuaz, C., Flamary, R., Corneli, M., Vayer, T., and Courty, N. Semi-relaxed GromovWasserstein divergence and applications on graphs. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id $\\equiv$ RShaMexjc-x.   \nWolf, F. A., Angerer, P., and Theis, F. J. SCANPY: large-scale single-cell gene expression data analysis. Genome biology, 19:1\u20135, 2018.   \nXu, H., Luo, D., Zha, H., and Duke, L. C. Gromov-Wasserstein Learning for Graph Matching and Node Embedding. In International conference on machine learning, pp. 6932\u20136941. PMLR, 2019.   \nYang, J. and Leskovec, J. Defining and Evaluating Network Communities based on Ground-truth. In Proceedings of the ACM SIGKDD Workshop on Mining Data Semantics, pp. 1\u20138, 2012.   \nYang, K. D., Damodaran, K., Venkatachalapathy, S., Soylemezoglu, A. C., Shivashankar, G., and Uhler, C. Predicting cell lineages using autoencoders and optimal transport. PLoS computational biology, 16(4):e1007828, 2020.   \nYin, H., Benson, A. R., Leskovec, J., and Gleich, D. F. Local Higher-Order Graph Clustering. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 555\u2013564, 2017.   \nZeira, R., Land, M., Strzalkowski, A., and Raphael, B. J. Alignment and integration of spatial transcriptomics data. Nature Methods, 19(5):567\u2013575, 2022. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Low-rank optimal transport ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Low-rank factorizations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The set of low-rank couplings. Given $M\\in\\mathbb{R}_{+}^{n\\times m}$ , the nonnegative rank of $_M$ is the least number of nonnegative, rank-1 matrices that sum to $_M$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname{rk}_{+}(M)=\\operatorname*{min}_{r\\geq1}\\left\\{M=\\sum_{i=1}^{r}M_{i},\\ {\\mathrm{such}}\\,\\operatorname{that}\\operatorname{rk}(M_{i})=1\\ {\\mathrm{and}}\\ M_{i}\\geq0\\ {\\mathrm{for~all}}\\ i\\right\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $\\pmb{a}\\,\\in\\,\\Delta_{n},\\pmb{b}\\,\\in\\,\\Delta_{m}$ be probability vectors, and let $\\Pi_{a,b}(r)$ denote the set of rank- $^r$ coupling matrices with marginals $\\textbf{\\em a}$ and $^{b}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Pi_{a,b}(r)=\\{P\\in\\mathbb{R}_{+}^{n\\times m}:P^{\\mathrm{T}}\\mathbf{1}_{m}=a,P\\mathbf{1}_{n}=b,\\,\\mathrm{rk}_{+}(P)\\leq r\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To optimize any cost over $\\Pi_{a,b}(r)$ , one requires a parameterization of this set. ", "page_idx": 14}, {"type": "text", "text": "Factored couplings. The factored coupling parameterization of $\\Pi_{a,b}(r)$ introduced in Forrow et al. (2019), and used by Scetbon et al. (2021); Scetbon & Cuturi (2022); Scetbon et al. (2022, 2023) is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathsf{F}\\mathsf{C}_{a,b}(r):=\\{(Q,\\pmb{R},\\pmb{g})\\in\\mathbb{R}_{+}^{n\\times r}\\times\\mathbb{R}_{+}^{m\\times r}\\times(\\mathbb{R}_{+}^{*})^{r}:Q\\in\\Pi_{a,\\pmb{g}},\\pmb{R}\\in\\Pi_{b,\\pmb{g}}\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Cohen & Rothblum (1993) show that any $P\\in\\Pi_{a,b}(r)$ may be decomposed as $P=Q\\mathrm{diag}(1/g)R^{\\mathrm{T}}$ for some triple $(Q,R,g)\\in\\mathsf{F C}$ . Thus, for cost matrix $C\\in\\mathbb{R}^{n\\times m}$ , the general low-rank optimal transport problem is equivalent to an optimization over factored couplings: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{P\\in\\Pi_{a,b}(r)}\\langle C,P\\rangle_{F}=\\operatorname*{min}_{(Q,R,g)\\in\\mathsf{F C}_{a,b}(r)}\\langle C,Q\\mathrm{diag}(1/g)R^{\\mathrm{T}}\\rangle_{F}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Latent coupling factorization. The latent coupling parameterization of $\\Pi_{a,b}(r)$ introduced in Lin et al. (2021), and used in the present work is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{L}C_{a,b}(r):=\\{(Q,R,T)\\in\\mathbb{R}_{+}^{n\\times r}\\times\\mathbb{R}_{+}^{m\\times r}\\times\\mathbb{R}_{+}^{r\\times r}:Q\\in\\Pi_{a,\\cdot},R\\in\\Pi_{b,\\cdot},T\\in\\Pi_{g_{Q},g_{R}}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\scriptstyle g_{Q},\\,g_{R}$ are the inner marginals of $Q$ and $\\boldsymbol{R}$ . ", "page_idx": 14}, {"type": "text", "text": "Latent coupling diagonalization. The LC-factorization recovers the factorization of Forrow et al. (2019) as a sub-case. While the diagonal factorization of previous works cannot be directly converted to the LC-factorization, the LC-factorization can easily recover the diagonal factorization. In particular, taking $Q^{\\prime}\\leftarrow Q\\,\\mathrm{diag}(1/\\pmb{g}_{Q})\\pmb{T}$ one can refactor ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{r}=Q\\,\\mathrm{diag}(1/g_{Q}){\\pmb T}\\,\\mathrm{diag}(1/g_{R}){\\pmb R}^{\\mathrm{T}}=Q^{\\prime}\\,\\mathrm{diag}(1/g_{R}){\\pmb R}^{\\mathrm{T}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "or alternatively taking ${\\pmb R}^{\\prime}={\\pmb R}\\,\\mathrm{diag}(1/\\pmb g_{R}){\\pmb T}^{\\mathrm{T}}$ may refactor as ", "page_idx": 14}, {"type": "equation", "text": "$$\nP_{r}=Q\\,\\mathrm{diag}(1/g_{Q}){\\cal T}\\,\\mathrm{diag}(1/g_{R}){\\cal R}^{\\mathrm{T}}=Q\\,\\mathrm{diag}(1/g_{Q})({\\cal R}^{\\prime})^{\\mathrm{T}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "So that instead of returning $(Q,R,T)$ one may alternatively return $(Q,R,T)\\to(Q^{\\prime},R,\\mathrm{diag}(g_{R}))$ or $(Q,R,T)\\rightarrow(Q,R^{\\prime},\\mathrm{diag}(g_{Q}))$ to recover the Forrow et al. (2019) factorization. An example of this diagonal-conversion is offered in Figure 12. ", "page_idx": 14}, {"type": "text", "text": "A.2 Balanced low-rank optimal transport ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The FRLC optimization problem Our optimization problem is over the variables $(Q,R,T)$ and defined as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{(Q,R,T)\\in\\mathsf{L C}_{a,b}(r)}\\mathcal{L}_{\\mathrm{LC}}(Q,R,T),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where our objective function ${\\mathcal{L}}_{\\mathrm{LC}}$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{LC}}(Q,R,T)=\\langle C,Q(\\mathrm{diag}(1/Q^{\\mathrm{T}}\\mathbf{1}_{n}))T(\\mathrm{diag}(1/R^{\\mathrm{T}}\\mathbf{1}_{m}))R^{\\mathrm{T}}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Given $(Q,R,T)\\in\\mathsf{L C}_{a,b}(r)$ , sub-couplings $Q$ and $\\boldsymbol{R}$ are constrained by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{C}_{1}(a):=\\big\\{(Q,R,T)\\in\\mathcal{R}_{+}:Q\\mathbf{1}_{r}=a\\big\\},\\quad\\mathcal{C}_{1}(b):=\\big\\{(Q,R,T)\\in\\mathcal{R}_{+}:R\\mathbf{1}_{r}=b\\big\\},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "while the convex sets constraining the latent coupling matrix $_T$ are ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{C}_{2}(g_{Q}):=\\{(Q,R,T)\\in\\mathcal{R}_{+}:T\\mathbf{1}_{r}=g_{Q}\\},\\quad\\mathcal{C}_{2}(g_{R}):=\\{(Q,R,T)\\in\\mathcal{R}_{+}:T^{\\Pi}\\mathbf{1}_{r}=g_{R}\\},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $g_{Q}=Q^{\\mathrm{T}}\\mathbf{1}_{n}$ and ${\\pmb g}_{R}={\\pmb R}^{\\mathrm{T}}{\\bf1}_{m}$ as per Definition 3.1. Under these definitions, $\\mathsf{L C}_{a,b}(r)=$ $\\mathcal{C}_{1}\\cap\\mathcal{C}_{2}$ , where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{C}_{1}=\\mathcal{C}_{1}(\\pmb{a})\\cap\\mathcal{C}_{1}(\\pmb{b})\\quad\\mathrm{~and~}\\quad\\mathcal{C}_{2}=\\mathcal{C}_{2}(\\pmb{g}_{Q})\\cap\\mathcal{C}_{2}(\\pmb{g}_{R})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To solve (12), we separate the variables into two \u201cblocks\u201d of variables, $(Q,R)$ and $_T$ , and perform two block updates per iteration, as follows. Let $(\\gamma_{k})_{k\\geq0}$ be a positive sequence of stepsizes. Suppose we have $(\\bar{Q_{k}},R_{k},\\bar{T_{k}})\\in\\mathcal{C}$ . We update the first variable block $(Q,R)$ by taking a locally optimal (mirror descent) update step, while the second variable block $\\textbf{\\emph{T}}$ is held fixed: ", "page_idx": 15}, {"type": "equation", "text": "$$\n(Q_{k+1},R_{k+1})\\gets\\operatorname*{arg\\,min}_{(Q,R):(Q,R,T_{k})\\in\\mathcal{C}_{1}}\\langle(Q,R),\\nabla_{(Q,R)}\\mathcal{L}_{\\mathrm{LC}}\\rangle+\\frac{1}{\\gamma_{k}}\\mathrm{KL}((Q,R)\\|(Q_{k},R_{k}))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, we slightly abused the notation by putting $(Q,R)$ inside an inner product. The triple $(Q_{k+1},R_{k+1},T_{k})$ produced by this update lies in $\\mathcal{C}_{1}$ . Next, we update $_T$ , the second variable block, by taking another locally optimal (mirror descent) step, while the first variable block is held fixed. ", "page_idx": 15}, {"type": "equation", "text": "$$\nT_{k+1}\\gets\\operatorname*{arg\\,min}_{\\pmb{T}:(\\pmb{Q}_{k+1},\\pmb{R}_{k+1},\\pmb{T})\\in\\mathcal{C}_{2}}\\langle\\pmb{T},\\nabla_{\\pmb{T}}\\mathcal{L}_{\\mathrm{LC}}\\rangle+\\frac{1}{\\gamma_{k}}\\mathrm{KL}(\\pmb{T}\\|\\pmb{T}_{k}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By construction, the triple $(Q_{k+1},R_{k+1},T_{k+1})\\;\\in\\;{\\mathcal{C}}_{2}$ . However, because the set $\\mathcal{C}_{1}$ only constrains the first variable block $Q_{k+1},R_{k+1}$ , we have that $(Q_{k+1},R_{k+1},T_{k+1})\\in\\mathcal{C}_{2}$ , and hence $(Q_{k+1},R_{k+1},T_{k+1})\\;\\in\\;\\mathcal{C}$ . Thus, each iteration produces a feasible triple $(Q_{k+1},R_{k+1},T_{k+1})$ through a pair of locally optimal block updates. ", "page_idx": 15}, {"type": "text", "text": "The LOT optimization problem Scetbon et al. (2021) For comparison, recall the optimization problem that is solved in the LOT framework: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{(Q,R,g)\\in\\tilde{C}}\\mathcal{L}_{\\mathrm{LOT}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the objective function $\\mathcal{L}_{\\mathrm{LOT}}$ is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{LOT}}:=\\langle\\boldsymbol{C},\\boldsymbol{Q}\\mathrm{diag}(1/\\boldsymbol{g})\\boldsymbol{R}^{\\mathrm{T}}\\rangle\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and where $\\widetilde{\\mathcal{C}}=\\widetilde{\\mathcal{C}}_{1}\\cap\\widetilde{\\mathcal{C}}_{2}$ with: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\mathcal{C}}_{1}:=\\{(Q,R,g)\\in\\mathbb{R}_{+}^{n\\times r}\\times\\mathbb{R}_{+}^{m\\times r}\\times(\\mathbb{R}_{+}^{*})^{r}:Q\\mathbf{1}_{r}=\\pmb{a},R\\mathbf{1}_{r}=\\pmb{b}\\}}\\\\ &{\\widetilde{\\mathcal{C}}_{2}:=\\{(Q,R,g)\\in\\mathbb{R}_{+}^{n\\times r}\\times\\mathbb{R}_{+}^{m\\times r}\\times\\mathbb{R}_{+}^{r}:Q^{\\mathrm{T}}\\mathbf{1}_{n}=g=R^{\\mathrm{T}}\\mathbf{1}_{m}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, there are also three optimization variables $(Q,R,g)$ , but they are updated together in each iteration of LOT. That is, given feasible $(Q_{k},R_{k},g_{k})\\in\\mathcal{C}$ , an iteration of LOT updates this triple via ", "page_idx": 15}, {"type": "equation", "text": "$$\nQ_{k+1},R_{k+1},g_{k+1}):=\\operatorname*{\\arg\\operatorname*{min}}_{(Q,R,g)\\in\\tilde{C}}\\langle(Q,R,g),\\nabla_{(Q,R,g)}\\mathcal{L}_{\\mathrm{LOT}}\\rangle+\\frac{1}{\\gamma_{k}}\\mathrm{KL}((Q,R,g)\\|(Q_{k},R_{k},g_{k})),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $(\\gamma_{k})_{k\\geq0}$ is again a positive sequence of stepsizes. Scetbon et al. (2021) then compute the unconstrained argmin across all variables to yield a set of unconstrained kernels $(K_{Q},K_{R},k_{g})$ , using Dykstra to jointly project the unconstrained update onto the intersectionC of the constraint sets. ", "page_idx": 15}, {"type": "text", "text": "OT subroutine in FRLC To see why we do not need Dykstra in the FRLC scheme, observe that the update (15) of variables $(Q,R)$ can be equivalently expressed as ", "page_idx": 15}, {"type": "equation", "text": "$$\n(Q_{k+1},R_{k+1})\\gets\\operatorname*{arg\\,min}_{\\substack{Q,R:\\,(Q,R,T_{k})\\in\\mathcal{C}_{1}}}\\langle(Q,R),\\nabla_{(Q,R)}\\mathcal{L}_{\\mathrm{LC}}\\rangle+\\frac{1}{\\gamma_{k}}\\mathrm{KL}((Q,R)\\|(Q_{k},R_{k})),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, even though the pair $(Q,R)$ is being updated at this step, solving for $(Q_{k+1},R_{k+1})$ above is equivalent to updating each individually because $Q$ and $\\boldsymbol{R}$ do not share an inner marginal: ", "page_idx": 15}, {"type": "equation", "text": "$$\nQ_{k+1}\\gets\\operatorname*{arg\\,min}_{Q:Q{1}_{r}=a}\\langle Q,\\nabla_{Q}\\mathcal{L}_{\\mathrm{LC}}\\rangle+\\frac{1}{\\gamma_{k}}\\mathrm{KL}(Q\\|Q_{k})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{R}_{k+1}\\leftarrow\\underset{\\pmb{R}:\\,\\pmb{R}_{\\pmb{1}_{r}=b}}{\\arg\\operatorname*{min}}\\langle\\pmb{R},\\nabla_{\\pmb{R}}\\mathcal{L}_{\\mathrm{LC}}\\rangle+\\frac{1}{\\gamma_{k}}\\mathrm{KL}(\\pmb{R}\\|\\pmb{R}_{k})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We choose to add the regularization $\\tau\\mathrm{KL}((Q^{\\mathrm{T}}\\mathbf{1}_{n},R^{\\mathrm{T}}\\mathbf{1}_{m})||(Q_{k}^{\\mathrm{T}}\\mathbf{1}_{n},R_{k}^{\\mathrm{T}}\\mathbf{1}_{m}))$ to turn each update here into an entropy-regularized semi-relaxed optimal transport problem, and to ensure $\\beta$ -smoothness: ", "page_idx": 16}, {"type": "equation", "text": "$$\nQ_{k+1}\\gets\\operatorname*{arg\\,min}_{Q:Q_{1}=a}\\langle Q,\\nabla_{Q}\\mathcal{L}_{\\mathrm{LC}}\\rangle+\\frac{1}{\\gamma_{k}}\\mathrm{KL}(Q\\|Q_{k})+\\tau\\mathrm{KL}(Q^{\\mathrm{T}}\\mathbf{1}_{n}\\|Q_{k}^{\\mathrm{T}}\\mathbf{1}_{n})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\nR_{k+1}\\gets\\operatorname*{arg\\,min}_{\\pmb{R}:\\,\\pmb{R}\\mathbf{1}_{r}=\\pmb{b}}\\langle\\pmb{R},\\nabla_{\\pmb{R}}\\mathcal{L}_{\\mathrm{LC}}\\rangle+\\frac{1}{\\gamma_{k}}\\mathrm{KL}(\\pmb{R}\\|\\pmb{R}_{k})+\\tau\\mathrm{KL}(\\pmb{R}^{\\mathrm{T}}\\mathbf{1}_{m}\\|\\pmb{R}_{k}^{\\mathrm{T}}\\mathbf{1}_{m})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "After updating $Q$ and $\\boldsymbol{R}$ , the update on $\\textbf{\\emph{T}}$ then follows a similar form: ", "page_idx": 16}, {"type": "equation", "text": "$$\nT_{k+1}\\gets\\operatorname*{arg\\,min}_{\\pmb{T}:\\,(\\pmb{Q}_{k+1},\\pmb{R}_{k+1},\\pmb{T})\\in\\mathcal{C}_{2}}\\langle\\pmb{T},\\nabla_{\\pmb{T}}\\mathcal{L}_{\\mathrm{LC}}\\rangle+\\frac{1}{\\gamma_{k}}\\mathrm{KL}(\\pmb{T}\\|\\pmb{T}_{k}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "leading to balanced constraints on $_T$ of the form $T^{\\mathrm{T}}\\mathbf{1}_{r}=Q_{k+1}^{\\mathrm{T}}\\mathbf{1}_{n}$ and $\\mathbf{T1}_{r}=R_{k+1}^{\\mathrm{T}}\\mathbf{1}_{m}$ , allowing the problem to be solved by Sinkhorn. ", "page_idx": 16}, {"type": "text", "text": "Importantly, there are no constraints in $\\mathcal{C}_{1}$ involving both $Q$ and $\\boldsymbol{R}$ , which is what allows the optimization to split in this way. If there were such constraints, we would have needed to use Dykstra to update the pair $(Q,R)$ . Because our update scheme is equivalent to the three updates of individual variables given in (21), (22), (23), we can solve for each update using optimal transport. As $Q$ and $\\boldsymbol{R}$ are not required to match the inner marginals exactly, the OT problems associated to $Q$ and $\\boldsymbol{R}$ are semi-relaxed by construction. ", "page_idx": 16}, {"type": "text", "text": "The separation of our block updates into a step where $(Q,R,T_{k})\\in\\mathcal{C}_{1}$ and $(Q_{k+1},R_{k+1},T)\\in\\mathcal{C}_{2}$ allows us to entirely remove the optimization over inner marginals $_{g Q}$ and $\\scriptstyle g_{R}$ , as done in all previous works on low-rank optimal transport which optimize $\\textbf{\\textit{g}}$ explicitly as both a variable and a constraint of the optimization Scetbon et al. (2021, 2023); Scetbon & Cuturi (2022). If one were to introduce an extended loss in the style of previous works which adds $\\scriptstyle g_{Q}$ and $\\scriptstyle g_{R}$ as variables in the form $\\begin{array}{r}{\\mathcal{H}(Q,R,T,g_{Q},g_{R})=\\langle Q\\mathrm{diag}(1/g_{Q})T\\mathrm{diag}(1/g_{R})R^{\\mathrm{T}},C\\rangle_{F}.}\\end{array}$ , one observes an equivalence to simply taking a semi-relaxed projection. ", "page_idx": 16}, {"type": "text", "text": "Lemma A.1. Define the function $\\begin{array}{r}{\\mathcal{H}(Q,R,T,g_{Q},g_{R})=\\langle C,Q\\mathrm{diag}(1/g_{Q})T\\mathrm{diag}(1/g_{R}){\\cal R}^{\\mathrm{T}}\\rangle_{F}}\\end{array}$ and let $\\mathcal{L}_{\\mathrm{LC}}(Q,R,T)$ be as in (13). One has the following equivalence: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{g_{R}\\in\\Delta_{r},g_{Q}\\in\\Delta_{r},Q\\in\\Pi_{a,g_{Q}},R\\in\\Pi_{b,g_{R}}}\\mathcal{H}(Q,R,T_{k},g_{Q},g_{R})=\\operatorname*{min}_{(Q,R,T_{k})\\in\\mathcal{C}_{1}}\\mathcal{L}_{\\mathrm{LC}}(Q,R,T_{k})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus the semi-relaxed projections yield locally optimal inner marginals. ", "page_idx": 16}, {"type": "text", "text": "Proof. To see why this is true, notice that, so long as the outer marginals are tightly satisfied $\\scriptstyle Q\\mathbf{1}_{r}\\;=\\;{\\mathbfit{a}}$ and $R\\mathbf{1}_{r}=b$ for ${\\cal Q}\\ge\\mathbf{0}_{n\\times r},R\\ge\\mathbf{0}_{m\\times r}$ , one has ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i}g_{R,i}=\\sum_{i}\\langle{\\pmb R}_{i,.}^{\\mathrm{T}},{\\pmb1}_{m}\\rangle=\\sum_{i}\\langle{\\pmb R}_{.,i},{\\pmb1}_{m}\\rangle=\\sum_{\\ell}\\sum_{i}R_{\\ell,i}=\\sum_{\\ell}b_{\\ell}=1\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i}g_{Q,i}=\\sum_{i}\\langle Q_{i,\\,,}^{\\mathrm{T}},\\mathbf{1}_{n}\\rangle=\\sum_{i}\\langle Q_{\\cdot,i},\\mathbf{1}_{m}\\rangle=\\sum_{\\ell}\\sum_{i}Q_{\\ell,i}=\\sum_{\\ell}a_{\\ell}=1\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, the inner marginals ${\\pmb g}_{Q}^{*}=({\\pmb Q}^{*})^{\\mathrm{T}}{\\bf1}_{n}$ and ${\\pmb g}_{R}^{*}=({\\pmb R}^{*})^{\\mathrm{T}}{\\bf1}_{m}$ induced by the optimal $Q^{*},R^{*}$ of the optimization problem on the right hand side satisfy the constraints $\\pmb{g}_{R}^{*}\\in\\Delta_{r},\\pmb{g}_{Q}^{*}\\in\\Delta_{r}$ on the left hand side, so the two minimums coincide. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "This implies that an extra optimization for $_{g Q}$ and $\\scriptstyle g_{R}$ is unnecessary in a coordinate update which alternates $(Q,R)$ and $\\textbf{\\emph{T}}$ . If we did not perform a block-update, in the form of standard MD in the objective described above, we would encounter some difficulty. In particular $_{g Q}$ and $\\scriptstyle g_{R}$ would optimized with the constraint $g_{Q}\\in\\Delta_{r}$ and ${\\pmb g}_{R}\\in\\Delta_{r}$ , and would concurrently constrain all of the other variables as $Q^{\\mathrm{T}}\\mathbf{1}_{n}=g_{Q}$ , $T\\mathbf{1}_{r}=g_{Q}$ , and $\\pmb{R}^{\\mathrm{T}}\\mathbf{1}_{m}=\\pmb{g}_{R},\\pmb{T}^{\\mathrm{T}}\\mathbf{1}_{r}=\\pmb{g}_{R}$ . ", "page_idx": 16}, {"type": "text", "text": "B Block-Coordinate steps for the OT sub-problems ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We use a latent non-diagonal coupling instead of an inner diagonal coupling $\\mathrm{diag}(g)$ of the form of Forrow et al. (2019). This allows us to loosen the constraint that the inner marginals have to be joined by a common coupling $Q^{\\mathrm{T}}\\mathbf{1}_{n}=R^{\\mathrm{T}}\\mathbf{1}_{m}=g$ . The fundamental advantage of this choice is that we can decouple the convex-optimization problem for $(Q,R,T)$ entirely. One can simply solve for the optimal $Q$ and $\\boldsymbol{R}$ independently, yield the associated inner marginals for each $Q^{\\mathrm{T}}\\mathbf{i}_{n}^{\\mathsf{\\Delta}}=g_{Q}$ and $\\pmb{R}^{\\mathrm{T}}\\mathbf{1}_{m}=\\pmb{g}_{R}$ , and then find the optimal $\\textbf{\\emph{T}}$ which links the two. This link is provided by the aforementioned form of the problem, where: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\boldsymbol{P}=\\boldsymbol{Q}\\boldsymbol{X}\\boldsymbol{R}^{\\mathrm{T}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For $Q,R$ in either the appropriate set of couplings or a relaxation thereof (which we will describe shortly). $\\mathbf{\\deltaX}$ is related to $\\textbf{\\emph{T}}$ by: ", "page_idx": 17}, {"type": "equation", "text": "$$\nX=\\operatorname{diag}(1/g_{Q})T\\operatorname{diag}(1/g_{R})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "And, $T\\,\\in\\,\\Pi_{g_{Q},g_{R}}$ consistently for all cases. As the semi-relaxed case is intermediate between fully-relaxed and balanced, it has ideas which generalize to both directly. As such, we use it as the leading example again. As in Scetbon et al. (2021), we take proximal-steps of the form: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\zeta}\\langle\\nabla\\mathcal{L}(\\zeta)\\;|_{\\zeta_{k}},\\zeta\\rangle_{F}+\\frac{1}{\\gamma_{k}}\\mathrm{KL}(\\zeta\\|\\pmb{K}^{(k)})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Where these steps are now in block-wise fashion on $(Q,R)$ and $\\textbf{\\emph{T}}$ , rather than joint. One may identify for each sub-factor in $(Q,R)$ and $_T$ a linearized gradient as before, which yields a set of objectives which each solve an independent optimal-transport for the sub-factors. In particular, we have that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle Q X R^{\\mathrm{T}},C\\rangle_{F}=\\operatorname{Tr}\\left[Q X R^{\\mathrm{T}}C^{\\mathrm{T}}\\right]=\\langle C R X^{\\mathrm{T}},Q\\rangle_{F}}\\\\ {\\langle Q X R^{\\mathrm{T}},C\\rangle_{F}=\\operatorname{Tr}\\left[Q X R^{\\mathrm{T}}C^{\\mathrm{T}}\\right]=\\langle C^{\\mathrm{T}}Q X,R\\rangle_{F}}\\\\ {\\langle Q X R^{\\mathrm{T}},C\\rangle_{F}=\\operatorname{Tr}\\left[R^{\\mathrm{T}}C^{\\mathrm{T}}Q X\\right]=\\langle Q^{\\mathrm{T}}C R,X\\rangle_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A linearization in the left-slot of the inner product as $\\langle Q,C R X(Q_{k})^{\\mathrm{T}}\\rangle\\;:=\\;\\langle Q,C R X^{\\mathrm{T}}\\rangle$ or $\\langle{\\cal C}^{\\mathrm{T}}{\\cal Q}{\\cal X}({\\cal R}_{k}),{\\cal R}\\rangle_{F}:=\\langle{\\cal C}^{\\mathrm{T}}{\\cal Q}{\\cal X},{\\cal R}\\rangle_{F}$ is common practice for quadratic problems. In this case, the directional derivative of $Q$ and $\\boldsymbol{R}$ in the matrix-direction $V$ are respectively: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D\\langle{C R X^{\\mathrm{T}},Q}\\rangle_{F}\\circ(V)=\\langle{C R X^{\\mathrm{T}},V}\\rangle_{F}\\implies\\nabla_{Q}\\mathcal{L}=C R X^{\\mathrm{T}}}\\\\ &{D\\langle{C^{\\mathrm{T}}Q X,R}\\rangle_{F}\\circ(V)=\\langle{C^{\\mathrm{T}}Q X,V}\\rangle_{F}\\implies\\nabla_{R}\\mathcal{L}=C^{\\mathrm{T}}Q X}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Without this linearization assumption on $\\mathbf{\\deltaX}$ , the full gradient may be evaluated as well. In particular, we note that for $\\mathrm{diag^{-1}(\\cdot)}$ the matrix-to-vector extraction of the diagonal, the directional derivative on $Q$ is: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D\\langle{\\boldsymbol C},Q X{\\boldsymbol R}^{\\mathrm{T}}\\rangle_{F}\\circ{\\boldsymbol V}=\\langle{\\boldsymbol C}R X^{\\mathrm{T}},{\\boldsymbol V}\\rangle_{F}+\\langle{\\boldsymbol C},Q D X^{\\mathrm{T}}\\circ(V){\\boldsymbol R}^{\\mathrm{T}}\\rangle_{F}}\\\\ &{\\qquad\\qquad\\qquad=\\langle{\\boldsymbol C}R X^{\\mathrm{T}}-\\mathbf{1}_{n}\\mathrm{diag}^{-1}(({\\boldsymbol C}R X^{\\mathrm{T}})^{\\mathrm{T}}Q\\mathrm{diag}(1/g_{Q}))^{\\mathrm{T}},{\\boldsymbol V}\\rangle_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, without the linearization assumption one may use product rule on $\\mathbf{\\deltaX}$ as an implicit function of $Q$ (resp. $\\boldsymbol{R}$ ) to take the total derivative: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla_{Q}\\mathcal{L}_{\\mathrm{FRLC}}=C R X^{\\mathrm{T}}-\\mathbf{1}_{n}\\mathrm{diag}^{-1}((C R X^{\\mathrm{T}})^{\\mathrm{T}}Q\\mathrm{diag}(1/g_{Q}))^{\\mathrm{T}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Likewise for $\\boldsymbol{R}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D\\langle C,Q X{\\cal R}^{\\mathrm{T}}\\rangle_{F}\\circ V=\\langle C^{\\mathrm{T}}Q X,V\\rangle_{F}+\\langle C,Q D X\\circ(V){\\cal R}^{\\mathrm{T}}\\rangle_{F}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\langle C^{\\mathrm{T}}Q X-\\mathbf{1}_{m}\\mathrm{diag}^{-1}(\\mathrm{diag}(1/g_{R}){\\cal R}^{\\mathrm{T}}C^{\\mathrm{T}}Q X)^{\\mathrm{T}},V\\rangle_{F},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and so ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{R}\\mathcal{L}_{\\mathrm{FRLC}}=C^{\\mathrm{T}}Q X-\\mathbf{1}_{m}\\mathrm{diag}^{-1}(\\mathrm{diag}(1/g_{R})R^{\\mathrm{T}}C^{\\mathrm{T}}Q X)^{\\mathrm{T}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Both the $\\mathrm{W}$ and GW problem have these rank-one perturbations of the gradient from the derivative with respect to $\\mathbf{\\deltaX}$ . ", "page_idx": 17}, {"type": "text", "text": "Lastly, owing to the block-coordinate updates which fix $(Q,R)$ preceding the update on $\\textbf{\\emph{T}}$ , the derivative on $_T$ follows directly by chain rule on $\\mathbf{\\deltaX}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D\\langle Q^{\\mathrm{T}}C R,X\\rangle_{F}\\circ(V)=\\langle Q^{\\mathrm{T}}C R,V\\rangle_{F}\\implies\\nabla_{X}\\mathcal{L}=Q^{\\mathrm{T}}C R}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\nabla_{T}\\mathcal{L}=\\operatorname{diag}(1/g_{Q})Q^{\\mathrm{T}}C R\\operatorname{diag}(1/g_{R})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $\\left(\\gamma_{k}\\right)$ be a sequence of step sizes and consider the first-order conditions required for the proximal step as before. From these we have the updated proximal-step updates: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K_{Q}^{(k)}\\gets\\underset{Q}{\\mathrm{min}}\\langle C R X^{\\mathrm{T}},Q\\rangle_{F}+\\frac{1}{\\gamma_{k}}\\mathrm{KL}(Q_{k}\\|K_{Q}^{(k)})}\\\\ &{K_{R}^{(k)}\\gets\\underset{R}{\\mathrm{min}}\\langle C^{\\mathrm{T}}Q X,R\\rangle_{F}+\\frac{1}{\\gamma_{k}}\\mathrm{KL}(R_{k}\\|K_{R}^{(k)})}\\\\ &{K_{T}^{(k)}\\gets\\underset{T}{\\mathrm{min}}\\langle Q^{\\mathrm{T}}C R,X\\rangle_{F}+\\frac{1}{\\gamma_{k}}\\mathrm{KL}(T_{k}\\|K_{T}^{(k)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with kernels $K_{\\zeta_{j}}^{(k)}$ , for $j=1,2,3$ given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K_{Q}^{(k)}:=Q_{k}\\odot\\exp(-\\gamma_{k}C R_{k}X_{k}^{\\mathrm{T}})}\\\\ &{K_{R}^{(k)}:=R_{k}\\odot\\exp(-\\gamma_{k}C^{\\mathrm{T}}Q_{k}X_{k}\\,)}\\\\ &{K_{T}^{(k)}:=\\mathbf T_{k}\\odot\\exp(-\\gamma_{k}\\mathrm{diag}(g_{Q}^{-1})Q^{\\mathrm{T}}C R\\mathrm{diag}(g_{R}^{-1})\\,)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Or, dropping the linearization assumption on $(Q,R)$ the updates are: $K_{\\zeta_{j}}^{(k)}$ , for $j=1,2$ given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K_{Q}^{(k)}:=Q_{k}\\odot\\exp(-\\gamma_{k}(C R_{k}X_{k}^{\\mathrm{T}}-\\mathbf1_{n}\\mathrm{diag}^{-1}((C R_{k}X_{k}^{\\mathrm{T}})^{\\mathrm{T}}Q_{k}\\mathrm{diag}(1/g_{Q_{k}}))^{\\mathrm{T}}))}\\\\ &{K_{R}^{(k)}:=R_{k}\\odot\\exp(-\\gamma_{k}(C^{\\mathrm{T}}Q_{k}X_{k}-\\mathbf1_{m}\\mathrm{diag}^{-1}(\\mathrm{diag}(1/g_{R})R_{k}^{\\mathrm{T}}C^{\\mathrm{T}}Q_{k}X_{k})^{\\mathrm{T}}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The first projection is onto the set that satisfies the marginal constraint $R\\mathbf{1}_{r}\\;=\\;b$ . In particular, following the discussion above, one has the coordinate-MD step: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{(Q,R,T)}{\\operatorname*{min}}}&{{}\\quad\\frac{1}{\\gamma_{k}}\\mathrm{KL}\\left((Q,R,T)\\,\\|\\,(K_{Q},K_{R},K_{T})\\right)+\\tau\\mathrm{KL}(Q\\mathbf{1}_{r}\\|\\boldsymbol{a})}\\\\ {s.t.}&{{}\\quad R\\mathbf{1}_{r}=\\boldsymbol{b}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As before, there is no difference from the previous case, where one takes the unconstrained projection $R=\\mathrm{diag}(b/K_{R}{\\bf1}_{r})K_{R}$ . To generalize this, we also consider adding a soft-constraint on the inner marginal of $\\boldsymbol{R}$ to be near that of the previous iteration. In particular, we consider the problem: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{(Q,R,T)}{\\operatorname*{min}}\\quad}&{\\frac{1}{\\gamma_{k}}\\mathrm{KL}((Q,R,T)\\|(K_{Q},K_{R},K_{T}))}\\\\ &{\\qquad+\\tau\\mathrm{KL}(Q\\mathbf{1}_{r}\\|\\boldsymbol{a})+\\tau\\mathrm{KL}(R^{\\mathrm{T}}\\mathbf{1}_{m}\\|g_{R}^{(k-1)}\\!\\equiv R_{k-1}^{\\mathrm{T}}\\mathbf{1}_{m})}\\\\ {s.t.\\quad}&{R\\mathbf{1}_{r}=b}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Which yields the relaxed solution of $\\pmb{R}=\\mathrm{SR}^{\\mathrm{R}}$ -projection $(K_{R},\\gamma_{k},\\tau,b,g_{R}^{(k-1)})$ which generalizes the original projection and is equivalent to it for . This regularization is essential, as it ensures $\\beta$ -smoothness of the objective. For $Q$ , as the constraint on $\\textbf{\\textit{g}}$ is fully relaxed, the Lagrange multiplier $\\lambda_{1}=\\mathbf{0}$ entirely, such that the problem 40 now becomes fully-unconstrained: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{Q}\\left(\\frac{1}{\\gamma_{k}}\\mathrm{KL}(Q\\|K_{Q})+\\tau\\mathrm{KL}(Q\\mathbf{1}_{r}\\|a)\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To generalize this solution, we again consider adding a soft-regularization on the inner marginal of $Q$ , where we consider the alternate problem: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{Q}\\left(\\frac{1}{\\gamma_{k}}\\mathrm{KL}(Q\\|K_{Q})+\\tau\\mathrm{KL}(Q\\mathbf{1}_{r}\\|\\boldsymbol{a})+\\tau\\mathrm{KL}(Q^{\\mathrm{T}}\\mathbf{1}_{n}\\|g_{Q}^{(k-1)}\\ @Q_{k-1}^{\\mathrm{T}}\\mathbf{1}_{n})\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Which trivially recovers the original for $\\tau=0$ . This form has a solution given by an unbalanced optimal transport with kernel $K_{Q}$ . We see these two, convex problems in sequence give independent optimal solutions for $Q$ and $\\boldsymbol{R}$ as the two matrices are not required to share an inner marginal. The last step is to link them via $_T$ , corresponding to the projection of $(Q_{k},R_{k},T)$ onto the set of valid rank- $r$ couplings $\\begin{array}{r}{\\operatorname*{min}_{(Q_{k},R_{k},T)\\in{\\cal C}}\\mathcal{L}_{\\mathrm{LC}}(Q_{k},R_{k},T):=\\operatorname*{min}_{T\\in\\Pi(g_{Q},g_{R})}\\mathcal{L}_{\\mathrm{LC}}(Q_{k},R_{k},T)}\\end{array}$ We verify in C that if $\\begin{array}{r}{T\\in\\Pi_{g_{Q}=Q^{\\mathrm{T}}\\mathbf{1}_{n},g_{R}=R^{\\mathrm{T}}\\mathbf{1}_{m}}}\\end{array}$ then $P\\in\\Pi_{a,b}$ . As such, one does not require any projection onto the intersection of convex sets, as done in Scetbon et al. (2021, 2023) via the Dykstra projection algorithm Dykstra (1983). Alternating a coordinate-MD step in $(Q,R)$ and a step on $_T$ , one not only minimizes the objective in an alternating fashion but remains in the feasible set without the need for projection algorithms beyond Sinkhorn. As such, the final linking step is done via: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{T}{\\operatorname*{min}}\\quad}&{\\frac{1}{\\gamma_{k}}\\mathrm{KL}(T\\|K_{T})}\\\\ {s.t.\\quad}&{T\\mathbf{1}_{r}=g_{Q},T^{\\mathrm{T}}\\mathbf{1}_{r}=g_{R}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This formulation amounts to solving a balanced Sinkhorn problem on $_T$ with respect to the proximal step kernel matrix. ", "page_idx": 19}, {"type": "table", "img_path": "hGgkdFF2hR/tmp/30fd644d0b9c1f9cdfbfe9962a7549f1098e6c2f69b421463127ffc53e79c68d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "hGgkdFF2hR/tmp/cc9d01f6a3a5f43cabf7b96fd13fb1b5aa3ebf95c1f15bfa7bce1e177e69a36b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "C The Latent Coupling Matrix ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To solve the balanced form (and generalize the principle to the relaxed problems), we consider an alternative parametrization of the inner matrix. In particular, previous works Scetbon et al. (2021, 2023) consider $\\operatorname{diag}(1/9)$ to be the inner matrix, with marginals $Q^{\\mathrm{T}}\\mathbf{1}_{n}=R^{\\mathrm{T}}\\mathbf{1}_{m}=g$ to ensure that the outer conditions $\\scriptstyle Q\\mathbf{1}_{r}\\;=\\;{\\mathbfit{a}}$ and $\\scriptstyle R\\mathbf{1}_{r}\\;=\\;b$ hold. We instead relax the constraint that $Q^{\\mathrm{T}}\\mathbf{1}_{n}$ and ${\\cal R}^{\\mathrm{T}}\\mathbf{1}_{m}$ are equal, by allowing $Q^{\\mathrm{T}}\\mathbf{1}_{n}=g_{Q}$ and ${\\pmb R}^{\\mathrm{T}}{\\bf1}_{m}={\\pmb g}_{R}$ to vary arbitrarily, and considering a non-diagonal inner matrix $\\boldsymbol{X}\\in\\mathbb{R}^{r\\times r}$ in the place of $\\operatorname{diag}(1/g)$ where we have the conditions: ", "page_idx": 19}, {"type": "equation", "text": "$$\nX g_{R}=X R^{\\mathrm{T}}\\mathbf{1}_{m}=\\mathbf{1}_{r}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "And ", "page_idx": 19}, {"type": "equation", "text": "$$\nX^{\\mathrm{T}}g_{Q}=X Q^{\\mathrm{T}}\\mathbf{1}_{n}=\\mathbf{1}_{r}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, if one considers the coupling formed as $\\pmb{P}_{r}=\\pmb{Q}\\pmb{X}\\pmb{R}^{\\mathrm{T}}$ , one maintains that $P_{r}\\in\\Pi_{a,b}$ from the condition of $\\mathcal{C}_{1}$ , defined in the balanced case as in (14). We clearly have that: ", "page_idx": 20}, {"type": "equation", "text": "$$\nP_{r}\\mathbf{1}_{m}=Q X R^{\\mathrm{T}}\\mathbf{1}_{m}=Q\\mathbf{1}_{r}=\\pmb{a}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "And: ", "page_idx": 20}, {"type": "equation", "text": "$$\nP_{r}^{\\mathrm{T}}\\mathbf{1}_{n}=R X^{\\mathrm{T}}Q\\mathbf{1}_{n}=R\\mathbf{1}_{r}=b\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We first consider two approaches to optimizing for such a $\\mathbf{\\deltaX}$ , given that it is not a coupling. First, we consider the appropriate proximal step for $\\mathbf{\\deltaX}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\zeta}\\langle\\nabla\\mathcal{L}(\\zeta)\\;|_{\\xi_{k}},\\zeta\\rangle_{F}+\\frac{1}{\\gamma_{k}}\\mathrm{KL}(\\zeta\\|K^{(k)})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We first note that the gradient of our loss with respect to $\\mathbf{\\deltaX}$ is given as $Q^{\\mathrm{T}}C R$ . If one supposes that $\\mathbf{\\deltaX}$ is invertible with $\\mathbf{\\bar{\\boldsymbol{X}}}^{-1}=\\mathbf{\\boldsymbol{T}}$ , we have that for any such $\\textbf{\\emph{T}}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\nX^{-1}\\mathbf{1}_{r}=T\\mathbf{1}_{r}=g_{R}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "And ", "page_idx": 20}, {"type": "equation", "text": "$$\nX^{-T}\\mathbf{1}_{r}=T^{\\mathrm{T}}\\mathbf{1}_{r}=\\pmb{g}_{Q}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This implies that this inverse matrix $\\textbf{\\emph{T}}$ is a coupling such that $\\pmb{T}\\in\\Pi_{\\pmb{g}_{R},\\pmb{g}_{Q}}$ , which also suggests one might be able to update it using Sinkhorn. In fact, being a density in $\\mathbb{R}_{+}^{r\\times r}$ it represents a transition matrix between the latent $r$ -dimensional variables. In particular, writing the proximal step in full, we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{T}\\langle Q^{\\mathrm{T}}C R,T^{-1}\\rangle_{F}+\\frac{1}{\\gamma_{k}}\\mathrm{KL}(T_{k}\\|K_{T}^{(k)})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Noting the derivative $D(X^{-1})\\circ V=-X^{-1}V X^{-1}$ , we have from the first-order condition that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n-T^{-T}{\\pmb Q}^{\\mathrm{T}}C{\\pmb R}{\\pmb T}^{-T}+\\frac{1}{\\gamma_{k}}\\log\\left[\\frac{{\\pmb T}_{k}}{K_{T}^{(k)}}\\right]={\\bf0}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This implies the kernel matrix update: ", "page_idx": 20}, {"type": "equation", "text": "$$\nK_{T}^{(k)}=T_{k}\\odot\\exp\\{+\\gamma_{k}T^{-T}Q^{\\mathrm{T}}C R T^{-T}\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Where one then takes the Sinkhorn projection J onto the set $\\Pi_{g_{R},g_{Q}}$ as $\\mathcal{P}_{\\Pi_{g_{R},g_{Q}}}(K_{T}^{(k)})$ using the Sinkhorn algorithm Cuturi (2013b). However, a more stable and inversion-free update exists which ensures $\\mathbf{\\deltaX}$ remains positive by a diagonal re-scaling in the form introduced by Lin et al. (2021). In particular, if one takes $X=\\operatorname{diag}(1/\\mathbf{\\bar{g}}_{Q})T\\operatorname{diag}(1/\\mathbf{\\bar{g}}_{R})$ , then ", "page_idx": 20}, {"type": "equation", "text": "$$\nX g_{R}=\\mathrm{diag}(1/g_{Q})T\\,\\mathrm{diag}(1/g_{R})g_{R}=\\mathrm{diag}(1/g_{Q})T{\\bf1}_{r}={\\bf1}_{r}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and likewise ", "page_idx": 20}, {"type": "equation", "text": "$$\nX^{\\mathrm{T}}g_{Q}=\\operatorname{diag}(1/g_{R}){\\mathbf{T}}^{\\mathrm{T}}\\operatorname{diag}(1/g_{Q})g_{Q}=\\operatorname{diag}(1/g_{R}){\\mathbf{T}}^{\\mathrm{T}}\\mathbf{1}_{r}=\\mathbf{1}_{r}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "so that $\\mathbf{\\deltaX}$ necessarily satisfies ${\\cal X}g_{\\cal R}={\\bf1}_{r}$ and $X^{\\mathrm{T}}g_{Q}=\\mathbf{1}_{r}$ . Thus $T\\mathbf{1}_{r}=g_{Q}$ and $\\pmb{T}^{\\mathrm{T}}\\mathbf{1}_{r}=\\pmb{g}_{R}$ and $\\pmb{T}\\in\\Pi_{\\pmb{g}_{Q},\\pmb{g}_{R}}$ . With analogous reasoning to before, one has a step for the coupling $\\textbf{\\emph{T}}$ in the form: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{T}\\langle Q^{\\mathrm{T}}C R,\\mathrm{diag}(1/g_{Q})T\\,\\mathrm{diag}(1/g_{R})\\rangle_{F}+\\frac{1}{\\gamma_{k}}\\mathrm{KL}(T_{k}\\|K_{T}^{(k)})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Which yields the kernel matrix: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{K_{T}^{(k)}=T_{k}\\odot\\exp\\{-\\gamma_{k}\\operatorname{diag}(g_{Q})^{-1}Q^{\\mathrm{T}}C R\\operatorname{diag}(g_{R})^{-1}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Which is likewise projected onto $\\Pi_{g_{Q},g_{R}}$ using the Sinkhorn algorithm. From this $T\\in\\Pi_{g_{Q},g_{R}}$ , one takes ${\\pmb X}=\\mathrm{diag}(1/{\\pmb g}_{Q}){\\pmb T}\\,\\mathrm{diag}(1/{\\pmb g}_{R})$ as the inner matrix that corresponds the unequal marginals $_{g Q}$ and $\\scriptstyle g_{R}$ which ensuring $P_{r}\\in\\Pi_{a,b}$ . ", "page_idx": 20}, {"type": "text", "text": "Initialize gQ,2 gR = r11r2, r12 1r2 Input $C,r,r_{2},a,b,\\tau,\\tau_{2},\\gamma,\\delta,\\varepsilon$   \nQ0, R0, T0 \u2190Initialize-Couplings(a, b, gQ, gR)   \nif $r=r_{2}$ then $\\bar{X_{0}}\\overset{=}\\leftarrow T_{0}^{-1}$ # Invertible case   \nelse $X_{0}\\gets\\mathrm{diag}(1/Q_{0}^{\\mathrm{T}}\\mathbf{1}_{n})T_{0}\\,\\mathrm{diag}(1/R_{0}^{\\mathrm{T}}\\mathbf{1}_{m})$ # General case   \nend if   \nwhile $\\Delta((Q_{k},R_{k},T_{k}),(Q_{k-1},R_{k-1},T_{k-1}))>\\varepsilon\\,\\epsilon$ do $\\nabla_{Q}\\gets C R_{k}{\\boldsymbol{X}}_{k}^{\\mathrm{T}}-\\mathbf{1}_{n}\\mathrm{diag}^{-1}((C R_{k}\\boldsymbol{X}_{k}^{\\mathrm{T}})^{\\mathrm{T}}Q_{k}\\mathrm{diag}(1/g_{Q}))^{\\mathrm{T}}$ $\\nabla_{R}\\gets C^{\\mathrm{T}}Q_{k}\\mathbf{X}_{k}-\\mathbf{1}_{m}\\mathrm{diag}^{-1}(\\mathrm{diag}(1/g_{R})R_{k}^{\\mathrm{T}}C^{\\mathrm{T}}Q_{k}\\mathbf{X}_{k})^{\\mathrm{T}}$ $\\gamma_{k}\\leftarrow\\gamma/\\operatorname*{max}\\{\\|\\nabla_{\\b{Q}}\\|_{\\infty},\\|\\nabla_{\\b{R}}\\|_{\\infty}\\}\\quad\\#\\ \\ell^{\\infty}\\mathrm{{-norma}}$ Semi-relaxed OT $K_{Q}^{(k)},K_{R}^{(k)}\\gets Q_{k}\\odot\\exp(-\\gamma_{k}\\nabla_{Q}\\,),R_{k}\\odot\\exp(-\\gamma_{k}\\nabla_{R}\\,)$ $\\begin{array}{r l}{\\lefteqn{Q_{k}\\leftarrow\\mathrm{SR}^{\\mathrm{R}}\\ \u2013p r o j e c t i o n({K}_{Q}^{(k)},\\gamma_{k},\\tau,\\mathbf{a},Q_{k-1}^{\\mathrm{T}}\\mathbf{1}_{n},\\delta)}\\quad}&{{}}\\end{array}$ $\\begin{array}{r l}&{\\pmb{\\mathscr{A}}_{k}\\gets\\mathrm{~ont~}\\cdot\\pmb{\\mathscr{p}}^{\\prime\\prime\\sigma g e c t i o n(\\pmb{\\mathscr{A}}_{Q}\\textit{,\\div}\\gamma_{k},\\cdot\\pmb{\\mathscr{\\tau}},\\pmb{\\mathscr{u}},\\pmb{\\mathscr{C}}_{k-1}\\pmb{\\mathscr{\\tau}}_{n},\\sigma)}}\\\\ &{\\pmb{R}_{k}\\gets\\mathrm{~SR}^{\\mathrm{R}}\\ \u2013\\b{\\mathscr{p}}r o j e c t i o n(\\pmb{K}_{R}^{(k)},\\gamma_{k},\\tau,b,\\pmb{R}_{k-1}^{\\mathrm{T}}\\pmb{1}_{m},\\delta)}\\end{array}$ # Semi-relaxed OT els $\\widetilde{Q}_{k}\\gets\\mathrm{U}\\!-\\!p r o j e c t i o n(K_{Q}^{(k)},\\gamma_{k},\\tau,a,Q_{k-1}^{\\mathrm{T}}\\mathbf{1}_{n},\\delta)$ # Unbalanced OT $R_{k}\\leftarrow\\mathrm{U}\\ \u2013p r o j e c t i o n(K_{R}^{(k)},\\gamma_{k},\\tau_{2},b,R_{k-1}^{\\mathrm{T}}\\mathbf{1}_{m},\\delta)$ # Unbalanced OT else if Semi-Relaxed Left then $Q_{k}\\gets\\mathrm{U}_{\\mathbf{-}p r o j e c t i o n}(K_{Q}^{(k)},\\gamma_{k},\\tau,\\mathbf{a},Q_{k-1}^{\\mathrm{T}}\\mathbf{1}_{n},\\delta)$ # Unbalanced OT els $R_{k}\\leftarrow\\mathrm{SR}^{\\mathrm{R}}.p r o j e c t i o n(\\dot{\\pmb{K}}_{R}^{(k)},\\gamma_{k},\\tau,b,\\pmb{R}_{k-1}^{\\mathrm{T}}\\pmb{1}_{m},\\delta)$ # Semi-relaxed OT ${Q_{k}}\\gets\\mathrm{{SR}}^{\\mathrm{{R}}}.\\mathrm{{projection}}({K_{Q}^{(k)},\\gamma_{k},\\tau,a,Q_{k-1}^{\\mathrm{{T}}}\\mathbf{1}_{n},\\delta})$ K(Qk ), \u03b3k, \u03c4, a, QkT\u221211n, \u03b4) # Semi-relaxed OT end Rifk \u2190U-projection $(K_{R}^{(k)},\\gamma_{k},\\tau,b,R_{k-1}^{\\mathrm{T}}\\mathbf{1}_{m},\\delta)$ # Unbalanced OT $g_{Q},g_{R}\\leftarrow Q_{k}^{\\mathrm{T}}\\mathbf{1}_{n},R_{k}^{\\mathrm{T}}\\mathbf{1}_{m}$ $\\nabla_{\\mathbf{\\calT}}=\\mathrm{diag}(g_{Q})^{-1}Q_{k}^{\\mathrm{T}}C R_{k}\\,\\mathrm{diag}(g_{R})^{-1}$ $\\gamma_{T}=\\gamma/\\|\\nabla_{T}\\|_{\\infty}$ # \u2113\u221e-normalization $K_{T}^{(k)}\\gets T_{k}\\odot\\exp\\{-\\gamma_{T}\\nabla_{T}\\}$ $T_{k}\\gets S i n k h o r n(K_{T}^{(k)},g_{R},g_{Q},\\delta)$ # Balanced OT $X_{k}\\leftarrow\\mathrm{diag}(1/g_{Q})T\\,\\mathrm{diag}(1/g_{R})$   \nend while   \nReturn $\\boldsymbol{P_{r}}=\\boldsymbol{Q}\\boldsymbol{X}\\boldsymbol{R}^{\\mathrm{T}}$ ", "page_idx": 21}, {"type": "text", "text": "Algorithm 5 Sinkhorn Algorithm (Cuturi (2013b), balanced OT) ", "page_idx": 21}, {"type": "text", "text": "Input K, a, b, \u03b4   \nu \u21901n   \nv \u21901m   \nwhile $\\begin{array}{r l}&{\\|\\mathrm{diag}(\\boldsymbol{u})\\boldsymbol{K}\\boldsymbol{v}-\\boldsymbol{a}\\|_{1}+\\|\\mathrm{diag}(\\boldsymbol{v})\\boldsymbol{K}^{\\mathrm{T}}\\boldsymbol{u}-\\boldsymbol{b}\\|_{1}>\\delta\\,\\mathbf{do}}\\\\ &{\\ l+1)\\,\\leftarrow\\,\\boldsymbol{a}/\\boldsymbol{K}\\boldsymbol{v}^{(l)}}\\\\ &{\\ l+1)\\,\\leftarrow\\,\\boldsymbol{b}/\\boldsymbol{K}^{\\mathrm{T}}\\boldsymbol{u}^{(l+1)}}\\end{array}$ u   \nend while   \nReturn $\\mathrm{diag}(\\boldsymbol{\\mathfrak{u}})K\\,\\mathrm{diag}(\\boldsymbol{\\mathfrak{v}})$ ", "page_idx": 21}, {"type": "text", "text": "D Gromov-Wasserstein (GW) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "As defined in, the general Gromov-Wasserstein problem concerns a minimization of the energy: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{Q}_{A,B}(P)=\\sum_{i,j,k,l}(A_{i k}-B_{j l})^{2}P_{i j}P_{k l}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Where the minimization is over the set of all couplings $\\Pi_{a,b}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{GW}(\\mu,\\nu):=\\operatorname*{min}_{P\\in\\Pi_{a,b}}\\mathcal{Q}_{A,B}(P)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We consider extending the semi-relaxed framework to the GW problem under the low-rank restriction on $_{P}$ . This extension has thus far been considered for the balanced and unbalanced case in two previous works Scetbon et al. (2023, 2022). In both of these works, the algorithms for the Wasserstein case extend trivially to the Gromov-Wasserstein (GW) problem. In particular, each kernel with variable $\\zeta$ has an update of the form $K\\gets\\zeta\\odot\\exp\\left(-\\gamma_{k}\\nabla_{\\zeta}\\mathcal{L}(\\zeta)\\right)$ for ${\\mathcal{L}}(\\zeta)$ heretofore taken to be the Wasserstein loss $\\langle P(\\zeta),C\\rangle_{F}$ where the coupling $\\boldsymbol{P}=\\boldsymbol{Q}\\boldsymbol{X}\\boldsymbol{R}^{\\mathrm{T}}$ is interpreted as a function of the low-rank sub-factor variables $\\zeta\\in\\{Q,R,X\\}$ . Taking $\\mathcal{L}:=\\mathcal{Q}_{A,B}(P\\bar{(}\\zeta))$ one can simply extend the gradient through the GW-loss and directly use it in place of the Wasserstein gradient in the update $K\\gets\\zeta\\odot\\exp\\left(-\\gamma_{k}\\nabla_{\\zeta}\\mathcal{L}(\\zeta)\\right)$ for ${\\mathcal{L}}(\\zeta)$ of each algorithm. The matrix-form of the GW-cost is expressed as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{Q}_{A,B}(P)=\\mathbf{1}_{m}^{\\mathrm{T}}P^{\\mathrm{T}}A^{\\mathrm{G2}}P\\mathbf{1}_{m}+\\mathbf{1}_{n}^{\\mathrm{T}}P B^{\\mathrm{G2}}P^{\\mathrm{T}}\\mathbf{1}_{n}-2\\langle A P B,P\\rangle\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Which, using the constraints of $\\mathcal{C}_{2}$ reduces the cost as a function of $Q,R,X$ to: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{Q}_{A,B}(Q,R,X)=\\mathbf{1}_{r}^{\\mathrm{T}}Q^{\\mathrm{T}}A^{\\odot2}Q\\mathbf{1}_{r}+\\mathbf{1}_{r}^{\\mathrm{T}}R^{\\mathrm{T}}B^{\\odot2}R\\mathbf{1}_{r}-2\\langle Q X R^{\\mathrm{T}},A Q X R^{\\mathrm{T}}B\\rangle_{F}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{Q}\\mathcal{Q}_{A,B}(Q,R,X)=2A^{\\odot2}Q{\\bf1}_{r}{\\bf1}_{r}^{\\mathrm{T}}-4A Q X R^{\\mathrm{T}}B R X^{\\mathrm{T}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Which is proportional to $\\nabla_{Q}\\mathcal{Q}_{A,B}(Q,R,X)\\propto-4A Q X R^{\\mathrm{T}}B R X^{\\mathrm{T}}$ for the balanced and rightmarginal semi-relaxed case. And: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{R}\\mathcal{Q}_{A,B}(Q,R,X)=2B^{\\odot2}R{\\bf1}_{r}{\\bf1}_{r}^{\\mathrm{T}}-4B R X^{\\mathrm{T}}Q^{\\mathrm{T}}A Q X\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Which likewise can be reduced in proportionality to $\\nabla_{R}\\mathcal{Q}_{A,B}(Q,R,g)\\propto-4B R X^{\\mathrm{T}}Q^{\\mathrm{T}}A Q X$ in the balanced and left-marginal semi-relaxed case. The gradients, as presented above, assume a linearization in $X\\ \\leftarrow\\ X_{k}$ . If one does not make this assumption and takes ${\\cal X}(Q,R)\\;=\\;$ $\\mathrm{diag}(1/Q^{T}\\mathbf{1}_{n}){\\cal T}\\,\\mathrm{diag}(1/R^{T}\\mathbf{\\dot{1}}_{m})$ , a rank-one perturbation must be added to the $Q$ and $\\boldsymbol{R}$ gradient of the form: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\nabla_{Q}^{(2)}=4{\\bf1}_{n}\\operatorname{diag}^{-1}(X R^{\\mathrm{T}}B(Q X R^{\\mathrm{T}})^{\\mathrm{T}}A Q\\operatorname{diag}(1/g_{Q}))^{\\mathrm{T}}}}\\\\ {{\\nabla_{R}^{(2)}=4{\\bf1}_{m}\\operatorname{diag}^{-1}(X^{T}Q^{T}A Q X R^{\\mathrm{T}}B R\\operatorname{diag}(1/g_{R}))^{\\mathrm{T}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Analogously, for the gradient on $_T$ one can simply take the gradient with respect to $\\mathbf{\\deltaX}$ , and subsequently $_T$ as $\\pmb{X}(\\pmb{\\breve{T}})\\,=\\,\\mathrm{diag}(\\pmb{g}_{Q})^{-1}\\pmb{T}\\,\\mathrm{diag}(\\pmb{\\dot{g_{R}}}\\dot{\\bf)}^{-1}$ . The gradient with respect to $\\mathbf{\\deltaX}$ is given as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{X}\\mathcal{Q}_{A,B}(Q,R,X)=-4Q^{\\mathrm{T}}A Q X R^{\\mathrm{T}}B R\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "And thus, the directional derivative with respect to $\\mathbf{\\deltaX}$ in the direction $\\begin{array}{r l}{V_{X}}&{{}=}\\end{array}$ $\\mathrm{diag}(g_{Q})^{-1}V_{T}\\,\\mathrm{diag}(g_{R})^{-1}$ and thus by the chain rule $V_{T}$ is: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D\\mathcal{Q}_{A,B}(Q,R,X)\\circ(V_{X})=-4\\langle Q^{\\mathrm{T}}A Q X R^{\\mathrm{T}}B R,V_{X}\\rangle_{F}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=-4\\langle Q^{\\mathrm{T}}A Q X R^{\\mathrm{T}}B R,\\mathrm{diag}(1/g_{Q})V_{T}\\,\\mathrm{diag}(1/g_{R})\\rangle_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "So that the gradient with respect to the coupling matrix $_T$ is given as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{T}\\mathcal{Q}_{A,B}(Q,R,T)=-4\\operatorname{diag}(1/g_{Q})Q^{\\mathrm{T}}A Q X R^{\\mathrm{T}}B R\\operatorname{diag}(1/g_{R})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "E Convergence Analysis and Other Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "E.1 Convergence and Smoothness of the Objective ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We show in Proposition 3.3 that directly applying the block-descent lemma of Beck & Tetruashvili (2013) to the template of Ghadimi et al.\u2019s proof Ghadimi et al. (2014) is sufficient to show the nonasymptotic stationary convergence of a coordinate mirror descent procedure in Ghadimi\u2019s criterion. The non-asymptotic guarantee of Ghadimi et al. (2014) follows directly in the case of coordinate mirror descent using Lemma E.1 and Lemma E.2 below. For completeness, we define all notation used, describe the coordinate mirror descent algorithm in general, and discuss a few relevant preliminaries. ", "page_idx": 22}, {"type": "text", "text": "Suppose that the vector of $n$ variables $\\mathbf{x}\\in\\mathbb{R}^{n}$ is partitioned into $p$ blocks, $\\mathbf x=(\\mathbf x(1),\\bot...\\,,\\mathbf x(p))$ , where $\\mathbf{x}(i)\\in\\mathbb{R}^{n_{i}}$ . Here, $n_{1},\\dots,n_{p}$ are positive integers summing to $n$ . Following the notation of Beck & Tetruashvili (2013); Nesterov (2012), we define matrices $\\mathbf{U}_{i}\\in\\mathbb{R}^{n\\times n_{i}}$ such that $\\mathbf{x}(i)=\\mathbf{U}_{i}^{\\mathrm{T}}\\mathbf{x}$ for all $i=1,\\hdots,p$ . This also implies $\\begin{array}{r}{\\mathbf{x}=\\sum_{i=1}^{p}\\mathbf{U}_{i}\\mathbf{x}(i)}\\end{array}$ . This allows us to define the vector of partial derivatives corresponding to each block of variables $\\mathbf{x}(i)$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla_{i}f(\\mathbf{x}):=\\mathbf{U}_{i}^{\\mathrm{T}}\\nabla f(\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In Beck & Tetruashvili (2013), the gradient of $f$ is assumed to be block-coordinate-wise Lipschitz, with $L_{i}$ the smoothness constant associated to the $i$ -th block of variables: for all $\\mathbf{h}_{i}\\in\\mathbb{R}^{n_{i}}$ , one has ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\nabla_{i}f(\\mathbf{x}+\\mathbf{U}_{i}\\mathbf{h}_{i})-\\nabla_{i}f(\\mathbf{x})\\|\\leq L_{i}\\|\\mathbf{h}_{i}\\|,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and for such functions we denote by $L:=\\operatorname*{max}_{i}L_{i}$ the (global) smoothness constant of $\\nabla f$ . To be clear, a smoothness constant associated to $f$ is a Lipschitz constant of its gradient. ", "page_idx": 23}, {"type": "text", "text": "Lemma E.1 (Block descent lemma, Beck & Tetruashvili (2013), Lemma 3.2.). Suppose $f~\\in$ $C^{1}(\\mathbb{R}^{n},\\mathbb{R})$ is a continuously differentiable function over $\\mathbb{R}^{n}$ whose gradient is block-coordinatewise Lipschitz (32) for $L_{i}$ the smoothness constant associated to the $i$ -th block of variables $\\mathbf{x}(i)$ . Let $\\mathbf{u},\\mathbf{v}$ be two vectors differing only in the $i$ -th block: there exists $\\mathbf{h}_{i}\\in\\mathbb{R}^{n_{i}}$ such that $\\mathbf{v}-\\mathbf{u}=\\mathbf{U}_{i}\\mathbf{h}_{i}$ . Then, ", "page_idx": 23}, {"type": "equation", "text": "$$\nf(\\mathbf v)\\leq f(\\mathbf u)+\\langle\\nabla f(\\mathbf u),\\mathbf v-\\mathbf u\\rangle+\\frac{L_{i}}{2}\\|\\mathbf u-\\mathbf v\\|^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma E.1 is central to adapting the proof of Theorem 1 of Ghadimi et al. (2014) to our case. Their Theorem 1 concerns the non-asymptotic convergence of mirror descent for objectives of the form ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{X}}f(\\mathbf{x})+h(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\mathcal{X}$ is a closed, convex subset of $\\mathbb{R}^{n}$ , $f\\in C^{1}(\\mathscr{X},\\mathbb{R})$ is a possibly non-convex objective, and where $h$ is an $\\alpha$ -strongly convex function. Using notation similar to Ghadimi et al. (2014), we write $\\Phi(\\mathbf{x})=f(\\mathbf{x})+h(\\mathbf{x})$ . Additionally, Ghadimi et al. (2014) assume $\\nabla f$ is $L$ -Lipschitz for some $L>0$ . To unify our assumptions on $f$ , we suppose that $\\nabla f\\in C^{1}(\\mathcal{X},\\mathbb{R})$ is block-coordinate Lipschitz (32) with block Lipschitz constants $(L_{i})_{i=1}^{p^{-}}$ , and that $\\mathcal{X}$ itself decomposes as a product $\\begin{array}{r}{\\mathcal{X}\\stackrel{}{=}\\prod_{i=1}^{p}\\mathcal{X}_{i}}\\end{array}$ , where each $\\mathcal{X}_{i}$ is a closed convex set constraining the block variables $\\mathbf{x}(i)$ . ", "page_idx": 23}, {"type": "text", "text": "The proof of Ghadimi relies on $\\beta$ -smoothness of the objective in all variables. We show that component-wise smoothness in each block is sufficient to achieve an analogous convergence result for a coordinate mirror descent. To provide context for Proposition 3.3, we now describe in general (1) mirror descent, and (2) block-coordinate mirror descent. ", "page_idx": 23}, {"type": "text", "text": "We again follow the notation of Ghadimi et al. (2014). A function $\\omega:\\mathcal{X}\\,\\rightarrow\\,\\mathbb{R}$ is a distance generating function with modulus $\\alpha>0$ , with respect to the Euclidean norm $\\|\\cdot\\|$ , if $\\omega$ is continuously differentiable and strongly convex, so that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\mathbf{x}-\\mathbf{z},\\nabla\\omega(\\mathbf{x})-\\nabla\\omega(\\mathbf{z})\\rangle\\geq\\alpha\\|\\mathbf{x}-\\mathbf{z}\\|^{2},\\quad\\mathrm{~for~all~}\\mathbf{x},\\mathbf{z}\\in\\mathcal{X}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The prox-function (or Bregman divergence) associated with $\\omega$ is then ", "page_idx": 23}, {"type": "equation", "text": "$$\nV(\\mathbf{x},\\mathbf{z})=\\omega(\\mathbf{x})-\\omega(\\mathbf{z})-\\langle\\nabla\\omega(\\mathbf{z}),\\mathbf{x}-\\mathbf{z}\\rangle,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and from this prox-function, $\\gamma>0$ , and some $\\mathbf{g}\\in\\mathbb{R}^{n}$ , we define the generalized projection ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{x}^{+}:=\\underset{\\mathbf{u}\\in\\mathcal{X}}{\\arg\\operatorname*{min}}\\left(\\langle\\mathbf{g},\\mathbf{u}\\rangle+\\frac{1}{\\gamma}V(\\mathbf{u},\\mathbf{x})+h(\\mathbf{u})\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "To describe the projected gradient descent algorithm (which coincides with mirror descent in our case of interest), we first define the generalized projected gradient of $\\Phi$ at $\\mathbf{x}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\nP_{\\mathcal{X}}(\\mathbf{x},\\mathbf{g},\\gamma):=\\frac{1}{\\gamma}(\\mathbf{x}-\\mathbf{x}^{+}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The mirror descent $(M D)$ algorithm is as follows: given initial point $\\mathbf{x}_{0}\\in\\mathcal{X}$ , a total number of iterations $N$ , and positive stepsizes $(\\gamma_{k})_{k=1}^{N}$ , at step $k$ , the $(k+1)$ -st iterate is computed via ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{x}_{k+1}\\leftarrow\\underset{\\mathbf{u}\\in\\mathcal{X}}{\\arg\\operatorname*{min}}\\left(\\langle\\nabla f(\\mathbf{x}_{k}),\\mathbf{u}\\rangle+\\frac{1}{\\gamma_{k}}V(\\mathbf{u},\\mathbf{x}_{k})+h(\\mathbf{u})\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Among all iterates $\\mathbf{x}_{k}$ , the MD algorithm outputs the one at which the generalized projected gradient is of least norm. Concretely, $\\mathbf{x}_{R}$ is the output of the MD algorithm, where ", "page_idx": 24}, {"type": "equation", "text": "$$\nR:=\\underset{k=0,\\ldots,N}{\\arg\\operatorname*{min}}\\;\\|\\mathbf{g}_{\\mathcal{X},k}\\|^{2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and where ${\\bf g}_{\\mathcal{X},k}$ is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{g}_{\\mathcal{X},k}:=P_{\\mathcal{X}}(\\mathbf{x}_{k},\\nabla f(\\mathbf{x}_{k}),\\gamma_{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Having described the MD algorithm, let us consider a block-coordinate variant; we assume $\\mathbf{x}$ admits the block-coordinate structure described above. To simplify the presentation, we suppose that in a given iteration $k$ , the block variables are updated sequentially from $i=1,\\hdots,p$ . This leads to doubly-indexed iterates $(\\mathbf{x}_{k}^{i})$ with $k=1,\\ldots,N$ indexing each full iteration through all variables, and $i=0,1,\\dotsc,p$ indexing the sub-iterations which update one block of variables at a time. ", "page_idx": 24}, {"type": "text", "text": "The coordinate mirror descent (CMD) algorithm takes as input an initial point $\\mathbf{x}_{0}\\in\\mathcal{X}$ , a number of iterations $N$ , and a sequence of positive stepsizes $(\\gamma_{k,i})_{k=1,i=1}^{N,p}$ . We set $\\mathbf{x}_{0}^{0}\\,=\\,\\mathbf{x}_{0}$ , and for $k=0,\\ldots,N-1$ and $i=1,\\hdots,p$ , we compute $\\mathbf{x}_{k}^{i}$ from $\\mathbf{x}_{k}^{i-1}$ as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}_{k}^{i}(i)\\gets\\underset{\\mathbf{u}_{i}\\in\\mathcal{X}_{i}}{\\mathrm{arg}\\,\\mathrm{min}}\\left(\\langle\\nabla_{i}f(\\mathbf{x}_{k}^{i-1}),\\mathbf{u}_{i}\\rangle+\\frac{1}{\\gamma_{k,i}}V_{i}(\\mathbf{u}_{i},\\mathbf{U}_{i}^{\\mathrm{T}}\\mathbf{x}_{k}^{i-1})+h_{i}(\\mathbf{u}_{i})\\right)}\\\\ &{\\mathbf{x}_{k}^{i}(j)\\gets\\mathbf{x}_{k}^{i-1}(j)\\quad\\mathrm{~for~}j\\neq i}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here, we have assumed that $V$ can be written as a composite function of the block variables, ", "page_idx": 24}, {"type": "equation", "text": "$$\nV(\\mathbf{x},\\mathbf{z})=\\sum_{i=1}^{p}V_{i}(\\mathbf{x}(i),\\mathbf{z}(i)),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "as is the case for the $\\mathrm{KL}$ divergence. We also have assumed that $h$ has this composite structure (as with entropy): ", "page_idx": 24}, {"type": "equation", "text": "$$\nh(\\mathbf{x})=\\sum_{i=1}^{p}h_{i}(\\mathbf{x}(i)).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lastly, for $k=0,\\ldots,N-1$ , we set $\\mathbf{x}_{k+1}^{0}:=\\mathbf{x}_{k}^{p}$ . We define $\\mathbf{g}_{\\mathcal{X},k}:=(\\mathbf{g}_{\\mathcal{X},k,1},\\ldots,\\mathbf{g}_{\\mathcal{X},k,p})$ to be the collection of block-wise differences, where by definition ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{g}_{\\mathcal{X},k,i}=P_{\\mathcal{X}_{i}}(\\mathbf{x}_{k}^{i-1},\\nabla_{i}f(\\mathbf{x}_{k}^{i-1}),\\gamma_{k,i})=\\frac{1}{\\gamma_{k,i}}\\left(\\mathbf{x}_{k}^{i-1}-\\mathbf{x}_{k}^{i}\\right)=\\mathbf{U}_{i}^{\\mathrm{T}}\\mathbf{g}_{\\mathcal{X},k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The convergence criterion $\\Delta$ we use in Proposition 3.3 is the one used by Ghadimi et al. (2014) summed across blocks. In particular, the CMD algorithm returns iterate $\\mathbf{x}_{R}$ , where ", "page_idx": 24}, {"type": "equation", "text": "$$\nR:=\\arg\\operatorname*{min}_{k=0,\\ldots,N}\\Delta(\\mathbf{x}_{k},\\mathbf{x}_{k-1}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Delta(\\mathbf{x}_{k},\\mathbf{x}_{k-1}):=\\|\\mathbf{g}_{\\mathcal{X},k}\\|^{2}=\\sum_{i=1}^{p}\\|\\mathbf{g}_{\\mathcal{X},k,i}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For $\\Phi=f+h$ as above ( $f$ has global smoothness constant $L$ ), let $\\begin{array}{r}{\\Phi^{*}=\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{X}}\\Phi(\\mathbf{x})}\\end{array}$ , and define ", "page_idx": 24}, {"type": "equation", "text": "$$\nD:=\\left(\\frac{\\Phi(x_{0})-\\Phi^{*}}{L}\\right)^{1/2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The other lemma used in the proof of Proposition 3.3 is as follows. ", "page_idx": 24}, {"type": "text", "text": "Lemma E.2 (Ghadimi et al. (2014), Lemma 1). Le $\\begin{array}{r}{t\\,\\mathbf{x}^{+}=\\arg\\operatorname*{min}_{\\mathbf{u}\\in\\mathcal{X}}\\{\\langle\\mathbf{g},\\mathbf{u}\\rangle+\\frac{1}{\\gamma}V(\\mathbf{u},\\mathbf{x})+h(\\mathbf{u})\\}}\\end{array}$ and $\\begin{array}{r}{P_{\\mathcal{X}}(\\mathbf{x},\\mathbf{g},\\gamma)=\\frac{1}{\\gamma}(\\mathbf{x}-\\mathbf{x}^{+})}\\end{array}$ . Then for all $\\mathbf{x}\\in\\mathbb{R}^{n}$ , all $\\mathbf{g}\\in\\mathbb{R}^{n}$ , and $\\gamma>0$ , one has: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\langle\\mathbf{g},P_{\\mathcal{X}}(\\mathbf{x},\\mathbf{g},\\gamma)\\rangle\\geq\\alpha\\|P_{\\mathcal{X}}(\\mathbf{x},\\mathbf{g},\\gamma)\\|^{2}+\\frac{1}{\\gamma}(h(\\mathbf{x}^{+})-h(\\mathbf{x})).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proposition E.3 (Proposition 3.3). Suppose one has $f\\,\\,\\in\\,\\,C^{1}(\\mathcal{X},\\mathbb{R})$ whose gradient is blockcoordinate Lipschitz, with block smoothness constants $(L_{i})_{i=1}^{p}$ , and a function $h\\in C(\\mathcal{X},\\mathbb{R})$ which is $\\alpha$ -strongly convex. For $\\Phi\\,=\\,f\\,+\\,h,$ , suppose one performs a coordinate mirror descent on $\\Phi$ minimized over a product of closed convex sets $\\begin{array}{r}{\\mathcal{X}=\\prod_{i=1}^{p}\\mathcal{X}_{i}}\\end{array}$ . Let the sub-iterates with respect to the $i$ -th block update be $\\{\\mathbf{x}_{k}^{i}\\}_{i=0}^{p}$ where $\\mathbf{x}_{k}:=\\mathbf{x}_{k}^{0}$ for $k\\in[N]$ outer iterations. Then one has: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k}\\Delta(\\mathbf{x}_{k},\\mathbf{x}_{k-1})\\leq\\frac{D^{2}L}{N(\\alpha^{2}/2L)}=\\frac{2D^{2}L^{2}}{N\\alpha^{2}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $D$ is (36), $L$ is the global smoothness constant of $f$ , and convergence criterion $\\Delta(\\mathbf{x}_{k},\\mathbf{x}_{k-1})$ is given in (35). Above, the stepsizes $\\gamma_{k,i}$ in the coordinate mirror descent are $\\gamma_{k,i}:=\\alpha/L$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. As $f$ satisfies the hypotheses of the block descent lemma, Lemma E.1, we apply (33) to obtain: ", "page_idx": 25}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{k}^{i})\\leq f(\\mathbf{x}_{k}^{i-1})+\\langle\\nabla_{i}f(\\mathbf{x}_{k}^{i-1}),\\mathbf{x}_{k}^{i}-\\mathbf{x}_{k}^{i-1}\\rangle+\\frac{L_{i}}{2}\\|\\mathbf{x}_{k}^{i}-\\mathbf{x}_{k}^{i-1}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Noting the definition $\\begin{array}{r}{\\mathbf{g}_{\\mathcal{X},k,i}=\\frac{1}{\\gamma_{k,i}}\\left(\\mathbf{x}_{k}^{i-1}-\\mathbf{x}_{k}^{i}\\right)}\\end{array}$ , one has ", "page_idx": 25}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{k}^{i})\\leq f(\\mathbf{x}_{k}^{i-1})-\\gamma_{k,i}\\langle\\nabla_{i}f(\\mathbf{x}_{k}^{i-1}),\\mathbf{g}_{\\mathcal{X},k,i}\\rangle+\\frac{L_{i}}{2}\\gamma_{k,i}^{2}\\|\\mathbf{g}_{\\mathcal{X},k,i}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma 1 of Ghadimi et al. (2014) (stated as Lemma E.2 above) applies identically through blockwise optimality on $\\mathcal{X}_{i}$ because $\\begin{array}{r}{\\mathbf{g}_{\\mathcal{X},k,i}=P_{\\mathcal{X}_{i}}(\\mathbf{x}_{k}^{i-1},\\nabla_{i}f(\\mathbf{x}_{k}^{i-1}),\\gamma_{k,i})}\\end{array}$ . Thus for any value $\\bar{\\nabla_{i}}f(\\mathbf{x}_{k}^{i-1})$ takes, ", "page_idx": 25}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{k}^{i})\\leq f(\\mathbf{x}_{k}^{i-1})-\\big[\\alpha\\gamma_{k,i}\\|\\mathbf{g}_{\\mathcal{X},k,i}\\|^{2}+h(\\mathbf{x}_{k}^{i})-h(\\mathbf{x}_{k}^{i-1})\\big]+\\frac{L_{i}}{2}\\gamma_{k,i}^{2}\\|\\mathbf{g}_{\\mathcal{X},k,i}\\|^{2},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and thus, ", "page_idx": 25}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{k}^{i})+h(\\mathbf{x}_{k}^{i})\\leq f(\\mathbf{x}_{k}^{i-1})+h(\\mathbf{x}_{k}^{i-1})-\\left[\\alpha\\gamma_{k,i}-\\frac{L_{i}}{2}\\gamma_{k,i}^{2}\\right]\\|g_{\\mathcal{X},k,i}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The right-hand side above only becomes larger, taking $L_{i}=L$ to be the global smoothness constant. Introducing a sum over sub-iterates and total iterates, one has ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{k,i}^{N,p}f(\\mathbf{x}_{k}^{i})+h(\\mathbf{x}_{k}^{i})\\leq\\displaystyle\\sum_{k,i}^{N,p}f(\\mathbf{x}_{k}^{i-1})+h(\\mathbf{x}_{k}^{i-1})-\\displaystyle\\sum_{k,i}^{N,p}\\left[\\alpha\\gamma_{k,i}-\\frac{L}{2}\\gamma_{k,i}^{2}\\right]\\|\\mathbf{g}_{\\mathcal{X},k,i}\\|^{2},}\\\\ {\\displaystyle\\sum_{k,i}^{N,p}\\Phi(\\mathbf{x}_{k}^{i})\\leq\\displaystyle\\sum_{k,i}^{N,p}\\Phi(\\mathbf{x}_{k}^{i-1})-\\displaystyle\\sum_{k,i}^{N,p}\\left[\\alpha\\gamma_{k,i}-\\frac{L}{2}\\gamma_{k,i}^{2}\\right]\\|\\mathbf{g}_{\\mathcal{X},k,i}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Noting the end-point condition $\\Phi(\\mathbf{x}_{k}^{p})=\\Phi(\\mathbf{x}_{k+1}^{0})$ , one may cancel all intermediate terms: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Phi^{*}\\leq\\Phi(\\mathbf{x}_{N})\\leq\\Phi(\\mathbf{x}_{0})-\\sum_{k,i}^{N,p}\\left[\\alpha\\gamma_{k,i}-\\frac{L}{2}\\gamma_{k,i}^{2}\\right]\\|\\mathbf{g}_{\\mathcal{X},k,i}\\|^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus one finds the upper bound in terms of $\\Phi(\\mathbf{x}_{0})$ and the minimum value $\\begin{array}{r}{\\Phi^{*}=\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{X}}\\Phi(\\mathbf{x})}\\end{array}$ of $\\Phi$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{k}^{N}\\sum_{i}^{p}\\left[\\alpha\\gamma_{k,i}-\\frac{L}{2}\\gamma_{k,i}^{2}\\right]\\|\\mathbf{g}_{\\mathcal{X},k,i}\\|^{2}\\leq\\Phi(\\mathbf{x}_{0})-\\Phi^{*}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Taking $\\gamma_{k,i}=\\alpha/L$ as in Ghadimi et al. (2014), the bracketed term directly above becomes $\\alpha^{2}/2L$ , and one has: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k}^{N}\\left[\\frac{\\alpha^{2}}{2L}\\right]\\left(\\underset{k}{\\operatorname*{min}}\\,\\Delta(\\mathbf{x}_{k},\\mathbf{x}_{k-1})\\right)=\\displaystyle\\sum_{k}^{N}\\left[\\frac{\\alpha^{2}}{2L}\\right]\\left(\\underset{k}{\\operatorname*{min}}\\displaystyle\\sum_{i}^{p}\\lVert\\mathbf{g}_{\\mathcal{X},k,i}\\rVert^{2}\\right)}\\\\ &{\\displaystyle\\leq\\displaystyle\\sum_{k}^{N}\\left[\\frac{\\alpha^{2}}{2L}\\right]\\sum_{i}^{p}\\lVert\\mathbf{g}_{\\mathcal{X},k,i}\\rVert^{2}}\\\\ &{\\displaystyle\\leq\\Phi(\\mathbf{x}_{0})-\\Phi^{*}=D^{2}L,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $D$ is as in (36), and where we used (37) to obtain the last line. Thus, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k}\\Delta(\\mathbf{x}_{k},\\mathbf{x}_{k-1})\\leq\\frac{D^{2}L}{N(\\alpha^{2}/2L)}=\\frac{2D^{2}L^{2}}{N\\alpha^{2}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "completing the proof. ", "page_idx": 26}, {"type": "text", "text": "Definition E.4 (Relative smoothness). Let $\\beta>0$ and let $g\\in{\\mathcal{C}}^{1}(\\mathbb{R}^{n},\\mathbb{R})$ be continuously differentiable. Additionally, let $\\omega$ be a distance generating function with associated prox-function $V$ . The function $g$ is $\\beta$ -smooth relative to $\\omega$ if the following holds: ", "page_idx": 26}, {"type": "equation", "text": "$$\ng(\\pmb{y})\\leq g(\\pmb{x})+\\langle\\nabla\\omega(\\pmb{x}),\\pmb{x}-\\pmb{y}\\rangle+\\beta V(\\pmb{y},\\pmb{x})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proposition E.5. Let $\\epsilon>0$ be a predefined error tolerance, $r>0$ a small rank parameter, $\\delta\\in(0,\\frac{1}{r})$ a lower-bound parameter, and $N$ the number of inner iterations required for the semi-relaxed projection to converge for each iteration $k$ as g(Qk\u22121)\u22252 < \u03f5 = N1 r1 \u2212\u03b4 for g(Q0) = . Then, the FRLC objective ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{LC}}(Q,R,T)=\\langle Q\\,\\mathrm{diag}(1/Q^{\\mathrm{T}}\\mathbf{1}_{n})T\\,\\mathrm{diag}(1/R^{\\mathrm{T}}\\mathbf{1}_{m})R^{\\mathrm{T}},C\\rangle_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "is component-wise smooth with respect to the variables $Q,R,T$ with smoothness constants $\\{\\beta_{i}\\}_{i=1}^{3}$ where $\\beta_{i}=\\mathrm{poly}(\\Vert C\\Vert_{F},n,m,r,\\bar{\\delta})$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. The addition of a pair of regularizations on the inner marginals $\\tau\\mathrm{KL}(Q^{\\mathrm{T}}\\mathbf{1}_{n}\\Vert Q_{k}^{\\mathrm{T}}\\mathbf{1}_{n})$ and $\\tau\\mathrm{KL}(R^{\\mathrm{T}}\\mathbf{1}_{m}\\|R_{k}^{\\mathrm{T}}\\mathbf{1}_{m})$ in 21 and 22 ensures that we may use standard results on relaxed optimaltransport which bound how far the marginal $Q_{k}^{\\mathrm{T}}\\mathbf{1}_{n}$ deviates across iterations. ", "page_idx": 26}, {"type": "text", "text": "In particular, for all $\\epsilon>0$ there exists $\\tau$ and $N$ (number of iterations) sufficiently large, so that $\\lVert\\dot{R^{\\mathrm{T}}}\\mathbf{1}_{m}-g_{R}\\rVert_{2}^{2}\\,<\\,\\epsilon$ and $\\lVert Q^{\\mathrm{T}}\\mathbf{1}_{n}-g_{Q}\\rVert_{2}^{2}<\\epsilon$ . In particular, Pham et al. (2020) shows that one can attain convergence to any $\\epsilon$ for the unbalanced problem in $N\\,=\\,\\tilde{O}(m^{2}/\\epsilon)$ iterations (hiding logarithmic and $\\tau$ -factors) for each sub-problem solving for $\\scriptstyle R_{k}$ in Algorithm 4 and analogously $\\bar{N_{\\mathrm{\\Phi}}}=\\tilde{O}(n^{2}/\\epsilon)$ for $Q$ . Under the uniform initialization of $\\scriptstyle g_{R}$ , and after $N$ iterations, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|g_{R}^{(0)}-g_{R}^{(N)}\\|_{2}=\\left\\|\\frac{1}{r}\\mathbf{1}_{r}-g_{R}^{(N)}\\right\\|_{2}\\leq\\sum_{k=1}^{N}\\|g_{R}^{(k)}-g_{R}^{(k-1)}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "With sufficiently large $\\tau$ and $N=\\tilde{O}(m^{2}/\\epsilon)$ sub-iterations, one can guarantee that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|g_{R}^{(k)}-g_{R}^{(k-1)}\\|_{2}<\\epsilon=\\frac{1}{N}\\left(\\frac{1}{r}-\\delta\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This implies, for all iterations of the algorithm and all indices $i$ , that ", "page_idx": 26}, {"type": "equation", "text": "$$\n({\\pmb g}_{R_{k}})_{i}>\\delta,\\ \\ \\ \\ \\mathrm{and\\analogously},\\ \\ \\ ({\\pmb g}_{Q_{k}})_{i}>\\delta.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, by adding the regularization on the inner marginal, one may guarantee a lower-bound on the entries of $\\scriptstyle g_{R}$ and $_{g_{Q}}$ . This is essential for demonstrating smoothness of the objective. ", "page_idx": 26}, {"type": "text", "text": "First, we consider smoothness in $Q$ . We note that the gradient in $Q$ splits into two terms: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{Q}\\mathcal{L}_{\\mathrm{LC}}(Q,R,T)=\\nabla_{Q}^{(A)}\\mathcal{L}_{\\mathrm{LC}}+\\nabla_{Q}^{(B)}\\mathcal{L}_{\\mathrm{LC}}}\\\\ &{\\qquad\\qquad\\qquad=C R X^{\\mathrm{T}}-\\mathbf{1}_{n}\\mathrm{diag}^{-1}((C R X^{\\mathrm{T}})^{\\mathrm{T}}Q\\mathrm{diag}(1/g_{Q}))^{\\mathrm{T}}}\\\\ &{\\qquad\\qquad\\qquad=\\nabla_{Q}^{(A)}\\mathcal{L}_{\\mathrm{LC}}-\\mathbf{1}_{n}\\mathrm{diag}^{-1}((\\nabla_{Q}^{(A)}\\mathcal{L}_{\\mathrm{LC}})^{\\mathrm{T}}Q\\mathrm{diag}(1/g_{Q}))^{\\mathrm{T}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{Q}\\mathcal{L}_{\\mathrm{LC}}(Q_{k+1},R_{k},T_{k})-\\nabla_{Q}\\mathcal{L}_{\\mathrm{LC}}(Q_{k},R_{k},T_{k})\\|_{F}}\\\\ &{\\qquad\\leq\\|\\nabla_{Q}^{(A)}\\mathcal{L}_{\\mathrm{LC}}(Q_{k+1},R_{k},T_{k})-\\nabla_{Q}^{(A)}\\mathcal{L}_{\\mathrm{LC}}(Q_{k},R_{k},T_{k})\\|_{F}}\\\\ &{\\qquad\\qquad+\\,\\|\\nabla_{Q}^{(B)}\\mathcal{L}_{\\mathrm{LC}}(Q_{k+1},R_{k},T_{k})-\\nabla_{Q}^{(B)}\\mathcal{L}_{\\mathrm{LC}}(Q_{k},R_{k},T_{k})\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Starting with the first term on the right side of (39), one has: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{Q}^{(A)}\\mathcal{L}_{\\mathrm{LC}}(Q_{k+1},R_{k},T_{k})-\\nabla_{Q}^{(A)}\\mathcal{L}_{\\mathrm{LC}}(Q_{k},R_{k},T_{k})\\|_{F}}\\\\ &{\\qquad=\\|C R_{k}(\\mathrm{diag}(1/g_{Q_{k}})T_{k}\\mathrm{diag}(1/g_{R_{k}}))^{\\mathrm{T}}-C R_{k}(\\mathrm{diag}(1/g_{Q_{k-1}})T_{k}\\mathrm{diag}(1/g_{R_{k}}))^{\\mathrm{T}}\\|_{F}}\\\\ &{\\qquad\\leq\\|\\mathrm{diag}(1/g_{R_{k}})\\|_{F}\\|C\\|_{F}\\|R_{k}\\|_{F}\\|T_{k}\\|_{F}\\|\\mathrm{diag}(1/g_{Q_{k}})-\\mathrm{diag}(1/g_{Q_{k-1}})\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\|\\pmb{R}_{k}\\|_{F}^{2}=\\sum_{i,j}\\left(\\pmb{R}_{k}\\right)_{i,j}^{2}<\\sum_{i,j}\\left(\\pmb{R}_{k}\\right)_{i,j}=1}\\end{array}$ , as $\\scriptstyle R_{k}$ has marginals which sum to one. The same bound holds for $\\|\\pmb{T}_{k}\\|_{F}^{2}$ , which is also a coupling. Invoking the lower-bound (38) of $\\delta$ on the entries of the inner marginals, and continuing from the above display, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{Q}^{(A)}\\mathcal{L}_{\\mathrm{LC}}(Q_{k+1},R_{k},T_{k})-\\nabla_{Q}^{(A)}\\mathcal{L}_{\\mathrm{LC}}(Q_{k},R_{k},T_{k})\\|_{F}}\\\\ &{\\qquad\\leq\\frac{\\|C\\|_{F}}{\\delta}\\|\\mathrm{diag}(1/g_{Q_{k}})-\\mathrm{diag}(1/g_{Q_{k-1}})\\|_{F}}\\\\ &{\\qquad=\\frac{\\|C\\|_{F}}{\\delta}\\|\\mathrm{diag}(1/g_{Q_{k-1}})\\mathrm{diag}(1/g_{Q_{k}})(\\mathrm{diag}(g_{Q_{k-1}})-\\mathrm{diag}(g_{Q_{k}}))\\|_{F}}\\\\ &{\\qquad\\leq\\frac{\\|C\\|_{F}}{\\delta^{3}}\\|\\mathrm{diag}(g_{Q_{k-1}})-\\mathrm{diag}(g_{Q_{k}})\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To further bound the right-hand side above, consider: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\mathrm{diag}(g_{Q_{k}})-\\mathrm{diag}(g_{Q_{k-1}})\\|_{F}^{2}=\\|Q_{k}^{\\mathrm{T}}\\mathbf{1}_{n}-Q_{k-1}^{\\mathrm{T}}\\mathbf{1}_{n}\\|_{2}^{2}=\\sum_{i=1}^{r}\\left(\\sum_{j=1}^{r}(Q_{k})_{i,j}-(Q_{k-1})_{i,j}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "While can easily be upper-bounded by an application of Jensen\u2019s inequality as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\sum_{i=1}^{r}r^{2}\\left(\\sum_{j=1}^{r}\\frac{1}{r}\\left((Q_{k})_{i,j}-(Q_{k-1})_{i,j}\\right)\\right)^{2}}\\\\ &{\\leq\\displaystyle\\sum_{i=1}^{r}r^{2}\\left(\\sum_{j=1}^{r}\\frac{1}{r}\\left((Q_{k})_{i,j}-(Q_{k-1})_{i,j}\\right)^{2}\\right)}\\\\ &{=r\\displaystyle\\sum_{i=1}^{r}\\sum_{j=1}^{r}\\left((Q_{k})_{i,j}-(Q_{k-1})_{i,j}\\right)^{2}}\\\\ &{=r\\|Q_{k}-Q_{k-1}\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Likewise, we have that $\\begin{array}{r}{\\|\\mathrm{diag}(g_{R_{k}})-\\mathrm{diag}(g_{R_{k-1}})\\|_{F}^{2}\\leq r\\|{\\cal R}_{k}-{\\cal R}_{k-1}\\|_{F}^{2}}\\end{array}$ . Thus, it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{\\|C\\|_{F}}{\\delta^{3}}\\|g_{Q_{k}}-g_{Q_{k-1}}\\|_{2}\\leq\\frac{\\|C\\|_{F}\\sqrt{r}}{\\delta^{3}}\\|Q_{k}-Q_{k-1}\\|_{F}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next, we focus on the \u2207(QB)term. Observe that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{1}_{n}\\mathrm{diag}^{-1}\\mathbf{X}\\|_{F}^{2}=\\mathrm{Tr}(\\mathbf{1}_{n}\\mathrm{diag}^{-1}\\mathbf{X})^{\\mathrm{T}}(\\mathbf{1}_{n}\\mathrm{diag}^{-1}\\mathbf{X})}\\\\ &{\\qquad\\qquad\\qquad=n\\|\\mathrm{diag}^{-1}\\mathbf{X}\\|_{2}^{2}\\leq n\\|\\mathbf{X}\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus: ", "page_idx": 27}, {"type": "text", "text": "$\\begin{array}{r}{\\|\\nabla_{Q_{k+1}}^{(B)}-\\nabla_{Q_{k}}^{(B)}\\|_{F}\\leq\\sqrt{n}\\|(\\nabla_{Q_{k+1}}^{(A)})^{\\mathrm{T}}Q_{k+1}\\mathrm{diag}(1/g_{Q_{k+1}})-(\\nabla_{Q_{k}}^{(A)})^{\\mathrm{T}}Q_{k}\\mathrm{diag}(1/g_{Q_{k}})\\|_{F}}\\end{array}$ Adding and subtracting terms in the norm and applying triangle inequality: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{n}\\|(\\nabla_{Q_{k+1}}^{(A)})^{\\mathrm{T}}Q_{k+1}\\mathrm{diag}(1/g_{Q_{k+1}})-(\\nabla_{Q_{k}}^{(A)})^{\\mathrm{T}}Q_{k+1}\\mathrm{diag}(1/g_{Q_{k+1}})}\\\\ &{\\qquad+\\,(\\nabla_{Q_{k}}^{(A)})^{\\mathrm{T}}Q_{k+1}\\mathrm{diag}(1/g_{Q_{k+1}})-(\\nabla_{Q_{k}}^{(A)})^{\\mathrm{T}}Q_{k}\\mathrm{diag}(1/g_{Q_{k}})\\|_{F}}\\\\ &{\\qquad\\qquad\\leq\\sqrt{n}\\|\\nabla_{Q_{k+1}}^{(A)}-\\nabla_{Q_{k}}^{(A)}\\|_{F}\\|Q_{k+1}\\|_{F}\\|\\mathrm{diag}(1/g_{Q_{k+1}})\\|_{F}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\sqrt{n}\\|\\nabla_{Q_{k}}^{(A)}\\|_{F}\\|(Q_{k+1}\\mathrm{diag}(1/g_{Q_{k+1}})-Q_{k}\\mathrm{diag}(1/g_{Q_{k}})\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Invoking the lower-bound on the marginal, continuing from the above display, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\leq\\displaystyle\\frac{\\sqrt{n}}{\\delta}\\|\\nabla_{\\boldsymbol{Q}_{k+1}}^{(A)}-\\nabla_{\\boldsymbol{Q}_{k}}^{(A)}\\|_{F}}\\\\ &{\\quad+\\sqrt{n}\\|\\nabla_{\\boldsymbol{Q}_{k}}^{(A)}\\|_{F}\\|(\\boldsymbol{Q}_{k+1}\\mathrm{diag}(1/\\boldsymbol{g}_{\\boldsymbol{Q}_{k+1}})-\\boldsymbol{Q}_{k}\\mathrm{diag}(1/\\boldsymbol{g}_{\\boldsymbol{Q}_{k}})\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Let us consider $\\|\\nabla_{\\pmb{Q}_{k}}^{(A)}\\|_{F}\\leq\\|\\pmb{X}_{k}\\|_{F}\\|\\pmb{C}\\|_{F}\\|\\pmb{R}_{k}\\|_{F}\\leq\\|\\pmb{X}_{k}\\|_{F}\\|\\pmb{C}\\|_{F}$ . We have that: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|X_{k}\\|_{F}^{2}=\\sum_{i,j}\\frac{1}{g_{(Q_{k})_{i}}^{2}}T_{i j}^{2}\\frac{1}{g_{(R_{k})_{j}}^{2}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Where we always have that $T_{i j}\\leq g_{Q i}$ and $T_{i j}\\leq g_{R j}$ by definition of $_T$ as a coupling. As such: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\leq\\sum_{i j}\\frac{1}{g_{Q i}^{2}}\\left(g_{Q i}g_{R j}\\right)\\frac{1}{g_{R j}^{2}}=\\sum_{i j}\\frac{1}{g_{Q i}}\\frac{1}{g_{R j}}=\\left\\langle g_{Q}^{-1}g_{R}^{-T},\\mathbf{1}_{m}\\mathbf{1}_{r}^{\\mathrm{T}}\\right\\rangle_{F}\\leq\\frac{m r}{\\delta^{2}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus $\\begin{array}{r}{\\|\\nabla_{Q_{k}}^{(A)}\\|_{F}\\leq\\frac{\\sqrt{m r}}{\\delta}\\|C\\|_{F}}\\end{array}$ , and the bound above reduces to ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\leq\\frac{\\sqrt{n}}{\\delta}\\|\\nabla_{\\mathbf{Q}_{k+1}}^{(A)}-\\nabla_{\\mathbf{Q}_{k}}^{(A)}\\|_{F}}}\\\\ {~~}\\\\ {{\\displaystyle+\\,\\frac{\\sqrt{n m r}}{\\delta}\\|C\\|_{F}\\|(\\mathbf{\\boldsymbol{Q}}_{k+1}\\mathrm{diag}(1/\\mathbf{\\boldsymbol{g}}_{Q_{k+1}})-\\mathbf{\\boldsymbol{Q}}_{k}\\mathrm{diag}(1/\\mathbf{\\boldsymbol{g}}_{Q_{k}})\\|_{F}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Further bounding the last term in the norm ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\sqrt{n m r}}{\\delta}\\|C\\|_{F}\\|({Q_{k+1}}\\mathrm{diag}(1/{g_{Q_{k+1}}})-{Q_{k}}\\mathrm{diag}(1/{g_{Q_{k+1}}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+{Q_{k}}\\mathrm{diag}(1/{g_{Q_{k+1}}})-{Q_{k}}\\mathrm{diag}(1/{g_{Q_{k}}})\\|_{F}}\\\\ &{\\leq\\frac{\\sqrt{n m r}}{\\delta}\\|C\\|_{F}\\big(\\|\\mathrm{diag}(1/{g_{Q_{k+1}}})\\|_{F}\\|Q_{k+1}-{Q_{k}}\\|_{F}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\|Q_{k}\\|_{F}\\|\\mathrm{diag}(1/{g_{Q_{k+1}}})-\\mathrm{diag}(1/{g_{Q_{k}}})\\|_{F}\\big)}\\\\ &{\\leq\\frac{\\sqrt{n m r}}{\\delta}\\|C\\|_{F}\\left(\\frac{1}{\\delta}+\\frac{\\sqrt{r}}{\\delta^{2}}\\right)\\|Q_{k+1}-Q_{k}\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, the final bound on the $\\nabla_{Q}^{(B)}$ term is: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\displaystyle\\frac{\\sqrt{n}}{\\delta}\\|\\nabla_{Q_{k+1}}^{(A)}-\\nabla_{Q_{k}}^{(A)}\\|_{F}+\\displaystyle\\frac{\\sqrt{n m r}}{\\delta}\\|C\\|_{F}\\|(Q_{k+1}\\mathrm{diag}(1/g_{Q_{k+1}})-Q_{k}\\mathrm{diag}(1/g_{Q_{k}})\\|_{F}}\\\\ &{\\le\\left(\\displaystyle\\frac{\\|C\\|_{F}\\sqrt{n r}}{\\delta^{4}}+\\displaystyle\\frac{\\sqrt{n m r}}{\\delta^{2}}\\|C\\|_{F}\\left(1+\\displaystyle\\frac{\\sqrt{r}}{\\delta}\\right)\\right)\\|Q_{k+1}-Q_{k}\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The total component-wise smoothness bound on $Q$ is then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla q\\mathcal{L}_{\\mathrm{LC}}(Q_{k+1},R_{k},T_{k})-\\nabla q\\mathcal{L}_{\\mathrm{LC}}(Q_{k},R_{k},T_{k})\\|_{F}}\\\\ &{\\leq\\frac{\\|C\\|_{F}}{\\delta^{2}}\\left(\\left(\\frac{\\sqrt{n r}}{\\delta^{2}}+\\sqrt{n m r}\\left(1+\\frac{\\sqrt{r}}{\\delta}\\right)\\right)+\\frac{\\sqrt{r}}{\\delta}\\right)\\|Q_{k+1}-Q_{k}\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Identical reasoning applies for $\\nabla_{R}\\mathcal{L}_{\\mathrm{LC}}$ , where we similarly have the gradient split into two terms: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{R}\\mathcal{L}_{\\mathrm{LC}}(Q,R,T)=\\nabla_{R}^{(A)}\\mathcal{L}_{\\mathrm{LC}}\\nabla_{R}^{(B)}\\mathcal{L}_{\\mathrm{LC}}}\\\\ &{\\qquad\\qquad=C^{\\mathrm{T}}Q X-\\mathbf{1}_{m}\\mathrm{diag}^{-1}(\\mathrm{diag}(1/\\pmb{g}_{R})\\pmb{R}^{\\mathrm{T}}C^{\\mathrm{T}}Q\\pmb{X})^{\\mathrm{T}}}\\\\ &{\\qquad\\qquad=\\nabla_{R}^{(A)}\\mathcal{L}_{\\mathrm{LC}}-\\mathbf{1}_{m}\\mathrm{diag}^{-1}(\\mathrm{diag}(1/\\pmb{g}_{R})\\pmb{R}^{\\mathrm{T}}\\nabla_{R}^{(A)}\\mathcal{L}_{\\mathrm{LC}})^{\\mathrm{T}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "As before, we may first show smoothness in $\\nabla_{R}^{(A)}$ using the same steps as for $Q$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{R}^{(A)}\\mathcal{L}_{\\mathrm{LC}}(Q_{k+1},R_{k+1},T_{k})-\\nabla_{R}^{(A)}\\mathcal{L}_{\\mathrm{LC}}(Q_{k+1},R_{k},T_{k})\\|_{F}}\\\\ &{\\quad=\\|C^{\\mathrm{T}}Q_{k+1}\\mathrm{diag}(1/g_{Q_{k+1}})T_{k}\\mathrm{diag}(1/g_{R_{k+1}})-C^{\\mathrm{T}}Q_{k+1}\\mathrm{diag}(1/g_{Q_{k+1}})T_{k}\\mathrm{diag}(1/g_{R_{k}})\\|_{F}}\\\\ &{\\quad\\le\\|C\\|_{F}\\|\\mathrm{diag}(1/g_{Q_{k+1}})\\|_{F}\\|Q_{k+1}\\|_{F}\\|T_{k}\\|\\mathrm{diag}(1/g_{R_{k+1}})-\\mathrm{diag}(1/g_{R_{k}})\\|_{F}}\\\\ &{\\quad\\le\\frac{\\|C\\|_{F}}{\\delta}\\|\\mathrm{diag}(1/g_{R_{k+1}})-\\mathrm{diag}(1/g_{R_{k}})\\|_{F}}\\\\ &{\\quad\\le\\frac{\\|C\\|_{F}}{\\delta^{3}}\\|g_{R_{k+1}}-g_{R_{k}}\\|_{2}}\\\\ &{\\quad\\le\\frac{\\|C\\|_{F}\\sqrt{r}}{\\delta^{3}}\\|R_{k+1}-R_{k}\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "$\\nabla_{R}^{(B)}$ , one may use the same reasoning as before to find: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\qquad\\qquad\\quad\\|\\nabla_{R}^{(B)}\\mathcal{L}_{\\mathrm{LC}}(Q_{k+1},R_{k+1},T_{k})-\\nabla_{R}^{(B)}\\mathcal{L}_{\\mathrm{LC}}(Q_{k+1},R_{k},T_{k})\\|_{F}}\\\\ &{\\leq\\|{\\bf1}_{m}\\left[\\mathrm{diag}^{-1}(\\mathrm{diag}(1/g_{R_{k+1}})R_{k+1}^{\\mathrm{T}}\\nabla_{R_{k+1}}^{(A)}\\mathcal{L}_{\\mathrm{LC}})-\\mathrm{diag}^{-1}(\\mathrm{diag}(1/g_{R_{k}})R_{k}^{\\mathrm{T}}\\nabla_{R_{k}}^{(A)}\\mathcal{L}_{\\mathrm{LC}})\\right]^{\\mathrm{T}}\\|_{F}}\\\\ &{\\qquad\\qquad\\leq\\sqrt{m}\\|\\mathrm{diag}(1/g_{R_{k+1}})R_{k+1}^{\\mathrm{T}}\\nabla_{R_{k+1}}^{(A)}\\mathcal{L}_{\\mathrm{LC}}-\\mathrm{diag}(1/g_{R_{k}})R_{k}^{\\mathrm{T}}\\nabla_{R_{k}}^{(A)}\\mathcal{L}_{\\mathrm{LC}}\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "As before, one may apply three rounds of triangle inequality inside the norm to bound this directly in terms of \u2225\u2207(RAk)+1 $\\|\\nabla_{R_{k+1}}^{(A)}-\\nabla_{R_{k}}^{(A)}\\|_{F}$ , $\\|\\mathrm{diag}(1/R_{k+1})-\\mathrm{diag}(1/R_{k})\\|_{F}$ , and $\\|{\\cal R}_{k+1}-{\\cal R}_{k}\\|_{F}$ . Each of these terms is smooth in $\\boldsymbol{R}$ by the lower-bound argument, so that smoothness in $\\boldsymbol{R}$ holds analogously to $Q$ . The remainder of the proof for smoothness in $\\boldsymbol{R}$ thus follows identically to that of $Q$ above. ", "page_idx": 29}, {"type": "text", "text": "For $\\textbf{\\emph{T}}$ , the component-wise bound of ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\nabla_{T_{k+1}}\\mathcal{L}_{\\mathrm{LC}}-\\nabla_{T_{k}}\\mathcal{L}_{\\mathrm{LC}}\\|_{F}=\\|Q_{k+1}C R_{k+1}^{\\mathrm{T}}-Q_{k+1}C R_{k+1}^{\\mathrm{T}}\\|_{F}^{2}\\leq L_{T}\\|T_{k+1}-T_{k}\\|_{F}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "holds trivially for any $L_{T}>0$ as the gradient is uniquely determined by $Q$ and $\\boldsymbol{R}$ alone. Thus there exist $L_{Q},L_{R},L_{T}>0$ as component-wise smoothness constants for $\\mathcal{L}_{\\mathrm{LC}}(Q,R,T)$ . \u53e3 ", "page_idx": 29}, {"type": "text", "text": "We next prove Proposition 3.4, restated just below for convenience. ", "page_idx": 29}, {"type": "text", "text": "Proposition E.6 (Proposition 3.4). Consider the FRLC objective (8). The FRLC algorithm, Algorithm 4, yields $\\beta$ -smooth iterates for $\\beta=\\mathrm{poly}(\\|C\\|_{F},m,r,\\delta)$ , where $\\delta$ denotes the lower-bound on the entries of $g_{Q},g_{R}$ . Consider the convergence metric of 3.3 adapted from Ghadimi et al. (2014), given as: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Delta_{k}({\\pmb x}_{k},{\\pmb x}_{k+1})=\\sum_{i=1}^{p}\\|{\\pmb g}_{{\\pmb\\chi},k,i}\\|^{2}=\\frac{1}{\\gamma_{k}^{2}}\\left[\\|{\\pmb Q}_{k}-{\\pmb Q}_{k-1}\\|_{F}^{2}+\\|{\\pmb R}_{k}-{\\pmb R}_{k-1}\\|_{F}^{2}+\\|{\\pmb T}_{k}-{\\pmb T}_{k-1}\\|_{F}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for $\\pmb{x}_{k}=(Q_{k},T_{k},R_{k})$ . Define the gap to the optimal solution $D$ as in (36), and let $L=\\operatorname*{sup}_{i}(L_{i})$ to be the global smoothness constant across all components. Then for $\\gamma_{k}=\\alpha/L$ as defined in 3.3 the FRLC algorithm has the non-asymptotic stationary convergence guarantee that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in1,\\ldots,N-1}\\Delta_{k}\\leq\\frac{2D^{2}L^{2}}{N\\alpha^{2}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. The proof of the non-asymptotic stationary convergence of mirror descent of Ghadimi et al. (2014), adapted for coordinate mirror descent using the block-descent lemma in 3.3, only requires component-wise smoothness in $(Q,R,T)$ . The proof of this for FRLC is given in ??, and the guarantee follows directly for this value of $\\overset{\\cdot}{L}=\\operatorname*{max}(L_{Q},L_{R},L_{T})=\\mathrm{poly}(\\Vert\\overset{\\cdot}{C}\\Vert_{F},m,r,\\delta)$ \uff0e\u53e3 ", "page_idx": 29}, {"type": "text", "text": "Proposition E.7. Low-rank Approximation Error. Let SR- $\\mathrm{W}_{r}^{\\star}$ denote the optimal rank-r approximation for the semi-relaxed low-rank optimal transport problem, and let SR-W\u22c6denote the optimal solution for the full-rank semi-relaxed optimal transport problem. ", "page_idx": 29}, {"type": "text", "text": "Additionally, suppose $\\textstyle c_{b}=\\sum_{j=1}^{m}b_{j}$ denotes the sum of the entries of the second marginal $^{\\prime}c_{b}=1\\:i f$ a probability measure). Then we have the following upper-bound on the objective error: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\vert\\mathrm{SR}\\cdot\\mathrm{W}_{r}^{\\star}(\\mu_{b})-\\mathrm{SR}\\cdot\\mathrm{W}^{\\star}(\\mu_{b})\\vert\\leq c_{b}\\left(\\operatorname*{max}_{p,q}\\{C_{p q}\\}-\\operatorname*{min}_{p,q}\\{C_{p q}\\}\\right)\\ln\\left(\\operatorname*{min}\\{n,m\\}/(r-1)\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We note that this bound also applies for the standard balanced optimal transport case, giving: ", "page_idx": 30}, {"type": "equation", "text": "$$\n|\\mathrm{W}_{r}^{\\star}(\\mu_{a},\\mu_{b})-\\mathrm{W}^{\\star}(\\mu_{a},\\mu_{b})|\\leq\\left(\\operatorname*{max}_{p,q}\\{C_{p q}\\}-\\operatorname*{min}_{p,q}\\{C_{p q}\\}\\right)\\ln\\left(\\operatorname*{min}\\{n,m\\}/(r-1)\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and improves the previous bound of $|\\mathrm{W}_{r}^{\\star}(\\mu_{a},\\mu_{b})-\\mathrm{W}^{\\star}(\\mu_{a},\\mu_{b})|\\leq\\|C\\|_{\\infty}\\ln\\left(\\operatorname*{min}\\{n,m\\}/(r-1)\\right)$ as the distance matrix $_{C}$ contains only non-negative entries. ", "page_idx": 30}, {"type": "text", "text": "Proof. We adapt the proof from Scetbon & Cuturi (2022) for the balanced case, which previously gave the bound: ", "page_idx": 30}, {"type": "equation", "text": "$$\n|\\mathrm{W}_{r}^{\\star}(\\mu_{a},\\mu_{b})-\\mathrm{W}^{\\star}(\\mu_{a},\\mu_{b})|\\leq\\|C\\|_{\\infty}\\ln\\left(\\operatorname*{min}\\{n,m\\}/(r-1)\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In particular, for $z=\\operatorname*{min}\\{m,n\\}$ , there exists an optimal $\\mathrm{rank}_{+}(P^{*})\\leq z$ where one may express the optimal solution for the non-negative coupling matrix $_{P}$ as a sum of $z$ rank-one, non-negative outer products $\\tilde{q}_{k}\\tilde{r}_{k}^{\\mathrm{T}}\\succcurlyeq0$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\nP^{*}=\\sum_{k=1}^{z}\\tilde{q}_{k}\\tilde{r}_{k}^{\\mathrm{T}}=\\sum_{k=1}^{z}\\lambda_{k}q_{k}r_{k}^{\\mathrm{T}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Where we write this sum in terms of normalized vectors $\\mathbf{q}_{k}\\,=\\,\\tilde{\\mathbf{q}}_{k}/\\lVert\\tilde{\\mathbf{q}}_{k}\\rVert_{1},\\,r_{k}\\,=\\,\\tilde{r}_{k}/\\lVert\\tilde{r}_{k}\\rVert_{1}$ , and $\\lambda_{k}=\\|\\tilde{\\boldsymbol{r}}_{k}\\|_{1}\\|\\tilde{\\boldsymbol{q}}_{k}\\|_{1}$ . Without any loss of generality, $\\left(\\lambda_{k}\\right)_{k=1}^{z}$ is ordered in terms of decreasing value such that $\\lambda_{1}\\geq\\lambda_{2}\\geq...\\geq\\lambda_{z}$ . Note that we have a fixed constraint for the sum of the entries of $_{P}$ for the semi-relaxed case, assuming $^{b}$ is a general positive measure, as $\\mathbf{1}_{n}^{\\mathrm{T}}P\\mathbf{1}_{m}=\\left(P^{\\mathrm{T}}\\mathbf{1}_{n}\\right)^{\\mathrm{T}}\\mathbf{1}_{m}=$ $\\begin{array}{r}{b^{\\mathrm{T}}\\mathbf{1}_{m}=\\sum_{j=1}^{m}b_{j}:=c_{b}}\\end{array}$ (where $c_{b}=1$ if $^{b}$ is chosen to be a probability measure, i.e. in the balanced case). Moreover, it is simple to observe for these ordered values that $\\lambda_{k}\\leq(c_{b}/k)$ . As in Scetbon $\\&$ Cuturi (2022), define the weighted average of the bottom $z-r+1$ vectors of the decomposition to be: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha_{r}=\\frac{\\sum_{i=r}^{z}\\lambda_{i}q_{i}}{\\sum_{i=r}^{z}\\lambda_{i}}}\\\\ {\\beta_{r}=\\frac{\\sum_{i=r}^{z}\\lambda_{i}r_{i}}{\\sum_{i=r}^{z}\\lambda_{i}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "And take the rank- $^r$ approximation using the optimal $r-1$ vectors of OPT and this weighted average of the bottom to be: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\tilde{P}_{r}=\\sum_{i=1}^{r-1}\\lambda_{i}\\pmb{q}_{i}\\pmb{r}_{i}^{\\mathrm{T}}+\\left(\\sum_{i=r}^{z}\\lambda_{i}\\right)\\pmb{\\alpha}_{r}\\beta_{r}^{\\mathrm{T}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Where, by the assumption that $P^{*}\\in\\Pi_{b}$ is feasible: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\tilde{P}_{r}^{\\mathrm{T}}\\mathbf{1}_{n}=\\sum_{i=1}^{r-1}\\lambda_{i}r_{i}q_{i}^{\\mathrm{T}}\\mathbf{1}_{n}+\\left(\\sum_{i=r}^{z}\\lambda_{i}\\right)\\beta_{r}\\alpha_{r}^{\\mathrm{T}}\\mathbf{1}_{n}=\\sum_{i=1}^{r-1}\\lambda_{i}r_{i}+\\left(\\sum_{i=r}^{z}\\lambda_{i}\\right)\\beta_{r}=\\sum_{i=1}^{z}\\lambda_{i}r_{i}=b\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus $\\tilde{P}_{r}\\in\\Pi_{b,r}$ is a feasible rank- $^r$ solution by feasibility of $P^{*}$ . One can verify that if $P^{*}\\in\\Pi_{a,b}$ , then $\\tilde{P}_{r}\\mathbf{1}_{m}=\\mathbf{a}$ and the solution is again feasible. From this, we observe that the difference between this solution and $P^{*}$ is an upper-bound to the difference between $P^{*}$ and the optimal rank- $^r$ solution: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathrm{SR}\\mathrm{-}\\mathrm{W}_{r}^{\\star}(\\mu_{a},\\mu_{b})-\\mathrm{SR}\\mathrm{-}\\mathrm{W}^{\\star}(\\mu_{a},\\mu_{b})\\right|=|\\langle P_{r}^{\\star},C\\rangle_{F}-\\langle P^{\\star},C\\rangle_{F}|\\leq\\langle\\tilde{P}_{r},C\\rangle_{F}-\\langle P^{\\star},C\\rangle_{F}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\left\\langle\\displaystyle\\sum_{i=1}^{r-1}\\lambda_{i}q_{i}r_{i}^{\\mathrm{T}}+\\left(\\displaystyle\\sum_{i=r}^{z}\\lambda_{i}\\right)\\alpha_{r}\\beta_{r}^{\\mathrm{T}}-\\displaystyle\\sum_{i=1}^{z}\\lambda_{i}q_{i}r_{i}^{\\mathrm{T}},C\\right\\rangle_{F}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\left\\langle\\left(\\displaystyle\\sum_{i=r}^{z}\\lambda_{i}\\right)\\alpha_{r}\\beta_{r}^{\\mathrm{T}}-\\displaystyle\\sum_{i=r}^{z}\\lambda_{i}q_{i}r_{i}^{\\mathrm{T}},C\\right\\rangle_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Noting that $\\alpha_{r},\\beta_{r}$ and $\\mathbfit{q}_{i},\\mathbfit{r}_{i}$ are unit normalized positive vectors, the sum of the entries of the outer product $\\mathbf{1}_{n}^{\\mathrm{T}}q_{i}r_{i}^{\\mathrm{T}}\\mathbf{1}_{m}=1$ , and likewise for $\\alpha_{r}\\beta_{r}^{\\mathrm{T}}$ . Thus, continuing from the above display: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=(\\displaystyle\\sum_{i=r}^{z}\\lambda_{i})(\\alpha_{r}\\beta_{r}^{\\top},C)_{F}-\\displaystyle\\sum_{i=r}^{z}\\lambda_{i}\\langle q_{i}r_{i}^{\\top},C\\rangle_{F}}\\\\ &{\\le(\\displaystyle\\sum_{i=r}^{z}\\lambda_{i})\\langle\\alpha_{r}\\beta_{r}^{\\top},C\\rangle_{F}-(\\displaystyle\\sum_{i=r}^{z}\\lambda_{i})\\operatorname*{min}_{p,q}\\{C_{p q}\\}}\\\\ &{\\le\\left(\\displaystyle\\operatorname*{max}_{p,q}\\{C_{p q}\\}-\\operatorname*{min}_{p,q}\\{C_{p q}\\}\\right)\\displaystyle\\sum_{i=r}^{z}\\lambda_{i}}\\\\ &{\\le c_{b}\\left(\\displaystyle\\operatorname*{max}_{p,q}\\{C_{p q}\\}-\\operatorname*{min}_{p,q}\\{C_{p q}\\}\\right)\\displaystyle\\sum_{i=r}^{z}\\displaystyle\\frac{1}{i}}\\\\ &{\\le c_{b}\\left(\\displaystyle\\operatorname*{max}_{p,q}\\{C_{p q}\\}-\\operatorname*{min}_{p,q}\\{C_{p q}\\}\\right)\\ln\\{z/(r-1)\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Concluding the proof. As discussed, this directly applies to the balanced case (for $c_{b}=1$ ). ", "page_idx": 31}, {"type": "text", "text": "F Initialization ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We propose a new initialization of the sub-couplings $Q,R,T$ for the LC-factorization. Algorithm 6 generates a random full-rank initial condition in the set of couplings $\\Pi_{a,b}$ which still satisfies the marginal constraints. It accomplishes this by sampling random matrices which are full-rank and applying the Sinkhorn algorithm to each of them. Scetbon et al. (2021) proposed an initialization which represents an improvement over the rank-1 product measure which is rank-2. Follow-up work proposed initialization using k-means Scetbon & Cuturi (2022). However, this assumes the previous diagonal factorization and is thus not application for generating a latent coupling which may be non-diagonal, non-square, and with two distinct inner marginals. Our initialization is tailored to the LC-factorization, is effective, and has a full-rank guarantee. In particular, higher-rank initializations may exhibit better convergence properties by allowing the gradient to explore a larger set of directions immediately in the optimization. This initialization is given in Algorithm 6. ", "page_idx": 31}, {"type": "table", "img_path": "hGgkdFF2hR/tmp/8cf204ba55922f83704d6e2eb303418d3481e7a927f2273089cb297cab6b6e23.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "Proposition F.1. Suppose one samples an initial condition on the optimal transport coupling using Algorithm $^{6}$ , where we assume $C_{i j}\\sim\\mathrm{Unif}(0,1)$ such that $\\mathbb{P}(\\operatorname{rank}(C)<\\operatorname*{min}\\{n,m\\})=0$ . Additionally, suppose that $\\mathbf{\\delta}a,b>0$ holds elementwise for both marginals $a\\in\\Delta_{n},b\\in\\Delta_{r}$ . Then the elementwise exponential $\\exp\\{C\\}$ (o $\\textstyle{\\gamma\\exp\\{-C\\}},$ ) has full-rank and the return Sinkhorn $(e^{-C},a,b)$ has full-rank. ", "page_idx": 31}, {"type": "text", "text": "Proof. It is established that a random matrix $C\\sim[0,1]^{n\\times m}$ has full-rank with probability one. For $K=\\exp\\{C\\}$ , it holds that the matrix must be entry-wise positive with $K_{i j}\\ge0$ . If columns ${\\mathbf{{C}}_{\\cdot,i}}\\neq{\\mathbf{{C}}_{\\cdot,j}}$ then clearly $C_{.,i}^{\\odot k}\\neq C_{.,j}^{\\odot k}$ , and if $C_{.,i},C_{.,j}\\succcurlyeq0$ and are independent remain so under element-wise powers. One may easily show this by contrapositive. Suppose there exist constants $c_{1},c_{2}$ such that: ", "page_idx": 31}, {"type": "equation", "text": "$$\nc_{1}C_{.,i}^{\\odot k}+c_{2}C_{.,j}^{\\odot k}=0\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "As $C_{.,i},C_{.,j}\\succcurlyeq0$ , without loss of generality one may assume $c_{1}>0$ and $c_{2}<0$ . Then: ", "page_idx": 32}, {"type": "equation", "text": "$$\nc_{1}{\\bf C}_{,\\cdot,i}^{\\odot k}=c_{1}{\\bf1}\\odot{\\bf C}_{,\\cdot,i}^{\\odot k}=-c_{2}{\\bf1}\\odot{\\bf C}_{,\\cdot,j}^{\\odot k}=-c_{2}{\\bf C}_{,\\cdot,j}^{\\odot k}\\implies\\left(-{\\frac{c_{1}}{c_{2}}}\\right)^{1/k}{\\bf1}=c{\\bf1}={\\frac{C_{,\\cdot,j}}{C_{,\\cdot,i}}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "So clearly one has that ${\\pmb C}_{.,j}\\mathrm{~-~}c{\\pmb C}_{.,i}={\\bf0}$ for $c>0$ . This implies the columns $C_{.,j}$ and ${C_{.,i}}$ are dependent. Thus it is clear that elementwise powers of entrywise positive independent vectors preserve independence. The same principle extends trivially to exponentiation of the columns, where if one assumes by contradiction that $c_{1}e^{C.,i}+c_{2}e^{C.,j}=\\mathbf{0}$ for $c_{1}>0,\\,c_{2}<0,$ , one finds $\\begin{array}{r}{\\log\\left(-\\frac{c_{1}}{c_{2}}\\right)\\mathbf{1}=C_{.,j}-\\dot{C}_{.,i}}\\end{array}$ . Without loss of generality, assume $0<-c_{2}\\leq c_{1}$ and ${\\cal{C}}_{.,j}>{\\cal{C}}_{.,i}>0$ so that $\\begin{array}{r}{\\delta=\\log\\left(-\\frac{c_{1}}{c_{2}}\\right)\\geq0}\\end{array}$ . Then, considering constants $q_{1},q_{2}$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\nq_{2}{\\cal C}_{.,j}+q_{1}{\\cal C}_{.,i}=(q_{1}+q_{2}){\\cal C}_{.,i}+\\delta q_{2}{\\bf1}={\\bf0}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Assuming $C_{.,i}$ has greater than one unique entry, which we assume as the entries are sampled densely in $\\mathbb{R}$ , the two vectors are dependent if and only if $q_{1}\\,=\\,-q_{2}$ and $\\delta\\,=\\,0$ , implying ${\\cal C}_{.,i}\\,=\\,{\\cal C}_{.,j}$ Thus, for the set of independent column vectors of $_{C}$ , given as $\\{C.,i\\}_{i=1}^{m}$ , the set $\\{e^{\\pmb{C}_{\\cdot\\,,i}}\\}_{i=1}^{m}$ , is also linearly independent. This holds analogously for the row vectors. As $_{C}$ is full-rank and $\\operatorname{span}(\\{C_{\\cdot,i}\\})\\,=\\,\\mathbb{R}^{\\operatorname*{min}\\{m,n\\}}$ , we have that $\\operatorname{span}(\\boldsymbol{K})\\,=\\,\\mathbb{R}^{\\operatorname*{min}\\{m,n\\}}$ as $\\begin{array}{r}{K\\,=\\,e^{C}\\,=\\,\\sum_{k=0}^{\\infty}\\frac{C^{\\odot k}}{k!}}\\end{array}$ (analogously $e^{-C}$ ) and remains full-rank. ", "page_idx": 32}, {"type": "text", "text": "Sinkhorn expresses each variable as ", "page_idx": 32}, {"type": "equation", "text": "$$\n{\\pmb X}=\\mathrm{diag}({\\pmb u}){\\pmb K}\\,\\mathrm{diag}({\\pmb v})\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\operatorname{rank}(X)\\mathop{=}\\operatorname{rank}(\\mathrm{diag}(u)K\\,\\mathrm{diag}(v))$ . As Cuturi (2013b) updates the vectors ${\\pmb u}\\leftarrow{\\pmb a}/K{\\pmb v}$ and $\\pmb{v}\\leftarrow\\pmb{b}/\\pmb{K}^{\\mathrm{T}}\\pmb{u}$ from ${\\pmb u}_{0}\\,=\\,{\\bf1}_{n}$ and $\\pmb{v}_{0}\\,=\\,\\mathbf{1}_{m}$ , if $a,b\\,>\\,0$ holds element-wise, one has that ${\\boldsymbol u},{\\boldsymbol v}>0$ elementwise as well. Then, one has that null $\\mathrm{diag}(v)=\\{\\mathbf{0}\\}$ and null $\\mathrm{diag}(\\boldsymbol{u})\\,=\\,\\{\\mathbf{0}\\}$ , implying that rank $:(\\mathrm{diag}({\\boldsymbol{u}})K\\,\\mathrm{diag}({\\boldsymbol{v}}))=\\mathrm{rank}(K)=\\mathrm{min}\\lbrace{\\boldsymbol{\\dot{n}}},{\\boldsymbol{\\dot{m}}}\\rbrace$ . ", "page_idx": 32}, {"type": "text", "text": "Thus, our initialization returns a random coupling matrix $X\\in\\Pi_{a,b}$ of full-rank. ", "page_idx": 32}, {"type": "text", "text": "In the next proposition, we show that one can analytically solve for the block-optimal weights $\\textbf{\\textit{g}}$ for the factorization of the coupling matrix $_{P}$ as $P=Q\\,\\mathrm{diag}(1/g)R^{\\mathrm{T}}$ Forrow et al. (2019); Scetbon et al. (2021). ", "page_idx": 32}, {"type": "text", "text": "Proposition F.2. For the minimization problem expressed as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{g}\\in\\Delta_{r}}\\langle\\pmb{Q}\\operatorname{diag}(1/\\pmb{g})\\pmb{R}^{\\mathrm{T}},\\pmb{C}\\rangle_{F}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "One has the closed-form minimizer of $g^{*}$ defined entrywise as: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\pmb{g}_{i}^{*}=\\frac{\\sqrt{\\omega_{i}}}{\\sum_{j=1}^{r}\\sqrt{\\omega_{j}}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For $\\omega=\\mathrm{diag}^{-1}(Q^{\\mathrm{T}}C R)$ , when $\\omega\\ge0$ holds entrywise. ", "page_idx": 32}, {"type": "text", "text": "Proof. As $\\omega\\ge0$ , we consider the simplex condition $\\textstyle\\sum_{j=1}^{r}\\pmb{g}_{j}=1$ alone. Writing out the Lagrangian associated to our objective, with $\\lambda\\in\\mathbb{R}$ our equality-condition dual variable, we have: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{g},\\lambda)=\\langle\\pmb{Q}\\operatorname{diag}(1/\\pmb{g})\\pmb{R}^{\\mathrm{T}},\\pmb{C}\\rangle_{F}+\\lambda(1-\\sum_{j}\\pmb{g}_{j})\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Let us consider a rewriting of the inner product term: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\langle Q\\operatorname{diag}(1/g)R^{\\top},C\\rangle_{F}=\\sum_{i=1}^{n}\\displaystyle\\sum_{j=1}^{m}C_{i j}\\displaystyle\\sum_{k=1}^{r}Q_{i k}\\left(\\frac{1}{g_{k}}\\right)R_{k j}^{\\top}}\\\\ {\\displaystyle=\\sum_{k=1}^{r}\\left(\\frac{1}{g_{k}}\\right)\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{m}Q_{k i}^{\\top}C_{i j}R_{j k}}\\\\ {\\displaystyle=\\sum_{k=1}^{r}\\left(\\frac{1}{g_{k}}\\right)\\left(Q^{\\top}C R\\right)_{k,k}}\\\\ {\\displaystyle=\\sum_{k=1}^{r}\\frac{\\omega_{k}}{g_{k}}=\\omega^{\\top}\\left(1/g\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Where division is interpreted element-wise in the last line. Thus, one can interpret the problem as minimizing the weighted sum of reciprocals of a density. As a result, we can simplify our Lagrangian\u2019s Froebenius inner product to a vector dot-product as: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{g},\\lambda)=\\pmb{\\omega}^{\\mathrm{T}}\\left(1/\\pmb{g}\\right)-\\lambda(1-\\sum_{j=1}^{r}\\pmb{g}_{j})\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, the first order condition tells us that the value of the coupling weight ${\\mathbfit{g}}_{j}$ is related to $\\lambda$ as: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\partial_{g_{j}}\\mathcal{L}(g,\\lambda)=-\\frac{\\omega_{j}}{g_{j}^{2}}+\\lambda=0\\implies g_{j}=\\sqrt{\\frac{\\omega_{j}}{\\lambda}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "And by relying on the summation condition on the probability density $\\textbf{\\textit{g}}$ , yields the Langrange multiplier as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{r}g_{j}=1=\\sum_{j=1}^{r}{\\sqrt{\\frac{\\omega_{j}}{\\lambda}}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "so that one finds ", "page_idx": 33}, {"type": "equation", "text": "$$\n1={\\frac{1}{\\sqrt{\\lambda}}}\\sum_{j=1}^{r}{\\sqrt{\\pmb{\\omega}_{j}}}\\implies\\lambda=\\left(\\sum_{j=1}^{r}{\\sqrt{\\pmb{\\omega}_{j}}}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Plugging our Lagrange-multiplier into the above expression yields: ", "page_idx": 33}, {"type": "equation", "text": "$$\ng_{j}={\\sqrt{\\frac{\\omega_{j}}{\\lambda}}}={\\sqrt{\\frac{\\omega_{j}}{\\left(\\sum_{i=1}^{r}{\\sqrt{\\omega_{i}}}\\right)^{2}}}}={\\frac{\\sqrt{\\omega_{j}}}{\\sum_{i=1}^{r}{\\sqrt{\\omega_{i}}}}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "As the Hessian $\\begin{array}{r}{\\nabla_{g}^{2}\\mathcal{L}=\\mathrm{diag}\\big(\\frac{\\omega}{g^{3}}\\big)\\succcurlyeq\\mathbf{0}}\\end{array}$ , we conclude that this value of $\\textbf{\\textit{g}}$ indeed minimizes the loss over $\\Delta_{r}$ . \u53e3 ", "page_idx": 33}, {"type": "text", "text": "G Alternating updates on the dual variables ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "For the problem: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{(Q,R_{k},g_{k})\\in\\mathcal C_{1}\\cap\\mathcal C_{2}}\\left(\\frac{1}{\\gamma_{k}}\\mathrm{KL}(Q\\|K_{Q})+\\tau\\mathrm{KL}(Q\\mathbf{1}_{r}\\|a)-\\lambda_{1}^{\\mathrm{T}}Q^{\\mathrm{T}}\\mathbf{1}_{n}\\right)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "one can find a simple set of semi-relaxed updates for the coupling matrix. We note the primal-dual relationship of Sinkhorn, $Q=\\mathrm{diag}(e^{\\gamma_{k}f_{1}})\\mathbf{\\dot{{\\calK}}}_{{\\cal Q}}\\,\\mathrm{diag}(e^{\\gamma_{k}{\\pmb h}_{1}})$ , and consider the entry-wise first-order condition required for the sub-coupling $Q$ : ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0=\\gamma_{k}^{-1}\\log\\left(\\frac{\\mathbf{\\displaystyleQ}_{i j}}{\\left(\\mathbf{\\displaystyleK}_{\\mathbf{\\displaystyleQ}}\\right)_{i j}}\\right)+\\tau\\log\\left(\\frac{\\left<\\mathbf{\\displaystyleQ}_{i,\\cdot},\\mathbf{1}_{r}\\right>}{a_{i}}\\right)-\\lambda_{1,i}}\\\\ {\\implies\\log Q_{i j}=\\tau\\gamma_{k}\\log\\left(\\frac{a_{i}}{\\left<\\mathbf{\\displaystyleQ}_{i,\\cdot},\\mathbf{1}_{r}\\right>}\\right)+\\log\\left(\\mathbf{\\displaystyleK}_{\\mathbf{\\displaystyleQ}}\\right)_{i j}-\\lambda_{1,i}\\gamma_{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus: ", "page_idx": 33}, {"type": "equation", "text": "$$\nQ_{i j}=\\left(\\frac{a_{i}}{Q_{i,.}^{\\mathrm{T}}\\mathbf{1}_{r}}\\right)^{\\tau\\gamma_{k}}\\left(K_{Q}\\right)_{i j}e^{-\\mathbf{\\lambda}_{\\mathbf{1}j}\\gamma_{k}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "And in matrix-form, this yields: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal Q}=\\mathrm{diag}\\left(\\frac{a}{{\\cal Q}{\\bf1}_{r}}\\right)^{\\tau_{\\gamma_{k}}}{\\cal K}_{\\cal Q}\\,\\mathrm{diag}(e^{-\\gamma_{k}{\\bf1}_{1}})}}\\\\ {{\\displaystyle\\quad=\\mathrm{diag}(e^{\\gamma_{k}f_{1}}){\\cal K}_{\\cal Q}\\,\\mathrm{diag}(e^{\\gamma_{k}h_{1}})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "And expanding the ${\\bf Q1}_{r}$ term explicitly, noting that $X\\,\\mathrm{diag}(v)\\mathbf{1}=X v$ , we have: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{diag}\\left(\\frac{\\displaystyle a}{\\displaystyle Q\\mathbf{1}_{r}}\\right)^{\\tau\\gamma_{k}}K_{Q}\\,\\mathrm{diag}(e^{-\\gamma_{k}\\mathbf{\\lambda}_{1}})}\\\\ &{\\quad\\quad=\\mathrm{diag}\\left(\\frac{\\displaystyle a}{\\mathrm{diag}\\left(e^{\\gamma_{k}f_{1}}\\right)K_{Q}\\,\\mathrm{diag}\\left(e^{\\gamma_{k}h_{1}}\\right)\\mathbf{1}_{r}}\\right)^{\\tau\\gamma_{k}}K_{Q}\\,\\mathrm{diag}(e^{-\\gamma_{k}\\mathbf{\\lambda}_{1}})}\\\\ &{\\quad\\quad=\\mathrm{diag}\\left(\\frac{\\displaystyle a}{e^{\\gamma_{k}f_{1}}\\odot K_{Q}e^{\\gamma_{k}h_{1}}}\\right)^{\\tau\\gamma_{k}}K_{Q}\\,\\mathrm{diag}(e^{-\\gamma_{k}\\mathbf{\\lambda}_{1}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, we identify $e^{\\gamma_{k}h_{1}}=e^{-\\gamma_{k}\\lambda_{1}}$ as the right dual vector, and identify the following relationship in terms of the left dual vector: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left(\\frac{a}{e^{\\gamma_{k}f_{1}}\\odot K_{Q}e^{\\gamma_{k}h_{1}}}\\right)^{\\tau\\gamma_{k}}=e^{\\gamma_{k}f_{1}}\\implies e^{\\gamma_{k}f_{1}}=\\left(\\frac{a}{K_{Q}e^{\\gamma_{k}h_{1}}}\\right)^{\\frac{\\tau}{\\tau+1/\\gamma_{k}}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "From 40, the condition that $(Q,R_{k},g_{k})\\in\\mathcal{C}_{1}\\cap\\mathcal{C}_{2}$ implies that $Q^{\\mathrm{T}}\\mathbf{1}_{m}=\\pmb{g}_{k}:=\\pmb{g}_{\\mathrm{:}}$ . As such, we find that: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{Q}^{\\mathrm{T}}\\mathbf{1}_{m}=\\operatorname{diag}(e^{\\gamma_{k}h_{1}})\\pmb{K}_{\\boldsymbol{Q}}^{\\mathrm{T}}\\operatorname{diag}(e^{\\gamma_{k}f_{1}})\\mathbf{1}_{m}=\\operatorname{diag}(e^{\\gamma_{k}h_{1}})\\pmb{K}_{\\boldsymbol{Q}}^{\\mathrm{T}}e^{\\gamma_{k}f_{1}}=\\pmb{g}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Implying an update for $e^{\\gamma_{k}h_{1}}$ in the form: ", "page_idx": 34}, {"type": "equation", "text": "$$\ne^{\\gamma_{k}h_{1}}=\\left(\\frac{g}{K_{Q}^{\\mathrm{T}}e^{\\gamma_{k}f_{1}}}\\right)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Analogous reasoning applies for a relaxation of the other marginal, yielding the $\\mathrm{SR}^{\\mathrm{R}}$ -projection and $\\mathrm{SR}^{\\mathrm{L}}$ -projection (i.e. semi-relaxed OT). ", "page_idx": 34}, {"type": "table", "img_path": "hGgkdFF2hR/tmp/b904d6a7e99dffd1554876da84a205c341954aa27fdb641134ac3e908ab18f7d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "H Discussion of Complexity ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "iFf otrh $(Q,R,T)\\in\\mathbb{R}_{+}^{n\\times r_{1}}\\times\\mathbb{R}_{+}^{m\\times r_{2}}\\times\\mathbb{R}_{+}^{r_{1}\\times r_{2}}$ ,a tllh ec osnpsatcaen tcso. mTphlee xtiitmy $O(n r_{1}+r_{1}r_{2}+m r_{2})$ i tihs lmi n4e aisr $r_{1},r_{2}=o(1)$   \n$O(B L r^{2}(n+m))$ for $B$ the number of inner Sinkhorn iterations, $L$ the number of mirror-descent steps, $n,m$ the number of samples in the first and second dataset, and $\\boldsymbol{r}\\,=\\,\\operatorname*{max}\\{\\boldsymbol{r}_{1},\\boldsymbol{r}_{2},d\\}$ for $r_{1},r_{2}$ the ranks of the latent coupling and $d$ the rank of the factorized distance matrix $_{C}$ (generally chosen to be a constant near $r_{1},r_{2})$ . Each matrix-multiplication is of max order $(n+m)\\bar{r^{2}}$ , which happens a constant number of times in the computation of each gradient $\\nabla_{i}$ , and for the respective Sinkhorn matrix-vector multiplications $\\kappa v$ and $\\kappa^{\\mathrm{T}}\\pmb{u}$ . The $L$ outer steps follow from the mirrordescent convergence rate and the number of iterations $B$ required for each projection follow from the convergence of Sinkhorn. In particular, for $\\varepsilon$ a fixed error tolerance and $\\eta$ the entropy constant, one finds a $\\pm\\varepsilon D$ approximation for $D$ the diameter of the data in $B=\\mathrm{poly}(1/\\eta\\varepsilon)$ iterations using the Sinkhorn algorithm Charikar et al. (2023); Cuturi (2013a). ", "page_idx": 34}, {"type": "text", "text": "I Review of Background Material ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "I.1 Low-Rank Approximation of Pairwise Distance Matrices ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "As mentioned previously, works such as Charikar et al. (2023) have developed algorithms with linear $O((n+m)^{1+o(1)}\\mathrm{poly}(1/\\epsilon))$ time-complexity and $O((n+m)d)$ space-complexity for sketching the optimal transport cost value. Recent works on low-rank factorization of the optimal transport coupling matrix $_{P}$ (the matrix associated to the coupling $\\gamma\\in\\Pi(\\mu,\\nu))$ Scetbon & Cuturi (2022); Scetbon et al. (2023, 2021) have achieved per-iteration time-complexities of $O(T(n+m)d r)$ for some constant non-negative rank $r\\geq1$ , $d$ the dimension of the metric space, and $T$ the number of iterations. By the JL-lemma one can simply embed the points in dimension $d=O(\\log{(n m)}/\\epsilon^{2})$ while preserving pairwise distances, however, currently no proofs exist which offer the number of iterations $T$ until convergence to some tolerance $\\varepsilon$ . This is partially due to how recent these works are, and also to the non-convexity of the objective which is sensitive to initial conditions Scetbon et al. (2021). However, the space complexity of the algorithm is $O((n+m)d r)$ , which is noteworthy for being linear in the number of points and avoids storing the potentially intractable $O(n m)$ coupling matrix $_{P}$ . To accomplish this, however, these works rely on a low-rank approximation of the pairwise distance matrix. A number of works by Indyk and Woodruff have concerned algorithms for finding low-rank approximations for such distance matrices. A seminal work Bakshi & Woodruff (2018) developed an algorithm which, given two point sets $\\{{\\pmb x}_{i}\\}_{i=1}^{n}$ and $\\{y_{j}\\}_{j=1}^{m}$ in some metric space $\\mathcal{X}$ , finds a rank $r$ approximation in $O((n+m)^{1+\\gamma}\\mathrm{poly}(r,1/\\varepsilon))$ for $\\gamma\\,>\\,0$ an arbitrarily small constant and $\\varepsilon>0$ an error parameter. A more recent work Indyk et al. (2019) improves on this one, by reading a sample-optimal $O((n+m)r/\\varepsilon)$ entries of the input matrix with a run-time which removes dependence on $\\gamma$ that is merely $O(({\\dot{n}}+m)\\mathrm{poly}(r,1/\\varepsilon))$ . This algorithm is used by all of the low-rank optimal transport works. These works, by finding a low-rank approximation to the coupling matrix $\\b{\\dot{P}}\\approx\\b{A}\\b{B}^{\\mathrm{T}}\\in\\mathbb{R}_{+}^{n\\times m}$ due to space limitations on the coupling, necessarily cannot store the full distance matrix $C\\in\\mathbb{R}_{+}^{n\\times m}$ of the same size in memory either. As such, it must also be approximated as $C\\approx V U$ before input to the algorithm, where we necessarily require very effective approximations of $U$ and $V$ to tolerate the additional source of error from coarse-graining the distance matrix to be low-rank. As such, we present some of the details and algorithm of Indyk et al. (2019) as an essential component of the existing low-rank optimal transport solvers. We begin by summarizing the main theorems in Indyk et al. (2019), which provide an algorithm (upper-bound) on the low-rank distance-matrix approximation problem and a lower-bound on the number of entries which must be read. ", "page_idx": 35}, {"type": "text", "text": "Theorem I.1. Indyk et al. (2019) There is a randomized algorithm that, given a distance matrix $C\\in\\mathbb{R}^{n\\times m}$ , reads $O((n\\!+\\!m)r/\\varepsilon)$ entries of $C$ , runs in time $\\bar{\\tilde{O}}(n\\,{+}\\,m)\\,{\\cdot}\\,\\mathrm{poly}(r,1/\\varepsilon)^{1}$ and computes low-rank factors $V\\in\\ensuremath{\\mathbb{R}}^{n\\times r}$ , $\\b{U}\\in\\mathbb{R}^{r\\times m}$ that with probability 0.99 satisfy: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|C-V U\\|_{F}^{2}\\leq\\|C-C_{r}\\|_{F}^{2}+\\varepsilon\\|C\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "For ${\\cal C}_{r}$ the optimal rank- $r$ approximation of $_{C}$ . ", "page_idx": 35}, {"type": "text", "text": "This is a remarkable result, especially in light of the next theorem. ", "page_idx": 35}, {"type": "text", "text": "Theorem I.2. Indyk et al. (2019) Let $r\\leq m\\leq n$ and $\\varepsilon>0$ such that $r/\\varepsilon=O(\\operatorname*{min}\\{m,n^{1/3}\\})$ . Any randomized and possibly adaptive algorithm that given a distance matrix $C\\in\\mathbb{R}^{n\\times m}$ computes $\\boldsymbol{V}\\in\\mathbb{R}^{n\\times r}$ , $U\\in\\mathbb{R}^{r\\times m}$ satisfying $\\|C-\\bar{V}U\\|_{F}^{2}\\leq\\|\\bar{C}-C_{r}\\|_{F}^{2}\\!+\\!\\varepsilon\\|C\\|_{F}^{2}$ must read $\\Omega((n\\!+\\!m)r/\\varepsilon)$ entries of $_{C}$ in expectation. This lower bound also holds for symmetric distance matrices $C\\in S_{n}$ . ", "page_idx": 35}, {"type": "text", "text": "The lower-bound follows from a difficult argument which involves constructing a hard distribution over distance matrices, involving the use of random matrix theory. The upper-bound, however, follows relatively straightforwardly from a number of previous algorithms and their associated guarantees, along with the algorithm presented below. We introduce the algorithm and also offer a proof of I.1 for completeness. ", "page_idx": 35}, {"type": "text", "text": "Input point sets $\\{{\\pmb x}_{i}\\}_{i=1}^{n}$ , $\\{y_{j}\\}_{j=1}^{M}$ in metric space $\\mathcal{X}$ and metric $d$   \nPick indices $i^{*}\\in[n],j^{*}\\in[m]$ uniformly at random   \nfor $i=1$ to $n$ do Update sample probability $\\begin{array}{r}{p_{i}=d(\\pmb{x}_{i},\\pmb{y}_{j^{*}})^{2}+d(\\pmb{x}_{i^{*}},\\pmb{y}_{j^{*}})^{2}+\\frac{1}{m}\\sum_{j=1}^{m}d(\\pmb{x}_{i^{*}},\\pmb{y}_{j})^{2}}\\end{array}$   \nend for   \nSample $O(r/\\varepsilon)$ rows $\\begin{array}{r}{C_{i,..}\\sim C a t e g o r i c a l\\left(\\frac{p_{i}}{\\sum_{i}p_{i}}\\right)}\\end{array}$   \nCompute $U$ using Frieze et al. (2004)   \nCompute $V$ using Chen & Price (2017)   \nreturn $V,U$ ", "page_idx": 36}, {"type": "text", "text": "This algorithm relies on two previous works, whose main results we summarize here. ", "page_idx": 36}, {"type": "text", "text": "Theorem I.3. Frieze et al. (2004) Let $C\\,\\in\\,\\mathbb{R}^{n\\times m}$ . For a sample of $O(r/\\varepsilon)$ rows according to a distribution $\\pmb{p}\\in\\Delta_{n}$ which satisfies $p_{i}\\,\\geq\\,\\Omega(1)\\|\\pmb{C}_{i,.}\\|_{2}^{2}/\\|\\pmb{C}\\|_{F}^{2}$ for $i\\,\\in\\,[n]$ . Then in $O(m r\\bar{/}\\epsilon+$ $p o l y(r,1/\\varepsilon))$ ) time one may compute a matrix $U\\in\\mathbb{R}^{r\\times m}$ from this sample which satisfies: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|C-C U^{\\mathrm{T}}U\\|_{F}^{2}\\leq\\|C-C_{k}\\|_{F}^{2}+\\varepsilon\\|C\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "With probability 0.99. ", "page_idx": 36}, {"type": "text", "text": "Thus, to compute the first low-rank factor $U$ , we need to ensure the $p_{i}$ generated from the algorithm satisfies this requirement and offer the (short) proof below. ", "page_idx": 36}, {"type": "text", "text": "Proof. First, it is helpful to note $d(x,y)^{2}\\,\\leq\\,(d(x,z)+d(z,y))^{2}\\,=\\,d(x,z)^{2}+2d(x,z)d(y,\\underline{{{z}}})\\,+$ $d(y,z)^{2}\\leq2(d(x,z)^{2}+d(y,z)^{2})$ . Where in the last step one uses AM-GM where $\\begin{array}{r}{\\prod_{i}a_{i}^{1/n}\\leq\\frac{\\sum_{i}a_{i}}{n}}\\end{array}$ . Rewriting the norm of row $i$ we have: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|C_{i\\cdot,\\parallel}|_{2}^{2}=\\displaystyle\\sum_{j=1}^{m}d(x_{i},y_{j})^{2}\\leq2\\displaystyle\\sum_{j=1}^{m}d(x_{i},y_{j\\cdot})^{2}+d(y_{j},x_{j})^{2}}\\\\ {\\displaystyle}\\\\ {\\qquad=2m d(x_{i},y_{j\\cdot})^{2}+2\\displaystyle\\sum_{j=1}^{m}d(y_{j},y_{j})^{2}}\\\\ {\\displaystyle}\\\\ {\\qquad\\leq2m d(x_{i},y_{j\\cdot})^{2}+4\\displaystyle\\sum_{j=1}^{m}d(y_{j},x_{i\\cdot})^{2}+d(y_{j},x_{i\\cdot})^{2}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle=2m d(x_{i},y_{j\\cdot})^{2}+4m d(y_{j},x_{i\\cdot})^{2}+4\\displaystyle\\sum_{j=1}^{m}d(y_{j},x_{i\\cdot})^{2}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle=4m\\left(\\frac{1}{2}d(x_{i},y_{j\\cdot})^{2}+d(y_{j},\\cdot,x_{i\\cdot})^{2}+\\frac{1}{m}\\displaystyle\\sum_{j=1}^{m}d(y_{j},x_{i\\cdot})^{2}\\right)\\leq4m p,}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "As we have the re-normalization $p_{i}\\leftarrow\\frac{p_{i}}{\\sum_{i}p_{i}}$ before sampling, we need to consider the value of the expectation $\\mathbb{E}[\\sum_{i}p_{i}]$ to conclude. ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\sum_{i=1}^{n}p_{i}\\right]=\\sum_{i=1}^{n}\\mathbb{E}\\left[d(x_{i},y_{j})^{2}+d(x_{i},y_{j};)^{2}+\\frac{1}{m}\\sum_{j=1}^{m}d(x_{i},y_{j})^{2}\\right]}\\\\ {\\displaystyle}&{\\displaystyle=\\sum_{i=1}^{n}\\mathbb{E}_{j\\sim\\sim\\lceil m\\rceil}\\left[d(x_{i},y_{j})^{2}\\right]+\\mathbb{E}_{i\\sim\\sim\\lceil n\\rfloor,j\\sim\\rceil}\\left[d(x_{i},y_{j})^{2}\\right]}\\\\ {\\displaystyle}&{\\displaystyle\\qquad\\qquad+\\frac{1}{m}\\sum_{j=1}^{m}\\mathbb{E}_{i\\sim\\sim\\lceil n\\rceil}\\left[d(x_{i},y_{j})^{2}\\right]}\\\\ {\\displaystyle}&{\\displaystyle=\\sum_{i=1}^{n}\\frac{1}{m}\\sum_{j=1}^{m}d(x_{i},y_{j})+\\underbrace{n\\sum_{i=1}^{n}\\sum_{j=1}^{m}\\frac{1}{n m}d(x_{i},y_{j})}_{=1}+\\frac{n}{m}\\sum_{j=1}^{m}\\frac{1}{n}\\sum_{i=1}^{n}d(x_{i},y_{j})}\\\\ {\\displaystyle}&{\\displaystyle=\\frac{3}{m}\\sum_{i=1}^{n}\\sum_{j=1}^{m}d(x_{i},y_{j})=\\frac{3}{m}\\|C\\|_{F}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By an application of Markov\u2019s inequality we have that: ", "page_idx": 37}, {"type": "equation", "text": "$$\nP\\left[\\left(\\sum_{i=1}^{n}p_{i}\\right)^{-1}\\geq\\left(\\frac{3\\|C\\|_{F}^{2}}{m\\delta}\\right)^{-1}\\right]=P\\left[\\sum_{i=1}^{n}p_{i}\\leq\\frac{3\\|C\\|_{F}^{2}}{m\\delta}\\right]\\geq1-\\frac{\\mathbb{E}[\\sum_{i=1}^{n}p_{i}]}{\\frac{3\\|C\\|_{F}^{2}}{m\\delta}}=1-\\delta\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thus with probability $1-\\delta$ we have: ", "page_idx": 37}, {"type": "equation", "text": "$$\n{\\frac{p_{i}}{\\sum_{i^{\\prime}=1}^{n}p_{i^{\\prime}}}}\\geq{\\frac{m p_{i}\\delta}{3\\|C\\|_{F}^{2}}}={\\frac{4m p_{i}\\delta}{12\\|C\\|_{F}^{2}}}\\geq{\\frac{\\|C_{i,\\cdot}\\|_{2}^{2}\\delta}{12\\|C\\|_{F}^{2}}}=\\Omega(\\delta){\\frac{\\|C_{i,\\cdot}\\|_{2}^{2}}{\\|C\\|_{F}^{2}}}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This indicates the algorithm presented has probabilities with an appropriate bound for using the algorithm of Frieze et al. (2004) to sample the $O(r/\\varepsilon)$ rows of $_{C}$ and generate a rank-r factor $U$ . ", "page_idx": 37}, {"type": "text", "text": "To conclude the result of Indyk et al. (2019) requires reference to an additional work which solves a regression problem for $V$ given $_{C}$ and $U$ . ", "page_idx": 37}, {"type": "text", "text": "Theorem I.4. Chen & Price (2017) There is a randomized algorithm $\\boldsymbol{\\mathcal{A}}$ , given matrices $C\\in\\mathbb{R}^{n\\times m}$ , $U\\in\\mathbb{R}^{r\\times m}$ reads only $O(r/\\varepsilon)$ columns of $_{C}$ with time-complexity $O(m r)+p o l y(r/\\varepsilon)$ and returns $V\\in\\ensuremath{\\mathbb{R}}^{n\\times r}$ which satisfies ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|C-V U\\|_{F}^{2}\\leq(1+\\varepsilon)\\operatorname*{min}_{z\\in\\mathbb{R}^{n\\times r}}\\|C-Z U\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "with probability 0.99. ", "page_idx": 37}, {"type": "text", "text": "Thus, using the result of Chen and Price Chen & Price (2017), one may find a satisfying $V$ easily for a fixed $U,C$ . In particular, Indyk et al. (2019) concludes by tying together the low-rank distance matrix algorithm and the guarantees of the algorithms from Frieze et al. (2004) Chen & Price (2017) as follows ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|C-V U\\|_{F}^{2}\\le(1+\\varepsilon)\\operatorname*{min}_{Z}\\|C-Z U\\|_{F}^{2}}&{}\\\\ {\\le(1+\\varepsilon)\\|C-C U^{\\mathrm{T}}U\\|_{F}^{2}}&{}\\\\ {\\le(1+\\varepsilon)(\\|C-C_{r}\\|_{F}^{2}+\\varepsilon\\|C\\|_{F}^{2})}&{}\\\\ {=\\|C-C_{r}\\|_{F}^{2}+\\varepsilon\\|C-C_{r}\\|_{F}^{2}+(1+\\varepsilon)\\varepsilon\\|C\\|_{F}^{2}}&{}\\\\ {\\le\\|C-C_{r}\\|_{F}^{2}+(1+2\\varepsilon)\\varepsilon\\|C\\|_{F}^{2}\\le\\|C-C_{r}\\|_{F}^{2}+\\tilde{\\varepsilon}\\|C\\|_{F}^{2}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Which achieves the bound up to a constant scaling of $\\varepsilon$ and shows the result of Indyk et al. (2019). We next investigate low-rank optimal transport solvers, which assume the result and algorithm of Indyk et al. (2019) to tractably scale to massive datasets. ", "page_idx": 37}, {"type": "text", "text": "J Connection of Optimal Transport to Projection Problems ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Before discussing works which have address the problem of finding low-rank decompositions of the coupling matrix $_{P}$ , we discuss a few relevant preliminaries. The Bregman-divergence of some function $F(x)$ , defined by the first-order Taylor expansion of $F$ : ", "page_idx": 38}, {"type": "equation", "text": "$$\nD_{F}(\\mathbf{x},\\pmb{y})=F(\\pmb{x})-F(\\pmb{y})+\\nabla F(\\pmb{y})^{\\mathrm{T}}\\left(\\pmb{x}-\\pmb{y}\\right)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For the negative entropy function $-\\mathrm{H}(\\pmb{\\xi})$ , this corresponds to the KL-divergence $\\mathrm{KL}(\\zeta\\mid\\xi)$ . We introduce the iterative-Bregman projection algorithm in the context of the KL-divergence owing to the direct connection with entropically-regularized optimal transport. ", "page_idx": 38}, {"type": "text", "text": "Definition J.1. Iterative Bregman ProjectionsBregman (1967) ", "page_idx": 38}, {"type": "text", "text": "Suppose ${\\mathcal{C}}=\\cap_{l=1}^{L}{\\mathcal{C}}_{l}$ is an intersection of closed convex sets $\\{\\mathcal{C}_{l}\\}_{l=1}^{L}$ . For $n>L$ , let the indexing be $L$ -periodic as ${\\mathcal{C}}_{n}:={\\mathcal{C}}_{n}$ mod $L$ . Suppose we want to find a minimizer $\\zeta$ of the KL-divergence with some positive  Cvecto r $\\pmb{\\xi}\\in\\mathbb{R}_{+}^{n\\times m}$ such that $\\zeta\\in{\\mathcal{C}}$ . This is to say, we hope to solve the problem of minimizing a distance subject to the condition that one remains in this intersection of convex sets: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\zeta\\in{\\mathcal{C}}}\\mathrm{KL}({\\zeta}\\|{\\pmb\\xi})\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Where the projection of $\\xi$ onto the set $\\mathcal{C}$ is denoted by ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\zeta^{*}=\\underset{\\zeta\\in\\mathcal{C}}{\\arg\\operatorname*{min}}\\,\\mathrm{KL}(\\zeta\\|\\pmb{\\xi}):=\\mathcal{P}_{\\mathcal{C}}^{K L}(\\pmb{\\xi})\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Supposing that each $\\mathcal{C}_{l}$ forms a quotient space $\\mathcal{C}_{l}\\;=\\;V/U$ for a subspace $U\\subset V$ , defined as $V/U=\\{\\pmb{v}+U\\mid\\pmb{v}\\in V\\}^{2}$ , the iterative Bregman projection algorithm alternates projections onto each set ${\\mathcal{C}}_{n}$ as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\boldsymbol{\\zeta}^{(n)}\\gets\\mathcal{P}_{C_{n}}^{K L}(\\boldsymbol{\\zeta}^{(n-1)})\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "starting from $\\pmb{\\zeta}^{(0)}=\\pmb{\\xi}$ . ", "page_idx": 38}, {"type": "text", "text": "One may show Bregman (1967) the convergence of Bregman projections to the unique minimizer in $\\mathcal{C},\\zeta^{*}$ , where we have the guarantee that $\\zeta^{({\\bar{n)}}}\\to\\zeta^{*}$ as $n\\to\\infty$ . These iterative projections only have convergence guarantees when the constraint sets are quotient spaces\u2013this is clearly not the case for the constraints of the optimal transport LP, and a few more notions are required. ", "page_idx": 38}, {"type": "text", "text": "Definition J.2. Dykstra\u2019s AlgorithmDykstra (1983) ", "page_idx": 38}, {"type": "text", "text": "Given a point $\\pmb{x}_{0}\\in E$ for $E$ a Euclidean space 3, to find the unique point in ${\\mathcal{C}}=\\cap_{l=1}^{L}{\\mathcal{C}}_{l}$ for closed, convex sets $\\mathcal{C}_{l}$ that minimize the distance to $\\scriptstyle x_{0}$ as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathbf{\\Delta}}\\pmb{x}^{*}=\\arg\\operatorname*{min}_{\\boldsymbol{\\mathbf{\\Delta}}\\alpha}\\|\\pmb{x}-\\boldsymbol{\\mathbf{\\Delta}}\\mathbf{x}_{0}\\|_{2}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "One may initialize the residuals $\\mathbf{q}_{-(L-1)}=\\ldots=\\mathbf{q}_{0}=0$ and apply the algorithm: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{{x}}_{n}=\\mathcal{P}_{\\mathcal{C}_{n}}\\!\\left(x_{n-1}+q_{n-1}\\right)}\\\\ &{\\mathbf{{q}}_{n}=\\left(x_{n-1}-x_{n}\\right)+q_{n-L}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Where ${\\mathcal{C}}_{n}:={\\mathcal{C}}_{n}$ mod $L$ as before and $\\mathcal{P}_{\\mathcal{C}_{n}}$ denotes the projection operator onto the convex set $\\mathcal{C}_{n}$ ", "page_idx": 38}, {"type": "text", "text": "One can note that Dykstra\u2019s algorithm for projections onto intersections of convex sets no longer relies on the assumption that the set is a quotient space, and applies in the case that it is merely closed under convex-combinations. To generalize Dykstra\u2019s for more general functions that than the $\\ell_{2}$ -norm, one may define the projection with respect to the Bregman divergence of a cost function $F$ and define the projection by the minimization: $\\begin{array}{r}{\\mathcal{P}\\mathcal{C}_{n}\\;:=\\;\\arg\\operatorname*{min}_{\\pmb x}D_{F}(\\pmb x,\\pmb y)}\\end{array}$ . It was proven in Bauschke & Lewis (2000) that the generalized form of Dykstra\u2019s iterations are given as: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{x}_{n}=\\mathcal{P}_{\\mathcal{C}_{n}}\\left(\\nabla F^{*}(\\nabla F(\\pmb{x}_{n-1})+\\pmb{q}_{n-1})\\right)}\\\\ &{\\pmb{q}_{n}=(\\nabla F(\\pmb{x}_{n-1})-\\nabla F(\\pmb{x}_{n}))+\\pmb{q}_{n-L}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For $F^{*}$ denoting the Fenchel-conjugate of $F$ , which we define later in connection to the low-rank dual problem. Notably, Bauschke & Lewis (2000) also provided guarantees of convergence to the optimal solution which extend to the case that $F$ is the negative entropy and $D_{F}$ the KL-divergence. These constitute Dykstra\u2019s algorithm with cyclic Bregman projections, and project a point to the closest point in the intersection of convex sets $\\mathcal{C}=\\cap_{l=1}^{\\overline{{L}}}\\mathcal{C}_{l}$ for an arbitrary cost function $F$ and its associated Bregman-divergence $D_{F}$ . ", "page_idx": 38}, {"type": "text", "text": "J.1 Dykstra\u2019s algorithm with cyclic Bregman projections ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "As such, we can see the updates for $F$ being the negative entropy and $D_{F}$ the KL-divergence. Without proof, the conjugate of the negative entropy is simply given as $F^{*}(\\zeta)=\\exp\\{\\zeta-{\\bf1}\\}$ . Thus, for the minimization problem: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\zeta^{*}=\\underset{\\zeta\\in\\mathcal{C}}{\\arg\\operatorname*{min}}\\,\\mathrm{KL}(\\zeta\\|\\pmb{\\xi}):=\\mathcal{P}_{\\mathcal{C}}^{\\mathrm{KL}}(\\pmb{\\xi})\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Letting $\\log{\\pmb q}_{-(L-1)}=\\ldots=\\log{\\pmb q}_{0}=0$ , one may combine the two algorithms above to solve this minimization using generalized Dykstra\u2019s iterations. In particular, we have: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\zeta^{(n)}=\\mathcal{P}_{\\mathcal{C}_{n}}\\left(\\nabla F^{*}(\\nabla F(\\zeta^{(n-1)})+\\log\\ensuremath{\\boldsymbol{q}}_{n-1})\\right)}\\\\ &{\\phantom{\\hat{\\zeta}^{(n)}=\\mathcal{P}_{\\mathcal{C}_{n}}}\\left(\\exp\\left(\\nabla F(\\zeta^{(n-1)})+\\log\\ensuremath{\\boldsymbol{q}}_{n-1}-\\mathbf{1}\\right)\\right)}\\\\ &{\\phantom{\\hat{\\zeta}^{(n)}=\\mathcal{P}_{\\mathcal{C}_{n}}}\\left(\\exp\\left(\\log\\zeta^{(n-1)}+\\mathbf{1}+\\log\\ensuremath{\\boldsymbol{q}}_{n-1}-\\mathbf{1}\\right)\\right)}\\\\ &{\\phantom{\\hat{\\zeta}^{(n)}=\\mathcal{P}_{\\mathcal{C}_{n}}}\\left(\\zeta^{(n-1)}\\odot\\ensuremath{\\boldsymbol{q}}_{n-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "And: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\ensuremath{\\pmb q}_{n}=(\\nabla F(\\zeta^{(n-1)})-\\nabla F(\\zeta^{(n)}))+\\log\\ensuremath{\\pmb q}_{n-N}}\\\\ &{\\qquad=\\left(\\log\\zeta^{(n-1)}+\\ensuremath{\\mathbf1}-(\\log\\zeta^{(n)}+\\ensuremath{\\mathbf1})+\\log\\ensuremath{\\pmb q}_{n-L}\\right)}\\\\ &{\\qquad=\\log\\left(\\ensuremath{\\pmb q}_{n-L}\\odot\\frac{\\zeta^{(n-1)}}{\\zeta^{(n)}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "So that: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\star(n)\\,\\leftarrow\\,\\mathcal{P}_{\\mathcal{C}_{n}}\\left(\\zeta^{(n-1)}\\odot q_{n-1}\\right)}\\\\ &{\\quad\\quad\\quad q_{n}\\,\\leftarrow\\,q_{n-L}\\odot\\frac{\\zeta^{(n-1)}}{\\zeta^{(n)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Where division is interpreted to be element-wise, $\\odot$ refers to the Hadamard product, and the logarithm is applied elementwise. ", "page_idx": 39}, {"type": "text", "text": "J.2 Connection to Sinkhorn distances ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Interestingly, the Sinkhorn algorithm described in Algorithm 5 can be alternatively derived in the context of Bregman iterations Benamou et al. (2015) as a minimization of the form: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\operatorname{W}_{\\epsilon}(\\mu,\\nu)=\\epsilon\\operatorname*{min}_{P\\in\\Pi(\\mu,\\nu)}\\operatorname{KL}(P\\|\\xi)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Where $\\xi$ is the kernel $\\pmb{\\xi}=e^{-C/\\epsilon}$ and $\\Pi(\\mu,\\nu)=\\mathcal{C}_{1}\\cap\\mathcal{C}_{2}$ for the convex constraint sets $\\begin{array}{r}{\\mathcal{C}_{1}=\\{P:}\\end{array}$ $P\\mathbf{1}_{m}=\\pmb{a}\\}$ and ${\\mathcal{C}}_{2}=\\left\\{P:{P}^{\\mathrm{T}}\\mathbf{1}_{n}=b\\right\\}$ . To cast this into the Bregman-projection framework, one alternates between the two updates: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P^{(l)}=\\mathcal{P}_{\\mathcal{C}_{1}}(P^{(l-1)})}\\\\ {P^{(l+1)}=\\mathcal{P}_{\\mathcal{C}_{2}}(P^{(l)})\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Where for the first projection one has the following first-order KKT condition: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla\\left(\\epsilon\\mathrm{KL}(P||P^{(l-1)})+\\lambda^{\\mathrm{T}}(P\\mathbf{1}_{m}-\\pmb{a})\\right)=\\epsilon\\log\\left(\\frac{P}{P^{(l-1)}}\\right)+\\lambda\\mathbf{1}_{m}^{\\mathrm{T}}=\\mathbf{0}}\\\\ {\\implies P=\\mathrm{diag}(e^{-\\lambda/\\epsilon})P^{(l-1)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "With the constraint of $\\mathcal{C}_{1}$ that $P\\mathbf{1}_{m}\\;=\\;\\mathbf{a}$ , this implies $\\mathrm{diag}(a/P^{(l-1)}{\\bf1}_{m})\\,=\\,\\mathrm{diag}(e^{-\\lambda/\\epsilon})$ and recovers the first update of Sinkhorn $\\mathbf{P}^{(l)}\\leftarrow\\mathrm{diag}(a/P^{(l-1)}\\mathbf{1}_{m})P^{(l-1)}$ . An analogous argument gives the second, where all iterates satisfy $\\pmb{P}^{(l)}=\\mathrm{diag}(\\pmb{u}^{(l)})e^{-\\pmb{C}/\\epsilon}\\,\\mathrm{diag}(\\pmb{v}^{(l)})$ for $\\mathbf{\\Delta}u,\\,v$ as defined in Algorithm 5. ", "page_idx": 39}, {"type": "image", "img_path": "hGgkdFF2hR/tmp/db9f3cf658799d05d6aab751c4c75d36303f51f46fc19e8260d4083dae4acd45.jpg", "img_caption": ["Figure 5: Transport cost $\\langle C,P\\rangle_{F}$ against number of iterations for FRLC with rank 200 on the synthetic dataset of two moons and eight Gaussians. Smooth convergence is observed for both rank-2 and full-rank random initialization. "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "K Additional Simulations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "We tested on two additional synthetic datasets, both used as benchmarking datasets in Scetbon et al. (2021). We follow exactly the parameters provided in Scetbon et al. (2021) to simulate these datasets. For the first one, we simulated $n=m=10,000$ points from two Gaussian mixtures in 2D (Fig. 6). The first Gaussian mixture is a mixture of three Gaussian distributions with means $(0,0),(0,1),(1,1)$ respectively. The mixture proportion is $\\begin{array}{l}{{\\frac{1}{3}}}\\end{array}$ and the covariance is $0.05\\times$ identity for each Gaussian. The second Gaussian mixture is a mixture of two Gaussian distributions with means (0.5, 0.5), $(-0.5,0.5)$ respectively. The mixture proportions is $\\frac{1}{2}$ and the covariance is $0.05\\times$ identity for each Gaussian. ", "page_idx": 40}, {"type": "text", "text": "For the second dataset, we simulated $n~=~m~=~5,000$ points from two Gaussian mixtures in 10D. The first Gaussian mixture is a mixture of three Gaussian distributions with means $(0,0,0,\\cdots,0),(0,1,0,\\cdots,0),(1,1,0,\\cdots,0)$ respectively. The mixture proportions i s 13 and the covariance is $0.05~\\times$ identity for each Gaussian. The second Gaussian mixture is a mixture of two Gaussian distributions with means $(0.5,0.5,0,\\cdots,0)$ , $(-0.5,0.5,0,\\cdots\\,,0)$ respectively. The mixture proportions is $\\frac{1}{2}$ and the covariance is $0.05\\times$ identity for each Gaussian. ", "page_idx": 40}, {"type": "text", "text": "For each dataset, we repeat the same procedure as in $\\S\\,4.1$ , running FRLC and LOT with Euclidean distance as cost to find a low-rank coupling matrix between the two Gaussian mixtures with rank between 50 and 200. We observe the same pattern as Fig. 2b. FRLC obtains lower transport cost with increasing rank, and achieves lower cost for each rank than LOT under all initializations (Fig. 23c, Fig. 7). ", "page_idx": 40}, {"type": "text", "text": "L Graph Partitioning ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "L.1 Evaluation on a Graph Partitioning Task ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "We next evaluate FRLC on an unsupervised graph partitioning (node clustering) problem described by Chowdhury & Needham (2021). Specifically, given a graph $G=(V,E)$ of $n$ nodes, we represent the graph as $G=(A,h)$ , where $A\\in\\mathbb{R}^{n\\times n}$ encodes the intra-graph node relationship (e.g. adjacency matrix) and $h\\in\\Delta_{n}$ is a uniform measure. We cluster the nodes of $G$ by estimating a GW coupling between $G$ and a smaller graph $\\overline{{G}}=(\\overline{{B}},\\overline{{h}})$ , where $\\overline{{B}}\\in\\mathbb{R}^{m\\times m}$ represents the relationship between each of the $m$ clusters and $\\overline{{h}}\\in\\Delta_{m}$ is the proportion of nodes of $G$ in each cluster. Without a priori knowledge, $\\overline{h}$ is set to be uniform and $\\overline{B}$ is set as the identity matrix. Vincent-Cuaz et al. (2022) notes that instead of solving a balanced GW problem, semi-relaxed GW with the right marginal $\\overline{h}$ relaxed learns $\\overline{h}$ from data and leads to more accurate node clustering. ", "page_idx": 40}, {"type": "image", "img_path": "hGgkdFF2hR/tmp/5fbe309f3e9e71d1c7d1521a0498e4d22de713143c5d972ac86de94aeca571ad.jpg", "img_caption": ["Figure 6: Plot of the two simulated mixtures of Gaussians in 2D, following the same parameters as Scetbon et al. (2021). "], "img_footnote": [], "page_idx": 41}, {"type": "image", "img_path": "hGgkdFF2hR/tmp/314b277792788cd8d03d17f0246a0dd3e7fb4540d9686126ce09a8e51206e5ce.jpg", "img_caption": ["Figure 7: Transport cost $\\langle C,P\\rangle_{F}$ achieved by LOT Scetbon et al. (2021) and FRLC across different ranks and different initializations on the Wasserstein problem on the synthetic dataset of two mixtures of Gaussians in 2D. "], "img_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "hGgkdFF2hR/tmp/fc90cc1d563a588e83ad13a8421e27b2a7b9b65c511ff44df3431110e2e87354.jpg", "table_caption": [], "table_footnote": ["Table 2: Runtime of FRLC and LOT on the synthetic datasets of 1000 samples, as well as cost value $\\langle C,P\\rangle_{F}$ and marginal tightness for context. This was done with the FRLC setting max_inneriters_balanced $=\\!1000$ , max_inneriters_relaxed $=\\!50$ , min_iter $^{\\,=7}$ and rank $r=100$ . This time excludes the extra time incurred for ott-jax problem setup and includes the setup time for FRLC. "], "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "table", "img_path": "hGgkdFF2hR/tmp/db57978a1b3dd0d6d16054b39b02fef9502808510ba5d2c618cde5b131bc764e.jpg", "table_caption": [], "table_footnote": ["Table 3: Performance (measured using Adjusted Mutual Information (AMI)) in graph partitioning for full-rank OT algorithms GWL, SpecGWL and full-rank semi-relaxed FRLC. The top performing method for each dataset is highlighted in bold. "], "page_idx": 42}, {"type": "text", "text": "We benchmark FRLC for semi-relaxed GW on four real-world graphs: a Wikipedia hyperlink network with 15 webpage categories Yang & Leskovec (2012); an email interaction network within a European institute with 42 departments Yin et al. (2017); an Amazon product network with 12 product categories Yang & Leskovec (2012); and a network of interactions between 12 Indian villages Banerjee et al. (2013). We also test on the symmetric and noisy versions of each graph provided by Chowdhury & Needham (2021). We compare with two OT-based methods: (1) GWL Xu et al. (2019), which solves a balanced GW problem between $G$ and $\\overline{G}$ with the adjacency matrix of $G$ as the intra-domain cost matrix $\\pmb{A}$ ; (2) SpecGWL Chowdhury & Needham (2021) which uses the heat kernel on the graph Laplacian as $\\pmb{A}$ . We similarly run our FRLC algorithm using with both the adjacency matrix (denoted FRLC-SR-GW) and heat kernel (denoted SpecFRLC-SR-GW). Since the number of clusters in each dataset is not large, we compute the full-rank coupling matrix in each case. ", "page_idx": 42}, {"type": "text", "text": "Overall, FRLC achieves the best clustering performance on 9 out of 12 datasets (Table 3). When using the adjacency matrix, our semi-relaxed algorithm achieves better clustering result than GWL on 9 out of 12 datasets. When using the heat kernel, our semi-relaxed algorithm achieves better clustering result than SpecGWL on 11 out of 12 datasets. These results show the importance of semi-relaxed OT on real-world problems, as well as the accuracy of FRLC. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "L.2 Problem Statement ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "As discussed in Vincent-Cuaz et al. (2022); Chowdhury & Needham (2021), it is possible to achieve unsupervised graph partitioning (node clustering) through Gromov-Wasserstein (GW) OT. Given a graph $(V,E)$ of $n$ nodes, we encode it as $G=(A,h)$ , where $A\\in\\mathbb{R}^{n\\times n}$ encodes the intra-graph node relationship (e.g. adjacency matrix, Laplacian) and $h\\,\\in\\,\\Delta n$ is a uniform measure over the nodes. If we want to cluster the nodes of $G$ into $m$ clusters, we can define a new graph $\\overline{{G}}=(\\overline{{B}},\\overline{{h}})$ , where $\\overline{{B}}\\in\\mathbb{R}^{m\\times m}$ is a diagonal matrix representing the cluster\u2019s connections and $\\overline{h}$ is a distribution over clusters estimating the proportion of nodes in $G$ in each cluster. Since we don\u2019t know the density of clusters a priori, we can set $\\bar{\\overline{{h}}}$ to be uniform. Usually $\\overline{B}$ is set as the identity matrix. ", "page_idx": 43}, {"type": "text", "text": "To cluster the nodes in $G$ , we can solve a GW problem matching nodes in $G$ with nodes in $\\overline{G}$ , with intra-domain cost matrices $\\pmb{A}$ and $\\overline{B}$ : ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{min}}&{\\quad\\displaystyle\\sum_{i j^{\\prime}k l^{\\prime}}(A_{i k}-\\overline{{B}}_{j^{\\prime}l^{\\prime}})^{2}P_{i j^{\\prime}}P_{k l^{\\prime}}}\\\\ {s.t.}&{\\quad\\pmb{P}\\in\\Pi_{h,\\overline{{h}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "The cluster assignment of each node in $G$ can then be recovered from $_{P}$ by finding the node in G mapped to it with the maximum weight. ", "page_idx": 43}, {"type": "text", "text": "However, since the proportion of nodes in $G$ in each cluster is not known a priori, solving a balanced GW problem fixing the marginal of $_{P}$ on $\\overline{G}$ to be a uniform $\\overline{h}$ significantly constrains the expressivity of the algorithm. Therefore, as proposed in Vincent-Cuaz et al. (2022), we can instead solve a semi-relaxed GW problem with the right marginal relaxed, fixing the marginal on $G$ to be $^h$ but allowing the marginal on $\\overline{G}$ to deviate from $\\overline{h}$ : ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{min}\\quad}&{\\displaystyle\\sum_{i j^{\\prime}k l^{\\prime}}(A_{i k}-\\overline{{B}}_{j^{\\prime}l^{\\prime}})^{2}P_{i j^{\\prime}}P_{k l^{\\prime}}+\\tau\\mathrm{KL}(\\pmb{P}^{\\mathrm{T}}\\mathbf{1}_{n}\\mid\\overline{{h}})}\\\\ {s.t.\\quad}&{\\displaystyle\\pmb{P}\\in\\Pi_{h,}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "The learned $\\overline{h}$ from semi-relaxed GW estimates the posterior proportion of nodes in $G$ in each of the $m$ clusters. ", "page_idx": 43}, {"type": "text", "text": "L.3 Dataset and Preprocessing ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We run our semi-relaxed FRLC algorithm on four real-world graph datasets: a Wikipedia hyperlink network with 1998 nodes and 15 clusters Yang & Leskovec (2012), a directed graph of email interactions in a European research institute with 1005 nodes and 42 clusters Yin et al. (2017), an Amazon product network with 1501 nodes and 12 clusters Yang & Leskovec (2012), and a network of interactions between Indian villages with 1991 nodes and 12 clusters Banerjee et al. (2013). The Wikipedia and EU-email graphs are directed, so we also use undirected versions of them. We also use noisy version of each graph by adding up to $10\\%$ additional edges following Chowdhury & Needham (2021), leading to a total of 12 different graphs to cluster. ", "page_idx": 43}, {"type": "text", "text": "L.4 Experiment Settings ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We compare our algorithm with two baseline methods, GWL and SpecGWL. GWL Xu et al. (2019) solves the entropy-regularized version of the balanced GW problem of (46) with the adjacency matrix of $G$ as $\\pmb{A}$ . We set $^h$ such that the density of each node is proportional to its degree. We set $\\overline{h}$ to be a distribution estimated by sorting the weights of $^h$ and sampling $m$ values via linear interpolation, following Chowdhury & Needham (2021). We set $\\overline{{B}}=d i a g(\\overline{{h}})$ . We set the entropy regularization $\\eta=10^{-6}$ . SpecGWL Chowdhury & Needham (2021) solves the same problem as GWL, but instead of using the adjacency matrix as $\\pmb{A}$ , it uses the heat kernel on the normalized graph Laplacians Chung (2005). We set the heat parameter $t=10$ . ", "page_idx": 43}, {"type": "image", "img_path": "hGgkdFF2hR/tmp/63685f253f94fa60288cc89d6481a5797a65e4bc51edd20b869b6e3199dc8115.jpg", "img_caption": ["Figure 8: FRLC achieves lower primal cost $\\langle C,P\\rangle_{F}$ for $P\\,\\in\\,\\Pi_{a,b}$ than Scetbon et al. (2021) on a spatial-transcriptomics dataset of mouse embryonic development Chen et al. (2022). FRLC demonstrates a more robust trend of improved performance with higher rank. "], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "For our method, we solve the semi-relaxed GW problem of (47) with full rank solution. We use both adjacency matrix and heat kernel as $_{C}$ and label the result of the two representations as FRLC-SR-GW and SpecFRLC-SR-GW. We set $\\tau=0.01$ as to minimize the conformation to the right marginal. Since our method depends on random initialization, we run our method 10 times on each dataset and report the mean performance. We evaluate the resulting clusterings of all methods by computing the Adjusted Mutual Information (AMI) between the computed clustering and the ground truth clustering. This experiment, and the experiments on mouse embryo spatial transcriptomics, were conducted on cluster GPUs. ", "page_idx": 44}, {"type": "text", "text": "M Spatial Transcriptomics Alignment ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "M.1 Problem Statement ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "In this problem, we use FRLC to find a low-rank alignment matrix between cells from two spatial transcriptomics (ST) St\u00e5hl et al. (2016) slices, collected at two timepoints, then use the computed alignment matrix for two downstream prediction tasks. An ST experiment on a 2D tissue slice yields a pair $(X,Z)$ . $X\\in\\mathbb{R}^{n\\times p}$ is the gene expression matrix, where $n$ is the number of cells on the slice and $p$ is the number of genes measured. $X_{i j}\\in\\mathbb{R}$ is the gene expression level of gene $j$ in cell $i$ , where a higher number indicates stronger expression. $Z\\in\\mathbb{R}^{n\\times2}$ is the spatial coordinate matrix, where each row $i$ stores the $\\mathbf{X}_{\\mathrm{~}}$ -y coordinate of cell $i$ on the slice. Therefore, each cell on the slice has a gene expression vector of length $p$ , which encodes the feature of the cell, as well as a coordinate vector of length two, which encodes the geometrical information of the cell on the slice. ", "page_idx": 44}, {"type": "text", "text": "Our input data is a pair of ST slices ${\\cal S}_{0}=(X^{0},Z^{0}),{\\cal S}_{1}=(X^{1},Z^{1})$ , with $n$ and $m$ cells, of the same tissue region. We assume ${\\mathcal S}_{0}$ is collected at timepoint $t=0$ and $S_{1}$ is collected at timepoint $t=1$ , hence the transition from ${\\mathcal S}_{0}$ to $S_{1}$ reflects the biological development of the tissue during the time period. We would like to find the ancestor-descendant relationship between cells from ${\\mathcal{S}}_{0}$ and $S_{1}$ by computing an optimal transport coupling matrix between cells from ${\\mathcal{S}}_{0}$ and cells from $S_{1}$ . The state-of-the-art spatial transcriptomics alignment method moscot Klein et al. (2023) claims that unbalanced OT is the most appropriate setup for this problem. Specifically, given discrete uniform measure $\\textbf{\\em a}$ and $^{b}$ over cells from ${\\mathcal{S}}_{0}$ and $S_{1}$ , we solve the unbalanced Wasserstein problem ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{P\\in\\mathbb{R}_{+}^{n\\times m}}\\langle C,P\\rangle+\\tau\\mathrm{KL}(P\\mathbf{1}_{m}\\|\\boldsymbol{a})+\\tau\\mathrm{KL}(P^{\\mathrm{T}}\\mathbf{1}_{n}\\|\\boldsymbol{b})\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "$C\\in\\mathbb{R}_{+}^{n\\times m}$ has entries $C_{i j}=c(X_{i}^{0},X_{j}^{1})$ , where $c$ is the Euclidean distance between the features of cell $i$ from ${\\mathcal S}_{0}$ and cell $j$ from $S_{1}$ . ", "page_idx": 45}, {"type": "text", "text": "The above formulation only considers the feature information of the two slices, but not the geometrical information. Therefore, we can also solve the unbalanced GW problem ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{P\\in\\mathbb{R}_{+}^{n\\times m}}\\sum_{i,j^{\\prime},k,l^{\\prime}}(A_{i k}-B_{j^{\\prime}l^{\\prime}})^{2}P_{i j^{\\prime}}P_{k l^{\\prime}}+\\tau\\mathrm{KL}(P\\mathbf{1}_{m}\\|a)+\\tau\\mathrm{KL}(P^{\\mathrm{T}}\\mathbf{1}_{n}\\|b)\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $A\\in\\mathbb{R}^{n\\times n}$ is the Euclidean distance matrix between the spatial location of cells within ${\\mathcal{S}}_{0}$ , and $B\\in\\mathbb{R}^{m\\times m}$ is the Euclidean distance matrix between the spatial location of cells within $S_{1}$ Similarly, we can combine the above two formulations to solve the unbalanced FGW problem. ", "page_idx": 45}, {"type": "text", "text": "Notice the inherent asymmetry stemming from the temporal nature of the problem: all cells on $S_{1}$ should have an ancestor from $S_{1}$ , but not all cells from $S_{1}$ need to have a descendent in ${\\mathcal{S}}_{0}$ because of cell death. Therefore, the most natural OT task for this problem is semi-relaxed OT with the left marginal (the marginal on the first/ancestor slice) relaxed. Specifically, we can also solve the semi-relaxed Wasserstein problem ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{P\\in\\Pi_{\\cdot,b}}\\langle C,P\\rangle+\\tau\\mathrm{KL}(P\\mathbf{1}_{m}||\\pmb{a})\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "as well as semi-relaxed GW and FGW problem. ", "page_idx": 45}, {"type": "text", "text": "Gene expression prediction task Given the alignment matrix $_{P}$ linking cells from ${\\mathcal{S}}_{0}$ to $S_{1}$ , we can predict properties of cells in $S_{1}$ from properties of cells in ${\\mathcal S}_{0}$ . Let the expression of a gene $j$ in ${\\mathcal{S}}_{0}$ be a vector $f_{j}\\in\\mathbb{R}^{n}$ , such that $f_{j i}$ is the expression level of gene $j$ in cell $i$ , we can predict the expression of gene $j$ in $S_{1}$ as $\\widetilde{\\pmb{f}}_{j}=m\\times\\pmb{P}^{\\mathrm{T}}\\times\\pmb{f}_{j}\\in\\mathbb{R}^{m}$ . The accuracy of the prediction can be measured by the Spearman correlation between the predicted expression and the ground truth expression $\\overline{{\\pmb{f}}}_{j}$ of gene $j$ in $S_{1}$ : $\\rho(\\widetilde{\\pmb{f}}_{j},\\overline{{\\pmb{f}}}_{j})$ . In this work, we test the prediction accuracy on 10 test genes: Tubb2b, Pantr1, Actc1, T nni1, Afp, Hbb-bh1, Fez1, Crabp1, Crabp2, Col3a1, which are markers genes for various cell types in mouse embryo. ", "page_idx": 45}, {"type": "text", "text": "Cell type prediction task We can also use the cell type labels of cells in ${\\mathcal{S}}_{0}$ to predict the cell type labels of cells in $S_{1}$ . Specifically, for each cell $j$ in $S_{1}$ , we can assign it the type of the cell argmax ${}_{i}P_{i j}$ in ${\\mathcal{S}}_{0}$ . We can measure the accuracy of the cell type prediction by computing the Adjusted Rand Index (ARI) and Adjusted Mutual Information (AMI) between the predicted clustering of cells in $S_{1}$ and the ground truth clustering. ", "page_idx": 45}, {"type": "text", "text": "M.2 Dataset and Preprocessing ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "We use the large-scale real-world dataset of mouse embryo development Chen et al. (2022), consisting of eight timepoints of ST slices during the whole process of mouse embryo development. In this work, we align the pair of adjacent timepoints of E11.5 and E12.5 embryos, consisting of 30,124 cells and 51,365 cells, respectively. We preprocess the dataset using the standard SCANPY Wolf et al. (2018) pipeline. We first filter the two slices to have the same set of genes, resulting in 26,436 genes for all cells from both slices. We then log-normalize the gene expression of all cells from the two slices, and apply Principle Component Analysis (PCA) to reduce the dimensionality of gene expressions to 30. We take the Euclidean distance between the gene expression in the PCA space as the cost matrix $_{C}$ . We take the Euclidean distance between the 2D coordinate of each cell within each slice as the intra-domain cost matrices $\\pmb{A}$ and $_B$ . ", "page_idx": 45}, {"type": "text", "text": "Fig. 9 visualizes the two slices in this dataset, with each cell annotated with a cell type from the original publication. ", "page_idx": 45}, {"type": "image", "img_path": "hGgkdFF2hR/tmp/714078c39d5c7e68b5630c197f62aae4b328042258593b36ca0215feefa7e98a.jpg", "img_caption": ["Figure 9: Visualization of the E11.5 and E12.5 mouse embryos, with each cell colored by the cell type annotated by Chen et al. (2022). "], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "M.3 Experiment Settings ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "We compare with the unbalanced low-rank optimal transport algorithm of Scetbon et al. (2023), the backbone of the spatial transcriptomics alignment method moscot Klein et al. (2023), which was shown by Scetbon et al. (2023); Klein et al. (2023) to achieve state-of-the art performance on spatial transcriptomics alignment. We use Scetbon et al. (2023) to solve three unbalanced problems for spatial transcriptomics alignment: the unbalanced Wasserstein problem of (48) (result denoted as LOT-U-W), the unbalanced GW problem of (49) (LOT-U-GW), and the unbalanced FGW problem with a convex combination of the previous two costs (LOT-U-FGW). We use FRLC to solve the same three unbalanced problems (results denoted as FRLC-U-W, FRLC-U-GW, FRLC-U-FGW). Since the two slices contain $>30{,}000$ cells and $>50{,}000$ cells respectively, a full-rank solution is not feasible, hence we solve for low-rank solutions with the rank validated. We also solve semi-relaxed versions of Wasserstein (result denoted as FRLC-SR-W), GW (FRLC-SR-GW), FGW (FRLC-SR-FGW) problems using our FRLC algorithm, as well as using a particular setting of LOT-U that is equivalent to semi-relaxed solver (results denoted as LOT-SR-W, LOT-SR-GW, LOT-SR-FGW). ", "page_idx": 46}, {"type": "text", "text": "We perform extensive grid search to find the best hyperparameter combinations for each method and each problem. The grid of hyperparameters searched for each method is reported in Table. 6. The best performing hyperparameter combination for each method is reported in Table. 7 along with the performance on the validation genes. We pick the best hyperparameters using the Spearman correlation on the gene expression prediction task for 10 validation genes: Ckb, Fabp7, Myl4, Tnnt2, Apoa2, Hba-x, Tubb3, Epha7, Ldha, Col1a2, which are marker genes of various cell types as well. We report the performance of the alignment computed by each method using the Spearman correlation on the gene expression prediction task for 10 test genes, as well as the ARI and AMI on the cell type prediction task. Fig. 10 visualizes the ground truth cell type classification versus the classification predicted by our method FRLC-SR-W. ", "page_idx": 46}, {"type": "text", "text": "M.4 Runtime ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "We report the runtime and OT cost of FRLC and LOT Scetbon et al. (2021) on this dataset (mouse embryo E11.5-12.5) as well as two other datasets (E9.5-10.5, E10.5-11.5) from Chen et al. (2022) in Table 4. For all three datasets, FRLC achieves a better OT cost in a shorter time. ", "page_idx": 46}, {"type": "table", "img_path": "hGgkdFF2hR/tmp/bb0a6830c4c212ee08792d12fabfaa1436b411125e58f29c8656d9721b77a91a.jpg", "table_caption": [], "table_footnote": ["Table 4: Comparison of methods on Stereo-Seq mouse embryo spatial transcriptomics datasets using GPU and default settings: min_iter $=\\mathtt{10}$ , max_iter $\\mathtt{=}100$ , rank $r=50$ . "], "page_idx": 46}, {"type": "text", "text": "N $n^{\\mathrm{th}}$ -roots of unity ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "N.1 Problem Statement ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "To test if FRLC can effectively coarse-grain transport between two datasets with obvious cluster structure, we generate a pair of two-dimensional datasets $\\mathbf{Z}^{(1)}$ and $\\mathbf{Z}^{(2)}$ , with ten and five clusters respectively. We run FRLC with $\\pmb{T}\\,\\in\\,\\mathbb{R}_{+}^{10\\times5}$ to see if the barycentric projections for the LC factorization (Definition 4.1) can recover the cluster structure. These projections induce 10 and 5 barycenters for the first and second dataset, and are defined by ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbf{Y}^{(1)}:=\\mathrm{diag}(1/g_{Q})Q^{\\mathrm{T}}\\mathbf{Z}^{(1)},\\quad\\mathbf{Y}^{(2)}:=\\mathrm{diag}(1/g_{R})R^{\\mathrm{T}}\\mathbf{Z}^{(2)}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "We examine, visually, whether these barycenters are good representatives of the clusters in each dataset, and whether the latent coupling depicts a reasonable transfer of mass between barycenters. We also run FRLC with $\\pmb{T}\\in\\mathbb{R}_{+}^{10\\times10}$ and plot the barycenters from the resulting factorization for comparison. As discussed in $\\S\\,4.2$ , the barycentric projections defined above, and in Definition 4.1 can be applied to factored couplings Forrow et al. (2019); Scetbon et al. (2021), yielding projections of the form: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbf{Y}^{(1)}:=\\mathrm{diag}(1/g)Q^{\\mathrm{{T}}}\\mathbf{Z}^{(1)},\\quad\\mathbf{Y}^{(2)}:=\\mathrm{diag}(1/g)R^{\\mathrm{{T}}}\\mathbf{Z}^{(2)}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Thus, we also ran the method of Scetbon et al. (2021) on this data, called LOT throughout the experiment, with $\\pmb{g}\\in(\\mathbb{R}_{+}^{*})^{5}$ and $g\\in(\\mathbb{R}_{+}^{*})^{10}$ , plotting its barycenters in each case along with the diagonal latent coupling $\\mathrm{diag}(g)$ . ", "page_idx": 47}, {"type": "text", "text": "N.2 Dataset and Preprocessing ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "We instantiate a pair of two-dimensional datasets $\\mathbf{Z}^{(1)}$ and $\\mathbf{Z}^{(2)}$ as follows. Let $\\mathcal{U}_{n}$ denote the $n^{\\mathrm{th}}$ -roots of unity: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathcal{U}_{n}:=\\{e^{2\\pi i k/n}:k=0,\\ldots,n\\}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "These complex numbers can be equivalently expressed as ordered pairs on the unit circle: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathsf{c}_{k}=\\left(\\mathrm{Im}(e^{2\\pi i k/n})\\right)=\\left(\\mathsf{c o s}(2\\pi k/n))\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "We consider $n$ uniformly weighted mixtures of Gaussians, where for each sample $X$ , we first sample a root of unity uniformly, ", "page_idx": 47}, {"type": "equation", "text": "$$\nk\\sim\\mathrm{Uniform}(n),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and conditionally on $k$ , we sample $X$ from an isotropic Gaussian centered at this root of unity ", "page_idx": 47}, {"type": "equation", "text": "$$\nX\\sim{\\mathcal{N}}(\\mathsf{c}_{k},\\sigma^{2}\\mathrm{Id}_{2}),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "using a standard deviation $\\sigma=0.1$ . Samples are generated with the make_blobs function from sklearn.datasets. We generate two datasets in this way, $\\mathbf{Z}^{(1)}$ for $n=10$ , and $\\mathbf{Z}^{(2)}$ for $n=5$ To generate dataset $\\mathbf{Z}^{(1)}$ , we first homogeneously scale the roots of unity to lie on a circle of radius 3 before sampling, so that the two datasets do not overlap but still use the same standard deviation. We do not scale the centers used for $Z^{(2)}$ , so they all lie on the unit circle. We generate $\\mathbf{Z}^{(1)}\\,=\\,\\{\\mathbf{z}_{1}^{(1)},\\dots,\\mathbf{z}_{1000}^{(1)}\\}$ and $\\mathbf{Z}^{(2)}\\;=\\;\\{\\mathbf{z}_{1}^{(2)},\\dots,\\mathbf{z}_{1000}^{(2)}\\}$ using 1000 samples each and form the empirical measures ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mu:=\\sum_{i=1}^{1000}\\frac{1}{1000}\\delta_{{\\bf z}_{i}^{(1)}},\\quad\\nu:=\\sum_{j=1}^{1000}\\frac{1}{1000}\\delta_{{\\bf z}_{j}^{(2)}}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "supported on $\\mathbf{Z}^{(1)}$ and $\\mathbf{Z}^{(2)}$ , corresponding to uniform probability vectors $a,b\\in\\Delta_{1000}$ . ", "page_idx": 47}, {"type": "text", "text": "N.3 Experiment Settings ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "We used default hyperparameter settings for both methods. In particular, FRLC sets $\\tau=75$ and $\\gamma=90$ , with a maximum of 200 iterations and a minimum of 25 iterations, subject to the stopping criterion $\\Delta$ given in (10). The LOT default settings are $\\gamma=10$ (there is no hyperparameter analogous to $\\tau$ in LOT). Both methods use a random initialization and were run on CPU. ", "page_idx": 47}, {"type": "text", "text": "O Discussion of differences between FRLC and existing low-rank optimal transport algorithms ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "We provide an extensive comparison of FRLC against the parametrization and objective of Scetbon et al. (2021) in Appendix A. As Latent-OT of Lin et al. (2021) is an extension of the $k$ -Wasserstein barycenter problem, it has a distinct objective and thus performs worse on primal OT cost (Table 5), making a direct experimental comparison on primal cost minimization only appropriate relative to the works of Scetbon et al. (2021, 2023, 2022). This stated, we still list a number of distinctions between FRLC and Latent-OT \u2013 noting that most differences between FRLC and Forrow et al. (2019) transfer from this discussion since Lin et al. (2021) extends the $k$ -Barycenter problem of Forrow et al. (2019). (1) Lin et al. optimize two sets of variables: sub-couplings $(Q,R,T)$ and anchor points $(z^{x},z^{y})$ on which the sub-couplings depend. FRLC only has $(Q,R,T)$ as variables of the optimization. (2) Cost matrices used in Lin et al. (2021) are built from distances between each dataset and its representative anchor points for $Q,R$ , or the distances between the two sets of anchor points for $_T$ . In contrast, ground costs used in FRLC to update $(Q,R,T)$ are always derived from the distance matrix $_{C}$ in the Wasserstein objective $\\langle C,P{\\bar{\\rangle}}_{F}$ . Specifically, the cost matrices used by Lin et al. are: ", "page_idx": 48}, {"type": "equation", "text": "$$\n[C_{Q}]_{i k}=\\|x_{i}-z_{k}^{x}\\|_{2}^{2},\\quad[C_{R}]_{j\\ell}=\\|y_{j}-z_{\\ell}^{y}\\|_{2}^{2},\\quad[C_{T}]_{k\\ell}=\\|z_{k}^{x}-z_{\\ell}^{y}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "optionally using a Wasserstein distance for the entries of $C_{T}$ . (3) FRLC costs are given in the exponents of the Gibbs kernels written above and below Equation 9. These are derived directly from the rank- $^r$ Wasserstein problem $\\mathrm{min}_{P\\in\\Pi_{r}(a,b)}\\langle C,P\\rangle_{F}$ and differ substantially from those of the proxy objective in Lin et al. (2021); Forrow et al. (2019). (4) The different objectives and variables lead to very different algorithms: Lin et al. alternate updates to the sub-couplings $(Q,R,T)$ using Dykstra, with updates to the latent anchor points $(z^{x},z^{y})$ using first-order conditions. In contrast, FRLC alternates semi-relaxed OT to update $(Q,R)$ and balanced OT to update $_T$ . (5) Because FRLC does not require anchor points to define costs, FRLC can handle cost matrices which are not simple functions of distance. For example, if $C_{i j}$ is the price of transporting good $i$ to warehouse $j$ one may not be able to re-evaluate a price $c(x_{i},z_{k}^{x})$ between $x_{i}$ and latent anchor $z_{k}^{x}$ . In such situations, while finding a low-rank plan may make sense (e.g. to approximate an assignment for a massive dataset), an \u201canchor\" may not have clear definition in the setting of general cost matrices. (6) The Lin et al. objective is only a proxy for a Wasserstein-type loss, and Lin et al. do not explore extensions to Gromov-Wasserstein (GW), or Fused GW, which FRLC readily generalizes to. A summary of the existing low-rank OT algorithms and key distinctions between them is given in Table 1. ", "page_idx": 48}, {"type": "text", "text": "For completeness, we offer a compare against the work Latent OT Lin et al. (2021), which solves a variation of the $k$ -Wasserstein barycenter problem. As discussed, while their factorization is similar, their problem is distinct from FRLC as it does not solve the primal OT problem for general cost. We report the cost obtained by FRLC and by Latent OT on various simulated datasets in Table 5. ", "page_idx": 48}, {"type": "table", "img_path": "hGgkdFF2hR/tmp/9d78c42cc4a1152192b5f6bfe9f34113a6d6220d3cba7ad15de015c6815f7a59.jpg", "table_caption": [], "table_footnote": ["Table 5: Comparison against Lin et al. (2021) in primal OT-cost $\\langle{\\pmb C},{\\pmb P}\\rangle_{F}$ . "], "page_idx": 48}, {"type": "text", "text": "P Limitations ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Our method introduces an additional hyperparameter $\\tau$ relative to previous approaches Scetbon et al.   \n(2021), controlling the strength of the KL penalty on the inner marginals when updating $Q$ and $\\boldsymbol{R}$ . ", "page_idx": 48}, {"type": "text", "text": "Empirically, we found FRLC to be robust to different choices of $\\tau$ , but applying the method optimally requires this additional hyperparamter in any grid-search. ", "page_idx": 49}, {"type": "text", "text": "We also note that the non-asymptotic criterion $\\Delta(\\cdot,\\cdot)$ is weak relative to stronger notions of convergence, and that often users might prefer to simply run the method up to some number of maximal iterations by setting the parameter for whether $\\Delta(\\cdot,\\cdot)$ is used to False. The W optimization empirically converges to minima smoothly, so for the most part there is not much of a need for $\\Delta$ except for early stopping. We recommend that users plot the loss over iterations and use it to set the tolerance parameter tol, and the minimum and maximum iteration parameters for the time-being. The needs for these parameters might vary widely\u2013the minimum number of iterations should be very low (around 5) for simple datasets and substantially higher for high-dimensional, structured ones. ", "page_idx": 49}, {"type": "text", "text": "Although we demonstrate strong performance already, there is massive room for improvement as our implementation is preliminary and not at the level of a high-performance library like ott-jax. We use lightweight vanilla implementations of Sinkhorn as a sub-routine, not taking advantage of the momentum-based techniques which could accelerate it massively. Thus, one can imagine that the potential scalability and speed of this method could be much higher than reported in this document. ", "page_idx": 49}, {"type": "text", "text": "Q Broader impacts ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "FRLC is general enough to be used modularly within any ML algorithm using OT as a subroutine to help with scalability. We also note that the LC factorization is similar to a PCA in the context of OT, yielding an optimal low-rank coupling with an interpretable latent coupling factor. ", "page_idx": 49}, {"type": "table", "img_path": "hGgkdFF2hR/tmp/b4295244bb5d25e3782097e134c6a8f508147374cea0cf2115c79f7ceeb1ecb6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 50}, {"type": "table", "img_path": "hGgkdFF2hR/tmp/ffccc981a67203f53bc3d065ff366a5bbb1f378d9f2c50c29667ac63d68e1c64.jpg", "table_caption": ["Table 6: Hyperparameter grid considered in hyperparameter search for validation. Scetbon et al. (2023); Klein et al. (2023) scales $\\begin{array}{r}{\\tau^{\\prime}=\\frac{\\tau}{1-\\tau}}\\end{array}$ and their $\\tau^{\\prime}$ are in the same range. "], "table_footnote": ["Table 7: The best performing hyperparameters for each solver and the performance on the validation genes. "], "page_idx": 50}, {"type": "image", "img_path": "hGgkdFF2hR/tmp/f3650611961ca16b19d06e63d79a44e1d02fa665b868b2724073b4ea9d5a2bfa.jpg", "img_caption": ["Figure 10: Ground truth and the predicted cell type classification of the E12.5 embryo. "], "img_footnote": [], "page_idx": 50}, {"type": "image", "img_path": "hGgkdFF2hR/tmp/ae94eb4555e8a8b00de1d9d11d7e9063bf5799494b4b10da51c3f311bbc5a19d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 51}, {"type": "text", "text": "Figure 11: (a) LC-projection barycenters aligned with FRLC latent-coupling $_T$ on (a) two moons and eight Gaussians $(r=30)$ ), (b) LC-projection barycenters aligned with $\\mathrm{diag}(g)$ from Scetbon et al. (2021) $(r=30)$ ), (c) the checkerboard dataset with FRLC latent coupling aligned barycenters $(r=12)$ ), and (d) with diagonal alignment Scetbon et al. (2021) $(r=12)$ ). We show in A.1 that the output of FRLC can be diagonalized to the factorization of Forrow et al. (2019) (Figure 12). ", "page_idx": 51}, {"type": "image", "img_path": "hGgkdFF2hR/tmp/215aff0dd0e0f05f864c6736ed5b3ce3d52dc55cc42d718d1b1c9ee315156b32.jpg", "img_caption": ["Figure 12: As discussed in A.1, one may recover the factorization of Forrow et al. (2019) as a sub-case of the LC-factorization. Shown is the factorization found by diagonalizing the output of FRLC from $(Q,R,T)\\mapsto(Q^{\\prime},R,\\mathrm{diag}(g_{R}))$ . "], "img_footnote": [], "page_idx": 52}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: The abstract and introduction exactly describe the background preceding this paper and placing it into context, and exactly describe the contribution and scope of the paper to the field. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 53}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Justification: We describe the limitations of our method in Section P. These include an additional hyperparameter that FRLC introduces relative to previous work, and the substantial room for code optimization. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 53}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Justification: All theoretical results state their assumptions clearly and all propositions are followed by thorough line-by-line proofs with careful justifications made for each step. They have checked over by all of the authors. All proofs are provided in the supplement with detail and are referenced in the main body where appropriate. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 54}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: We provide all code used for generating the results of our paper. This not only includes the source code for the optimization, but the experimental code used for benchmarking. The algorithm is implemented exactly as described in the paper, and reviewers can freely consult the code we provide to them. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 54}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 55}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: We will upload all code, and all code required to generate the synthetic data experiments used. Any real data experiments using especially large-scale data have publicly available and easily accessible datasets which we provide references for. We provide comprehensive descriptions of how the data was pre-processed, and provide experimental code which can be followed easily. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 55}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Justification: We introduce all hyper-parameters with rigorous justification of their utility. The values of the default hyper-parameters are accessible in the code we upload and any experiment which does not use the default hyperparameter (e.g. for a validation search) has the full table provided and the code used for the experiment. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 55}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: The optimization is deterministic, so generally there\u2019s no need for reporting statistical significance. One small exception is that one of our proposed initializations is randomized, and for experiments which use it we do include $\\pm1\\sigma$ error bars. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 56}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Justification: We indicate when experiments are run on CPU versus GPU in the experimental details. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 56}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: This research was conducted ethnically and conforms to the guidelines of the code of ethics. We do not anticipate any major negative societal impact to result from this work on optimal transport. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 56}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 57}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: We address broader impacts in Section Q: FRLC can be used for scaling an interpretability in any ML method using OT as a subroutine. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 57}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Justification: We do not generate any new datasets other than a toy dataset of Gaussians centered at $\\mathfrak{n}^{\\mathrm{th}}$ roots of unity, which has no societal impact. We do not believe our optimal transport work has significant risk of misuse. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 57}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: All data that we use and all work that we build on are cited and credited heavily. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 58}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: We provide no new assets. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 58}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: This paper involves no research with human subjects. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 58}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 58}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 59}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 59}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 59}]