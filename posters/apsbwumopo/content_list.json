[{"type": "text", "text": "Optimized Feature Generation for Tabular Data via LLMs with Decision Tree Reasoning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jaehyun $\\mathbf{Nam}^{*1}$ , Kyuyoung $\\mathbf{Kim}^{*1}$ , Seunghyuk Oh1 Jihoon Tack1, Jaehyung $\\mathbf{Kim^{2}}$ , Jinwoo Shin1 1KAIST, 2Yonsei University {jaehyun.nam,kykim,jinwoos}@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In tabular prediction tasks, tree-based models combined with automated feature engineering methods often outperform deep learning approaches that rely on learned representations. While these feature engineering techniques are effective, they typically depend on a pre-defined search space and primarily use validation scores for feature selection, thereby missing valuable insights from previous experiments. To address these limitations, we propose a novel tabular learning framework that utilizes large language models (LLMs), termed Optimizing Column feature generator with decision Tree reasoning (OCTree). Our key idea is to leverage the reasoning capabilities of LLMs to identify effective feature generation rules without manually specifying the search space and provide language-based reasoning information highlighting past experiments as feedback for iterative rule improvements. We use decision trees to convey this reasoning information, as they can be easily represented in natural language, effectively providing knowledge from prior experiments (i.e., the impact of the generated features on performance) to the LLMs. Our empirical results demonstrate that OCTree consistently enhances the performance of various prediction models across diverse benchmarks, outperforming competing automated feature engineering methods. Code is available at https://github.com/jaehyun513/OCTree. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning useful representations from raw data is key to the success of deep learning algorithms, and their effectiveness has been demonstrated across multiple domains, e.g., vision [1, 2, 3, 4] and language [5, 6]. However, in the tabular domain, deep learning approaches are often perceived as less effective [7, 8, 9, 10]. For instance, tree-based approaches utilizing raw column features of tabular data [11, 12] often outperform deep learning models in tabular prediction tasks such as classification and regression [13, 14, 15]. As a result, practitioners commonly resort to using tree-based methods coupled with manual feature engineering, such as computing the product of two column features [16]. ", "page_idx": 0}, {"type": "text", "text": "Generating suitable column features, even with domain knowledge, can be challenging and costly. For instance, manual validation to identify useful features is infeasible due to the exponentially many possible combinations to explore [17]. To address this issue, existing feature engineering methods [17, 18, 19] use additional filtering schemes [20, 21, 22] to evaluate and select useful features automatically. While these approaches reduce manual effort and improve feature quality, they still present several challenges. First, practitioners often rely on manually defined search spaces to generate candidate features due to the inherent ambiguity of what constitutes informative features [17, 23]. However, this still requires substantial computation for validating candidate features, ", "page_idx": 0}, {"type": "image", "img_path": "APSBwuMopO/tmp/a94950d94b4c781937bdbbd1f08ddf69a77da967925706e0c7dc6e9c7efc4dfd.jpg", "img_caption": ["Step 4 Repeat steps 1 3 a fixed number of times, then select the rule with the best validation score "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Overview of OCTree. (Step 0) Prompt the LLM to propose a name for the new column. (Step 1) Generate a rule by prompting the LLM with feedback on previously generated rules and relevant information for reasoning about the data. (Step 2) Generate a new column feature based on the proposed rule. (Step 3) Train a prediction model on the new data and compute the validation score and tree-based reasoning, provided as feedback for iterative improvements. (Step 4) Repeat steps 1-3 a fixed number of times and select the rule with the best validation score. ", "page_idx": 1}, {"type": "text", "text": "particularly as the number of features and the complexity of the search space grow. Furthermore, they neglect more effective experimental designs, relying solely on validation scores to select good features, despite the value of past experiment data for improving selection. ", "page_idx": 1}, {"type": "text", "text": "Motivated by this, we propose to approach this problem from a novel perspective: optimization to discover effective generation rules, leveraging the language understanding and reasoning capabilities of large language models (LLMs). Recent research has demonstrated that LLMs can optimize various non-differentiable problems using prompts that describe the optimization task in natural language [24, 25, 26]. This suggests the potential for LLMs to automatically generate and iteratively refine feature generators without the need for manually specifying the rule space. For example, the reasoning capabilities of LLMs allow incorporating feedback on their previous outputs into the process for iterative refinement. Moreover, linguistic contexts, such as column names (e.g., \u2018Gender\u2019 and \u2018Age\u2019) and categorical values (e.g., \u2018Female\u2019 and \u2018Male\u2019), could be naturally integrated into the optimization [27, 28, 29, 30], which is difficult, if not impossible, with conventional methods. ", "page_idx": 1}, {"type": "text", "text": "Contributions. In this work, we leverage LLMs to generate novel column features for tabular prediction tasks, proposing Optimizing Column feature generator with decision Tree reasoning (OCTree), a generic framework for automated feature generation using LLMs. Figure 1 illustrates an overview of our framework. Our approach begins by prompting an LLM to propose a name for a novel column feature based on the task description, such as \u2018Trading Volume\u2019 for stock price prediction. This initial suggestion guides the LLM in exploring and refining the corresponding feature values. Then, we further leverage the reasoning capability of LLMs to produce a good rule that generates values for the newly introduced column feature based on the existing ones. Specifically, starting from an initial rule $r_{0}$ , we let the LLM iteratively improve the current rule $r_{t}$ by using extracted reasonings $d_{0},d_{1},\\ldots,d_{t}$ and validation scores ${\\mathfrak{s o}},{\\mathfrak{s o}}_{1},\\ldots,{\\mathfrak{s}}_{t}$ attained by the prediction model as feedback. Here, $d_{t}$ denotes the language description of a decision tree fitted to the entire dataset, including the new feature generated by $r_{t}$ . Specifically, we propose using the decision tree reasoning to provide the LLM with effective knowledge from the past experiments, i.e., the prediction model trained with the generated features, providing learned knowledge about the entire dataset. This procedure is iterated for a fixed number of times, after which we select the rule with the highest validation score. ", "page_idx": 1}, {"type": "text", "text": "We assess the effectiveness of OCTree on a wide range of real-world datasets (e.g., stock price and patient mortality prediction) from various sources, including recent Kaggle competitions. Our experimental results demonstrate that OCTree consistently enhances the performance of various prediction models, including gradient-boosted decision trees [11] and deep neural networks [31, 32], for both classification and regression tasks. We also assess OCTree on datasets where language descriptions are unavailable, i.e., all feature values and column names are anonymized during preprocessing. Even on these datasets, OCTree reduces relative prediction errors by an average of $5.0\\%$ compared to the best baseline model, i.e., XGBoost on the 19 classification tasks benchmarked by Grinsztajn et al. [13]. Here, we use the Llama 2 7B model fine-tuned on high-quality dialogue data to enhance its ability to understand and generate contextually relevant, coherent rules. We also show that OCTree outperforms recent automatic feature engineering methods, including CAAFE [19] and OpenFE [17], often using our 7B LLM, even when these methods are combined with significantly more advanced models like GPT-4. Lastly, we demonstrate that features generated for one type of model (e.g., a simpler model like XGBoost) can enhance the performance of other model types (e.g., more complex models like neural networks). This illustrates a potential approach to scaling the method for larger, more complex models. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Tabular learning with LLMs. Recent developments in LLMs have encouraged investigation into their applications to tabular prediction tasks. Dinh et al. [27] and Hegselmann et al. [28] fine-tune GPT-3 [33] and T0 [34], respectively, by serializing tabular data into natural language. Nam et al. [35] utilizes unlabeled data expressed in natural language for few-shot semi-supervised tabular classification tasks via prompting LLMs. More recently, Yan et al. [36] introduced a tabular-specific tokenization method to pre-train a single language model capable of performing well across multiple tabular datasets. Instead of using LLMs as prediction models, we explore whether they can effectively generate useful column features for tabular prediction tasks. Specifically, we propose enhancing various prediction models by using LLMs as optimizers to generate novel column features. ", "page_idx": 2}, {"type": "text", "text": "LLMs as optimizers. Various prompting techniques have demonstrated the use of LLMs for solving optimization problems. This is achieved by describing optimization problems in natural language and instructing LLMs to iteratively generate new solutions based on previously found solutions and their evaluation. In particular, Yang et al. [25] uses LLMs to optimize linear regression, the traveling salesman problem, and prompt optimization (i.e., refining instructions to improve LLM outputs). Building on these insights, we leverage LLMs to optimize feature generators for tabular prediction tasks. Unlike prior work, we incorporate decision tree reasoning as feedback, providing the model with learned knowledge about the dataset in natural language for more effective optimization. ", "page_idx": 2}, {"type": "text", "text": "Automated feature engineering. Automated feature engineering involves generating features from raw data without human effort to improve the performance on prediction tasks [19]. Various methods have been developed for this purpose [37, 38, 39], including iterative feature subsampling with beam search to select informative features [23] and feature boosting and pruning algorithms for efficient and accurate filtering [17]. More recently, Hollmann et al. [19] introduced a context-aware feature engineering approach that leverages LLMs to generate semantically meaningful features based on the description of a given task. Unlike previous approaches, our method leverages the optimization and reasoning capabilities of LLMs to discover effective feature generation rules without the need for manually defining a search space. Furthermore, while methods such as CAAFE [19] rely on language-based context, our approach is applicable to both context-aware and context-agnostic settings, enabling it to handle a broader range of prediction tasks. ", "page_idx": 2}, {"type": "text", "text": "3 Optimizing feature generator with decision tree reasoning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce a framework for automated tabular feature engineering by leveraging the language understanding and reasoning capabilities of LLMs. In a nutshell, our approach utilizes LLMs as optimizers to propose and refine the rules for generating column features. Specifically, we iteratively improve the rules by guiding the LLMs using (1) the validation performance of previously proposed rules and (2) decision tree-based reasoning derived from the training data as inputs, enabling more effective optimization. We begin by framing the feature engineering problem as an optimization of the rules for generating column features (Section 3.1) and then introduce the core framework, termed Optimizing Column feature generator with decision Tree reasoning (OCTree), designed to solve this optimization task (Section 3.2). ", "page_idx": 2}, {"type": "text", "text": "Problem setup. Formally, the goal of tabular prediction tasks is to train a prediction model $f:\\mathcal X\\to$ $\\boldsymbol{\\wp}$ , where $\\mathcal{X}$ is the input space and $\\mathbf{x}\\in\\mathcal{X}$ is an $M$ -dimensional column feature with corresponding column names $\\mathcal{C}\\,=\\,\\{c_{1},...\\,,c_{M}\\}$ . For example, $\\cdot_{x_{1}}$ : Female\u2019 and $\\cdot_{x_{2}}$ : $36^{\\circ}$ are values for the columns $\\cdot_{c_{1}}$ : Gender\u2019 and $'c_{2}$ : Age\u2019, respectively. In classification tasks, $\\mathbf{y}\\,\\in\\,\\mathcal{Y}\\,=\\,\\{0,1\\}^{K}$ represents the label space with $K$ classes, while in regression tasks, $\\mathbf{y}\\in\\mathcal{Y}\\subset\\mathbb{R}$ . We denote by $c_{\\tt t a r g e t}$ the name of the column corresponding to the label y. ", "page_idx": 3}, {"type": "text", "text": "3.1 Tabular feature generation as rule generator optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We frame tabular feature engineering as the optimization of feature-generating rules, where the rules define a mapping from the original set of features to a new feature. Our objective is to generate a one-dimensional column feature, $\\mathcal{X}^{\\prime}$ , by optimizing the rule $r:\\mathcal{X}\\rightarrow\\mathcal{X}^{\\prime}$ . Specifically, we aim to create a novel column feature that improves the performance of the prediction model when trained with the new feature, $f:\\mathcal{X}\\oplus\\mathcal{X}^{\\prime}\\to\\mathcal{Y}$ . Our optimization problem can be formalized as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{r}\\mathcal L_{f^{*}}(\\mathcal D_{\\mathrm{val}}\\oplus r)\\quad\\mathrm{subject}\\ \\mathrm{to}\\quad f^{*}=\\arg\\operatorname*{min}_{f}\\mathcal L_{f}(\\mathcal D_{\\mathrm{train}}\\oplus r),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $g$ is the rule generator (i.e., LLM $\\mathcal{M}$ in our case), $r:=g(D_{\\tt t r a i n})$ is the rule generated based on the training dataset $\\mathcal{D}_{\\mathtt{t r a i n}}$ , and $\\mathcal{D}\\oplus r:=\\{\\mathbf{x}_{i}\\oplus r(\\mathbf{x}_{i}),\\mathbf{y}_{i}\\}_{i=1}^{N}$ denotes the dataset augmented with the new column feature generated. $\\mathcal{L}_{f}$ is the objective function for the given prediction task, such as mean absolute error for regression tasks, evaluated using the model $f$ . In summary, we optimize the rule $r$ to achieve the best validation score measured on $\\ensuremath{\\mathcal{D}}_{\\mathrm{val}}\\oplus r$ with the model $f^{*}$ trained to minimize the loss on $\\ensuremath{\\mathcal{D}}_{\\mathtt{t r a i n}}\\oplus\\ensuremath{r}$ . ", "page_idx": 3}, {"type": "text", "text": "However, such bi-level optimization is often non-differentiable or computationally demanding, as it involves computing gradients through the optimization of $f^{*}$ . Moreover, the rule itself may involve non-differentiable operations, such as logical conjunctions between categorical features (e.g., \u2018Is a Smoker $=$ Has Fever $\\wedge$ Has Difficulty Breathing\u2019 as in Figure 1). One approach to addressing the issue is to use black-box optimization methods, such as evolutionary strategies [40] or reinforcement learning [41]. However, these methods also have limitations, including the need for a manually defined search space, which is complex, as well as the potentially suboptimal use of valuable feedback from previously proposed solutions that could enhance the optimization process. ", "page_idx": 3}, {"type": "text", "text": "To address this issue, we propose leveraging LLMs as optimizers for the tabular feature engineering problem. Our approach involves iteratively proposing and refining rules by prompting LLMs with a trajectory of feedback, which includes the history of previously proposed rules, the validation scores, and the associated reasoning information. Optimization using LLMs [25] has proven to be an effective tool, particularly for black-box optimization problems, which we show can also be effective in tabular feature engineering. Furthermore, LLMs can leverage the semantics of column and feature values for better optimization and provide the flexibility to operate without a pre-defined search space. ", "page_idx": 3}, {"type": "text", "text": "3.2 Generating column features with OCTree ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now present the core algorithm. First, we prompt the LLM to propose a name of a new column, such as \u2018Smoking Status\u2019 in Figure 1, and the corresponding rule based on the task description. We then compute the validation score of the prediction model and extract decision tree reasoning from the training dataset, initializing the optimization trajectory. This trajectory provides feedback to the LLM, with the decision tree reasoning component, which effectively conveys the knowledge of past experiments and captures the quality of previously suggested features. As the optimization process continues, the trajectory is updated, enabling the LLM to refine and enhance the rule iteratively. ", "page_idx": 3}, {"type": "text", "text": "Column name generation. OCTree begins by generating a name of a new column feature, $\\mathit{C}_{\\mathtt{n e w}}$ , through the LLM $\\mathcal{M}$ . This is done by prompting $\\mathcal{M}$ with the prompt $p_{\\mathsf{c o l}}$ (see Appendix A.1), which asks for a new column name that would be useful for predicting the target: $c_{\\mathrm{new}}=\\mathcal{M}(p_{\\mathsf{c o l}}(\\mathcal{C},c_{\\mathsf{t a r g e t}}))$ . Leveraging its language understanding capabilities, the LLM is able to generate semantically coherent and relevant column names. For instance, it might suggest using trading volume as a new feature for predicting stock prices. ", "page_idx": 3}, {"type": "text", "text": "Initialize optimization and extract decision tree reasoning. OCTree then generates an initial rule $r_{0}$ for deriving a new column feature from the original set of columns $\\mathcal{C}$ . This is done by prompting the LLM $\\mathcal{M}$ with the prompt $p_{\\mathtt{i n i t}}$ (see Appendix A.2) to propose a rule for predicting $\\mathcal{C}_{\\tt\\Omega e w}$ with $c_{1},\\ldots,c_{M}$ : $r_{0}\\,=\\,\\mathcal{M}(p_{\\mathrm{init}}(\\mathcal{C},c_{\\mathrm{new}}))$ . The score $s_{0}$ for the initial rule is then evaluated with the prediction model $f^{*}\\colon s_{0}={\\mathcal{L}}_{f^{*}}(D_{\\mathrm{val}}\\oplus r_{0})$ . Additionally, decision tree reasoning $d_{0}$ is extracted using CART [42] fitted to the training dataset: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nd_{0}=\\mathsf{C A R T}({\\mathcal D}_{\\mathsf{t r a i n}}\\oplus r_{0}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "CART is a binary decision tree that recursively splits the data based on criteria such as Gini impurity to predict the outcome. We employ CART for two main reasons: (i) tree-based models, often ensembles of simple decision trees like CART, outperform deep learning on many tabular prediction tasks, and (ii) CART is easily interpretable and can be expressed in natural language. For example, as illustrated in Figure 1, CART can be expressed using a simple if-else syntax. Intuitively, the decision tree reasoning extracted by CART provides valuable insights learned from the entire training dataset. It explicitly highlights the columns that are considered more significant (as nodes in the tree) and the corresponding values (as thresholds of the nodes) used for prediction. ", "page_idx": 4}, {"type": "text", "text": "Optimization with decision tree reasoning. To optimize the rule, we describe the task in natural language and provide the trajectory $\\boldsymbol{\\mathcal{T}_{t}}=\\{(\\bar{s}_{i},d_{i},\\bar{r_{i}})\\}_{i=0}^{t}$ , which includes the history of previously proposed rules, the corresponding scores, and associated reasoning information. We then generate a new rule $r_{t+1}$ using the LLM $\\mathcal{M}$ with the prompt $p_{\\mathtt{g e n}}$ (see Appendix A.3), which asks $\\mathcal{M}$ to propose a new rule that is not present in $\\mathcal{T}_{t}$ and that would improve on the scores from the previous iterations: $r_{t+1}=\\mathcal{M}(p_{\\mathbf{gen}}(\\mathcal{T}_{t},\\bar{\\mathcal{C}},c_{\\mathbf{target}}))$ . The elements in the optimization trajectory are ordered by the scores, as LLMs tend to generate suggestions that appear later in the list [25, 43]. Afterward, we append the new score $\\boldsymbol{s}_{t+1}=\\boldsymbol{\\mathcal{L}}_{f^{*}}(D_{\\mathrm{val}}\\hat{\\oplus}\\boldsymbol{r}_{t+1})$ , the decision tree reasoning $d_{t+1}=\\operatorname{CART}(\\mathcal{D}_{\\tan\\Phi}\\,r_{t+1})$ , and the rule $r_{t+1}$ to the trajectory: $T_{t+1}=T_{t}\\cup\\{(s_{t+1},d_{t+1},r_{t+1})\\}$ . The optimization proceeds for a fixed number of iterations, with the best rule selected based on the highest validation score. ", "page_idx": 4}, {"type": "text", "text": "Generating multiple features. The optimization process can be repeated to generate multiple useful features. For example, after generating the column \u2018Smoking Status\u2019, an additional column \u2018Physical Activity Level\u2019 can be generated based on the original features and the newly created \u2018Smoking Status\u2019. Formally, we first generate a new column $\\mathcal{X}^{\\prime}=r_{\\mathrm{opt}}(\\mathcal{X})$ , where $r_{\\tt o p t}$ is the optimized rule for generating the new feature $\\mathcal{X^{\\prime}}$ . This results in an augmented input space, ${\\mathcal{X}}^{\\mathrm{new}}={\\mathcal{X}}\\oplus{\\mathcal{X}}^{\\prime}$ . Using the updated ", "page_idx": 4}, {"type": "image", "img_path": "APSBwuMopO/tmp/b40d4f2a5db8d0bac0575e29ecd217d8a1b9b6f448f1ce1c29d69899cab996ee.jpg", "img_caption": ["Figure 2: Generation of multiple features. The optimization process is repeated to generate multiple column features in sequence. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "dataset $\\mathcal{D}^{\\mathrm{new}}\\subseteq\\mathcal{X}^{\\mathrm{new}}\\times\\mathcal{Y}$ , OCTree iteratively generates additional column features. This process continues, introducing new features in sequence until the validation score no longer improves. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we evaluate the effectiveness of OCTree across a range of tabular classification and regression tasks using diverse datasets. Our findings demonstrate that OCTree consistently improves the performance of various types of prediction models (Section 4.1 and Section 4.2). Furthermore, ablation studies confirm the value of the proposed decision tree reasoning in the optimization and demonstrate that features generated using one type of prediction model can effectively transfer to others, suggesting an approach to scaling the framework to more complex models (Section 4.3). ", "page_idx": 4}, {"type": "text", "text": "Datasets. First, we select real-world datasets with language descriptions of the column features from diverse sources, including the Disease, Academic, Enefti, and Tesla Stock datasets, recently released on Kaggle, and the Clinical Trial dataset from the US National Library of Medicine. These prediction tasks are highly practical and relevant to domains such as healthcare (e.g., diagnostics), academia (e.g., student dropout prediction), and finance (e.g., stock price forecasting). In addition, to evaluate OCTree on datasets without language descriptions of the columns, we include the 19 classification datasets benchmarked by Grinsztajn et al. [13]. When selecting datasets, it is important to consider the heterogeneous nature of tabular data [44] and ensure coverage of both categorical and numerical features, as well as both classification and regression tasks. We conduct experiments on this diverse selection of datasets to evaluate OCTree\u2019s general applicability across various types of tabular data. Further details on the datasets are provided in Appendix B. ", "page_idx": 4}, {"type": "table", "img_path": "APSBwuMopO/tmp/c13d36381e833253ed525878097d199cb428b798bed9e8e3fe069ee90a75a6a7.jpg", "table_caption": ["Table 1: Performance improvement by OCTree on datasets with language descriptions. We report test error rates $(\\%)$ for three classification tasks $(^{*})$ and mean absolute error $(\\times10^{-3})$ for two regression tasks $(^{\\dagger})$ . The lowest errors are highlighted in bold. Values in parentheses indicate the relative error rate reduction from the baseline. We report the mean error and standard deviation across three random splits, except for two regression tasks (time series tabular data), which are split by time index. N/A indicates that the method is not applicable, as HyperFast is a classification model. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Baselines. To validate our method, we evaluate it across three types of prediction models. We first consider XGBoost [11], a highly competitive tree-based model known for its effectiveness in the tabular domain. Second, we apply our method to multilayer perceptron (MLP; Gorishniy et al. [31]), the base architecture for deep learning models. Lastly, we show that OCTree improves the performance of HyperFast [32], a recently introduced model designed for fast classification of tabular data. Implementation details, including hyperparameter search space, are provided in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "Common setup. For all datasets, $60\\%$ of the data is used for training, $20\\%$ for validation, and $20\\%$ for testing. Following Gorishniy et al. [31], we use learned embeddings for categorical features when training MLPs, while HyperFast handles categorical features automatically. For all experiments, we use CART with a maximum depth of 4 to extract decision tree reasoning, provided to the rule generating LLM in the prompt. Unless noted otherwise, we use the Llama 2 model [45] at the 7B scale, fine-tuned on UltraChat [46], a dialogue dataset that has been used to develop strong chat models such as UltraLM [46]. Our findings indicate that open models, even at moderate scales, can be highly effective, particularly when equipped with strong chat capabilities. Further comparisons across different types of LLMs are provided in Section 4.2. ", "page_idx": 5}, {"type": "text", "text": "4.1 Main results: Context-aware feature engineering ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets with language descriptions. We first experiment with datasets where language descriptions of the columns and categorical feature values are available. In these cases, the LLM generates a logical rule in natural language, which is then converted into Python code for use in our experiments. For this task, we utilize both GPT-4o and our Llama 2 model, as the Llama 2 model can be limited by its relatively short context length on some datasets. See Appendix A.4 for the prompt used. ", "page_idx": 5}, {"type": "text", "text": "As shown in Table 1, OCTree consistently enhances the performance of various baseline models. For instance, when generating the column \u2018Trading Volume\u2019 for the Tesla Stock dataset using Llama 2 for XGBoost, the relative error is reduced by $15.9\\%$ . OCTree is compatible with arbitrary LLMs, allowing more advanced models to enhance the quality of generated features further. Specifically, with GPT-4o, one of the latest LLMs from OpenAI, our method achieves a relative error reduction of $17.1\\%$ on the same dataset for XGBoost. We provide results on additional datasets in Table 13. ", "page_idx": 5}, {"type": "text", "text": "Comparison with CAAFE. CAAFE [19] also introduces a feature engineering approach that utilizes LLMs to construct features based on the linguistic context. However, it is important to note that CAAFE requires explicit language descriptions of the features, which limits its applicability when such information is unavailable. For example, feature names and values are often obfuscated for confidentiality, a common practice in the financial and medical domains. In contrast, our method can be effectively applied to datasets without linguistic descriptions, as demonstrated in Table 3. ", "page_idx": 5}, {"type": "table", "img_path": "APSBwuMopO/tmp/6b751a8c7f75846a98c186d28b5ac9972796406217acac929803a05dacb0250b.jpg", "table_caption": ["Table 2: Applicability and comparison of automated feature engineering methods. We report the mean error $(\\%)$ and standard deviation across the six datasets with language descriptions used in Tables 1 and 13. The lowest error is highlighted in bold. Values in parentheses indicate the relative error rate reduction from the baseline model (i.e., XGBoost [11]), while N/I indicates no gain. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Thus, to compare CAAFE with OCTree, we evaluate both methods on datasets with contextual information, particularly all of the six classification datasets used in Tables 1 and 13. The results in Table 2 show that our method significantly outperforms CAAFE with GPT-4o, even when using our custom Llama 2 model fine-tuned on open dialogue data. Also, conventional feature engineering methods, such as OpenFE [17], often struggle to generate meaningful features, particularly due to the difficulty of applying arithmetic operations to categorical features. A key distinction between CAAFE and OCTree is that our approach generates more semantically meaningful column names, which serve as a basis for creating high-quality features. Leveraging the LLM\u2019s reasoning and in-context learning capabilities, we guide the model in effectively navigating the feature space to generate coherent, relevant rules, using a history of feedback on candidate features and decision tree reasoning to enhance its understanding of the data. In contrast, CAAFE primarily relies on language understanding to suggest simple combinations of existing feature. Moreover, CAAFE tends to adopt a greedy approach, evaluating the validation score for a candidate feature only once and discarding it if no improvement is observed. We provide further results with case studies in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "4.2 Main result: Context-agnostic feature engineering ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets without language descriptions. In practice, datasets do not always include clear language descriptions of the prediction task. For example, feature names and values in financial datasets are often obfuscated with arbitrary symbols to protect confidentiality [47]. OCTree adapts easily to such datasets without language descriptions and can generate features using various arithmetic rules. As shown in our ablations in Section 4.3, this is due to the more effective use of LLMs\u2019 optimization capabilities with decision tree reasoning that enhances the model\u2019s understanding of the data. ", "page_idx": 6}, {"type": "text", "text": "To evaluate datasets without language descriptions, we apply ordinal encoding to categorical features and normalize all features using a min-max scaler, transforming the original numeric values. We also use non-descriptive column names, such as $\\mathcal{C}=\\{\\cdot\\mathbf{x}1^{\\flat},\\,\\cdot\\mathbf{x}2^{\\flat},\\ldots,\\,\\cdot\\mathbf{x}5^{\\flat}\\}$ for a dataset with $M=5$ columns. To initialize the feedback trajectory, we create an initial rule that is the product of the two columns with the highest importance weights computed using an XGBoost model, e.g., $x_{6}=x_{1}\\times\\,x_{5}$ . As shown in Table 3, our framework enhances baseline models even in the absence of language descriptions, achieving an average error reduction of approximately $5.0\\%$ for both the XGBoost classifier and MLP. For HyperFast, OCTree also improves the test error on 16 out of 19 datasets. ", "page_idx": 6}, {"type": "text", "text": "Analysis of experiments with various open LLMs. To evaluate how our method performs with various types of LLMs, we assess the performance of several open LLMs as rule generators. As shown in Table 4, while all models yield improvements over the baseline, we find our own model (i.e., Llama 2 fine-tuned on UltraChat) to be particularly effective. This shows that our framework can be effectively implemented even with open models at a moderate scale, especially those with sufficiently strong chat capabilities. We suspect that this improvement stems from the enhanced ability of these models to understand and generate contextually relevant and coherent rules, resulting in better optimization outcomes. Specifically, as illustrated by the examples in Appendix F, our model navigates a broader space of features more effectively than the base Llama 2 chat model and demonstrates the ability to utilize built-in Python functions such as \u2018abs()\u2019. Code Llama also demonstrates strong performance due to its training on code data that includes a variety of arithmetic rules, which is especially useful for generating rules for datasets lacking language descriptions. Notably, the model can leverage a range of NumPy operations (e.g., \u2018np.sin\u2019), allowing it to produce more mathematically complex features. This suggests that LLMs trained on high-quality code and dialogue data could serve as even more effective rule generators within our framework. ", "page_idx": 6}, {"type": "table", "img_path": "APSBwuMopO/tmp/82f025f0fc56bcfe873187954015ba03d0eafdc0266ea4d301dc8e1624b237e0.jpg", "table_caption": ["Table 4: OCTree with Llama 2 variants. We report the average test error rates $(\\%)$ and standard deviations across three random seeds on the 19 datasets without language descriptions. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "APSBwuMopO/tmp/dd817e883a8e4d4c6f526bdbbd69fcfbf25981b54f00aea55c9302f1712d161a.jpg", "table_caption": ["Table 3: Performance improvement by OCTree on datasets without language descriptions. We report test error rates $(\\%)$ on the 19 classification tasks from Grinsztajn et al. [13]. The lowest error is in bold. Values in parentheses indicate the relative error rate reduction from the baseline, while N/I indicates no gain. We report the mean error and standard deviation across the three random splits. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "APSBwuMopO/tmp/a0470be6028d54061c0b11dd223cd19065364d87aac75430db59d1fd1057d6a3.jpg", "table_caption": ["Table 5: Comparison with automatic feature engineering methods. We report the mean error $(\\%)$ and standard deviation across the 22 datasets used in Tables 1 and 3. The lowest error is highlighted in bold, and the second lowest is underlined. Values in parentheses indicate the relative error rate reduction from the baseline model. OCTree\u2020 refers to our method integrated with other approaches. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.3 Ablations and analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Integrating with other automated feature engineering methods. OCTree is complementary to existing feature engineering methods, allowing for integration in several natural ways. One simple approach is to first apply OCTree to generate features, followed by the use of other methods to further refine and augment the feature set. In Table 5, we first compare the standalone performance of OCTree, demonstrating that it outperforms competing state-of-the-art automated feature engineering methods (i.e., AutoFeat [23] and OpenFE [17]) for both XGBoost and MLP models. Moreover, combining OCTree with OpenFE (denoted as OCTree\u2020 in Table 5) further boosts performance, achieving a $7.9\\%$ reduction in relative error for XGBoost. ", "page_idx": 7}, {"type": "text", "text": "Ablation study on the proposed components. As shown in Table 6, our framework consists of two essential components: (i) generating new column features (denoted as Gen. Feat. in Table 6), and (ii) providing explicit decision tree reasoning as feedback (denoted as DT reasoning in Table 6) during the optimization process. First, note that the rules are sufficiently well-optimized even without an explicit decision tree provided as feedback; the LLM improves performance based solely on score feedback. However, providing the decision tree as feedback to the LLM can lead to even better performance. We believe that decision tree reasoning, which highlights important columns and their threshold values, enables the LLM to understand the data better, resulting in the generation of more contextually relevant and useful rules. Moreover, decision trees can be easily represented in natural language using if-else syntax, effectively conveying the information about the data to the LLM. ", "page_idx": 7}, {"type": "table", "img_path": "APSBwuMopO/tmp/25184a60b1cc60e1f6b09950aba0b39467e8c3f3ac2aea289ee5aa418dcc5860.jpg", "table_caption": ["Table 6: Ablation study of the proposed decision tree reasoning. We report the mean error $(\\%)$ and standard deviation across three random splits on two datasets with language descriptions $(^{*})$ and two datasets without language descriptions $\\dot{(^{\\dag})}$ . The lowest error is highlighted in bold. Values in parentheses indicate the relative error rate reduction from the baseline model. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "APSBwuMopO/tmp/89c85860c6f18d8915dcf14eafcb54f24d9dbc3bfcd4a4df537d8dff1b59fab9.jpg", "table_caption": ["Table 7: Performance improvement through feature transfer. We optimize the feature generation rule using XGBoost and transfer the generated features to improve MLP and HyperFast $\\mathrm{(OCTree_{trans})}$ . We report the test error rates $(\\%)$ and standard deviation across three random seeds for two datasets with language descriptions $(^{*})$ and two datasets without $(^{\\dagger})$ . The lowest error is in bold, with values in parentheses indicating the relative error rate reduction from the baseline model. N/I denotes cases where no improvement was observed. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Transferring generated rules to other prediction models. While we optimize feature generation rules to improve the performance of a specific prediction model, the generated features can also be utilized in other models to achieve similar improvements. For example, it would be more efficient to generate features using XGBoost, which typically trains and evaluates faster than larger deep neural networks, and then apply these features to more complex models. To assess whether such feature transfer is feasible within our framework, we first optimize the column generation rules using XGBoost and then train MLP and HyperFast models with the generated features. As shown in Table 7, these features significantly improve the performance of both models, demonstrating the effectiveness of this approach, especially when only limited computational resources are available. ", "page_idx": 8}, {"type": "text", "text": "Evaluating the validity of generated features. The rule generator LLM is asked to recommend a new column feature that is not already present in the dataset. Here, we evaluate whether the LLM is capable of generating features that are, in fact, valid and relevant to the target task. We perform this analysis from two perspectives: (i) whether the LLM can identify the most relevant column feature when provided with multiple candidate columns, and (ii) whether using real-world data, when available, for the suggested column leads to improved performance of prediction models. ", "page_idx": 8}, {"type": "text", "text": "Our method is based on the assumption that sufficiently capable LLMs can understand the relationship between the target task and column features, enabling them to generate new, relevant features for the task. To evaluate this assumption, we first examine whether the LLMs can identify the features that are more relevant to the prediction task. For this experiment, we begin by removing two existing features from a dataset and then prompt the LLM to rank the two features according to their importance for the target task. Specifically, we remove \u2018Cholesterol Level\u2019 and \u2018Cough\u2019 from the original Disease dataset and then ask the LLM to identify the attribute more relevant to predicting whether a patient has a disease. Both the GPT-4o and Llama 2 models indicate that \u2018Cough\u2019 is more important than \u2018Cholesterol Level\u2019. As shown in Table 8, XGBoost achieves a lower error rate when trained with the \u2018Cough\u2019 feature compared to when trained with \u2018Cholesterol Level\u2019, which is consistent with the LLMs\u2019 assessment that the former is more relevant to the task. ", "page_idx": 8}, {"type": "text", "text": "Table 8: LLM identifies important features. We report the mean error $(\\%)$ and standard deviation across three random splits on the Disease dataset. Both GPT4o and Llama 2 identify the cough feature as more important, consistent with the accuracy seen in XGBoost models trained with and without these features. ", "page_idx": 9}, {"type": "table", "img_path": "APSBwuMopO/tmp/b2ca0bbd992b372ee60a410b00b89e9e9a097fdaf1494fab545f1ddb859e2987.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "APSBwuMopO/tmp/01ac4b94120469fefd2d8a77ab436c3586a58a6a1b34d4bcf141e3994fc8f0a9.jpg", "img_caption": ["Figure 3: Imputing features with real data, i.e., Age. We report the mean accuracy $(\\%)$ across three random splits on the Clinical dataset using XGBoost. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Building on the LLM\u2019s ability to identify important features for the target task, we further assess whether the generated columns can be imputed with real-world data to enhance prediction. In this experiment, we use the Clinical Trial dataset, where the LLM introduced the column \u2018Age\u2019 when prompted to suggest a new column. We then incorporated real-world age data from the US National Library of Medicine to augment the original dataset. As shown in Figure 3, training on this augmented data results in a notable improvement, demonstrating that the LLM-generated columns align well with real-world data. In conclusion, we recommend that practitioners utilize OCTree to either (i) identify additional column features and collect the corresponding real-world data or (ii) optimize feature generation rules in the absence of real-world data. ", "page_idx": 9}, {"type": "text", "text": "Analysis of the rule optimization process. We analyze how the output rules evolve throughout the optimization rounds on the electricity dataset. Due to space constraints, we show the first and last five output rules in Appendix H. In the early stages of optimization, the LLM generates a diverse range of outputs, indicating an active exploration of potential rules. In contrast, during the later stages, the LLM focuses on refining the solution space around previously identified rules, making only minor adjustments. This again demonstrates that with appropriate guidance through the optimization process, sufficiently capable LLMs can serve as highly effective optimizers. ", "page_idx": 9}, {"type": "text", "text": "Handling hallucinations. While LLMs may occasionally suggest suboptimal or semantically incoherent rules, our method is designed to address these hallucinations. Specifically, we provide feedback on previously generated rules to guide the LLMs in iteratively improving the rule generation. This feedback loop helps the LLMs avoid hallucinations that might lead to low validation scores in subsequent iterations. Empirically, we find that these issues are more common in the early stages when the LLMs explore the rule space more broadly and in less capable models, e.g., those without additional training on dialogue or code generation data. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose OCTree, a generic framework that leverages the power of LLMs (e.g., reasoning capability) for automatically generating column features for tabular prediction tasks. We evaluate the effectiveness of OCTree across various prediction tasks and demonstrate that our method consistently enhances the performance of diverse prediction models, often significantly more effectively than competing feature engineering methods. As future work, exploring feedback-based alignment methods, such as reinforcement learning from human feedback, to further enhance LLMs as rule generators would be an exciting direction to explore. ", "page_idx": 9}, {"type": "text", "text": "Limitation. One potential limitation of our work is that evaluating the generated features involves computing the validation scores of the prediction model, which can be time-consuming if the model requires extensive training. However, as demonstrated by the results in Table 7, this issue can be mitigated by first generating features with a simpler prediction model and then transferring those features to the target model, reducing the overall runtime and computational requirements. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements and disclosure of funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank Sihyun Yu, Subin Kim, Changyeon Kim, Daewon Choi, Jinseong Park, and anonymous reviewers for their helpful feedback and discussions. This work was supported by Institute for Information & communications Technology Promotion (IITP) grant funded by the Korea government (MSIT) (No.RS-2019-II190075, Artificial Intelligence Graduate School Program (KAIST); No.RS-2022-II220184, 2022-0-00184, Development and Study of AI Technologies to Inexpensively Conform to Evolving Policy on Ethics), the NIPA (National IT Industry Promotion Agency), through the Ministry of Science and ICT (Hyperscale AI flagship project), and Hyundai Motor Group. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012.   \n[2] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1\u20139, 2015.   \n[3] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738, 2020.   \n[4] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XI 16, pages 776\u2013794. Springer, 2020.   \n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[6] Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence representations. arXiv preprint arXiv:1803.02893, 2018.   \n[7] Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. Vime: Extending the success of self-and semi-supervised learning to tabular domain. Advances in Neural Information Processing Systems, 33:11033\u201311043, 2020.   \n[8] Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. Tabtransformer: Tabular data modeling using contextual embeddings. arXiv preprint arXiv:2012.06678, 2020. [9] Talip Ucar, Ehsan Hajiramezanali, and Lindsay Edwards. Subtab: Subsetting features of tabular data for self-supervised representation learning. Advances in Neural Information Processing Systems, 34:18853\u201318865, 2021.   \n[10] Bingzhao Zhu, Xingjian Shi, Nick Erickson, Mu Li, George Karypis, and Mahsa Shoaran. Xtab: Cross-table pretraining for tabular transformers. arXiv preprint arXiv:2305.06090, 2023.   \n[11] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785\u2013794, 2016.   \n[12] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. Catboost: unbiased boosting with categorical features. Advances in neural information processing systems, 31, 2018.   \n[13] L\u00e9o Grinsztajn, Edouard Oyallon, and Ga\u00ebl Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? Advances in Neural Information Processing Systems, 35:507\u2013520, 2022.   \n[14] Kuan-Yu Chen, Ping-Han Chiang, Hsin-Rung Chou, Ting-Wei Chen, and Tien-Hao Chang. Trompt: Towards a better deep neural network for tabular data. arXiv preprint arXiv:2305.18446, 2023.   \n[15] Duncan McElfresh, Sujay Khandagale, Jonathan Valverde, Ganesh Ramakrishnan, Micah Goldblum, Colin White, et al. When do neural nets outperform boosted trees on tabular data? Advances in Neural Information Processing Systems, 2023.   \n[16] Valeriia Cherepanova, Roman Levin, Gowthami Somepalli, Jonas Geiping, C Bayan Bruss, Andrew Gordon Wilson, Tom Goldstein, and Micah Goldblum. A performance-driven benchmark for feature selection in tabular deep learning. Advances in Neural Information Processing Systems, 2023.   \n[17] Tianping Zhang, Zheyu Aqa Zhang, Zhiyuan Fan, Haoyan Luo, Fengyuan Liu, Qian Liu, Wei Cao, and Li Jian. Openfe: Automated feature generation with expert-level performance. In International Conference on Machine Learning, pages 41880\u201341901. PMLR, 2023.   \n[18] Avinash Amballa, Anmol Mekala, Gayathri Akkinapalli, Manas Madine, Naga Pavana Priya Yarrabolu, and Przemyslaw A Grabowicz. Automated model selection for tabular data. arXiv preprint arXiv:2401.00961, 2024.   \n[19] Noah Hollmann, Samuel M\u00fcller, and Frank Hutter. Large language models for automated data science: Introducing caafe for context-aware automated feature engineering. Advances in Neural Information Processing Systems, 36, 2024.   \n[20] Leo Breiman. Random forests. Machine learning, 45:5\u201332, 2001.   \n[21] Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pages 1189\u20131232, 2001.   \n[22] Huan Liu and Lei Yu. Toward integrating feature selection algorithms for classification and clustering. IEEE Transactions on knowledge and data engineering, 17(4):491\u2013502, 2005.   \n[23] Franziska Horn, Robert Pack, and Michael Rieger. The autofeat python library for automated feature engineering and selection. In Machine Learning and Knowledge Discovery in Databases: International Workshops of ECML PKDD 2019, W\u00fcrzburg, Germany, September 16\u201320, 2019, Proceedings, Part I, pages 111\u2013120. Springer, 2020.   \n[24] Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt\u00e4schel. Promptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint arXiv:2309.16797, 2023.   \n[25] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.   \n[26] Michael R Zhang, Nishkrit Desai, Juhan Bae, Jonathan Lorraine, and Jimmy Ba. Using large language models for hyperparameter optimization. arXiv e-prints, pages arXiv\u20132312, 2023.   \n[27] Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong Sohn, Dimitris Papailiopoulos, and Kangwook Lee. Lift: Language-interfaced fine-tuning for non-language machine learning tasks. Advances in Neural Information Processing Systems, 35: 11763\u201311784, 2022.   \n[28] Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. Tabllm: Few-shot classification of tabular data with large language models. In International Conference on Artificial Intelligence and Statistics, pages 5549\u20135581. PMLR, 2023.   \n[29] Hariharan Manikandan, Yiding Jiang, and J Zico Kolter. Language models are weak learners. Advances in Neural Information Processing Systems, 2023.   \n[30] Jaehyun Nam, Woomin Song, Seong Hyeon Park, Jihoon Tack, Sukmin Yun, Jaehyung Kim, and Jinwoo Shin. Semi-supervised tabular classification via in-context learning of large language models. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023.   \n[31] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Revisiting deep learning models for tabular data. Advances in Neural Information Processing Systems, 34: 18932\u201318943, 2021.   \n[32] David Bonet, Daniel Mas Montserrat, Xavier Gir\u00f3-i Nieto, and Alexander G Ioannidis. Hyperfast: Instant classification for tabular data. AAAI Conference on Artificial Intelligence, 2024.   \n[33] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[34] Sanh Victor, Webson Albert, Raffel Colin, Bach Stephen, Sutawika Lintang, Alyafeai Zaid, Chaffin Antoine, Stiegler Arnaud, Raja Arun, Dey Manan, et al. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022.   \n[35] Jaehyun Nam, Woomin Song, Seong Hyeon Park, Jihoon Tack, Sukmin Yun, Jaehyung Kim, Kyu Hwan Oh, and Jinwoo Shin. Tabular transfer learning via prompting llms. Conference on Language Modeling, 2024.   \n[36] Jiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Chen, Jimeng Sun, Jian Wu, and Jintai Chen. Making pre-trained language models great on tabular prediction. In International Conference on Learning Representations, 2024.   \n[37] Wei Fan, Erheng Zhong, Jing Peng, Olivier Verscheure, Kun Zhang, Jiangtao Ren, Rong Yan, and Qiang Yang. Generalized and heuristic-free feature construction for improved accuracy. In Proceedings of the 2010 SIAM International Conference on Data Mining, pages 629\u2013640. SIAM, 2010.   \n[38] James Max Kanter and Kalyan Veeramachaneni. Deep feature synthesis: Towards automating data science endeavors. In 2015 IEEE international conference on data science and advanced analytics (DSAA), pages 1\u201310. IEEE, 2015.   \n[39] Liyao Li, Haobo Wang, Liangyu Zha, Qingyi Huang, Sai Wu, Gang Chen, and Junbo Zhao. Learning a data-driven policy network for pre-training automated feature engineering. In The Eleventh International Conference on Learning Representations, 2022.   \n[40] Benjamin Doerr and Frank Neumann. A survey on recent progress in the theory of evolutionary algorithms for discrete optimization. ACM Transactions on Evolutionary Learning and Optimization, 1(4):1\u201343, 2021.   \n[41] Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev. Reinforcement learning for combinatorial optimization: A survey. Computers & Operations Research, 134:105400, 2021.   \n[42] Wei-Yin Loh. Classification and regression trees. Wiley interdisciplinary reviews: data mining and knowledge discovery, 1(1):14\u201323, 2011.   \n[43] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023.   \n[44] Jaehyun Nam, Jihoon Tack, Kyungmin Lee, Hankook Lee, and Jinwoo Shin. Stunt: Few-shot tabular learning with self-generated tasks from unlabeled tables. In International Conference on Learning Representations, 2023.   \n[45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[46] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023.   \n[48] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 2623\u20132631, 2019.   \n[49] Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Pieter Gijsbers, Frank Hutter, Michel Lang, Rafael G Mantovani, Jan N van Rijn, and Joaquin Vanschoren. Openml benchmarking suites. arXiv preprint arXiv:1708.03731, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix: Optimized Feature Generation for Tabular Data via LLMs with Decision Tree Reasoning ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Prompt examples ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Generation of a new column ", "page_idx": 14}, {"type": "text", "text": "In Listing 1, we present the prompt $p_{\\mathsf{c o l}}$ , which instructs the LLM to generate a new column name $\\mathit{C}_{\\mathtt{n e w}}$ . The prompt includes a detailed explanation of each feature column, specifying its type and values. For convenience, we restricted $\\mathcal{C}_{\\sf n e w}$ to be either binary or categorical. ", "page_idx": 14}, {"type": "image", "img_path": "APSBwuMopO/tmp/50b596dc2d2243807c755af8f3779b7618fa440d9558eaae87341e432fe8de26.jpg", "img_caption": ["Listing 1: Prompt for the generation of a new column $c_{\\tt n e w}$ . "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.2 Rule initialization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Listing 2, we present the prompt $p_{\\mathtt{i n i t}}$ , which instructs the LLM to create an initial rule for generating a new column $\\mathcal{C}_{\\mathtt{n e w}}$ . The LLM generates a new column, $\\mathcal{C}_{\\mathtt{n e w}}$ , by considering the existing column features in the dataset and the semantic meaning of the name of $\\mathit{C}_{\\mathtt{n e w}}$ . ", "page_idx": 14}, {"type": "image", "img_path": "APSBwuMopO/tmp/a0d09bc66a178f7b3c2f34b331cd1cb800389a4414c66ebacd71a0e29cfd007c.jpg", "img_caption": ["Listing 2: Prompt for the rule initialization. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.3 Generation of a rule ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Listing 3, we present the prompt $p_{\\mathtt{g e n}}$ , which instructs the LLM to generate an improved rule compared to those previously generated. The prompt includes a list of (rule, tree-based reasoning, score) tuples. Since the score represents the accuracy of the XGBoost classifier, the trajectory is sorted in ascending order of score. For regression tasks using mean absolute error, the trajectory is sorted in descending order. ", "page_idx": 15}, {"type": "image", "img_path": "APSBwuMopO/tmp/df2fb07f11768780c0db4507cf872654541428b80373cf7d766d3b87e6ac70c4.jpg", "img_caption": ["Listing 3: Prompt for the rule generation. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.4 Translation of a rule into Python code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Listing 4, we present the prompt $p_{\\mathsf{c o d e}}$ , which instructs the LLM to translate the rule into Python code. To minimize potential syntactic errors, we impose several restrictions. First, we explicitly define the variable types wherever possible. We also instruct the LLM to account for the feature value types before performing calculations, e.g., to avoid the addition of categorical and numerical values. ", "page_idx": 16}, {"type": "image", "img_path": "APSBwuMopO/tmp/a11fe1e007946bd0537c4685e8f7db517c65e4dc169b3d69d0c5224a2364d6e8.jpg", "img_caption": ["Listing 4: Prompt for translating rule in natural language into Python code. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "B Dataset details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide further details on the datasets. ", "page_idx": 17}, {"type": "text", "text": "B.1 Datasets with language descriptions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Disease1 ", "page_idx": 17}, {"type": "text", "text": "A classification task to predict a patient\u2019s disease diagnosis based on the following attributes: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Binary (Yes or No): Fever, Fatigue   \n\u2022 Numerical (Range): Age $25\\sim90\\$ )   \n\u2022 Categorical: Gender (Female or Male), Blood Pressure (High, Normal, or Low), Cholesterol Level (High, Normal, or Low) ", "page_idx": 17}, {"type": "text", "text": "2. Clinical Trial2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A classification task to predict patient mortality in clinical trials based on the following attributes: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Binary (Yes or No) - Historical Disease: Deep Vein Thrombosis, Pulmonary Embolism, Antiandrogen Therapy, Cardiac Failure, Respiratory Failure, Venous Insufficiency, Coronary Artery Disease, Myocardial Infarction, Hypertension, Peripheral Arterial Occlusive Disease \u2022 Binary (Yes or No) - Medication: Dexamethasone, Ondansetron, Heparin, Fluorouracil, Ranitidine, Cisplatin, Metoclopramide, Carboplatin, Furosemide ", "page_idx": 17}, {"type": "text", "text": "3. Academic3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A classification task to predict whether a student would dropout based on the following attributes: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Numerical: Marital Status, Daytime/Evening Attendance, Previous Qualification, Nationality, Father\u2019s Qualification, Father\u2019s Occupation, Displaced, Debtor, Tuition Fees up to Date, Gender, Scholarship Holder, Age at Enrollment, International, Curricular Units 1st Sem (Approved), Curricular Units 1st Sem (Grade), Curricular Units 2nd Sem (Approved), Curricular Units 2nd Sem (Grade). ", "page_idx": 17}, {"type": "text", "text": "4. Enefit4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A regression task to predict daily energy consumption based on the following attributes: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Numerical: Prediction Unit Id, Day, Hour, Lowest Price Per MWh, Highest Price Per MWh, Installed Capacity, Euros Per MWh, Local Forecast Temperature, Local Forecast Dewpoint, Local Forecast Cloudcover Total, Local Forecast 10 Metre U Wind Component, Local Forecast 10 Metre V Wind Component, Local Forecast Direct Solar Radiation, Local Forecast Surface Solar Radiation Downwards, Local Forecast Total Precipitation. ", "page_idx": 17}, {"type": "text", "text": "5. Tesla Stock5 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A regression task to predict the target day\u2019s highest stock price based on the following attributes: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Numerical: Open Price of 2 Days Before, Highest Price of 2 Days Before, Lowest Price of 2 Days Before, Close Price of 2 Days Before, Open Price of 1 Day Before, Highest Price of 1 Day Before, Lowest Price of 1 Day Before, Close Price of 1 Day Before, Open Price of the Target Day, Time Index. ", "page_idx": 17}, {"type": "text", "text": "B.2 Datasets without language descriptions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For our main results on datasets without language descriptions (see Table 3), we evaluate the 19 classification datasets from the tabular benchmark proposed by Grinsztajn et al. [13]. Following the curation approach from Grinsztajn et al. [13]), we uniformly subsample to 50,000 instances for datasets exceeding this size. We provide brief dataset statistics below. ", "page_idx": 18}, {"type": "table", "img_path": "APSBwuMopO/tmp/d30cc73821ebb4d9bab12a9ba925846f06e985b59a7121654463ad948ccfc3b3.jpg", "table_caption": ["Table 9: Dataset statistics. 19 classification datasets benchmarked by Grinsztajn et al. [13]. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "C Baseline details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we describe the hyperparameter search space for the baseline models. For each random split of every dataset, we find the optimal set of hyperparameters using a random sampler run for 400 trials. We utilize the Optuna library [48] for the hyperparamter tuning. ", "page_idx": 18}, {"type": "text", "text": "C.1 XGBoost ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For XGBoost, we adopt the hyperparameter search space used in Grinsztajn et al. [13]. ", "page_idx": 18}, {"type": "table", "img_path": "APSBwuMopO/tmp/a950b0ed3d847fbb6167c8c270eb087d400d9336b86a4fd280f474c83347cf2d.jpg", "table_caption": ["Table 10: XGBoost [11] hyperparameters space. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.2 Multilayer perception (MLP) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For MLP, we adopt the hyperparameter search space from Grinsztajn et al. [13] and the architecture from Gorishniy et al. [31], which includes learning embeddings for categorical features. The models are trained for up to 300 epochs with early stopping, with the model that achieves the best validation score selected for evaluation. If validation scores do not improve for 40 epochs, training is stopped early. For learning rate scheduling, we use PyTorch\u2019s ReduceOnPlateau implementation. ", "page_idx": 19}, {"type": "table", "img_path": "APSBwuMopO/tmp/e0fb29d6718e04aa79dd9064b753b16c3083df78e77207ac7040449fea6fe39a.jpg", "table_caption": ["Table 11: MLP [31] hyperparameters space. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.3 HyperFast ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For HyperFast [32], we adopt the hyperparameter space from the original paper. ", "page_idx": 19}, {"type": "table", "img_path": "APSBwuMopO/tmp/af814039997e4876758227d72985f76a7971d4694db0fb49aae0286f4195d162.jpg", "table_caption": ["Table 12: HyperFast [32] hyperparameters space. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "D Compute resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We conducted our experiments on a variety of machines, including ", "page_idx": 19}, {"type": "text", "text": "\u2022 CPU: Intel(R) Xeon(R) Gold 6226R, GPU: RTX 3090  \n\u2022 CPU: Intel(R) Xeon(R) Gold 6426Y, GPU: RTX 4090  \n\u2022 CPU: Intel(R) Xeon(R) Gold 6426Y, GPU: RTX A6000", "page_idx": 19}, {"type": "text", "text": "E Comparison with CAAFE ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "E.1 Main results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table 13: Performance improvements by OCTree on datasets with language descriptions. We report test error rates $(\\%)$ on six classification tasks $(^{*})$ ) and mean absolute errors $(\\times10^{\\bar{-}3})$ for two regression tasks $(^{\\dagger})$ . The lowest error is in bold. Values in parentheses indicate the relative error rate reduction from the baseline. We report the mean error and standard deviation across three random splits, except for the two regression tasks (time series tabular data), which are split by time index. GPT-4o was used for both CAAFE and OCTree. ", "page_idx": 20}, {"type": "table", "img_path": "APSBwuMopO/tmp/97e6fef0ea756c5cee1e45af674ab2521fbb9a9b442ed38a803d1c68afb097f6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "We conduct a comparative evaluation using all datasets (along with additional ones) with contextual information from the experiments summarized in Table 1. First, CAAFE [19] exhibited high variance, even on the same dataset split, partly due to the randomness associated with GPT-4o\u2019s temperaturebased sampling. At times, it failed to improve upon the baseline. To address this, we average the performance over three trials per random split and report the mean and variance. As shown in Table 13, our method consistently outperforms CAAFE. Note the official implementation of CAAFE has been slightly modified to accommodate regression tasks. ", "page_idx": 20}, {"type": "text", "text": "E.2 Case study ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "APSBwuMopO/tmp/d316e5efa0f39c97f43884b6a5b5048a3a84d0aed95568a5d0062553a085ea8c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Listing 5: New columns introduced by CAAFE [19]. ", "page_idx": 20}, {"type": "text", "text": "F Case studies on different types of LLMs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In Section 4.2, we evaluate three different LLMs and provide additional comparisons in this section. As shown in Table 4, our custom model (the Llama 2 model trained fine-tuned on a high-quality open dialogue dataset) achieves the lowest error rate, followed by Code Llama and the Llama 2 chat base model, both of which still outperform the baseline XGBoost. We also observed several notable differences in feature generation patterns among the models. ", "page_idx": 21}, {"type": "text", "text": "First of all, Code Llama utilizes various NumPy operations (e.g., \u2018np.sin\u2019) to generate mathematically sophisticated features: ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\bullet\\ \\mathbf{x}9=\\left(\\left(\\mathbf{x}4+0.24\\right)\\ast\\mathbf{x}1\\right)+\\left(\\left(\\mathbf{x}5+0.27\\right)\\ast\\mathbf{x}2\\right)}\\\\ &{\\bullet\\ \\mathbf{x}9=\\mathbf{np.sin}(\\mathbf{x}1)*\\mathbf{np.cos}(\\mathbf{x}4)}\\\\ &{\\bullet\\ \\mathbf{x}9=\\mathbf{x}4*\\mathbf{np.tan}(\\mathbf{x}8)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Secondly, Llama 2 chat base favors simple polynomial combinations: ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\bullet\\ \\mathbf{x}9=\\mathbf{x}4*\\mathbf{x}1*\\left(\\mathbf{x}2+\\mathbf{x}6\\right)**2}\\\\ &{\\bullet\\ \\mathbf{x}9=\\mathbf{x}4*\\mathbf{x}1**3*\\left(\\mathbf{x}2+\\mathbf{x}6\\right)**4*\\left(\\mathbf{x}7+\\mathbf{x}8\\right)}\\\\ &{\\bullet\\ \\mathbf{x}9=\\mathbf{x}4*\\mathbf{x}1**2*\\left(\\mathbf{x}2+\\mathbf{x}6\\right)**3*\\left(\\mathbf{x}7+\\mathbf{x}8\\right)*\\left(1+\\mathbf{x}5**4\\right)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, our custom model tends to explore a polynomial space of features while also utilizing built-in Python functions such as $\\mathbf{\\dot{a}}\\mathtt{b}\\mathbf{s}(\\mathbf{})^{\\bullet}$ . Compared to the Llama 2 chat base model, it more effectively navigates a broader space of features: ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\bullet\\;\\;\\mathbf{x9}=\\mathbf{x1}**2*\\left(\\mathbf{x2}-\\mathbf{x3}\\right)}\\\\ &{\\bullet\\;\\;\\mathbf{x9}=\\mathbf{abs}(\\mathbf{x1})**\\mathbf{x1}+\\mathbf{x2}-\\mathbf{x3}-\\mathbf{x4}-\\mathbf{x5}-\\mathbf{x6}-\\mathbf{x7}-\\mathbf{x8}}\\\\ &{\\bullet\\;\\;\\mathbf{x9}=\\mathbf{x1}*\\left(\\mathbf{x1}+2\\right)-\\mathbf{x2}*\\left(\\mathbf{x2}-0.5\\right)*\\left(\\mathbf{x2}-0.5\\right)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "These observations suggest that, while all three models can generate useful features, our custom model more effectively navigates a broader feature space, leading to the generation of even more valuable features. We suspect that further training on code data, particularly to enhance the ability to leverage scientific computing libraries like NumPy, could lead to additional performance improvements. ", "page_idx": 21}, {"type": "text", "text": "G Scalability of OCTree ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Here, we evaluate the scalability of our method on datasets with hundreds of features (e.g., 501) from the OpenML repository [49]. We choose XGBoost [11] as the baseline model, because it is the most competitive baseline in our main experiments (see Table 3). Additionally, we use GPT-3.5-Turbo as the rule generator, as the Llama 2-based model we primarily use is constrained by a relatively short context length, which becomes limiting as the prompt size in", "page_idx": 21}, {"type": "table", "img_path": "APSBwuMopO/tmp/76e9b177b9f05700e0d72d2bfca3bbb28fe0b30aae6e2cd17e357dbaa7954f7e.jpg", "table_caption": ["Table 14: OCTree on datasets with hundreds of features. We report the mean error $(\\%)$ and the lowest error is highlighted in bold. Values in parentheses indicate the relative error reduction from the baseline model (i.e., XGBoost [11]). "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "creases with the number of features. As shown in Table 14, our method scales effectively to datasets with a larger number of features. For example, on the madelon dataset, which contains 501 columns, our method reduces the relative error by $6.3\\%$ compared to the XGBoost baseline. ", "page_idx": 21}, {"type": "text", "text": "H Examples of rule optimization ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To identify an effective feature generation rule, we conduct 50 rounds of optimization with the LLM. In each round, the LLM is provided with an optimization trajectory that includes reasoning information from previous experiments. Consequently, the input prompt evolves throughout the optimization process, with later rounds incorporating more accumulated information. Below, we show the first and last five output rules generated during the optimization on the electricity dataset. ", "page_idx": 22}, {"type": "text", "text": "First five: ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\bullet\\ \\mathbf{x}12=\\mathbf{x}3+\\left(\\mathbf{x}5\\ast\\mathbf{x}8\\right)\\ast\\mathbf{x}2-\\mathbf{x}6}\\\\ &{\\bullet\\ \\mathbf{x}12=\\mathbf{x}5\\ast\\mathbf{x}2+\\left(\\mathbf{x}3\\ast\\mathbf{x}4-\\mathbf{x}6\\right)-\\mathbf{x}1}\\\\ &{\\bullet\\ \\mathbf{x}12=\\mathbf{x}11+\\left(\\mathbf{x}7\\ast\\mathbf{x}10\\right)\\ast\\mathbf{x}2-\\mathbf{x}8}\\\\ &{\\bullet\\ \\mathbf{x}12=\\mathbf{x}3+\\left(\\mathbf{x}6\\ast\\mathbf{x}8\\ast\\mathbf{x}2\\right)-\\mathbf{x}1}\\\\ &{\\bullet\\ \\mathbf{x}12=\\left(\\mathbf{x}1\\ast\\mathbf{x}6+\\mathbf{x}2\\ast\\mathbf{x}8\\right)\\ast\\mathbf{x}3-\\mathbf{x}7}\\end{array}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Last five: ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\bullet\\ \\mathbf{x}12=\\left(\\left(\\mathbf{x}2-\\mathbf{x}1\\right)\\ast\\left(\\mathbf{x}11-\\mathbf{x}4\\right)\\right)\\ast\\ast2-\\left(0.5\\ast\\left(\\mathbf{x}1-0.3\\right)\\right)}\\\\ &{\\bullet\\ \\mathbf{x}12=\\left(\\left(\\mathbf{x}2-\\mathbf{x}1\\right)\\ast\\left(\\mathbf{x}11-\\mathbf{x}4\\right)\\right)\\ast\\ast6}\\\\ &{\\bullet\\ \\mathbf{x}12=\\left(\\left(\\mathbf{x}2-\\mathbf{x}1\\right)\\ast\\left(\\mathbf{x}11-\\mathbf{x}4\\right)\\right)\\ast\\ast44-\\left(0.02\\ast\\left(\\mathbf{x}5-\\mathbf{x}7\\right)\\right)-0.5}\\\\ &{\\bullet\\ \\mathbf{x}12=\\left(\\mathbf{x}11-\\mathbf{x}1\\right)+\\left(\\mathbf{x}11\\ast\\left(\\mathbf{x}6-\\mathbf{x}7\\right)\\right)}\\\\ &{\\bullet\\ \\mathbf{x}12=\\left(\\left(\\mathbf{x}2-\\mathbf{x}1\\right)\\ast\\left(\\mathbf{x}11-\\mathbf{x}4\\right)\\right)\\ast\\ast3+\\left(0.1\\ast\\left(\\mathbf{x}5-\\mathbf{x}6\\right)\\right)\\ast\\ast2}\\end{array}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that in the early stages of optimization, the LLM tends to explore a wider variety of rules, allowing for greater exploration. In contrast, during the later stages, the model narrows its focus to refine features within a more constrained space. ", "page_idx": 22}, {"type": "text", "text": "I Broader impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Our method is particularly effective in scenarios where collecting real data is costly or restricted, such as in the finance or medical domains, where data availability is often limited due to privacy concerns. However, since the features generated by OCTree are artificial, it is crucial to carefully inspect these features for their relevance and reliability. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All claims in the introduction and abstract accurately reflect the contribution and scope. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We discuss in Section 5. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not have a theory in this paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide implementation details including hyperparameter search space of prediction models in Appendix C. We also provide where the datasets are from in Appendix B. Additionally, we provide all prompts we use in Appendix A. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide the URL of the code in the Abstract. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We provide the detail of data splits in Section 4, hyperparameters (including how they chosen, and optimizers) in Appendix C. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: All experiments are conducted with the same and commonly used random seed, and we report standard deviation across random splits. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide compute resources we used in Appendix D. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We do not have any ethical concerns. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have discussed the societal impact in Appendix I. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our framework does not introduce risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have cited all papers and datasets in Reference and Appendix B. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 27}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We do not involve crowdsourcing nor research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We do not have human subject. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]