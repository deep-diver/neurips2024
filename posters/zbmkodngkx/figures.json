[{"figure_path": "zBMKodNgKX/figures/figures_2_1.jpg", "caption": "Figure 1: An illustration of one round of FEDNE. Besides the general steps in FEDAVG: \u2460 \u2192 \u2464 \u2192 \u2465, our local surrogate model training (\u2461) can be smoothly integrated into the whole pipeline. Then, each client conducts its local training (\u2464) using the augmented data and the surrogate models received from all the other clients (\u2462\u2192\u2463).", "description": "This figure illustrates a single round of the FEDNE algorithm.  It shows the interaction between the server and multiple clients involved in federated neighbor embedding.  The steps highlight the flow of information:  the server distributes the neural network model, each client updates its local surrogate model and then trains its local model using augmented data incorporating surrogate models from other clients, then uploads its updated local model to the server, finally, the server aggregates the models.", "section": "4 Federated Neighbor Embedding: FEDNE"}, {"figure_path": "zBMKodNgKX/figures/figures_4_1.jpg", "caption": "Figure 1: An illustration of one round of FEDNE. Besides the general steps in FEDAVG: \u2460 5 \u2192\u2465, our local surrogate model training (\u2461) can be smoothly integrated into the whole pipeline. Then, each client conducts its local training (5) using the augmented data and the surrogate models received from all the other clients (\u2462\u2192\u2463).", "description": "This figure illustrates one round of the FEDNE algorithm.  It shows how the FEDAVG framework is extended with a surrogate model training step.  Each client trains a local surrogate model to approximate repulsion loss, which is then shared with other clients. This shared information helps to compensate for the lack of direct data sharing between clients during local training. FEDAVG steps 1 to 6 represent the distribution and aggregation of the central model parameters.", "section": "4 Federated Neighbor Embedding: FEDNE"}, {"figure_path": "zBMKodNgKX/figures/figures_6_1.jpg", "caption": "Figure 2: Toy examples for illustrating the major challenges in solving Federated NE. Color denotes the client identity, and different shapes represent the true categories of the data points (just for demonstration purposes). (a) Without repelling the dissimilar data from other clients, the projected data points from different clients may overlap with each other in the global embedding space. (b) Biased local kNN graphs may incorrectly connect distant data pairs as neighbors.", "description": "This figure uses two toy examples to illustrate the challenges of applying federated learning to neighbor embedding. The left panel (a) shows that without considering the repulsive forces between dissimilar data points across different clients, the resulting embedding may have overlapping points from different clients in the global embedding space. The right panel (b) shows that because data is partitioned across different clients, the locally constructed kNN graphs may be biased, leading to false-neighbor connections (connecting distant points) and invisible neighbors (missing true neighbors).", "section": "5 Experiments"}, {"figure_path": "zBMKodNgKX/figures/figures_8_1.jpg", "caption": "Figure 3: Visualization of the resulting global test 2D embeddings on the Fashion-MNIST dataset under four FL setups of 20 clients (M = 20).", "description": "This figure visualizes the 2D embeddings obtained from four different federated learning (FL) methods on the Fashion-MNIST dataset.  The methods compared are GlobalNE (a centralized approach using all data), FEDNE (the proposed method), and two baselines, FedAvg+NE and FedProx+NE.  The visualization shows the distribution of data points in the 2D embedding space for different classes, and for four different non-IID data partitioning strategies, and the effectiveness of the proposed FEDNE method in preserving the data's original structure across different clients.", "section": "5.2 Results"}, {"figure_path": "zBMKodNgKX/figures/figures_14_1.jpg", "caption": "Figure 4: Experimental results on different step sizes in grid sampling for training the surrogate models. The experiments are conducted under the setting of Dirichlet(0.1) on the MNIST dataset with 20 clients. In the main paper, the default step size is set to 0.3. The results demonstrate that the performance of FedNE is stable when the step size is below 1.0.", "description": "The figure shows the experimental results on different step sizes used for grid sampling during the training of surrogate models. The experiments were performed using the Dirichlet(0.1) setting on the MNIST dataset with 20 clients.  The default step size used in the main paper was 0.3.  The results indicate that the performance of the proposed FEDNE method remains stable when the step size is below 1.0, demonstrating its robustness to variations in this hyperparameter.", "section": "4.2 Neighboring Data Augmentation"}, {"figure_path": "zBMKodNgKX/figures/figures_16_1.jpg", "caption": "Figure 5: The quantitative evaluation on testing the surrogate function under five different retraining frequencies. The line chart shows the results of the MNIST test data with the FL setting of 20 clients and the Shards partition with two classes of data per client.", "description": "This figure shows the impact of different retraining frequencies of the surrogate function on the performance of the FEDNE model. The x-axis represents the number of rounds, while the y-axis shows different metrics such as continuity, trustworthiness, and kNN accuracy. The results show that the performance of the FEDNE model is relatively stable when the surrogate loss function is updated frequently (every round), but it begins to decrease when the update frequency is reduced (every 5, 10, 15, or 20 rounds).", "section": "B.5 Frequency of surrogate function update"}, {"figure_path": "zBMKodNgKX/figures/figures_16_2.jpg", "caption": "Figure 5: The quantitative evaluation on testing the surrogate function under five different retraining frequencies. The line chart shows the results of the MNIST test data with the FL setting of 20 clients and the Shards partition with two classes of data per client.", "description": "This figure shows how the performance of different metrics (continuity, trustworthiness, and kNN accuracy) changes when the surrogate function is updated at different frequencies during the training process. The x-axis represents the percentage of total training rounds at which the surrogate function is updated, while the y-axis represents the values of the metrics. The results suggest that frequent updates (every round) are beneficial but excessive updates can lead to performance degradation.", "section": "B.5 Frequency of surrogate function update"}, {"figure_path": "zBMKodNgKX/figures/figures_18_1.jpg", "caption": "Figure 7: Visualization results from centralized setting, FEDNE and FEDAVG on MNIST test dataset under eight different FL settings (i.e., Shards with two classes or three classes per client; and Dirichlet with a = 0.1 or a = 0.5).", "description": "This figure compares the visualization results of the MNIST test dataset using three different methods: GlobalNE (centralized setting), FEDNE, and FedAvg+NE.  The comparison is shown across eight different federated learning (FL) settings, which vary in data distribution among clients. Four settings use the \"Shards\" approach, dividing data so each client has two or three classes. Another four settings use the \"Dirichlet\" distribution approach, creating varied non-IID data. The visualization demonstrates the effect of each method under different data distributions.", "section": "5.2 Results"}, {"figure_path": "zBMKodNgKX/figures/figures_18_2.jpg", "caption": "Figure 3: Visualization of the resulting global test 2D embeddings on the Fashion-MNIST dataset under four FL setups of 20 clients (M = 20).", "description": "This figure visualizes the 2D embeddings generated by four different federated learning (FL) methods on the Fashion-MNIST dataset.  The methods compared are FEDNE (the proposed method), FedAvg+NE, and FedProx+NE (baseline methods), and GlobalNE (a centralized approach serving as a performance upper bound). The visualization shows the resulting embeddings for 20 clients under different data distribution scenarios (Dir(0.1) and Dir(0.5)). The figure helps to illustrate how well each method preserves the neighborhood structure and separation of classes in the 2D embedding space.  Visual inspection reveals that FEDNE better maintains the original relationships between data points than the baseline methods.", "section": "5.2 Results"}, {"figure_path": "zBMKodNgKX/figures/figures_18_3.jpg", "caption": "Figure 10: Visualization results from centralized setting, FEDNE and FEDAVG on scRNA-Seq test dataset under four different FL settings (i.e., Dirichlet with a = 0.1 or a = 0.5).", "description": "This figure visualizes the results of dimensionality reduction on scRNA-Seq data using three different methods: a centralized approach (GlobalNE), FEDNE (the proposed method), and FedAvg+NE (a baseline).  Four different federated learning settings (20 or 50 clients, and Dirichlet distributions with alpha=0.1 or alpha=0.5) are compared. Each setting's results are displayed in a separate subplot, showing the 2D embedding of the high-dimensional data.  The color-coding indicates different cell types, demonstrating how well each method maintains the separation of cell types in the lower-dimensional representation.", "section": "5 Experiments"}, {"figure_path": "zBMKodNgKX/figures/figures_19_1.jpg", "caption": "Figure 3: Visualization of the resulting global test 2D embeddings on the Fashion-MNIST dataset under four FL setups of 20 clients (M = 20).", "description": "This figure visualizes the 2D embeddings generated by four different federated learning (FL) methods on the Fashion-MNIST dataset.  The methods compared are: GlobalNE (a centralized approach using all data), FEDNE (the proposed method), FedAvg+NE (a baseline using FedAvg), and FedProx+NE (another baseline using FedProx). Each method's embedding is shown for four different non-IID data distributions (Shards(2), Shards(3), Dir(0.1), Dir(0.5)), representing different levels of data heterogeneity across clients.  The visualization helps to assess how well each method preserves the original data structure and separates different classes in the 2D embedding space.  The differences in cluster separation and overall structure highlight the effectiveness of the proposed FEDNE method in handling non-IID data.", "section": "5.2 Results"}]