[{"figure_path": "ucxQrked0d/figures/figures_0_1.jpg", "caption": "Figure 1: Our approach for offline visual RL.", "description": "The figure illustrates the CoWorld approach for offline visual reinforcement learning. It shows how an online environment with a source world model and agent is used to provide knowledge transfer and constraints for a target world model and agent trained on offline data. The key is to leverage off-the-shelf RL simulators as a test bed for offline policies, enabling online-to-offline knowledge transfer that mitigates cross-domain discrepancies in state and reward spaces.", "section": "1 Introduction"}, {"figure_path": "ucxQrked0d/figures/figures_1_1.jpg", "caption": "Figure 2: To address value overestimation in offline RL (a), we can directly penalize the estimated values beyond the distribution of offline data, which may hinder the agent's exploration of potential states with high rewards (b). Unlike existing methods, CoWorld trains a cross-domain critic model in an online auxiliary domain to reassess the offline policy (c), and regularizes the target values with flexible constraints (d). The feasibility of this approach lies in the domain alignment techniques during the world model learning stage.", "description": "This figure illustrates the core idea of CoWorld in addressing value overestimation in offline RL.  Panel (a) shows the problem of overestimation in offline RL, where the estimated values are higher than the true values, especially for out-of-distribution (OOD) data. Panel (b) demonstrates how directly penalizing overestimated values can lead to over-conservatism, preventing exploration of potentially high-reward states. In contrast, CoWorld (c, d) leverages an online auxiliary domain and a cross-domain critic to provide more flexible value constraints. The online critic reassesses the offline policy, resulting in a milder form of conservatism (d) that balances the risk of overestimation and the need for exploration.", "section": "Method"}, {"figure_path": "ucxQrked0d/figures/figures_6_1.jpg", "caption": "Figure 3: Left: The value in each grid indicates the ratio of returns achieved by CoWorld compared to Offline DV2. Highlighted grids represent the top-performing source domain. Right: Returns on Drawer Close (DC*) with different source domains, where the multi-source CoWorld (yellow line) is shown to automatically discover (i.e., Door Close) as the source domain and achieve comparable results with the top-performing single-source CoWorld (red line).", "description": "The left part of the figure shows a heatmap visualizing the performance improvement of CoWorld over Offline DV2 across various source-target domain combinations for Meta-World tasks. The right part shows a line graph comparing the performance of CoWorld with different source domains on a specific task (Drawer Close), highlighting the ability of multi-source CoWorld to automatically select a high-performing source domain.", "section": "4.2 Cross-Task Experiments on Meta-World"}, {"figure_path": "ucxQrked0d/figures/figures_6_2.jpg", "caption": "Figure 4: Quantitative results in domain transfer scenarios of Meta-World \u2192 RoboDesk.", "description": "This figure presents the quantitative results of the CoWorld model in domain transfer scenarios from Meta-World to RoboDesk. It shows the learning curves (episode return vs. training iterations) for four different domain transfer tasks: Button Press \u2192 Push Button, Window Close \u2192 Open Slide, Drawer Close \u2192 Drawer Open, and Handle Press \u2192 Upright Block off Table.  The plots compare the performance of CoWorld (best-source, multi-source), Offline DV2, and DV2 Finetune.  This visualization helps demonstrate CoWorld's ability to effectively transfer knowledge from the source domain (Meta-World) to the target domain (RoboDesk) despite the differences between the two environments in terms of visual observations, physical dynamics, action spaces, reward definitions, etc.", "section": "4.2 Cross-Task Experiments on Meta-World"}, {"figure_path": "ucxQrked0d/figures/figures_7_1.jpg", "caption": "Figure 5: (a) Ablation studies on state alignment, reward alignment, and min-max value constraint. (b) The disparities between the estimated value by various models and the true value. Please see the text in Section 4.5 for the implementation of CoWorld w/o Max.", "description": "This figure presents ablation study results for the CoWorld model, showing the impact of removing each stage (state alignment, reward alignment, and min-max constraint) on the model's performance.  The left subplot shows the learning curves for the complete model and the versions with each stage removed, illustrating their individual contributions. The right subplot visualizes the value overestimation problem, comparing the estimated values of different models against the true values.  This demonstrates CoWorld's ability to address value overestimation.", "section": "4.5 Further Analyses"}, {"figure_path": "ucxQrked0d/figures/figures_9_1.jpg", "caption": "Figure 6: Sensitivity analysis of the hyperparameters on Meta-World (DC \u2192 BP).", "description": "This figure shows the sensitivity analysis of three hyperparameters used in the CoWorld model: the domain KL loss scale (\u03b22), the target-inclined reward factor (k), and the target critic value loss scale (\u03b1). Each subfigure shows how the episode return varies with different values of a specific hyperparameter while holding the other two hyperparameters constant.  The plots reveal the optimal range for each hyperparameter to achieve the best performance. For instance, a smaller \u03b22 value leads to lower episode return, while an excessively larger \u03b1 value results in value over-conservatism in the target critic.", "section": "4 Experiments"}, {"figure_path": "ucxQrked0d/figures/figures_13_1.jpg", "caption": "Figure 1: Our approach for offline visual RL.", "description": "This figure illustrates the CoWorld approach for offline visual reinforcement learning.  It shows how offline data, an online environment (acting as a source world), and two world models (one for the source world and one for the target offline world) are used to learn a policy. The source world provides rich interactions, the source agent explores the environment, and value constraints are applied to help avoid overestimation.  The state and reward spaces are aligned between the two world models to improve knowledge transfer.", "section": "1 Introduction"}, {"figure_path": "ucxQrked0d/figures/figures_15_1.jpg", "caption": "Figure 8: Additional qualitative results of policy evaluation on the DMC tasks.", "description": "This figure shows a qualitative comparison of policy evaluations on three DeepMind Control (DMC) tasks: Walker Downhill, Walker Uphill, and Walker Nofoot.  For each task, the figure displays a sequence of images (t=1 to t=45) illustrating the agent's actions over time, as performed by four different methods: CURL, LOMPO, Offline DV2, and CoWorld. The images provide a visual representation of the policies learned by each algorithm, allowing for a direct comparison of their performance and behavior.", "section": "B.1 Visualizations on Policy Evaluation"}, {"figure_path": "ucxQrked0d/figures/figures_15_2.jpg", "caption": "Figure 8: Additional qualitative results of policy evaluation on the DMC tasks.", "description": "This figure shows a qualitative comparison of policy evaluation results across four different methods (CURL, LOMPO, Offline DV2, and CoWorld) on three distinct DeepMind Control (DMC) tasks: Walker Downhill, Walker Uphill, and Walker Nofoot. For each task and method, the figure displays a sequence of images showing the agent's movements at different time steps (t=1, 5, 10, ..., 45). This visualization helps understand how well each algorithm learns to control the agent's movements in various situations and compares their relative performance.", "section": "B.1 Visualizations on Policy Evaluation"}, {"figure_path": "ucxQrked0d/figures/figures_15_3.jpg", "caption": "Figure 8: Additional qualitative results of policy evaluation on the DMC tasks.", "description": "This figure shows a qualitative comparison of the policies learned by different methods (CURL, LOMPO, Offline DV2, and CoWorld) on three different DeepMind Control (DMC) tasks: Walker Downhill, Walker Uphill, and Walker Nofoot. Each row represents a different method, and each column shows the state of the agent at a specific time step (t=1, 5, 10, ..., 45).  The visualizations help illustrate how effectively each method learns to control the agent in the challenging DMC environments, highlighting the differences in their approaches to locomotion.", "section": "B.1 Visualizations on Policy Evaluation"}, {"figure_path": "ucxQrked0d/figures/figures_16_1.jpg", "caption": "Figure 9: Policy evaluation on the Meta-World Button Topdown task. The model-free method CURL cannot complete the task (green box). CoWorld achieves better performance and finishes the task in fewer steps (red box) than Offline DV2 (blue box).", "description": "This figure shows a qualitative comparison of the performance of different offline reinforcement learning methods on the Meta-World Button Topdown task.  The images depict the agent's actions over time. CURL fails to complete the task. Offline DV2 completes it, but CoWorld shows a more efficient and successful approach.", "section": "B.1 Visualizations on Policy Evaluation"}, {"figure_path": "ucxQrked0d/figures/figures_17_1.jpg", "caption": "Figure 1: Our approach for offline visual RL.", "description": "This figure illustrates the overall approach proposed in the paper for offline visual reinforcement learning.  It shows an offline dataset being used to train a target world model and agent.  The key innovation is the inclusion of an online environment and source agent, which interact with a source world model.  This allows for knowledge transfer between the online and offline domains, enabling more effective learning by mitigating the challenges of overfitting and overestimation often seen in offline visual RL.", "section": "1 Introduction"}, {"figure_path": "ucxQrked0d/figures/figures_18_1.jpg", "caption": "Figure 1: Our approach for offline visual RL.", "description": "This figure illustrates the CoWorld approach for offline visual reinforcement learning.  It shows how offline data is combined with an online environment (a simulator) to train a model. The offline dataset informs the target world model and agent, while the online environment, through the source world model and agent, helps refine the value estimation, mitigating overestimation and promoting better exploration. The process is iterative, moving between offline and online phases.", "section": "1 Introduction"}]