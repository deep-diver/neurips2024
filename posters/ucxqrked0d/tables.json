[{"figure_path": "ucxQrked0d/tables/tables_2_1.jpg", "caption": "Table 1: RoboDesk (target domain) vs. Meta-World (auxiliary source domain).", "description": "This table compares the target domain, RoboDesk, with the auxiliary source domain, Meta-World, highlighting key differences in task dynamics, action spaces, reward scales, and observation types to emphasize the cross-domain challenges addressed by the proposed CoWorld approach.  The differences underscore the difficulty of direct knowledge transfer and the need for domain alignment strategies.", "section": "4 Experiments"}, {"figure_path": "ucxQrked0d/tables/tables_2_2.jpg", "caption": "Table 1: RoboDesk (target domain) vs. Meta-World (auxiliary source domain).", "description": "This table compares the target domain, RoboDesk, with the auxiliary source domain, Meta-World, highlighting key differences across various aspects.  These differences include the type of robot arm used (Sawyer vs. Franka), the dimensionality of the action space, the reward scaling, and the viewpoints of the observations (right-view images vs. top-view images). Understanding these discrepancies is crucial for effective cross-domain knowledge transfer in the offline visual reinforcement learning task.", "section": "4 Experiments"}, {"figure_path": "ucxQrked0d/tables/tables_5_1.jpg", "caption": "Table 2: Mean episode returns and standard deviations of 10 episodes over 3 seeds on Meta-World.", "description": "This table presents the mean and standard deviation of episode returns achieved by different offline reinforcement learning methods across six different tasks within the Meta-World environment.  Each result represents an average over 10 episodes and 3 random seeds, providing a robust comparison of the algorithms' performance. The tasks include Door Close (DC*), Button Press (BP), Window Close (WC), Handle Press (HP), Drawer Close (DC), and Button Topdown (BT).  The methods compared include Offline DV2, DrQ + BC, CQL, CURL, LOMPO, DV2 Finetune, DV2 Finetune + EWC, LOMPO Finetune, and CoWorld (with both best-source and multi-source configurations).  The table allows for a comprehensive comparison of the various algorithms' performance across various Meta-World tasks.", "section": "4.2 Cross-Task Experiments on Meta-World"}, {"figure_path": "ucxQrked0d/tables/tables_7_1.jpg", "caption": "Table 2: Mean episode returns and standard deviations of 10 episodes over 3 seeds on Meta-World.", "description": "This table presents the average episode returns and standard deviations achieved by various offline reinforcement learning methods across six different tasks within the Meta-World environment.  The results are averaged over 10 episodes and three random seeds, providing a robust comparison of the algorithms' performance. The methods compared include Offline DV2, DrQ+BC, CQL, CURL, LOMPO, and CoWorld (with best-source and multi-source strategies).  The table highlights the relative performance of CoWorld compared to existing offline RL approaches on a visual control benchmark.", "section": "4.2 Cross-Task Experiments on Meta-World"}, {"figure_path": "ucxQrked0d/tables/tables_8_1.jpg", "caption": "Table 4: Experiments with significantly distinct observation spaces across domains. We use low-dimensional state data as inputs for the RL agents in the source domain and high-dimensional image observations in the target domain. MW represents Meta-World and RD stands for RoboDesk.", "description": "This table presents results from experiments designed to test the robustness of the CoWorld model when there are significant differences in the observation spaces between the source and target domains.  The source domain uses low-dimensional state data, while the target domain uses high-dimensional image data.  The table shows the mean episode return and standard deviation for two different task transfer scenarios: Meta-World Button Press to RoboDesk Push Button and Meta-World Window Close to RoboDesk Open Slide. The results demonstrate the performance of both the Offline DV2 baseline and the CoWorld model in these challenging conditions.", "section": "4.4 Cross-Dynamics Experiments on DMC"}, {"figure_path": "ucxQrked0d/tables/tables_8_2.jpg", "caption": "Table 4: Experiments with significantly distinct observation spaces across domains. We use low-dimensional state data as inputs for the RL agents in the source domain and high-dimensional image observations in the target domain. MW represents Meta-World and RD stands for RoboDesk.", "description": "This table shows the results of experiments conducted to evaluate the performance of CoWorld and DV2 Finetune when there are significant differences in observation spaces between the source and target domains.  In particular, the source domain uses low-dimensional state data, while the target domain uses high-dimensional image data.  The table presents the mean episode return \u00b1 standard deviation for two different cross-domain transfer tasks (Meta-World Button Press \u2192 RoboDesk Push Button and Meta-World Window Close \u2192 RoboDesk Open Slide).", "section": "4.4 Cross-Dynamics Experiments on DMC"}, {"figure_path": "ucxQrked0d/tables/tables_8_3.jpg", "caption": "Table 4: Experiments with significantly distinct observation spaces across domains. We use low-dimensional state data as inputs for the RL agents in the source domain and high-dimensional image observations in the target domain. MW represents Meta-World and RD stands for RoboDesk.", "description": "This table presents the results of experiments conducted to evaluate the performance of the proposed CoWorld method and a baseline method (Offline DV2) in scenarios where there are significant differences in observation spaces between the source and target domains.  Specifically, the source domain uses low-dimensional state data while the target domain uses high-dimensional image data. The table shows the average episode returns and standard deviations for two different transfer tasks: Meta-World Button Press to RoboDesk Push Button and Meta-World Window Close to RoboDesk Open Slide.", "section": "4.4 Cross-Dynamics Experiments on DMC"}, {"figure_path": "ucxQrked0d/tables/tables_14_1.jpg", "caption": "Table 1: RoboDesk (target domain) vs. Meta-World (auxiliary source domain).", "description": "This table compares the target domain, RoboDesk, and the auxiliary source domain, Meta-World, highlighting key differences for visual reinforcement learning.  These differences include the type of robot arm used (Sawyer vs. Franka), the action space dimensionality, the reward scale, and the viewpoint of the observations (right-view images vs. top-view images).  Understanding these differences is crucial because the paper proposes a method that leverages knowledge transfer between these domains to improve offline visual reinforcement learning performance.", "section": "4 Experiments"}, {"figure_path": "ucxQrked0d/tables/tables_16_1.jpg", "caption": "Table 2: Mean episode returns and standard deviations of 10 episodes over 3 seeds on Meta-World.", "description": "This table presents the mean episode returns and standard deviations achieved by different reinforcement learning models across six tasks within the Meta-World environment.  Each result represents the average over 10 episodes and 3 random seeds, providing a robust comparison of model performance across various tasks. The models compared include Offline DV2, DrQ + BC, CQL, CURL, LOMPO, and CoWorld (with both best-source and multi-source configurations).  The table highlights CoWorld's superior performance compared to existing methods.", "section": "4.2 Cross-Task Experiments on Meta-World"}, {"figure_path": "ucxQrked0d/tables/tables_17_1.jpg", "caption": "Table 9: Performance on DMC medium-expert dataset. We report the mean rewards and standard deviations of 10 episodes over 3 seeds.", "description": "This table presents the results of different reinforcement learning methods on the DeepMind Control (DMC) benchmark using a medium-expert dataset.  It shows the mean and standard deviation of episode rewards across six different tasks (three for Walker and three for Cheetah robots) for each method.  The \"Avg.\" column represents the average performance across all six tasks. The methods compared include Offline DV2, DrQ+BC, LOMPO, DV2 Finetune, and CoWorld.", "section": "4.4 Cross-Dynamics Experiments on DMC"}, {"figure_path": "ucxQrked0d/tables/tables_18_1.jpg", "caption": "Table 10: Results with more significant domain gaps.", "description": "This table presents the results of CoWorld and DV2 Finetune on tasks with more significant domain discrepancies compared to other experiments.  The target domains include different tasks from DMC (DeepMind Control) and Meta-World, showcasing the robustness of CoWorld across various scenarios with varying degrees of domain similarity. The noise magnitude (w) added in some Meta-World Button Topdown experiments is also specified.", "section": "4.3 Cross-Environments: Meta-World to RoboDesk"}, {"figure_path": "ucxQrked0d/tables/tables_19_1.jpg", "caption": "Table 11: Comparison of CoWorld with using a pre-trained foundation model, R3M.", "description": "This table compares the performance of CoWorld against three other models: R3M (trained with expert data), R3M (trained with the authors' data), and DV2 Finetune. The comparison is made across three Meta-World tasks: Button Press Topdown, Drawer Close, and Handle Press.  The results show CoWorld outperforming the other models in most cases, suggesting its effectiveness even when compared to pre-trained foundation models like R3M.", "section": "4.4 Cross-Dynamics Experiments on DMC"}, {"figure_path": "ucxQrked0d/tables/tables_19_2.jpg", "caption": "Table 2: Mean episode returns and standard deviations of 10 episodes over 3 seeds on Meta-World.", "description": "This table presents the mean episode returns and standard deviations achieved by different reinforcement learning models across six tasks in the Meta-World environment.  The results are averaged over 10 episodes and 3 different random seeds for each model and task, providing a comprehensive comparison of their performance.  The models compared include Offline DV2, DrQ + BC, CQL, CURL, LOMPO, and CoWorld (with both a best-source and multi-source variant). The table allows for a direct comparison of the performance of various methods on the same set of tasks in the same environment.", "section": "4.2 Cross-Task Experiments on Meta-World"}, {"figure_path": "ucxQrked0d/tables/tables_20_1.jpg", "caption": "Table 1: RoboDesk (target domain) vs. Meta-World (auxiliary source domain).", "description": "This table compares the target domain, RoboDesk, with the auxiliary source domain, Meta-World, highlighting key differences in task, dynamics, action space, reward scale, and observations (images).  The differences are relevant to the challenge of transferring knowledge between the two domains for offline visual reinforcement learning.  The table shows that while both domains involve robot manipulation tasks, there are significant differences in the robot arm used, the action space dimensionality, reward scaling, and most notably, the viewpoint of the camera used for generating the visual observations.", "section": "4 Experiments"}]