[{"figure_path": "KNDUBpWV9b/tables/tables_2_1.jpg", "caption": "Table 2: U-Net Comparison. Tx means Transformer. SDM-v2.0 [36] uses 7682 resolution, while SDXL and KOALA models use 10242 resolution. CKPT means the trained checkpoint file.", "description": "This table compares the U-Net architecture of several models: SDM-v2.0, SDXL-1.0, BK-SDXL, KOALA-1B, and KOALA-700M.  It shows the number of parameters, checkpoint file size, the number of transformer blocks in each stage, the presence of a mid block, and inference latency. The comparison highlights the reduction in model size and latency achieved by the KOALA models compared to the original SDXL.", "section": "2 In-depth Analysis: Stable Diffusion XL"}, {"figure_path": "KNDUBpWV9b/tables/tables_3_1.jpg", "caption": "Table 3: Analysis of feature level knowledge distillation of U-Net in SDXL [31]. Type HPSv2 Loc. HPSv2 Baseline 25.53 Baseline 25.53 SA 26.74 DW-2 25.32 CA 26.11 DW-3 25.57 Res 26.27 Mid 25.66 FFN 26.48 UP-1 26.52 LF 26.63 UP-2 26.05 (a) Distillation type (b) Distill stage", "description": "This table presents the results of an ablation study on different feature levels for knowledge distillation in the U-Net of Stable Diffusion XL (SDXL). It investigates which features (self-attention (SA), cross-attention (CA), residual (Res), feed-forward network (FFN), and last feature (LF)) and at which stage (DW-2, DW-3, Mid, UP-1, UP-2) of the U-Net are most effective for knowledge distillation. The results are evaluated using the Human Preference Score (HPSv2).", "section": "3.1.2 Exploring Knowledge Distillation for SDXL"}, {"figure_path": "KNDUBpWV9b/tables/tables_5_1.jpg", "caption": "Table 4: Training Data comparison. AR and ACL mean average resolution and average caption length, respectively. synCap means synthetic captions by LLaVA-v1.5 [24]", "description": "This table compares three different training datasets used in the paper, showing the number of images, average resolution (AR), average caption length (ACL), and the resulting HPSv2 and CompBench scores.  It also shows how synthetic captions generated by the LLaVA-v1.5 model were used to augment the dataset. The results highlight the impact of data quality (resolution and caption length) on the performance of the text-to-image models.", "section": "3.2 Lesson 2. Data: the impact of sample size, image resolution, and caption length"}, {"figure_path": "KNDUBpWV9b/tables/tables_7_1.jpg", "caption": "Table 6: Performance comparison to state-of-the-art models. We measure latency and memory usage with a bath size of 1 on NVIDIA 4090 GPU. We obtain HPSv2 and Compbench scores of all models on the same GPU and library environment by using their official weights. We highlight the best value in green, and the second-best value in blue. The full scores of HPSv2 and Compbench are shown in Tab. 10.", "description": "This table presents a quantitative comparison of the KOALA models with other state-of-the-art text-to-image synthesis models.  The comparison includes model size (U-Net parameters), memory usage, inference latency (on an NVIDIA 4090 GPU), and two evaluation metrics (HPSv2 and CompBench).  The best and second-best results for each metric are highlighted.", "section": "4.2 Main results"}, {"figure_path": "KNDUBpWV9b/tables/tables_7_2.jpg", "caption": "Table 7: Comparison to BK [21]. All models are trained for 50K iterations same as BK-SDM.", "description": "This table compares the performance of the proposed KOALA models with the BK-SDM model [21] which is a baseline for architectural model compression using knowledge distillation.  Both models are trained for 50,000 iterations on the same dataset and backbone to ensure a fair comparison.  The results illustrate the effectiveness of the proposed knowledge distillation method by comparing the HPSv2 and CompBench scores achieved by both KOALA-1B and BK-Small models.", "section": "3.1.2 Exploring Knowledge Distillation for SDXL"}, {"figure_path": "KNDUBpWV9b/tables/tables_7_3.jpg", "caption": "Table 8: KD feature types in Diffusion Transformer (Pixart-\u03a3 [6]).", "description": "This table presents a comparison of knowledge distillation (KD) feature types used in the Pixart-\u03a3 model for text-to-image synthesis.  It shows the HPSv2 and CompBench scores achieved by using self-attention (SA), cross-attention (CA), feed-forward network (FFN) features, and the last feature (LF) used by BK [21] for distillation. The results highlight the superior performance of using self-attention features for knowledge distillation compared to other methods.", "section": "3.1.2 Exploring Knowledge Distillation for SDXL"}, {"figure_path": "KNDUBpWV9b/tables/tables_8_1.jpg", "caption": "Table 9: Synergy effect with a step-distilled method, PCM [53]. We conduct step-distillation training using PCM with our KOALA backbones and compare with PCM-SDXL-Base.", "description": "This table presents the results of experiments that combined the step-distillation method (PCM) with the KOALA backbones.  It shows the synergy effect, comparing the performance of models trained using PCM with SDXL-Base as the backbone against those trained using PCM with the lighter KOALA-700M and KOALA-1B backbones. The metrics evaluated include the number of steps, model parameters, memory usage, latency, Human Preference Score (HPS), and Compbench.", "section": "4 Experiments"}, {"figure_path": "KNDUBpWV9b/tables/tables_15_1.jpg", "caption": "Table 6: Performance comparison to state-of-the-art models. We measure latency and memory usage with a bath size of 1 on NVIDIA 4090 GPU. We obtain HPSv2 and Compbench scores of all models on the same GPU and library environment by using their official weights. We highlight the best value in green, and the second-best value in blue. The full scores of HPSv2 and Compbench are shown in Tab. 10.", "description": "This table compares the performance of the KOALA models (KOALA-Turbo-700M, KOALA-Turbo-1B, KOALA-Lightning-700M, KOALA-Lightning-1B) against several state-of-the-art text-to-image synthesis models (SDM-v2.0, SDXL-Base-1.0, SDXL-Turbo, SDXL-Lightning, Pixart-a, Pixart-\u03a3, SSD-1B, SSD-Vega). The comparison is based on latency, memory usage, HPSv2 (Human Preference Score), and Compbench scores.  Latency and memory usage were measured using a batch size of 1 on an NVIDIA 4090 GPU.  The best and second-best scores for HPSv2 and Compbench are highlighted in green and blue, respectively.  Complete HPSv2 and Compbench scores are available in Table 10.", "section": "4.2 Main results"}, {"figure_path": "KNDUBpWV9b/tables/tables_16_1.jpg", "caption": "Table 3: Analysis of feature level knowledge distillation of U-Net in SDXL [31].", "description": "This table presents an analysis of different feature-level knowledge distillation strategies applied to the U-Net of the Stable Diffusion XL model.  It explores which types of features (self-attention, cross-attention, feed-forward network, residual block, last feature) and at which stages (down-sampling, middle, up-sampling) are most effective for knowledge distillation.  The results are evaluated using the Human Preference Score (HPSv2).", "section": "3.1.2 Exploring Knowledge Distillation for SDXL"}]