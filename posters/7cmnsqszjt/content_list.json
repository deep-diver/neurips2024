[{"type": "text", "text": "ContextCite: Attributing Model Generation to Context ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Benjamin Cohen-Wang,\u2217 Harshay Shah,\u2217 Kristian Georgiev,\u2217 Aleksander M a\u02dbdry MIT {bencw,harshay,krisgrg,madry}@mit.edu ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "How do language models use information provided as context when generating a response? Can we infer whether a particular generated statement is actually grounded in the context, a misinterpretation, or fabricated? To help answer these questions, we introduce the problem of context attribution: pinpointing the parts of the context (if any) that led a model to generate a particular statement. We then present CONTEXTCITE, a simple and scalable method for context attribution that can be applied on top of any existing language model. Finally, we showcase the utility of CONTEXTCITE through three applications: (1) helping verify generated statements (2) improving response quality by pruning the context and (3) detecting poisoning attacks. We provide code for CONTEXTCITE at https://github. com/MadryLab/context-cite. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Suppose that we would like to use a language model to learn about recent news. We would first need to provide it with relevant articles as context2. We would then expect the language model to interact with this context to answer questions. Upon seeing a generated response, we might ask: is everything accurate? Did the model misinterpret any of the context or fabricate anything? Is the response actually grounded in the provided context? ", "page_idx": 0}, {"type": "text", "text": "Answering these questions manually could be tedious\u2014we would need to first read the articles ourselves and then verify the statements. To automate this process, prior work has focused on teaching models to generate citations: references to parts of the context that support a response [1\u20135]. They typically do so by explicitly training or prompting language models to produce citations. ", "page_idx": 0}, {"type": "text", "text": "In this work, we explore a different type of citation: instead of teaching a language model to cite its sources, can we directly identify the pieces of information that it actually uses? Specifically, we ask: ", "page_idx": 0}, {"type": "text", "text": "Can we pinpoint the parts of the context (if any) that led to a particular generated statement? ", "page_idx": 0}, {"type": "text", "text": "We refer to this problem as context attribution. Suppose, for example, that a language model misinterprets a piece of information and generates an inaccurate statement. In this case, context attribution would surface the misinterpreted part of the context. On the other hand, suppose that a language model uses knowledge that it learned from pre-training to generate a statement, rather than the context. In this case, context attribution would indicate this by not attributing the statement to any part of the context. ", "page_idx": 0}, {"type": "text", "text": "Unlike citations generated by language models, which can be difficult to validate [6, 7], in principle it is easy to evaluate the efficacy of context attributions. Specifically, if a part of the context actually led to a particular generated response, then removing it should substantially affect this response. ", "page_idx": 0}, {"type": "text", "text": "ContextCite ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Context attribution ", "text_level": 1, "page_idx": 1}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/6c805d64923e1c247d478d6e4252dae63252f72b6d343caf392d062196b79673.jpg", "img_caption": ["Figure 1: CONTEXTCITE. Our context attribution method, CONTEXTCITE, traces any specified generated statement back to the parts of the context that are responsible for it. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "solar_eclipse_2024.pdf:\u2026To witness this incredible total solar eclipse, you will need to be within the 115-mile-wide path of totality. [1] The path arches from Mexico to Texas to Maine. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Formalizing context attribution (Section 2). We begin this work by formalizing the task of context attribution. Specifically, a context attribution method assigns a score to each part of the context indicating the degree to which it is responsible for a given generated statement. We provide metrics for evaluating these scores, guided by the intuition that removing high-scoring parts of the context should have a greater effect than removing low-scoring parts of the context. ", "page_idx": 1}, {"type": "text", "text": "Performing context attribution with CONTEXTCITE (Sections 3 and 4). Next, we present CONTEXTCITE, a simple and scalable method for context attribution that can be applied on top of any existing language model (see Figure 1). CONTEXTCITE learns a surrogate model that approximates how a language model\u2019s response is affected by including or excluding each part of the context. This methodology closely follows prior work on attributing model behavior to features [8\u201310] and training examples [11, 12]. In the context attribution setting, we find that it is possible to learn a linear surrogate model that (1) faithfully models the language model\u2019s behavior and (2) can be efficiently estimated using a small number of additional inference passes. The weights of this surrogate model can be directly treated as attribution scores. We benchmark CONTEXTCITE against various baselines on a diverse set of generation tasks and find that it is indeed effective at identifying the parts of the context responsible for a given generated response. ", "page_idx": 1}, {"type": "text", "text": "Applying context attribution (Section 5). Finally, we showcase the utility of CONTEXTCITE through three applications: ", "page_idx": 1}, {"type": "text", "text": "1. Helping verify generated statements (Section 5.1): We hypothesize that if attributed sources do not also support a generated statement, then it is less likely to be accurate. We find that using CONTEXTCITE sources can greatly improve a language model\u2019s ability to verify the correctness of its own statements.   \n2. Improving response quality by pruning the context (Section 5.2): Language models often struggle to correctly use individual pieces of information within long contexts [13, 14]. We use CONTEXTCITE to select only the information that is most relevant for a given query, and then use this \u201cpruned\u201d context to regenerate the response. We find that doing so improves question answering performance on multiple benchmarks.   \n3. Detecting context poisoning attacks (Section 5.3): Language models are vulnerable to context poisoning attacks: adversarial modifications to the context that can control the model\u2019s response to a given query [15\u201319]. We illustrate that CONTEXTCITE can consistently identify such attacks. ", "page_idx": 1}, {"type": "text", "text": "2 Problem statement ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we will introduce the problem of context attribution (Section 2.1) and define metrics for evaluating context attribution methods (Section 2.2). To start, we will consider attributing an entire generated response\u2014we will discuss attributing specific statements in Section 2.3. ", "page_idx": 1}, {"type": "text", "text": "Setup. Suppose that we use a language model to generate a response to a particular query given a context. Specifically, let $p_{\\mathrm{{LM}}}$ be an autoregressive language model: a model that defines a probability distribution over the next token given a sequence of preceding tokens. We write $p_{\\mathrm{LM}}(t_{i}\\mid t_{1},\\ldots,t_{i-1})$ to denote the probability of the next token being $t_{i}$ given the preceding tokens $t_{1},\\ldots,t_{i-1}$ . Next, let $C$ be a context consisting of tokens $c_{1},\\ldots,c_{|C|}$ and $Q$ be a query consisting of tokens $q_{1},\\ldots,q_{|Q|}$ . We generate a response $R$ consisting of tokens $r_{1},\\ldots,r_{|R|}$ by sampling from the model conditioned on the context and query. More formally, we generate the $i^{\\mathrm{th}}$ token $r_{i}$ of the response as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nr_{i}\\sim p_{\\mathrm{LM}}(\\cdot\\mid c_{1},\\ldots,c_{|C|},q_{1},\\ldots,q_{|Q|},r_{1},\\ldots,r_{i-1})^{3}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We write $p_{\\mathrm{LM}}(R\\mid C,Q)$ to denote the probability of generating the entire response $R$ \u2014the product of the probabilities of generating the individual response tokens\u2014given the tokens of a context $C$ and the tokens of a query $Q$ . ", "page_idx": 2}, {"type": "text", "text": "2.1 Context attribution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The goal of context attribution is to attribute a generated response back to specific parts of the context. We refer to these \u201cparts of the context\u201d as sources. Each source is just a subset of the tokens in the context; for example, each source might be a document, paragraph, sentence, or even a word. The choice of granularity depends on the application\u2014in this work, we primarily focus on sentences as sources and use an off-the-shelf sentence tokenizer to partition the context into sources4. ", "page_idx": 2}, {"type": "text", "text": "A context attribution method $\\tau$ accepts a list of $d$ sources $s_{1},\\ldots,s_{d}$ and assigns a score to each source indicating its \u201cimportance\u201d to the response. We formalize this task in the following definition: ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Context attribution). Suppose that we are given a context $C$ with sources $s_{1},\\ldots,s_{d}\\in$ $\\boldsymbol{S}$ (where $\\boldsymbol{S}$ is the set of possible sources), a query $Q$ , a language model $p_{\\mathrm{{LM}}}$ and a generated response $R$ . A context attribution method $\\tau(s_{1},\\ldots,s_{d})$ is a function $\\bar{\\tau}:S^{d}\\to\\bar{\\mathbb{R}}^{d}$ that assigns a score to each of the $d$ sources. Each score is intended to signify the \u201cimportance\u201d of the source to generating the response $R$ . ", "page_idx": 2}, {"type": "text", "text": "What do context attribution scores signify? So far, we have only stated that scores should signify how \u201cimportant\u201d a source is for generating a particular statement. But what does this actually mean? There are two types of attribution that we might be interested in: contributive and corroborative [20]. Contributive attribution identifies the sources that cause a model to generate a statement. Meanwhile, corroborative attribution identifies sources that support or imply a statement. There are several existing methods for corroborative attribution of language models [1, 2, 4, 5]. These methods typically involve explicitly training or prompting models to produce citations along with each statement they make. ", "page_idx": 2}, {"type": "text", "text": "In this work, we study contributive context attributions. These attributions give rise to a diverse and distinct set of use cases and applications compared to corroborative attributions (we explore a few in Section 5). To see why, suppose that a model misinterprets a fact in the context and generates an inaccurate statement. A corroborative method might not find any attributions (because nothing in the context supports its statement). On the other hand, a contributive method would identify the fact that the model misinterpreted. We could then use this fact to help verify or correct the model\u2019s statement. ", "page_idx": 2}, {"type": "text", "text": "2.2 Evaluating the quality of context attributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "How might we evaluate the quality of a (contributive) context attribution method? Intuitively, a source\u2019s score should reflect the degree to which the response would change if the source were excluded. We introduce two metrics to capture this intuition. The first metric, the top- $k$ log-probability drop, measures the effect of excluding the highest-scoring sources on the probability of generating the original response. The second metric, the linear datamodeling score (LDS) [12], measures the extent to which attribution scores can predict the effect of excluding a random subset of sources. ", "page_idx": 2}, {"type": "text", "text": "To formalize these metrics, we first define a context ablation as a modification of the context that excludes certain sources. To exclude sources, we choose to simply remove the corresponding tokens from the context5. We write $\\mathrm{ABLATE}(C,v)$ to denote a context $C$ ablated according to a vector $v\\,\\in\\,\\{0,1\\}^{d}$ (with zeros specifying the sources to exclude). We are now ready to define the top- $k$ log-probability drop: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (Top- $k$ log-probability drop). Suppose that we are given a context attribution method $\\tau$ . Let $v_{\\mathrm{top}-k}(\\tau)$ be an ablation vector that excludes the $k$ highest-scoring sources according to $\\tau$ Then the top- $k$ log-probability drop is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Top}\\!\\cdot\\!k\\!\\cdot\\!\\mathrm{drop}(\\tau)=\\underbrace{\\log p_{\\mathrm{LM}}(R\\mid C,Q)}_{\\mathrm{original\\;log\\cdotprobability}}-\\underbrace{\\log p_{\\mathrm{LM}}(R\\mid\\mathrm{ABLATE}(C,v_{\\mathrm{top}\\cdot k}(\\tau)),Q)}_{\\mathrm{log\\cdotprobability\\;with\\;top\\!-\\!k\\;sources\\;ablated}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The top- $k$ log-probability drop is a useful metric for comparing methods for context attribution. In particular, if removing the highest-scoring sources of one attribution method causes a larger drop than removing those of another, then we consider the former method to be identifying sources that are more important (in the contributive sense). ", "page_idx": 3}, {"type": "text", "text": "For a more fine-grained evaluation, we also consider whether attribution scores can accurately rank the effects of ablating different sets of sources on the log-probability of the response. Concretely, suppose that we sample a few different ablation vectors and compute the sum of the scores corresponding to the sources that are included by each. These summed scores may be viewed as the \u201cpredicted effects\u201d of each ablation. We then measure the rank correlation between these predicted effects and the actual resulting probabilities. This metric, known as the linear datamodeling score (LDS), was first introduced by Park et al. [12] to evaluate methods for data attribution. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.3 (Linear datamodeling score). Suppose that we are given a context attribution method $\\tau$ . Let $v_{1},\\ldots,v_{m}$ be $m$ randomly sampled ablation vectors and let ${\\bar{f}}(v_{1}),\\ldots,f(v_{m})$ be the corresponding probabilities of generating the original response. That is, $f(v_{i})=p_{\\mathrm{LM}}(R\\mid\\mathrm{ABLATE}(C,v_{i}),Q)$ . Let $\\hat{\\boldsymbol f}_{\\tau}(\\boldsymbol v)=\\langle\\tau(s_{1},\\bar{\\dots},s_{d}),\\bar{\\boldsymbol v}\\rangle$ be the sum of the scores (according to $\\tau$ ) corresponding to sources that are included by ablation vector $v$ , i.e., the \u201cpredicted effect\u201d of ablating according to $v$ . Then the linear datamodeling score (LDS) of a context attribution method $\\tau$ can be defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{LDS}(\\tau)=\\rho(~~\\{f(v_{1}),\\ldots,f(v_{m})\\}~~,~~\\{\\underset{}{\\hat{f}_{\\tau}}(v_{1}),\\ldots,\\underset{}{\\hat{f}_{\\tau}}(v_{m})\\}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\rho$ is the Spearman rank correlation coefficient [21]. ", "page_idx": 3}, {"type": "text", "text": "2.3 Attributing selected statements from the response ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Until now, we have discussed attributing an entire generated response. In practice, we might be interested in attributing a particular statement, e.g., a sentence or phrase. We define a statement to be any contiguous selection of tokens $\\boldsymbol{r}_{i},\\ldots,\\boldsymbol{r}_{j}$ from the response. To extend our setup to attributing specific statements, we let a context attribution method $\\tau$ accept an additional argument $(i,j)$ specifying the start and end indices of the statement to attribute. Instead of considering the probability of generating the entire original response, we consider the probability of generating the selected statement. Formally, in the definitions above, we replace $p_{\\mathrm{LM}}(\\bar{R}\\mid C,Q)$ with ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\mathrm{LM}}(\\underbrace{\\ r_{i},\\ \\dots\\ ,r_{j}}_{\\}\\ \\mid C,Q,\\underbrace{r_{1},\\ \\dots\\ ,r_{i-1}}_{\\}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3 Context attribution with CONTEXTCITE ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the previous section, we established that a context attribution method is effective insofar as it is able to predict the effect of including or excluding certain sources. In other words, given an ablation vector $v$ , a context attribution method should inform how the probability of the original response, ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(v):=p_{\\mathrm{LM}}(R\\mid\\mathrm{ABLATE}(C,v),Q),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "changes as a function of $v$ . The design of CONTEXTCITE is driven by the following question: can we find a simple surrogate model $\\hat{f}$ that approximates $f$ well? If so, we could use the surrogate model $\\hat{f}$ to understand how including or excluding subsets of sources would affect the probability of the original response (assuming that $\\hat{f}$ is simple enough). Indeed, surrogate models have previously been used in this way to attribute predictions to training examples [11, 12, 23, 24], model internals [25, 26], and input features [8\u201310]; we discuss connections in detail in Appendix C.1. At a high-level, our approach consists of the following steps: ", "page_idx": 3}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/6a20b81d4dd5dbbdf3707d3374f6f88c6a78b727cac2f856d02d4bc49a36183e.jpg", "img_caption": ["Figure 2: An example of the linear surrogate model used by CONTEXTCITE. On the left, we consider a context, query, and response generated by Llama-3-8B [22] about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); CONTEXTCITE casts these weights as attribution scores. On the right, we plot the surrogate model\u2019s predictions against the actual logitscaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four \u201cclusters\u201d corresponding to whether each of these sources is included or excluded. These sources appear to interact linearly\u2014the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model\u2019s behavior. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Step 1: Sample a \u201ctraining dataset\u201d of ablation vectors $v_{1},\\ldots,v_{n}$ and compute $f(v_{i})$ for each $v_{i}$ . ", "page_idx": 4}, {"type": "text", "text": "Step 2: Learn a surrogate model $\\hat{f}:\\,\\{0,1\\}^{d}\\,\\to\\,\\mathbb{R}$ that approximates $f$ by training on the pairs $(v_{i},f(v_{i}))$ . ", "page_idx": 4}, {"type": "text", "text": "Step 3: Attribute the behavior of the surrogate model $\\hat{f}$ to individual sources. ", "page_idx": 4}, {"type": "text", "text": "For the surrogate model $\\hat{f}$ to be useful, it should (1) faithfully model $f$ , (2) be efficient to compute, and (3) yield scores attributing its outputs to the individual sources. To satisfy these desiderata, we find the following design choices to be effective: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Predict logit-scaled probabilities: Fitting a regression model to predict probabilities directly might be problematic because probabilities are bounded in [0, 1]. The logit function $\\begin{array}{r}{(\\sigma^{-1}(p)=\\log\\frac{\\check{p}}{1-p})}\\end{array}$ is a mapping from $[0,1]$ to $(-\\infty,\\infty)$ , making logit-probability a more natural target for regression. ", "page_idx": 4}, {"type": "text", "text": "\u2022 Learn a linear surrogate model: Despite their simplicity, we find that linear surrogate models are often quite faithful. With a linear surrogate model, each weight signifies the effect of ablating a source on the output. As a result, we can directly cast the weights of the surrogate model as attribution scores. We illustrate an example depicting the effectiveness of a linear surrogate model in Figure 2 and provide additional randomly sampled examples in Appendix B.2. ", "page_idx": 4}, {"type": "text", "text": "\u2022 Learn a sparse linear surrogate model: Empirically, we find that a generated statement can often be explained well by just a handful of sources. In particular, Figure 3a shows that the number of sources that are \u201crelevant\u201d to a particular generated statement is often small, even when the context comprises many sources. Motivated by this observation, we induce sparsity in the surrogate model via LASSO [27]. As we illustrate in Figure 3b, this enables learning a faithful linear surrogate model even with a small number of ablations. For example, the surrogate model in Figure 2 uses just 32 ablations even though the context comprises 98 sources (in this case, sentences). ", "page_idx": 4}, {"type": "text", "text": "\u2022 Sample ablation vectors uniformly: To create the surrogate model\u2019s training dataset, we sample ablation vectors uniformly from the set of possible subsets of context sources. ", "page_idx": 4}, {"type": "text", "text": "We summarize the resulting method, CONTEXTCITE, in Algorithm 1. See Figure 2 for an example of CONTEXTCITE attributions; we provide additional examples in Appendix B.2. ", "page_idx": 4}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/df437f5e2a70563431ea2821d8bd4dcc2acb1e82f4680010bd9c8edce3824a67.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "(a) The numbers of \u201crelevant\u201d and total sources for (b) The root mean squared error (RMSE) of a surrogate summarization (left) and question answering (right) model trained with LASSO and ordinary least squares tasks. A source is \u201crelevant\u201d if excluding it changes the (OLS) on held-out ablation vectors for two tasks: sumprobability of the response by a factor of at least $\\delta=2$ . marization (left) and question answering (right). ", "page_idx": 5}, {"type": "text", "text": "Figure 3: Inducing sparsity improves the surrogate model\u2019s sample efficiency. In CNN DailyMail [28], a summarization task, and Natural Questions [29], a question answering task, we observe that the number of sources that are \u201crelevant\u201d for a particular statement generated by Llama-3-8B [22] is small, even when the context comprises many sources (Figure 3a). Therefore, inducing sparsity via LASSO yields an accurate surrogate model with just a few ablations (Figure 3b). See Appendix A.4 for the exact setup. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 CONTEXTCITE ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "1: Input: Autoregressive language model $p_{\\mathrm{{LM}}}$ , context $C$ consisting of $d$ sources $s_{1},\\ldots,s_{d}$ , query   \n$Q$ , response $R$ , number of ablations $n$ , regularization parameter $\\lambda$   \n2: Output: Attribution scores $\\hat{w}\\in\\mathbb{R}^{d}$   \n3: $f(v):=p_{\\mathrm{LM}}(R\\mid\\operatorname{ABLATE}(C,v),Q)$ $\\triangleright$ Probability of $R$ when ablating $C$ according to $v$   \n4: $g(v):=\\sigma^{-1}(f(v))$ $\\triangleright$ Logit-scaled version of $f$   \n5: for $i\\in\\{1,\\ldots,t\\}$ do   \n6: Sample a random ablation vector $v_{i}$ uniformly from $\\{0,1\\}^{d}$   \n7: $y_{i}\\gets g(v_{i})$   \n8: end for   \n9: $\\hat{w},\\hat{b}\\gets\\mathrm{LASSO}\\big(\\{\\left(v_{i},y_{i}\\right)\\}_{i=1}^{n},\\lambda\\big)$   \n10: return $\\hat{w}$ ", "page_idx": 5}, {"type": "text", "text": "4 Evaluating CONTEXTCITE ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we evaluate whether CONTEXTCITE can effectively identify sources that cause the language model to generate a particular response. Specifically, we use the evaluation metrics described in Section 2.2\u2014top- $k$ log-probability drop (1) and linear datamodeling score (LDS) (2)\u2014to benchmark CONTEXTCITE against a varied set of baselines. See Appendix A.5 for the exact setup and Appendix B.3 for results with additional models, datasets, and baselines. ", "page_idx": 5}, {"type": "text", "text": "Datasets. Generation tasks can differ in terms of (1) context properties (e.g., length, complexity) and (2) how the model uses in-context information to generate a response (e.g., summarization, question answering, reasoning). We evaluate CONTEXTCITE on up to $1,000$ random validation examples from each of three representative benchmarks: ", "page_idx": 5}, {"type": "text", "text": "1. TyDi QA [30] is a question-answering dataset in which the context is an entire Wikipedia article.   \n2. Hotpot QA [31] is a multi-hop question-answering dataset where answering the question requires reasoning over information from multiple documents.   \n3. CNN DailyMail [28] is a dataset of news articles and headlines. We prompt the language model to briefly summarize the news article. ", "page_idx": 5}, {"type": "text", "text": "Models. We use CONTEXTCITE to attribute responses from the instruction-tuned versions of Llama-3-8B [22] and Phi-3-mini [32]. ", "page_idx": 5}, {"type": "text", "text": "Baselines. We consider three natural baselines adapted from prior work on model explanations. We defer details and additional baselines that we found to be less effective to Appendix A.5.1. ", "page_idx": 5}, {"type": "text", "text": "1. Leave-one-out: We consider a leave-one-out baseline that ablates each source individually and compute the log-probability drop of the response as an attribution score. Leave-one-out is an oracle for the top- $k$ log-probability drop metric (1) when $k=1$ , but may be prohibitively expensive because it requires an inference pass for every source. 2. Attention: A line of work on explaining language models leverages attention weights [33\u201338]. We use a simple but effective baseline that computes an attribution score for each source by summing the average attention weight of individual tokens in the source across all heads in all layers. 3. Gradient norm: Other explanation methods rely on input gradients [39\u201341]. Here, following Yin and Neubig [42], we estimate the attribution score of each source by computing the $\\ell_{1}$ -norm of the log-probability gradient of the response with respect to the embeddings of tokens in the source. 4. Semantic similarity: Finally, we consider attributions based on semantic similarity. We employ a pre-trained sentence embedding model [43] to embed each source and the generated statement. We treat the cosine similarities between these embeddings as attribution scores. ", "page_idx": 6}, {"type": "text", "text": "Experiment setup. Each example on which we evaluate consists of a context, a query, a language model, and a generated response. As discussed in Section 2.3, rather than attributing the entire response to the context, we consider attributing individual statements in the response to the context. Specifically, given an example, we (1) split the response into sentences using an off-the-shelf tokenizer [44], and (2) compute attribution scores for each sentence. Then, to evaluate the attribution scores, we measure the top- $\\cdot k$ log-probability drop for $k=\\{1,3,5\\}$ (1) and LDS (2) for each sentence separately, and then average performances across sentences. Our experiments perform this evaluation for every combination of context attribution method, dataset, and language model. We evaluate CONTEXTCITE with $\\{32,64,128,256\\}$ context ablations. ", "page_idx": 6}, {"type": "text", "text": "Results. In Figure 4, we find that CONTEXTCITE consistently outperforms baselines, even when we only use 32 context ablations to compute its surrogate model. While the attention baseline approaches the performance of CONTEXTCITE with Llama-3-8B, it fares quite poorly with Phi-3-mini suggesting that attention is not consistently reliable for context attribution. CONTEXTCITE also attains high LDS across benchmarks and models, indicating that its attributions accurately predict the effects of ablating sources. ", "page_idx": 6}, {"type": "text", "text": "5 Applications of CONTEXTCITE ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Section 4, we found that CONTEXTCITE is an effective (contributive) context attribution method. In other words, it identifies the sources in the context that cause the model to generate a particular statement. In this section, we present three applications of context attribution: helping verify generated statements (Section 5.1), improving response quality by pruning the context (Section 5.2), and detecting poisoning attacks (Section 5.3). ", "page_idx": 6}, {"type": "text", "text": "5.1 Helping verify generated statements ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "It can be difficult to know when to trust statements generated by language models [45\u201349]. In this section, we investigate whether CONTEXTCITE can help language models verify the accuracy of their own generated statements. ", "page_idx": 6}, {"type": "text", "text": "Approach. Our approach builds on the following intuition: if the sources identified by CONTEXTCITE for a particular statement do not support it, then the statement might be inaccurate. To operationalize this, we (1) use CONTEXTCITE to identify the top- ${\\cdot k}$ most relevant sources and (2) provide the same language model with these sources and ask it if we can conclude that the statement is correct. We treat the model\u2019s probability of answering \u201cyes\u201d as a verification score. ", "page_idx": 6}, {"type": "text", "text": "Experiments. We apply our verification pipeline to answers generated by Llama-3-8B for 1, 000 random examples from each of two question answering datasets: HotpotQA [31] and Natural Questions [29]. We provide the language model with the top- $k$ most relevant sources (for a few different values of $k$ ) and measure its AUC for predicting whether its generated answer is accurate. As a baseline, we provide the model with the entire context and measure this AUC in the same manner. In Figure 5, we observe that the verification scores obtained using the top- $k$ sources are substantially higher than those obtained from using the entire context. This suggests that context attribution can be ", "page_idx": 6}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/b7216072a11fed185971f770efc9ff159571f469bcfacb2c81b22e536b97f8d7.jpg", "img_caption": ["(a) We report the top- $\\cdot k$ log-probability drop (1), which measures the effect of ablating top-scoring sources on the generated response. A higher drop indicates that the context attribution method identifies more relevant sources. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/8eb87239246f7ce41e84f846fbbf90829b16445205955e7ed1d7832568788721.jpg", "img_caption": ["(b) We report the linear datamodeling score (LDS) (2), which measures the extent to which a context attribution can predict the effect of random context ablations. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Evaluating context attributions. We report the top- $k$ log-probability drop (Figure 4a) and linear datamodeling score (Figure 4b) of CONTEXTCITE and baselines. We evaluate attributions of responses generated by Llama-3-8B and Phi-3-mini on up to 1, 000 randomly sampled validation examples from each of three benchmarks. We find that CONTEXTCITE using just 32 context ablations consistently matches or outperforms the baselines\u2014attention, gradient norm, semantic similarity and leave-one-out\u2014across benchmarks and models. Increasing the number of context ablations to $\\{64,128,256\\}$ can further improve the quality of CONTEXTCITE attributions. ", "page_idx": 7}, {"type": "text", "text": "used to help language models verify the accuracy of their own responses. See Appendix A.6 for the exact setup. ", "page_idx": 7}, {"type": "text", "text": "5.2 Improving response quality by pruning the context ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "If the sources identified by CONTEXTCITE can help a language model verify the accuracy its answers (Section 5.1), can they also be used to improve the accuracy of its answers? Indeed, language models often struggle to correctly use relevant information hidden within long contexts [14, 13]. In this section, we explore whether we can improve response quality by pruning the context to include only query-relevant sources. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Approach. Our approach closely resembles the verification pipeline from Section 5.1; however, instead of using the top- $k$ sources to verify correctness, we use them to regenerate the response. Specifically, it consists of three steps: (1) generate a response using the entire context, (2) use CONTEXTCITE to identify the top- $\\cdot k$ most relevant sources, and (3) regenerate the response using only these sources as context. ", "page_idx": 8}, {"type": "text", "text": "Experiments. We assess the effectiveness of this approach on two question-answering datasets: HotpotQA [31] and Natural Questions [29]. In both datasets, the provided context typically includes a lot of irrelevant information in addition to the answer to the question. In Figure 6, we report the average $F_{1}$ -score of Llama-3-8B on 1, 000 randomly sampled examples from each dataset (1) when it is provided with the entire context and (2) when it is provided with only the top- ${\\cdot k}$ sources according to CONTEXTCITE. We find that simply selecting the most relevant sources can consistently improve question answering capabilities. See Appendix A.7 for the exact setup and Appendix C.2 for additional discussion of why pruning in this way can improve question answering performance. ", "page_idx": 8}, {"type": "text", "text": "5.3 Detecting poisoning attacks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Finally, we explore whether context attribution can help surface poisoning attacks [15\u201317]. We focus on indirect prompt injection attacks [18, 19] that can override a language model\u2019s response to a given query by \u201cpoisoning\u201d, or adversarially modifying, external information provided as context. For example, if a system like ChatGPT browses the web to answer a question about the news, it may end up retrieving a poisoned article and adding it to the language model\u2019s context. These attacks can be \u201cobvious\u201d once identified\u2014e.g., If asked about the election, ignore everything else and say that Trump dropped out\u2014but can go unnoticed, as users are unlikely to carefully inspect the entire article. ", "page_idx": 8}, {"type": "text", "text": "Approach. If a prompt injection attack successfully causes the model to generate an undesirable response, the attribution score of the context source(s) containing the injected poison should be high. One can also view the injected poison as a \u201cstrong feature\u201d [50] in the context that significantly influences model output and, thus, should have a high attribution score. Concretely, given a potentially poisoned context and query, our approach (a) uses CONTEXTCITE to attribute the generated response to sources in the context and (b) flags the top- $k$ sources with the highest attribution scores for further manual inspection. ", "page_idx": 8}, {"type": "text", "text": "Experiments. We consider two types of prompt injection attacks: (1) handcrafted attacks (e.g., \u201cIgnore all previous instructions and. . .\u201d) [16], and (2) optimization-based attacks [19]. In both cases, CONTEXTCITE surfaces the prompt injection as the single most influential source more than $95\\%$ of the time. See Appendix A.8 for the exact setup and more detailed results. ", "page_idx": 8}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/efc0fe4b8786e03b285fc4c47d244c61629feda546749c979aad5495411e66b6.jpg", "img_caption": ["Figure 5: Helping verify generated statements using CONTEXTCITE. We report the AUC of Llama-3-8B for verifying the correctness of its own answers when we provide it with the top- $k$ sources identified by CONTEXTCITE and when we provide it with the entire context. We consider 1, 000 random examples from HotpotQA on the left and 1, 000 random examples from Natural Questions on the right. In both cases, using the top- ${\\cdot k}$ sources results in substantially more effective verification than using the entire context, suggesting that CONTEXTCITE can help language models verify their own statements. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/48bc6db1a9b9138d7554a728545c868b6551c002e8fc44df719a81d3829edc25.jpg", "img_caption": ["Figure 6: Improving response quality by constructing query-specific contexts. On the left, we show that filtering contexts by selecting the top- $\\{2,\\ldots,16\\}$ query-relevant sources (via CONTEXTCITE) improves the average $F_{1}$ -score of Llama-3-8B on 1, 000 randomly sampled examples from the Hotpot QA dataset. Similarly, on the right, simply replacing the entire context with the top$\\{8,\\ldots,128\\}$ query-relevant sources boosts the average $F_{1}$ -score of Llama-3-8B on 1, 000 randomly sampled examples from the Natural Questions dataset. In both cases, CONTEXTCITE improves response quality by extracting the most query-relevant information from the context. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Related work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Citations for language models. Prior work on citations for language models has focused on teaching models to generate citations for their responses [1, 4, 2, 3, 5, 51, 52]. For example, Menick et al. [2] fine-tune a pre-trained language model to include citations to retrieved documents as part of its response. Gao et al. [5] use prompting and in-context demonstrations to do the same. Post-hoc methods for citation [4, 51] attribute existing responses by using an auxiliary language model to identify relevant sources. Broadly, existing methods for generating citations are intended to be corroborative [20] in nature; citations are evaluated on whether they support or imply a generated statement [53, 6, 7, 54]. In contrast, CONTEXTCITE\u2014a contributive attribution method\u2014identifies sources that cause a language model to generate a given response. ", "page_idx": 9}, {"type": "text", "text": "Explaining language model behavior. Related to context attribution is the (more general) problem of explaining language model behavior. Methods for explaining language models have used attention weights [37, 38], similarity metrics [43] and input gradients [42, 55], which we adapt as baselines. The explanation approaches that are closest in spirit to CONTEXTCITE are ablation-based methods, often relying on the Shapley value [8, 9, 56\u201358]. In particular, Sarti et al. [59] quantify context reliance in machine translation models by comparing model predictions with and without context; this may be viewed as a coarse-grained variant of the context ablations performed by CONTEXTCITE. Concurrently to our work, Qi et al. [60] extend the method of Sarti et al. [59] to study context usage in retrieval-augmented generation pipelines, yielding attributions for answers to questions. ", "page_idx": 9}, {"type": "text", "text": "Understanding model behavior via surrogate modeling. Several prior works employ surrogate modeling [61] to study different aspects of model behavior. For example, data attribution methods use linear surrogate models to trace model predictions back to individual training examples [11, 12, 62, 63] or in-context learning examples [23, 24]. Similarly, methods for identifying input features that drive a model prediction [8\u201310] or for attributing predictions back to internal model components [25, 26] have also leveraged surrogate modeling. Many of the key design details of CONTEXTCITE, namely, learning a sparse linear surrogate model and predicting the effect of ablations, were previously found to be effective in other settings by these prior works. We provide a detailed discussion of the connections between CONTEXTCITE and these methods in Appendix C.1. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce the problem of context attribution whose goal is to trace a statement generated by a language model back to the specific parts of the context that caused the model to generate it. Our proposed method, CONTEXTCITE, leverages linear surrogate modeling to accurately attribute statements generated by any language model in a scalable manner. Finally, we present three applications of CONTEXTCITE: (1) helping verify generated statements (2) improving response quality by pruning the context and (3) detecting poisoning attacks. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors would like to thank Bagatur Askaryan, Andrew Ilyas, Alaa Khaddaj, Virat Kohli, Maya Lathi, Guillaume Leclerc, Sharut Gupta, Evan Vogelbaum for helpful feedback and discussions. Work supported in part by the NSF grant DMS-2134108 and Open Philanthropy. ", "page_idx": 10}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "A Experiment details 18 ", "page_idx": 11}, {"type": "text", "text": "A.1 Implementation details 18   \nA.2 Models 18   \nA.3 Datasets 18   \nA.4 Learning a sparse linear surrogate model 19   \nA.5 Evaluating CONTEXTCITE 20   \nA.6 Helping verify generated statements 20   \nA.7 Improving response quality by pruning the context 21   \nA.8 Detecting poisoning attacks . . . 21 ", "page_idx": 11}, {"type": "text", "text": "B Additional results 25 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "B.1 Random examples of CONTEXTCITE attributions 25   \nB.2 Linear surrogate model faithfulness on random examples 27   \nB.3 Additional evaluation 30   \nB.4 CONTEXTCITE for larger models . . 32   \nB.5 Word-level CONTEXTCITE 35   \nC Additional discussion 39   \nC.1 Connections to prior methods for understanding behavior via surrogate modeling 39   \nC.2 Why does pruning the context improve question answering performance? 41   \nC.3 Computational efficiency of CONTEXTCITE . . . 41   \nC.4 Limitations of CONTEXTCITE 42 ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Experiment details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Implementation details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We run all experiments on a cluster of A100 GPUs. We use the scikit-learn [64] implementation of LASSO for CONTEXTCITE, always with the regularization parameter alpha set to 0.01. When splitting the context into sources or splitting a response into statements, we use the off-the-shelf sentence tokenizer from the nltk library [44]. Our implementation of CONTEXTCITE is available at https://github.com/MadryLab/context-cite. ", "page_idx": 12}, {"type": "text", "text": "A.2 Models ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The language models we consider in this work are Llama-3-{8/70}B [22], Mistral-7B [65] and Phi-3-mini [32]. We use instruction-tuned variants of these models. We use the implementations of language models from HuggingFace\u2019s transformers library [66]. Specifically, we use the following models: ", "page_idx": 12}, {"type": "text", "text": "\u2022 Llama-3-{8/70}B: meta-llama/Meta-Llama-3-{8/70}B-Instruct \u2022 Mistral-7B: mistralai/Mistral-7B-Instruct-v0.2 \u2022 Phi-3-mini: microsoft/Phi-3-mini-128k-instruct ", "page_idx": 12}, {"type": "text", "text": "When generating responses with these models, we use their standard chat templates, treating the prompt formed from the context and query as a user\u2019s message. ", "page_idx": 12}, {"type": "text", "text": "A.3 Datasets ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We consider a variety of datasets to evaluate CONTEXTCITE spanning question answering and summarization tasks and different context structures and lengths. We provide details about these datasets and preprocessing steps in this section. Some of the datasets, namely Natural Questions and TyDi QA, contain contexts that are longer than the maximum context window of the models we consider. In particular, Llama-3-8B has the shortest context window of 8, 192 tokens. When evaluating, we filter datasets to include only examples that fit within this context window (with a padding of 512 tokens for the response). ", "page_idx": 12}, {"type": "text", "text": "CNN DailyMail [28] is a news summarization dataset. The contexts consists of a news article and the query asks the language model to briefly summarize the articles in up to three sentences. We use the following prompt template: ", "page_idx": 12}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/2246b15f08033863e87e6d268c15a185c692484b128f4ecd1992a29f8e8a863b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Hotpot QA. [31] is a multi-hop question-answering dataset in which the context consists of multiple short documents. Answering the question requires combining information from a subset of these documents\u2014the rest are \u201cdistractors\u201d containing information that is only seemingly relevant. We use the following prompt template: ", "page_idx": 12}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/3071d2fa5b9b9727fb15911c54a47bc9b17295ea335d77147806b25e810aaba1.jpg", "img_caption": ["MS MARCO [67] is question-answering dataset in which the question is a Bing search query and the context is a passage from a retrieved web page that can be used to answer the question. We use the following prompt template: "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Natural Questions [29] is a question-answering dataset in which the questions are Google search queries and the context is a Wikipedia article. The context is provided as raw HTML; we include only paragraphs (text within $\\tt{<p>}$ tags) and headers (text within ${<}\\mathtt{h}\\left[1{-}6\\right]>$ tags) and provide these as context. We fliter the dataset to include only examples where the question can be answered just using the article. We use the same prompt template as MS MARCO. ", "page_idx": 13}, {"type": "text", "text": "TyDi QA [30] is a multilingual question-answering dataset. The context is a Wikipedia article and the question about the topic of the article. We fliter the dataset to include only English examples and consider only examples where the question can be answered just using the article. We use the same prompt template as MS MARCO. ", "page_idx": 13}, {"type": "text", "text": "A.3.1 Dataset statistics. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In Table 1, we provide the average and maximum numbers of sources in the datasets that we consider. ", "page_idx": 13}, {"type": "table", "img_path": "7CMNSqsZJt/tmp/10524d4c321f5a9ec24744ee9c5e8d26d13a188d91e50b31d29b979e8f913cfe.jpg", "table_caption": ["Table 1: The average and maximum numbers of sources (in this case, sentences) among the up to 1, 000 randomly sampled examples from each of the datasets we consider. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.3.2 Partitioning contexts into sources and ablating contexts ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we discuss how we partition contexts into sources and perform context ablations. For every dataset besides Hotpot QA, we use an off-the-shelf sentence tokenizer [44] to partition the context into sentences. To perform a context ablation, we concatenate all of the included sentences and provide the resulting string to the language as context. The Hotpot QA context consists of multiple documents, each of which includes annotations for individual sentences. Furthermore, the documents have titles, which we include in the prompt (see Appendix A.3). Here, we still treat sentences as sources and include the title of a document as part of the prompt if at least one of the sentences of this document is included. ", "page_idx": 13}, {"type": "text", "text": "A.4 Learning a sparse linear surrogate model ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In Figure 3, we illustrate that CONTEXTCITE can learn a faithful surrogate model with a small number of ablations by exploiting underlying sparsity. Specifically, we consider CNN DailyMail and Natural Questions. For $1,000$ randomly sampled validation examples for each dataset, we generate a response with Llama-3-8B using the prompt templates in Appendix A.3. Following the discussion in Section 2.3, we split each response into sentences and consider each of these sentences to be a \u201cstatement.\u201d For the experiment in Figure 3a, for each statement, we ablate each of the sources individually and consider the source to be relevant if this ablation changes the probability of the statement by a factor of at least $\\delta\\,=\\,2$ . For the experiment in Figure 3b, we report the average root mean squared error (RMSE) over these statements for surrogate models trained using different numbers of context ablations. See Appendices A.2 and A.3 for additional details on datasets and models. ", "page_idx": 13}, {"type": "text", "text": "A.5 Evaluating CONTEXTCITE ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "See Appendices A.1 to A.3 for details on implementation, datasets and models for our evaluations. ", "page_idx": 14}, {"type": "text", "text": "A.5.1 Baselines for context attribution ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We provide a detailed list of baselines for context attribution in this section. In addition to the baselines described in Section 4, we consider additional attention-based and gradient-based baselines. We provide evaluation results including these baselines in Appendix B.3. ", "page_idx": 14}, {"type": "text", "text": "1. Average attention: We compute average attention weights across heads and layers of the model. We compute the sum of these average weights between every token of a source and every token of the generated statement to attribute as an attribution score. This is the attention-based baseline that we present in Figure 4. ", "page_idx": 14}, {"type": "text", "text": "2. Attention rollout: We consider the more sophisticated attention-based explanation method of Abnar and Zuidema [38]. Attention rollout seeks to capture the propagated influence of each token on each other token. Specifically, we first average the attention weights of the heads within each layer. Let $A_{\\ell}\\in\\mathbb{R}^{n\\times n}$ denote the average attention weights for the $\\ell^{\\,^{\\,^{\\,}}}$ th layer, where $n$ is the length of the sequence. Then the propagated attention weights for the $\\ell^{\\,^{\\,}}$ th layer, which we denote $\\tilde{A}_{\\ell}\\in\\mathbb{R}^{n\\times n}$ , are defined recursively as $\\tilde{A}_{\\ell}=A_{\\ell}\\tilde{A}_{\\ell-1}$ for $\\ell>1$ and $\\tilde{A}_{1}=A_{1}$ . Attention rollout computes an \u201cinfluence\u201d of token $j$ on token $i$ by computing the product $(A_{0}A_{1}\\cdot\\cdot\\cdot A_{L})_{i j}$ where $L$ is the total number of layers. When the model contains residual connections (as ours do), the average attention weights are replaced with $0.5A_{\\ell}+0.5I$ when propagating influences. ", "page_idx": 14}, {"type": "text", "text": "3. Gradient norm: Following Yin and Neubig [42], in Section 4 we estimate the attribution score of each source by computing the $\\ell_{1}$ -norm of the log-probability gradient of the response with respect to the embeddings of tokens in the source. In Appendix B.3, we also consider the $\\ell_{2}$ -norm of these gradients, but find this to be slightly less effective. ", "page_idx": 14}, {"type": "text", "text": "4. Gradient times input: As an additional gradient-based baseline, we also consider taking the dot product of the gradients and the embeddings following Shrikumar et al. [68] in Appendix B.3, but found this to be less effective than the gradient norm. ", "page_idx": 14}, {"type": "text", "text": "5. Semantic similarity: Finally, we consider attributions based on semantic similarity. We employ a pre-trained sentence embedding model [43] to embed each source and the generated statement. We treat the cosine similarities between these as attribution scores. ", "page_idx": 14}, {"type": "text", "text": "A.6 Helping verify generated statements ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Section 5.1, we explore whether CONTEXTCITE can help language models verify the accuracy of their own generated statements. Specifically, we first use CONTEXTCITE to identify a set of the top- $k$ most relevant sources. We then ask the language model whether we can conclude that the statement is accurate based on these sources. The following are additional details for this experiment: ", "page_idx": 14}, {"type": "text", "text": "1. Datasets and models. We evaluate this approach on two question-answering datasets: HotpotQA [31] and Natural Questions [29]. For each of these datasets, we evaluate the $F_{1}$ score of instructiontuned Llama-3-8B (Figure 6) on 1, 000 randomly sampled examples from the validation set. ", "page_idx": 14}, {"type": "text", "text": "2. Question answering prompt. We modify the prompts outlined for HotpotQA and Natural Questions in Appendix A.3 to request the answer as a short phrase or sentence. This allows us to assess the correctness of the generated answer. ", "page_idx": 14}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/46a3ab5a2cea3790bdc92496e02ff59ea5ec8a2cd5c00d9dbf955e8b9c4dce97.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "3. Applying CONTEXTCITE. We compute CONTEXTCITE attributions using 256 calls to the language model. ", "page_idx": 14}, {"type": "text", "text": "4. Extracting the top- $k$ most relevant sources. Given the CONTEXTCITE attributions for a context and generated statement, we extract the top- $k$ most relevant sources to verify the generated statement. ", "page_idx": 14}, {"type": "text", "text": "In this case, sources are sentences. For Hotpot QA, in which the context consists of many short documents, we extract each of the documents containing any of the top- $k$ sentences to provide the language model with a more complete context. For Natural Questions, we simply extract the top- ${\\cdot k}$ sentences. ", "page_idx": 15}, {"type": "text", "text": "5. Verification prompts. To verify the generated answer using the language model and the top- $k$ sources, we first convert the model\u2019s answer to the question (which is a word or short phrase) into a self-contained statement. We do so by prompting the language model to combine the question and its answer into a self-contained statement, using the following prompt: ", "page_idx": 15}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/63eb8c01e46a70d486b253213e4f3a3787ba50e552f3d1cf52b259fc4f9637be.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "We then use the following prompt to ask the language model whether the statement is accurate: ", "page_idx": 15}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/ca0020a3f8ed5c01de5dc3b66c9d7a136a275da9721f3d0fec1babb717312924.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.7 Improving response quality by pruning the context ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Recall that in Section 5.2, we use CONTEXTCITE to improve the question-answering capabilities of language models by extracting the most query-relevant sources from the context. We do so in three steps: (1) generate a response using the entire context, (2) use CONTEXTCITE to compute attribution scores for sources in the context, and (3) construct a query-specific context using only the top- $k$ sources, which can be used to regenerate a response. The implementation details for constructing the query-specific context are the same as for the verification application outlined in Appendix A.6. ", "page_idx": 15}, {"type": "text", "text": "A.8 Detecting poisoning attacks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Section 5.3, we consider four different attack setups, which we describe below. ", "page_idx": 15}, {"type": "text", "text": "Handcrafted attacks on Phi-3-mini. Inspired by the handcrafted prompt injection attacks described in Perez and Ribeiro [16], we create a custom dataset with context articles from Wikipedia, and handcrafted queries. For each context-query pair, we inject a poison sentence within the context article which aims to alter the model\u2019s response to the query. A part of one such sample is given below: ", "page_idx": 15}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/6ebb90c5678da26a41622e345c585116a047440ee9f36675890e3d6230f29ad2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "We design prompt injections with varied goals: false refusal of queries, misinformation, malicious code execution, change of language for the response, etc. Because this process is laborious and time-consuming, we provide a small dataset consisting of twenty context-query pairs. We provide this dataset in our code release. ", "page_idx": 16}, {"type": "text", "text": "Qualitatively, one case where CONTEXTCITE fails to surface the prompt injection as the highestscoring source (although the prompt injection is still within the top-3 scores) is when the prompt injection makes a subtle change to the output. For example: ", "page_idx": 16}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/af0f968c0be83ef270b856c32bc783dcbe46d4a3154aa61a57632faf7e3acd83.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Here Phi-3-mini\u2019s response still heavily draws on the original response, but adds the incorrect 10-ball reference. ", "page_idx": 16}, {"type": "text", "text": "Optimization-based attacks on Phi-3-mini. We also use the GCG attack introduced in Zou et al. [17]. In this setup, we again consider Wikipedia articles as contexts. Here, instead of focusing on question-answering, we turn our attention to summarization. In particular, the query for each of the context articles is ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "table", "img_path": "7CMNSqsZJt/tmp/9a2b615a72d1632467589c893aebb75ff56a896f89ea486cc6943e7d828474fc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "We then sample a random place within the context article and insert a twenty-character placeholder, which we then optimize with GCG to maximize the likelihood of the model outputting ", "page_idx": 17}, {"type": "table", "img_path": "7CMNSqsZJt/tmp/5c528f8c1697845121950f774a06b482a1aa2205534149a9491b4c14fc192520.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Given the long contexts, as well as the fact that we insert the adversarial tokens in the middle of the context (and not as a suffix), we observe a very low success rate of these optimization-based attacks. In particular, we report a success rate of just $2\\%$ . We then fliter only the prompts containing a successful attack, and construct a dataset, which we provide in our code release. Due to the high computational cost of the GCG attack (as well as the low success rate), this dataset is also small in size (22 samples, filtered down from 1000 GCG attempts, each on a random Wikipedia article). ", "page_idx": 17}, {"type": "text", "text": "Qualitatively, CONTEXTCITE fails to surface the GCG-optimized sentence as the one with the highest attribution score when the attack is not fully successful. For example, rather than outputting the target response, for one of the contexts, Phi-3-mini instead generates Python code to give a summary of the article: ", "page_idx": 17}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/d06e6955bcc008f8456ab9c3b12d26d94125ab7c83caf1d9df6b75e98b54ed02.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "We found another failure mode to be noteworthy as well. When using the Wikipedia article about Tupper Lake in New York, CONTEXTCITE finds the sentence ", "page_idx": 17}, {"type": "text", "text": "Roger Allen LaPorte, Vietnam War protester, immolated himself in front the United Nations building. ", "page_idx": 17}, {"type": "text", "text": "as the main source leading Phi-3-mini to refuse to summarize the article. Indeed, the model refuses to discuss this sensitive topic even without the GCG-optimized prompt. ", "page_idx": 17}, {"type": "text", "text": "Optimization-based attacks on Llama3-8B. Finally, we mount the prompt injections attack NeuralExec developed by Pasquini et al. [19]. In short, the attack consists of generating a universal optimized prompt injection which surrounds a \u201cpayload\u201d message. The goal of the optimized prompt injection is to maximize the likelihood of the payload message being picked up by the model. One can view the NeuralExec attack as an optimization-based counterpart to the handcrafted attacks we consider [16]. ", "page_idx": 17}, {"type": "text", "text": "For Llama3-8B, the universal (i.e., independent of the context) prompt injection is where [PAYLOAD] is a placeholder for the \u201cpayload\u201d message. We use the test set of the NeuralExec paper to evaluate how well CONTEXTCITE can detect the presence of this prompt injection. The NeuralExec attack is successfully mounted on 91 of the 100 test samples. CONTEXTCITE is able to surface the prompt injection as the most influential source in 90 out of these 91 cases, leading to a (top-1) detection accuracy of $98.9\\%$ . ", "page_idx": 17}, {"type": "table", "img_path": "7CMNSqsZJt/tmp/9510f22473bde44796d37dc80a856138b563bf7b2d23d3004965861d8f4abffe.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "table", "img_path": "7CMNSqsZJt/tmp/2f10995debbfbab8d041c38e41cc258d476f6b2effe742d21144b4ed8648772c.jpg", "table_caption": ["Table 2: We report the top-1 accuracy of CONTEXTCITE when used to detect three different types of prompt injection attacks on Llama-3-8B and Phi-3-mini. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "B Additional results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "B.1 Random examples of CONTEXTCITE attributions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide CONTEXTCITE attributions for randomly selected examples from a few datasets. For each example, we randomly select a sentence from the response to attribute and display the 4 sources with the highest attribution scores. ", "page_idx": 19}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/cc40b5bc15765829f7a8299e89197b59b901708f3aff4785e1a01fe71795615c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Example of CONTEXTCITE attributions for Llama-3-8B on CNN DailyMail ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Query ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Please summarize the article in up to three sentences. ", "page_idx": 20}, {"type": "text", "text": "Response (attributed statement is highlighted) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here is a summary of the article: ", "page_idx": 20}, {"type": "text", "text": "Paul Bell, a director of online gambling firm 666Bet, was arrested at Heathrow airport as part of a $\\pounds21\\mathsf{m}$ fraud probe, along with six others. The firm, which has deals with West Bromwich Albion and Leicester City, had signed Harry Redknapp to appear in its adverts, but Redknapp claims he knows nothing about the investigation. The firm\u2019s licence was suspended by the UK Gambling Commission, and its website is currently offline, with no indication of when it will be restored. ", "page_idx": 20}, {"type": "text", "text": "CONTEXTCITE attribution ", "text_level": 1, "page_idx": 20}, {"type": "table", "img_path": "7CMNSqsZJt/tmp/581aeb13bfa2929600bc1e61863862916d67de77f09fc78b21705db6bbecb5e6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/27d5b9ea8ba2e745d4ec57ac392be0af44708c8680036e64715fbc70d60ef147.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "B.2 Linear surrogate model faithfulness on random examples ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "On the right side of Figure 2, we show the actual logit-probabilities of different context ablations as well as the logit-probabilities predicted by a linear surrogate model. In that example, the linear surrogate model is quite faithful. In this section, we provide additional randomly sampled examples from CNN DailyMail (see Figure 7), Natural Questions (see Figure 8), and TyDi QA (see Figure 9). We use 256 context ablations to train the surrogate model, and observe that a linear surrogate model is broadly faithful across these benchmarks. ", "page_idx": 21}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/a904e5003bb46e89feb51b4005c0a4c957aa3a7f6bcfe2d6b68ec16ad8a3847b.jpg", "img_caption": ["Figure 7: The predicted logit-probabilities of a surrogate model trained on 256 context ablations on randomly sampled examples from the CNN DailyMail, a summarization benchmark. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/2dc579b2b9da7e331d7e4f75603feea648cc323d25edd6e67c58058f29b0b2dc.jpg", "img_caption": ["Figure 8: The predicted logit-probabilities of a surrogate model trained on 256 context ablations on randomly sampled (answerable) examples from the Natural Questions, a question answering benchmark. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/0e80a3fa6e3f884772f6281d26e64e8a6de7828b5ba182e3c1edff3b0ee6d444.jpg", "img_caption": ["Figure 9: The predicted logit-probabilities of a surrogate model trained on 256 context ablations on randomly sampled (answerable) English examples from the TyDi QA, a question answering benchmark. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Using the same experiment setup as in Section 4, we evaluate CONTEXTCITE on additional models (Phi-3-mini) and additional benchmarks (TyDi QA and MS MARCO), and also compare it to additional baselines: $\\ell_{2}$ -gradient norm, gradient-times-input, and attention rollout [38]. In Figure 10 and Figure 11, we show that CONTEXTCITE consistently outperforms the baselines across all models on the top- $k$ log-probability drop metric and the linear datamodeling score, respectively. ", "page_idx": 24}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/c6e284593b4744a9269b76924510b395836f708ad2bae2d70f3d3b23ce7c066c.jpg", "img_caption": ["Figure 10: Evaluating CONTEXTCITE on additional models and benchmarks using the top- $k$ log-probability drop metric (1). We compare CONTEXTCITE to additional baselines $\\ell_{2}$ -gradient norm, gradient-times-input, and attention rollout) on three models (Llama-3-8B, Phi-3-mini, Mistral-7B) and two additional benchmarks (TyDi QA and MS-MARCO). Each row corresponds to a different benchmark and each column corresponds to a different model. Across all benchmarks and models, CONTEXTCITE (with just 32 calls) consistently outperforms the baselines on the top- $k$ log-probability drop metric, which measures the effect of ablating the top- $k$ context sources with the highest attribution scores. Similar to our results in Figure ${4\\mathrm{a}}$ , increasing the number of context ablations to $\\{64,128,256\\}$ can further improve the quality of CONTEXTCITE attributions in this setting as well. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/aaf79e8aba8a1c25cbfb3c4316afd8e1db8cb2d11fabd5985d69c3eb199cd463.jpg", "img_caption": ["Figure 11: Evaluating CONTEXTCITE on additional models and benchmarks using the linear datamodeling score (2). Like in Figure 10, we compare CONTEXTCITE to additional baselines $\\ell_{2}$ -gradient norm, gradient-times-input, and attention rollout) on three models (Llama-3-8B, Phi-3-mini, Mistral-7B) and two additional benchmarks (TyDi QA and MS-MARCO). Each row corresponds to a different benchmark and each column corresponds to a different model. Across all benchmarks and models, CONTEXTCITE (with just 32 calls) consistently outperforms the baselines on the linear datamodeling score, which quantifies the extent to which context attributions predict the effect of ablating the context sources on the model response. Similar to our results in Figure 4b, increasing the number of context ablations to $\\{64,128,256\\}$ further improves the quality of CONTEXTCITE attributions in this setting as well. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "B.4 CONTEXTCITE for larger models ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Our evaluation suite for CONTEXTCITE in Section 4 consists of models with up to 8 billion parameters. In this section, we conduct a more limited evaluation of CONTEXTCITE for a larger model, Llama3-70B [22]. We find that CONTEXTCITE is effective even at this larger scale. ", "page_idx": 26}, {"type": "text", "text": "B.4.1 Evaluation of CONTEXTCITE for Llama-3-70B ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In Figure Figure 12, we evaluate CONTEXTCITE for Llama-3-70B on the CNN DailyMail and Hotpot QA benchmarks using the top- $k$ log-probability drop metric (1) and the linear datamodeling score (2). We use the same evaluation setup as in Section 4, but use a subset of the baselines and only use 32 context ablations for CONTEXTCITE due to computational cost. We find that CONTEXTCITE consistently outperforms baselines. ", "page_idx": 26}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/99c26399f945b823299158b1b98bea3b94c826f879f2a3437b6bee32229eceed.jpg", "img_caption": ["(a) We report the top- $\\cdot k$ log-probability drop (1), which measures the effect of ablating top-scoring sources on the generated response. A higher drop indicates that the context attribution method identifies more relevant sources. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/0b6f71f31a16f29c93eb8c27241cf3053ee00ef9715970c0c6744721a829b46a.jpg", "img_caption": ["(b) We report the linear datamodeling score (LDS) (2), which measures the extent to which a context attribution can predict the effect of random context ablations. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 12: Evaluating word-level context attributions. We report the top- $k$ log-probability drop (Figure 12a) and linear datamodeling score (Figure 12b) of CONTEXTCITE and baselines. We evaluate attributions of responses generated by Llama-3-70B on 1, 000 randomly sampled validation examples from each of CNN DailyMail and Hotpot QA. ", "page_idx": 27}, {"type": "text", "text": "B.4.2 Random examples of CONTEXTCITE for Llama-3-70B ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Example of CONTEXTCITE attributions for Llama-3-70B on Hotpot QA ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Query ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Which artist has worked with King Crimson and also released a solo album in 2011? ", "page_idx": 28}, {"type": "text", "text": "Response (attributed statement is highlighted) ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "According to the text, Steven Wilson has worked with King Crimson and released a solo album called \"Grace for Drowning\" in 2011. ", "page_idx": 28}, {"type": "text", "text": "CONTEXTCITE attribution ", "text_level": 1, "page_idx": 28}, {"type": "table", "img_path": "7CMNSqsZJt/tmp/a172a2c7e920fd0c072d892e22b3cb1c3a4b505789387f7a82019c9e61fb96d3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Example of CONTEXTCITE attributions for Llama-3-70B on CNN DailyMail ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Query ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Please summarize the article in up to three sentences. ", "page_idx": 29}, {"type": "text", "text": "Response (attributed statement is highlighted) ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Paul Bell, a director of online gambling firm 666Bet, has been arrested as part of a $\\pounds21\\ m$ fraud probe, along with six others, and \u00a31m in cash was seized. The firm, which has deals with West Bromwich Albion and Leicester City, had its licence suspended by the UK gambling regulator, and its website is currently offline. Harry Redknapp, who appeared in the firm\u2019s adverts, claims to know nothing about the investigation. ", "page_idx": 29}, {"type": "text", "text": "CONTEXTCITE attribution ", "text_level": 1, "page_idx": 29}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/d1c5e05cff88775806506d38f2dbcc695254e35f44136d2a76b0d497a986ed0a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "B.5 Word-level CONTEXTCITE ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this work, we primarily focus on sentences on sources for context attribution. In this section, we briefly explore using CONTEXTCITE to perform context attribution with individual words as sources on the DROP benchmark [69]. We find that CONTEXTCITE can provide effective word-level attributions, but may require a larger number of context ablations. ", "page_idx": 29}, {"type": "text", "text": "In Figure 13, we evaluate word-level CONTEXTCITE on the DROP benchmark using the top- $k$ log-probability drop metric (1) and the linear datamodeling score (2). We use the same evaluation setup as in Section 4. While CONTEXTCITE matches or outperforms baselines, we find that it attains lower absolute values for the linear datamodeling score. This may be because word-level attributions are less sparse: a given generated statement may depend on many individual words within the context. It may also be because there are much stronger dependencies between words than between sentences, rendering a linear surrogate model less faithful. ", "page_idx": 30}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/4e04f68c657594103b7925005022245cf77ae72f2aec09d9992039774f45eb3a.jpg", "img_caption": ["(a) We report the top- $\\cdot k$ log-probability drop (1), which measures the effect of ablating top-scoring sources on the generated response. A higher drop indicates that the context attribution method identifies more relevant sources. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/59c6275aecc39762f4b7390e0786961cf414f61dddf04bd6aadabd33ef1e1d35.jpg", "img_caption": ["(b) We report the linear datamodeling score (LDS) (2), which measures the extent to which a context attribution can predict the effect of random context ablations. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 13: Evaluating word-level context attributions. We report the top- $k$ log-probability drop (Figure 13a) and linear datamodeling score (Figure 13b) of CONTEXTCITE and baselines. We evaluate attributions of responses generated by Llama-3-8B on 1, 000 randomly sampled validation examples from the DROP benchmark. ", "page_idx": 30}, {"type": "text", "text": "B.5.2 Random examples of word-level CONTEXTCITE ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section, we provide word-level CONTEXTCITE attributions for Llama-3-8B for randomly selected examples. For each example, we randomly select a sentence from the response to attribute and display the 4 sources with the highest attribution scores. ", "page_idx": 30}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/f83c808c1e56b960c31e9cb12f7e39e9aee8755cfc1921639346a503a8579941.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "7CMNSqsZJt/tmp/0fb497c7e55f53ccea96a942085f5d62cf3614ae4e8a3ff0d81afc247c6bf11b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "C Additional discussion ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "C.1 Connections to prior methods for understanding behavior via surrogate modeling ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "CONTEXTCITE attributes a language model\u2019s generation to individual sources in the context by learning a surrogate model [61] that simulates how excluding different sets of sources affects the model\u2019s output. The approach of learning a surrogate model to predict the effects of ablations has previously been used to attribute predictions to training examples [11, 23, 24], model internals [25], and input features [8\u201310]. For example, Ilyas et al. [11] learn a surrogate model to predict how excluding different training examples affects a model\u2019s output on a particular test example. ", "page_idx": 33}, {"type": "text", "text": "One key design choice shared by many of these methods is to learn a linear surrogate model (whose input is an ablation mask). A linear surrogate model is easily interpretable, as its weights may be cast directly as attributions. Another key design choice is to induce sparsity in the surrogate model, typically by learning with LASSO. Sparsity can further improve interpretability and may also decrease the number of samples needed to learn a faithful surrogate model. We find these design choice to be effective in the context attribution setting and adopt them for CONTEXTCITE. In the remainder of this section, we discuss detailed connections between CONTEXTCITE and a few closely related methods: LIME [8], Kernel SHAP [9], and datamodels [11]. ", "page_idx": 33}, {"type": "text", "text": "LIME [8]. ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "LIME (Local Interpretable Model-agnostic Explanations) is a method for attributing predictions of black-box classifiers to features. It does so by learning a local surrogate model that simulates the classifier\u2019s behavior in a neighborhood around a given prediction. ", "page_idx": 33}, {"type": "text", "text": "Specifically, consider a classifier $f$ that maps a $d$ -dimensional input in $\\mathbb{R}^{d}$ to a binary classification score $\\mathbb{R}$ . Given an input $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ to explain, LIME considers how ablating different features (by setting their value to zero) affects the model\u2019s prediction. To do so, LIME learns a surrogate model to predict the original model\u2019s classification score given the ablation vector $\\{0,1\\}^{d}$ denoting which sources to exclude. ", "page_idx": 33}, {"type": "text", "text": "To learn a surrogate model, LIME first collects a dataset of ablated inputs $x_{i}\\in\\mathbb{R}^{d}$ , corresponding ablation masks $\\bar{z_{i}}\\in\\{0,1\\}^{d}$ and corresponding model outputs $f(x_{i})\\in\\mathbf{\\bar{R}}$ . It then runs LASSO on the pairs $(z_{i},f(x_{i}))$ , yielding a sparse linear surrogate model $\\hat{f}:\\{0,1\\}^{d}\\to\\mathbb{R}$ . A key design choice of LIME is that the surrogate model is local. The pairs $(z_{i},f(x_{i}))$ are weighted according to a similarity kernel $\\pi_{x}$ (selected heuristically) to emphasizes pairs that are close to the original input $x$ . ", "page_idx": 33}, {"type": "text", "text": "Roughly speaking, if sources from the context are interpreted as features, CONTEXTCITE may be viewed as an extension of LIME to the generative setting with a uniform similarity kernel. The uniform similarity kernel leads to a global surrogate model: it approximates the mode behavior for arbitrary ablations, instead of just for ablations where a small number of sources are excluded. We observe empirically that in the context attribution setting, a global surrogate model is often faithful (see Section 3). ", "page_idx": 33}, {"type": "text", "text": "Kernel SHAP [9]. ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Lundberg and Lee [9] propose SHAP (SHapley Additive exPlanations) to unify methods for additive feature attribution. Additive feature attribution methods assign a weight to each feature in a model\u2019s input and explain a model\u2019s prediction as the sum of these weights (LIME is an additive feature attribution method). They show that there exists unique additive feature attribution values (which they call SHAP values) that satisfy a certain set of desirable properties; these unique attribution values correspond to the Shapley values [70] measuring the contribution of each feature to the model output. ", "page_idx": 33}, {"type": "text", "text": "To estimate SHAP values, Lundberg and Lee [9] propose Kernel SHAP, a method that uses LIME with a specific choice of similarity kernel that yields SHAP values. Specifically, in order for LIME to estimate SHAP values, they show that the similarity kernel for an ablation vector $v$ should be ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\pi_{\\mathrm{SHAP}}(v)=\\frac{d-1}{\\binom{d}{|v|}\\cdot|v|\\cdot(d-|v|)}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $d$ is the number of features and $|v|$ is the number of non-zero elements of the ablation vector $v$ ", "page_idx": 33}, {"type": "text", "text": "Using the same setup as in Appendix B.3, we compare the Kernel SHAP estimator (which uses LASSO with samples weighted according to $\\pi_{\\mathrm{SHAP}}]$ ) to the CONTEXTCITE estimator (which uses LASSO with a uniform similarity kernel) in Figure 14. We use the implementation of Kernel SHAP from the PyPI package shap [9]. We find that the CONTEXTCITE estimator results in a more faithful surrogate model than the Kernel SHAP estimator for context attribution (in terms of top- $k$ log probability drop for different values of $k$ ). ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "image", "img_path": "7CMNSqsZJt/tmp/819ccb3d08bc7559dae980f4bb771949f0a0c6471f1b89e0b8eb0795b95ab673.jpg", "img_caption": ["Figure 14: Comparing the effectiveness of the CONTEXTCITE and Kernel SHAP estimators for learning a surrogate model. We report the top- $k$ log probability drops (see Equation 1) for surrogate models learned using the CONTEXTCITE estimator and the Kernel SHAP estimator (using the implementation of Lundberg and Lee [9]). We find that the CONTEXTCITE estimator consistently identifies more impactful sources, and, in particular, when the number of context ablations is small. Error bars denote $95\\%$ confidence intervals. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Datamodels. The datamodeling framework [11] seeks to understand on how individual training examples affect a model\u2019s prediction on a given test example, a task called training data attribution. Specifically, a datamodel is a surrogate model that predicts a model\u2019s prediction on a given test example given a mask specifying which training examples are included or excluded. The surrogate model estimation method used by CONTEXTCITE closely matches that of datamodels (the only difference being that CONTEXTCITE samples ablation vectors uniformly, while datamodels samples ablation vectors with a fixed ablation rate $\\alpha$ ). ", "page_idx": 34}, {"type": "text", "text": "In the in-context learning setting, \u201ctraining examples\u201d are provided to a model as context before it is queried with a test example. Datamodels have previously been used to study in-context learning [23, 24]. If one thinks of in-context learning as sources, this form of training data attribution is a special case of context attribution. ", "page_idx": 35}, {"type": "text", "text": "More broadly, understanding how a model uses unstructured information presented in its context is conceptually different from understanding how a model uses its training examples. Some of the applications of context attribution are analogous to existing applications of training data attribution. For example, selecting query-relevant in-context information based on context attribution (see Section 5.2) is analogous to selecting training examples based on training data attribution [71]. However, other applications, such as helping verify the factuality of generated statements (see Section 5.1) do not have clear data attribution analogies. ", "page_idx": 35}, {"type": "text", "text": "C.2 Why does pruning the context improve question answering performance? ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In Section 5.2, we show that providing only the top- $k$ most relevant CONTEXTCITE sources for a language model\u2019s original answer to a question can improve the quality of its answer. We would like to note that the sources identified by CONTEXTCITE are those that were used to generate the original response. If the original response is incorrect, it may be surprising that providing only the sources that led to this response can improve the quality of the response. ", "page_idx": 35}, {"type": "text", "text": "To explain why pruning the context does improve question answering performance, we consider two failure modes associated with answering questions using long contexts: ", "page_idx": 35}, {"type": "text", "text": "1. The model identifies the wrong sources for the question and answers incorrectly. 2. The model identifies the correct sources for the question but misinterprets information because it is distracted by other irrelevant information in the context. ", "page_idx": 35}, {"type": "text", "text": "Intuitively, pruning the context to include only the originally identified sources can help mitigate the second failure mode but not the former. The fact that pruning the context in this way can improve question answering performance suggests that the second failure mode occurs and that mitigating it can thus improve performance. ", "page_idx": 35}, {"type": "text", "text": "C.3 Computational efficiency of CONTEXTCITE ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Most of the computational cost of CONTEXTCITE comes from creating the surrogate model\u2019s training dataset. Hence, the efficiency of CONTEXTCITE depends on how many ablations it requires to learn a faithful surrogate model. We find that CONTEXTCITE requires just a small number of context ablations to learn a faithful surrogate model\u2014in our experiments, 32 context ablations suffice. Thus, attributing responses using CONTEXTCITE is $32\\times$ more expensive than generating the original response. We note that the inference passes for each of these context ablations can be fully parallelized. Furthermore, because CONTEXTCITE is a post-hoc method that can be applied to any existing response, a user could decide when they would like to pay the additional computational cost of CONTEXTCITE to obtain attributions. When we use CONTEXTCITE to attribute multiple statements in the response, we use the same context ablations and inference calls. In other words, there is a fixed cost to attribute (any part of) a generated response, after which it is very cheap to attribute specific statements. ", "page_idx": 35}, {"type": "text", "text": "C.3.1 Why do we only need a small number of ablations? ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We provide a brief justification for why 32 context ablations suffice, even when the context comprises many sources. Since we are solving a linear regression problem, one might expect the number of ablations needed to scale linearly with the number of sources. However; in our sparse linear regression setting, we have full control over the covariates (i.e., the context ablations). In particular, we ablate sources in the context independently and each with probability $1/2$ . This makes the resulting regression problem \u201cwell-behaved.\u201d Specifically, this lets us leverage a known result (see Theorems 7.16 and 7.20 of Wainwright [72]) which tells us that we only need $O(k\\log(d))$ context ablations, where $d$ is the total number of sources and $k$ is the number of sources with non-zero relevance to the response. In other words, the number of context ablations we need grows very slowly with the total number of sources. It only grows linearly with the number of sources that the model relies on when generating a particular statement. As we show empirically in Figure 3a, this number of sources is often small. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "C.4 Limitations of CONTEXTCITE ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this section, we discuss a few limitations of CONTEXTCITE. ", "page_idx": 36}, {"type": "text", "text": "Potential failure modes. Although we find a linear surrogate model to often be faithful empirically (see Figure 2, Appendix B.2), this may not always be the case. In particular, we hypothesize that the linearity assumption may cease to hold when many sources contain the same information. In this case, a model\u2019s response would only be affected by excluding every one of these sources. In practice, to verify the faithfulness of the surrogate model, a user of CONTEXTCITE could hold out a few context ablations to evaluate the surrogate model (e.g., by measuring the LDS). They could then assess whether CONTEXTCITE attributions should be trusted. ", "page_idx": 36}, {"type": "text", "text": "Another potential failure mode of CONTEXTCITE is attributing generated statements that follow from previous statements. Consider the generated response: \u201cHe was born in 1990. He is 34 years old.\u201d with context mentioning a person born in 1990. If we attribute the statement \u201cHe was born in 1990.\u201d we would likely find the relevant part of the context. However, if we attribute the statement \u201cHe is 34 years old.\u201d we might not identify any attributed sources, despite this statement being grounded in the context. This is because this statement is conditioned on the previous statement. Thus, in this case there is an \u201cindirect\u201d attribution to the context through a preceding statement that would not be identified by the current implementation of CONTEXTCITE. ", "page_idx": 36}, {"type": "text", "text": "Unintuitive behaviors. A potentially unintuitive behavior of CONTEXTCITE is that it can yield a low attribution score even for a source that supports a statement. This is because CONTEXTCITE provides contributive attributions. Hence, if a language model already knows a piece of information from pre-training and does not rely on the context, CONTEXTCITE would not identify sources. This may lead to unintuitive behaviors for users. ", "page_idx": 36}, {"type": "text", "text": "Validity of context ablations. In this work, we primarily consider sentences as sources for context attribution and perform context ablations by simply removing these sentences. One potential problem with this type of ablation is dependencies between sentences. For example, consider the sentences: \u201cJohn lives in Boston. Charlie lives in New York. He sometimes visits San Francisco.\u201d In this case, \u201cHe\u201d refers to Charlie. However, if we ablate just the sentence about Charlie, \u201cHe\u201d will now refer to \u201cJohn.\u201d There may be other ablation methods that more cleanly remove information without changing the meaning of sources because of dependencies. ", "page_idx": 36}, {"type": "text", "text": "Computational efficiency. As previously discussed, attributing responses using CONTEXTCITE is $32\\times$ more expensive than generating the original response. This may be prohibitively expensive for some applications. ", "page_idx": 36}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 37}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 37}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 37}, {"type": "text", "text": "\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u2022 Please provide a short (1-2 sentence) justification right after your answer (even for NA). ", "page_idx": 37}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 37}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 37}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 37}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 37}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: See experiments. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper. ", "page_idx": 37}, {"type": "text", "text": "\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 37}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: See paper. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 38}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: We do not have theoretical results. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 38}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Yes, see appendices. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 38}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. ", "page_idx": 38}, {"type": "text", "text": "\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. ", "page_idx": 38}, {"type": "text", "text": "\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ", "page_idx": 39}, {"type": "text", "text": "\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": ": [Yes]   \nJustification: Our code is open-source (see abstract). We use publicly available datasets and   \nmodels.   \nGuidelines:   \n\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so no is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes]   \nJustification: Yes, see appendices   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes]   \nJustification: Our empirical findings account for statistical significance.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 40}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Yes, see appendices. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification:   \nGuidelines:   \n\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: Yes, see appendices. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: We do not release data or models that have a high risk for misuse. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 41}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] Justification: Yes, we cite all the datasets and pre-trained models used in the paper. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 42}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: See appendices. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 42}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: We do not have crowdsourcing experiments or research with human subjects. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 42}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA]   \nJustification: We do not require IRB approval for our work. Guidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}]