[{"figure_path": "83e3DPVrFC/figures/figures_0_1.jpg", "caption": "Figure 1: The proposed method demonstrates the ability to accurately generate objects with complex descriptions in the correct locations while faithfully preserving the details specified in the text. In contrast, existing methods such as BoxDiff [57], R&B [56], GLIGEN [25], and InstDiff [54] struggle with the complex object descriptions, leading to errors in the generated objects.", "description": "This figure shows a comparison of layout-to-image generation results between the proposed method and four existing methods (BoxDiff, R&B, GLIGEN, and InstDiff).  The input is a layout specifying the position and description of four mugs. The proposed method accurately generates the mugs according to their descriptions. In contrast, the existing methods make errors in generating the objects, demonstrating the superiority of the proposed method in handling complex object descriptions.", "section": "Abstract"}, {"figure_path": "83e3DPVrFC/figures/figures_2_1.jpg", "caption": "Figure 2: An example of regional cross-attention with two overlapping objects. Cross-attention is applied to each pair of regional visual and grounded textual tokens. The overlapping region cross-attends with the textual tokens containing both objects, while the non-object region attends to a learnable \u201cnull\u201d token.", "description": "This figure illustrates the Regional Cross-Attention mechanism with two overlapping objects.  Each object's bounding box is processed individually.  The textual description for each object is used in a cross-attention process with the corresponding image region. Where bounding boxes overlap, the cross-attention mechanism considers both overlapping object's descriptions. Regions without any objects attend to a 'null' token. This process ensures that each object's description is accurately reflected in the generated image while handling overlapping objects.", "section": "3.2 Regional Cross-Attention"}, {"figure_path": "83e3DPVrFC/figures/figures_4_1.jpg", "caption": "Figure 3: Sequenced Grounding Encoding with box coordinates as indicators.", "description": "This figure illustrates the Sequenced Grounding Encoding (SGE) process.  The input is a sequence of text tokens representing object descriptions, such as \"This is a cat; A dog is here.\"  These tokens are processed by a text encoder.  Simultaneously, positional encoding is applied to bounding box coordinates, represented as numerical vectors.  These coordinate vectors are then concatenated with the text embeddings to create \"grounding tokens\". Each grounding token combines the textual description with its corresponding object's spatial location. This combined representation is then used in the regional cross-attention mechanism.", "section": "3.2 Regional Cross-Attention"}, {"figure_path": "83e3DPVrFC/figures/figures_5_1.jpg", "caption": "Figure 4: Statistical comparisons between the synthetic object descriptions generated by GLIGEN [25], InstDiff [54], and our method. We measure the 1) average caption length, 2) the Gunning Fog Score, which estimates the text complexity from the education level required to understand the text, 3) the number of unique words per sample which indicates the text diversity, and 4) the object-label CLIP Alignment Score to measure object-label alignment. The results show that the pseudo-labels generated for our dataset are more complex, diverse, lengthier, and align better with objects, compared to those generated by GLIGEN and InstDiff.", "description": "This figure presents a statistical comparison of synthetic object descriptions generated by three different methods: GLIGEN, InstDiff, and the proposed method in the paper.  Four metrics are used for comparison: average caption length, Gunning Fog Score (complexity), unique words per sample (diversity), and object-label CLIP alignment score. The results demonstrate that the descriptions generated by the proposed method are more complex, diverse, and better aligned with the objects compared to the other two methods.", "section": "5 Experiments"}, {"figure_path": "83e3DPVrFC/figures/figures_6_1.jpg", "caption": "Figure 5: Qualitative comparison of rich-context L2I generation, showcasing our method alongside open-set L2I approaches GLIGEN [25] and InstDiff [54], based on detailed object descriptions. Our method consistently generates more accurate representations of objects, particularly in terms of specific attributes such as colors and shapes. Strikethrough text indicates missing content in the generated objects from the descriptions.", "description": "This figure compares the results of three different layout-to-image (L2I) generation methods: the proposed method and two existing open-vocabulary methods, GLIGEN and InstDiff.  Each method is given the same complex, detailed description of objects to generate. The figure visually demonstrates the superiority of the proposed method, which accurately generates objects with the correct attributes (e.g., color, shape), while the other two methods often omit details or generate inaccurate representations.", "section": "5 Experiments"}, {"figure_path": "83e3DPVrFC/figures/figures_8_1.jpg", "caption": "Figure 4: Statistical comparisons between the synthetic object descriptions generated by GLIGEN [25], InstDiff [54], and our method. We measure the 1) average caption length, 2) the Gunning Fog Score, which estimates the text complexity from the education level required to understand the text, 3) the number of unique words per sample which indicates the text diversity, and 4) the object-label CLIP Alignment Score to measure object-label alignment. The results show that the pseudo-labels generated for our dataset are more complex, diverse, lengthier, and align better with objects, compared to those generated by GLIGEN and InstDiff.", "description": "This figure presents a statistical comparison of synthetic object descriptions generated by three different methods: GLIGEN, InstDiff, and the proposed method.  Four metrics are used for comparison: average caption length, Gunning Fog Score (text complexity), unique words per sample (diversity), and object-label CLIP alignment score (alignment accuracy). The results demonstrate that the proposed method generates more complex, diverse, and longer descriptions that align better with the objects compared to the other two methods.", "section": "5 Experiments"}, {"figure_path": "83e3DPVrFC/figures/figures_14_1.jpg", "caption": "Figure 7: All methods are tested with float16 precision and 25 inference steps. The results are averaged over 20 runs. Notably, the overall throughput of our method is not significantly hampered. In a typical scenario with 5 objects, the throughput of our method exceeds 60% of the throughput of the original backbone model. Please note that while the official backbone of GLIGEN is SD1.4, its network structure and throughput are identical to those of SD1.5.", "description": "This figure compares the throughput (images per second) of different layout-to-image generation methods (ours, GLIGEN, InstDiff, and baselines using Stable Diffusion 1.5 and 1.5 XL) across varying numbers of objects in a scene. The results demonstrate that the proposed method's throughput is not significantly impacted by the increased number of objects compared to baseline methods.  While GLIGEN uses Stable Diffusion 1.4, its architecture and throughput are virtually identical to Stable Diffusion 1.5.", "section": "C Throughput of Different Layout-to-Image Methods"}, {"figure_path": "83e3DPVrFC/figures/figures_16_1.jpg", "caption": "Figure 8: The model with region reorganization can accurate generated objects that better align with the designated layouts, while the feature averaging solution can result in objects in incorrect location, generating undesired instances or making the overlapping instances inseparable.", "description": "This figure compares the results of layout-to-image generation using two different methods: feature averaging and region reorganization.  The top row shows the layout boxes for two example scenarios (a seascape and a pair of earrings).  The middle row shows the results using feature averaging, where the model struggles with overlapping objects. For the seascape, the boat is incorrectly positioned, and for the earrings, the two earring instances are inseparable. The bottom row illustrates the superior performance of region reorganization, generating accurately positioned and distinct objects for both examples.", "section": "G Effectiveness of Regional Cross-Attention with Visual Example"}, {"figure_path": "83e3DPVrFC/figures/figures_17_1.jpg", "caption": "Figure 2: An example of regional cross-attention with two overlapping objects. Cross-attention is applied to each pair of regional visual and grounded textual tokens. The overlapping region cross-attends with the textual tokens containing both objects, while the non-object region attends to a learnable \"null\" token.", "description": "This figure illustrates the regional cross-attention mechanism used in the proposed model. It shows how cross-attention is applied separately to different regions of an image based on object bounding boxes.  For regions containing multiple objects (overlapping boxes), the cross-attention considers the textual tokens related to all intersecting objects. Regions not containing any objects attend to a learned \"null\" token. The figure highlights the model's ability to handle complex object relationships and rich textual descriptions.", "section": "3.2 Regional Cross-Attention"}, {"figure_path": "83e3DPVrFC/figures/figures_17_2.jpg", "caption": "Figure 2: An example of regional cross-attention with two overlapping objects. Cross-attention is applied to each pair of regional visual and grounded textual tokens. The overlapping region cross-attends with the textual tokens containing both objects, while the non-object region attends to a learnable \"null\" token.", "description": "This figure illustrates the regional cross-attention mechanism used in the proposed layout-to-image generation model. It shows how the model handles overlapping objects by applying cross-attention to both the overlapping and non-overlapping regions.  Specifically, the overlapping region attends to textual tokens describing both objects, while non-overlapping regions attend to tokens related to only one object, or a special \"null\" token if no object is present in that region. This ensures that the model accurately represents the objects and their relationships in the generated image.", "section": "3.2 Regional Cross-Attention"}, {"figure_path": "83e3DPVrFC/figures/figures_18_1.jpg", "caption": "Figure 5: Qualitative comparison of rich-context L2I generation, showcasing our method alongside open-set L2I approaches GLIGEN [25] and InstDiff [54], based on detailed object descriptions. Our method consistently generates more accurate representations of objects, particularly in terms of specific attributes such as colors and shapes. Strikethrough text indicates missing content in the generated objects from the descriptions.", "description": "This figure compares the results of three different layout-to-image generation methods (the proposed method, GLIGEN, and InstDiff) on several complex object descriptions.  Each row shows a layout (top) with the desired object location and descriptions, and then the generated images by each method. The proposed method demonstrates superior accuracy in capturing the detailed descriptions, while the other methods often miss key details or generate incorrect objects.  Strikethroughs in the comparison highlight features missing from the other methods' generated images.", "section": "5 Experiments"}]