[{"type": "text", "text": "Mixture of Adversarial LoRAs: Boosting Robust Generalization in Meta-Tuning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xu Yang1 Chen Liu\u2020 1 Ying Wei\u2020 2 ", "page_idx": 0}, {"type": "text", "text": "1 City University of Hong Kong 2 Zhejiang University xyang337-c@my.cityu.edu.hk chen.liu@cityu.edu.hk ying.wei@zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper introduces AMT, an Adversarial Meta-Tuning methodology, to boost the robust generalization of pre-trained models in the out-of-domain (OOD) few-shot learning. To address the challenge of transferring knowledge from source domains to unseen target domains, we construct the robust LoRAPool by meta-tuning LoRAs with dual perturbations applied to not only the inputs but also singular values and vectors of the weight matrices at various robustness levels. On top of that, we introduce a simple yet effective test-time merging mechanism to dynamically merge discriminative LoRAs for test-time task customization. Extensive evaluations demonstrate that AMT yields significant improvements, up to $12.92\\%$ in clean generalization and up to $49.72\\%$ in adversarial generalization, over previous state-of-the-art methods across a diverse range of OOD few-shot image classification tasks on three benchmarks, confirming the effectiveness of our approach to boost the robust generalization of pre-trained models. Our code is available at https://github.com/xyang583/AMT. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Few-shot learning (FSL) has recently been revolutionized by large-scale pre-trained vision transformer models [1, 2, 3, 4, 5]. Their generalization capability can be further enhanced with a few annotated examples, achieving impressive performance across a broad spectrum of downstream tasks [6, 7, 8, 9, 10]. Building on this foundation, meta-tuning emerges as a powerful strategy that integrates the broad generalization capabilities of pre-trained prior knowledge with the adaptive flexibility of meta-learning, allowing models to quickly adapt to new tasks in few-shot scenarios [11, 12]. ", "page_idx": 0}, {"type": "text", "text": "Despite its success, the robust generalization of meta-tuning to defend against adversarial attacks [13, 14, 15] and adapt to out-of-distribution (OOD) downstream tasks [16, 17, 18] remains an ongoing challenge. However, it is crucial for various real-world applications such as medical imaging diagnostics and autonomous driving to simultaneously achieve competitive performance on adversarial examples or out-of-distribution data. Deployed models often encounter novel environments with distribution shifts between training and test data, including variations in hospital equipment and protocols [19] or diverse urban road scenarios [20]. Moreover, these models are vulnerable to adversarial attacks leading to harmful diagnoses or unsafe driving decisions. For instance, adversaries can perturb sensor signals to deceive 2D or 3D medical imaging models [21], manipulate traffic signs with malicious stickers [22], or fool the autopilot into following unsafe trajectories [23]. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we delve into leveraging adversarial training and meta-tuning to enhance robust generalization of pre-trained vision transformers across different domains. Compared with previous meta-tuning approaches, this involves two unique aspects. Firstly, when incorporating adversarial examples, the model should learn to adapt to the worst-case tasks while preserving its inherent generalization capabilities. Inspired by the observation that the singular values distribution of weight parameters undergoes significant changes during fine-tuning [24], we aim to explicitly strengthen the principal components of pre-trained model weight matrices during meta-tuning. To this end, we inject perturbations on both input and principal singular values and vectors via the incremental meta-update of the Low-rank Adapter (LoRA) [25, 26] on top of frozen pre-trained parameters. Secondly, the adversarial perturbation needs to simulate wide distribution variations from the training environment, and care must be taken to avoid interference when training with multiple perturbation types [27, 28]. Thus, we introduce an adaptive robust LoRAPool constructed by meta-tuning different LoRAs in parallel for different attack strengths. To adapt to novel tasks from unseen distributions, we view the robust LoRAPool as the basis and integrate meta-updated principal components into the pre-trained model through a test-time merging mechanism for downstream task customization. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose AMT, a novel adversarial meta-tuning approach for enhancing the robust generalization of pre-trained vision transformers across diverse domains.   \n\u2022 By injecting the adversarial perturbations on the inputs, singular values and vectors of the weight matrices, the core components of pre-trained model weights are consolidated for worst-case tasks. We further enhance this approach with the adaptive robust LoRAPool meta-tuned under varying perturbation budgets, without compromising the pre-trained model\u2019s inherent capabilities.   \n\u2022 We integrate discriminative principle components into the pre-trained model via a simple yet effective test-time merging mechanism for customizing task-specific feature extractors, which is compatible with other test-time fine-tuning methods.   \n\u2022 We experimentally evaluate our method on challenging large-scale out-of-domain few-shot image classification benchmarks, including Meta-Dataset [16] that consists of 9 OOD datasets, as well as BSCD-FSL [29] and fine-grained datasets [30] comprising another 8 OOD datasets. Our method achieves impressive few-shot performance across domains, significantly outperforming previous state-of-the-art methods in clean generalization by up to $12.92\\%$ and in adversarial generalization by up to $49.72\\%$ . ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Out-of-Domain Few-shot Learning and Meta-Learning. Out-of-Domain Few-Shot Learning (OOD-FSL) aims to transfer prior knowledge learned on source domains to unseen target domains to address the few-shot learning problem [16, 17, 18, 29, 31, 32, 33, 34, 35, 36]. Meta-learning relies on episodic training to learn parameter initialization [37, 38, 39], optimization rule [40, 41, 42] or a transferable metric space [43, 44, 45, 46, 47] as prior knowledge for quick adaptation to new tasks. To tackle distribution shifts, many methods are proposed by building a universal feature representation with multiple feature extractors [32, 31], conditioning batch normalization parameters [48, 30, 33], or test-time gradient-based fine-tuning [34]. Most related to our work is FLUTE [33], which jointly trains the feature extractor with multiple sets of Feature-wise Linear Modulation (FiLM) [49] parameters on multiple training datasets and combines them as the initialization for gradient descent at test time. Our method AMT stands in the single source domain setting and differs from previous works in that our adversarial meta-tuning does not compromise the pre-trained model, and the adaptive merging mechanism of the robust LoRAPool performs task customization in a non-parametric manner without the requirement of gradient descent, ensuring scalability with newly added components to the pool. ", "page_idx": 1}, {"type": "text", "text": "Vision Transformers in Few-shot Learning. Vision Transformers (ViTs) have gained prominence as the foundation model due to their ability to capture long-range dependencies in data [50, 51, 52]. Selfsupervised pre-training effectively endows vision transformer with data-driven and well-generalized prior [1, 2, 53, 54], especially for the few-shot learning task. In the spirit of transfer learning, one line of works leverages self-distillation framework to seek universal feature representations without meta-training [55, 56, 57] and directly learns auxiliary visual prompts [58] and attention scaling matrices [59] on the support set through gradient descent during meta-testing. Another important research direction is developing meta-learning techniques to enhance pre-trained models with input-conditioned prompts [8] and task-specific masks [11]. PMF [12] contributes a strong baseline by meta-tuning the full model. In this work, we also ground our method on pre-trained vision transformers and show that adversarial meta-tuning can further boost their robust generalization across downstream tasks. Also, our contribution is orthogonal to other existing test-time fine-tuning methods and provides a better starting point to improve their performance at test time. ", "page_idx": 1}, {"type": "text", "text": "Adversarial Training for Out-of-Distribution Generalization. Adversarial training [13] is one of the most effective defense techniques to improve the model adversarial robustness by minimizing a locally maximized loss function via adversarial perturbation on inputs [13, 60, 61, 62, 63, 64, 65] and model parameters [66]. Despite widely recognized trade-offs between adversarial robustness and clean accuracy [67, 68], and between in-distribution (ID) and out-of-distribution (OOD) generalization [63, 69], there exist strategies to achieve better balances among these trade-offs. These strategies include modified adversarial training regime [65], dual sets of parameters [70, 71, 72], model ensemble [73], multi-scale patch perturbations [74], or partial fine-tuning strategy [75]. Furthermore, since adversarially perturbed input data can be viewed as a special type of OOD data [76], recent studies [77, 78, 79] have demonstrated that adversarial pre-training can enhance generalization performance on downstream datasets and improve robustness against distribution shifts. Compared with sample-wise adversarial attacks, where all samples in each domain share the universal perturbation, the distributional attacks in a low-rank structure show the capability of making the models resistant against adversarial perturbations of higher magnitude [80, 81] Inspired yet different from the previous attack methods, our method utilizes a mixture of adversarial low-rank adaptors customized for meta-tuning to enhance the robust generalization of clean pre-trained models. ", "page_idx": 2}, {"type": "text", "text": "Adversarial Meta-Learning. There is a series of works that leverage adversarial training to enhance the few-shot learner\u2019s adversarial robustness [14, 15, 82, 83]. However, compared with standard few-shot learning, the adversarially trained model has degraded clean accuracy [14]. Adversarial training is also utilized to improve the cross-domain few-shot learning performance by attacking individual image pixels [84] and features [85, 86]. For example, StyleAdv [86] perturbs each sample style in a task through attacking statistical information of AdaIN [87] and updating all parameters. Our approach diverges from these existing methods, aiming to further enhance the generalization performance of large-scaled pre-trained models. To achieve this, we propose to inject double perturbations on inputs as well as singular values and vectors over the entire query set as a whole during meta-tuning, while keeping all pre-trained parameters frozen to preserve prior knowledge. ", "page_idx": 2}, {"type": "text", "text": "Parameter-Efficient Few-Shot Learning. To reduce the computational cost associated with fullmodel fine-tuning, various parameter-efficient fine-tuning (PEFT) methods have been proposed that only update a small set of parameters, including inserting soft prompts [88, 89], adding adapter modules [90, 91, 92], and introducing low-rank matrices [25, 93, 94]. Recent works have shown that PEFT achieves comparable or superior performance than standard fine-tuning in the few-shot setting for large language models [95]. In this work, we explore crafting the small parameter sets via meta-tuning to boost robust generalization of pre-trained vision transformers. Concretely, we leverage LoRA [25] as the core parameter-efficient component for constructing the adaptive robust pool, as it enables low-rank updates to be merged into network weights without additional computational or memory costs incurred during inference. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we focus on out-of-domain few-shot image classification where our goal is to find parameters $\\theta$ that generalize well on unseen target domains with the single-source training domain. In this context, the model not only needs to learn novel concepts from limited data but also to generalize well across diverse domains. For each domain, there exists a dataset collected from that environment. During training, we only have access to the single source training dataset $\\mathcal{D}_{t r}^{\\mathrm{seen}}$ , from which each task $\\boldsymbol{\\mathcal{T}}=\\left(\\boldsymbol{S},\\boldsymbol{\\mathcal{Q}}\\right)$ is randomly sampled as the input. The support set $\\boldsymbol{S}$ contains $K$ annotated images for each of the $N$ categories: $\\boldsymbol{S}=\\left\\{x_{s},y_{s}\\right\\}_{s=1}^{N K}$ , while the query set $\\mathcal{Q}$ contains $M$ images Q = {xq, yq}q=1 . At evaluation time, the aim is to tackle tasks with novel classes sampled from previously unseen datasets $\\mathcal{D}_{t e s t}^{\\mathrm{unseen}}$ . ", "page_idx": 2}, {"type": "text", "text": "4 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce our approach in this section. The overall framework of our AMT is illustrated in Figure 1. It contains two components: (i) adversarial singular value and vector perturbation, which explicitly perturbs the singular values and vectors to highlight the principal components in the worstcase tasks; (ii) Adaptive robust LoRAPool, which consists of several adversarially meta-tuned LoRA modules and test-time merging mechanism to adaptively merge them for task customization. ", "page_idx": 2}, {"type": "image", "img_path": "HxGdbAmYYr/tmp/7ab6fe2f22c464016f1e624eef3288cbc8fbaf35a9e6e0762123243713d55417.jpg", "img_caption": ["Figure 1: Overview of our method. Adversarial perturbations, bounded by different budgets $\\epsilon$ , are incorporated into the clean query set. To construct the robust LoRAPool, the LoRA modules initialized with SVD results are meta-tuned on the adversarial examples, upon which adversarial perturbations are injected into singular values and vectors. The discriminative incremental updates of principal components are adaptively merged into the pre-trained weights for test-time task customization. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "4.1 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Adversarial Meta-Tuning. We ground our method on a large-scale pre-trained Vision Transformer [50] and then meta-tune the model in an episodic manner [43], following PMF [12]. To robustify the learned meta-knowledge, adversarial meta-tuning adopts the worst-case optimization by injecting the adversarial perturbation $\\delta$ to the query image $x_{q}$ through the minimax strategy [14, 15]. The intuition here is to make the meta-tuned model have the same prediction in the worst-case task. ", "page_idx": 3}, {"type": "text", "text": "We consider the $l_{\\infty}$ norm bounded perturbations in this work, so the corresponding optimization problem can be formulated as $\\begin{array}{r}{\\operatorname*{min}_{\\theta}\\operatorname*{lim}_{\\substack{\\operatorname*{max}_{\\parallel\\delta\\parallel_{\\infty}}\\le\\epsilon}}\\mathcal{L}\\left(f_{\\theta}\\left(S,x_{q}+\\delta\\right),y_{q}\\right)}\\end{array}$ where $f_{\\theta}$ denotes predicted logits of a query example with the model parameters $\\theta$ , and $\\mathcal{L}$ is the meta-task loss, which is usually the cross-entropy loss for image classification. The inner maximization problem can be efficiently solved by gradient-based methods. In practice, Projected Gradient Descent (PGD) [13] is the most popular method to generate adversarial perturbations $\\delta$ . Specifically, when the step size is $\\alpha$ , PGD optimizes $\\delta$ by running the following update rule for multiple iterations. Here, $\\Pi$ is the projection operator to clip $\\delta$ so that $\\lVert\\delta\\rVert_{\\infty}\\leq\\epsilon$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\delta\\gets\\Pi_{\\epsilon}\\left(\\delta+\\alpha\\cdot\\mathrm{sign}\\left(\\nabla_{\\delta}\\mathcal{L}\\left(f_{\\theta}(S,x_{q}+\\delta),y_{q}\\right)\\right)\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Low Rank Adaptation. LoRA [25] is one of the popular parameter-efficient fine-tuning approaches for transformer models. Given a pre-trained weight matrix $W\\in\\mathbb{R}^{d_{i n}\\times d_{o u t}}$ , LoRA approximates incremental updates to the parameter matrix with a low-rank decomposition $\\triangle W=A B$ , where $A\\,\\in\\,\\mathbb{R}^{d_{i n}\\times r}$ and $\\boldsymbol{B}\\in\\mathbb{R}^{r\\times d_{o u t}}$ , and the rank $r\\,\\ll\\,\\operatorname*{min}(d_{i n},d_{o u t})$ . The LoRA approach can be applied to all the linear layers in the vision transformer. For an input $x$ and a hidden state $h=W x$ , LoRA modifies forward process as $h=(W+\\triangle W)x=W x+A B x$ . When fine-tuning, $W$ is frozen while $A$ and $B$ are trainable. In addition, $A$ is randomly initialized via Gaussian initialization while $B$ is initialized to zero, resulting in the incremental update $A B=0$ at the beginning. ", "page_idx": 3}, {"type": "text", "text": "4.2 Adversarial Singular Value and Vector Perturbation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Drawing inspiration from the insight that the distribution of singular values undergoes significant changes during fine-tuning [24], we aim to explicitly strengthen the principal components of pretrained model weight matrices to enhance the model\u2019s generalization capability across diverse target domains. Using the on-the-fly generated adversarial query samples, we inject the worstcase perturbation on singular values and vectors over the entire query set. However, meta-tuning the full model and performing multiple singular value decomposition (SVD) during training are computationally expensive. To this end, we adopt the LoRA formulation to update model parameters during meta-tuning and initialize the incremental updates of LoRA with the result of SVD of weight matrices for the multi-head self-attention (MHA) layer and feed-forward network (FFN) layer in the vision transformer [26]. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Formally, for a weight matrix $W~\\in~\\mathbb{R}^{d_{i n}\\times d_{o u t}}$ and its singular value decomposition $W\\,=$ $U\\operatorname{diag}(S)V^{T}$ , where $U\\,\\in\\,\\mathbb{R}^{d_{i n}\\,\\times\\,\\mathrm{min}\\left(d_{i n},d_{o u t}\\right)}$ , $V\\,\\in\\,\\mathbb{R}^{d_{o u t}\\times\\operatorname*{min}\\left(d_{i n},d_{o u t}\\right)}$ and $\\bar{S}\\,\\in\\,\\mathbb{R}^{\\mathrm{min}\\left(d_{i n},d_{o u t}\\right)}$ represent the left/right singular vectors and the singular values in descending order, respectively. In the LoRA formulation $\\triangle W=A B$ with the rank $r$ , the top $r$ singular values and corresponding vectors are utilized to initialize $A\\in\\mathbb{R}^{d_{i n}\\times r}$ and $\\boldsymbol{B}\\in\\mathbb{R}^{r\\times d_{o u t}}$ , while the residual singular values and vectors are used to calculate the residual matrix $W^{r e s}\\in\\mathbb{R}^{d_{i n}\\times d_{o u t}}$ for error correction: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\cal{A}}=U_{[:r]}\\,\\mathrm{diag}\\left(S_{[:r]}^{1/2}\\right)\\in\\mathbb{R}^{d_{i n}\\times r}\\mathrm{~\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}}\\\\ {{\\cal{B}}=\\mathrm{diag}\\left(S_{[:r]}^{1/2}\\right)V_{[:r]}^{T}\\in\\mathbb{R}^{r\\times d_{o u t}}}\\\\ {{\\cal{W}}^{r e s}=U_{[r;]}\\,\\mathrm{diag}\\left(S_{[r;]}\\right)V_{[r;]}^{T}\\in\\mathbb{R}^{d_{i n}\\times d_{o u t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In Equation (2), we have $W=W^{r e s}+A B$ . During training, $W^{r e s}$ is kept frozen, so the updates of $A$ and $B$ in the subspace approximate the modification of principle singular value and vectors. ", "page_idx": 4}, {"type": "text", "text": "To boost the generalization performance of the model, we utilize the sharpness-aware minimization (SAM) [96] to update $A$ and $B$ . Specifically, we find the worst-case perturbation $\\delta_{A}$ and $\\delta_{B}$ in the neighborhood of $A$ and $B$ by gradient ascent. $\\delta_{A}$ is calculated by the following equation where $M$ is the size of query set and $\\eta_{1}$ depicts the size of the neighbourhood. $\\delta_{B}$ can be calculated similarly. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\delta_{A}=\\eta_{1}\\cdot\\frac{1}{M}\\sum_{q=1}^{M}\\nabla_{A}\\mathcal{L}\\big(f_{W^{r e s}+A B}(S,x_{q}^{a d v}),y_{q})\\big)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, we omit other parameters in $\\theta$ for notation simplicity. We then use the gradient based on the worst-case neighborhood to update $A$ and $B$ . Given the learning rate $\\eta_{2}$ , the update rule for $A$ is as follows. $B$ is updated similarly using the same learning rate. ", "page_idx": 4}, {"type": "equation", "text": "$$\nA\\gets A-\\eta_{2}\\cdot\\frac{1}{M}\\sum_{q=1}^{M}\\nabla_{A}\\mathcal{L}(f_{W^{r e s}+(A+\\delta_{A})B}(\\mathcal{S},x_{q}^{a d v}),y_{q}))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Different from prior adversarial meta-learning works [14, 15], this paper focuses on improving both the clean accuracy and cross-domain robustness for few-shot learning [63]. The meta-objective function of AMT is the combination of both aspects: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{C E}\\left(f_{W^{r e s}+A B}\\left(\\boldsymbol{S},\\boldsymbol{x}_{q}\\right),\\boldsymbol{y}_{q}\\right)+\\lambda_{a d v}D_{\\mathrm{KL}}\\left(f_{W^{r e s}+A B}\\left(\\boldsymbol{S},\\boldsymbol{x}_{q}^{a d v}\\right)\\left\\Vert f_{W^{r e s}+A B}\\left(\\boldsymbol{S},\\boldsymbol{x}_{q}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{L}_{C E}$ is the original cross-entropy loss, $D_{\\mathrm{KL}}$ is the Kullback-Leibler divergence and $\\lambda_{a d v}$ is the trade-off coefficient. Note that here we use few-shot task loss instead of global classification loss in StyleAdv [86] to generate the adversarial attacks, by which we leverage label randomness to avoid the potential performance degradation caused by true label leaking effect [62]. ", "page_idx": 4}, {"type": "text", "text": "4.3 Adaptive Robust LoRAPool ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Robust LoRAPool Construction. To simulate various distributional shifts for the unseen tasks, we adversarially meta-tune $P$ LoRA modules in parallel by Equation (4), each corresponding to a different robustness level controlled by the size of the adversarial budget, i.e., $\\epsilon$ in Equation 1. Therefore, we will obtain a robust LoRAPool composed of $P$ LoRA modules $\\phi=\\left[A_{1}B_{1},\\ldots,A_{P}B_{P}\\right]$ . Algorithm 1 shows our adversarial meta-tuning pipeline. ", "page_idx": 4}, {"type": "text", "text": "Test-time Merging. Given several LoRA modules, the challenge in the evaluation time is to adaptively merge these modules in robust LoRAPool into the pre-trained model to fit the new tasks. It is commonly assumed in domain generalization that unseen distributions fall within the convex hull of the training environments [97, 98], so we consider the LoRAs in the pool as the bases and learn a convex combination adapted to the task at hand. ", "page_idx": 4}, {"type": "text", "text": "1: Input: Source training domain $\\mathcal{D}_{t r}^{s e e n}$ ; pre-trained weight residual matrix $W^{r e s}$ ; $P$ sets of attack   \nconfiguration candidates;   \n2: Output: Adversarially meta-trained LoRAPool;   \n3: Initialize adversarial LoRAPool: $\\phi=\\{\\}$   \n4: for $i=1$ to $P$ (in parallel) do   \n5: Sample the $i$ -th set of $\\epsilon_{i}$ , $\\alpha_{i}$ from attack configuration candidates.   \n6: Initialize the LoRA parameter $A B$ via Eq. (2);   \n7: while not converged do   \n8: Sample a task $\\dot{T}=\\{S,\\mathcal{Q}\\}\\sim\\mathcal{D}_{t r}^{s e e n}$ .   \n9: Generate adversarial query set $\\dot{\\mathcal{Q}_{a d v}}=\\{x_{q}^{a d v},y_{q}\\}_{q=1}^{M}$ qadv, yq}qM=1 with \u03f5i, \u03b1i via Eq. (1)   \n10: // Perturb singular value and vectors   \n11: $\\begin{array}{r l}&{\\_{i}^{\\prime}=\\eta_{1}\\cdot\\frac{1}{M}\\sum_{q=1}^{M}\\nabla_{A}\\mathcal{L}\\big(f_{W^{r e s}+A B}\\big(S,x_{q}^{a d v}\\big),y_{q}\\big)\\big)}\\\\ &{\\delta_{B}=\\eta_{1}\\cdot\\frac{1}{M}\\sum_{q=1}^{M}\\nabla_{B}\\mathcal{L}\\big(f_{W^{r e s}+A B}\\big(S,x_{q}^{a d v}\\big),y_{q}\\big)\\big)}\\\\ &{/\\big/\\operatorname{Update}\\,A B\\mathrm{~via~SGD}}\\\\ &{A\\leftarrow A-\\eta_{2}\\cdot\\frac{1}{M}\\sum_{q=1}^{M}\\nabla_{A}\\mathcal{L}\\big(f_{W^{r e s}+(A+\\delta_{A})B}\\big(S,x_{q}^{a d v}\\big),y_{q}\\big)\\big)}\\\\ &{B\\leftarrow B-\\eta_{2}\\cdot\\frac{1}{M}\\sum_{q=1}^{M}\\nabla_{B}\\mathcal{L}\\big(f_{W^{r e s}+A(B+\\delta_{B})}\\big(S,x_{q}^{a d v}\\big),y_{q}\\big)\\big)}\\\\ &{\\_{\\_{i}\\setminus A}.}\\end{array}$   \n12:   \n13:   \n14:   \n15:   \n16: end while   \n17: \u03d5 = \u03d5 AB   \n18: end for ", "page_idx": 5}, {"type": "text", "text": "To estimate the coefficient of this combination, we propose blending intra-class compactness and inter-class divergence on the support set as the criterion to extract the most discriminative features for classification. To reduce the computational cost of calculating pair-wise similarity between all support samples, we leverage the class prototype to approximate the cluster center of each class and calculate sample-prototype distances. Formally, for the $i$ -th LoRA in the pool and the $c$ -th class out of the total $N$ classes, we denote the class prototype as the average of per-class support features $\\begin{array}{r}{{\\bf p}_{i,c}=\\frac{1}{K}\\sum_{y_{s}=c}{\\bf f}_{W^{r e s}+A_{i}B_{i}}(x_{s})}\\end{array}$ . The intra-class compactness $C_{i}$ and the inter-class divergence $V_{i}$ are then respectively defined as, ", "page_idx": 5}, {"type": "equation", "text": "$$\nC_{i}=\\frac{1}{N K}\\sum_{s=1}^{N K}\\gamma\\left(\\mathbf{f}_{W^{r e s}+A_{i}B_{i}}\\left(x_{s}\\right),\\mathbf{p}_{i,y_{s}}\\right),\\quad V_{i}=\\frac{1}{N K}\\sum_{s=1}^{K}\\sum_{c=1}^{N}\\gamma\\left(\\mathbf{f}_{W^{r e s}+A_{i}B_{i}}\\left(x_{s}\\right),\\mathbf{p}_{i,c}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\gamma(\\cdot,\\cdot)$ denotes the cosine similarity between two feature vectors. After calculating the intraclass compactness and inter-class divergence for each LoRA, the merging coefficient $\\zeta_{i}$ for each LoRA module can be estimated as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\zeta_{i}=\\frac{\\mathrm{Top}_{k}\\left(\\exp\\left(-\\beta(1-(\\lambda C-(1-\\lambda)V)\\right)\\right)_{i}}{\\sum_{i=1}^{k}\\mathrm{Top}_{k}\\left(\\exp\\left(-\\beta(1-(\\lambda C-(1-\\lambda)V)\\right)\\right)_{i}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\beta$ and $\\lambda$ stand for smooth and balance factors, respectively. The operation $\\mathrm{Top}_{k}$ before softmax refers to selecting the top $k$ LoRA modules with the largest score and the rest LoRAs are deactivated for the current task. The merged weight matrix is then calculated as $\\begin{array}{r}{W^{\\prime}=W^{r e s}+\\sum_{i=1}^{P}\\zeta_{i}A_{i}B_{i}}\\end{array}$ To address the issue of interference stemming from redundant components during me rging [99], we introduce singular value trimming, retaining only the largest top- $\\rho\\%$ singular values and resetting the rest to zero to obtain the final task-specific weightW: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{W}=\\operatorname{trim}\\left(W^{\\prime}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This design provides high expressiveness and flexibility by specifying suitable LoRAs for novel tasks, significantly enhancing the model\u2019s adaptation ability to generalize across unseen domains. Algorithm 2 in the Appendix A shows our test-time merging algorithm pipeline. ", "page_idx": 5}, {"type": "text", "text": "Network Inference. After obtaining the task-specific feature extractor through test-time merging, we can employ it directly for inference and perform the nearest-centroid classification [44, 12]. To further improve the few-shot performance in each novel task, our AMT is compatible with other cutting-edge test-time fine-tuning approaches, and thus we introduce a variant AMT-FT, which allows for additional full [12] or efficient [59] fine-tuning. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate the effectiveness of the proposed AMT on three cross-domain few-shot image classification benchmarks in Section 5.1. Additionally, we present ablation studies in Section 5.2, conduct a broader analysis in Section 5.3, and compare our approach with other PEFT methods in Section 5.4. ", "page_idx": 6}, {"type": "text", "text": "Experimental setup. We evaluate AMT using the large-scale cross-domain few-shot classification benchmarks Meta-Dataset [16], BSCD-FSL [29] and fine-grained datasets [30].Note that, in the main experiments, all methods utilize a single model trained on the source domain ImageNet to analyze the trade-offs between robustness and generalization. The details of each benchmark are described in Appendix B.1. And training and evaluation details are included in Appendix B.2. We conduct a comprehensive hyperparameter study in Appendix H. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We adopt the state-of-the-art PMF [12] as the meta-tuning baseline method and use ATTNSCALE [59] as the baseline for an efficient test-time fine-tuning approach. To evaluate our approach against previous adversarial few-shot learning methods, we choose StyleAdv [86] as the representative. All methods employ a Vision Transformer [50] which is DINO-pretrained [1] on ImageNet-1K in our main experiments. ", "page_idx": 6}, {"type": "text", "text": "5.1 Comparison with State-Of-The-Art Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Clean OOD-FSL. In Table 1, we evaluate AMT on Meta-Dataset to investigate its generalization performance on OOD few-shot learning problem in both the 5-way 1-shot and 5-shot settings. We group approaches in two settings. The tuning-free setting does not involve additional training on the support set. We adaptively merge meta-tuned LoRA into pre-trained models via our nonparametric test-time merging mechanism and perform prototype-based classification. Aside from this, the test-time fine-tuning setting allows for training on the support set according to different fine-tuning methods, such as fine-tuning full parameters [12] or partial parameters [59]. Our proposed method AMT consistently achieves superior performance across all domains in the tuning-free setting, up to $12.92\\%$ on Omniglot, compared with previous state-of-the-art methods. Moreover, thanks to the flexible design of LoRAPool and the meta-learned well-generalized initialization for pre-trained models, AMT demonstrates strong compatibility with advanced fine-tuning approaches, further boosting few-shot learning performance, with the improvements of ${\\bf3.92\\%}$ and ${\\bf4.3\\%}$ over PMF [12] and ATTNSCALE [59], respectively. Notably, unlike previous StyleAdv [86], our robust generalization improvement does not sacrifice the in-domain clean accuracy. We attribute this to our adaptive robust LoRAPool design, which completely inherits the pre-trained knowledge and performs customization by injecting discriminative information for unseen tasks. We take a further comparative analysis on BSCD-FSL [29] and fine-grained dataset [30] in Table 2 and under the variable-way-variable-shot setting in Table 18 of Appendix K. The overall performance improvement demonstrates the effectiveness of our method. ", "page_idx": 6}, {"type": "text", "text": "Adversarial OOD-FSL. We evaluate adversarial robustness under distribution shifts for previous state-of-the-art methods using the PGD-10 attack [13] in Table 3. We observe that the naturally trained meta-tuning method PM [12] is not adversarially robust. The style adversarial attack method StyleAdv [86] is also highly vulnerable to adversarial attacks in most domains and sacrifices nearly seven percentage points in-domain performance. In contrast, our method AMT consistently outperforms previous state-of-the-art methods by a wide margin in terms of both in-domain and out-ofdomain robust accuracy, achieving up to $49.72\\%$ on Omniglot. Additionally, our method AMT-FT exhibits synergy with the adversarial test-time fine-tuning strategy, further boosting the in-domain and out-of-domain few-shot adversarial robustness. To take a step further, we measure adversarial robustness against AutoAttack [100] and unseen attacks under distribution shifts in Table 19 and Table 20 of Appendix M, respectively. The results indicate that AMT consistently boosts adversarial generalization across domains. Intriguingly, as shown in Figure 4 of Appendix L, AMT can also handle natural corruptions under distribution shifts. As a whole, AMT improves the trade-offs between adversarial robustness and clean accuracy [68, 63], as well as between ID and OOD generalization [3]. ", "page_idx": 6}, {"type": "text", "text": "5.2 Ablation Study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Component Analysis. In Table 4, we demonstrate the effectiveness of various components in our method: adversarial perturbation on query images and singular values and vectors, robust LoRAPool, test-time merging, and singular value trimming. For the method incorporating adversarial perturbation on query images, we randomly sample the attack budget from attack configuration candidates used in training the robust LoRAPool. We find that this strategy improves OOD generalization but sacrifices in-domain accuracy. For the method without test-time merging, we adaptively determine the suitable LoRA in the pool based on the minimum cross-entropy loss observed in the support set. Relying solely on minimizing the cross-entropy loss on the support set can lead to overfitting, particularly on Omniglot and Traffic Sign, which have a large domain gap relative to the source domain. The degraded overall performance when removing adversarial perturbations on singular values and vectors, regardless of whether we use test-time merging strategy, verifies the role of our double-perturbation mechanism for effective robust generalization enhancement. ", "page_idx": 6}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/5316e101fac59a8e559903bcbb6fcc2688493f9292d9096235a6f33a26f9fef0.jpg", "table_caption": ["Table 1: Few-shot classification clean accuracy $(\\%)$ on Meta-Dataset benchmark [16] in the 5-way 1-shot and 5-shot settings. We report the average accuracy in each domain for all methods. TTF: test-time fine-tuning, Avg.: Average. Bold entries indicate the best for each task configuration. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/c2cb0d707c62f48c58b9d66acb533f5465d94dab2c8fa776c46f61198aa18cf2.jpg", "table_caption": ["Table 2: Few-shot classification clean accuracy $(\\%$ ) on BSCD-FSL [29] and fine-grained datasets [30] in the 5-way 1-shot and 5-shot settings. We report the average accuracy and $95\\%$ confidence interval in each domain for all methods. TTF: test-time fine-tuning. Avg.: Average. Bold entries indicate the best for each task configuration. Rows with \u2020 indicate results from [86]. Other results are based on our implementations. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Effectiveness of Adversarial Perturbation on Singular Value and Vectors. To demonstrate the beneftis of our adversarial attack strategy, we further compare AMT against the variant removing the adversarial perturbations on singular values and vectors, as shown in Figure 2. We find that our AMT can significantly amplify the magnitude of the top singular values for FFN layers (see Figure 3 in Appendix D for MHA layers). We argue that the double adversarial perturbation explicitly forces the model to focus more on the most critical components, thereby enhancing its resilience against worst-case scenarios during meta-tuning. Therefore, this improves the model\u2019s robust generalization capability, allowing it to adapt more effectively to novel downstream tasks across diverse domains. ", "page_idx": 7}, {"type": "text", "text": "Different Pool Designs and Adversarial Perturbation Strategies. As shown in Table 8 of Appendix G, the proposed robust LoRAPool with perturbation-specific parameters effectively avoids interference between different attacks and significantly enhances the out-of-domain generalization without in-domain compromise. Furthermore, we compare with SAM [96] and the original LoRA initialization [25] in Table 9 and Table 10 of Appendix G, where the superior performance validates the efficacy of our adversarial singular value and vector perturbations in boosting the model\u2019s generalization capability. See Appendix G for more details. Moreover, in Table 16 of Appendix I, we observe that the simple pixel-level adversarial attacks can effectively simulate larger domain shifts than static data augmentation [101] and achieve comparable or superior generalization improvements compared to the learnable adversarial transformation method [102]. ", "page_idx": 7}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/dcdf7c7674d6b3596424a986320c0fe71a579eafb3f6e2f1a601e11b1e3e542b.jpg", "table_caption": ["Table 3: Few-shot classification adversarial robust accuracy on Meta-Dataset in the 5-way 1-shot and 5-shot settings. Adv. TTF: adversarial test-time fine-tuning. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/edb435657401993aba7c2caa02e47b00b757cbe81354395b273ba5e03da95bd5.jpg", "table_caption": ["Table 4: Component ablation studies on Meta-Dataset in the 5-way 1-shot setting. APQ: adversarial perturbation on query set, APSV: adversarial perturbation on singular values and vectors, RLP: Robust LoRAPool, TTM: test-time merging, STr: singular value trimming. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.3 More Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Alternative Test-time Merging Strategies. Before finalizing our test-time merging mechanism, we experimented with various design choices. The first idea involves employing a parametric linear classifier to evaluate the compatibility of LoRAs with novel tasks, similar to FLUTE [33]. To train the classifier, we input a batch of adversarial samples, each generated by attacking a different robust LoRA within the pool, to estimate which LoRA generated it. The classifier\u2019s mean output on the support set serves as the merging coefficients for a novel task. Additionally, we explored simply averaging LoRA weights or logits, similar to model soups [103]. Appendix E Table 6 compares these alternative strategies using the robust LoRAPool. We see that our AMT, with the introduced intra-class compactness and inter-class divergence criteria, achieves superior overall generalization. In contrast, the linear classifier may not accurately indicate the robustness level of adversarial perturbations based on semantic characteristics. Though logit averaging demonstrates comparable performance, it requires storing all LoRA parameters for extracting query features on each task. Our method merges the LoRAPool into the pre-trained model for adaptation on the support set, maintaining the same amount of parameters for query feature extraction as the baselines [12, 59]. ", "page_idx": 8}, {"type": "text", "text": "Compatibility with Other Pre-training Methods. We evaluate the effectiveness of our AMT across different pre-training regimes on the Meta-Dataset. Previous state-of-the-art methods [12, 58, 59] employ DINO [1] pre-training on ImageNet, which utilizes the class token for self-distillation learning. We choose iBOT [2] as the representative approach using patch reconstruction as a proxy task for self-supervised pre-training, DeIT [104] for supervised pre-training with strong regularizations and AdvPre [28] for adversarial pre-training. As shown in Table 7 of Appendix F, AMT achieves average performance improvements of $5.97\\%$ , $4.58\\%$ , $6.41\\%$ and $6.36\\%$ over DINO, iBOT, DeIT and AdvPre, respectively, demonstrating its effectiveness across supervised, self-supervised and robust pre-training methods. Intriguingly, we find that AMT significantly enhances the compromised in-domain clean accuracy for the adversarially robust model [28], even outperforming clean pre-trained models. ", "page_idx": 8}, {"type": "image", "img_path": "HxGdbAmYYr/tmp/990d8add9720a3c46da03ba33277bddb5b6fd0161dcc1936bd85c63d1ff2b294.jpg", "img_caption": ["Figure 2: Effectiveness of the adversarial perturbation on singular values and vectors. The accuracy on Meta-Dataset in the 5-way 1-shot is reported. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5.4 Comparison with Other Parameter-Efficient Fine-Tuning Methods ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We compare AMT with other parameter-efficient fine-tuning methods in Table 17 of Appendix J. We observe that single Adapter-based and LoRA-based methods achieve comparable performance in adversarial meta-tuning and outperform full-model and FiLM-based meta-tuning. Besides, the superiority of the FiLM/Adapter Pool over the FiLM/Adapter signifies that our adversarial pool design contributes to the OOD performance without compromising in-domain accuracy. Also, our approach, which incorporates additional perturbation in singular values/vectors and non-parametric test-time merging mechanism utilizing the criteria (i.e., Algorithm 2) that adaptively integrates the LoRAPool into pre-trained weights, enjoys significant performance improvement over FiLM/Adapter Pool. Moreover, unlike the FLUTE-style test-time fine-tuning strategy that requires further tuning of pool components, our framework shows better compatibility with different test-time fine-tuning approaches, including LoRA tuning, full fine-tuning [12], and attention scaling [59]. More details are included in Appendix J. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces AMT employing adversarial meta-tuning to augment the robust generalization for pre-trained vision transformers. Upon generated adversarial query images at various robustness levels, we perturb the singular values and vectors to explicitly reinforce the principal components and maintain a robust LoRAPool containing perturbation-specific low-rank updates. The discriminative meta-updated components in the pool are adaptively selected and merged for customizing the model to adapt to novel tasks through a non-parametric test-time merging mechanism. Extensive experiments have demonstrated that AMT with substantial improvements in robust generalization sets new benchmarks in out-of-domain few-shot image classification tasks. Our analysis also contributes to the deeper understanding of adversarial training advancement in the few-shot setting. ", "page_idx": 9}, {"type": "text", "text": "Although LoRAPool has demonstrated effectiveness across different datasets and tasks, one limitation is the need for manual setting of the adversarial budget, particularly the size $\\epsilon$ , for each module. Furthermore, our exploration has been limited to adversarial budgets containing $l_{\\infty}$ bounded perturbations, potentially restricting the ability of our method to model various types of distributional shifts. In our future work, we aim to address these limitations by expanding our exploration to include different types of adversarial perturbations and enhancing the adaptability of our method based on the specific dataset used in meta-tuning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by City University of Hong Kong Project No. 9229130 and No. 9610614. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9650\u20139660, 2021. [2] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. International Conference on Learning Representations, 2022. [3] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763, 2021. [4] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [5] Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Yu Qiao, Peng Gao, and Hongsheng Li. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15211\u201315222, 2023. [6] Kun Song, Huimin Ma, Bochao Zou, Huishuai Zhang, and Weiran Huang. Fd-align: Feature discrimination alignment for fine-tuning pre-trained models in few-shot learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[7] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprint arXiv:2111.03930, 2021.   \n[8] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16816\u201316825, 2022. [9] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision (IJCV), 2022.   \n[10] Xiangyang Zhu, Renrui Zhang, Bowei He, Aojun Zhou, Dong Wang, Bin Zhao, and Peng Gao. Not all features matter: Enhancing few-shot clip with adaptive prior refinement. arXiv preprint arXiv:2304.01195, 2023.   \n[11] Markus Hiller, Rongkai Ma, Mehrtash Harandi, and Tom Drummond. Rethinking generalization in few-shot classification. arXiv preprint arXiv:2206.07267, 2022.   \n[12] Shell Xu Hu, Da Li, Jan St\u00fchmer, Minyoung Kim, and Timothy M Hospedales. Pushing the limits of simple pipelines for few-shot learning: External data and fine-tuning make a difference. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9068\u20139077, 2022.   \n[13] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.   \n[14] Micah Goldblum, Liam Fowl, and Tom Goldstein. Adversarially robust few-shot learning: A meta-learning approach. Advances in Neural Information Processing Systems, 33:17886\u2013 17895, 2020.   \n[15] Ren Wang, Kaidi Xu, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Chuang Gan, and Meng Wang. On fast adversarial robustness adaptation in model-agnostic meta-learning. arXiv preprint arXiv:2102.10454, 2021.   \n[16] Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, et al. Meta-dataset: A dataset of datasets for learning to learn from few examples. In International Conference on Learning Representations, 2020.   \n[17] Luisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast context adaptation via meta-learning. In International Conference on Machine Learning, pages 7693\u20137702, 2019.   \n[18] John Cai, Bill Cai, and Shen Sheng Mei. Damsl: Domain agnostic meta score-based learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2591\u20132595, 2021.   \n[19] Till J Bungert, Levin Kobelke, and Paul F Jaeger. Understanding silent failures in medical image classification. In International Conference on Medical Image Computing and ComputerAssisted Intervention, pages 400\u2013410. Springer, 2023.   \n[20] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3354\u20133361, 2012.   \n[21] Fu Wang, Zeyu Fu, Yanghao Zhang, and Wenjie Ruan. Self-adaptive adversarial training for robust medical segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 725\u2013735. Springer, 2023.   \n[22] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning visual classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1625\u20131634, 2018.   \n[23] Qingzhao Zhang, Shengtuo Hu, Jiachen Sun, Qi Alfred Chen, and Z Morley Mao. On adversarial robustness of trajectory prediction for autonomous vehicles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15159\u201315168, 2022.   \n[24] Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning? Advances in Neural Information Processing Systems, 33:512\u2013523, 2020.   \n[25] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[26] Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models. arXiv preprint arXiv:2404.02948, 2024.   \n[27] Florian Tramer and Dan Boneh. Adversarial training and robustness for multiple perturbations. Advances in Neural Information Processing Systems, 32, 2019.   \n[28] Naman Deep Singh, Francesco Croce, and Matthias Hein. Revisiting adversarial training for imagenet: Architectures, training and generalization across threat models. Advances in Neural Information Processing Systems, 36, 2024.   \n[29] Yunhui Guo, Noel C Codella, Leonid Karlinsky, James V Codella, John R Smith, Kate Saenko, Tajana Rosing, and Rogerio Feris. A broader study of cross-domain few-shot learning. In European Conference on Computer Vision, pages 124\u2013141, 2020.   \n[30] Hung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang, and Ming-Hsuan Yang. Cross-domain fewshot classification via learned feature-wise transformation. In International Conference on Learning Representations, 2020.   \n[31] Nikita Dvornik, Cordelia Schmid, and Julien Mairal. Selecting relevant features from a multi-domain representation for few-shot classification. In European Conference on Computer Vision, pages 769\u2013786, 2020.   \n[32] Lu Liu, William Hamilton, Guodong Long, Jing Jiang, and Hugo Larochelle. A universal representation transformer layer for few-shot image classification. arXiv preprint arXiv:2006.11702, 2020.   \n[33] Eleni Triantafillou, Hugo Larochelle, Richard Zemel, and Vincent Dumoulin. Learning a universal template for few-shot dataset generalization. In International Conference on Machine Learning, pages 10424\u201310433, 2021.   \n[34] Wei-Hong Li, Xialei Liu, and Hakan Bilen. Cross-domain few-shot learning with task-specific adapters. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7161\u20137170, 2022.   \n[35] Yanbin Liu, Juho Lee, Linchao Zhu, Ling Chen, Humphrey Shi, and Yi Yang. A multimode modulator for multi-domain few-shot classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8453\u20138462, 2021.   \n[36] Xu Luo, Jing Xu, and Zenglin Xu. Channel importance matters in few-shot image classification. In International Conference on Machine Learning, pages 14542\u201314559, 2022.   \n[37] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, pages 1126\u20131135, 2017.   \n[38] Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv preprint arXiv:1803.02999, 2018.   \n[39] Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse? towards understanding the effectiveness of maml. arXiv preprint arXiv:1909.09157, 2019.   \n[40] Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017.   \n[41] Eunbyung Park and Junier B Oliva. Meta-curvature. Advances in Neural Information Processing Systems, 32, 2019.   \n[42] Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your maml. arXiv preprint arXiv:1810.09502, 2018.   \n[43] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. Advances in Neural Information Processing Systems, 29, 2016.   \n[44] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in Neural Information Processing Systems, 30, 2017.   \n[45] Boris Oreshkin, Pau Rodr\u00edguez L\u00f3pez, and Alexandre Lacoste. Tadam: Task dependent adaptive metric for improved few-shot learning. Advances in Neural Information Processing Systems, 31, 2018.   \n[46] Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Cross attention network for few-shot classification. Advances in Neural Information Processing Systems, 32, 2019.   \n[47] Carl Doersch, Ankush Gupta, and Andrew Zisserman. Crosstransformers: spatially-aware few-shot transfer. Advances in Neural Information Processing Systems, 33:21981\u201321993, 2020.   \n[48] James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, and Richard E Turner. Fast and flexible multi-task classification using conditional neural adaptive processes. Advances in Neural Information Processing Systems, 32, 2019.   \n[49] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence, 2018.   \n[50] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[51] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10012\u201310022, 2021.   \n[52] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxvit: Multi-axis vision transformer. arXiv preprint arXiv:2204.01697, 2022.   \n[53] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000\u201316009, 2022.   \n[54] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEit: BERT pre-training of image transformers. In International Conference on Learning Representations, 2022.   \n[55] Yangji He, Weihan Liang, Dongyang Zhao, Hong-Yu Zhou, Weifeng Ge, Yizhou Yu, and Wenqiang Zhang. Attribute surrogates learning and spectral tokens pooling in transformers for few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9119\u20139129, 2022.   \n[56] Bowen Dong, Pan Zhou, Shuicheng Yan, and Wangmeng Zuo. Self-promoted supervision for few-shot transformer. In European Conference on Computer Vision, pages 329\u2013347, 2022.   \n[57] Han Lin, Guangxing Han, Jiawei Ma, Shiyuan Huang, Xudong Lin, and Shih-Fu Chang. Supervised masked knowledge distillation for few-shot transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19649\u201319659, 2023.   \n[58] Chengming Xu, Siqian Yang, Yabiao Wang, Zhanxiong Wang, Yanwei Fu, and Xiangyang Xue. Exploring efficient few-shot adaptation for vision transformers. arXiv preprint arXiv:2301.02419, 2023.   \n[59] Samyadeep Basu, Shell Hu, Daniela Massiceti, and Soheil Feizi. Strong baselines for parameter-efficient few-shot fine-tuning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 11024\u201311031, 2024.   \n[60] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.   \n[61] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.   \n[62] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236, 2016.   \n[63] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In International Conference on Machine Learning, pages 7472\u20137482, 2019.   \n[64] Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adversarial robustness requires revisiting misclassified examples. In International conference on learning representations, 2019.   \n[65] Tianyu Pang, Min Lin, Xiao Yang, Jun Zhu, and Shuicheng Yan. Robustness and accuracy could be reconcilable by (proper) definition. In International Conference on Machine Learning, pages 17258\u201317277, 2022.   \n[66] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. Advances in Neural Information Processing Systems, 33:2958\u20132969, 2020.   \n[67] Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust generalization requires more data. Advances in Neural Information Processing Systems, 31, 2018.   \n[68] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018.   \n[69] Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understanding and mitigating the tradeoff between robustness and accuracy. arXiv preprint arXiv:2002.10716, 2020.   \n[70] Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L Yuille, and Quoc V Le. Adversarial examples improve image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 819\u2013828, 2020.   \n[71] Sylvestre-Alvise Rebuff,i Francesco Croce, and Sven Gowal. Revisiting adapters with adversarial training. arXiv preprint arXiv:2210.04886, 2022.   \n[72] Ruiyi Zhang, Rushi Qiang, Sai Ashish Somayajula, and Pengtao Xie. Autolora: Automatically tuning matrix ranks in low-rank adaptation based on meta learning. arXiv preprint arXiv:2403.09113, 2024.   \n[73] Sanchari Sen, Balaraman Ravindran, and Anand Raghunathan. Empir: Ensembles of mixed precision deep networks for increased robustness against adversarial attacks. arXiv preprint arXiv:2004.10162, 2020.   \n[74] Charles Herrmann, Kyle Sargent, Lu Jiang, Ramin Zabih, Huiwen Chang, Ce Liu, Dilip Krishnan, and Deqing Sun. Pyramid adversarial training improves vit performance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13419\u201313429, 2022.   \n[75] Kaijie Zhu, Xixu Hu, Jindong Wang, Xing Xie, and Ge Yang. Improving generalization of adversarial training via robust critical fine-tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4424\u20134434, 2023.   \n[76] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. Advances in Neural Information Processing Systems, 31, 2018.   \n[77] Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do adversarially robust imagenet models transfer better? Advances in Neural Information Processing Systems, 33:3533\u20133545, 2020.   \n[78] Zhun Deng, Linjun Zhang, Kailas Vodrahalli, Kenji Kawaguchi, and James Y Zou. Adversarial training helps transfer learning via better representations. Advances in Neural Information Processing Systems, 34:25179\u201325191, 2021.   \n[79] Mingyang Yi, Lu Hou, Jiacheng Sun, Lifeng Shang, Xin Jiang, Qun Liu, and Zhiming Ma. Improved ood generalization via adversarial training and pretraing. In International Conference on Machine Learning, pages 11987\u201311997, 2021.   \n[80] Shiji Xin, Yifei Wang, Jingtong Su, and Yisen Wang. On the connection between invariant learning and adversarial training for out-of-distribution generalization. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 10519\u201310527, 2023.   \n[81] Qixun Wang, Yifei Wang, Hong Zhu, and Yisen Wang. Improving out-of-distribution generalization by adversarial training with structured priors. Advances in Neural Information Processing Systems, 35:27140\u201327152, 2022.   \n[82] Chengxiang Yin, Jian Tang, Zhiyuan Xu, and Yanzhi Wang. Adversarial meta-learning. arXiv preprint arXiv:1806.03316, 2018.   \n[83] Junhao Dong, Yuan Wang, Jian-Huang Lai, and Xiaohua Xie. Improving adversarially robust few-shot image classification with generalizable representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9025\u20139034, 2022.   \n[84] Haoqing Wang and Zhi-Hong Deng. Cross-domain few-shot classification via adversarial task augmentation. arXiv preprint, 2021.   \n[85] Yanxu Hu and Andy J Ma. Adversarial feature augmentation for cross-domain few-shot classification. In European Conference on Computer Vision, 2022.   \n[86] Yuqian Fu, Yu Xie, Yanwei Fu, and Yu-Gang Jiang. Styleadv: Meta style adversarial training for cross-domain few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24575\u201324584, 2023.   \n[87] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1501\u20131510, 2017.   \n[88] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582\u20134597, 2021.   \n[89] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pages 709\u2013727, 2022.   \n[90] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790\u20132799, 2019.   \n[91] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition. Advances in Neural Information Processing Systems, 35:16664\u201316678, 2022.   \n[92] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations, 2022.   \n[93] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation. arXiv preprint arXiv:2210.07558, 2022.   \n[94] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In International Conference on Learning Representations, 2023.   \n[95] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950\u20131965, 2022.   \n[96] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020.   \n[97] J Andrew Bagnell. Robust supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 714\u2013719, 2005.   \n[98] Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervised learning give robust classifiers? In International Conference on Machine Learning, pages 2029\u20132037, 2018.   \n[99] Prateek Yadav, Derek Tam, Leshem Choshen, Colin A Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems, 36, 2024.   \n[100] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International Conference on Machine Learning, pages 2206\u20132216, 2020.   \n[101] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. International Conference on Learning Representations, 2021.   \n[102] Tejas Gokhale, Rushil Anirudh, Jayaraman J Thiagarajan, Bhavya Kailkhura, Chitta Baral, and Yezhou Yang. Improving diversity with adversarially learned transformations for domain generalization. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2023.   \n[103] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael GontijoLopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning, pages 23965\u201323998, 2022.   \n[104] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347\u201310357, 2021.   \n[105] Jaehoon Oh, Sungnyun Kim, Namgyu Ho, Jin-Hwa Kim, Hwanjun Song, and Se-Young Yun. Understanding cross-domain few-shot learning: An experimental study. arXiv preprint arXiv:2202.01339, 2022.   \n[106] Panagiotis Eustratiadis, \u0141ukasz Dudziak, Da Li, and Timothy Hospedales. Neural fine-tuning search for few-shot learning. arXiv preprint arXiv:2306.09295, 2023.   \n[107] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Mixture of Adversarial LoRAs: Boosting Robust Generalization in ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Meta-tuning -Supplementary Material", "page_idx": 17}, {"type": "text", "text": "A Algorithm of Test-time merging ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The complete algorithm of our test-time merging mechanism is presented in Algorithm 2. ", "page_idx": 17}, {"type": "text", "text": "Algorithm 2 Test-Time Merging   \n1: Iandapputti:v eS ruopbpuosrtt  sLeot RofA Pmoeotal- $\\boldsymbol{S}=\\{x_{i}^{s},y_{i}^{s}\\}_{i=1}^{N K}$ , pre-trained residual weight matrix $W^{r e s}$ ,   \n$\\phi=[A_{1}B_{1},\\ldots,A_{P}B_{P}]$   \n2: for $i=1,\\dots,P$ (in parallel) do   \n3: // Calculate the intra-class compactness   \n4: $\\begin{array}{r}{\\dot{C}_{i}=\\frac{1}{N K}\\sum_{s=1}^{N K}\\gamma\\left(\\mathbf{f}_{W^{r e s}+A_{i}B_{i}}\\left(x_{s}\\right),\\mathbf{p}_{y_{s}}\\right)}\\end{array}$   \n5: // Calculate the inter-class divergence   \n6: $\\begin{array}{r}{\\stackrel{\\triangledown}{V_{i}}=\\frac{1}{N K}\\sum_{s=1}^{K}\\sum_{c\\=1}^{N}\\underset{\\b{c}\\neq\\boldsymbol{y}_{s}}{\\sum}\\gamma\\left(\\mathbf{f}_{W^{r e s}}\\overline{{\\cdot}}\\mathbf{\\bar{\\alpha}}_{i B_{i}}\\left(x_{s}\\right),\\mathbf{p}_{c}\\right)}\\end{array}$   \n8: \u03b6i = $\\begin{array}{r l}&{\\zeta_{i}=\\frac{\\mathrm{Top}_{k}(\\exp(-\\beta(1-(\\lambda C-(1-\\lambda)V)))_{i}}{\\sum_{i=1}^{k}\\mathrm{Top}_{k}(\\exp(-\\beta(1-(\\lambda C-(1-\\lambda)V)))_{i}}}\\end{array}$   \n9: $\\begin{array}{r}{W^{\\prime}=W_{\\mathrm{res}}\\,+\\sum_{i=1}^{P}\\zeta_{i}A_{i}B_{i}}\\end{array}$   \n10: // Singular Va lue Trimming   \n11: $\\widehat{W}=\\operatorname{trim}\\left(W^{\\prime}\\right)$ ", "page_idx": 17}, {"type": "text", "text": "B Setup for Cross-Domain Few-Shot Evaluation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Datasets Used for Benchmarks ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Meta-Dataset [16] is a more challenging and realistic large-scale benchmark consisting of ten image datasets including ImageNet, Omniglot, Aircraft, CUB, DTD, QuickDraw, Fungi, VGG Flower, Traffic Signs, and MSCOCO, each with specified training, validation and test splits. In this paper, we utilize the ImageNet training split as the single source domain for meta-training while employing the test splits of all datasets for meta-testing. We refer to [16] for an in-depth exploration of Meta-Dataset. ", "page_idx": 17}, {"type": "text", "text": "BSCD-FSL [29] consists of four datasets from different domains: CropDisease, EuroSAT, ISIC, and ChestX, covering plant disease images, satellite pictures, human skin lesions, and X-Ray images. We follow [29] for the dataset split. ", "page_idx": 17}, {"type": "text", "text": "Fine-Grained Datasets [30] includes four additional commonly used datasets in CD-FSL: CUB, Car, Plantae, and Places, which contain birds, cars, plant and scene images and fine-grained classes. We follow the splitting procedure of previous methods [30, 86]. We refer to [30, 105] for a more detailed description of each dataset. ", "page_idx": 17}, {"type": "text", "text": "B.2 Implementation Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We follow the pipeline delineated by PMF [12] and use the same DINO pre-training checkpoint [1] for our AMT and all baselines in main experiments. We perform adversarial meta-tuning on the ImageNet training split following the Meta-Dataset protocal [16]. The SGD optimizer with a momentum of 0.9 and a cosine-decayed learning rate $\\eta_{2}$ starting at $5\\times10^{-4}$ are adopted. Training is conducted for a maximum of 30 epochs, with a 5-epoch warming-up stage. The loss trade-off coefficient $\\lambda_{a d v}$ is set to 6. The input image size is $128\\times128$ as per PMF [12]. The pre-trained model is kept frozen while each LoRA is meta-updated to construct the robust LoRAPool. We use a pool of size $P=4$ and a LoRA rank of $r=8$ , choosing the top 2 from the pool for merging at test time. Following the state-of-the-art method PMF [12], we sample five tasks from each domain as the validation set for hyperparameter selection. The adversarial query set is generated using untargeted weak and strong patch perturbations [74] with $l_{\\infty}$ -bounded budgets $\\epsilon\\in\\{0.01/255,0.1/255,6/255,8/255\\}$ in 2 steps, and a step size of $\\alpha\\,\\in\\,\\left\\{\\frac{\\epsilon}{2},\\frac{\\epsilon}{10}\\right\\}$ . The size of the neighborhood $\\eta_{1}$ is set to $1e-4$ for adversarial perturbation on singular values and vectors. We search domain-wise hyperparameters on the validation set, including $\\lambda$ in the range of $[0,1]$ , $\\beta$ in the range of [1, 12], and $\\rho$ in the range of $[0,1]$ . The experiments were conducted on one NVIDIA A6000 GPU. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "B.3 Evaluation Metric ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Clean Few-shot Classification Accuracy. We compute the average accuracy on the query set across 600 randomly sampled few-shot classification tasks from the test set of each dataset on three benchmarks. ", "page_idx": 18}, {"type": "text", "text": "Adversarial Few-shot Classification Accuracy. To evaluate adversarial robustness, we calculate the adversarial accuracy of the query set over 600 few-shot classification tasks. For each task, we generate adversarial examples by employing the PGD-10 attack with $l_{\\infty}$ -bounded budgets $\\epsilon=4.5/255$ and a step size $\\begin{array}{r}{\\alpha=\\frac{\\epsilon}{10}}\\end{array}$ on clean images. ", "page_idx": 18}, {"type": "text", "text": "C Mathimatical Symbols ", "text_level": 1, "page_idx": 18}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/8121be7a0eba536b00408ada5dffb279feb48936f7b180819318301dfa417589.jpg", "table_caption": ["Table 5: Meaning of Math Symbols and First Appearance "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D More Analysis on Changes in Singular Values ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Figure 3 shows the change in top singular values of the projection weight matrix across multi-head self-attention layers. It can be observed that our adversarial double-perturbation strategy can help the model learn to strengthen its principal components to defend against the strongest attacks during meta-tuning and thus improve generalization and robustness. ", "page_idx": 19}, {"type": "image", "img_path": "HxGdbAmYYr/tmp/e6c6bc7482d4b075e6d7b3a6ca809c6981c729a338626920792556a11356ef7e.jpg", "img_caption": ["Figure 3: Changes in top singular values of MHA across layers "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Results of Alternative Test-time Merging Strategies ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We compare alternative test-time merging strategies using the robust LoRAPool in Table 6. We find that our method outperforms other alternative approaches. ", "page_idx": 19}, {"type": "text", "text": "Table 6: Comparison of AMT with the alternative merging strategies on Meta-Dataset in the 5-way 1-shot setting. ", "page_idx": 19}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/249c32dd50d772cd8a68c7902ac118da04ebe0db3f2c2c84defce2e2f3c9f58d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "F Results With Other Pre-traing Methods ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We evaluate the effectiveness of our AMT for other pre-training regimes on Meta-Dataset in Table 7. The results show that AMT achieves consistent performance improvements for various supervised, self-supervised and robust pre-training methods. ", "page_idx": 19}, {"type": "text", "text": "G More Ablation Studies of AMT ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our AMT equips a pre-trained model with a pool of adversarially meta-tuned LoRAs at varying levels of adversarial perturbation to boost the robust generalization of pre-trained models in out-of-domain few-shot learning. Thus, we perform more ablation studies on LoRAPool design and adversarial singular value and vector perturbations. ", "page_idx": 19}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/c88b9dfa455144941d4e8ba367a82d7ed0abb10a01e6588a4eedea1cf3cdcd4d.jpg", "table_caption": ["Table 7: The compatibility of AMT with other pre-training methods on Meta-Dataset in the 5-way 1-shot setting. All methods employ the ViT-Small architecture. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Different designs of robust LoRAPool. We first adopt the uniform strategy to use the average attack strength $\\epsilon=3.5)$ ) of AMT\u2019s candidate configurations. Also, we develop a variant, coined random strategy, to randomly sample one attack budget $\\epsilon$ for each training task from the same attack pool of candidate configurations. For a fair comparison with AMT, we meta-tune 4 LoRAs with different seeds for both the uniform and random strategies. The results are given in Table 8. We notice that the proposed robust LoRAPool with perturbation-specific parameters effectively avoids interference between different attacks and significantly enhances the out-of-domain generalization without in-domain compromise. ", "page_idx": 20}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/6c19a13f6a49a638c9cde979ddac9f315630fb2c63683877a9ded978a185b8c9.jpg", "table_caption": ["Table 8: The influence of attack pool strategy on Meta-Dataset in the 5-way 1-shot setting. Bold entries indicate the best for each task dataset. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Different perturbation strategies. Unlike SAM [96], which employs clean examples and perturbs the weight matrices, our method applies adversarial perturbation in the spectral space, specifically targeting singular values and vectors. The results, reported in Table 9, show that AMT outperforms SAM by an average of $1.56\\%$ , substantiating the effectiveness of our adversarial perturbation strategy. We additionally compare AMT initialization against the original LoRA initialization, for which we introduce adversarial perturbations in the weight space. The superior performance, as shown in Table 10, further validates that the efficacy of our adversarial singular value and vector perturbations in boosting the model\u2019s generalization capability. ", "page_idx": 20}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/49ccd94d56f3fca8d2ffe4b354796549322bbc711c9b80ecd20ae277411a42ee.jpg", "table_caption": ["Table 9: Comparison with SAM [96] for the perturbation input and space on Meta-Dataset in the 5-way 1-shot setting. Bold entries indicate the best for each task dataset. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/027771b413ea39b1ba2212258adce3807477b05a2fb2c1bdace243dcc173f9c2.jpg", "table_caption": ["Table 10: The influence of adversarial perturbation space on Meta-Dataset in the 5-way 1-shot setting. Bold entries indicate the best for each task dataset. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 11: The influence of loss trade-off coefficient $\\lambda_{a d v}$ on Meta-Dataset in the 5-way 1-shot setting. Bold entries indicate the best for each task dataset. \u22c6denotes our choice. ", "page_idx": 21}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/adf122820d2dbb3bf076653f8d9f3ea1632a913db97c0f525a324d9ace77eab5.jpg", "table_caption": ["(a) Clean Few-shot Accuracy "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/051123769daa4b67c37732dad75a0f4c3e434b35556b4bf38b10332e91e0775b.jpg", "table_caption": ["(b) Adversarial Few-shot Accuracy "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "H Hyper-parameter Studies of AMT ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The robust LoRAPool in AMT provides the flexibility to adjust the trade-offs between adversarial robustness and clean accuracy by modifying the pool components. In Table 11, we conduct additional experiments to highlight this benefti, using different values of $\\lambda_{a d v}$ . The results reveal that $\\lambda_{a d v}$ can be used to tune LoRAPool\u2019s preference towards either clean or adversarial environments. ", "page_idx": 21}, {"type": "text", "text": "To analyze the impact of key hyper-parameters, we conduct experiments with various hyper-parameter values, yielding several noteworthy observations. The results in Table 12 and 13 suggest that our model is relatively insensitive to the rank of LoRA and the number of attack steps. Also, the results in Table 15 justify our choice of top-2. Furthermore, as shown in Table 14, varying the pool size $P$ and the mean and variance statistics of perturbation budget candidates $\\epsilon$ demonstrates that a sufficiently diverse but large pool improves performance. ", "page_idx": 21}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/fcbdbabcdf9c3d39267780444a94048a3ad0946804cfa0d4123302e9fc5dec41.jpg", "table_caption": ["Table 12: The influence of LoRA rank $r$ on Meta-Dataset in the 5-way 1-shot setting. Bold entries indicate the best for each task dataset. \u22c6denotes our choice. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "I Comparison with Other Data Augmentation Techniques ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Our AMT constructs the robust LoRAPool with adaptive test-time merging to boost the robust generalization of pre-trained vision transformers in out-of-domain few-shot learning. In this context, we use adversarial attacks, characterized by the size of the perturbation budget, to mimic different distributional shifts and meta-tune diverse LoRAs. The experiments in the main paper demonstrate the effectiveness of using adversarial training. In this section, we compare AMT with other data augmentation methods using a single LoRA $\\;P=1\\;\\;$ ) for meta-tuning. Specifically, for ALT [102], we employ a learnable adversarial transformation network consisting of 5 convolutional layers with a kernel size of 3 and LeakyReLU activation. The adversarial learning rate was set to $5\\times10^{\\bar{-5}}$ , with 10 adversarial steps. For the method leveraging an attack candidate pool, we randomly select the attack budget from candidates for each training task, with $\\epsilon$ values of 8/255, 6/255, 0.1/255, 0.01/255 for AMT, and step number of 1, 3, 5, 10 for ALT. As shown in Table 16, static data augmentation [101] cannot effectively simulate the large domain shifts required for robust generalization across diverse datasets (e.g., Omniglot). Compared to ALT, our AMT, utilizing only 2 steps of standard pixel-level adversarial attacks, achieves comparable or superior improvements in generalization for pre-trained vision transformers on OOD tasks. ", "page_idx": 21}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/b75840e6a30e1008b5f8973f00e101627c977cfd83c6acb9dd76c79703ee6a91.jpg", "table_caption": ["Table 13: The influence of the number of attack steps on Meta-Dataset in the 5-way 1-shot setting. Bold entries indicate the best for each task dataset. \u22c6denotes our choice. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/8b909b66160103a5d29a31989254ca28e6cee04769ed8cba385fc49810702420.jpg", "table_caption": ["Table 14: The influence of the pool size $P$ and diversity on Meta-Dataset in the 5-way 1-shot setting. Bold entries indicate the best for each task dataset. \u22c6denotes our choice. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/14a6b5e44e32e69fddd5135c1e3e584ac9bc6fe1ed7985204540e9f43b181b57.jpg", "table_caption": ["Table 15: The influence of top- $k$ on Meta-Dataset in the 5-way 1-shot setting. Bold entries indicate the best for each task dataset. $\\star$ denotes our choice. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/bc3d2632da18faca70c9c2bcdd6eaa899de11a51eb4a1122097db277feb6719c.jpg", "table_caption": ["Table 16: Comparison with other data augmentation methods on Meta-Dataset in the 5-way 1-shot setting. Single LoRA ( $P=1$ ) is used for all methods. Bold entries indicate the best for each task dataset. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "J Comparison with Other Parameter-Efficient Fine-Tuning Methods ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we compare AMT with other parameter-efficient fine-tuning methods. For FiLM [49], we implement it after LN layers since there are no BN layers in ViT. We also compare Adapter [91], using the default bottleneck size of 64. The attack budget $\\epsilon$ is randomly sampled from the same candidate pool as AMT for each training task. As shown in Table 17, single Adapter-based and LoRA-based methods achieve comparable performance in adversarial meta-tuning and outperform full and FiLM-based meta-tuning. Regarding the FiLM pool [49] and Adapter pool [91], we conduct additional experiments by setting the pool size to 4 and adopting the same attack pool strategy used in AMT during adversarial meta-tuning. To estimate the combination coefficients, we follow the method outlined in FLUTE [32]. Specifically, a classifier is trained in a separate stage to predict which FiLM or Adapter the input belongs to, taking as input a batch of adversarial examples generated by attacking different FiLMs or Adapters in the pool. Results show that: 1) The superiority of the FiLM/Adapter Pool over FiLM/Adapter signifies that our adversarial pool design contributes to the out-of-distribution performance without compromising in-domain accuracy. 2) Our approach, which incorporates additional perturbation in singular values/vectors and non-parametric test-time merging mechanism utilizing the criteria (i.e., Algorithm 2) that adaptively integrates the LoRAPool into pre-trained weights, enjoys significant performance improvement over FiLM/Adapter Pool. 3) Unlike the FLUTE-style test-time fine-tuning strategy that requires further tuning of pool components (either a FiLM or an adapter), our framework shows better compatibility with different test-time fine-tuning approaches, including LoRA tuning, full fine-tuning [12], and attention scaling [59]. ", "page_idx": 23}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/d48d1ebad5225a3e195b1837d6972cfcb3211e178b006f35138ab92e14c26a42.jpg", "table_caption": ["Table 17: Comparison with other parameter-efficient fine-tuning methods on Meta-Dataset in the 5-way 1-shot setting. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "K Comparison with the SOTAs in Other Settings on Meta-Dataset ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we demonstrate the effectiveness of the proposed AMT under the variable-wayvariable-shot setting on Meta-Dataset [16]. Table 18 presents the results of in-domain and out-ofdomain few-shot classification. AMT achieves state-of-the-art overall performance in both tuning-free and test-time fine-tuning settings. Notably, it obtains $17.94\\%$ accuracy gain $(56.91\\rightarrow74.85)$ ) on Omniglot tasks, which exhibit a large domain gap from the training distribution. Furthermore, with a better starting point for the test-time fine-tuning, our method shows it is promising that a plain fine-tuning approach can achieve competitive performance, even when compared to complex neural architecture search (NAS) methods, such as NFTS [106]. ", "page_idx": 23}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/33fb05e4626ae08c1dc8e8cabd7cb180c2b6a2c30121236f3f958af1a3b31b14.jpg", "table_caption": ["Table 18: Comparison with SOTA methods on Meta-Dataset in the variable-way-variable-shot setting. TTF: test-time fine-tuning, Avg.: Average. Bold entries indicate the best for each task dataset. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "L Few-shot Robustness against Natural Corruptions Under Distribution Shifts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We take a step further and investigate few-shot robustness against various types of natural visual corruptions on out-of-domain datasets, reflecting real-world conditions. We adapt ImageNet-C\u2019s methodology [107] to the Meta-Dataset benchmark by applying each category of corruption to 10 datasets. To be specific, we evaluate robustness against 15 common distortions across 4 categories (noise, blur, weather, and digital-based corruptions) with 5 severity levels. Figure 4 shows that our method AMT consistently outperforms previous counterparts across various common corruptions, demonstrating superior robustness. The ability of AMT to handle natural corruptions underscores its potential in practical applications, particularly in environments where robustness to visual corruption is critical, such as autonomous driving and medical image analysis. ", "page_idx": 24}, {"type": "image", "img_path": "HxGdbAmYYr/tmp/1a6d754f705413fc094712ec9ab8e29f06475610ca9738152e0e380861f95f3a.jpg", "img_caption": ["Figure 4: The robustness averaged over Meta-Dataset datasets of different methods in the 5-way 1-shot setting. Robustness is evaluated against 15 common distortions across four categories with varying severity levels. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "M More Adversarial Robustness Evaluations Under Distribution Shifts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To show the effectiveness of our method for boosting adversarial robustness generalization, we conduct more robustness evaluations against unseen threat models and the stronger AutoAttack [100]. ", "page_idx": 24}, {"type": "text", "text": "M.1 Adversarial Robustness against AutoAttack ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To measure adversarial robustness against AutoAttack [100] under distribution shifts, we ground our method and the baseline on the adversarially pre-trained ViT-Small [28]. The APGD with crossentropy and targeted DLR loss, FAB-attack and the Square Attack are used to generate adversarial examples on 100 sampled 5-way 1-shot tasks for each dataset. We adopt $\\ell_{\\infty}$ -bounded perturbations with a radius of $\\epsilon_{\\infty}\\,=\\,4/255$ . The results, shown in Table 19, indicate that our method AMT consistently boosts adversarial generalization across domains, even under the stronger AutoAttack, improving both in-domain and out-of-domain robust accuracy. ", "page_idx": 24}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/7dd55525c6d42ae003cfca35a8a818e49e4cccdd6a5f236db63f934408388ae1.jpg", "table_caption": ["Table 19: Few-shot classification adversarial robust accuracy of AutoAttack [100] on MetaDataset in the 5-way 1-shot setting. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "M.2 Adversarial Robustness against Unseen Attacks ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The robust LoRAPool is constructed by adversarial meta-tuning with $\\ell_{\\infty}$ -bounded perturbations on the source domain ImageNet. To evaluate the adversarial generalization against unseen attacks under distribution shifts, we use the same meta-tuned LoRAPool and employ PGD-10 attacks constrained by both $\\ell_{\\infty}$ and $\\ell_{2}$ norms with varying perturbation budgets $\\epsilon$ . Specifically, we sample 600 5-way 1-shot tasks for each dataset and generate adversarial examples using 10 steps of PGD with the step size $\\epsilon/10$ for $\\ell_{\\infty}$ and $\\epsilon/8.5$ for $\\ell_{2}$ attacks, respectively. The results, shown in Table 20, demonstrate that our method, AMT, significantly enhances adversarial robustness against unseen attacks under various domains for pre-trained vision transformers. Also, compared with the previous style-based adversarial few-shoe learning method, StyleAdv [86], our AMT achieves an $\\ell_{\\infty}$ and $\\ell_{2}$ -robustness improvement of $14.76\\%$ and $13.36\\%$ in average without compromising in-domain performance. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "table", "img_path": "HxGdbAmYYr/tmp/dd8aec062a02a7eb32ec6658b20e335988fd1cf32ebf586ab24d4102a7e95167.jpg", "table_caption": ["Table 20: Few-shot classification adversarial $\\ell_{\\infty}$ , $\\ell_{2}$ -robust accuracy at different radii $\\epsilon$ on Meta-Dataset in the 5-way 1-shot setting. The evaluated models are trained on the single source domain ImageNet. Bold entries indicate the best for each task dataset. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Justification: We evaluate the effectiveness of our method on three benchmarks. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: See the Section 6 for details. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Justification: [TODO] Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. \u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Yes] Justification: We elaborate our implementation details and evaluation metrics in Appendix B.2. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We use public datasets, and we provide the dataset details in Appendix B.1. We have released the code.   \nGuidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not ", "page_idx": 27}, {"type": "text", "text": "including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We include the training details and evaluation metrics in Appendix B.2. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate   \ninformation about the statistical significance of the experiments?   \nAnswer: [No] Justification: The variance of experimental results is smaller than the gap between different methods.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We include details in Appendix B.2. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We conform with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative   \nsocietal impacts of the work performed?   \nAnswer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA]   \nJustification: The paper poses no such risks.   \nGuidelines: \u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Justification: We cite the original paper for datasets used and include dataset details in Appendix B.1.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?   \nAnswer: [NA]   \nJustification: The paper does not release new assets.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?   \nAnswer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 30}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "ustification: The paper does not involve crowdsourcing nor research with human subjects.   \nGuidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]