[{"figure_path": "SqW42eR2wC/figures/figures_1_1.jpg", "caption": "Figure 1: The distribution of vaso for patients with the same state. The physician makes different decisions due to referencing historical information, while the agent based on Markov decision-making can only make the same decision.", "description": "This figure is a histogram comparing the distribution of vaso (vasopressor) dosages prescribed by physicians versus an RL agent for patients with identical states.  The physician's dosages are more varied, indicating the use of historical patient information in their decision-making. In contrast, the RL agent, based on a Markov decision process, demonstrates a much narrower and less varied dosage distribution, highlighting the limitations of Markovian assumptions in healthcare scenarios. The figure emphasizes that real-world healthcare decision-making often relies on non-Markovian factors and patient history.", "section": "1 Introduction"}, {"figure_path": "SqW42eR2wC/figures/figures_3_1.jpg", "caption": "Figure 2: The overview of the safe healthcare policy learning with offline CT.", "description": "This figure illustrates the overall architecture of the Offline Constraint Transformer (CT) for safe healthcare policy learning.  It shows two main components: the Constraint Transformer itself (Section 4.1), which uses a causal attention mechanism and a non-Markovian layer to capture patient history and crucial states for constraint inference, and the Model-based Offline RL (Section 4.2), which generates violating trajectories by imitating actions and observations in healthcare from expert and generated data. These trajectories, combined with expert trajectories, inform constraint learning. The entire system aims to learn safe policies for healthcare decision making by maximizing rewards while respecting inferred constraints.", "section": "4 Method"}, {"figure_path": "SqW42eR2wC/figures/figures_4_1.jpg", "caption": "Figure 3: The structure of the Constraint Transformer.", "description": "The figure shows the architecture of the Constraint Transformer, which consists of a causal transformer for sequential modeling and a non-Markovian layer for weighted constraints. The causal transformer processes sequential input embeddings of states and actions to generate output embeddings.  The non-Markovian layer then uses attention to assign weights to these outputs, creating a weighted sum that represents the overall constraint cost for the trajectory.", "section": "4.1 Constraint Transformer"}, {"figure_path": "SqW42eR2wC/figures/figures_5_1.jpg", "caption": "Figure 4: The structure of the model-based offline RL.", "description": "This figure shows the architecture of the model-based offline reinforcement learning used in the paper.  It takes as input a sequence of tokens representing return-to-go, states, and actions from expert trajectories. The model simultaneously learns a policy model (to predict future actions) and a generative world model (to predict future states and rewards) using a causal transformer. The generative world model uses this to produce violating trajectories by setting a high initial target reward and decreasing it incrementally at each timestep.", "section": "4.2 Model-based Offline RL"}, {"figure_path": "SqW42eR2wC/figures/figures_6_1.jpg", "caption": "Figure 5: The relationship between cost and mortality.", "description": "The figure shows the relationship between the expected cost and the proportion of mortality.  The green line represents the model with the attention layer (CT), and the blue line represents the model without the attention layer. The shaded areas represent the standard error of the mean. The plot demonstrates that as the expected cost increases, the proportion of mortality also increases, especially for the model with the attention layer. This highlights the model's ability to capture unsafe patient states through the cost function and assign higher penalties to them.", "section": "5.1 Can Offline CT Learn Effective Constraints?"}, {"figure_path": "SqW42eR2wC/figures/figures_6_2.jpg", "caption": "Figure 6: The relationship between physiological indicators and cost values. As SOFA and lactate levels become increasingly unsafe, the cost increases. Mean BP and HR at lower values within the safe range incur a lower cost, but as they move into unsafe ranges, the cost increases, penalizing previous state-action pairs. The cost can differentiate between relatively safe and unsafe regions.", "description": "This figure shows the relationship between several physiological indicators (SOFA, Lactate, MeanBP, HR) and their associated cost values as determined by the Constraint Transformer (CT) model.  The x-axis represents the value of each indicator, while the y-axis represents the cost assigned by CT.  The shaded areas represent confidence intervals. The figure demonstrates that as SOFA and Lactate values move out of their typical safe ranges into unsafe ranges, the cost assigned by CT significantly increases.  Conversely, MeanBP and HR show that costs remain relatively low within safe ranges but increase sharply when those ranges are exceeded. This visualization highlights CT's ability to differentiate between safe and unsafe states based on physiological indicators, assigning higher penalties to unsafe conditions.", "section": "5.1 Can Offline CT Learn Effective Constraints?"}, {"figure_path": "SqW42eR2wC/figures/figures_7_1.jpg", "caption": "Figure 7: The relationship between DIFF and the mortality rate in sepsis. The x-axis represents the DIFF. The y-axis indicates the mortality rate of patients at a given DIFF. The solid line represents the mean, while the shaded area indicates the Standard Error of the Mean (SEM).", "description": "This figure shows the relationship between the difference (DIFF) in drug dosages recommended by two different methods (CDT+CT and DDPG) and the mortality rate in sepsis patients.  The left panel displays the relationship between VASO DIFF (difference in vasopressor dosages) and mortality, while the right panel shows the relationship between IV DIFF (difference in intravenous fluid dosages) and mortality.  The shaded areas represent the standard error of the mean, indicating the uncertainty in the measurements. The figure suggests that CDT+CT tends to achieve lower mortality rates, especially when the DIFF is close to zero.", "section": "5.2 Can Offline CT Improve the Performance of CRL?"}, {"figure_path": "SqW42eR2wC/figures/figures_8_1.jpg", "caption": "Figure 8: The relationship between the DIFF of actions and mortality in mechanical ventilator. The actions mainly consist of Positive End Expiratory Pressure (PEEP) and Fraction of Inspired Oxygen (FiO2), which are crucial parameters in ventilator settings.", "description": "This figure shows the relationship between the difference (DIFF) in actions taken by different reinforcement learning models (DDQN, CQL, CDT, and CDT+CT) and the resulting mortality rate in mechanical ventilation scenarios.  The actions considered are Positive End Expiratory Pressure (PEEP) and Fraction of Inspired Oxygen (FiO2). Each subplot displays the mortality rate against the DIFF for a specific model and action. The purpose is to illustrate how well each model approximates the optimal policy that minimizes mortality, considering differences in PEEP and FiO2 from the optimal policy.", "section": "5.2 Can Offline CT Improve the Performance of CRL?"}, {"figure_path": "SqW42eR2wC/figures/figures_13_1.jpg", "caption": "Figure 9: Drug dosage distribution under custom constraint functions in sepsis.", "description": "This figure compares the distribution of intravenous fluid (IV) and vasopressor (Vaso) dosages suggested by the proposed policy (Our Policy) and the dosages administered by physicians (Physician's Policy). The left panels display IV dosages, while the right panels display Vaso dosages.  The plots show that the proposed policy leads to extremely low Vaso dosages (Vaso_max = 0.0011), suggesting that the custom constraint function used in this method is not suitable for the task, as it fails to appropriately balance safety and efficacy.", "section": "A Design and Analysis of the Custom Constraint Function"}, {"figure_path": "SqW42eR2wC/figures/figures_15_1.jpg", "caption": "Figure 10: The accuracy of predicting different state values within the legal range.", "description": "This figure shows the accuracy of the model in predicting different state values within their legal ranges.  The x-axis represents the ranges of accuracy (from <60% to >=95%), while the y-axis shows the number of features falling within each range.  The majority of features (44 out of 48) show a prediction accuracy of >=95%, indicating high reliability.  A smaller number of features (GCS, mechvent, gender, re_admission) exhibit lower prediction accuracy, possibly because these features have limited correlation with other metrics.", "section": "B.3 The Evaluation of Model-based Offline RL"}, {"figure_path": "SqW42eR2wC/figures/figures_15_2.jpg", "caption": "Figure 11: The relationship between average prediction error and trajectory length.", "description": "This figure shows the relationship between the average prediction error and the length of the trajectory generated by the model-based offline RL method. The average prediction error is measured using the Euclidean distance between the generated trajectories and the expert trajectories.  The plot shows that as the trajectory length increases, the average prediction error decreases, while the error at the last point of the trajectory generally stabilizes. This suggests that longer trajectories may offer a balance of accuracy and efficiency for learning effective constraints and safe policies in offline settings. The results support the choice of trajectory length (10-15) used in the experiments, as it provides a reasonable balance between accuracy and model efficiency.", "section": "4.2 Model-based Offline RL"}, {"figure_path": "SqW42eR2wC/figures/figures_15_3.jpg", "caption": "Figure 12: The distribution and penalty values of violating data and expert data.", "description": "This figure shows a comparison of the distribution of actions in the expert dataset and the violating dataset generated by the model. The colors represent the cost or penalty associated with each action pair.  In the expert dataset, the actions tend to cluster within a certain region indicating a safe policy whereas the violating dataset includes actions with higher penalties. This visualization helps to illustrate how the model identifies unsafe actions and assigns them higher penalties. This visual helps in understanding the effectiveness of the model in discovering unsafe behaviors and the distribution of safe vs unsafe actions in the model's generated data.", "section": "4.2 Model-based Offline RL"}, {"figure_path": "SqW42eR2wC/figures/figures_17_1.jpg", "caption": "Figure 2: The overview of the safe healthcare policy learning with offline CT.", "description": "This figure presents a schematic overview of the Offline Constraint Transformer (CT) for safe healthcare policy learning.  It illustrates the process, highlighting three key components: the Constraint Transformer (incorporating causal attention and a non-Markovian layer to model constraints from historical data), a Model-based Offline RL method (generating violating trajectories to improve constraint learning), and a Safe-Critical Decision Making module that employs a traditional CRL method (such as BCQ-Lag, COpiDICE, VOCE, or CDT) to learn a safe policy using the inferred constraints.  The diagram shows how expert data and the generated violating data feed into the Constraint Transformer, which in turn outputs constraints used in the policy learning process.", "section": "4 Method"}, {"figure_path": "SqW42eR2wC/figures/figures_18_1.jpg", "caption": "Figure 13: The relationship between all states and cost values", "description": "This figure shows the relationship between all the states (various physiological indicators) and their corresponding cost values as determined by the Constraint Transformer (CT) model. The x-axis represents the range of values for each physiological indicator, while the y-axis represents the cost value.  Each subplot represents a different physiological indicator, allowing for a visual analysis of how the model assigns costs to different states. This visualization helps understand how CT captures unsafe states by assigning higher costs to values outside the safe range of each indicator. The model is intended to learn to avoid these high-cost states during treatment.", "section": "5.1 Can Offline CT Learn Effective Constraints?"}]