[{"type": "text", "text": "Offline Inverse Constrained Reinforcement Learning for Safe-Critical Decision Making in Healthcare ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Reinforcement Learning (RL) applied in healthcare can lead to unsafe medical   \n2 decisions and treatment, such as excessive dosages or abrupt changes, often due   \n3 to agents overlooking common-sense constraints. Consequently, Constrained   \n4 Reinforcement Learning (CRL) is a natural choice for safe decisions. However,   \n5 specifying the exact cost function is inherently difficult in healthcare. Recent   \n6 Inverse Constrained Reinforcement Learning (ICRL) is a promising approach that   \n7 infers constraints from expert demonstrations. ICRL algorithms model Markovian   \n8 decisions in an interactive environment. These settings do not align with the   \n9 practical requirement of a decision-making system in healthcare, where decisions   \n10 rely on historical treatment recorded in an offilne dataset. To tackle these issues, we   \n11 propose the Constraint Transformer (CT). Specifically, 1) utilize causal attention   \n12 mechanism to incorporate historical decisions and observations into the constraint   \n13 modeling and employ a non-Markovian layer for weighted constraints to capture   \n14 critical states, 2) generative world model to perform exploratory data augmentation,   \n15 thereby enabling offline RL methods to generate unsafe decision sequences. In   \n16 multiple medical scenarios, empirical results demonstrate that CT can capture   \n17 unsafe states and achieve strategies that approximate lower mortality rates, reducing   \n18 the occurrence probability of unsafe behaviors. ", "page_idx": 0}, {"type": "text", "text": "19 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "20 In recent years, the doctor-to-patient ratio imbalance has drawn attention, with the U.S. having   \n21 only 223.1 physicians per 100,000 people [1]. AI-assisted therapy emerges as a promising solution,   \n22 offering timely diagnosis, personalized care, and reducing dependence on experienced physicians.   \n23 Therefore, the development of an effective AI healthcare assistant is crucial.   \n24 Reinforcement learning (RL) offers a promising approach   \n25 to develop AI assistants by addressing sequential decision  \n26 making tasks. However, this method can still lead to   \n27 unsafe behaviors, such as administering excessive drug   \n28 dosages, inappropriate adjustments of medical parameters,   \n29 or abrupt changes in medication dosages. These behaviors,   \n30 such as \u201ctoo high\u201d or \u201csudden change\u201d can significantly   \n31 endanger patients, potentially resulting in acute hypoten  \n32 sion, hypertension, arrhythmias, and organ damage, with   \n33 fatal consequences [4, 5, 6]. For example, in sepsis treat  \n34 ment, patients receiving vasopressors (vaso) at dosages   \n35 exceeding $1\\mu g/(k g\\cdot m i n)$ have a mortality rate of $90\\%$   \n36 [7]. Moreover, the \u201csudden change\u201d in vaso can rapidly   \n37 affect blood vessels, causing acute fluctuations in blood ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Table 1: The proportion of unsafe behaviors occurrences in vaso suggested by physician and DDPG. The typical range for vaso is $0.1\\sim0.2\\mu g/(k g\\!\\cdot\\!m i n)$ , with doses exceeding 0.5 considered high [2]. A cutoff value of 0.75 is identified as a critical threshold associated with increased mortality [3]. ", "page_idx": 0}, {"type": "table", "img_path": "SqW42eR2wC/tmp/51081608d75fceed5edc8316a062752122c01adc28dae491a62b95c2a55a0abf.jpg", "table_caption": [], "table_footnote": ["$\\Delta$ vaso: The change in vaso between two-time points. "], "page_idx": 0}, {"type": "text", "text": "pressure and posing life-threatening risks to patients [8]. Our experiments demonstrate that the work ", "page_idx": 0}, {"type": "text", "text": "39 [9] applying the Deep Deterministic Policy Gradient (DDPG) algorithm in sepsis indeed exhibits   \n40 \u201ctoo high\u201d and \u201csudden change\u201d 1 unsafe behaviors in vaso recommendations, as shown in Table 1.   \n41 This paper aims to achieve safe healthcare policy learning to mitigate unsafe behaviors. The most   \n42 common method for learning safe policies is Constrained Reinforcement Learning (CRL) [10, 11],   \n43 with the key to its success lying in the constraints representation. However, in healthcare, we can   \n44 only design the cost function based on prior knowledge, which limits its application due to a lack of   \n45 personalization, universality, and reliance on prior knowledge. For more details about issues, please   \n46 refer to Appendix A. Therefore, Inverse Constrained Reinforcement Learning (ICRL) [12] emerges as   \n47 a promising approach, as it can infer the constraints adhered to by experts from their demonstrations.   \n48 However, directly applying ICRL in healthcare presents several challenges:   \n49 1) The Markov decision is not compatible with medi  \n50 cal decisions. ICRL algorithms model Markov decisions,   \n51 where the next state depends only on the current state and   \n52 not on the history [13, 14]. However, in healthcare, the   \n53 historical states of patients are crucial for medical decision  \n54 making [15], as demonstrated in the experiments shown   \n55 in Figure 1. Therefore, ICRL algorithms based on Markov   \n56 assumption can not capture patient history, and ignore in  \n57 dividual patient differences, thereby limiting effectiveness.   \n58 2) Interactive environment is not available for health  \n59 care or medical decisions. ICRL algorithms [12, 16]   \n60 follow an online learning paradigm, allowing agents to   \n61 explore and learn from interactive environments. How  \n62 ever, unrestricted exploration in healthcare often entails   \n63 unsafe behaviors that could breach constraints and result ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "SqW42eR2wC/tmp/693ce9a8d1f417b34f8f56715fde26bb55a62b77d6cb4eee4187b29d93193579.jpg", "img_caption": ["Figure 1: The distribution of vaso for patients with the same state. The physician makes different decisions due to referencing historical information, while the agent based on Markov decision-making can only make the same decision. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "64 in substantial losses. Therefore, it is necessary to infer constraints using only offline datasets. ", "page_idx": 1}, {"type": "text", "text": "65 In this paper, we introduce offline Constraint Transformer (CT), a novel ICRL framework that   \n66 incorporates patients\u2019 historical information into constraint modeling and learns from offilne data to   \n67 infer constraints in healthcare. Specifically,   \n68 1) Inspired by the recent success of transformers in sequence modeling [17, 18, 19], we incorporate   \n69 historical decisions and observations into constraint modeling using a causal attention mechanism. To   \n70 capture key events in trajectories, we introduce a non-Markovian transformer to generate constraints   \n71 and importance weights, and then define constraints using weighted sums. CT takes trajectories as   \n72 input, allowing for the observation of patients\u2019 historical information and evaluation of key states.   \n73 2) To learn from an offilne dataset, we introduce a model-based offilne RL method that simultaneously   \n74 learns a policy model and a generative world model via auto-regressive imitation of the actions and   \n75 observations in medical decisions. The policy model employs a stochastic policy with entropy   \n76 regularization to prevent it from overfitting and improve its robustness. Utilizing expert datasets,   \n77 the generative world model uses an auto-regressive exploration generation paradigm to effectively   \n78 discover a set of violating trajectories. Then, CT can infer constraints in healthcare through these   \n79 unsafe trajectories and expert trajectories.   \n80 In the medical scenarios of sepsis and mechanical ventilation, we conduct experimental evaluations of   \n81 offilne CT. Experimental evaluations demonstrate that offilne CT can capture patients\u2019 unsafe states   \n82 and assign higher penalties, thereby providing more interpretable constraints compared to previous   \n83 works [9, 20, 21]. Compared to unconstrained and custom constraints, CT achieves strategies that   \n84 closely approximate lower mortality rates with a higher probability (improving by $8.85\\%$ compared to   \n85 DDPG). To investigate the avoidance of unsafe behaviors with offilne CT, we evaluate the probabilities   \n86 of \u201ctoo high\u201d and \u201csudden changes\u201d occurring in the sepsis. The experimental results show that CRL   \n87 with CT can reduce the probability of unsafe behaviors to zero. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "88 2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "89 Reinforcement Learning in Healthcare. RL has made great progress in the realm of healthcare, such   \n90 as sepsis treatment [9, 20, 21, 22], mechanical ventilation [23, 24, 25], sedation [26] and anesthesia   \n91 [27, 28]. However, these works mentioned above have not addressed potential safety issues such as   \n92 sudden changes or too high doses of medication. Therefore, the development of policies that are both   \n93 safe and applicable across various healthcare domains is crucial.   \n94 Inverse Constrained Reinforcement Learning. Previous works inferred constraint functions by   \n95 determining the feasibility of actions under current states. In discrete state-action spaces, Chou et al.   \n96 [29] and Park et al. [30] learned constraint sets to differentiate constrained state-action pairs. Scobee   \n97 & Sastry [31] proposed inferring constraint sets based on the principle of maximum entropy, while   \n98 some studies [32, 33] extended this approach to stochastic environments using maximum causal   \n99 entropy [34]. In continuous domains, Malik et al. [12], Gaurav et al. [16], and Qiao et al. [35] used   \n100 neural networks to approximate constraints. Some works [11, 29] applied Bayesian Monte Carlo and   \n101 variational inference to infer the posterior distribution of constraints in high-dimensional state spaces.   \n102 Xu et al. [36] modeled uncertainty perception constraints for arbitrary and epistemic uncertainties.   \n103 However, these methods can only be applied online and lack historical dependency.   \n104 Transformers for Reinforcement Learning. Transformer has produced exciting progress on RL   \n105 sequential decision problems [17, 18, 37, 38]. These works no longer explicitly learn Q-functions   \n106 or policy gradients, but focus on action sequence prediction models driven by target rewards. Chen   \n107 et al. [18] and Janner et al. [37] perform auto-regressive modeling of trajectories to achieve policy   \n108 learning in an offline environment. Furthermore, Zheng et al. [17] unify offline pretraining and   \n109 online fine-tuning within the Transformer framework. Liu et al. [38] and Kim et al. [19] integrate the   \n110 transformer architecture into constraint learning and preference learning. The transformer architecture,   \n111 with its sequence modeling capability and independence from the Markov assumption, can capture   \n112 temporal dependencies in medical decision-making. Thus, it is well-suited for trajectory learning and   \n113 personalized learning in medical settings. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "114 3 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "115 We model the medical environment with a Constrained Markov Decision Process (CMDP) ${\\mathcal{M}}^{c}$ [39],   \n116 which can be defined by a tuple $\\left(S,A,\\mathcal{P},\\mathcal{R},\\mathcal{C},\\gamma,\\kappa,\\rho_{0}\\right)$ . Similar to studies [23, 40], we extract data   \n117 7 within 72 hours of patient admission, with each 4-hour interval constituting a window or time step.   \n118 The state indicators of the patient at each time step are denoted as $s\\in S$ . The administered drug   \n119 doses or instrument parameters of interest are considered as actions $a\\in{\\mathcal{A}}$ , while reward function   \n120 $\\mathcal{R}$ is used to describe the quality of the patient\u2019s condition and provided by experts based on prior   \n121 work [9, 23]. At each time step $t$ , an agent performs an action $a_{t}$ at a patient\u2019s state $s_{t}$ . This process   \n122 generates the reward $r_{t}\\sim\\mathcal{R}(s_{t},a_{t})$ , the cost $c_{t}\\sim\\mathcal{C}$ and the next state $s_{t+1}\\sim\\mathcal{P}\\left(\\cdot\\mid s_{t},a_{t}\\right)$ , where   \n123 $\\mathcal{P}$ defines the transition probabilities. $\\gamma$ denotes the discount factor. $\\kappa\\in\\mathbb{R}_{+}$ denotes the bound of   \n124 cumulative costs. $\\rho_{0}$ defines the initial state distribution. The goal of the CRL policy $\\pi$ is to maximize   \n125 the reward return while limiting the cost in a threshold $\\kappa$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\arg\\operatorname*{max}_{\\pi}\\bar{\\mathbb{E_{\\pi}}}_{,\\rho_{0}}[\\sum_{t=1}^{T}\\gamma^{t}r_{t}],\\quad\\mathrm{~s.t.~}\\;\\;\\mathbb{E}_{\\pi,\\rho_{0}}[\\sum_{t=1}^{T}\\gamma^{t}c_{t}]\\leq\\kappa.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "126 where $T$ is the length of the trajectory $\\tau$ . CRL commonly assumes that constraint signals are directly   \n127 observable. However, in healthcare, such signals are not easily obtainable. Therefore, Our objective   \n128 is to infer reasonable constraints for CRL to achieve safe policy learning in healthcare.   \n129 Safe-Critical Decision Making with Constraint Inference in Healthcare. Our general goal is for   \n130 our policy to approximate the optimal policy, which refers to the strategy under which the patient\u2019s   \n131 mortality rate is minimized (achieving a zero mortality rate is often difficult since there are patients   \n132 who can not recover, regardless of all potential future treatment sequences [41]). Decision-making   \n133 with constraints can formulate safer strategies by discovering and avoiding unsafe states, thereby   \n134 approaching the optimal policy.   \n135 However, most offline RL algorithms rely on online evaluation, where the agent is evaluated in   \n136 an interactive environment, whereas in medical scenarios, only offline evaluation can be utilized.   \n137 In previous works [5, 9, 40, 42], they qualitatively analyzed by comparing the differences (DIFF)   \n138 between the drug dosage recommended by our policy $\\pi$ and the dosage administered by clinical   \n139 physicians $\\hat{\\pi}$ , and its relationship with mortality rates, through graphical analysis. In the graph   \n140 depicting the relationship between the DIFF and mortality rate, at the point when DIFF is zero, the   \n141 lower the mortality rate of patients, the better the performance of the policy [40]. To provide a more   \n142 accurate quantitative evaluation, we introduce the concept of the probability of approaching the   \n143 optimal policy, defined as $\\omega$ : ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\omega={\\frac{\\mathrm{Number~of~survivors~among~the~top~}N\\mathrm{~patients}}{N}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "144 We randomly collect $2N$ patients (with an equal number of known survivors and non-survivors under   \n145 doctor\u2019s policy $\\hat{\\pi}$ ) from the offline dataset. We then calculate the DIFF and sort it in ascending order.   \n146 The optimality of the policy can be evaluated through the following two points: 1) The higher the   \n147 survival probability (i.e., $\\omega$ ) of the top $N$ patients, the lower the mortality rate can be achieved by   \n148 executing $\\pi;2$ ) The smaller the DIFF among the surviving patients in the top $N$ , the greater the   \n149 probability that $\\pi$ is optimal. ", "page_idx": 3}, {"type": "text", "text": "150 4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "151 To infer constraints and achieve safe decision-making in healthcare, we introduce the Offline Con  \n152 straint Transformer (Figure 2), a novel ICRL framework.   \n153 Inverse Constrained Reinforcement Learning. ICRL aims to recover the cost function $\\mathcal{C}^{*}$ by   \n154 leveraging a set of trajectories $\\mathcal{D}_{e}=\\{\\tau_{e}^{(i)}\\}_{i}^{N}$ sampled from an expert policy $\\pi_{e}$ , where $N$ denotes   \n155 the number of the trajectories. ICRL is commonly based on the Maximum Entropy framework [31],   \n156 and the likelihood function is articulated as [12]: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\np(\\mathcal{D}_{e}\\mid\\mathcal{C})=\\frac{1}{\\left(Z_{\\mathcal{M}}c^{\\right)^{N}}}\\prod_{i=1}^{N}\\exp\\left[R(\\tau^{(i)})\\right]\\mathbb{I}^{\\mathcal{M}^{c}}(\\tau^{(i)})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "157 Here, $\\begin{array}{r}{Z_{\\mathcal{M}}=\\int\\exp(\\beta r(\\tau))\\mathbb{I}^{\\mathcal{M}}(\\tau)d\\tau}\\end{array}$ is the normalizing term. The indicator $\\mathbb{I}^{\\mathcal{M}^{c}}(\\tau^{(i)})$ signifies the   \n158 extent to which the trajectory $\\tau^{(i)}$ satisfies the constraints. It can be approximated using a neural   \n159 network $\\zeta_{\\theta}(\\tau^{(i)})$ parameterized with $\\theta$ , defined as $\\begin{array}{r}{\\zeta_{\\theta}(\\tau^{(i)})=\\prod_{t=0}^{T}\\zeta_{\\theta}^{\\overline{{\\theta}}}(s_{t}^{i},a_{t}^{i})}\\end{array}$ . Consequently, the   \n160 cost function can be formulated as $C_{\\theta}=1-\\zeta_{\\theta}$ . Substituting th e neural network for the indicator, we   \n161 can update $\\theta$ through the gradient of the log-likelihood function: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{L}\\left(\\theta\\right)=\\mathbb{E}_{\\tau^{(i)}\\sim\\pi_{e}}\\left[\\nabla_{\\theta}\\log[\\zeta_{\\theta}(\\tau^{(i)})]\\right]-\\mathbb{E}_{\\hat{\\tau}\\sim\\pi_{\\mathcal{M}}^{\\hat{\\zeta}_{\\theta}}}\\left[\\nabla_{\\theta}\\log[\\zeta_{\\theta}(\\hat{\\tau}^{(i)})]\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "162 where $\\mathcal{M}^{\\hat{\\zeta}_{\\theta}}$ denotes the MDP obtained after augmenting $\\mathcal{M}$ with the cost function $C_{\\theta}$ , using the   \n163 executing policy $\\pi_{\\mathcal{M}^{\\hat{\\varsigma}_{\\theta}}}$ . And $\\hat{\\tau}$ are sampled from the policy. In practice, ICRL can be conceptualized   \n164 as a bi-level optimization task [11]. We can 1) update this policy based on Equation 1, and 2) employ   \n165 Equation 4 for constraint learning. Intuitively, the objective of Equation 4 is to distinguish between   \n166 trajectories generated by expert policies and imitation policies that may violate the constraints.   \n167 Specifically, task 1) involves updating the policy using advanced CRL methods. Significant progress   \n168 has been made in some works such as BCQ-Lagrangian (BCQ-Lag), COpiDICE [43], VOCE [44],   \n169 and CDT [38]. Meanwhile, task 2) focuses on learning the constraint function, as shown in Figure   \n170 2. Our research primarily improves the latter process due to two main challenges facing ICRL   \n171 in healthcare: Challenge 1) pertains to the limitations of the Markov property, and Challenge 2)   \n172 involves the issue of inferring constraints only from offilne datasets. To address these challenges, we   \n173 propose the offline CT as our solution.   \n174 Offilne Constraint Transformer. To address the first challenge, we delve into the inherent issues of   \n175 applying the Markov property to healthcare and draw inspiration from the successes of Transformer   \n176 in decision-making, redefining the representation of the constraints. To realize the offilne training, we   \n177 consider the essence of ICRL updates, proposing a model-based RL to generate unsafe behaviors   \n178 used to train CT. We outline three parts: establishing the constraint representation model (Section   \n179 4.1), creating an offline RL for violating data (Section 4.2), and learning safe policies (Section 4.3). ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "SqW42eR2wC/tmp/9a15f5c72972f339ea46a4a9024c48d6e059e5458f4694d568f105f9a319db1c.jpg", "img_caption": ["Figure 2: The overview of the safe healthcare policy learning with offline CT. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "180 4.1 Constraint Transformer ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "181 ICRL methods relying on the Markov prop  \n182 erty overlook patients\u2019 historical informa  \n183 tion, focusing only on the current state.   \n184 However, both current and historical states,   \n185 along with vital sign changes are crucial   \n186 for a human doctor\u2019s decision-making pro  \n187 cess [15]. To emulate the observational   \n188 approach of humans, we draw inspiration   \n189 from the Decision Transformer (DT) [18]   \n190 to incorporate historical information into   \n191 constraints for a more comprehensive ob  \n192 servation and judgment. We propose a   \n193 constraint modeling approach based on a   \n194 causal attention mechanism, as shown in Figure 3. The structure comprises a causal Transformer for   \n195 sequential modeling and a non-Markovian layer for weighted constraints learning.   \n196 Sequential Modeling for Constraints Inference. For a trajectory segment of length $T$ , $2T$ input   \n197 embeddings are generated, with each position containing state $s$ and action $a$ embeddings. Addi  \n198 tionally, these embeddings undergo linear and normalization layers before being fed into the causal   \n199 Transformer, which produces output embeddings $\\{d_{t}\\}_{t=1}^{T}$ determined by preceding input embeddings   \n200 from $\\left({{s_{1}},{a_{1}},...,{s_{T}},{a_{T}}}\\right)$ . Here, $d_{t}$ depends only on the previous $t$ states and actions.   \n201 Modeling Non-Markovian for Weighted Constraints Learning. Although $d_{t}$ represents the cost   \n202 function $c_{t}$ derived from observations over long trajectories, it doesn\u2019t pinpoint which previous key   \n203 actions or states led to its increase. In healthcare, identifying key actions or states is vital for analyzing   \n204 risky behaviors and status, and enhancing model interpretability. To address this, we draw inspiration   \n205 from the design of the preference attention layer in [19] and introduce an additional attention layer.   \n206 This layer is employed to define the cost weight for non-Markovians. It takes the output embeddings   \n207 from the causality transformer as input and generates the corresponding cost and importance weights.   \n208 The output of the attention layer is computed by weighting the values through the normalized dot   \n209 product between the query and other keys: ", "page_idx": 4}, {"type": "image", "img_path": "SqW42eR2wC/tmp/3209993b3fe46563f0ae60a6b61d9eb1a131fd5c67692224f0dac9c9e5af18f8.jpg", "img_caption": ["Figure 3: The structure of the Constraint Transformer. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathrm{softmax}\\left(\\left\\{\\langle q_{t},k_{t^{\\prime}}\\right\\rangle\\right\\}_{t^{\\prime}=1}^{T}\\right)_{t}\\cdot c_{t}=\\sum_{t=1}^{T}w_{t}\\cdot c_{t}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "210 Here, the key $k_{t}\\,\\in\\,\\mathbb{R}^{m}$ , query $q_{t}\\,\\in\\,\\mathbb{R}^{m}$ , and value $c_{t}\\,\\in\\,\\mathbb{R}^{m}$ are derived from the $t$ -th input $d_{t}$   \n211 through linear transformations, where $m$ denotes the embedding dimension. Furthermore, for each   \n212 time step $t$ , since $d_{t}$ depends only on the previous state-action pairs $\\{(s_{i},a_{i})\\}_{i=1}^{t}$ and serves as the   \n213 input embedding for the attention layer, $c_{t}$ is also associated solely with the preceding $t$ time steps.   \n214 The representation of the cost function as a weighted sum is defined as $\\begin{array}{r}{C\\left(\\tau\\right)=\\sum_{t=1}^{T}w_{t}\\cdot c_{t}}\\end{array}$ . Then,   \n215 we can also determine the constraint function values for each preceding subsequence. Introducing the   \n216 newly defined cost function, we redefine Equation 4 for CT as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\phi}\\mathcal{L}\\left(\\phi\\right)=\\mathbb{E}_{\\hat{\\tau}\\sim\\mathcal{D}_{v}}\\left[\\nabla_{\\phi}\\log[C_{\\phi}(\\hat{\\tau})]\\right]-\\mathbb{E}_{\\tau\\sim\\mathcal{D}_{e}}\\left[\\nabla_{\\phi}\\log[\\,C_{\\phi}(\\tau)]\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "217 where $\\phi$ is the parameter of CT, $\\mathcal{D}_{e}$ and $\\mathcal{D}_{v}$ represent the expert data and the violating data. This   \n218 formulation implies that the constraint should be minimized on the expert policy and maximized on   \n219 the violating policy. We construct an expert and a violating dataset to evaluate Equation 6 in offline.   \n220 The expert data can be acquired from existing medical datasets or hospitals. Regarding the violating   \n221 dataset, we introduce a generative model to establish it, as detailed in Section 4.2.   \n223 To train CT offilne, we introduce a model  \n224 based offline RL method (Figure 4) to gen  \n225 erate violating data that refers to unsafe   \n226 behavioral data and can be represented as   \n227 $\\tau_{v}=(s_{1},a_{1},r_{1},s_{2},\\ldots)\\in\\mathcal{D}_{v}$ . The model   \n28 ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "SqW42eR2wC/tmp/8b3922f42023a8bd2a998bb67eea7075e18fdc6d4e26c978bb7b8663a4dd017c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "sivmultan1eou1sly 1lea2rns a policvy model and a Figure 4: The structure of the model-based offline RL. 29 generative world model via auto-regressive imitation of the actions and observations in healthcare. 30 The model processes a trajectory, $\\tau_{e}\\in\\mathcal{D}_{e}$ , as a sequence of tokens encompassing the return-to-go, 223312 $t$ t iast etsh, ea nsdu amc toiof nfsu, tdueref inreedw aars $(\\hat{R}_{1},s_{1},a_{1},...,\\hat{R}_{T},s_{T},a_{T})$ $\\hat{R}_{t}\\ =\\ \\sum_{t^{\\prime}=t}^{T}r_{t^{\\prime}}$ b. tth ee arcethu rtin-mteos-tgeop $\\hat{R}_{t}$ $t$ iatt  teimmpelsoteyps $K$ timesteps as its in put, where $K$ represents the context length. 34 Thus, the input tokens for it at timestep $t$ are denoted as $h_{t}\\,=\\,\\{\\hat{R}_{-K:t},s_{-K:t},a_{-K:t-1}\\}$ , where 35 $\\hat{R}_{-K:t}=\\{\\hat{R}_{K},...,\\hat{R}_{t}\\}$ , $s_{-K:t}=\\left\\{s_{K},...,s_{t}\\right\\}$ and $a_{-K:t-1}=\\left\\{a_{K},...,a_{t-1}\\right\\}$ . ", "page_idx": 5}, {"type": "text", "text": "236 Policy Model. The input tokens are encoded through a linear layer for each modality. Subsequently,   \n237 the encoded tokens pass through a casual transformer to predict future action tokens. We use   \n238 a stochastic policy [38] to achieve policy learning. Additionally, we utilize a Shannon entropy   \n239 regularizer $\\mathcal{H}\\left[\\pi_{\\vartheta}(\\cdot\\mid h)\\right]$ to prevent policy overfitting and enhance robustness. The optimization   \n240 objective is to minimize the negative log-likelihood loss while maximizing the entropy with weight $\\lambda$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\vartheta}\\quad\\mathbb{E}_{\\boldsymbol{h}_{t}\\sim\\mathcal{D}_{e}}[-\\log\\pi_{\\vartheta}(\\cdot\\mid\\boldsymbol{h}_{t})-\\lambda\\mathcal{H}\\left[\\pi_{\\vartheta}(\\cdot\\mid\\boldsymbol{h}_{t})\\right]]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "242 where the policy $\\pi_{\\vartheta}\\left(\\cdot\\mid h_{t}\\right)=\\mathcal{N}\\left(\\mu_{\\vartheta}\\left(h_{t}\\right),\\Sigma_{\\vartheta}\\left(h_{t}\\right)\\right)$ adopts the stochastic Gaussian policy represen  \n243 tation and $\\vartheta$ is the parameter.   \n244 Generative World Model. To predict states and rewards, we use $x_{t}=\\{h_{t}\\cup a_{t}\\}$ as input encoded   \n245 by linear layers. The encoded tokens pass through the casual transformer to predict hidden tokens.   \n246 Then we utilize two linear layers to fti the rewards and states. The optimization objective for the two   \n247 linear layers $\\ell$ with the parameters $\\varphi$ and $\\mu$ can be defined as: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\varphi,\\mu}~~~\\mathbb{E}_{s_{t},r_{t-1}\\in x_{t}\\sim\\mathcal{D}_{e}}[(s_{t}-\\ell_{\\varphi}(x_{t}))^{2}+(r_{t-1}-\\ell_{\\mu}(x_{t}))^{2}]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "248 Generating Violating Data. In RL, excessively high rewards, surpassing those provided by domain   \n249 experts, may incentivize agents to violate the constraints in order to maximize the total reward [11].   \n250 Therefore, we set a high initial target reward $\\hat{R}_{1}$ to obtain violation data. We feed $\\hat{R}_{1}$ and initial state   \n251 s(1i)into the model-based offline RL to generate \u03c4 v(i)in an auto-regressive manner, as depicted in   \n252 model-based offilne RL of Figure 2, where $\\widetilde{a}$ , $\\widetilde r$ and $\\widetilde{s}$ are predicted by the model. The target reward   \n253 $\\hat{R}$ decreases incrementally and can be represented as $\\bar{R}_{t+1}=\\hat{R}_{t}-\\widetilde{r}_{t}^{\\phantom{\\dagger}}$ . Considering the average error   \n254 in trajectory prediction, we generate trajectories with the length $K=10$ , as detailed in Appendix   \n255 B.3. Repeating $N$ initial states, we can get violating data $\\mathcal{D}_{v}=\\{\\tau_{v}^{(i)}\\}_{i=1}^{N}$ .   \n256 Note that certain other generative models, such as Variational Auto-Encoder (VAE) [45], Generative   \n257 Adversarial Networks (GAN) [46, 47], and Denoising Diffusion Probabilistic Models (DDPM)   \n258 [48, 49], may be better at generating data. We introduce the model-based offline RL primarily   \n259 because it has been shown to generate violating data with exploration [38] and possess the ability to   \n260 process time-series features efficiently. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "261 4.3 Safe-Critical Decision Making with Constraints. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "262 To train offilne CT, we gather the medical expert dataset $\\mathcal{D}_{e}$ from the environment. Then, we employ   \n263 gradient descent to train the model-based offilne RL, guided by Equation 7 and Equation 8, continuing   \n264 until the model converges. Using this RL model, we automatically generate violating data denoted   \n265 as $\\mathcal{D}_{v}$ . Subsequently, CT is optimized based on Equation 6 to get the cost function $C$ , leveraging   \n266 samples from both $\\mathcal{D}_{e}$ and $\\mathcal{D}_{v}$ . To learn a safe policy, we train the policy $\\pi$ using $C$ until it converges   \n267 based on Equation 1. The detailed training procedure is presented in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "268 5 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "269 In this section, we first provide a brief overview of the task, as well as data extraction and prepro  \n270 cessing. Subsequently, in Section 5.1, we demonstrate that CT can describe constraints in healthcare   \n271 and capture critical patient states. We emphasize its applicability to various CRL methods and its   \n272 ability to approach the optimal policy for reducing mortality rates in Section 5.2. Finally, Section 5.3   \n273 discusses the realization of the objective of safe medical policies. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Safe Policy Learning with Offline CT ", "page_idx": 6}, {"type": "text", "text": "Input: Expert trajectories $\\mathcal{D}_{e}$ , context length $K$ , target reward $\\hat{R}_{1}$ , samples $N$ , episode length $T$ 1: Train model-based offline RL $\\mathcal{M}$ : Update $\\vartheta$ , $\\varphi$ and $\\mu$ using the Equation (7) and Equation (8) 2: for ${\\mathfrak{t}}=1,...,\\mathrm{T}$ do ", "page_idx": 6}, {"type": "text", "text": "3: Sample initial states $S_{1}$ from $\\mathcal{D}_{e}$   \n4: Generate the violating dataset: $\\mathcal{D}_{v}\\leftarrow\\mathcal{M}$ .generate_data $(S_{1},\\hat{R}_{1},K)$   \n5: Sample set of trajectories $\\{\\tau_{e}^{(i)}\\}_{i=1}^{N}$ and $\\{\\tau_{v}^{(i)}\\}_{i=1}^{N}$ from $\\mathcal{D}_{e}$ and $\\mathcal{D}_{v}$   \n6: Train offline CT: Use $\\{\\tau_{e}^{(i)}\\}_{i=1}^{N}$ and $\\{\\tau_{v}^{(i)}\\}_{i=1}^{N}$ to update $\\phi$ based on Equation (6)   \n7: Safe policy learning: Update $\\pi$ using the cost function $C_{\\phi}(\\tau)$ based on Equation (1)   \n8 ndf ", "page_idx": 6}, {"type": "text", "text": "Output: $\\pi$ and $C(\\tau)$ ", "page_idx": 6}, {"type": "text", "text": "274 Tasks. We primarily use the sepsis task that is commonly used in previous works [9, 20, 42, 22], and   \n275 supplement some experiments on the mechanical ventilator task [23, 50]. The detailed definition of   \n276 the two tasks mentioned above can be found in Appendix B.1 and B.2.   \n277 Data Extraction and Pre-processing. Our medical dataset is derived from the Medical Information   \n278 Mart for Intensive Care III (MIMIC-III) database [51]. For each patient, we gather relevant physio  \n279 logical parameters, including demographics, lab values, vital signs, and intake/output events. Data is   \n280 grouped into 4-hour windows, with each window representing a time step. In cases of multiple data   \n281 points within a step, we record either the average or the sum. We eliminate variables with significant   \n282 missing values and use the $k$ -nearest neighbors method to flil in the rest. Notably, the training dataset   \n283 consists of data from surviving patients, while the validation set includes survivors and non-survivors.   \n284 Model-based Offilne RL Evaluation. To ensure the rigor of the experiments, we evaluate the validity   \n285 of the model-based offline RL, as detailed in Appendix B.3. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "286 5.1 Can Offline CT Learn Effective Constraints? ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "287 In this section, we primarily assess the efficacy of the cost function   \n288 learned by offline CT in sepsis, focusing particularly on its capa  \n289 bility to evaluate patient mortality rates and capture critical events.   \n290 First, we employ the cost function to compute cost values for the   \n291 validation dataset. Subsequently, we statistically analyze the rela  \n292 tionship between these cost values and mortality rates. As shown in   \n293 Figure 5, there is an increase in patient mortality rates with rising   \n294 cost values. It\u2019s noteworthy that such increases in mortality rates are   \n295 often attributed to suboptimal medical decisions. Therefore, these Figure 5: The relationship be", "page_idx": 6}, {"type": "image", "img_path": "SqW42eR2wC/tmp/6a2f66ac240a3cfa60dc7b15d8af9789488a63bb9ae78ca936d9606967bb9211.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "96 experimental findings affirm that the cost values effectively reflect tween cost and mortality. ", "page_idx": 6}, {"type": "text", "text": "297 the quality of medical decision-making. To observe the impact of the attention layer (non-Markovian   \n298 layer), we conduct experiments by removing the attention layer from CT. The results reveal that the   \n299 penalty values do not correlate proportionally with mortality rates. This indicates that the attention   \n300 layer plays a crucial role in assessing constraints. ", "page_idx": 6}, {"type": "image", "img_path": "SqW42eR2wC/tmp/737fe6fee831f62b1d8af82f4aa3af9f20d4509ba237d7a983a032ba9a140e8d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 6: The relationship between physiological indicators and cost values. As SOFA and lactate levels become increasingly unsafe, the cost increases. Mean BP and HR at lower values within the safe range incur a lower cost, but as they move into unsafe ranges, the cost increases, penalizing previous state-action pairs. The cost can differentiate between relatively safe and unsafe regions. ", "page_idx": 6}, {"type": "text", "text": "301 To assess the capability of the cost function to capture key events, we analyze the relationship   \n302 between physiological indicators and cost values. We focus on four key indicators in sepsis treatment:   \n303 Sequential Organ Failure Assessment (SOFA) score [52], lactate levels [53], Mean Arterial Pressure   \n304 (MeanBP) [54], and Heart Rate (HR) [55]. The SOFA score and lactate levels are critical indicators   \n305 for assessing sepsis severity, with higher values indicating greater patient risk. MeanBP and HR   \n306 are essential physiological metrics, typically ranging from 70 to $\\mathrm{100\\mmHg}$ and 60 to 100 beats,   \n307 respectively. Deviations from these ranges can signify patient risk. As depicted in Figure 6, the cost   \n308 values effectively distinguish between high-risk and safe conditions, reflecting changes in patient   \n309 status. Additional details on other parameters\u2019 relationship with cost are in Appendix B.4. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "310 5.2 Can Offline CT Improve the Performance of CRL? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "311 Baselines. We adopt the DDPG method as the baseline in sepsis research [9], and the Double Deep   \n312 Q-Learning (DDQN) and Conservative Q-Learning (CQL) methods as baselines in ventilator research   \n313 [23]. Since there are no other offline inverse reinforcement learning works available for reference,   \n314 we have included two additional settings: no cost and custom cost. In the case of no cost, the cost is   \n315 set to zero, while the design of custom constraints is outlined in Appendix A. These settings help   \n316 evaluate whether CT can infer effective constraints.   \n317 Metrics. To assess effectiveness, we use $\\omega$ to indicate the probability that the policy is optimal and   \n318 analyze the relationship between DIFF and mortality rate through a graph. Recently, Kondrup et   \n319 al. [23] use the Fitted Q Evaluation (FQE) [56] to evaluate the policy in healthcare. However, the   \n320 value estimates of FQE depend solely on the dataset $\\mathcal{D}$ and the actions chosen by the policy $\\pi$ used to   \n321 train FQE. This reliance can lead to inaccurate estimates when evaluating unseen state-action pairs.   \n322 Therefore, we do not adopt this method as an evaluation metric.   \n323 Results. We combine our method CT with com  \n324 mon CRL algorithms (e.g., VOCE, COpiDICE,   \n325 BCQ-Lag, and CDT), and compare them with   \n326 both no-cost and custom cost settings. Each   \n327 CRL model is trained using no cost, custom cost,   \n328 and CT separately, with other parameters set the   \n329 same during training. For evaluation metrics,   \n330 we use IV difference (IV DIFF), vaso differ  \n331 ence (VASO DIFF), and combined [IV, VASO]   \n332 difference (ACTION DIFF) as the metrics to   \n333 be ranked. We measure the mean and variance   \n334 of $\\omega\\%$ in 10 sets of random seeds, and the re  \n335 sults are shown in Table 2. From the results,   \n336 we can conclude: (1) In different CRL meth  \n337 ods, CT consistently makes the strategy closer   \n338 to the one with lower mortality rates, with a   \n339 probability $8.85\\%$ higher than DDPG. (2) We   \n340 find that $\\mathrm{CDT+CT}$ achieves better results on all   \n341 three metrics. CDT is also a transformer-based   \n342 method, which indicates that transformer-based   \n343 architecture indeed exhibits more outstanding   \n344 performance in healthcare.   \n345 Figure 7 illustrates the relationship between IV   \n346 and VASO DIFF with mortality rates under the   \n347 DDPG and $\\mathrm{CDT+CT}$ methods in sepsis. In   \n348 VASO DIFF, when the gap is zero, the mor  \n349 tality rate under $\\mathrm{CDT+CT}$ is lower than that   \n350 under DDPG, indicating that following the for  \n351 mer strategy could lead to a lower mortality   \n352 rate. Similarly, in IV DIFF, the same trend is ob  \n353 served. Notably, for the IV strategy, the lowest   \n354 mortality rate for DDPG does not occur at the   \n355 point where the difference is zero, indicating a   \n356 significant estimation bias. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "SqW42eR2wC/tmp/aa1f418fd893e50e32bfe53c56722fd75bfb4b44892f547a00a616b35708dd56.jpg", "table_caption": ["Table 2: Performance of sepsis strategies under various offilne CRL models and different constraints. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "SqW42eR2wC/tmp/e72af12a83949be4619729a4ba82d591f2514690a024c17f18d01427d6e184e2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 7: The relationship between DIFF and the mortality rate in sepsis. The $\\mathbf{X}$ -axis represents the DIFF. The y-axis indicates the mortality rate of patients at a given DIFF. The solid line represents the mean, while the shaded area indicates the Standard Error of the Mean (SEM). ", "page_idx": 7}, {"type": "text", "text": "357 In addition, corresponding experiments are conducted on the mechanical ventilator, as shown in   \n358 Figure 8. Compared to previous methods DDQN and CQL, under the $\\mathrm{CDT+CT}$ approach, a noticeable   \n359 trend is observed where the proportion of mortality rates increases with increasing differences. When ", "page_idx": 7}, {"type": "text", "text": "360 there is a significant difference in DIFF, the results may be unreliable, possibly due to the limited data distribution in the tail. ", "page_idx": 8}, {"type": "image", "img_path": "SqW42eR2wC/tmp/1debf02787ebbb72e42c3e986af9900c391338b5832a92e2ca27616b820bf5e0.jpg", "img_caption": ["Figure 8: The relationship between the DIFF of actions and mortality in mechanical ventilator. The 361 (aFcitiOo2n)s,  mwahiinclhy  acroe ncsrisutc ioafl  Ppoasriatimveet eErns di nE vxepinrtialtaotroyr  Psreettsisnugrse. (PEEP) and Fraction of Inspired Oxygen "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "362 5.3 Can CRL with Offline CT Learn Safe Policies? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "363 We have confirmed the existence   \n364 of two unsafe strategy issues,   \n365 namely \u201ctoo high\u201d and \u201csudden   \n366 change\u201d in the treatment of sep  \n367 sis, particularly in vaso in Sec  \n368 tion 1. To validate whether   \n369 the $\\mathrm{CRL+CT}$ approach could ad  \n370 dress these concerns, we employ   \n371 the same statistical methods to   \n372 evaluate our methodology, shown in Table 3. To elucidate the efficacy of CT, we compare it with   \n373 CDT+No-cost and CDT+Custom-cost approaches. We find that only the custom cost and CT methods   \n374 successfully mitigated the risks associated with \u201ctoo high\u201d and \u201csudden change\u201d behaviors. However,   \n375 the custom cost approach opts to avoid administering drugs to mitigate these risks. Without these   \n376 drugs, the patient\u2019s condition may not be alleviated, potentially leading to patient mortality. The   \n377 $\\mathrm{CDT+CT}$ approach can give a more appropriate drug dosage.   \n378 Ablation Study. To investigate the impact of each component on the model\u2019s performance, we   \n379 conducted experiments by sequentially removing each component from the $\\mathrm{CDT+CT}$ model. The   \n380 results are presented in the lower half of Table 2. Both CT and its non-Markovian layer (attention   \n381 layer) are indispensable and crucial components; removing either one results in a decrease in perfor  \n382 mance. Additionally, we observed that even a pure generative model outperforms DDPG in terms   \n383 of performance. This is primarily because it inherently operates as a sequence-based reinforcement   \n384 learning model, possessing exploration and consideration for long-term history. Therefore, this   \n385 further underscores the effectiveness of sequence-based approaches in healthcare applications. ", "page_idx": 8}, {"type": "table", "img_path": "SqW42eR2wC/tmp/d59827b12f0c503078dfb13d64f61cd1cd70d8046f092201812315007f7393eb.jpg", "table_caption": ["Table 3: The proportion of \u201ctoo high\u201d and \u201csudden change\u201d occurrences in drug dosage recommended by RL methods. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "386 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "387 In this paper, we propose offline CT, a novel ICRL algorithm designed to address safety issues   \n388 in healthcare. This method utilizes a causal attention mechanism to observe patients\u2019 historical   \n389 information, similar to the approach taken by actual doctors and employs non-Markovian importance   \n390 weights to effectively capture critical states. To achieve offline learning, we introduce a model-based   \n391 offilne RL for exploratory data augmentation to discover unsafe decisions and train CT. Experiments   \n392 in sepsis and mechanical ventilation demonstrate that our method avoids risky behaviors while   \n393 achieving strategies that closely approximate the lowest mortality rates.   \n394 Limitations. There are also several limitations of offilne CT: (1) Lack of rigorous theoretical analysis:   \n395 We did not precisely define the types of constraint sets, thereby conducting rigorous theoretical   \n396 analysis on constraint sets remains challenging; (2) Need for more computational resources: Due   \n397 to the Transformer architecture, more computational resources are required; (3) Fewer evaluation   \n398 metrics: There is a lack of more medical-specific evaluation metrics in the experimental evaluation   \n399 section; (4) Unrealistic assumptions of expert demonstrations: we assume that expert demonstrations   \n400 are optimal in both constraint satisfaction and reward maximization. However, in reality, this   \n401 assumption may not always hold. Therefore, researching a more effective approach to address the   \n402 aforementioned issues holds promise for the field of secure medical reinforcement learning. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "403 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "404 [1] Stephen Petterson, Robert McNellis, Kathleen Klink, David Meyers, and Andrew Bazemore.   \n405 The state of primary care in the united states: A chartbook of facts and statistics. Washington,   \n406 DC: Robert Graham Center, 2018.   \n407 [2] Estev\u00e3o Bassi, Marcelo Park, Luciano Cesar Pontes Azevedo, et al. Therapeutic strategies for   \n408 high-dose vasopressor-dependent shock. Critical care research and practice, 2013, 2013.   \n409 [3] Thomas Auchet, Marie-Alix Regnier, Nicolas Girerd, and Bruno Levy. Outcome of patients   \n410 with septic shock and high-dose vasopressor therapy. Annals of Intensive Care, 7:1\u20139, 2017.   \n411 [4] Davide Tommaso Andreis and Mervyn Singer. Catecholamines for inflammatory shock: a   \n412 jekyll-and-hyde conundrum. Intensive care medicine, 42:1387\u20131397, 2016.   \n413 [5] Yan Jia, John Burden, Tom Lawton, and Ibrahim Habli. Safe reinforcement learning for sepsis   \n414 treatment. In 2020 IEEE International conference on healthcare informatics (ICHI), pages 1\u20137.   \n415 IEEE, 2020.   \n416 [6] Rui Shi, Olfa Hamzaoui, Nello De Vita, Xavier Monnet, and Jean-Louis Teboul. Vasopressors   \n417 in septic shock: which, when, and how much? Annals of Translational Medicine, 8(12), 2020.   \n418 [7] Claude Martin, Sophie Medam, Francois Antonini, Julie Alingrin, Malik Haddam, Emmanuelle   \n419 Hammad, Bertrand Meyssignac, Coralie Vigne, Laurent Zieleskiewicz, and Marc Leone. Nore  \n420 pinephrine: not too much, too long. Shock, 44(4):305\u2013309, 2015.   \n421 [8] Kristin Lavigne Fadale, Denise Tucker, Jennifer Dungan, and Valerie Sabol. Improving nurses\u2019   \n422 vasopressor titration skills and self-efficacy via simulation-based learning. Clinical Simulation   \n423 in Nursing, 10(6):e291\u2013e299, 2014.   \n424 [9] Yong Huang, Rui Cao, and Amir Rahmani. Reinforcement learning for sepsis treatment: A   \n425 continuous action space solution. In Machine Learning for Healthcare Conference, pages   \n426 631\u2013647. PMLR, 2022.   \n427 [10] Yongshuai Liu, Avishai Halev, and Xin Liu. Policy learning with constraints in model-free   \n428 reinforcement learning: A survey. In The 30th International Joint Conference on Artificial   \n429 Intelligence (IJCAI), 2021.   \n430 [11] Guiliang Liu, Yudong Luo, Ashish Gaurav, Kasra Rezaee, and Pascal Poupart. Benchmarking   \n431 constraint inference in inverse reinforcement learning. arXiv preprint arXiv:2206.09670, 2022.   \n432 [12] Shehryar Malik, Usman Anwar, Alireza Aghasi, and Ali Ahmed. Inverse constrained reinforce  \n433 ment learning. In International conference on machine learning, pages 7390\u20137399. PMLR,   \n434 2021.   \n435 [13] Masaaki Kijima. Markov processes for stochastic modeling. Springer, 2013.   \n436 [14] Zhiyue Zhang, Hongyuan Mei, and Yanxun Xu. Continuous-time decision transformer for   \n437 healthcare applications. In International Conference on Artificial Intelligence and Statistics,   \n438 pages 6245\u20136262. PMLR, 2023.   \n439 [15] Catherine Plaisant, Brett Milash, Anne Rose, Seth Widoff, and Ben Shneiderman. Lifelines:   \n440 visualizing personal histories. In Proceedings of the SIGCHI conference on Human factors in   \n441 computing systems, pages 221\u2013227, 1996.   \n442 [16] Ashish Gaurav, Kasra Rezaee, Guiliang Liu, and Pascal Poupart. Learning soft constraints from   \n443 constrained expert demonstrations. arXiv preprint arXiv:2206.01311, 2022.   \n444 [17] Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. In international   \n445 conference on machine learning, pages 27042\u201327059. PMLR, 2022.   \n446 [18] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter   \n447 Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning   \n448 via sequence modeling. Advances in neural information processing systems, 34:15084\u201315097,   \n449 2021.   \n450 [19] Changyeon Kim, Jongjin Park, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee.   \n451 Preference transformer: Modeling human preferences using transformers for rl. arXiv preprint   \n452 arXiv:2303.00957, 2023.   \n453 [20] Aniruddh Raghu, Matthieu Komorowski, Imran Ahmed, Leo Celi, Peter Szolovits, and Marzyeh   \n454 Ghassemi. Deep reinforcement learning for sepsis treatment. arXiv preprint arXiv:1711.09602,   \n455 2017.   \n456 [21] Xuefeng Peng, Yi Ding, David Wihl, Omer Gottesman, Matthieu Komorowski, H Lehman Li  \n457 wei, Andrew Ross, Aldo Faisal, and Finale Doshi-Velez. Improving sepsis treatment strategies   \n458 by combining deep and kernel-based reinforcement learning. In AMIA Annual Symposium   \n459 Proceedings, volume 2018, page 887. American Medical Informatics Association, 2018.   \n460 [22] Thanh Cong Do, Hyung Jeong Yang, Seok Bong Yoo, and In-Jae Oh. Combining reinforcement   \n461 learning with supervised learning for sepsis treatment. In The 9th International Conference on   \n462 Smart Media and Applications, pages 219\u2013223, 2020.   \n463 [23] Flemming Kondrup, Thomas Jiralerspong, Elaine Lau, Nathan de Lara, Jacob Shkrob, My Duc   \n464 Tran, Doina Precup, and Sumana Basu. Towards safe mechanical ventilation treatment using   \n465 deep offline reinforcement learning. In Proceedings of the AAAI Conference on Artificial   \n466 Intelligence, volume 37, pages 15696\u201315702, 2023.   \n467 [24] Wei Gong, Linxiao Cao, Yifei Zhu, Fang Zuo, Xin He, and Haoquan Zhou. Federated inverse   \n468 reinforcement learning for smart icus with differential privacy. IEEE Internet of Things Journal,   \n469 2023.   \n470 [25] Chao Yu, Guoqi Ren, and Yinzhao Dong. Supervised-actor-critic reinforcement learning for   \n471 intelligent mechanical ventilation and sedative dosing in intensive care units. BMC medical   \n472 informatics and decision making, 20:1\u20138, 2020.   \n473 [26] Niloufar Eghbali, Tuka Alhanai, and Mohammad M Ghassemi. Patient-specific sedation   \n474 management via deep reinforcement learning. Frontiers in Digital Health, 3:608893, 2021.   \n475 [27] Giulia Calvi, Eleonora Manzoni, and Mirco Rampazzo. Reinforcement q-learning for closed  \n476 loop hypnosis depth control in anesthesia. In 2022 30th Mediterranean Conference on Control   \n477 and Automation (MED), pages 164\u2013169. IEEE, 2022.   \n478 [28] Gabriel Schamberg, Marcus Badgeley, Benyamin Meschede-Krasa, Ohyoon Kwon, and   \n479 Emery N Brown. Continuous action deep reinforcement learning for propofol dosing dur  \n480 ing general anesthesia. Artificial Intelligence in Medicine, 123:102227, 2022.   \n481 [29] Glen Chou, Dmitry Berenson, and Necmiye Ozay. Learning constraints from demonstrations. In   \n482 Algorithmic Foundations of Robotics XIII: Proceedings of the 13th Workshop on the Algorithmic   \n483 Foundations of Robotics 13, pages 228\u2013245. Springer, 2020.   \n484 [30] Daehyung Park, Michael Noseworthy, Rohan Paul, Subhro Roy, and Nicholas Roy. Inferring   \n485 task goals and constraints using bayesian nonparametric inverse reinforcement learning. In   \n486 Conference on robot learning, pages 1005\u20131014. PMLR, 2020.   \n487 [31] Dexter RR Scobee and S Shankar Sastry. Maximum likelihood constraint inference for inverse   \n488 reinforcement learning. arXiv preprint arXiv:1909.05477, 2019.   \n489 [32] David L McPherson, Kaylene C Stocking, and S Shankar Sastry. Maximum likelihood constraint   \n490 inference from stochastic demonstrations. In 2021 IEEE Conference on Control Technology   \n491 and Applications (CCTA), pages 1208\u20131213. IEEE, 2021.   \n492 [33] Mattijs Baert, Pietro Mazzaglia, Sam Leroux, and Pieter Simoens. Maximum causal entropy   \n493 inverse constrained reinforcement learning. arXiv preprint arXiv:2305.02857, 2023.   \n494 [34] Brian D Ziebart, J Andrew Bagnell, and Anind K Dey. Modeling interaction via the principle of   \n495 maximum causal entropy. 2010.   \n496 [35] Guanren Qiao, Guiliang Liu, Pascal Poupart, and Zhiqiang Xu. Multi-modal inverse constrained   \n497 reinforcement learning from a mixture of demonstrations. Advances in Neural Information   \n498 Processing Systems, 36, 2024.   \n499 [36] Sheng Xu and Guiliang Liu. Uncertainty-aware constraint inference in inverse constrained   \n500 reinforcement learning. In The Twelfth International Conference on Learning Representations,   \n501 2023.   \n502 [37] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big   \n503 sequence modeling problem. Advances in neural information processing systems, 34:1273\u2013   \n504 1286, 2021.   \n505 [38] Zuxin Liu, Zijian Guo, Yihang Yao, Zhepeng Cen, Wenhao Yu, Tingnan Zhang, and Ding   \n506 Zhao. Constrained decision transformer for offilne safe reinforcement learning. arXiv preprint   \n507 arXiv:2302.07351, 2023.   \n508 [39] Eitan Altman. Constrained markov decision processes with total cost criteria: Lagrangian   \n509 approach and dual linear program. Mathematical methods of operations research, 48:387\u2013417,   \n510 1998.   \n511 [40] Aniruddh Raghu, Matthieu Komorowski, Leo Anthony Celi, Peter Szolovits, and Marzyeh   \n512 Ghassemi. Continuous state-space models for optimal sepsis treatment: a deep reinforcement   \n513 learning approach. In Machine Learning for Healthcare Conference, pages 147\u2013163. PMLR,   \n514 2017.   \n515 [41] Mehdi Fatemi, Taylor W Killian, Jayakumar Subramanian, and Marzyeh Ghassemi. Medi  \n516 cal dead-ends and learning to identify high-risk states and treatments. Advances in Neural   \n517 Information Processing Systems, 34:4856\u20134870, 2021.   \n518 [42] Matthieu Komorowski, Leo A Celi, Omar Badawi, Anthony C Gordon, and A Aldo Faisal. The   \n519 artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care.   \n520 Nature medicine, 24(11):1716\u20131720, 2018.   \n521 [43] Jongmin Lee, Cosmin Paduraru, Daniel J Mankowitz, Nicolas Heess, Doina Precup, Kee-Eung   \n522 Kim, and Arthur Guez. Coptidice: Offline constrained reinforcement learning via stationary   \n523 distribution correction estimation. arXiv preprint arXiv:2204.08957, 2022.   \n524 [44] Jiayi Guan, Guang Chen, Jiaming Ji, Long Yang, Zhijun Li, et al. Voce: Variational optimization   \n525 with conservative estimation for offline safe reinforcement learning. Advances in Neural   \n526 Information Processing Systems, 36, 2024.   \n527 [45] Yongju Kim, Hyung Keun Park, Jaimyun Jung, Peyman Asghari-Rad, Seungchul Lee, Jin You   \n528 Kim, Hwan Gyo Jung, and Hyoung Seop Kim. Exploration of optimal microstructure and   \n529 mechanical properties in continuous microstructure space using a variational autoencoder.   \n530 Materials & Design, 202:109544, 2021.   \n531 [46] Tim Hsu, William K Epting, Hokon Kim, Harry W Abernathy, Gregory A Hackett, Anthony D   \n532 Rollett, Paul A Salvador, and Elizabeth A Holm. Microstructure generation via generative   \n533 adversarial network for heterogeneous, topologically complex 3d materials. Jom, 73:90\u2013102,   \n534 2021.   \n535 [47] Akshay Iyer, Biswadip Dey, Arindam Dasgupta, Wei Chen, and Amit Chakraborty. A conditional   \n536 generative model for predicting material microstructures from processing methods. arXiv   \n537 preprint arXiv:1910.02133, 2019.   \n538 [48] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion   \n539 models in vision: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence,   \n540 2023.   \n541 [49] Christian D\u00fcreth, Paul Seibert, Dennis R\u00fccker, Stephanie Handford, Markus K\u00e4stner, and   \n542 Maik Gude. Conditional diffusion-based microstructure reconstruction. Materials Today   \n543 Communications, 35:105608, 2023.   \n544 [50] Arne Peine, Ahmed Hallawa, Johannes Bickenbach, Guido Dartmann, Lejla Begic Fazlic, Anke   \n545 Schmeink, Gerd Ascheid, Christoph Thiemermann, Andreas Schuppert, Ryan Kindle, et al.   \n546 Development and validation of a reinforcement learning algorithm to dynamically optimize   \n547 mechanical ventilation in critical care. NPJ digital medicine, 4(1):32, 2021.   \n548 [51] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad   \n549 Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii,   \n550 a freely accessible critical care database. Scientific data, 3(1):1\u20139, 2016.   \n551 [52] Yonglin Li, Chunjiang Yan, Ziyan Gan, Xiaotu Xi, Zhanpeng Tan, Jun Li, and Guowei Li.   \n552 Prognostic values of sofa score, qsofa score, and lods score for patients with sepsis. Annals of   \n553 palliative medicine, 9(3):1037044\u20131031044, 2020.   \n554 [53] Seung Mok Ryoo, JungBok Lee, Yoon-Seon Lee, Jae Ho Lee, Kyoung Soo Lim, Jin Won   \n555 Huh, Sang-Bum Hong, Chae-Man Lim, Younsuck Koh, and Won Young Kim. Lactate level   \n556 versus lactate clearance for predicting mortality in patients with septic shock defined by sepsis-3.   \n557 Critical care medicine, 46(6):e489\u2013e495, 2018.   \n558 [54] Nishant Raj Pandey, Yu-yao Bian, and Song-tao Shou. Significance of blood pressure variability   \n559 in patients with sepsis. World journal of emergency medicine, 5(1):42, 2014.   \n560 [55] Marta Carrara, Bernardo Bollen Pinto, Giuseppe Baselli, Karim Bendjelid, and Manuela Ferrario.   \n561 Baroreflex sensitivity and blood pressure variability can help in understanding the different   \n562 response to therapy during acute phase of septic shock. Shock, 50(1):78\u201386, 2018.   \n563 [56] Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In   \n564 International Conference on Machine Learning, pages 3703\u20133712. PMLR, 2019.   \n565 [57] Mervyn Singer, Clifford S Deutschman, Christopher Warren Seymour, Manu Shankar-Hari,   \n566 Djillali Annane, Michael Bauer, Rinaldo Bellomo, Gordon R Bernard, Jean-Daniel Chiche,   \n567 Craig M Coopersmith, et al. The third international consensus definitions for sepsis and septic   \n568 shock (sepsis-3). Jama, 315(8):801\u2013810, 2016.   \n569 [58] Flavio Lopes Ferreira, Daliana Peres Bota, Annette Bross, Christian M\u00e9lot, and Jean-Louis   \n570 Vincent. Serial evaluation of the sofa score to predict outcome in critically ill patients. Jama,   \n571 286(14):1754\u20131758, 2001.   \n573 We base our design on prior knowledge that intravenous (IV) intake exceeding $2000m L/4h$ or   \n574 vasopressor (Vaso) dosage surpassing $1g/(k g\\!\\cdot\\!m i n)$ is generally considered unsafe in sepsis treatment   \n575 [6]. To design a reasonable constraint function, we refer to the constraint function designed by Liu et   \n576 al. in the Bullet safety gym environments[38]. We define the cost function as shown in Equation 9.   \n577 Thus, during the treatment of sepsis, if the agent exceeds the maximum dosage thresholds of the two   \n578 medications, it incurs a cost due to constraint violation. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\nc\\left(s,a\\right)\\mathrm{\\,=}\\,\\mathbf{1}\\left(a_{I V}\\mathrm{\\,>}\\,a_{I V\\mathrm{\\,\\max}}\\right)+\\mathbf{1}\\left(a_{V a s o}\\mathrm{\\,>}\\,a_{V a s o\\mathrm{\\,\\max}}\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "579 where, $s$ and $a$ represent the patient\u2019s state and action, respectively. $a_{I V\\mathrm{~\\max}}=2000$ indicates that   \n580 the maximum fluid intake through IV is $2000m L$ , and $a_{V a s o\\ m a x}=1$ signifies that the maximum   \n581 Vaso dosage is $1\\mu g/(k g\\cdot m i n)$ .   \n582 We applied our custom constraint function in the CDT [38] method, and the results are shown in   \n583 Figure 9. Compared to the Vaso dosage recommended by doctors, our strategy exhibits excessive   \n584 suppression of the Vaso. The maximum dosage of Vaso is $0.0011\\mu g/(k g\\cdot m i n)$ , which is minimal   \n585 and insufficient to provide the patient with effective therapeutic effects.   \n586 Therefore, Equation 9 is not suitable. The primary issues may include uniform constraint strength   \n587 for excessive drug dosages, for instance, the cost for IV exceeding $2000~\\mathrm{mL}$ and IV exceeding   \n588 $3000\\;\\mathrm{mL}$ is the same at 1; lack of generalization, where the constraint cost does not vary with the   \n589 patient\u2019s tolerance. If a patient has an intolerance to VASO, the maximum value for VASO maybe 0,   \n590 which cannot be captured by the self-imposed constraint function. Moreover, it lacks generalization,   \n591 requiring redesign of the constraint function when addressing other unsafe medical issues; and it\u2019s   \nessential to ensure the correctness of the underlying medical knowledge premises. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "SqW42eR2wC/tmp/bf62ccbfec31b86f172ad8f189f91c2e7f07940ebfc33bdc54c4672b9ffbe70d.jpg", "img_caption": ["Figure 9: Drug dosage distribution under custom constraint functions in sepsis. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "592 ", "page_idx": 13}, {"type": "text", "text": "593 B Experiment Supplement ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "594 B.1 Sepsis Problem Define ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "595 Our definition is similar to [40]. We extract data from adult patients meeting the criteria for sepsis-3   \n596 criteria [57] and collect their data within the first 72 hours of admission.   \n597 State Space. We use a 4-hour window and select 48 patient indicators as the state for a one-time unit   \n598 of the patient. The state indicators include Demographics/Static, Lab Values, Vital Signs, and Intake   \n599 and Output Events, detailed as follows [40]: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "\u2022 Demographics/Static: Shock Index, Elixhauser, SIRS, Gender, Re-admission, GCS - Glasgow Coma Scale, SOFA - Sequential Organ Failure Assessment, Age ", "page_idx": 13}, {"type": "text", "text": "\u2022 Lab Values Albumin: Arterial pH, Calcium, Glucose, Hemoglobin, Magnesium, PTT - Partial Thromboplastin Time, Potassium, SGPT - Serum Glutamic-Pyruvic Transaminase, Arterial Blood Gas, BUN Blood Urea Nitrogen, Chloride, Bicarbonate, INR - International Normalized Ratio, Sodium, Arterial Lactate, CO2, Creatinine, Ionised Calcium, PT - Prothrombin Time, Platelets Count, SGOT Serum Glutamic-Oxaloacetic Transaminase, Total bilirubin, White Blood Cell Count   \n\u2022 Vital Signs: Diastolic Blood Pressure, Systolic Blood Pressure, Mean Blood Pressure, PaCO2, PaO2, FiO2, PaO/FiO2 ratio, Respiratory Rate, Temperature (Celsius), Weight (kg), Heart Rate, SpO2   \n\u2022 Intake and Output Events: Fluid Output - 4 hourly period, Total Fluid Output, Mechanical Ventilation ", "page_idx": 14}, {"type": "text", "text": "613 Action Space. Regarding the treatment of sepsis, there are two main types of medications: in  \n614 travenous fluids and vasopressors. We select the total amount of intravenous fluids for each time   \n615 unit and the maximum dose of vasopressors as the two dimensions of the action space, defined as   \n616 (sum(IV), max (Vaso)). Each dimension is a continuous value greater than 0. ", "page_idx": 14}, {"type": "text", "text": "617 Reward Function. We refer to the reward function used in [9], as shown in the following equation: ", "page_idx": 14}, {"type": "equation", "text": "$$\nr\\left(s_{t},s_{t+1}\\right)=\\lambda_{1}\\operatorname{tanh}\\left(s_{t}^{\\mathrm{SOFA}}-6\\right)+\\lambda_{2}\\left(s_{t+1}^{\\mathrm{SOFA}}-s_{t}^{\\mathrm{SOFA}}\\right))\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "618 Where $\\lambda_{0}$ and $\\lambda_{1}$ are hyperparameters set to $-0.25$ and $-0.2$ , respectively. This reward function is   \n619 designed based on the SOFA score, as it is a key indicator of the health status for sepsis patients and   \n620 widely used in clinical settings. The formula describes a penalty when the SOFA score increases and   \n621 a reward when the SOFA score decreases. We set 6 as the cutoff value because the mortality rate   \n622 sharply increases when the SOFA score exceeds 6 [58]. ", "page_idx": 14}, {"type": "text", "text": "623 B.2 Mechanical Ventilation Treatment Problem Define ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "624 The RL problem definition for Mechanical Ventilation Treatment is referenced from [23]. ", "page_idx": 14}, {"type": "text", "text": "625 State Space. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "626   \n627   \n628   \n629   \n630   \n631   \n632   \n633 ", "page_idx": 14}, {"type": "text", "text": "\u2022 Demographics/Static: Elixhauser, SIRS, Gender, Re-admission, GCS, SOFA, Age   \n\u2022 Lab Values Albumin: Arterial pH, Glucose, Hemoglobin, Magnesium, PTT, BUN Blood Urea Nitrogen, Chloride, Bicarbonate, INR, Sodium, Arterial Lactate, CO2, Creatinine, Ionised Calcium, PT, Platelets Count, White Blood Cell Count, Hb   \n\u2022 Vital Signs: Diastolic Blood Pressure, Systolic Blood Pressure, Mean Blood Pressure, Temperature, Weight (kg), Heart Rate, SpO2   \n\u2022 Intake and Output Events: Urine output, vasopressors, intravenous fluids, cumulative fluid balance ", "page_idx": 14}, {"type": "text", "text": "634 Action Space. The action space mainly consists of Positive End Expiratory Pressure (PEEP) and 635 Fraction of Inspired Oxygen (FiO2), which are crucial parameters in ventilator settings. Here, we 636 consider a discrete space configuration, with each parameter divided into 7 intervals. Therefore, our action space is $7\\times7$ , depicted as 4. ", "page_idx": 14}, {"type": "table", "img_path": "SqW42eR2wC/tmp/8bfac518eb6a88ed636a8c4e6d6c5bc4c5e9f0ccaad262e44a147bf3e86d2b46.jpg", "table_caption": ["Table 4: The action space of the mechanical ventilator. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "637 ", "page_idx": 14}, {"type": "text", "text": "638 Reward Function. The primary objective of setting respiratory parameters is to ensure the patient\u2019s   \n639 survival. We adopt the same reward function design as the work [23], defined as Equation 11. This   \n640 reward function first considers the terminal reward: if the patient dies, the reward $r$ is set to $-1$ ;   \n641 otherwise, it is $+1$ in the terminal state. Additionally, to provide more frequent rewards, intermediate   \n642 rewards are considered. Intermediate rewards mainly focus on the Apache II score, which evaluates   \n643 various parameters to describe the patient\u2019s health status. This reward function utilizes the increase or   \n644 decrease in this score to reward the agent. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\nr\\left(s_{t},a_{t},s_{t+1}\\right)=\\left\\{\\begin{array}{l l}{+1}&{\\mathrm{~if~}t=T\\mathrm{~and~}m_{t}=1}\\\\ {-1}&{\\mathrm{~if~}t=T\\mathrm{~and~}m_{t}=0}\\\\ {\\frac{\\left(A_{t+1}-A_{t}\\right)}{\\operatorname*{max}_{A}-\\operatorname*{min}_{A}}}&{\\mathrm{~otherwise~}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "645 In Equation 11, $T$ represents the length of the patient\u2019s trajectory, $m$ indicates whether the patient   \n646 ultimately dies, $A$ denotes the Apache $\\mathrm{II}$ score, and $\\mathrm{max}_{A}$ and $\\operatorname*{min}_{A}$ respectively denote the maximum   \n647 and minimum values. ", "page_idx": 15}, {"type": "text", "text": "648 B.3 The Evaluation of Model-based Offline RL ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "649 Generating data within a reasonable range. To validate model-based offline RL, we first check   \n650 whether the values it produces fall within the legal range. The results are depicted in Figure 10. After   \n651 analyzing the generated data, we find that the majority of state values have a probability of over $99\\%$   \n652 of being within the legal range. A few values related to gender and re-admission range between $60\\%$   \n653 and $70\\%$ . This could be due to these two indicators having limited correlation with other metrics,   \nmaking them more challenging for the model to assess. ", "page_idx": 15}, {"type": "image", "img_path": "SqW42eR2wC/tmp/08c8f9210a5a4105fce1cf31b149ad1be9b5590243445fdf396533e38ec6df23.jpg", "img_caption": ["Figure 10: The accuracy of predicting different state values within the legal range. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "SqW42eR2wC/tmp/18293cd75e8b9581e0a955a18b87ebaacad8577f75db199fb6fa811f498f7578.jpg", "img_caption": ["Figure 11: The relationship between average prediction error and trajectory length. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "654 ", "page_idx": 15}, {"type": "text", "text": "655 Generating violating data. In addition, we evaluate the violating actions generated by the model, as   \n656 shown in Figure 12. When compared with expert strategies and penalty distributions, we find that the   \n657 actions generated by the model mostly fall within the legal range. However, it occasionally produces   \n658 behaviors that are inappropriate for the current state, constituting violating data. This indicates that   \nour generative model can produce legally violating data. ", "page_idx": 15}, {"type": "image", "img_path": "SqW42eR2wC/tmp/fe33a17201f141c1358ef83e091952448a544fa4cbcfdedfd0b1cdbad5192d71.jpg", "img_caption": ["Figure 12: The distribution and penalty values of violating data and expert data. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "659 ", "page_idx": 15}, {"type": "text", "text": "660 The length of a trajectory. Regarding the selection of trajectory length, we consider the relationship   \n661 between the average prediction error, the error of the last point in the trajectory, and the trajectory   \n662 length. We use the model-based offline RL to generate trajectories and compare them with expert   \n663 data using the Euclidean distance to measure their differences. We evaluate the average error and   \n664 the error of the last point in the trajectory, as shown in Figure 11. We observe that with an increase   \n665 in trajectory length, the average prediction error at each time step decreases, while the state error   \n666 stabilizes. Taking into account the observation length and prediction accuracy, we ultimately choose   \n667 to generate trajectories with lengths ranging from 10 to 15. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "668 B.4 The Evaluation of Cost function in Sepsis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "669 To validate that the CT method captures key states, we conduct statistical analysis on the relationship   \n670 between state values and penalty values. We collect penalty values under different state values   \n671 for all patients, and the complete information is shown in Figure 13. We find that the CT method   \n672 successfully captures unsafe states and imposes higher penalties accordingly. The safe range of state   \n673 values is shown in Table 5.   \n674 To validate the role of the attention layer in capturing states in CT, we conducted tests, and the   \n675 experimental results are presented in Figure 14 and 13. We found that the attention layer plays a   \n676 crucial role in state capture. For instance, in the case of an increase in the SOFA score, without the   \n677 attention layer, this increase cannot be captured, while with the attention layer, it clearly captures the   \n678 change. Thus, this indicates that SOFA, as a key diagnostic indicator of sepsis, with the help of the   \nattention layer, CT can accurately capture its changes. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "table", "img_path": "SqW42eR2wC/tmp/05cb4d8a02994e873b2bbd6d7b24d625c7fe33e366b36ea33574a627cf93f94e.jpg", "table_caption": ["Table 5: State indicators and their normal ranges. "], "table_footnote": ["$\\uparrow$ indicates higher values are more normal, while $\\downarrow$ indicates lower values are more normal. The maximum value for GCS is 15. The minimum value for SIRS, SOFA, and Shock_Index is 0. "], "page_idx": 16}, {"type": "text", "text": "680 B.5 Experimental Settings ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "681 To train the $\\mathrm{CRL+CT}$ model, we use a total of 3 NVIDIA GeForce RTX 3090 GPUs, each with   \n682 24GB of memory. Training a $\\mathrm{CRL+CT}$ model typically takes 5-6 hours. We employ 5 random seeds   \n683 for validation. We use the Adam optimization algorithm to optimize all our networks, updating the   \n684 learning rate using a decay factor parameterization at each iteration. The main hyperparameters are   \n685 summarized in Table 6 and 7. ", "page_idx": 16}, {"type": "image", "img_path": "SqW42eR2wC/tmp/12d536546cc1a5adbfeafe404a9bdd61a25b0e38daff53e50a75cc857ccdb1ef.jpg", "img_caption": ["Figure 13: The relationship between all states and cost values "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "SqW42eR2wC/tmp/fca151a5db2b6593009f27d2633489e7fc9e7916c26bca144d5e2adef5e61c5c.jpg", "img_caption": ["Figure 14: The performance contrast between CT with and without an attention layer. The blue line represents the absence of an attention layer, while the green line indicates the presence of an attention layer. "], "img_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "SqW42eR2wC/tmp/1872854aa01bd4005f6b84843ef80d4c34485b529b659370ad40c1ed0bc6c709.jpg", "table_caption": ["Table 6: List of the utilized hyperparameters in CT. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "SqW42eR2wC/tmp/2627dff6a23cbd48694f94f54196e0ca76f15c4808859375d3165dec175c4456.jpg", "table_caption": ["Table 7: List of the utilized hyperparameters in CRL "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "686 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "687 1. Claims   \n688 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n689 paper\u2019s contributions and scope?   \n690 Answer: [Yes]   \n691 Justification: In the abstract and introduction, we delineate the main motivations and   \n692 contributions of this paper and its application in the field of safe reinforcement learning in   \n693 healthcare.   \n694 Guidelines:   \n695 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n696 made in the paper.   \n697 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n698 contributions made in the paper and important assumptions and limitations. A No or   \n699 NA answer to this question will not be perceived well by the reviewers.   \n700 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n701 much the results can be expected to generalize to other settings.   \n702 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n703 are not attained by the paper.   \n704 2. Limitations   \n705 Question: Does the paper discuss the limitations of the work performed by the authors?   \n706 Answer: [Yes]   \n707 Justification: In the final section, this paper discusses the limitations of the method.   \n708 Guidelines:   \n709 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n710 the paper has limitations, but those are not discussed in the paper.   \n711 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n712 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n713 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n714 model well-specification, asymptotic approximations only holding locally). The authors   \n715 should reflect on how these assumptions might be violated in practice and what the   \n716 implications would be.   \n717 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n718 only tested on a few datasets or with a few runs. In general, empirical results often   \n719 depend on implicit assumptions, which should be articulated.   \n720 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n721 For example, a facial recognition algorithm may perform poorly when image resolution   \n722 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n723 used reliably to provide closed captions for online lectures because it fails to handle   \n724 technical jargon.   \n725 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n726 and how they scale with dataset size.   \n727 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n728 address problems of privacy and fairness.   \n729 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n730 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n731 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n732 judgment and recognize that individual actions in favor of transparency play an impor  \n733 tant role in developing norms that preserve the integrity of the community. Reviewers   \n734 will be specifically instructed to not penalize honesty concerning limitations.   \n735 3. Theory Assumptions and Proofs   \n736 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n737 a complete (and correct) proof?   \n42 \u2022 The answer NA means that the paper does not include theoretical results.   \n43 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n44 referenced.   \n45 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n46 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n47 they appear in the supplemental material, the authors are encouraged to provide a short   \n48 proof sketch to provide intuition.   \n49 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n50 by formal proofs provided in appendix or supplemental material.   \n51 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "752 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "753 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n754 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n755 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Justification: Our approach is reproducible, and our code can be made publicly available after the paper is published, including the relevant data processing procedures. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "791 5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "792 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n793 tions to faithfully reproduce the main experimental results, as described in supplemental   \n794 material?   \n795 Answer: [Yes]   \n796 Justification: Our code can be made publicly available after the paper is published.   \n797 Guidelines:   \n798 \u2022 The answer NA means that paper does not include experiments requiring code.   \n799 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n800 public/guides/CodeSubmissionPolicy) for more details.   \n801 \u2022 While we encourage the release of code and data, we understand that this might not be   \n802 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n803 including code, unless this is central to the contribution (e.g., for a new open-source   \n804 benchmark).   \n805 \u2022 The instructions should contain the exact command and environment needed to run to   \n806 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n807 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n808 \u2022 The authors should provide instructions on data access and preparation, including how   \n809 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n810 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n811 proposed method and baselines. If only a subset of experiments are reproducible, they   \n812 should state which ones are omitted from the script and why.   \n813 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n814 versions (if applicable).   \n815 \u2022 Providing as much information as possible in supplemental material (appended to the   \n816 paper) is recommended, but including URLs to data and code is permitted.   \n817 6. Experimental Setting/Details   \n818 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n819 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n820 results?   \n821 Answer: [Yes]   \n822 Justification: We provided a detailed description of the experimental setup and metrics.   \n823 Guidelines:   \n824 \u2022 The answer NA means that the paper does not include experiments.   \n825 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n826 that is necessary to appreciate the results and make sense of them.   \n827 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n828 material.   \n829 7. Experiment Statistical Significance   \n830 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n831 information about the statistical significance of the experiments?   \n832 Answer: [Yes]   \n833 Justification: We tested our method with multiple random seeds and calculated the standard   \n834 error.   \n835 Guidelines:   \n836 \u2022 The answer NA means that the paper does not include experiments.   \n837 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n838 dence intervals, or statistical significance tests, at least for the experiments that support   \n839 the main claims of the paper.   \n840 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n841 example, train/test split, initialization, random drawing of some parameter, or overall   \n842 run with given experimental conditions).   \n843 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n844 call to a library function, bootstrap, etc.)   \n845 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n846 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n847 of the mean.   \n848 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n849 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n850 of Normality of errors is not verified.   \n851 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n852 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n853 error rates).   \n854 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n855 they were calculated and reference the corresponding figures or tables in the text.   \n856 8. Experiments Compute Resources   \n857 Question: For each experiment, does the paper provide sufficient information on the com  \n858 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n859 the experiments?   \n860 Answer: [Yes]   \n861 Justification: We explain the required computational resources and related information in   \n862 the appendix.   \n863 Guidelines:   \n864 \u2022 The answer NA means that the paper does not include experiments.   \n865 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n866 or cloud provider, including relevant memory and storage.   \n867 \u2022 The paper should provide the amount of compute required for each of the individual   \n868 experimental runs as well as estimate the total compute.   \n869 \u2022 The paper should disclose whether the full research project required more compute   \n870 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n871 didn\u2019t make it into the paper).   \n872 9. Code Of Ethics   \n873 Question: Does the research conducted in the paper conform, in every respect, with the   \n874 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n875 Answer: [Yes]   \n876 Justification: Although our work is related to healthcare, we train and test our models on   \n877 offline data, adhering to ethical standards.   \n878 Guidelines:   \n879 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n880 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n881 deviation from the Code of Ethics.   \n882 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n883 eration due to laws or regulations in their jurisdiction).   \n884 10. Broader Impacts   \n885 Question: Does the paper discuss both potential positive societal impacts and negative   \n886 societal impacts of the work performed?   \n887 Answer:[Yes]   \n888 Justification: Our work has a positive impact on safe healthcare, promoting the expansion of   \n889 artificial intelligence technology into the medical field.   \n890 Guidelines:   \n891 \u2022 The answer NA means that there is no societal impact of the work performed.   \n892 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n893 impact or why the paper does not address societal impact.   \n894 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n895 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n896 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n897 groups), privacy considerations, and security considerations.   \n898 \u2022 The conference expects that many papers will be foundational research and not tied   \n899 to particular applications, let alone deployments. However, if there is a direct path to   \n900 any negative applications, the authors should point it out. For example, it is legitimate   \n901 to point out that an improvement in the quality of generative models could be used to   \n902 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n903 that a generic algorithm for optimizing neural networks could enable people to train   \n904 models that generate Deepfakes faster.   \n905 \u2022 The authors should consider possible harms that could arise when the technology is   \n906 being used as intended and functioning correctly, harms that could arise when the   \n907 technology is being used as intended but gives incorrect results, and harms following   \n908 from (intentional or unintentional) misuse of the technology.   \n909 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n910 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n911 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n912 feedback over time, improving the efficiency and accessibility of ML).   \n913 11. Safeguards   \n914 Question: Does the paper describe safeguards that have been put in place for responsible   \n915 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n916 image generators, or scraped datasets)?   \n917 Answer: [NA]   \n918 Justification: Our work does not pose security risks because it is based on publicly available   \n919 datasets and models.   \n920 Guidelines:   \n921 \u2022 The answer NA means that the paper poses no such risks.   \n922 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n923 necessary safeguards to allow for controlled use of the model, for example by requiring   \n924 that users adhere to usage guidelines or restrictions to access the model or implementing   \n925 safety filters.   \n926 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n927 should describe how they avoided releasing unsafe images.   \n928 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n929 not require this, but we encourage authors to take this into account and make a best   \n930 faith effort.   \n931 12. Licenses for existing assets   \n932 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n933 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n934 properly respected?   \n935 Answer: [Yes]   \n936 Justification: The code, data, and models we referenced are all cited, and we followed the   \n937 licenses and terms of use throughout the process.   \n938 Guidelines:   \n939 \u2022 The answer NA means that the paper does not use existing assets.   \n940 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n941 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n942 URL.   \n943 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n944 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n945 service of that source should be provided.   \n946 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n947 package should be provided. For popular datasets, paperswithcode.com/datasets   \n948 has curated licenses for some datasets. Their licensing guide can help determine the   \n949 license of a dataset.   \n950 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n951 the derived asset (if it has changed) should be provided.   \n952 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n953 the asset\u2019s creators.   \n954 13. New Assets   \n955 Question: Are new assets introduced in the paper well documented and is the documentation   \n956 provided alongside the assets?   \n957 Answer: [Yes]   \n958 Justification: We will provide detailed data extraction code and model code as part of the   \n959 submission files.   \n960 Guidelines:   \n961 \u2022 The answer NA means that the paper does not release new assets.   \n962 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n963 submissions via structured templates. This includes details about training, license,   \n964 limitations, etc.   \n965 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n966 asset is used.   \n967 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n968 create an anonymized URL or include an anonymized zip file.   \n969 14. Crowdsourcing and Research with Human Subjects   \n970 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n971 include the full text of instructions given to participants and screenshots, if applicable, as   \n972 well as details about compensation (if any)?   \n973 Answer: [NA]   \n974 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n975 Guidelines:   \n976 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n977 human subjects.   \n978 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n979 tion of the paper involves human subjects, then as much detail as possible should be   \n980 included in the main paper.   \n981 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n982 or other labor should be paid at least the minimum wage in the country of the data   \n983 collector.   \n984 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n985 Subjects   \n986 Question: Does the paper describe potential risks incurred by study participants, whether   \n987 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n988 approvals (or an equivalent approval/review based on the requirements of your country or   \n989 institution) were obtained?   \n990 Answer: [NA]   \n991 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n992 Guidelines:   \n993 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n994 human subjects.   \n995 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n996 may be required for any human subjects research. If you obtained IRB approval, you   \n997 should clearly state this in the paper. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]