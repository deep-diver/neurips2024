[{"figure_path": "2vywag2lVC/figures/figures_6_1.jpg", "caption": "Figure 1: Plot of the exponents of e\u22121 in the cases of Table 1.", "description": "This figure is a plot showing the exponents of epsilon inverse in the sample complexity results of the C-PG algorithm. It illustrates how the sample complexity scales differently depending on whether the gradients are exact or estimated and the value of psi (\u03c8). The plot helps visualize the trade-off between computational cost and convergence rate.", "section": "3 Last-Iterate Global Convergence of C-PG"}, {"figure_path": "2vywag2lVC/figures/figures_9_1.jpg", "caption": "Figure 2: Average return and cost in CostLQR and DGWW environments (5 runs, mean \u00b1 95% C.I.).", "description": "This figure presents the results of the experiments conducted in two different environments: the Discrete Grid World with Walls (DGWW) and the Linear Quadratic Regulator with costs (CostLQR).  The plots show the average trajectory return and the average trajectory cost over the number of trajectories observed during learning. Two separate subfigures display results for DGWW and CostLQR, each with multiple lines representing different algorithms. The shaded regions show the 95% confidence intervals. The figure highlights the performance of C-PGAE and C-PGPE in comparison to other algorithms (NPG-PD, RPG-PD, NPG-PD2, RPG-PD2).", "section": "5 Numerical Validation"}, {"figure_path": "2vywag2lVC/figures/figures_9_2.jpg", "caption": "Figure 3: Cost distributions with (hyper)policies learned considering different risk measures (5 runs).", "description": "The figure shows the empirical distributions of costs over 100 trajectories of the learned (hyper)policies via C-PGPE and C-PGAE for the Swimmer-v4 environment.  Each subplot represents a different risk measure (average cost, CVaR, MV, and chance) used as a constraint during training. The table shows average return for each algorithm and risk measure.  The results illustrate how different risk measures affect the cost distribution and the overall performance of the learned policies.", "section": "Risk constraints on Swimmer"}, {"figure_path": "2vywag2lVC/figures/figures_14_1.jpg", "caption": "Figure 2: Average return and cost in CostLQR and DGWW environments (5 runs, mean \u00b1 95% C.I.)", "description": "This figure presents the results of applying the proposed C-PGAE algorithm and comparing it with other baselines (NPG-PD, RPG-PD, NPG-PD2, RPG-PD2) on two different environments: Discrete Grid World with Walls (DGWW) and Linear Quadratic Regulator with Costs (CostLQR).  The plots show the average trajectory return and the average trajectory cost across 5 independent runs, with 95% confidence intervals displayed.  The DGWW results demonstrate the learning performance of tabular softmax policies on a discrete environment. The CostLQR results demonstrate the performance of continuous gaussian policies on a continuous environment. In both cases, C-PGAE is shown to achieve comparable performance with fewer trajectories.", "section": "5 Numerical Validation"}, {"figure_path": "2vywag2lVC/figures/figures_43_1.jpg", "caption": "Figure 3: Cost distributions with (hyper)policies learned considering different risk measures (5 runs).", "description": "The figure shows the empirical distributions of costs over 100 trajectories of the learned (hyper)policies via C-PGPE and C-PGAE. This experiment considers the cost-based version of the Swimmer-v4 MuJoCo (Todorov et al., 2012) environment, with a single constraint over the actions (see Appendix H), for which we set b = 50. The experimental results show that C-PGPE learns a hyperpolicy paying less cost when using risk measures compared to average cost, with the smallest costs attained by CVaR. C-PGAE shows similar results, although the difference between CVaR or the Chance constraints and average cost constraints are not very significant. Notice that, the minimum amount of cost is obtained using MV constraints even if the learned policy exhibits poor performances (Table 3c). In all the other cases, both C-PGPE and C-PGAE learns (hyper)policies exhibiting similar performance scores.", "section": "Risk constraints on Swimmer"}, {"figure_path": "2vywag2lVC/figures/figures_44_1.jpg", "caption": "Figure 6: Lagrangian, performance and cost curves for C-PGPE over CostLQR with regularization values \u03c9 \u2208 {0, 10\u207b\u2074, 10\u207b\u00b2} (5 runs, mean \u00b1 95% C.I.).", "description": "This figure displays the Lagrangian, performance (average trajectory return), and cost (average trajectory cost) curves for the C-PGPE algorithm over the CostLQR environment.  The experiment compares the effect of different regularization values (\u03c9 \u2208 {0, 10\u207b\u2074, 10\u207b\u00b2}) on the algorithm's convergence behavior and its ability to meet the specified cost constraint.  Each curve represents the average over 5 runs, with 95% confidence intervals shown.", "section": "H.6 Regularization Sensitivity Study"}, {"figure_path": "2vywag2lVC/figures/figures_44_2.jpg", "caption": "Figure 2: Average return and cost in CostLQR and DGWW environments (5 runs, mean \u00b1 95% C.I.)", "description": "This figure shows the comparison results of the proposed algorithms (C-PGAE and C-PGPE) against state-of-the-art baselines in two different environments: a discrete grid world with walls (DGWW) and a continuous linear quadratic regulator with costs (CostLQR).  The plots display the average trajectory return and the average trajectory cost over multiple independent runs, with 95% confidence intervals shown. The results illustrate the effectiveness of the proposed algorithms in achieving better performance in terms of both return and cost compared to baselines, particularly in the CostLQR environment. Note that the y-axis is in log scale for better visualization.", "section": "5 Numerical Validation"}, {"figure_path": "2vywag2lVC/figures/figures_45_1.jpg", "caption": "Figure 6: Lagrangian, performance and cost curves for C-PGPE over CostLQR with regularization values w \u2208 {0, 10\u22124, 10\u22122} (5 runs, mean \u00b1 95% C.I.).", "description": "This figure shows the results of the regularization sensitivity study of the C-PGPE algorithm on the CostLQR environment. Three different regularization values (\u03c9 = 0, \u03c9 = 0.0001, \u03c9 = 0.01) were tested, and the plots show the average Lagrangian value, average trajectory return, and average trajectory cost over 10000 iterations.  Each plot shows the mean and 95% confidence interval across 5 runs.", "section": "H.6 Regularization Sensitivity Study"}, {"figure_path": "2vywag2lVC/figures/figures_45_2.jpg", "caption": "Figure 2: Average return and cost in CostLQR and DGWW environments (5 runs, mean \u00b195% C.I.).", "description": "This figure presents the results of two experiments conducted to compare the performance of the proposed algorithms (C-PGAE and C-PGPE) against state-of-the-art baselines on two different environments.  The first experiment uses a discrete grid world with walls (DGWW), and the second experiment employs a continuous linear quadratic regulator with costs (CostLQR).  Both experiments involve a single constraint on the average trajectory cost. The plots show the average trajectory return and average trajectory cost over the course of training, demonstrating the effectiveness of the proposed algorithms in achieving the desired balance between maximizing return and satisfying constraints.  Error bars represent 95% confidence intervals across 5 independent runs.", "section": "Numerical Validation"}, {"figure_path": "2vywag2lVC/figures/figures_46_1.jpg", "caption": "Figure 8: \u03bb curves for C-PGPE and C-PGAE over CostLQR with regularization values \u03c9 \u2208 {0, 10\u22124, 10\u22122} (5 runs, mean \u00b1 95% C.I.).", "description": "This figure shows the evolution of the Lagrangian multipliers \u03bb during the learning process for both C-PGPE and C-PGAE algorithms, considering different regularization values (\u03c9 \u2208 {0, 10\u207b\u2074, 10\u207b\u00b2}).  The plots illustrate how the Lagrangian multipliers, which represent the dual variables in the primal-dual optimization framework, adapt based on the level of regularization applied.  The shaded areas show the 95% confidence interval for five independent runs of each experiment. The figure helps to visually understand the impact of regularization on the algorithm's convergence behavior.", "section": "H.7 Computational Resources"}]