{"references": [{"fullname_first_author": "Richard S. Sutton", "paper_title": "Reinforcement learning: an introduction", "publication_date": "2018-01-01", "reason": "It is a foundational textbook in reinforcement learning, providing the basis for many concepts used in the paper."}, {"fullname_first_author": "Adam Stooke", "paper_title": "Responsive safety in reinforcement learning by PID Lagrangian methods", "publication_date": "2020-01-01", "reason": "This paper introduces a Lagrangian-based approach to safety in reinforcement learning, which is directly relevant to the paper's focus on constrained reinforcement learning."}, {"fullname_first_author": "Dongsheng Ding", "paper_title": "Natural policy gradient primal-dual method for constrained Markov decision processes", "publication_date": "2020-01-01", "reason": "This work is highly relevant because it proposes a primal-dual method for solving constrained Markov decision processes, directly addressing the core problem tackled in the current paper."}, {"fullname_first_author": "Donghao Ying", "paper_title": "A dual approach to constrained Markov decision processes with entropy regularization", "publication_date": "2022-01-01", "reason": "This paper provides theoretical guarantees for a primal-dual algorithm for constrained reinforcement learning, which the current paper aims to improve and generalize."}, {"fullname_first_author": "Yinlam Chow", "paper_title": "Risk-constrained reinforcement learning with percentile risk criteria", "publication_date": "2017-01-01", "reason": "This paper extends reinforcement learning to the risk-aware setting, which is relevant to the paper\u2019s discussion on risk-constrained reinforcement learning."}]}