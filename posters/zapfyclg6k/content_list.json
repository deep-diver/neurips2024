[{"type": "text", "text": "On Differentially Private U Statistics ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kamalika Chaudhuri University of California San Diego kamalika@ucsd.edu ", "page_idx": 0}, {"type": "text", "text": "Po-Ling Loh University of Cambridge pll28@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Shourya Pandey University of Texas at Austin shouryap@utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Purnamrita Sarkar University of Texas at Austin purna.sarkar@austin.utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the problem of privately estimating a parameter $\\mathbb{E}[h(X_{1},\\cdot\\cdot\\cdot,X_{k})]$ , where $X_{1},X_{2},\\ldots,X_{k}$ are i.i.d. data from some distribution and $h$ is a permutationinvariant function. Without privacy constraints, the standard estimators for this task are U-statistics, which commonly arise in a wide range of problems, including nonparametric signed rank tests, symmetry testing, uniformity testing, and subgraph counts in random networks, and are the unique minimum variance unbiased estimators under mild conditions. Despite the recent outpouring of interest in private mean estimation, privatizing U-statistics has received little attention. While existing private mean estimation algorithms can be applied in a black-box manner to obtain confidence intervals, we show that they can lead to suboptimal private error, e.g., constant-factor inflation in the leading term, or even $\\bar{\\Theta}(1/n)$ rather than ${\\cal O}(1/n^{2})$ in degenerate settings. To remedy this, we propose a new thresholding-based approach that reweights different subsets of the data using local H\u00e1jek projections. This leads to nearly optimal private error for non-degenerate U-statistics and a strong indication of near-optimality for degenerate U-statistics. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A fundamental task in statistical inference is to estimate a parameter of the form $\\mathbb{E}[h(X_{1},\\cdot\\cdot\\cdot,X_{k})]$ , where $h$ is a possibly vector-valued function and $\\{X_{i}\\}_{i=1}^{n}$ are i.i.d. draws from an unknown distribution. U-statistics are a well-established class of estimators for such parameters and can be expressed as averages of functions of the form $h(X_{1},\\ldots,X_{k})$ . U-statistics arise in many areas of statistics and machine learning, encompassing diverse estimators such as the sample mean and variance, hypothesis tests such as the Mann-Whitney, Wilcoxon signed rank, and Kendall\u2019s tau tests, symmetry and uniformity testing [26, 21], goodness-of-fit tests [58], counts of combinatorial objects such as the number of subgraphs in a random graph [30], ranking and clustering [19, 18], and subsampling [52]. ", "page_idx": 0}, {"type": "text", "text": "U-statistics are a natural generalization of the sample mean. However, little work has been done on U-statistics under differential privacy, in contrast to the sizable body of existing work on private mean estimation [42, 39, 14, 40, 10, 41, 24, 11, 33, 15]. Ghazi et al. [29] and Bell et al. [8] consider Ustatistics in the setting of local differential privacy [43], while we are interested in privacy guarantees under the central model. Moreover, existing work on private U-statistics focuses on discrete data, and relies on simple privacy mechanisms (such as the Laplace mechanism [25]) which are usually optimal in these settings. ", "page_idx": 0}, {"type": "text", "text": "Many U-statistics converge to a limiting Gaussian distribution with variance $O(k^{2}/n)$ when suitably scaled. This is commonly used in hypothesis testing [35, 5, 37]. However, there are also examples of non-degenerate U-statistics, which often arise in a variety of hypothesis tests [26, 58, 21], where the statistic is degenerate at the null hypothesis (in which case the U-statistic converges to a sum of centered chi-squared distributions [31]). Another interesting U-statistic arises in subgraph counts in random geometric graphs [30]. When the probability of an edge being present tends to zero with $n$ , creating a private estimator by simply adding Laplace noise with a suitable scale may not be effective. ", "page_idx": 0}, {"type": "text", "text": "Our contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "1. We present a new algorithm for private mean estimation that achieves nearly optimal private and non-private errors for non-degenerate U-statistics with sub-Gaussian kernels. ", "page_idx": 1}, {"type": "text", "text": "2. We provide a lower bound for privately estimating non-degenerate sub-Gaussian kernels, which nearly matches the upper bound of our algorithm. We also derive a lower bound for degenerate kernels and provide evidence that the private error achieved by our algorithm in the degenerate case is nearly optimal. A summary of the utility guarantees of our algorithm and adaptations of existing private mean estimation methods is presented in Table 1. ", "page_idx": 1}, {"type": "text", "text": "3. The computational complexity of our first algorithm scales as $\\tilde{O}(\\binom{n}{k})$ . We generalize this algorithm and develop an estimator based on subsampled data, providing theoretical guarantees for a more efficient version with $O(n^{2})$ computational complexity. ", "page_idx": 1}, {"type": "text", "text": "The paper is organized as follows. Section 2 reviews the background on U-statistics and key concepts in differential privacy. Section 3 introduces an initial set of estimators based on the CoinPress algorithm for private mean estimation [10]. Section 4 presents our main algorithm, which leverages what we term local H\u00e1jek projections. Section 5 discusses applications of our algorithm to private uniformity testing and density estimation in sparse geometric graphs. Section 6 concludes the paper. ", "page_idx": 1}, {"type": "text", "text": "2 Background and Problem Setup ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let $n$ and $k$ be positive integers with $k\\leq n$ . Let $\\mathcal{D}$ be an unknown probability distribution over a set $\\mathcal{X}$ , and let $h:\\chi^{k}\\to\\overline{{\\mathbb{R}}}$ be a known function symmetric in its arguments1. Let $\\mathcal{H}$ be the distribution of $h(X_{1},X_{2},\\ldots,X_{k})$ , where $X_{1},X_{2},\\ldots,X_{k}\\sim{\\cal D}$ are i.i.d. random variables. We are interested in providing a $\\epsilon$ -differentially private confidence interval for the estimable parameter [32] $\\theta=\\mathbb{E}[h(X_{1},X_{2},\\cdot\\cdot\\cdot,X_{k})]$ , which is the mean of the distribution $\\mathcal{H}$ , given access to $n$ i.i.d. samples from $\\mathcal{D}$ ; we use $X_{1},X_{2},\\ldots,X_{n}$ to denote these $n$ samples. The kernel $h$ , the degree $k$ , and the estimable parameter $\\theta$ are allowed to depend on $n$ ; we omit the subscript $n$ for the sake of brevity. ", "page_idx": 1}, {"type": "text", "text": "We consider bounded kernels $h$ and unbounded kernels $h$ where the distribution $\\mathcal{H}$ is sub-Gaussian. We write $Y\\sim$ sub-Gaussian $(\\tau)$ if $\\mathbb{E}[\\exp(\\lambda(Y-\\mathbb{E}Y))]\\leq\\exp(\\tau\\lambda^{2}/2)$ for all $\\lambda\\in\\mathbb R$ . The quantity $\\tau$ is called a variance proxy for the distribution $\\mathcal{H}$ and satisfies the inequality $\\tau\\geq\\sigma^{2}$ [59]. Throughout the paper, we assume that the privacy parameter $\\epsilon=O(1)$ . We also use the notation $\\tilde{O}(\\cdot)$ in error terms, which hides poly-logarithmic factors in $n/\\alpha$ . ", "page_idx": 1}, {"type": "text", "text": "2.1 U-Statistics ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let $[n]$ denote $\\{1,\\ldots,n\\}$ , and let $\\mathcal{T}_{n,k}$ be the set of all $k$ -element subsets of $[n]$ . Denote the $n$ i.i.d. samples by $X_{1},X_{2},\\ldots,X_{n}$ . For any $S\\in{\\mathcal{T}}_{n,k}$ , let $X_{S}$ be the (unordered) $k$ -tuple $\\{X_{i}:i\\in S\\}$ . The U-statistic associated with the data and the function $h$ is ", "page_idx": 1}, {"type": "equation", "text": "$$\nU_{n}:={\\frac{1}{\\binom{n}{k}}}\\sum_{\\{i_{1},\\ldots,i_{k}\\}\\in{\\mathcal{Z}}_{n,k}}h(X_{i_{1}},\\ldots,X_{i_{k}}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The function $h$ is the kernel of $U_{n}$ and $k$ is the degree of $U_{n}$ . While U-statistics can be vector-valued, we consider scalar U-statistics in this paper. The variance of $U_{n}$ can be expressed in terms of the conditional variances of $h(X_{1},X_{2},\\ldots,{\\bar{X}}_{n})$ . For $c\\in[k]$ , we define the conditional variance ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\zeta_{c}:=\\operatorname{Var}\\left(\\mathbb{E}\\left[h(X_{1},\\ldots,X_{k})\\vert X_{1},\\ldots,X_{c}\\right]\\right).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Equivalently, $\\zeta_{c}=\\operatorname{cov}\\left(h(X_{S_{1}}),h(X_{S_{2}})\\right)$ where $S_{1},S_{2}\\in{\\mathcal{I}}_{n,k}$ and $|S_{1}\\cap S_{2}|=c$ . The number of such pairs of sets $S_{1}$ and $S_{2}$ is equal to $\\left(\\!_{k}^{n}\\!\\right)\\left(\\!_{c}^{k}\\!\\right)\\left(\\!_{k-c}^{n-k}\\!\\right)$ , which implies ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname{Var}(U_{n})={\\binom{n}{k}}^{-1}\\sum_{c=1}^{k}{\\binom{k}{c}}{\\binom{n-k}{k-c}}\\zeta_{c}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Since $\\mathbb{E}[U_{n}]=\\theta$ , $U_{n}$ is an unbiased estimate of $\\theta$ . Moreover, the variance of $U_{n}$ is a lower bound on the variance of any unbiased estimator of $\\theta$ . (cf Lee [45, Chapter 1, Theorem 3]). We also have the following inequality from Serfling [54] (see Appendix A.2 for a proof): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\zeta_{1}\\leq{\\frac{\\zeta_{2}}{2}}\\leq{\\frac{\\zeta_{3}}{3}}\\leq\\cdots\\leq{\\frac{\\zeta_{k}}{k}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Infinite-order U-Statistics: Classical U-statistics typically have small, fixed $k$ . However, important estimators that appear in the contexts of subsampling [52] and Breiman\u2019s random forest algorithm [56, 50] have $k$ growing with $n$ . These types of U-statistics are sometimes referred to as infinite-order Ustatistics [27, 48]). U-statistics also frequently appear in the analysis of random geometric graphs [30]. The difference between this setting and the examples above is that the conditional variances $\\{\\zeta_{c}\\}$ vanish with $n$ in the sparse setting. (See Section 5.) ", "page_idx": 2}, {"type": "text", "text": "Degenerate U-statistics: A U-statistic is degenerate of order $\\ell\\leq k-1$ if $\\zeta_{i}=0$ for all $i\\in[\\ell]$ and $\\zeta\\ell{+}1>0$ (if $\\zeta_{k}=0$ , the distribution is almost surely constant). Degenerate U-statistics arise in hypothesis tests such as Cramer-Von Mises and Pearson tests of goodness of fit [31, 3, 55] and tests for unformity [21]. They also appear in tests for model misspecification in econometrics [46, 47]. For more examples of degenerate U-statistics, see [20, 61, 34]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Differential privacy ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The main idea of differential privacy [25] is that the participation or non-participation of a single person should not affect the outcome significantly. A (randomized) algorithm $M$ , that takes as input a dataset $D\\in\\mathcal{X}^{n}$ and outputs an element of its range space $\\boldsymbol{S}$ , satisfies $\\epsilon$ -differential privacy if for any pair of adjacent datasets $D$ and $D^{\\prime}$ and any measurable subset $S\\subseteq S$ of the range space $\\boldsymbol{S}$ $\\operatorname*{Pr}(M(D)\\in S)\\leq e^{\\epsilon}\\operatorname*{Pr}(M(D^{\\prime})\\in S)$ . A dataset is $D:=(X_{1},X_{2},\\ldots,X_{n})$ from some domain $\\mathcal{X}$ , for some $n$ which is public. Two datasets $D$ and $D^{\\prime}$ are adjacent if they differ in exactly one index. An important property of differentially private algorithms is composition. We defer composition theorems for differentially private algorithms to Appendix A.2. ", "page_idx": 2}, {"type": "text", "text": "Basic DP algorithms. One way to ensure an algorithm satisfies differential privacy is through the Laplace Mechanism [25]. The global sensitivity of a function $f:\\mathcal{X}^{n}\\rightarrow\\mathcal{S}$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\nG S(f)=\\operatorname*{max}_{|D\\Delta D^{\\prime}|=1}|f(D)-f(D^{\\prime})|,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $D\\Delta D^{\\prime}:=|\\{i:D_{i}\\neq D_{i}^{\\prime}\\}|$ A fundamental result in differential privacy is that we can achieve privacy for $f$ by adding noise calibrated to its global sensitivity. ", "page_idx": 2}, {"type": "text", "text": "Lemma 1. (Laplace mechanism $[25J)\\,L e t\\;f:\\mathcal{X}^{n}\\rightarrow\\mathcal{S}$ be a function and let $\\epsilon>0$ be the privacy parameter. Then the algorithm $\\begin{array}{r}{A(D)=f(D)+\\mathrm{Lap}\\left(\\frac{G S(f)}{\\epsilon}\\right)^{2}}\\end{array}$ is $\\epsilon$ -differentially private. ", "page_idx": 2}, {"type": "text", "text": "The global sensitivity of a function is the worst-case change in the function value and may be high on atypical datasets. To account for the small sensitivity on \u201ctypical\u201d datasets, the notion of local sensitivity is useful. The local sensitivity of a function $f:\\mathcal{X}^{n}\\rightarrow S$ at $D$ is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\nL S(f,D)=\\operatorname*{max}_{|D\\Delta D^{\\prime}|=1}|f(D)-f(D^{\\prime})|.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Unfortunately, adding noise proportional to the local sensitivity does not ensure differential privacy, because variation in the magnitude of noise itself may leak information. Instead, [49] proposed the notion of a smooth upper bound on $L S(f,D)$ . A function $S S(f,\\cdot)$ is said to be an $\\epsilon$ -smooth upper bound on the local sensitivity of $f$ if (i) $S S(\\hat{f},D)\\ge L S(f,D)$ for all $D$ , and (ii) $S S(f,D)\\le$ $e^{\\epsilon}S S(f,D^{\\prime})$ for all $\\vert D\\Delta D^{\\prime}\\vert=1$ . Intuitively, (i) ensures that enough noise is added, and (ii) ensures that the noise itself does not leak information about the data. ", "page_idx": 2}, {"type": "text", "text": "Lemma 2. (Smoothed Sensitivity mechanism $I^{49}J,$ ) Let $f:\\mathcal{X}^{n}\\rightarrow\\mathcal{S}$ be a function, $\\epsilon\\,>\\,0$ , and $S S(f,\\,\\cdot\\,)$ be an $\\epsilon$ -smooth upper bound on $L S(f,\\mathbf{\\xi}\\cdot\\mathbf{\\xi})$ . Then, the algorithm ${\\mathcal A}(D)\\;=\\;f(D)\\;+$ $S S(f,D)/\\epsilon\\cdot Z$ , where $Z$ has density $\\begin{array}{r}{h(z)\\propto\\frac{1}{1+z^{4}}}\\end{array}$ , is $\\epsilon/10$ -differentially private. ", "page_idx": 2}, {"type": "table", "img_path": "zApFYcLg6K/tmp/aa7d95d8a004cd8cfcadbc31b4503d5a397fb28f8248fa82ce64607e8abf9256.jpg", "table_caption": ["3 Lower Bounds and Application of Off-the-shelf Tools "], "table_footnote": ["Table 1: We compare our application of off-the-shelf tools to Algorithm 1. We only provide the leading terms in the private error. The non-private lower bound on $\\mathbb{E}(\\hat{\\theta}-\\mathbb{E}h(X_{1},\\ldots,X_{k}))^{2}$ for all unbiased $\\hat{\\theta}$ is $\\operatorname{Var}(U_{n})$ , which our private algorithms nearly match. "], "page_idx": 3}, {"type": "text", "text": "We start with a simple, non-private estimator involving an average of independent quantities. Let $m=\\lfloor n/k\\rfloor$ , and define $S_{j}=\\{(j-1)k+1,(j-1)k+2,\\ldots,j k\\}$ for all $j\\in[m]$ . Define the naive estimator $\\begin{array}{r}{\\widehat{\\theta}_{\\mathrm{naive}}:=\\sum_{j=1}^{m}h(X_{S_{j}})/m}\\end{array}$ . Directly applying existing private mean estimation algorithms [42, 40, 39, 10] to our setting yields an error bound of3 ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{O}\\left(\\sqrt{\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{naive}})}+k\\sqrt{\\tau}/(n\\epsilon)\\right)=\\tilde{O}\\left(\\sqrt{k\\zeta_{k}/n}+k\\sqrt{\\tau}/(n\\epsilon)\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "since $\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{naive}})\\,=\\,k\\zeta_{k}/n$ . Note that this variance is larger than the dominant term $k^{2}\\zeta_{1}/n$ of $\\mathrm{Var}(U_{n})$ (see Lemma A.1 and Eq 4); indeed, $\\hat{\\theta}_{\\mathrm{naive}}$ is a suboptimal estimator of $\\theta$ . ", "page_idx": 3}, {"type": "text", "text": "In Algorithms A.2 and A.3, we present a general extension of the CoinPress algorithm [10], which is then used to obtain a private estimate of $\\theta$ with the non-private error term matching $\\sqrt{\\mathrm{Var}(U_{n})}$ . For completeness, we present the algorithms and their proofs in Appendix A.3. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (All-tuples family). Let $\\textstyle M={\\binom{n}{k}}$ and let $S_{a l l}=\\{S_{1},S_{2},\\ldots,S_{M}\\}=\\mathbb{Z}_{n,k}$ be the set of all $k$ -element subsets of $[n]$ . Call $\\mathit{S}_{a l l}$ the \u201call-tuples\u201d family. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Suppose $\\theta\\ \\in\\ [-R,R]$ . Let $\\mathit{S}_{a l l}$ be the all-tuples family in Definition $^{\\,l}$ . Then Wrapper $^{\\,l}$ , with $f=$ all, failure probability $\\alpha$ , and ${\\mathcal{A}}=U$ -StatMean (Algorithm A.2) returns an estimate $\\tilde{\\theta}_{a l l}$ of the mean $\\theta$ such that, with probability at least $1-\\alpha,$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n|\\tilde{\\theta}_{a l l}-\\theta|\\leq O\\left(\\sqrt{\\mathrm{Var}(U_{n_{\\alpha}})}\\right)+\\tilde{O}\\left(\\frac{k^{3/2}\\sqrt{\\tau}}{n_{\\alpha}\\epsilon}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "as long as $\\begin{array}{r}{n_{\\alpha}=\\tilde{\\Omega}\\left(\\frac{k}{\\epsilon}\\log\\frac{R}{\\sqrt{k\\tau}}\\right)}\\end{array}$ . Moreover, the algorithm is $\\epsilon$ -differentially private and runs in time $\\begin{array}{r}{\\tilde{O}\\left(\\log(1/\\alpha)\\left(k+\\log\\frac{R}{\\sqrt{k\\tau}}\\right)\\binom{n_{\\alpha}}{k}\\right)}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Rem\u221aark 1. While Lemma $^{\\,l}$ recovers the correct first term of the deviation, the private error term is a $\\sqrt{k}$ factor worse. Moreover, we need $k^{2}/n=o(1)$ for the private error to be asymptotically smaller than the non-private error. Note, however, that existing concentration [35, 5] or convergence in probability results $[58,48]$ only require $k=o(n)$ (see Lemmas A.1 and A.3 in the Appendix). ", "page_idx": 3}, {"type": "text", "text": "Remark 2 (Degenerate and sparse settings). While Lemma 1 improves over the naive estimator, the private error can overwhelm the non-private error for degenerate and sparse $U$ -statistics (see Section 5). We show that for uniformity testing, using this estimator can lead to suboptimal sample complexity if the distribution is already close to uniform. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1 improves over the naive estimator at the cost of computational complexity. We can trade off the computational and statistical efficiencies using a different family $\\boldsymbol{S}$ parameterized by the size $M$ of $\\boldsymbol{S}$ . In Appendix 3, we show a result similar to 1 for the subsampled family. ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Subsampled Family). Draw $M$ i.i.d. samples $S_{1},\\ldots,S_{M}$ from the uniform distribution over the elements of $\\mathcal{T}_{n,k}$ , and let $\\mathcal{S}_{s s}:=\\{S_{1},\\ldots,S_{M}\\}$ . Call $\\mathit{S}_{a l l}$ the \u201csubsampled\u201d family. ", "page_idx": 4}, {"type": "text", "text": "The next result shows a nearly optimal dependence on $n$ and $\\epsilon$ in the bounds for $\\hat{\\theta}_{\\mathrm{naive}}$ and $\\tilde{\\theta}_{\\mathrm{all}}$ . In particular, the dependence of the modified Coinpress algorithm (Lemma 1) on $k$ is suboptimal. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Lower bound for non-degenerate kernels). Let n and $k$ be positive integers with $k<n/2$ and let $\\epsilon=\\Omega(k/n)$ . Let $\\mathcal{F}$ be the set of all sub-Gaussian distributions over $\\mathbb{R}$ with variance proxy 1, and let $\\tilde{\\mu}$ be the output of any $\\epsilon$ -differentially private algorithm applied to n i.i.d. observations from $\\mathcal{D}$ . Then, $\\begin{array}{r}{\\operatorname*{sup}_{h,\\mathcal{D}:\\mathcal{H}\\in\\mathcal{F}}\\mathbb{E}|\\tilde{\\mu}(X_{1},\\ldots,X_{n})-\\mathbb{E}[h(X_{1},\\ldots,X_{k})]|=\\Omega\\left(\\frac{k}{n\\epsilon}\\sqrt{\\log\\frac{n\\epsilon}{k}}\\right)}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Among unbiased estimators, $U_{n}$ is the best non-private estimator [35, 45]. The most widely used non-private estimators are $U$ - and $V$ -statistics, which share similar asymptotic properties [58]. The above lower bound also has a log factor arising from an optimal choice of the sub-Gaussian proxy for Bernoulli random variables [4]. Proofs are deferred to Appendix A.4. ", "page_idx": 4}, {"type": "text", "text": "3.1 Boosting the error probability via median-of-means ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "If we us\u221ae Algorithm A.2 as stated with failure probability $\\alpha$ , then the error in the algorithm has a $O(1/\\sqrt{\\alpha})$ factor, which is undesirable. Instead, we use the Algorithm with a constant failure probability (say, 0.25) and then boost this failure probability to $\\alpha$ via a median-of-means procedure. We incorporate the median-of-means in all of our theoretical results. ", "page_idx": 4}, {"type": "text", "text": "Wrapper 1 (MedianOfMeans ${}^{n,k}$ , Algorithm $\\boldsymbol{\\mathcal{A}}$ , Parameters $\\Lambda$ , Failure probability $\\alpha$ , Family type $f\\in\\{\\mathrm{all},\\mathrm{ss}\\})$ ). Divide $[n]$ into $q=8\\log(1/\\alpha)$ independent chunks $I_{i},i\\in[q]$ of roughly the same size. For each $i\\in[q]$ , run Algorithm $\\boldsymbol{\\mathcal{A}}$ with subset family $S_{i}:=S_{f}(I_{i})$ , Dataset $\\{h(X_{S})\\}_{S\\in S_{i}}$ , and other parameters $\\Lambda$ for $\\boldsymbol{\\mathcal{A}}$ to output $\\hat{\\theta}_{i},i\\in[q]$ . Return ${\\tilde{\\theta}}=\\operatorname{median}({\\hat{\\theta}}_{1},\\dots,{\\hat{\\theta}}_{q})$ . ", "page_idx": 4}, {"type": "text", "text": "In the above wrapper, $S_{f}(D_{i})$ simply creates the appropriate family of subsets for the dataset $D_{i}$ . For example, if $D_{i}\\,=\\,\\{X_{1},\\ldots X_{n_{\\alpha}}\\}$ , $f={\\mathrm{all}}$ , then $S_{\\mathrm{all}}(D_{i})$ is $\\{h(X_{S})\\}_{S\\in{\\mathbb{Z}_{n_{\\alpha},k}}}$ . If $f={\\bf s}{\\bf s}$ , then Sss(Di) is {h(XS)}S\u2208Si, where Si is the set of M subsampled subsets of Di. Here, n\u03b1 :=8 log(n1/\u03b1). Wrapper 1 can be used to boost the failure probability from constant to $\\alpha$ by splitting the data into ${\\cal O}(\\log(1/\\alpha))$ chunks, applying the algorithm on each of the chunks, and taking the median output. The expense of this procedure is the reduction in the effective sample size to $\\Bar{n_{\\alpha}}=\\Theta(n/\\log(1/\\Bar{\\alpha}))$ . Details and proofs on the median-of-means procedure can be found in Section A.3.2. ", "page_idx": 4}, {"type": "text", "text": "4 Main Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In Section 3, we showed that off-the-shelf private mean estimation tools applied to U-statistics either achieve a sub-optimal non-private error (see Remark 1) or a sub-optimal private error. If the U-statistic is degenerate of order 1, the non-private and private errors (assuming $\\epsilon=\\Theta(1))$ ) are $\\tilde{\\Theta}(1/n)$ . We now present an algorithm that achieves nearly optimal private error for sub-Gaussian non-degenerate kernels. Our algorithm can be viewed as a generalization of the algorithm proposed in Ullman and Sealfon [57] for privately estimating the edge density of an Erd\u02ddos-R\u00e9nyi graph. We provide strong evidence that, for bounded degenerate kernels, we achieve nearly optimal non-private error. All proofs for this section can be found in Section A.5. ", "page_idx": 4}, {"type": "text", "text": "4.1 Key intuition ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our key insight is to leverage the H\u00e1jek projection [58, 45], which gives the best representation of a U-statistic as a linear function of the form $\\bar{\\sum}_{i=1}^{n}\\,f(X_{i})$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{S}_{n}\\overset{(i)}{=}\\sum_{i=1}^{n}\\mathbb{E}[T_{n}|X_{i}]-(n-1)\\mathbb{E}[T_{n}]\\overset{(i i)}{=}\\frac{k}{n}\\sum_{i=1}^{n}\\mathbb{E}[h(X_{S})|X_{i}]-(n-1)\\theta.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Equality (i) gives the form of the H\u00e1jek projection for a general statistic $T_{n}$ , whereas (ii) gives the form when $T_{n}$ is a U-statistic. Let ${\\mathcal{T}}_{n,k}^{(i)}=\\{S\\in{\\mathcal{Z}}_{n,k}:i\\in S\\}$ . In practice, one uses the estimates ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}[h(X_{S})|X_{i}]:=\\frac{1}{\\binom{n-1}{k-1}}\\sum_{S\\in\\mathbb{Z}_{n,k}^{(i)}}h(X_{S}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which we call local H\u00e1jek projections. In some sense, this is the U-statistic when viewed locally at $X_{i}$ . When the dataset is clear from context, we write $\\hat{h}_{\\mathcal{Z}_{n,k}}(i)$ , or simply $\\hat{h}(i)$ , for $\\widehat{\\mathbb{E}}[h(X_{S})|X_{i}]$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 Proposed algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Consider a family of subsets ${\\mathcal{S}}\\subseteq{\\mathcal{T}}_{n,k}$ of size $M$ . Let $S_{i}=\\{S\\in\\mathcal{S}:i\\in S\\}$ and $M_{i}=|S_{i}|$ , and suppose $M_{i}\\neq0$ for all $i\\in[n]$ . Assume also that $\\boldsymbol{S}$ satisfies the inequalities ", "page_idx": 5}, {"type": "equation", "text": "$$\nM_{i}/M\\le3k/n\\;\\;\\;\\mathrm{and}\\;\\;\\;M_{i j}/M_{i}\\le3k/n\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for any distinct indices $i$ and $j$ in $[n]$ (one such family is $S={\\mathcal{T}}_{n,k}$ , for which $M_{i}/M=k/n$ and $M_{i j}/\\dot{M}_{i}=(k-1)/(n-1)\\le k/\\dot{n}$ , but there are other such families). Define ", "page_idx": 5}, {"type": "equation", "text": "$$\nA_{n}(\\boldsymbol{S}):=\\frac{1}{M}\\sum_{\\boldsymbol{S}\\in\\mathcal{S}}h(\\boldsymbol{X}_{\\boldsymbol{S}}),\\quad\\mathrm{~and~}\\quad\\hat{h}_{\\boldsymbol{S}}(i):=\\frac{1}{M_{i}}\\sum_{\\boldsymbol{S}\\in\\mathcal{S}_{i}}h(\\boldsymbol{X}_{\\boldsymbol{S}}),\\quad\\forall i\\in[n].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$U_{n}$ in Eq (1) and $\\widehat{\\mathbb{E}}[h(X_{S})|X_{i}]$ in equation (9) are the same as the quantities $A_{n}({\\mathcal{Z}}_{n,k})$ and $\\hat{h}_{\\mathcal{Z}_{n,k}}(i)$ . ", "page_idx": 5}, {"type": "text", "text": "A standard procedure in private estimation algorithms is to clip the data to an appropriate interval [10, 41] in such a way that the sensitivity of the overall estimate can be bounded. Similarly, we use the concentration of the local H\u00e1jek projections to define an interval such that each $i$ can be classified as \u201cgood\" or \u201cbad\u201d based on the distance between ${\\hat{h}}_{S}(i)$ and the interval. The final estimator is devised so the contribution of the bad indices to the estimator is low and the estimator has low sensitivity. ", "page_idx": 5}, {"type": "text", "text": "Let $\\xi$ and $C$ be parameters to be chosen later; they will be chosen in such a way that with high probability, (i) $|\\hat{h}(i)-\\theta|\\leq\\xi$ for all $i$ , and (ii) each $h(X_{S})$ is at most $C$ away from $\\theta$ . Define ", "page_idx": 5}, {"type": "equation", "text": "$$\n{L_{S}}:=\\underset{t\\in\\mathbb{N}>0}{\\arg\\operatorname*{min}}\\left(\\left|\\left\\{i:\\left|\\hat{h}_{S}(i)-{A_{n}}(S)\\right|>\\xi+6k C t/n\\right\\}\\right|\\leq t\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In other words, $L_{S}$ is the smallest positive integer $t$ such that at most $t$ indices $i\\;\\in\\;[n]$ satisfy $\\begin{array}{r}{|\\hat{h}_{S}(i)-A_{n}(S)|>\\xi+\\frac{6k C t}{n}}\\end{array}$ (such an integer $t$ always exists because $t=n$ works). Define ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{Good}(\\mathcal{S}):=\\left\\{i:\\left|\\hat{h}_{S}(i)-A_{n}(S)\\right|\\leq\\xi+6k C L_{S}/n\\right\\},\\qquad\\operatorname{Bad}(\\mathcal{S}):=[n]\\setminus\\operatorname{Good}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For each index $i\\in[n]$ , define the weight of $i$ with respect to $\\boldsymbol{S}$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{w}\\mathfrak{t}_{S}(i):=\\operatorname*{max}\\left(0,1-\\frac{\\epsilon n}{6C k}\\cdot\\mathrm{dist}\\left(\\hat{h}_{S}(i)-A_{n},[-\\xi-6k C L_{S}/n,\\xi+6k C L_{S}/n]\\right)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $\\epsilon$ is the privacy parameter and dist $(x,I)$ is the distance between $x$ and the interval $I$ . ", "page_idx": 5}, {"type": "text", "text": "Based on whether a datapoint is good or bad, we will define a weighting scheme that reweights the $h(X_{S})$ in equation (1). For each $S\\in S$ , let ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{wt}_{S}(S):=\\operatorname*{min}_{i\\in S}\\mathbf{wt}_{S}(i),\\quad\\mathrm{and}\\quad g s(X_{S}):=h(X_{S})\\mathbf{wt}_{S}(S)+A_{n}(S)\\left(1-\\mathbf{wt}_{S}(S)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In particular, if $\\mathrm{wt}_{\\ensuremath{S}}(S)=1$ , then $g_{S}(X_{S})=h(X_{S})$ ; and if $\\mathrm{wt}_{\\ensuremath{S}}(S)=0$ , then $g_{S}(X_{S})=A_{n}(S)$ Finally, define the quantities ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{A}_{n}(\\boldsymbol{S}):=\\frac{1}{M}\\sum_{S\\in\\mathcal{S}}g_{S}(X_{S}),\\qquad\\hat{g}_{\\boldsymbol{S}}(i):=\\frac{1}{M_{i}}\\sum_{S\\in\\mathcal{S}_{i}}g_{S}(X_{S})\\,\\,\\,\\forall i\\in[n].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To simplify notation, we will drop the argument $\\boldsymbol{S}$ from $L$ $\\mathsf{L},\\,A_{n},\\,\\tilde{A}_{n},\\,\\hat{h},\\,\\hat{g}$ , Good, and Bad. ", "page_idx": 5}, {"type": "text", "text": "Idea behind the algorithm: If all $\\hat{h}(i)$ \u2019s are within $\\xi$ of the empirical mean $A_{n}$ , then $\\mathrm{Bad}=\\,\\mathcal{D}$ and ${\\tilde{A}}_{n}=A_{n}$ . Otherwise, for any set $S$ containing a bad index, we replace $h(X_{S})$ by a weighted combination of $h(X_{S})$ and $A_{n}$ . This averaging-out of the bad indices allows a bound on the local sensitivity of $\\tilde{A}_{n}$ . We then provide a smooth upper bound on the local sensitivity characterized by the quantity $L$ , which can be viewed as an indicator of how well-concentrated the data is. The choice of $\\xi$ will be such that $L=1$ with high probability and that the smooth sensitivity of $\\tilde{A}_{n}$ at $\\mathbf{X}$ is small. This ensures that a smaller amount of noise is added to $\\tilde{A}_{n}$ to preserve privacy. ", "page_idx": 5}, {"type": "text", "text": "1: $M\\gets|S|$ 2: $S_{i}\\gets\\{S\\in\\mathcal{S}:i\\in S\\}$ 3: $M_{i}\\gets|S_{i}|$ 4: if there exist indices $i\\neq j$ such that $M_{i}=0$ or $M_{i}/M>3k/n$ or $M_{i j}/M_{i}>3k/n$ , then 5: return $\\bot$ 6: end if 7: $\\begin{array}{r}{A_{n}\\gets\\sum_{S\\in\\mathcal{S}}h(X_{S})/M}\\end{array}$ 8: for $i=1,2,\\dots,n$ do 9: $\\begin{array}{r}{\\hat{h}(i)\\leftarrow\\sum_{S\\in{\\cal S}_{i}}h(X_{S})/M_{i}}\\end{array}$ 10: end for 11: Let $L$ be the smallest positive integer such that $\\begin{array}{r}{\\left|\\left\\{i:\\left|\\hat{h}(i)-A_{n}\\right|>\\xi+\\frac{6k C L}{n}\\right\\}\\right|\\leq L}\\end{array}$ 12: $\\begin{array}{r l}&{\\mathsf{G o o d}(\\mathcal{S})\\gets\\left\\{i:\\left|\\hat{h}(i)-A_{n}\\right|\\leq\\xi+\\frac{6k C L}{n}\\right\\};\\mathsf{B a d}(\\mathcal{S})\\gets[n]\\setminus\\mathsf{G o o d}(\\mathcal{S})}\\\\ &{\\mathsf{f o r}\\ i=1,2,\\ldots,n\\,\\mathsf{d o}}\\\\ &{\\mathsf{w t}(i)\\gets\\operatorname*{max}\\left(0,1-\\frac{\\epsilon}{6C k/n}\\mathsf{d i s t}\\left(\\hat{h}(i)-A_{n},\\left[-\\xi-\\frac{6k C L}{n},\\xi+\\frac{6k C L}{n}\\right]\\right)\\right)}\\\\ &{\\quad\\_{s}\\,}\\end{array}$ 13: 14: 15: end for 16: for $S\\in S$ do 17: $\\begin{array}{r}{g(X_{S})\\gets h(X_{S})\\operatorname*{min}_{i\\in S}\\mathbf{wt}(i)+A_{n}\\left(1-\\operatorname*{min}_{i\\in S}\\mathbf{wt}(i)\\right)}\\end{array}$ 18: end for 19: $\\begin{array}{r}{\\tilde{A}_{n}\\gets\\sum_{S\\in\\mathcal{S}}g(X_{S})/M}\\end{array}$ 20: $\\begin{array}{r}{S(S)\\longleftarrow\\operatorname*{max}_{0\\leq\\ell\\leq n}e^{-\\epsilon\\ell}\\left(\\frac{k}{n}\\left(\\xi+\\frac{k C(L+\\ell)}{n}\\right)(1+\\epsilon(L+\\ell))+\\frac{k^{2}C(L+\\ell)^{2}\\operatorname*{min}(k,(L+\\ell))}{n^{2}}\\left(\\epsilon+\\frac{k}{n}\\right)+\\frac{k^{2}C}{n^{2}\\epsilon}\\right)}\\end{array}$ 21: Draw $Z$ from distribution with density $h(z)\\propto1/(1+|z|^{4})$ 22: return $\\tilde{A}_{n}+S(S)/\\epsilon\\cdot Z$ ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Algorithm $^{\\,l}$ is 10\u03f5-differentially private for any $\\xi$ . Moreover, suppose $h$ is bounded with additive range $C,{^4}$ and with probability at least 0.99, we have $\\mathrm{max}_{i}\\,|\\hat{h}_{\\mathcal{S}}(i)-A_{n}|\\leq\\xi$ . Run Wrapper $^{\\,l}$ with $f=a l l,$ , and $A=P r$ ivateMeanLocalHajek (Algorithm $^{\\,l}$ ). With probability at least $1-\\alpha,$ , the output $\\tilde{\\theta}$ satisfies $\\begin{array}{r}{|\\tilde{\\theta}-\\theta|=O\\left(\\sqrt{\\mathrm{Var}(U_{n_{\\alpha}})}+\\frac{k\\xi}{n_{\\alpha}\\epsilon}+\\left(\\frac{k^{2}}{n_{\\alpha}^{2}\\epsilon^{2}}+\\frac{k^{3}}{n_{\\alpha}^{3}\\epsilon^{3}}\\right)C\\right).}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Connections to Ullman and Sealfon [57]: Ullman and Sealfon [57] estimate the edge density of an Erdo\u02dds Renyi graph using strong concentration properties of its degrees. This idea can be loosely generalized to a broader setting of U-statistics: consider a hypergraph with $n$ nodes and $\\binom{n}{k}$ edges, where the $i^{t h}$ node corresponds to index $i$ . An edge corresponds to a $k$ -tuple of data points $S\\in{\\mathcal{Z}}_{n,k}$ , and the edge weight is given by $h(X_{S})$ . The natural counterpart of a degree in a graph becomes a local H\u00e1jek projection, defined as in equation (9). In degenerate cases and cases where $k^{2}\\zeta_{1}\\ll k\\zeta_{k}$ , the local H\u00e1jek projections are tightly concentrated around the mean $\\theta$ . We exploit this fact and reweight the edges ( $k$ -tuples) so that the local sensitivity of the reweighted U-statistic is small. ", "page_idx": 6}, {"type": "text", "text": "4.3 Application to non-degenerate and degenerate kernels ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Algorithm 1 can be extended from bounded kernels to sub-Gaussian $(\\tau)$ kernels. First, split the samples into two roughly equal halves. The first half of the samples will be used to obtain a coarse estimate of the mean $\\theta$ . For this, we can use any existing private mean estimation algorithm to obtain an $\\epsilon/2$ -differentially pr\u221aivate estimate $\\tilde{\\theta}_{\\mathrm{coarse}}$ such that with probability at least $1\\,-\\,\\alpha$ , $|\\tilde{\\theta}_{\\mathrm{coarse}}\\,-\\,\\theta|\\;=\\;\\tilde{O}(\\sqrt{k\\zeta_{k}/n}\\,+\\,k\\sqrt{\\tau}/(n\\epsilon))$ . By a union bound, with probability at least ${1\\mathrm{~-~}\\alpha}$ , $|h(X_{S})-\\theta|$ is within $4{\\sqrt{\\tau\\log\\left(\\binom{n}{k}/\\alpha\\right)}}$ for all $S\\in{\\mathcal{Z}}_{n,k}$ , and therefore also within $c\\sqrt{k\\tau\\log(n/\\alpha)}$ of the coarse estimate $\\tilde{\\theta}_{\\mathrm{coarse}}$ , for some universal constant $c$ , as long as $\\epsilon=\\tilde{\\Omega}(\\sqrt{k}/n)$ . ", "page_idx": 6}, {"type": "text", "text": "Define the projected function $\\tilde{h}(X_{1},X_{2},\\ldots,X_{k})$ to be the value $h(X_{1},X_{2},\\ldots,X_{k})$ projected to the interval $[\\tilde{\\theta}_{\\mathrm{coarse}}\\,-\\,c\\sqrt{k\\tau\\log(n/\\alpha)},\\tilde{\\theta}_{\\mathrm{coarse}}\\,+\\,c\\sqrt{k\\tau\\log(n/\\alpha)}]$ . The final estimate of the mean $\\theta$ is obtained by applying Algorithm 1 to the other half of the samples, the function $\\tilde{h}$ , and the privacy parameter $\\epsilon/2$ . The following lemma shows that $\\sqrt{2\\tau\\log(2n/\\alpha)}$ is a valid choice of the concentration parameter $\\xi$ for sub-Gaussian, non-degenerate kernels. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Lemma 3. If $\\mathcal{H}$ is sub-Gaussian $\\left(\\tau\\right)$ , the local H\u00e1jek projections $\\hat{h}(i)$ are also sub-Gaussian $(\\tau)$ . In particular, with probability at least $1-\\alpha$ , we have $\\begin{array}{r}{\\operatorname*{max}_{1\\leq i\\leq n}|\\hat{h}(i)-\\theta|\\leq\\sqrt{2\\tau\\log(2n/\\alpha)}}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Combining these parameters with Theorem 2 gives us the following result: ", "page_idx": 7}, {"type": "text", "text": "Corollary 1 (Non-degenerate sub-Gaussian kernels). Suppose h is sub-Gaussian $(\\tau)$ and the privacy parameter $\\epsilon=\\tilde{\\Omega}(k^{1/2}/n)$ . Split the samples into two halves and compute a private estimate of the mean by applying the naive estimator on the first half of the samples with privacy parameter $\\epsilon/2$ to obtain $\\tilde{\\theta}_{c o a r s e}$ . Let $\\tilde{h}$ be the projection of of the function $h$ onto the interval $\\tilde{\\theta}_{c o a r s e}\\pm O(\\sqrt{k\\tau\\log(2n/\\alpha)})$ . Run Wrapper $^{\\,I}$ on the remaining half of the samples with $f=a l l,$ , failure probability $\\alpha/2$ , algorithm ${\\mathcal{A}}=P!$ rivateMeanLocalH\u00e1jek (Algorithm $^{\\,l}$ ), $C=\\sqrt{2k\\tau\\log(n/2\\alpha)}$ , $\\xi\\,=\\,\\sqrt{2\\tau\\log(n/2\\alpha)},$ , function $\\tilde{h}$ , and privacy parameter $\\epsilon/20$ . Then, the output $\\tilde{\\theta}$ is $\\epsilon$ -differentially private. Moreover, with probability at least $1-\\alpha$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n|\\tilde{\\theta}-\\theta|=O\\left(\\sqrt{\\mathrm{Var}(U_{n_{\\alpha}})}\\right)+\\tilde{O}\\left(\\frac{k\\sqrt{\\tau}}{n_{\\alpha}\\epsilon}+\\frac{k^{2.5}\\sqrt{\\tau}}{n_{\\alpha}^{2}\\epsilon^{2}}+\\frac{k^{3.5}\\sqrt{\\tau}}{n_{\\alpha}^{3}\\epsilon^{3}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "From our lower bound on non-degenerate kernels in Theorem 1, we see that the above corollary is optimal in terms of $k,n,\\epsilon$ (up to log factors). In contrast, Lemma 1 is suboptimal in $k$ . ", "page_idx": 7}, {"type": "text", "text": "Many degenerate U-statistics (e.g., all the degenerate ones in Section 5) have bounded kernels. For these, we see that the local H\u00e1jek projections concentrate strongly around the U-statistic. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4. Suppose $\\mathcal{H}$ is bounded, with additive range $C$ . Let $i\\in[n]$ be an arbitrary index and $S_{i}\\in{\\mathcal{T}}_{n,k}$ be a set containing $i$ , and suppose $x_{i}\\in\\mathbb{R}$ is some element in the support of $\\mathcal{D}$ . With probability at least 1 \u2212 \u03b2n, conditioned on Xi = xi, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left|\\widehat{\\mathbb{E}}[h(X_{S})|X_{i}=x_{i}]-\\mathbb{E}\\left[h(X_{S_{i}})|X_{i}=x_{i}\\right]\\right|\\le2\\sigma_{i}\\sqrt{\\frac{k}{n}\\log\\left(\\frac{2n}{\\beta}\\right)}+\\frac{8C k}{3n}\\log\\left(\\frac{2n}{\\beta}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For bounded kernels with additive range $C$ , $\\sigma_{i}\\leq C/2$ by Popoviciu\u2019s inequality [53]. Moreover, for degenerate kernels, $\\zeta_{1}=0$ . That is, the conditional expectation $\\mathbb{E}\\left[h(X_{S_{i}})\\right]X_{i}=x_{i}\\right]$ is equal to $\\theta$ for all $x_{i}$ , because the variance of this conditional expectation is $\\zeta_{1}$ . Based on this, we can show that the choice of $\\xi=\\tilde{O}\\left(C k^{1/2}/n^{1/2}\\right)$ satisfies the requirement that the local H\u00e1jek projections are within $\\xi$ of $\\theta$ with probability at least $1-\\alpha$ . ", "page_idx": 7}, {"type": "text", "text": "Corollary 2 (Degenerate bounded kernels). Suppose $h$ is bounded with additive range $C$ and the kernel is degenerate $\\zeta_{1}=0$ . Let $\\epsilon=\\Omega(k^{1/2}/n)$ be the privacy parameter. Run Wrapper $^{\\,l}$ with $f\\,=\\,a l l,$ , failure probability $\\alpha$ , and algorithm ${\\mathcal{A}}=P$ rivateMeanLocalH\u00e1jek (Algorithm $^{\\,l}$ ) with $\\xi=O(C\\sqrt{k/n}\\log(n/\\alpha))$ , to output $\\tilde{\\theta}$ . With probability $1-\\alpha$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left|\\tilde{\\theta}-\\theta\\right|=O\\left(\\sqrt{\\mathrm{Var}(U_{n_{\\alpha}})}\\right)+\\tilde{O}\\left(\\frac{k^{1.5}}{n_{\\alpha}^{1.5}\\epsilon}C+\\frac{k^{2}}{n_{\\alpha}^{2}\\epsilon^{2}}C+\\frac{k^{3}}{n_{\\alpha}^{3}\\epsilon^{3}}C\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Obtaining a result for sub-Gaussian degenerate kernels poses difficulties on bounding the concentration parameter $\\xi$ . However, for bounded kernels, we see that the above result obtains better private error than the application of off-the-shelf methods (Lemma 1). In the next subsection, we provide a lower bound for degenerate bounded kernels, which, together with Corollary 2, gives strong indication that our algorithm achieves optimal private error for private degenerate kernels. ", "page_idx": 7}, {"type": "text", "text": "4.4 Lower bound ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To obtain a lower bound of the\u221a private error, we construct a dataset and kernel function such that the local H\u00e1jek projections are $1/\\sqrt{n}$ concentrated around the corresponding U-statistic. This is one way of characterizing a degenerate U-statistic. The proof of the following theorem is in Appendix A.4. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3. For any $n,k\\in\\mathbb{N}$ with $k\\leq n$ , $\\epsilon=\\Omega((k/n)^{1-1/2(k-1)})$ , and $\\epsilon$ -differentially private algorithm $A:\\,\\mathcal{X}^{n}\\,\\rightarrow\\,\\mathbb{R}$ , there exists a function $h\\;:\\;\\mathcal{X}^{k}\\;\\rightarrow\\;\\{0,1\\}$ and dataset $D$ such that $|\\hat{h}(i)\\!-\\!U_{n}|\\leq\\sqrt{k/n}\\,(w h e r e\\,\\hat{h}(i)$ and $U_{n}$ are computed on $D$ ) for every $i\\in[n]$ and $\\mathbb{E}|{\\mathcal{A}}(D)-U_{n}|=$ $\\begin{array}{r}{\\Omega\\left(\\frac{k^{3/2}}{n^{3/2}\\epsilon}\\right)}\\end{array}$ , where the expectation is taken over the randomness of $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 8}, {"type": "text", "text": "Remark 3. The above lower bound is in some sense informal because we created a deterministic dataset and $h$ that mimics the property of a degenerate $U$ statistic; the local H\u00e1jek projections concentrate around $U_{n}$ at a rate $\\sqrt{k/n}$ . However, it gives us a strong reason to believe that the private error cannot be smaller than $O(k^{3/2}/n^{3/2}\\epsilon)$ for degenerate $U$ statistics of order $k$ . Note that for bounded kernels, Corollary 2 does achieve this bound, as opposed to Lemma $^{\\,I}$ . ", "page_idx": 8}, {"type": "text", "text": "4.5 Subsampling estimator ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now focus on subsampled U-statistics. Previous work has shown how to use random subsampling to obtain computationally efficient approximations of U-statistics [38, 52, 17], where the sum is replaced with an average of samples (drawn with or without replacement) from $\\mathcal{T}_{n,k}$ . ", "page_idx": 8}, {"type": "text", "text": "Recall Definition 2. Let $\\mathcal{S}:=\\{S_{1},\\ldots,S_{M}\\}$ denote the subsampled set of subsets, let $S_{i}:=\\{S\\in\\$ $S:i\\in S\\}$ , and let $M_{i}:=|S_{i}|$ . The proof of Theorem 2 with $S={\\mathcal{T}}_{n,k}$ (cf. Appendix A.5) uses the property of $\\mathcal{T}_{n,k}$ that $M_{i}/M=k/n$ and $M_{i j}/M_{i}=(k-1)/(n-\\overset{.}{1})$ , so the inequalities (10) certainly hold. Indeed, we can show that for subsampled data (cf. Lemma A.11), the following inequalities hold with probability at least $1-\\alpha$ , provided $M=\\Omega(n^{2}/k^{2}\\log(n/\\alpha))$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\nM_{i}/M\\leq3k/n\\;\\;\\;\\mathrm{and}\\;\\;\\;M_{i j}/M_{i}\\leq3k/n.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Algorithmically, we check if the bounds (17) hold for $\\boldsymbol{S}$ , and output $\\bot$ if not. Privacy is not compromised because the check only depends on $\\boldsymbol{S}$ and is agnostic to the data. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4. Let $M_{n}=\\Omega\\left((n^{2}/k^{2})\\log n\\right)$ . Then Algorithm $^{\\,l}$ , modified to output $\\perp$ if the bounds (17) do not hold, is 10\u03f5-differentially private. Moreover, suppose that with probability at least 0.99, we have $\\mathrm{max}_{i}\\,|\\hat{h}_{S}(i)-A_{n}|\\leq\\xi$ and $|h(S)-\\theta|\\leq C$ for all $S\\in{\\mathcal{Z}}_{n,k}$ . Run Wrapper $^{\\,l}$ with $f=s s$ , failure probability $\\alpha$ , and $A=P;$ rivateMeanLocalHajek (Algorithm $^{\\,l}$ ) to output $\\tilde{\\theta}$ . With probability at least $1-\\alpha$ , we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n|A(\\mathbf{X})-\\theta|=O\\left(\\sqrt{\\mathrm{Var}(U_{n_{\\alpha}})}+\\sqrt{\\frac{\\zeta_{k}}{M_{n_{\\alpha}}}}+\\frac{k\\xi}{n_{\\alpha}\\epsilon}+\\left(\\frac{k^{2}C}{n_{\\alpha}^{2}\\epsilon^{2}}+\\frac{k^{3}C}{n_{\\alpha}^{3}\\epsilon^{3}}\\right)\\operatorname*{min}\\left(k,\\frac{1}{\\epsilon}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Remark 4. If the kernel is non-degenerate and the number of times we subsample (for each run of the algorithm) is $\\tilde{\\Omega}\\left(n_{\\alpha}^{2}/k^{2}\\right)$ , then Theorem $^{4}$ nearly achieves the same error as Algorithm $^{\\,l}$ with $S={\\mathcal{T}}_{n,k}$ with a better computational complexity for $k\\geq3$ . The lower-order terms have an additional $\\operatorname*{min}(k,1/\\epsilon)$ factor, which can be removed with $\\Omega(n^{3})$ subsamples. ", "page_idx": 8}, {"type": "text", "text": "5 Applications ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We apply our methods to private uniformity testing and estimation in random networks. For more applications, see Appendix A.6.4. ", "page_idx": 8}, {"type": "text", "text": "1. Uniformity testing: A fundamental task in distributional property testing [6, 7] is deciding whether a discrete distribution is uniform on its domain, called the problem of uniformity testing. Let $X_{1},X_{2},\\ldots,X_{n}$ be $n$ i.i.d. samples from a discrete distribution with support $[m]$ , characterized by the probability masses $p_{1},p_{2},\\ldots,p_{m}$ on the atoms. Given an error toleranc\u221ae $\\delta>0$ , the task is to distinguish between approximately uniform distributions $\\left\\{p:\\ell_{2}(p,U)\\leq\\delta/{\\sqrt{2m}}\\right\\}$ and far-fromuniform distributions $\\{p:\\ell_{2}(p,U)\\geq\\delta/{\\sqrt{m}}\\}$ . ", "page_idx": 8}, {"type": "text", "text": "Without the constraint of privacy, Diakonikolas et al. [21] perform this test by rejecting the uniformity hypothesis whenever the test statistic $\\begin{array}{r}{U_{n}:=\\sum_{i<j}\\mathbb{1}(\\bar{X_{i}^{\\cdot}}=X_{j})/\\binom{n}{k}>(\\mathbb{1}+\\overleftarrow{\\mathfrak{3}\\delta^{2}}/\\bar{4})/m}\\end{array}$ , and show that this test succeeds with probability 0.9 as long as $n=\\Omega\\left(m^{1/2}/\\delta^{2}\\right)$ . As detailed in Algorithm A.4 in the appendix, instead of using $U_{n}$ , we use the private estimate ${\\tilde{U}}_{n}$ using Algorithm 1. ", "page_idx": 8}, {"type": "text", "text": "For our algorithm to work, we require the distributions to satisfy $p_{i}\\,\\leq\\,2/m$ for all $i$ . Let $p_{i}=$ $(1+a_{i})/m$ for all $i$ , with $a_{i}\\in[-1,1]$ . Under $H_{1}$ , we have $\\mathbb{E}[\\mathbb{1}(X_{1}=X_{2})]=(1+\\left\\Vert a\\right\\Vert^{2}/m)/m\\ge$ $(1+\\delta^{2})/m$ , where $\\lVert a\\rVert$ is the $\\ell_{2}$ norm of $(a_{1},a_{2},\\ldots,a_{m})$ . Under $H_{0}$ , the mean is $1/m$ . The difference between the threshold $(1+\\delta^{2}/2)/m$ and the mean (under either of the two hypotheses) is at least $\\delta^{2}/(2m)$ . Moreover, [21] shows that the standard deviation of $U_{n}$ is much smaller than the difference in the means under $H_{0}$ and $H_{1}$ as long as $n\\,=\\,\\Omega(m^{1/2}/\\delta^{2})$ . However, we must also account for the noise added to ensure privacy. In Appendix A.6.1, we show that the choice of $\\xi=\\tilde{\\Theta}(1/m+1/n)$ works and establish the following result: ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Theorem 5. Let $p_{i}=(1\\!+\\!a_{i})/m$ with $a_{i}\\in[-1,1]$ , $\\textstyle\\sum_{i=1}^{m}a_{i}=0.$ . Let $\\{X_{j}\\}_{j=1}^{n}$ be i.i.d. multinomial random variables such that $P(X_{1}\\,=\\,i)\\,=\\,p_{i}$ , for all $i\\,\\in\\,[m]$ . There exists an algorithm that distinguishes between $\\begin{array}{r l r}{\\frac{\\|a\\|^{2}}{m^{2}}}&{{}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\frac{\\|{a}||^{2}}{m}}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!$ from $\\begin{array}{r l r}{\\frac{\\|a\\|^{2}}{m^{2}}}&{{}<}&{\\frac{\\delta^{2}}{2m}}\\end{array}$ with probability at least $1-\\alpha$ , as long as $\\begin{array}{r}{n_{\\alpha}=\\Omega\\left(\\frac{m^{1/2}}{\\delta^{2}}+\\frac{m^{1/2}}{(\\delta\\epsilon)}+\\frac{m^{1/2}\\log(m/\\delta\\epsilon)}{\\delta\\epsilon^{1/2}}+\\frac{m^{1/3}}{\\delta^{2/3}\\epsilon}+\\frac{1}{\\delta^{2}}\\right)}\\end{array}$ , and is $10\\epsilon$ -differentially private. ", "page_idx": 9}, {"type": "text", "text": "The non-private error term of Theorem 5 is the same as in Theorem 1 of [21] and is optimal [22]. Proposition 1 shows that Algorithm A.2 with the all-tuples family leads to a private error bounded by $\\tilde{O}(1/n\\epsilon)$ . This private error is ${\\cal O}(\\delta^{2}/m)$ only when $n=\\Omega(m/\\delta^{2}\\epsilon)$ . In comparison, Algorithm A.4 has error ${\\cal O}(\\delta^{2}/m)$ for $n=\\Omega\\left(m^{1/2}/\\mathrm{min}(\\delta^{2},\\delta\\epsilon)\\right)$ , which is quadratically better in $m$ . ", "page_idx": 9}, {"type": "text", "text": "Remark 5 (Comparison with existing algorithms). Existing results for private uniformity testing $I I$ , $I3J$ distinguish between the uniform distributions $(\\ell_{1}(p,U)\\;=\\;0)$ ) and distributions away from uniform in TV-\u221adistance $(\\ell_{1}(p,U)\\geq\\delta)$ ). Our algorithm considers the alternative hypothesis to be $\\ell_{2}(p,U)\\geq\\delta/\\sqrt{m}$ . Hence, our results are not strictly comparable. One caveat is that we restrict ourselves to distributions $p$ such that $\\ell_{\\infty}(p,U)\\leq1/m$ . Our algorithm also allows some tolerance in the null hypothesis, similar to $I2I J$ and other collision-based testers. That is, can allow some slack and take the null hypothesis $H_{0}:\\ell_{2}(p,U)\\leq\\delta/\\sqrt{2m}$ instead of $\\ell_{2}(p,U)=0$ . ", "page_idx": 9}, {"type": "text", "text": "2. Sparse graph statistics: The geometric random graph (see [30]) has edges $h(X_{i},X_{j})\\;:=$ $\\bar{\\mathbb{1}}(\\|\\bar{X_{i}}-\\bar{X_{j}^{\\prime}}\\|_{2}^{-}\\leq\\;r_{n})$ , where $r_{n}$ governs the average degree. Under a suitable distribution for $X_{1},\\ldots,X_{n}$ , the subgraph counts show normal convergence for a large range of $r_{n}$ [30]. Typically, we only observe the graph and do not know the underlying distribution of the latent variables $X_{i}$ or the radius $r_{n}$ . This is why estimates of the network moments are of interest since they reveal information about the underlying unknown distribution and parameters. ", "page_idx": 9}, {"type": "text", "text": "Let the $X_{i}$ \u2019s be uniformly distributed on the three-dimensional sphere to ignore boundary conditions. For edge density, $\\mathbb{E}h(\\bar{X_{i}},X_{j})\\,\\propto\\,r_{n}^{2}$ . For any distinct indices $i,j,k$ and a given $X_{i}$ , the random variables $h(X_{i},X_{j})$ and $h(\\bar{X_{i}},X_{k})$ are independent. Therefore, $\\zeta_{1}=\\operatorname{cov}(h(X_{i},X_{j})h(X_{i},X_{k}))=$ 0. We have $\\zeta_{2}=\\operatorname{var}[h(X_{i},X_{j})]=O(r_{n}^{2})$ , so the non-private error is ${\\cal O}(r_{n}/n)$ . In Appendix A.6.2, we provide Algorithm A.5 that uses Algorithm 1 to obtain a private estimate of the edge density of a graph $\\{h(X_{i},\\mathbf{\\bar{\\boldsymbol{X}}}_{j})\\}_{1\\leq i<j\\leq n}.$ Note that the $X_{i}$ \u2019s themselves can be unknown. Our methods can also be used for private triangle density estimation. See the extended version [16] for details. ", "page_idx": 9}, {"type": "text", "text": "Theorem 6. Let $r_{n}=\\tilde{\\Omega}(n^{-1/2})$ and $\\epsilon=\\Omega\\left(1/n r_{n}^{2}\\right)$ . Let $\\{X_{1},\\ldots,X_{n}\\}$ be i.i.d. latent positions such that $X_{i}$ is distributed uniformly on $\\partial\\mathbb{S}^{2}$ . Let the observed geometric network have adjacency matrix $\\{A_{i j},1\\leq i<j\\leq n\\}$ where $A_{i j}=1(\\|X_{i}-X_{j}\\|\\leq r_{n})$ . There exists a $10\\epsilon$ -differentially private algorithm that estimates the edge density $\\theta$ of the geometric graph. With probability at least 1 \u2212\u03b1, the output \u03b8\u02dc satisfies |\u03b8\u02dc \u2212\u03b8| = O\u02dc nrn\u03b1 +n2\u03b11\u03f52 +n3\u03b11\u03f53 ", "page_idx": 9}, {"type": "text", "text": "Remark 6. By Lemma $^{\\,l}$ , the all-tuples estimator $(I)$ satisfies $\\lvert\\tilde{\\theta}_{a l l}-\\theta\\rvert\\le\\tilde{O}\\left(r_{n}/n+\\sqrt{\\tau}/n\\epsilon\\right)$ with probability $1-\\alpha,$ , where $\\tau$ is the variance proxy of the distribution. Since $\\tau={\\tilde{\\Omega}}(1)\\;{[4]},$ the private error overpowers the main variance term in sparse settings where $r_{n}=o(1)$ . ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have considered the problem of estimating $\\theta:=\\mathbb{E}h(X_{1},\\ldots,X_{k})$ for a broad class of kernel functions $h$ . The best non-private unbiased estimator is a $\\mathrm{U}$ statistic, which is widely used in estimation and hypothesis testing. While existing private mean estimation algorithms can be used for this setting, they can be suboptimal for large $k$ or for non-degenerate $\\mathrm{U}$ statistics, which have ${\\cal O}(1/n)$ limiting variance. We provide lower bounds for both degenerate and non-degenerate settings. We analyze bounded degenerate kernels motivated by typical applications with degenerate U statistics. To extend this to the subgaussian setting is part of future work. We propose an algorithm that matches our lower bounds for sub-Gaussian non-degenerate kernels and bounded degenerate kernels. We also provide applications of our theory to private hypothesis testing and estimation in sparse graphs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "PS and SP gratefully acknowledge NSF grants 2217069, 2019844, and DMS 2109155. We thank the reviewers and Gautam Kamath for their insightful comments and suggestions. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] J. Acharya, Z. Sun, and H. Zhang. Differentially private testing of identity and closeness of discrete distributions. Advances in Neural Information Processing Systems, 31, 2018.   \n[2] M. Aliakbarpour, I. Diakonikolas, D. Kane, and R. Rubinfeld. Private testing of distributions via sample permutations. Advances in Neural Information Processing Systems, 32, 2019.   \n[3] T. W. Anderson and D. Darling. Asymptotic theory of certain \"goodness of fit\" criteria based on stochastic processes. Annals of Mathematical Statistics, 23:193\u2013212, 1952.   \n[4] Julyan Arbel, Olivier Marchal, and Hien D Nguyen. On strict sub-gaussianity, optimal proxy variance and symmetry for bounded random variables. ESAIM: Probability and Statistics, 24:39\u201355, 2020.   \n[5] M. A. Arcones and E. Gine. Limit theorems for $U$ -processes. The Annals of Probability, 21(3):1494 \u2013 1542, 1993.   \n[6] Tugkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D Smith, and Patrick White. Testing that distributions are close. In Proceedings 41st Annual Symposium on Foundations of Computer Science, pages 259\u2013269. IEEE, 2000.   \n[7] Tu\u02d8gkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D Smith, and Patrick White. Testing closeness of discrete distributions. Journal of the ACM (JACM), 60(1):1\u201325, 2013.   \n[8] J. Bell, A. Bellet, A. Gasc\u00f3n, and T. Kulkarni. Private protocols for U-statistics in the local model and beyond. In International Conference on Artificial Intelligence and Statistics, pages 1573\u20131583. PMLR, 2020.   \n[9] Sergei Bernstein. On a modification of chebyshev\u2019s inequality and of the error formula of laplace. Ann. Sci. Inst. Sav. Ukraine, Sect. Math, 1(4):38\u201349, 1924.   \n[10] S. Biswas, Y. Dong, G. Kamath, and J. Ullman. Coinpress: Practical private mean and covariance estimation. Advances in Neural Information Processing Systems, 33:14475\u201314485, 2020.   \n[11] Gavin Brown, Samuel Hopkins, and Adam Smith. Fast, sample-efficient, affine-invariant private mean and covariance estimation for subgaussian distributions. In The Thirty Sixth Annual Conference on Learning Theory, pages 5578\u20135579. PMLR, 2023.   \n[12] C. Butucea, A. Rohde, and L. Steinberger. Interactive versus noninteractive locally differentially private estimation: Two elbows for the quadratic functional. The Annals of Statistics, 51(2):464\u2013 486, 2023.   \n[13] Bryan Cai, Constantinos Daskalakis, and Gautam Kamath. Priv\u2019it: Private and sample efficient identity testing. In International Conference on Machine Learning, pages 635\u2013644. PMLR, 2017.   \n[14] T. T. Cai, Y. Wang, and L. Zhang. The cost of privacy: Optimal rates of convergence for parameter estimation with differential privacy. The Annals of Statistics, 49(5):2825\u20132850, 2021.   \n[15] K. Chaudhuri and D. Hsu. Convergence rates for differentially private statistical estimation. In Proceedings of the International Conference on Machine Learning, volume 2012, page 1327. NIH Public Access, 2012.   \n[16] Kamalika Chaudhuri, Po-Ling Loh, Shourya Pandey, and Purnamrita Sarkar. On differentially private u statistics, 2024.   \n[17] X. Chen and K. Kato. Randomized incomplete $U$ -statistics in high dimensions. The Annals of Statistics, 47(6):3127 \u2013 3156, 2019.   \n[18] S. Cl\u00e9men\u00e7on. A statistical view of clustering performance through the theory of U-processes. Journal of Multivariate Analysis, 124:42\u201356, 2014.   \n[19] S. Cl\u00e9men\u00e7on, G. Lugosi, and N. Vayatis. Ranking and empirical minimization of U-statistics. The Annals of Statistics, 36(2):844\u2013874, 2008.   \n[20] Tertius de Wet. Degenerate u- and v-statistics. South African Statistical Journal, 21:99\u2013129, 1987.   \n[21] Ilias Diakonikolas, Themis Gouleakis, John Peebles, and Eric Price. Collision-based testers are optimal for uniformity and closeness. arXiv preprint arXiv:1611.03579, 2016.   \n[22] Ilias Diakonikolas, Themis Gouleakis, John Peebles, and Eric Price. Optimal identity testing with high probability. arXiv preprint arXiv:1708.02728, 2017.   \n[23] A. Dubois, T. B. Berrett, and C. Butucea. Goodness-of-fti testing for H\u00f6lder continuous densities under local differential privacy. In Foundations of Modern Statistics, pages 53\u2013119. Springer, 2019.   \n[24] John Duchi, Saminul Haque, and Rohith Kuditipudi. A fast algorithm for adaptive private mean estimation. arXiv preprint arXiv:2301.07078, 2023.   \n[25] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265\u2013284. Springer, 2006.   \n[26] Andrey Feuerverger and Roman A. Mureika. The Empirical Characteristic Function and Its Applications. The Annals of Statistics, 5(1):88 \u2013 97, 1977.   \n[27] E. W. Frees. Infinite order U-statistics. Scandinavian Journal of Statistics, 16(1):29\u201345, 1989.   \n[28] M. Gaboardi, H. Lim, R. Rogers, and S. Vadhan. Differentially private chi-squared hypothesis testing: Goodness of fit and independence testing. In International conference on Machine Learning, pages 2111\u20132120. PMLR, 2016.   \n[29] B. Ghazi, P. Kamath, R. Kumar, P. Manurangsi, and A. Sealfon. On computing pairwise statistics with local differential privacy. Advances in Neural Information Processing Systems, 33:14475\u201314485, 2020.   \n[30] E. N. Gilbert. Random plane networks. Journal of The Society for Industrial and Applied Mathematics, 9:533\u2013543, 1961.   \n[31] G. Gregory. Large sample theory for $U$ -statistics and tests of fti. Annals of Statistics, 5:110\u2013123, 1977.   \n[32] P. R. Halmos. The theory of unbiased estimation. The Annals of Mathematical Statistics, 17(1):34 \u2013 43, 1946.   \n[33] Moritz Hardt and Kunal Talwar. On the geometry of differential privacy. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 705\u2013714, 2010.   \n[34] H.-C. Ho and G. S. Shieh. Two-stage U-statistics for hypothesis testing. Scandinavian Journal of Statistics, 33, 2006.   \n[35] W. Hoeffding. A class of statistics with asymptotically normal distribution. The Annals of Mathematical Statistics, 19(3):293 \u2013 325, 1948.   \n[36] Wassily Hoeffding. The strong law of large numbers for u-statistics. Technical report, North Carolina State University. Dept. of Statistics, 1961.   \n[37] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58(301):13\u201330, 1963.   \n[38] S. Janson. The asymptotic distributions of incomplete U-statistics. Zeitschrift f\u00fcr Wahrscheinlichkeitstheorie und Verwandte Gebiete, 66(4):495\u2013505, 1984.   \n[39] G. Kamath, J. Li, V. Singhal, and J. Ullman. Privately learning high-dimensional distributions. In Conference on Learning Theory, pages 1853\u20131902. PMLR, 2019.   \n[40] G. Kamath, O. Sheffet, V. Singhal, and J. Ullman. Differentially private algorithms for learning mixtures of separated Gaussians. Advances in Neural Information Processing Systems, 32, 2019.   \n[41] G. Kamath, V. Singhal, and J. Ullman. Private mean estimation of heavy-tailed distributions. ArXiv, abs/2002.09464, 2020.   \n[42] V. Karwa and S. Vadhan. Finite sample differentially private confidence intervals. arXiv preprint arXiv:1711.03908, 2017.   \n[43] S. P. Kasiviswanathan, H. K. Lee, K. Nissim, S. Raskhodnikova, and A. Smith. What can we learn privately? SIAM Journal on Computing, 40(3):793\u2013826, 2011.   \n[44] J. Lam-Weil, B. Laurent, and J.-M. Loubes. Minimax optimal goodness-of-fit testing for densities and multinomials under a local differential privacy constraint. Bernoulli, 28(1):579\u2013 600, 2022.   \n[45] A J Lee. U-statistics: Theory and Practice. Routledge, 2019.   \n[46] C. Li and X. Fan. On nonparametric conditional independence tests for continuous variables. WIREs Computational Statistics, 12(3):e1489, 2020.   \n[47] O. Linton and P. Gozalo. Testing conditional independence restrictions. Econometric Reviews, 33(5-6):523\u2013552, 2014.   \n[48] S. Minsker. U-statistics of growing order and sub-Gaussian mean estimators with sharp constants, 2023.   \n[49] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth sensitivity and sampling in private data analysis. In Proceedings of the Thirty-Ninth Annual ACM Symposium on Theory of Computing, pages 75\u201384, 2007.   \n[50] W. Peng, T. Coleman, and L. Mentch. Asymptotic distributions and rates of convergence for random forests via generalized U-statistics. arXiv preprint arXiv:1905.10651, 2019.   \n[51] Yannik Pitcan. A note on concentration inequalities for u-statistics. arXiv preprint arXiv:1712.06160, 2017.   \n[52] D. N. Politis, J. P. Romano, and M. Wolf. Subsampling. Springer Science & Business Media, 2012.   \n[53] T. Popoviciu. Sur certaines in\u00e9galit\u00e9s qui caract\u00e9risent les fonctions convexes. Analele Stiintifice Univ.\u201cAl. I. Cuza\u201d, Iasi, Sectia Mat, 11:155\u2013164, 1965.   \n[54] R. J. Serfling. Approximation theorems of mathematical statistics. Wiley Series in Probability and Mathematical Statistics : Probability and Mathematical Statistics. Wiley, New York, NY [u.a.], [nachdr.] edition, 1980.   \n[55] Galen R Shorack and Jon A Wellner. Empirical processes with applications to statistics. SIAM, 2009.   \n[56] Yanglei Song, Xiaohui Chen, and Kengo Kato. Approximating high-dimensional infinite-order $u$ -statistics: Statistical and computational guarantees. Electronic Journal of Statistics, 13(2), January 2019.   \n[57] J. Ullman and A. Sealfon. Efficiently estimating erdos-renyi graphs with node differential privacy. Advances in Neural Information Processing Systems, 32, 2019.   \n[58] A. W. van der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 1998.   \n[59] R. Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.   \n[60] M. J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint, volume 48. Cambridge University Press, 2019.   \n[61] N. C. Weber. Incomplete degenerate u-statistics. Scandinavian Journal of Statistics, 8(2):120\u2013 123, 1981. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Roadmap of Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1. Section A.1: U statistics and concentration of U statistics ", "page_idx": 14}, {"type": "text", "text": "2. Section A.2: Composition theorems for differential privacy   \n3. Section A.3: Details on our extension of the Coinpress algorithm [10]   \n4. Section A.4: Proofs of lower bounds   \n5. Section A.5: Proofs of the main theorems 2 and 4   \n6. Section A.6: More applications and details on uniformity testing and edge-density estimation ", "page_idx": 14}, {"type": "text", "text": "A.1 U-Statistics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let $h:\\mathcal{X}^{k}\\,\\rightarrow\\,\\mathbb{R}$ be a symmetric function, and let $X_{1},\\ldots,X_{n}\\,\\in\\,{\\mathcal{X}}$ . The $\\mathrm{U}$ statistic on the $n$ variables $X_{1},\\ldots,X_{n},U_{n}\\overset{\\cdot}{(h)}$ , associated with $h$ is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\nU_{n}={\\frac{1}{\\binom{n}{k}}}\\sum_{\\{i_{1},\\dots,i_{k}\\}\\in{\\mathbb{Z}}_{n,k}}h(X_{i_{1}},\\dots,X_{i_{k}}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The mean of $U_{n}$ based on iid variables $X_{1},\\ldots,X_{n}\\sim{\\mathcal{D}}$ , for some distribution $\\mathcal{D}$ on $\\mathcal{X}$ , is simply $\\theta:=\\mathbb{E}[h(X_{1},\\cdot\\cdot\\cdot,X_{k})]$ . Moreover, the variance of $U_{n}$ can be expressed succinctly in terms of conditional expectations [45]. For $c=1,2,\\ldots,k$ , define $h_{c}:\\mathcal{X}^{c}\\to\\mathbb{R}$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\nh_{c}(X_{1},\\ldots,X_{c}):=\\mathbb{E}[h(X_{1},\\ldots,X_{k})|X_{1}=x_{1},\\ldots,X_{c}=x_{c}],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and let ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\zeta_{c}=\\operatorname{Var}\\left(h_{c}(X_{1},\\ldots,X_{c})\\right).}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Equivalently, $\\zeta_{c}=\\operatorname{cov}\\left(h(X_{S_{1}}),h(X_{S_{2}})\\right)$ where $S_{1},S_{2}\\in{\\mathcal{I}}_{n,k}$ and $|S_{1}\\cap S_{2}|=c$ . The number of such pairs of sets $S_{1}$ and $S_{2}$ is equal to $\\left(\\!_{k}^{n}\\!\\right)\\left(\\!_{c}^{k}\\!\\right)\\left(\\!_{k-c}^{n-k}\\!\\right)$ , which implies implies $\\operatorname{Eq}3$ . ", "page_idx": 14}, {"type": "text", "text": "Hoeffding decomposition. A U statistic of degree $k$ can be written as the sum of uncorrelated U statistics of degrees $1,2,\\ldots,k$ . Define ", "page_idx": 14}, {"type": "equation", "text": "$$\nh^{(1)}(X_{1})=h(X_{1})-\\theta\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and for all $2\\le c\\le k$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nh^{\\left(c\\right)}(X_{1},\\ldots,X_{c})=\\left(h_{c}(X_{1},\\ldots,X_{c})-\\theta\\right)-\\sum_{\\phi\\subseteq S\\subseteq{\\mathbb{Z}}_{c,i}}h^{\\left(i\\right)}\\left(X_{S}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, $U_{n}$ can be written as ", "page_idx": 14}, {"type": "equation", "text": "$$\nU_{n}=\\theta+\\sum_{c=1}^{k}\\binom{k}{c}U_{n}^{(c)},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $U_{n}^{(c)}$ is the $\\mathrm{U}$ statistic on $X_{1},\\ldots,X_{n}$ based on the kernel $h^{(c)}$ . Equation A.21 is called the Hoeffding decomposition (or the H-decomposition) of $U_{n}$ [36]. [36] also shows that the $c$ functions $h^{(1)},\\ldots,h^{(k)}$ are pairwise uncorrelated. That is, let $1\\leq c<d\\leq k$ and let $S_{c}$ and $S_{d}$ be subsets of $[n]$ of sizes $c$ and $d$ respectively. Then, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname{cov}\\left(h^{(c)}(X_{S_{c}}),h^{(d)}(X_{S_{d}})\\right)=0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This allows us to write the variance of $U_{n}$ in terms of the variances of $h^{(c)}$ . For all $c\\in[k]$ , define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\delta_{c}^{2}=\\mathrm{Var}(h^{(c)}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname{Var}(U_{n})=\\sum_{c=1}^{k}{\\binom{k}{c}}^{2}{\\binom{n}{c}}^{-1}\\delta_{c}^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Moreover, the conditional covariances $\\zeta_{c}$ are related to the variances $\\delta_{c}^{2}$ in the following manner: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\zeta_{c}=\\sum_{i=1}^{c}{\\binom{c}{i}}\\delta_{i}^{2},\\qquad\\delta_{c}^{2}=\\sum_{i=1}^{c}(-1)^{c-i}{\\binom{c}{i}}\\zeta_{i}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma A.1. Suppose $k\\leq n/2$ . ", "page_idx": 15}, {"type": "text", "text": "(i) If $\\zeta_{1}>0$ , then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{Var}(U_{n})={\\frac{k^{2}\\zeta_{1}}{n}}+O\\left(\\zeta_{k}{\\frac{k^{2}}{n^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(ii) If $\\zeta_{1}=0$ and $\\zeta_{2}>0$ , then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Var}(U_{n})=\\frac{k^{2}(k-1)^{2}\\zeta_{2}}{2n(n-1)}+O\\left(\\zeta_{k}\\frac{k^{3}}{n^{3}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. This result follows directly from a calculation appearing in the proof of Theorem 3.1 in [48]. Note that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\zeta_{k}=\\sum_{j=1}^{k}{\\binom{k}{j}}\\delta_{j}^{2}\\geq{\\binom{k}{j}}\\delta_{j}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for all $j\\in[k]$ . Moreover, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{Var}(U_{n})=\\sum_{j=1}^{k}{\\binom{k}{j}}^{2}{\\binom{n}{j}}^{-1}\\delta_{j}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For part (i), we write ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}(U_{n})=\\cfrac{k^{2}\\zeta_{1}}{n}+\\displaystyle\\sum_{j=2}^{k}{\\binom{k}{j}}^{2}{\\binom{n}{j}}^{-1}\\delta_{j}^{2}\\leq\\cfrac{k^{2}\\zeta_{1}}{n}+\\displaystyle\\sum_{j=2}^{k}{\\binom{k}{j}}{\\binom{n}{j}}^{-1}\\zeta_{k}}\\\\ &{\\qquad\\leq\\cfrac{k^{2}\\zeta_{1}}{n}+\\zeta_{k}\\displaystyle\\sum_{j=2}^{k}{\\left(\\frac{k}{n}\\right)}^{j}\\leq\\cfrac{k^{2}\\zeta_{1}}{n}+\\cfrac{k^{2}\\zeta_{k}}{n^{2}}\\left(1-\\cfrac{k}{n}\\right)^{-1}\\leq\\cfrac{k^{2}\\zeta_{1}}{n}+\\cfrac{2k^{2}\\zeta_{k}}{n^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For part (ii), we write ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}(U_{n})=\\displaystyle\\frac{k^{2}\\zeta_{1}}{n}+\\frac{k^{2}(k-1)^{2}\\zeta_{2}}{2n(n-1)}+\\sum_{j=3}^{k}\\frac{\\binom{k}{j}^{2}}{\\binom{n}{j}}\\zeta_{j}\\leq\\frac{k^{2}(k-1)^{2}\\zeta_{2}}{2n(n-1)}+\\zeta_{k}\\sum_{j=3}^{k}\\frac{\\binom{k}{j}}{\\binom{n}{j}}}\\\\ &{\\qquad\\leq\\frac{k^{2}(k-1)^{2}\\zeta_{2}}{2n(n-1)}+\\zeta_{k}\\displaystyle\\sum_{j=3}^{k}\\left(\\frac{k}{n}\\right)^{j}=\\frac{k^{2}(k-1)^{2}\\zeta_{2}}{2n(n-1)}+\\frac{2k^{3}\\zeta_{k}}{n^{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma A.2. For all $1\\leq c\\leq d\\leq k$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\zeta_{c}}{c}\\leq\\frac{\\zeta_{d}}{d}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In particular, $k\\zeta_{1}\\leq\\zeta_{k}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Using equation A.24, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\zeta_{c}}{c}=\\sum_{i=1}^{c}\\frac{1}{c}\\binom{c}{i}\\delta_{i}^{2}=\\sum_{i=1}^{c}\\frac{1}{i}\\binom{c-1}{i-1}\\delta_{i}^{2}\\leq\\sum_{i=1}^{c}\\frac{1}{i}\\binom{d-1}{i-1}\\delta_{i}^{2}\\leq\\sum_{i=1}^{d}\\frac{1}{d}\\binom{d}{i}\\delta_{i}^{2}=\\frac{\\zeta_{d}}{d}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma A.3 (Concentration of U-statistics). [37, 9] ", "page_idx": 15}, {"type": "text", "text": "(i) If $\\mathcal{H}$ is sub-Gaussian with variance proxy $\\tau$ , then for all $t>0$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|U_{n}-\\theta|\\ge t\\right)\\le2\\exp\\left(-\\frac{\\lfloor\\frac{n}{k}\\rfloor t^{2}}{2\\tau}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(ii) If $\\mathcal{H}$ is almost surely bounded in $(-C,C)$ , then for all $t>0$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|U_{n}-\\theta\\right|\\geq t\\right)\\leq\\exp\\left(\\frac{-\\left\\lfloor\\frac{n}{k}\\right\\rfloor t^{2}}{2\\zeta_{k}+2C t/3}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Without loss of generality, let $\\theta=0$ . For any permutation $\\sigma$ of $[n]$ , let ", "page_idx": 16}, {"type": "equation", "text": "$$\nV_{\\sigma}:=\\frac{1}{m}\\sum_{i=1}^{m}h(X_{\\sigma(k(i-1)+1)},X_{\\sigma(k(i-1)+2)},\\ldots,X_{\\sigma(k i)}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $m=\\lfloor n/k\\rfloor$ . By symmetry, $\\begin{array}{r}{U_{n}=\\frac{1}{n!}\\sum_{\\sigma}V_{\\sigma}}\\end{array}$ . For any $s>0$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(U_{n}\\geq t\\right)=\\mathbb{P}\\left(e^{s U_{n}}\\geq e^{s t}\\right)\\leq e^{-s t}\\mathbb{E}\\left[e^{s U_{n}}\\right]=e^{-s t}\\mathbb{E}\\left[\\exp\\left(\\frac{s}{n!}\\sum_{\\sigma}V_{n}\\right)\\right]}\\\\ &{\\qquad\\qquad\\leq e^{-s t}\\mathbb{E}\\left[\\frac{1}{n!}\\sum_{\\sigma}\\exp(s V_{n})\\right]=e^{-s t}\\mathbb{E}\\left[\\exp(s V_{\\mathrm{id}})\\right]}\\\\ &{\\qquad\\qquad=e^{-s t}\\mathbb{E}\\left[\\exp\\left(\\frac{s}{m}h(X_{1},...\\,,X_{k})\\right)\\right]^{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If $\\mathcal{H}$ is sub-Gaussian with variance proxy $\\tau$ , then we can further bound the inequality above as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(U_{n}\\ge t\\right)\\le e^{-s t}\\left(\\exp\\left(\\frac{s^{2}\\tau}{2m^{2}}\\right)\\right)^{m}=\\exp\\left(-s t+\\frac{s^{2}\\tau}{2m}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Set $\\textstyle s={\\frac{t m}{\\tau^{2}}}$ to get the desired result. Note how the argument is similar to the classical Hoeffding\u2019s inequality argument after applying Jensen\u2019s inequality on $V_{\\sigma}$ . The second result follows similarly by adapting the tricks of Bernstein\u2019s inequality [9] to [37]; for a detailed proof, see [51]. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.2 Details on Privacy Mechanisms ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma A.4. (Basic Composition) If $A_{i}:\\mathcal{X}^{n}\\rightarrow S_{i}$ is $\\epsilon_{i}$ -differentially private for all $i\\in[k]$ , then the mechanism ${\\mathcal{A}}:{\\mathcal{X}}^{n}\\rightarrow S_{1}\\times\\cdot\\cdot\\cdot\\times S_{k}$ defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\mathcal{A}}\\left(X_{1},\\ldots,X_{n}\\right)=\\left(A_{1}\\left(X_{1},\\ldots,X_{n}\\right),\\ldots,A_{k}\\left(X_{1},\\ldots,X_{n}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "is $\\textstyle\\sum_{i=1}^{k}\\epsilon_{i}$ -differentially private. ", "page_idx": 16}, {"type": "text", "text": "Lemma A.5. (Parallel Composition) If $\\mathcal A_{i}:\\mathcal X^{n}\\rightarrow S_{i}$ is $\\epsilon$ -differentially private for all $i\\in[k]$ , then the mechanism $\\mathcal{A}:\\mathcal{X}^{k n}\\rightarrow S_{1}\\times\\cdot\\cdot\\times S_{k}$ defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{A}\\left(X_{1},\\ldots,X_{k n}\\right)=\\left(A_{1}\\left(X_{1},\\ldots,X_{n}\\right),\\mathcal{A}_{2}\\left(X_{n+1},\\ldots,X_{2n}\\right),\\ldots,\\mathcal{A}_{k}\\left(X_{\\left(k-1\\right)n+1},\\ldots,X_{k n}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "is $\\epsilon$ -differentially private. ", "page_idx": 16}, {"type": "text", "text": "A.2.1 Private mean estimation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A fundamental task in private statistical inference is to privately estimate the mean based on a set of IID observations. One way to do this is via the global sensitivity method, wherein the standard deviation of the noise scales with the ratio between the range of the distribution and the size of the dataset. In the fairly realistic case where the range is large or unbounded, this leads to highly noisy estimation even in the setting where typical samples are small in size. ", "page_idx": 16}, {"type": "text", "text": "To remedy this effect, a line of work [42, 39, 14, 40, 24, 11] has looked into designing better private mean estimators for (sub)-Gaussian vectors. Our work will build on one such method: CoinPress [10]. The idea is to iteratively refine an estimate for the parameters until one obtains a small range containing most of the data with high probability; noise is then added proportional to this smaller range. Note that some dependence on the range of the mean is inevitable for estimation with pure differential privacy [33, 15]. ", "page_idx": 16}, {"type": "text", "text": "A.3 Details for Section 3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A.3.1 General result ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We will prove a more general theorem than Lemma 1, from which Lemma 1 and related Lemmas using other families of subsets are derived. ", "page_idx": 17}, {"type": "text", "text": "In Algorithms A.2 and A.3, we present a natural extension of the CoinPress algorithm [10], which is then used to obtain a private estimate of $\\theta$ with the non-private term matching $\\operatorname{Var}(U_{n})$ . Originally, this algorithm was used for private mean and covariance estimation of i.i.d. (sub)Gaussian data. We extend the algorithm to take as input data $\\{Y_{j}\\}_{j\\in[m]}$ such that (i) each $Y_{j}$ is equal to $h(X_{S})$ for some $S$ , (ii) the $Y_{j}$ \u2019s are weakly dependent on each other, and (iii) each $Y_{j}$ , as well as their mean $\\textstyle\\sum_{j\\in[m]}Y_{j}/m$ , has sufficiently strong concentration around the population mean. ", "page_idx": 17}, {"type": "text", "text": "For instance, suppose $m\\,=\\,\\lfloor n/k\\rfloor$ and $Y_{j}\\,=\\,h(X_{S_{j}})$ for all $j\\,\\in\\,[m]$ , where $S_{j}\\,=\\,\\{(j\\,-\\,1)k\\,+$ $1,\\ldots,(j\\mathrm{~-~}1)k+k\\}$ . Then Algorithm A.2 reduces to the CoinPress algorithm applied to $n/k$ independent observations $h(X_{S_{1}}),h(X_{S_{2}}),\\ldots,h(X_{S_{m}})$ . ", "page_idx": 17}, {"type": "text", "text": "$\\mathbf{Algorithm\\;A.2\\;U.StatMean}\\Big(n,k,h,\\{X_{i}\\}_{i\\in[n]}\\;,\\mathcal{F}=\\{S_{1},\\ldots,S_{m}\\},R,\\epsilon,\\gamma,Q(\\cdot),Q^{\\mathrm{avg}}(\\cdot)\\Big)$   \n1: $t\\leftarrow\\log\\left(R/Q(\\gamma)\\right),\\ \\left[l_{0},r_{0}\\right]\\leftarrow\\left[-R,R\\right]$   \n2: for $j=1,\\dots,m$ do   \n3: $Y_{0,j}\\gets h(X_{S_{j}})$   \n4: end for   \n5: for $i=1,2,...,t$ do   \n6: $\\begin{array}{r}{\\{Y_{i,j}\\}_{j\\in[m]},\\left[l_{i},r_{i}\\right]\\gets\\mathrm{U-StatOneStep}\\left(n,k,\\{Y_{i-1,j}\\},\\mathcal{F},[l_{i-1},r_{i-1}],\\frac{\\epsilon}{2t},\\frac{\\gamma}{t},Q(\\cdot),Q^{\\mathrm{avg}}(\\cdot)\\right)}\\end{array}$   \n7: end for   \n8: $\\{Y_{t+1,j}\\}_{j\\in[m]},[l_{t+1},r_{t+1}]\\gets\\mathsf{U-S t a t O n e S t e p}\\left(n,k,\\{Y_{t,j}\\},\\mathcal{F},[l_{t},r_{t}],\\epsilon/2,\\gamma,Q(\\cdot),Q^{\\mathrm{avg}}(\\cdot)\\right)$   \n9: return $(l_{t+1}+r_{t+1})/2$   \n$\\mathbf{Algorithm\\A.3\\U.Stat{OneStep}}\\Big(n,k,\\{Y_{i}\\}_{i\\in[m]}\\,,\\mathcal{F},[l,r],\\epsilon^{\\prime},\\beta,Q(\\cdot),Q^{\\mathrm{avg}}(\\cdot)\\Big)$   \n1: $Y_{j}\\gets\\mathrm{proj}_{l-Q(\\beta),r+Q(\\beta)}\\left(Y_{j}\\right)$ for all $1\\leq j\\leq m$ .   \n2: $\\Delta\\gets\\mathrm{dep}_{n,k}\\left(\\mathcal{F}\\right)\\left(r-l+2Q(\\beta)\\right)$   \n3: $\\begin{array}{r}{Z\\gets\\frac{1}{m}\\sum_{j=1}^{m}Y_{j}+W}\\end{array}$ , where $\\begin{array}{r}{W\\sim\\mathrm{Lap}\\left(\\frac{\\Delta}{\\epsilon^{\\prime}}\\right)}\\end{array}$   \n4: $\\begin{array}{r}{[l,r]\\gets\\left[Z-\\left(Q^{\\mathrm{avg}}(\\beta)+\\frac{\\Delta}{\\epsilon^{\\prime}}\\log\\frac{1}{\\beta}\\right),Z+\\left(Q^{\\mathrm{avg}}(\\beta)+\\frac{\\Delta}{\\epsilon^{\\prime}}\\log\\frac{1}{\\beta}\\right)\\right]}\\end{array}$   \n5: return $\\{\\bar{Y}_{j}\\}_{j\\in[m]},[l,r]$ ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Setting 1. Let n and $k$ be positive integers with $k\\,\\leq\\,n/2$ , and let $h:\\mathcal{X}^{k}\\to\\mathbb{R}$ be a symmetric function and let $\\mathcal{D}$ be an unknown distribution over $\\mathcal{X}$ with $\\mathsf{E}\\left[h(\\mathscr{D}^{k})\\right]=\\theta$ such that $|\\theta|<R$ for some known parameter $R$ . ", "page_idx": 17}, {"type": "text", "text": "Let $m$ be an integer and $\\mathcal{F}=\\{S_{1},S_{2},...,S_{m}\\}$ be a family of not necessarily distinct elements of $\\mathcal{T}_{n,k}$ . Define ", "page_idx": 17}, {"type": "equation", "text": "$$\nf_{i}:=\\frac{|\\{j\\in[m]:i\\in S_{j}\\}|}{m},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "the fraction of indices $j$ such that $S_{j}$ contains $i$ , and define the maximal dependence fraction ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\exp_{n,k}\\left(S\\right):=\\displaystyle\\operatorname*{max}_{i\\in\\left[n\\right]}f_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For each $j\\in[m]$ , let $Y_{j}$ denote $h(X_{S_{j}})$ . Clearly, $\\mathbb{E}\\left[Y_{j}\\right]=\\theta$ . To allow for small noise addition while ensuring privacy, it is desirable to choose $\\boldsymbol{S}$ with small $\\mathrm{dep}_{n,k}\\left(S\\right)$ . ", "page_idx": 17}, {"type": "text", "text": "Define functions $Q(\\beta)=Q_{n,k,h,\\mathcal{D},S}(\\beta)$ and $Q^{\\mathrm{avg}}(\\beta)=Q_{n,k,h,\\mathscr{D},S}^{\\mathrm{avg}}(\\beta)$ on $\\beta\\in(0,1]$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{sup}_{j\\in[m]}|Y_{j}-\\theta|>Q(\\beta)\\right)<\\beta,\\;\\;\\mathbb{P}\\left(\\left|\\frac{1}{m}\\sum_{j=1}^{m}Y_{j}-\\theta\\right|>Q^{\\mathrm{avg}}(\\beta)\\right)<\\beta.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We will refer to $Q(\\beta)$ and $Q^{\\mathrm{avg}}(\\beta)$ as $\\beta$ -confidence bounds for $\\operatorname*{sup}_{j\\in[m]}|Y_{j}-\\theta|$ and $\\begin{array}{r}{\\left|\\frac{1}{m}\\sum_{j\\in[m]}Y_{j}-\\theta\\right|.}\\end{array}$ , respectively. We apply Theorem A.1 (specifically, the form obtained in Lemma A.6) to different $\\mathcal{F}$ to obtain private estimates of $\\theta$ , with statistical and computational tradeoffs depending on the family $\\mathcal{F}$ . As Remark A.7 suggests, we will also need to privately estimate concentration bounds on the $Y_{j}$ \u2019s and their average. Naturally, this requires a private estimate of the variance $\\zeta_{k}$ . We provide guarantees from Biswas et al. [10] for private variance estimation and mean estimation here, where we have translated the mean estimation guarantee to fit our setting. ", "page_idx": 18}, {"type": "text", "text": "Before we do that, a natural approach to this problem is to view it as a standard private mean estimation task: split the data into $n/k$ equally-sized chunks, apply the function $h$ to each chunk, and run any existing private mean estimation algorithm to these $n/k$ values. We show that the error guarantee of such an algorithm is suboptimal compared to the error guarantee using the all-tuples estimator 1 or even the subsampling estimator 2 if sufficiently many samples are used. Before stating the propositions associated with these families, we state and prove the mother theorem. ", "page_idx": 18}, {"type": "text", "text": "Theorem A.1. For $\\epsilon>0$ , Algorithm A.2 with input $\\left(n,k,h,\\{X_{i}\\}_{i\\in[n]},\\mathcal{F},R,\\epsilon,\\gamma,Q(\\cdot),Q^{a\\nu g}(\\cdot)\\right)$ returns $\\widetilde{\\theta}_{n}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\tilde{\\theta}_{n}-\\theta|\\leq O\\left(\\underbrace{\\frac{\\sqrt{\\mathrm{Var}(\\sum_{j\\in[m]}Y_{j})}}{m\\sqrt{\\gamma}}}_{n o n\\cdot p r i v a t e\\;e r r o r}+\\underbrace{\\frac{d e p_{n,k}\\left(\\mathcal{F}\\right)Q(\\gamma)}{\\epsilon}\\cdot\\log\\left(\\frac{1}{\\gamma}\\right)}_{p r i v a t e\\;e r r o r}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with probability at least $1-6\\gamma,$ ,5 as long as ", "page_idx": 18}, {"type": "equation", "text": "$$\nd e p_{n,k}\\left(\\mathcal{F}\\right)\\leq\\frac{Q(\\gamma)\\epsilon}{10t Q(\\gamma/t)\\log t/\\gamma}\\qquad a n d\\qquad Q^{a v g}(\\gamma/t)<Q(\\gamma),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $t=\\lceil C\\log\\left(R/Q(\\gamma)\\right)\\rceil$ . Moreover, Algorithm A.2 is $\\epsilon$ -differentially private and runs in time $O(n+m\\log{(R/\\dot{Q}(\\gamma))}+k|\\mathcal{F}|)$ . ", "page_idx": 18}, {"type": "text", "text": "Remark A.7. Theorem A.1 assumes that $Q(\\cdot)$ and $Q^{a\\nu g}(\\cdot)$ are known, despite the mean $\\theta$ being unknown. Note that we only need to know the value of these functions at $\\gamma$ and $\\gamma/t,$ , for $a$ given $\\gamma$ . If these bounds are not known, we may first need to (privately) compute $Q(x)$ and $Q^{a\\nu g}(x)$ and then use those privately computed bounds in the algorithm. For example, if the $Y_{i}^{\\prime}s$ are sub-Gaussian with variance proxy $^{\\,I}$ , then $Q(x)={\\sqrt{\\log(m/x)}}$ . We will see how to estimate these parameters for various families $\\mathcal{F}$ of indices used in Algorithm A.2. ", "page_idx": 18}, {"type": "text", "text": "Proof of Theorem A.1. We will prove privacy and accuracy guarantees separately. ", "page_idx": 18}, {"type": "text", "text": "Privacy. Algorithm A.2 makes $t+1$ calls to Algorithm A.3; let $\\Delta_{i},W_{i}$ , and $Z_{i}$ be the values taken by $\\Delta,W,$ and $Z$ in the $i^{\\mathrm{th}}$ call to Algorithm A.3, for $1\\leq i\\leq t+1$ . Let $\\begin{array}{r}{\\beta:=\\frac{\\gamma}{t}}\\end{array}$ . It can be shown inductively that the interval lengths $r_{i}-l_{i}$ and the values $\\Delta_{i}$ do not depend on the dataset. For any $1\\leq i\\leq t$ , note that $Y_{i,j}=\\mathrm{proj}_{l_{i-1}-Q(\\beta),r_{i-1},Q(\\beta)}\\left(Y_{i-1,j}\\right)$ for all $1\\leq j\\leq m$ . Suppose we change $X_{w}$ to $X_{w}^{\\prime}$ for some index $w$ . For any $1\\leq i\\leq t+1$ , conditioned on the values of $Z_{i^{\\prime}}$ for $1\\leq i^{\\prime}<i$ , at most a $\\exp_{n,k}\\left(\\mathcal{F}\\right)$ fraction of $\\{Y_{i,j}\\}_{j\\in[m]}$ depend on $w$ (this is true by the definition of $\\mathrm{dep}_{n,k}\\left(\\mathcal{F}\\right))$ . Since $Y_{i,j}=\\mathrm{proj}_{l_{i-1}-Q(\\beta),r_{i-1}+Q(\\beta/m)}\\left(Y_{i-1,j}\\right)$ has range $r_{i-1}-l_{i-1}+2Q(\\beta)$ , the sensitivity of $\\textstyle{\\frac{1}{m}}\\sum_{j=1}^{m}Y_{i,j}$ is at most $\\begin{array}{r}{\\mathrm{dep}_{n,k}\\left(\\mathcal{F}\\right)\\left(r_{i-1}-l_{i-1}+2Q(\\beta)\\right)=\\Delta_{i}}\\end{array}$ . Therefore, by standard results (cf. Lemma 1), for all $1\\leq i\\leq t$ , the output $Z_{i}$ (and therefore the interval , conditioned on $Z_{i^{\\prime}}$ for $1\\leq i^{\\prime}<i$ , is $\\frac{\\epsilon}{2t}$ -differentially private. Similarly, the output $(l_{t+1}+r_{t+1})/2=Z_{t+1}$ , conditioned on {Zi}i\u2208[t], is \u03f52 -differentially private. By Basic Composition (see Lemma A.4), Algorithm A.2 is $\\epsilon$ -differentially private. ", "page_idx": 18}, {"type": "text", "text": "Utility. First, we show that if Algorithm A.3 is invoked with $\\theta\\in[l,r]$ , it returns an interval $[l^{\\prime},r^{\\prime}]$ such that $\\theta\\in[l^{\\prime},r^{\\prime}]$ with probability at least $1-3\\beta$ . Consider running a variant of Algorithm A.2 with the projection step omitted in every call of Algorithm A.3. With probability at least $1-\\beta$ , we have $\\begin{array}{r}{\\left|\\frac{1}{m}\\sum_{i=1}^{m}Y_{i}-\\theta\\right|\\leq Q^{\\mathrm{avg}}(\\beta)}\\end{array}$ , and with probability at least $1-\\beta$ , we have $\\begin{array}{r}{|W|\\leq\\frac{\\Delta}{\\epsilon^{\\prime}}\\log\\frac{1}{\\beta}}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "Therefore, with probability at least $1-2\\beta$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n|Z-\\theta|\\leq Q^{\\mathrm{avg}}(\\beta)+\\frac{\\Delta}{\\epsilon^{\\prime}}\\log\\frac{1}{\\beta},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "in which case $\\theta\\in[l^{\\prime},r^{\\prime}]$ . ", "page_idx": 19}, {"type": "text", "text": "Finally, reintroducing the projection step only increases the error probability by at most $\\beta$ . Taking a union bound over $t$ steps, we see that $\\bar{\\theta^{\\dag}}\\bar{[l^{\\prime},r^{\\prime}]}$ , with probability at least $1-3\\gamma$ . ", "page_idx": 19}, {"type": "text", "text": "Next, we claim that if $r-l>28Q(\\gamma)$ , then $r^{\\prime}-l^{\\prime}\\leq(r-l)/2$ . Using the assumption, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\deg_{n,k}{(\\mathcal{F})}\\leq\\frac{Q(\\gamma)\\epsilon}{10t Q(\\gamma/t)\\log t/\\gamma}\\leq\\operatorname*{min}\\left(\\frac{\\epsilon^{\\prime}}{5\\log{1/\\beta}},\\frac{Q(\\gamma)\\epsilon^{\\prime}}{5Q(\\beta)\\log{1/\\beta}}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second inequality follows from taking $\\epsilon^{\\prime}=\\frac{\\epsilon}{2t}$ and using the fact that $\\begin{array}{r}{Q(\\gamma)\\leq Q\\left(\\frac{\\gamma}{t}\\right)}\\end{array}$ , since the quantile function is nonincreasing. Furthermore, by the assumption $Q^{\\mathrm{avg}}(\\beta)<Q(\\gamma)$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r^{\\prime}-l^{\\prime}=\\frac{2\\ensuremath{\\mathrm{dep}}_{n,k}\\left(\\mathcal{F}\\right)\\log1/\\beta}{\\epsilon^{\\prime}}\\left(r-l\\right)+\\left(2Q^{\\mathrm{avg}}(\\beta)+\\frac{4\\ensuremath{\\mathrm{dep}}_{n,k}\\left(\\mathcal{F}\\right)Q\\left(\\beta\\right)\\log1/\\beta}{\\epsilon^{\\prime}}\\right)}\\\\ &{\\qquad\\le\\frac{2\\left(r-l\\right)}{5}+\\left(2Q(\\gamma)+\\frac{4}{5}Q(\\gamma)\\right)\\le\\frac{r-l}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, after t = \u2126 log Q(R\u03b3) iterations, we are guaranteed that the length of the final interval $[l_{t},r_{t}]$ is at most $28Q(\\gamma)$ . ", "page_idx": 19}, {"type": "text", "text": "Finally, consider lines 8 and 9 of Algorithm A.2. The algorithm returns the midpoint of the interval $[l_{t+1},r_{t+1}]$ , which is $Z_{t+1}$ in the final call of Algorithm A.3. By Chebyshev\u2019s inequality, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{m}\\sum_{j=1}^{m}Y_{0,j}-\\theta\\right|\\leq\\sqrt{\\frac{1}{\\gamma}\\cdot\\mathrm{Var}\\left(\\frac{1}{m}\\sum_{i=1}^{m}Y_{0,j}\\right)},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with probability at least $1-\\gamma$ , and with probability at least $1-\\gamma$ , none of the $Y_{i}$ \u2019s are truncated in the projection step in the final call of Algorithm A.3. Finally, with probability at least $1-\\gamma$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nW_{t+1}=O\\left(\\frac{\\Delta_{t+1}}{\\epsilon}\\right)=O\\left(\\frac{\\mathrm{dep}_{n,k}\\left(\\mathcal{F}\\right)Q(\\gamma)}{\\epsilon}\\log\\frac{1}{\\gamma}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The conclusion follows from a union bound over all events. ", "page_idx": 19}, {"type": "text", "text": "A.3.2 Boosting the error probability via median-of-means ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Algorithm A.2 incurs a $1/\\sqrt{\\gamma}$ multiplicative factor in the non-private error, stemming from an application of Chebyshev\u2019s inequality to bound $\\begin{array}{r}{|\\sum_{j=1}^{m}h(X_{S_{j}})/m-\\theta|}\\end{array}$ . For specific families $\\mathcal{F}$ , we may be able to provide stronger concentration bounds for $\\textstyle\\sum_{j=1}^{m}h(X_{S_{j}})/m$ in inequality (A.35). Instead, we complement the result of Theorem A.1 by applying the following median-of-means procedure that allows for an improved dependence on the failure probability $\\alpha$ with only a $\\log\\left(1/\\alpha\\right)$ multiplicative blowup in the sample complexity: ", "page_idx": 19}, {"type": "text", "text": "Lemma A.6. Let $\\alpha\\in(0,1)$ and $\\epsilon\\geq0$ . Let $\\boldsymbol{\\mathcal{A}}$ be an $\\epsilon$ -differentially private algorithm. Consider a size n dataset $D_{n}\\stackrel{i.i.d}{\\sim}\\mathcal{D}$ , for a distribution $\\mathcal{D}$ with some unknown parameter $\\theta$ such that with probability at least 0.75, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n|A(D_{n})-\\theta|\\leq r_{n}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Split $D_{n}$ into $q\\,:=\\,8\\log(1/\\alpha)$ equal independent chunks,6and run $\\boldsymbol{\\mathcal{A}}$ on each chunk to obtain $\\epsilon$ -differentially private estimates $\\{\\widetilde{\\theta}_{n,i}\\}_{i\\in[d]}$ of $\\theta$ . Let $\\widetilde{\\theta}_{n}^{m e d}$ be the median of these q estimates. Then $\\widetilde{\\theta}_{n}^{m e d}$ is $\\epsilon$ -differentially private, and with probability at least $1-\\alpha$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|\\tilde{\\theta}_{n}^{m e d}-\\theta\\right|\\leq r_{n/q}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. The privacy of $\\widetilde{\\theta}_{n}^{\\mathrm{med}}$ follows from parallel composition (Lemma A.5). For utility, we know from the hypothesis that for each $i\\in[q]$ , with probability at least $3/4$ , the estimate $\\tilde{\\theta}_{n,i}$ satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\tilde{\\theta}_{n,i}-\\theta|\\leq r_{n/q}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If more than half the estimates $\\tilde{\\theta}_{n,i}$ satisfy the above equation, then so does the median. Let $T_{i}$ be the random variable that assumes the value 0 if $\\tilde{\\theta}_{n,i}$ satisfies the above equation and assumes the value 1 otherwise. Then, $\\mathbb{E}[T_{i}]\\le1/4$ , and it suffices to show that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(T_{1}+T_{2}+\\cdot\\cdot\\cdot+T_{q}\\leq q/2\\right)\\geq1-\\alpha.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This follows from a standard Hoeffding inequality; as long as $q\\geq8\\log(1/\\alpha)$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(T_{1}+T_{2}+\\cdots+T_{q}>q/2\\right)\\le\\operatorname*{Pr}\\left(\\sum_{i}(T_{i}-E[T_{i}])>q/4\\right)\\le e^{-2(1/4)^{2}q}\\le\\alpha.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "A.3.3 Application to the all-tuples family: proof of Proposition 1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For any $i\\in[n]$ , there are exactly $\\binom{n-1}{k-1}$ sets $S\\in{\\mathcal{Z}}_{n,k}$ such that $i\\in S$ . Following the notation from the setting of Theorem A.1, we have $\\begin{array}{r}{f_{i}\\,=\\,{\\binom{n-1}{k-1}}/{\\binom{n}{k}}\\,=\\,\\frac{k}{n}}\\end{array}$ for all $i\\,\\in\\,[n]$ , so $\\begin{array}{r}{\\mathrm{dep}_{n,k}\\left(\\mathcal{F}_{\\mathrm{all}}\\right)\\,=\\,\\frac{k}{n}}\\end{array}$ . Moreover, for each $S\\in{\\mathcal{Z}}_{n,k}$ , we have $\\begin{array}{r}{\\mathbb{P}\\left(\\left|h(X_{S})-\\theta\\right|\\geq y\\right)\\leq2\\exp\\left(\\frac{-y^{2}}{2\\tau}\\right)}\\end{array}$ . Letting ", "page_idx": 20}, {"type": "equation", "text": "$$\nQ(\\gamma):=\\sqrt{2\\tau k\\log\\left(\\frac{2n}{\\gamma}\\right)}>\\sqrt{2\\tau\\log\\left(\\frac{2}{\\gamma}\\binom{n}{k}\\right)},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we see that each $Y_{i}$ is within $Q(\\gamma)$ of $\\theta$ with probability at least $\\frac{\\gamma}{\\binom{n}{k}}$ . A union bound implies that this choice of $Q(\\gamma)$ is valid. For the concentration of the average $\\textstyle{\\frac{1}{m}}\\sum_{j\\in[m]}Y_{j}$ , which is simply $U_{n}$ , we can use Lemma A.3) to see that ", "page_idx": 20}, {"type": "equation", "text": "$$\nQ^{\\mathrm{avg}}(\\gamma):={\\sqrt{\\frac{2\\tau k\\log{\\frac{2}{\\gamma}}}{n}}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "is a valid choice. We now verify the conditions in Theorem A.1: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\deg_{n,k}{(S)}=\\frac{k}{n}\\leq\\frac{Q(\\gamma)\\epsilon}{10t Q(\\gamma/t)\\log(t/\\gamma)}=\\frac{\\epsilon}{10t\\log(t/\\gamma)}\\sqrt{\\frac{\\log2n/\\gamma}{\\log2n t/\\gamma}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "if and only if ", "page_idx": 20}, {"type": "equation", "text": "$$\nn\\geq\\frac{10k t\\log(t/\\gamma)}{\\epsilon}\\sqrt{\\frac{\\log2n t/\\gamma}{\\log2n/\\gamma}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Recalling that $t=\\lceil C\\log\\left(R/Q(\\gamma)\\right)\\rceil$ , we see that this holds under the sample complexity assumption on $n$ . Furthermore, we have $Q^{\\mathrm{avg}}(\\gamma/t)\\leq Q(\\gamma)$ if and only if ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{2\\tau k\\log\\frac{2t}{\\gamma}}{n}}\\leq\\sqrt{2\\tau k\\log\\left(\\frac{n}{\\gamma}\\right)}\\iff n\\geq\\frac{\\log2t/\\gamma}{\\log n/\\gamma},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is also true by assumption. Therefore, with probability at least $1-6\\gamma$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\tilde{\\theta}_{\\mathrm{all}}-\\theta|\\leq O\\left(\\frac{1}{\\sqrt{\\gamma}}\\sqrt{\\mathrm{Var}(U_{n})}+\\frac{k}{n\\epsilon}\\sqrt{2\\tau k\\log\\left(\\frac{2n}{\\gamma}\\right)}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Algorithm A.3 uses a constant failure probability of $\\gamma=0.01$ , which assures a success probability of at least 0.75. This is further boosted by Wrapper 1. Now, an application of Lemma A.6 gives the stated result. ", "page_idx": 20}, {"type": "text", "text": "A.3.4 Application to the naive family ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Definition A.3. Consider the following estimator: divide the n data points into $n/k$ disjoint chunks, compute $h(X_{S})$ on each of these chunks, and apply the CoinPress algorithm [10] to obtain a private estimate of the mean $\\theta$ . We will call this naive estimator $\\hat{\\theta}_{n a i\\nu e}$ . ", "page_idx": 21}, {"type": "text", "text": "The following proposition records the guarantee of the naive estimator $\\hat{\\theta}_{\\mathrm{naive}}$ : ", "page_idx": 21}, {"type": "text", "text": "Proposition A.1. The naive estimator $\\hat{\\theta}_{n a i\\nu e}$ satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\hat{\\theta}_{n a i\\nu e}-\\theta|\\leq O\\left(\\sqrt{\\frac{k\\zeta_{k}}{n}}\\right)+\\tilde{O}\\left(\\frac{k\\sqrt{\\tau}}{n\\epsilon}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with probability at least 0.9, as long as $\\begin{array}{r}{n=\\tilde{\\Omega}\\left(\\frac{k}{\\epsilon}\\log\\frac{R}{\\sqrt{\\tau}}\\right)}\\end{array}$ . The estimate $\\hat{\\theta}_{n a i\\nu e}$ is $\\epsilon$ -differentially private and the algorithm runs in time $\\begin{array}{r}{\\tilde{O}\\left(n+\\frac{n}{k}\\log\\frac{R}{\\sqrt{\\tau}}\\right)}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "First, suppose the variance $\\zeta_{k}$ is known. It is easy to see that $\\begin{array}{r}{\\mathrm{dep}_{n,k}\\left(\\mathcal{F}_{\\mathrm{naive}}\\right)=\\frac{k}{n}}\\end{array}$ . By the assumption that $h(X_{S})$ is $\\tau$ -sub-Gaussian, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nP(|h(X_{S})-\\theta|\\geq y)\\leq2\\exp\\left(-\\frac{y^{2}}{2\\tau}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence, with probability at least $1-\\gamma/m$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n|Y_{j}-\\theta|\\leq\\sqrt{2\\tau\\log(2m/\\gamma)},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for each $1\\leq j\\leq m$ , where we use the notation as in the setting of Theorem A.1. By a union bound, we can take the quantile function $\\begin{array}{r}{Q(\\gamma)=\\sqrt{2\\tau\\log\\left(\\frac{2n}{k\\gamma}\\right)}}\\end{array}$ . Moreover, since the $Y_{j}$ \u2019s are independent, the average $\\textstyle{\\frac{1}{m}}\\sum_{j\\in[m]}Y_{j}$ is $\\frac{\\tau}{m}$ -sub-Gaussian with variance $\\frac{\\zeta_{k}}{m}$ . Therefore, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nP\\left(\\left|\\frac{1}{m}\\sum_{j=1}^{m}Y_{j}-\\theta\\right|\\geq y\\right)\\leq2\\exp\\left(-\\frac{m y^{2}}{2\\tau}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This yields a bound of $\\begin{array}{r}{Q^{\\mathrm{avg}}(\\gamma)=\\sqrt{\\frac{2k\\tau\\log(2/\\gamma)}{n}}}\\end{array}$ 2k\u03c4 long(2/\u03b3). It remains to verify the conditions of Theorem A.1. We have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{k}{n}\\leq\\frac{Q(\\gamma)\\epsilon}{10t Q(\\gamma/t)\\log(t/\\gamma)}\\iff\\frac{k}{n}\\leq\\frac{\\epsilon}{10t\\log(t/\\gamma)}\\sqrt{\\frac{\\log(2n/k\\gamma)}{\\log(2n t/k\\gamma)}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{Q^{\\mathrm{avg}}(\\gamma/t)<Q(\\gamma)\\iff\\sqrt{\\frac{2k\\tau\\log(2t/\\gamma)}{n}}\\leq\\sqrt{2\\tau\\log(2n/k\\gamma)}}&{}\\\\ {\\iff n\\geq k\\frac{\\log\\left(2t/\\gamma\\right)}{\\log\\left(2n/k\\gamma\\right)},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which are both true by the sample size assumption, noting that $t=\\lceil C\\log(R/Q(\\gamma))\\rceil$ . Therefore, with probability at least $1-6\\gamma$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|\\hat{\\theta}_{\\mathrm{naive}}-\\theta\\right|\\le O\\left(\\frac{1}{\\sqrt{\\gamma}}\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{naive}})+\\frac{k}{n\\epsilon}\\sqrt{2\\tau\\log(2n/k\\gamma)}\\log\\frac{1}{\\gamma}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Choosing $\\gamma$ to be an appropriate constant, we arrive at the deisred result. ", "page_idx": 21}, {"type": "text", "text": "A.3.5 Application to the subsampled family ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Unlike the all-pairs family $\\ensuremath{S_{\\mathrm{all}}}$ , the subsampled $\\ensuremath{S_{\\mathrm{ss}}}$ in Definition 2 is randomized. Define $\\hat{\\theta}_{\\mathrm{ss}}\\,=$ $\\textstyle\\sum_{j=1}^{M}h(X_{S_{j}})/M$ . Recall from our discussion before (cf. Theorem A.1) that we want each of the $h(X_{S_{j}})$ \u2019s, as well as $\\hat{\\theta}_{\\mathrm{ss}}$ , to concentrate around $\\theta$ , and we also want $\\mathrm{dep}_{n,k}\\left(S_{\\mathrm{ss}}\\right)$ to be small. As we show, the former concentration holds as in the all-tuples case, and the latter holds with high probability. ", "page_idx": 22}, {"type": "text", "text": "Proposition A.2. Let $M_{n}=\\Omega((n/k)\\log n)$ . Then Wrapper $^{\\,l}$ , with $f=s s_{;}$ , failure probability $\\alpha$ , algorithm $A=U$ -StatMean (Algorithm A.2), $S(I_{i})$ a set of $M_{n_{\\alpha}}$ i.i.d. subsets of size $k$ picked with replacement from $I_{i}$ , returns an estimate $\\tilde{\\theta}_{s s}$ such that, with probability at least $1-\\alpha$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|{\\tilde{\\theta}}_{s s}-\\theta\\right|\\leq O\\left(\\sqrt{\\mathrm{Var}(U_{n_{\\alpha}})}\\right)+{\\tilde{O}}\\left(\\sqrt{\\frac{\\zeta_{k}}{M_{n_{\\alpha}}}}+\\frac{k^{3/2}\\sqrt{\\tau}}{n_{\\alpha}\\epsilon}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "as long as $\\begin{array}{r}{n_{\\alpha}=\\tilde{\\Omega}\\left(\\frac{k}{\\epsilon}\\left(\\log\\frac{R}{\\sqrt{k\\tau}}\\right)\\right)}\\end{array}$ . Moreover, the estimator $\\tilde{\\theta}_{s s}$ is $\\epsilon$ -differentially private and runs in time $\\begin{array}{r}{\\tilde{O}\\left(\\log(1/\\alpha)\\left(k+\\log\\frac{R}{\\sqrt{k\\tau}}\\right)M_{n_{\\alpha}}\\right)}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "First, we need the following helper lemmas: ", "page_idx": 22}, {"type": "text", "text": "Lemma A.7. Defin $\\begin{array}{r}{\\circ\\hat{\\theta}_{s s}=\\frac{1}{M}\\sum_{S\\in\\mathcal{F}_{s s}}h(X_{S}).\\ W e\\,h a\\nu e\\,\\operatorname{Var}\\left[\\hat{\\theta}_{s s}\\right]=\\left(1-\\frac{1}{M}\\right)\\operatorname{Var}(U_{n})+\\frac{1}{M}\\zeta_{k}.}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Proof. Clearly, $\\mathbb{E}[\\hat{\\theta}_{\\mathrm{ss}}]=\\theta$ . We compute both terms of the following decomposition of the variance of $\\hat{\\theta}_{\\mathrm{ss}}$ separately; recall that ${\\bf X}=\\{X_{i}\\}_{i\\in[n]}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname{Var}\\left(\\hat{\\theta}_{\\mathrm{ss}}\\right)=\\operatorname{Var}\\left(\\mathbb{E}\\left[\\hat{\\theta}_{\\mathrm{ss}}|\\mathbf{X}\\right]\\right)+\\mathbb{E}\\left[\\operatorname{Var}\\left(\\hat{\\theta}_{\\mathrm{ss}}|\\mathbf{X}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname{Var}\\left(\\mathbb{E}\\left[\\hat{\\theta}_{\\mathrm{ss}}|\\mathbf{X}\\right]\\right)=\\operatorname{Var}\\left(\\mathbb{E}\\left[\\left.\\left[\\frac{1}{M}\\sum_{j\\in[M]}h\\left(X_{S_{i}}\\right)\\right|\\mathbf{X}\\right]\\right)=\\operatorname{Var}(U_{n}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathrm{Var}\\left(\\left.\\hat{\\theta}_{\\mathrm{ss}}\\right|\\mathbf{X}\\right)\\right]=\\mathbb{E}\\left[\\mathrm{Var}\\left(\\frac{1}{M}\\sum_{j=1}^{M}h(X_{S_{j}})\\Bigg|\\mathbf{X}\\right)\\right]=\\frac{1}{M}\\mathbb{E}\\left[\\mathrm{Var}\\left(\\left.h(X_{S})\\right|\\mathbf{X}\\right)\\right]}\\\\ &{\\phantom{=}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{1}{M}\\mathbb{E}\\left[\\frac{1}{\\binom{n}{k}}\\sum_{\\kappa\\in\\mathcal{T}_{n,k}}h(X_{S})^{2}-\\left(\\frac{1}{\\binom{n}{k}}\\sum_{\\kappa\\in\\mathcal{T}_{n,k}}h(X_{S})\\right)^{2}\\right]}\\\\ &{\\phantom{=}=\\frac{1}{M}\\left(\\left(\\zeta_{k}+\\theta^{2}\\right)-\\left(\\mathrm{Var}(U_{n})+\\theta^{2}\\right)\\right)=\\frac{\\zeta_{k}-\\mathrm{Var}(U_{n})}{M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Adding the two equalities yields the result. ", "page_idx": 22}, {"type": "text", "text": "Lemma A.8. Let $\\gamma>0$ , and let $\\begin{array}{r}{M=\\Omega(\\frac{n}{k}\\log\\frac{n}{\\gamma})}\\end{array}$ . Then $\\begin{array}{r}{d e p_{n,k}\\left(\\mathcal{F}_{s s}\\right)\\leq\\frac{4k}{n}}\\end{array}$ with probability at least $1-\\gamma$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. Let $Z_{i}$ be the number of sampled subsets of which $i$ is an element. Observe that $Z_{i}\\,\\sim$ $\\mathrm{Binom}(M,k/n)$ , with mean $\\mu:=M k/n$ . By a Chernoff bound, for any $\\delta>0$ and any $i\\in[n]$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(Z_{i}\\geq(1+\\delta)\\mu\\right)\\leq\\left(\\frac{e^{\\delta}}{(1+\\delta)^{1+\\delta}}\\right)^{\\mu}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By a union bound, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathbf{dep}_{n,k}\\left(\\mathcal{F}_{\\mathrm{ss}}\\right)>\\frac{4k}{n}\\right)=\\mathbb{P}\\left(\\operatorname*{max}_{i}Z_{i}>4\\mu\\right)\\leq n\\left(\\frac{e^{3}}{(1+3)^{1+3}}\\right)^{\\mu}\\leq n\\exp\\left(-\\frac{M k}{n}\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which is at most $\\gamma$ by our choice of $M$ . ", "page_idx": 23}, {"type": "text", "text": "Let $\\mathcal{G}_{\\gamma}$ denote the event that $\\begin{array}{r}{\\mathrm{dep}_{n,k}\\left(\\mathcal{F}_{\\mathrm{ss}}\\right)\\leq\\frac{4k}{n}}\\end{array}$ , which occurs with high probability by Lemma A.8. Note also that conditioned on any family ${\\mathcal{F}}_{\\mathrm{ss}}$ of subsets of $\\mathcal{T}_{n,k}$ , the run of Algorithm A.2 is $\\epsilon\\cdot$ - differentially private. Since the randomness of $\\mathcal{F}_{\\mathrm{ss}}$ is independent of the data, the algorithm (along with the private variance estimation) is still $2\\epsilon$ -differentially private. ", "page_idx": 23}, {"type": "text", "text": "Let ", "page_idx": 23}, {"type": "equation", "text": "$$\nQ(\\gamma)=\\sqrt{2\\tau k\\log\\left(\\frac{4n}{\\gamma}\\right)}\\ \\ \\mathrm{and}\\ \\ Q^{\\mathrm{avg}}(\\gamma)=4\\sqrt{\\frac{\\tau k}{\\operatorname*{min}\\left(M,n\\right)}}\\log\\frac{4n}{\\gamma}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We show that these are indeed the corresponding confidence bounds for $Y_{S},S\\in\\mathcal{F}_{s s}$ and $\\hat{\\theta}_{s s}$ . ", "page_idx": 23}, {"type": "text", "text": "By a sub-Gaussian tail bound, for any $S\\in{\\mathcal{Z}}_{n,k}$ , the probability that $|h(X_{S})-\\theta|>Q(\\gamma)$ is at most $\\begin{array}{r}{2\\left(\\frac{\\gamma}{4n}\\right)^{k}\\leq\\frac{\\gamma}{2n^{k}}}\\end{array}$ . By a union bound over all $\\binom{n}{k}$ sets $S$ , we then have $|h(X_{S})-\\theta|\\leq Q(\\gamma)$ for all $S\\in{\\mathcal{T}}_{n,k}$ , with probability at least $\\textstyle1-{\\frac{\\gamma}{2}}$ . Call this event $\\mathcal{E}_{\\gamma}$ . ", "page_idx": 23}, {"type": "text", "text": "Next, $\\mathbb{E}[\\widehat{\\theta}_{\\mathrm{ss}}|X_{1},\\ldots X_{n}]=U_{n}$ . Moreover, for any $c>0$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{P}\\left(|\\hat{\\theta}_{\\mathrm{ss}}-\\theta|\\ge c\\right)\\le\\mathbb{P}\\left(|\\hat{\\theta}_{\\mathrm{ss}}-U_{n}|\\ge c/2\\right)+P\\left(|U_{n}-\\theta|\\ge c/2\\right)}\\\\ {\\le\\mathbb{E}_{X_{1},\\hdots,X_{n}}\\mathbb{P}\\left(|\\hat{\\theta}_{\\mathrm{ss}}-U_{n}|\\ge c/2|X_{1},\\hdots,X_{n}\\right)+2\\exp\\left(-\\frac{n c^{2}}{8k\\tau}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we used Lemma A.3 to bound the second term. For the first term in inequality (A.40), note that conditioned on the data $X_{1},\\ldots,X_{n}$ , the $h(X_{S_{j}})$ \u2019s are independent draws from a uniform distribution over the $\\binom{n}{k}$ values $\\{h(X_{S})\\}_{S\\in{\\mathbb{Z}_{n,k}}}$ , with mean $U_{n}$ , and the $\\left|h(X_{S_{j}})-\\theta\\right|$ \u2019s are bounded by $\\mathrm{max}_{S\\in\\mathbb{Z}_{n,k}}\\,|h(X_{S})\\!-\\!\\theta|\\leq Q(\\gamma)$ . Therefore, each $h(X_{S_{j}}){-}U_{n}$ is sub-Gaussian $(Q(\\gamma)^{2})$ , implying that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathbb{P}\\left(\\left|\\hat{\\theta}_{\\mathrm{ss}}-U_{n}\\right|\\geq c/2|X_{1},\\ldots,X_{n}\\right)\\right]\\leq2\\mathbb{E}\\left[\\exp\\left(-\\frac{M c^{2}}{8Q(\\gamma)^{2}}\\right)|X_{1},\\ldots,X_{n},\\mathcal{E}_{\\gamma}\\right]+P(\\mathcal{E}_{\\gamma}^{c})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2\\exp\\left(-\\frac{M c^{2}}{16\\tau k\\log(4n/\\gamma)}\\right)+P(\\mathcal{E}_{\\gamma}^{c})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\exp\\left(-\\frac{M c^{2}}{16\\tau k\\log(4n/\\gamma)}\\right)+\\frac{\\gamma}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining inequalities (A.40) and (A.39), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|\\hat{\\theta}_{\\mathrm{ss}}-\\theta|\\ge c\\right)\\le2\\exp\\left(-\\frac{M c^{2}}{16\\tau k\\log(4n/\\gamma)}\\right)+\\frac{\\gamma}{2}+2\\exp\\left(-\\frac{n c^{2}}{8k\\tau}\\right)\\le\\gamma,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "as long as ", "page_idx": 23}, {"type": "equation", "text": "$$\nc\\geq4{\\sqrt{\\frac{\\tau k}{\\operatorname*{min}\\left(M,n\\right)}\\log\\left(\\frac{2n}{\\gamma}\\right)\\log\\left(\\frac{8}{\\gamma}\\right)}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This justifies the choice of $Q^{\\mathrm{avg}}(\\gamma)$ . We now verify the conditions in Theorem A.1. Conditioned on $\\mathcal{G}_{\\gamma}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nd e p_{n,k}(S)=\\frac{4k}{n}\\leq\\frac{Q(\\gamma)\\epsilon}{10t Q(\\gamma/t)\\log(t/\\gamma)}=\\frac{\\epsilon}{10t\\log(t/\\gamma)}\\sqrt{\\frac{\\log4n/\\gamma}{\\log4n t/\\gamma}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "if and only if ", "page_idx": 24}, {"type": "equation", "text": "$$\nn\\geq\\frac{40k t\\log(t/\\gamma)}{\\epsilon}\\sqrt{\\frac{\\log4n t/\\gamma}{\\log4n/\\gamma}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Recalling that $t=\\lceil C\\log\\left(R/Q(\\gamma)\\right)\\rceil$ , we see that the above holds under the sample complexity assumption on $n$ . Furthermore, $Q^{\\mathrm{avg}}(\\gamma/t)\\leq Q(\\gamma)$ iff ", "page_idx": 24}, {"type": "equation", "text": "$$\n4\\sqrt{\\frac{\\tau k}{\\operatorname*{min}\\left(M,n\\right)}}\\log\\frac{4n t}{\\gamma}\\leq\\sqrt{2\\tau k\\log\\left(\\frac{4n}{\\gamma}\\right)}\\iff\\operatorname*{min}(M,n)\\geq\\frac{\\log(4n t/\\gamma)^{2}}{\\log4n/\\gamma}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $\\begin{array}{r}{M\\geq\\frac{n}{k}\\log\\left(\\frac{n}{\\gamma}\\right)}\\end{array}$ , the assumption on the sample complexity of $n$ implies the above result. Conditioned on $\\mathcal{E}_{\\gamma}$ , the projection steps are never invoked in Algorithm A.2 or A.3, so we have $\\tilde{\\theta}_{\\mathrm{ss}}=$ $\\widehat{\\theta}_{\\mathrm{ss}}+W_{t+1}$ , where $W_{t+1}$ is a Laplace random variable with parameter $\\frac{2\\Delta}{\\epsilon}$ , where $\\begin{array}{r}{\\Delta=\\frac{\\deg_{n,k}(\\mathcal{F})Q(\\gamma)}{\\epsilon}}\\end{array}$ (coming from the noise added to $\\textstyle{\\frac{1}{M}}\\sum_{i=1}^{M}Y_{i}$ in the $(t+1)^{t h}$ step of Algorithm A.2). Finally, using Lemma A.8, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\bigl|\\tilde{\\theta}_{\\mathrm{ss}}-\\hat{\\theta}_{\\mathrm{ss}}\\bigr|=|W_{t+1}|\\leq\\frac{2\\Delta}{\\epsilon}\\log\\frac{1}{\\gamma}=\\frac{2\\mathsf{d e p}_{n,k}\\left({\\mathcal{S}}\\right)Q(\\gamma)}{\\epsilon}\\log\\left(\\frac{1}{\\gamma}\\right)\\leq\\frac{8k}{n\\epsilon}\\log\\frac{1}{\\gamma}\\sqrt{2\\tau k\\log\\left(\\frac{4n}{\\gamma}\\right)}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "on the event $\\mathcal{G}_{\\gamma}$ . Combined with Lemmas A.6, A.7, inequality (A.40), and Theorem A.1, with probability at least $1-7\\gamma$ , we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\tilde{\\theta}_{\\mathrm{ss}}-\\theta|\\leq O\\left(\\frac{1}{\\sqrt{\\gamma}}\\sqrt{\\mathrm{Var}(U_{n})}+\\frac{1}{\\sqrt{\\gamma}}\\sqrt{\\frac{\\zeta_{k}}{M}}+\\frac{k}{n\\epsilon}\\log\\frac{1}{\\gamma}\\sqrt{\\tau k\\log\\left(\\frac{4n}{\\gamma}\\right)}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The success probability is $1-7\\gamma$ instead of $1-6\\gamma$ because we also require $\\begin{array}{r}{\\mathrm{dep}_{n,k}\\left(\\mathcal{F}_{\\mathrm{ss}}\\right)\\le\\frac{4k}{n}}\\end{array}$ , which holds with probability $1-\\gamma$ as in Lemma A.8. Algorithm A.3 uses a constant failure probability of $\\gamma=0.01$ , which assures a success probability of at least 0.75. This is further boosted by Wrapper 1. Now, an application of Lemma A.6 gives the stated result. ", "page_idx": 24}, {"type": "text", "text": "A.4 Proofs of Lower Bounds ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this appendix, we provide the proofs of our two lower bound results. ", "page_idx": 24}, {"type": "text", "text": "A.4.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma A.9 (Lemma 6.2 in [41]). Let $\\mathcal{P}=\\{P_{1},P_{2},\\dots\\}$ be a finite family of distributions over a domain $\\mathcal{X}$ such that for any $i\\neq j$ , the total variation distance between $P_{i}$ and $P_{j}$ is at most $\\alpha$ . Suppose there exists a positive integer n and an $\\epsilon$ -differentially private algorithm $B:\\mathcal{X}^{n}\\to[|\\mathcal{P}|]$ such that for every $P_{i}\\in\\mathcal{P}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{X_{1},\\ldots,X_{n}\\sim P_{i},{\\cal B}}\\left(\\beta(X_{1},\\ldots,X_{n})=i\\right)\\geq2/3.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, $\\begin{array}{r}{n=\\Omega\\left(\\frac{\\log{|\\mathcal{P}|}}{\\alpha\\epsilon}\\right)}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "Lemma A.10 (Proposition 4.1 in [4]). The Bernoulli distribution ${\\mathrm{Bern}}(p)$ is sub-Gaussian with optimal variance proxy $\\tau_{p}$ , where ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tau_{p}=\\tau_{1-p}=\\frac{\\frac{1}{2}-p}{\\log\\left(\\frac{1}{p}-1\\right)},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for p \u2208(0, 1) \\ {1/2}. In particular, if 0 < p < 1/10, then \u03c4p \u2264 2 lo1g21p . ", "page_idx": 24}, {"type": "text", "text": "Define $\\mathcal{D}_{0}=\\mathrm{Bern}(1)$ and $\\mathcal{D}_{1}=\\mathrm{Bern}(1-\\beta)$ , where $\\begin{array}{r}{\\beta=\\frac{c}{n\\epsilon}}\\end{array}$ and $c>0$ is small enough such that $k\\beta<1/10$ . The TV-distance between $\\mathcal{D}_{0}$ and $\\mathcal{D}_{1}$ is $\\beta$ . Since $\\begin{array}{r}{\\bar{n}=\\frac{c}{\\beta\\epsilon}}\\end{array}$ , we also choose $c$ small enough ", "page_idx": 24}, {"type": "text", "text": "such that Lemma A.9 is violated: for any $\\epsilon$ -differentially private algorithm $B:\\{0,1\\}^{n}\\rightarrow\\{0,1\\}$ , there exists an $i\\in\\{0,1\\}$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left({\\mathcal{B}}(X_{1},\\ldots,X_{n})=i\\right)<2/3,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "by Lemma A.9. ", "page_idx": 25}, {"type": "text", "text": "Consider now the task of $\\epsilon$ -privately estimating the parameter ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\boldsymbol{\\theta}(D):=\\mathbb{E}_{X_{1},\\dots,X_{k}\\sim\\mathcal{D}}\\left[h(X_{1},X_{2},\\dots,X_{k})\\right],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $X_{i}\\sim\\mathcal{D}$ and ", "page_idx": 25}, {"type": "equation", "text": "$$\nh(X_{1},X_{2},\\ldots,X_{k})={\\frac{1}{\\sqrt{\\tau_{(1-\\beta)^{k}}}}}\\mathbb{1}\\left(X_{1}=X_{2}=\\cdots=X_{k}=1\\right),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for some distribution $\\mathcal{D}$ . Suppose there exists an $\\epsilon$ -differentially private algorithm $\\boldsymbol{\\mathcal{A}}$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X_{1},\\ldots,X_{n}\\sim\\mathcal{D},\\mathcal{A}}\\Big[|\\mathcal{A}(X_{1},\\ldots,X_{n})-\\theta(\\mathcal{D})|\\Big]\\leq\\frac{1}{8}\\cdot\\frac{1-(1-\\beta)^{k}}{\\sqrt{\\tau_{(1-\\beta)^{k}}}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for any $\\mathcal{D}$ such that the distribution $\\mathcal{H}$ of $h(X_{1},\\ldots,X_{k})$ is sub-Gaussian $(1)$ . If $\\mathcal{D}=\\mathrm{Bern}(1)$ or $\\mathrm{Bern}(1\\!-\\!\\beta)$ , Lemma A.10 shows that the distribution $\\mathcal{H}$ of $h(X_{1},\\ldots,X_{k})$ is indeed sub-Gaussian(1). If inequality (A.42) holds, then by Markov\u2019s inequality, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{X_{i}\\sim\\mathcal{D}_{i}}\\left(|\\mathcal{A}(X_{1},\\ldots,X_{n})-\\theta\\left(\\mathcal{D}_{i}\\right)|\\leq\\frac{3}{8}\\cdot\\frac{1-\\left(1-\\beta\\right)^{k}}{\\sqrt{\\tau_{(1-\\beta)^{k}}}}\\right)\\geq\\frac{2}{3},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for $i\\in\\{0,1\\}$ . ", "page_idx": 25}, {"type": "text", "text": "Also, $\\theta\\left(\\mathcal{D}_{0}\\right)=1/\\sqrt{\\tau_{1-\\beta}}$ and $\\theta\\left(\\mathcal{D}_{1}\\right)=(1-\\beta)^{k}/\\sqrt{\\tau_{1-\\beta}}.$ . The difference between these means is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\theta\\left(\\mathcal{D}_{0}\\right)-\\theta\\left(\\mathcal{D}_{1}\\right)=\\frac{1-(1-\\beta)^{k}}{\\sqrt{\\tau_{(1-\\beta)^{k}}}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, the following algorithm violates inequality (A.41): Run $\\boldsymbol{\\mathcal{A}}$ on $X_{1},\\ldots,X_{n}$ to obtain $\\tilde{\\theta}$ Output 0 if $\\tilde{\\theta}$ is closer to $\\theta(\\mathcal{D}_{0})$ than to $\\theta(\\mathcal{D}_{1})$ , and output 1 otherwise. ", "page_idx": 25}, {"type": "text", "text": "This implies inequality (A.42) does not hold, so we have a lower bound on the expected error. Theorem 1 follows from the calculation ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1-(1-\\beta)^{k}}{\\sqrt{\\tau_{(1-\\beta)^{k}}}}\\ge\\frac{k\\beta}{2}\\cdot\\sqrt{2\\log\\frac{1}{2\\,(1-(1-\\beta)^{k})}}=\\Theta\\left(k\\beta\\sqrt{\\log\\frac{1}{k\\beta}}\\right),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we used $1-k\\beta<(1-\\beta)^{k}<1-k\\beta/2$ for $k\\beta<1/10$ . ", "page_idx": 25}, {"type": "text", "text": "A.4.2 Proof of Theorem 3 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Consider two datasets $D_{0}$ and $D_{1}$ of size $n$ each, differing in at most $1/\\epsilon$ data points. Suppose $\\begin{array}{r}{\\mathbb{E}_{A}|A(D)-U_{n}(D)|<\\frac{1}{10}|U_{n}(D_{0})-U_{n}(D_{1})|}\\end{array}$ for $D\\,\\in\\,\\{D_{0},D_{1}\\}$ . By Markov\u2019s inequality, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(|{\\mathcal{A}}(D)-U_{n}(D)|<{\\frac{1}{2}}|U_{n}(D_{0})-U_{n}(D_{1})|\\right)\\geq0.8,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for $D\\in\\{D_{0},D_{1}\\}$ . Moreover, since $\\boldsymbol{\\mathcal{A}}$ is $\\epsilon$ -differentially private, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\bigg(\\vert A(D_{1})-U_{n}(D_{0})\\vert<\\frac{\\vert U_{n}(D_{0})-U_{n}(D_{1})\\vert}{2}\\bigg)}\\\\ &{\\quad\\ge\\frac{1}{e}\\operatorname*{Pr}\\bigg(\\vert A(D_{0})-U_{n}(D_{0})\\vert<\\frac{\\vert U_{n}(D_{0})-U_{n}(D_{1})\\vert}{2}\\bigg)\\ge\\frac{0.8}{e}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By the triangle inequality, the event $\\begin{array}{r}{\\Big\\{|A(D_{1})-U_{n}(D_{0})|<\\frac{|U_{n}(D_{0})-U_{n}(D_{1})|}{2}\\Big\\}}\\end{array}$ is disjoint from the event $\\begin{array}{r}{\\Big\\{|A(D_{1})-U_{n}(D_{1})|<\\frac{|U_{n}(D_{0})-U_{n}(D_{1})|}{2}\\Big\\}}\\end{array}$ . The sum of the probabilities of these two events is ", "page_idx": 25}, {"type": "text", "text": "at least $0.8+0.8e^{-1}>1$ , a contradiction. Therefore, $\\boldsymbol{\\mathcal{A}}$ has expected error $\\Omega(|U_{n}(D_{0})-U_{n}(D_{1})|)$ on at least one of $D_{0}$ or $D_{1}$ . Next, we will define appropriate choices of $D_{0}$ and $D_{1}$ . ", "page_idx": 26}, {"type": "text", "text": "For simplicity, assume $1/\\epsilon$ is an integer. Define $b_{n}=\\lceil k+k^{1/(2k-2)}n^{1-1/(2k-2)}-\\frac{1}{\\epsilon}\\rceil$ . The assumed range of $\\epsilon$ implies that $\\dot{b_{n}}\\geq2k/\\epsilon$ . Let $h(x_{1},\\ldots,x_{k})=1(x_{1}=\\cdots=x_{k})$ . We define $D_{0}$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\nx_{i}=\\left\\{{1,\\begin{array}{c l}{{i\\leq b_{n},}}\\\\ {{i,}}&{{i>b_{n}.}}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We define $D_{1}=\\{y_{1},\\ldots,y_{n}\\}$ such that $y_{i}=x_{i}$ for all $i\\not\\in\\{b_{n}+1,\\ldots,b_{n}+1/\\epsilon\\}$ and $y_{i}=1$ for $\\begin{array}{r}{b_{n}<i\\le b_{n}+\\frac{1}{\\epsilon}}\\end{array}$ . Hence, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{U_{n}(D_{1})-U_{n}(D_{0})=\\frac{{\\binom{b_{n}+1/\\epsilon}{k}}}{{\\binom{n}{k}}}-\\frac{{\\binom{b_{n}}{k}}}{{\\binom{n}{k}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Furhtermore, for $\\frac{k}{\\epsilon}\\leq b_{n}$ , using the fact that $\\begin{array}{r}{(1+x)^{r}\\leq\\frac{1}{1-r x}}\\end{array}$ for $x\\in[-1,1/r)$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle1-\\frac{\\binom{b_{n}}{k}}{\\binom{b_{n}+1/\\epsilon}{k}}=1-\\prod_{i=0}^{k-1}\\frac{b_{n}-i}{b_{n}+1/\\epsilon-i}\\geq1-\\bigg(\\frac{b_{n}}{b_{n}+1/\\epsilon}\\bigg)^{k}}\\\\ {\\displaystyle=1-\\Bigg(1-\\frac{1/\\epsilon}{b_{n}+1/\\epsilon}\\Bigg)^{k}\\geq1-\\frac{1}{1+\\frac{k/\\epsilon}{b_{n}+1/\\epsilon}}=\\frac{k/\\epsilon}{b_{n}+(k+1)/\\epsilon},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "implying that ", "page_idx": 26}, {"type": "equation", "text": "$$\nU_{n}(D_{1})-U_{n}(D_{0})\\ge\\frac{k/\\epsilon}{b_{n}+(k+1)/\\epsilon}\\;{\\binom{b_{n}+1/\\epsilon}{k}}\\bigg/\\left(\\!\\!\\begin{array}{c}{{\\!\\!\\Big/n\\!\\!\\!}_{k}}\\\\ {{\\!\\!\\!\\Big/k\\!\\!\\!}}\\end{array}\\!\\!\\!\\right)\\bigg/\\left(\\!\\!\\!\\begin{array}{c}{{n}}\\\\ {{k}}\\end{array}\\!\\!\\!\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For $i$ with $x_{i}=1$ in $D_{0}$ , we have $\\begin{array}{r}{\\hat{h}_{D_{0}}(i)=\\frac{\\binom{b_{n}-1}{k-1}}{\\binom{n-1}{k-1}}\\leq U_{n}(D_{0})}\\end{array}$ . For $i$ with $x_{i}=1$ in $D_{1}$ , we have $\\begin{array}{r}{\\hat{h}_{D_{1}}(i)=\\frac{\\binom{b_{n}+1/\\epsilon-1}{k-1}}{\\binom{n-1}{k-1}}\\leq U_{n}(D_{1})}\\end{array}$ . Therefore, we have $|\\hat{h}_{D}(i)-U_{n}(D)|\\le\\hat{h}_{D}(i)\\le\\xi$ for all $i$ and $D\\in\\{D_{0},D_{1}\\}$ , where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\xi:=\\frac{\\binom{b_{n}+1/\\epsilon-1}{k-1}}{\\binom{n-1}{k-1}}=\\prod_{i=1}^{k-1}\\left(\\frac{b_{n}+1/\\epsilon-i}{n-i}\\right)\\le\\left(\\frac{b_{n}+1/\\epsilon}{n}\\right)^{k-1}=O\\left(\\sqrt{\\frac{k}{n}}\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "by our choice of $b_{n}$ . Moreover, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\xi\\ge\\left(\\frac{b_{n}+1/\\epsilon-k}{n-k}\\right)^{k-1}\\ge\\left(\\frac{k^{1/(2k-2)}n^{1-1/(2k-2)}}{n}\\right)^{k-1}=\\sqrt{\\frac{k}{n}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By inequality (A.43) and the definition of $\\xi$ , we see that ", "page_idx": 26}, {"type": "equation", "text": "$$\nU_{n}(D_{1})-U_{n}(D_{0})\\geq\\frac{k/\\epsilon}{b_{n}+2k/\\epsilon}\\frac{b_{n}+1/\\epsilon}{n}\\xi\\geq\\frac{k}{3n\\epsilon}\\xi,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second inequality follows from the assumption that $k/\\epsilon\\le b_{n}$ . Using the lower bound on $\\xi$ as in inequality (A.44), we obtain the desired result. ", "page_idx": 26}, {"type": "text", "text": "A.5 Proof of Theorems 2 and 4 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We first prove Theorem 4 with $\\boldsymbol{S}$ equal to any subsampled family that satisfies the inequalities (17). In particular, the following lemma guarantees that the required bounds hold with high probability for a subsampled family chosen uniformly at random from $\\mathcal{T}_{n,k}$ : ", "page_idx": 26}, {"type": "text", "text": "Lemma A.11. Let $\\gamma\\,>\\,0$ , and let $\\begin{array}{r}{M\\,=\\,\\Omega\\left(\\frac{n^{2}}{k^{2}}\\log\\left(\\frac{n}{\\gamma}\\right)\\right)}\\end{array}$ . Let $\\boldsymbol{S}$ be a collection of $M$ i.i.d sets sampled uniformly from $\\mathcal{T}_{n,k}$ . For each $i\\in[n]$ , let ${\\mathbf{}}S_{i}$ be the number of sets in $\\boldsymbol{S}$ containing $i$ , and define $M_{i}=|S_{i}|$ . For each $i\\neq j\\in[n]$ , let ${\\mathcal{S}}_{i j}$ be the number of sets in $\\boldsymbol{S}$ containing $i$ and $j$ , and define $M_{i j}=|S_{i j}|$ . With probability at least $1-\\gamma,$ , for all distinct indices $i$ and $j$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{k}{2n}\\leq\\frac{M_{i}}{M}\\leq\\frac{2k}{n},\\qquad\\frac{k}{2n}\\leq\\frac{M_{i j}}{M_{i}}\\leq\\frac{2k}{n}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Note that $M_{i}\\sim\\operatorname{Binom}(M,k/n)$ . By a Chernoff bound, for any $\\delta>0$ and any $i\\in[n]$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\left|M_{i}-{\\frac{k M}{n}}\\right|>{\\frac{k M}{2n}}\\right)\\leq e^{-(0.5)^{2}(k M/n)/3}\\leq e^{-\\Omega\\left({\\frac{n}{k}}\\log{\\frac{n}{\\gamma}}\\right)},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which is much smaller than $\\scriptstyle{\\frac{\\gamma}{2n}}$ . Call this event $\\mathcal{E}_{i}$ , and for the remaining argument, assume $\\mathcal{E}_{i}$ holds for all $i$ (which, by a union bound, holds with probability at least $\\textstyle1-{\\frac{\\gamma}{2}})$ . This gives the first inequality. For any distinct $i,j\\in[n]$ , conditioned on the value of $M_{i}$ , we have $M_{i j}\\sim\\mathrm{Binom}(m_{i},(k-1)/(n-$ 1)). By a Chernoff bound, for any $\\delta>0$ and $i,j\\in[n]$ with $j\\neq i$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\left|M_{i j}-\\frac{(k-1)M_{i}}{(n-1)}\\right|>\\frac{(k-1)M_{i}}{2(n-1)}\\right|\\,M_{i}\\right)\\le e^{-\\frac{(0.5)^{2}((k-1)M_{i}/(n-1))}{3}}\\le e^{-\\frac{k^{2}M}{48n^{2}}}\\le\\frac{\\gamma}{2n^{2}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "using the fact that $\\begin{array}{r}{M_{i}\\geq\\frac{k M}{2n}}\\end{array}$ and our assumption on $M$ . The second inequality then follows from a union bound over all pairs of indices. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "A.5.1 Proof of Theorem 4 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Consider two adjacent datasets $\\mathbf{X}=(X_{1},X_{2},\\ldots,X_{n})$ and $\\mathbf{X}^{\\prime}=(X_{1}^{\\prime},X_{2}^{\\prime},\\ldots,X_{n}^{\\prime})$ differing only in the index $i^{*}$ , that is, $X_{i}^{\\prime}=X_{i}$ for all $i\\neq i^{*}$ . Throughout the proof, we will use the superscript prime to denote quantities related to $\\mathbf{X^{\\prime}}$ . ", "page_idx": 27}, {"type": "text", "text": "Let $B:=(\\mathrm{Bad}(\\mathbf{X})\\cup\\mathrm{Bad}(\\mathbf{X}^{\\prime}))\\setminus\\{i^{*}\\}$ and $G:=({\\mathrm{Good}}(\\mathbf{X})\\cap{\\mathrm{Good}}(\\mathbf{X}^{\\prime}))\\mid\\{i^{*}\\}.$ . Then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle m\\left(\\tilde{A}_{n}-\\tilde{A}_{n}^{\\prime}\\right)=\\sum_{S\\in\\mathcal{S}}\\left(g(X_{S})-g(X_{S}^{\\prime})\\right)\\quad}\\\\ {\\displaystyle=\\sum_{S\\cap B\\neq\\emptyset\\atop\\underbrace{i^{*}\\notin S}}\\left(g(X_{S})-g(X_{S}^{\\prime})\\right)+\\sum_{S\\cap B=\\emptyset\\atop i^{*}\\notin S}\\left(g(X_{S})-g(X_{S}^{\\prime})\\right)+\\sum_{\\underbrace{i^{*}\\in S}_{T_{3}}}\\left(g(X_{S})-g(X_{S}^{\\prime})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We bound each of the three terms separately. The term $T_{2}$ is equal to 0: all indices $i\\in S$ have weight 1, and $i^{*}\\notin S$ , so $g(X_{S})=h(X_{S})=h(X_{S}^{\\prime})=g(X_{S}^{\\prime})$ . We prove some preliminary lemmas before bounding the first and last terms. Recall the definitions of $L$ and wt in equations (12) and (14), respectively. ", "page_idx": 27}, {"type": "text", "text": "Lemma A.12. We have: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "(ii) For all $i\\neq i^{*}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left||{\\hat{h}}\\prime(i)-A_{n}^{\\prime}|-|{\\hat{h}}(i)-A_{n}|\\right|\\leq{\\frac{4k C}{n}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n|w t(i)-w t^{\\prime}(i)|\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and for $S$ , such that $i^{*}\\notin S$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n|w t(S)-w t^{\\prime}(S)|\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. For (i), note that ", "page_idx": 27}, {"type": "equation", "text": "$$\n|A_{n}-A_{n}^{\\prime}|={\\frac{1}{M}}\\left|\\sum_{S\\in S_{i^{*}}}\\left(h(X_{S})-h(X_{S}^{\\prime})\\right)\\right|\\leq{\\frac{M_{i}C}{M}}\\leq{\\frac{2k C}{n}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last inequality comes from Lemma A.11. Similarly, for any $i\\neq i^{*}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Big|\\hat{h}(i)-\\hat{h},(i)\\Big|=\\frac{1}{M_{i}}\\left|\\sum_{S\\in\\mathcal{S}_{i j}}\\left(h(X_{S})-h(X_{S}^{\\prime})\\right)\\right|\\le\\frac{M_{i j}C}{M_{i}}\\le\\frac{2k C}{n}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "image", "img_path": "zApFYcLg6K/tmp/2105b45f9da11d8bb9e09c176f833eebd79510caaba1d481b34cc93cbc5ae597.jpg", "img_caption": ["Figure A.1: Weighting scheme in Eq (14) "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Therefore, if an index $i\\neq i^{*}$ is in $\\operatorname{Good}(\\mathbf{X})$ , using inequalities (A.49) and (A.50), we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\hat{h},(i)-A_{n}^{\\prime}\\right|\\le\\left|\\hat{h},(i)-\\hat{h}(i)\\right|+\\left|\\hat{h}(i)-A_{n}\\right|+|A_{n}-A_{n}^{\\prime}|}\\\\ &{\\qquad\\qquad\\qquad\\le\\displaystyle\\frac{2k C}{n}+\\left(\\xi+\\frac{6k C L}{n}\\right)+\\frac{2k C}{n}=\\xi+\\frac{k C\\left(4+6L\\right)}{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which leaves at most $1+L$ potential indices $i$ for which $\\begin{array}{r}{|\\hat{h}\\prime(i)-A_{n}^{\\prime}|>\\xi+\\frac{6k C(1+L)}{n}}\\end{array}$ : the indices in $\\operatorname{Bad}(\\mathbf{X})$ and the index $i^{*}$ . Therefore, $L^{\\prime}\\leq L+1$ . Similarly, $L\\leq L^{\\prime}+1$ . ", "page_idx": 28}, {"type": "text", "text": "For (ii), note that from inequalities (A.49) and (A.50), we have $\\begin{array}{r}{\\left|\\left|\\hat{h}\\prime(i)-A_{n}^{\\prime}\\right|-\\left|\\hat{h}(i)-A_{n}\\right|\\right|\\le\\frac{4k C}{n}}\\end{array}$ for $i\\neq i^{*}$ . Recalling the definition (14), this implies that the difference between the weights on an index $i$ can never be greater than $\\epsilon$ . ", "page_idx": 28}, {"type": "text", "text": "Finally, note that the weight of a subset $S$ , $w t(S)=\\operatorname*{min}_{i\\in S}w t(i)$ . Now, by inequality (A.47), each $w t(i)$ differs by $\\epsilon$ . Say $a=\\arg\\operatorname*{min}_{i\\in S}w t(i)$ . In order to make the difference between $w t(S)$ and $w t(S^{\\prime})$ large we will set $w t^{\\prime}(a)\\,=\\,w t(a)\\,+\\,\\epsilon$ and take some other $b$ and set $w t^{\\prime}(b)\\,=\\,w t(b)\\,-\\,\\epsilon$ such that $\\bar{w}t^{\\prime}(b)\\leq w t^{\\prime}(a)$ . But then, $w t(a)\\leq w t(b)\\leq w t(a)+2\\epsilon$ . This completes the proof of inequality (A.48). \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Next, we show that the weighted H\u00e1jek variants ${\\hat{g}}(i)$ are close to the empirical mean $A_{n}$ and have low sensitivity. ", "page_idx": 28}, {"type": "text", "text": "Lemma A.13. For all indices $i$ , we have $\\begin{array}{r}{|\\hat{g}(i)-A_{n}|\\leq\\xi+\\frac{9k C L}{n}+\\frac{6k C}{n\\epsilon}}\\end{array}$ . Moreover, if $i\\neq i^{*}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n|(\\hat{g}(i)-A_{n})-(\\hat{g}_{\\prime}(i)-A_{n}^{\\prime})|\\le\\left(\\xi+\\frac{k C(14+6L)}{n}\\right)\\epsilon+\\frac{10k C}{n}+\\frac{4k^{2}C}{n^{2}}(1+2L).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. If $\\mathrm{wt}(i)=0$ , then $g(X_{S})=A_{n}$ for all $S\\ni i$ and ${\\hat{g}}(i)=A_{n}$ . For clarity, we add a picture of this weighting scheme here: Otherwise, we write ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\hat{g}(i)-A_{n}=\\cfrac{1}{M_{i}}\\sum_{S\\in S_{i}}(h(X_{S})-A_{n})\\mathrm{wt}(S)}}\\\\ &{=\\cfrac{1}{M_{i}}\\sum_{S\\in S_{i}}\\left(h(X_{S})-A_{n}\\right)\\mathrm{wt}(i)+\\cfrac{1}{M_{i}}\\sum_{S\\in S_{i}}\\left(h(X_{S})-A_{n}\\right)\\left(\\mathrm{wt}(S)-\\mathrm{wt}(i)\\right)}\\\\ &{=(\\hat{h}(i)-A_{n})\\mathrm{wt}(i)+\\cfrac{1}{M_{i}}\\sum_{S\\in S_{i}}\\left(h(X_{S})-A_{n}\\right)\\left(\\mathrm{wt}(S)-\\mathrm{wt}(i)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "From equation (14) and the assumption that the weight of index $i$ is strictly positive, the magnitude of the first term in equation (A.52) is bounded by $\\begin{array}{r}{\\xi+\\frac{6k C L}{n}+\\frac{6k C}{n\\epsilon}}\\end{array}$ . For the second term, note ", "page_idx": 28}, {"type": "text", "text": "that $\\mathrm{wt}(S)\\,=\\,\\mathrm{wt}(i)$ unless an index $j$ with a lower weight than $i$ exists. Note that such an index $j$ is necessarily in $\\operatorname{Bad}(\\mathbf{X})$ . Therefore, the absolute value of the second term is bounded above by $\\begin{array}{r}{\\frac{1}{m_{i}}\\sum_{j\\in\\mathrm{Bad}(\\mathbf{X}),j\\neq i}C\\leq\\frac{2k C L}{n}}\\end{array}$ . This proves the first part of the lemma. ", "page_idx": 29}, {"type": "text", "text": "To bound the sensitivity of ${\\hat{g}}(i)$ , by the triangle inequality, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|(\\hat{h}(i)-A_{n})\\mathrm{wt}(i)-(\\hat{h},(i)-A_{n}^{\\prime})\\mathrm{wt}^{\\prime}(i)\\right|}\\\\ &{\\le\\left|\\hat{h}(i)-A_{n}\\right||\\mathrm{wt}(i)-\\mathrm{wt}^{\\prime}(i)|+\\left|(\\hat{h}(i)-A_{n})-(\\hat{h},(i)-A_{n}^{\\prime})\\right|\\mathrm{wt}^{\\prime}(i)}\\\\ &{\\le\\left(\\xi+\\displaystyle\\frac{k C\\left(4+6L\\right)}{n}+\\frac{6k C}{n\\epsilon}\\right)\\epsilon+\\displaystyle\\frac{6k C}{n}}\\\\ &{=\\left(\\xi+\\displaystyle\\frac{k C\\left(10+6L\\right)}{n}\\right)\\epsilon+\\displaystyle\\frac{6k C}{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the argument for the second inequality is as follows: To bound the first term, note that $\\operatorname{wt}(i)\\,=\\,\\operatorname{wt}^{\\prime}(i)$ $\\mathrm{wt}(i)\\,>\\,0$ $\\begin{array}{r}{|\\hat{h}(i)-A_{n}|\\le\\xi+\\frac{6k C L}{n}+\\frac{6k C L}{n\\epsilon}}\\end{array}$ m. mNa oAw.,1 i2f, $\\begin{array}{r}{|\\hat{h}(i)-A_{n}|>\\xi+\\frac{6k C L}{n}+\\frac{6k C L}{n\\epsilon}+\\frac{4k C}{n}}\\end{array}$ $\\mathrm{wt}(i)=0$   \nwe see that $\\begin{array}{r}{|\\hat{h}(i)-A_{n}^{\\prime}|\\,>\\,\\xi+\\frac{6k C L}{n}\\,+\\frac{6k C L}{n\\epsilon}}\\end{array}$ , so $\\mathrm{wt}^{\\prime}(i)$ will also be zero. The second term is bounded directly by Lemma A.12. Overall, we arrive at a bound on the sensitivity of the first term in equation (A.52). ", "page_idx": 29}, {"type": "text", "text": "For the sensitivity of the second term of equation (A.52), note that if $i$ has minimum weight among the indices in $S$ , then $\\mathrm{wt}(S)=\\mathrm{wt}(i)$ . Otherwise, some index $j\\in S$ has strictly lower weight than $i$ Such an index $j$ is necessarily in $\\operatorname{Bad}(\\mathbf{X})\\cup\\operatorname{Bad}(\\mathbf{X}^{\\prime})$ because it has weight less than 1. If $S$ also does not contain the index $i^{*}$ , then $h(X_{S})=\\dot{h}(X_{S}^{\\prime})$ , so by Lemma A.12, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle|(h(X_{S}^{\\prime})-A_{n}^{\\prime})-(h(X_{S})-A_{n})|\\leq|A_{n}-A_{n}^{\\prime}|\\leq\\frac{2k C}{n},}\\\\ {\\displaystyle|(\\mathrm{wt}^{\\prime}(S)-\\mathrm{wt}^{\\prime}(i))-(\\mathrm{wt}(S)-\\mathrm{wt}(i))|\\leq2\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and letting ", "page_idx": 29}, {"type": "equation", "text": "$$\nT_{S}:=\\left((h(X_{S})-A_{n})\\left(\\operatorname{wt}(S)-\\operatorname{wt}(i)\\right)-(h(X_{S}^{\\prime})-A_{n}^{\\prime})\\left(\\operatorname{wt}^{\\prime}(S)-\\operatorname{wt}^{\\prime}(i)\\right)\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "we have $\\begin{array}{r}{|T_{S}|\\le\\frac{2k C}{n}+2C\\epsilon}\\end{array}$ . ", "page_idx": 29}, {"type": "text", "text": "Moreover, there are at most $M_{i,i^{*}}$ sets $S$ containing both $i$ and $i^{*}$ , and for each such set $S$ , the change in $\\mathbf{\\chi}(h(X_{S})-A_{n})\\left(\\mathbf{wt}(S)-\\mathbf{wt}(i)\\right)$ is at most $2C$ , since the weights lie in $[0,1]$ and $|h(X_{\\cal S})-A_{n}|\\leq$ $C$ . Combining these bounds, we obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{M_{i}}\\displaystyle\\sum_{\\hat{\\mathcal{C}}\\in\\mathcal{E}_{i}}\\frac{\\big((h(X_{S})-A_{n})\\big)\\big(\\Psi(X)-\\Psi(i)\\big)-\\big(h(X_{S}^{\\prime})-A_{n}^{\\prime}\\big)\\big(\\Psi^{\\prime}(S)-\\Psi^{\\prime}(i)\\big)\\Big)}{T_{S}}\\Big|}\\\\ &{\\le\\displaystyle\\frac{1}{M_{i}}\\displaystyle\\sum_{\\hat{\\mathcal{C}}\\in\\mathcal{E}_{i}\\setminus\\mathcal{D}_{i,n}\\setminus\\mathcal{C}}\\|T_{i}\\|\\big(|S\\cap(\\mathbb{B a d}(\\mathbf{X}))\\cup\\mathbf{Bad}(\\mathbf{X}^{\\prime})|\\big)>0+\\displaystyle\\sum_{s\\in\\mathcal{C}_{i}\\setminus\\mathcal{C}_{i}}\\|T_{S}\\|\\Bigg)}\\\\ &{\\le\\left(2c\\epsilon+\\displaystyle\\frac{2k C}{n}\\right)\\displaystyle\\frac{1}{M_{i}}\\displaystyle\\sum_{\\hat{\\mathcal{C}}\\in\\mathcal{X}_{i}\\setminus\\mathcal{C}_{i}}\\|\\big(|S\\cap\\big(\\mathbb{B a d}(\\mathbf{X})\\cup\\mathbb{B a d}(\\mathbf{X})\\big)|>0+\\frac{1}{M_{i}}\\displaystyle\\sum_{s\\in\\mathcal{C}_{i}\\setminus\\mathcal{C}_{i}}2C}\\\\ &{\\le\\left(2c\\epsilon+\\displaystyle\\frac{2k C}{n}\\right)\\displaystyle\\frac{1}{M_{i}}\\displaystyle\\sum_{\\hat{\\mathcal{C}}\\in\\mathcal{W}(\\mathbb{X})\\cup\\mathbb{B a d}(\\mathbf{X})}\\displaystyle\\sum_{j\\in\\mathcal{C}_{i}}1+2C\\frac{M_{i}\\varepsilon}{M_{i}}}\\\\ &{\\le\\left(2c\\epsilon+\\displaystyle\\frac{2k C}{n}\\right)\\left(|\\mathbb{B a d}(\\mathbf{X})|+|\\mathbb{B a d}(\\mathbf{X})|\\right)}\\\\ &{\\le\\frac{2k}{n}\\left(2c\\epsilon+\\displaystyle\\frac{2k C}{n}\\right)\\left(1+2L\\right)+\\frac{4k C}{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the first inequality uses the fact that the weights are all equal if $S\\cap\\left(\\operatorname{Bad}(\\mathbf{X})\\cup\\operatorname{Bad}(\\mathbf{X}^{\\prime})\\right)=\\emptyset$ , and the last inequality uses Lemma A.11. Combining inequalities (A.53) and (A.54) into equation (A.52) yields the result. ", "page_idx": 29}, {"type": "text", "text": "To bound the term $T_{1}$ in (A.45), we decompose it as ", "page_idx": 30}, {"type": "equation", "text": "$$\nT_{1}=\\sum_{\\stackrel{i\\in B}{i\\in B}}\\sum_{\\stackrel{S\\in S_{i}}{i^{*}\\not\\in S}}\\left(g(X_{S})-g(X_{S}^{\\prime})\\right)-\\sum_{\\stackrel{a=2}{a=2}}^{\\operatorname*{min}(k,\\lfloor B\\rfloor)}\\sum_{\\stackrel{S\\in S}{\\stackrel{|S\\cap B|=a}{i^{*}\\not\\in S}}}\\left(a-1\\right)\\left(g(X_{S})-g(X_{S}^{\\prime})\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The first term sums over all subsets that contain some element in $B$ . However, this leads to overcounting every subset with $a$ elements in common with $B$ exactly $a-1$ times. The second term corrects for this over-counting, akin to an inclusion-exclusion argument. The following lemmas bound each of the two terms: ", "page_idx": 30}, {"type": "text", "text": "Lemma A.14. For all $i\\in B$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\frac{1}{M_{i}}}\\left|\\sum_{S\\in S_{i},i^{*}\\notin S}\\left(g(X_{S})-g(X_{S}^{\\prime})\\right)\\right|\\leq\\left(\\xi+{\\frac{20k C L}{n}}\\right)\\epsilon+{\\frac{12k C}{n}}+{\\frac{14k^{2}C L}{n^{2}}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. We have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{\\stackrel{S\\in S_{i}}{i^{*}\\not\\in S}}\\left(g(X_{S})-g(X_{S}^{\\prime})\\right)=M_{i}\\left(\\hat{g}(i)-\\hat{g}(i)\\right)-\\sum_{S\\in S_{i,i^{*}}}\\left(g(X_{S})-g(X_{S}^{\\prime})\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By Lemmas A.12 and A.13, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n|\\hat{g}(i)-\\hat{g}(i)|\\leq\\left(\\xi+\\frac{k C(14+6L)}{n}\\right)\\epsilon+\\frac{12k C}{n}+\\frac{4k^{2}C}{n^{2}}(1+2L).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Moreover, the second term in equation (A.56) is clearly upper-bounded by ", "page_idx": 30}, {"type": "equation", "text": "$$\n2C M_{i,i^{*}}\\leq\\frac{4k C}{n}M_{i}\\leq\\frac{8k^{2}C}{n^{2}}M_{i}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Summing these two bounds yields the result. ", "page_idx": 30}, {"type": "text", "text": "For the second term of equation (A.55), we use the following lemma: ", "page_idx": 30}, {"type": "text", "text": "Lemma A.15. We have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{\\stackrel{a=2}{a=2}}^{\\mathrm{in}(k,\\left|B\\right|)}\\sum_{\\stackrel{S\\in\\mathcal{S}}{|S\\cap B|=a}}\\,\\left(a-1\\right)\\left|g(X_{S})-g(X_{S}^{\\prime})\\right|\\leq\\frac{36k^{2}L^{2}}{n^{2}}\\left(2C\\epsilon+\\frac{6k C}{n}\\right)\\operatorname*{min}(k,2L)M.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Moreover, if $S={\\mathcal{T}}_{n,k}$ , with $m={\\binom{n}{k}}$ , we have the stronger inequality ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{a=2\\atop a\\ne b\\/B\\/=a}^{\\operatorname*{min}(k,\\vert B\\vert)}\\sum_{{S\\in\\mathcal{S}}\\atop{i^{*}\\notin S}}\\ (a-1)\\,\\vert g(X_{S})-g(X_{S}^{\\prime})\\vert\\leq\\frac{9k^{2}L^{2}}{n^{2}}\\,\\biggl(2C\\epsilon+\\frac{6k C}{n}\\biggr)\\,m.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. For any $S$ not containing $i^{*}$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g(X_{S})-g(X_{S}^{\\prime})\\vert=|(h(X_{S})-A_{n})\\mathrm{wt}(S)+A_{n}-(h(X_{S}^{\\prime})-A_{n}^{\\prime})\\mathrm{wt}^{\\prime}(S)-A_{n}^{\\prime}|}\\\\ &{\\phantom{g s d(X_{S})=}\\leq|(h(X_{S})-A_{n})(\\mathrm{wt}(S)-\\mathrm{wt}^{\\prime}(S))|+|(h(X_{S})-h(X_{S}^{\\prime}))\\mathrm{wt}^{\\prime}(S)|+|A_{n}-A_{n}^{\\prime}|}\\\\ &{\\phantom{g s d(X_{S})=}\\leq2C\\epsilon+\\frac{6k C}{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "using Lemma A.12 and the fact that the second term is zero. Moreover, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{a=2\\atop a}^{\\operatorname{min}(k,|B|)}}&{\\displaystyle\\sum_{\\stackrel{S\\in S}{|S\\cap B|=a}}\\;\\;(a-1)\\leq\\displaystyle\\sum_{a=2\\atop|S\\cap B|=a}^{k}\\displaystyle\\sum_{\\stackrel{{S\\in S}}{|S\\cap B|\\geq2}}\\operatorname*{min}(k,|B|)=\\displaystyle\\sum_{\\stackrel{S\\in S}{|S\\cap B|\\geq2}}\\operatorname*{min}(k,|B|)}\\\\ &{\\leq\\displaystyle\\sum_{\\stackrel{i,j\\in B}{i\\neq j}}\\sum_{S\\in S_{i j}}\\operatorname*{min}(k,|B|)\\leq\\displaystyle\\frac{9k^{2}}{n^{2}}\\operatorname*{min}(k,|B|)|B|^{2}m.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The last inequality follows from Lemma A.11, which implies that $\\begin{array}{r}{{\\frac{M_{i j}}{M}}={\\cal O}\\left(\\frac{k^{2}}{n^{2}}\\right)}\\end{array}$ . In the case when $S={\\mathcal{T}}_{n,k}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{a=2}{\\operatorname*{min}}\\underset{0\\leq t\\leq s}{\\sum}\\underset{0\\leq t\\leq s}{\\sum}\\underset{0=2}{\\overset{\\operatorname*{min}}{\\sum}}(a-1)\\leq\\underset{a=2}{\\operatorname*{min}}\\underset{0}{\\overset{\\lfloor B\\rfloor}{\\sum}}\\biggr(\\frac{|B|}{a}\\biggr)\\biggr(\\frac{n-|B|}{k-a}\\biggr)}&\\\\ &{\\overset{\\leq(n)}{+}\\underset{\\neq\\phi}{\\sum}\\frac{\\biggr(a-1)}{2}=}\\\\ &{\\leq\\biggr(\\frac{n}{k}\\biggr)\\left(\\frac{k|B|}{n}-1\\right)+\\biggr(\\frac{n-|B|}{k}\\biggr)}\\\\ &{\\leq\\binom{n}{k}\\left(\\frac{k|B|}{n}-1\\right)+\\binom{n}{k}\\left(1-\\frac{|B|}{n}\\right)^{k}}\\\\ &{\\leq\\binom{n}{k}\\left(\\frac{k|B|}{n}-1\\right)+\\binom{n}{k}\\frac{1}{\\frac{|B|}{n|n}+1}\\leq\\frac{k^{2}|B|^{2}}{n^{2}}\\binom{n}{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the first equality used the identities $\\begin{array}{r}{\\sum_{a=0}^{k}{\\binom{n}{a}}{\\binom{m}{k-a}}\\ =\\ {\\binom{n+m}{k}}}\\end{array}$ and $\\begin{array}{r l r}{\\sum_{a=0}^{k}a\\binom{n}{a}\\binom{m}{k-a}\\,=}&{{}}&{}\\end{array}$ ${\\frac{n k}{m+n}}\\left({n+m\\atop k}\\right)$ , and the third inequality used t he fact that $\\begin{array}{r}{(1-x)^{k}\\,\\leq\\,\\frac{1}{1+k x}}\\end{array}$ for all $x\\in[0,1]$ . The statement in the lemma follows because . ", "page_idx": 31}, {"type": "text", "text": "Combining the results of Lemma A.14 and A.15 yields Lemma A.16. ", "page_idx": 31}, {"type": "text", "text": "Lemma A.16. We have ", "page_idx": 31}, {"type": "equation", "text": "$$\n|T_{1}|\\le{\\frac{k M}{n}}\\left(\\left(\\xi+{\\frac{20k C L}{n}}\\right)2\\epsilon+{\\frac{24k C}{n}}+{\\frac{28k^{2}C L}{n^{2}}}+{\\frac{9k L^{2}}{n}}\\left(2C\\epsilon+{\\frac{6k C}{n}}\\right)\\gamma\\right),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\gamma=4\\operatorname*{min}(k,2L)$ . If ${\\cal S}={\\cal Z}_{n,k}.$ , then the bound also holds for $\\gamma=1$ . ", "page_idx": 31}, {"type": "text", "text": "It remains to bound the third term, $T_{3}$ , of equation (A.45), which we do in the following lemma: Lemma A.17. We have ", "page_idx": 31}, {"type": "equation", "text": "$$\n|T_{3}|\\le\\frac{2k}{n}\\left(2\\xi+\\frac{k C(11+18L)}{n}+\\frac{12k C}{n\\epsilon}\\right)M.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. Using Lemmas A.12 and A.13, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{M_{i^{*}}}\\left|T_{3}\\right|=\\left|\\hat{g}(i^{*})-\\hat{g}\\prime(i^{*})\\right|\\le\\left|\\hat{g}(i^{*})-A_{n}\\right|+\\left|\\hat{g}\\prime(i^{*})-A_{n}^{\\prime}\\right|+\\left|A_{n}-A_{n}^{\\prime}\\right|}\\\\ &{\\qquad\\qquad\\le\\left(\\xi+\\displaystyle\\frac{9k C L}{n}+\\displaystyle\\frac{6k C}{n\\epsilon}\\right)+\\left(\\xi+\\displaystyle\\frac{9k C L^{\\prime}}{n}+\\displaystyle\\frac{6k C}{n\\epsilon}\\right)+\\displaystyle\\frac{2k C}{n}}\\\\ &{\\qquad\\qquad\\le2\\xi+\\displaystyle\\frac{k C(11+18L)}{n}+\\displaystyle\\frac{12k C}{n\\epsilon}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The lemma follows after using the fact that $\\begin{array}{r}{M_{i^{*}}\\leq\\frac{2k}{n}M}\\end{array}$ . ", "page_idx": 31}, {"type": "text", "text": "Combining the bounds on $T_{1}$ and $T_{3}$ from Lemmas A.16 and A.17 in equation (A.45), the local sensitivity of $\\tilde{A}_{n}$ at $\\mathbf{X}$ is then bounded as ", "page_idx": 31}, {"type": "equation", "text": "$$\nL S_{\\tilde{A}_{n}}\\left(\\mathbf{X}\\right)=O\\left(\\frac{k}{n}\\left(\\xi+\\frac{k C L}{n}\\right)\\left(1+\\epsilon L\\right)+\\frac{k^{2}C L^{2}\\operatorname*{min}(k,L)}{n^{2}}\\left(\\epsilon+\\frac{k C}{n}\\right)+\\frac{k^{2}C}{n^{2}\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Let $g(\\xi,L,n)$ denote the upper bound, where to simplify the following argument, we assume the constant prefactor is 1, i.e., ", "page_idx": 31}, {"type": "equation", "text": "$$\ng(\\xi,L,n):=\\frac{k}{n}\\left(\\xi+\\frac{k C L}{n}\\right)(1+\\epsilon L)+\\frac{k^{2}C L^{2}\\operatorname*{min}(k,L)}{n^{2}}\\left(\\epsilon+\\frac{k}{n}\\right)+\\frac{k^{2}C}{n^{2}\\epsilon}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Note that $g$ is strictly increasing in $L$ . Also define ", "page_idx": 31}, {"type": "equation", "text": "$$\nS(\\mathbf{X})=\\operatorname*{max}_{\\ell\\in\\mathbb{Z}_{\\geq0}}e^{-\\epsilon\\ell}g(\\boldsymbol{\\xi},L_{\\mathbf{X}}+\\ell,n).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Lemma A.18. The function $S(\\mathbf{X})$ is an $\\epsilon$ -smooth upper bound on $L S_{\\tilde{A}_{n}}(\\mathbf{X})$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\Im(\\mathbf{X})=O\\left(\\frac{k}{n}\\left(\\xi+\\frac{k C(1/\\epsilon+L)}{n}\\right)(1+\\epsilon L)+\\frac{k^{2}C(1/\\epsilon+L)^{2}\\operatorname*{min}(k,1/\\epsilon+L)}{n^{2}}\\left(\\epsilon+\\frac{k}{n}\\right)+\\frac{k^{2}C(1/\\epsilon+L)^{2}}{n^{2}\\epsilon^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof of Lemma A.18. Clearly, we have $S(\\mathbf{X})\\geq g(\\xi,L_{\\mathbf{X}},n)\\geq L S_{\\tilde{A}_{n}}(\\mathbf{X})$ , and for any two adjacent $\\mathbf{X}$ and $\\mathbf{X^{\\prime}}$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S(\\mathbf{X}^{\\prime})=\\underset{\\ell\\in\\mathbb{Z}_{\\geq0}}{\\mathrm{max~}}e^{-\\epsilon\\ell}g(\\xi,L_{\\mathbf{X}^{\\prime}}+\\ell,n)\\leq\\underset{\\ell\\in\\mathbb{Z}_{\\geq0}}{\\mathrm{max~}}e^{-\\epsilon\\ell}g(\\xi,L_{\\mathbf{X}}+\\ell+1,n)}\\\\ &{\\quad\\quad=\\underset{\\ell\\in\\mathbb{Z}_{>0}}{\\mathrm{max~}}e^{-\\epsilon(\\ell-1)}g(\\xi,L_{\\mathbf{X}}+\\ell,n)\\leq e^{\\epsilon}\\underset{\\ell\\in\\mathbb{Z}_{\\geq0}}{\\mathrm{max~}}e^{-\\epsilon\\ell}g(\\xi,L_{\\mathbf{X}}+\\ell,n)=e^{\\epsilon}S(\\mathbf{X}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This shows that $S$ is indeed a $\\epsilon$ -smooth upper bound on the local sensitivity. As for the upper bound on $S$ , for any $\\ell\\geq0$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{\\cdot}{\\sim}\\epsilon\\ell(\\xi,L_{\\mathbf{X}}+\\ell,n)=\\frac{k}{n}\\left(\\xi e^{-\\epsilon\\ell/2}+\\frac{k C\\left(\\ell e^{-\\epsilon\\ell/2}+L e^{-\\epsilon\\ell/2}\\right)}{n}\\right)\\left(e^{-\\epsilon\\ell/2}+\\epsilon(\\ell e^{-\\epsilon\\ell/2}+L e^{-\\epsilon\\ell/2})\\right)}\\\\ &{+\\frac{k^{2}}{n^{2}}C(\\ell e^{-\\epsilon\\ell/3}+L e^{-\\epsilon\\ell/3})^{2}\\operatorname*{min}(k e^{-\\epsilon\\ell/3},\\ell e^{-\\epsilon\\ell/3}+L e^{-\\epsilon\\ell/3})\\left(\\epsilon+\\frac{k}{n}\\right)+\\frac{k^{2}C e^{-\\epsilon\\ell}}{n^{2}\\epsilon}}\\\\ &{\\leq\\frac{k}{n}\\left(\\xi+\\frac{k C(1/\\epsilon+L)}{n}\\right)(1+\\epsilon(1+L))+\\frac{k^{2}}{n^{2}}C\\left(\\frac{2}{\\epsilon}+L\\right)^{2}\\operatorname*{min}\\left(k,\\frac{2}{\\epsilon}+L\\right)\\left(\\epsilon+\\frac{k}{n}\\right)+\\frac{k^{2}C}{n^{2}\\epsilon},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By Lemma A.18, it is clear that the term $S(\\mathbf{X})$ added to $\\tilde{A}_{n}$ in Algorithm 1 is the smoothed sensitivity defined in equation (A.57). ", "page_idx": 32}, {"type": "text", "text": "Therefore, $\\tilde{A}_{n}+\\frac{S(\\mathbf{X})}{\\epsilon}\\cdot Z$ , where $Z$ is sampled from the distribution with density $h(z)\\propto1/(1\\!+\\!|z|^{4})$ is $O(\\epsilon)$ -differentially private, by Lemma 2. Moreover, if $S={\\mathcal{T}}_{n,k}$ , the above bound on the smooth sensitivity holds without the $\\operatorname*{min}(k,1/\\epsilon+L)$ term, due to Lemma A.15. ", "page_idx": 32}, {"type": "text", "text": "Utility. By Chebyshev\u2019s inequality, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n|A_{n}-\\theta|\\leq\\frac{1}{\\sqrt{\\gamma}}\\sqrt{\\mathrm{Var}(A_{n})}=\\frac{1}{\\sqrt{\\gamma}}\\left(\\sqrt{\\mathrm{Var}(U_{n})}+\\sqrt{\\frac{\\zeta_{k}}{m}}\\right),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "with probability at least $1\\mathrm{~-~}\\gamma$ . Moreover, with probability at least $1\\mathrm{~-~}\\gamma$ , each of the H\u00e1jek projections is within $\\xi$ of $A_{n}$ . This implies that every index $i$ has weight 1, which further implies that $g(X_{S})\\,=\\,h(X_{S})$ for all $S\\,\\in\\,S$ , and consequently, ${\\tilde{A}}_{n}\\,=\\,A_{n}$ . Also, $L\\,=\\,1$ for such an $\\mathbf{X}$ . Finally, with probability at least $1-\\gamma$ , we have $\\begin{array}{r}{\\dot{Z}\\leq\\frac{3}{\\sqrt{\\gamma}}}\\end{array}$ . Combining these inequalities and using Lemma A.18, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|A({\\bf X})-\\theta|\\le\\left|\\tilde{A}_{n}-A_{n}\\right|+|A_{n}-\\theta|+|S({\\bf X})/\\epsilon\\cdot Z|}\\\\ &{=\\frac{1}{\\sqrt{\\gamma}}O\\left(\\sqrt{\\mathrm{Var}(U_{n})}+\\sqrt{\\frac{\\zeta_{k}}{m}}+\\frac{k\\xi}{n\\epsilon}+\\left(\\frac{k^{2}C}{n^{2}\\epsilon^{2}}+\\frac{k^{3}C}{n^{3}\\epsilon^{3}}\\right)\\operatorname*{min}\\left(k,\\frac{1}{\\epsilon}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "with probability at least $1-4\\gamma$ , recalling that $\\epsilon=O(1)$ when simplifying the expression. Algorithm 1 uses a constant failure probability of $\\gamma=0.01$ , which ensures a success probability of at least 0.75. This is further boosted by Wrapper 1. Now, an application of Lemma A.6 gives the stated result. ", "page_idx": 32}, {"type": "text", "text": "A.5.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The proof of this theorem proceeds nearly identically to that of Theorem 4 with some exceptions. If $S={\\mathcal{T}}_{n,k}$ , the smoothed sensitivity bound has no $\\operatorname*{min}(k,1/\\epsilon)$ term, owing to Lemmas A.15 and A.16, which gives the bound ", "page_idx": 32}, {"type": "equation", "text": "$$\n|\\mathcal{A}(\\mathbf{X})-\\theta|\\leq\\frac{1}{\\sqrt{\\gamma}}O\\left(\\sqrt{\\mathrm{Var}(A_{n})}+\\frac{k\\xi}{n\\epsilon}+\\frac{k^{2}C}{n^{2}\\epsilon^{2}}+\\frac{k^{3}C}{n^{3}\\epsilon^{3}}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Furthermore, since $S={\\mathcal{T}}_{n,k}$ , we have $A_{n}=U_{n}$ with probability at least $1-3\\gamma$ . Algorithm 1 uses a constant failure probability of $\\gamma=0.01$ . This ensures a success probability of at least 0.75, which is further boosted by Wrapper 1. An application of Lemma A.6 gives the stated result. ", "page_idx": 32}, {"type": "text", "text": "A.5.3 Concentration of local H\u00e1jek projections ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Proof of Lemma 3. We first show that if $Y_{1},Y_{2},\\ldots,Y_{t}$ are random variables such that each $Y_{j}$ is $\\tau_{j}$ -sub-Gaussian, the sum $Y_{1}+\\cdot\\cdot\\cdot+Y_{t}$ is $\\left({\\sqrt{\\tau_{1}}}+{\\sqrt{\\tau_{2}}}+\\cdot\\cdot\\cdot+{\\sqrt{\\tau_{t}}}\\right)^{2}$ -sub-Gaussian. ", "page_idx": 33}, {"type": "text", "text": "Define it=\u221a1\u03c4j \u03c4i. Clearly, we have  tj=1 1/pj = 1. By H\u00f6lder\u2019s inequality, for any \u03bb > 0, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi\\left[\\exp\\left(\\lambda\\displaystyle\\sum_{i=1}^{t}Y_{i}\\right)\\right]=\\mathbb{E}\\left[\\prod_{i=1}^{t}\\exp(\\lambda Y_{i})\\right]\\leq\\displaystyle\\prod_{i=1}^{t}\\mathbb{E}\\left[\\exp(\\lambda Y_{i})^{p_{i}}\\right]^{1/p_{i}}\\leq\\displaystyle\\prod_{i=1}^{t}\\left(\\exp\\left(\\frac{\\lambda^{2}p_{i}^{2}\\tau_{i}}{2}\\right)\\right)^{1/p_{i}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\prod_{i=1}^{t}\\exp\\left(\\frac{\\lambda^{2}p_{i}\\tau_{i}}{2}\\right)=\\exp\\left(\\frac{\\lambda^{2}(\\sqrt{\\tau_{1}}+\\sqrt{\\tau_{2}}+\\cdots+\\sqrt{\\tau_{t}})^{2}}{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now, $h(X_{S})$ is sub-Gaussian $(\\tau)$ for all $S\\in{\\mathcal{T}}_{n,k}$ . Since $\\hat{h}(i)$ is the average of $t\\,=\\,{\\binom{n-1}{k-1}}$ such quantities, it is clear from the previous claim that $\\hat{h}(i)$ sub-Gaussian with parameter ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{1}{t^{2}}(\\underbrace{\\sqrt{\\tau}+\\sqrt{\\tau}+\\cdot\\cdot\\cdot+\\sqrt{\\tau}}_{t\\mathrm{\\terms}})^{2}=\\tau.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof of Lemma 4. Define $\\sigma_{i}^{2}:=\\operatorname{Var}\\left(h(X_{S_{i}})|X_{i}=x_{i}\\right)$ . First, conditioned on $X_{i}$ , the projection $\\hat{h}(i)$ can be viewed as a U-statistic on the other $n-1$ data points. First, for this lemma, we use $S={\\mathcal{T}}_{n,k}$ , and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\widehat{h}(i)=\\widehat{\\mathbb{E}}\\left[h(X_{S})|X_{i}\\right]=\\frac{\\sum_{S\\in S_{i}}h(X_{S})}{\\binom{n-1}{k-1}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Since $h$ is bounded, the random quantity $\\hat{h}(i)-\\mathbb{E}\\left[h(X_{S})|X_{i}\\right]\\in[-2C,2C]$ satisfies the Bernstein moment condition and also the Bernstein tail inequality (cf. Proposition 2.3 in [60]). By Bernstein\u2019s inequality for U-statistics (see inequality (A.29)), for all $t>0$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\left|\\hat{h}(i)-\\mathbb{E}\\left[h(X_{S})|X_{i}\\right]\\right|\\geq t\\Big|\\,X_{i}\\right)\\leq2\\exp\\left(\\frac{-\\left|\\frac{n-1}{k-1}\\right|t^{2}}{2\\sigma_{i}^{2}+4C t/3}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which is at most $\\beta/n$ as long as $\\begin{array}{r}{t=\\sigma_{i}\\sqrt{\\frac{4k}{n}}\\sqrt{\\log\\left(\\frac{2n}{\\beta}\\right)}+\\frac{8C k}{n}\\log\\Big(\\frac{2n}{\\beta}\\Big).}\\end{array}$ ", "page_idx": 33}, {"type": "text", "text": "A.6 Applications ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "A.6.1 Uniformity testing ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "To motivate the test, consider the expectation $\\theta:=\\mathbb{E}[h(X_{i},X_{j})]$ and the variance var $\\left(U_{n}\\right)$ : ", "page_idx": 33}, {"type": "text", "text": "Lemma A.19. We have $\\begin{array}{r}{\\mathbb{E}[h(X_{1},X_{2})]=\\frac{1}{m}\\!+\\!\\frac{||a||^{2}}{m^{2}}}\\end{array}$ . In particular, the means under the two hypothesis classes differ by at least $\\frac{\\delta^{2}}{2m}$ ", "page_idx": 33}, {"type": "text", "text": "Proof. We have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}[h(X_{1},X_{2})]=\\sum_{i=1}^{m}{p_{i}^{2}}=\\sum_{i=1}^{m}\\frac{1+2a_{i}+a_{i}^{2}}{m^{2}}=\\frac{1}{m}+\\frac{{\\|a\\|}^{2}}{m^{2}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Under approximate uniformity, this is at most $\\frac{\\delta^{2}}{2m}$ ; and under the alternative hypothesis, this quantity is at least $\\frac{\\delta^{2}}{m}$ \u53e3 m ", "page_idx": 33}, {"type": "text", "text": "Lemma A.20. The variance of $U_{n}$ is ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\nu a r(U_{n})=\\frac{2}{n(n-1)}\\left(2(n-2)\\sum_{i<j}p_{i}p_{j}(p_{i}-p_{j})^{2}+\\sum_{i=1}^{m}p_{i}^{2}-\\left(\\sum_{i=1}^{m}p_{i}^{2}\\right)^{2}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. The conditional variances $\\zeta_{1}$ and $\\zeta_{2}$ can be written as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\zeta_{1}=\\mathsf{c o v}(h(X_{1},X_{2}),h(X_{1},X_{3}))}\\\\ {\\displaystyle}&{\\displaystyle}&{\\displaystyle=\\mathbb{E}[\\mathbb{I}[X_{1}=X_{2}]\\mathbb{I}[X_{1}=X_{3}]]-\\mathbb{E}[\\mathbb{I}[X_{1}=X_{2}]]\\mathbb{E}[\\mathbb{I}[X_{1}=X_{3}]]}\\\\ {\\displaystyle}&{\\displaystyle}&{\\displaystyle=\\sum_{i}p_{i}^{3}-\\left(\\sum_{i}p_{i}^{2}\\right)^{2}=\\sum_{i<j}(p_{i}^{3}p_{j}+p_{i}p_{j}^{3}-2p_{i}^{2}p_{j}^{2})=\\sum_{i<j}p_{i}p_{j}(p_{i}-p_{j})^{2}\\ge0,}\\\\ {\\displaystyle}&{\\zeta_{2}=\\mathsf{c o v}(h(X_{1},X_{2}),h(X_{1},X_{2}))=\\sum_{i=1}^{m}p_{i}^{2}-\\left(\\sum_{i=1}^{m}p_{i}^{2}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Also from equation (3), we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname{var}(U_{n})={\\binom{n}{2}}^{-1}\\left(2(n-2)\\zeta_{1}+\\zeta_{2}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Combining the above bounds with equation (3) shows the result. ", "page_idx": 34}, {"type": "text", "text": "Algorithm A.4 PrivateUniformityTest n, m, X = {Xi}i\u2208[n] , \u03f5   \n1: $C\\gets1$ , $\\gamma\\leftarrow0.01$   \n2: $\\xi\\leftarrow6/m+8\\log(4n/\\gamma)/n$   \n3: $S\\gets\\{(i,j):1\\leq i<j\\leq n\\}$   \n4: $\\widetilde{\\theta}\\gets\\mathbf{PrivateMeanHajek}(n,2,\\{1(X_{i}=X_{j}),(i,j)\\in S\\},\\epsilon,\\alpha,C,\\xi,S)$   \n5: if $\\begin{array}{r}{\\tilde{\\theta}\\ge\\frac{1+3\\delta^{2}/4}{m}}\\end{array}$ then   \n6: $\\mathrm{DEC}\\leftarrow1$ {Reject approximate uniformity}   \n7: else   \n8: $\\mathrm{DEC}\\leftarrow0$ {Accept approximate uniformity}   \n9: end if   \n10: return DEC ", "page_idx": 34}, {"type": "text", "text": "Proof of Theorem 5: Recall that $\\tilde{\\theta}$ denotes the private test statistic, which is thresholded at the value 1+3m\u03b42/4to determine the output of the hypothesis test. We claim that the validity of the test is established if we can show that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|{\\tilde{\\theta}}-\\mathbb{E}[U_{n}]|\\leq{\\frac{\\delta^{2}}{4m}}\\right)\\geq1-O(\\gamma)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "under both hypotheses. Indeed, it would then hold that: ", "page_idx": 34}, {"type": "text", "text": "(i) Under approximate uniformity, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\tilde{\\theta}<\\frac{1}{m}+\\frac{\\delta^{2}}{2m}+\\frac{\\delta^{2}}{4m}=\\frac{1+3\\delta^{2}/4}{m}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "(ii) Under the alternative hypothesis, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\tilde{\\theta}\\geq\\frac{1}{m}+\\frac{\\delta^{2}}{m}-\\frac{\\delta^{2}}{4m}=\\frac{1+3\\delta^{2}/4}{m}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "To establish inequality (A.60), we further write ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|\\tilde{\\theta}-\\mathbb{E}[U_{n}]|>\\frac{\\delta^{2}}{4m}\\right)\\leq\\mathbb{P}\\left(|\\tilde{\\theta}-U_{n}|>\\frac{\\delta^{2}}{8m}\\right)+\\mathbb{P}\\left(|U_{n}-\\mathbb{E}[U_{n}]|>\\frac{\\delta^{2}}{8m}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The second term can be controlled using an argument in Diakonikolas et al. [21], which further develops the variance bound in Lemma A.20 for the two hypothesis classes and then uses Chebyshev\u2019s inequality. It is shown that the second probability term in inequality (A.61) can be bounded by $\\alpha$ if $\\begin{array}{r}{n=\\Omega\\left(\\frac{\\sqrt{m}}{\\gamma\\delta^{2}}\\right)}\\end{array}$ . ", "page_idx": 35}, {"type": "text", "text": "To bound the first probability term in inequality (A.61), we study the concentration parameter $\\xi$ for the local H\u00e1jek projection $\\begin{array}{r}{\\hat{h}(i)=\\frac{1}{n-1}\\sum_{j\\neq i}h(X_{i},X_{j})}\\end{array}$ . We have the following result: ", "page_idx": 35}, {"type": "text", "text": "Lemma A.21. $\\begin{array}{r}{I\\!f\\xi=\\frac{6}{m}+\\frac{8\\log(4n/\\gamma)}{n}}\\end{array}$ and $\\begin{array}{r}{n\\geq\\frac{16}{\\gamma}}\\end{array}$ , then $|\\hat{h}(i)-U_{n}|\\leq\\xi$ for all $i$ , with probability at least 1 \u2212\u03b3. ", "page_idx": 35}, {"type": "text", "text": "Proof. By the triangle inequality, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n|\\hat{h}(i)-U_{n}|\\le|\\hat{h}(i)-\\mathbb{E}[h(X_{1},X_{2})|X_{1}]|+|\\mathbb{E}[h(X_{1},X_{2})|X_{1}]-\\theta|+|U_{n}-\\theta|.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We will provide a bound on each of these three terms. Note that ", "page_idx": 35}, {"type": "equation", "text": "$$\nh(X_{1},X_{2})\\vert X_{1}\\sim\\operatorname{Bern}(p_{X_{1}}),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which has variance ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sigma_{i}^{2}=\\operatorname{var}(h(X_{i},X_{j})|X_{i})=p_{X_{i}}(1-p_{X_{i}})\\leq{\\frac{2}{m}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Hence, with probability at least $\\textstyle1-{\\frac{\\gamma}{2}}$ , the first term in inequality (A.62) can be bounded as ", "page_idx": 35}, {"type": "equation", "text": "$$\n|\\hat{h}(i)-\\mathbb{E}[h(X_{1},X_{2})|X_{1}]|\\leq2\\sqrt{\\frac{2}{m n}\\log\\left(\\frac{4n}{\\gamma}\\right)}+\\frac{16}{3n}\\log\\left(\\frac{4n}{\\gamma}\\right)\\leq\\frac{2}{m}+\\frac{7}{n}\\log\\left(\\frac{4n}{\\gamma}\\right),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where we have used the AM-GM inequality. The second term in inequality (A.62) can be bounded as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left|{\\frac{1+a_{X_{i}}}{m}}-\\left({\\frac{1}{m}}+{\\frac{\\left\\|a\\right\\|^{2}}{m^{2}}}\\right)\\right|\\leq{\\frac{a_{X_{i}}}{m}}+{\\frac{\\left\\|a\\right\\|^{2}}{m^{2}}}\\leq{\\frac{2}{m}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Finally, by Chebyshev\u2019s inequality, the third term can be bounded as $\\begin{array}{r}{|U_{n}-\\theta|\\leq\\sqrt{\\frac{2\\mathrm{var}(U_{n})}{\\gamma}}}\\end{array}$ with probability at least $1-{\\frac{\\gamma}{2}}$ . It remains to find the variance of $U_{n}$ . ", "page_idx": 35}, {"type": "text", "text": "By Lemma A.20, if $\\lvert a_{i}\\rvert\\leq1$ for all $i$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{ar}(U_{n})=\\frac{2}{n(n-1)}\\left(2(n-2)\\sum_{i<j}\\frac{(1+a_{i})(1+a_{j})(a_{i}-a_{j})^{2}}{m^{4}}+\\sum_{i}\\frac{(1+a_{i})^{2}}{m^{2}}-\\left(\\sum_{i}\\frac{(1+a_{i})^{2}}{m^{2}}\\right.\\right.}\\\\ {\\displaystyle\\left.\\left.\\leq\\frac{2}{n(n-1)}\\left(2(n-2)\\sum_{i<j}\\frac{(1+a_{i})(1+a_{j})(a_{i}-a_{j})^{2}}{m^{4}}+\\sum_{i}\\frac{(1+a_{i})^{2}}{m^{2}}\\right)\\right.\\right.}\\\\ {\\displaystyle\\left.\\left.\\leq\\frac{2}{n(n-1)}\\left(2(n-2)\\frac{4}{m^{4}}\\binom{m}{2}+\\frac{4m}{m^{2}}\\right)\\leq\\frac{8}{m^{2}n}+\\frac{8}{m n^{2}}.\\right.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Combining the three bounds into inequality (A.62), with probability at least $1-\\gamma$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\hat{h}(i)-U_{n}|\\le\\bigg(\\frac{2}{m}+\\frac{7}{n}\\log\\bigg(\\frac{4n}{\\gamma}\\bigg)\\bigg)+\\frac{2}{m}+\\frac{\\sqrt{2}}{\\sqrt{\\gamma}}\\left(\\frac{2\\sqrt{2}}{m\\sqrt{n}}+\\frac{2\\sqrt{2}}{\\sqrt{m}n}\\right)}\\\\ &{\\qquad\\qquad=\\frac{4}{m}+\\frac{7\\log(4n/\\gamma)}{n}+\\frac{4/\\sqrt{\\gamma}}{m\\sqrt{n}}+\\frac{4/\\sqrt{\\gamma}}{\\sqrt{m n}}}\\\\ &{\\qquad\\qquad\\le\\frac{6}{m}+\\frac{8\\log(4n/\\gamma)}{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where in the second inequality, we used the AM-GM inequality and the assumption $\\begin{array}{r}{n\\geq\\frac{16}{\\gamma}}\\end{array}$ . The statement of the lemma follows after discarding lower-order terms. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "By Lemma A.21, with probability at least $1-\\gamma$ , the weights of all projections in Algorithm 1 are equal to 1 and $U_{n}=\\tilde{A}_{n}$ . Then $|\\tilde{\\theta}-U_{n}|$ is simply the magnitude of the noise added in the final step of Algorithm 1 (which uses a constant $\\gamma=0.01$ ), which (cf. the proof of Theorem 2) takes the form ", "page_idx": 36}, {"type": "equation", "text": "$$\nO\\left(\\frac{\\xi}{n\\epsilon}+\\frac{1}{n^{2}\\epsilon^{2}}+\\frac{1}{n^{3}\\epsilon^{3}}\\right)=O\\left(\\frac{\\log n}{n^{2}\\epsilon}+\\frac{1}{m n\\epsilon}+\\frac{1}{n^{2}\\epsilon^{2}}+\\frac{1}{n^{3}\\epsilon^{3}}\\right),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "with probability at least 0.75. This is bounded by $\\frac{\\delta^{2}}{8m}$ as long as ", "page_idx": 36}, {"type": "equation", "text": "$$\nn=\\Omega\\left(\\frac{m^{1/2}}{\\delta\\epsilon^{1/2}}\\log\\left(\\frac{m^{1/2}}{\\delta\\epsilon^{1/2}}\\right)+\\frac{m^{1/2}}{\\delta\\epsilon}+\\frac{m^{1/3}}{\\delta^{2/3}\\epsilon}+\\frac{1}{\\delta^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Wrapper 1 further boosts this constant probability of success. Now, an application of Lemma A.6 gives the stated result. ", "page_idx": 36}, {"type": "text", "text": "A.6.2 Sparse graph statistics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Algorithm A.5 PrivateNetworkEdge(n, m, {Aij}1\u2264i<j\u2264n, \u03f5)   \n1: $C\\gets1$ , $\\gamma\\leftarrow0.01$   \n2: $\\begin{array}{r}{U_{n}\\leftarrow\\frac{1}{\\binom{n}{2}}\\sum_{i<j}A_{i j}}\\end{array}$   \n3: $\\begin{array}{r}{\\nu^{2}\\leftarrow U_{n}+\\frac{1}{n\\epsilon}Z}\\end{array}$ , where $Z$ is a standard Laplace random variable   \n4: if $\\nu<0$ then   \n5: return $\\bot$   \n6: end if   \n7: $\\begin{array}{r l}&{\\overline{{\\xi\\leftarrow24\\nu}}\\sqrt{\\frac{1}{n}\\log\\left(\\frac{2n}{\\gamma}\\right)}+\\frac{16}{3n}\\log\\left(\\frac{2n}{\\gamma}\\right)+\\frac{15\\nu}{n}\\sqrt{\\frac{1}{\\gamma}}}\\\\ &{\\mathcal{S}\\leftarrow\\{(i,j):1\\leq i<j\\leq n\\}}\\\\ &{\\tilde{\\theta}\\leftarrow\\mathbf{PrivateMeanHajek}(n,2,\\{1(A_{i j}=1),(i,j)\\in\\mathcal{S}\\},\\epsilon,C,\\xi,S)}\\end{array}$   \n8:   \n9:   \n10: return $\\tilde{\\theta}$ ", "page_idx": 36}, {"type": "text", "text": "A.6.3 Proof of Theorem 6 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "The privacy of the algorithm follows by composing (see Lemma A.4) the $\\epsilon_{}$ -privacy of $\\nu$ and the $O(\\epsilon)$ -privacy of $\\tilde{\\theta}$ conditioned on $\\nu$ . It remains to show the utility of the algorithm. ", "page_idx": 36}, {"type": "text", "text": "The kernel $h(x,y)=1(\\|x-y\\|\\leq r_{n})$ is degenerate, since $P(\\|X_{i}-X_{j}\\|\\leq r_{n}|X_{i})$ does not depend on $X_{i}$ . So v $\\operatorname{\\mathrm{{r}}}(\\mathbb{E}[h(X_{i},X_{j})|X_{i}=x])=0$ . We have var $[h(X_{i},X_{j})]\\le\\pi r_{n}^{2}$ , so the non-private error is ${\\cal O}(r_{n}/n)$ $\\operatorname{(Eq}\\,3)$ ). Using Proposition 2.3 from Arcones and Gine [5], there exist universal constants $c_{1},c_{2}$ , and $c_{3}$ such that ", "page_idx": 36}, {"type": "equation", "text": "$$\nP\\left(\\left|\\frac{n-1}{2}(U_{n}-r_{n}^{2}/4)\\right|\\geq t\\right)\\leq c_{1}\\exp\\left(-\\frac{c_{2}t}{c_{3}r_{n}+(t/n)^{1/3}}\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Setting $t=n r_{n}^{2}/16$ , we have, for large enough $n$ , since $r_{n}=\\Omega(n^{-1/2})$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\nP\\left(\\left|U_{n}-r_{n}^{2}/4\\right|\\geq r_{n}^{2}/8\\right)\\leq c_{1}\\exp\\left(-\\frac{c_{2}n r_{n}^{2}}{c_{3}r_{n}+r_{n}^{2/3}}\\right)\\leq c_{1}\\exp(-c^{\\prime}n r_{n}^{4/3})=\\tilde{O}\\left(\\exp(-n^{1/3})\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, with probability $1-o(1)$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{n}\\in[r_{n}^{2}/8,3r_{n}^{2}/8]}\\\\ &{\\nu^{2}:=U_{n}+Z/n\\epsilon\\in[r_{n}^{2}/9,r_{n}^{2}/2].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "In particular, the probability that $\\begin{array}{r}{U_{n}+\\frac{Z}{n\\epsilon}}\\end{array}$ computed in step 3 of Algorithm A.5 is then positive. From Lemma 4, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left|\\hat{h}(i)-\\mathbb{E}\\left[h(X_{1},X_{2})|X_{1}=x_{1}\\right]\\right|\\leq4\\sigma_{i}\\sqrt{\\frac{1}{n}\\log\\left(\\frac{2n}{\\gamma}\\right)}+\\frac{16}{3n}\\log\\left(\\frac{2n}{\\gamma}\\right),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "with probability at least $1-\\gamma$ , where $\\sigma_{i}^{2}=\\operatorname{Var}\\big(h(X_{1},X_{2})|X_{1}=x_{1}\\big)\\leq\\pi r_{n}^{2}$ . Thus, from $\\mathrm{Eq}\\,\\mathrm{A}.64$ , we see that: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sigma_{i}^{2}\\leq\\pi r_{n}^{2}\\leq9\\pi\\nu^{2}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Moreover, since $g$ is degenerate, we have $\\mathbb{E}[h(X_{i},X_{j})|X_{i}]=\\theta_{n}$ . Using the fact that $|U_{n}-\\theta_{n}|\\leq$ 2Var\u03b3(Un)\u22645rnn \u03b31 with probability at least 1 \u2212O(\u03b3), we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{i}{\\operatorname*{max}}\\left|\\hat{h}(i)-U_{n}\\right|\\leq\\underset{i}{\\operatorname*{max}}\\left|\\hat{h}(i)-\\theta_{n}\\right|+\\left|\\theta_{n}-U_{n}\\right|}\\\\ &{\\quad\\quad\\quad\\quad=4\\sigma_{i}\\sqrt{\\frac{1}{n}\\log\\left(\\frac{2n}{\\gamma}\\right)}+\\frac{16}{3n}\\log\\left(\\frac{2n}{\\gamma}\\right)+\\frac{5r_{n}}{n}\\sqrt{\\frac{1}{\\gamma}}}\\\\ &{\\quad\\quad\\quad\\quad\\leq24\\nu\\sqrt{\\frac{1}{n}\\log\\left(\\frac{2n}{\\gamma}\\right)}+\\frac{16}{3n}\\log\\left(\\frac{2n}{\\gamma}\\right)+\\frac{15\\nu}{n}\\sqrt{\\frac{1}{\\gamma}}=:\\xi,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "with probability $1-O(\\gamma)$ . Hence, using Theorem 2, and noting that $\\begin{array}{r}{\\xi=\\tilde{O}\\left(\\frac{r_{n}}{\\sqrt{n}}\\right)}\\end{array}$ , we can ensure that the estimate $\\tilde{\\theta}$ output by Algorithm 1 satisfies ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|{\\tilde{\\theta}-\\theta}\\right|=O\\left(\\sqrt{\\mathrm{Var}(U_{n_{\\alpha}})}+\\frac{k\\xi}{n_{\\alpha}\\epsilon}+\\left(\\frac{k^{2}}{n_{\\alpha}^{2}\\epsilon^{2}}+\\frac{k^{3}}{n_{\\alpha}^{3}\\epsilon^{3}}\\right)C\\right).}\\\\ &{\\qquad\\quad=O\\left(\\frac{r_{n}}{n_{\\alpha}}+\\frac{\\xi}{n_{\\alpha}\\epsilon}+\\frac{1}{n_{\\alpha}^{2}\\epsilon^{2}}+\\frac{1}{n_{\\alpha}^{3}\\epsilon^{3}}\\right)={\\tilde{O}}\\left(\\frac{r_{n}}{n_{\\alpha}}+\\frac{r_{n}}{n_{\\alpha}^{3/2}\\epsilon}+\\frac{1}{n_{\\alpha}^{2}\\epsilon}+\\frac{1}{n_{\\alpha}^{2}\\epsilon^{2}}+\\frac{1}{n_{\\alpha}^{3}\\epsilon^{3}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "with probability at least 0.75. Wrapper 1 further boosts this probability of success from constant to $1-\\alpha$ . Now, an application of Lemma A.6 gives the stated result. ", "page_idx": 37}, {"type": "text", "text": "A.6.4 Other applications ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In this section, we provide more applications of U statistics for important hypothesis testing problems. ", "page_idx": 37}, {"type": "text", "text": "1. Goodness-of-fit testing: The Cramer-Von Mises statistic for testing the hypothesis that the cumulative distribution function of a random variable is equal to a function $F_{0}$ is given by ", "page_idx": 37}, {"type": "equation", "text": "$$\n{\\frac{1}{n}}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\int\\left(1\\{X_{i}\\leq x\\}-F_{0}(x)\\right)\\left(1\\{X_{j}\\leq x\\}-F_{0}(x)\\right)d F_{0}(x).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Under the null $H_{0}:X\\sim F_{0}$ , the distribution of the statistic is degenerate [58]. Thus, our techniques from Section 4.2 provide a method for private goodness-of-fti testing based on the Cramer-Von Mises statistic. Private goodness-of-fit testing has so far mostly been studied in the setting of discrete data [28, 1, 2]. For continuous distributions, we are only aware of work that analyzes the local DP framework [23, 44, 12], which is therefore not directly comparable to our proposed approach. ", "page_idx": 37}, {"type": "text", "text": "2. Pearson\u2019s chi-squared test: The chi-squared goodness of fti test is widely used to test if a discrete random variable comes from a given distribution. The corresponding statistic (which can be written as a U statistic plus a smaller order term) is degenerate [20]. ", "page_idx": 37}, {"type": "text", "text": "3. Symmetry testing: Testing the symmetry of the underlying distribution of i.i.d. $X_{1},\\ldots,X_{n}$ is often used in paired tests. [26] use the test statistic $\\textstyle\\sum_{i,j}({\\bar{g}}(X_{i}-X_{j})-g(X_{i}+X_{j}))/n^{2}$ (which is a U statistic plus a lower-order term), where $g$ is the characteristic function of some distribution symmetric around 0. When the distribution of $X_{i}$ is symmetric, this is degenerate. ", "page_idx": 37}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 38}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 38}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 38}, {"type": "text", "text": "\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 38}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 38}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 38}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We have assured that the abstract and introduction accurately reflect our contributions. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 38}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We have acknowledged any limitations of our work in the paper and in the discussion section. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 39}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: We have provided proofs for all theoretical results in the paper. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 39}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 40}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 41}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 41}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 41}, {"type": "text", "text": "9. Code Of Ethics ", "page_idx": 41}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: Yes, we followed the NeurIPS Code of Ethics. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 42}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: This paper presents theoretical work whose goal is to advance the field of differential privacy and statistics. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 42}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 43}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 43}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA]   \nJustification:   \nGuidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 44}]