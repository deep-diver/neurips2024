[{"figure_path": "ZwiG9KjfHV/figures/figures_1_1.jpg", "caption": "Figure 1: The perplexity (lower scores mean better performance) of existing widely-used low-bit quantization methods on LLaMA-7B, reported on Wikitext2 [23]. All the examined previous approaches suffer from significant performance degradation when quantizing models to 2-bit values. Our 1-bit quantization method can outperform these 2-bit baselines.", "description": "This figure compares the perplexity achieved by several existing low-bit quantization methods (GPTQ, LLM-QAT, OmniQuant) against the proposed OneBit method on the LLaMA-7B model.  The x-axis represents the number of bits used for weight quantization, and the y-axis shows the resulting perplexity on the Wikitext2 benchmark. The graph highlights that the performance of existing methods significantly degrades as the bit-width decreases below 4 bits. In contrast, the OneBit method demonstrates superior performance, even when using only 1 bit for weight quantization.", "section": "1 Introduction"}, {"figure_path": "ZwiG9KjfHV/figures/figures_3_1.jpg", "caption": "Figure 2: The main idea of our method OneBit. The left is the original FP16 Linear Layer, in which both the activation X and the weight matrix W are in FP16 format. The right is our proposed architecture. Only value vectors g and h are in FP16 format, and the weight matrix consists of \u00b11 instead, which can be represented in INT1.", "description": "This figure illustrates the core idea behind the OneBit model's 1-bit linear layer.  The left panel shows a standard FP16 linear layer, where both the input activation (X) and the weight matrix (W) are in FP16 format. The right panel depicts the proposed OneBit architecture. Here, the weight matrix is quantized to 1-bit (represented as \u00b11), significantly reducing the memory footprint.  To compensate for the information loss due to this extreme quantization, two value vectors (g and h), which maintain FP16 precision, are introduced. These vectors help preserve the necessary floating-point precision during the linear transformation while keeping the weights themselves extremely low-bit.", "section": "3.2 1-bit Linear Layer Architecture"}, {"figure_path": "ZwiG9KjfHV/figures/figures_7_1.jpg", "caption": "Figure 3: Comparison of model capabilities and compressive degree.", "description": "This figure compares four different language models in terms of their performance on common sense reasoning tasks and general world knowledge tasks. The models compared are Pythia-1.0B, TinyLLaMA-1.1B, LowRank LLaMA, and OneBit-7B.  The figure shows that OneBit-7B, despite having a much smaller memory footprint and lower average bit-width, performs comparably to the other models on many tasks, particularly commonsense reasoning.  This demonstrates the effectiveness of the OneBit quantization technique in compressing large language models while maintaining their performance.", "section": "4 Experiments"}, {"figure_path": "ZwiG9KjfHV/figures/figures_8_1.jpg", "caption": "Figure 4: Tradeoff between size and PPL.", "description": "This figure shows the trade-off between model size and perplexity (PPL) for different models. The x-axis represents the model size in GB, and the y-axis represents the perplexity on the Wiki2 dataset.  The blue line represents the baseline performance using FP16 precision, while the orange line shows the performance of the OneBit method using 1-bit weights and 16-bit activations (W1A16).  The figure highlights that OneBit achieves comparable performance to the FP16 baseline with significantly reduced model sizes. For example, it achieves similar perplexity to OPT-2.7B at only 0.22x the model size.  Conversely, it shows improved perplexity over the baseline at the same model size for OPT-1.3B and OPT-2.7B.", "section": "5 Analysis and Discussion"}, {"figure_path": "ZwiG9KjfHV/figures/figures_9_1.jpg", "caption": "Figure 5: Training process of OneBit-7B.", "description": "This figure shows the training loss curves for different methods of initializing the 1-bit weight matrix in the OneBit-7B model.  Three methods are compared: Singular Value Decomposition (SVD), Non-negative Matrix Factorization (NMF), and simply copying from the original weight matrix. The plot shows that NMF converges faster and reaches a lower training loss compared to SVD and copying from the original weights. This illustrates the effectiveness of the SVID-based initialization method proposed in the paper.", "section": "3.3 Sign-Value-Independent Decomposition"}, {"figure_path": "ZwiG9KjfHV/figures/figures_17_1.jpg", "caption": "Figure 3: Comparison of model capabilities and compressive degree.", "description": "This figure compares four different models (Pythia-1.0B, TinyLLaMA-1.1B, LowRank LLAMA, and OneBit-7B) across various metrics.  Subfigure (a) shows performance on common sense reasoning tasks. Subfigure (b) shows performance on general world knowledge tasks (MMLU). Subfigure (c) provides a comparison of memory footprint and the average bit-width used in each model.  The figure demonstrates that the OneBit model, despite having a significantly smaller memory footprint and lower bit-width, achieves comparable performance to other models, particularly in common sense reasoning.", "section": "4 Experiments"}]