[{"figure_path": "ZwiG9KjfHV/tables/tables_5_1.jpg", "caption": "Table 1: Training details of knowledge distillation.", "description": "This table presents the hyperparameters used for training the models in the knowledge distillation experiments.  Specifically, it lists the learning rate, alpha (\u03b1) value which balances the loss functions, and the number of GPUs utilized for each model size (OPT-1.3B, OPT-2.7B, LLaMA-7B, LLaMA-13B, LLaMA2-7B, LLaMA2-13B).", "section": "4.1 Settings"}, {"figure_path": "ZwiG9KjfHV/tables/tables_6_1.jpg", "caption": "Table 2: Main results of evaluation experiment. We report the perplexity and zero-shot accuracy. \"FP16\" is the transformer with FP16 parameters and we refer to it as the upper-bound of all the methods. The best score is bolded.", "description": "This table presents the main experimental results, comparing the performance of OneBit against other state-of-the-art quantization methods across multiple LLMs (OPT and LLaMA).  It shows perplexity scores (lower is better) and zero-shot accuracy (higher is better) on various benchmark datasets (Wiki2, C4, Winograd Schema Challenge, HellaSwag, Physical Interaction QA, BoolQ, ARC-e, ARC-c).  The FP16 row serves as the upper bound performance representing the full-precision model. The table highlights the superior performance of OneBit, especially noticeable in larger models.", "section": "4.2 Main Results"}, {"figure_path": "ZwiG9KjfHV/tables/tables_8_1.jpg", "caption": "Table 3: Compression ratio of LLaMA models.", "description": "This table shows the compression ratios achieved by the OneBit method on various sizes of LLaMA models.  It compares the memory footprint (in gigabytes) of the original FP16 models to the compressed OneBit models (using 1-bit weights). The ratio column shows the percentage reduction in memory usage due to the OneBit compression technique.", "section": "4.1 Settings"}, {"figure_path": "ZwiG9KjfHV/tables/tables_15_1.jpg", "caption": "Table 2: Main results of evaluation experiment. We report the perplexity and zero-shot accuracy. \"FP16\" is the transformer with FP16 parameters and we refer to it as the upper-bound of all the methods. The best score is bolded.", "description": "This table presents the main experimental results, comparing the performance of the proposed OneBit method against several baselines (GPTQ, LLM-QAT, OmniQuant) and the full-precision FP16 model.  It shows perplexity scores (lower is better) and zero-shot accuracy (higher is better) across various tasks (Wiki2, C4, Winograd Schema Challenge, HellaSwag, PIQA, BoolQ, ARC-e, ARC-c) for different model sizes (OPT-1.3B, OPT-2.7B, LLaMA-7B, LLaMA-13B). The FP16 column represents the upper bound performance, showcasing the effectiveness of OneBit in achieving high performance with extremely low-bit quantization.", "section": "4.2 Main Results"}, {"figure_path": "ZwiG9KjfHV/tables/tables_16_1.jpg", "caption": "Table 2: Main results of evaluation experiment. We report the perplexity and zero-shot accuracy. \"FP16\" is the transformer with FP16 parameters and we refer to it as the upper-bound of all the methods. The best score is bolded.", "description": "This table presents the main results of the evaluation experiments.  It compares the performance of the proposed OneBit method against several baselines (GPTQ, LLM-QAT, OmniQuant) across different model sizes (OPT-1.3B/2.7B, LLaMA-7B/13B).  The metrics reported include perplexity (lower is better) on the WikiText2 and C4 datasets, and zero-shot accuracy on various downstream tasks (Winograd, HellaSwag, PIQA, BoolQ, ARC-e, ARC-c). The FP16 results serve as an upper bound for comparison.", "section": "4.2 Main Results"}, {"figure_path": "ZwiG9KjfHV/tables/tables_18_1.jpg", "caption": "Table 6: Ablation study of different loss on LLaMA-7B. \u201cATTN\u201d means attention score alignment.", "description": "This table presents the results of an ablation study conducted on the LLaMA-7B model to evaluate the effectiveness of different loss functions used in the knowledge distillation process.  The study compares the performance of using only the knowledge distillation loss (LKD) against versions that incorporate mean squared error loss (LMSE) with different weighting parameters (\u03b1) and attention score alignment loss (LATTN).  The results are evaluated using perplexity and zero-shot accuracy across several benchmarks (Wiki2, C4, Winograd Schema Challenge, HellaSwag, Physical Interaction QA, BoolQ, ARC-e, and ARC-c). The table shows that combining LKD with LMSE leads to improved results, particularly when \u03b1 = 1.", "section": "A.6 Discussion on Knowledge Distillation"}]