[{"type": "text", "text": "Deep Discriminative to Kernel Density Graph for Inand Out-of-distribution Calibrated Inference ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Deep discriminative approaches like random forests and deep neural networks have   \n2 recently found applications in many important real-world scenarios. However, de  \n3 ploying these learning algorithms in safety-critical applications raises concerns, par  \n4 ticularly when it comes to ensuring confidence calibration for both in-distribution   \n5 and out-of-distribution data points. Many popular methods for in-distribution (ID)   \n6 calibration, such as isotonic and Platt\u2019s sigmoidal regression, exhibit excellent ID   \n7 calibration performance. However, these methods are not calibrated for the entire   \n8 feature space, leading to overconfidence in the case of out-of-distribution (OOD)   \n9 samples. On the other end of the spectrum, existing out-of-distribution (OOD)   \n10 calibration methods generally exhibit poor in-distribution (ID) calibration. In this   \n11 paper, we address ID and OOD calibration problems jointly. We leveraged the   \n12 fact that deep models, including both random forests and deep-nets, learn internal   \n13 representations which are unions of polytopes with affine activation functions to   \n14 conceptualize them both as partitioning rules of the feature space. We replace the   \n15 affine function in each polytope populated by the training data with a Gaussian   \n16 kernel. Our experiments on both tabular and vision benchmarks show that the   \n17 proposed approaches obtain well-calibrated posteriors while mostly preserving or   \n18 improving the classification accuracy of the original algorithm for ID region, and   \n19 extrapolate beyond the training data to handle OOD inputs appropriately. ", "page_idx": 0}, {"type": "text", "text": "20 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "21 Machine learning methods, specially deep neural networks and random forests have shown excellent   \n22 performance in many real-world tasks, including drug discovery, autonomous driving and clinical   \n23 surgery [1\u20133]. However, calibrating confidence over the whole feature space for these approaches   \n24 remains a key challenge in the field [4]. Calibrated confidence within the training or in-distribution   \n25 (ID) region as well as in the out-of-distribution (OOD) region is crucial for safety critical applications   \n26 like autonomous driving and computer-assisted surgery, where any aberrant reading should be   \n27 detected and taken care of immediately [4, 5].   \n28 The approaches to calibrate OOD confidence for learning algorithms described in the literature can   \n29 be roughly divided into two groups: discriminative and generative. Intuitively, the easiest solution for   \n30 OOD confidence calibration is to learn a function that gives higher scores for in-distribution samples   \n31 and lower scores for OOD samples [6]. The discriminative approaches try to either modify the loss   \n32 function [7\u20139] or train the network exhaustively on OOD datasets to calibrate on OOD samples [10, 4].   \n33 Recently, Hein et al. [4] showed ReLU networks produce arbitrarily high confidence as the inference   \n34 point moves far away from the training data. Therefore, calibrating ReLU networks for the whole   \n35 OOD region is not possible without fundamentally changing the network architecture. As a result, all   \n36 of the aforementioned algorithms are unable to provide any guarantee about the performance of the   \n37 network throughout the whole feature space. The other group tries to learn generative models for the   \n38 in-distribution as well as the out-of-distribution samples. The general idea is to do likelihood ratio   \n39 test for a particular sample using the generative models [11], or threshold the ID likelihoods to detect   \n40 OOD samples. However, it is not obvious how to control likelihoods far away from the training data   \n41 for powerful generative models like variational autoencoders (VAEs) [12] and generative adversarial   \n42 networks (GAN) [13]. Moreover, Nalisnick et al. [14] and Hendrycks et al. [10] showed VAEs and   \n43 GANs can also yield overconfident likelihoods far away from the training data.   \n44 The algorithms described so far are concerned with OOD confidence calibration for deep-nets only.   \n45 However, we show that other approaches which partition the feature space, for example random forest,   \n46 can also suffer from poor confidence calibration both in the ID and the OOD regions. Moreover, the   \n47 algorithms described above are concerned about the confidence in the OOD region only and do not   \n48 address the confidence calibration within the ID region at all. This issue is addressed separately in   \n49 a different group of literature [15\u201320]. Instead, we consider both calibration problems jointly and   \n50 propose an approach that achieves good calibration throughout the whole feature space.   \n51 In this paper, we conceptualize both random forest and ReLU networks as partitioning rules with an   \n52 affine activation over each polytope. We consider replacing the affine functions learned over the   \n53 polytopes with Gaussian kernels. We propose two novel kernel density estimation techniques named   \n54 Kernel Density Forest (KDF) and Kernel Density Network (KDN). Our proposed approach completely   \n55 excludes the need for training on OOD examples for the model (unsupervised OOD calibration). We   \n56 conduct several simulation and real data studies that show both KDF and KDN are well-calibrated for   \n57 OOD samples while they maintain good performance in the ID region. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "58 2 Related Works and Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "59 There are a number of approaches in the literature which attempt to learn a generative model and   \n60 control the likelihoods far away from the training data. For example, Ren et al. [11] employed   \n61 likelihood ratio test for detecting OOD samples. Wan et al. [8] modified the training loss so that the   \n62 downstream projected features follow a Gaussian distribution. However, there is no guarantee of   \n63 performance for OOD detection for the above methods. To the best of our knowledge, apart from   \n64 us, only Meinke et al. [5] has proposed an approach to guarantee asymptotic performance for OOD   \n65 detection. Compared to the aforementioned methods, our approach differs in several ways:   \n66 \u2022 We address the confidence calibration problem for both ReLU-nets and random forests.   \n67 \u2022 We address ID and OOD calibration problem as a continuum.   \n68 \u2022 We provide an algorithm for OOD confidence calibration for both tabular and vision datatsets   \n69 whereas most of the existing methods are tailor-made for vision problems.   \n70 \u2022 We propose an unsupervised post-hoc OOD calibration approach. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "71 3 Technical Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "72 3.1 Setting ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "73 Consider a supervised learning problem with independent and identically distributed training samples   \n74 $\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{n}$ such that $({\\bf X},Y)\\sim P_{X,Y}$ , where $\\mathbf{X}\\sim P_{X}$ is a $\\mathcal{X}\\subseteq\\mathbb{R}^{D}$ valued input and $Y\\sim P_{Y}$ is a   \n75 $\\mathcal{V}=\\{1,\\cdots\\,,K\\}$ valued class label. Let $\\boldsymbol{S}$ be the high density region of the marginal, $P_{X}$ , thus $s\\subsetneq$   \n76 $\\mathcal{X}$ . Here the goal is to learn a confidence score, $\\mathbf{g}:\\mathbb{R}^{\\breve{D}}\\to[0,\\breve{1}]^{K}$ , $\\mathbf{\\bar{g}}(\\mathbf{x})=[g_{1}(\\mathbf{x}),g_{2}(\\mathbf{x}),\\dots,g_{K}(\\mathbf{x})]$   \n77 such that, ", "page_idx": 1}, {"type": "equation", "text": "$$\ng_{y}(\\mathbf x)=\\left\\{\\!\\!\\begin{array}{l l}{P_{Y|X}(y|\\mathbf x),}&{\\mathrm{~if~}\\mathbf x\\in S}\\\\ {P_{Y}(y),}&{\\mathrm{~if~}\\mathbf x\\notin S}\\end{array}\\!\\!\\right.\\quad\\forall y\\in\\mathcal{Y}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "78 where $P_{Y|X}(y|\\mathbf{x})$ is the posterior probability for class $y$ given by the Bayes formula: ", "page_idx": 1}, {"type": "equation", "text": "$$\nP_{Y|X}(y|\\mathbf x)=\\frac{P_{X|Y}(\\mathbf x|y)P_{Y}(y)}{\\sum_{k=1}^{K}P_{X|Y}(\\mathbf x|k)P_{Y}(k)},\\quad\\forall y\\in\\mathcal{Y}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "79 Here $P_{X|Y}(\\mathbf{x}|y)$ is the class conditional density which we will refer as $f_{y}(\\mathbf{x})$ hereafter for brevity. ", "page_idx": 1}, {"type": "text", "text": "80 3.2 Main Idea ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "81 Deep discriminative networks partition the feature space $\\mathbb{R}^{d}$ into a union of $p$ affine polytopes $Q_{r}$   \n82 such that $\\textstyle\\bigcup_{r=1}^{p}Q_{r}=\\mathbb{R}^{d}$ , and learn an affine function over each polytope [4, 21]. Mathematically,   \n83 the unnor malized class-conditional density for the label $y$ estimated by these deep discriminative   \n84 models at a particular point $\\mathbf{x}$ can be expressed as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{f}_{y}(\\mathbf x)=\\sum_{r=1}^{p}(\\mathbf a_{r}^{\\top}\\mathbf x+b_{r})\\mathbb{1}(\\mathbf x\\in Q_{r}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "85 For example, in the case of a decision tree, $\\mathbf{a}_{r}=\\mathbf{0}$ , i.e., decision tree assumes uniform distribution   \n86 for the class-conditional densities over the leaf nodes. Among these polytopes, the ones that lie on   \n87 the boundary of the training data extend to the whole feature space and hence encompass all the OOD   \n88 samples. Since the posterior probability for a class is determined by the affine activation over each of   \n89 these polytopes, the algorithms tend to be overconfident when making predictions on the OOD inputs.   \n90 Moreover, there exist some polytopes that are not populated with training data. These unpopulated   \n91 polytopes serve to interpolate between the training sample points. If we replace the affine activation   \n92 function of the populated polytopes with Gaussian kernels and prune the unpopulated ones, the tail of   \n93 the kernel will help interpolate between the training sample points while assigning lower likelihood to   \n94 the low density or unpopulated polytope regions of the feature space. This results in better confidence   \n95 calibration for the proposed modified approach. ", "page_idx": 2}, {"type": "text", "text": "96 3.3 Proposed Approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "97 We will call the above discriminative approaches as the \u2018parent approach\u2019 hereafter. Consider the   \n98 collection of polytope indices $\\mathcal{P}$ from the parent approach which are populated by the training data.   \n99 We replace the affine functions over the populated polytopes with Gaussian kernels $\\mathcal{G}(\\cdot;\\hat{\\mu}_{r},\\hat{\\Sigma}_{r})$ . For   \n100 a particular inference point $\\mathbf{x}$ , we consider the Gaussian kernel with the minimum distance from the   \n101 center of the kernel to the corresponding point: ", "page_idx": 2}, {"type": "equation", "text": "$$\nr_{\\mathbf{x}}^{*}=\\underset{r}{\\mathrm{argmin}}\\:\\|\\mu_{r}-\\mathbf{x}\\|,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "102 where $\\|\\cdot\\|$ denotes a distance. As we will show later, the type of distance metric considered in   \n103 Equation 4 highly impacts the performance of the proposed model. In short, we modify Equation 3   \n104 from the parent ReLU-net or random forest to estimate the class-conditional density (unnormalized): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\tilde{f}_{y}(\\mathbf{x})=\\frac{1}{n_{y}}\\sum_{r\\in\\mathcal{P}}n_{r y}\\mathcal{G}(\\mathbf{x};\\mu_{r},\\Sigma_{r})\\mathbb{1}(r=r_{\\mathbf{x}}^{*}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "105 where $n_{y}$ is the total number of samples with label $y$ and $n_{r y}$ is the number of samples from class $y$   \n106 that end up in polytope $Q_{r}$ . We add a small constant to the class conditional density $\\tilde{f}_{y}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\hat{f}}_{y}(\\mathbf{x})={\\tilde{f}}_{y}(\\mathbf{x})+{\\frac{b}{\\log(n)}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "107 Note that in Equation 6,logb(n) $\\frac{b}{\\log(n)}\\to0$ as the total training points, $n\\to\\infty$ . The intuition behind the   \n108 added constant will be clarified further later in Proposition 2. The confidence score $\\hat{g}_{y}(\\mathbf x)$ for class $y$   \n109 given a test point $\\mathbf{x}$ is estimated using the Bayes rule as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{g}_{y}(\\mathbf x)=\\frac{\\hat{f}_{y}(\\mathbf x)\\hat{P}_{Y}(y)}{\\sum_{k=1}^{K}\\hat{f}_{k}(\\mathbf x)\\hat{P}_{Y}(k)},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "110 where $\\hat{P}_{Y}(y)$ is the empirical prior probability of class $y$ estimated from the training data. We   \n111 estimate the class for a particular inference point $\\mathbf{x}$ as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{y}=\\underset{y\\in\\mathcal{Y}}{\\mathrm{argmax}}\\,\\hat{g}_{y}(\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "112 4 Model Parameter Estimation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "113 4.1 Gaussian Kernel Parameter Estimation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "114 We fti Gaussian kernel parameters to the samples that end up in the $r$ -th polytope. We set the kernel   \n115 center along the $d$ -th dimension: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{r}^{d}=\\frac{1}{n_{r}}\\sum_{i=1}^{n}x_{i}^{d}\\mathbb{1}(\\mathbf x_{i}\\in Q_{r}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "116 where $\\boldsymbol{x}_{i}^{d}$ is the value of $\\mathbf{x}_{i}$ along the $d$ -th dimension. We set the kernel variance along the $d$ -th   \n117 dimension: ", "page_idx": 3}, {"type": "equation", "text": "$$\n(\\hat{\\sigma}_{r}^{d})^{2}=\\frac{1}{n_{r}}\\{\\sum_{i=1}^{n}\\mathbb{1}(\\mathbf{x}_{i}\\in Q_{r})(x_{i}^{d}-\\hat{\\mu}_{r}^{d})^{2}+\\lambda\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "118 where $\\lambda$ is a small constant that prevents ${\\hat{\\sigma}}_{r}^{d}$ from being 0. We constrain our estimated Gaussian   \n119 kernels to have diagonal covariance. ", "page_idx": 3}, {"type": "text", "text": "120 4.2 Sample Size Ratio Estimation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "121 For a high dimensional dataset with low training sample size, the polytopes are sparsely populated   \n122 with training samples. For improving the estimate of the ratio $\\frac{n_{r y}}{n_{y}}$ in Equation 5, we incorporate the   \n123 samples from other polytopes $Q_{s}$ based on the similarity $w_{r s}$ between $Q_{r}$ and $Q_{s}$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\hat{n}_{r y}}{\\hat{n}_{y}}=\\frac{\\sum_{s\\in\\mathcal{P}}\\sum_{i=1}^{n}w_{r s}\\mathbb{1}(\\mathbf{x}_{i}\\in Q_{s})\\mathbb{1}(y_{i}=y)}{\\sum_{r\\in\\mathcal{P}}\\sum_{s\\in\\mathcal{P}}\\sum_{i=1}^{n}w_{r s}\\mathbb{1}(\\mathbf{x}_{i}\\in Q_{s})\\mathbb{1}(y_{i}=y)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "124 As $n\\to\\infty$ , the estimated weights $w_{r s}$ should satisfy the condition: ", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{r s}\\to\\left\\{0,\\ \\ \\mathrm{if}\\ Q_{r}\\neq Q_{s}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "125 For simplicity, we will describe the estimation procedure for $w_{r s}$ in the next sections. Note that if we   \n126 satisfy Condition 12, then we have n\u02c6n\u02c6ryy $\\begin{array}{r}{\\frac{\\hat{n}_{r y}}{\\hat{n}_{y}}\\to\\frac{n_{r y}}{n_{y}}}\\end{array}$ as $n\\to\\infty$ . Therefore, we modify Equation 5 as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{f}_{y}(\\mathbf{x})=\\frac{1}{\\hat{n}_{y}}\\sum_{r\\in\\mathcal{P}}\\hat{n}_{r y}\\mathcal{G}(\\mathbf{x};\\hat{\\mu}_{r},\\hat{\\Sigma}_{r})\\mathbb{1}(r=\\hat{r}_{\\mathbf{x}}^{*}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "127 where $\\hat{r}_{\\mathbf{x}}^{*}=\\mathrm{argmin}_{r}\\left\\lVert\\hat{\\mu}_{r}-\\mathbf{x}\\right\\rVert$ . Now we use $\\hat{f}_{y}({\\bf x})$ estimated using (13) in Equation (6), (7) and (8),   \n128 respectively. Below, we describe how we estimate $w_{r s}$ for KDF and KDN . ", "page_idx": 3}, {"type": "text", "text": "129 4.3 Forest Kernel ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "130 Consider $T$ number of decision trees in a random forest trained on $n\\ i i d$ training samples   \n131 $\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{n}$ . Each tree $t$ partitions the feature space into $p_{t}$ polytopes resulting in a set of polytopes:   \n132 $\\{\\{Q_{t,r}\\}_{r=1}^{p_{t}}\\}_{t=1}^{T}$ . The intersection of thes e polytopes gives a new set of polytopes $\\{Q_{r}\\}_{r=1}^{p}$ for the   \n133 forest. For any two points $\\mathbf{x}\\in Q_{r}$ and $\\mathbf{x}^{\\prime}\\in Q_{s}$ , we define the kernel $\\boldsymbol{\\kappa}(\\boldsymbol{r},s)$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\mathcal{K}(r,s)=\\frac{t_{r s}}{T},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "134 where $t_{r s}$ is the total number of trees, $\\mathbf{x}$ and $\\mathbf{x}^{\\prime}$ end up in the same leaf node. Here, $0\\leq K(r,s)\\leq1$ ", "page_idx": 3}, {"type": "text", "text": "135 If the two samples end up in the same leaf in all the trees, i.e., $\\textstyle K(r,s)=1$ , they belong to the same   \n136 polytope, i.e. $r=s$ . In short, $\\boldsymbol{\\kappa}(\\boldsymbol{r},s)$ is the fraction of total trees where the two samples follow the   \n137 same path from the root to a leaf node. We exponentiate $\\boldsymbol{\\kappa}(r,s)$ so that Condition 12 is satisfied: ", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{r s}=\\mathcal{K}(r,s)^{k\\log n}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "138 We choose $k$ using grid search on a hold-out dataset. ", "page_idx": 3}, {"type": "text", "text": "139 4.4 Network Kernel ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "140 Consider a fully connected $L$ layer ReLU-net trained on $n$ iid training samples $\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{n}$ . We   \n141 have the set of all nodes denoted by $\\mathcal{N}_{l}$ at a particular layer $l$ . We can randomly pick a node $n_{l}\\in\\mathcal{N}_{l}$   \n142 at each layer $l$ , and construct a sequence of nodes starting at the input layer and ending at the output   \n143 layer which we call an activation path: $m=\\{n_{l}\\in\\bar{\\mathcal{N}}_{l}\\}_{l=1}^{L}$ . Note that there are $\\bar{N}=\\Pi_{i=1}^{L}|\\bar{\\mathcal{N}}_{l}|$   \n144 possible activation paths for a sample in the ReLU-net. We index each path by a unique identifier   \n145 number $z\\in\\mathbb{N}$ and construct a sequence of activation paths as: $\\mathcal{M}=\\{\\bar{m_{z}}\\}_{z=1,\\cdots,N}$ . Therefore, $\\mathcal{M}$   \n146 contains all possible activation pathways from the input to the output of the network.   \n147 While pushing a training sample $\\mathbf{x}_{i}$ through the network, we define the activation from a ReLU unit   \n148 at any node as $\\mathbf{\\dot{\\rho}}_{1},$ when it has positive output and $\\mathrm{\\ddot{0}}$ otherwise. Therefore, the activation indicates   \n149 on which side of the affine function at each node the sample falls. The activation for all nodes in an   \n150 activation path $m_{z}$ for a particular sample creates an activation mode $a_{z}\\in\\{0,1\\}^{L}$ . If we evaluate   \n151 the activation mode for all activation paths in $\\mathcal{M}$ while pushing a sample through the network, we   \n152 get a sequence of activation modes: $\\bar{\\mathcal{A}}_{r}=\\{a_{z}^{r}\\}_{z=1}^{N}$ . Here $r$ is the index of the polytope where the   \n153 sample falls in.   \n154 If the two sequences of activation modes for two different training samples are identical, they belong   \n155 to the same polytope. In other words, if $\\mathcal{A}_{r}=\\mathcal{A}_{s}$ , then $Q_{r}=Q_{s}$ . This statement holds because the   \n156 above samples will lie on the same side of the affine function at each node in different layers of the   \n157 network. Now, we define the kernel $\\boldsymbol{\\kappa}(\\boldsymbol{r},s)$ as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{K}(r,s)=\\frac{\\sum_{z=1}^{N}\\mathbb{1}(a_{z}^{r}=a_{z}^{s})}{N}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "158 Note that $0\\,\\leq\\,K(r,s)\\,\\leq\\,1$ . In short, $\\boldsymbol{\\kappa}(\\boldsymbol{r},s)$ is the fraction of total activation paths which are   \n159 identically activated for two samples in two different polytopes $r$ and $s$ . We exponentiate the kernel   \n160 using Equation 15. Pseudocodes outlining the two algorithms are provided in Appendix D. ", "page_idx": 4}, {"type": "text", "text": "161 4.5 Geodesic Distance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "162 Consider $\\mathcal{P}_{n}=\\{Q_{1},Q_{2},\\cdot\\cdot\\cdot,Q_{p}\\}$ as a partition of $\\mathbb{R}^{d}$ given by a random forest or a ReLU-net after   \n163 being trained on $n$ training samples. We measure distance between two points $\\mathbf{x}\\in Q_{r},\\mathbf{x}^{\\prime}\\in Q_{s}$   \n164 using the kernel introduced in Equation 14 and Equation 16, and call it \u2018Geodesic\u2019 distance [22]: ", "page_idx": 4}, {"type": "equation", "text": "$$\nd(r,s)=-K(r,s)+\\frac{1}{2}(K(r,r)+K(s,s))=1-K(r,s).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "165 Proposition 1. $(\\mathcal{P}_{n},d)$ is a metric space. ", "page_idx": 4}, {"type": "text", "text": "166 Proof. See Appendix A.1 for the proof. ", "page_idx": 4}, {"type": "text", "text": "167 We use Geodesic distance to find the nearest polytope to the inference point. As Geodesic distance   \n168 cannot distinguish between points within the same polytope, it has a resolution similar to the size of   \n169 the polytope. For discriminating between two points within the same polytope, we fit a Gaussian   \n170 kernel within the polytope (described above). As $h_{n}\\,\\rightarrow\\,0$ , the resolution for Geodesic distance   \n171 improves. In Section 5, we will empirically show that using Geodesic distance scales better with   \n172 higher dimension compared to that of Euclidean distance.   \n173 Given $n$ training samples $\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{n}$ , we define the distance of an inference point $\\mathbf{x}$ from the   \n174 training points as: $d_{\\mathbf{x}}=\\operatorname*{min}_{i=1,\\cdots,n}\\left\\|\\mathbf{x}-\\mathbf{x}_{i}\\right\\|$ , where $\\Vert\\cdot\\Vert$ denotes Euclidean distance.   \n175 Proposition 2 (Asymptotic OOD Convergence). Given non-zero and bounded bandwidth of the   \n176 Gaussians, then we have almost sure convergence for $\\hat{g}_{y}$ as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{d_{\\mathbf{x}}\\to\\infty}\\hat{g}_{y}(\\mathbf{x})=\\hat{P_{Y}}(y).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "177 Proof. See Appendix A.2 for the proof. ", "page_idx": 4}, {"type": "text", "text": "179 We conduct several experiments on simulated, OpenML-CC18 [23] 1and vision benchmark datasets   \n180 to gain insights on the finite sample performance of KDF and KDN. The details of the simulation   \n181 datasets and hyperparameters used for all the experiments are provided in Appendix C. For Trunk   \n182 simulation dataset, we follow the simulation setup proposed by Trunk [24] which was designed   \n183 to demonstrate \u2018curse of dimensionality\u2019. In the Trunk simulation, a binary class dataset is used   \n184 where each class is sampled from a Gaussian distribution with higher dimensions having increasingly   \n185 less discriminative information. We use both Euclidean and Geodesic distance to detect the nearest   \n186 polytope (see Equation (4)) on simulation datasets and use only Geodesic distance for benchmark   \n187 datasets. For the simulation setups, we use classification error, Hellinger distance [25, 26] from   \n188 the true class conditional posteriors and mean max confidence [4] as performance statistics. While   \n189 measuring in-distribution calibration for the datasets in OpenML-CC18 data suite, we used maximum   \n190 calibration error as defined by Guo et al. [18] with a fixed bin number of $R=15$ across all the datasets.   \n191 Given $n$ OOD samples, we define OOD calibration error (OCE) to measure OOD performance for   \n192 the benchmark datasets as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{OCE}=\\frac{1}{n}\\sum_{i=1}^{n}\\left|\\operatorname*{max}_{y\\in\\mathcal P}(\\hat{P}_{Y|X}(y|\\mathbf x_{i}))-\\operatorname*{max}_{y\\in\\mathcal P}(\\hat{P}_{Y}(y))\\right|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "193 For the tabular and the vision datasets, we have used ID calibration approaches, such as Isotonic   \n194 [15, 16] and Sigmoid [17] regression, as baselines. Additionally, for the vision benchmark dataset,   \n195 we provide results with OOD calibration approaches such as: ACET [4], ODIN [6], OE (outlier exposure)   \n196 [10]. For each approach, $70\\%$ of the training data was used to fit the model and the rest of the data   \n197 was used to calibrate the model. ", "page_idx": 5}, {"type": "text", "text": "198 5.1 Empirical Study on Tabular Data ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "199 5.1.1 Simulation Study ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "200 Figure 1 leftmost column shows 10000 training samples with 5000 samples per class sampled within   \n201 the region $[-1,1]\\times[-1,1]$ from the six simulation setups described in Appendix C. Therefore, the   \n202 empty annular region between $[-1,1]\\times[-1,1]$ and $[-2,\\bar{2}]\\times[-2,2]$ is the low density or OOD region   \n203 in Figure 1. Figure 1 quantifies the performance of the algorithms which are visually represented   \n204 in Appendix Figure 4. KDF and KDN maintain similar classification accuracy to those of their parent   \n205 algorithms. We measure hellinger distance from the true distribution for increasing training sample   \n206 size within $[-1,1]\\times[-1,1]$ region as a statistics for in-distribution calibration. Column 3 and 6 in   \n207 Figure 1 show KDF and KDN are better at estimating the ID region compared to their parent methods.   \n208 In all of the simulations, using geodesic distance measure results in better performance compared   \n209 to those while using Euclidean distance. For measuring OOD performance, we keep the training   \n210 sample size fixed at 1000 and normalize the training data by the maximum of their l2 norm so that   \n211 the training data is confined within a unit circle. For inference, we sample 1000 inference points   \n212 uniformly from a circle where the circles have increasing radius and plot mean max posterior for   \n213 increasing distance from the origin. Therefore, for distance up to 1 we have in-distribution samples   \n214 and distances farther than 1 can be considered as OOD region. As shown in Column 4 and 7 of Figure   \n215 1, mean max confidence for KDF and KDN converge to the maximum of the class priors, i.e., 0.5 as we   \n216 go farther away from the training data origin.   \n217 Row 6 of Figure 1 shows KDF-Geodesic and KDN-Geodesic scale better with higher dimensions   \n218 compared to their Euclidean counterpart algorithms respectively. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "219 5.1.2 OpenML-CC18 Data Study ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "220 We use OpenML-CC18 data suite for tabular benchmark dataset study. We exclude any dataset   \n221 which contains categorical features or NaN values 2 and conduct our experiments on 45 datasets with   \n222 varying dimensions and sample sizes. For the OOD experiments, we follow a similar setup as that   \n223 of the simulation data. We normalize the training data by their maximum $l_{2}$ norm and sample 1000   \n224 testing samples uniformly from hyperspheres where each hypersphere has increasing radius starting   \n225 from 1 to 5. For each dataset, we measure improvement with respect to the parent algorithm: ", "page_idx": 5}, {"type": "image", "img_path": "cRs4jvF4mO/tmp/f22da544418f69e7327853100533fce006f1f3a267ee86c219017affad869369.jpg", "img_caption": ["Figure 1: Simulation datasets, Classification error, Hellinger distance from true posteriors, mean max confidence or posterior for A. five two-dimensional and B. a high dimensional (Trunk) simulation experiments, visualized for the first two dimensions. The median performance is shown as a dark curve with shaded region as error bars. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "cRs4jvF4mO/tmp/0bcbe3f21eb3ab37504fc1dd2f6662d9a52297e2790c4130ac129db103950c1e.jpg", "img_caption": ["Figure 2: Performance summary of KDF and KDN on OpenML-CC18 data suite. The dark curve in the middle shows the median of performance on 45 datasets with the shaded region as error bar. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\mathcal{E}_{p}-\\mathcal{E}_{M}}{\\mathcal{E}_{p}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "image", "img_path": "cRs4jvF4mO/tmp/307ef30b909353ce033f5e31f373015ac22ca1c5b20b32ae8afa95a1ab0f2249.jpg", "img_caption": ["Figure 3: KDN fliters out inference points with different kinds of semantic shifts from the training data. Simulated images: (A) circle with radius 10, (B) rectangle with sides (20, 50) and out-ofdistribution test points: (C) ellipse with minor and major axis $(10,30)$ . Mean max confidence of KDN are plotted for semantic shift of the inference points created by (D) changing the color intensity, $\\mathrm{(E)}$ taking convex combination of circle and rectangle, (F) changing one of the axes of the ellipse. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "226 where $\\mathcal{E}_{p}=$ classification error, MCE or OCE for the parent algorithm and $\\mathcal{E}_{M}$ represents the perfor  \n227 mance of the approach in consideration. Note that positive improvement implies the corresponding   \n228 approach performs better than the parent approach. We report the median of improvement on dif  \n229 ferent datasets along with the error bar in Figure 2. The extended results for each dataset is shown   \n230 separately in the appendix. Figure 2 left column shows on average KDF and KDN has nearly similar   \n231 or better classification accuracy compared to their respective parent algorithm whereas Isotonic   \n232 and Sigmoid regression have lower classification accuracy most of the cases. However, according   \n233 to Figure 2 middle column, KDF and KDN have similar in-distribution calibration performance to   \n234 the other baseline approaches. Most interestingly, Figure 2 right column shows that KDN and KDF   \n235 improves OOD calibration of their respective parent algorithms by a huge margin while the baseline   \n236 approaches completely fails to address the OOD calibration problem. ", "page_idx": 7}, {"type": "text", "text": "237 5.2 Empirical Study on Vision Data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "238 In vision data, each image pixel contains local information about the neighboring pixels. To extract   \n239 the local information, we use convolutional or vision transformer encoders at the front-end. More   \n240 precisely, we have a front-end encoder, $h_{e}\\,:\\,\\mathbb{R}^{D}\\,\\mapsto\\,\\mathbb{R}^{m}$ and typically, $m\\ <<\\ D$ . After the   \n241 encoder there is a few fully connected dense layers for discriminating among the $K$ class labels,   \n242 $h_{f}:\\mathbb{R}^{m}\\mapsto\\mathbb{R}^{K}$ . Note that the $m$ -dimensional embedding outputs from the encoder are partitioned   \n243 into polytopes by the dense layers (see Equation (3)) and we fti a KDN on the embedding outputs. The   \n244 above approach results in extraction of better inductive bias by KDN from the parent model and makes   \n245 KDN more scalable with larger parent models and training sample size. ", "page_idx": 7}, {"type": "text", "text": "246 5.2.1 Simulation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "247 For the simulation study, we use a simple CNN with one convolutional layer (3 channels with $3\\times3$   \n248 kernel) followed by two fully connected layers with 10 and 2 nodes in each. We train the CNN on   \n249 2000 circle (radius 10) and 2000 rectangle (sides 20, 50) images with their RGB values being fixed at   \n250 [127, 127, 127] and their centers randomly sampled within a square with sides 100. The other pixels   \n251 in the background where there is no object (circle, rectangle or ellipse) were set to 0.   \n252 We perform three experiments while inducing semantic shifts in the inference points as shown in   \n253 Figure 3. In the first experiment, we randomly sampled data similar to the training points. However,   \n254 we added the same shift to all the RGB values of an inference point (shown as color intensity in   \n255 Figure $3\\;\\mathrm{D}_{,}$ ). Therefore, the inference point is ID for color intensity at 127 and otherwise OOD. In the   \n256 second experiment, we kept the RGB values fixed at [127, 127, 127] while taking convex combination   \n257 of a circle and a rectangle. Let images of circles and rectangles be denoted by $X_{c}$ and $X_{r}$ . We derive   \n258 an interference point as $X_{i n f}$ : ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\nX_{i n f}=\\epsilon X_{c}+(1-\\epsilon)X_{r}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "259 Therefore, $X_{i n f}$ is maximally distant from the training points for $\\epsilon=0.5$ and closest to the ID points   \n260 at $\\epsilon=\\{0,1\\}$ . In the third experiment, we sampled ellipse images with the same RGB values as the   \n261 training points. However, this time we gradually change one of the ellipse axes from 0.01 to 40 while   \n262 keeping the other axis fixed at 10. As a result, the inference point becomes ID for the axis length of   \n263 10. As shown in Figure 3 (D, E, F), in all the experiments KDN becomes less confident for the OOD   \n264 points while the parent CNN remains overconfident throughout the semantic shifts of the test points. ", "page_idx": 8}, {"type": "text", "text": "265 5.2.2 Vision Benchmark Datasets Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "266 In this study, we use a $V i T_{-}B16$ (provided in keras-vit package) vision transformer encoder [27]   \n267 pretrained on ImageNet [28] dataset and finetuned on CIFAR-10 [29]. We use the same encoder for   \n268 all the baseline algorithms and finetune it with the corresponding loss function without freezing any   \n269 weight. As shown in Table 1, pretrained vision transformers are already well-calibrated for ID and   \n270 the OOD approaches (ACET, ODIN, OE) degrade ID calibration of the parent model. On the contrary,   \n271 ID calibration approaches (Isotonic, Sigmoid) perform poorly compared to that of KDN in the   \n272 OOD region. KDN achieves a compromise between ID and OOD performance while having reduced   \n273 confidence on wrongly classified ID samples. The number of populated polytopes (and Gaussians)   \n274 for KDN is $9323\\pm353$ . See Appendix F for the corresponding experiments using Resnet-50. ", "page_idx": 8}, {"type": "table", "img_path": "cRs4jvF4mO/tmp/831f06af69429a08bf3269ee787824c3f920ace8620cc5f5bb8d716172963f70.jpg", "table_caption": ["Table 1: KDN achieves good calibration at both ID and OOD regions whereas other approaches which excel either in the ID or the OOD region. Notably, KDN has reduced confidence on wrongly classified ID points. \u2018\u2191\u2019 and $\\downarrow\\,^{\\bullet}$ indicate whether higher and lower values are better, respectively. $\\mathbf{M}\\mathbf{M}\\mathbf{C}^{*}=\\mathbf{I}$ Mean Max Confidence on wrongly classified ID points. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "275 6 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "276 Training time complexity for KDF and KDN is $O(n^{2}l_{f})$ which is dominated by the Geodesic distance   \n277 calculation. Here ${l}_{f}=$ total number of leaves in the forest or total nodes in the dense layers of the   \n278 network and $n=$ total training samples. However, the distance calculation can be done in parallel   \n279 using our provided code. Additionally, note that the number of Gaussian kernel used by KDN is   \n280 upper bounded by number of training samples. Therefore, KDN may not scale for really big datasets   \n281 like ImageNet [28]. However, the scaling issue may be solved by selectively pruning neighboring   \n282 polytopes which we will pursue in future. ", "page_idx": 8}, {"type": "text", "text": "283 7 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "284 In this paper, we demonstrated a simple intuition that renders traditional deep discriminative models   \n285 into a type of binning and kerneling approach. The bin boundaries are determined by the internal   \n286 structure learned by the parent approach and Geodesic distance encodes the low dimensional structure   \n287 learned by the model. Moreover, Geodesic distance introduced in this paper may have broader impact   \n288 on understanding the internal structure of the deep discriminative models which we will pursue in   \n289 future. Our code, including the package and the experiments in this manuscript, will be made publicly   \n290 available upon acceptance of the paper. ", "page_idx": 8}, {"type": "text", "text": "291 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "292 [1] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural   \n293 networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International   \n294 Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,   \n295 pages 1321\u20131330. PMLR, 06\u201311 Aug 2017.   \n296 [2] Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. Being bayesian, even just a bit, fixes   \n297 overconfidence in ReLU networks. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings   \n298 of the 37th International Conference on Machine Learning, volume 119 of Proceedings of   \n299 Machine Learning Research, pages 5436\u20135446. PMLR, 13\u201318 Jul 2020.   \n300 [3] Haoyin Xu, Kaleab A. Kinfu, Will LeVine, Sambit Panda, Jayanta Dey, Michael Ainsworth,   \n301 Yu-Chung Peng, Madi Kusmanov, Florian Engert, Christopher M. White, Joshua T. Vogelstein,   \n302 and Carey E. Priebe. When are Deep Networks really better than Decision Forests at small   \n303 sample sizes, and how? arXiv preprint arXiv:2108.13637, 2021.   \n304 [4] Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield   \n305 high-confidence predictions far away from the training data and how to mitigate the problem. In   \n306 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages   \n307 41\u201350, 2019.   \n308 [5] Alexander Meinke, Julian Bitterwolf, and Matthias Hein. Provably robust detection of out-of  \n309 distribution data (almost) for free. arXiv preprint arXiv:2106.04260, 2021.   \n310 [6] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of  \n311 distribution image detection in neural networks. arXiv preprint arXiv:1706.02690, 2017.   \n312 [7] Jay Nandy, Wynne Hsu, and Mong Li Lee. Towards maximizing the representation gap between   \n313 in-domain & out-of-distribution examples. Advances in Neural Information Processing Systems,   \n314 33:9239\u20139250, 2020.   \n315 [8] Weitao Wan, Yuanyi Zhong, Tianpeng Li, and Jiansheng Chen. Rethinking feature distribution   \n316 for loss functions in image classification. In Proceedings of the IEEE conference on computer   \n317 vision and pattern recognition, pages 9117\u20139126, 2018.   \n318 [9] Terrance DeVries and Graham W Taylor. Learning confidence for out-of-distribution detection   \n319 in neural networks. arXiv preprint arXiv:1802.04865, 2018.   \n320 [10] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier   \n321 exposure. arXiv preprint arXiv:1812.04606, 2018.   \n322 [11] Jie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua Dillon,   \n323 and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. Advances in   \n324 neural information processing systems, 32, 2019.   \n325 [12] Diederik P Kingma, Max Welling, et al. An introduction to variational autoencoders. Founda  \n326 tions and Trends\u00ae in Machine Learning, 12(4):307\u2013392, 2019.   \n327 [13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil   \n328 Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications   \n329 of the ACM, 63(11):139\u2013144, 2020.   \n330 [14] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan.   \n331 Do deep generative models know what they don\u2019t know? arXiv preprint arXiv:1810.09136,   \n332 2018.   \n333 [15] Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision   \n334 trees and naive bayesian classifiers. In Icml, volume 1, pages 609\u2013616, 2001.   \n335 [16] R Caruana. Predicting good probabilities with supervised learning. In Proceedings of NIPS   \n336 2004 Workshop on Calibration and Probabilistic Prediction in Supervised Learning, 2004.   \n337 [17] John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized   \n338 likelihood methods. Advances in large margin classifiers, 10(3):61\u201374, 1999.   \n339 [18] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural   \n340 networks. In International conference on machine learning, pages 1321\u20131330. PMLR, 2017.   \n341 [19] Richard Guo, Ronak Mehta, Jesus Arroyo, Hayden Helm, Cencheng Shen, and Joshua T   \n342 Vogelstein. Estimating information-theoretic quantities with uncertainty forests. arXiv, pages   \n343 arXiv\u20131907, 2019.   \n344 [20] Meelis Kull, Miquel Perello Nieto, Markus K\u00e4ngsepp, Telmo Silva Filho, Hao Song, and Peter   \n345 Flach. Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with   \n346 dirichlet calibration. Advances in neural information processing systems, 32, 2019.   \n347 [21] Haoyin Xu, Kaleab A Kinfu, Will LeVine, Sambit Panda, Jayanta Dey, Michael Ainsworth,   \n348 Yu-Chung Peng, Madi Kusmanov, Florian Engert, Christopher M White, et al. When are deep   \n349 networks really better than decision forests at small sample sizes, and how? arXiv preprint   \n350 arXiv:2108.13637, 2021.   \n351 [22] Bernhard Sch\u00f6lkopf. The kernel trick for distances. Advances in neural information processing   \n352 systems, 13, 2000.   \n353 [23] Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Pieter Gijsbers, Frank Hutter, Michel   \n354 Lang, Rafael G Mantovani, Jan N van Rijn, and Joaquin Vanschoren. Openml benchmarking   \n355 suites. arXiv preprint arXiv:1708.03731, 2017.   \n356 [24] Gerard V Trunk. A problem of dimensionality: A simple example. IEEE Transactions on   \n357 pattern analysis and machine intelligence, (3):306\u2013307, 1979.   \n358 [25] Thomas Kailath. The divergence and bhattacharyya distance measures in signal selection. IEEE   \n359 transactions on communication technology, 15(1):52\u201360, 1967.   \n360 [26] C Radhakrishna Rao. A review of canonical coordinates and an alternative to correspondence   \n361 analysis using hellinger distance. Q\u00fcestii\u00f3: quaderns d\u2019estad\u00edstica i investigaci\u00f3 operativa,   \n362 1995.   \n363 [27] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,   \n364 Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.   \n365 An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint   \n366 arXiv:2010.11929, 2020.   \n367 [28] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large  \n368 scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern   \n369 Recognition, pages 248\u2013255, 2009. doi: 10.1109/CVPR.2009.5206848.   \n370 [29] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced   \n371 research). URL http://www.cs.toronto.edu/\\~kriz/cifar.html.   \n372 [30] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron   \n373 Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural   \n374 information processing systems, 33:18661\u201318673, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "375 A Proofs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "376 A.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "377 For proving that $d$ is a valid distance metric for ${\\mathcal{P}}_{n}$ , we need to prove the following four statements: ", "page_idx": 11}, {"type": "text", "text": "378 1. $d(r,s)=0$ when $r=s$ .   \n379 Proof: By definition, $\\textstyle K(r,s)=1$ and $d(r,s)=0$ when $r=s$ .   \n380 2. $d(r,s)>0$ when $r\\neq s$ .   \n381 Proof: By definition, $0\\leq\\mathcal{K}(r,s)<1$ and $d(r,s)>0$ for $r\\neq s$ .   \n382 3. $d$ is symmetric, i.e., $d(r,s)=d(s,r)$ .   \n383 Proof: By definition, $K(r,s)=K(s,r)$ which implies $d(r,s)=d(s,r)$ .   \n384 4. $d$ follows the triangle inequality, i.e., for any three polytopes $Q_{r},Q_{s},Q_{t}\\in\\mathcal{P}_{n};\\,d(r,t)\\leq$   \n385 $d(r,s)+d(s,t)$ .   \n386 Proof: Let $\\boldsymbol{\\mathcal{A}}_{r}$ denote the set of activation modes in a ReLU-net and the set of leaf nodes   \n387 in a random forest for a particular polytope $r$ . $N$ is the total number of possible activation   \n388 paths in a ReLU-net or total trees in a random forest. Below $c(\\cdot)$ denotes the cardinality of   \n389 the set. We can write: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N\\geq c((\\mathcal{A}_{r}\\cap\\mathcal{A}_{s})\\cup(\\mathcal{A}_{s}\\cap\\mathcal{A}_{t}))}\\\\ &{\\quad=c(\\mathcal{A}_{r}\\cap\\mathcal{A}_{s})+c(\\mathcal{A}_{s}\\cap\\mathcal{A}_{t})-c(\\mathcal{A}_{r}\\cap\\mathcal{A}_{s}\\cap\\mathcal{A}_{t})}\\\\ &{\\quad\\geq c(\\mathcal{A}_{r}\\cap\\mathcal{A}_{s})+c(\\mathcal{A}_{s}\\cap\\mathcal{A}_{t})-c(\\mathcal{A}_{r}\\cap\\mathcal{A}_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "390 ", "page_idx": 11}, {"type": "text", "text": "Rearranging the above equation, we get: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N-c(\\mathcal{A}_{r}\\cap\\mathcal{A}_{t})\\leq N-c(\\mathcal{A}_{r}\\cap\\mathcal{A}_{s})+N-c(\\mathcal{A}_{s}\\cap\\mathcal{A}_{t})}\\\\ &{\\implies1-\\frac{c(\\mathcal{A}_{r}\\cap\\mathcal{A}_{t})}{N}\\leq1-\\frac{c(\\mathcal{A}_{r}\\cap\\mathcal{A}_{s})}{N}+1}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-\\frac{c(\\mathcal{A}_{s}\\cap\\mathcal{A}_{t})}{N}}\\\\ &{\\implies d(r,t)\\leq d(r,s)+d(s,t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "391 A.2 Proof of Proposition 2 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "392 Note that first we find the nearest polytope to the inference point $x$ using Geodesic distance and use   \n393 Gaussian kernel locally for $x$ within that polytope. Here the Gaussian kernel uses Euclidean distance   \n394 from the kernel center to $x$ (within the numerator of the exponent). The value out of the Gaussian   \n395 kernel decays exponentially with the increasing distance of the inference point from the kernel center.   \n396 We first expand $\\hat{g}_{y}({\\bf x})$ : ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\hat{g}_{y}(\\mathbf x)=\\frac{\\hat{f}_{y}(\\mathbf x)\\hat{P}_{Y}(y)}{\\sum_{k=1}^{K}\\hat{f}_{k}(x)\\hat{P}_{Y}(k)}}}\\\\ &{}&{=\\frac{\\hat{f}_{y}(\\mathbf x)\\hat{P}_{Y}(y)+\\frac{b}{\\log(n)}\\hat{P}_{Y}(y)}{\\sum_{k=1}^{K}(\\hat{f}_{k}(\\mathbf x)\\hat{P}_{Y}(k)+\\frac{b}{\\log(n)}\\hat{P}_{Y}(k))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "As the inference point $\\mathbf{x}$ becomes more distant from training samples (and more distant from all of the Gaussian centers), we have that $\\mathcal{G}(\\mathbf{x},\\hat{\\mu}_{r},\\hat{\\Sigma}_{r})$ becomes smaller. Thus, $\\forall y,\\tilde{f}_{y}(\\mathbf{x})$ shrinks. More formally, $\\forall y$ , ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{d_{\\mathbf{x}}\\rightarrow\\infty}\\tilde{f}_{y}(\\mathbf{x})=0.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "397 We can use this result to then examine the limiting behavior of our posteriors as the inference point $\\mathbf{x}$   \n398 becomes more distant from the training data: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{d_{\\mathbf{x}}\\rightarrow\\infty}{\\mathrm{lim}}\\,\\hat{g}_{y}(\\mathbf{x})=\\underset{d_{\\mathbf{x}}\\rightarrow\\infty}{\\mathrm{lim}}\\,\\frac{\\tilde{f}_{y}(\\mathbf{x})\\hat{P}_{Y}(y)+\\frac{b}{\\log(n)}\\hat{P}_{Y}(y)}{\\sum_{k=1}^{K}(\\tilde{f}_{k}(\\mathbf{x})\\hat{P}_{Y}(k)+\\frac{b}{\\log(n)}\\hat{P}_{Y}(k))}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{(\\mathrm{lim}_{d_{\\mathbf{x}}\\rightarrow\\infty}\\,\\tilde{f}_{y}(\\mathbf{x}))\\hat{P}_{Y}(y)+\\frac{b}{\\log(n)}\\hat{P}_{Y}(y)}{\\sum_{k=1}^{K}(\\mathrm{lim}_{d_{\\mathbf{x}}\\rightarrow\\infty}\\,\\tilde{f}_{k}(\\mathbf{x}))\\hat{P}_{Y}(k)+\\frac{b}{\\log(n)}\\hat{P}_{Y}(k))}}\\\\ &{\\qquad=\\frac{\\hat{P}_{Y}(y)}{\\sum_{k=1}^{K}\\hat{P}_{Y}(k)}}\\\\ &{\\qquad=\\hat{P}_{Y}(y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "399 B Hardware and Software Configurations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "400 \u2022 Operating System: Linux (ubuntu 20.04), macOS (Ventura 13.2.1)   \n401 \u2022 VM Size: Azure Standard D96as v4 (96 vcpus, 384 GiB memory)   \n402 \u2022 GPU: Apple M1 Max   \n403 \u2022 Software: Python 3.8, scikit-learn $\\geq0.22.0$ , tensorflow-macos $\\leq\\!2.9$ , tensorflow-metal $\\leq$   \n404 0.5.0. ", "page_idx": 12}, {"type": "text", "text": "405 C Simulations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "406 We construct six types of binary class simulations: ", "page_idx": 12}, {"type": "text", "text": "407   \n408   \n409   \n410   \n411   \n412   \n413   \n414   \n415   \n416   \n417   \n418   \n419   \n420   \n421   \n422   \n423   \n424   \n425   \n426   \n427   \n428   \n429   \n430   \n431   \n432 ", "page_idx": 12}, {"type": "text", "text": "\u2022 Gaussian XOR is a two-class classification problem with equal class priors. Conditioned on being in class 0, a sample is drawn from a mixture of two Gaussians with means $\\pm[0.5,-0.5]^{\\top}$ and standard deviations of 0.25. Conditioned on being in class 1, a sample is drawn from a mixture of two Gaussians with means $\\pm[0.5,-0.5]^{\\top}$ and standard deviations of 0.25. \u2022 Spiral is a two-class classification problem with the following data distributions: let $K$ be the number of classes and $S\\sim\\mathrm{multinomial}({\\textstyle\\frac{1}{K}}\\vec{1}_{K},n)$ . Conditioned on $S$ , each feature vector is parameterized by two variables, the radius $r$ and an angle $\\theta$ . For each sample, $r$ is sampled uniformly in $[0,1]$ . Conditioned on a particular class, the angles are evenly spaced between $\\frac{4\\pi(k\\!-\\!1)t_{K}}{K}$ and $\\frac{4\\pi(k)t_{K}}{K}$ , where $t_{K}$ controls the number of turns in the spiral. To inject noise along the spirals, we add Gaussian noise to the evenly spaced angles $\\theta^{\\prime}:\\theta=\\theta^{\\prime}+\\mathcal{N}(0,0.09)$ . The observed feature vector is then $(r\\;\\cos(\\theta),r\\;\\sin(\\theta))$ . \u2022 Circle is a two-class classification problem with equal class priors. Conditioned on being in class 0, a sample is drawn from a circle centered at $(0,0)$ with a radius of $r\\,=\\,0.75$ . Conditioned on being in class 1, a sample is drawn from a circle centered at $(0,0)$ with a radius of $r=1$ , which is cut off by the region bounds. To inject noise along the circles, we add Gaussian noise to the circle radii $r^{\\prime}:\\bar{r}=r^{\\prime}+\\mathcal{N}(0,0.\\dot{0}1)$ . \u2022 Sinewave is a two-class classification problem based on sine waves. Conditioned on being in class 0, a sample is drawn from the distribution $y=\\cos(\\pi x)$ . Conditioned on being in class 1, a sample is drawn from the distribution $y=\\sin(\\pi x)$ . We inject Gaussian noise to the sine wave heights $y^{\\prime}:y=y^{\\prime}+\\mathcal{N}(0,0.01)$ . \u2022 Polynomial is a two-class classification problem with the following data distributions: $y=x^{a}$ . Conditioned on being in class 0, a sample is drawn from the distribution $y=x^{1}$ . Conditioned on being in class 1, a sample is drawn from the distribution $y=x^{3}$ . Gaussian noise is added to variables $y^{\\prime}:y=y^{\\prime}\\,\\bar{+}\\,\\mathcal{N}(0,0.01)$ . \u2022 Trunk is a two-class classification problem with gradually increasing dimension and equal 433 class priors. The class conditional probabilities are Gaussian: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P(X|Y=0)=\\mathcal{G}(\\mu_{1},I),}\\\\ {P(X|Y=1)=\\mathcal{G}(\\mu_{2},I),}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "image", "img_path": "cRs4jvF4mO/tmp/91d52c8c5c703b3e14fb7b12fa85b986c82a149e7bcc4a3fd1494607caf1a443.jpg", "img_caption": ["Figure 4: Visualization of true and estimated posteriors for class 0 from five binary class simulation experiments. Column 1: 10,000 training points with 5,000 samples per class sampled from 6 different simulation setups for binary class classification. Trunk simulation is shown for two dimensional case. The class labels are indicated by yellow and blue colors. Column 2-8: True and estimated class conditional posteriors from different approaches. The posteriors estimated from KDN and KDF are better calibrated for both in- and out-of-distribution regions compared to those of their parent algorithms. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "434 where $\\mu_{1}=\\mu,\\mu_{2}=-\\mu,\\mu$ is a $d$ dimensional vector whose $i$ -th component is $\\textstyle{\\left({\\frac{1}{i}}\\right)^{1/2}}$ and $I$   \n435 is $d$ dimensional identity matrix. ", "page_idx": 13}, {"type": "table", "img_path": "cRs4jvF4mO/tmp/a4c233a43437799e7119f9cd91191d6364e9333c695f94bfc95b992d51265bf8.jpg", "table_caption": ["Table 2: Hyperparameters for RF and KDF. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "436 D Pseudocodes ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "437 We provide the pseudocode for our porposed algorithms in Algorithm 1, 2 and 3. ", "page_idx": 13}, {"type": "text", "text": "438 E Extended Results on OpenML-CC18 data suite ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "439 See Figure 5, 6, 7 and 8 for extended results on OpenML-CC18 data suite. ", "page_idx": 13}, {"type": "text", "text": "Algorithm 1 Fit a KDX model. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Input: (1) \u03b8 \u25b7Parent learner (random forest or deep network model) (2) $\\mathcal{D}_{n}=(\\mathbf{X},\\mathbf{y})\\in\\mathbb{R}^{n\\times d}\\times\\{1,\\dots,K\\}^{n}$ $\\triangleright$ Training data   \nOutput: $\\mathcal{G}$ \u25b7a KDX model   \n1: function $\\mathtt{K G X.F I T}(\\theta,\\mathbf{X},\\mathbf{y})$   \n2: for $i=1,\\hdots,n$ do $\\triangleright$ Iterate over the dataset to calculate the weights   \n3: for $j=1,\\dots,n$ do   \n4: $\\boldsymbol{w}_{i j}\\gets\\mathrm{coMPUTEWEIGHTS}(\\mathbf{x}_{i},\\mathbf{x}_{j},\\boldsymbol{\\theta})$   \n5: end for   \n6: end for   \n7:   \n8:   \n9: $\\{Q_{r},\\mathbf{w}_{r s}\\}_{r=1}^{\\tilde{p}}\\leftarrow$ GETPOLYTOPES(w) \u25b7Identify the polytopes by clustering the samples with similar weight   \n10:   \n11: for $r=1,\\hdots,\\tilde{p}$ do $\\triangleright$ Iterate over each polytope   \n12: $\\mathcal{G}.\\hat{\\mu}_{r},\\mathcal{G}.\\hat{\\Sigma}_{r},\\mathcal{G}.\\hat{n}_{r y}\\gets1$ ESTIMATEPARAMETERS(X, y, $\\{\\mathbf{w}_{r s}\\}_{s=1}^{\\tilde{p}})$ \u25b7Fit Gaussians using MLE   \n13: end for   \n14: return $\\mathcal{G}$   \n15: end function ", "page_idx": 14}, {"type": "text", "text": "Algorithm 2 Computing weights in KDF ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Input: (1) xi, xj \u2208R1\u00d7d (2) \u03b8 ", "page_idx": 14}, {"type": "text", "text": "$\\triangleright$ two input samples to be weighted $\\triangleright$ parent random forest with $T$ trees ", "page_idx": 14}, {"type": "text", "text": "Output: $w_{i j}\\in[0,1]$ ", "page_idx": 14}, {"type": "text", "text": "$\\triangleright$ compute similarity between $i$ and $j$ -th samples. 1: function COMPUTEWEIGHT $\\mathbf{\\Gamma}_{\\mathrm{{s}}}(\\mathbf{x}_{i},\\mathbf{x}_{j},\\theta)$ 2: $\\mathcal{T}_{i}\\gets$ PUSHDOWNTREES $(\\mathbf{x}_{i},\\theta)~\\triangleright$ push $\\mathbf{x}_{i}$ down $T$ trees and get the leaf numbers it end up in. 3: $\\mathcal{T}_{j}\\gets$ PUSHDOWNTREES $(\\mathbf{x}_{j},\\theta)\\,\\triangleright$ push $\\mathbf{x}_{j}$ down $T$ trees and get the leaf numbers it end up in. 4: l \u2190COUNTMATCHES $(\\mathbb{Z}_{i},\\mathbb{Z}_{j})$ $\\triangleright$ count the number of times the samples end up in the same leaf 5: wij \u2190Tl 6: return wij 7: end function ", "page_idx": 14}, {"type": "text", "text": "Algorithm 3 Computing weights in KDN ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Input: $\\mathbf{x}_{i},\\mathbf{x}_{j}\\in\\mathbb{R}^{1\\times d}$ $\\triangleright$ two input samples to be weighted (2) \u03b8 $\\triangleright$ parent deep-net model   \nOutput: $w_{i j}\\in[0,1]$ \u25b7compute similarity between $i$ and $j$ -th samples.   \n1: function COMPUTEWEIGHT $\\mathbf{\\boldsymbol{s}}(\\mathbf{\\boldsymbol{x}}_{i},\\mathbf{\\boldsymbol{x}}_{j},\\boldsymbol{\\theta})$   \n2: $\\boldsymbol{A}_{i}\\gets$ PUSHDOWNNETWORK $(\\mathbf{x}_{i},\\theta)$ $\\triangleright$ get activation modes $\\boldsymbol{A}_{i}$   \n3: $\\mathcal{A}_{j}\\gets\\mathrm{PUSHDowNNETWORK}(\\mathbf{x}_{j},\\theta)$ $\\triangleright$ get activation modes $\\mathcal{A}_{j}$   \n4: $l\\leftarrow$ COUNTMATCHES $(\\mathcal{A}_{i},\\mathcal{A}_{j})$ ) $\\triangleright$ count the number of times the two samples activate the activation paths in a similar way   \n5: wij \u2190Nl \u25b7 $N$ is the total number of activation paths   \n6: return wij   \n7: end function ", "page_idx": 14}, {"type": "table", "img_path": "cRs4jvF4mO/tmp/8dd5953a684eabe14017893baa3acc2b153671769d64d90f1b0bba1dff567ab5.jpg", "table_caption": ["Table 3: Hyperparameters for ReLU-net and KDNon Tabular data. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 4: ID approaches (Sigmoid, Isotonic) are bad at OOD calibration and OOD approaches (ACET, ODIN, OE) are bad at ID calibration. KDN bridges between both ID and OOD calibration approaches. \u2018\u2191\u2019 and $\\mathbf{\\dot{\\varphi}}\\downarrow\\,^{\\bullet}$ indicate whether higher and lower values are better, respectively. Bolded indicates most performant, or within the margin of error of the most performant. ", "page_idx": 15}, {"type": "table", "img_path": "cRs4jvF4mO/tmp/f2db586ea54ac4f1d77e2befc6f7e0bbfb56a61ea9364ff4930eaf32eb0b867e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "440 F Extended Results on Vision datasets using Resnet-50 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "441 In this experiments, we use a Resnet-50 encoder pretrained using contrastive loss [30] as described   \n442 in http://keras.io/examples/vision/supervised-contrastive-learning. The encoder   \n443 projects the input images down to a 256 dimensional latent space and we add two dense layers with   \n444 200 and 10 nodes on top of the encoder. We use the same pretrained encoder for all the baseline   \n445 algorithms.   \n446 As shown in Table 4, KDN achieves good calibration for both ID and OOD datasets whereas the ID   \n447 calibration approaches are poorly calibrated in the OOD regions and the OOD approaches have poor   \n448 ID calibration. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "cRs4jvF4mO/tmp/6b2774d39b68711f90089d61d326c32d0f3655b196be573e18a49e6a0696c61b.jpg", "img_caption": ["Figure 5: Extended results on OpenML-CC18 datasets. Left: Performance (classification error, MCE and mean max confidence) of KDF on different Openml-CC18 datasets. Right: Performance (classification error, MCE and mean max confidence) of KDN on different Openml-CC18 datasets. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "cRs4jvF4mO/tmp/1807c53818f32c66052c6be1a4c7a4826f75f1a414c8a490182c7fef52fafef4.jpg", "img_caption": ["Figure 6: Extended results on OpenML-CC18 datasets (continued). Left: Performance (classification error, MCE and mean max confidence) of KDF on different Openml-CC18 datasets. Right: Performance (classification error, MCE and mean max confidence) of KDN on different Openml-CC18 datasets. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "cRs4jvF4mO/tmp/2e91a50341d8669980808bbf74feabc663662bae1043ca2cca6d747828c7599b.jpg", "img_caption": ["Figure 7: Extended results on OpenML-CC18 datasets (continued). Left: Performance (classification error, MCE and mean max confidence) of KDF on different Openml-CC18 datasets. Right: Performance (classification error, MCE and mean max confidence) of KDN on different Openml-CC18 datasets. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "cRs4jvF4mO/tmp/4a0b1a6cc315ee3db0437cf3c727b60c9caa7125e022cb4a8f27339bb7cde9e5.jpg", "img_caption": ["Figure 8: Extended results on OpenML-CC18 datasets (continued). Left: Performance (classification error, MCE and mean max confidence) of KDF on different Openml-CC18 datasets. Right: Performance (classification error, MCE and mean max confidence) of KDN on different Openml-CC18 datasets. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "449 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "450 1. Claims   \n451 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n452 paper\u2019s contributions and scope?   \n453 Answer: [Yes]   \n454 Justification: We enumerate list of contributions in Section 2 which are also mentioned in   \n455 the abstract and the introduction.   \n456 2. Limitations   \n457 Question: Does the paper discuss the limitations of the work performed by the authors?   \n458 Answer: [Yes]   \n459 Justification: We discussed the limitations of the work in Section 6 to the best of our   \n460 knowledge.   \n461 3. Theory Assumptions and Proofs   \n462 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n463 a complete (and correct) proof?   \n464 Answer: [Yes]   \n465 Justification: All of our theoretical results are discussed in Section 4.5 and proofs are   \n466 provided in the Appendix.   \n467 4. Experimental Result Reproducibility   \n468 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n469 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n470 of the paper (regardless of whether the code and data are provided or not)?   \n471 Answer: [Yes]   \n472 Justification:   \n473 5. Open access to data and code   \n474 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n475 tions to faithfully reproduce the main experimental results, as described in supplemental   \n476 material?   \n477 Answer: [Yes]   \n478 Justification: All code that was used to create the experiments has been provided as a part of   \n479 the supplementary material, and will be made public after the review process.   \n480 6. Experimental Setting/Details   \n481 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n482 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n483 results?   \n484 Answer: [Yes]   \n485 Justification: Experimental setting are described adequately in the main paper with more   \n486 details discussed in the Appendix.   \n487 7. Experiment Statistical Significance   \n488 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n489 information about the statistical significance of the experiments?   \n490 Answer: [Yes]   \n491 Justification: We have repeated the experiments over several Monte Carlo repetitions and   \n492 have reported the error bars in the figures and the tables.   \n493 8. Experiments Compute Resources   \n494 Question: For each experiment, does the paper provide sufficient information on the com  \n495 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n496 the experiments?   \n497 Answer: [Yes]   \n498 Justification: We have provided the computation platform and resources used in the Appendix   \n499 B.   \n500 9. Code Of Ethics   \n501 Question: Does the research conducted in the paper conform, in every respect, with the   \n502 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n503 Answer: [Yes]   \n504 Justification: We did not conduct experiments with human participants and we do not   \n505 anticipate any societal harmful consequences.   \n506 10. Broader Impacts   \n507 Question: Does the paper discuss both potential positive societal impacts and negative   \n508 societal impacts of the work performed?   \n509 Answer: [NA]   \n510 Justification: This paper demonstrates a simple intuition that can increase the confidence   \n511 calibration for modern deep machine learning approaches. Although the proposed work is   \n512 not directly related to any societal impact, down the line we anticipate it will pave the way   \n513 for safety critical application of machine learning models.   \n514 11. Safeguards   \n515 Question: Does the paper describe safeguards that have been put in place for responsible   \n516 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n517 image generators, or scraped datasets)?   \n518 Answer: [NA]   \n519 Justification: The paper poses no such risks.   \n520 12. Licenses for existing assets   \n521 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n522 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n523 properly respected?   \n524 Answer: [Yes]   \n525 Justification: We only use existing public datasets (appropriately cited in the paper) and   \n526 synthetic data. No new assets are created from this paper.   \n527 13. New Assets   \n528 Question: Are new assets introduced in the paper well documented and is the documentation   \n529 provided alongside the assets?   \n530 Answer: [NA]   \n531 Justification: No assets are created in this paper.   \n532 14. Crowdsourcing and Research with Human Subjects   \n533 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n534 include the full text of instructions given to participants and screenshots, if applicable, as   \n535 well as details about compensation (if any)?   \n536 Answer: [NA]   \n537 Justification: No such study was performed.   \n538 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n539 Subjects   \n540 Question: Does the paper describe potential risks incurred by study participants, whether   \n541 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n542 approvals (or an equivalent approval/review based on the requirements of your country or   \n543 institution) were obtained?   \n544 Answer: [NA]   \n545 Justification: No such study was performed. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}]