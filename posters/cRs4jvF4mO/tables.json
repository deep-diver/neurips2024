[{"figure_path": "cRs4jvF4mO/tables/tables_8_1.jpg", "caption": "Table 1: KDN achieves good calibration at both ID and OOD regions whereas other approaches which excel either in the ID or the OOD region. Notably, KDN has reduced confidence on wrongly classified ID points. \u2018\u2191\u2019 and \u2018\u2193\u2019 indicate whether higher and lower values are better, respectively.", "description": "This table presents a comparison of the performance of different calibration methods (KDN, ISOTONIC, SIGMOID, ACET, ODIN, OE) on CIFAR-10 and CIFAR-100 datasets for in-distribution (ID) and out-of-distribution (OOD) scenarios.  The metrics evaluated include accuracy, maximum calibration error (MCE), and mean maximum confidence (MMC) for ID data, and out-of-distribution calibration error (OCE) for OOD data. The results highlight KDN's ability to achieve good calibration in both ID and OOD regions, unlike other methods which excel in only one region. Notably, KDN also shows reduced overconfidence on misclassified ID samples.", "section": "5.2 Empirical Study on Vision Data"}, {"figure_path": "cRs4jvF4mO/tables/tables_13_1.jpg", "caption": "Table 4: ID approaches (SIGMOID, ISOTONIC) are bad at OOD calibration and OOD approaches (ACET, ODIN, OE) are bad at ID calibration. KDN bridges between both ID and OOD calibration approaches.\u2018\u2191' and '\u2193' indicate whether higher and lower values are better, respectively. Bolded indicates most performant, or within the margin of error of the most performant.", "description": "This table compares the performance of different in-distribution (ID) and out-of-distribution (OOD) calibration methods on CIFAR-10, CIFAR-100 and SVHN datasets. The methods are evaluated based on accuracy, maximum calibration error (MCE), out-of-distribution calibration error (OCE), and mean maximum confidence (MMC) on wrongly classified ID points. The results show that KDN achieves a good balance between ID and OOD calibration, while ID-focused methods perform poorly on OOD data and vice versa.", "section": "5.2 Empirical Study on Vision Data"}, {"figure_path": "cRs4jvF4mO/tables/tables_15_1.jpg", "caption": "Table 3: Hyperparameters for RELU-net and KDN on Tabular data.", "description": "This table shows the hyperparameters used for training both the RELU network and the proposed Kernel Density Network (KDN) on tabular datasets.  It lists the number of hidden layers, nodes per hidden layer, the optimizer used (Adam), the learning rate, and the values of lambda (\u03bb) and b which are regularization parameters for the KDN.", "section": "5.1 Empirical Study on Tabular Data"}, {"figure_path": "cRs4jvF4mO/tables/tables_15_2.jpg", "caption": "Table 1: KDN achieves good calibration at both ID and OOD regions whereas other approaches which excel either in the ID or the OOD region. Notably, KDN has reduced confidence on wrongly classified ID points.\u2018\u2191\u2019 and \u2018\u2193\u2019 indicate whether higher and lower values are better, respectively.", "description": "This table presents the performance comparison of various methods, including KDN, for in-distribution (ID) and out-of-distribution (OOD) calibration. The metrics used are accuracy, maximum calibration error (MCE), and mean maximum confidence (MMC) for ID, and outlier exposure (OCE) for OOD.  The results highlight KDN's ability to achieve good calibration across both ID and OOD regions, unlike other methods that excel only in one region.", "section": "5.2 Empirical Study on Vision Data"}]