[{"figure_path": "6KDZHgrDhG/figures/figures_0_1.jpg", "caption": "Figure 1: Given a (conjunctive) composition of deterministic finite automata (shown on the left), we construct its embedding using a graph neural network (GATv2) and use this embedding as a goal to condition the reinforcement learning policy.", "description": "This figure illustrates the overall architecture of the proposed approach for goal-conditioned reinforcement learning using compositional deterministic finite automata (cDFAs).  On the left, we see two simple deterministic finite automata (DFAs) representing individual sub-tasks. These DFAs are combined conjunctively (using an AND operation) to form a cDFA representing a more complex temporal goal.  This cDFA is then fed into a graph attention network (GATv2), which produces a vector embedding of the goal. This embedding is concatenated with the agent's current state representation (from a convolutional neural network). Finally, the concatenated vector is passed into a policy network to produce an action for the reinforcement learning agent. The image on the right is a sample illustrative environment.", "section": "3 Compositional Automata-Conditioned Reinforcement Learning"}, {"figure_path": "6KDZHgrDhG/figures/figures_4_1.jpg", "caption": "Figure 1: Given a (conjunctive) composition of deterministic finite automata (shown on the left), we construct its embedding using a graph neural network (GATv2) and use this embedding as a goal to condition the reinforcement learning policy.", "description": "This figure illustrates the architecture for goal-conditioned reinforcement learning using compositional deterministic finite automata (cDFAs).  On the left, we see a composition of multiple DFAs, each representing a sub-task. These DFAs are combined conjunctively (AND). A graph neural network (GATv2) processes this structure, generating an embedding that captures the overall goal.  This embedding is then used to condition a reinforcement learning policy, enabling the agent to achieve the complex goal represented by the composed DFAs. The right side shows the embedding process and how it conditions the policy network.", "section": "3 Compositional Automata-Conditioned Reinforcement Learning"}, {"figure_path": "6KDZHgrDhG/figures/figures_5_1.jpg", "caption": "Figure 3: An example of a sequence of local reach-avoid problems and a RAD DFA obtained from it.", "description": "This figure illustrates the process of generating a Reach-Avoid Derived (RAD) DFA from a simpler Sequential Reach-Avoid (SRA) DFA.  An SRA DFA is a DFA where paths correspond to a series of reach-avoid tasks. The figure shows an example SRA DFA, which is then mutated (a transition is randomly changed). After the mutation, the accepting state's outgoing edges are removed and the resulting DFA is minimized. This mutated and minimized DFA is then considered a RAD DFA.  The process demonstrates how a RAD DFA is a generalization of SRA DFAs, and incorporates a higher level of complexity and structure than a simple SRA DFA.", "section": "4 Pre-training on Reach-Avoid Derived (RAD) Compositional Automata"}, {"figure_path": "6KDZHgrDhG/figures/figures_6_1.jpg", "caption": "Figure 4: An example in which the myopia of hierarchical approaches causes them to find a suboptimal solution. If the task is to first go to orange and then green, the hierarchical approaches will choose the closest orange which takes them further from green whereas our approach finds the optimal solution.", "description": "The figure shows a comparison between hierarchical reinforcement learning methods and the proposed method for solving a goal-conditioned task in a grid-world environment.  The task involves reaching an orange square first and then a green square.  Hierarchical approaches, due to their myopic nature, might choose a closer orange square that ultimately leads to a suboptimal path to the green square.  The proposed method, by considering the entire task from the start, finds the optimal path.", "section": "Experiments"}, {"figure_path": "6KDZHgrDhG/figures/figures_7_1.jpg", "caption": "Figure 5: Training curves (error bars show a 90% confidence interval over 10 seeds) in Letterworld (discrete) and Zones (continuous) for policies trained on RAD cDFAs, showing that frozen pre-trained cDFA encoders perform better than non-frozen ones while no pre-training barely learns the tasks.", "description": "This figure compares the training performance of reinforcement learning policies trained with different methods on two environments: Letterworld (discrete) and Zones (continuous). The training curves show the discounted return achieved over time.  Different lines represent different training approaches:\n\n* **No pretraining:** Policies trained without any pre-training on cDFAs.\n* **Pretraining:** Policies trained with pre-training on reach-avoid derived (RAD) cDFAs, where the encoder is fine-tuned during policy training.\n* **Pretraining (frozen):** Policies trained with pre-training on RAD cDFAs, but the encoder is frozen (weights are not updated) during policy training. \n\nThe results indicate that pre-training significantly improves performance, especially when the encoder's weights are frozen after pre-training.  This suggests that pre-training effectively learns a good representation of the cDFAs, which can then be used to accelerate policy learning without needing to re-learn the cDFA representation during the reinforcement learning process.", "section": "5 Experiments"}, {"figure_path": "6KDZHgrDhG/figures/figures_8_1.jpg", "caption": "Figure 6: Visualizations of the embeddings generated by GATv2 pre-trained on RAD cDFAs, illustrating that the learned embedding space reflects the similarities between different task classes.", "description": "This figure visualizes the embedding space generated by the Graph Attention Network (GATv2) model pre-trained on Reach-Avoid Derived (RAD) compositional deterministic finite automata (cDFAs).  It uses three different visualization techniques to show how well the model clusters different cDFA task classes in its embedding space. Panel (a) shows a 2D t-SNE projection of the 32D embeddings, clearly showing distinct clusters for different task types.  Panels (b) and (c) present heatmaps of cosine similarity and Euclidean distance between all pairs of cDFA embeddings, respectively.  The heatmaps further confirm the clustering observed in the t-SNE plot and highlight the relationships between different task complexities. The figure demonstrates that GATv2 effectively learns a meaningful representation of cDFAs that captures their structural and semantic similarities.", "section": "RQ5: cDFA embedding space is well-clustered"}, {"figure_path": "6KDZHgrDhG/figures/figures_8_2.jpg", "caption": "Figure 1: Given a (conjunctive) composition of deterministic finite automata (shown on the left), we construct its embedding using a graph neural network (GATv2) and use this embedding as a goal to condition the reinforcement learning policy.", "description": "This figure illustrates the overall architecture of the proposed approach.  A composition of deterministic finite automata (cDFA) is used to represent temporal goals.  A graph attention network (GATv2) is used to generate an embedding vector from the cDFA. This embedding vector is then used as an input to condition a reinforcement learning policy network, which outputs actions to the agent in the environment.", "section": "3 Compositional Automata-Conditioned Reinforcement Learning"}, {"figure_path": "6KDZHgrDhG/figures/figures_9_1.jpg", "caption": "Figure 8: Satisfaction generalization capabilities on LTL tasks (from [38]) of LTL2Action [38] policies vs policies trained on RAD CDFAs. See Appendix C.8 for training curves of policies.", "description": "This figure compares the generalization capabilities of policies trained using two different methods on various LTL tasks.  The first set of policies were trained by the LTL2Action approach ([38]), while the second set were trained using the RAD CDFA method described in this paper. The figure shows the satisfaction rate (or success rate) for each policy on a range of LTL tasks.  The Appendix C.8 provides further details on the training curves for each set of policies.", "section": "5 Experiments"}, {"figure_path": "6KDZHgrDhG/figures/figures_15_1.jpg", "caption": "Figure 9: Learning curves for pre-training on RAD CDFAs in the dummy MDP.", "description": "This figure shows the learning curves for pre-training the Graph Attention Network (GATv2) and Relational Graph Convolutional Network (RGCN) on Reach-Avoid Derived (RAD) Compositional Deterministic Finite Automata (cDFAs) within a dummy Markov Decision Process (MDP).  The x-axis represents training frames (in millions), and the y-axis represents the discounted return.  The curves illustrate the training progress of the two different network architectures on the RAD cDFA encoding task, highlighting the faster convergence of GATv2 compared to RGCN. This showcases the benefit of the attention mechanism in GATv2 for learning efficient cDFA embeddings.", "section": "C Results"}, {"figure_path": "6KDZHgrDhG/figures/figures_16_1.jpg", "caption": "Figure 10: Generalization results of GATv2 and RGCN pre-trained on RAD CDFAs, where satisfaction likelihood (left) and step count (right) are shown by the solid lines and by cross-hatching, respectively.", "description": "This figure presents the generalization results of GATv2 and RGCN models pre-trained on Reach-Avoid Derived (RAD) Compositional Deterministic Finite Automata (cDFAs) when tested on various unseen cDFA tasks.  The left side shows the satisfaction likelihood (the probability of successfully completing the task), while the right shows the number of steps taken to complete the task.  The results demonstrate that both GATv2 and RGCN generalize well to different cDFA task classes, though GATv2 consistently outperforms RGCN in most instances.", "section": "C Results"}, {"figure_path": "6KDZHgrDhG/figures/figures_17_1.jpg", "caption": "Figure 1: Given a (conjunctive) composition of deterministic finite automata (shown on the left), we construct its embedding using a graph neural network (GATv2) and use this embedding as a goal to condition the reinforcement learning policy.", "description": "This figure illustrates the overall architecture of the proposed approach.  A composition of deterministic finite automata (cDFA) represents the temporal goal.  Each DFA within the composition is individually processed by a graph neural network (GATv2) which produces an embedding for that DFA. These individual DFA embeddings are then combined (using concatenation in this example) to create a single embedding representing the whole cDFA goal. This goal embedding is then used as input to a reinforcement learning policy network which determines the actions of the agent.", "section": "3 Compositional Automata-Conditioned Reinforcement Learning"}, {"figure_path": "6KDZHgrDhG/figures/figures_18_1.jpg", "caption": "Figure 2: Message passing illustration on the featurization of the cDFA given in Figure 1.", "description": "This figure illustrates the message passing mechanism used in the graph attention network (GATv2) to generate embeddings for compositional deterministic finite automata (cDFAs).  It shows how the network processes the nodes and edges of the cDFA graph to create a vector representation that captures the temporal logic encoded within the cDFA. This embedding is then used as an input to the reinforcement learning policy network.", "section": "3 Compositional Automata-Conditioned Reinforcement Learning"}, {"figure_path": "6KDZHgrDhG/figures/figures_18_2.jpg", "caption": "Figure 2: Message passing illustration on the featurization of the cDFA given in Figure 1.", "description": "This figure illustrates the message passing mechanism used in the proposed architecture for featurizing compositional deterministic finite automata (cDFAs).  It shows how the graph neural network (GATv2) processes the nodes and edges of a cDFA to generate an embedding representing the temporal task. The figure highlights the steps involved in creating the featurization, including adding new nodes for every transition, reversing edges, adding self-loops, and adding connections from initial states to a central 'AND' node. The message passing process is then visualized, showing how information is propagated through the network to produce the final cDFA embedding.", "section": "3 Compositional Automata-Conditioned Reinforcement Learning"}, {"figure_path": "6KDZHgrDhG/figures/figures_19_1.jpg", "caption": "Figure 14: Training curves for RA and RAR policies with RAD-pre-trained GATv2 in Letterworld.", "description": "This figure shows the training curves for three different types of policies in the Letterworld environment: RAD, ReachAvoid_1_1_1_5, and ReachAvoidRedemption_1_2_1_2.  The x-axis represents the number of frames (training steps), and the y-axis represents the discounted return, a measure of the policy's performance. The curves show how the average discounted return changes over the course of training for each policy.  The purpose is to demonstrate the impact of pre-training the graph attention network (GATv2) model on the RAD dataset on the learning curves of different policy types.  The plot visually illustrates the relative convergence speed and performance levels achieved by these different policy approaches in this specific reinforcement learning task.", "section": "5 Experiments"}, {"figure_path": "6KDZHgrDhG/figures/figures_19_2.jpg", "caption": "Figure 2: Message passing illustration on the featurization of the cDFA given in Figure 1.", "description": "This figure illustrates the message passing mechanism used in the graph attention network (GATv2) to generate an embedding for a compositional deterministic finite automaton (cDFA).  The cDFA, representing a complex temporal task, is first featurized into a graph structure.  The nodes of this graph represent states and transitions within the cDFA, with edge features encoding the symbols triggering those transitions.  The GATv2 then performs a series of message-passing steps, where each node updates its feature vector based on information from its neighbors.  This iterative process allows the network to capture the structure and semantics of the cDFA, finally culminating in a single vector embedding that represents the entire cDFA.", "section": "3 Compositional Automata-Conditioned Reinforcement Learning"}, {"figure_path": "6KDZHgrDhG/figures/figures_20_1.jpg", "caption": "Figure 1: Given a (conjunctive) composition of deterministic finite automata (shown on the left), we construct its embedding using a graph neural network (GATv2) and use this embedding as a goal to condition the reinforcement learning policy.", "description": "This figure illustrates the architecture for goal-conditioned reinforcement learning using compositional deterministic finite automata (cDFAs).  A cDFA is represented as a composition of several DFAs which are shown on the left.  The cDFA is passed to a graph attention network (GATv2), which creates an embedding of the cDFA. This embedding then serves as a goal to condition the reinforcement learning (RL) policy network on the right. The architecture incorporates an AND operation to combine the individual DFAs, a convolutional neural network (CNN) to encode observations, and a policy network to output actions. This design addresses the challenge of representing and using temporal goals in RL effectively.", "section": "3 Compositional Automata-Conditioned Reinforcement Learning"}, {"figure_path": "6KDZHgrDhG/figures/figures_20_2.jpg", "caption": "Figure 2: Message passing illustration on the featurization of the cDFA given in Figure 1.", "description": "This figure illustrates the message passing mechanism used in the graph attention network (GATv2) to generate embeddings for compositional deterministic finite automata (cDFAs).  It shows how node features are updated through message passing steps, involving attention scores and linear transformations. The process transforms the cDFA graph structure into a vector representation that captures its essential properties for use in reinforcement learning.", "section": "3 Compositional Automata-Conditioned Reinforcement Learning"}, {"figure_path": "6KDZHgrDhG/figures/figures_21_1.jpg", "caption": "Figure 2: Message passing illustration on the featurization of the cDFA given in Figure 1.", "description": "This figure illustrates the message-passing mechanism used in the proposed architecture to featurize a compositional DFA (cDFA).  It shows how a graph neural network processes the cDFA's structure to generate an embedding that captures the temporal logic of the task.  The process involves adding new nodes for transitions, reversing edges, adding self-loops, and connecting initial states to an 'AND' node to represent the conjunctive nature of the cDFA.  The resulting embedding is then used as input to a policy network in a reinforcement learning setting.", "section": "3 Compositional Automata-Conditioned Reinforcement Learning"}, {"figure_path": "6KDZHgrDhG/figures/figures_21_2.jpg", "caption": "Figure 2: Message passing illustration on the featurization of the cDFA given in Figure 1.", "description": "This figure illustrates the message passing mechanism used in the graph attention network (GATv2) to generate an embedding for a compositional deterministic finite automaton (cDFA).  The cDFA, representing a temporal task, is first featurized into a graph structure.  The message passing process iteratively updates node features by aggregating information from neighboring nodes, weighted by attention scores. The final embedding of the cDFA is obtained from the 'AND' node, which aggregates information from all constituent DFAs. This embedding then serves as a goal representation for the reinforcement learning policy.", "section": "3 Compositional Automata-Conditioned Reinforcement Learning"}, {"figure_path": "6KDZHgrDhG/figures/figures_22_1.jpg", "caption": "Figure 9: Learning curves for pre-training on RAD cDFAs in the dummy MDP.", "description": "This figure shows the learning curves obtained during the pre-training phase of the experiment. Two different models, GATv2 and RGCN, are compared in terms of their performance on learning to encode RAD cDFAs.  The x-axis represents the number of frames (millions) during training, while the y-axis displays the discounted return achieved by each model. The figure visually demonstrates that GATv2 converges faster than RGCN on this pre-training task.", "section": "C Results"}, {"figure_path": "6KDZHgrDhG/figures/figures_22_2.jpg", "caption": "Figure 5: Training curves (error bars show a 90% confidence interval over 10 seeds) in Letterworld (discrete) and Zones (continuous) for policies trained on RAD CDFAs, showing that frozen pre-trained CDFA encoders perform better than non-frozen ones while no pre-training barely learns the tasks.", "description": "This figure shows the training curves for two different reinforcement learning environments, Letterworld (discrete states and actions) and Zones (continuous states and actions).  The curves compare the performance of policies trained using three different methods:  no pre-training, pre-training with a frozen encoder, and pre-training with an unfrozen encoder. The results indicate that pre-training significantly improves the learning process, and that freezing the encoder after pre-training yields even better results.  The error bars represent the 90% confidence interval across 10 different training runs, showing the stability and consistency of the results.", "section": "5 Experiments"}, {"figure_path": "6KDZHgrDhG/figures/figures_22_3.jpg", "caption": "Figure 2: Message passing illustration on the featurization of the cDFA given in Figure 1.", "description": "This figure illustrates the message-passing mechanism used in the proposed architecture for featurizing compositional deterministic finite automata (cDFAs).  The architecture uses a graph attention network (GATv2) to encode cDFAs for use in goal-conditioned reinforcement learning. The figure shows how the nodes (states and transitions) of individual DFAs and their conjunctions are represented as a graph, and then how the message-passing steps are used to compute a vector embedding that represents the cDFA as a whole. The process involves multiple steps of message passing between nodes, updating node features at each step.", "section": "3 Compositional Automata-Conditioned Reinforcement Learning"}]