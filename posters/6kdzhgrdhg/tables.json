[{"figure_path": "6KDZHgrDhG/tables/tables_14_1.jpg", "caption": "Table 1: PPO pre-training and policy training hyperparameters.", "description": "This table lists the hyperparameters used for both pre-training the cDFA encoder and training the reinforcement learning policies.  It includes settings for learning rate, batch size, number of epochs, discount factor, entropy coefficient, generalized advantage estimation (GAE), clipping parameter, RMSprop alpha, maximum gradient norm, and value loss coefficient.  Separate hyperparameters are provided for pre-training in a dummy environment and for training in the Letterworld and Zones environments.", "section": "D Hyperparameters and Compute Usage"}, {"figure_path": "6KDZHgrDhG/tables/tables_23_1.jpg", "caption": "Table 1: PPO pre-training and policy training hyperparameters.", "description": "This table lists the hyperparameters used for Proximal Policy Optimization (PPO) in the pre-training and policy training phases of the experiments.  It shows values for various parameters such as learning rate, batch size, number of epochs, discount factor, entropy coefficient, Generalized Advantage Estimation (GAE) lambda, clipping epsilon, RMSprop alpha, maximum gradient norm, and value loss coefficient.  Separate values are provided for the pre-training environment, the Letterworld environment, and the Zones environment.", "section": "D Hyperparameters and Compute Usage"}]