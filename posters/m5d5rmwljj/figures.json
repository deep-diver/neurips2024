[{"figure_path": "M5D5rMwLjj/figures/figures_2_1.jpg", "caption": "Figure 1: The overall framework of Meta-Controller. First, the states and actions of various robot embodiments are tokenized into joint-level representations. The state tokens are then encoded by the state encoder to capture knowledge about the embodiments. Finally, a matching-based policy network uses few-shot demonstrations with the encoded state features to predict per-joint actions.", "description": "This figure illustrates the architecture of the Meta-Controller framework.  The process begins with the robot's states and actions, which are converted into joint-level representations.  These are then fed into a state encoder, which extracts features that capture both shared and embodiment-specific knowledge. This information is used by a matching-based policy network, which learns to predict the robot's actions using a small number of demonstrations. The output of the network are actions for each joint.", "section": "3 Method"}, {"figure_path": "M5D5rMwLjj/figures/figures_3_1.jpg", "caption": "Figure 2: The state encoder f consists of two component transformers. Joint-level state tokens are first encoded by the structure encoder fs along the joint axis, where the positional embedding and a part of backbone parameters adapt the model to each embodiment. The features are then passed to the motion encoder fm, which computes causal attentions of per-joint features along the temporal axis, where a part of backbone parameters adapt the model both to the embodiment and task.", "description": "This figure shows the architecture of the state encoder, a crucial component of the Meta-Controller framework.  The state encoder takes joint-level state tokens as input and processes them through two transformer networks: a structure encoder (fs) and a motion encoder (fm). The structure encoder captures the relationships between joints within an embodiment, adapting to specific morphologies via positional embeddings and embodiment-specific parameters.  The output of the structure encoder is then fed into the motion encoder, which considers the temporal dynamics of joint movements. The motion encoder utilizes causal self-attention to process the sequential data and includes parameters specific to both the embodiment and task, further enhancing adaptability. The final output (mt) represents the encoded state features, incorporating both structural and dynamic information.", "section": "3.2.1 State Encoder for Embodiment Generalization"}, {"figure_path": "M5D5rMwLjj/figures/figures_4_1.jpg", "caption": "Figure 3: An illustration of the matching-based policy network \u03c0. (a) Each state and action token in few-shot demonstrations is encoded by the corresponding encoders f and g, where we use the same encoder f used for the current state. A matching module & then computes the weighted sum of action features based on the joint-wise similarity between state features. Finally, an action decoder h decodes the joint-wise matching output to predict the current action. (b) Both the action encoder g and decoder h are causal transformers operating along the temporal axis of action tokens and features.", "description": "This figure illustrates the Meta-Controller's matching-based policy network.  It shows how the network uses joint-level representations of states and actions from few-shot demonstrations to predict actions for a new, unseen task. The process involves encoding states and actions using encoders (f and g), calculating joint-wise similarity between the current state and demonstrations, weighting demonstration actions based on this similarity, and finally decoding the weighted sum to produce the predicted action.", "section": "3.2.2 Few-shot Policy Adaptation for Task Generalization"}, {"figure_path": "M5D5rMwLjj/figures/figures_4_2.jpg", "caption": "Figure 3: An illustration of the matching-based policy network \u03c0. (a) Each state and action token in few-shot demonstrations is encoded by the corresponding encoders f and g, where we use the same encoder f used for the current state. A matching module & then computes the weighted sum of action features based on the joint-wise similarity between state features. Finally, an action decoder h decodes the joint-wise matching output to predict the current action. (b) Both the action encoder g and decoder h are causal transformers operating along the temporal axis of action tokens and features.", "description": "This figure illustrates the architecture of the matching-based policy network (\u03c0) used in the Meta-Controller framework.  It shows how the network takes joint-level state and action tokens (from both the current state and demonstrations) as input, processes them through encoders and decoders (f, g, h), and employs a matching module (\u03c3) to combine information for action prediction. The encoders and decoders are causal transformers operating on temporal sequences.", "section": "3.2.2 Few-shot Policy Adaptation for Task Generalization"}, {"figure_path": "M5D5rMwLjj/figures/figures_8_1.jpg", "caption": "Figure 1: The overall framework of Meta-Controller. First, the states and actions of various robot embodiments are tokenized into joint-level representations. The state tokens are then encoded by the state encoder to capture knowledge about the embodiments. Finally, a matching-based policy network uses few-shot demonstrations with the encoded state features to predict per-joint actions.", "description": "This figure illustrates the architecture of the Meta-Controller framework. It begins by tokenizing the states and actions of different robots into a joint-level representation. These tokens are then input to a state encoder which extracts features to represent the robot's embodiment.  Finally, a matching-based policy network uses this information, along with a small number of demonstrations, to predict the actions at the joint level.  This allows for generalization to unseen embodiments and tasks.", "section": "3 Method"}, {"figure_path": "M5D5rMwLjj/figures/figures_8_2.jpg", "caption": "Figure 5: Ablation study on the number of demonstrations. We plot the normalized scores for each pair of embodiment and task (E, T) and their average, varying the number of shots as 5, 10, and 20.", "description": "This ablation study evaluates the impact of the number of demonstrations on the model's performance.  The figure displays normalized scores for various continuous control tasks across different robot embodiments.  The number of demonstrations used (5, 10, and 20) is varied to show how performance improves with more data.  The average performance across all tasks and embodiments is also shown.", "section": "5.3 Ablation Studies"}, {"figure_path": "M5D5rMwLjj/figures/figures_17_1.jpg", "caption": "Figure 1: The overall framework of Meta-Controller. First, the states and actions of various robot embodiments are tokenized into joint-level representations. The state tokens are then encoded by the state encoder to capture knowledge about the embodiments. Finally, a matching-based policy network uses few-shot demonstrations with the encoded state features to predict per-joint actions.", "description": "This figure illustrates the architecture of the Meta-Controller model. The input consists of states and actions from various robot embodiments.  These are tokenized into joint-level representations before being processed by a state encoder which identifies embodiment-specific knowledge and shared knowledge. A matching-based policy network then generates actions based on the encoded states and a small number of demonstrations.", "section": "3 Method"}, {"figure_path": "M5D5rMwLjj/figures/figures_17_2.jpg", "caption": "Figure 1: The overall framework of Meta-Controller. First, the states and actions of various robot embodiments are tokenized into joint-level representations. The state tokens are then encoded by the state encoder to capture knowledge about the embodiments. Finally, a matching-based policy network uses few-shot demonstrations with the encoded state features to predict per-joint actions.", "description": "This figure illustrates the architecture of the Meta-Controller model.  It shows how the input states and actions from diverse robotic embodiments are first converted into joint-level representations. These representations are then fed into a state encoder, which extracts features capturing both shared physics and embodiment-specific characteristics. Finally, a matching-based policy network uses a few demonstrations to predict appropriate actions for new tasks and unseen embodiments.", "section": "3 Method"}, {"figure_path": "M5D5rMwLjj/figures/figures_19_1.jpg", "caption": "Figure 1: The overall framework of Meta-Controller. First, the states and actions of various robot embodiments are tokenized into joint-level representations. The state tokens are then encoded by the state encoder to capture knowledge about the embodiments. Finally, a matching-based policy network uses few-shot demonstrations with the encoded state features to predict per-joint actions.", "description": "This figure shows a schematic of the Meta-Controller framework. It illustrates how the states and actions of robots are tokenized into joint-level representations, which are then encoded by a state encoder to capture both embodiment-specific and shared knowledge. The encoded states, along with few-shot demonstrations, are used by a matching-based policy network to predict actions at the joint level. This approach allows for generalization to unseen embodiments and tasks.", "section": "3 Method"}, {"figure_path": "M5D5rMwLjj/figures/figures_19_2.jpg", "caption": "Figure 9: Cumulative rewards of the failure cases in Figure 8.", "description": "This figure shows the cumulative rewards over time for the failure cases illustrated in Figure 8. It provides a visual representation of the agent's performance in scenarios where it struggled initially but eventually achieved success. The plot helps in analyzing the agent's learning curve and identifying potential areas for improvement.", "section": "C.7 Failure Case Analysis"}, {"figure_path": "M5D5rMwLjj/figures/figures_19_3.jpg", "caption": "Figure 1: The overall framework of Meta-Controller. First, the states and actions of various robot embodiments are tokenized into joint-level representations. The state tokens are then encoded by the state encoder to capture knowledge about the embodiments. Finally, a matching-based policy network uses few-shot demonstrations with the encoded state features to predict per-joint actions.", "description": "This figure shows a schematic of the Meta-Controller framework.  It starts by tokenizing the states and actions from various robot embodiments into a joint-level representation. These tokens are fed into a state encoder, which extracts features capturing both embodiment-specific and shared knowledge.  Finally, a matching-based policy network predicts actions based on these encoded features and a few demonstration examples.", "section": "3 Method"}, {"figure_path": "M5D5rMwLjj/figures/figures_22_1.jpg", "caption": "Figure 1: The overall framework of Meta-Controller. First, the states and actions of various robot embodiments are tokenized into joint-level representations. The state tokens are then encoded by the state encoder to capture knowledge about the embodiments. Finally, a matching-based policy network uses few-shot demonstrations with the encoded state features to predict per-joint actions.", "description": "This figure illustrates the architecture of the Meta-Controller framework. It shows how the states and actions of robots are tokenized at the joint level, then encoded to capture both shared and embodiment-specific knowledge, and finally used by a matching-based policy network to predict actions from a few demonstrations.", "section": "3 Method"}, {"figure_path": "M5D5rMwLjj/figures/figures_23_1.jpg", "caption": "Figure 1: The overall framework of Meta-Controller. First, the states and actions of various robot embodiments are tokenized into joint-level representations. The state tokens are then encoded by the state encoder to capture knowledge about the embodiments. Finally, a matching-based policy network uses few-shot demonstrations with the encoded state features to predict per-joint actions.", "description": "This figure illustrates the architecture of the Meta-Controller, a framework designed for few-shot imitation learning. It takes as input the states and actions of a robot, which are first converted into joint-level representations. A state encoder processes these representations to capture both shared and embodiment-specific knowledge, which is then used by a matching-based policy network to predict actions based on a small number of demonstrations. This modular approach enables the framework to generalize effectively to unseen embodiments and tasks.", "section": "3 Method"}, {"figure_path": "M5D5rMwLjj/figures/figures_24_1.jpg", "caption": "Figure 1: The overall framework of Meta-Controller. First, the states and actions of various robot embodiments are tokenized into joint-level representations. The state tokens are then encoded by the state encoder to capture knowledge about the embodiments. Finally, a matching-based policy network uses few-shot demonstrations with the encoded state features to predict per-joint actions.", "description": "This figure illustrates the overall architecture of the Meta-Controller, a novel framework for few-shot imitation learning.  The framework begins by representing robot states and actions at the joint level, creating a unified representation regardless of the robot's specific morphology. A state encoder then processes these joint-level states, capturing both general physical principles and robot-specific characteristics. Finally, a matching-based policy network utilizes a small number of demonstrations to predict actions based on these encoded states. This allows for generalization to unseen embodiments and tasks.", "section": "3 Method"}, {"figure_path": "M5D5rMwLjj/figures/figures_25_1.jpg", "caption": "Figure 1: The overall framework of Meta-Controller. First, the states and actions of various robot embodiments are tokenized into joint-level representations. The state tokens are then encoded by the state encoder to capture knowledge about the embodiments. Finally, a matching-based policy network uses few-shot demonstrations with the encoded state features to predict per-joint actions.", "description": "This figure shows the architecture of the Meta-Controller, a few-shot behavior cloning framework. It consists of three main components: joint-level tokenization of states and actions, a state encoder to extract features that capture knowledge about the robot embodiments, and a matching-based policy network that predicts actions based on a few demonstrations. The framework aims to generalize to unseen robot embodiments and tasks using a few demonstrations.", "section": "3 Method"}, {"figure_path": "M5D5rMwLjj/figures/figures_26_1.jpg", "caption": "Figure 1: The overall framework of Meta-Controller. First, the states and actions of various robot embodiments are tokenized into joint-level representations. The state tokens are then encoded by the state encoder to capture knowledge about the embodiments. Finally, a matching-based policy network uses few-shot demonstrations with the encoded state features to predict per-joint actions.", "description": "This figure illustrates the overall architecture of the Meta-Controller model. It shows how the model processes the input (states and actions of a robot) through tokenization, encoding, and a matching-based policy network to produce output (actions).  The joint-level representation and state encoder are highlighted as key components for generalization across different robot embodiments.", "section": "3 Method"}, {"figure_path": "M5D5rMwLjj/figures/figures_27_1.jpg", "caption": "Figure 1: The overall framework of Meta-Controller. First, the states and actions of various robot embodiments are tokenized into joint-level representations. The state tokens are then encoded by the state encoder to capture knowledge about the embodiments. Finally, a matching-based policy network uses few-shot demonstrations with the encoded state features to predict per-joint actions.", "description": "This figure illustrates the architecture of the Meta-Controller, a novel framework for few-shot behavior cloning that generalizes to unseen embodiments and tasks.  It shows a three-stage process: First, the states and actions of different robots are converted into a unified joint-level representation. Then, a state encoder processes this representation to extract features capturing both embodiment-specific and shared knowledge. Finally, a matching-based policy network utilizes these features and a small number of demonstrations to predict the robot's actions in a new, unseen situation.", "section": "3 Method"}, {"figure_path": "M5D5rMwLjj/figures/figures_28_1.jpg", "caption": "Figure 1: The overall framework of Meta-Controller. First, the states and actions of various robot embodiments are tokenized into joint-level representations. The state tokens are then encoded by the state encoder to capture knowledge about the embodiments. Finally, a matching-based policy network uses few-shot demonstrations with the encoded state features to predict per-joint actions.", "description": "This figure illustrates the architecture of the Meta-Controller model.  The model takes in joint-level state and action representations as inputs. A state encoder processes the state information to capture both shared and embodiment-specific knowledge. A matching-based policy network then uses few-shot demonstrations to predict the actions at the joint level.", "section": "3 Method"}, {"figure_path": "M5D5rMwLjj/figures/figures_29_1.jpg", "caption": "Figure 1: The overall framework of Meta-Controller. First, the states and actions of various robot embodiments are tokenized into joint-level representations. The state tokens are then encoded by the state encoder to capture knowledge about the embodiments. Finally, a matching-based policy network uses few-shot demonstrations with the encoded state features to predict per-joint actions.", "description": "This figure shows a schematic of the Meta-Controller framework, illustrating the process of how it handles different robot embodiments and tasks for few-shot imitation learning.  First, the robot's state and actions are broken down into joint-level representations which are then encoded to capture both shared and embodiment-specific knowledge.  A matching-based policy network then uses this information to predict actions based on a few demonstrations.", "section": "3 Method"}]