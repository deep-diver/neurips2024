[{"figure_path": "pCVxYw6FKg/tables/tables_6_1.jpg", "caption": "Table 1: Test loss interpolation barriers at midpoint: L(2\u03b8\u2081+\u03b8\u2082)-(L(\u03b8\u2081)+L(\u03b8\u2082)). We use different methods of breaking symmetries in each column; from left to right: no symmetry breaking, Git-Rebasin [1], our \u03c3-Asym approach, and our W-Asym approach. We report mean and standard deviation of the barrier across at least 5 pairs of networks, and bold lowest barriers.", "description": "This table presents the results of an experiment measuring the test loss interpolation barriers at the midpoint of linear interpolation between two trained neural networks.  The experiment compares four different methods: standard networks, networks aligned using Git-Rebasin, \u03c3-Asymmetric networks, and W-Asymmetric networks.  Each method's performance is evaluated across multiple network architectures and datasets, reporting the mean and standard deviation of the loss barrier.  Lower values indicate better interpolation performance, suggesting a more linearly-connected loss landscape.", "section": "5.1 Linear Mode Connectivity without Permutation Alignment"}, {"figure_path": "pCVxYw6FKg/tables/tables_6_2.jpg", "caption": "Table 2: Bayesian neural network results. Reported loss is the negative log likelihood loss. All results (except for last column) are after 50 epochs of training. W-Asymmetric networks tend to improve over their standard counterparts, especially early in training. 16-layer MLPs fail to train, but 16-layer W-Asymmetric MLPs successfully train. Standard or Asymmetric networks better than their counterpart by a standard deviation are bolded.", "description": "This table presents the results of experiments comparing standard and asymmetric Bayesian neural networks on various datasets and architectures.  It shows that W-Asymmetric networks generally achieve lower training loss, lower expected calibration error (ECE), and higher test accuracy, particularly noticeable in the early training stages.  The table also highlights that 16-layer standard MLPs fail to train, while their W-Asymmetric counterparts succeed.  Results that are statistically significantly better than the corresponding standard models are bolded.", "section": "5.2 Bayesian Neural Networks"}, {"figure_path": "pCVxYw6FKg/tables/tables_7_1.jpg", "caption": "Table 3: Metanetwork performance for predicting the test accuracy of small ResNets and our W-Asym ResNets. Each row is a different metanetwork. Reported are R2 and Kendall 7 on the test set - higher is better.", "description": "This table presents the performance of four different metanetworks in predicting the test accuracy of two types of ResNet models: standard ResNets and W-Asymmetric ResNets.  The metanetworks used are MLP, DMC, DeepSets, and StatNN. The R-squared (R2) and Kendall's tau (\u03c4) correlation coefficients are reported, indicating the strength of the correlation between the metanetwork's prediction and the actual test accuracy. Higher values for both metrics are better, implying stronger predictive power.", "section": "5.3 Metanetworks"}, {"figure_path": "pCVxYw6FKg/tables/tables_8_1.jpg", "caption": "Table 4: Monotonic linear interpolation: properties of linear interpolations between 300 pairs of initialization and trained parameters. Arrows denote behavior that is more similar to convex optimization, e.g. there is a downarrow (\u2193) next to A because convex objectives have nonpositive \u2206, while nonconvex can have positive \u0394. For both types of Asymmetric networks, all differences from Standard ResNets are statistically significant (p < .001) under a two-sided T-test: Asymmetric networks have significantly more monotonic and convex linear interpolations from initialization.", "description": "This table presents the results of an experiment evaluating monotonic linear interpolation (MLI) in different neural network architectures.  Three types of networks are compared: standard ResNets, \u03c3-Asymmetric ResNets, and W-Asymmetric ResNets. The table shows the mean and standard deviation of four key metrics across 300 pairs of networks: the maximum increase in loss (\u0394), the percentage of monotonic interpolations, local convexity, and global convexity.  The results demonstrate that Asymmetric networks exhibit significantly better MLI properties, indicating smoother and more convex loss landscapes compared to standard ResNets.", "section": "5.4 Monotonic Linear Interpolation"}, {"figure_path": "pCVxYw6FKg/tables/tables_23_1.jpg", "caption": "Table 1: Test loss interpolation barriers at midpoint: L(201+02)-(L(01) + L(02)) . We use different methods of breaking symmetries in each column; from left to right: no symmetry breaking, Git-Rebasin [1], our \u03c3-Asym approach, and our W-Asym approach. We report mean and standard deviation of the barrier across at least 5 pairs of networks, and bold lowest barriers.", "description": "This table shows the results of an experiment measuring the test loss interpolation barrier at the midpoint for different methods of breaking parameter symmetries in neural networks.  The experiment compares standard networks, networks aligned using Git-Rebasin, \u03c3-Asymmetric networks, and W-Asymmetric networks.  The mean and standard deviation of the loss barrier are reported for each method across multiple pairs of networks, highlighting the lowest barrier values.", "section": "5.1 Linear Mode Connectivity without Permutation Alignment"}, {"figure_path": "pCVxYw6FKg/tables/tables_25_1.jpg", "caption": "Table 5: Number of learnable parameters of Standard / \u03c3-Asym and W-Asym nets for our experiments.", "description": "This table presents the number of trainable parameters for different neural network architectures used in the experiments.  It compares the number of learnable parameters in standard networks to those in the W-Asymmetric and \u03c3-Asymmetric versions. The reduction in parameters in the asymmetric networks is a result of fixing certain parameters to break parameter symmetries.", "section": "5 Experiments"}, {"figure_path": "pCVxYw6FKg/tables/tables_25_2.jpg", "caption": "Table 3: Metanetwork performance for predicting the test accuracy of small ResNets and our W-Asym ResNets. Each row is a different metanetwork. Reported are R2 and Kendall 7 on the test set - higher is better.", "description": "This table presents the performance of several metanetworks in predicting the test accuracy of small ResNets and W-Asymmetric ResNets.  The metanetworks tested include simple MLPs, DMC, DeepSets, and StatNN.  The R-squared (R2) and Kendall's tau (\u03c4) correlation coefficients are reported, with higher values indicating better predictive performance. The table aims to demonstrate that W-Asymmetric ResNets are easier for metanetworks to accurately predict performance for, likely due to the removal of parameter symmetries.", "section": "5.3 Metanetworks"}, {"figure_path": "pCVxYw6FKg/tables/tables_26_1.jpg", "caption": "Table 7: Bayesian NN test accuracy after 25 epochs. Decreasing standard ResNet20 parameters to match that of W-Asym ResNet20 does not substantially change performance.", "description": "This table presents the results of Bayesian Neural Network (BNN) experiments on CIFAR-10, comparing the test accuracy of W-Asymmetric ResNet20, Standard ResNet20, and a Smaller Standard ResNet20 (with the number of parameters reduced to match W-Asym ResNet20). The goal is to investigate whether the improved performance of the Asymmetric networks in Bayesian setting is due to the reduced number of parameters or due to the removal of parameter symmetries. The table shows that the test accuracy of W-Asymmetric ResNet20 remains higher even after adjusting the number of parameters in the standard ResNet20 model, suggesting that the removal of parameter symmetries plays a significant role in the improved performance.", "section": "5.2 Bayesian Neural Networks"}, {"figure_path": "pCVxYw6FKg/tables/tables_26_2.jpg", "caption": "Table 8: Test loss barrier when changing warmup steps. Results are very similar when lowering number of warmup epochs (W-Asym interpolates significantly better than Git-ReBasin). Adam optimizer with learning rate 1e-2 (ResNet20) and 1e-3 (GNN) is used.", "description": "This table presents the results of an experiment measuring the test loss barrier for linear interpolation between two independently trained networks (ResNet20 on CIFAR-10 and GNN on ogbn-arXiv).  The experiment varies the number of warmup epochs used during training (1 versus 20). The table compares the test loss barrier for standard networks, networks aligned using the Git-Rebasin method, \u03c3-Asymmetric networks, and W-Asymmetric networks.  The results show that W-Asymmetric networks consistently exhibit lower test loss barriers, indicating better interpolation performance, regardless of the number of warmup steps.", "section": "5.1 Linear Mode Connectivity without Permutation Alignment"}, {"figure_path": "pCVxYw6FKg/tables/tables_27_1.jpg", "caption": "Table 1: Test loss interpolation barriers at midpoint: L(2\u03b8\u2081+\u03b8\u2082)-(L(\u03b8\u2081)+L(\u03b8\u2082)). We use different methods of breaking symmetries in each column; from left to right: no symmetry breaking, Git-Rebasin [1], our \u03c3-Asym approach, and our W-Asym approach. We report mean and standard deviation of the barrier across at least 5 pairs of networks, and bold lowest barriers.", "description": "This table presents the results of an experiment comparing the test loss interpolation barriers of four different methods for breaking symmetries in neural networks. The methods are: no symmetry breaking, Git-Rebasin, \u03c3-Asymmetric approach, and W-Asymmetric approach. The table shows the mean and standard deviation of the test loss interpolation barrier for each method across at least 5 pairs of networks. The lowest barriers are shown in bold.", "section": "5.1 Linear Mode Connectivity without Permutation Alignment"}, {"figure_path": "pCVxYw6FKg/tables/tables_27_2.jpg", "caption": "Table 10: W-Asymmetric network hyperparameters for depth 4 MLPs. nfix refers to the number of weights we randomly fix per neuron. \u03ba refers to the standard deviation of the normal distribution that the fixed entries F are drawn from.", "description": "This table shows the hyperparameters used for the four-layer Multilayer Perceptron (MLP) experiments in the paper.  Specifically, it details the number of randomly fixed weights (nfix) and the standard deviation (\u03ba) of the normal distribution from which the fixed weights (F) are sampled for each of the four linear layers in the MLP.  The values of nfix and \u03ba influence the degree of asymmetry introduced into the network architecture.", "section": "5.1 Linear Mode Connectivity without Permutation Alignment"}, {"figure_path": "pCVxYw6FKg/tables/tables_27_3.jpg", "caption": "Table 1: Test loss interpolation barriers at midpoint: L(201+02)-(L(01) + L(02)) . We use different methods of breaking symmetries in each column; from left to right: no symmetry breaking, Git-Rebasin [1], our \u03c3-Asym approach, and our W-Asym approach. We report mean and standard deviation of the barrier across at least 5 pairs of networks, and bold lowest barriers.", "description": "This table presents the results of an experiment measuring the test loss interpolation barriers at the midpoint for four different methods: no symmetry breaking, Git-Rebasin, \u03c3-Asymmetric, and W-Asymmetric.  The mean and standard deviation are reported for each method across at least 5 pairs of networks.  The lowest barriers for each method are highlighted in bold. The table aims to show how different methods of removing parameter symmetries in neural networks affect the performance of linear interpolation between trained networks.", "section": "5.1 Linear Mode Connectivity without Permutation Alignment"}, {"figure_path": "pCVxYw6FKg/tables/tables_27_4.jpg", "caption": "Table 12: W-Asymmetric network hyperparameters for ResNet20s with width multiplier 8 on CIFAR-10. We use 3 times more fixed entries per output channel or neuron than for Table 11.", "description": "This table shows the hyperparameters used for the W-Asymmetric ResNet20 model with a width multiplier of 8, trained on the CIFAR-10 dataset.  It specifies the number of fixed entries (nfix) and the standard deviation (\u03ba) of the normal distribution from which the fixed entries are sampled for each block of the network (consisting of convolutional and skip layers).  These hyperparameters control the level of asymmetry introduced into the network.", "section": "5.2 Bayesian Neural Networks"}, {"figure_path": "pCVxYw6FKg/tables/tables_28_1.jpg", "caption": "Table 2: Bayesian neural network results. Reported loss is the negative log likelihood loss. All results (except for last column) are after 50 epochs of training. W-Asymmetric networks tend to improve over their standard counterparts, especially early in training. 16-layer MLPs fail to train, but 16-layer W-Asymmetric MLPs successfully train. Standard or Asymmetric networks better than their counterpart by a standard deviation are bolded.", "description": "This table presents the results of Bayesian neural network experiments using both standard and asymmetric network architectures.  It compares training and test loss, expected calibration error (ECE), and test accuracy across various network depths and configurations (MLPs and ResNets).  The results show that W-Asymmetric networks often achieve better performance, particularly faster training convergence and improved accuracy.", "section": "5.2 Bayesian Neural Networks"}, {"figure_path": "pCVxYw6FKg/tables/tables_28_2.jpg", "caption": "Table 3: Metanetwork performance for predicting the test accuracy of small ResNets and our W-Asym ResNets. Each row is a different metanetwork. Reported are R2 and Kendall \u03c4 on the test set - higher is better.", "description": "This table presents the performance of four different metanetworks in predicting the test accuracy of two types of ResNet models: standard ResNets and W-Asymmetric ResNets.  The metanetworks used are MLP, DMC, DeepSets, and StatNN.  The R-squared (R2) and Kendall's Tau (\u03c4) correlation coefficients are reported for each metanetwork and model type, providing a measure of how well each metanetwork predicts the test accuracy.", "section": "5.3 Metanetworks"}]