[{"figure_path": "QB6CvDqa6b/figures/figures_6_1.jpg", "caption": "Figure 8: Average utility and Hypervolume performance of all algorithms on D4MORL Expert datasets.", "description": "This figure presents the average utility and hypervolume achieved by different algorithms on the D4MORL Expert datasets.  The algorithms include PDOA using MODF and MORVS, Prompt-MODT, BC-Finetune, and the oracle performance (original MODF and MORVS). The results are shown for five different MuJoCo tasks: Ant, Swimmer, HalfCheetah, Hopper, and Walker2d. Each bar represents the average performance across multiple runs, and error bars indicate the standard deviation. The dashed lines represent the oracle performance, serving as the best achievable results given full knowledge of the preferences.", "section": "5.1 Preference Alignment for MORL Tasks"}, {"figure_path": "QB6CvDqa6b/figures/figures_6_2.jpg", "caption": "Figure 2: Pareto fronts of different algorithms on D4MORL Amateur datasets. Each point represents an adapted policy for a specific unknown target preference.", "description": "This figure compares the performance of several multi-objective reinforcement learning (MORL) algorithms on the D4MORL Amateur datasets. Each point on the Pareto front represents a different policy that has been adapted to a specific, unknown target preference. The Pareto front shows the trade-off between two objectives. A larger and more expansive Pareto front indicates better performance, as it shows that the algorithm can find a wider range of policies that achieve a good balance between the two objectives. The figure suggests that the proposed PDOA method outperforms other baselines in terms of achieving a wider range of policies with better balance between the objectives.", "section": "5.1 Preference Alignment for MORL Tasks"}, {"figure_path": "QB6CvDqa6b/figures/figures_7_1.jpg", "caption": "Figure 1: Results on D4MORL Amateur datasets. Higher average utility and Hypervolume are preferable. The dashed lines represent the best performance between the original MODF and MORVS.", "description": "This figure compares the performance of different algorithms (PDOA [MODF], PDOA [MORVS], Prompt-MODT, BC-Finetune) on the D4MORL Amateur datasets across five different multi-objective reinforcement learning tasks.  The y-axis represents the average utility and hypervolume, while the x-axis represents the different tasks. Higher values in both metrics indicate better performance. Dashed lines represent the best performance achieved by either MODF or MORVS (original, non-adapted methods) on each task. This figure visually demonstrates the superior performance of the proposed PDOA framework compared to the baselines.", "section": "5.1 Preference Alignment for MORL Tasks"}, {"figure_path": "QB6CvDqa6b/figures/figures_7_2.jpg", "caption": "Figure 4: The adapted policies\u2019 cost and utility of each algorithm under various safety thresholds. Here, the utility is the normalized reward, since there is only one unconstrained objective in DSRL tasks. The points above the black dashed line represent the policies that violate the constraints.", "description": "This figure compares the performance of different algorithms on safe RL tasks under various safety thresholds.  The x-axis represents the safety threshold, and the y-axis shows both the normalized cost (top row) and utility (bottom row) of the policies generated by each algorithm.  The black dashed line indicates the safety threshold; points above this line represent policies that violate the safety constraints. The plot shows how well each algorithm balances utility maximization with constraint satisfaction across different safety thresholds. The algorithms compared include PDOA (using MODF and MORVS), Prompt-MODT, BC-Finetune, and CDT (an oracle baseline that knows the thresholds).", "section": "5.2 Constraint Satisfaction for Safe RL Tasks"}, {"figure_path": "QB6CvDqa6b/figures/figures_8_1.jpg", "caption": "Figure 5: The maximum cost, the average utility and Hypervolume over all targets with a specific safety threshold.", "description": "This figure shows the performance of different algorithms on constrained multi-objective reinforcement learning tasks.  Specifically, it presents the maximum cost incurred, the average utility achieved, and the hypervolume covered by the Pareto front for various safety thresholds.  The results illustrate the ability of the proposed PDOA framework to maintain a balance between maximizing utility and satisfying safety constraints, particularly when comparing it to baselines that do not consider safety during training.", "section": "5.3 Evaluation for Constrained MORL Tasks"}, {"figure_path": "QB6CvDqa6b/figures/figures_8_2.jpg", "caption": "Figure 6: The performance across different demonstration sizes (abbr. DS) in Figure (a)(b) and the performance under various conservatism parameters (abbr. CVaR) in Figure (c)(d).", "description": "This figure presents ablation study results on the impact of demonstration size and conservatism parameter on the performance of PDOA[MODF].  Figures (a) and (b) show that increasing the demonstration size leads to better average utility and hypervolume in MORL tasks and higher reward in safe RL tasks.  Even with small demonstration sizes, the method shows sufficient diversity and safety. Figures (c) and (d) demonstrate that increasing the conservatism parameter reduces constraint violations and improves safety, validating the conservatism mechanism.", "section": "5.4 Ablation Study"}, {"figure_path": "QB6CvDqa6b/figures/figures_14_1.jpg", "caption": "Figure 7: The reward vector distribution of CMO datasets under various safety thresholds.", "description": "This figure shows the reward vector distributions for six different constrained multi-objective reinforcement learning (CMORL) tasks under different safety thresholds.  Each subfigure represents a specific task and cost range. The x and y axes show the rewards for two objectives, while the points are colored according to the behavioral preference used to generate the data.  The figure illustrates how the reward distributions change with different cost constraints and preferences.", "section": "A.2 Environments and Datasets Details"}, {"figure_path": "QB6CvDqa6b/figures/figures_16_1.jpg", "caption": "Figure 1: Results on D4MORL Amateur datasets. Higher average utility and Hypervolume are preferable. The dashed lines represent the best performance between the original MODF and MORVS.", "description": "This figure presents the results of five different algorithms (PDOA [MODF], PDOA [MORVS], Prompt-MODT, BC-Finetune, and the oracle performance from MODF and MORVS) on five multi-objective reinforcement learning tasks from the D4MORL Amateur dataset.  The performance is measured by two metrics: Average Utility and Hypervolume.  Higher values for both metrics indicate better performance. The dashed lines show the best performance achieved by the original MODF and MORVS algorithms, serving as a benchmark against which the other algorithms are compared.", "section": "5.1 Preference Alignment for MORL Tasks"}, {"figure_path": "QB6CvDqa6b/figures/figures_16_2.jpg", "caption": "Figure 9: Pareto fronts of different algorithms on D4MORL Expert datasets. Each point represents an adapted policy for a specific unknown target preference.", "description": "This figure compares the performance of different algorithms in multi-objective reinforcement learning using the D4MORL Expert dataset. Each point on the Pareto front represents a different policy adapted to a specific, unknown target preference. The algorithms compared include PDOA (using MODF and MORVS), Prompt-MODT, and BC-Finetune. The figure helps visualize how well each algorithm balances multiple objectives and achieves diversity in the resulting policies.", "section": "5.1 Preference Alignment for MORL Tasks"}, {"figure_path": "QB6CvDqa6b/figures/figures_17_1.jpg", "caption": "Figure 10: The comparison between real target preferences and adapted preferences on D4MORL Expert datasets.", "description": "This figure compares the real target preferences against the adapted preferences obtained using the PDOA (Preference Distribution Offline Adaptation) framework on the D4MORL Expert datasets.  Each subplot represents a different MuJoCo task (Ant, Swimmer, HalfCheetah, Hopper, Walker2d).  The x-axis shows the real target preference on objective 1, while the y-axis displays the adapted preference on objective 1, as estimated by the PDOA method.  The lines represent the results using two different offline MORL algorithms (MODF and MORvS) within the PDOA framework. The diagonal dashed line indicates a perfect match between real and adapted preferences. Deviations from this line illustrate the accuracy of the PDOA method in aligning adapted preferences with the ground truth.", "section": "5.1 Preference Alignment for MORL Tasks"}, {"figure_path": "QB6CvDqa6b/figures/figures_17_2.jpg", "caption": "Figure 11: The normalized cost and normalized reward of policies with various preferences obtained by our framework.", "description": "This figure displays the performance of the PDOA framework on four different safe RL tasks.  For each task, the x-axis represents the normalized cost, while the y-axis represents the normalized reward. Each point represents a policy generated by the PDOA [MODF] and PDOA [MORVS] algorithms, with different preferences. The plot shows the relationship between the cost and reward for policies with varying preferences, demonstrating the capability of the framework to generate policies that achieve different trade-offs between cost and reward while satisfying the constraints.", "section": "5.2 Constraint Satisfaction for Safe RL Tasks"}, {"figure_path": "QB6CvDqa6b/figures/figures_17_3.jpg", "caption": "Figure 13: The adapted policies' normalized cost and normalized reward of all algorithms under various safety thresholds on additional tasks.", "description": "This figure compares the performance of several algorithms (PDOA[MODF], PDOA[MORVS], Prompt-MODT, BC-Finetune, and CDT Oracle) on four additional safe reinforcement learning tasks (OfflineCarCircle-v0, OfflineDroneCircle-v0, OfflineCarRun-v0, OfflineDroneRun-v0) under different safety thresholds.  The plot shows the normalized cost versus normalized reward for each algorithm and threshold.  It illustrates the trade-off between safety (low cost) and performance (high reward). The goal is to find policies that satisfy the safety constraints while maximizing reward.", "section": "5.2 Constraint Satisfaction for Safe RL Tasks"}, {"figure_path": "QB6CvDqa6b/figures/figures_18_1.jpg", "caption": "Figure 4: The adapted policies\u2019 cost and utility of each algorithm under various safety thresholds. Here, the utility is the normalized reward, since there is only one unconstrained objective in DSRL tasks. The points above the black dashed line represent the policies that violate the constraints.", "description": "This figure compares the performance of different algorithms (PDOA [MODF], PDOA [MORvS], Prompt-MODT, BC-Finetune, and CDT Oracle) on four different safe RL tasks (OfflineAntCircle-v0, OfflineBallCircle-v0, OfflineAntRun-v0, and OfflineBallRun-v0) across various safety thresholds.  The x-axis represents the safety threshold, while the y-axis shows both the normalized cost (top row) and normalized utility (bottom row) achieved by the algorithms\u2019 resulting policies.  The black dashed line indicates the threshold where policies violate the constraints. The figure illustrates the ability of the PDOA algorithms to effectively meet the safety constraints while maintaining a reasonable level of utility, outperforming the other algorithms, especially at tighter safety thresholds.", "section": "5.2 Constraint Satisfaction for Safe RL Tasks"}, {"figure_path": "QB6CvDqa6b/figures/figures_18_2.jpg", "caption": "Figure 8: Average utility and Hypervolume performance of all algorithms on D4MORL Expert datasets.", "description": "This figure compares the performance of different algorithms (PDOA [MODF], PDOA [MORVS], Prompt-MODT, BC-Finetune, and DWBC) on the D4MORL Expert datasets across five multi-objective tasks: MO-Ant-v2, MO-Swimmer-v2, MO-HalfCheetah-v2, MO-Hopper-v2, and MO-Walker2d-v2.  For each task, two metrics are shown: Average Utility and Hypervolume,  both representing different aspects of the algorithms' performance in balancing multiple objectives.  The dashed lines represent the best performance achievable with full knowledge of the true target preferences (oracle performance). The results show the effectiveness of the proposed PDOA framework compared to existing baseline approaches.", "section": "5.1 Preference Alignment for MORL Tasks"}, {"figure_path": "QB6CvDqa6b/figures/figures_18_3.jpg", "caption": "Figure 4: The adapted policies' cost and utility of each algorithm under various safety thresholds. Here, the utility is the normalized reward, since there is only one unconstrained objective in DSRL tasks. The points above the black dashed line represent the policies that violate the constraints.", "description": "This figure presents the performance of several algorithms on safe reinforcement learning tasks with safety constraints. The x-axis represents the safety threshold, while the y-axes represent the normalized cost and utility (reward) of the learned policies. Each point represents the performance of a policy trained using a particular method. The black dashed line indicates the safety threshold that must not be violated. Policies with costs exceeding the threshold are shown above the line. The figure helps to assess the ability of different methods in meeting safety constraints while maximizing reward.", "section": "5.2 Constraint Satisfaction for Safe RL Tasks"}, {"figure_path": "QB6CvDqa6b/figures/figures_19_1.jpg", "caption": "Figure 16: The relationship between TD reward, action likelihood reward, and preference. Pref1 represents the first dimension of preference.", "description": "This figure visualizes the relationship between TD reward, action likelihood reward, and preference for both PDOA[MODF] and PDOA[MORVS].  It shows the correlation between these factors and the target preferences across various tasks (MO-HalfCheetah and OfflineBallCircle). The plots illustrate how the TD and action likelihood rewards vary with the first dimension of preference (Pref1), providing insights into how the algorithm identifies the target preference.", "section": "A.9 Relationship between TD Reward, Action Likelihood Reward and Preference"}]