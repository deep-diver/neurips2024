[{"figure_path": "mhhlZeAr67/figures/figures_1_1.jpg", "caption": "Figure 1: (A) Classical machine learning fits a model from the model space (restricted by red curve) to a realized sample from the sample space (blue-grey); figure replicated from \"The Elements of Statistical Learning\" [31, Figure 7.2]. (B) In reciprocal learning, the realized sample is no longer static, but changes in response to the model fit. Grey ellipse indicates restriction of sample space in t = 2 through realization in t = 1. Sample in t thus depends on model in t \u2013 1 and sample in t\u22121.", "description": "This figure illustrates the difference between classical machine learning and reciprocal learning. Panel (a) shows classical machine learning where the model is fitted to a static sample. Panel (b) shows reciprocal learning where the sample is iteratively altered based on the current model fit, creating a feedback loop between the model and data.", "section": "1 Introduction"}, {"figure_path": "mhhlZeAr67/figures/figures_1_2.jpg", "caption": "Figure 1: (A) Classical machine learning fits a model from the model space (restricted by red curve) to a realized sample from the sample space (blue-grey); figure replicated from \"The Elements of Statistical Learning\" [31, Figure 7.2]. (B) In reciprocal learning, the realized sample is no longer static, but changes in response to the model fit. Grey ellipse indicates restriction of sample space in t = 2 through realization in t = 1. Sample in t thus depends on model in t \u2013 1 and sample in t\u22121.", "description": "This figure compares classical machine learning with reciprocal learning.  Panel (a) shows classical machine learning where the model is fit to a static sample of data. Panel (b) shows reciprocal learning where the sample of data is iteratively updated based on the current model fit, leading to an oscillation between model fitting and data selection.", "section": "1 Introduction"}, {"figure_path": "mhhlZeAr67/figures/figures_3_1.jpg", "caption": "Figure 1: (A) Classical machine learning fits a model from the model space (restricted by red curve) to a realized sample from the sample space (blue-grey); figure replicated from \"The Elements of Statistical Learning\" [31, Figure 7.2]. (B) In reciprocal learning, the realized sample is no longer static, but changes in response to the model fit. Grey ellipse indicates restriction of sample space in t = 2 through realization in t = 1. Sample in t thus depends on model in t \u2013 1 and sample in t\u22121.", "description": "This figure compares classical machine learning and reciprocal learning. In classical machine learning, a model is fitted to a static dataset.  In reciprocal learning, the dataset is iteratively updated based on the current model fit, creating a feedback loop between model and data.", "section": "1 Introduction"}, {"figure_path": "mhhlZeAr67/figures/figures_7_1.jpg", "caption": "Figure 3: Reciprocal learning converges if the change in sample (purple) is bounded by the change in model (yellow) and previous sample.", "description": "This figure illustrates a key concept in reciprocal learning: convergence.  It shows an iterative process where a model is fitted to a sample (blue-grey circles), and then the sample is modified based on the model fit. The purple arrows represent changes in the sample space across iterations, while yellow arrows represent changes in the model space. The red curve represents the boundary of the model space, and the light grey area represents the sample space. The black dot represents the 'truth'. The figure demonstrates that for the algorithm to converge, the change in the sample (purple arrows) must be bounded by a constant multiple (L) of the combined change in the model and the previous sample (yellow arrows). This is expressed mathematically as d(P'', P''') \u2264 L.d((\u03b8, P), (\u03b8', P')).  This inequality highlights that the algorithm's stability relies on controlling data adaptation (the purple arrows) to prevent instability from disproportionate changes in the model and the data.", "section": "3 Convergence of reciprocal learning: Lipschitz is all you need"}, {"figure_path": "mhhlZeAr67/figures/figures_17_1.jpg", "caption": "Figure 1: (A) Classical machine learning fits a model from the model space (restricted by red curve) to a realized sample from the sample space (blue-grey); figure replicated from \"The Elements of Statistical Learning\" [31, Figure 7.2]. (B) In reciprocal learning, the realized sample is no longer static, but changes in response to the model fit. Grey ellipse indicates restriction of sample space in t = 2 through realization in t = 1. Sample in t thus depends on model in t \u2013 1 and sample in t\u22121.", "description": "This figure compares classical machine learning to reciprocal learning. Panel A shows how classical machine learning fits a model to a fixed sample of data. Panel B shows how reciprocal learning iteratively updates the data based on the current model fit. This illustrates the key difference between the two approaches and highlights the feedback loop that defines reciprocal learning.", "section": "1 Introduction"}, {"figure_path": "mhhlZeAr67/figures/figures_19_1.jpg", "caption": "Figure 1: (A) Classical machine learning fits a model from the model space (restricted by red curve) to a realized sample from the sample space (blue-grey); figure replicated from \"The Elements of Statistical Learning\" [31, Figure 7.2]. (B) In reciprocal learning, the realized sample is no longer static, but changes in response to the model fit. Grey ellipse indicates restriction of sample space in t = 2 through realization in t = 1. Sample in t thus depends on model in t \u2013 1 and sample in t\u22121.", "description": "This figure illustrates the difference between classical machine learning and reciprocal learning. In classical machine learning (A), a model is fitted to a static dataset, while in reciprocal learning (B), the dataset is iteratively updated based on the model fit in each iteration, creating a feedback loop between model and data.", "section": "1 Introduction"}, {"figure_path": "mhhlZeAr67/figures/figures_19_2.jpg", "caption": "Figure 1: (A) Classical machine learning fits a model from the model space (restricted by red curve) to a realized sample from the sample space (blue-grey); figure replicated from \"The Elements of Statistical Learning\" [31, Figure 7.2]. (B) In reciprocal learning, the realized sample is no longer static, but changes in response to the model fit. Grey ellipse indicates restriction of sample space in t = 2 through realization in t = 1. Sample in t thus depends on model in t \u2013 1 and sample in t\u22121.", "description": "This figure compares classical machine learning to reciprocal learning.  Panel (a) shows classical machine learning where a model is fitted to a static dataset. Panel (b) shows reciprocal learning, where the model iteratively refines itself by modifying the training data based on its current fit. The grey ellipse in (b) highlights how the sample space is restricted by the model fit, demonstrating the feedback loop between model and data.", "section": "1 Introduction"}, {"figure_path": "mhhlZeAr67/figures/figures_21_1.jpg", "caption": "Figure 6: Self-training with soft labels and varying selection criteria c(x, \u03b8), one of which (Bayes-crit-reg) is regularized, on banknote data [21] with 70% (a) and 80% (b) unlabeled data; y-axis shows L2-Norm of \u03b8t at iteration t. Iterations vary between (a), (b), and (c) due to varying size of unlabeled data. Model: Generalized additive regression. Data source: Public UCI Machine Learning Repository [21]. References for other selection criteria: Bayes-crit: Rodemann, J., et al. \"Approximately Bayes-optimal pseudo-label selection.\" [83]. Likelihood: H\u00fcllermeier, E., Cheng, W. \"Superset learning based on generalized loss minimization.\" [35] Predictive Var: Rizve, M, N., et al. \"In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning.\" [80]. Probability Score: Triguero, I., Garc\u00eda, S., Herrera, F. (2015). \"Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study.\" [103]. For details, see https://github.com/rodemann/simulations-self-training-reciprocal-learning.", "description": "This figure compares the stability of the parameter vector (\u03b8t) in self-training with and without data regularization across different selection criteria.  It shows the L2-norm of \u03b8t over iterations for three scenarios with varying proportions of unlabeled data (90%, 80%, and 70%). The results illustrate the stabilizing effect of data regularization, demonstrating that the regularized method is more stable than the unregularized method.", "section": "C Illustrative experiments on data regularization"}, {"figure_path": "mhhlZeAr67/figures/figures_21_2.jpg", "caption": "Figure 6: Self-training with soft labels and varying selection criteria c(x, \u03b8), one of which (Bayes-crit-reg) is regularized, on banknote data [21] with 70% (a) and 80% (b) unlabeled data; y-axis shows L2-Norm of \u03b8t at iteration t. Iterations vary between (a), (b), and (c) due to varying size of unlabeled data. Model: Generalized additive regression. Data source: Public UCI Machine Learning Repository [21]. References for other selection criteria: Bayes-crit: Rodemann, J., et al. \"Approximately Bayes-optimal pseudo-label selection.\" [83]. Likelihood: H\u00fcllermeier, E., Cheng, W. \"Superset learning based on generalized loss minimization.\" [35] Predictive Var: Rizve, M, N., et al. \"In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning.\" [80]. Probability Score: Triguero, I., Garc\u00eda, S., Herrera, F. (2015). \"Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study.\" [103]. For details, see https://github.com/rodemann/simulations-self-training-reciprocal-learning.", "description": "This figure shows the L2 norm of the parameter vector in each iteration of self-training with different data selection criteria, both with and without regularization.  The results are presented for datasets with varying proportions of unlabeled data (70%, 80%, and 90%). The figure illustrates the stabilizing effect of data regularization on the parameter vector.", "section": "C Illustrative experiments on data regularization"}, {"figure_path": "mhhlZeAr67/figures/figures_21_3.jpg", "caption": "Figure 6: Self-training with soft labels and varying selection criteria c(x, \u03b8), one of which (Bayes-crit-reg) is regularized, on banknote data [21] with 70% (a) and 80% (b) unlabeled data; y-axis shows L2-Norm of \u03b8t at iteration t. Iterations vary between (a), (b), and (c) due to varying size of unlabeled data. Model: Generalized additive regression. Data source: Public UCI Machine Learning Repository [21]. References for other selection criteria: Bayes-crit: Rodemann, J., et al. \"Approximately Bayes-optimal pseudo-label selection.\" [83]. Likelihood: H\u00fcllermeier, E., Cheng, W. \"Superset learning based on generalized loss minimization.\" [35] Predictive Var: Rizve, M, N., et al. \"In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning.\" [80]. Probability Score: Triguero, I., Garc\u00eda, S., Herrera, F. (2015). \"Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study.\" [103]. For details, see https://github.com/rodemann/simulations-self-training-reciprocal-learning.", "description": "This figure compares the stability of the parameter vector in self-training with and without data regularization across various selection criteria. The L2-norm of the parameter vector is plotted against the iteration number for three different datasets (90%, 80%, and 70% unlabeled data).", "section": "C Illustrative experiments on data regularization"}, {"figure_path": "mhhlZeAr67/figures/figures_22_1.jpg", "caption": "Figure 6: Self-training with soft labels and varying selection criteria c(x, \u03b8), one of which (Bayes-crit-reg) is regularized, on banknote data [21] with 70% (a) and 80% (b) unlabeled data; y-axis shows L2-Norm of \u03b8t at iteration t. Iterations vary between (a), (b), and (c) due to varying size of unlabeled data. Model: Generalized additive regression. Data source: Public UCI Machine Learning Repository [21]. References for other selection criteria: Bayes-crit: Rodemann, J., et al. \"Approximately Bayes-optimal pseudo-label selection.\" [83]. Likelihood: H\u00fcllermeier, E., Cheng, W. \"Superset learning based on generalized loss minimization.\" [35] Predictive Var: Rizve, M, N., et al. \"In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning.\" [80]. Probability Score: Triguero, I., Garc\u00eda, S., Herrera, F. (2015). \"Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study.\" [103]. For details, see https://github.com/rodemann/simulations-self-training-reciprocal-learning.", "description": "This figure displays the results of self-training experiments using various selection criteria, with one criterion being regularized.  The y-axis shows the L2 norm of the parameter vector at each iteration, illustrating the stability of the model under different conditions. Three subfigures present results for datasets with varying amounts (70%, 80%, and 90%) of unlabeled data.", "section": "C Illustrative experiments on data regularization"}, {"figure_path": "mhhlZeAr67/figures/figures_22_2.jpg", "caption": "Figure 6: Self-training with soft labels and varying selection criteria (c, c\u1d63), one of which (Bayes-crit-reg) is regularized, on banknote data [21] with 70% (a) and 80% (b) unlabeled data; y-axis shows L2-Norm of \u03b8\u209c at iteration t. Iterations vary between (a), (b), and (c) due to varying size of unlabeled data. Model: Generalized additive regression. Data source: Public UCI Machine Learning Repository [21]. References for other selection criteria: Bayes-crit: Rodemann, J., et al. \"Approximately Bayes-optimal pseudo-label selection.\" [83]. Likelihood: H\u00fcllermeier, E., Cheng, W. \"Superset learning based on generalized loss minimization.\" [35] Predictive Var: Rizve, M, N., et al. \"In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning.\" [80]. Probability Score: Triguero, I., Garc\u00eda, S., Herrera, F. (2015). \"Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study.\" [103]. For details, see https://github.com/rodemann/simulations-self-training-reciprocal-learning.", "description": "This figure compares the stability of the parameter vector (\u03b8\u209c) in self-training with and without data regularization, using different selection criteria.  It shows the L2 norm of \u03b8\u209c over iterations for self-training on banknote data with varying amounts of unlabeled data (70%, 80%, and 90%). The results illustrate the stabilizing effect of data regularization on the parameter vector.", "section": "C Illustrative experiments on data regularization"}, {"figure_path": "mhhlZeAr67/figures/figures_22_3.jpg", "caption": "Figure 6: Self-training with soft labels and varying selection criteria c(x, \u03b8), one of which (Bayes-crit-reg) is regularized, on banknote data [21] with 70% (a) and 80% (b) unlabeled data; y-axis shows L2-Norm of \u03b8t at iteration t. Iterations vary between (a), (b), and (c) due to varying size of unlabeled data. Model: Generalized additive regression. Data source: Public UCI Machine Learning Repository [21]. References for other selection criteria: Bayes-crit: Rodemann, J., et al. \"Approximately Bayes-optimal pseudo-label selection.\" [83]. Likelihood: H\u00fcllermeier, E., Cheng, W. \"Superset learning based on generalized loss minimization.\" [35] Predictive Var: Rizve, M, N., et al. \"In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning.\" [80]. Probability Score: Triguero, I., Garc\u00eda, S., Herrera, F. (2015). \"Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study.\" [103]. For details, see https://github.com/rodemann/simulations-self-training-reciprocal-learning.", "description": "The figure displays the L2 norm of the parameter vector (\u03b8t) over iterations for self-training with soft labels.  It compares different data selection criteria, both regularized and unregularized, on banknote datasets with varying amounts of unlabeled data (70%, 80%, and 90%). The results illustrate the impact of data regularization on parameter stability.", "section": "C Illustrative experiments on data regularization"}]