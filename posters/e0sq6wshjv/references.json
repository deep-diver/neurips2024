{"references": [{"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-26", "reason": "This paper is foundational to the work presented as it introduces the Vision Transformer architecture, which is the core model being adapted."}, {"fullname_first_author": "Shoufa Chen", "paper_title": "AdaptFormer: Adapting vision transformers for scalable visual recognition", "publication_date": "2022-12-01", "reason": "This paper introduces a prominent parameter-efficient fine-tuning method (PEFT) for Vision Transformers, which is directly compared against in the current work."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-06-21", "reason": "This is another significant PEFT method used for comparison and analysis against the proposed approach."}, {"fullname_first_author": "Yongming Rao", "paper_title": "DynamicViT: Efficient vision transformers with dynamic token sparsification", "publication_date": "2021-12-01", "reason": "This paper explores dynamic neural networks in vision transformers, a related area that the current work builds upon and differentiates from."}, {"fullname_first_author": "Xiaohua Zhai", "paper_title": "A large-scale study of representation learning with the visual task adaptation benchmark", "publication_date": "2019-10-21", "reason": "This paper introduces the VTAB-1K benchmark dataset, which is crucial for evaluating the presented method and is the primary benchmark dataset used for comparisons."}]}