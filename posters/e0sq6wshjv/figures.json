[{"figure_path": "e0SQ6wsHjv/figures/figures_1_1.jpg", "caption": "Figure 1: FLOPs and Accuracy of ViT-B/16 [20] on VTAB-1K [89]. \u201cFull tuning", "description": "This figure compares the computational cost (FLOPs) and accuracy of different vision transformer (ViT) adaptation methods on the VTAB-1K benchmark.  It shows that Dynamic Tuning (DyT) achieves superior accuracy while using significantly fewer FLOPs compared to full fine-tuning and other parameter-efficient fine-tuning (PEFT) methods like AdaptFormer, LoRA, and VPT.", "section": "1 Introduction"}, {"figure_path": "e0SQ6wsHjv/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of Dynamic Tuning. (a) In the fine-tuning stage, we adopt Gumbel Noise to enable end-to-end training. (b)In the inference stage, TD selects K activated tokens X, from X based on the mask M, which saves the computations on those deactivated tokens in Block. Block can represent a Attn block, a MLP block, or an entire transformer layer.", "description": "This figure illustrates the Dynamic Tuning (DyT) process during both fine-tuning and inference stages.  The fine-tuning stage (a) uses Gumbel Noise to allow for end-to-end training of the token dispatcher (TD), which learns to create a mask (M) determining which tokens are activated and passed through the main transformer block. Deactivated tokens bypass the main block but are still processed by a lightweight adapter. During inference (b), the trained TD uses the mask (M) to directly select only the activated tokens for processing, skipping the main block for deactivated tokens and thereby reducing computation.", "section": "3.2 Dynamic Tuning"}, {"figure_path": "e0SQ6wsHjv/figures/figures_4_1.jpg", "caption": "Figure 3: Model variants. For brevity, we omit the LayerNorm [1] in Attn and MLP blocks. \"DyT\" denotes the dynamic tuning presented in Figure 2.", "description": "This figure shows four different model variants of Dynamic Tuning (DyT).  Each variant applies the DyT token dispatch mechanism at a different level within the Transformer block.  \n\n* **Attn Dispatch:** The token selection happens before the multi-head self-attention (Attn) block. \n* **MLP Dispatch:** Tokens are selected before the multi-layer perceptron (MLP) block.\n* **Attn-MLP Dispatch:** Token selection is performed before both Attn and MLP blocks.\n* **Layer Dispatch:** A single token dispatcher controls token activation/deactivation for the entire layer (Attn and MLP). \n\nThe figure visually demonstrates how the token dispatcher (DyT) integrates into the Attn and MLP blocks in each variant.", "section": "3.3 Model Variants"}, {"figure_path": "e0SQ6wsHjv/figures/figures_5_1.jpg", "caption": "Figure 4: The architecture of the MoE-adapter. It is consist of N adapter experts.", "description": "The MoE-adapter consists of a routing layer (Wr) that generates weights (\u03b11, \u03b12,...\u03b1N) for N adapter experts. Each expert has a down-project layer (Wdown) and an up-project layer (Wup). The input (X) is processed by each expert independently and the results are combined with corresponding weights.  The output (Xadapt) is the weighted sum of the expert outputs.", "section": "3.4 MoE-Adapter"}, {"figure_path": "e0SQ6wsHjv/figures/figures_7_1.jpg", "caption": "Figure 5: Token activation rate in different layers. We visualize the token activation rates in ViT-B/16. \u201cOverall\u201d denotes the mean activation rate in the whole model, which arrives at around 50% when r is set to 0.5. \u201cLayer0\u201d and \u201cLayer11\u201d denote the lowest and highest level, respectively. Notably, the activation rate in the last layer is exactly 0% on CIFAR-100, SVHN, and K400 datasets.", "description": "This figure visualizes the token activation rates across different layers of a ViT-B/16 model for various datasets.  It shows how the model dynamically chooses to activate a subset of tokens in each layer during inference, with the activation rate varying across layers and datasets. The overall activation rate is around 50% when r=0.5, but the activation rate in the last layer (Layer11) is 0% for CIFAR-100, SVHN, and K400 datasets.", "section": "4.2 Analysis"}, {"figure_path": "e0SQ6wsHjv/figures/figures_7_2.jpg", "caption": "Figure 6: Visualization of activated tokens. We present two representative samples from the K400 dataset. Blue patches represent the tokens activated in token dispatcher (Detailed in Section 3.2). Results verify that the token dispatcher has learned to identify informative tokens during fine-tuning.", "description": "This figure visualizes how the token dispatcher in the Dynamic Tuning method identifies and activates informative tokens during fine-tuning.  It shows two examples from the K400 dataset, highlighting which tokens (represented by blue patches) are deemed important and processed by the network.  The visualization demonstrates the method's ability to selectively process only essential tokens, improving efficiency by skipping less important ones.", "section": "4.3 VTAB-1K Results"}, {"figure_path": "e0SQ6wsHjv/figures/figures_17_1.jpg", "caption": "Figure 1: FLOPs and Accuracy of ViT-B/16 [20] on VTAB-1K [89]. \"Full tuning\" denotes that all parameters are fine-tuned. AdaptFormer [12], LoRA [34] and VPT [36] are typical PEFT methods.", "description": "This figure compares the FLOPs (floating-point operations) and accuracy of different ViT adaptation methods on the VTAB-1K benchmark.  It shows that the proposed Dynamic Tuning (DyT) method achieves superior accuracy while using significantly fewer FLOPs compared to full fine-tuning and other parameter-efficient fine-tuning (PEFT) methods such as AdaptFormer, LoRA, and VPT.  The results highlight DyT's improvement in both parameter and inference efficiency.", "section": "1 Introduction"}, {"figure_path": "e0SQ6wsHjv/figures/figures_24_1.jpg", "caption": "Figure 8: Visualization of activated tokens. We present representative samples from the K400 [10] dataset. Blue patches represent the tokens activated in token dispatcher. Results verify that the token dispatcher has learned to identify informative tokens during fine-tuning. Zoom in for better view.", "description": "This figure visualizes the tokens activated by the token dispatcher in different layers of the ViT-B/16 model for several video samples from the K400 dataset. The blue patches highlight the activated tokens.  The results show that the model primarily activates tokens representing the main objects or actions within the scene, demonstrating the effectiveness of the token dispatcher in focusing on the most relevant information.", "section": "4.3 VTAB-1K Results"}, {"figure_path": "e0SQ6wsHjv/figures/figures_25_1.jpg", "caption": "Figure 8: Visualization of activated tokens. We present representative samples from the K400 [10] dataset. Blue patches represent the tokens activated in token dispatcher. Results verify that the token dispatcher has learned to identify informative tokens during fine-tuning. Zoom in for better view.", "description": "This figure visualizes which image tokens are activated by the model's token dispatcher during the fine-tuning phase for various samples from the K400 dataset.  Blue patches highlight the tokens selected for processing by the model. This visualization helps demonstrate that the token dispatcher effectively identifies and selects informative tokens, improving efficiency by skipping less important ones.", "section": "4.3 VTAB-1K Results"}]