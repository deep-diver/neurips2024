[{"figure_path": "e0SQ6wsHjv/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of model variants. \u201cParams. (M)\u201d indicates the additional parameters in backbones. \"FLOPs (G)\" denotes the average FLOPs on CIFAR-100.", "description": "This table compares four different model variants of the Dynamic Tuning (DyT) method.  The variants differ in where the token dispatcher is placed within the transformer block (Attention Dispatch, MLP Dispatch, Attention-MLP Dispatch, Layer Dispatch).  The table shows the number of additional parameters (in millions), the average FLOPs (in billions) on the CIFAR-100 dataset, and the image and video classification accuracy on CIFAR-100, SVHN, Food-101, K400, and SSv2 datasets for each variant. This allows for a comparison of the efficiency and effectiveness tradeoffs of the different DyT designs.", "section": "4.2 Analysis"}, {"figure_path": "e0SQ6wsHjv/tables/tables_6_2.jpg", "caption": "Table 2: Effectiveness of MoE-Adapter. DyT\u2020 denotes the DyT with MoE-adapters. Standard adapter is enough to handle image datasets while MoE-adapter is more suitable for challenging scenarios, such as video datasets. It theoretically does not increase extra computational cost, but the FLOPs slightly vary in different models since the learned token dispatch strategy in the TD is different. N represents the number of experts.", "description": "This table compares the performance of the Dynamic Tuning (DyT) model with and without the Mixture-of-Experts (MoE) adapter.  It shows that while the standard DyT adapter is sufficient for image datasets, the MoE adapter provides a performance boost for more complex tasks such as video recognition.  The computational cost remains largely unchanged, even though the FLOPs vary slightly between different configurations, due to variations in the token dispatch strategy during training. The number of experts (N) is a parameter adjusted in the MoE adapter configuration.", "section": "3.4 MoE-Adapter"}, {"figure_path": "e0SQ6wsHjv/tables/tables_8_1.jpg", "caption": "Table 3: Performance and efficiency comparison on VTAB-1K. \u201cGroup Mean\u201d indicates the averaged accuracy of three groups. \u201cParams. (M)\u201d denotes the number of trainable parameters in backbones. \u201cFLOPSs (G)\u201d is the average FLOPs across all datasets. Bold font and underline denote the best and the second-best performance respectively.", "description": "This table compares the performance and efficiency of Dynamic Tuning (DyT) against other parameter-efficient fine-tuning (PEFT) methods on the VTAB-1K benchmark.  It shows the average accuracy across various visual tasks, the number of trainable parameters, and the average FLOPs (floating point operations) used.  The best and second-best performing methods for each task are highlighted.", "section": "4.3 VTAB-1K Results"}, {"figure_path": "e0SQ6wsHjv/tables/tables_8_2.jpg", "caption": "Table 4: Comparison of throughput. \u201cVTAB-1K Accuracy \u2191\u201d denotes the averaged accuracy of three dataset groups in VTAB-1K [89] benchmark.", "description": "This table compares the inference throughput of different methods (full tuning, LoRA, AdaptFormer, VPT, and DyT) across three different hardware platforms: V100 GPU, T4 GPU, and Xeon(R) 8163 CPU.  The throughput is measured in images per second (img/s).  The table also shows the VTAB-1K accuracy and FLOPs (floating-point operations) for each method.  It demonstrates the speed improvements achieved by DyT while maintaining high accuracy, showcasing its efficiency on various hardware.", "section": "4.3 VTAB-1K Results"}, {"figure_path": "e0SQ6wsHjv/tables/tables_9_1.jpg", "caption": "Table 5: Comparison with efficient transformers. The throughput is measured on a Tesla V100 GPU. \"Params. (M) \u2193\" denotes the number of trainable parameters in backbones.", "description": "This table compares the performance and efficiency of Dynamic Tuning (DyT) against other efficient transformer methods, including DynamicViT, EViT, and AdaptFormer, with and without ToMe.  It shows that DyT achieves superior performance with significantly fewer FLOPs (floating point operations) and comparable or even higher throughput (images per second). The results demonstrate DyT's effectiveness in improving both parameter and inference efficiency for vision transformer adaptation.", "section": "4.3 VTAB-1K Results"}, {"figure_path": "e0SQ6wsHjv/tables/tables_9_2.jpg", "caption": "Table 1: Comparison of model variants. \u201cParams. (M)\u201d indicates the additional parameters in backbones. \"FLOPs (G)\" denotes the average FLOPs on CIFAR-100.", "description": "This table compares four different model variants of the proposed Dynamic Tuning (DyT) method on image and video datasets.  The variants differ in where the token dispatcher is applied (Attention, MLP, Attention-MLP, or Layer) and how tokens are skipped during processing.  The table shows the number of additional parameters (in millions), the average FLOPs (in billions), and the accuracy achieved on various benchmark datasets (CIFAR-100, SVHN, Food-101, K400, and SSv2).  The results show the impact of the different token dispatch strategies on model performance and efficiency.", "section": "4.2 Analysis"}, {"figure_path": "e0SQ6wsHjv/tables/tables_17_1.jpg", "caption": "Table 1: Comparison of model variants. \u201cParams. (M)\u201d indicates the additional parameters in backbones. \"FLOPs (G)\" denotes the average FLOPs on CIFAR-100.", "description": "This table compares four different model variants of the Dynamic Tuning (DyT) method.  Each variant differs in where the token dispatch mechanism is applied within the Transformer blocks (Attention, MLP, Attention-MLP, and Layer). The table shows the number of additional parameters (in millions), the average FLOPs (in billions) on the CIFAR-100 dataset, and the image and video classification accuracy for several datasets (CIFAR-100, SVHN, Food-101, K400, SSv2). This allows for a comparison of the performance and efficiency tradeoffs of each model variant.", "section": "4.2 Analysis"}, {"figure_path": "e0SQ6wsHjv/tables/tables_18_1.jpg", "caption": "Table 1: Comparison of model variants. \u201cParams. (M)\u201d indicates the additional parameters in backbones. \u201cFLOPs (G)\u201d denotes the average FLOPs on CIFAR-100.", "description": "This table compares four different model variants of the proposed Dynamic Tuning (DyT) method for Vision Transformer (ViT) adaptation.  The variants differ in where the token dispatcher is applied within the ViT's architecture: before the attention block, before the MLP block, before both, or before the entire transformer layer. The table shows the number of additional parameters (in millions), the average FLOPs (in billions) on the CIFAR-100 dataset, and the image and video accuracy for five different datasets (CIFAR-100, SVHN, Food-101, K400, SSv2). This allows for a comparison of the efficiency and effectiveness of each variant.", "section": "4.2 Analysis"}, {"figure_path": "e0SQ6wsHjv/tables/tables_18_2.jpg", "caption": "Table 9: Different temperature \u03c4 in dynamic tuning. \u201cSchedule\u201d denotes that the temperature gradually decays from 5 to 0.1 during fine-tuning. The default setting is marked in color.", "description": "This table presents the results of experiments conducted to determine the optimal temperature parameter (\u03c4) in the Gumbel-Sigmoid function used within the Dynamic Tuning method.  It compares the model's performance across various image and video datasets using different temperature schedules.  The 'Schedule' row indicates that the temperature was reduced gradually from 5 to 0.1 throughout the training process. The best performance across all datasets is highlighted in color.", "section": "3.3 Model Variants"}, {"figure_path": "e0SQ6wsHjv/tables/tables_19_1.jpg", "caption": "Table 10: Results on semantic segmentation. \"Avg.\" denotes the average results from the corresponding two datasets. The FLOPs is measured ADE20K.", "description": "This table compares the performance of different methods on two semantic segmentation datasets: ADE20K and COCO-stuff.  The metrics evaluated are mean Intersection over Union (mIOU) for ADE20K and COCO-stuff, and the average mIOU across both. The table also shows the number of trainable parameters (Params.) and Giga FLOPs (GFLOPs) for each method. The methods compared include Full tuning, Linear, Dynamic-Full, AdaptFormer, LoRA, VPT, and the proposed Dynamic Tuning (DyT) with and without the MoE-adapter. The results demonstrate that Dynamic Tuning achieves competitive performance while significantly reducing the number of parameters and FLOPs.", "section": "A.4 Generalization in semantic segmentation"}, {"figure_path": "e0SQ6wsHjv/tables/tables_19_2.jpg", "caption": "Table 11: Results on object detection and instance segmentation. We only measure the FLOPs in backbone and feature pyramid module.", "description": "This table compares the performance of different methods for object detection and instance segmentation on the COCO dataset.  It shows the number of trainable parameters, the GFLOPs (giga floating point operations), the mean Average Precision (mAP) for bounding boxes, and the mAP for segmentation masks. The methods compared include Full Tuning (as a baseline), AdaptFormer, DyT (Dynamic Tuning), and DyT with 4 MoE (Mixture-of-Experts) adapters.  The table highlights that DyT and DyT+MoE achieve comparable or better performance to AdaptFormer while using significantly fewer FLOPs.", "section": "4.3 VTAB-1K Results"}, {"figure_path": "e0SQ6wsHjv/tables/tables_20_1.jpg", "caption": "Table 12: Effectiveness of loss functions The default loss function in DyT is L = Lcls + L'cls + Ldistill + aLrate. We gradually remove the complete model loss L'els and distillation loss Ldistill from it, and find the performance drops.", "description": "This table shows the ablation study of the loss functions used in Dynamic Tuning (DyT). It demonstrates that the complete model loss (L'cls) and the distillation loss (Ldistill) contribute positively to the model's performance. Removing either one or both causes a decrease in the VTAB-1K accuracy.  The training time also slightly increases when using the complete model, but the improvement in accuracy may justify the increased training cost.", "section": "A.6 Effectiveness of the Complete Model and Distillation"}, {"figure_path": "e0SQ6wsHjv/tables/tables_20_2.jpg", "caption": "Table 3: Performance and efficiency comparison on VTAB-1K. \u201cGroup Mean\u201d indicates the averaged accuracy of three groups. \u201cParams. (M)\u201d denotes the number of trainable parameters in backbones. \u201cFLOPSs (G)\u201d is the average FLOPs across all datasets. Bold font and underline denote the best and the second-best performance respectively.", "description": "This table compares the performance and efficiency of Dynamic Tuning (DyT) against other methods on the VTAB-1K benchmark.  It shows the average accuracy across different subsets of VTAB-1K, the number of trainable parameters, and the FLOPs (floating point operations) for each method.  The best and second-best performing methods are highlighted for each task.  The table helps illustrate how DyT achieves superior performance and efficiency.", "section": "4.3 VTAB-1K Results"}, {"figure_path": "e0SQ6wsHjv/tables/tables_21_1.jpg", "caption": "Table 1: Comparison of model variants. \u201cParams. (M)\u201d indicates the additional parameters in backbones. \"FLOPs (G)\" denotes the average FLOPs on CIFAR-100.", "description": "This table compares four different model variants of the Dynamic Tuning (DyT) method proposed in the paper.  Each variant uses a different strategy for dynamically selecting tokens during processing within a transformer block (Attn, MLP, or a complete layer). The table shows the number of additional parameters (in millions), the average FLOPs (in billions) on the CIFAR-100 dataset, and the image and video accuracies on various datasets for each model variant. This allows for a comparison of the computational efficiency and performance trade-offs of different DyT configurations.", "section": "4.2 Analysis"}, {"figure_path": "e0SQ6wsHjv/tables/tables_21_2.jpg", "caption": "Table 15: Experimental settings for video datasets. We follow most of settings in [60]. The number of input frames is set to 8 in all experiments. lr = base_lr \u00d7 batch_size/256", "description": "This table details the hyperparameters used for training video models on the Kinetics-400 and Something-Something V2 datasets.  It specifies the optimizer, learning rate, weight decay, batch size, number of training epochs, image resizing strategy, crop size, learning rate schedule, frame sampling rate, use of mirroring and RandAugment, and the number of testing views used during evaluation.", "section": "4.1 Experiment Settings"}, {"figure_path": "e0SQ6wsHjv/tables/tables_22_1.jpg", "caption": "Table 10: Results on semantic segmentation. \"Avg.\" denotes the average results from the corresponding two datasets. The FLOPs is measured ADE20K.", "description": "This table compares the performance of different methods on two semantic segmentation datasets: ADE20K and COCO-stuff.  The metrics shown include Mean Intersection over Union (mIOU) for ADE20K and COCO-stuff, and the average mIOU across both.  The number of trainable parameters (Params. (M)) and GFLOPs are also given for each method. The methods compared include Full Tuning, Linear, Dynamic-Full, AdaptFormer, LoRA, VPT, and DyT (with and without the MoE-adapter).", "section": "A.4 Generalization in semantic segmentation"}, {"figure_path": "e0SQ6wsHjv/tables/tables_22_2.jpg", "caption": "Table 18: Scale up the model size to ViT-L [20]. \u201cParam. (M)\u201d denotes the number of trainable parameters in backbones. \u201cFLOPs (G)\u201d denotes the average FLOPs on CIFAR-100. The default setting is marked in color. The bottleneck dimension d is set to 64.", "description": "This table presents the results of scaling up the model size to ViT-L while using dynamic tuning. It compares the performance and efficiency of different activation rates (r) against the full tuning method across CIFAR-100, SVHN, and Food-101 datasets.  The table shows the number of trainable parameters, FLOPs (floating-point operations), and image accuracy for each configuration. The bottleneck dimension (d) of the adapter is fixed at 64.", "section": "A.10 Scaling-up Model Size"}, {"figure_path": "e0SQ6wsHjv/tables/tables_23_1.jpg", "caption": "Table 18: Scale up the model size to ViT-L [20]. \u201cParam. (M)\u201d denotes the number of trainable parameters in backbones. \u201cFLOPs (G)\u201d denotes the average FLOPs on CIFAR-100. The default setting is marked in color. The bottleneck dimension d is set to 64.", "description": "This table shows the impact of scaling up the model size to ViT-L while using the Dynamic Tuning method.  It compares the performance and computational cost (FLOPs) of different activation rates (r) within the Dynamic Tuning approach against the performance of the fully tuned ViT-L model on three image classification datasets: CIFAR-100, SVHN, and Food-101. The table highlights that even with a significantly smaller number of trainable parameters, Dynamic Tuning achieves comparable or even better accuracy than full tuning, especially at lower FLOPs.", "section": "A.10 Scaling-up Model Size"}, {"figure_path": "e0SQ6wsHjv/tables/tables_23_2.jpg", "caption": "Table 1: Comparison of model variants. \u201cParams. (M)\u201d indicates the additional parameters in backbones. \"FLOPs (G)\" denotes the average FLOPs on CIFAR-100.", "description": "This table compares four different model variants of the proposed Dynamic Tuning (DyT) method.  The variants differ in where the token dispatcher is applied within the transformer block (Attention, MLP, Attn-MLP, or Layer). The table shows the number of additional parameters (in millions), the average FLOPs (in billions) on the CIFAR-100 dataset, and the image and video classification accuracy for several benchmark datasets (CIFAR-100, SVHN, Food-101, K400, and SSv2).  This allows for a comparison of performance and efficiency trade-offs among the different DyT variants.", "section": "4.2 Analysis"}]