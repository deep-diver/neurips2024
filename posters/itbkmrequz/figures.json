[{"figure_path": "itbKmreqUZ/figures/figures_1_1.jpg", "caption": "Figure 1: An example of the satellite's earth observation image and this image's corresponding sensor data as a description.", "description": "The figure shows an example of a satellite image along with its associated metadata. The metadata includes ground sample distance, coordinates, cloud cover, timestamp, and entropy coding.  This illustrates the multimodal nature of satellite data, where the image is accompanied by rich contextual information that can aid in its interpretation and use. This multi-modal data concept is crucial to the COSMIC method, which uses the sensor data as additional input for the diffusion-based compensation model.", "section": "1 Introduction"}, {"figure_path": "itbKmreqUZ/figures/figures_4_1.jpg", "caption": "Figure 2: COSMIC framework. (a) Compression module for satellite images: a lightweight encoder and a compensation-based decoder (Sec. 4.3). (b) In the noise prediction network, each Cross-Attention (CA) block receives embedding of the Metadata Encoder (ME) (Sec. 4.4), and the Vanilla Convolution (VC) blocks use latent image discrete encoding to guide the prediction of noise for each diffusion step. (c) & (d) Convolution attention module and lightweight convolution block (Sec. 4.2).", "description": "This figure illustrates the architecture of the COSMIC model, which consists of two main modules: a compression module and a compensation module.  The compression module uses a lightweight encoder to compress satellite images on-board, saving bandwidth. The compensation module, deployed on the ground, uses a diffusion model to reconstruct image details lost during the compression process, leveraging additional sensor data (metadata) as guidance.  The figure details the components of both modules, including the lightweight convolution blocks (LCBs), the convolution attention module (CAM), the metadata encoder (ME), and the noise prediction network.", "section": "4 Method"}, {"figure_path": "itbKmreqUZ/figures/figures_6_1.jpg", "caption": "Figure 3: Trade-off between bitrates and different metrics on COSMIC and baselines. The \u2191(\u2193) means higher (lower) is better. The first row is for the fMoW test set (image size 256 \u00d7 256). The second is for the tile test set by comparing between the stitched images and their original ones.", "description": "This figure shows the rate-distortion performance of COSMIC compared to six other state-of-the-art image compression methods.  It presents four key metrics (PSNR, MS-SSIM, LPIPS, and FID) across a range of bitrates (bpp). Two test sets were used: a standard fMoW test set and a tile test set composed of stitched sub-images. The results demonstrate COSMIC's superior performance across various metrics and bitrates, especially in perceptual quality (LPIPS and FID).  The tile test set highlights COSMIC's ability to maintain visual consistency even when stitching multiple compressed images.", "section": "5 Experiments"}, {"figure_path": "itbKmreqUZ/figures/figures_7_1.jpg", "caption": "Figure 4: Decompressed fMoW images (full images in supplementary material). 1st row: comparison under low bitrates, COSMIC shows better visual effects. Compared with CDC, COSMIC still gets slightly better visual reconstruction with less bitrates. 2nd row: comparison under high bitrates.", "description": "This figure compares the visual quality of images reconstructed by COSMIC and several baseline methods at both low and high bitrates.  The top row showcases low-bitrate results where COSMIC demonstrates better visual quality, particularly compared to CDC. The bottom row shows high-bitrate results which further highlight the visual superiority of COSMIC, maintaining good detail preservation even with less information.", "section": "5.2 Comparison with baselines"}, {"figure_path": "itbKmreqUZ/figures/figures_8_1.jpg", "caption": "Figure 2: COSMIC framework. (a) Compression module for satellite images: a lightweight encoder and a compensation-based decoder (Sec. 4.3). (b) In the noise prediction network, each Cross-Attention (CA) block receives embedding of the Metadata Encoder (ME) (Sec. 4.4), and the Vanilla Convolution (VC) blocks use latent image discrete encoding to guide the prediction of noise for each diffusion step. (c) & (d) Convolution attention module and lightweight convolution block (Sec. 4.2).", "description": "This figure illustrates the architecture of COSMIC, a learned image compression solution for satellites. It consists of two main modules: a compression module and a compensation module. The compression module uses a lightweight encoder to compress images on the satellite, while the compensation module leverages a diffusion model to reconstruct the image details on the ground, compensating for the limitations of the lightweight encoder. The figure details the components of each module, highlighting the use of lightweight convolution blocks (LCB), cross-attention (CA) blocks, and a metadata encoder.", "section": "4 Method"}, {"figure_path": "itbKmreqUZ/figures/figures_9_1.jpg", "caption": "Figure 3: Trade-off between bitrates and different metrics on COSMIC and baselines. The \u2191(\u2193) means higher (lower) is better. The first row is for the fMoW test set (image size 256 \u00d7 256). The second is for the tile test set by comparing between the stitched images and their original ones.", "description": "This figure compares the performance of COSMIC against six baseline methods across two test sets: the fMoW test set (images of size 256x256) and a tile test set (stitched images).  The comparison considers four metrics: PSNR, MS-SSIM, LPIPS, and FID.  The results show COSMIC generally outperforms the baselines, demonstrating a trade-off between compression rate (bpp) and both distortion and perceptual metrics. The tile test set results highlight COSMIC's ability to maintain consistency in stitching compared to the baselines.", "section": "5 Experiments"}, {"figure_path": "itbKmreqUZ/figures/figures_9_2.jpg", "caption": "Figure 7: Contrast visual results. \u201cw/ DC\u201d means using of diffusion for compensation during decoding, while \u201cw/o DC\u201d indicates that diffusion compensation was not used during decoding.", "description": "This figure shows a comparison of visual results obtained with and without using diffusion for compensation during the decoding process.  The left two images show an example where the lack of diffusion compensation results in a degraded image at 0.48 bits per pixel (bpp), with significant artifacts.  The right two images demonstrate an example at 0.69 bpp where the impact of the diffusion compensation is less noticeable.  The figure visually emphasizes how the diffusion process improves visual quality by filling in detail lost in the lightweight compression encoding step, especially at lower bitrates.", "section": "5.4 Ablation study"}, {"figure_path": "itbKmreqUZ/figures/figures_9_3.jpg", "caption": "Figure 8: Compression performance with different numbers of decoding steps.", "description": "This figure shows the impact of varying the number of decoding steps in the reverse diffusion process on the quality of the reconstructed images.  It displays how different metrics (PSNR, MS-SSIM, LPIPS, and FID) change with different bitrates (bpp) when using 5, 10, 25, and 50 decoding steps. The results illustrate the trade-off between perceptual quality and distortion as the number of steps increases.", "section": "5 Experiments"}, {"figure_path": "itbKmreqUZ/figures/figures_15_1.jpg", "caption": "Figure 3: Trade-off between bitrates and different metrics on COSMIC and baselines. The \u2191(\u2193) means higher (lower) is better. The first row is for the fMoW test set (image size 256 \u00d7 256). The second is for the tile test set by comparing between the stitched images and their original ones.", "description": "This figure shows the rate-distortion performance comparison between COSMIC and six state-of-the-art baselines across different metrics, including PSNR, MS-SSIM, LPIPS, and FID. It showcases the performance on two different test sets: the fMoW test set with images of size 256x256 and a tile test set constructed from larger images divided into 256x256 patches.  The results demonstrate COSMIC's superior performance, especially at higher bitrates, on both distortion and perceptual metrics.", "section": "5 Experiments"}, {"figure_path": "itbKmreqUZ/figures/figures_16_1.jpg", "caption": "Figure 10: Failure examples.", "description": "This figure shows some failure cases of COSMIC at low bitrates. The failure cases are mainly due to the insufficient feature extraction ability of the lightweight encoder, and the diffusion model needs to generate many non-existent textures and contents. However, in ROI (region of interest) such as ship detection, COSMIC can still achieve good reconstruction result even in low bitrates.", "section": "C Failure cases analysis"}, {"figure_path": "itbKmreqUZ/figures/figures_17_1.jpg", "caption": "Figure 3: Trade-off between bitrates and different metrics on COSMIC and baselines. The \u2191(\u2193) means higher (lower) is better. The first row is for the fMoW test set (image size 256 \u00d7 256). The second is for the tile test set by comparing between the stitched images and their original ones.", "description": "This figure displays the rate-distortion performance comparison between COSMIC and six baseline methods on two distinct test sets: the fMoW test set (images of size 256x256) and a tile test set (stitched images compared to their original high-resolution counterparts).  The results are presented across various bitrates (bpp) for four different metrics: PSNR (Peak Signal-to-Noise Ratio), MS-SSIM (Multi-Scale Structural Similarity Index Measure), LPIPS (Learned Perceptual Image Patch Similarity), and FID (Fr\u00e9chet Inception Distance).  The arrows indicate whether a higher or lower value is preferable for each metric (higher is better for PSNR, MS-SSIM, and CW-SSIM, while lower is better for LPIPS and FID). The figure demonstrates COSMIC's superior performance across various bitrates.", "section": "5 Experiments"}, {"figure_path": "itbKmreqUZ/figures/figures_18_1.jpg", "caption": "Figure 12: Reconstructed fMoW images under low bitrates.", "description": "This figure compares the visual quality of reconstructed images from the fMoW dataset at low bitrates (around 0.2 bpp).  It shows the original image alongside reconstructions using COSMIC, CDC, JPEG2000, HIFIC, and COLIC.  The purpose is to visually demonstrate COSMIC's performance relative to several state-of-the-art compression methods under conditions of limited bitrate.  The visual differences highlight the strengths and weaknesses of each approach in terms of detail preservation and artifact reduction.", "section": "5 Experiments"}, {"figure_path": "itbKmreqUZ/figures/figures_19_1.jpg", "caption": "Figure 13: Reconstructed fMoW images under high bitrates.", "description": "This figure presents a visual comparison of image reconstruction results using different methods (COSMIC, CDC, JPEG2000, HIFIC, COLIC) at high bitrates. The original image is displayed alongside its reconstructed versions by each method, enabling a direct evaluation of visual quality and fidelity. This comparison helps demonstrate the performance of COSMIC in preserving image details and structural integrity in high-bitrate settings, comparing against traditional methods and other state-of-the-art learned image compression techniques.", "section": "5 Experiments"}, {"figure_path": "itbKmreqUZ/figures/figures_20_1.jpg", "caption": "Figure 1: An example of the satellite's earth observation image and this image's corresponding sensor data as a description.", "description": "The figure shows an example of a satellite image along with its associated metadata.  The image depicts a coastal area with industrial sites and urban development. The accompanying metadata includes geographic coordinates (latitude and longitude), ground sample distance (GSD), cloud cover percentage, timestamp, and a binary representation of entropy coding.  This illustrates the multi-modal nature of satellite data\u2014an image paired with rich sensor information that describes the image's context (location, time, acquisition parameters, etc.).  This multi-modal data is leveraged in the COSMIC method described in the paper.", "section": "1 Introduction"}, {"figure_path": "itbKmreqUZ/figures/figures_20_2.jpg", "caption": "Figure 1: An example of the satellite's earth observation image and this image's corresponding sensor data as a description.", "description": "This figure shows a sample satellite image and its associated metadata.  The image depicts a coastal area with a port, industrial facilities, and residential areas. The metadata includes information such as the image's geographic coordinates (latitude and longitude), ground sample distance (GSD), cloud cover percentage, and timestamp. This illustrates the multimodal nature of satellite data \u2013 the image itself plus rich contextual information \u2013 which the COSMIC method utilizes.", "section": "1 Introduction"}, {"figure_path": "itbKmreqUZ/figures/figures_21_1.jpg", "caption": "Figure 1: An example of the satellite's earth observation image and this image's corresponding sensor data as a description.", "description": "This figure shows an example of satellite imagery and its associated metadata. The image depicts a coastal area with industrial facilities, residential areas, and a port.  The accompanying metadata includes information such as geographical coordinates (latitude and longitude), ground sample distance (GSD), cloud cover percentage, timestamp, and possibly other sensor readings.  This illustrates the multi-modal nature of satellite data, where the image itself is complemented by rich contextual information that further enhances understanding and analysis.", "section": "1 Introduction"}, {"figure_path": "itbKmreqUZ/figures/figures_21_2.jpg", "caption": "Figure 1: An example of the satellite's earth observation image and this image's corresponding sensor data as a description.", "description": "This figure shows a sample satellite image with its metadata. The image depicts a coastal area with a port, industrial facilities, and residential areas.  The metadata includes geographical coordinates (latitude and longitude), ground sample distance, cloud cover percentage, and timestamp, illustrating the multi-modal nature of satellite data.  This multi-modal aspect is key to COSMIC's approach, using the metadata as conditional information to improve the image reconstruction during decompression. The satellite's sensor data acts as a description or context for the image, enhancing its reconstruction, especially when using lightweight encoders with limited feature extraction capabilities.", "section": "1 Introduction"}, {"figure_path": "itbKmreqUZ/figures/figures_22_1.jpg", "caption": "Figure 2: COSMIC framework. (a) Compression module for satellite images: a lightweight encoder and a compensation-based decoder (Sec. 4.3). (b) In the noise prediction network, each Cross-Attention (CA) block receives embedding of the Metadata Encoder (ME) (Sec. 4.4), and the Vanilla Convolution (VC) blocks use latent image discrete encoding to guide the prediction of noise for each diffusion step. (c) & (d) Convolution attention module and lightweight convolution block (Sec. 4.2).", "description": "This figure illustrates the architecture of COSMIC, which comprises a compression module and a compensation module. The compression module uses a lightweight encoder on the satellite to compress the images and then a decoder on the ground to decompress them. The compensation module uses a diffusion model on the ground to compensate for the loss of detail caused by the lightweight encoder.  The metadata encoder (ME) processes additional sensor data alongside the image for improved image reconstruction.", "section": "4 Method"}, {"figure_path": "itbKmreqUZ/figures/figures_22_2.jpg", "caption": "Figure 1: An example of the satellite's earth observation image and this image's corresponding sensor data as a description.", "description": "This figure shows an example of satellite imagery with associated metadata.  The image itself is a satellite photo, likely of a coastal area with industrial features. Accompanying the image are various pieces of sensor data including coordinates (latitude and longitude), ground sample distance (GSD), cloud cover percentage, and a timestamp.  This metadata provides crucial contextual information supplementing the visual data, emphasizing the multi-modal nature of satellite data which is a key component of the COSMIC method.", "section": "1 Introduction"}, {"figure_path": "itbKmreqUZ/figures/figures_23_1.jpg", "caption": "Figure 2: COSMIC framework. (a) Compression module for satellite images: a lightweight encoder and a compensation-based decoder (Sec. 4.3). (b) In the noise prediction network, each Cross-Attention (CA) block receives embedding of the Metadata Encoder (ME) (Sec. 4.4), and the Vanilla Convolution (VC) blocks use latent image discrete encoding to guide the prediction of noise for each diffusion step. (c) & (d) Convolution attention module and lightweight convolution block (Sec. 4.2).", "description": "This figure illustrates the architecture of the COSMIC model, which consists of two main modules: a compression module and a compensation module.  The compression module uses a lightweight encoder to compress the satellite image on the satellite itself, followed by entropy coding. This compressed information is then transmitted to the ground station. To compensate for information loss due to the lightweight encoder, a compensation module is employed at the ground station. This module uses a decoder and a diffusion model, incorporating metadata from the satellite's sensors, to reconstruct the image with enhanced details.", "section": "4 Method"}, {"figure_path": "itbKmreqUZ/figures/figures_23_2.jpg", "caption": "Figure 2: COSMIC framework. (a) Compression module for satellite images: a lightweight encoder and a compensation-based decoder (Sec. 4.3). (b) In the noise prediction network, each Cross-Attention (CA) block receives embedding of the Metadata Encoder (ME) (Sec. 4.4), and the Vanilla Convolution (VC) blocks use latent image discrete encoding to guide the prediction of noise for each diffusion step. (c) & (d) Convolution attention module and lightweight convolution block (Sec. 4.2).", "description": "This figure illustrates the architecture of the COSMIC model, which consists of two main modules: a compression module and a compensation module.  The compression module uses a lightweight encoder to compress satellite images on-board, and a decoder on the ground to reconstruct the images. Due to the reduced complexity of the on-board encoder, the compensation module uses a diffusion model to reconstruct details using metadata information from the satellite, improving the fidelity of the reconstructed images.  The figure also details the components of the lightweight encoder and the compensation module, including a lightweight convolution block (LCB), a Cross-Attention (CA) block, and a Vanilla Convolution (VC) block. ", "section": "4 Method"}, {"figure_path": "itbKmreqUZ/figures/figures_24_1.jpg", "caption": "Figure 2: COSMIC framework. (a) Compression module for satellite images: a lightweight encoder and a compensation-based decoder (Sec. 4.3). (b) In the noise prediction network, each Cross-Attention (CA) block receives embedding of the Metadata Encoder (ME) (Sec. 4.4), and the Vanilla Convolution (VC) blocks use latent image discrete encoding to guide the prediction of noise for each diffusion step. (c) & (d) Convolution attention module and lightweight convolution block (Sec. 4.2).", "description": "This figure illustrates the architecture of the COSMIC model, highlighting the compression and compensation modules.  The compression module uses a lightweight encoder to reduce computational burden on the satellite and an entropy model. The compensation module uses a diffusion model to compensate for information loss caused by the simplified encoder. Metadata information from the satellite is incorporated using a metadata encoder and cross-attention blocks within the diffusion model.  Lightweight convolution blocks (LCBs) are employed to further enhance the efficiency of the encoder.", "section": "4 Method"}, {"figure_path": "itbKmreqUZ/figures/figures_24_2.jpg", "caption": "Figure 2: COSMIC framework. (a) Compression module for satellite images: a lightweight encoder and a compensation-based decoder (Sec. 4.3). (b) In the noise prediction network, each Cross-Attention (CA) block receives embedding of the Metadata Encoder (ME) (Sec. 4.4), and the Vanilla Convolution (VC) blocks use latent image discrete encoding to guide the prediction of noise for each diffusion step. (c) & (d) Convolution attention module and lightweight convolution block (Sec. 4.2).", "description": "This figure illustrates the architecture of COSMIC, which consists of two main modules: the compression module and the compensation module. The compression module uses a lightweight encoder to compress the satellite image and an entropy model. The compensation module utilizes a noise prediction network and a decoder to reconstruct the image, compensating for details lost during the compression process using metadata from the satellite's sensors.  The figure also shows the internal structures of the lightweight convolution block (LCB) and convolution attention module (CAM).", "section": "4 Method"}, {"figure_path": "itbKmreqUZ/figures/figures_25_1.jpg", "caption": "Figure 2: COSMIC framework. (a) Compression module for satellite images: a lightweight encoder and a compensation-based decoder (Sec. 4.3). (b) In the noise prediction network, each Cross-Attention (CA) block receives embedding of the Metadata Encoder (ME) (Sec. 4.4), and the Vanilla Convolution (VC) blocks use latent image discrete encoding to guide the prediction of noise for each diffusion step. (c) & (d) Convolution attention module and lightweight convolution block (Sec. 4.2).", "description": "This figure shows the architecture of the COSMIC model, which consists of two main components: a compression module and a compensation module. The compression module uses a lightweight encoder to compress satellite images and an entropy model. The compensation module uses a decoder and a diffusion model to compensate for the loss of information during compression. The figure also shows the different components of the noise prediction network, including the cross-attention blocks, the vanilla convolution blocks, and the metadata encoder. The lightweight convolution block is also shown.", "section": "4 Method"}, {"figure_path": "itbKmreqUZ/figures/figures_25_2.jpg", "caption": "Figure 2: COSMIC framework. (a) Compression module for satellite images: a lightweight encoder and a compensation-based decoder (Sec. 4.3). (b) In the noise prediction network, each Cross-Attention (CA) block receives embedding of the Metadata Encoder (ME) (Sec. 4.4), and the Vanilla Convolution (VC) blocks use latent image discrete encoding to guide the prediction of noise for each diffusion step. (c) & (d) Convolution attention module and lightweight convolution block (Sec. 4.2).", "description": "This figure illustrates the architecture of COSMIC, which is composed of a compression module and a compensation module. The compression module is designed for satellite scenarios, it includes a lightweight encoder to reduce computation on the satellite and an entropy model, and a decoder deployed on the ground. The compensation module is located on the ground and consists of a metadata encoder, a noise prediction network and a vanilla convolution block. The noise prediction network uses both latent image discrete encoding and metadata embedding to predict noise for each diffusion step.", "section": "4 Method"}]