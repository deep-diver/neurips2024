[{"type": "text", "text": "An Analysis of Elo Rating Systems via Markov Chains ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sam Olesker-Taylor Luca Zanetti Department of Statistics Department of Mathematical Sciences University of Warwick University of Bath Coventry, CV4 7AL, UK Bath, BA2 7AY, UK sam.olesker-taylor@warwick.ac.uk lz2040@bath.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We present a theoretical analysis of the Elo rating system, a popular method for ranking skills of players in an online setting. In particular, we study Elo under the Bradley\u2013Terry\u2013Luce model and, using techniques from Markov chain theory, show that Elo learns the model parameters at a rate competitive with the state of the art. We apply our results to the problem of efficient tournament design and discuss a connection with the fastest-mixing Markov chain problem. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Elo rating system is a popular method for calculating the relative skills of players (or teams) in sports analytics and particularly chess [2, 4, 1, 3]. It is based on a simple zero-sum update rule: if player $i$ beats player $j$ , then the rating of player $i$ increases proportionally to the model probability that $i$ would lose to $j$ , while the rating of $j$ decreases by the same amount. This amount depends on the previously estimated difference in skills between $i$ and $j$ . ", "page_idx": 0}, {"type": "text", "text": "Despite their widespread popularity, Elo rating systems still lack a rigorous theoretical understanding [6]. Here, we take a probabilistic approach and study Elo under the well-known Bradley\u2013Terry\u2013 Luce model (BTL) [11, 28]. In this model, the probability $p_{i,j}$ that $i$ wins against $j$ is $w_{i}/(w_{i}+w_{j})$ , where $w_{k}$ is the strength of player $k$ . In Elo, this is usually reparametrised via $w_{k}=\\mathrm{e}^{\\rho_{k}}$ : ", "page_idx": 0}, {"type": "equation", "text": "$$\np_{i,j}=\\mathrm{e}^{\\rho_{i}}/(\\mathrm{e}^{\\rho_{i}}+\\mathrm{e}^{\\rho_{j}})=1/(1+\\mathrm{e}^{\\rho_{j}-\\rho_{i}})=\\sigma(\\rho_{i}-\\rho_{j}),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\rho_{k}$ is the true rating of $k$ and $\\sigma(z):=1/(1+\\exp(-z))$ for $z\\in\\mathbb{R}$ is the sigmoid function. In this setting, after observing $i$ beat $j$ , the corresponding Elo ratings $x_{i}$ and $x_{j}$ are updated as ", "page_idx": 0}, {"type": "equation", "text": "$$\nx_{i}\\leftarrow x_{i}+\\eta\\sigma(x_{j}-x_{i})\\quad\\mathrm{and}\\quad x_{j}\\leftarrow x_{j}-\\eta\\sigma(x_{j}-x_{i}),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where the step-size $\\eta>0$ is chosen by the modeller. The size of the update depends exponentially on the difference in ratings: beating a much lower rated opponent does not change the ratings much. ", "page_idx": 0}, {"type": "text", "text": "The goal of the Elo rating system is to estimate the true ratings of $n$ players by observing results of matches between pairs of players. It is, therefore, aiming to solve the problem of ranking from pairwise comparisons. Compared with most algorithms in the area [5, 20, 27, 31, 37], however, Elo benefits from three qualities that help explain its popularity in real-world applications: simplicity, interpretability and the ability to update a ranking of the players in an online fashion. ", "page_idx": 0}, {"type": "text", "text": "The knowledgeable reader might have noticed that Elo\u2019s update rule is actually based on the gradient of the BTL log-likelihood: Elo can be interpreted simply as stochastic gradient descent with fixed step-size. Rather than studying Elo from a convex optimisation angle, however, we take a more probabilistic point of view: we assume at each time $t$ , players $i$ and $j$ are selected to play against one another with probability $q_{i,j}$ . This allows us to interpret Elo as a Markov chain over $\\mathbb{R}^{n}$ and deploy powerful tools to study its behaviour. On the other hand, this approach also presents us with some challenges: Elo is not a reversible Markov chain and, while it has a unique stationary distribution, assuming a minor and natural condition on $(q_{i,j})_{i,j}$ [7], it does not converge to it in total variation [6]. ", "page_idx": 0}, {"type": "text", "text": "1.1 Our Results ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our main contribution (Theorem 2.5) shows that Elo ratings, averaged over time, well-approximate the true ratings of the players with high probability. In order to avoid the potential of unbounded ratings, which are unrealistic in practice, we consider a variant of Elo in which ratings are capped, whilst maintaining their zero-sum property. We obtain rates of convergence with respect to the number of observed matches that are competitive against the state of the art in the BTL literature. In contrast to most other algorithms for the BTL model studied in the literature, Elo learns the parameters of a BTL model in an online fashion. These rates of convergence is remarkable since Elo was originally conceived as a simple ranking system for chess players. Furthermore, our approach is very robust, and also applies to a parallel set-up in which multiple games are played concurrently. ", "page_idx": 1}, {"type": "text", "text": "We also discuss the problem of tournament design: we assume we are given a tournament (comparison) graph $G=([n],E)$ , and we would like to choose the match-up probabilities $(q_{i,j})_{\\{i,j\\}\\in E}$ so that Elo\u2019s convergence rate is maximised. We highlight a connection between this problem and that of finding the fastest mixing Markov chain on $G$ [10, 39], where the corresponding Markov chain on the graph is either in discrete or continuous time depending on whether we optimise number of games or parallel rounds, respectively. As far as we know, this connection has not been made formally before in the sequential set-up, and both the analysis and optimisation of the parallel set-up are new. In $\\S4$ , we also provide experimental results that showcase the usefulness of our strategy. ", "page_idx": 1}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Despite Elo ratings being the standard ranking system in many sports analytics communities [38], there is a scarcity of work analysing Elo from a probabilistic perspective. In particular, we are aware only of [6], and the unpublished notes [7] by the same author, in which Elo is studied as a Markov process and convergence in distribution to a unique stationary distribution is proved. ", "page_idx": 1}, {"type": "text", "text": "If we consider Elo as a technique to estimate the parameters of the BTL model, there is a wealth of recent literature on the topic by the machine learning community [20, 37, 31, 5, 27, 9]. In contrast to our setting, however, previous work typically considers an offilne scenario, in which the ratings of the players are computed after all the scheduled matches have taken place. By standard concentration inequalities, this allows one to obtain a very good approximation of the probability a player wins against their neighbours in the comparison graph. The goal is then to deduce a global ranking of the players from such (very good) local information. In our setting, Elo ratings are dynamically updated before a good local approximation is achieved, which makes the analysis more challenging and requires the use of powerful, but delicate, concentration inequalities for Markov chains. ", "page_idx": 1}, {"type": "text", "text": "A comparison between the rate of convergence for Elo ratings obtained in our work and the rate of convergence of other algorithms for the BTL model is discussed in $\\S2$ . ", "page_idx": 1}, {"type": "text", "text": "Finally, we mention that the connection between Markov chains and stochastic gradient descent with fixed step size has been studied before, e.g., in [17]. ", "page_idx": 1}, {"type": "text", "text": "2 Convergence Rates of Elo Ratings ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we state our main result on the convergence rate of the time-averaged Elo ratings, discuss related work and outline the most important parts of the proof. ", "page_idx": 1}, {"type": "text", "text": "Throughout the paper, \u201c $f\\lesssim g^{\\rangle}$ means $\\bullet\\,f=O(g)^{\\bullet}$ , \u201c $\\cdot f\\ll g^{,}$ means ${\\star}f=o(g)^{,}$ and \u201cwhp\u201d means \u201cwith probability $1-O(1/n)^{*}$ ; the notation $\\tilde{O}(\\cdot)$ hides logarithmic factors; finally, $\\mathbb{E}_{\\pi}[\\cdot]$ indicates that $\\bar{X^{0}}\\sim\\pi$ . ", "page_idx": 1}, {"type": "text", "text": "2.1 Our Results ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We start with the explicit definition of our Markov chain. ", "page_idx": 1}, {"type": "text", "text": "Definition 2.1 (Elo Process). Let $M\\in\\mathbb{R}$ and $n\\geq2$ . Let $\\rho\\in[-M,M]^{n}$ with $\\textstyle\\sum_{k}\\rho_{k}=0$ . Let $q$ be a distribution on unordered pairs in $[n]$ . Let $\\eta\\in(0,\\frac{1}{4})$ . A step of $\\operatorname{Elo}_{M}(q,\\rho;\\eta)$ proceeds as follows. 0. Suppose that the current vector of ratings is $x\\in\\mathbb{R}^{n}$ . ", "page_idx": 1}, {"type": "text", "text": "1. Choose unordered pair $\\{I,J\\}$ to play according to $q$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\{I,J\\}=\\{i,j\\}]=q_{\\{i,j\\}}\\quad\\mathrm{for~all}\\quad i,j\\in[n].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "2. Suppose that Player $I$ beats $J$ , which has probability $\\sigma(\\rho_{I}-\\rho_{J})$ . Update ratings $x_{I}$ and $x_{J}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{I}\\leftarrow x_{I}+\\eta\\sigma(x_{J}-x_{I});\\quad x_{J}\\leftarrow x_{J}-\\eta\\sigma(x_{J}-x_{I}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3. Orthogonally project the full vector of ratings to $\\begin{array}{r}{[-M,M]^{n}\\cap\\{x^{\\prime}\\in\\mathbb{R}^{n}\\mid\\sum_{k}x_{k}^{\\prime}=0\\}.}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "Let $X_{k}^{t}$ denote the rating of Player $k$ at time $t$ , and $\\pi$ the equilibrium distribution on $\\mathbb{R}^{n}$ . Denote by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{k}^{t,T}:=\\frac{1}{t}\\sum_{s=T}^{T+t-1}X_{k}^{s}\\quad\\mathrm{for}\\quad k\\in[n]\\quad\\mathrm{and}\\quad t,T>0}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "the time-averaged ratings. We typically start from $X^{0}=(0,...,0)$ . The (deterministic) time $T$ is a burn-in phase, which allows the Elo ratings to get \u2018near\u2019 the true skills, after which we start averaging. Remark 2.2. The projection step ensures the Elo ratings do not become too large. It is required for our analysis, but not usually implemented in practice. Indeed, our experiments, discussed in $\\S4$ , suggest that, as long as the step-size $\\eta$ is small enough, Elo ratings remain bounded. Algorithms which estimate BTL parameters typically require some sort of projection [20] or regularisation [27]. A simple and efficient algorithm to realise the orthogonal projection is presented in the Appendix. ", "page_idx": 2}, {"type": "text", "text": "We show, for a suitable choice of parameters $t$ , $T$ and $\\eta$ , that the time-averages are concentrated around the true ratings. In other words, we can use Elo to obtain an MCMC estimate of the true ratings. Elo ratings in equilibrium are, in general, a biased estimator of the true ratings: i.e., $\\mathbb{E}_{\\pi}[X_{k}^{0}]\\neq\\rho_{k}$ . Hence, there will be both a bias and an error term in our MCMC-type estimate of $\\|A^{t,T}-\\rho\\|_{2}$ . ", "page_idx": 2}, {"type": "text", "text": "The MCMC convergence rate depends on a spectral gap $\\lambda_{q}$ . This parameter quantifies how fast local information about the relative strengths of two players is propagated to the rest of the ratings. Similar parameters appear in most of the related literature, e.g., [37, 27]. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.3 (Spectral Gap). Let $q$ be a distribution on unordered pairs in $[n]$ . Define $q_{i,j}:=q_{\\{i,j\\}}$ , $\\begin{array}{r}{d_{i,j}:={\\bf1}\\{i=j\\}\\sum_{k}q_{i,k}}\\end{array}$ and $\\Delta_{i,j}\\,:=\\,d_{i,j}\\,-\\,q_{i,j}$ for $i,j\\,\\in\\,[n]$ . Let $\\lambda_{q}$ denote the spectral gap (second smallest eigenvalue) of the Laplacian $\\Delta$ . Always, $\\Delta\\mathbf{1}=\\mathbf{0}$ and $\\Delta$ is positive semi-definite. Remark 2.4. Equivalently, $\\lambda_{q}$ is the spectral gap of the continuous-time Markov chain on $[n]$ with transition rates $q_{i,j}=q_{\\{i,j\\}}$ for $i,j\\in[n]$ . Note the scaling: $\\textstyle\\sum_{i,j}q_{i,j}=2$ . So, the typical time until the continuous-time chain jumps is order $n$ , not order 1. This implies $\\lambda_{q}\\leq4/n$ ; see Lemma B.2. ", "page_idx": 2}, {"type": "text", "text": "We assume $\\lambda_{q}>0$ . This holds unless there exists a non-empty subset $S\\subsetneq[n]$ with $\\begin{array}{r l}{\\sum_{i\\in S,j\\notin S}q_{i,j}=}&{{}}\\end{array}$ 0\u2014i.e., players in $S$ never play those in $S^{c}$ . This makes estimation of the ratings impossible. ", "page_idx": 2}, {"type": "text", "text": "Our main result measures the disparity between the time-averaged Elo ratings and the true ratings. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2.5 (Convergence Rate). Let $X\\sim\\mathrm{Elo}_{M}(q,\\rho;\\eta)$ , as in Definition 2.1. Let $C_{1},C_{2}<\\infty$ . Then, there exists a constant $C_{0}$ , depending only on $\\left(C_{1},C_{2}\\right)$ , such that $i f$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\{\\lambda_{q},\\eta,1/t\\}\\ge n^{-{\\cal C}_{1}}\\quad a n d\\quad\\operatorname*{min}\\{t,{\\cal T}\\}\\ge C_{0}t_{\\star},\\quad w h e r e\\quad t_{\\star}:=e^{2M}\\eta^{-1}\\lambda_{q}^{-1}\\log n,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "then ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg[\\frac{1}{n}\\|A^{t,T}-\\rho\\|_{2}^{2}\\leq\\frac{C_{0}e^{4M}}{\\lambda_{q}n}\\bigg(\\eta+\\frac{(\\log n)^{2}}{\\lambda_{q}t}\\bigg)\\bigg]\\geq1-n^{-C_{2}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In particular, if $\\eta\\asymp(\\log n)^{2}/(\\lambda_{q}t)$ , then ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\|A^{t,T}-\\rho\\|_{2}^{2}\\lesssim\\frac{e^{4M}(\\log n)^{2}}{\\lambda_{q}n}\\frac{1}{\\lambda_{q}t}\\quad\\mathrm{whp}\\quad a s\\quad n\\to\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Remark 2.6. Ideally, $\\lambda_{q}n=\\tilde{\\Omega}(1)$ ; e.g., if $q$ is uniform over the edges of an expander graph, then $\\lambda_{q}n\\,\\asymp\\,1$ . In this case, we view $(\\log n)^{2}/(\\lambda_{q}n)\\,=\\,\\tilde{O}(1)$ as the error\u2019s pre-factor and $1/(\\lambda_{q}t)$ as the (squared) convergence rate. Also, we can then choose $\\eta\\,=\\,\\tilde{\\Omega}(1)$ and obtain non-trivial convergence results. This choice of $\\eta$ is comparable to that used in practice. Moreover, on average, only $\\bar{O}(1)$ games per player are need to be observed to guarantee good approximation of the ratings. ", "page_idx": 2}, {"type": "text", "text": "The $\\eta$ term arises from the average bias $\\frac{1}{n}\\|\\mathbb{E}_{X\\sim\\pi}[X]\\!-\\!\\rho\\|_{2}^{2}$ , which is non-zero in general. An estimate on the average variance $\\textstyle{\\frac{1}{n}}\\sum_{k}\\operatorname{\\mathbb{V}}\\!{\\mathbf{ar}}_{X\\sim\\pi}{\\big[}X_{k}{\\big]}$ is necessary to obtain the MCMC convergence rate. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2.7 (Bias and Variance). If $\\pi$ is the equilibrium distribution of $\\operatorname{Elo}_{M}(q,\\rho;\\eta)$ , then ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\|\\mathbb{E}_{X\\sim\\pi}[X]-\\rho\\|_{2}^{2}\\leq4e^{4M}\\eta/(\\lambda_{q}n)\\quad a n d\\quad\\frac{1}{n}\\sum_{k}\\mathbb{V}\\mathrm{ar}_{X\\sim\\pi}[X_{k}]\\leq4e^{2M}\\eta/(\\lambda_{q}n).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Despite Elo ratings being biased, the estimated probability a player wins their next match is not: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{j\\in[n]}q_{i,j}\\,\\mathbb{E}_{X\\sim\\pi}[\\sigma(X_{i}-X_{j})]=\\sum_{j\\in[n]}q_{i,j}\\sigma(\\rho_{i}-\\rho_{j})\\quad\\mathrm{for~all}\\quad i\\in[n].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This holds when no projection step is performed as part of the Elo update (equivalently, $M:=+\\infty)$ ). ", "page_idx": 3}, {"type": "text", "text": "The mixing time is more delicate, as the chain makes deterministic-size discrete jumps, but its equilibrium distribution is continuous and, therefore, the chain does not converge in total variation. Instead, we measure convergence in the Wasserstein, also known as transportation, distance. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.8 (Contraction). If $X,Y\\sim\\mathrm{Elo}_{M}(q,\\rho;\\eta)$ , then there exists a step-by-step coupling with ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{(x,y)}[\\|X^{t}-Y^{t}\\|_{2}^{2}]\\leq(1-\\kappa)^{t}\\|x-y\\|_{2}^{2}\\quad w h e r e\\quad\\kappa:=\\frac{1}{8}e^{-2M}\\eta\\lambda.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Markov chains satisfying the contraction property (i.e., positively curved) satisfy powerful concentration inequalities, developed particularly by Joulin and Ollivier [22, 23, 34, 33, 24]. The idea is that if $\\|X^{0}-\\dot{Y}^{0}\\|_{2}=D^{0}$ , then $\\bar{D^{t}}:=\\|X^{t}-\\mathbf{\\bar{\\boldsymbol{Y}}}^{t}\\|_{2}$ is small when $t\\asymp\\kappa^{-1}\\log D^{0}$ . For us, $D^{0}\\leq2M n$ , leading to $t_{\\star}\\asymp\\kappa^{-1}\\log n$ . In the reversible case, this can provide bounds, e.g., on the spectral gap. ", "page_idx": 3}, {"type": "text", "text": "The approach is particularly applicable to Markov chains on finite metric spaces, such as graphs. The set-up of finite graphs was analysed by Bubley and Dyer [12] under the name path coupling. In this case, $\\mathbb{E}[D^{t}]\\leq{\\frac{\\daleth}{2}}$ implies $\\mathbb{P}[X^{t}\\,{=}\\,Y^{t}]^{\\!\\!\\prime}\\!\\geq\\frac{1}{2}$ , since the minimum graph distance between $x\\neq y$ is 1. ", "page_idx": 3}, {"type": "text", "text": "Our underlying metric space is $(\\mathbb{R}^{n},\\|\\cdot\\|_{2})$ , however. Inequalities for this more general set-up have been developed, but they often give weaker bounds. In our case, they lose a factor $\\bar{1}/(\\eta\\lambda_{q})\\gtrsim\\bar{n}$ in the convergence rate. Morally, though, the exponential convergence at rate $\\kappa$ still implies that $1/\\kappa$ is the correct timescale for mixing and concentration, perhaps up to some logarithmic factors. Establishing this rigorously is the most challenging part of our work from a technical point of view. ", "page_idx": 3}, {"type": "text", "text": "2.2 Comparison with Related Work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we compare the convergence rate of Elo given by Theorem 2.5 against the state of the art for BTL estimation. We highlight, once again, that previous work focusses on the problem of offilne estimation, whilst Elo works in the more challenging online setting. Nevertheless, Elo is able to match the state-of-the-art algorithms in the offline setting for a wide range of parameters. ", "page_idx": 3}, {"type": "text", "text": "Our results imply that Elo provides an estimator $\\hat{\\rho}$ of $\\rho$ with $\\|\\hat{\\rho}-\\rho\\|_{2}^{2}\\lesssim\\mathrm{e}^{4M}(\\log n)^{2}/(\\lambda_{q}^{2}t)$ whp. This matches, up to a log factor, the results by Hajek, Oh and $\\mathrm{Xu}$ [20], who prove that the MLE constrained on $[{\\bar{-}}M,M]^{\\bar{n}}\\cap\\{x\\in\\mathbb{R}^{n}\\mid\\sum_{k}x_{k}=0\\}$ , $\\rho_{\\mathrm{MLC}}$ , satisfies $\\|\\bar{\\rho}_{\\mathrm{MLC}}\\!-\\!\\rho\\|_{2}^{2}\\lesssim\\mathrm{e}^{8M}\\log n/(\\lambda_{q}^{2}t)$ . The dependency on $\\lambda_{q}$ has been improved by Shah, Balakrishnan and Bradley [37], who show $\\|\\rho_{\\mathrm{MLC}}-\\rho\\|_{2}^{2}\\lesssim n\\mathrm{e}^{8M}\\mathrm{log}\\,n/(\\lambda_{q}t)$ . Since $n/\\lambda_{q}\\,\\geq\\,1/4$ , this is at least as good as our result, and potentially better for certain choices of $q$ . However, up to log factors, our result matches theirs when $q$ corresponds to sampling edges of an expander graph\u2014e.g., a complete graph, or an Erd\u02ddos\u2013R\u00e9nyi graph with parameter $p\\breve{\\gg}(\\log n)/n$ . Our result improves the constant in front of $M$ . ", "page_idx": 3}, {"type": "text", "text": "More recently, Li, Shrotriya and Rinaldo [27] proved a regularised version of the MLE, $\\rho_{\\mathrm{MLR}}$ , achieves $\\|\\rho_{\\mathrm{MLR}}-\\rho\\|_{2}^{\\tilde{2}}\\lesssim\\mathrm{e}^{2M_{E}}\\delta/\\tilde{(\\lambda_{q}^{2}t)}$ , where $\\delta$ is the ratio between the maximum and average degree of the comparison graph $G=([n],E)$ with $E=\\{\\{i,j\\}\\mid q_{i,j}>0\\}$ and $\\begin{array}{r}{M_{E}=\\operatorname*{max}_{i,j:q_{i,j}>0}\\left|\\rho_{i}-\\rho_{j}\\right|\\leq}\\end{array}$ $2\\operatorname*{max}_{k}\\left|\\rho_{k}\\right|$ . A version of our analysis also applies with $M_{E}$ , instead of $M$ , but we are not able to prove $\\mathrm{max}_{i,j:q_{i,j}>0}\\,|X_{i}^{t}-X_{j}^{t}|<\\dot{M}_{E}+1$ holds for polynomially long. Notice that their result is weaker when the comparison graph is relatively sparse but has a few high degree nodes. Moreover, they require each player to play the same number of matches; i.e., $q_{i,j}=1/|E|$ for $\\{i,j\\}\\in E$ . Both limitations are particularly problematic in the context of tournament design discussed in $\\S3$ . On the other hand, they obtain $\\ell_{\\infty}$ error bounds too. We refer to [27] for further discussion of previous work. ", "page_idx": 3}, {"type": "text", "text": "2.3 Outline of Proof ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now highlight the key steps in estimating the MCMC-type error $\\frac{1}{n}\\|A^{t,T}-\\rho\\|_{2}^{2}$ in Theorem 2.5, where $A^{t,T}$ is the $t$ -step time average of the ratings $X$ after a burn-in phase of length $T$ . We use ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|A^{t,T}-\\rho\\|_{2}\\leq\\|A^{t,T}-\\mathbb{E}_{\\pi}[X^{0}]\\|_{2}+\\|\\mathbb{E}_{\\pi}[X^{0}]-\\rho\\|_{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "bounding these error and bias terms separately, via Theorems 2.8 and 2.7, respectively. Their proofs rely on estimating the change in $\\ell_{2}$ norm a single step, and using the Lipschitz property of the sigmoid function $\\sigma$ , along with some other careful manipulations. The full proofs are given in the appendix. ", "page_idx": 4}, {"type": "text", "text": "The primary challenge is to leverage the positive curvature result of Theorem 2.8 to deduce the required concentration inequality in Theorem 2.5. As noted at the end of $\\S2.1$ , positive curvature is well-suited to this goal, but the specifics of our set-up cause significant difficulties. Particularly, the aforementioned results reliant only on curvature are not sufficiently strong for our purposes. ", "page_idx": 4}, {"type": "text", "text": "A further complication is that Elo is a non-reversible Markov chain. There is a general understanding, or perhaps belief, in the MCMC community that non-reversibility can improve concentration results. This has lead to many strategies being proposed, such as non-backtracking, lifting, or zig-zag; see, e.g., [14, 16, 30, 8, 25]. Even an alternative definition of the spectral gap has been proposed [13]; this gap controls convergence of empirical averages, rather than convergence to equilibrium. ", "page_idx": 4}, {"type": "text", "text": "Unfortunately, the majority of general concentration results based on spectral properties [19, 26, 35, 36] are actually worse in the non-reversible set-up: they bound the convergence rates by those of the multiplicative or additive reversibilisation. These can be hard to estimate, needing detailed knowledge of the equilibrium distribution, and the resulting bounds are often crude. ", "page_idx": 4}, {"type": "text", "text": "Finally, we mention that bounds on the total-variation mixing time can be leveraged to provide concentration bounds; see, particularly, [15, 35]. However, Elo does not converge in total variation! Indeed, $X^{t}$ is finitely supported, on at most $(2n)^{t}$ states, given $X^{0}$ , but $\\pi$ is continuous. These $(2n)^{t}$ states can be used to distinguish $X^{t}$ and $\\pi$ . Nevertheless, it is this mixing-time approach that we adapt. ", "page_idx": 4}, {"type": "text", "text": "The contraction property implies that two realisations $X$ and $Y$ can be coupled so that their expected relative distance decreases exponentially, with rate $\\kappa$ . This is ideal for using the coupling approach to mixing. However, it is not possible to guarantee that $X^{t}=Y^{t}$ at some point, due to the chain\u2019s finite support. Instead, we analyse a noisy version: after each update, some (continuous) noise is added to the ratings. This must be added carefully to preserve the contraction in Theorem 2.8. ", "page_idx": 4}, {"type": "text", "text": "Denote the noisy versions $U$ and $V$ . Once $U$ and $V$ are sufficiently close, the additive noise can be used to couple them exactly, to get $U^{t}=V^{t}$ . The size of the noise must be balanced: too small and this final coupling step is too difficult; too big and $U$ can no longer be compared with $X$ . ", "page_idx": 4}, {"type": "text", "text": "We delve deeper into this noisy approximation, explaining what noise to add, how it accumulates and what the resulting mixing time is. The interaction of the noise with the projection step is delicate and technical; we omit it from the description here, but it should be kept in the back of the mind. ", "page_idx": 4}, {"type": "text", "text": "Noisy Version & Error. We consider the following noisy version $U$ of the Elo process $X$ . Let $\\delta>0$ . 1. Draw $U^{t+1/2}$ according to an uncapped Elo step started from $U^{t}$ \u2014i.e., as if $M\\,=\\,\\infty$ . Suppose that Players $i$ and $j$ were chosen\u2014i.e., $\\{k\\mid U_{k}^{t+1/2}\\neq U_{k}^{t}\\}=\\{i,j\\}$ . 2. Draw $\\tilde{U}_{i},\\tilde{U}_{j}\\sim^{\\mathrm{iid}}\\mathrm{Unif}([-\\sqrt{\\delta},+\\sqrt{\\delta}])$ independently and set $\\tilde{U}_{k}:=0$ for $k\\notin\\{i,j\\}$ . Set $\\begin{array}{r}{U_{k}^{t+1}:=U_{k}^{t+1/2}+\\tilde{U}_{k}\\quad\\mathrm{for}\\quad k\\in[n].}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "This does not preserve the zero-sum property, as the additive noise is independent. The independence is crucial later to allow two version to coalesce. We need to control the cumulation until that point. ", "page_idx": 4}, {"type": "text", "text": "We control the difference between $X$ and $U$ via the natural coupling: pick the same pair of players to play, and observe the same result; sample the noise independently. Careful computation gives ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|X^{t+1}-U^{t+1}\\|_{1}\\le\\|X^{t+1/2}-U^{t+1/2}\\|_{1}+2\\sqrt\\delta\\le\\|X^{t}-U^{t}\\|_{1}+2\\sqrt\\delta\\le\\ldots\\le2t\\sqrt\\delta.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The second inequality actually says that Elo is non-negatively curved in $\\ell_{1}$ . Iterating this, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Big\\|\\frac1t\\sum_{s=0}^{t-1}X^{s}-\\frac1t\\sum_{s=0}^{t-1}U^{s}\\Big\\|_{1}\\le\\frac1t\\sum_{s=0}^{t-1}\\|X^{s}-U^{s}\\|_{1}\\le t\\sqrt{\\delta}\\quad\\mathrm{deterministically}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Curvature. We use the natural coupling between two noisy versions $U$ and $V$ : choose the same players, observe the same result and add the same noise. Then, there is still rate- $\\kappa$ contraction: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{u^{0},v^{0}}[\\|U^{1}-V^{1}\\|_{2}]\\le\\mathbb{E}_{u^{0},v^{0}}[\\|U^{1/2}-V^{1/2}\\|_{2}]\\le(1-\\kappa)\\|u^{0}-v^{0}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Mixing Time. We bound the total-variation mixing time of the noisy chain via the coupling method: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\big\\|\\mathbb{P}_{u^{0}}[U^{t}\\in\\cdot]-\\mathbb{P}_{v^{0}}[V^{t}\\in\\cdot]\\big\\|_{\\mathrm{TV}}\\leq\\mathbb{P}_{(u^{0},v^{0})}[U^{t}\\ne V^{t}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "First, we burn-in using the natural coupling, then use a new coupling which exploits the noise. We start with the natural coupling as given above. Using the rate- $\\kappa$ curvature, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|U^{t}-V^{t}\\|_{2}]\\le(1-\\kappa)^{t}\\|U^{0}-V^{0}\\|_{2}\\le2M n e^{-\\kappa t}\\le\\delta^{2}\\quad\\mathrm{if}\\quad t\\ge t_{\\delta}:=\\kappa^{-1}\\log(2M n/\\delta^{2}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Hence, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\|U^{t}-V^{t}\\|_{\\infty}>\\delta]\\le\\mathbb{P}[\\|U^{t}-V^{t}\\|_{2}>\\delta]\\le\\delta\\quad\\mathrm{if}\\quad t\\ge t_{\\delta}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Once the absolute difference $|U_{k}^{t_{\\delta}}-V_{k}^{t_{\\delta}}|$ are at most $\\delta$ for all $k$ , the additive noise, which is order $\\sqrt{\\delta}$ , dominates the change\u221a in diff\u221aerence after a single Elo step, which is order $\\delta$ . Thus, with complementary probability order $\\bar{\\delta^{\\prime}}\\sqrt{\\delta}=\\sqrt{\\delta}$ , we can couple the noise so that the rating of a player is the same in both $U$ and in $V$ after they play a game. Moreover, such a successful step preserves the $\\ell_{\\infty}$ bound of $\\delta$ . ", "page_idx": 5}, {"type": "text", "text": "The $\\ell_{\\infty}$ bound implies that all players are chosen within $t_{\\delta}$ steps with probability at least $1-\\delta$ . Hence, the probability of not successfully matching all ratings after $t_{\\delta}$ steps is at most order $\\delta+\\sqrt{\\delta}t_{\\delta}$ . ", "page_idx": 5}, {"type": "text", "text": "Combining these two bounds gives a bound of order $\\delta+\\sqrt{\\delta}t_{\\delta}$ on the total-variation distance at time $2t_{\\delta}$ . The polynomial-growth/decay assumptions \u221ain the theorem allow us to choose $\\delta$ to be an appropriate inverse polynomial (in $n$ ) and obtain $\\delta+\\sqrt{\\delta}t_{\\delta}\\ll1$ and $t_{\\delta}\\asymp\\kappa^{-1}\\log n\\asymp t_{\\star}$ . $\\triangle$ ", "page_idx": 5}, {"type": "text", "text": "The above mixing-time bound allows us to establish concentration of time-averages of the noisy Elo ratings, using results from [35]. Particularly, under certain assumptions, if $Z$ is a Markov chain with equilibrium distribution $\\pi$ and mixing time $t_{\\star}$ , then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-\\log\\mathbb{P}_{\\pi}\\big[\\big|\\frac{1}{t}\\sum_{s=0}^{t-1}f(Z^{s})-\\pi_{f}\\big|\\geq\\zeta\\big]\\gtrsim\\sigma_{f}^{-2}\\zeta^{2}t/t_{\\star},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\pi_{f}=\\operatorname{\\mathbb{E}}_{\\pi}[f]$ and $\\sigma_{f}^{2}:=\\mathbb{V}\\mathrm{ar}_{\\pi}[f]$ . Notice that this requires $Z^{0}\\sim\\pi$ , which we do not impose; this requirement can be circumnavigated by using a burn-in, again comparing with a noisy version. We want to take $f:=f_{k}$ to be the projection onto the $k$ -th coordinate, which is the $k$ -th player\u2019s rating, for each $k$ , then do a union bound over the $n$ players. This motivates taking ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\zeta_{k}^{2}\\asymp\\sigma_{f_{k}}^{2}t_{\\star}\\frac{\\log n}{t},\\quad\\mathrm{which~satisfies}\\quad\\frac{1}{n}\\sum_{k}\\zeta_{k}^{2}\\asymp\\frac{\\eta}{\\lambda_{q}n}\\frac{\\log n}{\\lambda_{q}\\eta}\\frac{\\log n}{t}=\\frac{1}{\\lambda_{q}n}\\frac{(\\log n)^{2}}{\\lambda_{q}t},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "using Theorem 2.7. This is the decay rate required in Theorem 2.5, but for the noisy version. ", "page_idx": 5}, {"type": "text", "text": "We need to compare the noisy version with the original. We do this via the estimate established earlier: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\frac{1}{t}\\sum_{s=0}^{t-1}X^{s}-\\frac{1}{t}\\sum_{s=0}^{t-1}U^{s}\\right\\|_{1}\\leq\\frac{1}{t}\\sum_{s=0}^{t-1}\\|X^{s}-U^{s}\\|_{1}\\leq t\\sqrt{\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We are taking $t$ to be polynomial in $n$ , and can choose $\\delta^{-1}$ to be a sufficiently large polynomial so that this accumulated error is small. Care must be taken to to make all this rigorous, but once it is done, we are able to deduce the concentration result for the original Elo process. ", "page_idx": 5}, {"type": "text", "text": "3 Tournament Design ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "3.1 Our Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we assume we are given a comparison graph $G=([n],E)$ , where edge $\\{i,j\\}\\in E$ indicates that Players $i$ and $j$ are able to play against one another. We want to construct a distribution $q$ over edges $E$ of $G$ so that Elo can most efficiently approximate the true ratings of the players. ", "page_idx": 5}, {"type": "text", "text": "The decay rate in Theorem 2.5 is governed by the spectral gap $\\lambda_{q}$ . So, we want to maximise $\\lambda_{q}$ . Let ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{\\mathrm{cts}}^{\\star}:=\\operatorname*{sup}\\bigl\\{\\lambda_{q}\\ \\big|\\ q\\in[0,1]^{E},\\,\\sum_{e\\in E}q_{e}=1\\bigr\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This equals the largest spectral gap achievable by a continuous-time Markov chain on $[n]$ with transitions only across edges of $G$ and average jump-rate $1/n$ . It is the fastest-mixing Markov chain problem, introduced by Sun et al. [39], and can be formulated as a semidefinite program. OleskerTaylor and Zanetti [32] recently proved that $\\lambda_{\\mathrm{cts}}^{\\star}n\\gtrsim1/(\\mathrm{diam}\\,G)^{2}$ . This implies the following. ", "page_idx": 5}, {"type": "text", "text": "Corollary 3.1 (Optimised $q$ ). Suppose that the comparison graph $G=([n],E)$ is given. In the set-up of Theorem 2.5, there exists a distribution $q$ on the edges $E$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\|A^{t,T}-\\rho\\|_{2}^{2}\\lesssim n(\\mathrm{diam}\\,G)^{2}(\\log n)^{2}/t\\quad\\mathrm{whp}\\quad i f\\quad\\eta\\lesssim n(\\mathrm{diam}\\,G)^{2}(\\log n)^{2}/t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This choice of probabilities $q$ can improve drastically over uniform weights\u2014as used by [27]. E.g., if $G$ consists of two cliques connected by $k$ edges, then $\\lambda_{q}n\\lesssim k/n^{2}$ when $q_{e}:=1/|E|$ is uniform over $E$ , whilst the optimal $\\lambda_{\\mathrm{cts}}^{\\star}n\\asymp1$ [32]. As a consequence, with the optimal choice of $q$ , only ${\\tilde{O}}(n)$ total matches need to be played to obtain a good approximation of the true ratings, compared with ${\\tilde{O}}(n^{3}/k)$ for a uniform $q$ . Here, $\\tilde{O}(\\cdot)$ indicates the asymptotic order up to logarithmic factors. This demonstrates the power of being able to choose $q$ , given $G$ . ", "page_idx": 6}, {"type": "text", "text": "Elo naturally parallelises: if two games consist of disjoint pairs of players, then the Elo update resulting from one is independent of the result of the other. Hence, if we wish to minimise the number of rounds (i.e., sets of games that can be played in parallel), rather than games, we should consider a distribution $\\tilde{q}$ on the set $\\mathcal{M}$ of matchings of the graph $G=([n],E)$ \u2014i.e., collections of disjoint edges. ", "page_idx": 6}, {"type": "text", "text": "Definition 3.2 (Parallel Elo). Let $\\tilde{q}$ be a distribution on $\\mathcal{M}$ . A single step of $\\mathrm{ParElo}_{M}(\\tilde{q},\\rho;\\eta)$ first selects a matching $S\\subseteq E$ according to $\\tilde{q}$ and applies the Elo update (with scale $\\eta$ ) to each pair in $S$ . Then, the resulting vector is orthogonally projected back to zero-sum vectors in $[-M,+{\\bar{M}}]^{n}$ . ", "page_idx": 6}, {"type": "text", "text": "Our analysis is robust enough to handle the parallel case, obtaining convergence rate $1/(\\lambda_{q}t)$ , where now $q_{e}$ is the marginal probability that edge $e\\in E$ appears in the matching: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q_{e}:=\\sum_{S\\in\\mathcal{M}:e\\in S}\\tilde{q}_{S}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 3.3 (Parallel). Let $X\\sim\\mathrm{ParElo}_{M}(\\tilde{q},\\rho;\\eta)$ . Then, under the conditions of Theorem 2.5, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\|A^{t,T}-\\rho\\|_{2}^{2}\\lesssim\\frac{e^{4M}(\\log n)^{2}}{\\lambda_{q}n/N}\\frac{1}{\\lambda_{q}t}\\quad\\mathrm{whp}\\quad i f\\quad\\eta\\asymp\\frac{(\\log n)^{2}}{\\lambda_{q}t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\textstyle N:=\\sum_{e\\in E}q_{e}$ is the mean size of the matching. To emphasise, here, $t$ and $T$ count rounds.   \nRemark 3.4. It can be shown that this factor $N$ is needed in the pre-factor via a time-change analysis. ", "page_idx": 6}, {"type": "text", "text": "It is natural to optimise $\\lambda_{q}$ over $q$ which can arise as the marginals of a distribution $\\tilde{q}$ on $\\mathcal{M}$ . Clearly, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q_{k}:=\\sum_{e\\in E:k\\in e}q_{e}\\leq1\\quad\\mathrm{for}\\,\\mathrm{all}\\quad k\\in[n]}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for any such $q$ . Thus, the matrix $Q=(q_{i,j})_{i,j\\in V}$ is substochastic, so $\\lambda_{q}$ corresponds to the spectral gap of the discrete-time Markov chain on $G=([n],E)$ with weights $q_{i,j}=q_{\\{i,j\\}}$ . Let ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{\\mathrm{disc}}^{\\star}:=\\operatorname*{sup}\\{\\lambda_{q}\\ |\\ q\\in[0,1]^{E},\\ \\operatorname*{max}_{k\\in[n]}q_{k}\\leq1\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This is the optimal spectral gap of a discrete-time Markov chain on $[n]$ with transitions allowed only across edges of $G$ and uniform stationary distribution. Again, $\\lambda_{\\mathrm{disc}}^{\\star}$ can be formulated as a semidefinite program [10] and is related to the vertex conductance via a Cheeger-type inequality [32]. ", "page_idx": 6}, {"type": "text", "text": "We show that a distribution $\\tilde{q}$ over matchings with $\\begin{array}{r}{\\lambda_{q}\\ge\\frac{1}{3}\\lambda_{\\mathrm{disc}}^{\\star}}\\end{array}$ can be found by decomposing the substochastic $Q$ into a convex combination of permutation matrices using the Birkhoff\u2013von-Neumann theorem, followed by decomposing each permutation into disjoint cycles. ", "page_idx": 6}, {"type": "text", "text": "Corollary 3.5 (Optimised $\\tilde{q}$ ). Suppose that the comparison graph $G=([n],E)$ is given. In the set-up of Theorem 3.3, there exists a distribution $\\tilde{q}$ on matchings $\\mathcal{M}$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\|A^{t,T}-\\rho\\|_{2}^{2}\\lesssim\\frac{e^{4M}(\\log n)^{2}}{\\lambda_{\\mathrm{disc}}^{\\star}n/N}\\frac{1}{\\lambda_{\\mathrm{disc}}^{\\star}t}\\quad\\mathrm{whp}\\quad i f\\quad\\eta\\asymp\\frac{(\\log n)^{2}}{\\lambda_{\\mathrm{disc}}^{\\star}t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In many examples, such as if $G=([n],E)$ is an expander, but also if $G$ is a cycle, then $\\lambda_{\\mathrm{disc}}^{\\star}\\asymp n\\lambda_{\\mathrm{cts}}^{\\star}$ . In this case, parallel Elo really is as good, up to constants, as $n$ steps of the original (\u2018series\u2019) Elo. (Recall that $\\begin{array}{r}{\\dot{N}=\\sum_{e}q_{e}=1}\\end{array}$ is required for $\\lambda_{\\mathrm{cts}}^{\\star}$ , but that $N\\asymp n$ is possible for $\\lambda_{\\mathrm{disc}}^{\\star}$ .) ", "page_idx": 6}, {"type": "text", "text": "Other times, there is already a continuous-time chain, with average jump-rate 1, which has spectral gap order 1; e.g., two cliques connected by a single edge. In this case, the optimal parallel version gives no real improvement, even measured by rounds, over the optimally weighted series version. ", "page_idx": 6}, {"type": "text", "text": "3.2 Comparison with Related Work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The problem of designing an efficient tournament graph is related to active ranking [21]. Active ranking, however, allows one to choose which matches to schedule next after observing the results of some matches. For example, Yan et al. [40] propose an algorithm to identify the most informative pair of players given previous outcomes, and obtain regret bounds between Elo and the true ratings when matchups are scheduled according to their algorithm. In contrast, we are interested in designing a probability distribution over matches in an offilne manner, without the possibility of changing such distribution after observing some results. ", "page_idx": 7}, {"type": "text", "text": "This problem has been considered by Li, Shrotriya and Rinaldo [27]. They discuss a divide-andconquer strategy that essentially requires oversampling edges across bottlenecks in the graph. The drawback of this strategy is that it requires partitioning the graph into well-connected pieces, which is a non-trivial task itself. Furthermore, [27] does not provide explicit bounds on the sample complexity that can be obtained in this way, besides discussing a few examples where the bottleneck is known. ", "page_idx": 7}, {"type": "text", "text": "Nonetheless, our approach shares some similarity with [27]: the fastest-mixing Markov chain implicitly up-weights edges across bottlenecks. The main advantage, however, is that it provides the optimal spectral gap allowed by a graph topology, which is related to the diameter of the graph [32]. Moreover, both $\\lambda_{\\mathrm{cts}}^{\\star}$ and $\\lambda_{\\mathrm{disc}}^{\\star}$ can be formulated as SDPs, for which fast (polynomialtime) solvers exist. We remark, however, that this construction might require certain nodes to play an overwhelmingly large number of matches: this would be problematic for the results of [27]. As far as we know, minimising the number of \u2018parallel rounds\u2019 has not been considered before. ", "page_idx": 7}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We close with discussion of some specific examples and experimental results. Additional experiments are discussed in the Appendix. We start by considering dumbbell tournament graphs consisting of two cliques of $n/2=\\bar{20}$ vertices connected by a matching of $k\\in\\{1,20\\}$ edges. For each graph, we perform Elo simulations where the match-ups between players are sampled according to the following probability distributions. ", "page_idx": 7}, {"type": "text", "text": "1. The uniform distribution $q_{\\mathcal{U}}$ over the edges of the graph. ", "page_idx": 7}, {"type": "text", "text": "2. The optimal sequential $q_{\\mathrm{seq}}^{\\star}$ derived from the fastest-mixing continuous-time Markov chain. 3. The distribution over matchings $q_{\\mathrm{par}}^{\\star}$ derived from the fastest mixing discrete-time Markov chain, where multiple games are played in parallel in each round. ", "page_idx": 7}, {"type": "text", "text": "We sample the true ratings of the players according to independent Gaussians, with mean equal to 1 on one clique, mean 2 on the other, and standard deviation equal to 0.2 in both. This difference in average ratings between cliques simulates a scenario where the two cliques correspond to two different leagues of slightly different strength on average. ", "page_idx": 7}, {"type": "text", "text": "We perform Elo simulations initialising the Elo ratings at zero and setting $\\eta=0.1$ . For each graph, we repeat each experiment ten times, each time sampling new true ratings. Experimental results are displayed in Figures 1 and 2. Simulations are repeated ten times: lines correspond to the average $\\ell_{2}^{2}$ -distance between time-averaged Elo ratings and the true ratings, divided by the number of players $\\left.n=40\\right]$ ). Shaded regions corresponds to 25\u201375 percentile over these ten trials. Cyan and blue lines correspond to the same experiments where we have sampled multiple games in parallel as described in $\\S3$ ; we display the decay of error wrt the total number of games played in cyan and wrt the number of parallel rounds in blue. The blue line is solid for the first $2\\cdot1\\bar{0}^{4}$ games (same number of games displayed in cyan). ", "page_idx": 7}, {"type": "text", "text": "We display the experimental results for $k=1$ , i.e., two cliques of 20 vertices connected by a single edge, in Figure 1. The experiments align with the theoretical results of $\\S3$ : when $k=1$ , the spectral gap $\\lambda_{q_{\\mathrm{seq}}^{\\star}}$ of the optimal sequential distribution is of the same order of the spectral gap for our nearly parallel construction $\\lambda_{q_{\\mathrm{par}}^{\\star}}$ . Indeed, if convergence is measured wrt the number of rounds, the two corresponding errors seems to decay at a similar same rate, with the parallel version being slightly better, but requiring more than ten times the total number of games. As predicted by the theory, both distributions result in much faster convergence than the uniform distribution. If we measure the total number of games rather than rounds, the parallel version still results in a faster convergence than the uniform distribution, but not overwhelmingly so. ", "page_idx": 7}, {"type": "image", "img_path": "kLiWXUdCEw/tmp/5fbe938aed9dc0ef4cf34f481aeca24b2906bae1ced8cd5c5aca7261e65eb05f.jpg", "img_caption": ["Figure 1: Elo simulation results for a dumbbell graph with one edge between two cliques of 20 vertices. Match-ups are sampled from three different probability distributions. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "kLiWXUdCEw/tmp/16a3bdb136a21f9c2ed65699933f0a3cbb3ae0c31fec59b9f2db397cb1cebc09.jpg", "img_caption": ["Figure 2: Elo simulation results for a dumbbell graph with a perfect matching of 20 edges between two cliques of 20 vertices. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We now consider a dumbbell graph with $k=n/2$ , i.e., two cliques connected by a perfect matching. In this case, the fastest discrete- and continuous-time Markov chains have the same order-1 spectral gap. However, since the spectral gap for sequential Elo is then rescaled by a factor of $1/n$ , to achieve convergence, we expect the number of rounds for the parallel version to be much smaller than the number of games required by the sequential one; convergence should instead happen at the same rate when measured in the total number of games. This is clearly shown in Figure 2. The uniform distribution performs much worse, which we would expect from its order- $\\cdot1/n^{\\bar{2}}$ spectral gap. In the optimal parallel and sequential distributions, the probability to sample an edge from the bottleneck or from inside the cliques is balanced, while the uniform distribution oversamples edges inside the cliques. Experiments for the intermediate case of $k\\in\\{5,10\\}$ are discussed in the Appendix. ", "page_idx": 8}, {"type": "text", "text": "We end this section by discussing experimental results concerning the maximum rating reached by Elo. Figure 3 displays the behaviour of the largest Elo rating in absolute value, where pairs of players are selected uniformly at random (i.e., the underlying graph is a complete graph). The true ratings are sampled uniformly at random in $[-1,1]$ . The initial Elo ratings are set equal to zero. We simulate up to 50000 matches for a number of players that goes from 100 to 1000. We observe that the maximum rating is always below 1.75, corroborating our belief that, in many scenarios, the maximum Elo rating does not diverge for a long time. Further experiments and discussions are given in the Appendix. ", "page_idx": 8}, {"type": "image", "img_path": "kLiWXUdCEw/tmp/e5ecaca2672681261e110f403a31643a9577ca324dc45aa41f547ded088c29d9.jpg", "img_caption": ["Figure 3: Largest Elo rating in absolute value for a complete graph of varying size. True ratings are uniformly distributed in $[-1,1]$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Conclusion and Open Problems ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our work is a first step towards establishing the theoretical foundations of the Elo rating system, a popular ranking method in sports analytics. In particular, our main contribution is an analysis of Elo under the BTL model, establishing convergence results competitive with the state of the art. ", "page_idx": 8}, {"type": "text", "text": "There are several questions prompted by our work. First, from a technical point of view, we would like to control the maximal Elo rating. This is necessary to understand when we can remove the projection step in our definition of the Elo Markov chain, and make our theoretical results more aligned with practice. ", "page_idx": 8}, {"type": "text", "text": "Moreover, we would like to better understand the shape of the stationary distribution of Elo. This could help us, for example, obtain bounds on the rate of convergence in $\\ell_{\\infty}$ . ", "page_idx": 9}, {"type": "text", "text": "Finally, a touted strength of the Elo rating system in practical applications is its ability to dynamically update the ratings in response to changes in players\u2019 skills. Can we model these changes in a way that allows us to prove Elo can keep track of them, and bound the corresponding mean squared error? Such questions are often studied in the statistics literature; see, e.g., the recent paper [18] for details. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] ATP Elo ratings (tennisabstract.com). https://tennisabstract.com/reports/atp_elo_ ratings.html. [2] Elo Rating System (chess.com). https://www.chess.com/terms/elo-rating-chess.   \n[3] Introducing NFL Elo Ratings (fivethirtyeight.com). https://fivethirtyeight.com/ features/introducing-nfl-elo-ratings/.   \n[4] World Football Elo Ratings. https://www.eloratings.net/.   \n[5] Arpit Agarwal, Prathamesh Patil, and Shivani Agarwal. Accelerated spectral ranking. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 70\u201379. PMLR, 10\u201315 Jul 2018.   \n[6] David Aldous. Elo ratings and the sports model: A neglected topic in applied probability? Statistical Science, 32(4):616\u2013629, 2017.   \n[7] David Aldous. Mathematical probability foundations of dynamic sports ratings. Draft, January 2017.   \n[8] Simon Apers, Francesco Ticozzi, and Alain Sarlette. Lifting markov chains to mix faster: Limits and opportunities. arXiv:1705.08253 [math], May 2017.   \n[9] Heejong Bong and Alessandro Rinaldo. Generalized results for the existence and consistency of the MLE in the bradley-terry-luce model. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 2160\u20132177. PMLR, 2022.   \n[10] Stephen Boyd, Persi Diaconis, and Lin Xiao. Fastest mixing markov chain on a graph. SIAM Review, 46(4):667\u2013689, 2004.   \n[11] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs. I. The method of paired comparisons. Biometrika, 39:324\u2013345, 1952.   \n[12] Ross Bubley and Martin Dyer. Path coupling: A technique for proving rapid mixing in markov chains. In Proceedings of the 38th Annual Symposium on Foundations of Computer Science, FOCS \u201997, pages 223\u2013, Washington, DC, USA, 1997. IEEE Computer Society.   \n[13] Sourav Chatterjee. Spectral gap of nonreversible markov chains. arXiv:2310.10876 [math], October 2023.   \n[14] Fang Chen, L\u00e1szl\u00f3 Lov\u00e1sz, and Igor Pak. Lifting markov chains to speed up mixing. In Annual ACM Symposium on Theory of Computing (Atlanta, GA, 1999), page 275\u2013281. ACM, New York, 1999.   \n[15] Kai-Min Chung, Henry Lam, Zhenming Liu, and Michael Mitzenmacher. Chernoff\u2013hoeffding bounds for markov chains: Generalized and simplified. In 29th International Symposium on Theoretical Aspects of Computer Science, volume 14 of LIPIcs. Leibniz Int. Proc. Inform., page 124\u2013135. Schloss Dagstuhl. Leibniz-Zent. Inform., Wadern, 2012.   \n[16] Persi Diaconis, Susan Holmes, and Radford M. Neal. Analysis of a nonreversible markov chain sampler. Annals of Applied Probability, 10(3):726\u2013752, 2000.   \n[17] Aymeric Dieuleveut, Alain Durmus, and Francis Bach. Bridging the gap between constant step size stochastic gradient descent and Markov chains. Ann. Statist., 48(3):1348\u20131382, 2020.   \n[18] Samuel Duffield, Samuel Power, and Lorenzo Rimella. A State-Space Perspective on Modelling and Inference for Online Skill Rating, September 2023.   \n[19] David Gillman. A chernoff bound for random walks on expander graphs. SIAM J. Comput., 27(4):1203\u20131220, 1998.   \n[20] Bruce Hajek, Sewoong Oh, and Jiaming Xu. Minimax-optimal inference from partial rankings. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014.   \n[21] Reinhard Heckel, Nihar B. Shah, Kannan Ramchandran, and Martin J. Wainwright. Active ranking from pairwise comparisons and when parametric assumptions do not help. Ann. Statist., 47(6):3099\u20133126, 2019.   \n[22] Ald\u00e9ric Joulin. Poisson-type deviation inequalities for curved continuous-time markov chains. Bernoulli. Official Journal of the Bernoulli Society for Mathematical Statistics and Probability, 13(3):782\u2013798, 2007.   \n[23] Ald\u00e9ric Joulin. A new poisson-type deviation inequality for markov jump processes with positive wasserstein curvature. Bernoulli. Official Journal of the Bernoulli Society for Mathematical Statistics and Probability, 15(2):532\u2013549, 2009.   \n[24] Ald\u00e9ric Joulin and Yann Ollivier. Curvature, concentration and error estimates for markov chain monte carlo. Annals of Probability, 38(6):2418\u20132442, 2010.   \n[25] Jere Koskela. Zig-zag sampling for discrete structures and nonreversible phylogenetic mcmc. Journal of Computational and Graphical Statistics, 31(3):684\u2013694, 2022.   \n[26] Pascal Lezaud. Chernoff and berry\u2013ess\u00e9en inequalities for markov processes. European Series in Applied and Industrial Mathematics, 5:183\u2013201, 2001.   \n[27] Wanshan Li, Shamindra Shrotriya, and Alessandro Rinaldo. $\\ell_{\\infty}$ -bounds of the mle in the btl model under general comparison graphs. In James Cussens and Kun Zhang, editors, Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, volume 180 of Proceedings of Machine Learning Research, page 1178\u20131187. PMLR, 2022.   \n[28] R. Duncan Luce. Individual choice behavior: A theoretical analysis. John Wiley & Sons, Inc., New York; Chapman & Hall, Ltd., London, 1959.   \n[29] Math Stack Exchange. Projections Onto Convex Sets Decrease Distances in Hilbert Spaces. Mathematics Stack Exchange.   \n[30] Radford M. Neal. Improving asymptotic variance of mcmc estimators: Non-reversible chains are better. arXiv: 0407281 [math], July 2004.   \n[31] Sahand Negahban, Sewoong Oh, and Devavrat Shah. Rank centrality: Ranking from pairwise comparisons. Operations Research, 65(1):266\u2013287, 2017.   \n[32] Sam Olesker-Taylor and Luca Zanetti. Geometric bounds on the fastest mixing markov chain. In Mark Braverman, editor, 13th Innovations in Theoretical Computer Science Conference (ITCS 2022), volume 215 of Leibniz International Proceedings in Informatics (LIPIcs), page 109:1\u2013109:1, Dagstuhl, Germany, January 2022. Schloss Dagstuhl \u2013 Leibniz-Zentrum f\u00fcr Informatik.   \n[33] Yann Ollivier. Ricci curvature of metric spaces. Comptes Rendus Mathematique. Academie des Sciences. Paris, 345(11):643\u2013646, 2007.   \n[34] Yann Ollivier. Ricci curvature of markov chains on metric spaces. Journal of Functional Analysis, 256(3):810\u2013864, 2009.   \n[35] Daniel Paulin. Concentration inequalities for markov chains by marton couplings and spectral methods. Electronic Journal of Probability, 20:no. 79, 32, 2015. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[36] Daniel Paulin. Mixing and concentration by ricci curvature. Journal of Functional Analysis, 270(5):1623\u20131662, 2016. ", "page_idx": 11}, {"type": "text", "text": "[37] Nihar Shah, Sivaraman Balakrishnan, Joseph Bradley, Abhay Parekh, Kannan Ramchandran, and Martin Wainwright. Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology Dependence. In Guy Lebanon and S. V. N. Vishwanathan, editors, Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, volume 38 of Proceedings of Machine Learning Research, pages 856\u2013865, San Diego, California, USA, 09\u201312 May 2015. PMLR.   \n[38] Ray Stefani. The methodology of officially recognized international sports rating systems. Journal of Quantitative Analysis in Sports, 7(4), 2011.   \n[39] Jun Sun, Stephen Boyd, Lin Xiao, and Persi Diaconis. The fastest mixing markov process on a graph and a connection to a maximum variance unfolding problem. SIAM Review, 48(4):681\u2013699, 2006.   \n[40] Xue Yan, Yali Du, Binxin Ru, Jun Wang, Haifeng Zhang, and Xu Chen. Learning to identify top Elo ratings: A dueling bandits approach. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8797\u20138805, 2022. ", "page_idx": 11}, {"type": "text", "text": "A Experiments: Further Discussion and Figures ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this appendix we discuss additional experimental results. Code for our experiments is included in the supplementary material. ", "page_idx": 12}, {"type": "text", "text": "In Figure 4 we display further experiments related to dumbbell graphs. In particular, we consider graphs consisting of two cliques of 20 vertices connected by 5 (left) and 10 (right) edges arranged in a matching. The experimental setup is the same as the one discussed in $\\S4$ . Notice how, the more edges are added to the bottleneck, the better the distribution optimised for parallel rounds performs. The optimal sequential distribution, instead, performs roughly the same: adding edges in the bottleneck doesn\u2019t really improve its spectral gap, which depends mainly on the (unchanged) diameter. ", "page_idx": 12}, {"type": "image", "img_path": "kLiWXUdCEw/tmp/41bb987c5b7cf2d6565eabdafb207e7654a888b37d3ee83ece10deee58048589.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "image", "img_path": "", "img_caption": ["Figure 4: Elo simulation results for dumbbell graphs with $k=5$ (left) and $k=10$ (right) edges between two cliques of 20 vertices. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "We also consider a pyramidal graph, which is constructed as follows. We first sample three Erd\u02ddos\u2013 R\u00e9nyi random graphs with size, resp., $n_{1}=64$ , $n_{2}=32\\$ and $n_{3}=16$ , and density $p=1/2$ . We then connect the first graph to the second and the second to the third with two sparse cuts (see Figure 5). This graph is constructed to loosely resemble the pyramidal structure of, e.g., national sport leagues. ", "page_idx": 12}, {"type": "text", "text": "We conduct Elo simulations with the same set-up as for dumbbell graphs discussed earlier. In particular, we repeat the simulations ten times, resampling each time the players\u2019 true ratings. The true ratings are sampled as follows: independent normal distributions of standard deviation 0.2 and mean 0 for the Erdo\u02dds\u2013R\u00e9nyi at the bottom of the pyramid, mean 1 for the Erdo\u02dds\u2013R\u00e9nyi in the middle, and mean 2 for the one at the top. This is, again, to loosely simulate the fact that sport leagues are characterised by stronger players/teams towards the top of the pyramid. Experimental results are shown in Figure 6. In particular, we observe that the rate of converge for the uniform distribution is much slower than for the optimal sequential one. This is, again, predicted by the results of $\\S3$ : the two bottlenecks slow down the convergence in the uniform case; while the small diameter assures faster convergence for the optimal distribution by Corollary 3.1. The distribution optimised for parallel Elo, instead, guarantees very fast convergence when measured according to the number of rounds. ", "page_idx": 12}, {"type": "image", "img_path": "kLiWXUdCEw/tmp/a1e74f7be580f8d978e558b5a752fd6cf554a268348fbd5f52c346069cb75b73.jpg", "img_caption": ["Figure 5: Schematic representation of the pyramidal graph. "], "img_footnote": [], "page_idx": 12}, {"type": "image", "img_path": "kLiWXUdCEw/tmp/c5b694c47c5a81d0f930f0e09953b64b46033355edb475e8dbec60f0a4e80023.jpg", "img_caption": ["Figure 6: Elo simulation results for the pyramidal graph. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "In Figure 7 we display experimental results for a topology corresponding to the giant component of an Erdo\u02dds\u2013R\u00e9nyi random graph of $n=100$ vertices and density $p=0.02$ . True ratings are distributed as independent standard Gaussians. This graph has reasonably good connectivity, so we expect the number of games required to converge to be roughly equal for all the sampling distributions considered, with a good scope for parallelisation. This is confirmed by the error plot. What is perhaps surprising is that the optimal sequential distribution actually performs slightly worse than the uniform one. This can be explained by the fact that the spectral gap measures the rate of convergence for the worst possible vector of ratings: if the ratings do not depend on the topology of the graph, like in this example, the inverse of the spectral gap is an overtly pessimistic upper bound. Indeed, the optimal sequential distribution tends to oversample nodes that are in a central position in the graph and undersample nodes at the periphery. This should make convergence faster because it allows faster movement of information between distant nodes, but in this specific example is not helpful: since ratings are sampled in a iid fashion, the faster movement of information globally is not very helpful and is upset by slower convergence for undersampled nodes. ", "page_idx": 13}, {"type": "image", "img_path": "kLiWXUdCEw/tmp/e4e1b38d0207b7111656ff22f3225db85c95890326a164d1f5bb746e0f6cd80e.jpg", "img_caption": ["Figure 7: Left: topology of the giant component of an Erd\u02ddos\u2013R\u00e9nyi random graph of $n\\,=\\,100$ vertices and density $p=0.02$ ; edges are reweighed according to the distribution corresponding to the fastest mixing continuous-time Markov chain. Right: Elo simulation results for the same graph. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "We now present further experiments about the maximum Elo rating (in absolute value). Again, we sample true ratings independently and uniformly in $[-1,1]$ . In Figure 8, we present experiments for a path (left) and star (right) topology of varying size. In both cases, the largest Elo rating in absolute value remains smaller than twice the largest true rating. Notice that in the star graph the central node plays all the matches; this means the same player plays in total fifty-thousands matches without its value becoming particularly large. ", "page_idx": 13}, {"type": "text", "text": "Of course, these simulations are far from definitive: there are countless of ways in which to choose the true ratings and the graph topology. Our belief that these experiments are indicative of a more general behaviour is due to the peculiarities of the Elo rating systems. In particular, the Elo update offers diminishing returns: if the rating of $i$ is relatively much larger than the rating of $j$ , if $i$ wins against $j$ , the rating of $i$ won\u2019t increase by much. This is because the sigmoid function $\\sigma$ approaches zero very fast. ", "page_idx": 13}, {"type": "text", "text": "Despite this, proving that the maximum rating cannot increase significantly appears much harder. This is due to the fact that the maximum rating is not a supermartingale: if all of the neighbours of the node $i$ with largest true rating have abnormally large rating, the rating of $i$ is likely to increase, no matter how large it already was. However, because the zero-sum property of the ratings, the neighbours of $i$ cannot maintain an abnormally large rating for too long. Indeed, it appears very unlikely that such balanced but \u201cabnormal\u201d configurations are ever reached. ", "page_idx": 13}, {"type": "image", "img_path": "kLiWXUdCEw/tmp/aa95d160de9374e519ef9af5a625cd0a04f146c7a880aece64583a701daa90ab.jpg", "img_caption": ["Figure 8: Behaviour of the largest Elo rating in absolute value for path (left) and star (right) graphs of varying size. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "In the two-player case, the situation is simpler: if player 1 has Elo rating $x_{1}$ , then player 2 has $x_{2}=-x_{1}$ , by the zero-sum nature. It is straightforward to check that the Elo ratings are biased towards the true skills: that is, if player 1 has rating $x_{1}$ before a game, then their rating $\\boldsymbol{x}_{1}^{\\prime}$ after the game has $\\mathbb{E}[|x_{1}^{\\prime}-\\rho_{1}|]<|x_{1}-\\rho_{1}|$ , if $\\eta<\\frac12$ . ", "page_idx": 14}, {"type": "text", "text": "Suppose that the same argument holds for $n>2$ players: i.e., typically, Elo ratings are biased towards the true skills. Biased random walks on $\\mathbb{R}$ have exponential tails. So, if $\\eta\\ll1/\\log n$ , then it is super-polynomially unlikely that a given Elo rating will be more than 1 away from the corresponding true skill, in equilibrium. A union bound over the $n$ players allows us to deduce that the maximum Elo rating is at most 1 more than the maximum true skill, with probability at least $1-1/n^{10}$ . ", "page_idx": 14}, {"type": "text", "text": "Unfortunately, we weren\u2019t able to make this argument formal for $n>2$ players. We leave proving that indeed, with high probability, the maximum rating remains small for a large number of steps as an open problem. ", "page_idx": 14}, {"type": "text", "text": "B Convergence Proofs: Preliminaries ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "There are a few results which we use repeatedly throughout the proofs. We collect them here. ", "page_idx": 14}, {"type": "text", "text": "B.1 Dirichlet Characterisation of the Spectral Gap ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Recall that $\\lambda_{q}$ is the spectral gap of the continuous-time Markov chain on $[n]$ with transition rates $(q_{i,j})_{i,j\\in[n]}$ . We repeatedly use the standard Dirichlet characterisation of the spectral gap. ", "page_idx": 14}, {"type": "text", "text": "Proposition B.1 (Dirichlet Characterisation of the Spectral Gap). The spectral gap $\\lambda_{q}$ satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{q}=\\frac{1}{2}\\underset{z\\in\\mathbb{R}^{n}\\setminus\\{0\\}:z\\perp1}{\\operatorname*{min}}\\sum_{i,j\\in[n]}q_{i,j}(z_{i}-z_{j})^{2}/\\|z\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In particular, for all $z\\in\\mathbb{R}^{n}$ with $\\sum_{k}z_{k}=0$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{i,j\\in[n]}q_{i,j}(z_{i}-z_{j})^{2}\\geq2\\lambda_{q}\\Vert z\\Vert_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma B.2. Let $q_{i,j}\\in[0,1]$ for all $i,j\\in[n]$ with $\\textstyle\\sum_{i,j}q_{i,j}=2.$ . Let $\\lambda_{q}$ denote the spectral gap of the continuous-time Markov chain on $[n]$ with transition rates $(q_{i,j})_{i,j\\in[n]}$ . Then, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lambda_{q}\\leq2/(n-1)\\leq4/n.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. We apply the Dirichlet characterisation (Proposition B.1) with $z_{i}:=\\mathbf{1}\\{i=k\\}-\\frac{1}{n}$ . Then, ", "page_idx": 14}, {"type": "equation", "text": "$$\nz\\perp1,\\quad\\|z\\|_{2}^{2}=(1-1/n)^{2}+(n-1)/n^{2}=1-1/n=(n-1)/n\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\sum_{i,j\\in[n]}q_{i,j}(z_{i}-z_{j})^{2}/\\|z\\|_{2}^{2}=\\frac{1}{2}\\sum_{i,j\\in[n]}q_{i,j}\\big(\\mathbf{1}\\{i=k\\}-\\mathbf{1}\\{j=k\\}\\big)^{2}\\frac{n}{n-1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{1}{2}\\frac{n}{n-1}\\big(\\sum_{j:j\\neq k}q_{k,j}+\\sum_{i:i\\neq k}q_{i,k}\\big)=\\frac{n}{n-1}\\sum_{\\ell}q_{k,\\ell}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Choosing $k$ to minimise this final sum gives $\\begin{array}{r}{\\sum_{\\ell}q_{k,\\ell}\\leq\\frac{2}{n}}\\end{array}$ since the average $\\begin{array}{r}{\\frac{1}{n}\\sum_{k}\\sum_{\\ell}q_{k,\\ell}=\\frac{2}{n}}\\end{array}$ . So, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{q}\\leq\\frac{n}{n-1}\\cdot\\frac{2}{n}=2/(n-1)\\leq4/n.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B.2 Capping the Ratings ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Capping at $\\pm M$ has the benefti of restricting the process remains in a compact set. The disadvantage, though, is that the Elo update is more complicated. It also biases the process compared with the original. However, as we discuss in Remark C.2 below, the original process is already biased in the sense that the expected rating in equilibrium is not the true rating: $\\mathbb{E}_{\\pi}[X]\\neq\\rho$ . ", "page_idx": 15}, {"type": "text", "text": "The only property of the projection that we use is the following monotonicity result. ", "page_idx": 15}, {"type": "text", "text": "Proposition B.3 (Projection Monotonicity). Let $\\Omega\\subseteq\\mathbb{R}^{n}$ be a closed, convex set. Define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Pi_{\\Omega}(x):=\\arg\\operatorname*{min}_{x^{\\prime}\\in\\Omega}\\|x-x^{\\prime}\\|_{2}\\quad f o r\\quad x\\in\\mathbb{R}^{n};}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "that is, $\\Pi_{\\Omega}$ is the orthogonal projection to $\\Omega$ . Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\Pi_{\\Omega}(x)-\\Pi_{\\Omega}(y)\\|_{2}\\le\\|x-y\\|_{2}\\ \\ \\,f o r\\,a l l\\ \\ \\ x,y\\in\\mathbb{R}^{n};\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "that is, $\\Pi_{\\Omega}(x)$ is at least as close to $\\Pi_{\\Omega}(y)$ as $x$ is to $y$ , in $\\ell_{2}$ . ", "page_idx": 15}, {"type": "text", "text": "Versions of this result are well-known. An elementary proof can be found at [29]. We apply this with $\\begin{array}{r}{\\Omega:=\\{x\\in\\mathbb{R}^{n}\\mid\\|x\\|_{\\infty}\\leq M,\\,\\sum_{k}x_{k}=0\\}}\\end{array}$ , which is convex. Our results apply whenever this monotonicity property holds. ", "page_idx": 15}, {"type": "text", "text": "The orthogonal projection $\\Pi_{\\Omega}(x)$ can be efficiently computed as follows. We first project $x$ to $\\{y\\in\\mathbb{R}^{n}\\mathrm{~}|\\mathrm{~}|y||_{\\infty}\\leq M\\}$ . This can be done by simply replacing any $x_{i}<-M$ with $-M$ and any $x_{i}\\,>\\,M$ with $M$ . Let $x^{\\prime}$ be the resulting vector. Notice that $x^{\\prime}$ might not satisfy the constraint $\\sum_{k}x_{k}^{\\prime}=0$ : we need to subtract the displaced mass $\\sum_{k}x_{k}^{\\prime}$ to the rest of the vector while minimising $\\overline{{\\Pi}}_{\\Omega}^{\\mathrm{\\tiny~\\\"o}}(x)\\::=\\:\\arg\\operatorname*{min}_{x^{\\prime\\prime}\\in\\Omega}\\|x\\mathrm{~-~}x^{\\prime\\prime}\\|_{2}$ . Assume $\\sum_{k}x_{k}^{\\prime}\\;>\\;0$ (the symmetric case can be handled similarly). We set $x_{i}^{\\prime\\prime}=-M$ for all the coordinates $i$ such that $\\bar{x_{i}^{\\prime}}\\mathrm{~=~}-M$ and call $i$ frozen. To minimise $\\|{\\boldsymbol{x}}-{\\boldsymbol{x}}^{\\prime\\prime}\\|_{2}$ , we want to distribute $-\\sum_{k}x_{k}^{\\prime}$ to the unfrozen coordinates so that $x-x^{\\prime\\prime}$ is as balanced as possible in each coordinate (this can be imagined as a water-fliling procedure). In doing so, however, we might make $x_{i}^{\\prime\\prime}=-M$ for a new, previously unfrozen, coordinate $i$ before we have subtracted all the displaced mass. If that happens, we simply freeze $i$ and proceed in distributing the remaining displaced mass to the unfrozen coordinates (until a new coordinate is frozen). We keep repeating the procedure since all the mass is allocated. Notice that we can allocate all the mass since we assumed $\\dot{\\sum}_{k}x_{k}^{\\prime}>0$ . Moreover, the procedure lasts at most $n$ steps since at each step we freeze a new coordin ate until we have completed constructing the desired vector. ", "page_idx": 15}, {"type": "text", "text": "C Convergence Proofs: Bias and Variance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "There is an inherent bias and variance in the Elo ratings. It would be natural to assume that the expected value of a player\u2019s rating in equilibrium is equal to their real skill. After all, at least in the two-player case, an individual step is always biased towards the real skill. However, even for two players, this is not true. In this section, we quantify the bias and the variance in equilibrium. ", "page_idx": 15}, {"type": "text", "text": "Throughout this section and the next, we denote ", "page_idx": 15}, {"type": "equation", "text": "$$\np_{i,j}(z):=\\sigma(z_{i}-z_{j})\\quad\\mathrm{for}\\quad z\\in\\mathbb{R}^{n}\\quad\\mathrm{and}\\quad i,j\\in[n].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In particular, $p_{i,j}(\\rho)$ is the true probability that Player $i$ beats $j$ and $p_{i,j}(x)$ is the model probability if the current ratings are $x$ . Additionally, we split up the Elo step into the update and the projection: ", "page_idx": 15}, {"type": "text", "text": "\u2022 if the current vector is $X^{t}$ , then let $X^{t+1/2}$ denote an uncapped step;   \n\u2022 then, $X^{t+1}=\\Pi_{M}(X^{t+1/2})$ , where $\\Pi_{M}$ is the orthogonal projection. ", "page_idx": 15}, {"type": "text", "text": "We do not quantify the bias and variance player-by-player, but rather average over all players. ", "page_idx": 15}, {"type": "text", "text": "Theorem C.1 (Bias and Variance; Theorem 2.7). The following bias and variance estimates hold: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\,\\mathbb{E}_{\\boldsymbol{\\pi}}[X^{0}]-\\rho\\|_{2}^{2}\\leq\\mathbb{E}_{\\boldsymbol{\\pi}}[\\|X^{0}-\\rho\\|_{2}^{2}]\\leq4e^{4M}\\eta/\\lambda_{q};}\\\\ {\\sum_{k}\\mathbb{V}\\mathrm{ar}_{\\boldsymbol{\\pi}}[X_{k}^{0}]=\\mathbb{E}_{\\boldsymbol{\\pi}}[\\|X^{0}-\\mathbb{E}_{\\boldsymbol{\\pi}}[X^{0}]\\|_{2}^{2}]\\leq4e^{2M}\\eta/\\lambda_{q}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof: Bias. First and foremost, the Cauchy\u2013Schwarz inequality gives ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\mathbb{E}_{\\pi}[X^{0}]-\\rho\\|_{2}^{2}\\leq\\mathbb{E}_{\\pi}[\\|X^{0}-\\rho\\|_{2}^{2}].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We start the system from equilibrium and use stationarity: $X^{0}\\sim\\pi\\iff X^{1}\\sim\\pi$ . Also, we can ignore the projection step, whilst still assuming that all vectors have $\\ell_{\\infty}$ norm at most $M$ , due to the projection monotonicity of Proposition B.3: noting that $\\|\\rho\\|_{\\infty}\\leq M$ , so $\\Pi_{M}(\\rho)=\\rho$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|X^{1}-\\rho\\|_{2}=\\|\\Pi_{M}(X^{1/2})-\\pi_{M}(\\rho)\\|_{2}\\leq\\|X^{1/2}-\\rho\\|_{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Write $\\mathbb{E}_{x,\\{i,j\\}}[\\cdot]$ to indicate that the pair $\\{i,j\\}$ is chosen in the first step and $X^{0}=x$ . Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{E}_{x,\\{i,j\\}}\\left[(X_{i}^{1/2}-\\rho_{i})^{2}\\right]=p_{i,j}(\\rho)\\big(x_{i}-\\rho_{i}+\\eta\\big(1-p_{i,j}(x)\\big)\\big)^{2}+\\big(1-p_{i,j}(\\rho)\\big)\\big(x_{i}-\\rho_{i}-\\eta p_{i,j}(x)\\big)^{2}}\\\\ &{\\qquad=(x_{i}-\\rho_{i})^{2}+\\eta^{2}\\big(p_{i,j}(\\rho)\\big(1-p_{i,j}(x)\\big)^{2}+\\big(1-p_{i,j}(\\rho)\\big)p_{i,j}(x)^{2}\\big)}\\\\ &{\\qquad\\qquad+\\,2\\eta\\big(p_{i,j}(\\rho)\\big(1-p_{i,j}(x)\\big)-\\big(1-p_{i,j}(\\rho)\\big)p_{i,j}(x)\\big)(x_{i}-\\rho_{i})}\\\\ &{\\qquad\\le\\big(x_{i}-\\rho_{i})^{2}+\\eta^{2}-2\\eta\\big(p_{i,j}(x)-p_{i,j}(\\rho)\\big)\\big(x_{i}-\\rho_{i}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "An analogous statement holds for $X_{j}^{1/2}-\\rho_{j}$ , with $i$ and $j$ swapped: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x,\\{i,j\\}}\\big[(X_{j}^{1/2}-\\rho_{j})^{2}\\big]\\leq(x_{j}-\\rho_{j})^{2}+\\eta^{2}-2\\eta\\big(p_{j,i}(x)-p_{j,i}(\\rho)\\big)(x_{j}-\\rho_{j})}\\\\ &{\\qquad=(x_{j}-\\rho_{j})^{2}+\\eta^{2}+2\\eta\\big(p_{i,j}(x)-p_{i,j}(\\rho)\\big)(x_{j}-\\rho_{j}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "using the fact that $p_{j,i}(\\cdot)=1-p_{i,j}(\\cdot)$ . For $k\\notin\\{i,j\\}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x,\\{i,j\\}}\\left[(X_{k}^{1/2}-\\rho_{k})^{2}\\right]=(x_{k}-\\rho_{k})^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Bringing these three cases (indices $k=i,k=j$ and $k\\in[n]\\setminus\\{i,j\\})$ together, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x,\\{i,j\\}}\\left[\\|X^{1/2}-\\rho\\|_{2}^{2}\\right]\\leq\\|x-\\rho\\|_{2}^{2}+2\\eta^{2}-2\\eta\\big(p_{i,j}(x)-p_{i,j}(\\rho)\\big)\\big((x_{i}-\\rho_{i})-(x_{j}-\\rho_{j})\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\|x-\\rho\\|_{2}^{2}+2\\eta^{2}-2\\eta\\big(p_{i,j}(x)-p_{i,j}(\\rho)\\big)\\big((x_{i}-x_{j})-(\\rho_{i}-\\rho_{j})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, $p_{i,j}(x)-p_{i,j}(\\rho)=\\sigma(x_{i}-x_{j})-\\sigma(\\rho_{i}-\\rho_{i})$ and $(x_{i}-x_{j})-(\\rho_{i}-\\rho_{j})$ have the same sign. Also, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|p_{i,j}(x)-p_{i,j}(\\rho)|=\\left|\\sigma\\big(\\big((x_{i}-x_{j})-(\\rho_{i}-\\rho_{j})\\big)+\\big(\\rho_{i}-\\rho_{j}\\big)\\big)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\geq\\operatorname*{min}z\\in[-4M,4M]|\\sigma^{\\prime}(z)|\\cdot|(x_{i}-x_{j})-(\\rho_{i}-\\rho_{j})|}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{1}{4}e^{-4M}|(x_{i}-x_{j})-(\\rho_{i}-\\rho_{j})|}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "since $x_{i},x_{j},\\rho_{i},\\rho_{j}\\,\\in\\,[-M,M]$ , so $|(x_{i}-x_{j})-(\\rho_{i}-\\rho_{j})|\\,\\leq\\,4M$ and $\\sigma^{\\prime}(z)\\,=\\,\\sigma(z)\\bigl(1-\\sigma(z)\\bigr)$ . Hence, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x,\\{i,j\\}}\\big[\\|X^{1/2}-\\rho\\|_{2}^{2}\\big]\\leq\\|x-\\rho\\|_{2}^{2}+2\\eta^{2}-\\frac{1}{2}\\eta\\big((x_{i}-x_{j})-(\\rho_{i}-\\rho_{j})\\big)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\|x-\\rho\\|_{2}^{2}+2\\eta^{2}-\\frac{1}{2}e^{-4M}\\eta\\big((x_{i}-\\rho_{i})-(x_{j}-\\rho_{j})\\big)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We now average this over $\\{i,j\\}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x}\\big[\\|X^{1/2}-\\rho\\|_{2}^{2}\\big]\\leq\\|x-\\rho\\|_{2}^{2}+2\\eta^{2}-\\frac{1}{2}e^{-4M}\\eta\\sum_{i,j}q_{\\{i,j\\}}|(x_{i}-\\rho_{i})-(x_{j}-\\rho_{j})|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, applying the Dirichlet characterisation of the spectral gap (Proposition B.1) with $z:=x-\\rho$ , ", "page_idx": 16}, {"type": "text", "text": "$\\begin{array}{r}{\\mathfrak{L}_{x}\\big[\\|X^{1/2}-\\rho\\|_{2}^{2}\\big]\\leq\\|x-\\rho\\|_{2}^{2}+2\\eta^{2}-\\frac{1}{2}e^{-4M}\\eta\\lambda_{q}\\|x-\\rho\\|_{2}^{2}=(1-\\frac{1}{2}e^{-4M}\\eta\\lambda_{q})\\|x-\\rho\\|_{2}^{2}+2\\eta^{2};}\\end{array}$ again, $\\begin{array}{r}{\\sum_{k}z_{k}=\\sum_{k}x_{k}-\\sum_{k}\\rho_{k}=0}\\end{array}$ . In particular, stationarity implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\|X^{0}-\\rho\\|_{2}^{2}\\right]=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\|X^{1}-\\rho\\|_{2}^{2}\\right]\\leq\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\|X^{1/2}-\\rho\\|_{2}^{2}\\right]\\leq4e^{4M}\\eta/\\lambda_{q}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We perform a very similar calculation when bounding the curvature; see Definition D.2. This is the exponential contraction rate between a pair of systems $X$ and $Y$ . The $\\left(1-{\\textstyle\\frac{1}{2}}e^{-4M}\\eta\\lambda_{q}\\right)$ -factor above suggests curvature $\\kappa\\asymp e^{-4M}\\eta\\lambda_{q}$ , which is indeed what we show in Proposition D.4. ", "page_idx": 16}, {"type": "text", "text": "We now turn to the variance, for which we use a similar approach. It is a little more technically challenging, using the slightly cumbersome law of total variance: for random variables $A$ and $B$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{V}\\!\\mathbf{ar}[B]=\\mathbb{E}[\\mathbb{V}\\!\\mathbf{ar}[B\\mid A]]+\\mathbb{V}\\!\\mathbf{ar}[\\mathbb{E}[B\\mid A]].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof: Variance. The sum of variances is the expectation of an $\\ell_{2}$ distance: ", "page_idx": 17}, {"type": "text", "text": "$\\begin{array}{r}{\\sum_{k}\\mathbb{V}\\mathrm{ar}[Z_{k}]=\\sum_{k}\\mathbb{E}\\big[(Z_{k}-\\mathbb{E}[Z_{k}])^{2}\\big]=\\mathbb{E}\\left[\\sum_{k}(Z_{k}-\\mathbb{E}[Z_{k}])^{2}\\right]=\\mathbb{E}\\left[\\|Z-\\mathbb{E}[Z]\\|_{2}^{2}\\right],}\\end{array}$ for any random variable $Z\\in\\mathbb{R}^{n}$ . Also, for any $c\\in\\mathbb{R}^{n}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{V}\\mathrm{ar}[Z]=\\mathbb{E}[(Z-\\mathbb{E}[Z])^{2}]\\leq\\mathbb{E}[(Z-c)^{2}].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Applying this and the monotonicity of the orthogonal projection from Proposition B.3 gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sum_{k}\\mathbb{V}\\mathrm{ar}[X_{k}^{1}]=\\mathbb{E}\\left[\\|X^{1}-\\mathbb{E}[X^{1}]\\|_{2}^{2}\\right]}&{}\\\\ {\\le\\mathbb{E}\\left[\\|\\Pi_{M}(X^{1/2})-\\mathbb{E}[X^{1/2}]\\|_{2}^{2}\\right]}&{}\\\\ {\\le\\mathbb{E}\\left[\\|X^{1/2}-\\mathbb{E}[X^{1/2}]\\|_{2}^{2}\\right]=\\sum_{k}\\mathbb{V}\\mathrm{ar}[X^{1/2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, it suffices to prove the bound for $X^{1/2}$ \u2014i.e., for the Elo update without capping. ", "page_idx": 17}, {"type": "text", "text": "Use subscript $\\{i,j\\}$ to indicate that this pair is chosen in the first game, as before. Let $\\{I,J\\}$ be a pair of (distinct) indices drawn according to $\\pmb q$ . Then, by the law of total variance, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{k}\\mathbb{V}\\mathrm{ar}[X_{k}^{1/2}]=\\sum_{k}\\mathbb{E}[\\mathbb{V}\\mathrm{ar}_{\\{I,J\\}}[X_{k}^{1/2}]]+\\sum_{k}\\mathbb{V}\\mathrm{ar}[\\mathbb{E}_{\\{I,J\\}}[X_{k}^{1/2}]].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We studied the first term (aka the \u201cunexplained variance\u201d) first. First, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{k}\\mathbb{E}[\\mathbb{V}\\mathrm{ar}_{\\{I,J\\}}[X_{k}^{1/2}]]=\\sum_{k}\\sum_{i,j}q_{\\{i,j\\}}\\,\\mathbb{V}\\mathrm{ar}_{\\{i,j\\}}[X_{k}^{1/2}]=\\sum_{i,j}q_{\\{i,j\\}}\\sum_{k}\\mathbb{V}\\mathrm{ar}_{\\{i,j\\}}[X_{k}^{1/2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now bound $\\mathbb{V}\\mathrm{ar}_{\\{i,j\\}}[X_{k}^{1/2}]$ over three cases: $k=i,k=j$ and $k\\notin\\{i,j\\}$ . We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}\\mathrm{ar}_{\\{i,j\\}}[X_{i}^{1/2}]=\\mathbb{V}\\mathrm{ar}_{\\{i,j\\}}[X_{i}^{0}-\\eta p_{i,j}(X^{0})]+\\eta^{2}\\mathbb{V}\\mathrm{ar}_{\\{i,j\\}}\\big[\\mathrm{Bern}(p_{i,j}(\\rho))\\big]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{V}\\mathrm{ar}[X_{i}^{0}]-2\\eta\\,\\mathbb{C}\\mathrm{ov}\\big[X_{i}^{0},\\,\\sigma(X_{i}^{0}-X_{j}^{0})\\big]+\\frac{1}{2}\\eta^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "since the maximal variance of a $[0,1]$ -valued random variable is $\\frac{1}{4}$ . Analogously, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}\\mathrm{ar}_{\\{i,j\\}}[X_{j}^{1/2}]\\leq\\mathbb{V}\\mathrm{ar}[X_{j}^{0}]-2\\eta\\,\\mathbb{C}\\mathrm{ov}[X_{j}^{0},\\,\\sigma(X_{j}^{0}-X_{i}^{0})]+\\frac{1}{2}\\eta^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{V}\\mathrm{ar}[X_{j}^{0}]+2\\eta\\,\\mathbb{C}\\mathrm{ov}\\big[X_{j}^{0},\\,\\sigma(X_{i}^{0}-X_{j}^{0})\\big]+\\frac{1}{2}\\eta^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "since $\\sigma(-z)=1-\\sigma(z)$ . For $k\\notin\\{i,j\\}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{V}\\mathrm{ar}_{\\{i,j\\}}[X_{k}^{1/2}]=\\mathbb{V}\\mathrm{ar}[X_{k}^{0}].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Bringing these three cases together, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{k}\\mathbb{V}\\mathrm{ar}_{\\{i,j\\}}[X_{k}^{1/2}]\\leq\\sum_{k}\\mathbb{V}\\mathrm{ar}[X_{k}^{0}]+\\eta^{2}-2\\eta\\mathbb{C}\\mathrm{ov}\\big[X_{i}^{0}-X_{j}^{0},\\,\\sigma(X_{i}^{0}-X_{j}^{0})\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now, $\\sigma:\\mathbb{R}\\rightarrow[0,1]$ is increasing. So, $\\mathbb{C}\\mathbf{ov}[Y,\\,\\sigma(Y)]\\ge0$ for any random variable $Y$ . Moreover, | $\\begin{array}{r}{\\sigma(y)\\!-\\!\\frac{1}{2}|=|\\sigma(y)\\!-\\!\\sigma(0)|\\geq\\operatorname*{min}z\\in[-2M,2M]|\\sigma^{\\prime}(z)|\\!\\cdot\\!|y|\\geq\\frac{1}{4}e^{-2M}|y|\\quad\\mathrm{for~all}\\quad y\\in[-2M,2M]}\\end{array}$ . Since $X_{i}^{0}-X_{j}^{0}\\in[-2M,2M]$ , applying this gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{C}\\mathrm{ov}_{\\{i,j\\}}\\big[X_{i}^{0}-X_{j}^{0},\\,\\sigma(X_{i}^{0}-X_{j}^{0})\\big]\\ge\\frac{1}{4}e^{-2M}\\operatorname{Var}_{\\{i,j\\}}[X_{i}^{0}-X_{j}^{0}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Plugging this in above, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{k}\\mathbb{V}\\mathrm{ar}_{\\{i,j\\}}[X_{k}^{1/2}]\\leq\\sum_{k}\\mathbb{V}\\mathrm{ar}[X_{k}^{0}]+\\eta^{2}-\\frac{1}{2}e^{-2M}\\eta\\mathbb{V}\\mathrm{ar}_{\\{i,j\\}}[X_{i}^{0}-X_{j}^{0}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now average over $\\{i,j\\}$ : ", "page_idx": 17}, {"type": "text", "text": "$\\begin{array}{r}{\\sum_{i,j}q_{\\{i,j\\}}\\sum_{k}\\mathrm{{War}}_{\\{i,j\\}}[X_{k}^{1/2}]\\leq\\sum_{k}{\\mathrm{Var}}[X_{k}^{0}]+\\eta^{2}-\\frac{1}{2}e^{-2M}\\eta\\sum_{i,j}q_{\\{i,j\\}}\\,{\\mathbb V}\\mathrm{ar}_{\\{i,j\\}}[X_{i}^{0}-X_{j}^{0}].}\\end{array}$ Variances are (weighted) sums of squares, which leads to a spectral-gap estimate again: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{i,j}q_{\\{i,j\\}}\\,\\mathrm{Var}_{\\{i,j\\}}[X_{i}^{0}-X_{j}^{0}]}\\\\ &{\\qquad=\\sum_{i,j}q_{\\{i,j\\}}\\,\\mathrm{Var}_{\\{i,j\\}}\\left[(X_{i}^{0}-\\mathbb{E}[X_{i}^{0}])-(X_{j}^{0}-\\mathbb{E}[X_{j}^{0}])\\right]}\\\\ &{\\qquad=\\sum_{i,j}q_{\\{i,j\\}}\\,\\mathbb{E}\\left[\\left((X_{i}^{0}-\\mathbb{E}[X_{i}^{0}])-(X_{j}^{0}-\\mathbb{E}[X_{j}^{0}])\\right)^{2}\\right]}\\\\ &{\\qquad=\\mathbb{E}\\left[\\sum_{i,j}q_{\\{i,j\\}}\\left((X_{i}^{0}-\\mathbb{E}[X_{i}^{0}])-(X_{j}^{0}-\\mathbb{E}[X_{j}^{0}])\\right)^{2}\\right]}\\\\ &{\\qquad\\geq\\mathbb{E}\\left[\\lambda_{q}\\sum_{k}(X_{k}^{0}-\\mathbb{E}[X_{k}^{0}])^{2}\\right]=\\lambda_{q}\\sum_{k}\\mathrm{Var}[X_{k}^{0}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "by applying the Dirichlet characterisation of the spectral gap with $z:=X^{0}-\\mathbb{E}[X^{0}]$ . Thus, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{i,j}q_{\\{i,j\\}}\\sum_{k}\\mathbb{V}\\mathbf{ar}_{\\{i,j\\}}[X_{k}^{1/2}]\\leq\\sum_{k}\\mathbb{V}\\mathbf{ar}[X_{k}^{0}]+\\eta^{2}-\\frac{1}{2}e^{-2M}\\eta\\lambda_{q}\\sum_{k}\\mathbb{V}\\mathbf{ar}[X_{k}^{0}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=(1-\\frac{1}{2}e^{-2M}\\eta\\lambda_{q})\\sum_{k}\\mathbb{V}\\mathbf{ar}[X_{k}^{0}]+\\eta^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We would like to deduce that $\\begin{array}{r}{\\sum_{k}\\mathbb{V}\\mathrm{ar}_{\\pi}[X_{k}^{0}]\\le4e^{2M}\\eta/\\lambda_{q}}\\end{array}$ now, analogously to before. But, we must remember the second term in  the law of total variance (aka the \u201cexplained variance\u201d). We have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{V}\\!a\\!\\left[\\mathbb{E}_{\\left\\{I,J\\right\\}}[X_{k}^{1/2}]\\right]\\le\\frac{1}{2}\\eta^{2}q_{k}=\\frac{1}{2}\\eta^{2}\\sum_{\\ell}q_{\\left\\{k,\\ell\\right\\}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Iwnhdieceh dd, iifff ePr lbayy ; $k$ f  iPs lapiycekr is\u2014 in.oet. , $k\\,\\in\\,\\{I,J\\}$ \u2014 nd $X_{k}^{1/2}$ otm movoevse . uTp/hde omwanx itom aol nvea roifa tncweo  ovf asluucehs $\\eta$ $k$ $X_{k}^{1/2}$ a random variable is $\\scriptstyle{{\\frac{1}{2}}\\eta^{2}q_{k}}$ , where $\\begin{array}{r}{q_{k}=\\sum_{\\ell}q_{\\{k,\\ell\\}}}\\end{array}$ is the probability that Player $k$ is picked. Hence, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{k}\\mathbb{V}\\mathrm{ar}\\big[\\mathbb{E}_{\\{I,J\\}}[X_{k}^{1/2}]\\big]\\leq\\frac{1}{2}\\eta^{2}\\sum_{k}q_{k}=\\eta^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "noting the double-counting of edges in $\\begin{array}{r}{\\sum_{k}q_{k}=\\sum_{k,\\ell}q_{\\{k,\\ell\\}}=2}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "Combining the bounds for the unexplained and explained components of the variance, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{k}\\mathrm{\\mathbb{V}a r}[X_{k}^{1/2}]\\leq(1-\\frac{1}{2}e^{-2M}\\eta\\lambda_{q})\\sum_{k}\\mathrm{\\mathbb{V}a r}[X_{k}^{0}]+2\\eta^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In particular, stationarity implies that $\\begin{array}{r}{\\sum_{k}\\mathbb{V}\\mathrm{ar}_{\\pi}[X_{k}^{1/2}]=\\sum_{k}\\mathbb{V}\\mathrm{ar}_{\\pi}[X_{k}^{0}].}\\end{array}$ so ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{k}\\mathbb{V}\\mathrm{ar}_{\\pi}[X_{k}^{0}]=\\sum_{k}\\mathbb{V}\\mathrm{ar}_{\\pi}[X_{k}^{1}]\\le\\sum_{k}\\mathbb{V}\\mathrm{ar}_{\\pi}[X_{k}^{1/2}]\\le4e^{2M}\\eta/\\lambda_{q}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We controlled the bias, but did not actually argue that it is non-zero. Remark C.2 (Bias in Expected Rating). The uncapped Elo update $M=\\infty)$ ) implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\pi}[X_{i}]=\\mathbb{E}_{\\pi}\\big[X_{i}+\\eta\\sum_{j}q_{\\{i,j\\}}\\big(p_{i,j}(\\rho)-p_{i,j}(X)\\big)\\big]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{\\pi}[X_{i}]+\\eta\\big(\\sum_{j}q_{\\{i,j\\}}\\mathbb{E}_{\\pi}[p_{i,j}(\\rho)]-\\sum_{j}q_{\\{i,j\\}}\\mathbb{E}_{\\pi}[p_{i,j}(X)]\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{j}q_{\\{i,j\\}}p_{i,j}(\\rho)=\\sum_{j}q_{\\{i,j\\}}\\,\\mathbb{E}_{\\pi}[p_{i,j}(X)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The right-hand side is the expectation of the estimated probability that Player $i$ wins their next game (not conditioning on their opponent) in equilibrium and the left-hand side is the true probability. The equality shows that the win-probability estimator for each player is unbiased. ", "page_idx": 18}, {"type": "text", "text": "In the two-player case, this actually implies that $\\mathbb{E}_{\\pi}[p_{1,2}(X)]=p_{1,2}(\\rho)$ . We can deduce from this, however, that the estimated rating is biased, if $\\rho\\neq(0,0)$ . We do this now, but only informally. ", "page_idx": 18}, {"type": "text", "text": "The ratings are zero-sum, so it suffices to consider $X_{1}$ and $\\rho_{1}$ . Suppose that $\\rho_{1}>0$ and that $\\eta$ is small\u2014much smaller than $\\rho_{1}$ . Then, the rating $X_{1}$ concentrates around $\\rho_{1}$ . The win-probability function $\\sigma$ is strictly convex and increasing in $(0,\\infty)$ , which suggests that ", "page_idx": 18}, {"type": "equation", "text": "$$\np_{1,2}(\\mathbb{E}_{\\pi}[X_{1}])<\\mathbb{E}_{\\pi}\\bigl[p_{1,2}(X_{1})\\bigr]=p_{1,2}(\\rho),\\quad\\mathrm{and~hence}\\quad\\mathbb{E}_{\\pi}[X_{1}]\\neq\\rho.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Of course, $\\mathbb{P}_{\\pi}[X_{1}>0]\\neq1$ . This can be handled using the quantified version of Jensen\u2019s inequality and large deviation estimates on $\\mathbb{P}_{\\pi}[X_{1}<0]$ . If $\\rho_{1}$ is large, then, very roughly, the latter probability is like $e^{-\\rho_{1}^{2}}$ , whilst the quantified difference from equality is like $e^{-\\rho_{1}}$ ; the latter dominates. ", "page_idx": 18}, {"type": "text", "text": "The $n$ -player case is more complicated. It is possible that a particular choice of $(\\rho,q)$ could lead to certain players\u2019 having unbiased estimates. However, they will be biased in general. ", "page_idx": 18}, {"type": "text", "text": "The ratings are biased, typically, but the win-probabilities at equilibrium are unbiased in the uncapped setting. Moreover, the real ratings are the only vector $\\rho$ giving rise to these win-probabilities. ", "page_idx": 18}, {"type": "text", "text": "Proposition C.3. Let $x,\\rho\\in\\mathbb{R}^{n}$ with $\\textstyle\\sum_{k}\\rho_{k}=0=\\sum_{k}x_{k}$ . Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{j}q_{\\{i,j\\}}p_{i,j}(x)=\\sum_{j}q_{\\{i,j\\}}p_{i,j}(\\rho)\\quad\\rho r\\,a l l\\quad i\\in[k]\\quad i f a n d\\,o n l y\\,i f\\quad x=\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. The \u201cif\u201d direction is obvious. We prove the \u201conly if\u201d direction by constructing a minimisation problem over zero-sum vectors in $\\mathbb{R}^{n}$ with the following two properties: ", "page_idx": 18}, {"type": "text", "text": "1. it has a unique minimum at $\\rho$ ; ", "page_idx": 19}, {"type": "text", "text": "2. $x$ is a minimum if and only if it satisfies the equations in the statement. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(\\boldsymbol x):=\\sum_{i}\\left(\\frac{1}{2}\\sum_{j}q_{\\{i,j\\}}\\log\\left(1+e^{x_{i}-x_{j}}\\right)-x_{i}\\sum_{j}q_{\\{i,j\\}}\\left(p_{i,j}(\\rho)-\\frac{1}{2}\\right)\\right)\\quad\\mathrm{for}\\quad\\boldsymbol x\\in\\mathbb{R}^{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, $f$ is strictly convex in $\\{x\\in\\mathbb{R}^{n}\\mid\\sum_{k}x_{k}=0\\}$ , and thus has a unique minimiser in that set. We now take the gradient of $f$ and compare it with 0: $\\begin{array}{r}{\\frac{\\partial f}{\\partial x_{i}}(x)=\\frac{1}{2}\\sum_{j}q_{\\{i,j\\}}\\left(2p_{i,j}(x)\\!-\\!1\\right)\\!-\\!\\sum_{j}q_{\\{i,j\\}}\\left(p_{i,j}(\\rho)\\!-\\!\\frac{1}{2}\\right)=\\sum_{j}q_{\\{i,j\\}}p_{i,j}(x)\\!-\\!\\sum_{j}q_{\\{i,j\\}}p_{i,j}(\\rho).}\\end{array}$ Hence, $x$ is a minimum if and only if $\\textstyle\\sum_{k}x_{k}=0$ and $\\begin{array}{r}{\\sum_{j}q_{\\{i,j\\}}p_{i,j}(x)=\\sum_{j}q_{\\{i,j\\}}p_{i,j}(\\rho).}\\end{array}$ \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Notice that if we only desire estimates on the win-probabilities, then the whole Elo framework is not needed: simply tracking the empirical proportion of games won between each pair of players suffices. ", "page_idx": 19}, {"type": "text", "text": "D Convergence Proofs: Curvature and Concentration ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our time-averaged MCMC-type concentration estimates rely crucially on curvature bounds. We start by introducing curvature, then bounding it in the case of the Elo ratings. We then apply it to obtain concentration results for time-averaged ratings. ", "page_idx": 19}, {"type": "text", "text": "D.1 Curvature Definitions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We introduce the concept of curvature for a Markov chain $P$ on a general metric space $(\\Omega,d)$ . ", "page_idx": 19}, {"type": "text", "text": "Definition D.1 (Transportation Distance). Let $\\mu$ and $\\pi$ be probability measures on $\\Omega$ . The transportation distance $W_{1}\\bar{(}\\mu,\\pi)$ represents the \u2018best\u2019 way to send $\\mu$ to $\\pi$ so that, on average, points are moved by the smallest distance: ", "page_idx": 19}, {"type": "equation", "text": "$$\nW_{1}(\\mu,\\pi):=\\operatorname*{inf}_{\\mathbb{Q}}\\mathbb{E}_{\\mathbb{Q}}\\big[d(X,Y)\\big],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the infimum is over all couplings $\\mathbb{Q}$ of $(\\mu,\\pi)\\cdot$ \u2014i.e., $X\\sim\\mu$ and $Y\\sim\\pi$ , marginally, under $\\mathbb{Q}$ . Definition D.2 (Curvature of Markov Chains). The (Ricci) curvature of $P$ is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\kappa_{P}:=\\operatorname*{inf}_{x,y\\in\\Omega}\\kappa_{x,y}\\quad\\mathrm{where}\\quad\\kappa_{x,y}:=1-W_{1}(P_{x,\\cdot},P_{y,\\cdot})/d(x,y)\\quad\\mathrm{for}\\quad x,y\\in\\Omega.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A standard application of the triangle inequality and iteration establishing the following result. ", "page_idx": 19}, {"type": "text", "text": "Lemma D.3 (Contraction of Distance). For all measures $\\mu$ and $\\pi$ and all $t\\geq0$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\nW_{1}(\\mu P,\\pi P)\\leq(1-\\kappa_{P})W_{1}(\\mu,\\pi)\\quad a n d\\quad W_{1}(\\mu P^{t},\\pi P^{t})\\leq(1-\\kappa_{P})^{t}W_{1}(\\mu,\\pi).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This reduces to the well-known set-up of path coupling [12] when $(\\Omega,d)$ is a finite graph endowed with the usual graph distance. Our set-up, however, is very different: $(\\Omega,d)=(\\mathbb{R}^{n},\\|\\cdot\\|_{2})$ . ", "page_idx": 19}, {"type": "text", "text": "Exact calculation of the curvature is rarely required. Rather, a particular coupling is analysed, giving an upper bound on the transportation distance and hence, by extension, an upper bound on the curvature. The key is finding as close to optimal a coupling as possible. ", "page_idx": 19}, {"type": "text", "text": "The next subsection establishes an upper bound on the curvature of the Elo process. The final subsection of the section develops applies curvature to concentration. ", "page_idx": 19}, {"type": "text", "text": "D.2 Curvature Bounds for the Elo Process ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We determine the worst-case rate of contraction rate $1-\\kappa$ starting from ratings $(x,y)\\in\\Omega^{2}$ : we show that $\\kappa\\gtrsim\\lambda_{q}$ where $\\lambda_{q}$ is the spectral gap of the auxiliary random walk with rates $\\left(q_{i,j}\\right)$ . ", "page_idx": 19}, {"type": "text", "text": "Proposition D.4 (Curvature). Let $\\kappa$ denote the curvature of Elo in $\\Vert\\cdot\\Vert_{2}$ . Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\kappa\\geq{\\textstyle\\frac{1}{8}}\\eta e^{-2M}\\lambda_{q}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Let $x=(x_{k})_{k\\in[n]}\\in\\Omega$ and $y=(y_{k})_{k\\in[n]}\\in\\Omega$ be two sets of ratings. We want to bound ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x,y}[\\|X^{1}-Y^{1}\\|_{2}]\\leq(1-\\rho)\\|x-y\\|_{2}\\quad\\mathrm{for~some}\\quad\\rho\\geq0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "First, we observe that we can ignore the projection step in the Elo update, by Proposition B.3: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|X^{1}-Y^{1}\\|_{2}=\\|\\Pi_{M}(X^{1/2})-\\Pi_{M}(Y^{1/2})\\|_{2}\\le\\|X^{1/2}-Y^{1/2}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Again, this is using the notation $X^{1/2}$ to indicate the uncapped Elo update, and $X^{1}=\\Pi_{M}(X^{1/2})$ . We thus study $(X^{1/2},Y^{1/2})$ , under the assumption $\\|x\\|_{\\infty},\\|y\\|_{\\infty}\\leq M$ . We use the trivial coupling: ", "page_idx": 20}, {"type": "text", "text": "\u2022 the same pair $(I,J)$ of players is chosen; ", "page_idx": 20}, {"type": "text", "text": "\u2022 the result of the match is the same\u2014i.e., the same player wins\u2014in both systems. ", "page_idx": 20}, {"type": "text", "text": "This is legitimate because neither the choice of players nor the law of the outcome depends on the current state\u2014the estimated probability that $I$ beats $J$ depends on the state, but the real probability does not. Write $\\mathbb{E}_{\\{i,j\\}}[\\cdot]$ for the law conditional on choosing pair $\\{i,j\\}$ to play. ", "page_idx": 20}, {"type": "text", "text": "Recall that, for $z\\in\\mathbb{R}^{n}$ and $i,j\\in[n]$ , we write $p_{i,j}(z):=\\sigma(z_{j}-z_{i})$ for the estimated probability that $i$ beats $j$ using ratings $z$ . If Player $i$ beats $j$ , then $i$ gains $\\eta p_{j,i}$ points; similarly, $i$ loses $\\eta p_{j,i}$ points if Player $j$ beats $i$ . Observing that $p_{j,i}(x)-p_{j,i}(y)=p_{i,j}(\\bar{y})-p_{i,j}(x),$ , we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}_{\\{i,j\\}}\\left[(X_{i}^{1/2}-Y_{i}^{1/2})^{2}\\right]=p_{i,j}(\\rho)\\big((x_{i}-y_{i})+\\eta\\big(p_{j,i}(x)-p_{j,i}(y)\\big)\\big)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,(1-p_{i,j}(\\rho))\\big((x_{i}-y_{i})-\\eta\\big(p_{i,j}(x)-p_{i,j}(y)\\big)\\big)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\big((x_{i}-y_{i})-\\eta\\big(p_{i,j}(x)-p_{i,j}(y)\\big)\\big)^{2}}\\\\ &{\\qquad\\qquad\\qquad=(x_{i}-y_{i})^{2}+\\eta^{2}\\big(p_{i,j}(x)-p_{i,j}(y)\\big)^{2}-2\\eta(x_{i}-y_{i})\\big(p_{i,j}(x)-p_{i,j}(y)\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Switching the roles of $i$ and $j$ and using the fact that $p_{i,j}=1-p_{j,i}$ again, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathfrak{z}_{\\{i,j\\}}\\big[\\big(X_{j}^{1/2}-Y_{j}^{1/2}\\big)^{2}\\big]=(x_{j}-y_{j})^{2}+\\eta^{2}\\big(p_{j,i}(x)-p_{j,i}(y)\\big)^{2}-2\\eta(x_{j}-y_{j})\\big(p_{j,i}(x)-p_{j,i}(y)\\big)}\\\\ &{}&{\\qquad=(x_{j}-y_{j})^{2}+\\eta^{2}\\big(p_{j,i}(x)-p_{j,i}(y)\\big)^{2}+2\\eta(x_{j}-y_{j})\\big(p_{i,j}(x)-p_{i,j}(y)\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For all other $k$ \u2014i.e., for $k\\notin\\{i,j\\}$ \u2014we have /2= xk and Y k1/2= . Hence, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\{i,j\\}}\\big[\\|X^{1/2}-Y^{1/2}\\|_{2}^{2}\\big]-\\|x-y\\|_{2}^{2}}\\\\ &{\\qquad\\le2\\eta^{2}\\big(p_{i,j}(x)-p_{i,j}(y)\\big)^{2}-2\\eta\\big((x_{i}-x_{j})-(y_{i}-y_{j})\\big)\\big(p_{i,j}(x)-p_{i,j}(y)\\big)}\\\\ &{\\qquad\\le-\\eta\\big|(x_{i}-x_{j})-(y_{i}-y_{j})\\big|\\big|\\sigma(x_{i}-x_{j})-\\sigma(y_{i}-y_{j})\\big|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with the final inequality using the fact that $\\sigma$ is 1-Lipschitz and $\\eta<\\frac12$ ", "page_idx": 20}, {"type": "text", "text": "We now bound the difference in probabilities. For $\\bar{x},\\bar{y}\\in\\mathbb{R}$ with $\\bar{x}\\geq\\bar{y}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\sigma(\\bar{x})-\\sigma(\\bar{y})|\\geq\\operatorname*{min}_{\\bar{z}\\in[\\bar{y},\\bar{x}]}|\\sigma^{\\prime}(\\bar{z})||\\bar{x}-\\bar{y}|\\geq\\frac{1}{4}e^{-\\operatorname*{max}\\{|\\bar{x}|,|\\bar{y}|\\}}|\\bar{x}-\\bar{y}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining the previous results, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\{i,j\\}}\\left[\\|X^{1/2}-Y^{1/2}\\|_{2}^{2}\\right]-\\|x-y\\|_{2}^{2}\\leq-\\frac{1}{4}\\eta e^{-\\operatorname*{max}\\{|x_{i}-x_{j}|,|y_{i}-y_{j}|\\}}|(x_{i}-x_{j})-(y_{i}-y_{j})|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l r}{\\mathbb{1}\\{i,j\\}\\lfloor\\lfloor1\\rfloor-\\!\\!}&{=\\!\\!}&{\\!\\!\\|2\\rfloor-\\|1\\boldsymbol{\\varepsilon}-\\boldsymbol{y}\\|\\boldsymbol{2}\\,\\le\\,{-\\overline{{\\varepsilon}}}\\,\\eta c}\\\\ &{\\quad}&{\\le-\\frac{1}{4}\\eta e^{-2M}|(x_{i}-x_{j})-(y_{i}-y_{j})|^{2}=-\\frac{1}{4}\\eta e^{-2M}|(x_{i}-y_{i})-(x_{j}-y_{j})|^{2},}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "using the fact that $\\|x\\|_{\\infty},\\|y\\|_{\\infty}\\leq M$ . Summing over $(i,j)$ , weighted by $q_{i,j}$ , gives ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\|X^{1/2}-Y^{1/2}\\|_{2}^{2}\\right]-\\|x-y\\|_{2}^{2}}\\\\ &{\\qquad=\\sum_{i,j\\in[n]:i<j}q_{\\{i,j\\}}\\mathbb{E}_{\\{i,j\\}}\\big[\\|X^{1/2}-Y^{1/2}\\|_{2}^{2}-\\|x-y\\|_{2}^{2}\\big]}\\\\ &{\\qquad\\leq-\\frac{1}{4}\\eta e^{-2M}\\sum_{i,j\\in[n]:i<j}q_{i,j}|(x_{i}-y_{i})-(x_{j}-y_{j})|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, applying the Dirichlet characterisation of the spectral gap (Proposition B.1) with $z:=x-y$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\|X^{1/2}-Y^{1/2}\\|_{2}^{2}\\right]-\\|x-y\\|_{2}^{2}\\leq-\\frac{1}{4}\\eta e^{-2M}\\lambda_{q}\\|x-y\\|_{2}^{2};}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "note that $\\textstyle\\sum_{k}x_{k}=0=\\sum_{k}y_{k}$ , $\\mathrm{so}\\,\\sum_{k}z_{k}=0$ . Jensen\u2019s inequality then gives ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\|X^{1/2}-Y^{1/2}\\|_{2}\\big]\\leq\\mathbb{E}\\big[\\|X^{1/2}-Y^{1/2}\\|_{2}^{2}\\big]^{1/2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq(1-\\frac{1}{4}\\eta e^{-2M}\\lambda_{q})^{1/2}\\|x-y\\|_{2}\\leq(1-\\frac{1}{8}\\eta e^{-2M}\\lambda_{q})\\|x-y\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, recalling that $\\|X^{1}-Y^{1}\\|_{2}\\le\\|X^{1/2}-Y^{1/2}\\|_{2}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\kappa_{x,y}\\geq\\frac{1}{8}\\eta e^{-2M}\\lambda_{q},\\quad\\mathrm{and~so}\\quad\\kappa=\\operatorname*{inf}_{x,y\\in\\Omega}\\kappa_{x,y}\\geq\\frac{1}{8}\\eta e^{-2M}\\lambda_{q}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "D.3 Concentration Statements ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Our goal is to establish concentration of time-averaged statistics of the Elo process. General results of this form as known as \u201cChernoff-type bounds for Markov chains\u201d. There is a great deal of literature on such concentration bounds. The version we state here is most-closely related to Theorem 3.4 and Proposition 3.4 in [35]; see also Theorem 3 in [15], particularly. ", "page_idx": 21}, {"type": "text", "text": "Definition D.5 (Mixing Time). Let $\\mu$ and $\\pi$ be measures on a state space $\\Omega$ . Then, the total-variation $(T V)$ distance between $\\mu$ and $\\pi$ is defined to be ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mu-\\pi\\|_{\\mathrm{TV}}:=\\operatorname*{sup}_{A}|\\mu(A)-\\pi(A)|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the supremum is over measurable subsets of $\\Omega$ . Let $X=(X_{t})_{t\\geq0}$ be a Markov chain on $\\Omega$ , and let $\\pi$ denote its equilibrium distribution. The (precision- $\\varepsilon$ ) mixing time is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{t_{\\mathrm{mix}}(\\varepsilon):=\\operatorname*{inf}\\{t\\geq0\\mid\\operatorname*{max}_{x\\in\\Omega}\\|\\mathbb{P}_{x}[X_{t}\\in\\cdot]-\\pi\\|_{\\mathrm{TV}}\\leq\\varepsilon\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By convention, we abbreviate $t_{\\mathrm{mix}}:=t_{\\mathrm{mix}}(\\textstyle\\frac{1}{4})$ . ", "page_idx": 21}, {"type": "text", "text": "Theorem D.6 (Concentration). Let $X=(X_{t})_{t\\geq0}$ be a uniformly ergodic, irreducible Markov chain on a state space $\\Omega$ , started from its equilibrium distribution $\\pi$ . Let $t_{\\mathrm{mix}}$ denote its $\\frac14$ -mixing time. Let $f:\\Omega\\to\\mathbb{R}$ be a bounded function: $\\|f\\|_{\\infty}<\\infty$ . Let $\\textstyle\\pi(f):=\\mathbb{E}_{\\pi}[f]\\,=\\,\\int_{\\Omega}f d\\pi$ and $\\sigma_{f}^{2}:=$ $\\begin{array}{r}{\\mathbb{V}\\mathrm{ar}_{\\pi}[f]=\\int_{\\Omega}(f-\\pi(f))^{2}d\\pi}\\end{array}$ denote the mean and variance, respectively, of $f$ under $\\pi$ . Let $\\zeta>0$ and $t\\geq0$ be an integer. Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}_{\\pi}\\!\\left[\\big|\\frac{1}{t}\\sum_{s=0}^{t-1}f(X^{s})-\\pi(f)\\big|\\ge\\zeta\\right]\\le2\\,\\exp\\!\\left(-\\frac{\\zeta^{2}t/t_{\\mathrm{mix}}}{16(1+2t_{\\mathrm{mix}}/t)\\sigma_{f}^{2}+80\\zeta\\|f\\|_{\\infty}/t}\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In particular, $i f t\\ge\\operatorname*{max}\\{32t_{\\mathrm{mix}},\\,40\\zeta\\sigma_{f}^{2}\\|f\\|_{\\infty}\\}$ , then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}_{\\pi}\\!\\left[\\left|\\frac{1}{t}\\sum_{s=0}^{t-1}f(X^{s})-\\pi(f)\\right|\\ge\\zeta\\right]\\le2\\,\\exp\\!\\left(-\\frac{1}{20}\\sigma_{f}^{-2}\\cdot\\zeta^{2}t/t_{\\mathrm{mix}}\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Remark D.7 (Connection to Curvature). The following description applies to finite state spaces $\\Omega$ ;   \nwe have to be more careful in our Elo application later, since that state space is uncountably infinite. ", "page_idx": 21}, {"type": "text", "text": "It is well known that curvature bounds the spectral gap $\\lambda$ and relaxation time $t_{\\mathrm{rel}}\\,=\\,1/\\lambda$ in the reversible case: $\\lambda\\geq\\kappa$ , and hence $t_{\\mathrm{rel}}=1/\\lambda\\le\\kappa^{-1}$ . However, it also upper-bounds the mixing time without requiring reversibility, with an additional factor depending on the diameter: ", "page_idx": 21}, {"type": "equation", "text": "$$\nt_{\\mathrm{mix}}(\\varepsilon)\\leq\\kappa^{-1}\\bigl(\\log\\mathrm{diam}\\,\\Omega+\\log(1/\\varepsilon)\\bigr),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "assuming that $d(x,y)\\geq\\mathbf{1}\\{x\\neq y\\}$ . The argument for this is straight-forward: briefly, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}[X_{t}\\neq Y_{t}]=\\mathbb{E}[\\mathbf{1}\\{X_{t}\\neq Y_{t}\\}]\\leq\\mathbb{E}[d(X_{t},Y_{t})]\\leq(1-\\kappa)^{t}d(X_{0},Y_{0})\\leq e^{-\\kappa t}\\operatorname{diam}\\Omega,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then use the standard TV\u2013coupling relation. This can then be plugged into the previous concentration bound, obtaining exponential decay in $\\kappa t/\\log\\dim\\Omega$ . ", "page_idx": 21}, {"type": "text", "text": "Applying the theorem to the Elo process has a number of complications, primarily that it does not have a finite mixing time: given the initial ratings, it is always supported on a certain countable set; thus, its TV distance to equilibrium, which is continuously-supported, is 1 (maximal). We circumnavigate this by introducing a noisy version in the proof. This is detailed later. ", "page_idx": 21}, {"type": "text", "text": "Theorem D.8 (MCMC Convergence of Time Averages). Let $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ be a 1-Lipschitz function and $\\zeta\\in(0,1)$ . Let $\\mu_{f}:=\\mathbb{E}_{\\pi}[f]$ and $\\sigma_{f}^{2}:=\\mathbb{V}\\mathrm{ar}_{\\pi}[f]$ denote, respectively, its mean and variance under $\\pi$ , the unique invariant distribution of $X$ . For $t,T>0,$ , denote the time average of $f$ in $[T,T+t-1]$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{f}^{t,T}:=\\frac{1}{t}\\sum_{s=T}^{T+t-1}f(X^{s}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Suppose that $\\begin{array}{r}{\\|f\\|_{\\infty}\\leq\\frac{1}{5}t}\\end{array}$ . Let $C_{1},C_{2}<\\infty$ . Then, there exists a constant $C_{0}<\\infty,$ , depending only on $C_{1}$ , $C_{2}$ and $f$ , such that $i f$ ", "page_idx": 21}, {"type": "text", "text": "$\\operatorname*{min}\\{\\lambda_{q},\\,\\eta,\\,1/t,\\,\\zeta\\}\\ge n^{-{C_{1}}}\\quad a n d\\quad\\operatorname*{min}\\{t,T\\}\\ge C_{0}t_{\\star}\\quad w h e r e\\quad t_{\\star}:=e^{2M}\\eta^{-1}\\lambda_{q}^{-1}\\log n,$ ", "page_idx": 21}, {"type": "text", "text": "then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}_{0}\\!\\left[\\left|\\frac{1}{t}\\sum_{s=T}^{T+t-1}f(X^{s})-\\mu_{f}\\right|\\ge C_{0}\\zeta\\right]\\le n^{-C_{2}}+2\\exp\\!\\left(-\\sigma_{f}^{-2}\\zeta^{2}t/t_{\\star}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In particular, under these assumptions, taking $\\zeta\\asymp\\sigma_{f}\\sqrt{t_{\\star}\\log n/t}=e^{M}\\sigma_{f}\\log n/\\sqrt{\\eta\\lambda_{q}t},$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg[\\big|A_{f}^{t,T}-\\mu_{f}\\big|\\geq C_{0}e^{M}\\frac{\\sigma_{f}}{\\sqrt{\\eta}}\\frac{\\log n}{\\sqrt{\\lambda_{q}t}}\\bigg]\\leq n^{-C_{2}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Remark D.9 (Convergence Rate). If $X^{1},X^{2},\\ldots$ were iid, then we would have $1/\\sqrt{t}$ decay. Chernoff bounds for reversible Markov chains require $t$ to be replaced by $\\lambda t$ , where $\\lambda$ is the spectral gap. The idea is that $t_{\\mathrm{rel}}=1/\\lambda$ steps of the Markov chain are required to decorrelate terms when near equilibrium. For non-reversible Markov chains, running for $t_{\\star}$ does the job. ", "page_idx": 22}, {"type": "text", "text": "Connecting this to the curvature $\\kappa$ , roughly, $t_{\\star}\\lesssim\\kappa^{-1}\\log\\dim\\Omega$ if $\\kappa>0$ ; see Remark D.7. The Elo process is not reversible and we, in essence, bound $t_{\\star}\\lesssim\\kappa^{-1}\\log n$ and $\\kappa\\asymp\\eta\\lambda_{q}$ . ", "page_idx": 22}, {"type": "text", "text": "D.4 Concentration Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Remark D.7 connects curvature and the mixing time for finite Markov chains: in essence, curvature allows us to bring two copies exactly together. The Elo process, which lives in the continuum $\\mathbb{R}^{n}$ , does not have this property. We can get them extremely close, though. So, morally, the bound of $\\kappa^{-1}\\log\\dim\\Omega\\asymp\\bar{\\lambda}_{q}^{-\\bar{1}}\\log\\bar{n}$ on \u2018mixing\u2019 suggested by curvature feels correct. ", "page_idx": 22}, {"type": "text", "text": "We rigorise this idea by adding a small amount of independent noise to the ratings after every step.   \nThis noise is then used to couple the two copies once they are extremely close. ", "page_idx": 22}, {"type": "text", "text": "Outline of Proof of Theorem D.8. The proof has multiple steps, which we outline now. ", "page_idx": 22}, {"type": "text", "text": "I Approximate the Elo process by a noisy version and control the error. II Check that the curvature of the noisy process is at least as good as the original. III Control the mixing time, and hence concentration, of the noisy process. IV Use a burn-in to get the process quantitatively close to equilibrium. $\\mathsf{v}$ Compare the equilibrium distributions for the original and noisy versions. ", "page_idx": 22}, {"type": "text", "text": "Finally, we bring all the piece together to conclude the proof, checking a variety of conditions. $\\triangle$ ", "page_idx": 22}, {"type": "text", "text": "We approximate the Elo process by a noisy version. This circumnavigates issues surrounding the discrete support of $X_{t}$ versus the continuous support of its equilibrium distribution. ", "page_idx": 22}, {"type": "text", "text": "All the statements consist of generic constants $C_{0},C_{1},C_{2}<\\infty$ . Rather than carry all these dependencies, we make the specific (arbitrary) choice $C_{1}:=5$ and exhibit a $C_{2}$ for which this works. The results can easily be extended to the general set-up, albeit with more notation. With this in mind, let \u03b4 := n\u221224. ", "page_idx": 22}, {"type": "text", "text": "Step I: Noisy Version & Error. We consider the following noisy version $U$ of the Elo process $X$ . ", "page_idx": 22}, {"type": "text", "text": "1. Suppose $U^{0}=u^{0}$ . Draw $u^{1/3}$ according to an uncapped Elo step\u2014i.e., as if $M=\\infty^{}$ \u2014 started from $u^{0}$ . Suppose Players $i$ and $j$ were chosen\u2014i.e., $\\{k\\in[n]\\mid u_{k}^{1/3}\\neq u_{k}^{0}\\}=\\{i,j\\}$ . 2. Draw $\\tilde{u}_{i},\\tilde{u}_{j}\\,\\sim^{\\mathrm{iid}}\\,\\mathrm{Unif}([-\\sqrt{\\delta},+\\sqrt{\\delta}])$ independently of all else and set $\\tilde{u}_{k}:=0$ for $k\\not\\in$ $\\{i,j\\}$ . Set ", "page_idx": 22}, {"type": "equation", "text": "$$\nu^{2/3}:=u^{1/3}+\\tilde{u}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "3. Define $U^{1}:=u^{1}$ by orthogonally projecting $u^{2/3}$ to $[-M,M]^{n}$ preserving the sum. ", "page_idx": 22}, {"type": "text", "text": "Note that this does not preserve the zero-sum property, as $\\tilde{u}_{i}\\neq\\tilde{u}_{j}$ . We see later, though, that it is important\u2014crucial, even\u2014 for these additive noise terms to be taken independently. ", "page_idx": 22}, {"type": "text", "text": "We must control the error between the noisy and original versions. To do this, we use the trivial coupling, as before: the same players and result is used. The error does not accumulate super-linearly due to the contractive nature of the Elo update step, as we explain now. ", "page_idx": 22}, {"type": "text", "text": "Suppose that $(X^{0},Y^{0})=(x,y)$ and generate $(X^{1},Y^{1})=(x^{\\prime},y^{\\prime})$ from a single step of the trivial Elo-update coupling. Suppose that Players $i$ and $j$ play. Let $s\\in\\{0,1\\}$ be the indicator that Player $i$ beats $j$ \u2014the \u2018score\u2019. Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\nx_{i}^{\\prime}=x_{i}+\\eta\\big(s-p_{i,j}(x)\\big)\\quad\\mathrm{and}\\quad y_{i}^{\\prime}=y_{i}+\\eta\\big(s-p_{i,j}(y)\\big).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Subtracting the second from the first gives ", "page_idx": 22}, {"type": "equation", "text": "$$\nx_{i}^{\\prime}-y_{i}^{\\prime}=x_{i}-y_{i}-\\eta\\big(p_{i,j}(x)-p_{i,j}(y)\\big).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Analogously, ", "page_idx": 23}, {"type": "equation", "text": "$$\nx_{j}^{\\prime}-y_{j}^{\\prime}=x_{i}-y_{j}+\\eta\\bigl(p_{i,j}(x)-p_{i,j}(y)\\bigr).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Subtracting these, ", "page_idx": 23}, {"type": "equation", "text": "$$\n(x_{i}^{\\prime}-y_{i}^{\\prime})-(x_{j}^{\\prime}-y_{j}^{\\prime})=(x_{i}-y_{i})-(x_{j}-y_{j})-2\\eta\\big(p_{i,j}(x)-p_{i,j}(y)\\big).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "But, $p_{i,j}(z)=\\sigma(z_{i}-z_{j})$ and $\\sigma$ is 1-Lipschitz, so ", "page_idx": 23}, {"type": "equation", "text": "$$\nx_{i}-x_{j}\\leq y_{i}-y_{j}\\quad{\\mathrm{if~and~only~if}}\\quad p_{i,j}(x)\\leq p_{i,j}(y)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n|p_{i,j}(x)-p_{i,j}(y)|\\leq|(x_{i}-x_{j})-(y_{i}-y_{j})|=|(x_{i}-y_{i})-(x_{j}-y_{j})|;\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "also, $\\eta\\leq\\frac{1}{2}$ . Hence, ", "page_idx": 23}, {"type": "equation", "text": "$$\n|(x_{i}^{\\prime}-y_{i}^{\\prime})-(x_{j}^{\\prime}-y_{j}^{\\prime})|\\leq|(x_{i}-y_{i})-(x_{j}-y_{j})|\\leq|x_{i}-y_{i}|+|x_{j}-y_{j}|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Also, trivially, ", "page_idx": 23}, {"type": "equation", "text": "$$\n|(x_{i}^{\\prime}-y_{i}^{\\prime})+(x_{j}^{\\prime}-y_{j}^{\\prime})|=|(x_{i}-y_{i})+(x_{j}-y_{j})|\\leq|x_{i}-y_{i}|+|x_{j}-y_{j}|,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "since the Elo update is zero-sum. Combining these bounds, ", "page_idx": 23}, {"type": "equation", "text": "$$\nx_{i}^{\\prime}-y_{i}^{\\prime}|+|x_{j}^{\\prime}-y_{j}^{\\prime}|=\\operatorname*{max}\\{|(x_{i}^{\\prime}-y_{i}^{\\prime})-(x_{j}^{\\prime}-y_{j}^{\\prime})|,|(x_{i}^{\\prime}-y_{i}^{\\prime})+(x_{j}^{\\prime}-y_{j}^{\\prime})|\\}\\leq|x_{i}-y_{i}|+|x_{j}-y_{j}|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In other words, an Elo update does not increase the $\\ell_{1}$ distance between the two sets of ratings: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|x^{\\prime}-y^{\\prime}\\|_{1}\\leq\\|x-y\\|_{1}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This actually immediately implies that the Elo process is non-negatively curved in $\\ell_{1}$ . ", "page_idx": 23}, {"type": "text", "text": "A consequences of this is that that error can only arise from the additive-noise step: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|X^{s}-U^{s}\\|_{1}\\leq\\|X^{s-1/2}-U^{s-1/2}\\|_{1}\\leq\\|X^{s-1}-U^{s-1}\\|_{1}+{\\sqrt{\\delta}}\\leq.\\,.\\,.\\leq s{\\sqrt{\\delta}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Iterating this completes Step I: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{t}\\sum_{s=0}^{t-1}\\|X^{s}-U^{s}\\|_{1}\\leq\\frac{1}{2}t\\sqrt{\\delta}\\leq n^{-7}\\quad\\mathrm{deterministically}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We can now work with the noisy version $U$ , rather than the original version $X$ . The key statistic for our analysis is the curvature. This is not hurt by adding noise. ", "page_idx": 23}, {"type": "text", "text": "Step II: Curvature. We use the same coupling as before and the same noise in each process. ", "page_idx": 23}, {"type": "text", "text": "(i) Suppose that $(U^{0},V^{0})\\,=\\,(u^{0},v^{0})$ . Draw $(u^{1/3},v^{1/3})$ according to a single step of the trivial Elo coupling started from $(\\bar{u^{0}},v^{0})$ . Suppose that Players $i$ and $j$ were chosen. ", "page_idx": 23}, {"type": "text", "text": "(ii) Draw a single noise vector $n$ \u2014i.e., $\\tilde{u}_{i},\\tilde{u}_{j}\\,\\sim^{\\mathrm{iid}}\\,\\mathrm{Unif}([-\\sqrt{\\delta},+\\sqrt{\\delta}])$ and $\\tilde{u}_{k}:=0$ for $k\\not\\in$ $\\{i,j\\}$ . Set ", "page_idx": 23}, {"type": "equation", "text": "$$\nu^{2/3}:=u^{1/3}+\\tilde{u}\\quad\\mathrm{and}\\quad v^{2/3}:=v^{1/3}+\\tilde{u}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(iii) Define $U^{1}:=u^{1}$ and $V^{1}:=v^{1}$ by projecting to $[-M,M]^{n}$ preserving the respective sums. ", "page_idx": 23}, {"type": "text", "text": "We have already shown that the trivial Elo coupling contracts. The added noise does not hurt: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|u^{1}-v^{1}\\|_{2}\\leq\\|u^{2/3}-v^{2/3}\\|_{2}\\leq\\|u^{1/3}-v^{1/3}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, the two-stage coupling contracts at least as well as the original, completing Step II: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|U^{1}-V^{1}\\|_{2}]\\le\\mathbb{E}[\\|U^{1/3}-V^{1/3}\\|_{2}]\\le(1-\\kappa)\\|U^{0}-V^{0}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The reason for adding the noise is to be able to couple two systems $U$ and $V$ , and hence bound the mixing time. We can then apply the general concentration result of Theorem D.6. ", "page_idx": 23}, {"type": "text", "text": "Step III: Mixing Time. Suppose that $(U^{0},V^{0})=(u^{0},v^{0})$ with $\\|u^{0}-v^{0}\\|_{\\infty}\\leq\\delta$ . From this point, we proceed via a slightly different coupling, replacing (i, ii, iii) with $(\\mathrm{i}^{\\prime},\\mathrm{ii}^{\\prime},\\mathrm{ii}^{\\prime})$ , defined below. ", "page_idx": 23}, {"type": "text", "text": "(i\u2032) Suppose that $(U^{0},V^{0})\\,=\\,(u^{0},v^{0})$ . Draw $(u^{1/3},v^{1/3})$ according to a single step of the trivial Elo coupling started from $(u^{0},v^{0})$ without capping. Suppose that Players $i$ and $j$ were chosen. ", "page_idx": 24}, {"type": "text", "text": "$(\\mathrm{ii^{\\prime}})$ Draw $\\tilde{u}_{k}\\sim\\operatorname{Unif}([-\\sqrt{\\delta},+\\sqrt{\\delta}))$ and set $\\tilde{v}_{k}:=\\tilde{u}_{k}\\!+\\!\\big(u_{k}^{1/3}\\!-\\!v_{k}^{1/3}\\big)\\in[-\\sqrt{\\delta},+\\!\\sqrt{\\delta}]$ mod $2\\sqrt\\delta$ in\u221adependently for $k\\in\\{i,j\\}$ . (Her\u221ae, \u201cmo\u221ad $2\\sqrt\\delta^{,}$ means \u201cadjusting by additive multiples of $2\\sqrt\\delta$ as necessary so that $\\tilde{v}_{k}\\in[-\\sqrt\\delta,+\\sqrt\\delta)^{\\circ}$ .) Set $\\tilde{u}_{k},\\tilde{v}_{k}:=0$ for $k\\notin\\{i,j\\}$ . Set ", "page_idx": 24}, {"type": "equation", "text": "$$\nu^{2/3}:=u^{1/3}+\\tilde{u}\\quad\\mathrm{and}\\quad v^{2/3}:=v^{1/3}+\\tilde{v}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "(iii\u2032) Set $U^{1}:=u^{1}:=\\Pi_{M}(u^{2/3})$ and $V^{1}:=v^{1}:=\\Pi_{M}(v^{2/3})$ ", "page_idx": 24}, {"type": "text", "text": "If $\\|u^{0}-v^{0}\\|_{\\infty}\\leq\\delta$ and Players $i$ and $j$ play, leading to $(u^{1/3},v^{1/3})$ under the Elo coupling, then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{|u_{i}^{1/3}-v_{i}^{1/3}|,\\,|u_{j}^{1/3}-v_{j}^{1/3}|\\}\\le3\\delta,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "regardless of which player won. Hence, in the notation of the coupling, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\iota_{k}^{2/3}\\equiv u_{k}^{1/3}+\\tilde{u}_{k}=v_{k}^{1/3}+\\tilde{v}_{k}\\equiv v_{k}^{2/3}\\quad\\mathrm{for~both}\\quad k\\in\\{i,j\\}\\quad\\mathrm{if}\\quad\\tilde{u}_{i},\\tilde{u}_{j}\\in[-\\sqrt{\\delta}+3\\delta,+\\sqrt{\\delta}-3\\delta].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Sa\u221ay that a step is successful if this holds and fails otherwise. The probability of failure is at most $6\\sqrt\\delta$ . If it succeeds, then Players $i$ and $j$ are coupled and the bound of $\\delta$ on the $\\ell_{\\infty}$ norm is preserved. We now iterate. Suppose that $\\|U^{0}-V^{0}\\|_{\\infty}\\le\\delta$ . and Players $i_{t}$ and $j_{t}$ are chosen in step $t$ . Let ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tau_{\\mathrm{c}}:=\\operatorname*{inf}\\left\\{t\\geq0\\ \\big|\\ \\cup_{s\\leq t}\\{i_{t},j_{t}\\}=[n]\\right\\}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "be the first time that all players have been chosen. If all the first $\\tau_{\\mathrm{c}}$ steps\u221a are successful, then all players\u2019 ratings are coupled. Call this event $\\mathcal{C}$ ; then, $\\mathbb{P}[\\mathcal{C}\\mid\\tau_{\\mathrm{c}}\\leq t]\\geq1-6\\sqrt{\\delta}t$ . ", "page_idx": 24}, {"type": "text", "text": "It remains to analyse two times: ", "page_idx": 24}, {"type": "text", "text": "\u2022 the \u2018burn-in\u2019 time $\\tau_{\\mathrm{b}}$ to get to $\\ell_{\\infty}$ norm at most $\\delta$ , using coupling (i, ii, iii);   \n\u2022 the remaining \u2018coupling time\u2019 $\\tau_{\\mathrm{c}}$ started after the burn in, using coupling $(\\mathrm{i^{\\prime},\\,i i^{\\prime},\\,i i i^{\\prime}})$ . ", "page_idx": 24}, {"type": "text", "text": "The first is easy to handle using curvature: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|U^{t}-V^{t}\\|_{\\infty}]\\le\\mathbb{E}[\\|U^{t}-V^{t}\\|_{2}]\\le e^{-\\kappa t}\\|U^{0}-V^{0}\\|_{2}\\le e^{-\\kappa t}\\cdot2M n.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\eta_{\\mathrm{b}}>t]\\le\\mathbb{P}[\\|U^{t}-V^{t}\\|_{\\infty}>\\delta]\\le\\mathbb{E}[\\|U^{t}-V^{t}\\|_{\\infty}]/\\delta\\le\\delta\\quad\\mathrm{if}\\quad t\\ge t_{\\delta}:=\\kappa^{-1}\\log(2M n/\\delta^{2}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The second is equally easy. Start with $(U^{0},V^{0})$ such that $|U_{i}^{0}\\,-\\,V_{i}^{0}|\\;\\geq\\;1$ for all $i$ . Now, if $\\|U^{t}-V^{t}\\|_{\\infty}\\leq\\mathring{\\delta}<\\dot{1}$ , then all players must have been chosen. Hence, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\tau_{\\mathrm{c}}>t\\ |\\ c]\\le\\mathbb{P}[\\|U^{t}-V^{t}\\|_{\\infty}>\\delta]\\le\\delta\\quad\\mathrm{if}\\quad t\\ge t_{\\delta}=\\kappa^{-1}\\log(2M n/\\delta^{2}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $\\tau:=\\operatorname*{inf}\\{t\\geq0\\mid U^{t}=V^{t}\\}$ denote the coalesce time. Putting the above parts together, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}[\\tau>2t_{\\delta}]\\le\\mathbb{P}[\\tau_{\\mathrm{b}}>t_{\\delta}]+\\mathbb{P}[\\tau_{\\mathrm{c}}>t_{\\delta}]+\\mathbb{P}[\\mathcal{C}^{c}\\mid\\tau_{\\mathrm{c}}\\le t_{\\delta}]\\le2\\delta+6\\sqrt{\\delta}t_{\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We have $\\kappa^{-1}=8e^{2M}\\eta^{-1}\\lambda_{q}^{-1}\\leq n^{11}$ . So, recalling that $\\delta=n^{-24}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n6\\sqrt{\\delta}t_{\\delta}\\leq6n^{-12}\\cdot n^{11}\\cdot72\\log n=432n^{-1}\\log n\\ll1.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, $2\\delta+6\\sqrt{\\delta}t_{\\delta}\\leq\\textstyle{\\frac{1}{4}}$ . Also, $\\delta=n^{-24}$ and $M\\leq{\\frac{1}{2}}n$ , so ", "page_idx": 24}, {"type": "equation", "text": "$$\nt_{\\delta}=\\kappa^{-1}\\log(2M n/\\delta^{2})\\leq8e^{2M}\\eta^{-1}\\lambda_{q}^{-1}\\cdot50\\log n=400e^{2M}\\eta^{-1}\\lambda_{q}^{-1}\\log n=400t_{\\star}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This completes Step III: ", "page_idx": 24}, {"type": "equation", "text": "$$\nt_{\\star}(\\textstyle{\\frac{1}{4}})\\leq t_{\\star}(2\\delta+6\\sqrt{\\delta}t_{\\delta})\\leq2t_{\\delta}\\leq800t_{\\star}=800e^{2M}\\eta^{-1}\\lambda_{q}^{-1}\\log n.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The general concentration result of Theorem D.6 applies only for a Markov chain started from equilibrium. We start another chain $V$ from equilibrium and use a burn-in to get $U$ and $V$ close, quantified by curvature. The error between the time-averaged $U$ - and $V$ -sums is then small. ", "page_idx": 24}, {"type": "text", "text": "Step IV: Burn- $.I n$ . Let $V$ be a noisy Elo process, started from its equilibrium distribution\u2014which will differ from $\\pi$ slightly. Under the above coupling, which has contraction rate $1-\\kappa$ by (II), ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{s=T}^{T+t-1}\\mathbb{E}[\\|U^{s}-V^{s}\\|_{1}]\\le\\sum_{s=T}^{T+t-1}\\mathbb{E}[\\|U^{s}-V^{s}\\|_{2}]\\le\\sum_{s\\ge T}e^{-\\kappa s}\\cdot2M n}\\\\ &{\\qquad\\le4M n\\kappa^{-1}e^{-\\kappa T}\\le\\delta^{1/3}=n^{-8}\\quad\\mathrm{if}\\quad T\\ge T_{\\delta}:=\\kappa^{-1}\\log(2M n\\kappa^{-1}/\\delta^{1/3}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Analogously to the previous step, this time using also $\\kappa^{-1}\\leq n^{5}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nT_{\\delta}=\\kappa^{-1}\\log(2M n\\kappa^{-1}/\\delta)\\leq8e^{2M}\\eta^{-1}\\lambda_{q}^{-1}\\cdot31\\log n\\leq250e^{2M}\\eta^{-1}\\lambda_{q}^{-1}\\log n=250t_{\\star}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Also, $t\\geq\\kappa^{-1}\\geq n$ . We can then deduce the estimate desired for Step IV: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\frac{1}{t}\\sum_{s=T}^{T+t-1}\\|U^{s}-V^{s}\\|_{1}\\right]\\leq\\delta^{1/3}/t\\leq n^{-9}\\quad\\mathrm{if}\\quad T\\geq250t_{\\star}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Step V: Equilibrium Distributions. The original and noisy Elo processes have different equilibrium distributions; call them $\\pi$ and $\\tilde{\\pi}$ , respectively. Let $Y$ and $V$ be an original, respectively noisy, Elo process started from $\\pi$ , respectively $\\tilde{\\pi}$ . Then, by the strong law of large numbers, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\textstyle|\\pi(f)-\\tilde{\\pi}(f)|=\\operatorname*{lim}_{k\\to\\infty}\\bigl|\\frac{1}{k}\\sum_{\\ell=L}^{L+k-1}(f(Y^{\\ell})-f(V^{\\ell}))\\bigr|\\quad\\mathrm{almost~surely}\\quad\\mathrm{for~all}\\quad L\\in\\mathbb{N}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In the curvature proof, we showed a $\\ell_{2}^{2}$ -contraction of $1-2\\kappa$ for the Elo \u221aprocess; see Proposition D.4. Adding the noise can increase the $\\ell_{2}$ distance squared by at most $10M\\sqrt\\delta$ . Hence, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{\\ell}:=\\mathbb E[\\|Y^{\\ell}-V^{\\ell}\\|_{2}^{2}]\\quad\\mathrm{satisfies}\\quad d_{\\ell}\\leq(1-2\\kappa)d_{\\ell-1}+10M\\sqrt\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Iterating this, ", "page_idx": 25}, {"type": "equation", "text": "$$\nd_{\\ell}\\leq\\cdot\\cdot\\cdot\\leq2M n(1-2\\kappa)^{\\ell}+5M\\sqrt{\\delta}\\kappa^{-1}\\leq6M\\sqrt{\\delta}\\kappa^{-1}\\quad\\mathrm{if}\\quad\\ell\\geq L,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for some sufficiently large $L$ . By Cauchy\u2013Schwarz, $\\mathbb{E}[\\|Y^{\\ell}-V^{\\ell}\\|_{2}]\\le\\sqrt{d_{\\ell}}$ . Hence, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\frac{1}{k}\\sum_{\\ell=L}^{L+k-1}\\|Y^{\\ell}-V^{\\ell}\\|_{2}\\right]\\leq\\frac{1}{k}\\sum_{\\ell=L}^{L+k-1}\\sqrt{d_{\\ell}}\\leq3\\delta^{1/4}(M/\\kappa)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Plugging this in above, for a 1-Lipschitz function $f$ , using $\\|\\cdot\\|_{1}\\leq\\sqrt{n}\\|\\cdot\\|_{2}$ , we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n|\\pi(f)-\\tilde{\\pi}(f)|\\leq\\delta^{1/4}(10M n/\\kappa)^{1/2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally, we observe that $10M n/\\kappa\\leq n^{7}$ , completing Step $\\mathsf{v}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n|\\pi(f)-\\tilde{\\pi}(f)|\\leq n^{7/2}\\delta^{1/4}=n^{-5/2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We bring the previous give steps together to conclude the proof. ", "page_idx": 25}, {"type": "text", "text": "Conclusion of Proof of Theorem D.8. We use the following notation, inline with the above: ", "page_idx": 25}, {"type": "text", "text": "\u2022 $X$ is the original Elo process, started from an arbitrary initial condition;   \n\u2022 $U$ is the noisy Elo process, started from the same state as $X$ \u2014i.e., $U^{0}=X^{0}$ ;   \n\u2022 $V$ is the noisy Elo process, started from equilibrium $\\tilde{\\pi}$ . ", "page_idx": 25}, {"type": "text", "text": "Recall that $\\|f\\|_{\\mathrm{Lip}}\\leq1$ . This with the triangle inequality allows us to control the steps individually: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\frac{1}{t}\\sum_{s=T}^{T+t-1}f(X^{s})-\\pi(f)\\right|}\\\\ &{\\qquad\\le\\frac{1}{t}\\sum_{s=T}^{T+t-1}\\|X^{s}-U^{s}\\|_{1}}\\\\ &{\\qquad+\\,\\frac{1}{t}\\sum_{s=T}^{T+t-1}\\|U^{s}-V^{s}\\|_{1}}\\\\ &{\\qquad+\\,\\Big|\\frac{1}{t}\\sum_{s=T}^{T+t-1}f(V^{s})-\\tilde{\\pi}(f)\\Big|}\\\\ &{\\qquad+\\,\\Big|\\tilde{\\pi}(f)-\\pi(f)\\Big|}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Step IV ", "page_idx": 25}, {"type": "text", "text": "Step III ", "page_idx": 25}, {"type": "text", "text": "Step V ", "page_idx": 25}, {"type": "text", "text": "The first term is at most ${\\scriptstyle{\\frac{1}{2}}}t{\\sqrt{\\delta}}\\leq n^{-7}$ deterministically by Step I and the last is at most $n^{7/2}\\delta^{1/4}=$ $n^{-5/2}$ by Step $\\mathbf{v}$ . The second term is at most $\\delta^{1/3}/n=n^{-9}$ in expectation by Step IV, which we plug into Markov\u2019s inequality. We assume that $\\zeta\\geq\\dot{n}^{-2}$ , so that each of the first two terms is smaller ", "page_idx": 25}, {"type": "text", "text": "than $\\zeta$ , and $n^{-5}/\\zeta\\leq n^{-3}$ . The remaining term is controlled via the general concentration result of Theorem D.6, using the bound $\\begin{array}{r}{t_{\\star}(\\frac{1}{4})\\leq800t_{\\star}=800e^{2M}\\eta^{-1}\\lambda_{q}^{-1}\\log^{\\upsilon}{n}}\\end{array}$ from Step III. Hence, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\big[\\big|\\frac{1}{t}\\sum_{s=T}^{T+t-1}f(X^{s})-\\pi(f)\\big|>9\\zeta\\big]}\\\\ &{\\qquad\\le\\mathbb{P}\\big[\\big|\\frac{1}{t}\\sum_{s=T}^{T+t-1}U^{s}-\\frac{1}{t}\\sum_{s=T}^{T+t-1}V^{s}\\big|\\big|_{1}>\\zeta\\big]}\\\\ &{\\qquad+\\,\\mathbb{P}\\big[\\big|\\frac{1}{t}\\sum_{s=T}^{T+t-1}f(V^{s})-\\pi(f)\\big|>5\\zeta\\big]}\\\\ &{\\qquad\\le n^{-6}+2\\exp\\bigl(-\\sigma_{f}^{-2}\\zeta^{2}t/t_{\\star}\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We can apply Theorem D.6 if $t\\geq25600t_{\\star}=32\\cdot800t_{\\star}$ due to Step III and Step $\\mathsf{I V}$ if $T\\geq250t_{\\star}$ . Finally, we make a specific choice of $\\zeta$ . There is already a polynomial term in the error probability, so we want to choose $\\zeta$ as small as possible whilst preserving this. Thus, we take ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\zeta:=2\\sigma_{f}\\sqrt{t_{\\star}\\log n/t}=2e^{M}\\log n/\\sqrt{\\eta\\lambda_{q}t}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "D.5 Deduction of Theorem 2.5 from Theorems 2.7 and D.8 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We start by recalling Theorem 2.5 for the reader\u2019s convenience, numbered here as Theorem D.10. Theorem D.10 (MCMC Estimator of Time-Averaged Ratings; Theorem 2.5). Denote the timeaveraged rating ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{k}^{t,T}:=\\frac{1}{t}\\sum_{s=T}^{T+t-1}X_{k}^{s}\\quad f o r\\quad k\\in[n]\\quad a n d\\quad t,T>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let $C_{1},C_{2}<\\infty$ . Then, there exists a constant $C_{0}<\\infty$ , depending only on $C_{1}$ and $C_{2}$ , such that $i f$ ", "page_idx": 26}, {"type": "text", "text": "$\\operatorname*{min}\\{\\lambda_{q},\\,\\eta,\\,1/t\\}\\ge n^{-C_{1}}\\quad a n d\\quad\\operatorname*{min}\\{t,T\\}\\ge C_{0}t_{\\star}\\quad w h e r e\\quad t_{\\star}:=e^{2M}\\eta^{-1}\\lambda_{q}^{-1}\\log n,$ ", "page_idx": 26}, {"type": "text", "text": "then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg[\\frac{1}{n}\\|A^{t,T}-\\rho\\|_{2}^{2}\\leq\\frac{C_{0}e^{4M}}{\\lambda_{q}n}\\bigg(\\eta+\\frac{(\\log n)^{2}}{\\lambda_{q}t}\\bigg)\\bigg]\\geq1-n^{-C_{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The same result holds for $\\frac{1}{n}\\|A^{t,T}-\\rho\\|_{1}$ , the average distance in the $\\ell_{1}$ (rather than $\\ell_{2}$ ) sense. ", "page_idx": 26}, {"type": "text", "text": "Proof of Theorem $2.5/D.I O$ . The two terms inside the probability come from the MCMC-convergence error and the equilibrium-distribution bias. We use the abbreviations ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{k}^{t}:=\\frac{1}{t}\\sum_{s=T}^{T+t-1}X_{k}^{s},\\quad\\beta_{k}:=|\\pi(\\Pi_{k})-\\rho_{k}|\\quad\\mathrm{and}\\quad\\sigma_{k}^{2}:=\\sigma_{\\Pi_{k}}^{2}=\\mathbb{V}\\mathrm{ar}_{\\pi}[\\Pi_{k}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\Pi_{k}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is the $k$ -th projector. We separate the MCMC and bias parts: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|A^{t}-\\rho\\|_{2}^{2}\\leq2\\|A^{t}-\\pi(\\Pi)\\|_{2}^{2}+2\\|\\pi(\\Pi)-\\rho\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the bias part, we simply note for now that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\pi(\\Pi)-\\rho\\|_{2}^{2}=\\sum_{k}|\\pi(\\Pi_{k})-\\rho_{k}|^{2}=\\sum_{k}\\beta_{k}^{2}=:n\\bar{\\beta}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We now turn to the MCMC time-averages. Applying Theorem D.8 with $f:=\\Pi_{k}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\big[\\big|A_{k}^{t,T}-\\pi(\\Pi_{k})\\big|\\ge C_{0}e^{M}\\sigma_{k}\\log n/\\sqrt{\\eta\\lambda_{q}t}\\big]\\le n^{-C_{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "under the assumptions of that theorem. We perform a union bound over the $n$ players: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\big[\\frac1n\\|A^{t,T}-\\pi(\\Pi)\\|_{2}^{2}\\ge C_{0}e^{2M}\\bar{\\sigma}^{2}(\\log n)^{2}/(\\eta\\lambda_{q}t)\\big]\\le n^{-C_{2}+1}\\quad\\mathrm{where}\\quad\\bar{\\sigma}^{2}:=\\frac1n\\sum_{k\\in[n]}\\sigma_{k}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "These bias and (average) variance terms are exactly what we handled in Theorem 2.7: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\bar{\\beta}^{2}\\leq4e^{4M}\\eta/(\\lambda_{q}n)\\quad\\mathrm{and}\\quad\\bar{\\sigma}^{2}\\leq4e^{2M}\\eta/(\\lambda_{q}n).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The first part of Theorem 2.5 now follows from plugging these in above and some small manipulations. To obtain the $1/(\\lambda_{q}t)$ decay, we just need to check that $t\\gg t_{\\star}$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{t_{\\star}}{t}=\\frac{e^{2M}\\lambda_{q}^{-1}\\log n}{\\eta t}\\asymp\\frac{e^{2M}\\lambda_{q}^{-1}\\log n}{\\lambda_{q}^{-1}(\\log n)^{2}}=\\frac{e^{2M}}{\\log n}\\ll1.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "E Convergence Proofs: Parallel Matches ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Our previous bounds on the bias and the variance included the spectral gap $\\lambda_{q}$ obtained from sequential analysis\u2014namely, we used $\\sum_{e}q_{e}=1$ . These expressions change in the parallel set-up. This has a knock-on effect on the two terms in Theorem 2.5. ", "page_idx": 27}, {"type": "text", "text": "We now state an extended version of Theorem 3.3. ", "page_idx": 27}, {"type": "text", "text": "Theorem E.1. Denote the time-averaged rating ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{k}^{t,T}:=\\frac{1}{t}\\sum_{s=T}^{T+t-1}X_{k}^{s}\\quad f o r\\quad k\\in[n]\\quad a n d\\quad t,T>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $X^{s}$ is the state after $s\\geq0$ Elo rounds. Then, under the conditions of Theorem 2.5, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg[\\frac{1}{n}\\|A^{t,T}-\\rho\\|_{2}^{2}\\leq\\frac{C e^{4M}}{\\lambda_{q}n/N}\\bigg(\\eta+\\frac{(\\log n)^{2}}{\\lambda_{q}t}\\bigg)\\bigg]=1-o(1),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\textstyle N:=\\sum_{e\\in E}q_{e}$ is the mean size of the matching; to emphasise, here, $t$ and $T$ count rounds. Moreover, there exists a distribution $\\tilde{q}\\,=\\,(\\tilde{q}_{S})_{S\\in\\mathcal{M}}$ over matchings whose induced distribution $q=(q_{e})_{e\\in E}$ over pairs satisfies $\\begin{array}{r}{\\lambda_{q}\\geq\\frac{1}{3}\\lambda_{\\mathrm{disc}}^{\\star}}\\end{array}$ . In particular, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|A^{t,T}-\\rho\\|_{\\star}\\lesssim\\frac{e^{2M}\\log n}{\\sqrt{\\lambda_{\\mathrm{disc}}^{\\star}n/N}}\\frac{1}{\\sqrt{\\lambda_{\\mathrm{disc}}^{\\star}t}}\\quad\\mathrm{whp}\\quad i f\\quad\\eta\\asymp\\frac{(\\log n)^{2}}{\\lambda_{\\mathrm{disc}}^{\\star}t}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We separate the proof of Theorem E.1 into two parts: the convergence and the existence of a parallelisable $q$ with $\\lambda_{q}\\geq\\frac{1}{3}\\lambda_{\\mathrm{disc}}^{\\star}$ . The \u201cin particular\u201d part follows exactly as for Theorem 2.5. ", "page_idx": 27}, {"type": "text", "text": "Proof of Theorem $E.l$ : Convergence. Write $\\begin{array}{r}{\\bar{\\beta}^{2}:=\\frac1n\\|\\pi(\\Pi)-\\rho\\|_{2}^{2}}\\end{array}$ and $\\begin{array}{r}{\\bar{\\sigma}^{2}:=\\frac{1}{n}\\sum_{k}\\mathbb{V}\\mathrm{ar}_{\\pi}[\\Pi_{k}]}\\end{array}$ for the average $\\ell_{2}$ -bias and variance of the ratings, respectively, as we did in the deduction of Theorem 2.5. The analysis of the bias is unchanged until we average over the choice $\\{i,j\\}$ of players. Then, we implicitly used $\\sum_{e}q_{e}=1$ when averaging $2\\eta^{2}$ over $\\{i,j\\}$ . Letting $\\begin{array}{r}{N:=\\overleftarrow{\\sum_{e}q_{e}}=\\mathbb{E}_{S\\sim\\tilde{q}}[|S|]}\\end{array}$ denote the expected si ze of the random matching $\\tilde{q}$ corresponding to $q$ , the sam e analysis gives ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x}\\big[\\|X^{1/2}-\\rho\\|_{2}^{2}\\big]\\leq\\|x-\\rho\\|_{2}^{2}+2N\\eta^{2}-\\frac{1}{2}e^{-4M}\\eta\\sum_{i,j}q_{\\{i,j\\}}|(x_{i}-\\rho_{i})-(x_{j}-\\rho_{j})|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Inspecting the analysis of the variance to see where $\\textstyle\\sum_{e}q_{e}=1$ is used, we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{k}\\mathbb{V}\\mathrm{ar}[X_{k}^{1/2}]\\le\\sum_{k}\\mathbb{V}\\mathrm{ar}[X_{k}^{0}]+\\frac{3}{2}N\\eta^{2}-\\frac{1}{2}e^{-2M}\\eta\\lambda_{q}\\sum_{k}\\mathbb{V}\\mathrm{ar}[X_{k}^{0}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The proofs then proceed as before to give the bounds ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\bar{\\beta}^{2}\\leq4e^{4M}\\eta N/(\\lambda_{q}n)\\quad\\mathrm{and}\\quad\\bar{\\sigma}^{2}\\leq3e^{2M}\\eta N/(\\lambda_{q}n).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The argument for the deduction of Theorem 2.5 gives ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\big[\\frac{1}{n}\\|A^{t,T}-\\rho\\|_{2}^{2}\\leq C_{0}(\\bar{\\beta}^{2}+\\bar{\\sigma}^{2}/(\\eta\\lambda_{q}t_{)}\\big]\\geq1-n^{-C_{2}};}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "now, $(t,T)$ in $A^{t,T}$ correspond to the number of rounds and $q$ may have $\\sum_{e}q_{e}>1$ \u2014and, so, $\\lambda_{q}$ may be larger than $1/n$ , but not than $N/n$ . Plugging in the new bounds for $\\beta$ and $\\bar{\\sigma}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg[\\frac{1}{n}\\|A^{t,T}-\\rho\\|_{2}^{2}\\leq\\frac{C_{0}e^{4M}}{\\lambda_{q}n/N}\\bigg(\\eta+\\frac{(\\log n)^{2}}{\\lambda_{q}t}\\bigg)\\bigg]\\geq1-n^{-C_{2}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Theorem E.1: $\\lambda_{q}\\gtrsim\\lambda_{\\mathrm{disc}}^{\\star}$ . Let $q$ be an optimiser of $\\lambda_{\\mathrm{disc}}^{\\star}\\colon\\lambda_{q}=\\lambda_{\\mathrm{disc}}^{\\star}$ . It can be formulated as a semidefinite program [10], so its solution can be approximated arbitrarily well in polynomial time. Extending $q$ from $E$ to $[n]^{2}$ by $q_{i,j}:=q_{\\{i,j\\}}\\mathbf{1}\\{\\{i,j\\}\\in E\\}$ defines a symmetric and substochatic matrix with the same Dirichlet form. Adjusting the diagonal has no effect on the Dirichlet form, so we may assume that it is, in fact, stochastic: its row sums are all exactly 1. The spectral gp of this $q$ is still $\\lambda_{q}$ , by the Dirichlet characterisation (Proposition B.1). ", "page_idx": 27}, {"type": "text", "text": "The Birkhoff\u2013von Neumann theorem allows the decomposition of any $n\\times n$ doubly stochastic matrix into a convex combination of (at most) $n^{2}$ permutation matrices in polynomial time: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q=\\sum_{\\ell=1}^{n^{2}}\\alpha_{\\ell}P_{\\sigma_{\\ell}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\alpha\\in[0,1]^{n^{2}}$ with $\\textstyle\\sum_{\\ell}\\alpha_{\\ell}=1$ and $P_{\\sigma}$ is the permutation matrix for $\\sigma\\in\\mathsf{S y m m}(n)$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n(P_{\\sigma})_{i,j}=\\mathbf{1}\\{j=\\sigma(i)\\}\\quad\\mathrm{for\\;all}\\quad i,j\\in[n].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This can be chosen so that each permutation matrix has non-zero entries only over graph edges. ", "page_idx": 28}, {"type": "text", "text": "Given a permutation $\\sigma_{i}$ , we decompose it into disjoint cycles. These cycles actually correspond to cycles in the graph; we can discard cycles with only one element. Let $v_{1},v_{2},...,v_{k}$ be the vertices of a particular cycle of length $k$ . Then, we can decompose the cycle into three matchings: ", "page_idx": 28}, {"type": "text", "text": "\u2022 one containing all the edges in the cycle of type $\\{v_{i},v_{i+1}\\}$ with odd $i\\in\\{1,...,k-1\\}$ ;   \n\u2022 one containing all the edges in the cycle of type $\\{v_{i},v_{i+1}\\}$ with even $i\\in\\{1,...,k-1\\}$ ;   \n\u2022 one matching containing the single edge $\\{v_{k},v_{1}\\}$ . ", "page_idx": 28}, {"type": "text", "text": "This decomposition gives rise to a procedure to sample matchings in the graph. ", "page_idx": 28}, {"type": "text", "text": "1. Sample a permutation $\\Sigma$ according to the convex combination: $\\mathbb{P}[\\Sigma=\\sigma_{\\ell}]=\\alpha_{\\ell}$ for each $\\ell$ . 2. For each cycle in $\\Sigma$ , independently sample one of the three induced matchings listed above, each with probability $1/3$ . ", "page_idx": 28}, {"type": "text", "text": "The vertices in different cycles in a permutation are distinct. Hence, this procedure does indeed product a matching. Moreover, the probability that a given edge $\\{i,,j\\}\\in E$ belongs to the sampled matching is precisely ${\\textstyle\\frac{1}{3}}q_{i,j}\\,=\\,{\\textstyle\\frac{1}{3}}q_{\\{i,j\\}}^{\\prime}$ . Hence, we have constructed a matching with associated spectral gap at least ${\\textstyle\\frac{1}{3}}\\lambda_{q}$ . But, by assumption, $\\lambda_{q}=\\lambda_{\\mathrm{disc}}^{\\star}$ , completing the proof. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: the abstract and introduction make clear the contributions of our paper. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We discuss limitations of our work in the introduction and clearly state the assumptions of our results. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: complete proofs are included in the Appendix. An outline of the proof of our main result is included in the main body of the text. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: in the main body of the paper, we include enough details in order to reproduce our experimental results. The supplementary material includes our code so that our experiments can be reproduced and verified. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: code is included in the supplementary material. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: we have included our choice of parameters in the paper. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: we include confidence intervals in the plots of our experiments. Statistical significance tests are not really relevant for our experimental results. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: the computer resources are not really relevant for interpreting our experiments. We include the number of games simulated which is a better way to measure the complexity of the algorithm analysed. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: we have read the NeurIPS Code of Ethics and our paper conform to it. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: we do not foresee any particular societal impact of our paper. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 32}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: the paper poses no such risks. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: our code uses an existing library to compute a von Neumann-Birkhoff decomposition of a matrix. We have included the library and its licence in the supplementary material. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: no new assets released. Code included for reproducibility has been commented and include a README file. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]