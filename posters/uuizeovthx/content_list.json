[{"type": "text", "text": "Safe and Efficient: A Primal-Dual Method for Offline Convex CMDPs under Partial Data Coverage ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haobo Zhang ShanghaiTech University zhanghb2023@shanghaitech.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Xiyue Peng ShanghaiTech University pengyx2024@shanghaitech.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Honghao Wei Washington State University honghao.wei@wsu.edu ", "page_idx": 0}, {"type": "text", "text": "Xin Liu\u2217 ShanghaiTech University liuxin7@shanghaitech.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Offline safe reinforcement learning (RL) aims to find an optimal policy using a pre-collected dataset when data collection is impractical or risky. We propose a novel linear programming (LP) based primal-dual algorithm for convex MDPs that incorporates \u201cuncertainty\u201d parameters to improve data efficiency while requiring only partial data coverage assumption. Our theoretical results achieve a sample complexity of $O(1/(1-\\gamma){\\sqrt{n}})$ under general function approximation, improving the current state-of-the-art by a factor of $1/(1-\\gamma)$ , where $n$ is the number of data samples in an offline dataset, and $\\gamma$ is the discount factor. The numerical experiments validate our theoretical findings, demonstrating the practical efficacy of our approach in achieving improved safety and learning efficiency in safe offilne settings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Safe reinforcement learning (RL) aims to learn a reward-maximizing policy while satisfying multiple safety constraints, demonstrating its practicality in many real-world applications, such as autonomous driving [Kiran et al., 2021], robotics [Levine et al., 2016], and healthcare [Yu et al., 2021]. In these tasks, certain behaviors may potentially harm the agent or its surroundings, which is crucial for task completion. One way to mathematically characterize safe RL is through Constrained Markov Decision Processes (CMDPs) [Altman, 2021], where safety constraints are incorporated into the problem when optimizing the objective. ", "page_idx": 0}, {"type": "text", "text": "Offline RL aims to learn a sequence of actions from a pre-collected dataset to address scenarios where interacting with the environment is risky, expensive, or impractical. Ensuring sample efficiency in offline RL with function approximation typically requires additional assumptions about both the function classes and the dataset due to training instability and distribution shift issues [Fujimoto et al., 2019, Kostrikov et al., 2021, Paine et al., 2020]. Earlier studies [Chen and Jiang, 2019, Liao et al., 2022, Liu et al., 2019, Wang et al., 2019, Zhang et al., 2020b] in offline RL usually require that all functions in the function space are Bellman-complete and that the dataset has full coverage, meaning it covers the state-action distributions induced by all policies. This might be a mild and accepted assumption in offilne RL without considering safety. However, it is highly unacceptable and impractical in safe offilne RL, as it would require the dataset to cover all hazardous state-action pairs induced by all dangerous policies. To address the full coverage issue, later works [Chen and Jiang, 2022, Rashidinejad et al., 2021, Uehara and Sun, 2021, Xie et al., 2021, Zhan et al., 2022, Zhu et al., ", "page_idx": 0}, {"type": "table", "img_path": "UuiZEOVtHx/tmp/d31e9bd1a4d45ffa5b7dafc671ed1d4b7840cc2c324e07ac77b26e4d56f1d7e4.jpg", "table_caption": ["Table 1: Comparison of algorithms for offline safe RL with function approximation. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "2023] reduce the assumption to single-policy coverage by using pessimism in the face of uncertainty. Unfortunately, all existing studies [Chen et al., 2022, Hong et al., 2024, Le et al., 2019] in safe offilne RL still require coverage for all policies. ", "page_idx": 1}, {"type": "text", "text": "Beyond traditional offline safe RL, many applications do not fit the standard RL problem [Abel et al., 2021]. There is a substantial body of literature [Geist et al., 2022, Mutti et al., 2023, Zahavy et al., 2021] studying a more general scenario called convex MDPs, where the objective function is modeled as a convex (or concave) utility function instead of linear, as in the standard RL problem. This framework is quite general and captures various learning scenarios, including imitation [Abbeel and $\\mathrm{Ng}$ , 2004], exploration [Hazan et al., 2019], and more. However, studying convex MDPs introduces additional challenges. In convex MDPs, moving beyond cumulative rewards means that the Bellman equation fails to hold due to the lack of reward additivity. This leads to breakdowns in many techniques based on Dynamic Programming (DP) [Zhang et al., 2020b]. Despite a large body of practical literature [Lee et al., 2021, Xu et al., 2022, Zheng et al., 2023], robust theoretical analysis remains lacking in this setting. ", "page_idx": 1}, {"type": "text", "text": "To address the coverage issue and extend the general function approximation setting, we focus on convex MDPs in the safe offline setting. Our main contributions are summarized below (the detailed comparisons can be found in Table 1): ", "page_idx": 1}, {"type": "text", "text": "We are the first to study offline convex MDPs under safety constraints with partial data coverage assumption. We reformulate the problem using marginalized importance weights to avoid issues caused by the Bellman equation\u2019s failure in convex MDPs. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We analyze the convergence rate of our proposed approach under partial coverage assumption and theoretically prove that our algorithm achieves $\\begin{array}{r}{\\dot{O}\\left(\\frac{1}{(1-\\gamma)\\sqrt{n}}\\right)}\\end{array}$ in both objective and violation bounds with general function approximation, when the number of iteration is larger than $n$ . The sample complexity of $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{1}{(1-\\gamma)\\sqrt{n}}\\right)}\\end{array}$ improves the best existing result by a factor of $1/(1-\\gamma)^{2}$ . ", "page_idx": 1}, {"type": "image", "img_path": "UuiZEOVtHx/tmp/120a3eaf214835b52d3b308c9adb9eeca45c23402ab5db526ae6e0d4143a5afe.jpg", "img_caption": ["Figure 1: Performance of our algorithm on FrozenLake with completely random data. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "\u2022 Experimental results on Imitation Learning and standard CMDPs demonstrate the generality and effectiveness of our algorithm. As Figure 1 shows our algorithm performs well even with a completely random and safety-violated offline dataset with general function approximation, which verifies our theoretical findings. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Offline Safe RL: The offline safe reinforcement learning setting entails the agent learning from a fixed dataset while adhering to safety constraints. This involves a blend of offline RL and safe RL, yet research on this approach, particularly concerning theoretical analysis, remains limited. BCQL augmented by BCQ [Fujimoto et al., 2019] optimizes the policy in the offline phase and applies the Lagrange method to handle constraints. CPQ [Xu et al., 2022] tackles the safety constraints by overestimating the cost value function of out-of-distribution (\u2019unsafe\u2019) actions and updating the reward value function with \u2019safe\u2019 actions. Another line of work [Lee et al., 2021] using DICE-style technique optimizes policy by calculating the stationary distribution of state-action pairs instead of value function and extracts the policy by importance-sample behavior cloning. CDT [Liu et al., 2023b] combines Decision Transformer with safety constraints and utilizes data augmentation based on Pareto frontiers to enhance the safety and adaptability of the Transformer. It also focuses on the model\u2019s capability to different cost thresholds. The work [Le et al., 2019] proposes a meta-algorithm, using Fitted Q Evaluation and Fitted Q Iteration as subroutines to evaluate safety constraints and learn policy respectively. [Chen et al., 2022] analyzes the information-theoretic sample complexity lower bound and proposes a near-optimal primal-dual learning algorithm under partial data coverage but without function approximation. The most related work [Hong et al., 2024] approaches the problem from the perspective of the Actor-Critic algorithm, analyzing the sample complexity to be $\\begin{array}{r}{\\dot{\\mathcal{O}}(\\frac{1}{(1-\\gamma)^{2}\\sqrt{n}})}\\end{array}$ under Slater\u2019s condition and full data coverage assumption. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Convex MDP: Convex MDP problem extends the scope of MDP by focusing on convex objective functions of stationary distribution, rather than an inner product between reward and stationary distribution. To address the challenge, [Zhang et al., 2020b] introduces a variational Monte Carlo gradient estimation algorithm, demonstrating convergence to the optimal policy across general utility functions. The work [Zahavy et al., 2021] utilizes Fenchel duality to cast convex MDPs as min-max \"two-player\" games, proposing a meta-algorithm that addresses various convex MDPs through distinct subroutines. The work [Geist et al., 2022] approaches convex MDPs from the Mean-Field Game (MFG) perspective, establishing the equivalence between the optimal condition in convex MDPs and Nash equilibrium in MFGs. [Ying et al., 2023] studies the convex CMDP problem through a policy-based primal-dual algorithm and proves an ${\\mathcal{O}}\\left(T^{-1/3}\\right)$ convergence rate in both optimality gap and constraint violation. The existing literature on convex MDPs primarily focuses on the online setting, while this paper specifically targets the offline scenario. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Convex CMDP problem ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We study a discounted constrained Markov decision process (CMDP), denoted by $\\mathcal{M}\\quad=$ $(S,\\mathcal{A},R;C,T,\\gamma,\\mu_{0})$ , where $\\boldsymbol{S}$ is the state space (a finite set), $\\boldsymbol{\\mathcal{A}}$ is the action space (a finite set), $R:\\mathcal{S}\\times\\mathcal{A}\\to[0,1]$ and $C:\\mathcal{S\\times A}\\to[0,1]$ are reward and cost functions, respectively, $T:S\\times A\\to\\Delta(S)$ is the transition probability kernel, where $\\Delta(\\cdot)$ denotes the probability simplex, $\\gamma\\in[0,1)$ denotes the discount factor, and $\\mu_{0}\\,\\in\\,\\Delta(S)$ is the initial state distribution. We define a policy $\\pi:S\\to\\Delta(A)$ as a probability mapping from states to actions. At time slot $t$ , the agent observes state $s_{t}$ and takes action $a_{t}$ according to the policy $\\pi$ . For a policy $\\pi$ , we define its discounted state-action occupancy measure $d_{\\pi}$ as follows ", "page_idx": 2}, {"type": "equation", "text": "$$\nd_{\\pi}(s,a)=(1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{P}_{\\pi}\\left(s_{t}=s,a_{t}=a\\right),\\;\\;\\forall s\\in S,\\;a\\in\\mathcal{A},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbb{P}_{\\pi}\\left(s_{t},\\,a_{t}\\right)$ is the probability of state-action pair $(s,a)$ being visited at time slot $t$ under the policy $\\pi$ . Further, we let $\\begin{array}{r}{\\dot{d}_{\\pi}\\left(s\\right)=\\dot{\\sum}_{a\\in A}\\,d_{\\pi}\\left(s,a\\right)}\\end{array}$ be the discounted state occupancy measure. ", "page_idx": 2}, {"type": "text", "text": "Given the occupancy measure above, we can formulate the following convex CMDP problem ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{d_{\\pi}\\in K}\\;f\\left(d_{\\pi}\\right)\\quad s.t.\\;\\;g\\left(d_{\\pi}\\right)\\leq\\tau,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $f$ and $g$ are both convex functions, and $\\tau$ is the cost threshold. We present a single safety constraint for simple exposition, and our result can be readily generalized to the setting with multiple constraints. The set $\\kappa$ is a probability simplex that satisfies global balance equations of the underlying Markov process ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{K}=\\{d\\mid\\sum_{a}d(s,a)=(1-\\gamma)\\,\\mu_{0}(s)+\\sum_{s^{\\prime},a^{\\prime}}T\\,(s\\mid s^{\\prime},a^{\\prime})\\,d\\left(s^{\\prime},a^{\\prime}\\right),\\,\\forall s\\in\\mathcal{S}\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This set can also be written as a compact matrix form that ", "page_idx": 2}, {"type": "equation", "text": "$$\nK=\\{d\\;|\\;M d=(1-\\gamma)\\mu_{0}\\;\\},\\;M=\\mathrm{Diag}(\\mathbf{1}_{|A|}^{\\top},\\cdot\\cdot\\cdot,\\mathbf{1}_{|A|}^{\\top})-\\gamma P,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The convex CMDP problem in (1) is quite general to capture various learning scenarios, including apprenticeship learning [Abbeel and $\\mathrm{Ng}$ , 2004], standard CMDPs [Altman, 2021], pure exploration [Hazan et al., 2019], and inverse reinforcement learning in contextual MDPs [Belogolovsky et al., 2021]. Take the standard CMDP as an example, the agent aims to find a policy that maximizes cumulative reward while satisfying the safety constraints [Altman, 2021], which can be written as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{d_{\\pi}\\in K}\\ \\ \\sum_{s,a}r(s,a)d_{\\pi}(s,a)\\quad s.t.\\ \\sum_{s,a}c(s,a)d_{\\pi}(s,a)\\leq\\tau,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f$ and $g$ are linear functions w.r.t. the state-action occupancy measure. ", "page_idx": 3}, {"type": "text", "text": "Take safety-aware apprenticeship learning [Zhou and Li, 2018] for another example, the agent aims to mimic the expert\u2019s demonstrations while avoiding the unsafe states in $\\mathcal{H}$ , which can be written as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{d_{\\pi}\\in K}\\;\\;F\\left(d_{\\pi},d_{e}\\right)\\;\\;\\;\\;s.t.\\;\\;d_{\\pi}(s)\\leq\\tau,\\;\\forall s\\in\\mathcal{H}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $d_{e}$ represents the empirical distribution of the expert\u2019s demonstration and $F(\\cdot,\\cdot)$ is the convex distance function, such as KL-divergence or total variation distance [Zhang et al., 2020a]. ", "page_idx": 3}, {"type": "text", "text": "However, solving these problems in the online setting can be time-consuming, costly, and potentially dangerous in safety-critical contexts. In contrast, by leveraging historical data, offline RL offers a promising avenue for developing safe and effective algorithms, as introduced next. ", "page_idx": 3}, {"type": "text", "text": "2.2 Offline Reinforcement Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In offline RL, we cannot interact with the environment and only have access to dataset with a finite number of samples. Let $\\mathcal{D}=\\left(s_{i},a_{i},r_{i},c_{i}\\right)_{i=1}^{n}$ be a collected offilne dataset where we assume all the pairs $(s_{i},a_{i})_{i=1}^{n}$ are generated independently and identically distributed $(i.i.d.)$ from data distribution $\\mu(s,a)$ induced by a behavior policy $\\pi_{\\mu}$ . Let $n_{\\mathcal{D}}(s,a)$ represents the number of occurrences of the state-action pair $(s,a)$ in the offilne dataset $\\mathcal{D}$ , then $\\mu_{\\mathscr D}(s,\\bar{a})=n_{\\mathscr D}(s,a)/n$ is an empirical version of $\\mu(s,a)$ . In offilne RL, a major challenge is distribution shift, which measures the mismatch between data distribution and occupancy measure induced by candidate policies. To quantify the distribution shift, we make the following $\\pi^{*}$ concentrability assumption that refers to partial data coverage, ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 ( $\\pi^{*}$ \u2013concentrability). Let $\\pi^{*}$ be the optimal policy to problem (1), there exists constant $C_{\\pi^{*}}>0$ such that $d_{\\pi^{*}}(s,a)/\\mu(s,a)\\leq C_{\\pi^{*}}$ , for all $s\\in{\\mathcal{S}},a\\in{\\mathcal{A}}$ . ", "page_idx": 3}, {"type": "text", "text": "This assumption controls the distribution shift between offilne data distribution $\\mu$ and the occupancy measure $d_{\\pi^{*}}$ induced by optimal policy $\\pi^{*}$ . Specifically, the partial data coverage assumption indicates the offilne dataset $\\mathcal{D}$ should cover state-action pairs visited by the optimal policy $\\pi^{*}$ . Unlike the common full data coverage assumption in previous works that the dataset $\\mathcal{D}$ should encompass data visited by all policies [Chen and Jiang, 2019, Hong et al., 2024, Le et al., 2019], our assumption is considerably more relaxed. Furthermore, in the field of safe RL, partial coverage assumption also offers significant advantages, as the full coverage implies the behavior policy needs to visit every dangerous state and action space, which is obviously impractical. ", "page_idx": 3}, {"type": "text", "text": "Beyond using assumptions to limit distribution shift in offilne RL, we consider Marginalized Importance Weight (MIW), a method widely used in the existing literature [Hong et al., 2024, Ozdaglar et al., 2023, Zhan et al., 2022], to further address this challenge. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Marginalized Importance Weight). Given a policy $\\pi$ , let d be the occupancy measure induced by $\\pi$ . We define marginalized importance weight $w:S\\times A\\to\\mathbb{R}^{+}$ as $\\begin{array}{r}{\\bar{w(s,a)}=\\frac{d(s,a)}{\\mu(s,a)}}\\end{array}$ , $\\forall s\\in{\\mathcal{S}},\\,a\\in{\\mathcal{A}}$ . ", "page_idx": 3}, {"type": "text", "text": "Note $w$ can be regarded as the density ratio between the normalized discounted occupancy measure and data distribution. Moreover, recall the definition of $w$ and $M$ in equation (2) , we define matrix $K\\in\\mathbb{R}^{|S|\\times|S||A|}$ and $K_{\\mathcal{D}}\\in\\mathbb{R}^{|\\mathcal{S}|\\times|\\mathcal{S}||\\mathcal{A}|}$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\nK(s^{\\prime},(s,a))=M(s^{\\prime},(s,a))\\cdot\\mu(s,a),\\enspace K_{\\mathcal{D}}(s^{\\prime},(s,a))=M(s^{\\prime},(s,a))\\cdot\\mu_{\\mathcal{D}}(s,a),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $K_{D}$ can be seen as an empirical version of $K$ in dataset $\\mathcal{D}$ and it is straightforward to verify $K w=M d$ . With these notations, we establish an equivalent formulation with problem (1) w.r.t. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{w\\geq0}{\\operatorname*{min}}}&{f(\\mu\\cdot w)}\\\\ {\\mathrm{s.t.}\\;\\;}&{K w=(1-\\gamma)\\mu_{0}}\\\\ &{g(\\mu\\cdot w)\\leq\\tau}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the operator \u201c\u00b7\u201d denotes the element-wise product of vectors. As we aim to establish the sample-efficient learning algorithms in large state-action spaces, we regard the importance weight $w$ as a function, i.e. $w:S\\times A\\to\\mathbb{R_{+}}$ that belongs to the convex function class $\\mathcal{W}$ in the following assumption. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2 (Realizability). We assume $w^{*}\\in\\mathcal{W}$ where $w^{*}$ is the optimal solution to the problem (5)\u2013(7). ", "page_idx": 4}, {"type": "text", "text": "This assumption assumes the optimal solution $w^{\\ast}(\\pi^{\\ast})$ can be realizable for a convex function class $\\mathcal{W}$ . Note that the assumption of the convex function class $\\mathcal{W}$ is reasonable and standard, which is also achievable with the convexification process in case the function class is non-convex. The convexification process is a common practice in the offline safe RL literature, particularly in the context of general function approximation [Le et al., 2019, Hong et al., 2024]. Now we consider $\\mathcal{W}$ as a discrete function class for simplicity and it readily extends to continuous settings (in Remark 1). Further, we introduce a completeness assumption that is used to relax the key constraint in (6). ", "page_idx": 4}, {"type": "text", "text": "Assumption 3 (Completeness). Let $x_{w}$ be within the function class $\\mathcal{X}$ . Define a mapping $\\phi$ : such that $\\phi(w)^{\\top}(K w-(1-\\gamma)\\mu_{0})=x_{w}^{\\top}(K w-(1-\\gamma)\\mu_{0})=\\|K w-(1-\\gamma)\\mu_{0}\\|_{1}$ . Then, we have $(\\mathcal{W},\\mathcal{X})$ -completeness under the mapping $\\phi$ , i.e. $x_{w}\\in\\mathcal{X}$ for all $w\\in\\mathcal{W}$ . ", "page_idx": 4}, {"type": "text", "text": "Intuitively, the completeness assumption allows us to replace the computation of the $l_{1}$ -norm with a linear product and simplifies our subsequent analysis. Next, we introduce the standard boundness assumption in offilne RL literature with general function approximation [Chen and Jiang, 2019, 2022, Le et al., 2019, Munos and Szepesv\u00e1ri, 2008]. ", "page_idx": 4}, {"type": "text", "text": "Assumption 4 (Boundness of $\\mathcal{W}$ and $\\mathcal{X}$ ). We assume function classes $\\mathcal{W}$ and $\\mathcal{X}$ are bounded, i.e.   \n$\\|w\\|_{\\infty}\\leq B_{w}$ , $\\forall w\\in\\mathcal{W}$ and $\\|\\boldsymbol{x}_{w}\\|_{\\infty}\\le B_{w}$ , $\\forall x_{w}\\in\\mathcal{X}$ . ", "page_idx": 4}, {"type": "text", "text": "Combining this assumption with Assumption 1 implies $B_{w}\\;\\geq\\;C_{\\pi^{*}}$ . Lastly, we impose a mild assumption for the reward and cost functions in the problem (1). ", "page_idx": 4}, {"type": "text", "text": "Assumption 5 (Lipschitz condition). The functions $f(x)$ and $g(x)$ are convex and satisfy the Lipschitz condition, where there exist constants $L_{f}$ and $L_{g}$ such that for any $x,y,$ , the following inequalities hold $|f(x)-f(y)|\\leq L_{f}\\|x-y\\|$ and $|g(x)-g(y)|\\leq L_{g}\\|x-y\\|$ . ", "page_idx": 4}, {"type": "text", "text": "3 Algorithm Design and Main Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Despite the practical importance of offline safe RL in real-world applications, there is still a lack of theoretical research on this topic. The earlier literature on this setting often lacks robust theoretical analysis [Lee et al., 2021, Liu et al., 2023b, Xu et al., 2022], and articles with theoretical analysis either yield unsatisfactory results or rely on strong assumptions [Chen et al., 2022, Hong et al., 2024, Le et al., 2019]. ", "page_idx": 4}, {"type": "text", "text": "In this section, we propose a provable algorithmic framework and establish the first theoretical result in offilne convex CMDP, to the best of our knowledge. Moreover, when reducing to standard offilne CMDP, we achieve a sample complexity of O (1\u2212\u03b31)\u221an with general function approximation, which improves the current state-of-the-art result by a factor of $1/(1-\\gamma)$ . ", "page_idx": 4}, {"type": "text", "text": "3.1 Algorithm Design ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Inspired by [Ozdaglar et al., 2023], we first introduce an empirical version of problem (5)\u2013(7) in the offilne setting by incorporating a suitable relaxed parameter into the safety constraint. Intuitively, the empirical version of the problem is \"close\" to the original problem when the dataset is large. This is the key observation for analyzing the convergence performance and safety violations. To solve (5)\u2013(7), we present its empirical and relaxed problem: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{w\\in\\mathcal{W}}{\\mathrm{min}}}&{f(\\mu_{\\mathcal{D}}\\cdot w)}\\\\ {\\mathrm{s.t.}}&{\\|K_{\\mathcal{D}}w-(1-\\gamma)\\mu_{0}\\|_{1}\\leq\\zeta,}\\\\ &{g(\\mu_{\\mathcal{D}}\\cdot w)-\\tau\\leq\\kappa,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mu_{\\mathscr D}$ and $K_{D}$ are the empirical version of $\\mu$ and $K$ , respectively; $\\zeta$ and $\\kappa$ are the relaxed hyperparameters for the validity constraint (Bellman equation) and safety constraint. Intuitively, the parameters of $\\zeta$ and $\\kappa$ capture the \u201cuncertainty\u201d induced by the offline dataset $\\mathcal{D}$ in terms of distribution shift and safety concerns. The values of these parameters will be specified later and play an important role in our analysis. Next, we demonstrate that the mismatch of the objective and constraint violation bound depends on the \u201cuncertainty\u201d, which exhibits $\\mathcal{O}(1/\\sqrt{n})$ distance. ", "page_idx": 5}, {"type": "text", "text": "Note that all parameters in the optimization problem (8)\u2013(10) can be determined from the offline dataset. When the state-action space is not large, this problem can be efficiently solved by convex optimization solvers. However, when the state-action space is large, and the function approximation is necessary (e.g., $w$ is parameterized by a neural network), it would be quite challenging (if not impossible) to solve this problem. To address this challenge, we propose a primal-dual algorithm that is sample-efficient and computationally tractable to solve the problem iteratively. We first introduce the Lagrange function of problem (8)\u2013(10), ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}\\left(w,\\lambda,\\phi\\right)=f\\left(\\mu_{\\mathcal{D}}\\cdot w\\right)+\\lambda\\left(\\|K_{\\mathcal{D}}w-(1-\\gamma))\\mu_{0}\\|_{1}-\\zeta\\right)+\\phi\\left(g\\left(\\mu_{\\mathcal{D}}\\cdot w\\right)-\\tau-\\kappa\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda$ and $\\phi$ are Lagrange multipliers. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1: Primal-dual algorithm for Offline Convex CMDP (POCC) 1 Input: Dataset $\\mathcal{D}=\\{(s_{i},a_{i},r_{i},c_{i})\\}_{i=1}^{n}$ , the relaxed parameters $\\kappa$ , $\\zeta$ , and the step size $\\begin{array}{r}{\\eta=\\frac{1}{\\sqrt{K}}}\\end{array}$ ; 2 Initialization: Choose any $w^{1}\\in\\mathcal{W}$ and the Lagrangian multipliers $\\lambda^{1}=0,\\,\\phi^{1}=0;$ ; 3 for $k=1,2,\\ldots,K$ do Prim $\\begin{array}{r l}&{\\mathbf{i}\\colon\\;w^{k+1}=\\mathcal{P}_{\\mathcal{W}}\\left[w^{k}-\\eta\\nabla\\mathcal{L}_{w}(w^{k},\\lambda^{k},\\phi^{k})\\right],}\\\\ &{\\;\\;\\phi^{k+1}=\\left[\\tau^{k}-\\eta\\nabla\\mathcal{L}_{\\phi}(w^{k},\\lambda^{k},\\phi^{k})\\right]_{0}^{\\phi_{m a x}^{k+1}},\\;\\;\\lambda^{k+1}=\\left[\\lambda^{k}-\\eta\\nabla\\mathcal{L}_{\\lambda}(w^{k},\\lambda^{k},\\phi^{k})\\right]_{0}^{\\lambda_{\\operatorname*{max}}^{k+1}},}\\end{array}$ 4 where $\\mathcal{P}_{\\mathcal{W}}$ is the projection onto set $\\mathcal{W}$ and $[\\cdot]_{l}^{h}$ is the projection onto interval $[l,h]$ . 5 Compute the average $\\textstyle{\\overline{{w}}}_{K}=\\sum_{i=1}^{K}w^{i}$ ; 6 Extract the policy $\\overline{{\\pi}}_{K}$ with f ormula (12); 7 Output: Policy $\\overline{{\\pi}}_{K}$ ; ", "page_idx": 5}, {"type": "text", "text": "Given the Lagrange function above, we introduce our algorithm called POCC (in Algorithm 1), which takes the offilne dataset $\\mathcal{D}$ as input and runs a primal-dual method on the estimated Lagrange function. Specifically, at each iteration $k$ , POCC updates the importance weight by gradient descent and projects it back to function class $\\mathcal{W}$ , then updates Lagrange multipliers of validity constraint (6) and safety constraint (7) respectively. After $K$ steps, POCC returns an averaged $\\overline{{w}}_{K}$ , we can extract the corresponding policy $\\overline{{\\pi}}_{K}$ based on the offline dataset $\\mathcal{D}$ as follows ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi_{K}(a\\mid s):=\\left\\{\\frac{\\overline{{\\ w}}_{K}(s,a)\\pi_{\\mu}(a\\mid s)}{\\sum_{a^{\\prime}\\in A}\\overline{{w}}_{K}(s,a^{\\prime})\\pi_{\\mu}(a^{\\prime}\\mid s)},\\right.\\,\\,\\,\\,\\mathrm{if}\\,\\,\\,\\left.\\sum_{a^{\\prime}\\in A}\\overline{{w}}_{K}(s,a^{\\prime})\\pi_{\\mu}(a^{\\prime}\\mid s)>0\\right.}\\\\ {\\frac{1}{|A|},\\qquad\\qquad\\qquad\\qquad\\qquad\\,\\,\\mathrm{if}\\,\\,\\,\\left.\\sum_{a^{\\prime}\\in A}\\overline{{w}}_{K}(s,a^{\\prime})\\pi_{\\mu}(a^{\\prime}\\mid s)=0\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the second equality means that if $\\begin{array}{r}{\\sum_{a^{\\prime}\\in\\mathcal{A}}\\overline{{w}}_{K}(s,a^{\\prime})\\pi_{\\mu}(a^{\\prime}\\mid s)=0}\\end{array}$ we randomly choose an action for state $s$ . ", "page_idx": 5}, {"type": "text", "text": "3.2 Theoretical Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We present the theoretical results of our proposed approach in the following theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (Sample complexity of $\\overline{{\\pi}}_{K}$ ). Suppose Assumptions $^{I-7}$ hold. Denote $\\overline{{\\pi}}_{K}$ as the corresponding policy induced by $\\overline{{w}}_{K}$ . Set the relaxed parameters $\\begin{array}{r}{\\zeta\\,=\\,\\frac{2\\sqrt{2}B_{w}}{\\sqrt{n}}\\sqrt{\\log\\frac{|\\mathcal{W}||\\mathcal{X}|}{\\delta}}}\\end{array}$ and ", "page_idx": 5}, {"type": "text", "text": "$\\begin{array}{r}{\\kappa=\\frac{\\sqrt{2}L_{g}B_{w}}{\\sqrt{n}}\\sqrt{\\log\\frac{2|\\mathcal{W}|}{\\delta}}}\\end{array}$ , and the step size $\\begin{array}{r}{\\eta=\\frac{1}{\\sqrt{K}}}\\end{array}$ . Let the constants $\\begin{array}{r}{\\upsilon\\,=\\,\\frac{1}{1-\\gamma}\\left(4B+4L+2\\varepsilon\\right)}\\end{array}$ and $\\begin{array}{r}{\\iota=\\frac{1}{1-\\gamma}\\left(B^{2}+4B+4L+\\frac{L^{2}}{\\sqrt{K}}+\\varepsilon\\right)}\\end{array}$ , we have, with at least $1-8\\delta$ probability, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{J_{r}(\\overline{{\\pi}}_{K})-J_{r}(\\pi^{*})\\,\\leq\\frac{6L_{f}B_{w}\\sqrt{2\\log(|\\mathcal{W}||\\mathcal{X}|/\\delta)}}{\\displaystyle(1-\\gamma)\\,\\sqrt{n}}+\\frac{\\iota}{2\\sqrt{K}}}\\\\ {J_{c}\\left(\\overline{{\\pi}}_{K}\\right)-\\tau\\,\\leq\\frac{6L_{g}B_{w}\\sqrt{2\\log(2|\\mathcal{W}||\\mathcal{X}|/\\delta)}}{\\displaystyle(1-\\gamma)\\,\\sqrt{n}}+\\frac{\\upsilon}{\\sqrt{K}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $J_{r}(\\overline{{\\pi}}_{K})=f(\\overline{{d}}_{K})$ and $J_{c}(\\overline{{\\pi}}_{K})=g(\\overline{{d}}_{K})$ are objective and constraint performance of policy $\\pi$ respectively, $B$ represents the distance between initial value $w^{1}$ of the iteration and optimal solution $w_{\\mathscr D}$ to problem (8)\u2013(10), $\\varepsilon\\geq0$ is a constant, $L$ is the max Lipschitz constant of Lagrange function and $K$ is the number of iterations. ", "page_idx": 6}, {"type": "text", "text": "Remark 1. We remark that Theorem $^{\\,l}$ remains valid even when the function class $\\mathcal{W}$ is a continuous set. In this case, the cardinality $|\\mathcal{W}|$ can be replaced with the covering number of $\\mathcal{W}$ , and the union bound can be applied to its $\\epsilon$ -covering set. This adjustment also preserves the sample complexity order of $\\mathcal{O}(1/\\sqrt{n})$ , up to a constant dependent on \u03f5. For further details on the extension from discrete $\\mathcal{W}$ to a continuous set, please refer to [Le et al., 2019, Xie and Jiang, 2021]. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1 demonstrates that the convergence performance of our algorithm can be divided into two parts: $\\mathcal{O}(1/\\sqrt{n})+\\mathcal{O}(1/\\sqrt{K})$ . Regarding the first term, except for the size of dataset, it mainly depends on the function class (searching space) $\\log|\\mathcal{W}||\\mathcal{X}|$ and relaxed parameters $\\zeta$ and $\\kappa$ that capture the \u201cuncertainty\u201d for addressing the distribution shift and safety concern. Moreover, the second term ${\\mathcal{O}}(1/{\\sqrt{K}})$ connects with the error bound between $\\overline{{w}}_{K}$ and optimal solution $w_{\\mathscr D}$ , which decays at the rate of $1/\\sqrt{K}$ when the number of iterations increases. It is worth noting that the convergence rate of policy $\\overline{{\\pi}}_{K}$ is still $\\mathcal{O}(1/\\sqrt{n})$ when the iterative number is sufficient to satisfy $K\\geq n$ . The algorithm is gradient-based and does not involve additional computations for solving the optimization problem (8)\u2013(10). This suggests that we can improve the convergence performance w.r.t. $K$ by employing more advanced primal-dual techniques to reduce the convergence rate of the second term, such as those discussed in [Yu and Neely, 2017] with a convergence rate of ${\\mathcal{O}}(1/K)$ . ", "page_idx": 6}, {"type": "text", "text": "Unlike most of the previous work focuses on convex MDPs in online setting [Bai et al., 2023, Ying et al., 2023, Zahavy et al., 2021, Zhang et al., 2020b], Theorem 1 to the best of our knowledge, is the first provable result in offilne convex MDPs. Moreover, compared to the best result $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{1}{(1-\\gamma)^{2}\\sqrt{n}}\\right)}\\end{array}$ in offline CMDP [Hong et al., 2024, Chen et al., 2022], our results achieve a sample complexity of $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{1}{(1-\\gamma)\\sqrt{n}}\\right)}\\end{array}$ , which outperform the state-of-the-art by a factor of $1/(1-\\gamma)$ . Besides, our algorithm is appropriate in the scenario with large-scale state-action space due to the general function approximation for $w$ while the work [Chen et al., 2022] targets the tabular setting; our result is more favorable in the safety applications compared to [Hong et al., 2024] because we replace the realizability of value function $\\nu$ with a slightly stronger completeness assumption but reduce the data coverage from full to partial. As stated in Assumption 1, the full data coverage not only implies access to a highly exploratory dataset but also is impractical for offilne safe RL, as it assumes behavior policy needs to visit every dangerous state-action pair. Finally, we want to comment that the previous work all focuses on the standard RL, whose objective function is linear, while our algorithm is general enough to tackle the convex MDPs. ", "page_idx": 6}, {"type": "text", "text": "Remark 2. In the theorem, we assume prior knowledge of the behavior policy $\\pi_{\\mu}$ for the sake of exposition. However, in practice, it is often challenging to know the behavior policy in advance, as we typically only have access to the offilne dataset. The most popular approach to tackle this challenge is behavior clone. It posits that $\\hat{\\pi}_{\\mu}$ can be estimated as $\\begin{array}{r}{\\hat{\\pi}_{\\mu}(a|s)=\\frac{n(s,a)}{n(s)}}\\end{array}$ nn((s,sa)) , where n(s, a) denotes the number of the occurrences of the state-action pair $(s,a)$ in the offline dataset. We employed this estimation method in the experiments and results demonstrate its effectiveness. ", "page_idx": 6}, {"type": "text", "text": "4 Theoretical Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present a sketch of the proof of Theorem 1. We focus on illustrating the analysis of the constraint violation, and the convergence of the objective follows similar steps. The detailed proof can be found in Appendix A. ", "page_idx": 6}, {"type": "text", "text": "We first decompose the bound into different major terms and then sutdy them individually ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{c}(\\overline{{\\pi}}_{K})-\\tau=g(d_{\\overline{{\\pi}}_{K}})-\\tau}\\\\ &{\\qquad\\qquad=\\underbrace{g(d_{\\overline{{\\pi}}_{K}})-g(\\overline{{d}}_{K})}_{\\mathrm{I}}+\\underbrace{g(\\mu\\cdot\\overline{{w}}_{K})-g(\\mu_{\\mathcal{D}}\\cdot\\overline{{w}}_{K})}_{\\mathrm{II}}+\\underbrace{g(\\mu_{\\mathcal{D}}\\cdot\\overline{{w}}_{K})-\\tau}_{\\mathrm{III}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The first equality holds due to the definition: $J_{c}(\\overline{{\\pi}}_{K})=g(d_{\\overline{{\\pi}}_{K}})$ , where $d_{\\overline{{{\\pi}}}_{K}}$ represents the occupancy measure induced by returned policy $\\overline{{\\pi}}_{K}$ . Recall the definition $w(s,a)\\cdot{\\ddot{\\mu}}(s,a)=d(s,a)$ , we then have $g(\\overline{d}_{K})=g(\\boldsymbol\\mu\\boldsymbol\\cdot\\overline{w}_{K})$ . It is worth noting that the decomposition is intuitive since terms I and $\\mathrm{II}$ relates to the idea of constructing \u201cuncertainty\u201d parameter for the offilne dataset and term III mainly depends on the convergence of primal-dual method. ", "page_idx": 7}, {"type": "text", "text": "Specifically, term I is the distance between $g\\big(\\overline{{d}}_{K}\\big)$ and $g(d_{\\overline{{\\pi}}_{K}})$ . It represents the error that we rectify the unnormalized occupancy measure $\\overline{{d}}_{K}$ , which violates the validity constraint (6), to a satisfying one $d_{\\overline{{\\pi}}_{K}}$ . The term II illustrates the error incurred when applying the calculated $\\overline{{w}}_{K}$ from offline dataset $\\mathcal{D}$ to the real environment $\\mu$ , which depends on the sample size of the dataset. The term III is related to the relaxed parameter $\\kappa$ in safety constraint (7) and the distance between returned $\\overline{{w}}_{K}$ and optimal solution $w_{\\mathscr D}$ . Next, we present the following lemmas to bound these terms. ", "page_idx": 7}, {"type": "text", "text": "Lemma 1. Suppose Assumptions 1\u20134 hold, we have, with probability at least $1-2\\delta$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\nJ_{c}(\\overline{{\\pi}}_{K})-g(\\overline{{d}}_{K})\\leq\\frac{4L_{g}B_{w}\\sqrt{2\\log(|\\mathcal{W}||\\mathcal{X}|/\\delta)}}{(1-\\gamma)\\sqrt{n}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Lemma 2. Suppose Assumptions $^{l-4}$ hold. For $\\overline{{w}}_{K}\\in\\mathcal{W}$ , we have, with probability at least $1-\\delta$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\ng(\\mu\\cdot\\overline{{{w}}}_{K})-g(\\mu_{\\mathcal{D}}\\cdot\\overline{{{w}}}_{K})\\leq\\frac{\\sqrt{2}L_{g}B_{w}}{\\sqrt{n}}\\sqrt{\\log\\frac{2|\\mathcal{W}|}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Lemma 3. Suppose Assumptions $^{I-7}$ hold. For $\\overline{{w}}_{K}\\in\\mathcal{W}$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\ng(\\mu_{D}\\cdot\\overline{{w}}_{K})-\\tau\\leq\\kappa+\\frac{\\upsilon}{\\sqrt{K}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Combining the above lemmas, we have, with at least $1-3\\delta$ probability, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{c}(\\overline{{\\pi}}_{K})-\\tau=\\underbrace{g(d_{\\overline{{\\pi}}_{K}})-g(\\overline{{d}}_{K})}_{\\mathrm{I}}+\\underbrace{g(\\mu\\cdot\\overline{{w}}_{K})-g(\\mu_{\\mathcal{D}}\\cdot\\overline{{w}}_{K})}_{\\mathrm{II}}+\\underbrace{g(\\mu_{\\mathcal{D}}\\cdot\\overline{{w}}_{K})-\\tau}_{\\mathrm{III}}}\\\\ &{\\qquad\\quad\\leq\\frac{4L_{g}B_{w}\\sqrt{2\\log(|\\mathcal{W}||\\mathcal{X}|/\\delta)}}{(1-\\gamma)\\sqrt{n}}+\\frac{2\\sqrt{2}L_{g}B_{w}}{\\sqrt{n}}\\sqrt{\\log\\frac{2|\\mathcal{W}|}{\\delta}}+\\frac{v}{\\sqrt{K}}}\\\\ &{\\qquad\\quad\\leq\\frac{6\\sqrt{2}L_{g}B_{w}\\sqrt{2\\log(2|\\mathcal{W}||\\mathcal{X}|/\\delta)}}{(1-\\gamma)\\sqrt{n}}+\\frac{v}{\\sqrt{K}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where we set $\\begin{array}{r}{\\zeta=\\frac{2\\sqrt{2}B_{w}}{\\sqrt{n}}\\sqrt{\\log\\frac{|{\\mathcal W}||\\mathcal{X}|}{\\delta}},\\kappa=\\frac{\\sqrt{2}L_{g}B_{w}}{\\sqrt{n}}\\sqrt{\\log\\frac{2|{\\mathcal W}|}{\\delta}}.}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section aims to justify the effectiveness of our proposed framework through numerical experiments. We test a practical version of Algorithm 1, where we replace the gradient-type update with the \u201cAdam\u201d-type update (the detailed algorithm can be found in Algorithm 2 in the appendix). We test our algorithm to two specific offilne convex CMDPs: 1) Safe imitation learning and 2) standard offline CMDP. Our objective is to address the following questions: (i) Are the experimental results consistent with our theory? (ii) how does the data quality affect the performance of our algorithm? The additional details can be found in the Appendix B. ", "page_idx": 7}, {"type": "text", "text": "5.1 Safe Imitation Learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To showcase the generality of our algorithm, we choose imitation learning as a user case of convex MDPs and conduct the experiments in a maze environment. We design the environment, as illustrated in Figure 2, modified from [Geist et al., 2022]. The problem of (safe) imitation learning can be formulated as $F(d)=K L(d\\,||\\,d_{E})$ , where $d_{E}$ represents the stationary distribution of an expert. The environment is deterministic; agent has four actions (left, down, right, up); moving towards the wall (white) and the boundary does not change the state; the goal is to learn from the expert demonstrations (yellow) under safety constraints. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We collect data by expert demonstrations in (a) but randomly remove $25\\%$ states. We intend for the algorithm to learn to fill in the gaps using its inherent properties and function approximation, as emphasized in our theory. We are presenting two sets of results: one that considers safety constraints with a cost threshold of 0, and another that does not consider safety constraints. It\u2019s important to note that we cannot simply take the stationary distribution of the dataset as our final result. This is due to several reasons, including the fact that expert demonstrations are incomplete ( $25\\%$ of states are removed), simple replacement leads to poor results, and it\u2019s not suitable when considering the safety of the agents. ", "page_idx": 8}, {"type": "image", "img_path": "UuiZEOVtHx/tmp/e7e8ff5121bcc5c28c7a09d84301431ebef82fdfb97503800242c232d6bd1363.jpg", "img_caption": ["Figure 2: Reading order: (a) target demonstrations in yellow, wall in white; (b) result for log-density without considering the safety constraint; (c) target demonstrations that all states in top-right corners have cost; (d) result for log-density with safety constraint. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Our results are presented in Figure 2. In Figure (2b), it is demonstrated that our policy can accurately replicate the expert distribution using function approximation and completely bridge the gap required by the expert. Additionally, when taking into account the safety constraints, which involve setting the cost in the entire top-right corners, it is evident that our policy avoids states with cost and behaves appropriately, aligning with our theoretical framework. ", "page_idx": 8}, {"type": "text", "text": "5.2 Offline CMDP ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We consider an 8x8 grid world environment FrozenLake, with the initial state being the top-left grid. The agent has four actions: N (north), S (south), E (east), and W (west). The primary objective is to reach the goal while avoiding all holes. The game terminates if the agent achieves the goal within 25 steps. A reward of 1 is obtained when the agent achieves the goal, and the main cost function assigns a cost of 1 if the agent falls into a hole and 0 otherwise. The diagram of the environment has been presented in Figure 1 in the introduction. ", "page_idx": 8}, {"type": "text", "text": "Initially, we simulate a mixture of different percentages of optimal and uniform policies to collect the offline dataset. We employ various behavior policies $\\pi_{\\mathcal{D}}$ , running 200 trajectories to collect the offilne dataset $\\mathcal{D}$ , with each trajectory having a maximum of 50 time steps to ensure that the optimal goal is included in the dataset. ", "page_idx": 8}, {"type": "text", "text": "Additionally, we increase the difficulty of the environment compared to classical FrozenLake, such that if the agent falls into a hole, it can also come out in the next step. This implies that our cost constraint influences the training. We set the cost threshold as 0 here, which means that the agent is not supposed to incur any cost. Note that a higher percentage with a uniform policy indicates that the problem becomes more difficult. We set the discount factor as $\\gamma=0.99$ , $\\zeta=0.1$ , and $\\kappa$ in our algorithm. Furthermore, we encode the environment with one-hot encoding and employ a more practical algorithm. We refer to $w$ as a single hidden-layer neural network which we describe in Algorithm 2 in Appendix B. We set the learning rate of $10^{-5}$ for $w$ and $10^{-4}$ for Lagrange multipliers. ", "page_idx": 8}, {"type": "text", "text": "In remark (), we have stated that there are two approaches when facing with the scenario that the behavior policy is unknown, and here we choose to use the behavior clone method that we will estimate the behavior policy through the offilne dataset. We compare our algorithm with COptiDICE [Lee et al., 2021], which is a well-acknowledged baseline algorithm in the offline safe RL literature [Hong et al., 2024, Liu et al., 2023b]. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "UuiZEOVtHx/tmp/cc99766fa5f24dea132d06cb596e0b7c29865d88fe2439d937028fa7904ca27c.jpg", "img_caption": ["Figure 3: Performance on FrozenLake with general function approximation. Reading order: (a) and (b) show the training result with four different behavior policies of COptiDICE and ours. (c) and (d) demonstrate the variations in rewards and costs as the dataset increases. Each point is the average result of 10 independent runs. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Denote $p$ as the percentage of optimal policy within the behavior policy. We evaluate the algorithm with different behavior policies $p=\\{0.75,0.5,0.25,0\\}$ and dataset sizes. We present the results in Figure 3. In the first two figures, it can be seen that our algorithm can consistently find the optimal path even with completely random data. Conversely, COptiDICE behaves well when the proportion of optimal policy is 0.75, but cannot even learn a logical and safe policy when the majority of the data in dataset is random and constraint violated. In the last two figures, we test the performance of algorithms in different sizes of dataset, where $p=0.5$ in behavior policy. The results demonstrate that our algorithm can find a safe and optimal path as the dataset size increases. In contrast, the results from the COptiDICE algorithm show high variance, where it can only find a safe path in about $50\\%$ runs. ", "page_idx": 9}, {"type": "text", "text": "In summary, our algorithm performs well across various behavior policies and dataset sizes, which is consistent with our theoretical results and assumptions. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we consider convex CMDPs in the offline setting. We propose a sample efficient RL approach that addresses the challenges in offline convex CMDPs. We theoretically prove that we can suffer $\\mathcal{O}(1/\\sqrt{n})$ sample complexity in both performance and violation bound with general function approximation under mild data coverage assumption, which is the first result in offline convex MDPs as best of our knowledge and surpasses the state-of-the-art result in offilne safe RL by a factor of $1/(1-\\gamma)$ . Experimental studies further demonstrate the effectiveness and generality of our framework. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, page 1, 2004. ", "page_idx": 10}, {"type": "text", "text": "David Abel, Will Dabney, Anna Harutyunyan, Mark K Ho, Michael Littman, Doina Precup, and Satinder Singh. On the expressivity of markov reward. Advances in Neural Information Processing Systems, 34:7799\u20137812, 2021. ", "page_idx": 10}, {"type": "text", "text": "Eitan Altman. Constrained Markov decision processes. Routledge, 2021.   \nQinbo Bai, Amrit Singh Bedi, Mridul Agarwal, Alec Koppel, and Vaneet Aggarwal. Achieving zero constraint violation for concave utility constrained reinforcement learning via primal-dual approach. Journal of Artificial Intelligence Research, 78:975\u20131016, 2023.   \nStav Belogolovsky, Philip Korsunsky, Shie Mannor, Chen Tessler, and Tom Zahavy. Inverse reinforcement learning in contextual mdps. Machine Learning, 110(9):2295\u20132334, 2021.   \nDimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48(3): 334\u2013334, 1997.   \nFan Chen, Junyu Zhang, and Zaiwen Wen. A near-optimal primal-dual method for off-policy learning in cmdp. Advances in Neural Information Processing Systems, 35:10521\u201310532, 2022.   \nJinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In International Conference on Machine Learning, pages 1042\u20131051. PMLR, 2019.   \nJinglin Chen and Nan Jiang. Offilne reinforcement learning under value and density-ratio realizability: the power of gaps. In Uncertainty in Artificial Intelligence, pages 378\u2013388. PMLR, 2022.   \nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International conference on machine learning, pages 2052\u20132062. PMLR, 2019.   \nMatthieu Geist, Julien P\u00e9rolat, Mathieu Lauri\u00e8re, Romuald Elie, Sarah Perrin, Oliver Bachem, R\u00e9mi Munos, and Olivier Pietquin. Concave utility reinforcement learning: The mean-field game viewpoint. In Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems, pages 489\u2013497, 2022.   \nElad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efficient maximum entropy exploration. In International Conference on Machine Learning, pages 2681\u20132691. PMLR, 2019.   \nJean-Baptiste Hiriart-Urruty and Claude Lemar\u00e9chal. Convex analysis and minimization algorithms I: Fundamentals, volume 305. Springer science & business media, 1996.   \nKihyuk Hong, Yuhang Li, and Ambuj Tewari. A primal-dual-critic algorithm for offline constrained reinforcement learning. In International Conference on Artificial Intelligence and Statistics, pages 280\u2013288. PMLR, 2024.   \nB Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yogamani, and Patrick P\u00e9rez. Deep reinforcement learning for autonomous driving: A survey. IEEE Transactions on Intelligent Transportation Systems, 23(6):4909\u20134926, 2021.   \nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169, 2021.   \nHoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In International Conference on Machine Learning, pages 3703\u20133712. PMLR, 2019.   \nJongmin Lee, Cosmin Paduraru, Daniel J Mankowitz, Nicolas Heess, Doina Precup, Kee-Eung Kim, and Arthur Guez. Coptidice: Offilne constrained reinforcement learning via stationary distribution correction estimation. In International Conference on Learning Representations, 2021.   \nSergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. Journal of Machine Learning Research, 17(39):1\u201340, 2016.   \nPeng Liao, Zhengling Qi, Runzhe Wan, Predrag Klasnja, and Susan A Murphy. Batch policy learning in average reward markov decision processes. Annals of statistics, 50(6):3364, 2022.   \nBoyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural trust region/proximal policy optimization attains globally optimal policy. Advances in neural information processing systems, 32, 2019.   \nZuxin Liu, Zijian Guo, Haohong Lin, Yihang Yao, Jiacheng Zhu, Zhepeng Cen, Hanjiang Hu, Wenhao Yu, Tingnan Zhang, Jie Tan, et al. Datasets and benchmarks for offilne safe reinforcement learning. arXiv preprint arXiv:2306.09303, 2023a.   \nZuxin Liu, Zijian Guo, Yihang Yao, Zhepeng Cen, Wenhao Yu, Tingnan Zhang, and Ding Zhao. Constrained decision transformer for offline safe reinforcement learning. In International Conference on Machine Learning, pages 21611\u201321630. PMLR, 2023b.   \nR\u00e9mi Munos and Csaba Szepesv\u00e1ri. Finite-time bounds for ftited value iteration. Journal of Machine Learning Research, 9(5), 2008.   \nMirco Mutti, Riccardo De Santi, Piersilvio De Bartolomeis, and Marcello Restelli. Convex reinforcement learning in finite trials. Journal of Machine Learning Research, 24(250):1\u201342, 2023.   \nAngelia Nedi\u00b4c and Asuman Ozdaglar. Subgradient methods for saddle-point problems. Journal of optimization theory and applications, 142:205\u2013228, 2009.   \nAsuman E Ozdaglar, Sarath Pattathil, Jiawei Zhang, and Kaiqing Zhang. Revisiting the linearprogramming framework for offline rl with general function approximation. In International Conference on Machine Learning, pages 26769\u201326791. PMLR, 2023.   \nTom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna, Alexander Novikov, Ziyu Wang, and Nando de Freitas. Hyperparameter selection for offline reinforcement learning. arXiv preprint arXiv:2007.09055, 2020.   \nParia Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. In Advances Neural Information Processing Systems (NeurIPS), volume 34, pages 11702\u201311716, 2021.   \nMasatoshi Uehara and Wen Sun. Pessimistic model-based offilne reinforcement learning under partial coverage. arXiv preprint arXiv:2107.06226, 2021.   \nLingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global optimality and rates of convergence. arXiv preprint arXiv:1909.01150, 2019.   \nTengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. In Int. Conf. Machine Learning (ICML), pages 11404\u201311413. PMLR, 2021.   \nTengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for offline reinforcement learning. Advances in neural information processing systems, 34:6683\u20136694, 2021.   \nHaoran Xu, Xianyuan Zhan, and Xiangyu Zhu. Constraints penalized $\\mathbf{q}_{\\mathbf{\\lambda}}$ -learning for safe offilne reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8753\u20138760, 2022.   \nDonghao Ying, Mengzi Amy Guo, Yuhao Ding, Javad Lavaei, and Zuo-Jun Shen. Policy-based primal-dual methods for convex constrained markov decision processes. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 10963\u201310971, 2023.   \nChao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin. Reinforcement learning in healthcare: A survey. ACM Computing Surveys (CSUR), 55(1):1\u201336, 2021.   \nHao Yu and Michael J Neely. A simple parallel algorithm with an $\\mathrm{o}(1/\\mathrm{t})$ convergence rate for general convex programs. SIAM Journal on Optimization, 27(2):759\u2013783, 2017.   \nTom Zahavy, Brendan O\u2019Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough for convex mdps. Advances in Neural Information Processing Systems, 34:25746\u201325759, 2021.   \nWenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee. Offline reinforcement learning with realizability and single-policy concentrability. In Conference on Learning Theory, pages 2730\u20132775. PMLR, 2022.   \nJunyu Zhang, Amrit Singh Bedi, Mengdi Wang, and Alec Koppel. Cautious reinforcement learning via distributional risk in the dual domain. arXiv preprint arXiv:2002.12475, 2020a.   \nJunyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang. Variational policy gradient method for reinforcement learning with general utilities. Advances in Neural Information Processing Systems, 33:4572\u20134583, 2020b.   \nYinan Zheng, Jianxiong Li, Dongjie Yu, Yujie Yang, Shengbo Eben Li, Xianyuan Zhan, and Jingjing Liu. Feasibility-guided safe offilne reinforcement learning. In The Twelfth International Conference on Learning Representations, 2023.   \nWeichao Zhou and Wenchao Li. Safety-aware apprenticeship learning. In Computer Aided Verification: 30th International Conference, CAV 2018, Held as Part of the Federated Logic Conference, FloC 2018, Oxford, UK, July 14-17, 2018, Proceedings, Part I 30, pages 662\u2013680. Springer, 2018.   \nHanlin Zhu, Paria Rashidinejad, and Jiantao Jiao. Importance weighted actor-critic for optimal conservative offline reinforcement learning. arXiv preprint arXiv:2301.12714, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we give the complete proof of Theorem 1. Specifically, we bound the regret first and constraint violation second. Hence, we first present some auxiliary lemmas and combine them to prove the final theorem. ", "page_idx": 13}, {"type": "text", "text": "Proof of objective bound ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We also decompose the expression by adding and subtracting corresponding terms and prove them individually. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{J_{r}(\\overline{{\\pi}}_{K})-J_{r}(\\pi^{*})=f(d_{\\overline{{\\pi}}_{K}})-f(d^{*})}\\\\ {=f(d_{\\overline{{\\pi}}_{K}})-f(\\overline{{d}}_{K})+f(\\overline{{d}}_{K})-f(d^{*})}\\\\ {=f(d_{\\overline{{\\pi}}_{K}})-f(\\overline{{d}}_{K})+f(\\mu\\cdot\\overline{{w}}_{K})-f(\\mu\\cdot w^{*})}\\\\ {=\\underbrace{f(d_{\\overline{{\\pi}}_{K}})-f(\\overline{{d}}_{K})}_{\\mathrm{I}}+\\underbrace{f(\\mu\\cdot\\overline{{w}}_{K})-f(\\mu_{\\mathcal{D}}\\cdot\\overline{{w}}_{K})}_{\\mathrm{II}}+\\underbrace{f(\\mu_{\\mathcal{D}}\\cdot\\overline{{w}}_{K})-f(\\mu\\cdot w^{*})}_{\\mathrm{III}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Next, we will prove these items individually. ", "page_idx": 13}, {"type": "text", "text": "Lemma 4. $\\forall w^{*}\\in\\mathcal{W}$ , $w^{*}$ satisfies constraint functions (6) and (7) with at least $1-2\\delta$ probability. ", "page_idx": 13}, {"type": "text", "text": "Proof. First, we focus on the safety constraint (7). We prove it with Hoeffding inequality. Recall the Hoeffding inequality: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\bar{X}-\\mathbb{E}[X]\\right|\\geq t\\right)\\leq2\\exp\\left(-\\frac{2n^{2}t^{2}}{\\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Take $\\mu_{D}\\cdot w$ as the random variable, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}[(\\mu_{\\mathcal{D}}-\\mu)\\cdot w]=0\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $\\mu(s,a)\\in[0,1]$ , and meanwhile $w$ lies in $[-B_{w},B_{w}]$ , we have $(\\mu_{D}-\\mu)\\cdot w$ lies in interval $[-B_{w},\\dot{B}_{w}]$ . To satisfy the safety constraint function: ", "page_idx": 13}, {"type": "equation", "text": "$$\ng(\\mu_{\\mathscr D}\\cdot w)-\\tau\\leq\\kappa\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We set t = 2\u221aBw $\\begin{array}{r}{t=\\frac{\\sqrt{2}B_{w}}{\\sqrt{n}}\\sqrt{\\log\\frac{2|\\mathcal{W}|}{\\delta}}}\\end{array}$ 2|\u03b4W|. Combine with the Hoeffding inequality, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|\\mu_{\\mathcal{D}}\\cdot w-\\mu\\cdot w|\\geq t\\right)\\leq\\frac{\\delta}{|\\mathcal{W}|}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For the constraint function, recall we suppose that it satisfies the Lipschitz condition, ", "page_idx": 13}, {"type": "equation", "text": "$$\n|g(\\mu_{\\mathcal{D}}\\cdot w)-g(\\mu\\cdot w)|\\leq L_{g}||\\mu_{\\mathcal{D}}\\cdot w-\\mu\\cdot w||\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Before proceeding to the following derivation, we first state a fact: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(z\\geq t\\right)\\leq\\delta\\quad{\\mathrm{~and~}}\\quad y\\leq z\\quad\\Longrightarrow\\quad\\mathbb{P}\\left(y\\geq t\\right)\\leq\\delta\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The above equation means that if a variable $\\mathbf{z}$ is greater than $t$ with probability less than or equal to $\\delta$ , then any variable such as $\\mathbf{y}$ that is lower than that variable must also be lower than $t$ with probability less than or equal to $\\delta$ . ", "page_idx": 13}, {"type": "text", "text": "We get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|g(\\mu_{\\mathcal{D}}\\cdot w)-g(\\mu\\cdot w)|\\geq L_{g}\\cdot t\\right)\\leq\\frac{\\delta}{|\\mathcal{W}|}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note the fact that $(\\mu_{\\mathscr D}\\cdot w-\\mu\\cdot w)$ is obviously lower than $|\\mu_{\\mathcal \u1e0a D \u1e0c }\\cdot w-\\mu\\cdot w|$ , then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(g(\\mu_{\\mathcal{D}}\\cdot w)-g(\\mu\\cdot w)\\geq L_{g}\\cdot t\\right)\\leq\\frac{\\delta}{|\\mathcal{W}|}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Take the union bound for $w\\in\\mathscr{W}$ , then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall w\\in\\mathcal{W},\\quad\\mathbb{P}\\left(g(\\mu_{\\mathcal{D}}\\cdot w)-g(\\mu\\cdot w)\\geq L_{g}\\cdot t\\right)\\leq\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then for all $w^{*}\\in\\mathcal{W}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(g(\\mu_{\\mathcal{D}}\\cdot w^{*})-g(\\mu\\cdot w^{*})\\geq L_{g}\\cdot t\\right)\\leq\\delta\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Because $w^{*}$ is the optimal solution for problem (5)\u2013(7) and we suppose the assumption of realizability is set up, then in equation (7), $w^{*}$ satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\ng(\\mu\\cdot w^{*})\\leq\\tau\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combing with equation (19), we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(g(\\mu_{\\mathcal{D}}\\cdot w^{*})-\\tau\\geq L_{g}\\cdot t\\right)\\leq\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "So we have, with at least $1-\\delta$ probability, ", "page_idx": 14}, {"type": "equation", "text": "$$\ng(\\mu_{\\mathcal{D}}\\cdot w^{*})-\\tau\\leq L_{g}\\cdot t=\\kappa\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Next, we focus on the validity constraint function (6). ", "page_idx": 14}, {"type": "text", "text": "Here, we take $x^{\\top}(K_{D}-K)w$ as the random variable and note the fact that it lies in the interval $[-2B_{w},\\,2B_{w}]$ . Use the Hoeffding inequality we have that $\\forall x\\in B$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(x^{\\top}(K_{\\mathcal{D}}-K)w\\geq t)\\,\\leq\\,\\exp\\left(-\\frac{2n^{2}t^{2}}{\\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}\\right)}\\\\ &{\\,=\\,\\exp\\left(\\frac{-n t^{2}}{8B_{w}^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Set $\\begin{array}{r}{t=\\frac{2B_{w}\\sqrt{2\\log(|\\mathcal{W}||\\mathcal{X}|/\\delta)}}{\\sqrt{n}}}\\end{array}$ , the inequality becomes the form as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(x^{\\top}(K_{\\mathcal{D}}-K)w\\geq t\\right)\\leq\\frac{\\delta}{|\\mathcal{W}||\\chi|}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Take the union bound for all $w$ and $x$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(x^{\\top}(K_{\\mathcal{D}}-K)w\\geq t\\right)\\leq\\delta~,~~~\\forall w\\in\\mathcal{W},~\\forall x\\in\\mathcal{X}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "So for all $w^{*}\\in\\mathcal{W}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(x^{\\top}(K_{\\mathcal{D}}-K)w^{*}\\geq t\\right)\\leq\\delta\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note the fact $\\|K w^{*}-(1-\\gamma)\\mu_{0}\\|=0$ , we have the inequality for all $w^{*}\\in\\mathcal{W}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(x^{\\top}\\left(K_{\\mathcal{D}}w^{*}-\\left(1-\\gamma\\right)\\mu_{0}\\right)\\geq t\\right)\\leq\\delta\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "That is, with at least $1-\\delta$ probability, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\boldsymbol{x}^{\\top}\\left(K_{\\mathcal{D}}\\boldsymbol{w}^{*}-\\left(1-\\gamma\\right)\\mu_{0}\\right)\\le t=\\zeta\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Take the union bound for equation (23) and (29) completes the proof. ", "page_idx": 14}, {"type": "text", "text": "Next, we show that $f\\left(\\mu_{\\mathscr D}\\cdot w_{\\mathscr D}\\right)$ is close to $f\\left(\\boldsymbol{\\mu}\\cdot\\boldsymbol{w}^{*}\\right)$ . ", "page_idx": 14}, {"type": "text", "text": "Then the empirical covering number $n(\\epsilon,\\mathcal{W})$ is defined as the size of the smallest $\\epsilon$ -cover. Intuitively, the $\\epsilon$ cover set can represents the original set in the sense of $\\epsilon$ . This definition is very useful when the original set is continuous and we want to take the union bound of it. By this $\\epsilon$ cover we can take the union bound of the continuous set and the distance of them is measured by $\\epsilon$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma 5. We have ", "page_idx": 14}, {"type": "equation", "text": "$$\nf\\left(\\mu_{\\mathcal{D}}\\cdot w_{\\mathcal{D}}\\right)\\leq f\\left(\\mu\\cdot w^{*}\\right)+\\frac{\\sqrt{2}L_{f}B_{w}}{\\sqrt{n}}\\sqrt{\\log\\frac{2|\\mathcal{W}|}{\\delta}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with $1-3\\delta$ probability. ", "page_idx": 14}, {"type": "text", "text": "Proof. From Lemma 4, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nf\\left(\\mu_{\\mathcal{D}}\\cdot w_{\\mathcal{D}}\\right)\\leq f\\left(\\mu_{\\mathcal{D}}\\cdot w^{*}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with probability at least $1-2\\delta$ . Now, we use Hoeffding inequality to bound $f\\left(\\mu_{\\mathcal{D}}\\cdot w\\right)-f\\left(\\mu\\cdot w\\right)$ . Take $(\\mu_{D}-\\mu)\\cdot w$ as the random variable and note it lies in interval $[-B_{w},\\,B_{w}]$ , then we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\vert\\left(\\mu_{\\mathcal{D}}-\\mu\\right)\\cdot w\\vert\\ge t\\right)\\,\\le2\\exp\\left(-\\frac{2n^{2}t^{2}}{\\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}\\right)}\\\\ &{=2\\exp\\left(\\frac{-n t^{2}}{2B_{w}^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Set $\\begin{array}{r}{t=\\frac{B_{w}\\sqrt{2\\log(2|\\mathcal{W}|/\\delta)}}{\\sqrt{n}}}\\end{array}$ , the inequality becomes ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mid\\left(\\mu_{\\mathcal{D}}-\\mu\\right)\\cdot w\\right|\\geq t\\right)\\leq\\frac{\\delta}{|\\mathcal{W}|}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Use the Lipschitz condition and union bound of $w$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall w\\in\\mathcal{W}\\,,\\quad\\mathbb{P}\\left(f\\left(\\mu_{\\mathcal{D}}\\cdot w\\right)-f\\left(\\mu\\cdot w\\right)\\geq L_{f}\\cdot t\\right)\\leq\\delta\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Choose $w^{*}\\in\\mathcal{W}$ and combine with the inequality $f\\left(\\mu_{\\mathcal{D}}\\cdot w_{\\mathcal{D}}\\right)\\leq f\\left(\\mu_{\\mathcal{D}}\\cdot w^{*}\\right)$ , we get the final result ", "page_idx": 15}, {"type": "equation", "text": "$$\nf\\left(\\mu_{\\mathcal{D}}\\cdot w_{\\mathcal{D}}\\right)\\leq f\\left(\\mu\\cdot w^{*}\\right)+\\frac{\\sqrt{2}L_{f}B_{w}}{\\sqrt{n}}\\sqrt{\\log\\frac{2|\\mathcal{W}|}{\\delta}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with at least $1-3\\delta$ probability. ", "page_idx": 15}, {"type": "text", "text": "Next, we prove term $\\mathrm{II}$ that $f\\left(\\mu\\cdot\\overline{{w}}_{K}\\right)$ is close to $f\\left(\\mu_{D}\\cdot\\overline{{w}}_{K}\\right)$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 6. We have ", "page_idx": 15}, {"type": "equation", "text": "$$\nf\\left(\\mu\\cdot\\overline{{w}}_{K}\\right)\\leq f\\left(\\mu_{\\mathcal{D}}\\cdot\\overline{{w}}_{K}\\right)+\\frac{\\sqrt{2}L_{f}B_{w}}{\\sqrt{n}}\\sqrt{\\log\\frac{2|\\mathcal{W}|}{\\delta}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with probability $1-\\delta$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. This is easy to prove as we have a general result for all $w\\in\\mathscr{W}$ in equation (31). Here we take $\\overline{{w}}_{K}\\in\\mathcal{W}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(f\\left(\\mu\\cdot\\overline{{\\boldsymbol{w}}}_{K}\\right)-f\\left(\\mu_{\\mathcal{D}}\\cdot\\overline{{\\boldsymbol{w}}}_{K}\\right)\\geq t\\right)\\leq\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which completes the proof. ", "page_idx": 15}, {"type": "text", "text": "Note that $\\overline{{w}}_{K}$ violates the validity constraint in problem (5). This means that the calculated result may not satisfy the quality of the occupancy measure. To find the relation, we utilize the analogous lemmas in [Ozdaglar et al., 2023] to get the error bound between validity constraint violation and absolute objective difference. ", "page_idx": 15}, {"type": "text", "text": "Lemma 7. Let $\\overline{{d}}_{K}$ be a variable solved by Program (8) that violates the validity constraint (6) and $\\overline{{\\pi}}_{K}$ is the policy induced by $\\overline{{d}}_{K}$ . We can get that ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(d_{\\overline{{\\pi}}_{K}})\\leq f(\\overline{{d}}_{K})+\\frac{2B_{w}L_{f}\\sqrt{2\\log(|\\mathcal{W}||\\mathcal{X}|/\\delta)}}{(1-\\gamma)\\sqrt{n}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with at least $1-\\delta$ probability. ", "page_idx": 15}, {"type": "text", "text": "Proof. Define the marginalized occupancy measure as $\\begin{array}{r}{\\hat{d}_{K}(s)\\,=\\,\\sum_{a\\in\\mathcal{A}}\\overline{{d}}_{K}(s,a)}\\end{array}$ and $\\hat{d}_{\\overline{{\\pi}}_{K}}(s)\\;=\\;$ $\\textstyle\\sum_{a\\in{\\cal A}}d_{\\overline{{\\pi}}_{K}}(s,a)$ . Then, we can write ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{d}_{K}(s)=\\sum_{a\\in\\mathcal{A}}\\overline{{d}}_{K}(s,a)\\qquad\\mathrm{and}\\qquad\\hat{d}_{\\overline{{\\pi}}_{K}}(s)=\\sum_{a\\in\\mathcal{A}}d_{\\overline{{\\pi}}_{K}}(s,a).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $P_{\\pi}^{|S||A|}\\in\\mathbb{R}$ be the state transition matrix, i.e., ", "page_idx": 16}, {"type": "equation", "text": "$$\nP_{\\pi_{\\mathcal{D}}}(j,i)=\\sum_{a\\in A}P_{s^{i},a}(s^{j})\\cdot\\pi(a\\mid s^{i}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Also, we define the matrix $G_{\\pi}=\\mathrm{Diag}(\\pi_{d}(\\cdot\\ |\\ s^{1}),\\pi_{d}(\\cdot\\ |\\ s^{2}),\\cdot\\cdot\\cdot\\ ,\\pi_{d}(\\cdot\\ |\\ s^{|\\mathcal{S}|}))\\in\\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|\\times|\\mathcal{S}|}$ , and notice the fact that $M G_{\\pi}=I-\\gamma P_{\\pi}$ . Now, since $d_{\\pi}$ satisfies the constraints in Problem, we have $M d_{\\pi}=(1-\\gamma)\\mu_{0}$ . This implies: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|M\\overline{{d}}_{K}-(1-\\gamma)\\mu_{0}\\|_{1}=\\|M(\\overline{{d}}_{K}-d_{\\overline{{\\pi}}_{K}})\\|_{1}}&{}\\\\ {=\\|M G_{\\pi}(\\widehat{d}_{K}-\\widehat{d}_{\\overline{{\\pi}}_{K}})\\|_{1}}&{}\\\\ {=\\|(I-\\gamma P_{\\pi})(\\widehat{d}_{K}-\\widehat{d}_{\\overline{{\\pi}}_{K}})\\|_{1}}&{}\\\\ {\\geq(1-\\gamma)\\|\\widehat{d}_{K}-\\widehat{d}_{\\overline{{\\pi}}_{K}}\\|_{1}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here the last inequality is because $\\gamma\\|P_{\\pi}(\\hat{d}_{K}-\\hat{d}_{\\overline{{\\pi}}_{K}})\\|_{1}\\leq\\gamma\\|\\hat{d}_{K}-\\hat{d}_{\\overline{{\\pi}}_{K}}\\|_{1}.$ , which follows from the fact that $P_{\\pi}$ is a column stochastic matrix. ", "page_idx": 16}, {"type": "text", "text": "We have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|f(\\overline{{d}}_{K})-f(d_{\\overline{{\\pi}}_{K}})|\\leq L_{f}|\\overline{{d}}_{K}-d_{\\overline{{\\pi}}_{K}}|}\\\\ &{\\qquad\\qquad\\qquad=L_{f}|G_{\\pi}(\\widehat{d}_{K}-\\widehat{d}_{\\overline{{\\pi}}_{K}})|}\\\\ &{\\qquad\\qquad\\qquad\\leq L_{f}\\|\\widehat{d}_{K}-\\widehat{d}_{\\overline{{\\pi}}_{K}}\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The first inequality holds because of the Lipschitz condition of function $f$ . So combining the above two inequalities, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f\\left(d_{\\overline{{\\pi}}_{K}}\\right)-f\\left(\\overline{{d}}_{K}\\right)\\leq\\frac{L_{f}\\|M\\overline{{d}}_{K}-(1-\\gamma)\\mu_{0}\\|_{1}}{1-\\gamma}}\\\\ {=\\frac{L_{f}\\|K\\overline{{w}}_{K}-(1-\\gamma)\\mu_{0}\\|_{1}}{1-\\gamma}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which follows the definition of density ratio $w$ . Next, we give the bound of $\\|K\\overline{{w}}_{K}-(1-\\gamma)\\mu_{0}\\|_{1}$ . From inequality (26) above we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(x^{\\top}(K_{\\mathcal{D}}-K)w\\ge\\zeta\\right)\\le\\delta~,~~~\\forall w\\in\\mathcal{W},~\\forall x\\in\\mathcal{X}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then take $w$ as $\\overline{{w}}_{K}$ and $x$ as 1-norm, with at least $1-\\delta$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r}{\\|K\\overline{{w}}_{K}-(1-\\gamma)\\mu_{0}\\|_{1}\\leq\\|K_{\\mathcal{D}}\\overline{{w}}_{K}-(1-\\gamma)\\mu_{0}\\|_{1}+\\|(K-K_{\\mathcal{D}})\\overline{{w}}_{K}\\|_{1}}\\\\ {\\leq\\zeta+\\zeta\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first inequality is because of the triangle inequality and the last inequality follows the fact that $\\overline{{w}}_{K}$ is the optimal solution to problem (8) and the above inequality. Combining (40) and (38) completes the proof. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Now, we have the bound of term I and II. To further demonstrate the final bound, we also need to investigate the distance between the returned value $\\overline{{w}}_{K}$ by primal-dual method and optimal solution $w_{\\mathscr D}$ . ", "page_idx": 16}, {"type": "text", "text": "Our algorithm 1 is the classical type of primal-dual subgradient method and our analysis mostly refers to [Nedi\u00b4c and Ozdaglar, 2009]. We put it here for the sake of completeness and add the theoretical analysis of our algorithm. ", "page_idx": 16}, {"type": "text", "text": "First, recall the Lagrange function and updated rules of the algorithm ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}\\left(w,\\lambda,\\phi\\right)=f\\left(\\mu_{\\mathcal{D}}\\cdot w\\right)+\\lambda\\left(\\|K_{\\mathcal{D}}w-(1-\\gamma)\\right)\\mu_{0}\\|_{1}-\\zeta\\right)+\\phi\\left(g\\left(\\mu_{\\mathcal{D}}\\cdot w\\right)-\\tau-\\kappa\\right)}\\\\ &{\\qquad w^{k+1}=\\mathcal{P}_{\\mathcal{W}}\\left[w^{k}-\\eta\\nabla\\mathcal{L}_{w}(w^{k},\\lambda^{k},\\phi^{k})\\right]}\\\\ &{\\qquad\\lambda^{k+1}=\\left[\\lambda^{k}-\\eta\\nabla\\mathcal{L}_{\\lambda}(w^{k},\\lambda^{k},\\phi^{k})\\right]_{0}^{\\lambda_{\\operatorname*{max}}^{k+1}}}\\\\ &{\\qquad\\phi^{k+1}=\\left[\\phi^{k}-\\eta\\nabla\\mathcal{L}_{\\phi}(w^{k},\\lambda^{k},\\phi^{k})\\right]_{0}^{\\phi_{m a x}^{k+1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For simplicity, we combine the Lagrange multipliers into one variable and call it $\\nu$ , accordingly, we set the constraint $g\\left(w\\right)\\leq0$ . Note that constraints are analogous and we would split them in the end. Now the updated rules become ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w^{k+1}=\\mathcal{P}_{\\mathcal{W}}\\left[w^{k}-\\eta\\nabla\\mathcal{L}_{w}(w^{k},\\nu^{k})\\right]}\\\\ &{\\nu^{k+1}=\\mathcal{P}_{\\mathcal{M}}\\left[\\nu^{k}-\\eta\\nabla\\mathcal{L}_{\\nu}(w^{k},\\nu^{k})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\nu^{k}\\in\\mathcal{M}$ and note that we turn the interval of $\\nu$ into a projection set which is an equivalent transformation. The Lagrange function is denoted as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}\\left(w,\\nu\\right)=f\\left(w\\right)+\\nu g\\left(w\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Theorem 2 (Saddle-Point Theorem in [Bertsekas, 1997]). The pair $(w_{D},\\nu_{D})$ is a primal-dual optimal solution if and only $i f$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}\\left(w_{\\mathcal{D}},\\nu\\right)\\leq\\mathcal{L}\\left(w_{\\mathcal{D}},\\nu_{\\mathcal{D}}\\right)\\leq\\mathcal{L}\\left(w,\\nu_{\\mathcal{D}}\\right),\\ \\ \\,f o r\\ a l l\\ w\\in\\mathcal{W},\\ \\nu\\geq0,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $w_{D}\\in\\mathcal{W}$ and $\\nu_{D}\\geq0$ . ", "page_idx": 17}, {"type": "text", "text": "The analysis of convergence is based on this theorem and it is a standard result that characterizes the primal-dual optimal solutions as the saddle points of the Lagrange function[Nedic\u00b4 and Ozdaglar, 2009]. ", "page_idx": 17}, {"type": "text", "text": "Theorem 3. According to the updated rules, denote the solution to problem (8) as $\\overline{{w}}_{K}$ . Compared to the optimal solution $w_{\\mathscr D}$ , we have the upper bound for performance and constraint violation: ", "page_idx": 17}, {"type": "equation", "text": "$$\n(I)\\quad\\left\\|g\\left(\\overline{{\\boldsymbol{w}}}_{K}\\right)^{+}\\right\\|\\leq\\frac{2\\|\\boldsymbol{w}_{1}-\\boldsymbol{w}_{\\mathcal{D}}\\|}{K\\eta}+\\frac{2L}{\\sqrt{K}}+\\varepsilon\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\nf\\left(\\overline{{w}}_{K}\\right)\\leq f^{*}+\\frac{\\|w_{1}-w_{\\mathcal{D}}\\|^{2}}{2K\\eta}+\\eta L^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\eta$ is the constant learning rate, $w_{1}$ is an initial state, $\\varepsilon\\geq0$ , $K$ is the iteration step and $L$ is the max subgradient of the Lagrange function. Furthermore, we denote, $w^{*}=w_{D}$ , $f\\left(\\overline{{w}}_{K}\\right)=$ $f\\left(\\mu_{D}\\cdot{\\overline{{w}}}_{K}\\right)$ and $f^{*}=f\\left(\\boldsymbol{\\mu}\\boldsymbol{\\mathcal{D}}\\cdot\\boldsymbol{w}_{\\mathcal{D}}\\right)$ , we use the above notations for simplicity. ", "page_idx": 17}, {"type": "text", "text": "We first introduce some lemmas and prove the theorem step by step. ", "page_idx": 17}, {"type": "text", "text": "Lemma 8. Suppose that the sequence $\\{w_{k}\\}$ and $\\{\\nu_{k}\\}$ are generated by the updated rules. Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall w\\in\\mathcal{W},\\quad\\|w_{k+1}-w\\|^{2}\\leq\\|w_{k}-w\\|^{2}-2\\eta\\left(\\mathcal{L}\\left(w_{k},\\nu_{k}\\right)-\\mathcal{L}\\left(w,\\nu_{k}\\right)\\right)+\\eta^{2}\\|\\mathcal{L}_{w}\\left(w_{k},\\nu_{k}\\right)\\|^{2}}\\\\ &{\\forall\\nu\\in\\mathcal{M},\\quad\\|\\nu_{k+1}-\\nu\\|^{2}\\leq\\|\\nu_{k}-\\nu\\|^{2}-2\\eta\\left(\\mathcal{L}\\left(w_{k},\\nu_{k}\\right)-\\mathcal{L}\\left(w_{k},\\nu\\right)\\right)+\\eta^{2}\\|\\mathcal{L}_{\\nu}\\left(w_{k},\\nu_{k}\\right)\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. We prove (1) and (2) is similar to (1). ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|w_{k+1}-w\\|^{2}=\\|\\mathcal{P}\\left(w_{k}-\\eta\\mathcal{L}_{w}\\left(w_{k},\\nu_{k}\\right)\\right)-w\\|^{2}}&{}\\\\ {\\leq\\|w_{k}-\\eta\\mathcal{L}_{w}\\left(w_{k},\\nu_{k}\\right)-w\\|^{2}}&{}\\\\ {=\\|w_{k}-w\\|^{2}-2\\eta\\mathcal{L}_{w}\\left(w_{k},\\nu_{k}\\right)\\left(w_{k}-w\\right)+\\eta^{2}\\|\\mathcal{L}_{w}\\left(w_{k},\\nu_{k}\\right)\\|^{2}}&{}\\\\ {\\leq\\|w_{k}-w\\|^{2}-2\\eta\\left(\\mathcal{L}\\left(w_{k},\\nu_{k}\\right)-\\mathcal{L}\\left(w,\\nu_{k}\\right)\\right)+\\eta^{2}\\|\\mathcal{L}_{w}\\left(w_{k},\\nu_{k}\\right)\\|^{2}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first equality comes from the update rule and the last inequality is because of the convexity of the Lagrange function. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Assumption 6 (Boundness of subgradient). We assume that the subgradient of the Lagrange function is bounded, such as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathcal{L}_{w}\\left(w_{k},\\nu_{k}\\right)\\|\\leq L\\,,\\quad\\|\\mathcal{L}_{\\nu}\\left(w_{k},\\nu_{k}\\right)\\|\\leq L,\\quad\\forall k\\ge0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $L$ is the max Lipschitz constant in Lagrange function. ", "page_idx": 17}, {"type": "text", "text": "Lemma 9. Set $\\begin{array}{r}{\\overline{{w}}_{k}=\\frac{1}{k}\\sum_{i=1}^{k}w_{k_{i}}}\\end{array}$ and $\\begin{array}{r}{\\overline{{\\nu}}_{k}=\\frac{1}{k}\\sum_{i=1}^{k}\\nu_{k_{i}}}\\end{array}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n(I)\\quad\\frac{1}{k}\\sum_{i=1}^{k}\\mathcal{L}\\left(w_{i},\\boldsymbol{\\nu_{i}}\\right)-\\mathcal{L}\\left(w,\\boldsymbol{\\overline{{\\nu}}_{k}}\\right)\\leq\\frac{\\|w_{1}-w\\|^{2}}{2\\eta k}+\\frac{\\eta L^{2}}{2},\\quad\\forall w\\in\\mathcal{W}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n-\\left.\\frac{\\|\\nu_{1}-\\nu\\|^{2}}{2\\eta k}-\\frac{\\eta L^{2}}{2}\\leq\\frac{1}{k}\\sum_{i=1}^{k}\\mathcal{L}\\left(w_{i},\\nu_{i}\\right)-\\mathcal{L}\\left(\\overline{{w}}_{k},\\nu\\right),\\quad\\forall\\nu\\in\\mathcal{M}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. We only prove (1) and equation (2) is similar to (1). By using Lemma 8 we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}\\left(w_{i},\\nu_{i}\\right)-\\mathcal{L}\\left(w,\\nu_{i}\\right)\\leq\\frac{\\|w_{i}-w\\|^{2}-\\|w_{i+1}-w\\|^{2}}{2\\eta}+\\frac{\\eta L^{2}}{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Sum it from 1 to $k$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{k}\\sum_{i=1}^{k}\\mathcal{L}\\left(w_{i},\\nu_{i}\\right)-\\mathcal{L}\\left(w,\\nu_{i}\\right)\\leq\\frac{\\|w_{1}-w\\|^{2}-\\|w_{k}-w\\|^{2}}{2\\eta}+\\frac{\\eta L^{2}}{2}}}\\\\ &{\\leq\\frac{\\|w_{1}-w\\|^{2}}{2\\eta}+\\frac{\\eta L^{2}}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "And combine with $\\begin{array}{r}{\\frac{1}{k}\\sum_{i=1}^{k}\\mathcal{L}\\left(\\boldsymbol{w},\\boldsymbol{\\nu_{i}}\\right)\\geq\\mathcal{L}\\left(\\boldsymbol{w},\\overline{{\\boldsymbol{\\nu}}}_{k}\\right)}\\end{array}$ which is because of Jensen\u2019s inequality, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{k}\\sum_{i=1}^{k}\\mathcal{L}\\left(w_{i},\\boldsymbol{\\nu_{i}}\\right)-\\mathcal{L}\\left(w,\\overline{{\\boldsymbol{\\nu}}}_{k}\\right)\\leq\\frac{\\|\\boldsymbol{w}_{1}-\\boldsymbol{w}\\|^{2}}{2\\eta}+\\frac{\\eta L^{2}}{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Assumption 7 (Slater\u2019s condition). There exists a vector $\\hat{w}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\ng_{j}(\\hat{w})<0,\\quad\\forall j=1,2,\\cdot\\cdot\\cdot,N\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "we refer to w\u02c6 as a Slater\u2019s vector. ", "page_idx": 18}, {"type": "text", "text": "Note that Slater\u2019s condition is assumed in many primal-dual method literature [Chen et al., 2022, Hong et al., 2024, Nedic\u00b4 and Ozdaglar, 2009]. ", "page_idx": 18}, {"type": "text", "text": "Lemma 10. Under Assumption 7, suppose $q^{*}$ is the dual optimal solution and $\\bar{w}$ is a vector that satisfies Slater\u2019s condition, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\nu\\|_{1}\\leq\\frac{1}{\\gamma}\\left(f\\left(\\hat{w}\\right)-q^{*}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{\\gamma=\\operatorname*{min}_{1\\leq j\\leq m}\\{-g_{j}(\\hat{w})\\}}\\end{array}$ and $m$ is the number of constraints. ", "page_idx": 18}, {"type": "text", "text": "The proof is referred to [Hiriart-Urruty and Lemar\u00e9chal, 1996]. The lemma above motivates the choice of dual set, that is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{M}=\\left\\{\\nu\\geq0~\\vert~\\frac{f(\\hat{w})-\\tilde{q}}{\\gamma}+r\\right\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\tilde{q}\\leq q^{*}$ and $r\\geq0$ is a constant. ", "page_idx": 18}, {"type": "text", "text": "Proof. Now, we are ready to give the general proof of Theorem 3. Combine Lemma 8 and Assumption 6, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nu_{k+1}-\\nu\\|^{2}\\leq\\|\\nu_{k}-\\nu\\|^{2}-2\\eta\\left(\\mathcal{L}\\left(w_{k},\\nu_{k}\\right)-\\mathcal{L}\\left(w_{k},\\nu\\right)\\right)+\\eta^{2}\\|\\mathcal{L}_{\\nu}\\left(w_{k},\\nu_{k}\\right)\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\nu_{i}-\\nu_{\\mathcal{D}}\\right)\\mathcal{L}_{\\nu}\\left(w_{i},\\nu_{i}\\right)\\leq\\mathcal{L}\\left(w_{i},\\nu_{i}\\right)-\\mathcal{L}\\left(w_{i},\\nu_{\\mathcal{D}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathcal{L}\\left(w_{i},\\nu_{i}\\right)-f^{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the first equation is because of convexity of ${\\mathcal{L}}_{\\nu}$ and the last equation dues to the Slater\u2019s condition. We have, $\\forall\\nu\\in\\mathcal{M}$ and $i\\geq1$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\nu-\\nu_{D}\\right)^{\\top}\\mathcal{L}_{\\nu}\\left(w_{i},\\nu_{i}\\right)=\\left(\\nu-\\nu_{D}+\\nu_{i}-\\nu_{i}\\right)^{\\top}\\mathcal{L}_{\\nu}\\left(w_{i},\\nu_{i}\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\left(\\nu-\\nu_{i}\\right)^{\\top}\\mathcal{L}_{\\nu}\\left(w_{i},\\nu_{i}\\right)+\\left(\\nu_{i}-\\nu_{D}\\right)\\mathcal{L}_{\\nu}\\left(w_{i},\\nu_{i}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\left\\Vert\\nu_{i}-\\nu\\right\\Vert^{2}-\\left\\Vert\\nu_{i+1}-\\nu\\right\\Vert^{2}}{2\\eta}+\\frac{\\eta L^{2}}{2}+\\mathcal{L}\\left(w_{i},\\nu_{i}\\right)-f^{\\ast}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, sum over $i=1,\\cdots,k$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{k}\\left(\\nu-\\nu_{\\mathcal{D}}\\right)^{\\top}\\mathcal{L}\\left(w_{i},\\nu_{i}\\right)\\leq\\frac{\\|\\nu_{1}-\\nu\\|^{2}}{2\\eta}+\\frac{\\eta k L^{2}}{2}+\\sum_{i=0}^{k-1}\\mathcal{L}\\left(w_{i},\\nu_{i}\\right)-k f^{*}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Because the above function is for all $\\nu$ , then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\nu\\in{\\mathcal{M}}}\\left\\{\\sum_{i=1}^{k}\\left(\\nu-\\nu_{\\mathcal{D}}\\right)^{\\top}{\\mathcal{L}}\\left(w_{i},\\nu_{i}\\right)\\right\\}\\leq\\frac{1}{2\\eta}\\operatorname*{max}_{\\nu\\in{\\mathcal{M}}}\\|\\nu_{1}-\\nu\\|^{2}\\frac{\\eta k L^{2}}{2}+\\sum_{i=1}^{k}{\\mathcal{L}}\\left(w_{i},\\nu_{i}\\right)-k f^{*}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Set ", "page_idx": 19}, {"type": "equation", "text": "$$\ns=\\sum_{i=1}^{k}{\\mathcal{L}}_{\\nu}\\left(w_{i},\\nu_{i}\\right)=\\sum_{i=1}^{k}g\\left(w_{i}\\right)\\geq k g\\left({\\hat{w}}_{k}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, if $s^{+}=0$ , equation (52) holds. If $s^{+}\\neq0$ , define vector: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\nu}=\\nu_{\\mathscr{D}}+r\\frac{s^{+}}{\\|s^{+}\\|}\\geq0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combine Lemma 10, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\hat{\\nu}\\|\\leq\\|\\nu_{\\mathcal{D}}\\|+r\\leq\\frac{f\\left(\\hat{w}\\right)-\\tilde{q}}{\\gamma}+r\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which implies that for $\\hat{\\nu}\\in\\mathcal{M}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\hat{\\nu}-\\nu_{D}\\right)^{\\top}s=\\displaystyle\\sum_{i=1}^{k}\\left(\\hat{\\nu}-\\nu_{D}\\right)^{\\top}\\mathcal{L}\\left(w_{i},\\nu_{i}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\operatorname*{max}_{\\nu\\in\\mathcal{M}}\\left\\{\\displaystyle\\sum_{i=1}^{k}\\left(\\nu-\\nu_{D}\\right)^{\\top}\\mathcal{L}\\left(w_{i},\\nu_{i}\\right)\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recall the definition of $s$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{(\\dot{\\nu}-\\nu_{D})^{\\top}s=r\\|\\left[\\displaystyle\\sum_{i=0}^{k}g(w_{i})\\right]^{+}|}}\\\\ &{\\leq r\\operatorname*{max}\\left\\{\\displaystyle\\sum_{i=1}^{k}\\left(\\nu-\\nu_{D})^{\\top}\\mathcal{L}\\left(w_{i},\\nu_{i}\\right)\\right\\}}\\\\ &{\\leq\\displaystyle\\frac{1}{2\\eta\\,\\nu\\epsilon_{A}}\\Big\\{|\\nu_{1}-\\nu_{1}|^{2}+\\frac{\\eta k L^{2}}{2}+\\displaystyle\\sum_{i=1}^{k}\\mathcal{L}\\left(w_{i},\\nu_{i}\\right)-k f^{\\star}}\\\\ &{\\leq\\displaystyle\\frac{1}{2\\,\\eta\\,\\nu\\epsilon_{A}}(|\\nu_{1}|+||\\nu_{1}|)^{2}+\\frac{\\eta k L^{2}}{2}+\\frac{\\|w_{1}-w_{0}\\|^{2}}{2\\eta\\,k}+\\frac{\\eta L^{2}}{2}}\\\\ &{\\leq\\displaystyle\\frac{1}{2\\,\\eta\\,\\nu\\epsilon_{A}}\\|\\nu_{1}\\|+\\frac{\\eta k L^{2}}{2}+\\frac{\\|w_{1}-w_{0}\\|^{2}}{2\\,k}+\\frac{\\eta L^{2}}{2\\,\\eta\\,k}}\\\\ &{\\leq\\displaystyle\\frac{1}{2\\,\\eta\\,\\nu\\epsilon_{A}}\\|\\nu_{1}\\|+\\frac{\\eta k L^{2}}{2}+\\frac{\\|w_{1}-w_{0}\\|^{2}}{2\\,k}+\\frac{\\eta L^{2}}{2}}\\\\ &{\\leq\\displaystyle\\frac{1}{2\\,\\eta\\,\\nu}\\Big[\\frac{\\int(\\dot{\\nu})-\\tilde{\\eta}}{2}+r\\Big]+\\frac{\\eta L^{2}}{2\\,k}+\\frac{\\|w_{1}-w_{0}\\|^{2}}{2\\,\\eta\\,k}+\\frac{\\eta L^{2}}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|g\\left(\\overline{{\\boldsymbol{w}}}_{k}\\right)^{+}\\|\\leq\\frac{\\|\\boldsymbol{w}_{1}-\\boldsymbol{w}_{D}\\|^{2}}{2\\eta k}+\\frac{\\eta L^{2}}{2}+\\frac{2}{k\\eta r}\\left(\\frac{f\\left(\\hat{\\boldsymbol{w}}\\right)-\\tilde{\\boldsymbol{q}}}{\\gamma}+\\boldsymbol{r}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Until now, we assume the dual set is appropriately chosen in every iteration and we should quantify it. For the dual set ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{M}=\\left\\{\\nu\\geq0|\\|\\nu\\|\\leq\\frac{f\\left(\\hat{w}\\right)-\\tilde{q}}{\\gamma}+r\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and the result in (57), We should choose ", "page_idx": 20}, {"type": "equation", "text": "$$\nr=\\operatorname*{min}_{r\\ge0}\\left\\{\\frac{||w_{1}-w_{\\mathcal{D}}||^{2}}{2\\eta k}+\\frac{\\eta L^{2}}{2}+\\frac{2}{k\\eta r}\\left(\\frac{f\\left(\\hat{w}\\right)-\\tilde{q}}{\\gamma}+r\\right)^{2}\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is ", "page_idx": 20}, {"type": "equation", "text": "$$\nr^{*}\\left(k\\right)=\\sqrt{\\left(\\frac{f\\left(\\hat{w}\\right)-\\widetilde{q}}{\\gamma}\\right)^{2}+\\frac{\\|w_{1}-w_{\\mathcal{D}}\\|^{2}}{4}+\\frac{k\\eta^{2}L^{2}}{4}},\\quad\\forall k\\ge1.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "So the dual set in every iteration is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{M}_{k}=\\left\\{\\nu\\geq0|\\|\\nu\\|\\leq\\frac{f\\left(\\hat{w}\\right)-\\tilde{q}}{\\gamma}+r^{*}\\left(k\\right)\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which implies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|g\\left(\\overline{{w}}_{k}\\right)^{+}\\|\\le\\frac{4}{k\\eta}\\left(\\frac{f\\left(\\hat{w}\\right)-\\widetilde{q}}{\\gamma}+\\sqrt{\\left(\\frac{f\\left(\\hat{w}\\right)-\\widetilde{q}}{\\gamma}\\right)^{2}+\\frac{\\|w_{1}-w_{D}\\|^{2}}{4}+\\frac{k\\eta^{2}L^{2}}{4}}\\right)}}\\\\ &{}&{\\le\\frac{4}{\\eta k}\\left[\\frac{2\\left(f\\left(\\hat{w}\\right)-\\widetilde{q}\\right)}{\\gamma}+\\frac{\\|w_{1}-w_{D}\\|}{2}+\\frac{\\eta L\\sqrt{k}}{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For simplicity, we take the first term as a constant which doesn\u2019t affect our result: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|g\\left(\\overline{{w}}_{k}\\right)^{+}\\right\\|\\leq\\varepsilon+\\frac{2\\|w_{1}-w_{\\mathcal{D}}\\|}{k\\eta}+\\frac{2L}{\\sqrt{k}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The result of (1) in Theorem 3 completes. ", "page_idx": 20}, {"type": "text", "text": "Next, we give the bound of the objective value. From the definition of the Lagrange function, ", "page_idx": 20}, {"type": "equation", "text": "$$\nf\\left(\\overline{{\\boldsymbol{w}}}_{k}\\right)\\leq\\frac{1}{k}\\sum_{i=1}^{k}f\\left(\\boldsymbol{w}_{i}\\right)=\\frac{1}{k}\\sum_{i=0}^{k-1}\\mathcal{L}\\left(\\boldsymbol{w}_{i},\\boldsymbol{\\nu}_{i}\\right)-\\frac{1}{k}\\sum_{i=1}^{k}g\\left(\\boldsymbol{w}_{i}\\right)\\boldsymbol{\\nu}_{i}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{f\\left(\\overline{{w}}_{k}\\right)-f^{*}\\le\\frac{1}{k}\\sum_{i=1}^{k}\\mathcal{L}\\left(w_{i},\\nu_{i}\\right)-\\frac{1}{k}\\sum_{i=1}^{k}g\\left(w_{i}\\right)\\nu_{i}-f^{*}}}\\\\ &{}&{\\le\\frac{\\|w_{1}-w^{*}\\|^{2}}{2\\eta k}+\\frac{\\eta L^{2}}{2}-\\frac{1}{k}\\sum_{i=1}^{k}\\nu_{i}g\\left(w_{i}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n0\\in\\mathcal{M}=\\left\\{\\nu\\geq0|\\|\\nu\\|\\leq\\frac{f\\left(\\hat{w}\\right)-\\tilde{q}}{\\gamma}+r\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Set $\\nu=0$ and combine Lemma 8, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\nu_{k+1}\\|^{2}\\leq\\|\\nu_{k}\\|^{2}+2\\eta\\nu_{k}g\\left(w_{k}\\right)+\\eta^{2}L^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n2\\eta\\nu_{k}g\\left(w_{k}\\right)\\leq\\|\\nu_{k}\\|^{2}-\\|\\nu_{k+1}\\|^{2}+\\eta^{2}L^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Sum over $i=0,\\cdot\\cdot\\cdot,k-1$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n-\\sum_{i=1}^{k-1}\\nu_{i}g\\left(w_{i}\\right)\\leq\\frac{1}{2\\eta}\\left(\\|w_{1}\\|^{2}-\\|\\nu_{k}\\|^{2}\\right)+\\frac{\\eta^{2}L^{2}k}{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We get the final result, ", "page_idx": 21}, {"type": "equation", "text": "$$\nf\\left(\\overline{{\\boldsymbol{w}}}_{k}\\right)\\leq f^{*}+\\frac{\\|\\nu_{1}\\|^{2}}{2\\eta}+\\frac{\\|w_{1}-w_{\\overline{{D}}}\\|^{2}}{2\\eta k}+\\frac{\\eta^{2}L^{2}}{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which completes the proof of Theorem 3. ", "page_idx": 21}, {"type": "text", "text": "Then we have the all results to prove the regret bound in Theorem 1. ", "page_idx": 21}, {"type": "text", "text": "Theorem 3 indicates that after $K$ iterations the objective and violation bound for problem (8) are ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f\\left({\\mu_{D}}\\cdot\\overline{{{w_{K}}}}\\right)\\le f\\left({\\mu_{D}}\\cdot{w_{D}}\\right)+\\displaystyle\\frac{\\|{\\nu_{1}}\\|^{2}}{2\\eta}+\\displaystyle\\frac{\\|{w_{1}}-{w_{D}}\\|^{2}}{2\\eta K}+\\displaystyle\\frac{\\eta^{2}L^{2}}{2}}}\\\\ {{\\displaystyle=f\\left({\\mu_{D}}\\cdot{w_{D}}\\right)+\\displaystyle\\frac{B^{2}}{2\\eta K}+\\displaystyle\\frac{\\eta^{2}L^{2}}{2}}}\\\\ {{\\displaystyle=f\\left({\\mu_{D}}\\cdot{w_{D}}\\right)+\\displaystyle\\frac{B^{2}}{2\\sqrt{K}}+\\displaystyle\\frac{L^{2}}{2K}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{x^{\\top}\\left(K_{\\mathcal{D}}\\overline{{{w}}}_{K}-\\left(1-\\gamma\\right)\\mu_{0}\\right)\\leq\\zeta+\\displaystyle\\frac{2\\|w_{1}-w_{\\mathcal{D}}\\|}{K\\eta}+\\displaystyle\\frac{2L}{\\sqrt{K}}+\\displaystyle\\frac{\\varepsilon}{\\sqrt{K}}}}\\\\ {{\\displaystyle=\\zeta+\\displaystyle\\frac{2B}{\\sqrt{K}}+\\displaystyle\\frac{2L}{\\sqrt{K}}+\\displaystyle\\frac{\\varepsilon}{\\sqrt{K}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{g\\left({{h_{\\mathcal{D}}}\\cdot{{\\overline{{w}}}_{K}}}\\right)\\le\\tau+\\kappa+\\displaystyle\\frac{2\\|{w_{1}}-{w_{\\mathcal{D}}}\\|}{K\\eta}+\\displaystyle\\frac{2L}{\\sqrt{K}}+\\displaystyle\\frac{\\varepsilon}{\\sqrt{K}}}}\\\\ {{=\\tau+\\kappa+\\displaystyle\\frac{2B}{\\sqrt{K}}+\\displaystyle\\frac{2L}{\\sqrt{K}}+\\displaystyle\\frac{\\varepsilon}{\\sqrt{K}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we set the initial Lagrange values equal to 0, the constant step size \u03b7 = \u221a1K and $\\mathbf{B}$ is the distance between $w_{1}$ and optimal solution $w_{\\mathscr D}$ . ", "page_idx": 21}, {"type": "text", "text": "And recall Lemma 7, we have with at least $1-2\\delta$ probability, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle||K\\overline{{w}}_{K}-(1-\\gamma)\\mu_{0}||_{1}\\leq||K_{\\mathcal{D}}\\overline{{w}}_{K}-(1-\\gamma)\\mu_{0}||_{1}+||(K-K_{\\mathcal{D}})\\overline{{w}}_{K}||_{1}}\\\\ {\\leq2\\zeta+\\displaystyle\\frac{\\varepsilon}{\\sqrt{K}}+\\displaystyle\\frac{2||w_{1}-w_{\\mathcal{D}}||}{K\\eta}+\\displaystyle\\frac{2L}{\\sqrt{K}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, we have, with at least $1-\\delta$ probability, the bound is, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad J_{r}(\\overline{{\\pi}}_{K})-J_{r}(\\overline{{\\pi}}^{*})}\\\\ &{=\\underbrace{f(d_{R})-f(\\overline{{d}}_{K})}_{\\textnormal{\\texttt{l}}}+\\underbrace{f(\\mu\\cdot\\overline{{w}}_{K})-f(\\mu_{D}\\cdot\\overline{{w}}_{K})}_{\\textnormal{\\texttt{l}}}+\\underbrace{f(\\mu_{D}\\cdot\\overline{{w}}_{K})-f(\\mu\\cdot w^{*})}_{\\textnormal{\\texttt{m I}}}}\\\\ &{=\\underbrace{f(d_{\\overline{{\\pi}}_{K}})-f(\\overline{{d}}_{K})}_{\\textnormal{\\texttt{l}}}+\\underbrace{f(\\mu\\cdot\\overline{{w}}_{K})-f(\\mu_{D}\\cdot\\overline{{w}}_{K})}_{\\textnormal{\\texttt{l}}}+\\underbrace{f(\\mu\\cdot\\overline{{w}}_{K})-f(\\mu_{D}\\cdot w_{D})+f(\\mu_{D}\\cdot w_{D})-f(\\mu_{D}\\cdot w_{D})}_{\\textnormal{\\texttt{m I}}}}\\\\ &{\\leq\\frac{L_{f}}{1-\\gamma}\\cdot\\|K\\overline{{w}}_{K}-(1-\\gamma)\\mu_{0}\\|_{1}+\\frac{\\sqrt{2}B_{w}L_{f}}{\\sqrt{n}}\\sqrt{\\log\\frac{2|\\mathcal{W}|}{\\delta}}+\\frac{B^{2}}{2\\sqrt{K}}}\\\\ &{+\\underbrace{\\frac{L^{2}}{2K}}_{\\textnormal{\\texttt{J}}}+\\frac{\\sqrt{2}B_{w}L_{f}}{\\sqrt{n}}\\sqrt{\\log\\frac{2|\\mathcal{W}|}{\\delta}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle}&{\\leq\\frac{1}{1-\\gamma}\\left(\\frac{6\\sqrt{2}L_{f}B_{w}}{\\sqrt{n}}\\sqrt{\\log\\frac{2|\\mathcal{W}||\\mathcal{X}|}{\\delta}}+\\frac{2B}{\\sqrt{K}}+\\frac{2L}{\\sqrt{K}}+\\frac{B^{2}}{2\\sqrt{K}}+\\frac{L^{2}}{2K}+\\frac{\\varepsilon}{\\sqrt{K}}\\right)}\\\\ {\\displaystyle}&{\\leq\\frac{1}{1-\\gamma}\\left(\\frac{6\\sqrt{2}L_{f}B_{w}}{\\sqrt{n}}\\sqrt{\\log\\frac{2|\\mathcal{W}||\\mathcal{X}|}{\\delta}}+\\frac{\\iota}{2\\sqrt{K}}\\right)}\\\\ {\\displaystyle}&{\\leq\\mathcal{O}(1/\\sqrt{n})+\\mathcal{O}(1/\\sqrt{K})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of violation bound ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In the main text, we split the violation bound into three parts and list the relevant lemmas. Here we give the proofs for these lemmas and theorem. ", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma 1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. This proof is quite similar to Lemma 7 since we always want to establish connections between the required function and validity error bound. The main difference is the objective function and constraint function. So we keep the first half conclusion of the Lemma 7 which is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|M\\overline{{d}}_{K}-(1-\\gamma)\\mu_{0}\\|_{1}=\\|M(\\overline{{d}}_{K}-d_{\\overline{{\\pi}}_{K}})\\|_{1}}\\\\ &{\\qquad\\qquad\\qquad\\geq(1-\\gamma)\\|\\widehat{d}_{K}-\\widehat{d}_{\\overline{{\\pi}}_{K}}\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then for the safety constraint function, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lvert g(\\overline{{d}}_{K})-g(d_{\\overline{{\\pi}}_{K}})\\rvert\\leq L_{g}\\lvert\\overline{{d}}_{K}-d_{\\overline{{\\pi}}_{K}}\\rvert}&{}\\\\ {=L_{g}\\lvert G_{\\pi}(\\hat{d}_{K}-d_{\\hat{\\pi}_{K}}^{\\hat{\\alpha}})\\rvert}&{}\\\\ {\\leq L_{g}\\lVert\\hat{d}_{K}-\\hat{d}_{\\overline{{\\pi}}_{K}}\\rVert_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first inequality is because Lipschitz condition of the safety constraint function. Also by combining the above functions, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{g(\\overline{{d}}_{K})-g(d_{\\overline{{\\pi}}_{K}})\\le\\frac{L_{g}\\|M\\overline{{d}}_{K}-(1-\\gamma)\\mu_{0}\\|_{1}}{1-\\gamma}}}\\\\ &{}&{=\\frac{L_{g}\\|K\\overline{{w}}_{K}-(1-\\gamma)\\mu_{0}\\|_{1}}{1-\\gamma}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "And in Lemma 7 we have already established the bound of $\\|K\\overline{{w}}_{K}-(1-\\gamma)\\mu_{0}\\|_{1}$ , then with at least $1-2\\delta$ probability ", "page_idx": 22}, {"type": "equation", "text": "$$\ng(\\overline{{{d}}}_{K})\\leq g(d_{\\overline{{{\\pi}}}_{K}})+\\frac{2L_{g}B_{w}\\sqrt{2\\log(|\\mathcal{W}||\\mathcal{X}|/\\delta)}}{(1-\\gamma)\\sqrt{n}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which completes the proof. ", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma 2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. In this lemma, actually, we will prove that $g\\left(\\mu\\cdot\\overline{{w}}_{K}\\right)$ is close to $g\\left(\\mu_{D}\\cdot\\overline{{w}}_{K}\\right)$ . We take $(\\mu-\\mu_{D})\\cdot w$ as the random variable and it lies in $[-B_{w},B_{w}]$ . We have the Hoeffding inequality ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|\\boldsymbol{\\mu}\\cdot\\boldsymbol{w}-\\boldsymbol{\\mu}_{\\mathcal{D}}\\cdot\\boldsymbol{w}|\\ge t\\right)\\le2\\exp\\left(-\\frac{2n^{2}t^{2}}{\\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}\\right)=\\frac{\\delta}{|\\mathcal{W}|}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we set t = 2\u221aBnw $\\begin{array}{r}{t=\\frac{\\sqrt{2}B_{w}}{\\sqrt{n}}\\sqrt{\\log\\frac{2|\\mathcal{W}|}{\\delta}}}\\end{array}$ . Next, take the union bound for $w$ and use the Lipschitz condition for $g$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall w\\in\\mathcal{W},\\quad\\mathbb{P}\\left(g\\left(\\mu_{\\mathcal{D}}\\cdot w\\right)-g\\left(\\mu\\cdot w\\right)\\geq L_{g}\\cdot t\\right)\\,\\leq\\,\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "So for $\\overline{{w}}_{K}\\in\\mathcal{W}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(g\\left(\\mu_{\\mathcal{D}}\\cdot\\overline{{\\boldsymbol{w}}}_{K}\\right)-g\\left(\\mu\\cdot\\overline{{\\boldsymbol{w}}}_{K}\\right)\\geq L_{g}\\cdot t\\right)\\,\\leq\\,\\delta\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which means that with at least $1-\\delta$ probability, ", "page_idx": 23}, {"type": "equation", "text": "$$\ng\\left(\\mu_{\\mathcal{D}}\\cdot\\overline{{\\boldsymbol{w}}}_{K}\\right)-g\\left(\\mu\\cdot\\overline{{\\boldsymbol{w}}}_{K}\\right)\\leq\\frac{\\sqrt{2}L_{g}B_{w}}{\\sqrt{n}}\\sqrt{\\log\\frac{2|\\mathcal{W}|}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The proof is complete. ", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma 3 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof. This lemma is easy to be verified because we choose the relax parameter $\\kappa$ is equal to $\\frac{\\sqrt{2}L_{g}B_{w}}{\\sqrt{n}}\\sqrt{\\log\\frac{2|\\mathcal{W}|}{\\delta}}$ and recall the result in Theorem 3 which completes the proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Thus, take the Lemma 1, 2 and 3 together, we have, with at least $1-3\\delta$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\gamma_{c}(\\overline{{\\pi}}_{K})-\\tau=\\underbrace{g(d_{\\pi_{K}})-g(\\overline{{d}}_{K})}_{1}+\\underbrace{g(\\mu\\cdot\\overline{{w}}_{K})-g(\\mu_{D}\\cdot\\overline{{w}}_{K})}_{\\parallel}+\\underbrace{g(\\mu_{D}\\cdot w_{D})-\\tau}_{\\mathrm{H}}}\\\\ &{\\leq\\frac{L_{g}}{1-\\gamma}\\cdot\\|K\\overline{{w}}_{K}-(1-\\gamma)\\mu_{0}\\|_{1}+\\frac{\\sqrt{2}B_{w}}{\\sqrt{n}}\\sqrt{\\log\\frac{2|\\mathcal{W}|}{\\delta}}+\\kappa+\\frac{2B}{\\sqrt{K}}+\\frac{2L}{\\sqrt{K}}+\\frac{\\varepsilon}{\\sqrt{K}}}\\\\ &{\\leq\\frac{1}{1-\\gamma}\\left(\\frac{6\\sqrt{2}L_{g}B_{w}}{\\sqrt{n}}\\sqrt{\\log\\frac{2|\\mathcal{W}||\\mathcal{X}|}{\\delta}}+\\frac{2B}{\\sqrt{K}}+\\frac{2L}{\\sqrt{K}}+\\frac{\\varepsilon}{\\sqrt{K}}\\right)+\\frac{2B}{\\sqrt{K}}+\\frac{2L}{\\sqrt{K}}+\\frac{\\varepsilon}{\\sqrt{K}}}\\\\ &{\\leq\\frac{6\\sqrt{2}L_{g}B_{w}}{(1-\\gamma)\\sqrt{n}}\\sqrt{\\log\\frac{2|\\mathcal{W}||\\mathcal{X}|}{\\delta}}+\\frac{v}{\\sqrt{K}}}\\\\ &{\\leq\\mathcal{O}(1/\\sqrt{n})+\\mathcal{O}(1/\\sqrt{K})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we set $\\begin{array}{r}{\\upsilon=\\frac{1}{1-\\gamma}\\left(4B+4L+2\\varepsilon\\right)}\\end{array}$ which completes the proof. ", "page_idx": 23}, {"type": "text", "text": "B Experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We introduce a practical version of our primal-dual algorithm, following the key structure in Algorithm 1. We utilize this algorithm to conduct various experiments in this paper. ", "page_idx": 23}, {"type": "text", "text": "Algorithm 2: Practical version of POCC ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Input: Dataset $D=\\{(s_{i},a_{i},r_{i},c_{i})\\}_{i=1}^{n}$ , cost threshold $\\tau$ , learning rate $\\eta_{\\lambda},\\eta_{w}$ , $\\eta_{\\phi}$ , relaxed parameter $\\zeta,\\kappa.$ .; ", "page_idx": 23}, {"type": "text", "text": "2 Initialize network of importance weight $w_{\\psi}$ ; ", "page_idx": 23}, {"type": "text", "text": "4 Jointly optimize: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{\\psi}^{k+1}=\\mathrm{Adam}\\left(w_{\\psi}^{k}-\\eta_{\\phi}\\nabla L(w_{\\psi},\\lambda^{k},\\phi^{k})\\right),}\\\\ &{\\lambda^{k+1}=\\mathrm{Adam}\\left(\\lambda^{k}-\\eta_{\\lambda}\\nabla L(w_{\\psi}^{k},\\lambda,\\phi^{k})\\right.,}\\\\ &{\\phi^{k+1}=\\mathrm{Adam}\\left(\\phi^{k}-\\eta_{\\tau}\\nabla L(w_{\\psi}^{k},\\lambda^{k},\\phi)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "5 Compute the importance weight: $\\forall(s,a)$ , $w(s,a)=w_{\\psi}(s,a)$ ; ", "page_idx": 23}, {"type": "text", "text": "6 Extract the policy: $\\begin{array}{r}{\\pi(a\\mid s)={\\frac{w_{\\psi}(s,a)\\pi_{\\mu}(a\\mid s)}{\\sum_{a^{\\prime}\\in A}w_{\\psi}(s,a^{\\prime})\\pi_{\\mu}(a^{\\prime}\\mid s)}}}\\end{array}$ ;   \n7 Output: Policy $\\pi$ ; ", "page_idx": 23}, {"type": "text", "text": "Hyperparameters In the experiments of maze and FrozenLake environments, we set the discount factor as $\\gamma=0.99$ and the cost threshold as $\\tau=0$ , indicating that the agent is not supposed to incur any cost. For our algorithm 2 and COptiDICE [Lee et al., 2021], we perform a grid search on the set $\\{0.00001,0.00005,0.0001,0.0005,0.001,0.005\\}$ to determine the learning rate for parameters. We ultimately select a learning rate of 0.00001 for the neural network and a Lagrange scalar learning ", "page_idx": 23}, {"type": "text", "text": "rate of 0.0001. We set $\\zeta=0.1$ and $\\kappa=0.001$ in our algorithm, and we set $\\alpha=0.5$ in COptiDICE.   \nIn both environments, we run the algorithm for $K=100000$ iterations to ensure model convergence.   \nAdditionally, we use fully-connected neural networks with a single hidden layer of width 64. ", "page_idx": 24}, {"type": "text", "text": "Continuous environments We also run a list of experiments in SafetyGym with offline datasets provided by [Liu et al., 2023a] and compare with comprehensive baselines (e.g., CPQ in [R5], PDCA in [R4], CoptiDICE in [R6], and BEAR-Lagrangian in [R7], [R8]). Note that only PDCA and our algorithm provide theoretical results. In these experiments, to deal with the continuous state-action space, we use the fully connected single hidden-layer neural network of width 128 to represent $w$ . We summarize the evaluation results in the following table. All the rewards and costs are normalized and the cost threshold is 1. Each value is averaged over 20 evaluation episodes and 3 random seeds. ", "page_idx": 24}, {"type": "table", "img_path": "UuiZEOVtHx/tmp/76d8f1e49535555584dfa61451c2129c222930d15d9efae88e4711bb1893b35d.jpg", "table_caption": ["Table 2: All the rewards and costs are normalized. The cost threshold is 1. Blue: Safe agents with the highest reward. ", "Compute setting We run the experiments with NVIDIA GeForce RTX 3080 Ti 8-Core Processor. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Claims in the abstract and introduction accurately reflect our paper\u2019s contributions. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We include the limitations in our assumptions. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide a proof sketch of our theorem in the main paper and complete the full proof in the supplemental material. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We disclose all the information needed to reproduce the main experiments. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide the data and code. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We specify the experimental settings in the supplemental material. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We report error bars correctly in the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide the sufficient information on the computer resources. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The research conducted in this paper conform with the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We cite the original paper and abide by the license. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]