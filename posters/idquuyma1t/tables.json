[{"figure_path": "IdQuUYMA1t/tables/tables_9_1.jpg", "caption": "Table 1: Results of training with various datasets using ResNet-18. Bold values indicate the best performance. For the number of steps, bold formatting is used for all methods except warm-starting. Results are averaged across five random seeds, except for Tiny-ImageNet which uses three random seeds. Standard deviations are provided in parentheses.", "description": "This table presents a comparison of different neural network training methods (random initialization, warm-starting, Shrink & Perturb, and DASH) across four datasets (Tiny-ImageNet, CIFAR-10, CIFAR-100, and SVHN) using ResNet-18.  For each dataset and method, it reports the test accuracy achieved at the last experiment and the average test accuracy across all experiments, along with the number of steps taken in the last experiment and the average number of steps across all experiments.  Bold values highlight the best-performing method for each metric. The table demonstrates the impact of different warm-starting strategies on model performance and training efficiency.", "section": "Experimental Results"}, {"figure_path": "IdQuUYMA1t/tables/tables_19_1.jpg", "caption": "Table 1: Results of training with various datasets using ResNet-18. Bold values indicate the best performance. For the number of steps, bold formatting is used for all methods except warm-starting. Results are averaged across five random seeds, except for Tiny-ImageNet which uses three random seeds. Standard deviations are provided in parentheses.", "description": "This table presents a performance comparison of different neural network training methods (Random Init, Warm Init, S&P, DASH) across four datasets (Tiny-ImageNet, CIFAR-10, CIFAR-100, SVHN) using ResNet-18.  The metrics compared are test accuracy (at the last experiment and averaged across all experiments) and the number of training steps (at the last experiment and averaged across all experiments). Bold values highlight the best-performing method for each metric in each dataset.  Standard deviations indicate the variability of the results.", "section": "Experimental Results"}, {"figure_path": "IdQuUYMA1t/tables/tables_21_1.jpg", "caption": "Table 1: Results of training with various datasets using ResNet-18. Bold values indicate the best performance. For the number of steps, bold formatting is used for all methods except warm-starting. Results are averaged across five random seeds, except for Tiny-ImageNet which uses three random seeds. Standard deviations are provided in parentheses.", "description": "This table presents a comparison of different neural network training methods (Random Init, Warm Init, Warm ReM, S&P, DASH) across four datasets (Tiny-ImageNet, CIFAR-10, CIFAR-100, SVHN) using ResNet-18. For each method and dataset, the table shows the test accuracy achieved at the last experiment and the average test accuracy across all experiments.  It also includes the number of training steps used in the last experiment and the average number of steps across all experiments. Bold values highlight the best performing method for each metric.", "section": "5.2 Experimental Results"}, {"figure_path": "IdQuUYMA1t/tables/tables_21_2.jpg", "caption": "Table 1: Results of training with various datasets using ResNet-18. Bold values indicate the best performance. For the number of steps, bold formatting is used for all methods except warm-starting. Results are averaged across five random seeds, except for Tiny-ImageNet which uses three random seeds. Standard deviations are provided in parentheses.", "description": "This table presents a comparison of different neural network training methods (Random Init, Warm Init, Warm ReM, S&P, and DASH) on various datasets (Tiny-ImageNet, CIFAR-10, CIFAR-100, and SVHN) using ResNet-18.  The metrics reported include test accuracy at the last experiment and average across all experiments, as well as the number of training steps taken (at last experiment and average). Bold values highlight the best-performing method for each dataset and metric. The table shows that DASH generally outperforms other methods in terms of test accuracy, although sometimes at the cost of additional training steps.", "section": "Experimental Results"}, {"figure_path": "IdQuUYMA1t/tables/tables_21_3.jpg", "caption": "Table 1: Results of training with various datasets using ResNet-18. Bold values indicate the best performance. For the number of steps, bold formatting is used for all methods except warm-starting. Results are averaged across five random seeds, except for Tiny-ImageNet which uses three random seeds. Standard deviations are provided in parentheses.", "description": "This table presents a comparison of different neural network training methods (Random Init, Warm Init, Warm ReM, S&P, DASH) on four datasets (Tiny-ImageNet, CIFAR-10, CIFAR-100, SVHN) using ResNet-18.  For each dataset and method, it shows the test accuracy achieved at the last experiment, the average test accuracy across all experiments, the number of steps taken at the last experiment, and the average number of steps across all experiments.  Bold values indicate the best performance in each category. The table highlights the superior performance of the DASH method in most cases, achieving higher test accuracy while often requiring fewer steps to converge.", "section": "Experimental Results"}, {"figure_path": "IdQuUYMA1t/tables/tables_22_1.jpg", "caption": "Table 1: Results of training with various datasets using ResNet-18. Bold values indicate the best performance. For the number of steps, bold formatting is used for all methods except warm-starting. Results are averaged across five random seeds, except for Tiny-ImageNet which uses three random seeds. Standard deviations are provided in parentheses.", "description": "This table presents a comparison of different neural network training methods (Random Init, Warm Init, Warm ReM, S&P, DASH) on four datasets (Tiny-ImageNet, CIFAR-10, CIFAR-100, SVHN) using ResNet-18.  The table shows the test accuracy achieved at the last experiment and the average test accuracy across all experiments, along with the number of training steps for the last and average across all experiments.  Bold values highlight the best-performing method for each metric.  The results demonstrate the impact of different warm-starting strategies on model performance.", "section": "5.2 Experimental Results"}, {"figure_path": "IdQuUYMA1t/tables/tables_23_1.jpg", "caption": "Table 1: Results of training with various datasets using ResNet-18. Bold values indicate the best performance. For the number of steps, bold formatting is used for all methods except warm-starting. Results are averaged across five random seeds, except for Tiny-ImageNet which uses three random seeds. Standard deviations are provided in parentheses.", "description": "This table presents a comparison of different neural network training methods on various datasets using ResNet-18.  The methods compared include random initialization (cold-starting), warm-starting, Shrink & Perturb (S&P), and the proposed DASH method.  The table shows the test accuracy achieved at the last experiment and the average test accuracy across all experiments.  Additionally, it reports the number of steps (training iterations) required at the last experiment and the average across all experiments. Bold values highlight the best performing method for each metric. Note that the number of random seeds used for averaging varies between datasets.", "section": "5.2 Experimental Results"}, {"figure_path": "IdQuUYMA1t/tables/tables_24_1.jpg", "caption": "Table 1: Results of training with various datasets using ResNet-18. Bold values indicate the best performance. For the number of steps, bold formatting is used for all methods except warm-starting. Results are averaged across five random seeds, except for Tiny-ImageNet which uses three random seeds. Standard deviations are provided in parentheses.", "description": "This table presents a comparison of different neural network training methods (Random Init, Warm Init, Warm ReM, S&P, DASH) on four datasets (Tiny-ImageNet, CIFAR-10, CIFAR-100, SVHN) using ResNet-18.  For each dataset and method, it shows the test accuracy achieved at the last experiment, the average test accuracy across all experiments, the number of steps taken in the last experiment, and the average number of steps across all experiments.  Bold values highlight the best performance for each metric. The number of random seeds used in averaging the results is also specified.", "section": "Experimental Results"}, {"figure_path": "IdQuUYMA1t/tables/tables_28_1.jpg", "caption": "Table 9: Computational and memory requirements for each method, comparing two experiments on CIFAR-10 using ResNet-18. We report the total number of epochs, total training time, and total computational cost (in TeraFLOPS). Memory usage is measured in gigabytes, showing aggregated CPU and CUDA memory consumption.", "description": "This table compares the computational and memory resources used by four different neural network training initialization methods (Cold Init, Warm Init, S&P, and DASH) on the CIFAR-10 dataset using ResNet-18.  The metrics reported include the number of epochs required for training, the total training time in seconds, the total computational cost in TeraFLOPs, and the CPU and CUDA memory usage in gigabytes.  The table provides insights into the efficiency and resource demands of different warm-starting strategies.", "section": "C.5 Computation and Memory Overhead Comparison"}, {"figure_path": "IdQuUYMA1t/tables/tables_28_2.jpg", "caption": "Table 1: Results of training with various datasets using ResNet-18. Bold values indicate the best performance. For the number of steps, bold formatting is used for all methods except warm-starting. Results are averaged across five random seeds, except for Tiny-ImageNet which uses three random seeds. Standard deviations are provided in parentheses.", "description": "This table compares the performance of different neural network training methods (Random Init, Warm Init, S&P, and DASH) across four datasets (Tiny-ImageNet, CIFAR-10, CIFAR-100, and SVHN) using ResNet-18.  For each dataset and method, it shows the test accuracy achieved at the last experiment and the average test accuracy across all experiments.  It also presents the number of steps (training iterations) taken in the last experiment and the average number of steps across all experiments. Bold values highlight the best performance for each metric, except for the number of steps where bold formatting is only used for all methods except for warm-starting. Standard deviations are included to show variability across multiple runs.", "section": "5.2 Experimental Results"}]