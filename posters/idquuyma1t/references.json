{"references": [{"fullname_first_author": "Alessandro Achille", "paper_title": "Critical learning periods in deep networks", "publication_date": "2018-00-00", "reason": "This paper is foundational for understanding the loss of plasticity phenomenon in deep neural networks, which is a core theme of the DASH paper."}, {"fullname_first_author": "Jordan Ash", "paper_title": "On warm-starting neural network training", "publication_date": "2020-00-00", "reason": "This paper directly addresses the challenges of warm-starting neural networks in stationary settings, providing a baseline method (Shrink and Perturb) against which DASH is compared."}, {"fullname_first_author": "Zeyuan Allen-Zhu", "paper_title": "Towards understanding ensemble, knowledge distillation and self-distillation in deep learning", "publication_date": "2020-00-00", "reason": "This paper provides a theoretical framework for understanding feature learning in neural networks, which informs the theoretical framework developed in the DASH paper."}, {"fullname_first_author": "Tudor Berariu", "paper_title": "A study on the plasticity of neural networks", "publication_date": "2021-00-00", "reason": "This paper investigates the phenomenon of plasticity loss in neural networks, providing empirical evidence that supports the core findings of the DASH paper."}, {"fullname_first_author": "Maximilian Igl", "paper_title": "Transient non-stationarity and generalization in deep reinforcement learning", "publication_date": "2020-00-00", "reason": "This paper explores the impact of non-stationary data distributions on the generalization performance of neural networks, providing insights into the challenges addressed by DASH."}]}