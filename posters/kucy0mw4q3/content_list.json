[{"type": "text", "text": "VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yang Li ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shaobo Han ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shihao Ji\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dept. of Computer Science Georgia State University Atlanta, GA 30303 yli93@student.gsu.edu ", "page_idx": 0}, {"type": "text", "text": "Optical Networking and Sensing NEC Laboratories America Princeton, NJ 08540 shaobo@nec-labs.com ", "page_idx": 0}, {"type": "text", "text": "School of Computing University of Connecticut Storrs, CT 06269 shihao.ji@uconn.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As the adoption of large language models increases and the need for per-user or pertask model customization grows, the parameter-efficient fine-tuning (PEFT) methods, such as low-rank adaptation (LoRA) and its variants, incur substantial storage and transmission costs. To further reduce stored parameters, we introduce a \"divideand-share\" paradigm that breaks the barriers of low-rank decomposition across matrix dimensions, modules, and layers by sharing parameters globally via a vector bank. As an instantiation of the paradigm to LoRA, our proposed VB-LoRA composites all the low-rank matrices of LoRA from a shared vector bank with a differentiable top- $k$ admixture module. VB-LoRA achieves extreme parameter efficiency while maintaining comparable or better performance compared to state-of-the-art PEFT methods. Extensive experiments demonstrate the effectiveness of VB-LoRA on natural language understanding, natural language generation, instruction tuning, and mathematical reasoning tasks. When fine-tuning the Llama2-13B model, VBLoRA only uses $0.4\\%$ of LoRA\u2019s stored parameters, yet achieves superior results. Our source code is available at https://github.com/leo-yangli/VB-LoRA. This method has been merged into the Hugging Face PEFT package2. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Parameter-efficient fine-tuning (PEFT) casts a new paradigm that leverages strong prior knowledge built in foundation models and adapts them to a wide range of downstream tasks by updating a small amount of trainable parameters [He et al., 2021]. Compared to prefix/prompt tuning [Li and Liang, 2021, Lester et al., 2021] or in-context learning [Brown et al., 2020], fine-tuning a large-scale pre-trained model yields better domain specialization dictated by high-quality datasets [Brown et al., 2020, Liu et al., 2022, Zhao et al., 2023]. This process can be repeated to suit the needs of ever-changing deployment scenarios and personalizations. However, the sheer volume of parameter space across a multitude of instantiations [Sheng et al., 2023] poses challenges for storage, transmission, and computation, especially for low-resource hardware and consumer-grade networks [Borzunov et al., 2024]. ", "page_idx": 0}, {"type": "image", "img_path": "kuCY0mW4Q3/tmp/d1ad14b2711ceefcc9d9ffecabf1a9488f827c3f9f89d6c1463e481a69ffb65e.jpg", "img_caption": ["Figure 1: Comparison of the PEFT methods on RoBERTa-Large. Our VB-LoRA achieves higher scores with significantly smaller number of stored parameters. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "To mitigate these challenges, various PEFT methods have been proposed by adding or adapting a small amount of trainable parameters per task without sacrificing performance [Houlsby et al., 2019, ", "page_idx": 0}, {"type": "image", "img_path": "kuCY0mW4Q3/tmp/9fb4d03544a9bc67dc7946a43f15371d66f7c249a7dc49a9ad53218c2a632e2e.jpg", "img_caption": ["Figure 2: Left: The model parameters can be represented as a composition of vectors from a vector bank, which is shared across sub-vectors, modules and layers. Right: Architecture of VB-LoRA. We use a top- $k$ softmax function to select $k$ vectors from the vector bank. The selected vectors are then pooled into a sub-vector, which is arranged at a desired position, forming the parameters of LoRA. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Karimi Mahabadi et al., 2021, Ding et al., 2023]. These methods exploit the dependencies among model parameters to reduce the redundancy. For example, Hu et al. [2021] propose the low-rank adaptation (LoRA) to approximate the accumulated gradient update for self-attention modules, and induces the intra-matrix parameter coupling. Renduchintala et al. [2024] further study the options of allowing the inter-matrix parameter sharing via weight tying across all the layers. In both cases, the number of trainable parameters is reduced significantly. These two methods stand at the two extremes of spectrum in deciding the range of model components reuse (locally or across-layers) and designating which low-rank matrices needs to be shared and updated. However, as the model size increases and the demand for user-customized models across various services rises, the expense of storing and transmitting the customizations for each combination escalates and emerges as a critical issue. Hence, investigating PEFT methods with significantly smaller number of trainable parameters has attracted a flurry of research interests [Kopiczko et al., 2024, Renduchintala et al., 2024]. ", "page_idx": 1}, {"type": "text", "text": "This paper introduces VB-LoRA, extreme parameter-efficient fine-tuning with vector banks based on a simple yet effective \"divide-and-share\" paradigm. We push the limits of LoRA parameter efficiency by breaking the two barriers of low-rank decomposition: (1) locally within each module and each layer, and (2) only across the two original matrix dimensions (without division; see Sec. 3.2 for details). We argue that the parameters across different modules and layers can be shared, and thus the redundancy in parameters can be further reduced. In addition, by partitioning rank-one component vectors into sub-vectors, we introduce \"virtual\" dimensions such that deep structure in the parameter space can be represented by a highly compressed matrix factorization. ", "page_idx": 1}, {"type": "text", "text": "VB-LoRA draws inspirations from previous line of work on quantized tensor networks [Oseledets, 2010, Cichocki, 2014] in breaking the constraint of physical dimension for extreme parameter compression. Specifically, VB-LoRA reparameterizes LoRA\u2019s low-rank adaptation by a rank-one decomposition and then divides the resulting vectors into sub-vectors of the same size. A global sharing mechanism is then learnt based on a sparse top- $k$ admixture module. The same sized subvectors allows parameters to be shared across modules and layers at the sub-vector level. Moreover, compared to the post-hoc matrix compression methods [Oseledets, 2010, Khoromskij, 2011], VBLoRA is end-to-end differentiable, and therefore the fine-tuning process is aware of the compressed form, enabling task-oriented compression. Figure 1 illustrates the parameter efficiency of VB-LoRA as compared with state-of-the-art PEFT methods. Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We introduce a \"divide-and-share\" paradigm that breaks the barriers of low-rank decomposition across matrix dimensions, modules, and layers by sharing parameters globally via a vector bank. 2. We reparameterize LoRA\u2019s low-rank decomposition by a rank-one decomposition, and divide the resulting vectors further into sub-vectors of the same size, enabling extreme parameter efficiency at the sub-vector level. ", "page_idx": 1}, {"type": "text", "text": "3. We propose a sparse top- $k$ module based on the admixture model to learn a global sharing mechanism, making our framework end-to-end differentiable and compression-aware. ", "page_idx": 2}, {"type": "text", "text": "4. Our method achieves extreme parameter efficiency while maintaining comparable or better empirical performance compared to the state-of-the-art PEFT methods on natural language understanding, natural language generation, instruction tuning, and mathematical reasoning tasks. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Exploit Global Redundancy for Enhanced Parameter Efficiency The parameters of deep neural networks (DNNs) can be naturally divided by layers, heads, or types (MHA or FFN). While LoRA [Hu et al., 2021] only exploits the intra-matrix dependency, Tied-LoRA [Renduchintala et al., 2024] employs a simple weight tying scheme on the low-rank matrices $A$ and $B$ across layers to reduce the inter-matrix redundancy. When $A$ and $B$ are randomly initialized, frozen, and shared across all layers, Tied-LoRA degenerates to VeRA [Kopiczko et al., 2024], which only requires two scaling vectors to be updated, leading to impressive parameter efficiency. A concurrent work, LoRA-XS [Ba\u0142azy et al., 2024], further improves the parameter efficiency of LoRA by introducing small trainable matrices between frozen LoRA projection matrices, which are initialized using Singular Value Decomposition (SVD) of the pretrained module weights. Our VB-LoRA pushes the limits of LoRA parameter efficiency by sharing parameters globally across modules and layers at the sub-vector level. ", "page_idx": 2}, {"type": "text", "text": "On the low-dimensional reparameterization, Aghajanyan et al. [2020] empirically show that there exists a low-dimensional reparameterization that is as effective for fine-tuning as the full parameter space. The actualization of the random projection is achieved through the Fastfood transform [Le et al., 2013] for large-scale pre-trained language models. To make it structure-aware, a set of layerwise scaling parameters are included as part of the training parameters. Following this intuition, we study the lightweight fine-tuning within LoRA based on the customized reparameterization that arises from the rank-one matrix decomposition. ", "page_idx": 2}, {"type": "text", "text": "Moreover, tensor decomposition has been leveraged for PEFT in ViT models [Jie and Deng, 2023] based on classical formats, such as tensor-train or Tucker [Kolda and Bader, 2009]. We find that forcing multilinear decomposition across multiple modes results in a higher rank number, which is detrimental to the objective of parameter compression. An indirect comparison of VB-LoRA to Jie and Deng [2023] can be conducted by referring the compression rate to LoRA. From this perspective, our VB-LoRA can be viewed as a customized tensor format endowed with a convex geometry structure, which is enabled by the sparse top- ${\\cdot k}$ admixture model we proposed. ", "page_idx": 2}, {"type": "text", "text": "Compared to the deep fusion approach [Mazzawi et al., 2024] where LLM parameters are split and initialized using pre-trained smaller networks under a designed network growth mechanism, our parameter division operates on the rank-one component vectors. Sub-vector division allows for similar extensions to leverage pre-trained vector bank initializations from smaller models and distributed training using model parallelism. ", "page_idx": 2}, {"type": "text", "text": "Parameter Modeling based on Sparse Admixture Models Admixture models have been widely used in population genetics [Pritchard et al., 2000], topic modeling [Reisinger et al., 2010, Inouye et al., 2014], and hyperspectral unmixing [Li and Bioucas-Dias, 2008, Fu et al., 2015] to extract archetypal (or endmember) components from observed data. The archetypal components can be relaxed to have mixed sign [Ding et al., 2008] with identifiability guarantees [Lin et al., 2015]. Conventionally, parameters estimation are conducted based on linear programming [Chan et al., 2009] or combinatorial algorithms [Arora et al., 2013]. However, an involved integer programming problem arises when incorporating an extra top- $k$ constraint into the mixing weights that is especially challenging for the large-scale language models. In this work, we propose learning archetypal vector banks not from observed data but from model parameters of LLMs. By modifying the sparse top- $k$ module [Shazeer et al., 2016] commonly used in Mixture-of-Expert models [Jiang et al., 2024], the mixing weights and vector banks are optimized by back-propagation under the objective of downstream fine-tuning tasks. The proposed top- $k$ admixture model is model-agnostic in the sense that it can be readily integrated into any neural network parameters or accumulated gradient updates. ", "page_idx": 2}, {"type": "text", "text": "3 Proposed Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Preliminaries: Transformer Architecture and LoRA Adapters ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The transformer architecture [Vaswani et al., 2017] consists of $L$ layers, each containing two types of blocks: Multi-Head Attention (MHA) and Feed-Forward Network (FFN). We denote the query, key, value, and output matrices of MHA at layer $\\ell$ as $\\pmb{\\mathscr{W}}_{t}^{\\ell}\\,=\\,\\{{\\pmb{W}}_{t}^{i}\\}_{i=1}^{N_{h}}$ , $t\\,\\in\\,\\{q,k,v,o\\}$ , where $W_{t}^{i}\\in\\mathbb{R}^{d\\times d}$ , and $N_{h}$ is the number of heads. Given $\\mathrm{FFN}(\\pmb{x})=W_{\\mathrm{down}}\\mathrm{ReLU}(W_{\\mathrm{up}}\\pmb{x})$ with $\\pmb{x}\\in\\mathbb{R}^{d}$ , viewing FFN as a multi-head operation, we further divide $W_{\\mathrm{up}}\\in\\mathbb{R}^{c d\\times d}$ and $W_{\\mathrm{down}}\\in\\mathbb{R}^{d\\times c d}$ into $c$ matrices of size $d\\times d$ , denoted by $\\mathcal{W}_{\\mathrm{up}}^{\\ell}=\\{\\boldsymbol{W}_{\\mathrm{up}}^{\\ell,i}\\}_{i=1}^{c}$ and $\\pmb{\\mathcal{W}}_{\\mathrm{down}}^{\\ell}=\\{\\pmb{W}_{\\mathrm{down}}^{\\ell,i}\\}_{i=1}^{c}$ . $c=4$ . ", "page_idx": 3}, {"type": "text", "text": "Given a pre-trained matrix $W_{0}\\in\\mathbb{R}^{m\\times n}$ , LoRA [Hu et al., 2021] constrains the weight increments $\\Delta W$ as a low-rank decomposition $\\Delta W=B A$ , where $\\b{B}\\in\\mathbb{R}^{m\\times r}$ , $A\\in\\mathbb{R}^{r\\times n}$ are trainable parameters, with $r\\ll\\operatorname*{min}(m,n)$ . VeRA [Kopiczko et al., 2024] further limits the trainable parameters to two scaling vectors $b$ and $d$ , which form the diagonal elements of two diagonal matrices $\\Lambda_{b}$ and $\\Lambda_{d}$ . Hence, VeRA can be expressed as $\\Delta W=\\Lambda_{b}B\\Lambda_{d}A$ , where $_B$ and $\\pmb{A}$ are randomly initialized, frozen and shared across layers. ", "page_idx": 3}, {"type": "text", "text": "Collectively, we denote the model parameters of transformer as $\\Omega=\\{\\{\\pmb{\\mathscr{W}}_{q}^{\\ell},\\pmb{\\mathscr{W}}_{k}^{\\ell},\\pmb{\\mathscr{W}}_{v}^{\\ell},\\pmb{\\mathscr{W}}_{o}^{\\ell}\\}\\cup$ $\\{\\mathcal{W}_{\\mathrm{up}}^{\\ell},\\mathcal{W}_{\\mathrm{down}}^{\\ell}\\}\\}_{\\ell=1}^{L}\\in\\mathbb{R}^{12L\\times d\\times d}$ . In the sequel, we propose a global reparameterization on the weight increments of $W\\in\\Omega$ based on the LoRA decomposition $\\Delta W=B A$ . we will show how extreme parameter efficiency can be achieved by (1) parameter sharing across matrix dimensions of $\\pmb{A}$ and $_B$ based on a rank-one decomposition and sub-vector partitions (Sec. 3.2), and (2) across modules and layers regardless of the index or matrix type (Sec. 3.3). ", "page_idx": 3}, {"type": "text", "text": "3.2 Divide-and-Share: a New Paradigm for Parameter Sharing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The low rank decomposition of LoRA can be equivalently expressed in a rank-one form as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta W=B A=\\sum_{k=1}^{r}b_{k}\\otimes a_{k}=\\sum_{k=1}^{r}\\otimes_{i=1}^{2}v_{k}^{(i)},\\quad v_{k}^{(1)}=b_{k},\\quad v_{k}^{(2)}=a_{k},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\otimes$ denotes the outer product operator and $\\pmb{v}_{k}^{(i)}$ is a vector of size $d_{i}$ ", "page_idx": 3}, {"type": "text", "text": "Divide Based on the rank-one decomposition above, we further represent each component vector $\\pmb{v}_{k}^{(i)}$ as a concatenation of a set of sub-vectors, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{v}_{k}^{(i)}=\\mathrm{concat}(\\mathbf{u}_{k,1}^{(i)},\\mathbf{u}_{k,2}^{(i)},\\ldots,\\mathbf{u}_{k,d_{i}^{\\prime}}^{(i)}),\\quad\\pmb{u}_{k,j}^{(i)}\\in\\mathbb{R}^{b},\\quad j\\in\\{1,\\ldots,d_{i}^{\\prime}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\{d_{i}\\}_{i=1,2}$ represents the size of the matrix dimension of $\\Delta W$ . In general, $\\{d_{i}\\}_{i=1,2}$ are not equal across $\\pmb{A}$ and $_B$ , and we choose $b$ as a common factor of $d_{i}$ such that $d_{i}^{\\prime}=d_{i}^{\\bar{\\ }}/\\bar{b\\ }$ and $d_{i}^{\\prime}\\in\\mathbb{Z}$ . ", "page_idx": 3}, {"type": "text", "text": "Share To facilitate parameter sharing across model dimensions, we assume each sub-vector u(ki,)j as a top- ${\\cdot k}$ admixture of basic elements from vector bank $B=\\{\\pmb{\\alpha}_{1},\\dots,\\pmb{\\alpha}_{h}\\}$ , where $\\alpha_{i}\\in\\mathbb{R}^{b}$ for $i\\in\\{1,\\ldots,h\\}$ , and is defined as follows (with the subscripts omitted for clarity): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\pmb u}=\\sum_{s=1}^{h}w_{s}({\\pmb\\sigma}){\\alpha}_{s},\\quad{\\bf w}({\\pmb\\sigma})=\\mathrm{Softmax}(\\mathrm{TopK}({\\pmb\\sigma},{\\pmb k})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where TopK $({\\pmb\\sigma},k)_{i}=\\sigma_{i}$ if $\\sigma_{i}$ is among the top- $k$ of $\\pmb{\\sigma}$ and $\\mathrm{TopK}\\left(\\pmb{\\sigma},\\pmb{k}\\right)_{i}=-\\infty$ otherwise. For each sub-vector $\\textbf{\\em u}$ , we introduce logits $\\pmb{\\sigma}\\in\\mathbb{R}^{h}$ as its learnable parameters. We call the model expressed in Eq. 3.3 as the top- $k$ admixture module (TKAM), which is differentiable. This design enables the joint learning of vector bank $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ and logits $\\pmb{\\sigma}$ in an end-to-end manner, which is amenable for model fine-tuning to the downstream tasks. ", "page_idx": 3}, {"type": "text", "text": "The TKAM module promotes sparsity by selecting $k$ vectors of the largest logits from the vector bank. By setting $k\\ll h$ , we restrict the sub-vector $\\textbf{\\em u}$ to be sparse. That is, in each iteration, the updates to the vector bank remain locally dominated \u2013 with at most $k$ basis vectors $\\alpha\\in{\\mathcal{B}}$ affected by the backpropagation through ${\\pmb u}-\\mathrm{in}$ the hope that the learnt vectors can be more specialized and the knowledge encapsulated in the vector bank can be activated and updated sparsely. ", "page_idx": 3}, {"type": "text", "text": "Noise-free Top- $k$ module The Noisy Top- $k$ Gating module [Shazeer et al., 2016] has been widely used to replace the fully connected layers with the Mixture of Experts (MoE) layers in large language models [Jiang et al., 2024]. In contrast, we use Eq. 3.3 to learn the selective sharing scheme across the rank-one component vectors without changing the original model. Due to the decomposition, we find that the cumulative gradient parameter updates are more sensitive than the original model parameters during the training process. This may be related to the training instability issues observed in hypernetworks [Ortiz et al., 2024], where parameters are generated by another parameterized model as well. Therefore, keeping zero noise in the gating function can help make the learning more efficient and stable. An ablation study of different vector selection methods, including Gumbel-softmax, is provided in Sec. 4.5. ", "page_idx": 4}, {"type": "text", "text": "3.3 Breaking Boundaries of LoRA for Global Parameter Sharing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While LoRA only applies the low rank decomposition to each individual weight increment, the boundary can be broken by the divide-and-share scheme we proposed in Sec. 3.2. Our divideand-share approach can be interpreted as hierarchical and constrained tensor decomposition, which facilitates efficient global parameter sharing that goes beyond LoRA\u2019s low-rank representation of matrices. ", "page_idx": 4}, {"type": "text", "text": "The divide operator was first introduced in Quantized Tensor Train (QTT) for super compression of large-scale matrices [Oseledets, 2010, Cichocki, 2014]. For example, dyadic division reshapes a vector of length $L=2^{p}$ into a $p$ -dimensional array which facilitates the efficient Tensor Train decomposition to be used. Our divide operator instead applies to the rank-one component vectors $\\pmb{v}_{k}^{(i)}$ , and the resulting hierarchical tensorial representation of $\\Delta W$ can be viewed as a Canonical Polyadic Decomposition (CPD) [Kolda and Bader, 2009] with component vectors v(ki)folded into 2-dimensional arrays with sub-vectors ${\\pmb u}_{k,j}^{(i)}$ as columns. Each sub-vector $\\pmb{u}_{\\mathbf{i}}$ is composed from a globally shared vector bank $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ via TKAM, where $\\mathbf{i}=[\\mathbf{j},\\mathbf{v}]$ is a multi-index including physical indices j, such as module, layer, head, and left/right decomposed matrix, and virtual indices $\\mathbf{v}$ (created from vector partition). ", "page_idx": 4}, {"type": "text", "text": "The share operator (TKAM module) can be viewed as a factor model with simplex constraints on the mixing weight (e.g., $k\\,=\\,2$ , the sub-vector $\\textbf{\\em u}$ lies on the edges of the simplex) and common factors stored in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . Let $\\pmb{u}\\in\\mathbb{R}^{b}$ and $\\begin{array}{r}{{\\pmb u}=\\sum_{s=1}^{h}{\\pmb\\alpha}_{s}{\\boldsymbol w}_{s}}\\end{array}$ , where $\\pmb{\\alpha}_{s}$ is the $s$ -th factor, and $\\pmb{w}$ is the factor score for the sub-vector $\\textbf{\\em u}$ . We consi der the following options for $\\mathbf{\\nabla}w$ : (1) Admixture (convex combination): $w\\in[0,1]^{h}$ and $\\textstyle\\sum_{s=1}^{h}w_{s}=1$ , which is commonly used in various communities. (2) aSlploarwsee d.A dItm\u2019isx twuorret (h TmKeAntMio):n $w\\in[0,1]^{h}$ ianngd $\\textstyle\\sum_{s=1}^{h}w_{s}=1$ x  wiintfho romnlayt $k\\ll h$ hneo vn-ezcetroor  seleelemcteinotns mechanism can make the TKAM model structure-aware, potentially yielding additional benefits. One possibility is to make the logits of vector selection conditional on the embeddings of the layer, module, and matrix type, which can be implemented through a hypernetwork [Mahabadi et al., 2021]. However, we leave this for future work. ", "page_idx": 4}, {"type": "text", "text": "In summary, LoRA provides a local low-rank factorization for each $d_{1}\\!\\times\\!d_{2}$ matrix $\\Delta W$ independently. In contrast, our VB-LoRA introduces a global low-rank factorization on a $b\\times|\\{\\mathbf{i}\\}|$ matrix composed of partitioned rank-one vectors, where $|\\{\\}\\}$ denotes the cardinality of the index set including both physical and virtual indices. As we will see below, this differentiation can better leverage the redundancy in the cumulative gradients, leading to extreme parameter efficiency. ", "page_idx": 4}, {"type": "text", "text": "Figure 2 overviews our method. The left section demonstrates the high-level idea of VB-LoRA: the vector bank is shared across sub-vectors, modules, and layers. The right section details its architecture. To form each sub-vector, we use a top- $k$ softmax function to select $k$ vectors from the vector bank, which are then pooled into a sub-vector. These sub-vectors are arranged in the desired positions, forming the parameters for LoRA with negligible computational overhead. Algorithm 1 provides the PyTorch-like pseudocode for VB-LoRA, which can be seamlessly integrated into the PyTorch framework. ", "page_idx": 4}, {"type": "text", "text": "3.4 Parameter Count ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In full fine-tuning, the number of trainable parameters is equal to the model size, i.e., $L M d^{2}$ , where $L$ is the number of layers, $M$ is the number of fine-tuned modules, and $d$ is hidden dimension. ", "page_idx": 4}, {"type": "table", "img_path": "kuCY0mW4Q3/tmp/767a416b2fd95b029700f41a55884d1387d2f797c0d4b545760361f74148f304.jpg", "table_caption": ["Algorithm 1 Pseudocode of VB-LoRA in a PyTorch-like style "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "LoRA reduces this number to $2L M d r$ , while VeRA further reduces it to $L M(d+r)$ . The trainable parameters of LoRA and VeRA are the same as the parameters they need to store. ", "page_idx": 5}, {"type": "text", "text": "In VB-LoRA, the trainable parameters consist of two parts: the parameters of the vector bank $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ and the parameters of logits $\\pmb{\\sigma}$ . However, at the end of training, the logit parameters can be discarded and only the $k$ selected indices and the top- $k$ admixture weights need to be stored. Therefore, the stored parameters can be represented by a triplet $\\Theta=\\{\\boldsymbol{B},\\boldsymbol{\\mathcal{T}},\\boldsymbol{\\nu}\\}$ , where $\\boldsymbol{B}\\in\\mathbb{R}^{h\\times b}$ is a vector bank containing $h$ vectors of $b$ -dimensional, $\\mathcal{Z}\\in\\mathbb{R}^{2\\times L\\times M\\times r\\times(d/b)\\times k}$ is the top- $k$ indices of the vectors in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ for all sub-vectors, and $\\nu\\in\\mathbb{R}^{2\\times L\\times M\\times r\\times(d/b)\\times(k-1)}$ is the top- $k$ admixture weights used to composite the sub-vectors from the bank. It is worth noting that the top- ${\\cdot k}$ admixture weights have only $k-1$ degrees of freedom since they must be summed to 1. Additionally, depending on the size of the vector bank $h$ , the indices $\\mathcal{T}$ can be efficiently stored as unsigned integers (e.g., uint8 when $h\\leq256)$ ), and hence, we count the number of parameters as the float32-equivalent size for a fair comparison. When we use $k=2$ and uint8 for indices, the number of stored parameters of VB-LoRA is $h b+3L M r(d/b)$ . Unlike LoRA and VeRA, the number of parameters in VB-LoRA does not increase linearly with the model size (determined by $L$ and $d$ ) or the number of fine-tuned modules, i.e., $M$ . While the second term of VB-LoRA\u2019s parameters is a linear function of $L M d$ , the coefficient is $3r/b$ , which is typically very small. For example, in our experiments, the typical values are $r=4$ and $b=256$ , leading to a coefficient of 0.04, whereas the coefficient is $2r$ for LoRA and 1 for VeRA. Most of the parameters in VB-LoRA reside within the shared vector bank, whose size does not increase linearly with the model size or number of fine-tuned modules. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we conduct a comprehensive evaluation of our method through a series of experiments. We begin by comparing VB-LoRA to the state-of-the-art PEFT methods: LoRA, VeRA, and TiedLoRA on the GLUE benchmark. Next, we extend our analysis to natural language generation tasks using GPT-2, instruction tuning tasks on the Llama2, as well as mathematical reasoning tasks on Mistral and Gemma models. All our experiments were conducted on a server equipped with 8 NVIDIA A100 GPUs. For reproducibility, we provide detailed hyperparameters and specifications of computing resources for each experiment in the appendix. The source code is available at https://github.com/leo-yangli/VB-LoRA. ", "page_idx": 5}, {"type": "text", "text": "4.1 Natural Language Understanding ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We adopt the General Language Understanding Evaluation (GLUE) benchmark3 [Wang et al., 2018] to assess the performance of VB-LoRA across various natural language understanding tasks, including similarity, paraphrase, and inference tasks. Following Kopiczko et al. [2024], we focus on six tasks from GLUE: CoLA [Warstadt et al., 2019] (linguistic acceptability), SST-2 [Socher et al., 2013] (sentiment analysis), MRPC [Dolan and Brockett, 2005] (paraphrase detection), STS-B [Cer et al., 2017] (semantic textual similarity), QNLI [Rajpurkar et al., 2018] (inference), and RTE (inference). ", "page_idx": 5}, {"type": "table", "img_path": "kuCY0mW4Q3/tmp/a2937424da887adb1a31ca84749987e3fe5e9ab9cbba2a33dc68de774540b1fd.jpg", "table_caption": ["Table 1: Results with $\\mathbf{RoBERTa_{\\mathrm{base}}}$ and $\\mathrm{RoBERTa}_{\\mathrm{large}}$ on the GLUE benchmark. The best results in each group are shown in bold. We report Matthew\u2019s correlation for CoLA, Pearson correlation for STS-B, and accuracy for all other datasets. Results for $\\mathrm{LoRA}_{\\mathrm{qv}}$ and $\\mathrm{VeRA}_{\\mathrm{qv}}$ are sourced from their respective original papers, while the other results are based on our implementations. We report the median performance from 5 runs using different random seeds. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Our experiments are performed with $\\mathrm{RoBERTa_{base}}$ and $\\mathrm{RoBERTa}_{\\mathrm{large}}$ [Liu et al., 2019]. While LoRA and VeRA only finetune the query and value modules, we explore two fine-tuning strategies: query and value only $(\\mathrm{VB-LoRA}_{\\mathrm{qv}})$ ), and all linear modules $\\mathrm{{(VB-LoRA_{\\mathrm{{all}}})}}$ ), including $W_{q}$ , $W_{k}$ , $W_{v}$ , $W_{o}$ , $W_{\\mathrm{up}}$ , and $W_{\\mathrm{down}}$ . We create a vector bank of 90 vectors of a length of 256, initialized with a uniform distribution $\\mathcal{U}(-0.02,0.02)$ . The logits are initialized with a normal distribution $\\mathcal{N}(0,0.01)$ . The learning rates for the vector bank and logit parameters are set to 0.001 and 0.01, respectively. We set the rank to 4 and $k=2$ for all our experiments. ", "page_idx": 6}, {"type": "text", "text": "Table 1 reveals that VB-LoRA achieves competitive or superior performance compared to VeRA and Tied-LoRA, while being more parameter efficient. For example, when fine-tuning the query and value modules on the RoBERTa $\\mathrm{large}$ model, our method reduces the stored parameters to less than $40\\%$ of those required by VeRA or Tied-LoRA, while outperforming them across all tasks. These results suggest that model performance depends not only on the quantity of trainable parameters but also on how they are composed. ", "page_idx": 6}, {"type": "text", "text": "Moreover, the results consistently indicate that fine-tuning all modules, beyond just the query and value modules, enhances performance for all the methods. However, LoRA, VeRA and Tied-LoRA requires 2\u20134 times of the parameters in this case because their parameter counts increase linearly with the number of fine-tuned modules. In contrast, our method uses only $37.5\\%$ additional parameters as we maintain the same vector bank size but add additional parameters for indices and top- ${\\cdot k}$ weights. Thus, with only $12.8\\%$ of the parameters compared to $\\mathrm{VeRA_{\\mathrm{all}}}$ ( $4\\%$ compared to $\\mathrm{LoRA}_{\\mathrm{qv}}$ ), our method achieves the best average performance. ", "page_idx": 6}, {"type": "text", "text": "4.2 Natural Language Generation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For natural language generation experiments, we fine-tune the GPT-2 Medium and Large models [Radford et al., 2019] on the E2E dataset4 [Novikova et al., 2017], which contains approximately 42,000 training examples, 4,600 validation examples, and 4,600 test examples from the restaurant domain. We use a vector bank of size 256 for GPT-2 Medium and 350 for GPT-2 Large. The vector length is set to 256 and the rank is set to 4 for both models. To achieve the best performance, we fine-tune all attention layers and FFN layers. As shown in Table 2, our approach achieves competitive performance compared to VeRA, while requiring about $20\\%$ less stored parameters for both models. ", "page_idx": 6}, {"type": "table", "img_path": "kuCY0mW4Q3/tmp/632976dc9f597507d06771997ea23ed37bc631514ab90f10b5c7f87211be3072.jpg", "table_caption": ["Table 2: Results with GPT-2 Medium and GPT-2 Large on the E2E benchmark. The results for FT and LoRA are taken from Hu et al. [2021], and the results for VeRA are taken from Kopiczko et al. [2024]. We report the mean of 3 runs using different random seeds. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Instruction Tuning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Instruction tuning is a process of fine-tuning model with a set of instructions or prompts to enhance its performance on specific instructions [Ouyang et al., 2022]. We first experiment on a general instruction tuning dateset. We use the Cleaned Alpaca Dataset 5, which improves the data quality of the original Alpaca dataset [Taori et al., 2023]. We evaluate the fine-tuned models on the MTBench6 [Zheng et al., 2024], which contains 80 multi-turn questions. ", "page_idx": 7}, {"type": "text", "text": "Following Kopiczko et al. [2024], we fine-tune the Llama2 model [Touvron et al., 2023] within the QLoRA [Dettmers et al., 2023] framework7, which aims to reduce memory usage when finetuning large language models on a single GPU. We utilize the quantization strategy provided by QLoRA, including 4-bit NormalFloat for storage data, BFloat16 for computation parameters, double quantization and paged optimizers to train it on a single GPU. Our fine-tuned models generate responses to these questions, and subsequently, GPT-4 is employed to review and evaluate the generated answers, assigning a quantitative score on a scale of 10. Note that aligning with VeRA, we report the score of the first turn of the conversation. Following Kopiczko et al. [2024], we apply VB-LoRA to all linear layers except the top one. For Llama2 7B, we use a vector bank of 2,048 vectors, each with a length of 256, and the rank is set to 4, resulting in a total of $0.8\\mathrm{M}$ stored parameters. For Llama2 13B, we use the same-sized vector bank but increase the rank to 6, leading to 1.1M stored parameters. For all the experiments, we train for one epoch. ", "page_idx": 7}, {"type": "text", "text": "The results are reported in Table 3. Notably, we report two sets of LoRA results for each experiment: one from our implementation and the other from Kopiczko et al. [2024], due to a noticeable discrepancy between the scores. Since we closely follow the experimental settings of Kopiczko et al. [2024], we speculate that the difference is due to changes in the GPT-4 model over time. However, comparing the relative improvements of VeRA and VB-LoRA with their respective implementations of LoRA remains fair. VB-LoRA achieves higher scores than LoRA while using only $0.5\\%$ (Llama2 7B) and $0.4\\%$ (Llama2 13B) of the stored parameters. While VeRA can reach similar scores with their implementation of LoRA, it requires more than twice of parameters compared to VB-LoRA. ", "page_idx": 7}, {"type": "text", "text": "4.4 Mathematical Reasoning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To evaluate mathematical reasoning capabilities, we fine-tune the Mistral-7B-v0.1 and Gemma-7B models on the MetaMathQA8 [Yu et al., 2023] dataset and test them on $\\mathrm{GSM8K^{9}}$ [Cobbe et al., 2021] and $\\mathrm{MATH^{10}}$ [Hendrycks et al., 2021] datasets. We compare our results with the concurrent work LoRA-XS [Ba\u0142azy et al., 2024], following its experimental configuration. The result is shown in Table 4. Our method outperforms all baselines on GSM8K, with Mistral-7B utilizing only $0.4\\%$ of ", "page_idx": 7}, {"type": "text", "text": "Table 3: Results with Llama2 on MT-Bench, scored by GPT-4 out of 10. LoRA\u2020 and VeRA are sourced from Kopiczko et al. [2024]. LoRA\u2021 and VB-LoRA are from our implementations. The discrepancy between LoRA\u2020 and $\\mathrm{LoRA^{\\ddagger}}$ may be due to changes in the GPT4 model over time. ", "page_idx": 8}, {"type": "table", "img_path": "kuCY0mW4Q3/tmp/4b40d2eaf6acbaeedd2aa67ac8296e9a44b131f44fb2f2e1b75f7c1ed94d3d7d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 4: Results with Mistral-7B and Gemma-7B models on the GSM8K and MATH Benchmarks. Specifically, in VB-LoRA, we use a vector bank size of 2,048 with $b=256$ , set the rank to 4, and train with a batch size of 128 for 2 epochs. The warm-up ratio is 0.02, and training uses a cosine learning rate scheduler, with an initial learning rate of 0.001 for the vector bank and 0.01 for the logits. The baseline results are taken from Ba\u0142azy et al. [2024]. ", "page_idx": 8}, {"type": "table", "img_path": "kuCY0mW4Q3/tmp/3a60584e72909fd47f653eb51f8a1378faa30b3b0f268df1e3c0a2e25ea41205.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "the parameters compared to LoRA, and Gemma-7B using just $0.3\\%$ . Compared with LoRA-XS, our method outperforms on both evaluation datasets while using $70\\%$ (Mistral-7B) and $83\\%$ (Gemma-7B) of LoRA-XS parameters. ", "page_idx": 8}, {"type": "text", "text": "4.5 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct an ablation study to examine the impact of each individual component of VB-LoRA. The experiments are performed on RoBERTa-large, fine-tuning only the query and value modules. ", "page_idx": 8}, {"type": "text", "text": "Vector Selection Methods Besides the top- $\\cdot k$ admixture module (abbreviated as Top- $k$ below), there exist several commonly used discrete optimization methods for vector selection, including Noisy Top- $k$ [Shazeer et al., 2016], Gumbel-Softmax (GS), and Straight-Through Gumbel-Softmax [Jang et al., 2017, Maddison et al., 2016]. For Top- $k$ and Noisy Top- $k$ , we evaluate the impact of different $k$ to the performances on the CoLA dataset. For GS and Straight-Through GS, we set the temperature $\\tau=1/3$ during training and use Top-1 and Top-2 Softmax for inference. Additionally, we explore \"Select All\", a special case of Top- $k$ with $k$ equals to the vector bank size $h$ . As shown in Table 5, Noisy Top- $k$ , GS, and Straight-Through GS significantly underperform Top- $k$ and \"Select All\". We hypothesize that random noise injected by these methods likely disrupts the parameters of vector bank, leading to instability in the learning process. ", "page_idx": 8}, {"type": "text", "text": "We further investigate the impact of $k$ to the training dynamics and performance of VB-LoRA. As discussed in Sec. 3.4, the choice of $k$ affects not only the model\u2019s performance but also the number of parameters to be stored. Hence, a smaller $k$ is generally preferred for improved parameter efficiency. Table 5 shows that $k=2$ yields the best result on CoLA, whereas $k=1$ performs significantly worse. To explain this, we delve into the training dynamics of VB-LoRA. As shown in Figure 3 (a), when $k=1$ , the selected vectors remain largely unchanged during training. In contrast, when $k>1$ , the model actively explore the vector bank as illustrated in Figure 3 (b) and (c), i.e., different vectors are selected and updated actively during the training process. Additionally, we observed that this vector exploration primarily occurs in the early stages of training, with updates becoming progressively sparser in later stages, as shown in Figure 5 in the appendix. This suggests that the vectors become increasingly specialized for specific sub-vectors as training progresses. ", "page_idx": 8}, {"type": "text", "text": "Sub-vector Length $b$ VB-LoRA introduces a new virtual dimension that divides the original dimensions of LoRA matrices into sub-vectors of length $b$ . Note that $b$ must be a common factor of all hidden dimensions to ensure compatibility across the entire model. However, the optimal value of $b$ is task-specific and requires tuning as a hyperparameter. Theoretically, with a fixed vector bank budget, a larger $b$ reduces the number of vectors in the vector bank, potentially making each vector less specialized. On the other hand, a smaller $b$ increases the number of trainable parameters and complicates the vector selection process. As shown in Table 6, a moderate $b=256$ yields the best performance on the CoLA task. ", "page_idx": 8}, {"type": "table", "img_path": "kuCY0mW4Q3/tmp/f06f1612bb5f360ce3ff41662b1b99e1c32cb264ac5a9ec7d0e46122d89d20f1.jpg", "table_caption": ["Table 5: Ablation study of different vector selection methods. S.: Softmax, GS: Gumbel-Softmax, ST-GS: Straight Through Gumbel-Softmax. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "kuCY0mW4Q3/tmp/1c5ddcfd266f9982fba2a0a9d0fe632a41d823a4922976f4e3045fa3c293edbe.jpg", "table_caption": ["Table 6: Ablation study of sub-vector length. "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "kuCY0mW4Q3/tmp/04aeee0ccbc9c34a7e7f5d09df67ce04c1bc6b8f47156e5e8f6207f6e9af583a.jpg", "img_caption": ["Figure 3: VB-LoRA\u2019s vector selection footprints during training. The ${\\bf X}$ -axis represents the 96 sub-vectors formed by the vectors from a bank of 90 vectors, while the y-axis represents the indices of selected vectors from the bank. The blue blocks indicate the selection footprint during training. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces a \"divide-and-share\" paradigm and a differentiable top- $k$ admixture module for extreme parameter-efficient fine-tuning with vector banks. Our proposed VB-LoRA achieves the competitive or higher accuracy while using significantly smaller number of stored parameters compared to the state-of-the-art PEFT methods, including LoRA, VeRA, and Tied-LoRA. In addition, VB-LoRA is model-agnostic and applicable to other PEFT methods [Ding et al., 2023], including inserted adapters [Karimi Mahabadi et al., 2021], prompt tuning [Qin et al., 2021], and BitFit [Ben Zaken et al., 2022]. Although VB-LoRA focuses on reducing the storage and transmission costs for LLM fine-tuning, we believe the proposed scheme can be extended to memory-efficient fine-tuning and parameter-efficient pre-training. We leave these for future exploration. ", "page_idx": 9}, {"type": "text", "text": "Fine-tuning a pre-trained model requires making design choices about which layers of the model should be frozen or updated. Multitask fine-tuning adds extra complexity about which parameters should be shared or task-specific. Along this line of work, Polytropon [Ponti et al., 2022] jointly learns a small inventory of LoRA adapters and a routing function that selects a variable-sized subset of adapters for few-shot adaptation. Caccia et al. [2023] emphasize the importance of routing granularity and further propose a finer-grained mixing across multiple heads. Following these works, it would be interesting to explore a finer-grained parameter transfer across tasks, heads, types, and layers at the sub-vector level for multitask fine-tuning. ", "page_idx": 9}, {"type": "text", "text": "Limitations and broader impacts Our experiments are limited to monomodal (text-based), monolingual (English), and LoRA-only settings. Additionally, our exploration of the vector bank is somewhat limited, as we only examine a small range of configurations for bank size and vector length. In terms of broader impacts, VB-LoRA reduces the storage and transmission costs of LLM adapters and demonstrates improved memory-efficiency, making customized LLMs more accessible. We do not foresee any negative societal impact beyond those generally associated with LLMs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank the anonymous reviewers for their comments and suggestions, which helped improve the quality of this paper. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255, 2020. ", "page_idx": 10}, {"type": "text", "text": "Sanjeev Arora, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu, and Michael Zhu. A practical algorithm for topic modeling with provable guarantees. In International Conference on Machine Learning, pages 280\u2013288. PMLR, 2013. ", "page_idx": 10}, {"type": "text", "text": "Klaudia Ba\u0142azy, Mohammadreza Banaei, Karl Aberer, and Jacek Tabor. LoRA-XS: Low-rank adaptation with extremely small number of parameters. arXiv preprint arXiv:2405.17604, 2024.   \nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient finetuning for transformer-based masked language-models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1\u20139, Dublin, Ireland, May 2022.   \nAlexander Borzunov, Max Ryabinin, Artem Chumachenko, Dmitry Baranchuk, Tim Dettmers, Younes Belkada, Pavel Samygin, and Colin A Raffel. Distributed inference and fine-tuning of large language models over the internet. Advances in Neural Information Processing Systems, 36, 2024.   \nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877\u20131901, 2020.   \nLucas Caccia, Edoardo Ponti, Zhan Su, Matheus Pereira, Nicolas Le Roux, and Alessandro Sordoni. Multi-head adapter routing for cross-task generalization. In Advances in Neural Information Processing Systems, 2023.   \nDaniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Steven Bethard, Marine Carpuat, Marianna Apidianaki, Saif M. Mohammad, Daniel Cer, and David Jurgens, editors, Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1\u201314, Vancouver, Canada, August 2017.   \nTsung-Han Chan, Chong-Yung Chi, Yu-Min Huang, and Wing-Kin Ma. A convex analysis-based minimum-volume enclosing simplex algorithm for hyperspectral unmixing. IEEE Transactions on Signal Processing, 57(11):4418\u20134432, 2009.   \nAndrzej Cichocki. Era of big data processing: A new approach via tensor networks and tensor decompositions. arXiv preprint arXiv:1403.2048, 2014.   \nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of quantized LLMs. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 10088\u201310115, 2023.   \nChris HQ Ding, Tao Li, and Michael I Jordan. Convex and semi-nonnegative matrix factorizations. IEEE transactions on pattern analysis and machine intelligence, 32(1):45\u201355, 2008.   \nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence, 5(3):220\u2013235, 2023.   \nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005.   \nXiao Fu, Wing-Kin Ma, Kejun Huang, and Nicholas D Sidiropoulos. Blind separation of quasistationary sources: Exploiting convex geometry in covariance domain. IEEE Transactions on Signal Processing, 63(9):2306\u20132320, 2015.   \nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations, 2021.   \nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021.   \nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning, pages 2790\u20132799. PMLR, 2019.   \nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021.   \nDavid Inouye, Pradeep Ravikumar, and Inderjit Dhillon. Admixture of Poisson MRFs: A topic model with word dependencies. In International Conference on Machine Learning, pages 683\u2013691. PMLR, 2014.   \nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparametrization with Gumble-softmax. In International Conference on Learning Representations, 2017.   \nAlbert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.   \nShibo Jie and Zhi-Hong Deng. Fact: Factor-tuning for lightweight adaptation on vision transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 1060\u20131068, 2023.   \nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layers. Advances in Neural Information Processing Systems, 34:1022\u20131035, 2021.   \nBoris N Khoromskij. O (d log n)-quantics approximation of n-d tensors in high-dimensional numerical modeling. Constructive Approximation, 34:257\u2013280, 2011.   \nTamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3): 455\u2013500, 2009.   \nDawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. VeRA: Vector-based random matrix adaptation. In International Conference on Learning Representations, 2024.   \nQuoc Le, Tam\u00e1s Sarl\u00f3s, Alex Smola, et al. Fastfood-approximating kernel expansions in loglinear time. In International Conference on Machine Learning, volume 85, 2013.   \nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2021.   \nJun Li and Jos\u00e9 M Bioucas-Dias. Minimum volume simplex analysis: A fast algorithm to unmix hyperspectral data. In IGARSS 2008-2008 IEEE International Geoscience and Remote Sensing Symposium, volume 3, pages III\u2013250. IEEE, 2008.   \nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582\u20134597, 2021.   \nChia-Hsiang Lin, Wing-Kin Ma, Wei-Chiang Li, Chong-Yung Chi, and ArulMurugan Ambikapathi. Identifiability of the simplex volume minimization criterion for blind hyperspectral unmixing: The no-pure-pixel case. IEEE Transactions on Geoscience and Remote Sensing, 53(10):5530\u20135546, 2015.   \nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950\u20131965, 2022.   \nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \nChris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. In International Conference on Learning Representations, 2016.   \nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. Parameterefficient multi-task fine-tuning for transformers via shared hypernetworks. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 565\u2013576, 2021.   \nHanna Mazzawi, Javier Gonzalvo, Michael Wunder, Sammy Jerome, and Benoit Dherin. Deep fusion: Efficient network training via pre-trained initializations. In Forty-first International Conference on Machine Learning, 2024.   \nJekaterina Novikova, Ond\u02c7rej Du\u0161ek, and Verena Rieser. The E2E dataset: New challenges for end-toend generation. In Kristiina Jokinen, Manfred Stede, David DeVault, and Annie Louis, editors, Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 201\u2013206, Saarbr\u00fccken, Germany, August 2017.   \nJose Javier Gonzalez Ortiz, John Guttag, and Adrian V Dalca. Magnitude invariant parametrizations improve hypernetwork learning. In The Twelfth International Conference on Learning Representations, 2024.   \nIvan V Oseledets. Approximation of $2d\\times2d$ matrices using tensor decomposition. SIAM Journal on Matrix Analysis and Applications, 31(4):2130\u20132145, 2010.   \nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730\u201327744, 2022.   \nEdoardo M Ponti, Alessandro Sordoni, Yoshua Bengio, and Siva Reddy. Combining modular skills in multitask learning. arXiv preprint arXiv:2202.13914, 2022.   \nJonathan K Pritchard, Matthew Stephens, and Peter Donnelly. Inference of population structure using multilocus genotype data. Genetics, 155(2):945\u2013959, 2000.   \nYujia Qin, Xiaozhi Wang, Yusheng Su, Yankai Lin, Ning Ding, Jing Yi, Weize Chen, Zhiyuan Liu, Juanzi Li, Lei Hou, et al. Exploring universal intrinsic task subspace via prompt tuning. arXiv preprint arXiv:2110.07867, 2021.   \nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.   \nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable questions for SQuAD. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784\u2013 789, Melbourne, Australia, July 2018.   \nJoseph Reisinger, Austin Waters, Bryan Silverthorn, and Raymond J Mooney. Spherical topic models. In International Conference on Machine Learning, pages 903\u2013910. Citeseer, 2010.   \nAdithya Renduchintala, Tugrul Konuk, and Oleksii Kuchaiev. Tied-LoRA: Enhancing parameter efficiency of LoRA with weight tying. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 8686\u20138697, 2024.   \nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2016.   \nYing Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, et al. S-LoRA: Serving thousands of concurrent LoRA adapters. arXiv preprint arXiv:2311.03285, 2023.   \nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew $\\mathrm{Ng}$ , and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA, October 2013.   \nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.   \nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \nLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 9(86):2579\u20132605, 2008.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.   \nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Tal Linzen, Grzegorz Chrupa\u0142a, and Afra Alishahi, editors, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\u2013355, Brussels, Belgium, November 2018.   \nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625\u2013641, 2019.   \nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023.   \nXujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Li Yun, Hejie Cui, Zhang Xuchao, Tianjiao Zhao, et al. Domain specialization as the key to make large language models disruptive: A comprehensive survey. arXiv preprint arXiv:2305.18703, 2023.   \nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging LLM-as-a-judge with MT-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Hyperparameters and Computing Resources ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The hyperparameters used for the natural language understanding, natural language generation and instruction tuning are provided in Table 7, 8 and 9. All experiments were conducted on a server equipped with 8 NVIDIA A100 80GB GPUs. ", "page_idx": 14}, {"type": "text", "text": "Computation overhead The proposed factorization in VB-LoRA is simple to implement in modern deep learning frameworks such as PyTorch, allowing us to fully leverage GPU acceleration. However, the use of subvector decomposition does introduce some computational overhead. This additional overhead is limited to the training phase and does not affect inference, as both LoRA and VB-LoRA merge their parameters back into the original model parameters during this stage. ", "page_idx": 14}, {"type": "text", "text": "Memory efficiency Despite the training time overhead, the reduced number of trainable parameters in VB-LoRA results in lower memory consumption. During LoRA fine-tuning, the forward pass is $z=A x$ , $H=B z$ , without the need to materialize $\\Delta W$ . This memory-saving technique can be seamlessly incorporated in VB-LoRA and has been implemented in our source code. Table 9 shows that VB-LoRA requires approximately $15\\%{-}20\\%$ more training time than LoRA, while it consumes less memory than LoRA in both the LLaMA2 7B model and LLaMA2 13B models. ", "page_idx": 14}, {"type": "table", "img_path": "kuCY0mW4Q3/tmp/66031bc7e5d5242988b1170d174292a210b053d559eb7bbfcafa717326523342.jpg", "table_caption": ["Table 7: Hyperparameters and computing resources for natural language understanding experiments on the GLUE benchmark. Training time and GPU memory are reported as \"query and value only\" / \"all linear modules\". h: hour, m: minute. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.2 Visualization of the Vector Selection ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For visualization, we conducted experiments on the CoLA dataset using a 24-layer RoBERTa-large model with a vector bank of 30 vectors. We fine-tuned the query and value modules, setting the rank to 2 and the vector length to 1024, resulting in 192 sub-vectors. ", "page_idx": 14}, {"type": "text", "text": "Table 8: Hyperparameters and computing resources on natural language generation experiments on the E2E dataset. Training time and GPU memory are reported as \"query and value only\" / \"all linear modules\". h: hour, m: minute. ", "page_idx": 15}, {"type": "table", "img_path": "kuCY0mW4Q3/tmp/f4e55fb97edef0245bde4a5ea77931dd69bf05aa89ad03a5a893ad2722e3ff23.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "kuCY0mW4Q3/tmp/2d352e2faeece08117acffd0d8aac014228580200db74613265075a5ef37f02a.jpg", "table_caption": ["Table 9: Hyperparameters and computing resources on instruction tuning on the Cleaned Alpaca Dataset. h: hour. 7B: Llama2 7B, 13B: Llama2 13B. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 4 displays the vectors selected by sub-vectors at the initialization (red) and at the end of training (blue), respectively. As we can see, most of the final selections differ from the initial selections, demonstrating the training dynamics of the vector selection process. ", "page_idx": 15}, {"type": "text", "text": "In Figure 5, we plot the footprint at different training periods. This visualization demonstrates that vector exploration predominantly occurs in the early stages of training, and the updates become progressively sparser in the later stages of training. ", "page_idx": 15}, {"type": "text", "text": "Figure 6 illustrates the sum of the top- $\\cdot\\mathbf{k}$ weights for each vector, grouped by the first, middle, and last 8 layers. It shows that certain vectors are favored by deeper layers, such as vectors #1 and $\\#29$ , while some are favored by shallower layers, such as vectors $\\#20$ and $\\#26$ . ", "page_idx": 15}, {"type": "text", "text": "We then group the same data with respect to query and value modules, as well as matrices A and B, shown in Figure 7. As we can see, some vectors are predominantly utilized by specific module or matrix types. For instance, vector $\\#23$ is heavily utilized in the formation of matrix A, while vector $\\#29$ is predominantly used in the formation of Query modules. ", "page_idx": 15}, {"type": "text", "text": "Load balancing To demonstrate that the vector selection is free from load balancing issue, we present the vector usage in a Gemma-7B model trained on the MetaMathQA dataset, as shown in ", "page_idx": 15}, {"type": "image", "img_path": "kuCY0mW4Q3/tmp/9f46e919bd5f93462cff32c677c5875dc573e185fcbff687cd97fabd7f985e7b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 4: The $\\mathbf{X}$ -axis represents the 192 sub-vectors formed by the vectors in the vector bank, while the y-axis represents the 30 vectors in the vector bank. The vectors initially selected by each sub-vector are shown in red, the vectors finally selected are shown in blue, and the overlapping vectors are shown in green. ", "page_idx": 16}, {"type": "image", "img_path": "kuCY0mW4Q3/tmp/871952057ec35ca9fc76479009a14798114eeb33ad23ae70893e394ad2cd349e.jpg", "img_caption": ["Figure 5: VB-LoRA\u2019s vector selection footprints during training. The ${\\bf X}$ -axis represents the 96 sub-vectors formed by the vectors from a bank of 90 vectors, while the y-axis represents the indices of selected vectors from the bank. The blue blocks indicate the selection footprint during training. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 8. The vector bank contains 2048 vectors. The distribution of vector usage follows a roughly normal distribution, with most vectors being selected between 40 to 55 times. ", "page_idx": 16}, {"type": "text", "text": "A.3 Visualization of the Vector Bank and the Sub-vectors ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Figure 9 illustrates the positioning of the sub-vectors along the edge of the simplex spanned by the vector bank. The vector bank is projected into a 2-D space using T-SNE [van der Maaten and Hinton, 2008] for visualization. ", "page_idx": 16}, {"type": "image", "img_path": "kuCY0mW4Q3/tmp/496ca4451c2fc569f1055c6184ffdff582fe98df945a835e4b005113a4ab1db2.jpg", "img_caption": ["Figure 6: The sum of the top- $k$ weights for each vector, grouped by the first, middle, and last 8 layers. The vectors in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ are sorted by their norms. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "kuCY0mW4Q3/tmp/3e49499ea7bdc72aee6678bc0052fa0cb9b83610ad62d5ff94c4e8ca4a85fd3f.jpg", "img_caption": ["Figure 7: The sum of the top- $k$ weights for each vector, grouped by query (Q) and value (V) modules, and matrices A and B. The vectors in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ are sorted by their norms. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "kuCY0mW4Q3/tmp/56c3944473e1058f74b2097cbf97572fb85c02c1864a69c71d31b014fd676151.jpg", "img_caption": ["Figure 8: Histogram of vector usage frequency. The frequency ranges from 29 to 70, with most vectors being selected between 40 and 55 times. The distribution of vector usage follows an approximately normal pattern. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "kuCY0mW4Q3/tmp/212348944e948ad86b46efd7e6b3d040cb5ffc809c2f3cef45d15e915f1a8e36.jpg", "img_caption": ["Figure 9: Visualization of the learned vector bank and sub-vectors. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "A.4 Instruction-tuning Examples ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Tables 10 and 11 provide examples of the responses generated by the finetuned Llama2 13B model with LoRA and VB-LoRA. ", "page_idx": 20}, {"type": "table", "img_path": "kuCY0mW4Q3/tmp/b670d2dbb1d59dd6b64477831789da3170a24c138a0d060030b9fc2e0028426c.jpg", "table_caption": ["Table 10: Example #1 of the responses generated by the finetuned Llama2 13B model with LoRA and VB-LoRA. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "kuCY0mW4Q3/tmp/fb9a766257dbfc557d4dc0854346fdef8f1f46eb781e1d3955616c1e89d981fe.jpg", "table_caption": ["Table 11: Example $\\#2$ of the responses generated by the finetuned Llama2 13B model with LoRA and VB-LoRA. Parts of answers are omitted due to their length. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The abstract and introduction include the claims made in the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We discussed \"Limitations\" in Section 5. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper fully disclosed all the information needed to reproduce the main experimental results, and we also provide the source code for reproducibility. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide the source code in the supplemental material. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The experimental setting is clearly stated in the main paper and the hyperparameters are listed in the appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Our main experiments were repeated 3 to 5 times, and we report the standard deviation of the results. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide related information in the main paper and appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We discuss the potential societal impacts in Section 5. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: No data or models are released as part of this work, so safeguards for responsible release are not applicable. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We cite the original paper and provide the name of the license for the datasets we used. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]