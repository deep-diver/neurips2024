[{"figure_path": "LjnDqVcrE9/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison between the training method and our training-free method. The training method typically requires a large amount of in-domain data for training and cannot generalize to out-of-domain prompts. In contrast, our method can easily adapt to prompts from a new domain in a training-free manner.", "description": "This figure compares traditional training methods for Multimodal Large Language Models (MLLMs) with the proposed training-free method.  The traditional method requires extensive in-domain training data and struggles with out-of-domain generalization. In contrast, the training-free approach leverages learnable latent variables to adapt to new domains without retraining, showcasing its flexibility and efficiency.", "section": "1 Introduction"}, {"figure_path": "LjnDqVcrE9/figures/figures_3_1.jpg", "caption": "Figure 2: The attention maps in various layers of the MLLMs, with the numbers indicating the respective layer indices. The top line visualizes the attention between the prompt token \u201chat", "description": "This figure shows the attention maps at different layers (L_0, L_7, L_15, L_23, L_31) of a Multimodal Large Language Model (MLLM).  The top row displays the attention between the word \"hat\" from the prompt and the visual tokens. The bottom row shows the attention between a context token (not specified in the caption) and the visual tokens. The attention maps highlight which parts of the image the model focuses on when processing the prompt. The varying attention patterns across different layers illustrate the multi-stage processing of the MLLM.", "section": "4.1 Analysis of the Attention in LVLMs"}, {"figure_path": "LjnDqVcrE9/figures/figures_4_1.jpg", "caption": "Figure 3: Manipulating attention with various methods: (a), (b), and (c) demonstrate the manipulation of the attention map by adding an adjustment coefficient \u03b7 on the attention map in the first step during the model inference. (d) illustrates the step-by-step editing approach. (e) showcases that optimizing a learnable context tokens (mentioned in Sec. 4.2) instead of visual tokens, while (f) presents the results of our method optimizing the learnable latent variable.", "description": "This figure compares different methods for manipulating attention in a Multimodal Large Language Model (MLLM).  (a)-(c) show the impact of directly adjusting the attention map with varying strengths (\u03b2) of a coefficient. (d) demonstrates a step-by-step adjustment approach. (e) shows the effect of optimizing learnable context tokens. Finally, (f) presents the results of the authors' proposed method using latent variable optimization.", "section": "Method"}, {"figure_path": "LjnDqVcrE9/figures/figures_5_1.jpg", "caption": "Figure 4: The overview of our method. With the provided visual prompt, we convert it into a mask, and compute the mask-based energy function between the mask and the pooled attention map. During the inference process, we conduct backpropagation to optimize a learnable latent variable. This process is executed at the 0-th step of model inference and iterated T times.", "description": "This figure illustrates the training-free visual prompt learning method proposed in the paper.  It shows how a visual prompt (e.g., a bounding box) is converted into a mask and used to compute a mask-based energy function.  This function measures the relationship between the mask and a pooled attention map (an average pooling of attention maps from multiple layers of the Multimodal Large Language Model). Backpropagation is then used to optimize a learnable latent variable, which is added to the visual tokens before feeding into the LLM.  This process is repeated multiple times (T iterations) at the 0th step of the inference process, allowing the model to effectively incorporate the visual prompt without retraining.", "section": "Method"}, {"figure_path": "LjnDqVcrE9/figures/figures_6_1.jpg", "caption": "Figure 5: The examples of referring MLLM with four types of visual prompt, including box (a), mask (b), scribble (c) and point (d). The correct referring expressions are marked in green, incorrect referring expressions are marked in red, and hallucinated expressions are marked in orange. Compared to the baseline model, our method enhances interpretability and controllability with visual prompts, while also helping the model mitigate hallucination issues.", "description": "This figure showcases four examples of referring MLLMs using different visual prompt types: box, mask, scribble, and point. Each example includes the input image and visual prompt, attention maps from both the baseline LLaVA model and the proposed ControlMLLM method, and the corresponding output text.  Correct, incorrect, and hallucinated output text are highlighted in green, red, and orange respectively. The figure demonstrates how the proposed method improves interpretability and controllability and reduces hallucinations when using visual prompts.", "section": "Experiments"}, {"figure_path": "LjnDqVcrE9/figures/figures_7_1.jpg", "caption": "Figure 1: Comparison between the training method and our training-free method. The training method typically requires a large amount of in-domain data for training and cannot generalize to out-of-domain prompts. In contrast, our method can easily adapt to prompts from a new domain in a training-free manner.", "description": "This figure compares traditional training-based multimodal large language model (MLLM) methods with the proposed training-free approach.  Traditional methods require extensive in-domain data for training and struggle to adapt to unseen prompts or domains. The authors' training-free method, however, is shown to readily handle prompts from new domains without additional training.", "section": "1 Introduction"}, {"figure_path": "LjnDqVcrE9/figures/figures_18_1.jpg", "caption": "Figure 7: The input examples of ROC Task.", "description": "This figure shows input examples for the Referring Object Classification (ROC) task. It compares the performance of several methods: LLaVA, LLaVA + Color, LLaVA + Blur, and LLaVA + Ours.  Each method is tested with different types of visual prompts: a simple question without a region specified, a question with a red bounding box around the region of interest, a blurred image with only the region of interest visible, and a question specifying the exact coordinates of the region. The goal is to evaluate how well each method can correctly classify the object within the specified region.", "section": "5.2 Applications"}, {"figure_path": "LjnDqVcrE9/figures/figures_18_2.jpg", "caption": "Figure 5: The examples of referring MLLM with four types of visual prompt, including box (a), mask (b), scribble (c) and point (d). The correct referring expressions are marked in green, incorrect referring expressions are marked in red, and hallucinated expressions are marked in orange. Compared to the baseline model, our method enhances interpretability and controllability with visual prompts, while also helping the model mitigate hallucination issues.", "description": "This figure shows four examples of referring expressions generated by a multimodal large language model (MLLM) using different types of visual prompts: box, mask, scribble, and point.  The results demonstrate that the proposed training-free method improves the model's ability to correctly identify and describe the referenced objects or regions within the image, reducing errors and hallucinations compared to the baseline model. The color coding highlights the correctness of the generated expressions (green: correct, red: incorrect, orange: hallucinated).", "section": "5.2 Applications"}, {"figure_path": "LjnDqVcrE9/figures/figures_20_1.jpg", "caption": "Figure 2: The attention maps in various layers of the MLLMs, with the numbers indicating the respective layer indices. The top line visualizes the attention between the prompt token \u201chat", "description": "This figure shows the attention maps at different layers (L0, L7, L15, L23, L31) of a Multimodal Large Language Model (MLLM). The top row displays the attention between the word \"hat\" in the prompt and the visual tokens. The bottom row shows the attention between a context token (not explicitly defined in the caption but implied by the image) and the visual tokens.  The attention maps highlight which parts of the image the model focuses on when processing the prompt and context. This illustrates how the MLLM integrates visual and textual information.", "section": "4.1 Analysis of the Attention in LVLMs"}, {"figure_path": "LjnDqVcrE9/figures/figures_20_2.jpg", "caption": "Figure 11: Impact of the size of visual prompt. The larger prompt size attains a better performance.", "description": "This figure shows the results of using different sizes of visual prompts in the proposed method. It demonstrates that using a larger visual prompt results in improved performance. The larger prompt size likely provides more context for the model to understand the image, leading to better results.", "section": "Experiments"}, {"figure_path": "LjnDqVcrE9/figures/figures_21_1.jpg", "caption": "Figure 2: The attention maps in various layers of the MLLMs, with the numbers indicating the respective layer indices. The top line visualizes the attention between the prompt token \u201chat", "description": "This figure shows the attention maps at different layers of a Multimodal Large Language Model (MLLM).  The top row highlights the attention between the word \"hat\" from the prompt and the visual tokens, illustrating how the model focuses on relevant image regions. The bottom row displays the attention between a contextual token (not specified) and the visual tokens, showcasing a broader context understanding of the image.  The different layers (L0, L7, L15, L23, L31) show how the attention shifts and refines as the model processes the information.", "section": "4.1 Analysis of the Attention in LVLMs"}, {"figure_path": "LjnDqVcrE9/figures/figures_21_2.jpg", "caption": "Figure 12: Comparing highlight text token and context text token based optimization.", "description": "This figure compares the results of using either highlight text tokens or context tokens for the optimization process in the paper's proposed training-free method for injecting visual prompts into Multimodal Large Language Models (MLLMs).  The table shows how the model's output changes over multiple optimization steps (T=0 to T=5) when using the two different token types.   It illustrates the impact of using averaged attention information ('context tokens') versus the attention from only the most relevant words ('highlight tokens') on the model's ability to accurately describe the visual scene, specifically focusing on the relationship between textual prompts and visual context.", "section": "4.2 Manipulating Attention via Latent Variable Learning"}, {"figure_path": "LjnDqVcrE9/figures/figures_22_1.jpg", "caption": "Figure 3: Manipulating attention with various methods: (a), (b), and (c) demonstrate the manipulation of the attention map by adding an adjustment coefficient \u03b7 on the attention map in the first step during the model inference. (d) illustrates the step-by-step editing approach. (e) showcases that optimizing a learnable context tokens (mentioned in Sec. 4.2) instead of visual tokens, while (f) presents the results of our method optimizing the learnable latent variable.", "description": "This figure compares different methods for manipulating attention in a multimodal large language model (MLLM).  It shows how adding a coefficient to the attention map (a-c), step-by-step editing (d), optimizing learnable context tokens (e), and optimizing a learnable latent variable (f) affect the model's output.  The figure highlights the proposed method's effectiveness in controlling attention and generating desired results.", "section": "4 Method"}, {"figure_path": "LjnDqVcrE9/figures/figures_22_2.jpg", "caption": "Figure 3: Manipulating attention with various methods: (a), (b), and (c) demonstrate the manipulation of the attention map by adding an adjustment coefficient \u03b7 on the attention map in the first step during the model inference. (d) illustrates the step-by-step editing approach. (e) showcases that optimizing a learnable context tokens (mentioned in Sec. 4.2) instead of visual tokens, while (f) presents the results of our method optimizing the learnable latent variable.", "description": "This figure compares different methods of manipulating attention in a multimodal large language model (MLLM).  It shows how adding a coefficient to the attention map (a-c), step-by-step editing (d), optimizing learnable context tokens (e), and optimizing a latent variable (f) impact the model's output.  The goal is to show the effectiveness of the proposed latent variable optimization method for controlling attention, thus improving the ability of the MLLM to focus on specific regions of interest within an image.", "section": "4 Method"}]