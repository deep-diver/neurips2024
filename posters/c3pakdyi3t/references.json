{"references": [{"fullname_first_author": "Edward J. Hu", "paper_title": "LoRA: Low-Rank Adaptation of Large Language Models", "publication_date": "2021-00-00", "reason": "This paper introduces the LoRA method, which is the core technique that the current paper builds upon and aims to improve."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "publication_date": "2023-00-00", "reason": "This paper introduces Llama 2, one of the two base large language models used in the experiments of the current paper."}, {"fullname_first_author": "Gemini Team", "paper_title": "Gemini: A family of highly capable multimodal models", "publication_date": "2024-00-00", "reason": "This paper introduces the Gemma family of models, the other base large language model used in the experiments of the current paper."}, {"fullname_first_author": "Ian Goodfellow", "paper_title": "Generative adversarial networks", "publication_date": "2020-00-00", "reason": "This paper introduces Generative Adversarial Networks (GANs), the core methodology used in the synthetic data generation part of the current paper."}, {"fullname_first_author": "Brian Lester", "paper_title": "The power of scale for parameter-efficient prompt tuning", "publication_date": "2021-00-00", "reason": "This paper introduces Prompt Tuning, one of the parameter-efficient fine-tuning methods compared to LoRA in the current paper."}]}