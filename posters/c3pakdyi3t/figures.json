[{"figure_path": "c3Pakdyi3t/figures/figures_1_1.jpg", "caption": "Figure 1: Trans-LoRA overview. Examples from 'boolean expressions' BBH task illustrate the lower diversity of raw synthetic samples compared to the original task data, which is fixed by our filtering approach. The source model is used to: 1. train the source LoRA; 2. synthesize data for discriminator training; and 3. train the (LoRA) discriminator. Then, the target model is used to synthesize data for transfer (filtered by discriminator) and train target LoRA using the source LoRA teacher.", "description": "This figure provides a high-level overview of the Trans-LoRA process. It shows how a source model is used to train a source LoRA and a discriminator on task data. Then, synthetic data is generated from both the source and target models, filtered by the discriminator to match the characteristics of the original task data, and used to transfer the LoRA to a new target model.", "section": "1 Introduction"}, {"figure_path": "c3Pakdyi3t/figures/figures_3_1.jpg", "caption": "Figure 2: Detailed breakdown of Trans-LoRA. Task Finetuning is done beforehand and produces the source LORA for the source model and the discriminator. Task Transfer utilizes the source LORA and discriminator to transfer the LORA onto the target model and produce the target LoRA.", "description": "This figure illustrates the Trans-LoRA process, which involves two main stages: Task Finetuning and Task Transfer.  In Task Finetuning, the source model (Ms) is fine-tuned on task data using a PEFT method (e.g., LoRA) to obtain the source LoRA (\u03b8s) and a discriminator LORA (Mdisc). The discriminator is trained using synthetic data generated by the source model and a subset of the original task data, to filter the subsequent synthetic data. In Task Transfer, a new target model (Mt) is used with the source LoRA and discriminator.  The source model generates synthetic data, filtered by the discriminator to train the target LoRA (\u03b8t). This allows for transferring the LoRA from the source model to the target model without needing access to the original task data.", "section": "3 Trans-LORA"}, {"figure_path": "c3Pakdyi3t/figures/figures_4_1.jpg", "caption": "Figure 3: Transferred LoRA accuracy vs. source LoRA accuracy on BBH tasks. Details the rows of Table 1. Bottom left: row 3; Bottom right: row 4.", "description": "This figure shows the accuracy of transferred LoRA models compared to the accuracy of the original source LoRA models on the BigBench-Hard (BBH) tasks.  Each point represents a specific task, showing the relationship between the performance of the source LoRA and the performance of the transferred LoRA. The dashed line indicates where the transferred LoRA accuracy equals the source LoRA accuracy, with points above it indicating improved performance after transfer and those below indicating reduced performance. The caption refers to specific rows in Table 1, indicating which sets of source and target model combinations the points represent. This visualization helps assess the effectiveness of Trans-LoRA in maintaining or improving LoRA performance during transfer.", "section": "4 Experiments"}, {"figure_path": "c3Pakdyi3t/figures/figures_5_1.jpg", "caption": "Figure 4: Transferred LoRA accuracy vs. source LORA accuracy on MMLU tasks. Details the rows of Table 2. Bottom left: row 3; Bottom right: row 4.", "description": "This figure shows four scatter plots, each comparing the accuracy of the transferred LoRA model (y-axis) against the accuracy of the source LoRA model (x-axis) on the MMLU benchmark. Each plot represents a different combination of source and target models, as indicated in the plot title.  The diagonal dashed line indicates perfect transfer (the transferred LoRA model has the same accuracy as the source LoRA model). The majority of points are above this line, demonstrating successful transfer and often an improvement in performance.", "section": "4 Experiments"}, {"figure_path": "c3Pakdyi3t/figures/figures_5_2.jpg", "caption": "Figure 5: Scaling the number of synthetic samples generated through Trans-LoRA. Total training iterations in each experiment are kept identical for fair comparison. Done on BBH with Gemma-2b to Gemma-7b transfer and Gemma-2b as discriminator.", "description": "The figure shows how the accuracy of the transferred LoRA model changes as the number of filtered synthetic samples used for training increases.  The experiment was conducted on the BigBench-Hard (BBH) dataset, transferring a LoRA model from a Gemma-2b base model to a Gemma-7b base model, using a Gemma-2b discriminator. The results demonstrate that increasing the amount of filtered synthetic data improves the performance of the transferred LoRA model. The number of training iterations is held constant across different sample sizes to ensure a fair comparison.", "section": "4 Experiments"}, {"figure_path": "c3Pakdyi3t/figures/figures_9_1.jpg", "caption": "Figure 7: T-SNE plot of MPNet embeddings from us_foreign_policy (MMLU) dataset; Red points are our filtered synthetic data, blue points are real data.", "description": "This figure shows a t-SNE plot visualizing the high dimensional data distribution of both real and synthetic data generated by the Trans-LoRA model.  The plot helps to visually assess the similarity between the two distributions.  Red points represent the synthetic data generated by Trans-LoRA after filtering, and blue points represent the real data used for training.  The proximity of the red and blue points suggests how well the synthetic data distribution resembles the real data distribution, indicating the effectiveness of the Trans-LoRA's synthetic data generation and filtering process for effectively mimicking the properties of real data, which is crucial for successful LoRA transfer.", "section": "5 Analysis"}]