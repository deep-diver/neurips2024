[{"type": "text", "text": "Edit Distance Robust Watermarks for Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Noah Golowich Ankur Moitra nzg@mit.edu moitra@mit.edu MIT MIT ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Motivated by the problem of detecting AI-generated text, we consider the problem of watermarking the output of language models with provable guarantees. We aim for watermarks which satisfy: (a) undetectability, a cryptographic notion introduced in [CGZ24] which stipulates that it is computationally hard to distinguish watermarked language model outputs from the model\u2019s actual output distribution; and (b) robustness to channels which introduce a constant fraction of adversarial insertions, substitutions, and deletions to the watermarked text. Earlier schemes could only handle stochastic substitutions and deletions, and thus we are aiming for a more natural and appealing robustness guarantee that holds with respect to edit distance. ", "page_idx": 0}, {"type": "text", "text": "Our main result is a watermarking scheme which achieves both undetectability and robustness to edits when the alphabet size for the language model is allowed to grow as a polynomial in the security parameter. To derive such a scheme, we follow an approach introduced in [CG24], which proceeds via first constructing pseudorandom codes satisfying undetectability and robustness properties analogous to those above; our key idea is to handle adversarial insertions and deletions by interpreting the symbols as indices into the codeword, which we call indexing pseudorandom codes. Additionally, our codes rely on weaker computational assumptions than used in previous work. Then we show that there is a generic transformation from such codes over large alphabets to watermarking schemes for arbitrary language models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The rapid increase in AI-generated content represents a significant challenge for numerous societal institutions, ranging from education to social media. For instance, the ability of large language models such as GPT-4 to generate long bodies of text such as essays raises the possibility of increasing amounts of plagiarism, and the proliferation of AI-generated text, images, and videos on social media presents challenges regarding large-scale manipulation of online audiences. An important tool at our disposal in preventing the misuse of such content is watermarking schemes, which are procedures that embed hidden patterns in AI-produced content using a secret key. Watermarking schemes allow a detection algorithm with the aid of the secret key to determine with high probability that the content was produced by the AI model. They do so without noticeably altering the content from the perspective of any algorithm not possessing the secret key. ", "page_idx": 0}, {"type": "text", "text": "Despite a number of recently proposed watermarking schemes, taking both a theoretical perspective (e.g., [CGZ24, CG24, Zam24, $\\bar{\\mathrm{FGJ}}^{+}23]$ ) as well as a more empirical one (e.g., $[\\mathrm{KGW}^{+}23$ , $\\mathrm{KGW}^{+}24$ , KTHL23]), a crucial challenge that remains elusive is that of ensuring the watermark be robust to adversaries which can modify the generated content. A watermarking scheme is of little use if it is too easy to change the model\u2019s output so as to remove the watermark. On the other hand, a sufficiently resourceful adversary can simply train their own unwatermarked model. Thus it is necessary to strike a balance in terms of the power of adversaries to which a watermarking scheme enjoys robustness. In this paper, we give the first watermarking schemes with provable robustness to adversaries that make a constant fraction of arbitrary substitutions, insertions, and deletions to the output. This is a substantial improvement over previous schemes [CG24], which could only tolerate a constant fraction of adversarial substitutions or a constant fraction of i.i.d. deletions under additional stochasticity assumptions on the language model. Our results therefore represent progress towards the overarching goal of constructing watermarking schemes which are robust to resource-limited adversaries. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1.1 Setup: watermarking schemes ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider the task of watermarking the output of a language model Model, which is simply defined as a mapping from a sequence of tokens $\\mathrm{t}_{1},\\dotsc..,\\mathrm{t}_{i-1}$ to a distribution over the next token.1 Though we use the terminology \u201clanguage model\u201d and \u201ctext\u201d throughout the paper, our framework can be used to model a host of autoregressive models including those for generation of images, audio, and videos (e.g., $[\\mathrm{YXK}^{+}22$ , $\\mathrm{TJY}^{+}2\\bar{4}]$ ) in addition to text. As this paper is theoretical in nature, we work only with this abstraction of an autoregressive model, keeping in mind that single tokens could represent, e.g., sequences of letters [SHB16] or discretized patches of an image $[\\mathbf{Y}\\mathbf{L}\\mathbf{\\bar{K}}^{+}22$ , RvdOV19].2 ", "page_idx": 1}, {"type": "text", "text": "A watermarking scheme $\\mathcal{W}$ for Model consists of a tuple of efficient algorithms, ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathcal{W}=(\\mathsf{S e t u p},\\mathsf{W a t},\\mathsf{D e t e c t})\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where Setup generates a secret key at random, Wat uses Model and the secret key to produce a sequence of text with a watermark embedded in it, and Detect uses the secret key to determine whether a given sequence of text is watermarked. Moreover, Detect has no knowledge of Model. $\\mathcal{W}$ should satisfy the following three properties: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Undetectability [CGZ24]: Text generated from the watermarking protocol Wat should be computationally indistinguishable to text generated by the true model Model, to any polynomial-time algorithm which can repeatedly query either Wat or Model. (Other notions, namely distortionfreeness [KTHL23], have been considered in place of undetectability, but are significantly weaker; see Example A.1.) ", "page_idx": 1}, {"type": "text", "text": "\u2022 Soundness: Any fixed sequence of text (not produced by Wat) should not be detected as watermarked, with high probability (over the generation of the secret key). ", "page_idx": 1}, {"type": "text", "text": "\u2022 Edit-Robustness: A sequence which can be obtained by calling the watermarking procedure Wat and then making a constant fraction of adversarial edits (i.e., substitutions, insertions, and deletions) to its output will still be detected as watermarked by Detect with high probability. ", "page_idx": 1}, {"type": "text", "text": "Whereas several existing watermarking schemes achieve undetectability and soundness, the main contribution of our work is to achieve the notion of edit-robustness given above. Robustness, broadly construed, has long been recognized as essential for constructing useful watermarking schemes $[\\mathbf{A}\\mathbf{R}\\mathbf{C}^{+}01$ , $\\mathrm{ARH}^{+}\\bar{0}2]$ , yet nearly all of the existing provable guarantees on watermarking do not yield robustness to any constant fraction of edits. We remark that some works (e.g., [CGZ24]) can handle a subconstant fraction of edits. However, we argue that the right notion of adversary is that which makes a constant fraction of edits: there is a relatively low bar for a malicious adversary attempting to remove the watermark by changing the text at a constant rate while avoiding significant quality degradations, such as randomly replacing $5\\%$ of words with a synonym. Moreover, even if an adversary is not actively trying to remove a watermark, it is reasonable to expect that an \u201chonest\u201d editor who is merely trying to improve the text\u2019s quality will introduce edits at a constant rate. The lens of coding theory provides further motivation for recovering from such channels that introduce a constant fraction of errors: doing so is a longstanding gold standard in the field [GRS19]. ", "page_idx": 1}, {"type": "text", "text": "The only prior work in the literature which obtains watermarking schemes with provable robustness guarantees against such \u201cconstant-rate\u201d attacks was [CG24], which constructed watermarking schemes robust to a constant fraction of deletions and substitutions. Moreover, in order to handle any deletions at all, [CG24] needs to assume that: (a) the deletions and substitutions are made independently at each position with some fixed probability; and (b) the language model is equivalent to a binary symmetric channel in a certain sense, a strong assumption which is contradicted by decades of study in linguistics, which posit that earlier words strongly influence the distribution of subsequent words [Bib09].3 ", "page_idx": 2}, {"type": "text", "text": "Finally, we emphasize that the task of obtaining robustness against adversaries that can make a constant fraction of insertions, deletions, and substitutions, as opposed to merely substitutions, is wellmotivated by an extensive line of work on edit distance (e.g., [OR07, AKO10, AO11, AN20], amongst many others). The edit distance between two strings is the minimum number of insertions, deletions, and substitutions needed to transform one string into the other. Edit distance has found numerous applications in areas ranging from computational biology to information retrieval [Nav01]. This fundamental role of edit distance results from the fact that many natural processes induce small editdistance perturbations on strings: for instance, mutations of DNA can involve insertions or deletions as well as substitutions, as do the changes people typically make to documents. Procedures that make such changes therefore represent a reasonable adversary in our present setting of watermarking. ", "page_idx": 2}, {"type": "text", "text": "1.2 Main result ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our main result states that watermarking schemes with all of the aforementioned properties exist under a standard cryptographic assumption: ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.1 (Informal version of Theorem E.2). Suppose that local weak PRFs exist (per Assumption 3.1). Then for any security parameter $\\lambda\\in\\mathbb N$ , there is a watermarking scheme over an alphabet of size poly $(\\lambda)$ for model outputs of length $\\mathrm{poly}(\\lambda)$ , which is sound, undetectable with respect to algorithms running in time poly $(\\lambda)$ , and robust to a constant fraction of substitutions, insertions, and deletions when the sequence of generated text has \u201centropy rate\u201d at least some constant. ", "page_idx": 2}, {"type": "text", "text": "As is typical in cryptographic settings, the guarantee of Theorem 1.1 involves a security parameter $\\lambda$ , which controls the power of a distinguishing algorithm (in the context of undetectability) as well as the failure probability of soundness and edit-robustness (which are ${\\mathsf{n e g l}}(\\lambda)$ ; see the formal version in Appendix E). The requirement that the sequence of generated text have constant entropy rate (meaning that on average, the tokens are generated from conditional distributions which have entropy at least a constant fraction of the maximum possible entropy) is easily seen to be necessary, and lower bounds on the entropy are standard amongst essentially all watermarking schemes, even without any robustness requirement [CGZ24, $\\mathrm{FGJ}^{\\bar{+}}23$ , CG24, Zam24]. Indeed, watermarking a near-deterministic language model is impossible since it can only produce a small number of outputs; the entropy assumption quantifies the extent to which the language model is near-deterministic. ", "page_idx": 2}, {"type": "text", "text": "One potential limitation of Theorem 1.1 is the requirement that the alphabet size of the language model (i.e., number of tokens) grow as a polynomial in $\\lambda$ . This limitation is mitigated by the following observations: first, in many domains, the number of tokens used by existing (or future) tokenizers may already be quite large, i.e., at least in the thousands [SHB16, $\\mathrm{YL}\\dot{\\mathrm{K}}^{+}22]$ ; second, in practice one could use various heuristics to increase the number of tokens, such as by grouping together consecutive sequences of tokens to create larger alphabets of \u201cmega-tokens\u201d. Finally, again invoking the language of coding theory, a common approach to constructing good codes is via concatenation, which combine an outer code with large alphabet together with an inner code that encodes individual symbols of the large alphabet using a smaller alphabet. Theorem 1.1 (as well as Theorem 4.1, discussed below) could play the role of the outer code in such a concatenation strategy for watermarking schemes; finding the analogue of an inner code to decrease the alphabet size represents an important direction for future work. ", "page_idx": 2}, {"type": "text", "text": "Roadmap of techniques. The proof of Theorem 1.1 proceeds via constructing new pseudorandom codes $(P R C s)$ , which are cryptographic objects originally introduced by [CG24] to derive watermarking schemes robust to i.i.d. random (i.e., non-adversarial) subsitutions and deletions. Roughly speaking, a pseuorandom code is an error-correcting code equipped with a secret key used for encoding and decoding, which looks random to any polynomial-time algorithm that does not hold a secret key. We establish the following ingredients pertaining to PRCs: ", "page_idx": 3}, {"type": "text", "text": "\u2022 First, we design a a PRC over the binary alphabet which is robust to a constant fraction of adversarial substitutions, under Assumption 3.1 (Theorem 3.2; see Section 3). This assumption is, qualitatively speaking, weaker than the cryptographic assumptions in [CG24].   \n\u2022 Next, we give a generic reduction which, given any PRC over the binary alphabet robust to substitutions, yields a PRC over a polynomial-sized alphabet robust to any constant fraction of substitutions, insertions, and deletions (Theorem 4.1; see Section 4). A central idea in this latter PRC is to interpret symbols of the larger alphabet as indices into codewords of the former PRC; hence, we call it an indexing PRC.   \n\u2022 Finally, we establish a generic reduction which converts any PRC over a larger alphabet robust to substitutions, insertions, and deletions to a watermarking scheme with analogous robustness properties (Theorem E.1; see Section 5). ", "page_idx": 3}, {"type": "text", "text": "Theorem 1.1 follows by composing the components above. These components have the advantage of being modular in nature, meaning that one could, for instance, plug in the substitution PRCs of [CG24] in place of the first item to obtain a guarantee analogous to Theorem 1.1 but under the (qualitatively stronger) cryptographic assumptions of [CG24]. ", "page_idx": 3}, {"type": "text", "text": "In order to handle deletions, [CG24] uses a type of PRC they call a majority code. This code is only robust when the substitutions and deletions are made in an i.i.d. manner. In their reduction that converts a PRC to a watermarking scheme, the language model may induce certain errors on the PRC codewords. Since these errors may not be correctable by the majority code unless they are assumed to be i.i.d., [CG24] need to assume that the language model is equivalent to a BSC. Our use of indexing PRCs avoids this significant shortcoming. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We begin with the definition of a pseudorandom code $(P R C)$ , as introduced in [CG24]. A pseudorandom code is defined over an alphabet $\\Sigma$ , which is simply a finite set. We denote the set of all finite-length strings over $\\Sigma$ by $\\Sigma^{\\star}$ . As is typical when defining cryptographic primitives, our pseudorandom codes depend on a security parameter $\\lambda\\in\\mathbb N$ ; as $\\lambda$ is increased, the amount of security afforded by the PRC also increases. A function $f:\\mathbb{N}\\to\\mathbb{R}_{\\geq0}$ is called negligible if for any $C>0$ , there exists $\\lambda_{0}$ so that $f(\\lambda)\\leq\\lambda^{-C}$ for all $\\lambda>\\lambda_{0}$ . We use ${\\mathsf{n e g l}}(\\lambda)$ to denote a negligible function, whose precise value can change from line to line. ", "page_idx": 3}, {"type": "text", "text": "Given an alphabet $\\Sigma$ , a channel is a mapping $\\mathcal{E}$ which associates to any $x\\in\\Sigma^{\\star}$ a distribution ${\\mathcal{E}}(x)$ over $\\Sigma^{\\star}$ . With slight abuse of notation, we will often let ${\\mathcal{E}}(x)$ denote a random variable $y$ drawn from ${\\mathcal{E}}(x)$ : for instance, when we write a statement of the form \u201cwith probability $1-{\\mathsf{n e g l}}(n)$ , $D_{\\mathsf{H a m}}(x,\\mathcal{E}(x))\\leq p n^{*}$ , we mean that $D_{\\mathsf{H a m}}(x,y)\\leq p n$ with probability $1-{\\mathsf{n e g l}}(n)$ over $y\\sim{\\mathcal{E}}(x)$ . ", "page_idx": 3}, {"type": "text", "text": "We primarily focus on secret key PRCs for simplicitity; our arguments (in particular, those regarding edit-robust PRCs) apply identically to the case of public-key PRCs [CG24, Definition 2], though for simplicity we focus solely on secret-key PRCs. A secret-key PRC may be viewed as a variant of a secret-key encryption scheme in which the ciphertext enjoys certain robustness properties to adversarial perturbations. Formally, a PRC is specified by functions KeyGen, Encode, Decode, which behave as follows: KeyGen outputs a secret key, Encode uses the secret key to \u201cencrypt\u201d a message, and Decode uses the secret key to \u201cdecode\u201d a string. If the string passed to Decode is the output of Encode, perhaps with a bounded number of adversarial perturbations, then Decode should output the message originally passed to Encode (robustness). Outputs of Encode should also look random to polynomial-time algorithms (undetectability), and Decode should almost always fail when given any fixed string (i.e., not an output of Encode) as input. These definitions are formalized below: ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 (Secret-key Pseudorandom code (PRC)). Let $\\lambda\\in\\mathbb N$ denote a security parameter, and suppose that for each $\\lambda\\in\\mathbb N$ an associated alphabet $\\Sigma(\\lambda)$ is given. Moreover, suppose that for each $\\lambda$ , a collection $\\mathcal{E}(\\lambda)$ of channels $\\mathcal{E}:\\Sigma(\\lambda)^{\\star}\\rightarrow\\Delta(\\Sigma(\\lambda)^{\\star})$ is given. A secret-key pseudorandom code $(P R C)$ with robustness to $\\mathcal{E}$ is a triple of probabilistic polynomial-time algorithms (KeyGen, Encode, Decode) satisfying the following: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "\u2022 For some functions $n,k,\\ell_{\\mathsf{s k}}:\\mathbb{N}\\to\\mathbb{N},$ for all $\\lambda\\in\\mathbb N$ , we have $\\mathsf{K e y G e n}(1^{\\lambda})\\in\\{0,1\\}^{\\ell_{\\mathsf{s k}}(\\lambda)}$ , Encode : $\\{1^{\\lambda}\\}\\times\\{0,1\\}^{\\ell_{\\mathrm{sk}}(\\lambda)}\\times\\Sigma^{k(\\lambda)}\\to\\Sigma^{n(\\lambda)}$ , and Decode : $\\{1^{\\lambda}\\}\\times\\{0,1\\}^{\\ell_{\\mathrm{sk}}(\\lambda)}\\times\\Sigma^{\\star}\\to\\Sigma^{k(\\lambda)}\\cup\\{\\bot\\}.$ . \u2022 For any $\\lambda\\in\\mathbb N$ and message $\\mathsf{m}\\in\\Sigma^{k(\\lambda)}$ , the code is robust to any channel $\\mathcal{E}\\in\\mathcal{E}(\\lambda)$ , which means that: $\\operatorname*{Pr}_{{\\mathsf{s k e-K e y G e n}}(1^{\\lambda})}\\left(\\mathsf{D e c o d e}(1^{\\lambda},\\mathsf{s k},{\\mathcal{E}}(x))=\\mathsf{m}\\mid x\\gets\\mathsf{E n c o d e}(1^{\\lambda},\\mathsf{s k},\\mathsf{m})\\right)\\geq1-\\mathsf{n e g l}(\\lambda).$ ", "page_idx": 4}, {"type": "text", "text": "Moreover, for any fixed $y\\in\\Sigma^{\\star}$ , the code is sound, which means that: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{{\\mathsf{s k}}\\leftarrow{\\mathsf{K e y G e n}}(1^{\\lambda})}\\left({\\mathsf{D e c o d e}}(1^{\\lambda},{\\mathsf{s k}},y)=\\perp\\right)\\geq1-{\\mathsf{n e g l}}(\\lambda).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "\u2022 The code is undetectable which means that: for any $\\lambda\\in\\mathbb N$ for any probabilistic polynomial-time adversary Adv, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\vert\\operatorname*{Pr}_{\\mathsf{s k}\\gets\\mathsf{K e p G e n}(1^{\\lambda})}\\Big(\\mathsf{A d v}^{\\mathsf{E n c o d e}(1^{\\lambda},\\mathsf{s k},\\cdot)}(1^{\\lambda})=1\\Big)-\\operatorname*{Pr}_{\\mathsf{s k}\\gets\\mathsf{K e p G e n}(1^{\\lambda})}\\Big(\\mathsf{A d v}^{\\mathcal{U}}(1^{\\lambda})=1\\Big)\\right\\vert\\leq\\mathsf{n e g l}(\\lambda),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\boldsymbol{\\mathcal{U}}$ denotes an oracle which responds with a freshly drawn uniformly random string in $\\Sigma^{n(\\lambda)}$ on each call (to any input). ", "page_idx": 4}, {"type": "text", "text": "The block length of the PRC is defined to be $n(\\lambda)$ . For all of our pseudorandom codes, we will have $n(\\lambda),k(\\lambda),\\ell_{\\mathsf{s k}}(\\lambda)\\le\\mathrm{poly}(\\lambda)$ . Moreover, we take as a convention that $n(\\lambda)\\ge\\lambda$ for all $\\lambda$ (this may be ensured without loss of generality by rescaling $n(\\lambda)_{.}$ ). Our focus will primarily be on zero-bit $P R C s$ , which are particularly useful for watermarking language model outputs. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.2 (Zero-bit PRC). A zero-bit PRC is one for which $k(\\lambda)=0$ for all $\\lambda$ , i.e., the only possible message is $\\mathsf{m}=\\emptyset$ . ", "page_idx": 4}, {"type": "text", "text": "[CG24, Section 6] shows a generic reduction that converts any zero-bit PRC into a general PRC with constant rate, meaning that $\\bar{k}(\\lambda)/n(\\lambda)=\\Omega(1)$ . We remark that the same reduction can be applied to our PRCs with edit robustness, though we do not pursue this direction further in this paper. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.3 (Substitution-bounded). For $p\\in(0,1)$ , a channel $\\mathcal{E}$ over alphabet $\\Sigma$ is $p$ -substitutionbounded if for any $n\\in\\mathbb N$ , $y\\in\\Sigma^{n}$ , for $z\\sim\\mathcal{E}(y)$ , $D_{\\mathsf{H a m}}(y,z)\\leq p n$ holds almost surely. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.4 (Edit-bounded channel). Fix an alphabet $\\Sigma$ together with $p\\in(0,1)$ . A channel $\\mathcal{E}$ over $\\Sigma$ is defined to be $p$ -edit-bounded if for any $x\\in\\Sigma^{\\star}$ with $n:=|x|$ , $y\\sim{\\mathcal{E}}(x)$ may be obtained from $x$ by applying a total of at most pn substitutions, insertions, and deletions, almost surely. Moreover, there exists a probabilistic polynomial-time algorithm which, given $x$ , outputs a sample $y\\sim{\\mathcal{E}}(x)$ .5 ", "page_idx": 4}, {"type": "text", "text": "3 Secret-key substitution PRCs from weaker assumptions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we discuss a new construction of binary PRCs for substitution channels. Though such PRCs were also obtained by [CG24], the codes in [CG24] relied on relatively strong average-case hardness assumptions, in the sense that they imply the existence of public-key cryptography (i.e., in the context of Impagliazzo\u2019s Five Worlds [Imp95], they imply primitives in \u201cCryptomania\u201d). In contrast, our construction relies only on the hardness of the existence of a family of pseudorandom functions that enjoys a certain locality property; such an assumption is generally believed to be weaker than the ones in [CG24], in the sense that it is only known to yield cryptographic primitives in \u201cMinicrypt\u201d, though we are not aware of a formal separation. ", "page_idx": 4}, {"type": "text", "text": "We first recall the definition of pseudorandom function (PRF) families, which are PRF families for which the adversary can only query the pseudorandom function at a uniformly random input $x$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 3.1 (Weak PRF family). Fix functions $\\ell(\\lambda),n(\\lambda):\\mathbb{N}\\to\\mathbb{N}$ of a security parameter $\\lambda$ , and a collection of functions $\\{F_{s}\\,:\\,\\{0,1\\}^{n(\\lambda)}\\,\\rightarrow\\,\\{0,1\\}\\}$ , indexed by $s\\,\\in\\,\\{0,1\\}^{\\ell(\\lambda)}$ , for $\\lambda\\in\\mathbb N$ . We say that the collection $\\{F_{s}\\}_{s}$ is a weak pseudorandom function family (weak $P R F$ ) if for every probabilistic polynomial-time algorithm Adv which outputs a single bit (i.e., 0 or 1), it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{E}_{s\\sim\\{0,1\\}^{\\ell(\\lambda)}}\\left[\\widetilde{\\mathsf{A d v}}^{F_{s}(\\cdot)}(1^{n(\\lambda)})\\right]-\\mathbb{E}_{F_{\\mathsf{U n i f}}}\\left[\\widetilde{\\mathsf{A d v}}^{F_{\\mathsf{U n i f}}}(1^{n(\\lambda)})\\right]\\right|\\le\\mathsf{n e g}|(\\lambda),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\widetilde{\\mathsf{A d v}}^{G}$ , for a mapping $G:\\{0,1\\}^{n(\\lambda)}\\rightarrow\\{0,1\\}$ means that Adv can make calls to the function $G$ , and  for each call receives a tuple $(x,G)$ , for $x\\sim\\operatorname{Unif}(\\{0,1\\}^{n})$ . (In particular, the tilde refers to the fact that Adv can only call $G(x)$ on a uniformly chosen $x$ .) Moreover, $F_{\\mathsf{U n i f}}:\\{0,1\\}^{n}\\to\\{0,1\\}$ denotes a uniformly random function. We will often refer to the function $G$ that Adv can make queries to as the oracle Adv has access to. Given $q\\in[0,1/2)$ , we say that $\\{F_{s}\\}_{s}$ is a weak PRF family with noise level $q$ if (2) holds where each call to $F_{s}(\\cdot)$ by Adv returns $\\ensuremath{F_{s}}(x)\\ensuremath{\\oplus}e$ where $x\\sim\\operatorname{Unif}(\\{0,1\\})^{n}$ and $e\\sim\\mathrm{Ber}(q)$ . ", "page_idx": 5}, {"type": "text", "text": "Our new construction of PRCs is based off of local (weak) $P R F s$ , which are PRFs for which the output depends on a small number of input bits. ", "page_idx": 5}, {"type": "text", "text": "Definition 3.2 (Local function family). Let $\\tau\\in\\mathbb{N}$ . A family of functions $\\{F_{s}:\\{0,1\\}^{n}\\rightarrow\\{0,1\\}\\}$ indexed by $s$ is defined to be $\\tau$ -local if for each $s$ , $F_{s}(x)$ only depends on at most $\\tau$ bits of $x$ , i.e., for each $s$ there are distinct indices $j_{1},\\dotsc,j_{\\tau}\\in[n]$ together with a function $G_{s}:\\{0,1\\}^{\\tau}\\rightarrow\\{0,1\\}$ so that $F_{s}(x)=G_{s}(x_{j_{1}},\\dots,x_{j_{\\tau}})$ for all $x\\in\\{0,1\\}^{n}$ . ", "page_idx": 5}, {"type": "text", "text": "Our main computational assumption is the existence of a weak PRF family which is $\\tau$ -local for $\\tau$ of logarithmic size: ", "page_idx": 5}, {"type": "text", "text": "Assumption 3.1 (Local Weak PRFs). For some functions $\\ell(\\lambda),n(\\lambda),\\tau(\\lambda)~:~\\mathbb{N}~\\rightarrow~\\mathbb{N}$ with $\\ell(\\lambda),n(\\lambda)\\leq\\mathrm{{poly}}(\\lambda)$ and $\\tau(\\lambda)\\leq\\log n(\\lambda)$ , there exists a weak PRF family $\\{F_{s}\\,:\\,\\{0,1\\}^{n(\\lambda)}\\,\\rightarrow$ $\\{0,1\\}\\}_{s\\in\\{0,1\\}^{\\ell(\\lambda)}}$ for some noise level $q<1/2$ which is $\\tau(\\lambda)$ -local, for each $\\lambda\\in\\mathbb N$ . ", "page_idx": 5}, {"type": "text", "text": "In Appendix C.2, we discuss how Assumption 3.1 follows from standard average-case hardness assumptions, notably the hardness of learning $\\log(n)$ -juntas over ${\\mathrm{Unif}}(\\{0,1\\}^{n})$ . As specific examples, either hardness of the $\\log(n)$ -sparse noisy parity problem [FGKP09, BFJ $^{+}94$ , GRV11, Val15] or hardness of weakly learning a particular family of functions presented in [BFKL94] (see also [Blu03]) implies Assumption 3.1. ", "page_idx": 5}, {"type": "text", "text": "The PRC construction. Our construction of PRCs based on Assumption 3.1 is presented in Algorithm 2: given a function family $\\mathcal{F}$ satisfying Assumption 3.1 with noise level $q$ together with some $p\\bar{<}\\,1/2$ representing the maximum fraction of substitutions to correct, we construct F $^{\\mathsf{7}}\\mathsf{R F-P R C}[\\mathcal{F},p,q]\\,=\\,(\\mathsf{K e y G e n},\\mathsf{\\bar{E n c o d e}},\\mathsf{D e c o d e})$ as follows. The construction depends on some parameters $N(\\lambda),m(\\lambda)\\leq\\mathrm{poly}(\\lambda)$ , specified in (3): ", "page_idx": 5}, {"type": "text", "text": "\u2022 $\\mathsf{K e y G e n}(1^{\\lambda})$ chooses a function in $\\mathcal{F}$ (indexed by $s$ ) together with a uniformly random $z\\sim$ $\\operatorname{Unif}\\bigl(\\{0,1\\}^{N(\\lambda)}\\bigr)$ and a uniform permutation $\\pi:[N(\\lambda)]\\to[N(\\lambda)]$ , and returns $\\mathtt{s k}=(s,z,\\pi)$ . \u2022 $\\mathtt{E n c o d e}(1^{\\lambda},(s,z,\\pi),\\emptyset)$ draws $m(\\lambda)$ uniformly random elements of $\\{0,1\\}^{n(\\lambda)}$ , $x_{1},\\ldots,x_{m(\\lambda)}$ , applies $F_{s}$ to each of them and flips the result with probability $q$ to obtain bits $\\left(w_{1},\\ldots,w_{m(\\lambda)}\\right)$ , and perturbs the concatenation $((x_{i},w_{i}))_{i\\in[m(\\lambda)]}$ according to $z,\\pi$ as on Line 9. \u2022 Decode $(1^{\\lambda},(s,z,\\pi),y)$ first \u201cunperturbs\u201d $y$ to obtain a string $((x_{i},w_{i}))_{i\\in[m(\\lambda)]}$ as in Encode, and then outputs $\\varnothing$ if $\\begin{array}{r}{\\sum_{j=1}^{m(\\lambda)}\\mathbb{1}\\{w_{j}=F_{s}(x_{j})\\}}\\end{array}$ is above a threshold; otherwise, it outputs $\\perp$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2. Given $p,q\\,<\\,1/2$ and $a$ function family $\\mathcal{F}$ together with noise level $q$ satisfying Assumption 3.1, then PRF- $\\mathsf{P R C}[\\mathcal{F},p,q]$ (Algorithm 2) is a zero-bit binary-alphabet secret-key $P R C$ (per Definition 2.2) with robustness to all $p$ -bounded substitution channels. ", "page_idx": 5}, {"type": "text", "text": "The proof of Theorem 3.2 is given in Appendix C.3. ", "page_idx": 5}, {"type": "text", "text": "4 From substitution PRCs to edit-robust PRCs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we discuss our construction of PRCs which are robust to edit-bounded channels. To do so, we reduce to PRCs robust to substitution-bounded channels. Suppose we are given a PRC $\\mathsf{P R C}_{\\mathsf{S u b}}$ with block length $n(\\lambda)$ over the binary alphabet which is robust to any $(1/2-p_{0})$ -bounded substitution channel, for $p_{0}\\,\\in\\,(0,1/2)$ . Given a parameter $\\rho\\geq1$ , we construct an indexing $P R C$ , $\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{\\mathsf{S u b},\\,\\rho}]$ (in Algorithm 1), which is robust to any $p$ -edit-bounded channel, where the parameter $p$ depends on $p_{0},\\rho$ in a manner that will be explained below. The code $\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{\\mathsf{S u b},\\,\\rho}]$ has polynomially large alphabet $\\Sigma(\\lambda):=[q(\\lambda)]$ , where $\\bar{q}(\\lambda)=\\rho\\cdot n(\\lambda)$ . We denote the block length of $\\bar{\\mathsf{P R C}}_{\\mathsf{I d x}}[\\mathsf{P R C}_{\\mathsf{S u b},\\,\\bar{\\rho}}]$ by $m(\\lambda)$ (which is defined to be $\\lceil\\ln(2)\\cdot n(\\lambda)\\rceil\\}$ ) to distinguish it from the block length $n(\\lambda)$ of $\\mathsf{P R C}_{\\mathsf{S u b}}$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Construction of $\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{\\mathsf{S u b},\\,\\rho}]$ . The idea behind $\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{\\mathsf{S u b},\\,\\rho}]$ is simple: we interpret each symbol of $\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{\\mathsf{S u b},\\,\\rho}]$ as an index into a codeword of $\\mathsf{P R C}_{\\mathsf{S u b}}$ , so that the existence of a symbol in a given codeword of $\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{\\mathsf{S u b},\\,\\rho}]$ should be interpreted as the corresponding codeword for $\\mathsf{P R C}_{\\mathsf{S u b}}$ as having a \u201c1\u201d in the position corresponding to that symbol. To ensure stronger robustness guarantees, it turns out to be necessary to introduce redundancy in the sense that for each integer $j\\in[n(\\lambda)]$ (representing an index of a codeword of $\\mathsf{P R C}_{\\mathsf{S u b}}\\$ ), there are $\\rho$ different elements of $\\left[q(\\lambda)\\right]$ which correspond to index $j$ . The choice of these $\\rho$ elements for each $j$ is specified a mapping $\\psi:[q(\\lambda)]\\rightarrow[n(\\lambda)]$ with $|\\psi^{\\bar{-}1}(j)|=\\rho$ for all $j$ , which is chosen randomly in the KeyGen function of $\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{\\mathsf{S u b},\\,\\rho}]$ . With this intuition in mind, we proceed to overview the individual KeyGen, Encode, Decode functions of $\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{\\mathsf{S u b},\\,\\rho}]$ in Algorithm 1: ", "page_idx": 6}, {"type": "text", "text": "\u2022 The $\\mathsf{K e y G e n}(1^{\\lambda})$ function generates a secret key $\\mathsf{s k}$ for $\\mathsf{P R C}_{\\mathsf{S u b}}$ using KeyGenSub. It also generates a random function $\\psi$ as described above, and returns the tuple $(\\mathsf{s k},\\psi)$ , which is the secret key for $\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{\\mathsf{S u b},\\,\\rho}]$ . \u2022 The $\\mathsf{E n c o d e}(1^{\\lambda},(\\mathsf{s k},\\psi),\\mathsf{m})$ function calls the encoding method $\\mathsf{E n c o d e s u b}$ for $\\mathsf{P R C}_{\\mathsf{S u b}}$ , which yields a string $y^{0}\\in\\{0,1\\}^{n(\\lambda)}$ . It then chooses a string $y\\in[n(\\lambda)]^{m(\\lambda)}$ which has the property that the set of distinct elements of $y$ , which we denote by $\\mathsf{U n i q u e}(y)$ , has small set difference with the set $S^{0}:=\\{i\\in[n(\\lambda)]_{.}:\\;y_{i}^{0}=\\overset{\\circ}{1}\\}$ of indices at which $y^{0}$ has a \u201c1\u201d. The precise way in which the sets Unique $(y)$ and $\\bar{S^{0}}$ differ is determined by the function PerturbDifference in Algorithm 1, and is needed to ensure that the output of Encode $(1^{\\lambda},{\\mathsf{s k}},{\\mathsf{m}})$ is indistinguishable from the uniform distribution over $[n(\\lambda)]^{m(\\lambda)}$ (i.e., that the PRC is undetectable). Finally, Encode returns a string $z\\in[q(\\lambda)]^{m(\\lambda)}$ where each coordinate $z_{j}$ is a uniformly random pre-image of $y_{j}$ under $\\psi$ . \u2022 The Decode $(1^{\\lambda},(\\mathsf{s k},\\psi),z)$ function calls the substitution PRC decode function, DecodeSub, on the string $y^{\\prime}\\in\\{0,1\\}^{n(\\lambda)}$ which has a 1 in position $i\\in[n(\\lambda)]$ if and only if $i\\in\\mathsf{U n i q u e}(\\psi(z))$ . For future reference, we denote this string by $y^{\\prime}=D_{\\psi}(z)$ , i.e., $D_{\\psi}(z)_{i}=\\mathbb{1}\\{i\\in{\\sf U n i q u e}(\\dot{\\psi}(z))\\}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1 below shows that $\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{\\mathsf{S u b},\\,\\rho}]$ has robustness to any channel which makes a large fraction (at most $1-C_{\\mathrm{rob}}p_{0})$ of substitutions, insertions, and deletions. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1. There are constants $C_{0}$ , $C_{\\mathsf{r o b}}\\geq1$ so that the following holds. For any $p_{0}<(10C_{\\mathsf{r o b}})^{-1}$ and PRC $\\mathsf{P R C}_{\\mathsf{S u b}}$ with block length $n(\\lambda)$ which is robust to all $(1/2-p_{0})$ -substitution-bounded channels, for any $\\rho\\geq C_{0}/p_{0}$ , $\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{\\mathsf{S u b}},\\rho]$ (Algorithm $^{\\,l}$ ) is a pseudorandom code over an alphabet of size $\\lceil\\rho\\cdot n(\\lambda)\\rceil$ and block length at most $n(\\lambda)$ , which has robustness to any $(1-C_{\\mathsf{r o b}}p_{0})$ - edit-bounded channel (per Definition 2.4). ", "page_idx": 6}, {"type": "text", "text": "Proof overview for Theorem 4.1. To prove Theorem 4.1, we need to establish the soundness, undetectability, and robustness of $\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{\\mathsf{S u b},\\,\\rho}]$ . Soundness is an immediate consequence of soundness of $\\mathsf{P R C}_{\\mathsf{S u b}}$ (Lemma D.2). Undetectability is likewise straightforward, using undetectability of $\\mathsf{P R C}_{\\mathsf{S u b}}$ together with the fact that the output of PerturbDifference $(n,m,\\bar{y^{0}})$ , for $y^{0}\\sim\\mathrm{Unif}(\\{0,1\\})^{n}$ , is uniform on $[n]^{m}$ (Lemma D.2). The bulk of the proof consists in establishing robustness. ", "page_idx": 6}, {"type": "text", "text": "A natural attempt to establish robustness would proceed as follows: given a string $z\\in[n(\\lambda)]^{m(\\lambda)}$ (to be interpreted as an output of $\\mathsf{E n c o d e}(1^{\\lambda},(\\mathsf{s k},\\psi),\\mathsf{m}))$ , a single insertion or deletion in $z$ can change at most one symbol of $D_{\\psi}(z)$ , and a single substitution in $z$ can change at most two symbols of $\\bar{D_{\\psi}}(z)$ . Thus, it is straightforward to show a statement of the following form: if $\\mathsf{P R C}_{\\mathsf{S u b}}$ is $(1/2-p_{0})$ -substitution bounded and $2p<1/2-p_{0}$ , then $\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{\\mathsf{S u b},\\,\\rho}]$ is robust to the class of $p$ -edit-bounded channels. (Technically, some additional work is needed since the PerturbDifference function introduces some additional substitutions in the underlying binary codeword, though we ignore this detail for now since with an appropriate choice of parameters the number of such errors will be of lower order.) ", "page_idx": 6}, {"type": "text", "text": "Require: PRC for substitutions: $(\\mathsf{K e y G e n s u b}$ , EncodeSub, DecodeSub), together with functions $n:$   \n$\\mathbb{N}\\rightarrow\\mathbb{N},\\ell:\\mathbb{N}\\rightarrow\\mathbb{N}$ characterizing the block length and key length, respectively; parameter   \n$\\rho>1$ . Functions $m(\\lambda):=\\lceil\\ln(2)\\cdot n(\\lambda)\\rceil$ , $q(\\lambda):=\\rho\\cdot n(\\lambda)$ .   \n1: function $\\mathsf{K e y G e n}(1^{\\lambda})$   \n2: Define $\\mathsf{s k}\\gets\\mathsf{K e y G e n s u b}(1^{\\lambda})$ .   \n3: Let $\\psi:[q(\\lambda)]\\rightarrow[n(\\lambda)]$ be a uniformly random function conditioned on $|\\psi^{-1}(j)|=\\rho$ for   \neach $j\\in[n(\\lambda)]$ .   \n4: return $(\\mathsf{s k},\\psi)$ .   \n5: function E $\\mathtt{\\dot{n}c o d e(1^{\\lambda},(s k,\\psi),m)}$   \n6: Set $y^{0}\\gets\\mathsf{E n c o d e}_{\\mathsf{S u b}}(1^{\\lambda},\\mathsf{s k},\\mathsf{m})\\in\\{0,1\\}^{n(\\lambda)}.$   \n7: Set $y\\leftarrow$ PerturbDifference $(n(\\lambda),m(\\lambda),y^{0})$ .   \n8: For each $j\\in[m(\\lambda)]$ , choose $z_{j}\\sim\\operatorname{Unif}(\\{a\\ :\\ \\psi(a)=y_{j}\\})$ .   \n9: return $z=(z_{1},\\dots,z_{m(\\lambda)})$ .   \n10: function $\\mathsf{D e c o d e}(1^{\\lambda},(\\mathsf{s k},\\psi),z)$   \n11: Define $y:=\\psi(z)=(\\psi(z_{1}),\\dots,\\psi\\bigl(z_{m(\\lambda)}\\bigr))$ .   \n12: Define $y^{\\prime}\\in\\{0,1\\}^{n}$ by $y_{i}^{\\prime}=\\mathbb{1}\\{i\\in\\mathsf{U n i q u e}(y)\\}$ .   \n13: return $\\mathsf{D e c o d e}_{\\mathsf{S u b}}(1^{\\lambda},\\mathsf{s k},y^{\\prime})$ .   \n14: function PerturbDifference $(n,m,y^{0})$   \n15: Set $S^{0}:=\\{i\\in[n]\\ :\\ y_{i}^{0}=1\\}$ .   \n16: Sample $y^{1}\\sim\\mathrm{Unif}([n]^{m})$ .   \n17: Set $S^{1}:=\\mathsf{U n i q u e}(y^{1})\\subset[n]$ .   \n18: if $|S^{0}\\backslash S^{1}|\\geq|S^{1}\\backslash S^{0}|$ then   \n19: Let $\\sigma:S^{1}\\backslash S^{0}\\rightarrow S^{0}\\backslash S^{1}$ denote a uniformly random injective mapping.   \n20: For each $a\\in S^{1}\\backslash S^{0}$ , let $y$ be formed by replacing each instance of $a$ in $y^{1}$ with $\\sigma(a)$ .   \n21: else   \n22: Let $\\tau:S^{0}\\backslash S^{1}\\rightarrow S^{1}\\backslash S^{0}$ denote a uniformly random injective mapping.   \n23: For each $a\\in S^{0}\\backslash S^{1}$ , let $y$ be formed by replacing each instance of $\\tau(a)$ in $y^{1}$ with $a$ .   \n24: return $y$ . ", "page_idx": 7}, {"type": "text", "text": "Unfortunately, such a result does not have sufficiently good robustness for our application to watermarking. As will be discussed in Section 5, our procedure which watermarks a language model Model by \u201cembedding\u201d a codeword $z$ of a PRC in a sequence of text output by Model introduces a fraction $1\\mathrm{~-~}\\alpha$ of errors to $z$ , all of which are substitutions. Here $\\alpha$ is some constant which is related to the entropy rate of Model. Using the naive approach above, we are constrained to a rate of substitutions $p$ bounded as $p<1/4$ , thus forcing $1-\\alpha<1/4$ and so disallowing all but high entropy rates. ", "page_idx": 7}, {"type": "text", "text": "To compensate, we make use of the fact that the randomly chosen mapping $\\psi:[q(\\lambda)]\\rightarrow[n(\\lambda)]$ maps multiple (namely, $\\rho$ ) symbols in $\\left[q(\\lambda)\\right]$ to each symbol in $[n(\\lambda)]$ , when performing decoding. In particular, consider any fixed channel $\\mathcal{E}$ over the alphabet $[q(\\lambda)]$ . For simplicity in our overview here, we assume that $\\mathcal{E}$ is deterministic (so that it is specified by a mapping $\\mathcal{E}:[\\boldsymbol{q}(\\lambda)]^{m}\\to[\\boldsymbol{q}(\\lambda)]^{\\star}$ for each $m\\in\\mathbb{N}$ ), though essentially the same argument works for randomized $\\mathcal{E}$ . Consider a codeword $z\\,\\in\\,[q(\\lambda)]^{m(\\lambda)}$ which is output by $\\mathsf{E n c o d e}(1^{\\lambda},(\\mathsf{s k},\\psi),\\mathsf{m})$ . By undetectability of the code, with high probability over Encode, we will have that $|\\mathsf{U n i q u e}(\\psi(z))|\\approx n(\\lambda)/2\\pm O(\\sqrt{n(\\lambda)})$ , since by our choice of $m(\\lambda)=\\lceil n(\\lambda)\\cdot\\ln(2)\\rceil$ , a uniformly random string $\\tilde{z}\\sim\\mathrm{Unif}([q(\\lambda)]^{m(\\lambda)})$ satisfies $|\\mathsf{U n i q u e}(\\psi(\\tilde{z}))|\\approx n(\\lambda)/2\\pm O(\\sqrt{n(\\lambda)})$ with high probability.6 ", "page_idx": 7}, {"type": "text", "text": "Supposing that $\\mathcal{E}$ is promised to make at most a fraction $1-p$ of edits, then $z^{\\prime}:=\\mathcal{E}(z)$ shares at least $p\\cdot m(\\lambda)$ symbols with $z$ . Let us suppose for simplicity here that $z^{\\prime}\\in[q(\\lambda)]^{m(\\lambda)}$ , so that the insertions and deletions of $\\mathcal{E}$ are balanced (a slight modification of the argument handles the general case). Of the remaining $(1-p)\\cdot m(\\lambda)$ symbols of $z^{\\prime}$ , by the random choice of $\\psi$ and since $|\\mathsf{U n i q u e}(\\psi(z))|\\approx n(\\lambda)/2$ , each one is roughly equally likely to map (under $\\psi$ ) to an element in $\\mathsf{U n i q u e}(\\psi(z))$ as to an element in $[n(\\lambda)]\\backslash{\\mathsf{U n i q u e}}({\\bar{\\psi}}(z))$ . Thus, in going from the set Unique $\\big(\\psi(z)\\big)$ to the set Unique $\\big(\\psi(z^{\\prime})\\big)$ , we should expect to change at most roughly $(1-p)\\cdot n(\\lambda)/2$ elements. This intuition is made precise in the following lemma: ", "page_idx": 8}, {"type": "text", "text": "Lemma 4.2 (Informal version of Lemma D.7). Given a channel $\\mathcal{E}$ as above which makes a $(1-p)$ - fraction of edits (i.e., substitutions, insertions, and deletions), with $1-{\\mathsf{n e g l}}(\\lambda)$ probability over the draw of of $(\\mathsf{s k},\\psi)\\gets\\mathsf{K e y G e n}(1^{\\lambda})$ and $z\\gets\\mathsf{E n c o d e}(1^{\\lambda},(\\mathsf{s k},\\psi),\\mathsf{m})$ we can bound the set difference between Unique $\\left(\\psi(z)\\right)$ , Unique $\\psi(\\mathcal{E}(z))$ ) as follows: ", "page_idx": 8}, {"type": "equation", "text": "$$\n|\\Delta(\\mathsf{U n i q u e}(\\psi(z)),\\mathsf{U n i q u e}(\\psi(\\mathcal{E}(z)))|\\leq(1-\\Omega(p))\\cdot n(\\lambda).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Since the Hamming distance $D_{\\mathsf{H a m}}(D_{\\psi}(z),D_{\\psi}(z^{\\prime}))$ is equal to the size of the set difference $|\\Delta({\\mathsf{U n i q u e}}(\\psi(z))$ , Unique $(\\psi(z^{\\prime}))$ , we arrive at the conclusion that $D_{\\mathsf{H a m}}(D_{\\psi}(z),D_{\\psi}(z^{\\prime}))\\ \\lesssim$ $(1-\\Omega(p))\\cdot n(\\lambda)/2$ with high probability over the draw of $\\psi$ in KeyGen. Since $p_{0}$ can be chosen arbitrarily small, we have substitution PRCs $\\mathsf{P R C}_{\\mathsf{S u b}}$ which can correct a $(1-\\Omega(p))/2$ fraction of substitutions. Thus, we obtain as a consequence that $\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{\\mathsf{S u b},\\,\\rho}]$ can correct a $(1-p)$ fraction of substitutions, insertions, and deletions. ", "page_idx": 8}, {"type": "text", "text": "The argument above omits many details, notably involving the high-probability event referenced in Lemma 4.2. To establish that such an event occurs with high probability (over the draw of $\\psi$ ), we need to use Dobrushin\u2019s concentration inequality (Theorem F.3) for data with limited dependencies. Roughly speaking, this inequality comes into play because the sets $\\psi^{-1}(1),\\ldots,\\psi^{-1}(n(\\lambda))\\subset[q(\\lambda)]$ are not fully independent (since, e.g., they must be disjoint). Nevertheless, we may bound their dependencies, assuming that $n(\\lambda)$ is sufficiently large as compared to $\\rho$ . ", "page_idx": 8}, {"type": "text", "text": "5 From large-alphabet PRCs to watermarking ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we overview our reduction (stated in Theorem 5.1) that converts a PRC with robustness to channels making a bounded number of adversarial edits to a watermarking scheme with robustness to adversarial edits. At a high level, this reduction uses rejection sampling at each step of the language model generation to make the output of the model align with a PRC codeword. To formally state the result, we need the notion of empirical entropy: given a token sequence $\\mathfrak{t}\\in\\Sigma^{L}$ and $i,j\\in[L]$ , the empirical entropy of Model on the subsequence $[i,j]$ is ", "page_idx": 8}, {"type": "equation", "text": "$$\nH_{\\mathsf{e}}^{[i:j]}(\\mathsf{t},\\mathsf{M o d e l}):=-\\log_{\\mathsf{t}_{i:j}^{\\prime}\\sim\\mathsf{M o d e l}(\\cdot|\\mathsf{t}_{1:i-1})}(\\mathsf{t}_{i:j}^{\\prime}=\\mathsf{t}_{i:j}\\mid\\mathsf{t}_{1:i-1}),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the probability is over a sequence that is drawn, token by token, from the per-token distributions induced by Model (see Definition B.1 for discussion). The empirical entropy quantifies the degree to which the sequence $\\mathfrak{t}_{i:j}$ is \u201cfar from being deterministic\u201d under Model given the prefix $\\mathtt{t}_{1:i-1}$ . ", "page_idx": 8}, {"type": "text", "text": "The watermarking schemes we derive from PRCs satisfy a stronger property known as substring robustness [CGZ24, CG24], which means that if any sufficiently high-entropy substring of watermarked text is passed through a channel inducing a bounded number of edits, then the watermark will still be detected. More precisely, for a function $\\beta:\\mathbb{N}\\,\\rightarrow\\,\\mathbb{N}$ , a watermarking procedure $\\mathcal{W}=$ (Setup, Wat, Detect) over an alphabet $\\Sigma$ is $\\beta$ -substring robust to a channel $\\mathcal{E}$ if ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\underset{s\\in\\mathrm{d}(1^{\\lambda},\\leq k),\\tau^{\\prime}\\in\\mathcal{E}(\\mathbb{T})}{\\mathrm{Pr}}(1^{\\lambda})}{\\mathrm{Pr}}~\\left(\\exists i,\\ell\\;\\mathrm{s.t.~Detect}(1^{\\lambda},\\{\\mathbf{k},\\mathbf{t}^{\\prime}\\}=\\mathsf{F a l s e~a n d}~H_{\\mathrm{e}}^{[i:i+\\ell-1]}(\\mathbf{t})\\geq\\beta(\\ell)\\cdot\\ln|\\Sigma|\\right)\\leq\\mathsf{n e g l}(\\lambda)\\leq\\mathsf{m i n}}\\\\ {\\underset{\\leftarrow\\mathrm{Wat}(1^{\\lambda},\\leq k),\\tau^{\\prime}\\in\\mathcal{E}(\\mathbb{T})}{\\mathrm{expt}}(\\lambda)}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We remark that the channel $\\mathcal{E}$ is required to be non-adaptive in that it cannot depend on the particular draw of the secret key $\\mathsf{s k}$ . Some details are omitted; see Definition B.2 for a completely formal definition of substring-robustness. Our main result of this section shows that a PRC with robustness to edit-bounded channels yields a watermarking scheme with substring-robustness to edit-bounded channels: ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.1 (Informal version of Theorem E.1). Let $\\alpha\\in(0,1)$ be a given constant. Suppose PRC, defined over alphabet $\\Sigma_{\\mathsf{P R C}}$ , has block length n and is robust to any $\\textstyle(1-{\\frac{\\alpha}{16}})$ -edit-bounded channel. Further suppose Model is a language model over alphabet $\\Sigma$ satisfying $\\begin{array}{r}{|\\bar{\\Sigma}|\\geq(\\frac{8}{\\alpha}|\\Sigma_{\\mathsf{P R C}}|)^{2/\\alpha}}\\end{array}$ . Then there is a watermarking scheme $\\mathcal{W}$ [PRC, Model] (Algorithm 3) which is sound, undetectable, and $\\beta(\\ell)$ -substring robust to any $\\frac{\\alpha^{2}}{48}$ -edit-bounded channel, for $\\beta(\\ell):=8n+6\\alpha\\ell$ . ", "page_idx": 9}, {"type": "text", "text": "Theorem 5.1 establishes that, as long as the entropy from a substring of generated text is roughly a $\\Omega(\\alpha)$ -fraction of the maximum possible entropy, then a constant $\\bar{(O(\\alpha^{2}))}$ fraction of edits to that substring cannot remove the watermark. We remark that this constant fraction of edits cannot be impoved to beyond an $\\alpha$ -fraction (as is evident from the example in Footnote 4). The proof of Theorem 5.1 builds off of the reduction of [CG24], for which $\\dot{\\Sigma}\\,\\bar{=}\\,\\Sigma_{\\mathsf{P R C}}=\\{0,1\\}$ . Their reduction, however, breaks down in the setting when $\\Sigma_{\\mathsf{P R C}}$ is no longer binary, and we introduce some new ideas (roughly, involving a hashing technique) to deal with the setting of larger alphabets. Details of the proof may be found in Appendix E. Finally, we remark that by combining Theorems 3.2, 4.1 and 5.1, we obtain Theorem 1.1, which establishes the existence of edit-robust watermarking schemes under Assumption 3.1. ", "page_idx": 9}, {"type": "text", "text": "On implementation of the watermarking scheme. A natural question is how feasible it is to implement the watermarking scheme $\\mathcal{W}$ [PRC, Model] of Theorem 5.1. The main limitation of our present theoretical results which may complicate a practical implementation is as follows: The alphabet size $|\\Sigma(\\lambda)|$ is required to grow exponentially in the inverse of the parameter $\\alpha$ (see the statement of Theorem E.2). In turn, the parameter $\\alpha$ is proportional to the entropy rate of the text needed to guarantee substring robustness (see Definition B.2 and the setting of $\\dot{\\beta}_{\\lambda}(\\ell)\\asymp\\alpha\\cdot\\ell$ in Theorem E.2). For typical LLMs, the alphabet size is likely smaller than our required value of $|\\Sigma(\\lambda)|$ given the entropy rates observed empirically in natural language. On the other hand, we believe that future work aimed at developing modifications of our watermarking scheme with an eye towards practical implementation will be successful. One idea which seems promising is to simulate a larger alphabet by grouping tokens together, and to aim accordingly for a slightly weaker robustness guarantee. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "NG was supported by a Fannie & John Hertz Foundation Fellowship and an NSF Graduate Fellowship. AM is supported in part by a Microsoft Trustworthy AI Grant, an ONR grant and a David and Lucile Packard Fellowship. We thank Gabe Schoenbach for helpful comments on an earlier draft of this work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[AK22] Scott Aaronson and Hendrik Kirchner. Watermarking gpt outputs, 2022. Available at https://www.scottaaronson.com/talks/watermark.ppt.   \n[AKO10] Alexandr Andoni, Robert Krauthgamer, and Krzysztof Onak. Polylogarithmic approximation for edit distance and the asymmetric query complexity, 2010.   \n[AN20] Alexandr Andoni and Negev Shekel Nosatzki. Edit distance in near-linear time: it\u2019s a constant factor. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pages 990\u20131001, 2020.   \n[AO11] Alexandr Andoni and Krzysztof Onak. Approximating edit distance in near-linear time, 2011.   \n$[\\mathrm{ARC^{+}01}]$ Mikhail J. Atallah, Victor Raskin, Michael Crogan, Christian Hempelmann, Florian Kerschbaum, Dina Mohamed, and Sanket Naik. Natural language watermarking: Design, analysis, and a proof-of-concept implementation. In Ira S. Moskowitz, editor, Information Hiding, pages 185\u2013200, Berlin, Heidelberg, 2001. Springer Berlin Heidelberg.   \n$[\\mathrm{ARH^{+}02}]$ Mikhail J. Atallah, Victor Raskin, Christian F. Hempelmann, Mercan Topkara, Radu Sion, Umut Topkara, and Katrina E. Triezenberg. Natural language watermarking and tamperproofing. In Information Hiding, 2002.   \n$[\\mathbf{B}\\mathbf{F}\\mathbf{J}^{+}94]$ Avrim Blum, Merrick Furst, Jeffrey Jackson, Michael Kearns, Yishay Mansour, and Steven Rudich. Weakly learning dnf and characterizing statistical query learning using fourier analysis. In Proceedings of the Twenty-Sixth Annual ACM Symposium on Theory of Computing, STOC \u201994, page 253\u2013262, New York, NY, USA, 1994. Association for Computing Machinery.   \n[BFKL94] Avrim Blum, Merrick Furst, Michael Kearns, and Richard J. Lipton. Cryptographic primitives based on hard learning problems. In Douglas R. Stinson, editor, Advances in Cryptology \u2014 CRYPTO\u2019 93, pages 278\u2013291, Berlin, Heidelberg, 1994. Springer Berlin Heidelberg.   \n[BGH17] Boris Bukh, Venkatesan Guruswami, and Johan H\u00e5stad. An improved bound on the fraction of correctable deletions. IEEE Transactions on Information Theory, 63(1):93\u2013 103, 2017.   \n[Bib09] Douglas Biber. 159 Corpus-Based and Corpus-driven Analyses of Language Variation and Use. In The Oxford Handbook of Linguistic Analysis. Oxford University Press, 12 2009.   \n[Blu03] Avrim Blum. Learning a function of r relevant variables. In Bernhard Sch\u00f6lkopf and Manfred K. Warmuth, editors, Learning Theory and Kernel Machines, pages 731\u2013733, Berlin, Heidelberg, 2003. Springer Berlin Heidelberg.   \n[BR17] Andrej Bogdanov and Alon Rosen. Pseudorandom functions: Three decades later. Cryptology ePrint Archive, Paper 2017/652, 2017. https://eprint.iacr.org/ 2017/652.   \n$[\\mathrm{CBD}^{+}24]$ Liang Chen, Yatao Bian, Yang Deng, Deng Cai, Shuaiyi Li, Peilin Zhao, and Kam fai Wong. Watme: Towards lossless watermarking through lexical redundancy, 2024.   \n[CG24] Miranda Christ and Sam Gunn. Pseudorandom error-correcting codes. In Proceedings of the 44th Annual International Cryptology Conference (CRYPTO). COLT, 2024.   \n[CGZ24] Miranda Christ, Sam Gunn, and Or Zamir. Undetectable watermarks for language models. In Proceedings of the 37th Annual Conference on Learning Theory (COLT), 2024.   \n[Cha16] Sourav Chatterjee. Concentration inequalities with exchangeable pairs (ph.d. thesis), 2016.   \n$[\\mathrm{FGJ}^{+}23]$ Jaiden Fairoze, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody, and Mingyuan Wang. Publicly detectable watermarking for language models, 2023.   \n[FGKP09] Vitaly Feldman, Parikshit Gopalan, Subhash Khot, and Ashok Kumar Ponnuswami. On agnostic learning of parities, monomials, and halfspaces. SIAM Journal on Computing, 39(2):606\u2013645, 2009.   \n[FKQR23] Dylan J. Foster, Sham M. Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making, 2023.   \n[FS95] Yoav Freund and Robert E. Schapire. A desicion-theoretic generalization of on-line learning and an application to boosting. In Paul Vit\u00e1nyi, editor, Computational Learning Theory, pages 23\u201337, Berlin, Heidelberg, 1995. Springer Berlin Heidelberg.   \n[GL16] Venkatesan Guruswami and Ray Li. Efficiently decodable insertion/deletion codes for high-noise and high-rate regimes, 2016.   \n[GLLH24] Chenchen Gu, Xiang Lisa Li, Percy Liang, and Tatsunori Hashimoto. On the learnability of watermarks for language models. In The Twelfth International Conference on Learning Representations, 2024.   \n[GRS19] Venkatesan Guruswami, Atri Rudra, and Madhu Sudan. Essential Coding Theory. 2019.   \n[GRV11] Elena Grigorescu, Lev Reyzin, and Santosh Vempala. On noise-tolerant learning of sparse parities and related problems. In Jyrki Kivinen, Csaba Szepesv\u00e1ri, Esko Ukkonen, and Thomas Zeugmann, editors, Algorithmic Learning Theory, pages 413\u2013424, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg.   \n[GW17] Venkatesan Guruswami and Carol Wang. Deletion codes in the high-noise and high-rate regimes. IEEE Transactions on Information Theory, 63(4):1961\u20131970, 2017.   \n$[\\mathrm{HCW}^{+}24]$ Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, and Heng Huang. Unbiased watermark for large language models. In The Twelfth International Conference on Learning Representations, 2024.   \n[HS21a] Bernhard Haeupler and Amirbehshad Shahrasbi. Synchronization strings and codes for insertions and deletions \u2013 a survey, 2021.   \n[HS21b] Bernhard Haeupler and Amirbehshad Shahrasbi. Synchronization strings: Codes for insertions and deletions approaching the singleton bound. J. ACM, 68(5), sep 2021.   \n$[\\mathrm{HZZ}^{+}24]$ Baihe Huang, Hanlin Zhu, Banghua Zhu, Kannan Ramchandran, Michael I. Jordan, Jason D. Lee, and Jiantao Jiao. Towards optimal statistical watermarking, 2024.   \n[Imp95] R. Impagliazzo. A personal view of average-case complexity. In Proceedings of Structure in Complexity Theory. Tenth Annual IEEE Conference, pages 134\u2013147, 1995.   \n$[\\mathrm{KGW}^{+}23]$ John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 17061\u201317084. PMLR, 23\u201329 Jul 2023.   \n$[\\mathrm{KGW}^{+}24]$ John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, and Tom Goldstein. On the reliability of watermarks for large language models, 2024.   \n[KTHL23] Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy Liang. Robust distortion-free watermarks for language models, 2023.   \n[LB24] Yepeng Liu and Yuheng Bu. Adaptive text watermark for large language models, 2024.   \n$[\\mathrm{LPH}^{+}24\\mathrm{a}]$ Aiwei Liu, Leyi Pan, Xuming Hu, Shuang Li, Lijie Wen, Irwin King, and Philip S. Yu. An unforgeable publicly verifiable watermark for large language models. In The Twelfth International Conference on Learning Representations, 2024.   \n$[\\mathrm{LPH}^{+}24\\mathrm{b}]$ Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, and Lijie Wen. A semantic invariant robust watermark for large language models. In The Twelfth International Conference on Learning Representations, 2024.   \n[Nav01] Gonzalo Navarro. A guided tour to approximate string matching. ACM Comput. Surv., 33(1):31\u201388, mar 2001.   \n[ODo14] Ryan ODonnell. Analysis of Boolean Functions. Cambridge University Press, 2014.   \n[OR07] Rafail Ostrovsky and Yuval Rabani. Low distortion embeddings for edit distance. J. ACM, 54(5):23\u2013es, oct 2007.   \n[PHZS24] Qi Pang, Shengyuan Hu, Wenting Zheng, and Virginia Smith. Attacking llm watermarks by exploiting their strengths, 2024.   \n[RS23] Upul Rupassara and Bishnu Sedai. On the convergence of hypergeometric to binomial distributions. Computer and Information Science, 16:15, 07 2023.   \n[RvdOV19] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[SHB16] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany, August 2016. Association for Computational Linguistics.   \n$[\\mathrm{TJY}^{+}24]$ Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction, 2024.   \n[TTI05] Mercan Topkara, Cuneyt M. Taskiran, and Edward J. Delp III. Natural language watermarking. volume 5681, pages 441\u2013452. SPIE, 2005.   \n[Val15] Gregory Valiant. Finding correlations in subquadratic time, with applications to learning parities and the closest pair problem. J. ACM, 62(2), may 2015. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[WHZH23] Yihan Wu, Zhengmian Hu, Hongyang Zhang, and Heng Huang. Dipmark: A stealthy, efficient and resilient watermark for large language models, 2023. ", "page_idx": 12}, {"type": "text", "text": "$[\\mathbf{Y}\\mathbf{L}\\mathbf{K}^{+}22]$ Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN. In International Conference on Learning Representations, 2022.   \n$[\\mathrm{YXK}^{+}22]$ Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research, 2022. Featured Certification.   \n[ZALW24] Xuandong Zhao, Prabhanjan Vijendra Ananth, Lei Li, and Yu-Xiang Wang. Provable robust watermarking for AI-generated text. In The Twelfth International Conference on Learning Representations, 2024.   \n[Zam24] Or Zamir. Excuse me, sir? your language model is leaking (information), 2024.   \n$[Z\\mathrm{EF}^{+}23]$ Hanlin Zhang, Benjamin L. Edelman, Danilo Francati, Daniele Venturi, Giuseppe Ateniese, and Boaz Barak. Watermarks in the sand: Impossibility of strong watermarking for generative models, 2023. ", "page_idx": 12}, {"type": "text", "text": "A Additional related work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Watermarking has a long history, dating back to the work of [TTI05, $\\mathrm{ARC}^{+}01$ , $\\mathrm{ARH}^{+}02]$ : generally speaking, these early works considered procedures which embed a watermark into a given text, by altering the text in some subtle way. Spurred by the remarkable abilities of generative models, in particular large language models (LLMs), there has been a recent resurgence of interest in watermarking. In contrast to the classical works, most recent works (including this paper) aim to embed the watermark in a way that does not significantly alter the distribution of the generated content. This task is enabled by the autoregressive nature of LLMs, which generate each successive token using some fresh randomness. ", "page_idx": 13}, {"type": "text", "text": "Recent works [ZALW24, AK22, $\\mathrm{KGW}^{+}23]$ construct watermarking schemes by pereferentially generating certain tokens at each step of generation. For instance, [ZALW24] partitions tokens into two equal-sized sets, a \u201cgreen set\u201d and a \u201cred set\u201d. It slightly increases the probabilities of green tokens while slightly decreasing the probabilities of red tokens. A watermark is then detected if the proportion of green tokens in a text segment is noticeably higher than $\\frac{1}{2}$ . The works of [AK22, $\\mathrm{KGW}^{+}23]$ build on this protocol by generating the green and red sets at each position using a pseudorandom function seeded with a window of some number $k$ of previous tokens. These schemes all introduce some degree of noticeable bias to the watermarked model\u2019s output, by increasing the probability of certain $k$ -grams. ", "page_idx": 13}, {"type": "text", "text": "Distortion-freeness vs undetectability. [KTHL23] introduced a notion called distortion-freeness, which posits that a single text sample from the watermarked model is distributed identically to a single sample from the true model. [KTHL23] construct watermarking schemes satisfying distortionfreeness, with nontrivial edit-robustness guarantees: their watermarks have robustness to a constant fraction of substitutions (Lemma 2.5 within), but only to a slightly sub-constant fraction of edits when insertions and deletions are also allowed (Lemma 2.6 within).7 $[\\mathrm{HCW}^{+}24$ , WHZH23] also developed a similar approach to [KTHL23]. ", "page_idx": 13}, {"type": "text", "text": "[CGZ24] defined the stronger notion of undetectability (the focus of the present work), which requires that no computationally efficient algorithm which makes any number of queries to the language model can distinguish between the watermarked model and the original one. [CGZ24] constructed the first provably undetectable watermark by using a similar technique to [AK22, $\\mathrm{KGW}^{+}23]$ with the modification that they dynamically adjust the parameter $k$ denoting the length of text used to seed the pseudorandom function. This dynamic adjustment ensures that the sequence of $k$ tokens has sufficiently high entropy, which ensures undectability.8 Undetectable watermarking schemes were also constructed by [Zam24], which focused specifically on the application to steganography (see also [CG24] for robust steganography schemes), and $[\\mathrm{FGJ}^{+}23]$ , which concentrated on schemes with public attribution. ", "page_idx": 13}, {"type": "text", "text": "It is natural to wonder: why should one should prefer undetectable watermarking schemes over distortion-free ones? The simplest reason is that, as a requirement on generative models, distortionfreeness is very weak, as illustrated in the below example. ", "page_idx": 13}, {"type": "text", "text": "Example A.1 (Distortion-freeness). Consider the alphabet $\\Sigma=\\{0,1\\}$ and suppose that $\\overline{{\\mathsf{M o d e l}}}$ is the uniformly random distribution on outputs ${\\mathfrak{t}}\\in\\{0,1\\}^{n}$ , for some $n\\in\\mathbb N$ . Consider the watermarking scheme which draws a key $\\mathsf{s k}\\sim\\mathrm{Unif}(\\{0,1\\}^{n})$ and at each call to $\\mathsf{W a t}(\\mathsf{s k})$ , simply returns $\\mathsf{s k}$ . This scheme is distortion-free since the distribution of $\\mathsf{s k}$ is uniform, but has the property that once Wat is called once, the output of all future calls is determined (and so it is clearly not undetectable). ", "page_idx": 13}, {"type": "text", "text": "Example A.1 indicates that distortion-free watermarks may suffer from insufficient diversity in model outputs: in Example A.1 there is no variation whatsoever in the outputs. The distortion-free schemes of [KTHL23] are only slightly better on this front: they sample a long random key $\\mathsf{s k}$ and then generate a sequence of text from Model in a way that aims to be close to a random shift of sk. Having fixed the key $\\mathsf{s k}$ , the output text from the scheme of [KTHL23] is a deterministic function of the shift, meaning that at most $\\operatorname{len}(\\mathbf{s}\\mathbf{k})$ distinct sequences of text can be generated altogether. Moreover, by the birthday paradox, one would expect to see a repeat answer after only $\\sqrt{\\log(\\mathsf{s k})}$ calls. The effective number of distinct samples of the distribution of output text of [KTHL23] may be even smaller than $\\operatorname{len}(\\mathbf{s}\\mathbf{k})$ , if different shifts of $\\mathsf{s k}$ lead to similar text sequences (as they will for, e.g., the uniform Model in Example A.1). ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Beyond avoiding the issue of insufficient diversity, the stronger guarantees afforded by undetectability offer a more compelling argument for adopting watermarking in the first place. Watermarking is just one of many technologies that can be built into AI models to achieve better alignment with human values. Even if each of these degrades performance in minor but non-negligible ways, the aggregate impact of all of these additions could potentially potentially be substantial. The guarantee of undetectability, namely that no matter what efficient downstream algorithms are applied to the model\u2019s output, the result will be essentially the same, means that the addition of a watermark can only be blamed for a negligible amount of the overall performance degradation. ", "page_idx": 14}, {"type": "text", "text": "Additional work on watermarking. $[\\mathrm{HZZ}^{+}24]$ formulate a variant of the watermarking problem which is purely statistical in nature, and characterize the optimal rate of watermarking. [GLLH24] show that watermarking schemes can be learned, using a student-teacher framework. $[\\mathrm{KGW}^{+}23$ , CG24, PHZS24, $Z\\mathrm{EF}^{+}\\bar{2}3]$ develop various attacks on watermarking schemes. Generally speaking, these attacks are relatively expensive in terms of calls to the language model (as in [CG24, PHZS24]) or an additional \u201cquality oracle\u201d (as in $[Z\\mathrm{EF}^{+}23]$ ). Moreover, a sufficiently motivated adversary could simply train their own (unwatermarked) AI model and generate content from it. Therefore, we view robust watermarking as targeted more at \u201chonest\u201d or \u201clazy\u201d adversaries who are making edits to either improve the quality of content without explicitly trying to remove a watermark (e.g., for AI-generated news articles) or have very few resources to remove the watermark (e.g., a student asking an LLM to write their essay right before the deadline). ", "page_idx": 14}, {"type": "text", "text": "Finally, we remark that there is a sizeable body of work focusing on empirical approaches to watermarking (e.g., $[\\mathrm{LPH}^{+}24\\mathrm{b}$ , $\\mathrm{LPH}^{+}24\\mathrm{a}$ , LB24, $\\mathrm{CBD}^{+}24$ , $\\mathrm{KGW}^{+}\\bar{2}4\\mathrm{]},$ ). ", "page_idx": 14}, {"type": "text", "text": "Error-correcting codes for insertions and deletions. A recent line of work (e.g., [GL16, BGH17, GW17, HS21a, HS21b]) has focused on developing error-correcting codes for insertions and deletions. We remark that the idea of indexing is implicit in some of this work. One focus of these papers has been on obtaining codes which can correct a constant fraction of insertions and deletions with smaller (e.g., constant-size) alphabets. A common technique for this task is that of of synchronization strings [HS21a, HS21b]: it proceeds by increasing the alphabet size by a constant factor and attaching to each message character an \u201cauxiliary character\u201d coming from a so-called synchronization string, which has certain properties that make it easier to align a noisy codeword with the clean codeword. Unfortunately, the same synchronization string must be used with each call to the encoding algorithm, which makes it unclear how to construct such codes that are pseudorandom. ", "page_idx": 14}, {"type": "text", "text": "B Additional preliminaries ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Watermarking language models ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A language model Model over alphabet $\\Sigma$ which, for any positive integer $i$ , takes as input a sequence of tokens $\\mathfrak{t}_{1:i-1}=\\left(\\mathfrak{t}_{1},\\ldots,\\mathfrak{t}_{i-1}\\right)$ already output by the model and produces a distribution ${\\sf M o d e l}(\\sf t_{\\it i}=$ $\\cdot\\mid\\sf t_{1:i-1})\\in\\Delta(\\Sigma)$ , representing the distribution of the next token, conditioned on $\\mathtt{t}_{1:i-1}$ . We assume that Model is computationally efficient, meaning that it runs in time $\\mathrm{poly}(i,\\log|\\Sigma|)$ . We also assume that there is some token $\\mathtt{E N D}\\in\\ \\Sigma$ representing the end of the model\u2019s response. As a matter of convention, we assume that all subsequent tokens after a END token are also END (i.e., ${\\mathsf{M o d e l}}(\\mathtt{E N D}\\mid\\mathtt{t}_{1:i-1})=1$ if some $\\mathtt{t}_{j}$ for $j<i$ is END). Given a language model Model, we introduce the notation $\\overrightarrow{M o d e l}$ : given any sequence $\\mathfrak{t}_{1:i-1}\\,\\in\\,\\Sigma^{i-1}$ (which may be the empty sequence) and $m>0$ , the distribution $\\overline{{\\mathsf{M o d e l}}}(\\mathfrak{t}_{i:i+m-1}=\\cdot\\mid\\mathfrak{t}_{1:i-1})\\in\\Sigma^{m}$ is defined as follows: we sequentially generate $\\mathtt{t}_{i},\\ldots,\\mathtt{t}_{i+m-1}$ , where for each $0\\leq j<m$ , $\\mathtt{t}_{j}$ is distributed as ${\\sf M o d e l}(\\mathfrak{t}_{j}=\\cdot\\mid\\mathfrak{t}_{1:j-1})$ . In the case $i=1$ (i.e., $\\mathtt{t}_{1:i-1}=\\varnothing_{.}$ ), then we denote an output of Model a $\\mathrm{~t~}\\sim\\overline{{\\mathsf{M o d e l}}}$ . ", "page_idx": 14}, {"type": "text", "text": "An intuitive requirement on Model that allows for watermarking is that outputs of Model must be highentropy; indeed, a model that outputs a deterministic sequence of text is impossible to watermark, since the only distribution which is indistinguishable from its output is that same deterministic sequence of text. As we aim to measure the entropy of individual model outputs, we use the following notion of empirical entropy, following [CGZ24]: ", "page_idx": 15}, {"type": "text", "text": "Definition B.1 (Empirical entropy). Given a language model Model, a sequence ${\\mathfrak{t}}\\in\\Sigma^{L}$ , and $i,j\\in\\mathfrak{t}$ , we define the empirical entropy of Model for t on subsequence $[i,j]$ to be: ", "page_idx": 15}, {"type": "equation", "text": "$$\nH_{\\mathsf{e}}^{[i:j]}(\\mathsf{t},\\mathsf{M o d e l}):=-\\log\\overline{{\\mathsf{M o d e l}}}(\\mathsf{t}_{i:j}\\mid\\mathsf{t}_{1:i-1}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By definition of $\\overline{{\\mathsf{M o d e l}}}$ , we have $-\\log\\mathbb{M}\\mathsf{o d e l}(\\mathfrak{t}_{i:j}\\mid\\mathfrak{t}_{1:i-1})=-\\log\\mathrm{Pr}_{\\bar{\\mathfrak{t}}\\sim\\overline{{\\mathbb{M}\\ o d e l}}}(\\bar{\\mathfrak{t}}_{i:j}=\\mathfrak{t}_{i:j}\\mid\\bar{\\mathfrak{t}}_{1:i-1}=$ $\\sf t_{1:i-1})$ . When Model is clear from context, we typically drop Model from the notation, so that $H_{\\mathrm{e}}^{[i:j]}(\\mathrm{t}):=H_{\\mathrm{e}}^{[i:j]}(\\mathrm{t},\\mathsf{M o d e l})$ , and moreover write $H_{\\mathrm{e}}^{i}(\\mathbf{t}):=H_{\\mathrm{e}}^{i}(\\mathbf{t},\\mathsf{M o d e l}),H_{\\mathrm{e}}^{[i:j)}(\\mathbf{t}):=H_{\\mathrm{e}}^{[i:j-1]}(\\mathbf{t})$ . Finally, note that from the chain rule of probability, $\\begin{array}{r}{H_{\\mathrm{e}}^{[i:j]}(\\mathfrak{t})=\\sum_{a=i}^{j}H_{\\mathrm{e}}^{a}(\\mathfrak{t})}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Our main goal in this paper is to construct a watermarking scheme for Model, which is a tuple $\\mathcal{W}=$ (Setup, Wat, Detect) of probabilistic polynomial-time algorithms with the following semantics: ", "page_idx": 15}, {"type": "text", "text": "\u2022 Setup $(1^{\\lambda})$ takes as input a security parameter $\\lambda\\in\\mathbb N$ and outputs a secret key sk. \u2022 $\\mathsf{W a t}(1^{\\lambda},\\mathsf{s k})$ takes as input $\\lambda,s{\\sf k}$ and outputs a sequence $\\mathbf{t}=\\mathbf{t}_{1:L}\\in\\Sigma^{L}$ , for some $L\\in\\mathbb N$ . \u2022 Detect $(1^{\\lambda},{\\mathsf{s k}},{\\mathsf{t}})$ takes as input a sequence $\\uptau\\in\\Sigma^{\\ell}$ for some $\\ell\\in\\mathbb{N}$ and outputs either True or False, denoting whether t is detected as being watermarked. ", "page_idx": 15}, {"type": "text", "text": "We next define the security properties we desire in our watermarks, in terms of a security parameter $\\lambda\\ \\in\\ \\mathbb N$ . Some of our results operate in the \u201clarge-alphabet\u201d setting, meaning that the size of the alphabet for the language model can depend (polynomially) on $\\lambda$ . Formally, we consider a language model family indexed by $\\lambda$ , $(\\mathsf{M o d e l}(\\lambda))_{\\lambda\\in\\mathbb{N}}$ , where the alphabet for ${\\sf M o d e l}(\\lambda)$ is denoted by $\\Sigma(\\lambda)$ . Our watermarking procedure should produce a string in $\\Sigma(\\lambda)^{\\star}$ which is computationally indistinguishable from an output of ${\\sf M o d e l}(\\lambda)$ . In order for us to establish a computational separation between the process of outputting watermarked text and an adversary running in time given by an arbitrarily large polynomial in $\\lambda$ , we assume that the length of an output $\\mathsf{t}\\sim\\overline{{{\\sf M o d e l}}}(\\lambda)$ is bounded by $L_{\\mathrm{max}}(\\lambda)$ , where $L_{\\mathrm{max}}(\\lambda)$ is some function growing as $\\mathrm{poly}(\\lambda)$ .9 When the value of $\\lambda$ is clear from context, we drop the argument $\\lambda$ and simply write Model, $\\Sigma$ . ", "page_idx": 15}, {"type": "text", "text": "Definition B.2 (Soundness, Undetectability, ${\\mathcal{E}}^{\\circ}.$ -Robustness). Consider a language model family Model $(\\lambda)$ together with a watermarking scheme $\\mathcal{W}=$ (Setup, Wat, Detect). We define the following properties of $\\mathcal{W}$ : ", "page_idx": 15}, {"type": "text", "text": "\u2022 $\\mathcal{W}$ is sound if for all $\\lambda\\in\\mathbb N$ and $\\mathbf{t}\\in\\Sigma^{\\leq L_{\\operatorname*{max}}(\\lambda)}$ , $\\operatorname*{Pr}_{{\\mathsf{s k}}\\gets{\\mathsf{S e t u p}}(1^{\\lambda})}\\left({\\mathsf{D e t e c t}}(1^{\\lambda},{\\mathsf{s k}},{\\mathsf{t}})={\\mathsf{T r u e}}\\right)\\leq\\mathsf{n e g}|(\\lambda).$   \n\u2022 $\\mathcal{W}$ is undetectable if for all $\\lambda\\in\\mathbb N$ and any probabilistic polynomial time algorithm Dist, $\\left|\\operatorname*{Pr}\\left({\\mathsf{D i s t}}^{\\mathsf{\\overline{{M o d e l}}}}(1^{\\lambda})=1\\right)-\\operatorname*{Pr}\\left({\\mathsf{D i s t}}^{\\mathsf{W a t}(1^{\\lambda},\\mathsf{s k})}(1^{\\lambda})=1\\right)\\right|\\leq{\\mathsf{n e g l}}(\\lambda),$ where $\\mathsf{D i s t}^{\\mathcal{O}}$ means that Dist can make repeated calls to $\\scriptscriptstyle\\mathcal{O}$ , which generate a sample from the corresponding distribution (either t $\\sim\\overline{{\\mathsf{M o d e l}}}$ or $\\mathbf{t}\\sim\\mathsf{W a t}(1^{\\lambda},\\mathsf{s k}))$ .   \n\u2022 Fix some family of channels $(\\mathcal{E}(\\lambda))_{\\lambda\\in\\mathbb{N}}$ , where each $\\mathcal{E}(\\lambda)$ is a collection of channels $\\mathcal{E}:\\Sigma(\\lambda)^{\\star}\\rightarrow\\Delta(\\Sigma(\\lambda)^{\\star})$ , and a family of functions $\\beta_{\\lambda}:\\mathbb{N}\\rightarrow\\mathbb{N}$ . Then $\\mathcal{W}$ is defined to be $\\beta$ -substring robust to the family $(\\mathcal{E}(\\bar{\\lambda}))_{\\lambda}$ if for each $\\lambda\\in\\mathbb N$ , and each channel $\\mathcal{E}\\in\\mathcal{E}(\\lambda)$ , ", "page_idx": 15}, {"type": "text", "text": "$\\operatorname*{Pr}_{\\substack{\\mathrm{sk}\\leftarrow\\mathrm{Setup}(1^{\\lambda})}}\\left(\\exists i,j\\in[L_{\\mathsf{m a x}}(\\lambda)]\\;\\mathrm{s.t.}\\;\\mathsf{D e t e c t}(1^{\\lambda},\\mathsf{s k},\\mathsf{t}^{\\prime})=\\mathsf{F a l s e\\;a n d}\\;H_{\\mathsf{e}}^{[i:j)}(\\mathsf{t})\\geq\\beta_{\\lambda}(j-i)\\cdot\\ln(i)\\right)\\,,$ $H_{\\mathbf{e}}^{[i:j)}(\\mathbf{t})\\geq\\beta_{\\lambda}(j-i)\\cdot\\ln{\\left|\\Sigma(\\lambda)\\right|}\\Big)\\leq\\mathsf{n e g}|(\\lambda).$ ", "page_idx": 15}, {"type": "text", "text": "Above, we use the convention that t is to be interpreted as an element of $\\Sigma^{L_{\\mathtt{m a x}}(\\lambda)}$ by padding it with instances of the terminal token END. Thus, substring robustness requires that if we choose any substring of length $\\ell$ from the output of Wat and pass it through an channel $\\mathcal{E}$ , then the output of the channel is still detected as watermarked with high probability. ", "page_idx": 15}, {"type": "text", "text": "B.2 Notation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Given $n\\,\\in\\,\\mathbb{N}$ together with a mapping $\\pi:[n]\\,\\rightarrow\\,[n]$ and a string $x\\,=\\,(x_{1},.\\,.\\,.\\,,x_{n})\\,\\in\\,\\Sigma^{n}$ , we write $\\pi\\circ x\\,:=\\,\\left(x_{\\pi(1)},\\dots,x_{\\pi(n)}\\right)\\ \\in\\ \\Sigma^{n}$ . Given a mapping $\\phi\\,:\\,\\mathcal{X}\\,\\rightarrow\\,\\mathcal{Y}$ for (finite) sets $\\mathcal{X},\\mathcal{Y}$ and a distribution $P\\in\\Delta(\\mathcal{X})$ , we let $\\phi\\circ P\\in\\Delta(\\mathcal{Y})$ be the push-forward distribution for $\\phi$ , i.e., $(\\phi\\circ P)(y)=P(\\phi^{-1}(y))$ . ", "page_idx": 16}, {"type": "text", "text": "Given a mapping $\\psi:\\mathcal{X}\\rightarrow\\mathcal{Y}$ for finite sets $\\mathcal{X},\\mathcal{Y}$ , we let $\\psi({\\mathcal{X}}):=\\{\\psi(x)\\;:\\;x\\in{\\mathcal{X}}\\}\\subset{\\mathcal{Y}}$ , and for any ${\\mathcal{Y}}^{\\prime}\\subset{\\mathcal{Y}},\\,\\psi^{-1}({\\mathcal{Y}}^{\\prime}):=\\{x\\in{\\mathcal{X}}\\,:\\,\\psi(x)\\in{\\mathcal{Y}}^{\\prime}\\}$ . Finally, for a tuple $x=(x_{1},\\ldots,x_{n})\\in\\mathcal{X}^{n}$ , we let $\\psi(x):=(\\psi(x_{1}),\\dots,\\psi(x_{n}))\\in\\mathcal{V}^{n}$ . ", "page_idx": 16}, {"type": "text", "text": "For $y\\in\\Sigma^{N}$ and $\\boldsymbol J=(j_{1},\\dots,j_{t})$ , we let $y J\\in\\Sigma^{t}$ denote the vector $y_{J}:=\\left(y_{j_{1}},\\ldots,y_{j_{t}}\\right)$ . Given a set $\\boldsymbol{S}$ , $m\\in\\mathbb{N}$ , and a string $y\\in S^{m}$ , we define U $\\mathsf{I n i q u e}(y):=\\{y_{i}\\ :\\ i\\in[m]\\}\\subset\\bar{\\mathcal{S}}$ , i.e., Unique $(y)$ is the set of distinct elements of $y$ . ", "page_idx": 16}, {"type": "text", "text": "C Secret-key substitution PRCs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Hardness assumptions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we discuss several standard conjectures on the hardness of PAC learning which imply Assumption 3.1. We first recall the definition of PAC learning with respect to the uniform distribution: ", "page_idx": 16}, {"type": "text", "text": "Definition C.1 (PAC learning for the uniform distribution). Suppose that for each $n\\in\\mathbb N$ , we are given a class ${\\mathcal{F}}_{n}$ of boolean functions indexed by strings $s\\in\\{0,1\\}^{\\bar{\\ell}(n)}$ , for some function $\\ell:\\mathbb{N}\\to\\mathbb{N}$ ; write $\\mathcal{F}_{n}\\,=\\,\\{F_{s}\\,:\\,\\{0,1\\}^{n}\\,\\to\\,\\{0,1\\}\\}_{s\\in\\{0,1\\}^{\\ell(n)}}$ . Given $q\\,\\in\\,[0,1/2)$ , we say that ${\\mathcal{F}}=({\\mathcal{F}}_{n})_{n}$ is $\\left(\\epsilon(n),\\delta(n)\\right)$ -PAC learnable with noise level $q$ if there is a $\\mathrm{poly}(n)$ -time algorithm $\\mathsf{A l g}$ which, for some $m(n)\\in\\mathbb{N}$ , takes as input a dataset $\\bar{D^{'}}=((x_{i},y_{i}))_{i\\in[m(n)]}$ and $X\\,\\in\\,\\{0,1\\}^{n}$ , and satisfies the following with probability $1-\\delta$ over $s\\sim\\mathrm{Unif}(\\{0,1\\}^{\\ell(n)}),x_{i}\\sim\\mathrm{Unif}(\\{0,1\\}^{n}),y_{i}=F_{s}(x_{i})$ $(i\\in[m(n)])$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{X\\sim\\mathrm{Unif}(\\{0,1\\}^{n})}\\left(\\mathsf{A l g}((x_{i},y_{i})_{i\\in[m(n)]},X)\\neq F_{s}(X)\\right)\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proposition C.1. Suppose that a family of functions $\\mathcal{F}=(\\mathcal{F}_{n})_{n\\in\\mathbb{N}}$ is not $(1/3,1/3)$ -PAC learnable with noise level $q\\,\\in\\,[0,1/2)$ (per Definition ${\\cal C}.{\\cal I}$ ). Then, with $n(\\lambda)=\\lambda_{*}$ , the family of functions $(\\mathcal{F}_{n(\\lambda)})_{\\lambda}$ is a weak $P R F$ with noise level $q$ (per Definition 3.1). ", "page_idx": 16}, {"type": "text", "text": "Proposition C.1 is essentially folklore (see, e.g., [BR17]), but we provide a proof sketch for completeness: ", "page_idx": 16}, {"type": "text", "text": "Proof sketch of Proposition $C.I$ . Suppose that a polynomial-time adversary Adv satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{E}_{s\\sim\\{0,1\\}^{n(\\lambda)}}\\left[\\widetilde{\\mathsf{A d v}}^{F_{s}(\\cdot)}(1^{n(\\lambda)})=1\\right]-\\mathbb{E}_{F_{\\mathrm{Unit}}}\\left[\\widetilde{\\mathsf{A d v}}^{F_{\\mathrm{Unit}}}(1^{n(\\lambda)})\\right]\\right|\\ge\\lambda^{-O(1)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $F_{\\mathsf{U n i f}}:\\{0,1\\}^{n}\\to\\{0,1\\}$ denotes a uniformly random function. Suppose that Adv makes at most $m(\\lambda)$ queries to its oracle (either $F_{s}(\\cdot)$ or $F_{\\mathrm{Unif}})$ . Consider the algorithm which receives $m(n)$ samples $\\left({x_{i},y_{i}}\\right)$ with $x_{i}\\,\\sim\\,\\mathrm{Unif}(\\{0,1\\}^{n}),y_{i}\\,=\\,F_{s}(x_{i})$ for an unknown $s$ together with $X\\ \\sim\\ \\mathrm{Unif}\\big(\\{0,1\\}^{n(\\lambda)}\\big)$ , chooses a uniformly random index $m^{\\prime}\\;\\in\\;[m(\\lambda)\\mathrm{~-~}1]$ , and then runs Adv with the oracle responses given by $(((x_{i},\\mathbf{\\dot{\\boldsymbol{y}}}_{i}))_{i\\in[m^{\\prime}]},(X,b),(x_{j},y_{j})_{j\\in[m^{\\prime}+2,n(\\lambda)]})$ , where $x_{j}\\sim\\operatorname{Unif}(\\{0,1\\}^{n}),y_{j}\\sim\\operatorname{Unif}(\\{0,1\\})$ for $j\\geq m^{\\prime}+2$ , for each value of $b\\in\\{0,1\\}$ . By a hybrid argument, for some $b\\in\\{0,1\\}$ , this algorithm must have $\\lambda^{-O(1)}$ advantage at predicting $F_{s}(X)$ . A standard boosting argument [FS95] then establishes the existence of a polynomial-time algorithm that $(1/3,1/3)$ -PAC learns the family $\\mathcal{F}$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "By Proposition C.1, either of the two assumptions below implies Assumption 3.1: ", "page_idx": 16}, {"type": "text", "text": "Assumption C.2 (Sparse noisy parity). There is no polynomial-time algorithm which $(1/3,1/3)\u2013P A C$ learns the family $(\\mathcal{F}_{n})_{n}$ of $\\log(n)$ -sparse parities with noise level $q=1/3$ , i.e., ${\\mathcal F}_{n}=\\{F_{s}(x)=$ $\\begin{array}{r}{\\bigoplus_{i\\in[n]}x_{i}s_{i}\\;:\\;s\\in\\{0,1\\}^{n},\\|s\\|_{1}=\\log(n)\\}}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "It is straightforward to see (using a hybrid argument) [Blu03] that Assumption C.2 is equivalent to the assumption that there is no polynomial-time algorithm which identifies the hidden $\\log(n)$ -sparse string $s\\in\\bar{\\{0,1\\}}^{n}$ with constant probability. ", "page_idx": 17}, {"type": "text", "text": "Assumption C.3 ([BFKL94], Section 2.3). There is no polynomial-time algorithm which $(1/3,1/3)$ - PAC learns the family $(\\mathcal{G}_{n})_{n}$ defined below, with noise level $q=0$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\gamma}_{n}=\\left\\{G_{S_{1},S_{2}}(x)=\\mathsf{M a j}(x_{i}:\\ i\\in S_{1})\\oplus\\bigoplus_{i\\in S_{1}}x_{i}\\ |\\ \\mathcal{S}_{1},S_{2}\\subset[n]\\ a r e\\ d i s j o i n t\\ s u b s e t s\\ o f s i z e\\log(n)/2\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly to the case for Assumption C.2, Assumption C.3 is equivalent to the assumption that there is no polynomial-time algorithm which identifies the function $G_{S_{1},S_{2}}$ with constant probability. We remark that [FGKP09, Theorem 3] shows that an algorithm which learns $\\log(n)$ -sparse parities at noise level $\\textstyle{\\frac{1}{2}}-O(1/n)$ implies the existence of an algorithm which learns (noiseless) $\\log(n)$ -juntas (with both learning algorithms being over the uniform distribution). Notice, though, that Assumption C.2 assumes there is no efficient algorithm which learns $\\log(n)$ -sparse noisy parities with noise level $q$ bounded away from $1/2$ , which is stronger than assuming hardness of learning $\\log(n)$ -sparse noisy parities with noise level $q=1/2-O(1)$ . Thus, even given the reduction of [FGKP09], Assumption C.3 does not appear to imply Assumption C.2. ", "page_idx": 17}, {"type": "text", "text": "C.2 Additional preliminaries ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Weakly substitution-bounded channels. We will show that our PRCs are robust to a slightly more broad class of channels which are only guaranteed to introduce a bounded number of substitutions with high probability with respect to a uniformly random string from the channel\u2019s alphabet. The codes of [CG24] also enjoy robustness to such channels; thus, the robustness guarantee for our substitution codes is directly comparable to that of [CG24]. ", "page_idx": 17}, {"type": "text", "text": "Definition C.2 (Weakly substitution bounded channel). We say that a channel $\\mathcal{E}$ over an alphabet $\\Sigma$ is $p$ -weakly-substitution-bounded if for any $n\\in\\mathbb N$ , $\\operatorname*{Pr}_{y\\sim\\mathrm{Unif}(\\Sigma^{n}),z\\sim\\mathcal{E}(y)}$ $\\left(D_{\\mathsf{H a m}}(y,z)>p n\\right)\\leq$ ${\\mathsf{n e g l}}(n)$ . ", "page_idx": 17}, {"type": "text", "text": "Noise sensitivity. For $x\\in\\{0,1\\}^{n}$ , we let $N_{\\rho}(x)\\,\\in\\,\\Delta(\\{0,1\\}^{n})$ denote the distribution over $y$ where $y_{i}=x_{i}$ with probability $1/2\\!+\\!\\rho/2$ (and $y_{i}=1\\!-\\!x_{i}$ with probability $1/2\\!-\\!\\rho/2)$ , independently for each $i$ . Note that $y\\sim N_{1-2\\delta}(x)$ is generated by filpping each bit of $x$ with probability $\\delta$ . Define ${\\bf N S}_{\\delta}[f]:=\\operatorname*{Pr}_{x\\sim\\mathrm{Unif}(\\{0,1\\}^{n}),y\\sim N_{1-2\\delta}(x)}\\left(f(x)\\neq f(y)\\right)$ . Also write $\\begin{array}{r}{\\mathbf{W}^{i}[f]=\\sum_{S\\subset[n],|S|=i}{\\hat{f}}(S)^{2}}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma C.4 (Theorem 2.49 of [ODo14]). For any $f:\\{0,1\\}^{n}\\,\\rightarrow\\,\\{-1,1\\}$ , we have ${\\bf N S}_{\\delta}[f]=$ $\\begin{array}{r}{\\frac12\\sum_{i=0}^{n}(1-(1-2\\delta)^{i})\\cdot\\mathbf{W}^{i}[f]}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "Corollary C.5. For any $f:\\{0,1\\}^{n}\\rightarrow\\{0,1\\}$ , we have $\\begin{array}{r}{\\mathbf{NS}_{\\delta}[f]=\\frac{1}{2}-\\sum_{i=0}^{n}\\phi_{i}\\cdot(1-2\\delta)^{i},}\\end{array}$ , for some $\\phi_{0},\\ldots,\\phi_{n}\\geq0$ satisfying $\\phi_{0}+\\cdot\\cdot+\\phi_{n}=1/2$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. The corollary is an immediate consequence of Lemma C.4 applied to the mapping $x\\mapsto$ $2f(x)-1$ , together with the fact that any $f:{\\bar{\\{0,1\\}}}^{n}\\rightarrow\\{-1,1\\}$ satisfies $\\textstyle\\sum_{i=0}^{n}\\mathbf{W}^{i}[f]\\,{\\dot{=}}\\,1$ . ", "page_idx": 17}, {"type": "text", "text": "C.3 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof overview. The proof of soundness Lemma C.6 is straightforward; see Appendix C.3 for details. The proof of undetectability (Lemma C.7) of $\\mathsf{P R F-P R C}[\\mathcal{F},p,q]$ uses Assumption 3.1: if undetectability failed to hold as witnessed by some adversary Adv, then we could construct an adversary which violates pseudorandomness of $\\mathcal{F}$ (per (2)) by simulating Adv, using computational efficiency of Encode together with the PRF oracle to implement the oracle calls for Adv. ", "page_idx": 17}, {"type": "text", "text": "The bulk of the proof lies in establishing robustness to $p$ -bounded substitution channels (Lemma C.8). Let us first make sure that Decode $(1^{\\lambda},{\\mathsf{s k}},y)$ returns $\\varnothing$ with high probability if its input $y$ is simply an output of Encode $(1^{\\lambda},{\\mathsf{s k}},\\emptyset)$ (i.e., if the channel $\\mathcal{E}$ does nothing to its input). Indeed, the tuples $(x_{i},w_{i})$ (for $i\\in[m(\\bar{\\lambda})])$ computed in Line 11 of Decode are exactly equal to $(x_{i},F_{s}(x_{i})\\oplus e_{i})$ . As long as the noise rate $q$ of $e_{i}$ is bounded away from $1/2$ , the statistic $W$ computed in Decode will be larger than $m(\\lambda)/2+\\ln(m(\\lambda))\\sqrt{m(\\lambda)}$ with all but negligible probability, as desired. ", "page_idx": 17}, {"type": "text", "text": "Require: Weak PRF family $\\mathcal{F}=(\\mathcal{F}_{\\lambda})_{\\lambda}$ , where $\\mathcal{F}_{\\lambda}\\,=\\,\\{F_{s}\\,:\\,\\{0,1\\}^{n(\\lambda)}\\,\\to\\,\\{0,1\\}\\}$ indexed by $s\\in\\{0,1\\}^{\\ell(\\lambda)}$ , parameters $p,q\\in(0,1)$ . Functions $n(\\lambda),\\ell(\\lambda),m(\\lambda),N(\\lambda):\\mathbb{N}\\to\\mathbb{N}$ satisfying $N(\\lambda)\\geq(n(\\lambda)+1)m(\\lambda),$ ,   \n1: function $\\mathsf{K e y G e n}(1^{\\lambda})$   \n2: Sample $s\\sim\\mathrm{Unif}\\big(\\{0,1\\}^{\\ell(\\lambda)}\\big)$ and $z\\sim\\mathrm{Unif}(\\{0,1\\}^{N(\\lambda)}$ .   \n3: Let $\\pi:[N(\\lambda)]\\to[N(\\lambda)]$ be a uniformly random permutation.   \n4: Return $\\mathtt{s k}:=(s,z,\\pi)$ .   \n5: function Encod $\\mathfrak{z}(1^{\\lambda},\\mathsf{s k}=(s,z,\\pi),\\emptyset)$   \n6: Sample $x_{1},\\ldots,x_{m(\\lambda)}\\sim\\operatorname{Unif}(\\{0,1\\}^{n(\\lambda)})$ and $e_{1},\\ldots,e_{m(\\lambda)}\\sim\\operatorname{Ber}(q)$ .   \n7: Sample $r\\sim\\operatorname{Unif}\\!\\left(\\{0,1\\}^{N(\\lambda)-(n(\\lambda)+1)m(\\lambda)}\\right)$ .   \n8: Define $\\begin{array}{r}{a:=\\big(\\big(\\boldsymbol{x}_{1},\\boldsymbol{F}_{s}(\\boldsymbol{x}_{1})\\oplus\\boldsymbol{e}_{1}\\big),\\dots,\\big(\\boldsymbol{x}_{m(\\lambda)},\\boldsymbol{F}_{s}(\\boldsymbol{x}_{m(\\lambda)})\\oplus\\boldsymbol{e}_{m(\\lambda)}\\big),r\\big).}\\end{array}$   \n9: return the string $\\pi\\circ\\left(a\\oplus z\\right)$ .   \n10: function Decode(1 $\\boldsymbol{{\\lambda}},\\mathtt{s k}=(s,z,\\pi),\\boldsymbol{y})$   \n11: Let $x_{1},\\ldots,x_{m(\\lambda)}\\quad\\in\\quad\\{0,1\\}^{n(\\lambda)}$ and $w_{1},\\ldots,w_{m(\\lambda)}\\quad\\in\\quad\\{0,1\\}$ be defined by $\\big((x_{1},w_{1}),\\ldots,(x_{m(\\lambda)},w_{m(\\lambda)})\\big)=((\\pi^{-1}\\circ y)\\oplus z)_{1:(n(\\lambda)+1)m(\\lambda)}$ .   \n12: Let $\\begin{array}{r}{W:=\\sum_{j=1}^{m(\\lambda)}\\mathbb{1}\\{w_{j}=F_{s}(x_{j})\\}}\\end{array}$ .   \n13: If $\\begin{array}{r}{W>\\frac{m(\\lambda)}{2}+\\ln(m(\\lambda))\\sqrt{m(\\lambda)},}\\end{array}$ , return $\\varnothing$ ; otherwise, return $\\perp$ . ", "page_idx": 18}, {"type": "text", "text": "Now what if $y$ is drawn from ${\\mathcal{E}}(y^{0})$ , for $\\boldsymbol{y}^{0}\\gets\\mathsf{E n c o d e}(1^{\\lambda},\\mathsf{s k},\\emptyset)$ , for some $p$ -bounded substitution channel $\\mathcal{E}$ ? Then the tuples computed in Line 11 of Decode may be written as $(x_{i}^{\\prime},w_{i}^{\\prime})$ (for $i\\in$ $[m(\\lambda)])$ , where $(x_{i}^{\\prime},w_{i}^{\\prime})$ are \u201cperturbed\u201d versions of $(x_{i},w_{i})$ , where $x_{i}\\sim\\operatorname{Unif}(\\{0,1\\}^{n(\\lambda)}),w_{i}=$ $F_{s}(x_{i})\\oplus e_{i}$ are as computed in Encode. Although the channel $\\mathcal{E}$ may introduce substitutions in an adversarial fashion (i.e., it may not introduce substitutions at each position independently with probability $p$ ), by virtue of the fact that the output string of Encode is $\\pi\\circ\\left(a\\oplus z\\right)$ for uniformly random $z\\in\\{0,1\\}^{N(\\lambda)},\\pi:[N(\\lambda)]\\to[N(\\lambda)]$ , we can show that $(x_{i}^{\\prime},w_{i}^{\\prime})$ is close to being distributed by perturbing each bit of $(x_{i},w_{i})$ independently with some probability $p^{\\prime}\\leq p$ . The proof of this fact uses an approximation of a Hypergeometric distribution with a Binomial distribution: roughly speaking, the permutation $\\pi$ allows us to \u201cpick out\u201d, for each $i$ , a uniformly random subset of $n(\\lambda)+1$ positions corresponding to $(x_{i}^{\\prime},w_{i}^{\\prime})$ , out of $N(\\lambda)$ total positions of the string $y$ . Of these, $p^{\\prime}\\cdot N(\\lambda)\\leq p\\cdot N(\\lambda)$ are changed by $\\mathcal{E}$ , and as long as $\\dot{\\cal N}(\\lambda)\\gg n(\\lambda)$ , applying $\\mathcal{E}$ thus has nearly the same effect as changing each of the $n(\\lambda)+1$ positions of $(x_{i},w_{i})$ independently with probability $p^{\\prime}$ . ", "page_idx": 18}, {"type": "text", "text": "Given the above, we then use the fact that if $x_{i}\\sim\\operatorname{Unif}(\\{0,1\\}^{n(\\lambda)})$ and $\\ensuremath{\\boldsymbol{{x}}}_{i}^{\\prime}$ is formed by filpping each bit of $x_{i}$ with probability $p^{\\prime}\\leq p$ , then $\\begin{array}{r}{\\operatorname*{Pr}[F_{s}(x_{i})=F_{s}(x_{i}^{\\prime})]\\ge\\frac{1}{2}+(1-2p)^{\\tau}\\ge\\frac{1}{2}+(1-2p)^{\\log n(\\lambda)}}\\end{array}$ . This is a basic consequence of the Fourier analytic expression for the noise sensitivity of Boolean functions (Lemma C.4), together with the fact that $F_{s}(\\cdot)$ is $\\tau$ -local for some $\\tau\\leq\\log\\!{\\stackrel{}{(n(\\lambda))}}$ . Note that $(1-2p)^{\\log n(\\lambda)}\\,\\geq\\,n(\\lambda)^{-\\Omega_{p}(1)}$ ; thus, as long as $m(\\lambda)$ is a large enough polynomial in $n(\\lambda)$ , we can show that the statistic $\\begin{array}{r}{\\sum_{i=1}^{m(\\lambda)}\\mathbb{1}\\{w_{i}^{\\prime}=F_{s}(x_{i}^{\\prime})\\}}\\end{array}$ will be bounded away from $m(\\lambda)/2$ , which implies that Decode returns $\\varnothing$ with high probability. ", "page_idx": 18}, {"type": "text", "text": "One complication of the above argument comes from the fact that, due to the adversarial nature of the channel $\\mathcal{E}$ , the random variables $(x_{i},e_{i},x_{i}^{\\prime},w_{i}^{\\prime})$ may not be independent across different $i$ . Thus, to ensure concentration of the sum $\\begin{array}{r}{\\sum_{i=1}^{m(\\lambda)}\\mathbb{1}\\{w_{i}^{\\prime}=F_{s}(x_{i}^{\\prime})\\}}\\end{array}$ to its mean, we use Dobrushin\u2019s concentration inequality for weakly dependent data (Theorem F.3) and bound the dependence of these random variables for different $i$ using the fact that $N(\\lambda)$ is sufficiently large. ", "page_idx": 18}, {"type": "text", "text": "Formal proof. Suppose that we are given some $\\textit{p}<\\,1/2$ together with a weak PRF family $\\mathcal{F}=\\{F_{s}:\\{0,1\\}^{n(\\lambda)}\\rightarrow\\{0,1\\}\\}_{\\lambda\\in\\mathbb{N},s\\in\\{0,1\\}^{\\ell(\\lambda)}}$ with noise level $q<1/2$ which verifies Assumption 3.1, i.e., for some function $\\tau(\\lambda)\\leq\\log n(\\lambda)$ , the family is $\\tau(\\lambda)$ -local. We consider the construction $\\mathsf{P R F-P R C}[\\mathcal{F},p,q]$ in Algorithm 2. We let the functions $n(\\lambda),\\ell(\\lambda)$ be as given by the PRF family $\\mathcal{F}$ , ", "page_idx": 18}, {"type": "text", "text": "and choose ", "page_idx": 19}, {"type": "equation", "text": "$$\nm(\\lambda):=C_{0}\\cdot(1-2q)^{-4}\\cdot n(\\lambda)^{4\\log\\frac{1}{1-2p}},\\qquad N(\\lambda):=3m(\\lambda)\\cdot(n(\\lambda)+1)^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C_{0}$ is a constant chosen sufficient large as specified in the proof below. By Definition 2.1, to prove Theorem 3.2, it suffices to establish soundness, undetectability, and robustness to $p$ -weaklysubstitution-bounded channels: we do so in Lemma C.6, Lemma C.7, Lemma C.8, respectively, below. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.6 (Soundness). For any function family ${\\mathcal F}\\;=\\;\\{F_{s}\\}_{s}$ on $\\{0,1\\}^{n(\\lambda)}$ indexed by $s\\ \\in$ $\\{0,1\\}^{\\ell(\\lambda)}$ , and $p,q\\in(0,1)$ , PRF-PRC $[\\mathcal{F},p,q]$ is sound. ", "page_idx": 19}, {"type": "text", "text": "Proof. Fix $\\lambda\\ \\in\\ \\mathbb N$ , and write $n\\;=\\;n(\\lambda),\\ell\\;=\\;\\ell(\\lambda),m\\;=\\;m(\\lambda),N\\;=\\;N(\\lambda)$ . Fix any $y\\ \\in$ $\\{0,1\\}^{N}$ . Let $x_{1},\\ldots,x_{m}\\in\\{0,1\\}^{n},w_{1},\\ldots,w_{m}\\in\\{0,1\\}$ be defined as in $\\mathsf{D e c o d e}(1^{\\lambda},\\mathsf{s k},y)$ , i.e., $((x_{1},w_{1}),\\ldots,(x_{m},w_{m}))=((\\pi^{-1}\\circ y)\\oplus z)_{1:(n+1)m}.$ , where $\\mathtt{s k}=(s,z,\\pi)$ . Since $\\pi,z$ are uniformly random in their respective domains, it follows that $\\begin{array}{r}{W=\\sum_{j=1}^{m}\\mathbb{1}\\{w_{j}=F_{s}(x_{j})\\}}\\end{array}$ is distributed as $\\mathrm{Bin}(m,1/2)$ for any fixed $s$ , meaning that, for any $\\delta\\in(0,1)$ , $\\operatorname*{Pr}_{z,\\pi}(W>m/2+\\sqrt{m\\log{1/\\delta}})\\le\\delta$ . Choosing $\\delta=2^{-\\log^{2}m}\\leq\\mathsf{n e g l}(\\lambda)$ yields that $\\operatorname*{Pr}_{\\mathsf{s k-K e y G e n}(1^{\\lambda})}(W>m/2\\!+\\!\\log(m)\\sqrt{m})\\leq\\mathsf{n e g l}(\\lambda),$ , as desired. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Lemma C.7 (Undetectability). $F i x\\;p,q\\,\\in\\,[0,1/2)$ . Suppose that ${\\mathcal{F}}=\\{F_{s}\\}_{s}$ is a weak PRF on $\\{0,1\\}^{n(\\lambda)}$ with noise level $q$ (Definition 3.1), indexed by $s\\in\\{0,1\\}^{\\ell(\\lambda)}$ . Then PRF-PRC $[\\mathcal{F},p,q]$ is undetectable. ", "page_idx": 19}, {"type": "text", "text": "Proof. Fix $\\lambda\\in\\mathbb N$ , and write $n=n(\\lambda),\\ell=\\ell(\\lambda),m=m(\\lambda),N=N(\\lambda)$ . Consider any probabilistic polynomial-time adversary Adv. Let $\\mathsf{s k}=(s,z,\\pi)\\gets\\mathsf{K e y G e n}(1^{\\lambda})$ denote the secret key for the PRC. Suppose that the adversary makes a total of $Q$ queries to Encode $(1^{\\lambda},{\\mathsf{s k}},\\emptyset)$ ; its view consists of $Q$ tuples $\\bar{\\pi}\\circ(((x_{1}^{(i)},F_{s}(x_{1}^{(i)})\\oplus e_{1}^{(i)}),\\ldots,(x_{m}^{(i)},F_{s}(x_{m}^{(i)})\\oplus e_{m}^{(i)}))\\oplus z)$ , for $i\\in[Q]$ . If Adv satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{Pr}_{\\mathsf{s k}\\sim\\mathsf{K e y G e n}(1^{\\lambda})}\\left(\\mathsf{A d v}^{\\mathsf{E n c o d e}(1^{\\lambda},\\mathsf{s k},\\emptyset)}(1^{\\lambda})=1\\right)-\\operatorname*{Pr}_{U}\\left(\\mathsf{A d v}^{\\mathcal{U}}(1^{\\lambda})=1\\right)\\right|\\ge\\lambda^{-c}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some constant $c>0$ , then we could construct an adversary $\\mathsf{A d v^{\\prime}}$ which violates security of $F_{s}$ (namely, Definition 3.1), as follows: $\\mathsf{A d v^{\\prime}}$ makes $Q m$ calls to $F_{s}(\\cdot)$ on random inputs, which gives it samples $(x_{j}^{(i)},F_{s}(x_{j}^{(i)})\\oplus e_{j}^{(i)})$ for $j\\in[m],i\\in[Q]$ , draws a uniformly random permutation $\\pi:[N]\\to[N]$ and a uniformly random string $z\\in\\{0,1\\}^{N}$ , and outputs the result of Adv given the $Q$ tuples $\\boldsymbol{y}^{(i)}:=\\pi\\circ(((\\boldsymbol{x}_{1}^{(i)},\\boldsymbol{F}_{s}(\\boldsymbol{x}_{1}^{(i)})\\oplus\\boldsymbol{e}_{1}^{(i)}),\\dots,(\\boldsymbol{x}_{m}^{(i)},\\boldsymbol{F}_{s}(\\boldsymbol{x}_{m}^{(i)})\\oplus\\boldsymbol{e}_{m}^{(i)}),\\boldsymbol{r}^{(i)})\\oplus\\boldsymbol{z})$ , where $r^{(i)}$ are uniformly random strings of length $N-(n+1)m$ (for $i\\in[Q])$ . Note that if the $m Q$ calls made by $\\mathsf{A d v^{\\prime}}$ were generated by a random function, then the strings $\\boldsymbol y^{(i)}$ are uniformly random in $\\{0,1\\}^{N}$ , conditioned on the event that x(ji) are all distinct, for $j\\in[m],i\\in[Q]$ . The probability that all $\\boldsymbol{x}_{j}^{(i)}$ are distinct is at least $1-m Q/2^{n}\\geq1-{\\mathsf{n e g l}}(\\lambda)$ , meaning that, by (4), $\\mathsf{A d v^{\\prime}}$ achieves advantage $\\lambda^{-c}-{\\mathsf{n e g}}|(\\lambda)$ on distinguishing a random element of the family $\\{F_{s}\\}_{s}$ from a uniformly random function, thus contradicting the security of $\\mathcal{F}$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Given $N,t\\in\\mathbb{N}$ , let $\\mathcal{I}_{N,t}$ denote the distribution of $t$ -tuples $\\boldsymbol J=(j_{1},\\dots,j_{t})$ where $j_{1},\\ldots,j_{t}$ are drawn uniformly from $[N]$ without replacement. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.8 (Robustness). $F i x\\;p,q\\in[0,1/2)$ , and that $\\mathcal{F}=\\{F_{s}\\}_{s\\in\\{0,1\\}^{\\ell(\\lambda)},\\lambda\\in\\mathbb{N}}$ is a $\\tau(\\lambda)$ -local function family. Then PRF-PRC $[\\mathcal{F}p,q]$ is robust to any $p$ -weakly-substitution-bounded channel. ", "page_idx": 19}, {"type": "text", "text": "Proof. Fix $\\lambda\\ \\in\\ \\mathbb N$ , and write $n\\;=\\;n(\\lambda),\\ell\\;=\\;\\ell(\\lambda),m\\;=\\;m(\\lambda),N\\;=\\;N(\\lambda)$ . Let $\\mathcal{E}$ be a $p$ - weakly-substitution-bounded channel over $\\{0,1\\}$ . Given $x_{j},e_{j}\\ (j\\ \\in\\ [m])$ drawn as in Line 6 of Algorithm 2 and a uniformly random string $r\\,\\sim\\,\\mathrm{Unif}\\bigl(\\{0,1\\}^{N-(n+1)m}\\bigr)$ , consider the string $\\boldsymbol{a}:=((x_{1},F_{s}(x_{1})\\oplus e_{1}),\\dots,(x_{m},F_{s}(x_{m})\\oplus e_{m}),\\boldsymbol{r})$ , and note that the output of Encode $(1^{\\lambda},{\\mathsf{s k}},\\emptyset)$ given that $x_{1},...\\,,x_{m},e_{1},...\\,,e_{m},r$ are sampled is ${\\bar{y}}:=\\pi\\circ(a\\oplus z)$ . Given $v\\,\\in\\,\\{0,1\\}^{n}$ , we let, with slight abuse of notation, $\\mathcal{E}(v)\\in\\{0,1\\}^{n}$ denote a random variable drawn from the eponymous distribution $\\mathcal{E}(v)\\in\\Delta(\\{0,1\\}^{n})$ . We have ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u:=\\left(\\pi^{-1}\\circ\\mathcal{E}(\\bar{y})\\right)\\oplus z=\\left(\\pi^{-1}\\circ\\mathcal{E}(\\pi\\circ(a\\oplus z))\\right)\\oplus z}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\pi^{-1}\\circ((\\pi\\circ z)\\oplus\\mathcal{E}((\\pi\\circ a)\\oplus(\\pi\\circ z)))\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $z$ and $\\pi$ are chosen uniformly at random in their respective domains and independent of $a$ , it follows that the distribution of $u$ is the same as the distribution of ", "page_idx": 20}, {"type": "equation", "text": "$$\nu^{\\prime}:=\\pi^{-1}\\circ((\\pi\\circ a)\\oplus v\\oplus\\mathcal{E}(v))=a\\oplus(\\pi^{-1}\\circ(v\\oplus\\mathcal{E}(v))),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $v\\sim\\mathrm{Unif}(\\{0,1\\}^{n})$ is uniformly at random and independent of $\\pi,a$ . (In particular, we have used the reparametrization that sets $v:=(\\pi\\circ a)\\oplus(\\pi\\circ z)$ . ) ", "page_idx": 20}, {"type": "text", "text": "Let us unpack $(\\pi^{-1}\\circ(v\\oplus\\mathcal{E}(v)))_{1:(n+1)m}=((x_{1}^{\\prime},w_{1}^{\\prime}),\\ldots,(x_{m}^{\\prime},w_{m}^{\\prime}))$ , where $x_{j}^{\\prime}\\in\\{0,1\\}^{m},w_{j}^{\\prime}\\in$ $\\{0,1\\}$ for $j\\in[m]$ . Note that the distribution of the statistic $W$ computed in $\\mathsf{D e c o d e}(1^{\\lambda},\\mathsf{s k},{\\mathcal E}(\\bar{y}))$ is exactly the distribution of ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{m}\\mathbb{1}\\{w_{j}^{\\prime}\\oplus e_{j}\\oplus F_{s}(x_{j})=F_{s}(x_{j}\\oplus x_{j}^{\\prime})\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By our assumption that the family $F_{s}$ is $\\tau$ -local, for each $s\\,\\in\\,\\{0,1\\}^{\\ell}$ there is some $J_{s}\\;\\in\\;[n]^{\\tau}$ and $G_{s}:\\{0,\\bar{1}\\}^{\\tau}\\to\\{0,1\\}$ so that $F_{s}(x)=G_{s}(x_{J_{s}})$ for all $x\\,\\in\\,\\{0,1\\}^{n}$ . Define $t=\\tau+1$ and $G_{s}^{\\prime}:\\{0,1\\}^{t}\\rightarrow\\{0,1\\}$ by $G_{s}^{\\prime}(x)=G_{s}(x_{1},\\dots,x_{t-1})\\oplus x_{t}$ . ", "page_idx": 20}, {"type": "text", "text": "For each $j\\in[m]$ and fixed $s\\in\\{0,1\\}^{\\ell}$ , we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{v}_{\\Gamma}\\left(w_{j}^{\\prime}\\oplus e_{j}\\oplus F_{s}(x_{j})=F_{s}(x_{j}\\oplus x_{j}^{\\prime})\\right)=(1-2q)\\cdot\\operatorname*{Pr}\\left(F_{s}(x_{j})=w_{j}^{\\prime}\\oplus F_{s}(x_{j}\\oplus x_{j}^{\\prime})\\right)+q}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=(1-2q)\\cdot\\operatorname*{Pr}\\left(G_{s}^{\\prime}((x_{j})_{J_{s}},0)=G_{s}^{\\prime}((x_{j})_{J_{s}}\\oplus(x_{j}^{\\prime})_{J_{s}},w_{j}^{\\prime})\\right)+}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the probability is taken over $x_{j}\\,\\sim\\,\\mathrm{Unif}(\\{0,1\\}^{n}),e_{j}\\,\\sim\\,\\mathrm{Ber}(q).$ , $z\\,\\sim\\,\\mathrm{Unif}\\big(\\{0,1\\}^{n}\\big)$ , the uniformly random permutation $\\pi$ , and the randomness in $\\mathcal{E}$ (which together determine $w_{j}^{\\prime},x_{j}^{\\prime})$ . We now apply Lemma C.9 to the function ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(z_{1},\\ldots,z_{t})\\mapsto\\mathbb{1}\\{G_{s}^{\\prime}((x_{j})_{J_{s}},0)=G_{s}^{\\prime}((z_{1},\\ldots,z_{t-1})\\oplus(x_{j})_{J_{s}},z_{t})\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that, for fixed $v,\\mathcal{E}(v)$ , since $\\pi$ is chosen uniformly at random (and independent of $v,\\mathcal{E}(v),a)$ , the distribution of $((x_{j}^{\\prime})_{J_{s}},w_{j}^{\\prime})$ is exactly the distribution of $y_{J}$ for $J\\sim\\mathcal{I}_{N,t}$ , for $y=v\\oplus\\mathcal{E}(v)$ . Let us write $p(y):=\\mathrm{wt}(y)/N$ , for $y\\in\\{0,1\\}^{N}$ . Thus, for any fixed $s$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\operatorname*{Pr}\\left(G_{j}^{*}(x),z,0\\right)=G_{j}^{*}(x)_{j,1}\\leq\\tilde{\\alpha}\\left(x_{j}^{*}\\right),\\quad\\forall j\\in\\mathcal{N}_{j}^{*}\\right)}\\\\ &{=\\Xi_{j,x,x^{\\prime}\\in\\mathcal{N}_{j}}\\frac{\\left\\{1\\right\\}}{\\beta}\\sum_{\\tilde{\\alpha}\\in\\mathcal{N}_{j}}\\left[\\mathbb{I}_{\\tilde{\\alpha}}\\left(\\mathcal{L}_{j}\\big(x),z_{0}\\right)=G_{j}^{*}\\left((x)_{j,1},0)\\oplus\\tilde{\\alpha}_{j}\\leq j\\right)\\right]\\mid x_{j},\\forall\\tilde{\\alpha}(\\tilde{x})\\mid}\\\\ &{\\geq\\Xi_{j,x^{\\prime}\\in\\mathcal{N}_{j}}\\frac{\\gamma_{1}}{\\beta}\\sum_{\\tilde{\\alpha}\\in\\mathcal{N}_{j}}\\mathrm{trig}(\\tilde{x})^{*}\\left[\\mathbb{I}\\left(G_{j}^{*}\\left(x\\right),z_{0}\\right)=G_{j}^{*}\\left((x)_{j,1},0)\\oplus\\tilde{\\alpha}_{j}\\right]\\right]-\\frac{2t}{\\sqrt{N-t}}}\\\\ &{=\\Xi_{j,x^{\\prime}\\in\\mathcal{N}_{j}}\\frac{\\gamma_{1}}{\\beta}\\sum_{\\tilde{\\alpha},\\tilde{\\gamma}_{2}\\in\\mathcal{N}_{j}}\\mathrm{trig}(\\tilde{x})\\eta^{*}\\left[\\mathbb{I}\\left(G_{j}\\left(x\\right),z\\right)=G_{k}\\left((x)_{j,1},\\tilde{\\alpha}_{j}\\left(\\tilde{x}_{1},\\dots,\\tilde{z}_{k-1}\\right)\\oplus\\tilde{x}_{j}\\right)-\\frac{2t}{\\sqrt{N-t}}\\right]}\\\\ &{=\\Xi_{j,x^{\\prime}\\in\\mathcal{N}_{j}}\\left[\\mathbb{I}_{\\tilde{\\alpha}}\\left(\\Phi\\xi\\in\\mathcal{E}(x)\\right)+(1-p(\\varphi\\delta\\xi(\\tilde{x})))\\mathbb{I}_{\\infty}\\Xi_{j,x^{\\prime}\\in\\mathcal{N}_{j}}(\\exp(\\varphi))^{*}\\left[\\mathbb{I}\\left(\\mathcal{L}_{j}\\left(x\\right),z_{0}\\right)\\right]-1\\right]\\leq G_{k}\\left((x)_{j,1}\\right)\\right]}\\\\ &{\\quad-\\frac{2t}{\\sqrt{N-t}}}\\\\ &{=\\Xi_{j,x^{\\prime}\\in\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first inequality uses Lemma C.9, the fourth equality uses the fact that $x_{j}\\sim\\mathrm{Unif}(\\{0,1\\}^{n})$ independently of $v,\\mathcal{E}(v)$ and the definition of noise sensitivity, the second equality uses Corollary C.5, and the final inequality uses the fact that $\\mathrm{wt}(v\\oplus\\mathcal{E}(v))\\leq p\\dot{N}$ with probability $1-{\\mathsf{n e g l}}(N)$ since the marginal distribution of $v$ is ${\\mathrm{Unif}}(\\{0,1\\}^{n})$ and $\\mathcal{E}$ is $p$ -weakly-substitution-bounded. Since we have \u221aN2t\u2212t + negl(N) \u226414 \u00b7 (1 \u22122p)\u03c4+1 by our choice of N in (3) (which ensures that, as long as C0 is sufficiently large $N\\geq C_{0}\\cdot(1-2p)^{-4\\log(n)}$ , and in particular that $N-t\\geq16t(1-2p)^{-\\tau-1}$ as long as the security parameter $\\lambda$ is sufficiently large) , it follows from Eqs. (6) and (7) that for each $j\\in[m]$ , for sufficiently large $\\lambda$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{v}(w_{j}^{\\prime}\\oplus e_{j}\\oplus F_{s}(x_{j})=F_{s}(x_{j}\\oplus x_{j}^{\\prime}))\\geq(1-2q)\\cdot\\left(\\frac{1}{2}+\\frac{1}{4}\\cdot(1-2p)^{r+1}\\right)+q\\geq\\frac{1}{2}+\\frac{1}{4}(1-2q)(\\frac{r+1}{2}+\\frac{1}{4}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, we use Dobrushin\u2019s inequality to analyze the concentration of the sum (5); we utilize the notation of Appendix F.1 (in particular the influences defined in (43)). For each $j\\in[m]$ , define $\\Gamma_{j}=(x_{j},e_{j},w_{j}^{\\prime},x_{j}^{\\prime})$ . Let $P_{\\Gamma_{1},\\Gamma\\dots,\\Gamma_{m}}$ denote the joint law of $\\left(\\Gamma_{1},\\hdots,\\Gamma_{m}\\right)$ , and $P_{\\Gamma_{i}|\\Gamma_{-i}}$ denote the conditional law of $\\Gamma_{i}$ conditioned on $\\Gamma_{-i}$ . For any distinct $i,j\\in[m]$ , we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\nI_{j\\rightarrow i}(\\Gamma_{1:m})=\\operatorname*{max}_{\\substack{\\gamma_{-i}=j,\\,\\gamma_{j},\\gamma_{j}^{\\prime}}}d_{\\mathsf{T V}}(P_{\\Gamma_{i}|\\Gamma_{-i}}(\\cdot\\mid\\gamma_{j},\\gamma_{-i-j}),P_{\\Gamma_{i}|\\Gamma_{-i}}(\\cdot\\mid\\gamma_{j}^{\\prime},\\gamma_{-i-j}))\\leq\\frac{(n+1)^{2}}{N-m(n+1)},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "since if, for any $i\\in[m]$ , we condition on $x_{-i},e_{-i},v\\oplus\\mathcal{E}(v)$ , and $S:=\\{\\pi^{-1}((a-1)(n+1)+$ $b)\\ :\\ a\\ \\in\\ [m]\\backslash\\{i\\},\\bar{b}\\ \\in\\ [n+1]\\}$ , the distribution of $\\Gamma_{i}\\;=\\;(x_{i},e_{i},w_{i}^{\\prime},x_{i}^{\\prime})$ is given as follows: $x_{i}\\sim\\operatorname{Unif}(\\{0,1\\}^{n})$ , $e_{i}\\sim\\mathrm{Ber}(q)$ , and $(w_{i}^{\\prime},x_{i}^{\\prime})$ is distributed (independently of $x_{i},e_{i})$ as a tuple of $n+1$ distinct elements of $v\\oplus\\mathcal{E}(v)$ which are not indexed by coordinates in $\\boldsymbol{S}$ . Moreover, changing the value of $x_{j},e_{j}$ , and $\\{\\pi^{-1}((j-1)(n+1)+b)\\,:\\,b\\in[n+1]\\}$ changes only $n+1$ elements of $\\boldsymbol{S}$ , so by Lemma C.10 with $k=n+1$ and the data processing inequality for total variation distance, changes the conditional distribution of (wi\u2032, x\u2032i) by at mostN(\u2212nm+(1n)2+1). ", "page_idx": 21}, {"type": "text", "text": "Thus $\\begin{array}{r}{\\sum_{j\\in[m]\\backslash\\{i\\}}I_{j\\rightarrow i}(\\Gamma_{1:m})\\,\\leq\\,\\frac{m(n+1)^{2}}{N-m(n+1)}\\,\\leq\\,1/2}\\end{array}$ and $\\begin{array}{r}{\\sum_{i\\in[m]\\backslash\\{j\\}}I_{j\\rightarrow i}(\\Gamma_{1:m})\\,\\leq\\,\\frac{m(n+1)^{2}}{N-m(n+1)}\\,\\leq}\\end{array}$ $1/2$ since we have chosen $N=3m(n+1)^{2}$ in (3). It then follows from Theorem F.3 that for any $\\delta>0$ , with probability $1-\\delta$ over the draw of $\\Gamma_{1:m}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nV=\\sum_{j=1}^{m}\\mathbb{1}\\{w_{j}^{\\prime}\\oplus e_{j}\\oplus F_{s}(x_{j})=F_{s}(x_{j}\\oplus x_{j}^{\\prime})\\}\\geq\\sum_{j=1}^{m}\\operatorname{Pr}(w_{j}^{\\prime}\\oplus e_{j}\\oplus F_{s}(x_{j})=F_{s}(x_{j}\\oplus x_{j}^{\\prime}))-{\\sqrt{4m}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Choosing $\\delta\\,=\\,2\\exp(-\\ln^{2}(m))\\,\\leq\\,\\mathsf{n e g l}(m)\\,\\leq\\,\\mathsf{n e g l}(\\lambda)$ , and combining Eqs. (8) and (9) and the choice of $m$ in (3), we see that as long as the constant $C_{0}$ in (3) is sufficiently large, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{v}_{\\Gamma}\\left(W\\geq\\frac{m}{2}+\\ln(m)\\sqrt{m}\\right)\\geq\\operatorname*{Pr}\\left(W\\geq\\frac{m}{2}+m\\cdot(1-2q)(1-2p)^{\\tau+1}/4-\\ln(m)\\sqrt{4m}\\right)\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "(In parti\u221acular, we have used that our choice of $m$ ensures that $m\\cdot(1\\,-\\,2q)(1\\,-\\,2p)^{\\tau+1}/4\\;\\geq$ $3\\ln(m){\\sqrt{m}}.)$ Since $\\mathsf{D e c o d e}(1^{\\lambda},\\mathsf{s k},y)$ outputs $\\varnothing$ exactly when $\\begin{array}{r}{W\\,\\geq\\,\\frac{m}{2}+\\log(m)\\sqrt{m}}\\end{array}$ , we have established robustness, as desired. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Lemma C.9. Fix $N,k\\in\\mathbb{N}$ and let $y\\,\\in\\,\\{0,1\\}^{N}$ be given with $\\mathrm{wt}(y)\\,=\\,k$ . Fix $t\\,\\in\\,\\mathbb{N}$ , and let $f:\\{0,1\\}^{t}\\rightarrow\\{0,1\\}$ be a given function. Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{J\\sim\\mathcal{J}_{N,t}}[f(y_{J})]-\\mathbb{E}_{x\\sim\\mathrm{Ber}(k/N)^{t}}[f(x)]\\right|\\leq{\\frac{2t}{\\sqrt{N-t}}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. It suffices to upper bound the total variation distance between the distribution of $y_{J}$ and the distribution of $x\\sim\\bar{\\mathrm{Ber}}(k/N)^{t}$ . By symmetry, this total variation distance is the total variation distance between $\\mathrm{Bin}(t,k/N)$ and $\\mathrm{Hyp}(N,k,t)$ , where Hyp denotes the hypergeometric distribution (so that, in particular, $W\\sim\\mathrm{Hyp}(N,k,t)$ satisfies $\\begin{array}{r}{\\operatorname*{Pr}(W=w)=\\frac{\\binom{k}{w}\\binom{N-k}{t-w}}{\\binom{N}{t}}}\\end{array}$ = (wk)((NtN)\u2212\u2212wk)). By [RS23, Theorem 1] (restated in Lemma G.2), this total variation distance is bounded above by\u221aN2t\u2212t. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Lemma C.10. Let $S_{0},S_{1}$ be sets of size $N$ so that $\\vert S_{0}\\cap S_{1}\\vert=N-k,$ , for some $k<N$ . Let $n<N$ be given. For $b\\in\\{0,1\\}$ let $\\mathcal{I}_{b}$ denote the distribution of a tuple $J=(j_{1},\\ldots,j_{n})$ of n elements of $\\ensuremath{\\mathcal{S}}_{b}$ drawn uniformly without replacement. Then $\\begin{array}{r}{d\\uptau\\vee(\\mathcal{J}_{0},\\mathcal{J}_{1})\\le\\frac{n k}{N-n}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. For any tuple $J$ all of whose elements belong to $S_{0}\\cap S_{1}$ (which we write as $J\\subset S_{0}\\cap S_{1})$ , we have $\\mathcal{J}_{0}(J)=\\mathcal{J}_{1}(J)$ by symmetry. Thus we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{d_{\\mathsf{T V}}({\\mathcal I}_{0},{\\mathcal I}_{1})\\leq\\displaystyle\\operatorname*{Pr}_{J\\sim\\mathcal I_{0}}\\left(J\\not\\subset{\\mathcal S}_{0}\\cap{\\mathcal S}_{1}\\right)=1-\\frac{\\binom{N-k}{n}}{\\binom{N}{n}}=1-\\frac{\\left(N-k\\right)\\cdot\\cdot\\cdot\\left(N-k-n+1\\right)}{N\\cdot\\cdot\\cdot\\left(N-n+1\\right)}}\\\\ {\\leq1-\\left(\\frac{N-n-k}{N-n}\\right)^{n}\\leq\\frac{n k}{N-n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "D Insertion/Deletion PRCs from substitution PRCs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Suppose that $\\mathsf{P R C}_{\\mathsf{S u b}}$ is a PRC which is robust to $(1/2-p_{0})$ -substitution-bounded channels, for some $p_{0}>0$ . Choose $\\rho:=2C_{D.8}/p_{0}$ , where $C_{D.8}$ is the constant defined in Lemma D.8. Note that $\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{\\mathsf{S u b}},\\rho]$ (defined in Algorithm 1) has block length $m(\\lambda):=\\lceil n(\\lambda)\\cdot\\ln(2)\\rceil$ and alphabet size $\\begin{array}{r}{|\\Sigma(\\lambda)|=q(\\lambda)=\\rho\\cdot n(\\lambda)=\\frac{2C_{D.8}}{p_{0}}\\cdot n(\\lambda)}\\end{array}$ . Thus, to prove Theorem 4.1, it suffices to show that $\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{\\mathsf{S u b},\\,\\rho}]$ satisfies undetectability (Lemma D.3), soundness (Lemma D.1), and robustness to all $(1-C_{\\mathsf{r o b}}p_{0},p_{0})$ -edit-bounded channels (Lemma D.8), where $C_{\\mathsf{r o b}}$ is a constant defined in Lemma D.8. ", "page_idx": 22}, {"type": "text", "text": "Additional notation. Fix $q,m\\in\\mathbb{N}$ . For integers $j\\geq1$ , we define $\\mathsf{U n i q u e}_{j}(y):=\\left\\{a\\,\\in\\,[q]\\ :\\right.$ $|\\{i\\ :\\ y_{i}\\,=\\,a\\}|\\,=\\,j\\}$ , i.e., Unique $_j(y)$ is the set of elements $a\\,\\in\\,[q]$ so that exactly $j$ elements of $y$ are equal to $a$ . Given $j~\\in~\\mathbb{N}$ , we define $\\begin{array}{r}{\\mathsf{U n i q u e}_{\\ge j}(y)\\;=\\;\\bigcup_{j^{\\prime}\\ge j}\\mathsf{U n i q u e}_{j^{\\prime}}(y)}\\end{array}$ . Note that Unique $(y)=\\mathsf{U n i q u e}_{\\geq1}(y)$ . ", "page_idx": 22}, {"type": "text", "text": "Lemma D.1 (Soundness). Let $\\mathsf{P R C}_{\\mathsf{S u b}}$ be a PRC for substitutions. Then the PRC $\\mathsf{P R C}_{\\mathsf{I d x}}[\\mathsf{P R C}_{\\mathsf{S u b}}]$ in Algorithm $^{\\,l}$ is sound. ", "page_idx": 22}, {"type": "text", "text": "Proof. Fix $\\lambda\\in\\mathbb N$ , and consider $z\\in[q(\\lambda)]^{m(\\lambda)}$ . Define $y^{\\prime}\\in\\{0,1\\}^{n(\\lambda)}$ as in Line 12 of Algorithm 1. Since $\\mathsf{P R C}_{\\mathsf{S u b}}$ is sound, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\substack{\\mathrm{sk},\\psi\\rangle\\sim\\mathrm{KepGen}(1^{1})}}\\big(\\mathsf{D e c o d e}(1^{\\lambda},(\\mathsf{s k},\\psi),z)=\\!\\!\\perp\\!\\big)=\\!\\operatorname*{Pr}_{\\substack{\\mathrm{sk}\\sim\\mathrm{KepGen}(1^{\\lambda})}}\\big(\\mathsf{D e c o d e}_{\\mathsf{S u b}}(1^{\\lambda},\\mathsf{s k},y^{\\prime})=\\!\\!\\perp\\!\\big)\\geq\\!1-\\mathsf{i}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To establish undetectability, we first need the following lemma which states that PerturbDifference $(n,m,y^{0})$ outputs a uniformly random string in $\\bar{[}n]^{m}$ when its input $y^{0}$ is uniform over $\\{0,1\\}^{n}$ . ", "page_idx": 22}, {"type": "text", "text": "Lemma D.2. Given positive integers $m\\leq n$ , the distribution of $z\\gets$ PerturbDifference $(n,m,y^{0})$ (Algorithm 1), for $y^{0}\\,\\overset{\\,^{\\circ}}{\\sim}\\mathrm{Unif}(\\{0,\\bar{1}\\}^{n})$ , is exactly $\\mathrm{Unif}([n]^{m})$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. Suppose $y^{0}\\sim\\operatorname{Unif}(\\{0,1\\}^{n}),y^{1}\\sim\\operatorname{Unif}([n]^{m})$ , and write ${\\cal S}^{0}=\\{i\\in[n]\\ :\\ y_{i}^{0}=1\\},$ $S^{1}=$ Unique $(y^{1})$ as in PerturbDifference. ", "page_idx": 22}, {"type": "text", "text": "Given $y\\in[n]^{m}$ , denote its frequency mapping freq $(y)=\\mathsf{f}:[m]\\to\\mathbb{N}_{\\geq0}$ by $\\mathsf{f}(j)=|\\mathsf{U n i q u e}_{j}(y)|$ for $j\\in[m]$ . Note that freq $(y^{1})={\\mathsf{f r e q}}(y)$ with probability 1, where $y$ denotes the output string of PerturbDifference (this holds since the maps $\\sigma,\\tau$ are necessarily injective). Thus, the distribution of freq $(y)$ is exactly the distribution of freq $\\bar{(y^{1})}$ for $y^{1}\\sim\\mathrm{Unif}([n]^{m})$ . ", "page_idx": 22}, {"type": "text", "text": "Next, we claim that any two strings $z,z^{\\prime}\\,\\in\\,[n]^{m}$ with ${\\mathsf{f r e q}}(z)\\,={\\mathsf{f r e q}}(z^{\\prime})$ have equal probability of being output by PerturbDifference. To see this, we may choose a permutation $\\pi:[n]\\to[n]$ and $\\sigma:[m]\\stackrel{}{\\rightarrow}[\\dot{m}]$ so that $\\pi(z_{\\sigma(i)})\\,=\\,z_{i}^{\\prime}$ for $i\\in[m]$ . Next, it is straightforward to see from the definition of PerturbDifference that the distribution of its output remains unchanged if its input $y^{0}$ is replaced with the string $\\tilde{y}^{0}$ defined by $\\tilde{y}_{\\pi(i)}^{0}:=y_{i}^{0}$ , $i\\in[n]$ , and the sample $y^{1}$ in Line 16 is replaced with the string $\\tilde{y}^{1}$ defined by $\\tilde{y}_{i}^{1}:=\\pi(y_{\\sigma(i)}^{1})$ , $i\\in[m]$ . The distribution of the sets $S^{0},S^{1}\\,\\subset\\,[n]$ under this modified procedure is exactly the distribution of $\\pi(S^{0}),\\pi(S^{1})$ when $S^{0},S^{1}$ are drawn according to the original procedure. Thus, the probability of observing $z$ under the original execution of PerturbDifference is the same as the probability of observing $z^{\\prime}$ under this modified execution of PerturbDifference. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "It follows from the two previous paragraphs that each string in $[n]^{m}$ has equal probability of being output by PerturbDifference $(n,\\dot{m},y^{0}\\breve{)}$ , under $y^{0}\\sim\\mathrm{Unif}([\\bar{n}]^{m})$ , as desired. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Lemma D.3 (Undetectability). Let $\\mathsf{P R C}_{\\mathsf{S u b}}=(\\mathsf{K e y G e n}_{\\mathsf{S u b}}$ , EncodeSub, DecodeSub) be a PRC for substitutions. Then the PRC $\\mathsf{P R C}_{\\mathsf{I d x}}[\\mathsf{P R C}_{\\mathsf{S u b}}]$ in Algorithm 1 is undetectable. ", "page_idx": 23}, {"type": "text", "text": "Proof. Fix $\\lambda\\in\\mathbb N$ , and let us write $q:=q(\\lambda),n:=n(\\lambda)$ . Consider any $\\psi:[q]\\to[n]$ which satisfies $|\\psi^{-1}(j)|=q/n$ for each $j\\in[n]$ . ", "page_idx": 23}, {"type": "text", "text": "Given a string $y^{0}\\in\\{0,1\\}^{n}$ , let $E_{\\psi}(y^{0})\\in[q]^{n}$ denote the random variable which is the output of the following procedure: given $y^{0}$ , let $y\\leftarrow$ PerturbDifference $(n,m,y^{0})$ , then sample $z\\in[q]^{m}$ as on Line 8 of Algorithm 1 (using $\\psi$ ), and output the resulting string $z$ . ", "page_idx": 23}, {"type": "text", "text": "Claim D.4. For any fixed $\\psi$ as above, the distribution of $E_{\\psi}(y^{0})$ , for $y^{0}\\sim\\operatorname{Unif}(\\{0,1\\}^{n})$ , is uniform on $[q]^{m}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. By Lemma D.2, the distribution of $y\\leftarrow$ PerturbDifference $(n,m,y^{0})$ is uniform on $[n]^{m}$ . Since $|\\psi^{\\pm1}(j)|\\,=\\,q/n$ for each $j~\\in~[n]$ , it follows that the distribution of the output string $z$ is uniform on $[q]^{m}$ . \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Now consider any probabilistic polynomial-time adversary Adv, and suppose that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{Pr}_{(\\mathsf{s k},\\psi)\\in\\mathrm{-KeyGen}(1^{\\lambda})}\\left(\\mathsf{A d v}^{\\mathsf{E n c o d e}(1^{\\lambda},(\\mathsf{s k},\\psi),\\cdot)}(1^{\\lambda})=1\\right)-\\operatorname*{Pr}_{U}\\left(\\mathsf{A d v}^{\\mathcal{U}}(1^{\\lambda})=1\\right)\\right|=\\nu(\\lambda)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for some function $\\nu\\,:\\,\\mathbb{N}\\,\\rightarrow\\,\\mathbb{R}_{\\ge0}$ . We construct an adversary $\\mathsf{A d v^{\\prime}}$ for the substitution PRC $\\mathsf{P R C}_{\\mathsf{S u b}}$ , as follows: Adv\u2032first generates $\\psi\\,:\\,[q]\\,\\rightarrow\\,[n]$ conditioned on $|\\psi^{-1}(j)|\\,=\\,q/n$ for each $j\\in[n]$ . $\\mathsf{A d v^{\\prime}}$ then simulates $\\mathsf{A d v}$ , where each time Adv calls its oracle $\\mathcal{O}(\\mathfrak{m})$ for some message m, Adv\u2032 performs the following. It calls $y^{0}\\;\\leftarrow\\;{\\mathcal{O}}^{\\prime}({\\mathfrak{m}})$ (using its oracle $O^{\\prime}$ which is either a random oracle or $\\mathsf{E n c o d e}_{\\mathsf{S u b}}(1^{\\lambda},\\mathsf{s k},\\mathsf{m}))$ , then applies $y\\leftarrow$ PerturbDifference $(n,m,y^{0})$ , samples $z_{j}\\sim\\operatorname{Unif}(\\{a\\ :\\ \\psi(a)=y_{j}\\})$ for each $j\\in[m]$ , and then uses $z$ as the simulated output for $\\mathcal{O}(\\mathfrak{m})$ . By definition of Encode in Algorithm 1, the adversary $\\mathsf{A d v^{\\prime}}$ faithfully simulates the execution of AdvEncode(1\u03bb,sk,m) when the oracle O\u2032(\u00b7) for Adv\u2032 is EncodeSub(1\u03bb, sk, \u00b7). When the oracle O\u2032 for $\\mathsf{A d v^{\\prime}}$ is a random oracle $\\boldsymbol{\\mathcal{U}}$ (i.e., which outputs a uniformly random string $y^{0}\\sim\\operatorname{Unif}(\\{0,1\\}^{n}))$ , then Claim D.4 ensures that the adversary $\\mathsf{A d}\\bar{\\mathsf{v}}^{\\prime}$ generates a uniformly random string in $[q]^{m}$ . Thus, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{Pr}_{\\substack{\\mathrm{sk}\\leftarrow\\mathrm{KepGensub}(1^{\\lambda})}}\\left((\\mathsf{A d v}^{\\prime})^{\\mathsf{E n c o d e s u b}(1^{\\lambda},\\mathsf{s k},\\cdot)}(1^{\\lambda})=1\\right)-\\operatorname*{Pr}_{\\mathcal{U}}\\left((\\mathsf{A d v}^{\\prime})^{\\mathcal{U}}(1^{\\lambda})=1\\right)\\right|=\\nu(\\lambda),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and since $\\mathsf{P R C}_{\\mathsf{S u b}}$ is undetectable, we have $\\nu(\\lambda)\\;=\\;{\\mathsf{n e g l}}(\\lambda)$ , meaning that which implies that $\\mathsf{P R C}_{\\mathsf{I d x}}[\\mathsf{P R C}_{\\mathsf{S u b}}]$ is undetectable. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "D.1 Lemmas for robustness ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Next, we will establish several lemmas in the aim of showing robustness of $\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{\\mathsf{S u b}},\\rho]$ . The below lemma bounds the number of replacements PerturbDifference has to perform in Line 20 or Line 23 of Algorithm 1. ", "page_idx": 23}, {"type": "text", "text": "Lemma D.5. There is a sufficiently large constant $C_{D.5}\\geq1$ so that the following holds. Fix positive integers $n,m$ with $m=\\lceil n\\cdot\\ln(2)\\rceil$ and $\\epsilon\\,\\in\\,(0,1)$ so that $n\\ge C_{D.5}\\ln^{2}(1/\\epsilon)/\\epsilon^{2}$ . For $z\\in[n]^{m}$ , write $D(z)\\in\\{0,1\\}^{n}$ to be the string $D(z)_{i}:=\\mathbb{1}\\{i\\in{\\sf U n i q u e}(z)\\}$ . Then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\operatorname*{Pr}}\\\\ {\\displaystyle y^{0}\\!\\sim\\!\\mathrm{Unif}(\\{0,1\\}^{n})}\\\\ {\\displaystyle\\leftarrow\\!\\mathrm{PerturbDifference}(n,m,y^{0})}\\end{array}\\left(D_{\\mathsf{H a m}}(y^{0},D(z))\\geq\\epsilon n\\right)\\leq1-\\mathsf{n e g l}(n).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Given $y^{0}$ , let ${\\cal S}^{0}=\\{i\\in[n]\\ :\\ y_{i}^{0}=1\\}$ be defined as in Line 15 of PerturbDifference. By a Chernoff bound, for any $\\delta\\in(0,1)$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{y^{0}\\sim\\mathrm{Unif}(\\{0,1\\}^{n})}\\left(\\left||S^{0}|-\\frac{n}{2}\\right|\\ge\\sqrt{n\\log(2/\\delta)}\\right)\\le\\delta\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now consider $y^{1}\\sim\\mathrm{Unif}([n]^{m})$ and $S^{1}=\\mathsf{U n i q u e}(y^{1})$ as in Lines 15 and 16 of PerturbDifference. By our choice of $m,n$ and Lemma D.6 with $q=n$ , $\\overset{\\cdot}{\\boldsymbol{y}}^{1}$ is typical with probability $1-{\\mathsf{n e g l}}(m)\\geq$ $1-{\\mathsf{n e g l}}(n)$ , which implies that $\\begin{array}{r}{||S^{1}|-\\frac{n}{2}|\\leq2\\sqrt{m}\\ln(m)+1}\\end{array}$ . Combining this fact with (10) and choosing $\\delta=2\\exp(-\\ln^{2}(n))\\leq\\mathsf{n e g l}(n)$ gives that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{y^{0}\\sim\\mathrm{Unif}(\\{0,1\\}^{n}),y^{1}\\sim\\mathrm{Unif}([n]^{m})}{\\operatorname*{Pr}}\\left(\\left||S^{0}|-|S^{1}|\\right|\\geq\\epsilon n\\right)}\\\\ &{\\leq\\underset{y^{0}\\sim\\mathrm{Unif}(\\{0,1\\}^{n}),y^{1}\\sim\\mathrm{Unif}([n]^{m})}{\\operatorname*{Pr}}\\left(\\left||S^{0}|-|S^{1}|\\right|\\geq2\\sqrt{m}\\ln(m)+1+\\sqrt{n}\\ln(n)\\right)\\leq\\mathsf{n e g}|(n),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we have used that $\\epsilon n\\geq3\\sqrt{n}\\ln(n)+1\\geq2\\sqrt{m}\\ln(m)+1+\\sqrt{n}\\ln(n)$ as a result of our assumption that $\\begin{array}{r}{n\\ge\\frac{C\\ln^{2}(1/\\epsilon)}{\\epsilon^{2}}}\\end{array}$ for a sufficiently large constant $C$ . The conclusion of the lemma follows by noting that $\\left||S^{\\tilde{0}}|-|S^{1}|\\right|=D_{\\mathsf{H a m}}(y^{0},D(z))$ with probability 1. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Definition D.1 (Typical string). Fix $q,m\\in\\mathbb{N}$ . A string $z\\,\\in\\,[q]^{m}$ is defined to be typical if the following inequalities hold: ", "page_idx": 24}, {"type": "equation", "text": "$$\nq\\cdot(1-\\exp(-m/q))-2\\sqrt{m}\\ln m\\leq|\\mathsf{U n i q u e}(z)|\\leq q\\cdot(1-\\exp(-m/q))+2\\sqrt{m}\\ln m.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma D.6. Suppose that $q\\geq\\sqrt{m}/\\ln(m)$ . With probability $1-{\\mathsf{n e g l}}(m)$ , a uniformly random string $z\\sim\\mathrm{Unif}([q]^{m})$ is typical. ", "page_idx": 24}, {"type": "text", "text": "Proof. Consider a string $z\\in[q]^{m}$ , and define ", "page_idx": 24}, {"type": "equation", "text": "$$\nF_{1}(z):=|{\\sf U n i q u e}_{\\geq1}(z)|=\\sum_{a=1}^{q}\\mathbb{1}\\{|\\{i\\in[m]\\ :\\ z_{i}=a\\}|\\geq1\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that $F_{1}$ satisfies the bounded differences property with constants $c_{i}=1$ , for each $i\\in[m]$ . Note also that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname{Unif}([q]^{m})\\left[F_{1}(z)\\right]=q\\cdot\\left(1-\\sum_{k=0}^{j-1}{\\binom{m}{k}}\\cdot q^{-k}\\cdot(1-1/q)^{m-k}\\right)=q\\cdot\\left(\\sum_{k=j}^{m}{\\binom{m}{k}}\\cdot q^{-k}\\cdot(1-1/q)^{m}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We have that $\\mathbb{E}[F_{1}(z)]\\,=\\,q\\cdot(1-(1-1/q)^{m})$ , and using the bounds $\\exp(-1/q)\\geq1-1/q\\,\\geq$ $\\exp(-1/q-1/\\dot{q}^{2})$ for $q\\geq1$ , we conclude that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\prime\\cdot(1-\\exp(-m/q))\\leq\\mathbb{E}[F_{1}(z)]\\leq q\\cdot(1-\\exp(-m/q-m/q^{2}))\\leq q\\cdot(1-\\exp(-m/q))+m/q.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By Theorem F.1, for any $\\delta\\in(0,1)$ , we have that with probability $1-\\delta$ over $z\\sim\\mathrm{Unif}([q]^{m})$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{E}[F_{1}(z)]-F_{1}(z)|\\leq\\sqrt{m\\ln(2/\\delta)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Choose $\\delta\\;=\\;2\\exp(-\\ln^{2}(m))$ (so that $\\delta\\ \\leq\\ {\\mathsf{n e g l}}(m))$ , and note that our assumption that $q~\\geq$ ${\\sqrt{m}}/\\ln m$ gives that $m/q\\,\\leq\\,\\sqrt{m\\ln(2/\\delta)}$ . Combining the two displays above gives that with probability $1-{\\mathsf{n e g l}}(m)$ over $z\\sim\\mathrm{Unif}([q]^{m})$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nq\\cdot(1-\\exp(-m/q))-2\\sqrt{m}\\ln m\\leq F_{1}(z)\\leq q\\cdot(1-\\exp(-m/q))+2\\sqrt{m}\\ln m.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Given integers $n\\,<\\,q$ so that $q/n\\,\\in\\,\\mathbb{N}$ , let $P_{n,q}^{\\mathsf{p t n}}$ denote the uniform distribution over mappings $\\psi:[q]\\to[n]$ conditioned on the event that $|\\psi^{-\\bar{1}}(j)|=q/n$ for each $j\\in[n]$ . For sets $S,\\mathcal{T}\\subset\\Omega$ , define $\\Delta({\\dot{S}},{\\dot{T}}):=(S\\backslash T)\\cup(T\\backslash S)$ . ", "page_idx": 24}, {"type": "text", "text": "Lemma D.7. There is a constant $C_{D.7}\\,>\\,0$ so that the following holds. Consider $n,\\rho,q\\,\\in\\,\\mathbb{N}$ satisfying $q/\\rho=n$ , let $\\mathcal{Z}_{1},\\mathcal{Z}_{2}\\subset[q]$ be given, and write $\\mathcal{Z}:=\\mathcal{Z}_{1}\\cap\\mathcal{Z}_{2}$ . Define $Z:=|\\mathcal{Z}|,Z_{1}:=$ $|\\mathcal{Z}_{1}|,Z_{2}:=|\\mathcal{Z}_{2}|$ , and suppose that $\\bar{0}\\leq\\epsilon\\leq p\\leq1/10$ are given so that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{8}{\\epsilon}\\leq\\rho\\leq n^{1/4},\\quad\\frac{Z_{1}}{n}\\in[\\ln(2)-\\epsilon,\\ln(2)+\\epsilon],\\quad\\frac{Z_{2}}{n}\\leq2\\ln(2)+\\epsilon,\\quad Z\\geq p n,\\quad n\\geq\\frac{C_{D,7}}{\\epsilon^{3}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\psi\\sim P_{n,q}^{\\mathrm{ptn}}}\\bigg(|\\Delta(\\psi(\\mathcal{Z}_{1}),\\psi(\\mathcal{Z}_{2}))|\\geq n\\cdot\\bigg(\\frac{1}{2}-\\frac{p}{5}+23\\epsilon\\bigg)\\bigg)\\leq\\mathsf{n e g l}(n).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. The fact that $2/\\epsilon\\le\\rho$ and $\\operatorname*{max}\\{Z_{1}/n,Z_{2}/n\\}\\,\\leq\\,2$ implies that $\\operatorname*{max}\\{Z_{1}/\\rho,Z_{2}/\\rho\\}\\,\\le\\,\\epsilon n$ . Let us write $\\zeta_{1}:=\\exp(-Z_{1}/n),\\zeta_{2}:=\\exp(-Z_{2}/n)$ so that $\\zeta_{1}\\,\\in\\,[(1-\\epsilon)/2,1/2+\\epsilon]$ and $\\zeta_{2}\\in$ $[(1-\\epsilon)/4,1]$ . ", "page_idx": 25}, {"type": "text", "text": "Note that the mapping $\\psi$ : $\\begin{array}{r l r}{[q]}&{{}\\to}&{[n]}\\end{array}$ is specified by the random variables $\\psi^{-1}(1),\\psi^{-1}(2),\\ldots,\\bar{\\psi}^{-1}\\bar{(}n)\\subset[q]$ . Let us define ", "page_idx": 25}, {"type": "equation", "text": "$$\nF(\\psi):=|\\Delta(\\psi(\\mathcal{Z}_{1}),\\psi(\\mathcal{Z}_{2}))|=\\sum_{i=1}^{n}\\mathbb{1}\\left\\{i\\in\\Delta(\\psi(\\mathcal{Z}_{1}),\\psi(\\mathcal{Z}_{2}))\\right\\}=\\sum_{i=1}^{n}G(\\psi^{-1}(i);\\mathcal{Z}_{1},\\mathcal{Z}_{2}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $G(\\mathcal{T};\\mathcal{Z}_{1},\\mathcal{Z}_{2})\\in\\{0,1\\}$ (for some $\\tau\\subset[q])$ is defined to be equal to 1 if and only if either (a) $\\mathcal{T}\\cap\\mathcal{Z}_{1}\\neq\\emptyset$ but $\\mathcal{T}\\cap\\mathcal{Z}_{2}=\\emptyset$ or (b) $\\tau\\cap\\mathcal{Z}_{2}\\neq\\emptyset$ but $\\mathcal{T}\\cap\\mathcal{Z}_{1}=\\emptyset$ . ", "page_idx": 25}, {"type": "text", "text": "Step 1: Bounding the expectation of $F$ . Fix any $i\\in[n]$ , and note that $\\psi^{-1}(i)\\subset[q]$ is a uniformly random subset of size $\\rho$ . Note that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\psi\\sim P_{n,q}^{p_{\\mathrm{tr}}^{\\mathrm{tn}}}}{\\operatorname*{Pr}}(\\psi^{-1}(i)\\cap\\mathcal{Z}_{1}\\ne\\emptyset,\\,\\psi^{-1}(i)\\cap\\mathcal{Z}_{2}=\\emptyset)}\\\\ &{=\\underset{\\psi\\sim P_{n,q}^{p_{\\mathrm{tr}}^{\\mathrm{tn}}}}{\\operatorname*{Pr}}(\\psi^{-1}(i)\\cap\\mathcal{Z}_{1}\\ne\\emptyset\\ |\\ \\psi^{-1}(i)\\cap\\mathcal{Z}_{2}=\\emptyset)\\cdot\\underset{\\psi\\sim P_{n,q}^{p_{\\mathrm{tr}}^{\\mathrm{tn}}}}{\\operatorname*{Pr}}(\\psi^{-1}(i)\\cap\\mathcal{Z}_{2}=\\emptyset).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Lemma D.9, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\psi\\sim P_{n,q}^{\\mathrm{pt}}}(\\psi^{-1}(i)\\cap\\mathcal{Z}_{2}=\\emptyset)\\le\\exp(-\\rho Z_{2}/q)=\\exp(-Z_{2}/n)=\\zeta_{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\psi\\sim P_{n,q}^{p n}}{\\operatorname*{Pr}}(\\psi^{-1}(i)\\cap\\mathcal{Z}_{1}\\neq\\emptyset\\ |\\ \\psi^{-1}(i)\\cap\\mathcal{Z}_{2}=\\emptyset)}\\\\ &{\\leq1-\\exp\\left(-\\frac{\\rho(Z_{1}-Z)}{q-Z_{2}-\\rho}-\\frac{\\rho(Z_{1}-Z)^{2}}{(q-Z_{2}-\\rho)^{2}}\\right)}\\\\ &{\\leq1-\\exp\\left(-\\frac{Z_{1}-Z}{n-Z_{2}/\\rho-1}-\\frac{\\rho Z_{1}^{2}}{(q-Z_{2}-\\rho)^{2}}\\right)}\\\\ &{\\leq1-\\exp\\left(-(1+4\\epsilon)Z_{1}/n\\right)\\cdot\\exp(p)\\cdot\\exp(-Z_{1}^{2}(1+4\\epsilon)^{2}/(\\rho n^{2}))}\\\\ &{\\leq1-\\exp(\\ln(\\zeta_{1})-8\\epsilon)\\cdot\\exp(p)\\cdot\\exp(-8\\epsilon/\\rho)}\\\\ &{\\leq1-\\zeta_{1}\\cdot(1+p)\\cdot\\exp(-9\\epsilon)\\leq1-\\zeta_{1}\\cdot(1+p)\\cdot(1-9\\epsilon)\\leq(1-\\zeta_{1})-p\\zeta_{1}+10\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we have used the fact that, conditioned on $\\psi^{-1}(i)\\cap\\mathcal{Z}_{2}\\,=\\,\\emptyset.$ , $\\psi^{-1}(i)$ is distributed as a uniformly random subset of $[q]\\backslash\\mathcal{Z}_{2}$ , which has size $q-Z_{2}$ . Moreover, the third inequality above uses the upper bound $Z_{2}/\\rho\\doteq\\dot{\\epsilon}n$ and the lower bound $Z\\geq p n$ in (11), the fourth inequality uses the upper bound $Z_{1}/n\\le2$ from (11), and the remaining inequalities simplify and use the fact that $\\epsilon,p\\in\\bar{(0,1/10)}$ and $8/\\rho\\le\\epsilon$ . ", "page_idx": 25}, {"type": "text", "text": "Using Eqs. (12) to (14) together with a symmetrical argument to bound $\\operatorname*{Pr}(\\psi^{-1}(i)\\cap{\\mathcal{Z}}_{2}\\neq\\emptyset,\\,\\psi^{-1}(i)\\cap$ $\\mathcal{Z}_{1}=\\emptyset$ ),10 we see that, for each $i\\in[n]$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\psi\\sim P_{n,q}^{\\mathrm{pn}}}[G(\\psi^{-1}(i);\\mathcal{Z}_{1},\\mathcal{Z}_{2})]\\leq\\zeta_{2}\\cdot((1-\\zeta_{1})-p\\zeta_{1}+10\\epsilon)+\\zeta_{1}\\cdot((1-\\zeta_{2})-p\\zeta_{2}+10\\epsilon)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\zeta_{1}+\\zeta_{2}-2\\zeta_{1}\\zeta_{2}(1+p)+20\\epsilon}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(\\frac{1}{2}+\\epsilon\\right)+\\zeta_{2}-(1-\\epsilon)\\zeta_{2}\\cdot(1+p)+20\\epsilon}\\\\ &{\\qquad\\qquad\\qquad=\\left(\\frac{1}{2}+\\epsilon\\right)+\\epsilon\\zeta_{2}-\\zeta_{2}(1-\\epsilon)\\cdot p+20\\epsilon}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{1}{2}-\\frac{p}{5}+22\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second inequality uses that $\\zeta_{1},\\zeta_{2}\\leq1$ , the third inequality uses that $\\zeta_{1}\\in[(1\\!-\\!\\epsilon)/2,1/2\\!+\\!\\epsilon]$ , and the final inequality uses that $\\zeta_{2}\\in\\left[(1-\\epsilon)/4,1\\right]$ . It follows that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\psi\\sim P_{n,q}^{\\mathrm{ptn}}}[F(\\psi)]\\leq n\\cdot\\left(\\frac{1}{2}-\\frac{p}{5}+22\\epsilon\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Step 2: Concentration of $F$ to its expectation. Consider any subset $\\mathcal{H}\\subset[n]$ of size $H_{0}:=|\\mathcal{H}|$ satisfying $4\\rho H_{0}\\leq n$ , and define $\\begin{array}{r}{F_{\\mathcal{H}}(\\bar{\\boldsymbol{\\psi}}):=\\sum_{i\\in\\mathcal{H}}G(\\boldsymbol{\\psi}^{-1}(i);\\mathbf{\\bar{\\mathcal{Z}}}_{1},\\mathcal{Z}_{2})}\\end{array}$ . Note that $F_{\\mathcal{H}}$ satisfies the bounded differences property with respect to the random variables $\\psi^{-1}(i)$ , $i\\in\\mathcal{H}$ , with constants $c_{i}=1$ for each $i\\in\\mathcal{H}$ . For any distinct $i,j\\in\\mathcal{H}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\operatorname*{max}_{\\psi^{-1}(k)\\subset\\mathcal{U}\\backslash\\{i,j\\}}d_{\\mathsf{T V}}(P_{n,q}^{\\mathrm{ptn}}(\\psi^{-1}(i)=\\cdot\\mid\\psi^{-1}|_{\\mathcal{H}\\backslash\\{i,j\\}},\\psi^{-1}(j)),P_{n,q}^{\\mathrm{ptn}}(\\psi^{-1}(i)=\\cdot\\mid\\psi^{-1}|_{\\mathcal{H}\\backslash\\{i,j\\}},\\psi^{-1}(j)),}&\\\\ &{\\quad\\psi^{-1}(j),\\psi^{-1}(j)\\subset[q]}&\\\\ &{\\le\\operatorname*{max}_{\\psi^{-1}(k)\\subset\\mathcal{U}\\backslash\\{i,j\\}}P_{n,q}^{\\mathrm{ptn}}(\\psi^{-1}(i)\\cap\\tilde{\\psi}^{-1}(j)\\neq\\emptyset\\mid\\psi^{-1}|_{\\mathcal{H}\\backslash\\{i,j\\}},\\psi^{-1}(j))}&{(16)}\\\\ &{\\quad\\psi^{-1}(j),\\psi^{-1}(j)\\subset[q]}&\\\\ &{\\leq1-\\Bigg(1-\\frac{\\rho}{q-(H_{0}-1)\\rho}\\Bigg)\\cdots\\Bigg(1-\\frac{\\rho}{q-H_{0}\\rho+1}\\Bigg)}&\\\\ &{\\leq1-(1-2\\rho/q)^{\\rho}\\leq2\\rho^{2}/q\\leq1/(2H_{0}),}&{(17)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we use $\\psi^{-1}|_{\\mathcal{H}\\backslash\\{i,j\\}}$ to denote the collection of tuples $\\displaystyle(k,\\psi^{-1}(k))$ for $k\\in\\mathcal{H}\\backslash\\{i,j\\}$ . In (16), the probability is over $\\psi^{-1}(i)$ , whose conditional distribution is that of a uniformly random subset of $[\\!\\!\\!\\dot{q}]\\backslash(\\psi^{-1}(\\dot{j})\\cup(\\psi^{-1}(\\mathcal{H}\\backslash\\{\\dot{i},j\\})))$ of size $\\rho$ . The second inequality above uses Lemma D.9, and the second-to-last inequality uses the fact that $q-H_{\\mathrm{0}}\\rho+1>\\bar{q}-\\dot{H}_{\\mathrm{0}}\\rho\\geq q/2$ , since $2H_{0}\\rho\\leq q$ by assumption, and the final inequality uses that $q\\geq4\\rho^{2}H_{0}$ , by assumption. ", "page_idx": 26}, {"type": "text", "text": "The above chain of inequalities (17) guarantees that $I_{j\\to i}(\\psi^{-1}|\\varkappa)\\le1/(2H_{0})$ for all $i,j\\in\\mathcal{H}$ with $i\\neq j$ . By Theorem F.3, it follows that for any $\\delta\\in(0,1)$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\psi\\sim P_{n,q}^{\\mathrm{ptn}}}\\left(|F_{\\mathcal{H}}(\\psi)-\\mathbb{E}_{P_{n,q}^{\\mathrm{ptn}}}[F_{\\mathcal{H}}(\\psi)]|\\ge\\sqrt{4H_{0}\\ln(2/\\delta)}\\right)\\le\\delta.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Write $\\begin{array}{r}{H:=\\frac{C_{1}\\ln^{2}n}{\\epsilon^{2}}}\\end{array}$ , for 2a  sufficiently large constant $C_{1}$ to be specified below. Note that $4\\rho(H\\!+\\!1)\\leq$ $n$ ragse l ocnogn satsa $\\begin{array}{r}{n\\geq\\frac{8C_{1}\\rho\\ln^{2}n}{\\epsilon^{2}}}\\end{array}$ a, s wah fiuchn citni otnu ronf, c).e $\\rho\\leq n^{1/4}$ n $n\\ge C/\\epsilon^{3}$ a frtoirt iao ns uofff nftloyr $C$ $C_{1}$ $\\mathcal{H}_{1},\\ldots,\\mathcal{H}_{\\lfloor n/H\\rfloor}$ $[n]$ which $|\\mathcal{H}_{j}|\\in\\{H,H+1\\}$ . Since $4\\rho(H+1)\\leq n$ , we may apply (18) to each $\\mathcal{H}_{j}$ and use a union bound, which yields ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\psi\\sim P_{n,q}^{\\mathrm{ptn}}}\\left(|F(\\psi)-\\mathbb{E}_{P_{n,q}^{\\mathrm{ptn}}}[F(\\psi)]|\\geq\\frac{n\\sqrt{4\\ln(2/\\delta)}}{\\sqrt{H}}\\right)\\leq\\frac{n\\delta}{H}\\leq n\\delta.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Choosing $\\delta=2\\exp(-\\ln^{2}(n))\\leq\\mathsf{n e g l}(n)$ gives that $n\\sqrt{4\\ln(2/\\delta)}/\\sqrt{H}\\le2n\\ln(n)/\\sqrt{H}\\le\\epsilon n$ , as long as the constant $C_{1}$ is chosen sufficiently large. Combining Eqs. (15) and (19) yields that with probability $1-{\\mathsf{n e g l}}(n)$ over the draw of $\\psi\\sim P_{n,q}^{\\mathrm{ptn}}$ , $F(\\psi)\\leq\\bar{n}\\cdot\\left(\\frac{1}{2}-\\frac{p}{5}+23\\epsilon\\right)$ , which yields the claimed bound. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "D.2 Proof of robustness ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We are ready to show robustness of our PRC $\\mathsf{P R C}_{\\mathsf{I d x}}[\\mathsf{P R C}_{\\mathsf{S u b}}]$ to edit-bounded channels (Definition 2.4). ", "page_idx": 27}, {"type": "text", "text": "Lemma D.8 (Robustness). There are some constants $C_{D.8},C_{\\mathrm{rob}}~\\ge~1$ so that the following holds. Consider any $\\rho\\,>\\,1$ and security parameter $\\lambda\\ \\in\\ \\mathbb N$ satisfying $n(\\lambda)~\\geq~C_{D.8}\\rho^{4}$ , and $p_{0}\\;\\in\\;(C_{D.8}/\\rho,1/(10\\dot{C}_{\\mathrm{rob}}))$ and any $(1-C_{\\mathrm{rob}}p_{0})$ -edit-bounded channel $\\mathcal{E}$ . Then for any $P R C$ $\\mathsf{P R C}_{\\mathsf{S u b}}$ which is robust to $(1/2-p_{0})$ -substitution-bounded channels, the PRC $\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{\\mathsf{S u b}},\\rho]$ in Algorithm $^{\\,l}$ is robust to $\\mathcal{E}$ . ", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma $D.8$ . Fix $\\lambda\\ \\in\\ \\mathbb N$ , and write $n\\;:=\\;n(\\lambda),q\\;:=\\;q(\\lambda),m\\;:=\\;m(\\lambda)$ , so that $p_{0}~>$ $C_{D.8}/\\rho\\geq1/\\rho$ . Fix any $z\\in[q]^{m}$ which is typical (per Definition D.1), and consider any $z^{\\prime}\\in[q]^{m}$ which can be obtained form $z$ via a total of at most $(1-C_{\\mathsf{r o b}}\\cdot p_{0})\\cdot m$ substitutions, insertions, and deletions (which has probability 1 over $z^{\\prime}\\sim\\mathcal{E}(z)$ since $\\mathcal{E}$ is $(1-C_{\\mathsf{r o b}}p_{0})$ -edit-bounded. Write $\\mathcal{Z}:=\\mathsf{U n i q u e}(z)\\cap\\mathsf{U n i q u e}(z^{\\prime}).$ , i.e., $\\mathcal{Z}$ denotes those entries of $z$ which are preserved in $z^{\\prime}$ . Since $z$ is typical and $m/q\\leq m/(\\dot{n}\\rho)\\leq1/\\rho$ , we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathsf{U n i q u e}(z)|\\ge q\\cdot(1-\\exp(-m/q))-2\\sqrt{m}\\ln m}\\\\ &{\\qquad\\qquad\\ge q\\cdot(m/q-(m/q)^{2})-2\\sqrt{m}\\ln m\\ge m\\cdot(1-1/\\rho)-2\\sqrt{m}\\ln m,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the second inequality uses the fact that $\\exp(-x)\\leq1-x+x^{2}$ for $x\\in[0,1]$ . Since the requirements $n\\geq C_{D.8}^{\\phantom{\\Lambda}^{\\phantom{\\Lambda}}}\\rho^{4}$ and $m=\\left\\lceil\\ln(2)\\cdot n\\right\\rceil$ ensure that that $2\\sqrt{m}\\ln(m)\\leq\\,\\dot{m}/\\rho$ (as long as $C_{D.8}$ is large enough), we have that $|\\mathsf{U n i q u e}(z)|\\geq m\\cdot(1-2/\\rho)$ . Note also that it is immediate that $|\\mathsf{U n i q u e}(z)|\\leq m$ . ", "page_idx": 27}, {"type": "text", "text": "Step 1: Using Lemma D.7. Since $z^{\\prime}$ is obtained from $z$ via at most $1-C_{\\mathsf{r o b}}p_{0}$ insertions, deletions, and substitutions, we have $|\\mathcal{Z}|\\ge|\\mathsf{U n i q u e}(z)|-m\\cdot(1-C_{\\mathsf{r o b}}p_{0})\\ge m\\cdot(C_{\\mathsf{r o b}}p_{0}-2/\\rho)$ . We also have that $|\\mathsf{U n i q u e}(z^{\\prime})|\\leq2m$ . We now apply Lemma D.7 with ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{1}=\\mathsf{U n i q u e}(z),\\quad\\mathcal{Z}_{2}=\\mathsf{U n i q u e}(z^{\\prime}),\\quad\\epsilon=p_{0},\\quad p=\\frac{C_{\\mathsf{r o b}}-2}{2}\\cdot p_{0}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We must verify that the preconditions (11) are satisfied: first, we check that, as long as $C_{D.8}\\geq8$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{8}{\\epsilon}=\\frac{8}{p_{0}}<\\frac{8\\rho}{C_{D.8}}\\leq\\rho\\leq n^{1/4},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the final inequality follows from our choice of $n\\geq\\rho^{4}$ . Next, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n|\\mathsf{U n i q u e}(z)|\\in[m\\cdot(1-2/\\rho),m]\\subset[(\\ln(2)-\\epsilon)\\cdot n,(\\ln(2)+\\epsilon)\\cdot n],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we have used that $m\\,=\\,\\lceil\\ln(2)\\cdot n\\rceil$ and the fact that $2/\\rho\\,<\\,p_{0}\\,=\\,\\epsilon$ and $n\\epsilon\\,=\\,n p_{0}\\,\\geq\\,1$ . Similarly, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n|\\mathsf{U n i q u e}(z^{\\prime})|\\leq2m\\leq(2\\ln(2)+\\epsilon)\\cdot n.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next, we have $\\begin{array}{r}{|\\mathcal{Z}|\\ge m\\cdot(C_{\\mathsf{r o b}}p_{0}-2/\\rho)\\ge(C_{\\mathsf{r o b}}-2)p_{0}\\cdot m\\ge\\frac{C_{\\mathsf{r o b}}-2}{2}\\cdot p_{0}n=p n,}\\end{array}$ since $m\\geq n/2$ . Finally, we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\nn\\ge\\rho^{4}\\ge(C_{D.8}/p_{0})^{4}=(3C_{D.8}/\\epsilon)^{4}\\ge C_{D.7}/\\epsilon^{3},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "as long as $C_{D.8}$ is chosen sufficiently large. Thus, all constraints of Lemma D.7 are satisfied. Given $\\psi:[q]\\to[n]$ and $z\\in[q]^{\\star}$ , define $\\dot{D_{\\psi}}(z)\\bar{\\bf\\Delta}\\in\\{0,1\\}^{n}$ to be the vector defined by ", "page_idx": 27}, {"type": "equation", "text": "$$\nD_{\\psi}(z)_{i}:=\\mathbb{1}\\{i\\in{\\sf U n i q u e}(\\psi(z))\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then Lemma D.7 gives that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{\\psi\\sim P_{n,q}^{\\mathrm{pr}}}{\\mathrm{Pr}}\\left(D_{\\mathsf{H a m}}(D_{\\psi}(z),D_{\\psi}(z^{\\prime}))\\geq n\\cdot\\left(\\frac{1}{2}-\\frac{p}{5}+23\\epsilon\\right)\\right)}\\\\ &{=\\underset{\\psi\\sim P_{n,q}^{\\mathrm{pr}}}{\\mathrm{Pr}}\\left(|\\Delta(\\psi(\\mathsf{U n i q u e}(z)),\\psi(\\mathsf{U n i q u e}(z^{\\prime})))|\\geq n\\cdot\\left(\\frac{1}{2}-\\frac{p}{5}+23\\epsilon\\right)\\right)\\leq\\mathsf{n e g l}(n).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "As long as $C_{\\mathsf{r o b}}\\geq300$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{2}-\\frac{p}{5}+23\\epsilon\\leq\\frac{1}{2}-\\frac{C_{\\sf r o b}-2}{10}\\cdot p_{0}+23p_{0}\\leq\\frac{1}{2}-5p_{0}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Step 2: Averaging over $z$ . Let us consider the following \u201cidealized\u201d variant of $\\mathtt{E n c o d e}(1^{\\lambda},(\\mathsf{s k},\\psi),\\mathsf{m})$ , which we denote by Encode $'(1^{\\lambda},\\psi)$ (as the output of the below procedure does not depend on sk or $\\mathsf{m}$ ): ", "page_idx": 28}, {"type": "text", "text": "1. Sample $y\\sim\\mathrm{Unif}([n]^{m})$ . ", "page_idx": 28}, {"type": "text", "text": "3. return $z=(z_{1},\\dots,z_{m})$ . ", "page_idx": 28}, {"type": "text", "text": "For any fixed $z\\in[q]^{m}$ , note that $\\operatorname*{Pr}(\\mathsf{E n c o d e}^{\\prime}(1^{\\lambda},\\psi)=z)=q^{-m}$ , and in particular does not depend on $\\psi$ : this holds since each element $z_{j}$ of $z$ is drawn independently from the distribution which first draws $y_{j}\\sim\\mathrm{Unif}([n])$ and then draws $z_{j}\\sim\\operatorname{Unif}(\\{z\\ :\\ \\psi(z)=y_{j}\\})$ ; since $|\\psi^{-1}(a)|=q/n$ for each $a\\in[n]$ , this distribution is simply ${\\mathrm{Unif}}([q])$ . Let $Q$ denote the joint distribution of $(y,\\psi,z,z^{\\prime})$ , where $y\\sim\\mathrm{Unif}([n]^{m})$ , $\\psi\\sim P_{n,q}^{\\mathrm{ptn}}$ , $z$ is generated from $y,\\psi$ as in the above procedure Encode $\\mathbf{\\Phi}^{\\prime}(1^{\\lambda},\\psi)$ , and $z^{\\prime}\\sim\\mathcal{E}(z)$ . For any $z_{0}\\in[q]^{m}$ , it follows that the conditional distribution (under $Q$ ) of $\\psi$ given $z=z_{0}$ is $P_{n,q}^{\\mathsf{p t n}}$ . Since $\\psi$ and $z^{\\prime}$ are conditionally independent given $z$ , we have furthermore that for any $z_{0},z_{0}^{\\prime}\\in[q]^{m}$ , the conditional distribution (under $Q$ ) of $\\psi$ given $z=z_{0},z^{\\prime}=z_{0}^{\\prime}$ is $P_{n,q}^{\\mathsf{p t n}}$ . Thus, by (22), if $z_{0}$ is typical and $z_{0}^{\\prime}$ can be obtained from $z_{0}$ with at most $(1-C_{\\mathsf{r o b}}p_{0})\\cdot m$ subsitutions, insertions, and deletions, then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{Q}\\left(D_{\\mathsf{H a m}}(D_{\\psi}(z),D_{\\psi}(z^{\\prime}))\\geq n\\cdot\\left(\\frac{1}{2}-5p_{0}\\right)\\mid z=z_{0},z^{\\prime}=z_{0}^{\\prime}\\right)\\leq\\mathsf{n e g l}(n).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $z$ is typical with probability $1-{\\mathsf{n e g l}}(n)$ under $Q$ (by Lemma D.6) and and $z^{\\prime}$ can be obtained from $z$ using at most $(1-C_{\\mathsf{r o b}}p_{0})\\cdot m$ substitutions, insertions, and deletions with probability 1 under $Q$ , it follows that in fact ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{Q}\\left(D_{\\mathsf{H a m}}(D_{\\psi}(z),D_{\\psi}(z^{\\prime}))\\geq n\\cdot\\left(\\frac{1}{2}-5p_{0}\\right)\\right)\\leq\\mathsf{n e g l}(n)\\leq\\mathsf{n e g l}(m).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Step 3: using pseudorandomness. Let $\\tilde{Q}$ denote the joint distribution of $(y,\\psi,z,z^{\\prime})$ where $y$ is distributed as the random variable $y$ defined in Line 7 of Algorithm 1, $\\psi\\sim P_{n,q}^{\\mathsf{p t n}}$ , $z$ is distributed as the random variable $z$ defined in Line 8 of Algorithm 1 given the value of $\\psi$ and $y$ (i.e., $z_{j}\\sim$ $\\operatorname{Unif}(\\{a\\ :\\ \\psi(a)=y_{j}\\})$ ), and $z^{\\prime}\\sim\\mathcal{E}(z)$ . Using the fact a sample from $\\mathcal E(\\cdot)$ may be produced by a probabilistic polynomial-time algorithm together with Lemmas D.2 and D.3, we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{Q}\\left(D_{\\mathsf{H a m}}(D_{\\psi}(z),D_{\\psi}(z^{\\prime}))\\geq n\\cdot\\left(\\frac{1}{2}-5p_{0}\\right)\\right)-\\operatorname*{Pr}_{Q}\\left(D_{\\mathsf{H a m}}(D_{\\psi}(z),D_{\\psi}(z^{\\prime}))\\geq n\\cdot\\left(\\frac{1}{2}-5p_{0}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In more detail, to arrive at (24), we reason as follows: if the difference in (24) were non-negligible, then we could distinguish in polynomial time between a sample $y^{0}\\ \\gets\\mathsf{E n c o d e}_{\\mathsf{S u b}}(1^{\\bar{\\lambda}},\\mathsf{\\bar{s k}},\\mathsf{m})$ from a uniformly random string $y^{0}~\\sim~\\operatorname{Unif}(\\{0,1\\}^{n})$ , as follows: we first generate $y\\ \\gets$ PerturbDifference $(n,m,y^{0})$ (as in Line 7 of Algorithm 1, then sample $\\psi\\,\\sim\\,P_{n,q}^{\\mathrm{ptn}}$ , then sample $z\\in[q]^{m}$ by $z_{j}\\sim\\operatorname{Unif}(\\{a\\ :\\ \\psi(a)=y_{j}\\})$ for $j\\in[m]$ , then sample $z^{\\prime}\\sim\\mathcal{E}(z)$ , and finally evaluate $D_{\\mathsf{H a m}}(D_{\\psi}(z),D_{\\psi}(z^{\\prime}))$ . In the event that $y^{0}\\gets\\mathsf{E n c o d e}_{\\mathsf{S u b}}(1^{\\lambda},\\mathsf{s k},\\mathsf{m})$ , then the resulting distribution of $(y,\\psi,z,z^{\\prime})$ is $\\tilde{Q}$ , and in the event that $y^{0}\\sim\\operatorname{Unif}(\\{0,1\\}^{n})$ , Lemma D.2 gives that the induced distribution over $y$ is $\\mathrm{Unif}([n]^{m})$ and thus the resulting distribution of $(y,\\psi,z,z^{\\prime})$ is $Q$ . Thus we would get a contradiction to Lemma D.3. ", "page_idx": 28}, {"type": "text", "text": "Step 4: Wrapping up. We have from Eqs. (23) and (24) that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\tilde{Q}}\\left(D_{\\mathsf{H a m}}(D_{\\psi}(z),D_{\\psi}(z^{\\prime}))\\geq n\\cdot\\left(\\frac{1}{2}-3p_{0}\\right)\\right)\\leq\\mathsf{n e g l}(m).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By our construction of the distribution $\\tilde{Q}$ $\\tilde{\\mathrm{2}},\\,D_{\\psi}(z)=y$ with probability 1 under $\\tilde{Q}$ . Moreover, we have that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{(\\mathtt{s k},\\psi)\\leftarrow\\mathrm{{KepGen}}(1^{\\lambda})}{\\mathrm{Pr}}\\left(\\mathtt{D e c o d e}(1^{\\lambda},(\\mathtt{s k},\\psi),\\mathcal{E}(z))\\neq\\mathfrak{m}\\mid z\\leftarrow\\mathsf{E n c o d e}(1^{\\lambda},(\\mathtt{s k},\\psi),\\mathfrak{m})\\right)}\\\\ &{=\\underset{(\\mathtt{s k},\\psi)\\leftarrow{\\mathrm{KepGen}}(1^{\\lambda})}{\\mathrm{Pr}}\\left(\\mathtt{D e c o d e s}_{\\mathtt{s u b}}(1^{\\lambda},\\mathtt{s k},D_{\\psi}(\\mathcal{E}(z)))\\neq\\mathfrak{m}\\mid z\\leftarrow\\mathsf{E n c o d e}(1^{\\lambda},(\\mathtt{s k},\\psi),\\mathfrak{m})\\right)}\\\\ &{=\\underset{\\mathtt{s k}\\sim\\mathrm{KepGen}(1^{\\lambda}),\\psi\\sim P_{n,q}^{\\mathtt{p a n}}}{\\mathrm{Pr}}\\left(\\mathtt{D e c o d e s}_{\\mathtt{s u b}}(1^{\\lambda},\\mathtt{s k},\\bar{\\mathcal{E}}_{\\psi}(y^{0}))\\neq\\mathfrak{m}\\mid y^{0}\\leftarrow\\mathsf{E n c o d e s}_{\\mathtt{s u b}}(1^{\\lambda},\\mathtt{s k},\\mathfrak{m})\\right)\\leq\\mathsf{n e g l}(\\lambda),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where ${\\bar{\\mathcal E}}_{\\psi}\\;:\\;\\{0,1\\}^{n}\\,\\rightarrow\\,\\{0,1\\}^{n}$ denotes the (random) channel which, given $y^{0}\\;\\in\\;\\{0,1\\}^{n}$ , first applies the procedure in Lines 7 and 8 of Algorithm 1 to generate $z\\;\\in\\;[q]^{m}$ from $y^{0}$ , and then outputs $D_{\\psi}(z^{\\prime})$ for $z^{\\prime}\\sim\\mathcal{E}(z)$ . For future reference we let this distribution over $(y^{0},z)$ where $\\mathsf{s k}\\sim$ $\\mathsf{K e y G e n}_{\\mathsf{S u b}}(1^{\\lambda}),y^{0}\\gets\\mathsf{E n c o d e}_{\\mathsf{S u b}}(1^{\\lambda},\\mathsf{s k},\\mathsf{m})$ be denoted by $R$ . The first equality in the display above uses that the output of $\\mathsf{D e c o d e}(1^{\\lambda},(\\mathsf{s k},\\psi),z^{\\prime})$ is given by ${\\sf D e c o d e_{\\mathsf{S u b}}}(1^{\\lambda},{\\sf s k},{\\cal D}_{\\psi}(z^{\\prime}))$ . The second equality uses the definition of $\\mathtt{E n c o d e}(1^{\\lambda},(\\mathsf{s k},\\psi),\\mathsf{m})$ in Algorithm 1. Finally, the inequality follows since the PRC $\\mathsf{P R C}_{\\mathsf{S u b}}$ is robust to $(1/2\\mathrm{-}p_{0})$ -substitution bounded channels together with Lemma G.1 and the fact that with probability $1-{\\mathsf{n e g l}}(\\lambda)$ over the draw of $\\psi\\sim P_{n,q}^{\\mathrm{ptn}}$ , $\\mathsf{i k}\\sim\\mathsf{K e y G e n}_{\\mathsf{S u b}}(1^{\\lambda})$ , $y^{0}\\gets\\mathsf{E n c o d e}_{\\mathsf{S u b}}(1^{\\lambda},\\mathsf{s k},\\mathsf{m})$ , and $z^{\\prime}\\sim\\bar{\\mathcal{E}}_{\\psi}(y^{0})$ , we have $D_{\\mathsf{H a m}}(z^{\\prime},y^{0})\\leq(1/2-p_{0})\\cdot n$ . This fact follows from the following observations: ", "page_idx": 29}, {"type": "text", "text": "\u2022 By Lemma D.5 and undetectability of $\\mathsf{P R C}_{\\mathsf{S u b}}$ , for any $\\psi$ , with probability $1-{\\mathsf{n e g l}}(n)-$ ${\\mathsf{n e g l}}(\\lambda)\\geq1-{\\mathsf{n e g l}}(\\lambda)$ over $(y^{0},z)\\sim R$ , we have $D_{\\mathsf{H a m}}(y^{0},D_{\\bar{\\psi}}(z))\\leq p_{0}\\bar{n}$ . (In particular, Lemma D.5 ensures that $D_{\\mathsf{H a m}}(y^{0},D_{\\psi}(z))\\leq p_{0}n$ when $y^{0}\\sim\\operatorname{Unif}(\\{0,1\\})^{n}$ and then $z$ is generated from $y^{0}$ as in Lines 7 and 8 of Algorithm 1, and undetectability of $\\mathsf{P R C}_{\\mathsf{S u b}}$ ensures that this also holds when instead $y^{0}\\sim\\mathsf{\\check{E}n c o d e}_{\\mathsf{S u b}}(1^{\\lambda},\\mathsf{s k},\\mathsf{m})$ .) Here we have also used the fact that $\\psi(z)$ is the output string $y$ of PerturbDifference $(n(\\dot{\\lambda}),m(\\lambda),y^{0})$ (defined in Line 7), together with the fact that $n\\ge C_{D.5}\\ln^{2}(1/\\epsilon)/\\epsilon^{2}$ by (21), as long as the constant $C_{D.8}$ is chosen sufficiently large.   \n\u2022 By (25) together with the fact that the distribution of $(\\psi,z,z^{\\prime})$ where $\\psi\\,\\sim\\,P_{n,q}^{\\mathsf{p t n}}$ , $z\\sim$ $R$ , and $z^{\\prime}\\sim\\mathcal{E}(z)$ is exactly the marginal distribution of $(\\psi,z,z^{\\prime})\\,\\sim\\,\\tilde{Q}$ , we have that $\\begin{array}{r}{D_{\\mathsf{H a m}}(D_{\\psi}(z),\\dot{D_{\\psi}}(z^{\\prime}))\\le\\frac{1}{2}\\overleftarrow{-3p_{0}}}\\end{array}$ with probability $1-{\\mathsf{n e g l}}(n)\\geq1-{\\mathsf{n e g l}}(\\lambda)$ .   \n\u2022 Combining the two points above, we see that with probability $1-{\\mathsf{n e g}}|(\\lambda)$ over $\\mathsf{s k}\\sim$ $\\mathsf{K e y G e n}_{\\mathsf{S u b}}(1^{\\lambda}),y^{0}\\;\\sim\\;\\mathsf{E n c o d e}_{\\mathsf{S u b}}(1^{\\lambda},\\mathsf{s k},\\mathsf{m}),\\psi\\;\\sim\\;P_{n,q}^{\\mathsf{p t n}},z^{\\prime}\\;\\sim\\;\\bar{\\mathcal{E}}_{\\psi}(y^{0}),$ $D_{\\mathsf{H a m}}(y^{0},z^{\\prime})\\ \\leq$ $(1/2-p_{0})\\cdot n$ , as desired. ", "page_idx": 29}, {"type": "text", "text": "Summarizing, we have established (26), which yields the desired robustness guarantee. ", "page_idx": 29}, {"type": "text", "text": "Remark D.2 (Removing computational efficiency of channel). Though the proof of Lemma D.8 uses the fact that (per Definition 2.4), the channel $\\mathcal{E}$ is sampleable in polynomial time (namely, in Step 3), this assumption is not necessary, in the following sense. If we make the slightly stronger assumpgion that $\\mathsf{P R C}_{\\mathsf{S u b}}$ is in fact $(1/2-\\dot{p_{0}})$ -weakly substitution bounded (per Definition C.2), then the proof of Lemma D.8 can be modified as follows to remove the assumption that $\\mathcal{E}$ is computationally efficient: instead of showing that with probability $1-{\\mathsf{n e g l}}(\\lambda)$ over the draw of $y^{0}\\gets\\mathsf{\\dot{E n c o d e}}_{\\mathsf{S u b}}\\mathsf{\\dot{(}1^{\\lambda},s k,m\\mathsf{)}}$ and $z^{\\prime}\\sim\\bar{\\mathcal{E}}_{\\psi}(y^{0})$ , we have $D_{\\mathsf{H a m}}^{-}(z^{\\prime},y^{0})\\leq(1/2-p_{0})\\cdot n$ and using Lemma G.1, we would show that $D_{\\mathsf{H a m}}(z^{\\prime},y^{0})\\leq(1/2-p_{0})\\cdot n$ with probability $1-{\\mathsf{n e g l}}(\\lambda)$ when instead $y^{0}\\leftarrow\\operatorname{Unif}(\\{0,1\\}^{n})$ , which would imply that $\\bar{\\mathcal{E}}_{\\psi}$ is $(1/2-p_{0})$ -weakly robust. This latter argument avoids us to avoid needing to reason about the distribution of $y^{0}\\gets\\mathsf{E n c o d e}_{\\mathsf{S u b}}(1^{\\lambda},\\mathsf{s k},\\mathsf{m})$ , and so allows us to omit Step 3 in the proof of Lemma D.8. Moreover, we note that (as discussed in Appendix C.2) Theorem 3.2 in fact establishes the existence of PRCs robust to weakly substitution bounded channels (as do [CG24, Sections 5.3 & 5.4]). ", "page_idx": 29}, {"type": "text", "text": "Lemma D.9. Fix integers $N,\\rho\\in\\mathbb{N}$ and a subset $\\mathcal{Z}\\subset[N]$ with $Z:=|\\mathcal{Z}|$ , and suppose $Z\\leq N-\\rho$ . Let $\\tau\\subset[N]$ be a uniformly random subset of size $\\rho$ . Then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{r}({\\mathcal{Z}}\\cap{\\mathcal{T}}=\\emptyset)=\\left(1-{\\frac{Z}{N}}\\right)\\cdots\\left(1-{\\frac{Z}{N-\\rho+1}}\\right)\\in\\left[\\exp\\left(-{\\frac{\\rho Z}{N-\\rho}}-{\\frac{\\rho Z^{2}}{(N-\\rho)^{2}}}\\right),\\exp\\left(-{\\frac{\\rho Z}{N}}\\right)\\right]{\\mathrm{~as~}}\\leq\\alpha^{2}\\left(1-{\\frac{\\rho Z}{N-\\rho}}\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. We may choose $\\tau$ by selecting its elements without replacement from $[N]$ . After $i$ elements of $\\tau$ (not intersecting $\\mathcal{Z}$ ) have been chosen, the $i+1$ th element of $\\tau$ is distributed uniformly over a set of size $N-i$ , of which $Z$ elements belong to $\\mathcal{Z}$ . This establishes the equality. To see the following containment, we use the fact that $\\exp(-x)\\stackrel{-}{\\geq}1-x\\geq\\exp(-x-x^{2})$ for all $x\\in[0,1]$ . \u53e3 ", "page_idx": 30}, {"type": "text", "text": "E Watermarks from PRCs over larger alphabets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "E.1 Overview of the algorithm and guarantee ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section, we discuss how to use any PRC with security parameter $\\lambda$ and alphabet size $\\mathrm{poly}(\\lambda)$ to produce a watermarking scheme meeting the requirements of Definition B.2. The reduction of [CG24] for this task is limited to the case where the PRC has a binary alphabet. To extend to the setting where the alphabet size of the PRC is larger, some new ideas are needed. ", "page_idx": 30}, {"type": "text", "text": "Suppose that we are given a zero-bit pseudorandom code, PRC, with block length $n(\\lambda)$ over an alphabet $\\Sigma_{\\mathsf{P R C}}(\\lambda)$ , with $|\\Sigma_{\\mathsf{P R C}}(\\lambda)|\\geq\\bar{n}(\\lambda)$ , which is robust to a constant fraction of substitutions, insertions, and deletions (such a code is provided by Theorem 4.1). Given some constant $\\alpha$ and a family of language models $(\\mathsf{M o d e l}(\\lambda))_{\\lambda\\in\\mathbb{N}}$ over alphabet $(\\Sigma(\\lambda))_{\\lambda\\in\\mathbb{N}}$ with $|\\Sigma(\\lambda)|\\geq|\\Sigma_{\\mathsf{P R C}}(\\lambda)|$ , we construct a watermarking scheme $\\mathcal{W}[\\mathsf{P R C}\\,$ , Model] (Algorithm 3), as the following tuple of algorithms (Setup, Wat, Detect): ", "page_idx": 30}, {"type": "text", "text": "\u2022 $\\mathsf{S e t u p}(1^{\\lambda})$ produces a pair $\\mathtt{s k}_{\\mathtt{W a t}}=(\\mathtt{s k},\\phi)$ as the secret key of $\\mathcal{W}$ . Here sk is a secret key generated from $\\mathsf{P R C.K e y G e n}(\\lambda)$ , and $\\phi:\\Sigma(\\lambda)\\rightarrow\\Sigma_{\\mathsf{P R C}}(\\lambda)$ is chosen uniformly at random, which can be interpreted as a hash function.   \n\u2022 $\\mathsf{W a t}(1^{\\lambda},(\\mathsf{s k},\\phi))$ generates a sequence of tokens $\\sf t_{1},\\sf t_{2},\\ldots$ in blocks of length $n(\\lambda)$ , in the following manner: for each block of length $n(\\lambda)$ , we sample a codeword $x\\leftarrow\\mathsf{P R C}$ .Encode $(1^{\\lambda},{\\sf s k})$ (Line 13), and at each position $i$ , we consider the pushforward distribution $\\bar{p}_{i}:=\\phi\\circ\\mathsf{M o d e l}(\\mathsf{t}_{i}=$ $\\cdot\\mid\\mathfrak{t}_{1:i-1})\\in\\Delta(\\Sigma_{\\mathsf{P R C}}(\\bar{\\lambda}))$ of the next token under the hash function $\\phi$ (Line 15 of the subroutine EmbedChar). If $\\bar{p}_{i}$ puts enough mass on the corresponding token of the codeword $x$ (denoted by $x_{j}$ in Algorithm 3), then we let the output token of Wat at position $i$ be a uniformly random token   \nwhich hashes to $x_{j}$ (Lines 16 and 21). Otherwise, we sample the output token of Wat at position $i$ in a way that will ensure it does not alter the conditional distribution of the ith token (Lines 19 and 21). To obtain some intuition for the above procedure, it is straightforward to see that if the codewords $x$ are actually uniformly random, then the output of this procedure is identical to the distribution Model (Lemma E.4). Thus, if $x$ is drawn from a distribution computationally indistinguishable from random (as will be the case for the output of a PRC), it is simple to show that this procedure yields an undetectable watermark.   \n\u2022 Detect $(1^{\\lambda},(\\mathsf{s k},\\phi),\\mathsf{t})$ functions as follows, given a sequence $\\sf t=t_{1:\\ell}\\,\\in\\,\\Sigma(\\lambda)^{\\ell}$ of (potentially watermarked) text. It searches through all contiguous substrings of t, denoted $\\mathtt{t}_{i:j}$ and checks whether $(\\phi(\\mathfrak{t}_{i}),\\dots,\\phi(\\mathfrak{t}_{j}))$ decodes to $\\varnothing$ under PRC.Decode. If so, it returns True (and otherwise False). ", "page_idx": 30}, {"type": "text", "text": "In the below theorem, we suppose that PRC is a zero-bit PRC with block length $n(\\lambda)$ over alphabet $\\Sigma_{\\mathsf{P R C}}(\\lambda)$ , satisfying $|\\Sigma_{\\mathsf{P R C}}(\\bar{\\lambda})|\\geq n(\\lambda)$ . Furthermore, we suppose that $(\\mathsf{M o d e l}(\\lambda))_{\\lambda\\in\\mathbb{N}}$ is a family of language models defined over alphabet $\\Sigma(\\lambda)$ . ", "page_idx": 30}, {"type": "text", "text": "Theorem E.1 (Watermarking from PRCs). Suppose that $p,\\alpha\\in(0,1)$ are given, that PRC, Model $(\\lambda)$ are as described above and satisfy $\\begin{array}{r}{|\\Sigma(\\lambda)|\\ge(\\frac{8}{\\alpha}|\\Sigma_{\\mathsf{P R C}}(\\lambda)|)^{2/\\alpha}}\\end{array}$ , and that PRC satisfies robustness to any $\\begin{array}{r}{(1-\\frac{\\alpha}{8}+\\frac{3p}{\\alpha})}\\end{array}$ -edit-bounded channel. Then the watermarking scheme $\\mathcal{W}$ [PRC, Model] (Algorithm 3) is sound, undetectable, and $\\beta_{\\lambda}(\\ell)$ -substring robust to any $p$ -edit-bounded channel, where $\\beta_{\\lambda}(\\ell):=8n(\\lambda)+6\\alpha\\ell$ . ", "page_idx": 30}, {"type": "text", "text": "Theorem E.1 shows that for any constant $\\alpha$ , we can detect the watermark as long as the entropy rate of the model\u2019s text is at least $\\Omega(\\alpha)$ and as long as the fraction of errors (adversarial deletions/insertions) introduced to the watermarked text is at most $O(\\alpha^{2})$ . This latter statement follows since there are PRCs robust to $\\begin{array}{r}{(1-\\frac{\\alpha}{8}+\\frac{3p}{\\alpha})}\\end{array}$ -edit-bounded channels for $p=O(\\alpha^{2})$ . In turn, the alphabet $\\Sigma(\\lambda)$ of the language model needs to have size roughly $|\\Sigma_{\\mathsf{P R C}}(\\lambda)|^{2/\\alpha}$ , which is a polynomial in $|\\Sigma_{\\mathsf{P R C}}(\\lambda)$ as long as $\\alpha$ is a constant. ", "page_idx": 30}, {"type": "text", "text": "By combining Theorems 3.2, 4.1 and E.1 we can show our main theorem, which guarantees substringrobust watermarking schemes for edit-bounded channels under the existence of local weak PRFs (Assumption 3.1). To simplify notation, given constants $\\alpha,q>0$ together with a function family $\\mathcal{F}$ consisting of binary-valued functions on $\\{0,1\\}^{n(\\lambda)}$ , let us define ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{W}^{\\mathrm{comp}}[\\mathcal{F},q,\\alpha]:=\\mathcal{W}\\left[\\mathsf{P R C}_{\\mathsf{l d x}}\\left[\\mathsf{P R F}\\!\\!-\\!\\mathsf{P R C}\\left[\\mathcal{F},\\frac{1}{2}-\\frac{\\alpha}{16C_{\\mathrm{rob}}},q\\right],\\frac{16C_{\\mathrm{rob}}C_{0}}{\\alpha}\\right],\\mathsf{M o d e l}\\right],\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $C_{0},C_{\\sf r o b}\\geq1$ are the constants from Theorem 4.1. In words, $\\mathcal{W}^{\\mathsf{c o m p}}[\\mathcal{F},q,\\alpha]$ chains together the PRCs in Algorithms 1 and 2 and the watermarking scheme in Algorithm 3. ", "page_idx": 31}, {"type": "text", "text": "Theorem E.2 (Main theorem). There are absolute constants $c,C_{1},C_{2}>0$ so that the following holds. Fix any $\\alpha>0$ . Suppose there exists a function family $\\mathcal{F}$ , consisting of binary-valued functions on $\\{0,1\\}^{n(\\lambda)^{\\star}}$ , which is a $\\log n(\\lambda)$ -local weak PRF for some noise level $q\\in[0,1/2)$ ), per Assumption 3.1. Then for any language model family Model $(\\lambda)$ over alphabets $\\Sigma(\\lambda)$ , the watermarking scheme $\\mathcal{W}^{\\mathsf{c o m p}}[\\mathcal{F},q,\\alpha]$ ((27)) is sound, undetectable, and $\\beta_{\\lambda}(\\ell)$ -substring robust to any $c\\alpha^{2}$ -edit-bounded channel, as long as: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\beta_{\\lambda}(\\ell):=6\\alpha\\ell+8\\cdot n(\\lambda)^{C_{1}\\log\\frac{1}{\\alpha}},\\qquad|\\Sigma(\\lambda)|\\geq n(\\lambda)^{C_{2}\\frac{1}{\\alpha}\\log\\frac{1}{\\alpha}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Notice that the exponent of $n(\\lambda)$ in the $\\ell$ -independent term of $\\beta_{\\lambda}(\\ell)$ depends on $\\alpha$ , as does the exponent of $n(\\lambda)$ in the size of $\\Sigma(\\lambda)$ . While $\\alpha$ is a constant (and so this only leads to a polynomial blowup), it would be desirable to come up with more efficient reductions. We remark, however, that this issue is quite subtle, since our criterion of undetectability (in Definitions 2.1 and B.2) is phrased with respect to any polynomial-time algorithm, meaning that given a watermarking scheme $\\mathcal{W}=(\\mathsf{S e t u p},\\mathsf{W a t},\\mathsf{D e t e c t})$ we can construct a polynomially-more efficient watermarking scheme $\\mathcal{W}^{\\prime}=(\\mathsf{S e t u p}^{\\prime},\\mathsf{W a t}^{\\prime},\\mathsf{D e t e c t}^{\\prime})$ as follows. The algorithms Setup\u2032, Wat\u2032, Detect\u2032, given a security parameter $\\lambda$ , call the corresponding Setup, Wat, Detect algorithms with security parameter $\\lambda^{\\alpha}$ , for some $\\alpha<1$ . Since any function which is negl $(\\lambda^{\\alpha})$ is also ${\\mathsf{n e g l}}(\\lambda)$ and an algorithm running in time $\\mathrm{poly}(\\lambda)$ is also $\\mathrm{poly}(\\lambda^{\\alpha})$ , the new scheme $\\mathcal{W}^{\\prime}$ is sound, undetectable, and substring robust if $\\mathcal{W}$ is. Moreover, its various parameters (e.g., $|\\Sigma(\\lambda)|)$ will typically be smaller than those of $\\mathcal{W}$ by a polynomial. Of course, the guarantee afforded by undetectability of $\\mathcal{W}^{\\prime}$ (due to its use of $\\lambda^{\\alpha}$ as opposed to $\\lambda$ ) is weaker than that of $\\mathcal{W}$ , though only by a polynomial. ", "page_idx": 31}, {"type": "text", "text": "Indeed, the approach of [CG24] implicity suffers from the same $n(\\lambda)^{O(\\log{1/\\alpha})}$ dependence in their substring-robustness function $\\beta(\\ell)$ , though it is not explicitly stated as such since they essentially already scale down the security parameter. In particular, the sparsity parameter $t$ in [CG24, Lemmas 4 & 5] is required to be $O\\left({\\frac{\\log n}{\\log{\\frac{1}{{\\frac{1}{2}}-p}}}}\\right)$ (where $p\\,<\\,1/2$ denotes the rate of substitutions for the purpose of evaluating robustness; one can think of ${\\begin{array}{l}{{\\frac{1}{2}}}\\end{array}}-p$ as being roughly comparable to $\\alpha$ in Theorem E.2). Moreover, there is an algorithm which can distinguish the PRCs in [CG24] from uniform strings which runs in time roughly $n^{t}$ . To formally reason about polynomial-sized differences in the parameters, we would need to make fine-grained average case hardness assumptions, a direction we do not pursue in the present work. ", "page_idx": 31}, {"type": "text", "text": "E.2 Proof overview for Theorem E.1 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Fix a security parameter $\\lambda\\in\\mathbb N$ ; we will drop the argument $\\lambda$ in our notation for the proof overview. In the algorithm description above we discussed the idea behind undetectability of $\\mathcal{W}$ , and the proof of soundness is immediate. Therefore we focus on the (substring) robustness claim. The high-level idea of the proof of robustness is to show that the procedure Wat can be viewed as a substitution channel which (repeatedly) takes as input an output $x$ of PRC.Encode and changes some of the tokens to produce its output t. (Technically, we view it as a channel over $\\Sigma_{\\mathsf{P R C}}$ by applying the hash function $\\phi$ to the output t of Wat.) Our goal is to show that for a codeword $x\\in\\Sigma_{\\mathsf{P R C}}^{n}$ , Wat makes at most $(1-\\Omega(\\alpha))\\,\\bar{\\cdot}\\,n$ substitutions if the empirical entropy of the generated text derived from $x$ (i.e., the output of the channel) is at least $\\Omega(\\alpha\\cdot n\\cdot\\log|\\Sigma|)$ . In such a case, we say that the \u201cempirical entropy rate\u201d of the generated text is $\\Omega(\\alpha)$ . ", "page_idx": 31}, {"type": "text", "text": "The key technical difference between our watermarking procedure and that of [CG24] is the introduction of the hash function $\\phi$ . Let us first see what goes wrong without such a $\\phi$ , i.e., if $\\Sigma=\\Sigma_{\\mathsf{P R C}}$ and $\\phi$ is the identity map. Suppose that for each $i$ , Model $(\\mathsf{t}_{i}=\\cdot\\mid\\mathsf{t}_{1:i-1})$ is uniform over a fixed subset $\\Sigma^{\\prime}\\subset\\Sigma$ of size $|\\Sigma^{\\prime}|=|\\Sigma|^{\\alpha}$ . Then with high probability the output of $\\overline{{\\mathsf{M o d e l}}}$ will have empirical entropy rate $\\Omega(\\alpha)$ . However, a codeword $x$ produced by PRC will only have the property that a roughly $|\\Sigma|^{\\alpha-1}$ fraction of its tokens belong to $\\Sigma^{\\prime}$ . Thus, the procedure in Algorithm 3 will only be able to maintain roughly a $|\\Sigma|^{\\alpha-1}$ fraction of tokens of $x$ (i.e., the if statement on Line 16 will only succeed with probability $|\\Sigma|^{\\alpha-1})$ , and thus the error rate of the corresponding substitution channel will be roughly $1-|\\dot{\\Sigma}|^{\\dot{\\alpha}-\\dot{1}}=1-o(1)$ , since $|\\Sigma|\\geq n$ . This error rate is too large, since our pseudorandom codes (even over large alphabets) can only correct a $1-p$ fraction of errors, for any constant $p>0$ . ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "To circumvent this issue, we make the following observation: if the entropy of the distribution $p_{i}:=\\mathsf{M o d e l}(\\mathbf{t}_{i}=\\cdot\\mid\\mathbf{t}_{1:i-1})\\in\\Delta(\\Sigma)$ is at least $\\alpha\\cdot\\ln|\\Sigma|$ , then, if $\\Sigma$ is sufficiently large compared to $\\Sigma_{\\mathsf{P R C}}$ (roughly $|\\Sigma|\\geq|\\Sigma_{\\mathsf{P R C}}|^{\\Omega(1/\\alpha)})$ , then for a uniformly random hash function $\\phi:\\Sigma\\rightarrow\\Sigma_{\\mathsf{P R C}}$ , the pushforward distribution $\\bar{p}_{i}=\\phi\\circ p_{i}\\in\\Delta(\\Sigma_{\\mathsf{P R C}})$ will be \u201cclose to uniform\u201d in the folloing sense: At least an $\\Omega(\\alpha)$ fraction of the mass of $\\bar{p}_{i}$ will be on tokens $\\sigma\\in\\Sigma_{\\mathsf{P R C}}$ for which $\\bar{p}_{i}(\\sigma)\\leq1/|\\Sigma_{\\sf P R C}|$ . This claim is proven formally in Lemma E.3, and may be seen as a consequence of concentration of measure. Since each such token $\\sigma$ occurs with probability roughly $1/|\\Sigma|$ under a codeword output by the PRC (by undetectability), altogether such tokens account for a probability $\\Omega(\\alpha)$ event under which the condition on Line 16 of Algorithm 3 will evaluate to true. Thus, at least a fraction $\\Omega(\\alpha)$ of the tokens will not be substituted, as desired. This idea (together with the application of various concentration inequalities) allows us to ensure that the substitution channel induced by Wat introduces a fraction $1-\\Omega(\\alpha)$ of errors. ", "page_idx": 32}, {"type": "text", "text": "Next, the $p$ -edit bounded channel referred to in the statement of Theorem E.1 introduces an additional $p$ fraction of errors in the sequence of watermarked text. However, because an output of Wat consists of a sequence of text t consisting of multiple (say $M$ ) consecutive codewords of PRC (each of block length $n$ ), we run into the following issue: suppose that all of the entropy of $\\mathrm{t}$ is concentrated in $\\alpha\\cdot M$ of the codeword blocks. Also suppose that the $p$ -edit bounded channel concetrates all of its $p\\cdot M n$ errors in those same $\\alpha M$ blocks, meaning that the effective rate of (edit) errors in those $\\alpha M$ blocks is in fact pMn ${\\frac{p M n}{\\alpha M n}}={p/\\alpha}$ . Thus, we need the PRC to in fact be robust to $(1\\!-\\!\\Omega(\\alpha)\\!+\\!O(p/\\alpha))$ -edit-bounded channels in order to detect the watermark. It is straightforward to see that the aforementioned scenario is worst possible, meaning that such robustness is in fact sufficient. Full details of the proof may be found in Appendix E.3. ", "page_idx": 32}, {"type": "text", "text": "E.3 Formal algorithm and guarantee ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Algorithm 3 displays our watermarking procedure $\\mathcal{W}[\\mathsf{P R C}\\,$ , Model], given a pseudorandom code PRC over alphabet $\\Sigma_{\\mathsf{P R C}}(\\lambda)$ and a family of language models $(\\mathsf{M o d e l}(\\lambda))_{\\lambda\\in\\mathbb{N}}$ over alphabet $\\Sigma(\\lambda)$ . Whenever the security parameter $\\lambda$ is clear from context, we drop the argument $\\lambda$ , i.e., write $\\Sigma_{\\mathsf{P R C}},\\Sigma$ , Model. To aid in the analysis, given a language model Model over alphabet $\\Sigma$ , an alphabet $\\Sigma_{\\mathsf{P R C}}$ for the PRC, and a mapping $\\phi:\\Sigma\\rightarrow\\Sigma_{\\mathsf{P R C}}$ , we define an embedding channel $x\\mapsto\\mathcal{E}_{\\mathsf{E m b}}^{\\phi}(x;\\mathsf{t}_{1:i-1})$ for each choice of $i\\in\\mathbb N$ and $\\mathfrak{t}_{1:i-1}\\in\\Sigma^{i-1}$ , which maps $x\\in\\Sigma_{\\mathsf{P R C}}^{n}$ to some (random) string $\\mathcal{E}_{\\mathsf{E m b}}^{\\phi}(x;\\mathsf{t}_{1:i-1})\\in\\Sigma^{n}$ (as the input and output alphabets are different, our use of the term \u201cchannel\u201d is a slight abuse of terminology). Given $x\\in\\Sigma_{\\mathsf{P R C}}^{n}$ and $\\mathfrak{t}_{1:i-1}\\in\\Sigma^{i-1}$ , $\\mathcal{E}_{\\mathsf{E m b}}^{\\phi}(x;\\mathsf{t}_{1:i-1})$ performs the following for : for $p_{j}=\\mathsf{M o d e l}(\\mathsf{t}_{j+i-1}=\\cdot\\mid\\mathsf{t}_{1:j+i-2})$ , it generates $\\mathsf{t}_{j+i-1}\\leftarrow\\mathsf{E m b e d C h a r}(x_{j},p_{j},\\phi)$ . (If some token $\\mathtt{t}_{j+i-1}$ is the terminal token END, then all remaining tokens are also the terminal token END.) Note that this is exactly the procedure in Lines 9 and 10 for steps $i$ through $i+n-1$ of $\\mathsf{W a t}(1^{\\lambda},(\\mathsf{s k},\\phi),\\phi)$ of Algorithm 3. ", "page_idx": 32}, {"type": "text", "text": "Lemma E.3. Suppose $\\Sigma,\\Sigma^{\\prime}$ are finite alphabets, and $P\\,\\in\\,\\Delta(\\Sigma)$ is fixed. Let $\\phi:\\Sigma\\rightarrow\\Sigma^{\\prime}$ be $a$ uniformly random function. Then ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\phi}\\left(\\sum_{\\sigma^{\\prime}\\in\\Sigma^{\\prime}}\\operatorname*{min}\\left\\{\\frac{1}{\\left|\\Sigma^{\\prime}\\right|},\\phi\\circ P(\\sigma^{\\prime})\\right\\}\\geq\\frac{H(P)}{4\\ln|\\Sigma|}\\right)\\geq1-2^{\\left|\\Sigma^{\\prime}\\right|-\\frac{H(P)\\cdot\\exp(H(P)/2)}{4\\ln|\\Sigma|}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. Define $\\begin{array}{r}{\\eta:=\\frac{1}{\\exp(H(P)/2)}}\\end{array}$ . Set $\\mathcal{T}:=\\{\\sigma\\in\\Sigma\\,:\\,P(\\sigma)\\leq\\eta\\}$ . Note that $H(P)\\leq P({\\mathcal{T}})\\cdot\\ln|{\\boldsymbol{\\Sigma}}|+$ $P(\\Sigma\\backslash T)\\cdot H(P)/2$ , meaning that $\\begin{array}{r}{P(\\mathcal{T})\\ge\\frac{H(P)}{2\\ln|\\Sigma|}}\\end{array}$ . Writing M := |T |, we have M \u22652 lHn (|P\u03a3 |)\u00b7\u03b7. ", "page_idx": 32}, {"type": "text", "text": "Require: Pseudorandom code PRC with security parameter $\\lambda$ over alphabet $\\begin{array}{r}{\\Sigma_{\\mathsf{P R C}}=\\Sigma_{\\mathsf{P R C}}(\\lambda)}\\end{array}$ and   \nblock length $n(\\lambda)$ , Model over alphabet $\\Sigma(\\lambda)$ , maximum length of model text $L_{\\mathrm{max}}(\\lambda)$ .   \n1: function Setup $\\left\\langle1^{\\lambda}\\right\\rangle$ )   \n2: ${\\mathsf{s k}}\\gets{\\mathsf{P R C.K e y G e n}}(1^{\\lambda})$ .   \n3: Let $\\phi:\\Sigma(\\lambda)\\rightarrow\\Sigma_{\\mathsf{P R C}}(\\lambda)$ be chosen uniformly randomly.   \n4: return $(\\mathsf{s k},\\phi)$ .   \n5: function $\\mathsf{W a t}(1^{\\lambda},(\\mathsf{s k},\\phi))$   \n6: $x\\leftarrow\\mathsf{P R C.E n c o d e}(1^{\\lambda},\\mathsf{s k})$ . $\\triangleright x\\in\\Sigma_{\\mathsf{P R C}}^{n}$   \n7: $i\\gets1,j\\gets1$ .   \n8: whil $\\colon\\mathtt{E N D}\\not\\in\\{\\mathfrak{t}_{1},\\dots,\\mathfrak{t}_{i-1}\\}\\:\\mathbf{do}$   \n9: $p_{i}\\leftarrow\\mathsf{M o d e l}(\\mathfrak{t}_{i}=\\cdot\\mid\\mathfrak{t}_{1},\\ldots,\\mathfrak{t}_{i-1}).$ $\\mathsf{\\Delta}\\mathsf{\\Delta}\\mathsf{\\Sigma}\\mathsf{\\Sigma}\\sim p_{i}\\in\\Delta(\\Sigma)$   \n10: $\\mathtt{t}_{i}\\leftarrow\\mathsf{E m b e d C h a r}(x_{j},p_{i},\\phi).$   \n11: $i\\leftarrow i+1,j\\leftarrow j+1.$   \n12: if $j>n(\\lambda)$ then   \n13: $j\\gets1,x\\gets\\mathsf{P R C.E n c o d e}(1^{\\lambda},\\mathsf{s k}).$ .   \n14: function EmbedChar $(x_{j},p_{i},\\phi)$ $\\begin{array}{r}{\\triangleright x_{j}\\in\\Sigma_{\\mathsf{P R C}},\\,p_{i}\\in\\Delta(\\Sigma),\\,\\phi:\\Sigma\\to\\Sigma_{\\mathsf{P R C}}}\\\\ {\\triangleright\\bar{p}_{i}\\in\\Delta(\\Sigma_{\\mathsf{P R C}})}\\end{array}$   \n15: $\\bar{p_{i}}\\leftarrow\\phi\\circ p_{i}$ .   \n16: if $\\operatorname{Ber}(\\operatorname*{min}\\{1,|\\Sigma_{\\mathsf{P R C}}(\\lambda)|\\cdot\\bar{p}_{i}(x_{j})\\})=1$ then   \n17: Set yi \u2190xj. $\\triangleright y_{i}\\in\\Sigma_{\\mathsf{P R C}}$   \n18: else   \n19: Sample yi \u223cqi, where qi(\u03c3) \u221d p\u00afi(\u03c3) \u2212|\u03a3P1RC|  .   \n20:   \n21: Sample $\\mathfrak{t}_{i}$ from the distribution of $\\sigma\\sim p_{i}\\mid\\phi(\\sigma)=y_{i}$ .   \n22: return $\\mathfrak{t}_{i}$ .   \n23: function Detect $(1^{\\lambda},(\\mathsf{s k},\\phi),(\\mathsf{t}_{1},\\ldots,\\mathsf{t}_{\\ell}))$   \n24: for $i\\in[\\ell],j\\in[i,\\operatorname*{min}\\{i+n-1,\\ell\\}]$ do   \n25: if PRC.Decode $(\\mathsf{s k},(\\phi(\\mathsf{t}_{i}),\\ldots,\\phi(\\mathsf{t}_{j})))\\neq\\perp$ then   \n26: return True.   \n27: return False. ", "page_idx": 33}, {"type": "text", "text": "Next, define $Q\\in\\Delta(\\Sigma)$ by $\\begin{array}{r}{Q(\\sigma):=\\frac{\\mathbb{1}\\left\\{\\sigma\\in T\\right\\}\\cdot P(\\sigma)}{P(T)}}\\end{array}$ , and let $\\boldsymbol{\\mathcal{U}}$ denote the uniform distribution on $\\Sigma^{\\prime}$ For any subset $S\\subset\\Sigma^{\\prime}$ , by Hoeffding\u2019s inequality we have that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{s}_{\\phi}\\left(\\left|\\phi\\circ Q(S)-\\mathcal{U}(S)\\right|\\geq\\epsilon\\right)=\\operatorname*{Pr}_{\\phi}\\left(\\left|\\mathcal{U}(S)-\\sum_{\\sigma\\in\\Sigma}Q(\\sigma)\\cdot\\mathbb{1}\\{\\phi(\\sigma)\\in S\\}\\right|\\geq\\epsilon\\right)\\leq2\\exp\\left(-\\frac{2\\epsilon^{2}}{\\sum_{\\sigma\\in\\Sigma}Q(\\sigma)}\\right),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the randomness is over the draw of a uniformly random function $\\phi:\\Sigma\\rightarrow\\Sigma_{\\mathsf{P R C}}$ . Using that $\\begin{array}{r}{\\sum_{\\sigma\\in\\Sigma}Q(\\sigma)^{2}\\le\\operatorname*{max}_{\\sigma\\in\\Sigma}Q(\\sigma)\\le\\frac{\\eta}{P(\\mathcal{T})}\\le\\frac{\\eta\\cdot2\\ln|\\Sigma|}{H(P)}}\\end{array}$ together with a union bound over all subsets $S\\subset\\Sigma^{\\prime}$ ,11 we see that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\phi}\\big(d_{\\mathsf{T V}}(\\mathcal{U},\\phi\\circ Q)\\ge\\epsilon\\big)\\le2^{|\\Sigma^{\\prime}|}\\cdot\\exp\\left(\\frac{-2\\epsilon^{2}\\cdot H(P)}{\\eta\\cdot2\\ln|\\Sigma|}\\right)\\le2^{|\\Sigma^{\\prime}|-\\frac{-\\epsilon^{2}\\cdot H(P)\\cdot\\exp(H(P)/2)}{\\ln|\\Sigma|}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Under the event that $d_{\\mathsf{T V}}({\\mathcal{U}},\\phi\\circ Q)\\leq\\epsilon$ , we have that $\\begin{array}{r}{\\sum_{\\sigma^{\\prime}\\in\\Sigma^{\\prime}}\\operatorname*{min}\\{1/|\\Sigma^{\\prime}|,\\phi\\circ Q(\\sigma^{\\prime})\\}\\geq1-\\epsilon}\\end{array}$ , and thus, since $\\phi\\circ P\\geq P(T)\\cdot(\\phi\\circ Q)$ pointwise, $\\begin{array}{r}{\\sum_{\\sigma^{\\prime}\\in\\Sigma^{\\prime}}\\operatorname*{min}\\{1/|\\Sigma^{\\prime}|,\\phi\\circ P(\\sigma^{\\prime})\\}\\geq P(\\mathcal{T})\\cdot(1-\\epsilon)}\\end{array}$ . The conclusion of the lemma follows by setting $\\epsilon=1/2$ . \u53e3 ", "page_idx": 33}, {"type": "text", "text": "11Technically, we only need to do a union bound over half of all subsets, since $\\phi\\circ Q$ and $\\mathcal{U}$ are both probability measures; hence the multiplicative factor in front is $2^{|\\Sigma^{\\prime}|-1}$ as opposed to $2^{|\\Sigma^{\\prime}|}$ . ", "page_idx": 33}, {"type": "text", "text": "Lemma E.4. Fix any $\\phi:\\Sigma\\,\\rightarrow\\,\\Sigma_{\\mathsf{P R C}},$ , and $i,n\\,\\in\\,\\mathbb{N}$ . The distribution of $\\mathcal{E}_{\\mathsf{E m b}}^{\\phi}(x;\\mathsf{t}_{1:i-1})$ , for $x\\sim\\mathrm{Unif}(\\Sigma_{\\mathsf{P R C}}^{n}).$ , is exactly the distribution of $\\overline{{\\mathsf{M o d e l}}}(\\mathfrak{t}_{i:i+n-1}=\\cdot\\mid\\mathfrak{t}_{1:i-1})$ . ", "page_idx": 34}, {"type": "text", "text": "Proof. We use induction on $j\\in[i,i+n-1]$ . Fix any $j\\in[i,i+n-1]$ together with a sequence $\\mathtt{t}_{1:j-1}\\in\\Sigma^{j-1}$ . Let $p_{j}\\,\\in\\Delta(\\Sigma)$ denote the distribution of $\\mathfrak{t}_{j}\\sim\\mathcal{E}_{\\mathsf{E m b}}^{\\phi}(x;\\mathfrak{t}_{1:i-1})_{j-i+1}\\mid\\mathfrak{t}_{i:j-1}$ , i.e., the distribution of the $j-i+1$ th token of the output $\\mathcal{E}_{\\mathsf{E m b}}^{\\phi}(x;\\mathsf{t}_{1:i-1})$ , conditioned on $\\mathtt{t}_{i:j-1}$ . Let $p_{j}^{\\star}$ denote Model $(\\mathsf{t}_{j}=\\cdot\\mid\\mathsf{t}_{1:j-1})$ ; we wish to show that $p_{j}=p_{j}^{\\star}$ . ", "page_idx": 34}, {"type": "text", "text": "By Line 21 of Algorithm 3, it suffices to show that $\\phi\\circ p_{j}=\\phi\\circ p_{j}^{\\star}$ . To do so, write $\\bar{p}_{j}:=\\phi\\circ p_{j}^{\\star}$ (i.e., the quantity computed in Line 15 of Algorithm 3), and write $\\rho_{j}:=d_{\\mathsf{T V}}(\\bar{p}_{j},\\mathsf{U n i f}(\\Sigma_{\\mathsf{P R C}}))\\overset{\\circ}{=}$ $\\begin{array}{r l}{\\sum_{\\sigma\\in\\Sigma_{\\mathsf{P R C}}}[\\bar{p}_{j}(\\sigma)-1/|\\Sigma_{\\mathsf{P R C}}|]_{+}}&{{}}\\end{array}$ , so that $\\begin{array}{r}{\\rho_{j}=1-\\sum_{\\sigma\\in\\Sigma_{\\mathsf{P R C}}}\\operatorname*{min}\\{\\bar{p}_{j}(\\sigma),1/|\\Sigma_{\\mathsf{P R C}}|\\}}\\end{array}$ . By definition, for $\\sigma\\,\\overset{\\cdot}{\\in}\\,\\Sigma_{\\mathsf{P R C}}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\phi\\circ p_{j}(\\sigma)=\\frac{1}{|\\Sigma_{\\mathsf{P R C}}|}\\cdot\\operatorname*{min}\\{1,|\\Sigma_{\\mathsf{P R C}}|\\cdot\\bar{p}_{j}(\\sigma)\\}+\\rho_{j}\\cdot\\frac{[\\bar{p}_{j}(\\sigma)-1/|\\Sigma_{\\mathsf{P R C}}|]_{+}}{\\rho_{j}}\\,\\bar{p}_{j}(\\sigma),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "as desired. ", "page_idx": 34}, {"type": "text", "text": "Given integers $a,b\\in\\mathbb{N}$ with $a<b$ , a mapping $\\phi:\\Sigma\\rightarrow\\Sigma^{\\prime}$ , and a sequence $\\mathfrak{t}=\\mathfrak{t}_{1:b}\\in\\Sigma^{b}$ , we define the spread of the sequence with respect to $\\phi$ to be ", "page_idx": 34}, {"type": "equation", "text": "$$\nS^{\\phi,[a:b)}(\\mathfrak{t}):=\\sum_{i=a}^{b-1}\\sum_{\\sigma\\in\\Sigma^{\\prime}}\\operatorname*{min}\\left\\{\\frac{1}{|\\Sigma^{\\prime}|},\\phi\\circ P_{i}(\\sigma)\\right\\},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $P_{i}(\\sigma):=\\mathsf{M o d e l}(\\mathfrak{t}_{i}=\\sigma\\mid\\mathfrak{t}_{1:i-1})$ . ", "page_idx": 34}, {"type": "text", "text": "Additionally, we define the mean entropy for the sequence $\\mathfrak{t}\\in\\Sigma^{b}$ in the interval $[a:b)$ to be ", "page_idx": 34}, {"type": "equation", "text": "$$\nH_{\\mathsf{m}}^{[a:b)}(\\mathfrak{t}):=\\sum_{i=a}^{b-1}H(\\mathsf{M o d e l}(\\mathfrak{t}_{i}=\\cdot\\mid\\mathfrak{t}_{1:i-1}))=\\sum_{i=a}^{b-1}\\mathbb{E}[H_{\\mathsf{e}}^{i}(\\mathfrak{t})\\mid\\mathfrak{t}_{1:i-1}].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Lemma E.5. For any integers $a<b$ , we have that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\substack{\\mathrm{t}\\in-\\overline{{\\mathsf{M o d e l}}}}}\\left(\\frac32H_{\\mathsf{m}}^{[a:b)}(\\mathfrak{t})+8\\ln|\\Sigma|\\cdot\\ln^{4}(b-a)\\geq H_{\\mathsf{e}}^{[a:b)}(\\mathfrak{t})\\right)\\geq1-\\mathsf{n e g l}(b-a).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. Let us write $\\begin{array}{r}{\\bar{H}_{\\mathrm{e}}^{[a:b)}(\\mathfrak{t})=\\sum_{i=a}^{b-1}\\operatorname*{min}\\{H_{\\mathrm{e}}^{i}(\\mathfrak{t}),\\ln|\\Sigma|+\\ln^{2}(b-a)\\}.}\\end{array}$ For each $i$ , we have that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\bar{H}_{\\mathrm{e}}^{i}(\\mathfrak{t})>0)\\leq|\\Sigma|\\cdot\\exp(-\\ln|\\Sigma|-\\ln^{2}(b-a))=\\exp(-\\ln^{2}(b-a)).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By a union bound, it follows that $\\operatorname*{Pr}(H_{\\mathrm{e}}^{[a:b)}(\\mathfrak{t})>\\bar{H}_{\\mathrm{e}}^{[a:b)}(\\mathfrak{t}))\\le(b-a)\\cdot\\exp(-\\ln^{2}(b-a))$ . Next, Theorem F.2 gives that, for any $\\delta>0$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\substack{\\mathrm{t}\\in-\\overline{{\\mathrm{Model}}}}}\\left(\\bar{H}_{\\mathrm{e}}^{[a:b)}(\\mathfrak{t})-\\frac{3}{2}\\sum_{i=a}^{b-1}\\mathbb{E}\\left[\\bar{H}_{\\mathrm{e}}^{i}(\\mathfrak{t})\\mid\\mathfrak{t}_{1:i-1}\\right]>4(\\ln|\\Sigma|+\\ln^{2}(b-a))\\cdot\\ln(2/\\delta)\\right)\\le\\delta.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Finally, since $\\bar{H}_{\\mathrm{e}}^{i}(\\mathrm{t})\\leq H_{\\mathrm{e}}^{i}(\\mathrm{t})$ with probability 1 for each $i\\in[a,b)$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\bar{H}_{\\mathrm{e}}^{i}(\\mathbf{t})\\mid\\mathfrak{t}_{1:i-1}]\\le\\mathbb{E}[H_{\\mathrm{e}}^{i}(\\mathbf{t})\\mid\\mathfrak{t}_{1:i-1}]=H_{\\mathfrak{m}}^{i}(\\mathbf{t}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Combining Eqs. (29) to (31) and choosing $\\delta=2\\exp(-\\ln^{2}(b-a))$ , we see that with probability $1-2(b-a+1)\\cdot\\exp(-\\ln^{2}(b-a))\\geq1-\\mathsf{n e g l}(b-a)$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{l}{H_{\\mathrm{e}}^{[a;b)}(\\mathfrak{t})\\leq\\bar{H}_{\\mathrm{e}}^{[a;b)}(\\mathfrak{t})\\leq\\displaystyle\\frac{3}{2}\\sum_{i=a}^{b-1}\\mathbb{E}[H_{\\mathrm{e}}^{i}(\\mathfrak{t})\\mid\\mathfrak{t}_{1:i-1}]+4(\\ln|\\Sigma|+\\ln^{2}(b-a))\\cdot\\ln^{2}(2/\\delta)}\\\\ {\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{3}{2}H_{\\mathrm{m}}^{[a;b)}(\\mathfrak{t})+8\\ln|\\Sigma|\\cdot\\ln^{4}(b-a).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Lemma E.6. Let alphabets $\\Sigma,\\Sigma^{\\prime}$ be given, and let $\\phi:\\Sigma\\rightarrow\\Sigma^{\\prime}$ be a uniformly random function. For any integers $a<b$ and any $\\alpha\\in[0,1]$ satisfying $\\begin{array}{r}{|\\Sigma|\\geq\\left(\\frac{8}{\\alpha}|\\Sigma^{\\prime}|\\right)^{2/\\alpha}}\\end{array}$ , we have that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\underbrace{\\operatorname*{Pr}}_{\\substack{\\mathrm{t\\leftarrow~\\mathrm{Model}},\\phi}}\\left(H_{\\mathrm{e}}^{[a;b)}(\\mathfrak{t})\\leq3\\alpha\\cdot(b-a)\\ln{|\\Sigma|}+8\\ln{|\\Sigma|}\\cdot\\ln^{4}(b-a)\\right)\\geq1-\\mathsf{n e g l}(b-a)-(b-a)\\cdot2^{-\\frac{1}{8}\\alpha|\\Sigma|^{\\alpha}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Let us fix any sequence $\\upt\\in\\upSigma^{L}$ . For $i\\in[L]$ , define $P_{i}\\in\\Delta(\\Sigma)$ by $P_{i}(\\sigma):=\\mathsf{M o d e l}(\\mathfrak{t}_{i}=\\sigma\\mid$ $\\sf t_{1:i-1})$ . By Lemma E.3, for each $i\\in[a:b)$ , we have, over a random (uniform) draw of $\\phi$ , that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\phi}\\left(\\sum_{\\sigma^{\\prime}\\in\\Sigma^{\\prime}}\\operatorname*{min}\\left\\{\\frac{1}{|\\Sigma^{\\prime}|},\\phi\\circ P_{i}(\\sigma^{\\prime})\\right\\}\\geq\\frac{H(P_{i})}{4\\ln|\\Sigma|}\\right)\\geq1-2^{|\\Sigma^{\\prime}|-\\frac{H(P_{i})\\cdot\\exp(H(P_{i})/2)}{4\\ln|\\Sigma|}},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $c>0$ is a universal constant. Thus, for each index $i\\in[a,b-1]$ for which $H(P_{i})=H_{\\mathfrak{m}}^{i}({\\mathfrak{t}})\\geq$ $\\alpha\\ln{\\left|\\boldsymbol{\\Sigma}\\right|}$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\phi}\\left(\\sum_{\\sigma^{\\prime}\\in\\Sigma^{\\prime}}\\operatorname*{min}\\left\\{\\frac{1}{|\\Sigma^{\\prime}|},\\phi\\circ P_{i}(\\sigma^{\\prime})\\right\\}\\geq\\frac{H(P_{i})}{4\\ln|\\Sigma|}\\right)\\geq1-2^{|\\Sigma^{\\prime}|-\\frac{1}{4}\\cdot\\alpha|\\Sigma|^{\\alpha/2}}\\geq1-2^{-\\frac{1}{8}\\alpha|\\Sigma|^{\\alpha/2}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the second inequality follows by our requirement that $\\begin{array}{r}{|\\Sigma|\\geq\\left(\\frac{8}{\\alpha}|\\Sigma^{\\prime}|\\right)^{2/\\alpha}}\\end{array}$ . For any sequence t for which $H_{\\mathfrak{m}}^{[a:b)}(\\mathfrak{t})\\,\\geq\\,2\\alpha\\cdot(b-a)\\ln|\\Sigma|$ , we must have that $\\textstyle\\sum_{i\\in[a,b)}$ : $H_{\\mathfrak{m}}^{i}(\\mathfrak{t}){\\geq}\\alpha\\ln|\\Sigma|\\,^{H_{\\mathfrak{m}}^{i}(\\mathfrak{t})}\\,\\geq$ $\\alpha\\cdot(b-a)\\ln|\\Sigma|$ . Thus, for any such t, (32) together with a union bound gives that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\phi}\\left(S^{\\phi,[a:b)}(\\mathfrak{t})\\geq\\frac{\\alpha\\cdot(b-a)}{4}\\right)\\geq1-(b-a)\\cdot2^{-\\frac{1}{8}\\alpha|\\Sigma|^{\\alpha}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Next, Lemma E.5 gives that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{Pr}}{\\leftarrow\\mathrm{Modet}}\\Big(H_{\\mathrm{e}}^{[a;b)}(\\mathbf{t})\\leq3\\alpha\\cdot(b-a)\\ln|\\Sigma|+8\\ln|\\Sigma|\\cdot\\ln^{4}(b-a)\\;\\mathrm{or}\\;H_{\\mathrm{m}}^{[a;b)}(\\mathbf{t})\\geq2\\alpha\\cdot(b-a)\\ln|\\Sigma|\\Big)\\geq0\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Combining the two above displays, we see that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\underbrace{\\operatorname*{Pr}}_{\\substack{\\mathrm{t}\\leftarrow\\mathrm{Model},\\phi}}\\left(H_{\\mathrm{e}}^{[a:b)}(\\mathfrak{t})\\leq3\\alpha\\cdot(b-a)\\ln|\\Sigma|+8\\ln|\\Sigma|\\cdot\\ln^{4}(b-a)}\\\\ {\\substack{\\mathrm{or}\\,\\,S^{\\phi,[a:b)}(\\mathfrak{t})\\geq\\frac{\\alpha\\cdot(b-a)}{4}}}\\end{array}\\right)\\geq1-\\mathsf{n e g l}(b-a)-(b-a)\\cdot2^{-\\frac{1}{8}\\alpha|\\Sigma|^{\\alpha}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Lemma E.7. Fix $L\\in\\mathbb{N}$ , alphabets $\\Sigma$ , \u03a3PRC, and integers $a,b\\in[L]$ . Suppose $\\alpha\\in[0,1]$ satisfies $\\begin{array}{r}{\\alpha\\,\\geq\\,\\frac{32\\cdot\\ln^{4}(b-a)}{b-a}}\\end{array}$ and $\\begin{array}{r}{|\\Sigma|\\,\\ge\\,(\\frac{8}{\\alpha}|\\Sigma^{\\prime}|)^{2/\\alpha}}\\end{array}$ . Then for $x\\sim\\operatorname{Unif}(\\Sigma_{\\mathsf{P R C}}^{L})$ and $\\phi:\\Sigma\\rightarrow\\Sigma_{\\mathsf{P R C}}$ drawn uniformly, it holds that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underset{\\substack{0\\,;\\,k=1\\,\\cdots\\,\\sqrt{\\xi_{\\mathrm{tmb}}^{\\ell}}(x_{\\alpha},b-1;\\ell_{1};a_{-1})}}{\\phi_{\\mathcal{X}}}}&{\\bigg(\\,\\rho r\\,D_{\\mathsf{H a m}}\\big(x_{\\alpha;b-1}^{\\prime\\,(\\alpha\\cdot b)}(\\mathsf{t}_{\\alpha;b-1})\\big)\\!\\le\\!(b-a)\\!\\cdot\\!(1-\\!\\alpha/8)\\bigg)\\geq1-\\mathsf{n e g}|(b-a)-(b-a)\\cdot2^{-\\frac{1}{8}\\mathsf{e}_{\\alpha}}}&\\\\ {\\underset{\\substack{1\\,;\\,k=1\\,\\cdots\\,\\sqrt{\\xi_{\\mathrm{tmb}}^{\\ell}}(x_{\\alpha;b-1}^{\\prime};\\ell_{1};a_{-1})}}{\\phi_{\\mathcal{X}}}}&\\\\ {=}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where we recall that $\\phi(\\mathsf{t}_{a:b-1})$ denotes the string $\\big(\\phi\\big(\\mathsf{t}_{a}\\big),\\dots,\\phi\\big(\\mathsf{t}_{b-1}\\big)\\big)$ . ", "page_idx": 35}, {"type": "text", "text": "Proof. For any fixed $\\phi:\\Sigma\\rightarrow\\Sigma_{\\mathsf{P R C}}$ and $\\mathfrak{t}_{1:a-1}\\,\\in\\,\\Sigma$ , Lemma E.4 gives that the distribution of $\\mathcal{E}_{\\mathsf{E m b}}^{\\phi}(x_{a:b-1};\\mathsf{t}_{1:a-1})$ , under $x_{a:b-1}\\sim\\mathrm{Unif}(\\Sigma_{\\mathsf{P R C}}^{b-a})$ , is exactly the distribution of $\\overline{{\\mathsf{M o d e l}}}(\\mathtt{t}_{a:b-1}=\\cdot\\mid$ $\\mathtt{t}_{1:a-1})$ . Thus, by Lemma E.6 and the fact that $\\alpha\\cdot(b-a)\\ln|\\Sigma|\\geq8\\ln|\\Sigma|\\cdot\\ln^{4}(b-a)$ , we have that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta,x}{\\operatorname*{Pr}}\\quad\\qquad\\qquad\\Big(H_{\\mathrm{e}}^{[a;b)}(\\mathfrak{t})\\le4\\alpha\\cdot(b-a)\\ln{|\\Sigma|}\\cot{S^{\\phi,[a;b)}(\\mathfrak{t})}\\ge\\frac{\\alpha\\cdot(b-a)}{4}\\Big)}\\\\ &{\\underset{\\mathrm{ta};b-1\\sim\\mathcal{E}_{\\mathrm{Enb}}^{\\phi}(x_{a:b-1};\\mathfrak{t}_{1:a-1})}{\\tan{S}\\hat{E}_{\\mathrm{min}}^{\\phi}(x_{a:b-1};\\mathfrak{t}_{1:a-1})}}\\\\ &{\\ge\\qquad\\qquad\\underset{\\phi,x}{\\operatorname*{Pr}}\\quad\\quad\\quad\\quad\\Big(H_{\\mathrm{e}}^{[a;b)}(\\mathfrak{t})\\le3\\alpha\\cdot(b-a)\\ln{|\\Sigma|}+8\\ln{|\\Sigma|}\\mathrm{1}\\mathrm{.1}\\mathrm{h}^{4}(b-a)\\Big)}\\\\ &{\\qquad\\qquad\\quad\\quad\\hat{\\phi}_{\\mathrm{mom}}^{\\phi}(\\lvert\\phi\\rvert,\\rvert\\hat{\\phi})\\ge\\frac{\\alpha\\cdot(b-a)}{4}}\\\\ &{\\underset{\\mathrm{ta};b-1\\sim\\mathcal{N}\\mathrm{ode}[(-\\mu)]}{\\tan{S}\\hat{E}_{\\mathrm{min}}^{\\phi}(x_{a:b-1};\\mathfrak{t}_{1:a-1})}}\\\\ &{\\ge\\underline{{\\imath}}-\\mathfrak{n}\\mathrm{e}\\mathrm{g}\\vert(b-a)-(b-a)\\cdot2^{-\\frac{1}{8}\\alpha\\vert\\Sigma\\vert^{\\alpha}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "(To be precise, the first inequality above uses the lower bound on $\\alpha$ in the lemma statement and the second inequality uses Lemma E.6.) For each $i\\in[L]$ , let $p_{i}\\,\\in\\,\\Delta(\\Sigma)$ denote the distribution Mo $\\mathsf{d e l}(\\mathfrak{t}_{i}\\,=\\,\\cdot\\,\\mid\\,\\mathfrak{t}_{1},\\ldots,\\mathfrak{t}_{i-1})$ , which is itself a random variable (depending on $\\mathsf{t}_{1},\\dotsc,\\mathsf{t}_{i-1})$ . For $i\\in[a,b-1]$ , let $Z_{i}\\sim\\operatorname{Ber}(\\operatorname*{min}\\{1,|\\Sigma_{\\mathsf{P R C}}|\\cdot(\\phi\\circ p_{i})(x_{i-a+1})\\})$ be the random variable used in the definition of $\\mathcal{E}_{\\mathsf{E m b}}^{\\phi}(x_{a:b-1};\\mathsf{t}_{1:a-1})$ (i.e., corresponding to Line 16 of Algorithm 3). Note that the output $\\mathfrak{t}_{a:b-1}=\\mathcal{E}_{\\mathsf{E m b}}^{\\phi}(x_{a:b-1};\\mathfrak{t}_{1:a-1})$ of the embedding channel satisfies $\\phi(\\mathbf{t}_{i-a+1})\\,=\\,x_{i-a+1}$ if $Z_{i}=1$ for each $i\\in[a,b-1]$ . Therefore, ", "page_idx": 36}, {"type": "equation", "text": "$$\nD_{\\mathsf{H a m}}(x_{a:b-1},\\phi(\\mathsf{t}_{a:b-1}))\\leq\\sum_{i=a}^{b-1}(1-Z_{i}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For each $i\\in[a,b-1]$ , note that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim\\mathrm{Unif}(\\Sigma_{\\mathtt{P R C}}^{L})}[Z_{i}\\mid\\phi,\\mathfrak{t}_{1:i-1},Z_{1:i-1}]=\\sum_{\\sigma\\in\\Sigma_{\\mathtt{P R C}}}\\frac{1}{|\\Sigma_{\\mathtt{P R C}}|}\\cdot\\operatorname*{min}\\{1,|\\Sigma_{\\mathtt{P R C}}|\\cdot(\\phi\\circ p_{i})(\\sigma)\\},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and using the definition of the spread in (28) with $\\Sigma^{\\prime}=\\Sigma_{\\mathsf{P R C}}$ , we see that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{i=a}^{b-1}\\mathbb{E}_{x\\sim\\mathrm{Unif}(\\Sigma_{\\mathrm{PRC}}^{L})}[Z_{i}\\mid\\phi,\\mathbf{t}_{1:i-1},Z_{1:i-1}]=\\sum_{i=a}^{b-1}\\sum_{\\sigma\\in\\Sigma_{\\mathrm{PRC}}}\\operatorname*{min}\\left\\{\\frac{1}{|\\Sigma_{\\mathrm{PRC}}|},(\\phi\\circ p_{i})(\\sigma)\\right\\}=S^{\\phi,[a:b)}(\\mathbf{t}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By Theorem F.2, for any fixed $\\phi$ and for any $\\delta\\,\\in\\,(0,1)$ , with probability $1-\\delta$ over the draw of $\\mathsf{t}_{1:a-1}\\sim\\overline{{\\mathsf{M o d e l}}}(\\cdot\\mid\\emptyset),x\\sim\\mathsf{U n i f}(\\Sigma_{\\mathsf{P R C}}^{L}),\\mathsf{t}_{a:b-1}\\sim\\mathcal{E}_{\\mathsf{E m b}}(x_{a:b-1};\\mathsf{t}_{1:a-1}).$ , and $Z_{i}$ , it holds that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{i=a}^{b-1}Z_{i}\\geq\\frac{1}{2}S^{\\phi,[a:b)}(\\mathbf{t})-4\\log(2/\\delta).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Choose $\\delta=2\\exp(-\\ln^{2}(b-a))\\,\\leq\\,\\mathsf{n e g l}(b-a)$ . In the event that $\\begin{array}{r}{S^{\\phi,[a:b)}(\\mathfrak{t})\\geq\\frac{\\alpha\\cdot(b-a)}{4}}\\end{array}$ and (35) both hold, using the lower bound on $\\alpha$ in the lemma statement together with (34), we see that ", "page_idx": 36}, {"type": "equation", "text": "$$\nD_{\\mathsf{H a m}}(x_{a:b-1},\\phi(\\mathsf{t}_{a:b-1}))\\leq(b-a)-\\frac{\\alpha\\cdot(b-a)}{4}+4\\ln^{2}(b-a)\\leq(b-a)\\cdot(1-\\alpha/8).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Combining (33) and the fact that (35) holds with probability $1-{\\mathsf{n e g}}|(b-a)$ together with (36), we see that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\phi_{\\mathrm{rim}}}{\\overset{\\mathrm{Pr}}{\\sum}}\\quad\\quad\\quad\\quad\\quad\\bigg(\\underset{\\phi_{\\mathrm{t}}}{\\overset{H[\\phi>\\phi)}{\\sum}}(\\mathsf{t})\\leq4\\alpha\\cdot(b-a)\\ln|\\Sigma|}\\\\ &{\\quad\\quad\\quad\\frac{\\phi_{\\mathrm{r}}x}{\\mathsf{t}_{1:a-1}\\sqrt{\\mathsf{M o d e l}}(\\cdot|\\theta)}}\\\\ &{\\quad\\quad\\quad\\:\\mathsf{a}_{\\phi_{\\mathrm{r}}\\to\\phi_{\\mathrm{mb}}^{\\delta}(x_{a:b-1}\\{1:a-1\\})}}\\end{array}\\bigg(\\underset{z_{\\mathrm{c}},b-1}{\\overset{H[\\phi>\\phi)}{\\sum}}(\\mathsf{t}_{a:b-1})\\leq(b-a)\\cdot(1-\\alpha/8)\\bigg)\\geq1-\\mathsf{n e g l}(b-a)-(b-a)\\cdot2^{-\\frac{1}{8}\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Lemma E.8 (Substring robustness of the watermark). Suppose that $p,\\alpha\\,\\in\\,(0,1)$ are given, and PRC is zero-bit PRC with block length $n(\\lambda)$ over alphabet $\\Sigma_{\\mathsf{P R C}}(\\lambda)$ , satisfying $|\\dot{\\Sigma_{\\mathsf{P R C}}}(\\bar{\\lambda})|\\geq n(\\lambda)$ . Suppose further that PRC is robust to any $\\begin{array}{r}{(1-\\frac{\\alpha}{8}+\\frac{3p}{\\alpha})}\\end{array}$ -edit-bounded channel. Let Model $(\\lambda)$ be defined over some alphabet $\\Sigma(\\lambda)$ satisfying $\\begin{array}{r}{|\\Sigma(\\lambda)|\\,\\ge\\,(\\frac{8}{\\alpha}|\\Sigma_{\\mathsf{P R C}}(\\lambda)|)^{2/\\alpha}}\\end{array}$ . Then the watermarking scheme $\\mathcal{W}$ [PRC, Model] (defined in Algorithm $^3$ ) is $\\beta_{\\lambda}(\\ddot{\\ell})$ -substring robust to any $p$ -edit-bounded channel $\\mathcal{E}$ , where $\\beta_{\\lambda}(\\ell)\\,\\!\\stackrel{\\cdot}{=}8n(\\lambda)+6\\alpha\\ell$ . ", "page_idx": 36}, {"type": "text", "text": "Proof. For convenience we write $p_{1}:=\\alpha/8$ , $p_{0}:=3p/\\alpha$ ; then PRC is robust to any $(1-p_{1}+p_{0})$ - bounded channel. Let $\\lambda$ denote the security parameter for the given PRC. We will show that for all $i,j\\in[L_{\\operatorname*{max}}(\\lambda)]$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\left(\\mathtt{S k},\\phi\\right)\\leftarrow\\operatorname{Setup}(1^{\\mathtt{1}})}{\\operatorname*{Pr}}\\left(\\mathsf{D e t e c t}(1^{\\lambda},(\\mathtt{s k},\\phi),\\mathfrak{t}^{\\prime})=\\mathsf{F a l s e~a n d~}H_{\\mathrm{e}}^{[i:j)}(\\mathfrak{t})\\geq\\beta_{\\lambda}(j-i)\\cdot\\ln|\\Sigma(\\lambda)|\\right)\\leq\\mathsf{n e g}}\\\\ &{-\\mathsf{N a t}(1^{\\lambda},(\\mathtt{s k},\\phi)),\\mathfrak{t}^{\\prime}\\leftarrow\\varepsilon(\\mathfrak{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Using (37), the fact that $L_{\\mathtt{m a x}}(\\lambda)\\leq\\mathrm{poly}(\\lambda)$ , and a union bound over all possible choices of $i,j$ will yield the desired claim of $\\beta_{\\lambda}(\\ell)$ -substring robustness. To establish (37), fix $\\lambda\\in\\mathbb N$ , and set $\\mathsf{M o d e l}=\\mathsf{M o d e l}(\\lambda),n=n(\\lambda),\\Sigma=\\Sigma(\\lambda),\\Sigma_{\\mathsf{P R C}}=\\Sigma_{\\mathsf{P R C}}(\\lambda)$ , as well as $i,j\\in[L_{\\operatorname*{max}}(\\lambda)]$ ; we argue in two parts: ", "page_idx": 36}, {"type": "text", "text": "Step 1: defining channels. Let us write $\\ell:=j-i$ and let $a_{1}<a_{2}<\\cdot\\cdot<a_{h}$ be the indices in $[i,j-1]$ denoting the start positions of blocks for the PRC in the execution of $\\mathsf{W a t}(1^{\\lambda},(\\mathsf{s k},\\phi))$ (in particular, $a_{1},\\ldots,a_{h}$ are simply the integers in $[i,j-1]$ congruent to $\\{\\pmod{n}\\}$ ). We will write $\\mathsf{t}^{\\prime}:=\\mathcal{E}(\\mathsf{t}_{i:j-1})$ to denote the output of the edit-bounded channel $\\mathcal{E}$ given $\\mathtt{t}_{i:j-1}$ as input. Since $\\mathcal{E}$ is $p_{0}$ -edit-bounded, we have $\\mathrm{len}(\\mathsf{t}^{\\prime})\\,\\leq\\,\\ell^{\\prime}:=\\,\\lfloor(1+p_{0})\\cdot\\ell\\rfloor$ . Given a mapping $\\phi:\\Sigma\\rightarrow\\Sigma_{\\mathsf{P R C}}$ , we let $\\mathcal{F}^{\\phi}(x,\\mathrm{t})$ (given $x\\,\\in\\,\\Sigma_{\\mathsf{P R C}}^{L},\\mathsf{t}\\,\\in\\,\\Sigma^{L})$ denote the joint distribution over random variables $\\left(y_{g,b,b^{\\prime}}\\right)_{g\\in[h-1],b,b^{\\prime}\\in[\\ell^{\\prime}]}$ : $b^{\\prime}\\!-\\!b\\!\\in\\![n(1\\!-\\!p_{0}),n(1\\!+\\!p_{0})]$ , defined as follows: ", "page_idx": 37}, {"type": "text", "text": "\u2022 It draws $\\mathsf{t}^{\\prime}\\sim\\mathcal{E}(\\mathsf{t}_{i:j-1})$ . ", "page_idx": 37}, {"type": "text", "text": "\u2022 For each choice of $g,b,b^{\\prime}$ as above, it lets $y_{g,b,b^{\\prime}}$ to be either: (a) the substring $\\sf t_{b:b^{\\prime}-1}^{\\prime}$ , if $\\phi\\big(\\mathrm{t}_{b;b^{\\prime}-1}^{\\prime}\\big)$ can be obtained from $x_{a_{g}:a_{g+1}-1}$ via a sequence of at most $(1-p_{1}+p_{0})n$ subsitutions, insertions, and deletions, or (b) if not, any string satisfying $\\phi(y_{g,b,b^{\\prime}})\\;=\\;$ xag:ag+1\u22121.12 ", "page_idx": 37}, {"type": "text", "text": "Next, given $\\phi,g,b,b^{\\prime}$ as above, we let Eg\u03c6,b,b\u2032 be the channel which, given as input a string x \u2208\u03a3Pn performs the following operations: ", "page_idx": 37}, {"type": "text", "text": "\u2022 First, it generates $(\\mathsf{s k},\\phi)\\;\\sim\\;\\mathsf{S e t u p}(1^{\\lambda})$ , and generates $\\mathbf{t}~\\sim\\mathsf{W a t}(1^{\\lambda},(\\mathsf{s k},\\phi))$ , with the following modification: when generating output for the block starting at index $a_{g}$ , instead of using a fresh output of $\\mathsf{P R C.E n c o d e}(1^{\\lambda},\\mathsf{s k})$ , it uses the given input string $x$ . Let $x^{1},\\dotsc,x^{\\lceil L_{\\operatorname*{max}}(\\lambda)/n(\\lambda)\\rceil}\\;\\;\\in\\;\\;\\Sigma_{\\mathsf{P R C}}^{n}$ denote the codewords (output by PRC.Encode $(1^{\\lambda},{\\mathsf{s k}}))$ used in the Wat procedure, so that in particular $x^{a_{g}}\\;=\\;x$ . Write $\\Bar{x}=(x^{1},\\cdot\\cdot\\cdot,x^{\\lceil L_{\\operatorname*{max}}(\\lambda)/n(\\lambda)\\rceil})$ . ", "page_idx": 37}, {"type": "text", "text": "\u2022 Then, it samples from the marginal $y_{g,b,b^{\\prime}}\\sim\\mathcal{F}^{\\phi}(\\bar{x},\\mathfrak{t})$ and outputs $\\phi(y_{g,b,b^{\\prime}})$ . ", "page_idx": 37}, {"type": "text", "text": "Claim E.9. For any $g\\in[h-1],\\,b,b^{\\prime}\\in[\\ell^{\\prime}].$ , and $\\phi:\\Sigma\\rightarrow\\Sigma_{\\mathsf{P R C}}$ , the channel Eg\u03c6,b,b\u2032 is (1 \u2212p1 + p0)- edit-bounded. ", "page_idx": 37}, {"type": "text", "text": "Proof. It is immediate from the definition of $\\mathcal{E}_{g,b,b^{\\prime}}^{\\phi}$ and ${\\mathcal{F}}^{\\phi}$ that, almost surely, for a sample $y_{g,b,b^{\\prime}}\\sim$ $\\mathcal{F}^{\\phi}(x,\\mathrm{t})$ , $\\phi(y_{g,b,b^{\\prime}})$ can be obtained from $x$ via a sequence of at most $(1-p_{1}+p_{0})n$ substitutions, insertions, and deletions. ", "page_idx": 37}, {"type": "text", "text": "Finally, an output of Eg\u03c6,b, can be sampled in polynomial time: here we use that $\\mathbf{\\boldsymbol{\\cdot}}\\sim\\mathsf{W a t}(1^{\\lambda},(\\mathsf{s k},\\phi))$ can be sampled in polynomial time (as Model is assumed to be computationally efficient), as well as the fact that it can be determined in polynomial time if one string (namely, $\\phi\\big(\\sf t_{b;b^{\\prime}-1}^{\\prime}\\big)$ above) can be obtained from another string (namely, $x_{a_{g}:a_{g+1}-1}$ above) via a sequence of a given number (i.e., $(1-p_{1}+p_{0})n)$ substitutions, insertions, and deletions. \u53e3 ", "page_idx": 37}, {"type": "text", "text": "Claim E.10. For each $g\\in[h-1]$ , it holds that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{^{1,},^{2},\\ldots,,x^{\\lceil L\\mathrm{max}(\\lambda)\\rceil}}{\\operatorname*{Pr}}(\\lambda)}\\\\ {\\underset{^{1,},^{x^{2}},\\ldots,x^{\\lceil L\\mathrm{max}(\\lambda)/n(\\lambda)\\rceil}\\leftarrow\\operatorname{PRC},\\mathrm{Encode}(1^{\\lambda},\\mathfrak{s}\\mathbf{k})}{\\operatorname*{Pr}}\\left(\\ o r\\ D_{\\mathsf{t h a m}}(x_{a_{g}:a_{g+1}-1},\\phi(\\mathsf{t}_{a_{g}:a_{g+1}-1}))\\!\\leq\\!n\\cdot(1\\!-\\!\\alpha/8)\\right)\\geq1-\\mathsf{n e g l}(\\lambda)}\\\\ {\\textnormal{^{1,},^{x^{2}},\\ldots,x^{\\lceil L\\mathrm{max}(\\lambda)/n(\\lambda)\\rceil}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{\\lambda\\!\\!\\!\\!\\!\\!\\!}&{\\lambda\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ {\\quad x=(x^{1},x^{2},\\ldots,x^{\\lceil L\\mathrm{max}(\\lambda)/n(\\lambda)\\rceil})}\\\\ {\\underset{^{1,}\\sim\\mathcal{E}_{\\mathsf{E n b}}^{\\delta}(x;0)}{\\cdots\\mathcal{E}_{\\mathsf{E n b}}^{\\delta}(x;0)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We emphasize that the notation $x^{1},x^{2},\\ldots\\leftarrow\\mathsf{P R C.E n c o d e}(1^{\\lambda},\\mathsf{s k})$ , $\\mathfrak{t}\\sim\\mathcal{E}_{\\mathsf{E m b}}^{\\phi}(x;\\emptyset)$ means that $\\mathsf{P R C.E n c o d e}(1^{\\lambda},\\mathsf{s k})$ is called repeatedly to produce strings $x^{1},x^{2},\\ldots\\in\\Sigma_{\\mathsf{P R C}}^{n}$ , and the concatenated string $x=\\left(x^{1},x^{2},\\ldots,x^{\\lceil L_{\\operatorname*{max}}(\\lambda)/n(\\lambda)\\rceil}\\right)$ is used as input to the embedding channel EE\u03c6mb(x; \u2205). Notice that this procedure is identical to $\\mathsf{W a t}(1^{\\lambda},(\\mathsf{s k},\\phi))$ , meaning that the distribution of the output string t is identical to the output distribution of ${\\mathsf{W a t}}(1^{\\lambda},(\\mathsf{s k},\\phi))$ . ", "page_idx": 37}, {"type": "text", "text": "Proof of Claim E.10. Let $\\boldsymbol{\\mathcal{A}}$ denote the event that $H_{\\mathrm{e}}^{[a_{g}:a_{g+1}]}(\\mathbf{t})\\quad\\leq\\quad4\\alpha n\\ln|\\Sigma|$ or $D_{\\mathsf{H a m}}(x_{a_{g}:a_{g+1}-1},\\phi(\\mathsf{t}_{a_{g}:a_{g+1}-1}))\\ \\leq\\ n\\ \\cdot(1\\ -\\ \\alpha/8)$ . We have that $\\frac{32\\cdot\\ln^{4}(n)}{n}~\\leq~\\alpha$ ) \u2264 \u03b1 as long as $n=n(\\lambda)\\ge\\tilde{\\Omega}(1/\\alpha)$ , which holds for all $\\lambda$ greater than a sufficiently large constant depending on $\\alpha$ (and it is sufficient for us to only consider such $\\lambda$ as the claimed failure probability is ${\\mathsf{n e g l}}(\\lambda))$ . Thus, we may apply Lemma E.7 with $a=i,b=j,\\Sigma^{\\prime}=\\Sigma_{\\mathsf{P R C}}$ ; doing so, we see that if $\\phi:\\Sigma\\rightarrow\\Sigma_{\\mathsf{P R C}}$ is drawn uniformly and $x\\sim\\operatorname{Unif}\\!\\left(\\Sigma^{L_{\\operatorname*{max}}(\\lambda)}\\right)$ , the event $\\boldsymbol{\\mathcal{A}}$ occurs with probability $1-\\mathsf{n e g l}(n)-n\\cdot2^{-\\frac18\\alpha|\\Sigma|^{\\alpha}}\\geq1-\\mathsf{n e g l}(\\lambda)-n\\cdot2^{-\\frac18\\alpha|\\Sigma|^{\\alpha}}$ (here we have used $n\\geq\\lambda$ ). Since a draw of $\\mathsf{t}\\sim\\mathcal{E}_{\\mathsf{E m b}}^{\\phi}(\\boldsymbol{x};\\emptyset)$ may be implemented in polynomial time (as we have assumed that Model is computationally efficient), and since $H_{\\mathrm{e}}^{[a_{g}:a_{g+1})}(\\mathfrak{t})$ , $D_{\\mathsf{H a m}}(x_{a_{g}:a_{g+1}-1},\\phi(\\mathsf{t}_{a_{g}:a_{g+1}-1}))$ can be com puted in polynomial time, it follows that $\\boldsymbol{\\mathcal{A}}$ must occur with probability $1-\\mathsf{n e g l}(\\lambda)-n\\cdot2^{-\\frac{1}{8}\\alpha|\\Sigma|^{\\alpha}}$ when $x^{1},x^{2},\\dotsc,x^{\\lceil L_{\\mathsf{m a x}}(\\lambda)/n(\\lambda)\\rceil}\\gets\\mathsf{P R C.E n c o d e}(1^{\\lambda},\\mathsf{s k})$ . (Otherwise, we could violate undetectability of PRC by generating codewords $x^{1},x^{2},\\therefore$ from the encoding oracle $\\scriptscriptstyle\\mathcal{O}$ , representing either PRC.Encode or a random oracle, and checking whether $\\boldsymbol{\\mathcal{A}}$ occurs.) ", "page_idx": 38}, {"type": "text", "text": "Finally, we remark that n \u00b7 2\u221218 \u03b1|\u03a3|\u03b1\u2264negl(n) \u2264negl(\u03bb) by our requirement that |\u03a3|\u03b1 \u2265|\u03a3PRC| \u2265 $n$ . ", "page_idx": 38}, {"type": "text", "text": "Since the channel $\\mathcal{E}$ is $p$ -edit-bounded, for most values of $g\\in[h-1]$ indexing full blocks of $\\mathtt{t}_{i:j-1}$ , there must be some $b,b^{\\prime}$ so that $\\sf t_{b:b^{\\prime}-1}^{\\prime}$ can be obtained from $\\mathtt{t}_{a_{g}:a_{g+1}-1}$ by at most $O(p n)$ insertions and deletions; this is formalized in the below claim: ", "page_idx": 38}, {"type": "text", "text": "Claim E.11. Fix any $C\\geq1$ and suppose $h\\geq2$ . For any string $\\mathfrak{t}_{i:j-1}\\,\\in\\,\\Sigma^{\\ell}$ , there are at least $\\lfloor(h-1)\\cdot(1-1/C)\\rfloor$ values of $g\\in[h-1]$ so that there exist $b,b^{\\prime}\\in[\\ell^{\\prime}]$ for which $\\sf t_{b:\\,b^{\\prime}-1}^{\\prime}$ can be obtained from $\\mathtt{t}_{a_{g}:a_{g+1}-1}$ by applying at most $3C p\\cdot n$ substitutions, insertions, and deletions (where we have $\\mathsf{t}^{\\prime}=\\mathcal{E}(\\mathsf{t}_{i:j-1}),$ ). ", "page_idx": 38}, {"type": "text", "text": "Proof of Claim E.11. For each $g\\,\\in\\,[h-1]$ and $\\mathfrak{t}_{i:j-1}\\,\\in\\,\\Sigma^{\\ell}$ , the edit-bounded channel $\\mathcal{E}$ sends the substring $\\mathtt{t}_{a_{g}:a_{g+1}-1}$ to some substring $\\bar{\\mathrm{t}}_{b_{g}:b_{g+1}-1}^{\\prime}$ of the (possibly random) channel output $\\mathsf{t}^{\\prime}=\\mathcal{E}(\\mathsf{t}_{i:j-1})$ . (In particular, $b_{g}$ denotes the index in $\\mathrm{t^{\\prime}}$ of the first token in $\\mathtt{t}_{a_{g}:a_{g+1}-1}$ which is not deleted by $\\mathcal{E}$ , or, if all of $\\mathtt{t}_{a_{g}:a_{g+1}-1}$ is deleted, the index in $\\mathrm{t^{\\prime}}$ of the first token following $\\mathtt{t}_{a_{g+1}-1}$ which is not deleted by $\\mathcal{E}$ .) Note that $b_{g}$ is a function of $\\mathtt{t}_{i:j-1}$ ; we omit this dependence from the notation to avoid clutter. ", "page_idx": 38}, {"type": "text", "text": "Fix $\\mathtt{t}_{i:j-1}$ and $\\mathsf{t}^{\\prime}\\,=\\,\\mathcal{E}(\\mathsf{t}_{i:j-1})$ . For each $g\\,\\in\\,[h-1]$ , let $e_{g}$ denote the number of substitutions, insertions, and deletions applied by $\\mathcal{E}$ to obtain $\\mathrm{t}_{b_{g}:b_{g+1}-1}^{\\prime}$ from $\\mathtt{t}_{a_{g}:a_{g+1}-1}$ . Since $\\mathcal{E}$ is $p$ -editbounded, we have $\\begin{array}{r}{\\sum_{g=1}^{h-1}e_{g}\\le p\\ell\\le p\\cdot(h+1)n}\\end{array}$ . Therefore, at least $\\lfloor(h-1)\\cdot(1-1/C)\\rfloor$ values of $g\\in[h-1]$ satisfy $\\begin{array}{r}{e_{g}\\leq\\frac{C}{h-1}\\cdot p(h+1)n\\leq3C p\\cdot n}\\end{array}$ , as desired. \u53e3 ", "page_idx": 38}, {"type": "text", "text": "Step 2: using robustness of the PRC. Since each channel $\\mathcal{E}_{g,b,b^{\\prime}}^{\\phi}$ (for any $\\phi:\\Sigma\\rightarrow\\Sigma_{\\mathsf{P R C}}$ , $g\\in$ $[h-1],\\;b,b^{\\prime}\\in[\\ell^{\\prime}]$ with $b^{\\prime}-b\\in[n(1-p_{0}),n(1+p_{0})])$ is $(1-p_{1}+p_{0})$ -bounded (by Claim E.9), it holds that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\quad\\;\\;\\left(\\mathsf{P R C}.\\mathsf{D e c o d e}(1^{\\lambda},\\mathsf{s k},y)\\neq\\perp\\right)\\ge1-\\mathsf{n e g l}(\\lambda).}\\\\ &{\\quad(\\mathsf{s k},\\phi)\\!\\gets\\!\\mathsf{S e t u p}(1^{\\lambda})\\,}\\\\ &{\\quad\\!\\!x\\!\\gets\\!\\mathsf{P R C}.\\mathsf{E n c o d e}(1^{\\lambda},\\mathsf{s k})}\\\\ &{\\quad\\quad y\\!\\sim\\!\\mathcal{E}_{g,b,b^{\\prime}}^{\\phi}(x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Note that the distribution of $y\\,\\sim\\,\\mathcal{E}_{g,b,b^{\\prime}}^{\\phi}(x)$ , for $x\\,\\gets\\,\\mathsf{P R C.E n c o d e}(1^{\\lambda},\\mathsf{s k})$ , is the same as the marginal distribution of $\\phi(y_{g,b,b^{\\prime}})\\ \\sim\\ \\mathcal{F}^{\\phi}(x,{\\sf t})$ , for $x_{1},x_{2},\\dotsc\\leftarrow\\mathsf{P R C.E n c o d e}(1^{\\lambda},\\mathsf{s k})$ , $x:=$ $(x^{1},\\ldots,x^{\\lceil L_{\\operatorname*{max}}(\\lambda)/n(\\lambda)\\rceil})$ , $\\mathsf{t}\\sim\\mathcal{E}_{\\mathsf{E m b}}^{\\phi}(\\boldsymbol{x};\\emptyset)$ . Therefore, using (39) together with a union bound over the (polynomially many) choices of $g,b,b^{\\prime}$ , we see that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\qquad\\qquad\\qquad\\qquad\\left(\\forall g,b,b^{\\prime}:\\,\\mathsf{P R C}.\\mathsf{D e c o d e}(1^{\\lambda},\\mathsf{s k},\\phi(y_{g,b,b^{\\prime}}))\\neq\\bot\\right)\\ge1-\\mathsf{n e g l}(\\lambda)}\\\\ &{\\qquad\\qquad(\\mathsf{s k},\\phi)\\!\\circ\\!\\mathsf{S e t u p}(1^{\\lambda})}\\\\ {\\qquad\\cdot\\!\\,\\lambda,x^{2},\\ldots,x^{\\lceil L_{\\mathsf{m a x}}(\\lambda)/n(\\lambda)\\rceil}\\!\\downarrow\\!\\to\\!\\mathsf{P R C}.\\mathsf{E n c o d e}(1^{\\lambda},\\mathsf{s k})}\\\\ &{x\\!=\\!(x^{1},x^{2},\\ldots,x^{\\lceil L_{\\mathsf{m a x}}(\\lambda)/n(\\lambda)\\rceil}),\\mathsf{t}\\!\\sim\\!\\mathcal{E}_{\\mathsf{t m b}}^{\\mathsf{a i n}}(x;\\theta)}\\\\ &{\\qquad\\qquad(y_{g,b,b^{\\prime}})_{g,b,b^{\\prime}}\\!\\sim\\!\\mathcal{F}^{\\phi}(x,\\mathsf{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For any $i\\in[L_{\\sf m a x}(\\lambda)]$ , by Lemma E.5 with $a=i,b=i+n$ , there is some event $B_{i}$ that occurs with probability $1-\\mathsf{n e g l}(\\ell)\\geq1-\\mathsf{n e g l}(\\lambda)$ under $\\mathtt{t}\\leftarrow\\overline{{\\mathsf{M o d e l}}}$ so that, under $B_{i}$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\nH_{\\mathbf{e}}^{[i:i+n)}(\\mathbf{t})\\leq\\frac{3}{2}H_{\\mathbf{m}}^{[i:i+n)}(\\mathbf{t})+8\\ln|\\Sigma|\\cdot\\ln^{4}(n)\\leq\\frac{3}{2}n\\ln|\\Sigma|+8\\ln|\\Sigma|\\cdot\\ln^{4}(n)\\leq2n\\ln|\\Sigma|,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the final inequality holds as long as $\\lambda$ is chosen sufficiently large so that $n(\\lambda)\\ge20\\ln^{4}(n(\\lambda))$ . By undetectability of PRC, Lemma E.4, and efficiency of Model, the event $B_{i}$ also holds with probability $1-{\\mathsf{n e g l}}(\\lambda)$ under the distribution of t specified in (38). Let us write $B:=B_{a_{1}-n}\\cap B_{a_{1}}\\cap$ $\\cdots\\cap B_{a_{h}}$ ; thus, under $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H_{\\mathrm{e}}^{[i:a_{1})}(\\mathfrak{t})\\leq2n\\ln|\\Sigma|,\\qquad H_{\\mathrm{e}}^{[a_{g}:a_{g+1})}\\leq2n\\ln|\\Sigma|\\,\\,\\,\\forall g\\in[h-1],\\qquad H_{\\mathrm{e}}^{[a_{h}:j)}(\\mathfrak{t})\\leq2n\\ln|\\Sigma|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Consider the distribution of $\\boldsymbol{(x,\\mathsf t,t^{\\prime})}$ where $(x,\\sf t)$ are drawn as specified in (38) and $\\mathrm{t^{\\prime}~\\sim~}$ $\\mathcal{E}(\\mathfrak{t}_{i:j-1})$ . Let $\\boldsymbol{\\mathcal{A}}$ denote the event that for all $g\\ \\in\\ [h\\,-\\,1],\\ H_{\\mathsf{e}}^{[a_{g}:a_{g+1})}\\ \\leq\\ 4\\alpha\\,\\cdot\\,n\\ln|\\Sigma|$ or $D_{\\mathsf{H a m}}(x_{a_{g}:a_{g+1}-1},\\phi(\\mathsf{t}_{a_{g}:a_{g+1}-1}))\\leq n\\cdot(1\\!-\\!\\alpha/8)$ ; by Claim E.10 and a union bound over $g\\in[h\\!-\\!1]$ , $\\boldsymbol{\\mathcal{A}}$ holds with probability $1-{\\mathsf{n e g l}}(\\lambda)$ . ", "page_idx": 39}, {"type": "text", "text": "Under the event $A\\cap B$ (which occurs with probability $1-{\\mathsf{n e g l}}(\\lambda))$ ), one of the following must be the case: ", "page_idx": 39}, {"type": "equation", "text": "$D_{\\mathsf{H a m}}(x_{a_{g}:a_{g+1}-1},\\phi(\\mathsf{t}_{a_{g}:a_{g+1}-1}))\\leq n\\cdot(1-\\alpha/8);$ ", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "\u2022 It holds that $H_{\\mathrm{e}}^{[i:j)}\\leq8n\\ln{|\\Sigma|}+6\\alpha\\ell\\ln{|\\Sigma|}$ ; let this event be denoted $\\mathcal{C}_{2}$ . ", "page_idx": 39}, {"type": "text", "text": "Indeed, under $A\\cap B$ , if $\\mathcal{C}_{1}$ does not occur, then we must have that $\\mathcal{C}_{2}$ occurs since we would have: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal H}_{\\mathrm{e}}^{[i:j)}={\\cal H}_{\\mathrm{e}}^{[i:a_{1})}(\\mathfrak{t})+{\\cal H}_{\\mathrm{e}}^{[a_{h}:j)}(\\mathfrak{t})+\\sum_{g=1}^{h-1}{\\cal H}_{\\mathrm{e}}^{[a_{g}:a_{g+1})}(\\mathfrak{t})}\\ ~}\\\\ {{\\displaystyle~~~~~~\\leq4n\\ln|\\Sigma|+(\\lfloor\\alpha\\cdot(h-1)\\rfloor+2)\\cdot2n\\ln|\\Sigma|+(h-1-\\lfloor\\alpha\\cdot(h-1)\\rfloor-1)\\cdot4\\alpha n\\ln|\\Sigma|}\\ ~}\\\\ {{\\displaystyle~~~~~~\\leq8n\\ln|\\Sigma|+2\\alpha\\ell\\ln|\\Sigma|+4\\alpha\\ell\\ln|\\Sigma|=8n\\ln|\\Sigma|+6\\alpha\\ell\\ln|\\Sigma|,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the first inequality uses that $A\\cap B$ occurs and the second inequality uses the fact that $(h-1)n\\leq\\ell$ . ", "page_idx": 39}, {"type": "text", "text": "Note that if $h<2$ , under the event $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , the event $\\mathcal{C}_{2}$ occurs. ", "page_idx": 39}, {"type": "text", "text": "Next, if $h\\ \\geq\\ 2$ , by Claim E.11 with $C\\,=\\,1/\\alpha$ , for any string $\\mathfrak{t}_{i:j-1}\\,\\in\\,\\Sigma^{\\ell}$ , with probability 1 over $\\mathrm{t}^{\\prime}\\,\\sim\\,\\mathcal{E}(\\mathfrak{t}_{i:j-1})$ , there are at least $\\lfloor(h-1)\\cdot(1-\\alpha)\\rfloor$ values of $g\\,\\in\\,[h\\mathrm{~-~}1]$ so that there exist $b,b^{\\prime}\\in[\\ell^{\\prime}]$ for which $\\sf t_{b:b^{\\prime}-1}^{\\prime}$ can be obtained from $\\mathtt{t}_{a_{g}:a_{g+1}-1}$ by applying at most ${\\frac{3}{\\alpha}}\\cdot\\,p n$ substitutions, insertions and deletions. Note that $\\lfloor\\alpha\\cdot(h-1)\\rfloor+2+\\lfloor(h-1)\\cdot(1-\\alpha)\\rfloor>h-1$ . Thus, under the event $A\\cap B\\cap\\mathcal{C}_{1}$ , there is some value of $g_{\\star}\\in[h-1]$ and $b_{\\star},b_{\\star}^{\\prime}\\,\\in\\,[\\ell^{\\prime}]$ so that $D_{\\mathsf{H a m}}(x_{a_{g_{\\star}}:a_{g_{\\star}+1}-1},\\phi(\\mathfrak{t}_{a_{g_{\\star}}:a_{g_{\\star}+1}-1}))\\leq n\\cdot(1-\\alpha/8)=n\\cdot(1-\\dot{p}_{1})$ and $\\mathsf{t}_{b_{\\star}:b_{\\star}^{\\prime}-1}^{\\prime}$ can be obtained from $\\mathrm{t}_{a_{g\\star}}\\!:\\!a_{g\\star+1}\\!-\\!1$ by applying at most $\\textstyle p_{0}n={\\frac{3}{\\alpha}}\\cdot p n$ substitutions, insertions and deletions. By definition of $\\mathcal{F}^{\\phi}(x,\\mathrm{t})$ , we may couple a draw of $(y_{g,b,b^{\\prime}})_{g,b,b^{\\prime}}\\sim\\mathcal{F}^{\\phi}(x,\\mathfrak{t})$ to this distribution over $\\boldsymbol{(x,\\mathsf{t},\\mathsf{t^{\\prime}})}$ so that under the event $A\\cap B\\cap C_{1}$ that $(g_{\\star},b_{\\star},b_{\\star}^{\\prime})$ as above exist, we have $y_{g_{\\star},b_{\\star},b_{\\star}^{\\prime}}=\\mathbf{t}_{b_{\\star}:b_{\\star}^{\\prime}-1}^{\\prime}$ . ", "page_idx": 39}, {"type": "text", "text": "Let the event of (40) (which holds with probability $1-{\\mathsf{n e g l}}(\\lambda))$ be denoted by $\\mathcal{D}$ . Then, if $h\\ge$ 2, under the event $A\\cap B\\cap\\mathcal{D}\\cap\\mathcal{C}_{1},\\,g_{\\star},b_{\\star},b_{\\star}^{\\prime}$ are well-defined as above, and so we have that PRC.Dec $\\mathsf{d e}(1^{\\lambda},\\mathsf{s k},\\phi(\\mathsf{t}_{g_{\\star},b_{\\star},b_{\\star}^{\\prime}}^{\\prime}))\\neq\\perp$ , and in particular, that Detect $(1^{\\lambda},({\\sf s k},\\phi),{\\sf t^{\\prime}})=7$ True. Under the event $A\\cap B\\cap D\\cap\\mathcal{C}_{2}\\subset\\mathcal{C}_{2}$ (which must occur under the event $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ if $h\\,<\\,2,$ ), we have that $H_{\\mathrm{e}}^{\\lfloor i:j)}(\\mathfrak{t})\\leq8n\\ln|\\Sigma|+6\\alpha\\ell\\ln|\\Sigma|=\\beta_{\\lambda}(\\ell)\\cdot\\ln|\\Sigma|$ . Since $(A\\cap B\\cap{\\mathcal{D}}\\cap{\\mathcal{C}}_{1})\\cup(A\\cap B\\cap{\\mathcal{D}}\\cap{\\mathcal{C}}_{2})\\supseteq$ $A\\cap B\\cap{\\mathcal{D}}$ occurs with probability $1-{\\mathsf{n e g l}}(\\lambda)$ , we have established (37). \u53e3 ", "page_idx": 39}, {"type": "text", "text": "Lemma E.12 (Soundness of the watermark). For any PRC PRC, the watermarking scheme $\\mathcal{W}[\\mathsf{P R C}]$ (defined in Algorithm 3) is sound. ", "page_idx": 39}, {"type": "text", "text": "Proof. Fix any sequence $\\mathfrak{t}\\in\\Sigma^{\\ell}$ , for some $\\ell\\leq\\lceil L_{\\sf m a x}(\\lambda)/n(\\lambda)\\rceil$ . We wish to show that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{(\\mathsf{s k},\\phi)\\gets\\mathsf{S e t u p}(1^{\\lambda})}\\left(\\mathsf{D e t e c t}(1^{\\lambda},(\\mathsf{s k},\\phi),\\mathsf{t})\\neq\\perp\\right)\\leq\\mathsf{n e g l}(\\lambda).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By definition of Detect (in Algorithm 3), Detect $(1^{\\lambda},(\\mathsf{s k},\\phi),\\mathsf{t})\\neq\\perp$ only if there are some $i,j\\in[\\ell]$ so that PRC.Decode( $\\mathsf{s k},\\phi(\\mathsf{t}_{i:j}))\\neq\\perp$ . Since PRC is sound, for each fixed choice of $\\phi$ and $i,j$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\mathsf{s k\\sim P R C.K e y G e n}(1^{\\lambda})}\\left(\\mathsf{P R C.D e c o d e}(\\mathsf{s k},\\phi(\\mathfrak{t}_{i:j}))\\neq\\perp\\right)\\leq\\mathsf{n e g l}(\\lambda).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Taking expectation over $\\phi$ (which is drawn independently from $\\mathsf{s k}$ ) and a union bound over $i,j\\leq$ $\\ell\\le\\mathrm{poly}(\\lambda)$ , we see that (41) holds. ", "page_idx": 40}, {"type": "text", "text": "Lemma E.13 (Undetectability of the watermark). For any PRC PRC, the watermarking scheme W[PRC] (defined in Algorithm 3) is undetectable. ", "page_idx": 40}, {"type": "text", "text": "Proof. Suppose for the sake of contradiction that there is a polynomial-time adversary Adv so that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{Pr}\\left(\\mathsf{A d v}^{\\overline{{\\mathsf{M o d e l}}}}(1^{\\lambda})=1\\right)-\\operatorname*{Pr}_{\\mathsf{s k}\\sim\\mathsf{S e t u p}(1^{\\lambda})}\\left(\\mathsf{A d v}^{\\mathsf{W a t}(1^{\\lambda},\\mathsf{s k})}(1^{\\lambda})=1\\right)\\right|\\geq\\frac{1}{\\mathsf{p o l y}(\\lambda)}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Using Adv, we can construct an adversary $\\mathsf{A d v^{\\prime}}$ to break the undetectability of PRC, as follows. $\\mathsf{A d v^{\\prime}}$ has access to an oracle $\\scriptscriptstyle\\mathcal{O}$ which is either PRC.Encode $(1^{\\lambda},{\\sf s k})$ (where ${\\mathsf{s}}{\\dot{\\mathsf{k}}}\\gets{\\mathsf{P R C}}.{\\mathsf{K e y G e n}}(1^{\\lambda}))$ or outputs a uniformly random string of length $n(\\lambda)$ at each call. Adv\u2032 then draws $\\phi:\\Sigma\\rightarrow\\Sigma_{\\mathsf{P R C}}$ uniformly at random and simulates Adv, simulating each call to $\\mathsf{W a t}(1^{\\lambda},(\\mathsf{s k},\\phi))$ as in Algorithm 3 but replacing each call to PRC.Encode $(1^{\\lambda},{\\sf s k})$ (Line 13 of Algorithm 3) with a call to $\\scriptscriptstyle\\mathcal{O}$ . In the case that $\\scriptscriptstyle\\mathcal{O}$ is PRC.Encode $(1^{\\lambda},{\\sf s k})$ , then this procedure is exactly $\\mathsf{A d v}^{\\mathsf{W a t}(1^{\\lambda},\\mathsf{s k})}$ . In the case that $\\scriptscriptstyle\\mathcal{O}$ outputs uniformly random strings in response to each call, then Lemma E.4 gives that this procedure is exactly AdvModel. Thus, (42) gives that $\\mathsf{A d v^{\\prime}}$ can distinguish with inverse polynomial advantage between the two cases, a contradiction to undetectability of PRC. Note that in order to simulate Wat, $\\mathsf{A d v^{\\prime}}$ needs to be able to evaluate Model $(\\mathsf{t}_{i}=\\cdot\\mid\\mathsf{t}_{1:i-1}\\right)$ (Line 9 of Algorithm 3), which is possible in polynomial time by assumption on Model (see Appendix B). \u53e3 ", "page_idx": 40}, {"type": "text", "text": "Proof of Theorem E.2. Fix $\\alpha>0$ together with a function family $\\mathcal{F}$ which is a $\\log n(\\lambda)$ -local weak PRF for noise level $q\\,\\in\\,[0,1/2)$ , and a family of language models ${\\sf M o d e l}(\\lambda)$ as in the theorem statement. Write $\\begin{array}{r}{p_{0}:=\\frac{\\alpha}{16C_{\\mathrm{rob}}}}\\end{array}$ 16\u03b1Crob , where Crob is the constant from Theorem 4.1. ", "page_idx": 40}, {"type": "text", "text": "By Theorem 3.2, $\\mathsf{P R C}_{1}:=\\mathsf{P R F-P R C}[\\mathcal{F},\\frac{1}{2}-p_{0},q]$ is a zero-bit binary alphabet PRC with robustness to all $\\left({\\frac{1}{2}}\\right.-\\left.p_{0}\\right)$ -bounded substitution channels. By definition in (3), the block length $N(\\lambda)$ of $\\mathsf{P R C_{1}}$ satisfies: ", "page_idx": 40}, {"type": "equation", "text": "$$\nN(\\lambda)=O\\left(n(\\lambda)^{4\\log(1/p_{0})}\\cdot n(\\lambda)^{2}\\right)\\leq n(\\lambda)^{O(\\log(1/p_{0}))}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Next recall that $C_{0},C_{\\sf r o b}\\geq1$ are the constants from Theorem 4.1. By Theorem 4.1, for $\\rho=C_{0}/p_{0}$ , $\\mathsf{P R C}_{2}:=\\mathsf{P R C}_{\\mathsf{l d x}}[\\mathsf{P R C}_{1},\\rho]$ is a zero-bit PRC with block length at most $N(\\lambda)$ and alphabet $\\Sigma_{\\mathsf{P R C}}(\\lambda)$ of size $\\begin{array}{r}{|\\Sigma_{\\mathsf{P R C}}(\\lambda)|\\leq\\lceil\\frac{C_{0}}{p_{0}}\\cdot N(\\lambda)\\rceil}\\end{array}$ which has robustness to any $(1-C_{\\mathrm{rob}}p_{0})$ -edit-bounded channel. Since $\\begin{array}{r}{1-C_{r\\mathrm{ob}}p_{0}\\geq1-\\frac{\\alpha}{8}+\\frac{3p}{\\alpha}}\\end{array}$ by our choice of $p_{0}$ and since $p\\leq\\alpha^{2}/48$ (which can be ensured by taking $c=1/48$ in the theorem statement), $\\mathsf{P R C_{2}}$ is robust to any $\\begin{array}{r}{(1-\\frac{\\alpha}{8}+\\frac{3p}{\\alpha})}\\end{array}$ -edit-bounded channel. ", "page_idx": 40}, {"type": "text", "text": "Finally, by Theorem E.1, as long as $\\begin{array}{r}{|\\Sigma(\\lambda)|\\,\\ge\\,(\\frac{8}{\\alpha}|\\Sigma_{\\mathsf{P R C}}(\\lambda)|)^{2/\\alpha}}\\end{array}$ the watermarking scheme $\\mathcal{W}=$ $\\mathcal{W}[\\mathsf{P R C}_{2}$ , Model] is sound, undetectable, and $\\beta_{\\lambda}(\\widetilde{\\ell})$ -substring robust to any $p$ -edit-bounded channel, for $\\beta_{\\lambda}(\\ell):=6\\alpha\\ell\\!+\\!8N(\\lambda)\\le6\\alpha\\ell\\!+\\!n(\\lambda)^{O(\\log{1/p_{0}})}$ . To ensure that the lower bound on $|\\Sigma(\\lambda)|$ holds, we note that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\frac{8}{\\alpha}|\\Sigma_{\\mathsf{P R C}}(\\lambda)|\\right)^{2/\\alpha}\\le\\left(\\frac{8}{\\alpha}\\cdot\\left\\lceil\\frac{C_{0}}{p_{0}}\\cdot N(\\lambda)\\right\\rceil\\right)^{2/\\alpha}\\le n(\\lambda)^{O\\left(\\frac{1}{\\alpha}\\log\\frac{1}{\\alpha}\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "which is bounded above by $n(\\lambda)^{C_{2}\\frac{1}{\\alpha}\\log\\frac{1}{\\alpha}}\\leq|\\Sigma(\\lambda)|$ (for sufficiently large $\\lambda$ ) as long as the constant $C_{2}$ in the theorem statement is sufficiently large. ", "page_idx": 40}, {"type": "text", "text": "F Technical tools: concentration inequalities ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Theorem F.1 (McDiarmid\u2019s inequality). Let sets $\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{m}$ equipped with sigma algebras be given, and suppose $f:\\mathcal{X}_{1}\\times\\cdot\\cdot\\cdot\\times\\mathcal{X}_{m}\\to\\mathbb{R}$ satisfies the bounded differences property, i.e., for each $j\\in[m]$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x_{i}\\in{\\mathcal{X}}_{i}~\\forall i\\in[m],x_{j}^{\\prime}\\in{\\mathcal{X}}_{j}}|f(x_{j},x_{-j})-f(x_{j}^{\\prime},x_{-j})|\\leq c_{j},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "for some positive real numbers $c_{j}$ . Then given independent random variables $X_{i}\\in\\mathcal{X}_{i}\\,(i\\in[m]),\\,i$ t holds that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\vert f(X_{1},\\ldots,X_{m})-\\mathbb{E}[f(X_{1},\\ldots,X_{m})]\\vert\\ge\\epsilon\\right)\\le2\\exp\\left(\\frac{-2\\epsilon^{2}}{\\sum_{i=1}^{m}c_{i}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Theorem F.2 (Corollary of Freedman\u2019s inequality; see Lemma A.3 of [FKQR23]). Let $(X_{t})_{t\\leq T}$ be a sequence of random variables adapted to a flitration $({\\mathcal{F}}_{t})_{t\\in[T]}$ . If $0\\le X_{t}\\le R$ almost surely, then each of the below inequalities holds with probability $1-\\delta$ : ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}X_{t}\\leq\\frac{3}{2}\\sum_{t=1}^{T}\\mathbb{E}_{t-1}[X_{t}]+4R\\ln(2/\\delta)}\\\\ &{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{t-1}[X_{t}]\\leq2\\displaystyle\\sum_{t=1}^{T}X_{t}+8R\\ln(2/\\delta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\mathbb{E}_{t-1}[X_{t}]$ denotes $\\mathbb{E}[X_{t}\\mid\\mathcal{F}_{t-1}]$ . ", "page_idx": 41}, {"type": "text", "text": "F.1 Concentration with weakly dependent random variables ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Fix a set $\\mathcal{X}$ equipped with a sigma-algebra together with a positive integer $n$ . Let $P\\in\\Delta(\\mathcal{X}^{n})$ denote a distribution (where $\\textstyle{\\mathcal{X}}^{n}$ is equipped with the product sigma algebra). For $X=(X_{1},\\ldots,X_{n})\\sim P$ , we define the influence of $X_{j}$ on $X_{i}$ to be ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\check{\\l}_{j\\rightarrow i}(X):=\\operatorname*{max}_{\\stackrel{x_{-i-j}\\in X^{n-2}}{x_{j},x_{j}^{\\prime}\\in X}}d_{\\mathrm{TV}}(P(X_{i}=\\cdot\\mid X_{-i-j}=x_{-i-j},X_{j}=x_{j}),P(X_{i}=\\cdot\\mid X_{-i-j}=x_{-i-j},X_{j}=x_{j}))\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Definition F.1. Given a random variable $X$ distributed according to some distribution $P$ over $\\mathcal{X}^{n}$ , define the Dobrushin coefficient of $X$ to be ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\alpha(X):=\\operatorname*{max}\\left\\{\\operatorname*{max}_{j\\in[n]}\\sum_{i\\neq j}I_{j\\to i}(X),\\operatorname*{max}_{i\\in[n]}\\sum_{j\\neq i}I_{j\\to i}(X)\\right\\}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Theorem F.3 (Dobrushin\u2019s concentration inequality; e.g., Theorem 4.3 of [Cha16]). Let $P$ be $a$ distribution over $\\mathcal{X}^{n}$ whose Dobrushin coefficient is $\\alpha$ . Let $f:\\mathcal{X}^{n}\\rightarrow\\mathbb{R}$ be a real-valued function satisfying the following bounded differences condition, for some constants $c_{1},\\ldots,c_{n}\\geq0$ : ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{(x_{1},\\ldots,x_{n})\\in{\\mathcal{X}}^{n},x_{j}^{\\prime}\\in{\\mathcal{X}}}|f(x_{j},x_{-j})-f(x_{j}^{\\prime},x_{-j})|\\leq\\!c_{j}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Then for all $\\epsilon>0$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{X\\sim P}\\left(\\vert f(X)-\\mathbb{E}[f(X)]\\vert\\ge\\epsilon\\right)\\le2\\exp\\left(-\\frac{(1-\\alpha)\\epsilon^{2}}{2\\sum_{i=1}^{n}c_{i}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "G Miscellaneous Lemmas ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Lemma G.1. Suppose a PRC PRC $=$ (KeyGen, Encode, Decode) is robust to any $p$ -substitution-bounded channel. Then for any channel $\\mathcal{E}$ satisfying $\\begin{array}{r}{\\mathrm{Pr}_{\\mathtt{S k}\\sim\\mathtt{K e y G e n}(1^{\\lambda}),x\\leftarrow\\mathtt{E n c o d e}(1^{\\lambda},\\mathtt{s k},\\mathfrak{m}),y\\sim\\mathcal{E}(x)}(D_{\\mathtt{H a m}}(x,y)\\emph{\\lambda}\\le\\emph{\\mathtt{p}n})\\emph{\\lambda}\\ge\\emph{\\lambda}1\\,-\\,\\mathsf{n e g l}(n),}\\end{array}$ PRC is robust to $\\mathcal{E}$ . ", "page_idx": 41}, {"type": "text", "text": "Proof. Let ${\\mathcal{E}}^{\\prime}$ be the channel which samples $y\\,\\sim\\,\\mathcal{E}(x)$ , outputs $y$ if $D_{\\mathsf{H a m}}(y,x)\\,\\leq\\,p n$ , and otherwise outputs $x$ . By assumption PRC is robust to ${\\mathcal{E}}^{\\prime}$ . Moreover, we can construct a joint distribution of $({\\sf s k},x,y,y^{\\prime})$ where: (a) the marginal of $(\\mathsf{s k},x,y)$ is as follows: $\\mathsf{s k}\\sim\\mathsf{K e y G e n}(1^{\\lambda}),x\\gets$ $\\mathsf{E n c o d e}(1^{\\lambda},\\mathsf{s k},\\mathsf{m}),y\\sim\\mathcal{E}(x)$ ; (b) the marginal of $(\\mathsf{s k},x,y^{\\prime})$ is identical except that $y^{\\prime}\\sim\\mathcal{E}^{\\prime}(x)$ ; (c) $y=y^{\\prime}$ with probability $1-{\\mathsf{n e g l}}(\\lambda)$ . Then the robustness criterion for ${\\mathcal{E}}^{\\prime}$ yields that PRC is robust to $\\mathcal{E}$ . \u53e3 ", "page_idx": 42}, {"type": "text", "text": "Lemma G.2 (Theorem 1 of [RS23]). For any positive integers $t,k,N$ with $k\\leq N$ and $t\\leq N$ , it holds that dTV(Bin(t, k/N), Hyp(N, k, t)) \u2264\u221aN2t\u2212t. ", "page_idx": 42}, {"type": "text", "text": "For completeness, we provide the proof of Lemma G.2 below. ", "page_idx": 42}, {"type": "text", "text": "Proof of Lemma G.2. Fix $t,k,N$ as in the lemma statement and let $\\mathbb{P}~=~\\mathrm{Bin}(t,k/N)~~\\in$ $\\Delta(\\{0,1,\\ldots,t\\})$ and $\\mathbb{Q}\\;=\\;\\operatorname{Hyp}(N,k,t)\\;\\in\\;\\Delta(\\{0,1,\\dots,t\\})$ . Also write $\\textit{p}=\\textit{k}/N$ . Then we have $\\mathbb{P}(w)={\\binom{t}{w}}\\cdot p^{w}(1-p)^{t-w}$ and ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{Q}(w)=\\frac{{\\binom{k}{w}}{\\binom{N-k}{t-w}}}{{\\binom{N}{t}}}=\\!\\!{\\binom{t}{w}}\\cdot\\frac{(N-k)\\cdot\\cdot\\cdot\\cdot(N-k-(t-w)+1)\\cdot k\\cdot\\cdot\\cdot(k-w+1)}{N\\cdot\\cdot\\cdot(N-t+1)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad={\\binom{t}{w}}\\cdot\\left(\\frac{k}{N}\\right)^{w}\\cdot\\frac{\\prod_{j=0}^{t-w-1}(1-p-j/N)\\cdot\\prod_{j=0}^{w-1}(1-j/k)}{\\prod_{j=0}^{t-1}(1-j/N)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{Q}(w)}{\\mathbb{P}(w)}=\\frac{\\prod_{j=0}^{t-w-1}(1-p-j/N)\\cdot\\prod_{j=0}^{w-1}(1-j/k)}{(1-p)^{t-w}\\prod_{j=0}^{t-1}(1-j/N)}\\le\\prod_{j=0}^{t-1}\\frac{1}{1-j/N},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "for each $0\\leq w\\leq t$ . By Pinsker\u2019s inequality, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{\\mathrm{PV}}(\\mathbb{Q},\\mathbb{P})\\leq2\\sqrt{\\displaystyle\\sum_{n=0}^{t}\\mathbb{Q}(w)\\cdot\\ln\\frac{\\mathbb{Q}(w)}{\\mathbb{P}}}}\\\\ &{\\qquad\\qquad\\leq2\\sqrt{\\displaystyle\\prod_{j=1}^{t-1}\\frac{1}{1-j/N}}}\\\\ &{\\qquad\\qquad=2\\sqrt{\\displaystyle\\sum_{j=0}^{t-1}\\ln\\frac{1}{1-j/N}}}\\\\ &{\\qquad\\qquad\\leq2\\sqrt{\\displaystyle\\sum_{j=0}^{t-1}\\frac{j}{N-j}}}\\\\ &{\\qquad\\qquad\\leq2\\sqrt{\\displaystyle\\sum_{j=0}^{t-1}\\frac{j}{N-j}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the second inequality uses (44), and the third inequality uses the fact that ln $\\begin{array}{r}{\\cdot\\frac{1}{1-x}\\leq\\frac{1}{(1/x)-1}}\\end{array}$ for all $x\\in[0,1]$ . \u53e3 ", "page_idx": 42}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The abstract and introduction give informal summaries of the results presented in the main body and appendix. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 43}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: Limitations are discussed following Theorem 1.1. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 43}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Proofs are provided in the appendix, and the main computational assumption is stated as Assumption 3.1. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 44}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 44}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not include experiments requiring code. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 45}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 45}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 46}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 46}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper is theoretical in nature and so any direct societal impacts are limited. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 46}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: Broader societal impacts are discussed in the introduction. Due to the theoretical nature of our paper, any direct societal impacts are negligible. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 47}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 47}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 48}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 48}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 48}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 48}]