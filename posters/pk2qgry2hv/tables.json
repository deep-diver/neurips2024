[{"figure_path": "pK2qGRY2Hv/tables/tables_1_1.jpg", "caption": "Table 1: Theoretical guarantees of our algorithms alongside results from literature in the different Tucker rank settings. See Table 3 for the regret bounds of algorithms in each Tucker rank setting with known latent representations.", "description": "This table presents the theoretical upper bounds on the sample complexity in the source phase and regret in the target phase of different transfer reinforcement learning algorithms. Each algorithm is designed for a specific Tucker rank setting of the Markov Decision Process (MDP), which represents the structure of the transition kernels in the source and target MDPs. The Tucker rank settings considered are (S, S, d), (S, d, A), (d, S, A), and (d, d, d), where S, A are the dimensions of the state and action spaces, and d represents the low rank of the latent structure. The table also shows a comparison with existing results from the literature.", "section": "Theoretical Results"}, {"figure_path": "pK2qGRY2Hv/tables/tables_6_1.jpg", "caption": "Table 1: Theoretical guarantees of our algorithms alongside results from literature in the different Tucker rank settings. See Table 3 for the regret bounds of algorithms in each Tucker rank setting with known latent representations.", "description": "This table presents the theoretical upper bounds on the source sample complexity and target regret for transfer reinforcement learning algorithms under four different Tucker rank settings: (S, S, d), (S, d, A), (d, S, A), and (d, d, d).  Each row represents a Tucker rank setting and provides the complexity and regret bounds achieved by the proposed algorithm in the paper, along with comparisons to existing results from the literature.  The transfer-ability coefficient 'a' is a key factor influencing these bounds, reflecting the difficulty of transferring knowledge between source and target MDPs.  The table highlights the algorithm's ability to achieve regret bounds that are independent of the state space (S) and/or action space (A) size in several settings, showcasing the effectiveness of leveraging low-rank structure for efficient transfer learning.", "section": "Theoretical Results"}, {"figure_path": "pK2qGRY2Hv/tables/tables_13_1.jpg", "caption": "Table 1: Theoretical guarantees of our algorithms alongside results from literature in the different Tucker rank settings. See Table 3 for the regret bounds of algorithms in each Tucker rank setting with known latent representations.", "description": "This table summarizes the theoretical guarantees of the proposed transfer reinforcement learning algorithms across different Tucker rank settings ((S, S, d), (S, d, A), (d, S, A), and (d, d, d)).  It compares the source sample complexity (the number of samples needed from source MDPs) and the target regret bound (the difference between the reward collected by the learner and the reward of an optimal policy in the target MDP) achieved by the proposed algorithms with existing results from the literature.  The table highlights how the algorithms' performance scales with various parameters such as the Tucker rank (d), the horizon (H), the number of episodes (T), the transfer-ability coefficient (a) which represents the difficulty of transferring the latent representation, and other common matrix estimation terms.  Note that Table 3 in the paper provides additional results for the case where latent representations are known.", "section": "Theoretical Results"}, {"figure_path": "pK2qGRY2Hv/tables/tables_14_1.jpg", "caption": "Table 1: Theoretical guarantees of our algorithms alongside results from literature in the different Tucker rank settings. See Table 3 for the regret bounds of algorithms in each Tucker rank setting with known latent representations.", "description": "This table summarizes the theoretical upper bounds on the regret achieved by the proposed algorithms in the paper, compared to existing algorithms from the literature.  The table shows regret bounds for four different Tucker rank settings of the transition kernel: (S, S, d), (S, d, A), (d, S, A), and (d, d, d). Each setting represents a different low-rank structure assumption on the relationship between the state, action, and next state in the Markov Decision Process (MDP). The regret bound indicates the difference in cumulative reward between the optimal policy and the policy learned by the algorithm.  The table also highlights the source sample complexity, representing the amount of data needed from source MDPs for effective transfer learning. Note that Table 3 in the paper provides additional regret bounds under the assumption that the latent representation of the MDP is already known.", "section": "Theoretical Results"}]