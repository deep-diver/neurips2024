{"references": [{"fullname_first_author": "Alekh Agarwal", "paper_title": "Provable benefits of representational transfer in reinforcement learning", "publication_date": "2023-00-00", "reason": "This paper is the most closely related work to the current paper, studying representational transfer RL in low-rank MDPs, which the current work extends to various Tucker rank settings."}, {"fullname_first_author": "Chi Jin", "paper_title": "Provably efficient reinforcement learning with linear function approximation", "publication_date": "2020-00-00", "reason": "This paper provides the LSVI-UCB algorithm, which is adapted in the current paper to various low-rank settings, achieving optimal regret bounds."}, {"fullname_first_author": "Mohammad Gheshlaghi Azar", "paper_title": "On the sample complexity of reinforcement learning with a generative model", "publication_date": "2012-00-00", "reason": "This paper establishes fundamental lower bounds on the sample complexity of reinforcement learning, which are used to prove the minimax optimality of the proposed algorithms."}, {"fullname_first_author": "Tyler Sam", "paper_title": "Overcoming the long horizon barrier for sample-efficient reinforcement learning with latent low-rank structure", "publication_date": "2023-00-00", "reason": "This paper introduces the low Tucker rank MDP setting and provides the LR-EVI algorithm, which is used in the source phase of the proposed transfer RL algorithm."}, {"fullname_first_author": "Jiafan He", "paper_title": "Nearly minimax optimal reinforcement learning for linear Markov decision processes", "publication_date": "2023-00-00", "reason": "This paper provides a nearly minimax optimal algorithm for linear MDPs, which serves as a comparison point for the regret bounds of the proposed transfer RL algorithms."}]}