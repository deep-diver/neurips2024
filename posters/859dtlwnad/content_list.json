[{"type": "text", "text": "Pin-Tuning: Parameter-Efficient In-Context Tuning for Few-Shot Molecular Property Prediction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Liang Wang1 2 Qiang Liu1 2 \u2020 Shaozhen Liu1 Xin Sun3 Shu $\\mathbf{W}\\mathbf{u}^{1:}$ 2 Liang Wang1 2 3 ", "page_idx": 0}, {"type": "text", "text": "1New Laboratory of Pattern Recognition (NLPR) State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS) Institute of Automation, Chinese Academy of Sciences (CASIA) 2 School of Artificial Intelligence, University of Chinese Academy of Sciences 3 University of Science and Technology of China ", "page_idx": 0}, {"type": "text", "text": "liang.wang@cripac.ia.ac.cn, qiang.liu@nlpr.ia.ac.cn, liushaozhen2025@ia.ac.cn sunxin000@mail.ustc.edu.cn, {shu.wu, wangliang}@nlpr.ia.ac.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Molecular property prediction (MPP) is integral to drug discovery and material science, but often faces the challenge of data scarcity in real-world scenarios. Addressing this, few-shot molecular property prediction (FSMPP) has been developed. Unlike other few-shot tasks, FSMPP typically employs a pre-trained molecular encoder and a context-aware classifier, benefiting from molecular pre-training and molecular context information. Despite these advancements, existing methods struggle with the ineffective fine-tuning of pre-trained encoders. We attribute this issue to the imbalance between the abundance of tunable parameters and the scarcity of labeled molecules, and the lack of contextual perceptiveness in the encoders. To overcome this hurdle, we propose a parameter-efficient in-context tuning method, named Pin-Tuning. Specifically, we propose a lightweight adapter for pre-trained message passing layers (MP-Adapter) and Bayesian weight consolidation for pretrained atom/bond embedding layers (Emb-BWC), to achieve parameter-efficient tuning while preventing over-fitting and catastrophic forgetting. Additionally, we enhance the MP-Adapters with contextual perceptiveness. This innovation allows for in-context tuning of the pre-trained encoder, thereby improving its adaptability for specific FSMPP tasks. When evaluated on public datasets, our method demonstrates superior tuning with fewer trainable parameters, improving few-shot predictive performance.\u2021 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the field of drug discovery and material science, molecular property prediction (MPP) stands as a pivotal task [5, 9, 63]. MPP involves the prediction of molecular properties like solubility and toxicity, based on their structural and physicochemical characteristics, which is integral to the development of new pharmaceuticals and materials. However, a major challenge encountered in real-world MPP scenarios is data scarcity. Obtaining extensive molecular data with well-characterized properties can be time-consuming and expensive. To address this, few-shot molecular property prediction (FSMPP) has emerged as a crucial approach, enabling predictions with limited labeled molecules [1, 41, 4]. ", "page_idx": 0}, {"type": "text", "text": "The methodology for general MPP typically adheres to an encoder-classifier framework [71, 23, 27, 56], as illustrated in Figure 2(a). In this streamlined framework, the encoder converts molecular structures into vectorized representations [12, 28, 50, 67, 2], and then the classifier uses these representations to predict molecular properties. In the context of few-shot scenarios, two significant discoveries have been instrumental in advancing this task. Firstly, pre-trained molecular encoders have demonstrated consistent effectiveness in FSMPP tasks [20, 14, 58]. This indicates the utility of leveraging pre-acquired knowledge in dealing with data-limited scenarios. Secondly, unlike typical few-shot tasks such as image classification [57, 48], FSMPP tasks greatly benefits from molecular context information. This involves comprehending the seen many-to-many relationships between molecules and properties [58, 45, 73], as molecules are multi-labeled by various properties. These two discoveries have collectively led to the development of the widely used FSMPP framework that utilizes a pre-trained encoder followed by a context-aware classifier, as shown in Figure 2(b). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite the progress, there are observed limitations in the current approaches to FSMPP. Notably, while using a pre-trained molecular encoder generally outperforms training from scratch, fine-tuning the pre-trained encoder often leads to inferior results compared to keeping it frozen, which can be observed in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "The observed ineffective fine-tuning can be attributed to two primary factors: (i) Imbalance between the abundance of tunable parameters and the scarcity of labeled molecules: fine-tuning all parameters of a pre-trained encoder with few labeled molecules leads to a disproportionate ratio of tunable parameters to available data. This imbalance often results in over-ftiting and catastrophic forgetting [7, 6]. (ii) Limited contextual perceptiveness in the encoder: while molecular ", "page_idx": 1}, {"type": "image", "img_path": "859DtlwnAD/tmp/8541c576259e11f0ecaa6eb217c0d565e5eea3ed7f1ff994394a9372208ef751.jpg", "img_caption": ["Figure 1: Comparison of molecular encoders trained via different paradigms: train-from-scratch, pretrain-then-freeze, and pretrain-then-finetune. The evaluation is conducted across two datasets and three encoder architectures [20, 47, 66]. The results consistently demonstrate that while pretraining outperforms training from scratch, the current methods do not yet effectively facilitate finetuning. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "context is leveraged to enhance the classifier [58, 73], the encoder typically lacks the explicit capability to perceive this context, relying instead on implicit gradient-based optimization. This leads to the encoder not directly engaging with the nuanced molecular context information that is critical in FSMPP tasks. In summary, while significant strides have been made, the challenges of imbalance between the number of parameters and labeled data, along with the need for contextual perceptiveness in the encoder, necessitate more sophisticated methodologies in this domain. ", "page_idx": 1}, {"type": "text", "text": "Based on the aforementioned analysis, we propose the parameter-efficient in-context tuning method, named Pin-Tuning, to address the two primary challenges in FSMPP. To overcome the parameterdata imbalance, we propose a parameter-efficient chemical knowledge adaptation approach for pre-trained molecular encoders. A lightweight adapters (MP-Adapter) are designed to tune the pretrained message passing layers efficiently. Additionally, we impose a Bayesian weight consolidation (Emb-BWC) on the pre-trained embedding layers to prevent aggressive parameter updates, thereby mitigating the risk of over-fitting and catastrophic forgetting. To address the second challenge, we further endow the MP-Adapter with the capability to perceive context. This innovation allows for in-context tuning of the pre-trained molecular encoders, enabling them to adapt more effectively to specific downstream tasks. Our approach is rigorously evaluated on public datasets. The experimental results demonstrate that our method achieves superior tuning performance with fewer trainable parameters, leading to enhanced performance in few-shot molecular property prediction. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of our work are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We analyze the deficiencies of existing FSMPP approaches regarding the adaptation of pre-trained molecular encoders. The key issues include an imbalance between the number of tunable parameters and labeled molecules, as well as a lack of contextual perceptiveness in the encoders. \u2022 We propose Pin-Tuning to adapt the pre-trained molecular encoders for FSMPP tasks. This includes the MP-Adapter for message passing layers and the Emb-BWC for embedding layers, facilitating parameter-efficient tuning of pre-trained molecular encoders. \u2022 We further endow the MP-Adapter with the capability to perceive context to allows for in-context tuning, which provides more meaningful adaptation guidance during the tuning process. \u2022 We conduct extensive experiments on benchmark datasets, which show that Pin-Tuning outperforms state-of-the-art methods on FSMPP by effectively tuning pre-trained molecular encoders. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Few-shot molecular property prediction. Few-shot molecular property prediction aims to accurately predict the properties of new molecules with limited training data [49]. Early research applied general few-shot techniques to FSMPP. IterRefLSTM [1] is the pioneer work to leverage metric learning to solve FSMPP problem. Following this, Meta-GGNN [41] and Meta-MGNN [14] introduce meta-learning with graph neural networks, setting a foundational framework that subsequent studies have continued to build upon [39, 40, 4]. It is noteworthy that Meta-MGNN employs a pre-trained molecular encoder [20] and achieves superior results through fine-tuning in the meta-learning process compared to training from scratch. In fact, pre-trained graph neural networks [64, 36, 17, 54, 37] have shown promise in enhancing various graph-based downstream tasks [52, 13], including molecular property prediction [60, 62, 38, 72]. Recent efforts have shifted towards leveraging unique nature in FSMPP, such as the many-to-many relationships between molecules and properties arising from the multi-labeled nature of molecules, often referred to as the molecular context. PAR [58] initially employs graph structure learning [32, 55] to connect similar molecules through a homogeneous context graph. MHNfs [45] introduces a large-scale external molecular library as context to augment the limited known information. GS-Meta [73] further incorporates auxiliary task to depict the many-to-many relationships. ", "page_idx": 2}, {"type": "text", "text": "Parameter-efficient tuning. As pre-training techniques have advanced, tuning of pre-trained models has become increasingly crucial. Traditional full fine-tuning approaches updates all parameters, often leading to high computational costs and the risk of over-fitting, especially when available data for downstream tasks are limited [33, 15]. This challenge has led to the emergence of parameter-efficient tuning [26, 29, 34]. The philosophy of parameter-efficient tuning is to optimize a small subset of parameters, reducing the computational costs while retaining or even improving performance on downstream tasks [19, 69]. Among the various strategies, the adapters [18, 42, 59] have gained prominence. Adapters are small modules inserted between the pre-trained layers. During the tuning process, only the parameters of these adapters are updated while the rest remains frozen, which not only improves tuning efficiency but also offers an elegant solution to the generalization [70, 30, 8]. By keeping the majority of the pre-trained parameters intact, adapters preserve the rich pre-trained knowledge. This attribute is particularly valuable in many real-world applications including FSMPP. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $\\{\\tau\\}$ be a collection of tasks, where each task $\\tau$ involves the prediction of a property $p$ . The training set comprising multiple tasks $\\{\\mathcal{T}_{\\mathrm{train}}\\}$ , is represented as $\\bar{\\mathcal{D}_{\\mathrm{train}}}=\\{(m_{i},y_{i,t})|\\bar{t}\\in\\{\\mathcal{T}_{\\mathrm{train}}\\}\\}$ , with $m_{i}$ indicating a molecule and $y_{i,t}$ its associated label for task $t$ . Correspondingly, the test set $\\mathcal{D}_{\\mathrm{test}}$ , formed by tasks $\\{\\mathcal{T}_{\\mathrm{test}}\\}$ , ensures a separation of properties between training and testing phases, as the property sets $\\{p_{\\mathrm{train}}\\}$ and $\\{p_{\\mathrm{test}}\\}$ are disjoint $(\\{\\dot{p}_{\\mathrm{train}}\\}\\cap\\{p_{\\mathrm{test}}\\}=\\emptyset)$ . ", "page_idx": 2}, {"type": "text", "text": "The goal of FSMPP is to train a model using $\\ensuremath{\\mathcal{D}}_{\\mathrm{train}}$ that can accurately infer new properties from a limited number of labeled molecules in $\\mathcal{D}_{\\mathrm{test}}$ . Episodic training has emerged as a promising strategy in meta-learning [10, 16] to deal with few-shot problem. Instead of retaining all $\\{\\mathcal{T}_{\\mathrm{train}}\\}$ tasks in memory, episodes $\\{E_{t}\\}_{t=1}^{B}$ are iteratively sampled throughout the training process. For each episode $E_{t}$ , a particular task $\\mathcal{T}_{t}$ is selected from the training set, along with corresponding support set $\\mathcal{S}_{t}$ and query set $\\mathcal{Q}_{t}$ . Typically, the prediction task involves classifying molecules into two classes: positive $(y\\,=\\,1)$ ) or negative $(y=0)$ . Then a 2-way $K$ -shot episode $E_{t}\\,=\\,(S_{t},Q_{t})$ is constructed. The support set ${\\cal S}_{t}=\\{(m_{i}^{s},y_{i,t}^{s})\\}_{i=1}^{2K}$ includes $2K$ examples, each class contributing $K$ molecules. The query set containing $M$ molecules is denoted as $\\mathcal{Q}_{t}=\\{(m_{i}^{q},y_{i,t}^{q})\\}_{i=1}^{M}$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Encoder-classifier framework for FSMPP ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Encoder-classifier framework is widely adopted in FSMPP methods. As illustrated in Figure 2(a), given a molecule $m$ whose property need to be predicted, a molecular encoder $f(\\cdot)$ first learns the molecule\u2019s representation based on its structure, i.e., $\\pmb{h}_{m}=\\b{f}(m)\\in\\mathbb{R}^{d}$ . The molecule $m$ is generally represented as a graph $m=({\\boldsymbol{\\nu}},\\mathbf{A},\\mathbf{X},\\mathbf{E})$ , where $\\mathcal{V}$ denotes the nodes (atoms), A represents the adjacent matrix defined by edges (chemical bonds), and $\\mathbf{X},\\mathbf{E}$ denote the original feature of atoms and bonds, then graph neural networks (GNNs) are employed as the molecular encoders [44, 51, 21]. Subsequently, the learned molecular representation is fed into a classifier $g(\\cdot)$ to obtain the prediction $\\hat{y}=g(\\pmb{h}_{m})$ . The model is trained by minimizing the discrepancy between $\\hat{y}$ and the ground truth $y$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Further, two key discoveries have been pivotal for FSMPP. The first is the proven effectiveness of pre-trained molecular encoders, while the second is the significant advantage gained from molecular context. Together, these discoveries have further reshaped the widely adopted FSMPP framework, which combines a pre-trained encoder followed by a context-aware classifier, as shown in Figure 2(b). ", "page_idx": 3}, {"type": "text", "text": "3.3 Pre-trained molecular encoders (PMEs) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Due to the scarcity of labeled data in molecular tasks, molecular pre-training has emerged as a crucial area, which involves training encoders on extensive molecular datasets to extract informative representations. Pre-GNN [20] is a classic pre-trained molecular encoder that has been widely used in addressing FSMPP tasks [14, 58, 73]. The backbone of Pre-GNN is a modified version of Graph Isomorphism Network (GIN) [65] tailored to molecules, which we call GIN-Mol, consisting of multiple atom/bond embedding layers and message passing layers. ", "page_idx": 3}, {"type": "text", "text": "Atom/Bond embedding layers. The raw atom features and bond features are both categorical vectors, denoted as $(i_{v,1},i_{v,2},\\ldots,i_{v,|E_{n}|})$ and $(j_{e,1},j_{e,2},\\ldots,j_{e,|E_{e}|})$ for atom $v$ and bond $e$ , respectively. These categorical features are embedded as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{v}^{(0)}=\\sum_{a=1}^{|E_{n}|}\\mathtt{E m b A t o m}_{a}(i_{v,a}),\\quad h_{e}^{(l)}=\\sum_{b=1}^{|E_{e}|}\\mathtt{E m b B o n d}_{b}^{(l)}(j_{e,b}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathtt{E m b A t o m}_{a}(\\cdot)_{a\\in\\{1,...,|E_{n}|\\}}$ and $\\mathtt{E m b B o n d}_{b}(\\cdot)_{b\\in\\{1,\\dots,|E_{e}|\\}}$ represent embedding operations that map integer indices to $d$ -dimensional real vectors, i.e., $h_{v}^{(0)},h_{e}^{(l)}\\ \\in\\ \\mathbb{R}^{d}$ , $l\\;\\in\\;\\{0,1,\\ldots,L\\,-\\,1\\}$ represents the index of encoder layers, and $L$ is the number of encoder layers. The atom embedding layer is present only in the first encoder layer, while an bond embedding layer exists in each layer. ", "page_idx": 3}, {"type": "text", "text": "Message passing layers. At the $l$ -th encoder layer, atom representations are updated by aggregating the features of neighboring atoms and chemical bonds: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{h}_{v}^{(l)}=\\mathtt{R e L U}\\left(\\mathtt{M L P}^{(l)}\\left(\\sum_{u}\\pmb{h}_{u}^{(l-1)}+\\sum_{e=(v,u)}\\pmb{h}_{e}^{(l-1)}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $u\\in\\mathcal{N}(v)\\cup\\{v\\}$ is the set of atoms connected to $v$ , and $\\pmb{h}_{v}^{(l)}\\in\\mathbb{R}^{d}$ is the learned representation of atom $v$ at the $l$ -th layer. $\\mathtt{M L P}(\\cdot)$ is implemented by 2-layer neural networks, in which the hidden dimension is $d_{1}$ . After MLP, batch normalization is applied right before the ReLU. The molecule-level representation $\\pmb{h}_{m}\\in\\mathbb{R}^{d}$ is obtained by averaging the atom representations at the final layer. ", "page_idx": 3}, {"type": "text", "text": "4 The proposed Pin-Tuning method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section delves into our motivation and proposed method. Our framework for FSMPP is depicted in Figure 2(c). The details of our principal design, Pin-Tuning for PMEs, is present in Figure 2(d). ", "page_idx": 3}, {"type": "text", "text": "As shown in Figure 1, pretraining then finetuning molecular encoders is a common approach. However, fully fine-tuning yields results inferior to simply freezing them. Thus, the following question arises: ", "page_idx": 3}, {"type": "image", "img_path": "859DtlwnAD/tmp/bb01996593f364dd9d2bcfd5d3d4d07f2657c398f1d7d17aa9e58a40001f8584.jpg", "img_caption": ["Figure 2: (a) The vanilla encoder-classifier framework for MPP. (b) The framework widely adopted by existing FSMPP methods, which contains a pre-trained molecular encoder and a context-aware property classifier. (c) Our proposed framework for FSMPP, in which we introduce a Pin-Tuning method to update the pre-trained molecular encoder followed by the property classifier. (d) The details of our proposed Pin-Tuning method for pre-trained molecular encoders. In (b) and (c), we use the property names like SR-HSE to denote the molecular context in episodes. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4.1.1 MP-Adapter: message passing layer-oriented adapter ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For message passing layers in PMEs, the number of parameters is disproportionately large compared to the training samples. To mitigate this imbalance, we design a lightweight adapter targeted at the message passing layers, called MP-Adapter. The pre-trained parameters in each message passing layer include parameters in the MLP and the following batch normalization. We freeze all pre-trained parameters in message passing layers and add a lightweight trainable adapter after MLP in each message passing layer. Formally, the adapter module for $l$ -th layer can be represented as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{v}^{(l)}=\\mathsf{F e e d F o r w a r d}_{\\mathsf{d o w n}}(h_{v}^{(l)})\\in\\mathbb{R}^{d_{2}},}\\\\ {\\Delta{h}_{v}^{(l)}=\\mathsf{F e e d F o r w a r d}_{\\mathsf{u p}}(\\phi(z_{v}^{(l)}))\\in\\mathbb{R}^{d},}\\\\ {\\tilde{h}_{v}^{(l)}=\\mathsf{L a y e r N o r m}(h_{v}^{(l)}+\\Delta{h}_{v}^{(l)})\\in\\mathbb{R}^{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where FeedForward $(\\cdot)$ denotes feed forward layer and LayerNorm $(\\cdot)$ denotes layer normalization. To limit the number of parameters, we introduce a bottleneck architecture. The adapters downscale the original features from $d$ dimensions to a smaller dimension $d_{2}$ , apply nonlinearity $\\phi$ , then upscale back to $d$ dimensions. By setting $d_{2}$ smaller than $d$ , we can limit the number of parameters added. The adapter module has a skip-connection internally. With the skip-connection, we adopt the near-zero initialization for parameters in the adapter modules, so that the modules are initialized to approximate identity functions. Therefore, the encoder with initialized adapters is equivalent to the pre-trained encoder. Furthermore, we add a layer normalization after skip-connection for training stability. ", "page_idx": 4}, {"type": "text", "text": "4.1.2 Emb-BWC: embedding layer-oriented Bayesian weight consolidation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Unlike message passing layers, embedding layers contain fewer parameters. Therefore, we directly fine-tune the parameters of the embedding layers, but impose a constraint to limit the magnitude of parameter updates, preventing aggressive optimization and catastrophic forgetting. ", "page_idx": 4}, {"type": "text", "text": "The parameters in an embedding layer consist of an embedding matrix used for lookups based on the indices of the original features. We stack the embedding matrices of all embedding layers to form $\\Phi\\in\\mathbb{R}^{E\\times d}$ , where $E$ represents the total number of lookup entries. Further, $\\Phi_{i}\\in\\mathbb{R}^{d}$ denotes the $i$ -th row\u2019s embedding vector, and $\\Phi_{i,j}\\in\\mathbb{R}$ represents the $j$ -th dimensional value of $\\Phi_{i}$ . ", "page_idx": 4}, {"type": "text", "text": "To avoid aggressive optimization of $\\Phi$ , we derive a Bayesian weight consolidation framework tailored for embedding layers, called Emb-BWC, by applying Bayesian learning theory [3] to fine-tuning. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1: (Emb-BWC ensures an appropriate stability-plasticity trade-off for pre-trained embedding layers.) Let $\\Phi\\in\\mathbb{R}^{E\\times d}$ be the pre-trained embeddings before fine-tuning, and $\\Phi^{\\prime}\\in\\mathbb{R}^{E\\times d}$ be the fine-tuned embeddings. Then, the embeddings can both retain the atom and bond properties obtained from pre-training and be appropriately updated to adapt to downstream FSMPP tasks, by introducing the following Emb-BWC loss into objective during the fine-tuning process: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{Emb-BWC}}=-\\frac{1}{2}\\sum_{i=1}^{E}(\\Phi_{i}^{\\prime}-\\Phi_{i})^{\\top}\\mathbf{H}(\\mathcal{D}_{\\mathcal{P}},\\Phi_{i})(\\Phi_{i}^{\\prime}-\\Phi_{i}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{H}(\\mathcal{D}_{\\mathcal{P}},\\Phi_{i})\\in\\mathbb{R}^{d\\times d}$ is the Hessian of the log likelihood $\\mathcal{L}_{\\mathcal{P}}$ of pre-training dataset $\\mathcal{D}_{\\mathcal{P}}$ at $\\Phi_{i}$ . ", "page_idx": 5}, {"type": "text", "text": "Details on the theoretical derivation of Eq. (6) are given in Appendix A. Since $\\mathbf{H}(\\mathcal{D}_{\\mathcal{P}},\\Phi_{i})$ is intractable to compute due to the great dimensionality of $\\Phi$ , we adopt the diagonal approximation of Hessian. By approximating $\\mathbf{H}$ as a diagonal matrix, the $j$ -th value on the diagonal of $\\mathbf{H}$ can be considered as the importance of the parameter $\\Phi_{i,j}$ . The following three choices are considered. ", "page_idx": 5}, {"type": "text", "text": "Identity matrix. When using the identity matrix to approximate the negation of $\\mathbf{H}$ , Eq. (6) is simplified to $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{Emb-BWC}}^{\\mathrm{IM}}=\\frac{1}{2}\\sum_{i=1}^{E}\\bar{\\sum_{j=1}^{d}}(\\Phi_{i,j}^{\\prime}-\\mathbf{\\bar{\\Phi}}_{i,j})^{2}}\\end{array}$ , assigning equal importance to each parameter. This loss function is also known as L2 penalty with pre-trained model as the starting point (L2-SP) [31]. ", "page_idx": 5}, {"type": "text", "text": "Diagonal of Fisher information matrix. The Fisher information matrix (FIM) $\\mathbf{F}$ is the negation of the expectation of the Hessian over the data distribution, i.e., $\\mathbf{F}=-\\mathbb{E}_{\\mathcal{D}_{\\mathcal{P}}[\\mathbf{H}]}$ , and the FIM can be further simplified with a diagonal approximation. Then, the Eq. (6) is simplified to ${\\mathcal{L}}_{\\mathrm{Emb-BWC}}^{\\mathrm{FIM}}=$ $\\begin{array}{r}{\\frac12\\sum_{i=1}^{E}\\hat{\\mathbf F}_{i}\\big(\\Phi_{i}^{\\prime}-\\Phi_{i}\\big)^{2}}\\end{array}$ , where $\\hat{\\mathbf{F}}_{i}\\in\\mathbb{R}^{d}$ is the diagonal of $\\mathbf{F}(\\mathcal{D}_{\\mathcal{P}},\\Phi_{i})\\in\\mathbb{R}^{d\\times d}$ and the $j$ -th value in $\\hat{\\mathbf{F}}_{i}$ is computed as $\\mathbb{E}_{\\mathcal{D}_{\\mathcal{P}}}(\\partial\\mathcal{L}_{\\mathcal{P}}/\\partial\\Phi_{i,j})^{2}$ . This is equivalent to elastic weight consolidation (EWC) [24]. ", "page_idx": 5}, {"type": "text", "text": "Diagonal of embedding-wise Fisher information matrix. In different property prediction tasks, the impact of the same atoms and inter-atomic interactions may be significant or negligible. Therefore, we propose this choice to assign importance to parameters based on different embeddings, rather than treating each parameter individually. By defining $\\begin{array}{r}{\\tilde{\\Phi}_{i}=\\sum_{j}\\Phi_{i,j}}\\end{array}$ , the total update of the embedding $\\Phi_{i}$ can be represented as $\\begin{array}{r}{\\Delta\\Phi_{i}=\\tilde{\\Phi}_{i}^{\\prime}-\\tilde{\\Phi}_{i}=\\sum_{j}(\\Phi_{i,j}^{\\prime}-\\bar{\\Phi_{i,j}})}\\end{array}$ . Then, the Eq. (6) is reformulated to $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{Emb-EWC}}^{\\mathrm{EFIM}}=\\frac{1}{2}\\sum_{i=1}^{E}\\tilde{\\mathbf{F}}_{i}(\\tilde{\\Phi}_{i}^{\\prime}-\\tilde{\\Phi}_{i})^{2}}\\end{array}$ , where $\\begin{array}{r}{\\tilde{\\mathbf{F}}_{i}=\\sum_{j}\\mathbb{E}_{\\mathcal{D}_{\\mathcal{P}}}(\\partial\\mathcal{L}_{\\mathcal{P}}/\\partial\\Phi_{i,j})^{2}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Detailed derivation is given in Appendix A. Intuitively, these three approximations employ different methods to assign importance to parameters. $\\mathcal{L}_{\\mathrm{Emb-BWC}}^{\\mathrm{IM}}$ assigns the same importance to each parameter, LFEImMb- BWC assigns individual importance to each parameter, and LEEFmIbM-B WC assigns the same importance to parameters within the same embedding vector. ", "page_idx": 5}, {"type": "text", "text": "4.2 Enabling contextual perceptiveness in MP-Adapter ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For different property prediction tasks, the decisive substructures vary. As shown in Figure 2, the ester group in the given molecule determines the property SR-HSE, while the carbon-carbon triple bond determines the property SR-MMP. If fine-tuning can be guided by molecular context, encoding context-specific molecular representations allows for dynamic representations of molecules tailored to specific tasks and enables the modeling of the context-specific significance of substructures. ", "page_idx": 5}, {"type": "text", "text": "Extracting molecular context information. In ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "each episode, we consider the labels of the support molecules on the target property and seen properties, as well as the labels of the query molecules on seen properties, as the context of this episode. We adopt the form of a graph to describe the context. Figure 3 demonstrates the transformation from original context data to a context graph. In the left table, the labels of molecules $\\bar{m}_{1}^{q},m_{2}^{2}$ for property $p_{t}$ are the prediction targets, and the other shaded values are the available context. The right side shows the ", "page_idx": 5}, {"type": "table", "img_path": "859DtlwnAD/tmp/d08ef5837a25d26f3ebbd490ed59e38460ef2117446c93046d7e91e612256e57.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "context graph constructed based on the available context. Specifically, we construct context graph $\\mathcal{G}_{t}=(\\mathcal{V}_{t},{\\mathbf{A}}_{t},{\\mathbf{X}}_{t})$ for episode $E_{t}$ . It contains $M$ molecule nodes $\\{m\\}$ and $P$ property nodes $\\{p\\}$ . Three types of edges indicate different relationships between molecules and properties. ", "page_idx": 5}, {"type": "text", "text": "Then we employ a GNN-based context encoder: $\\mathbf{C}=$ ContextEncoder $(\\mathcal{V}_{t},\\mathbf{A}_{t},\\mathbf{X}_{t})$ , where $\\mathbf{C}\\in$ $\\mathbb{R}^{(M+P)\\times d_{2}}$ denotes the learned context representation matrix for $E_{t}$ . $\\mathcal{V}_{t}$ and ${\\bf A}_{t}$ denote the node set and the adjacent matrix of the context graph, respectively, and $\\mathbf{X}_{t}$ denotes the initial features of nodes. The features of molecule nodes are initialized with a pre-trained molecular encoder. The property nodes are randomly initialized. When we make the prediction of molecule $m$ \u2019s target property $p$ , we take the learned representations of the this molecule $c_{m}$ and of the target property $c_{p}$ as the context vectors. Details about the context encoder are provided in Appendix F.2. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "In-context tuning with molecular context information. After obtaining the context vectors, we consider enabling the molecular encoder to use the context as a condition, achieving conditional molecular encoding. To achieve this, we further refine our adapter module. While neural conditional encoding has been explored in some domains, such as cross-attention [43] and ControlNet [68] for conditional image generation, these methods often come with a significant increase in the number of parameters. This contradicts our motivation of parameter-efficient tuning for few-shot tasks. In this work, we adopt a simple yet effective method. We directly concatenate the context with the output of the message passing layer, and feed them into the downscaling feed-forward layer in the MP-Adapter. Formally, the downscaling process defined in Eq. (3) is reformulated as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pmb{z}^{(l)}=\\mathbf{FeedForward}_{\\mathtt{d o w n}}(\\pmb{h}_{v}^{(l)}\\|\\pmb{c}_{m}\\|\\pmb{c}_{p}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\parallel$ denotes concatenation. Such learned molecular representations are more easily predicted on specific properties, verified in Section 5.5 and Appendix G. ", "page_idx": 6}, {"type": "text", "text": "4.3 Optimization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Following MAML [10], a gradient descent strategy is adopted. Firstly, $B$ episodes $\\{E_{t}\\}_{t=1}^{B}$ are randomly sampled. For each episode, in the inner-loop optimization, the loss on the support set is computed as $\\dot{\\mathcal{L}}_{t,S}^{c l s}(f_{\\theta})$ and the parameters $\\theta$ are updated by gradient descent: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\mathcal{L}_{t,S}^{c l s}(f_{\\theta})=-\\sum_{S_{t}}(y\\log(\\hat{y})+(1-y)\\log(1-\\hat{y})),}}\\\\ {{\\displaystyle\\theta^{\\prime}\\leftarrow\\theta-\\alpha_{i n n e r}\\nabla_{\\theta}\\mathcal{L}_{t,S}^{c l s}(f_{\\theta}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\alpha_{i n n e r}$ is the learning rate. In the outer loop, the classification loss of query set is denoted as $\\mathcal{L}_{t,Q}^{c l s}$ . Together with our Emb-BWC regularizer, the meta-training loss $\\mathcal{L}(f_{\\theta^{\\prime}})$ is computed and we do an outer-loop optimization with learning rate $\\alpha_{o u t e r}$ across the mini-batch: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathcal{L}(f_{\\theta^{\\prime}})=\\displaystyle\\frac{1}{B}\\sum_{t=1}^{B}\\mathcal{L}_{t,\\mathcal{Q}}^{c l s}(f_{\\theta^{\\prime}})+\\lambda\\mathcal{L}_{\\mathrm{Emb-BWC}},}}\\\\ {{\\theta\\leftarrow\\theta-\\alpha_{o u t e r}\\nabla_{\\theta}\\mathcal{L}(f_{\\theta^{\\prime}}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\lambda$ is the weight of Emb-BWC regularizer. The pseudo-code is provided in Appendix B. We also provide more discussion of tunable parameter size and total model size in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Evaluation setups ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We use five common few-shot molecular property prediction datasets from the MoleculeNet [61]: Tox21, SIDER, MUV, ToxCast, and PCBA. Standard data splits for FSMPP are adopted. Dataset statistics and more details of datasets can be found in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "Baselines. For a comprehensive comparison, we adopt two types of baselines: (1) methods with molecular encoders trained from scratch, including Siamese Network [25], ProtoNet [46], MAML [10], TPN [35], EGNN [22], and IterRefLSTM [1]; and (2) methods which leverage pretrained molecular encoders, including Pre-GNN [20], Meta-MGNN [14], PAR [58], and GS-Meta [73]. More details about these baselines are in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "Metrics. Following prior works [1, 58], ROC-AUC scores are calculated on the query set for each meta-testing task, to evaluate the performance of FSMPP. We run experiments 10 times with different random seeds and report the mean and standard deviations. ", "page_idx": 6}, {"type": "table", "img_path": "859DtlwnAD/tmp/2617ee9473fa01d6dc77b3ae9550a9caefca92f4cd148d8bd0ccce87db8d52c7.jpg", "table_caption": ["Table 1: ROC-AUC scores $(\\%)$ on benchmark datasets, compared with methods trained from scratch (first group) and methods that leverage pre-trained molecular encoder (second group). The best is marked with boldface and the second best is with underline. \u2206Improve. indicates the relative improvements over the baseline models in percentage. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "859DtlwnAD/tmp/4a4f4cb02d0b95d8095d8cf82ff80f89b8b4eb8eba4616579016bed93621ae94.jpg", "table_caption": ["Table 2: Ablation analysis on the MP-Adapter, in which we drop different components to form variants. We report ROC-AUC scores $(\\%)$ , and the best performance is highlighted in bold. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Performance comparison ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare Pin-Tuning with the baselines and the results are summarized in Table 1, Table 7, and Table 8. Our method significantly outperforms all baseline models under both the 10-shot and 5-shot settings, demonstrating the effectiveness and superiority of our approach. ", "page_idx": 7}, {"type": "text", "text": "Across all datasets, our method provides greater improvement in the 10-shot scenario than in the 5-shot scenario. This is attributed to the molecular context constructed based on support molecules. When there are more molecules in the support set, the uncertainty in the context is reduced, providing more effective adaptation guidance for our parameter-efficient tuning. ", "page_idx": 7}, {"type": "text", "text": "Among benchmark datasets, our method shows significant improvement on the SIDER dataset, increasing by $10.73\\%$ in the 10-shot scenario and by $8.81\\%$ in the 5-shot scenario. We consider this is related to the relatively balanced ratio of positive to negative samples, as well as the absence of missing labels in the SIDER dataset (Table 5). A balanced and low-uncertainty distribution can better benefit addressing the FSMPP task from our method. ", "page_idx": 7}, {"type": "text", "text": "We also observe that the standard deviations of our method\u2019s results under 10 seeds are slightly higher than that of baseline models. However, our worst-case results are still better than the best baseline model. For example, in 10-shot experiments on the Tox21 dataset, the performance of our method is $91.56\\pm2.57$ . However, our 10 runs yield specific results with the worst-case ROC-AUC reaching 88.02, which is also better than the best baseline model GS-Meta\u2019s result of $86.67\\pm0.41$ . Therefore, a high standard deviation does not mean our method is inferior to baseline models. ", "page_idx": 7}, {"type": "text", "text": "5.3 Ablation study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "For MP-Adapter, the main components consist of: (i) bottleneck adapter module (Adapter), (ii) introducing molecular context to adatpers (Context), and (iii) layer normalization (LayerNorm). The results of ablation experiments are summarized in Table 2. The bottleneck adapter and the modeling of molecular context are the most critical, having the most significant impact on performance. Removing them leads to a noticeable decline, which underscores the importance of parameter-efficient tuning and context perceptiveness in FSMPP tasks. Layer normalization is used to normalize the resulting representations, which is also important for improving the optimization effect and stability. ", "page_idx": 7}, {"type": "text", "text": "For Emb-BWC, we verify the effectiveness of finetuning the embedding layers and regularizing them with different approximations of $\\mathcal{L}_{\\mathrm{Emb-BWC}}$ (Table 3). Since the embedding layers have relatively few parameters, direct fine-tuning can also enhance performance. Applying our proposed regularizers to fine-tuning can further improve the effects. Among the three regularizers, the ", "page_idx": 8}, {"type": "table", "img_path": "859DtlwnAD/tmp/b06349138e41f40a15349614e5441abe8f280ee8033990e73cb635a72b4471af.jpg", "table_caption": ["Table 3: Ablation analysis on the Emb-BWC. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "$\\mathcal{L}_{\\mathrm{Emb-BWC}}^{\\mathrm{IM}}$ is the most effective. This indicates that keeping pre-trained parameters to some extent can better utilize pre-trained knowledge, but the parameters worth keeping in fine-tuning and the important parameters in pre-training revealed by Fisher information matrix are not completely consistent. ", "page_idx": 8}, {"type": "text", "text": "5.4 Sensitivity analysis ", "text_level": 1, "page_idx": 8}, {"type": "image", "img_path": "859DtlwnAD/tmp/ea8d201ab4622775560025a757f37b5a50803cc8391b64c256e4efc596189310.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: Effect of different hyper-parameters. The y-axis represents ROC-AUC scores $(\\%)$ and the $\\mathbf{X}$ -axis is the different hyper-parameters. ", "page_idx": 8}, {"type": "image", "img_path": "859DtlwnAD/tmp/39ceeca721265d04af374b5d4a59c1e2dc75959248f4e7afa1dc564fbc4f4beb.jpg", "img_caption": ["Figure 5: ROC-AUC $(\\%)$ and number of trainable parameters of Pin-Tuning with varied value of $d_{2}$ and full Fine-Tuning method (e.g., GS-Meta) on the Tox21 dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Effect of weight of Emb-BWC regularizer $\\lambda$ . Emb-BWC is applied on the embedding layers to limit the magnitude of parameter updates during fine-tuning. We vary the weight of this regularization $\\lambda$ from $\\{0.{\\bar{0}}1,0.1,1,{\\bar{1}}0\\}$ . The first subfigure in Figure 4 shows that the performance is best when $\\lambda=0.1$ or 1. When $\\lambda$ is too small, the parameters undergo too large updates on few-shot downstream datasets, leading to over-fitting and ineffectively utilizing the pre-trained knowledge. Too large $\\lambda$ causes the parameters of the embedding layers to be nearly frozen, which prevents effective adaptation. ", "page_idx": 8}, {"type": "text", "text": "Effect of hidden dimension of MP-Adapter $d_{2}$ . The results corresponding to different values of $d_{2}$ from $\\{25,50,75,100,150\\}$ are presented in the second subfigure of Figure 4. On the Tox21 dataset, we further analyze the impact of this hyper-parameter on the number of trainable parameters. As shown in Figure 5, the number of parameters that our method needs to train is significantly less than that required by the full fine-tuning method, such as GS-Meta, while our method also performs better in terms of ROC-AUC performance due to solving over-fitting and context perceptiveness issues. When $d=50$ , Pin-Tuning performs best on Tox21, and the number of parameters that need to train is only $14.2\\%$ of that required by traditional fine-tuning methods. ", "page_idx": 8}, {"type": "text", "text": "5.5 Case study ", "text_level": 1, "page_idx": 8}, {"type": "image", "img_path": "859DtlwnAD/tmp/82c7d28b3ec0d21fda5616c4c2ba4b8f1dd5972476435fed4f345d7066e2fd93.jpg", "img_caption": ["Figure 6: Molecular representations encoded by GS-Meta [73]. ", "Figure 7: Molecular representations encoded by Pin-Tuning. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We visualized the molecular representations learned by the GS-Meta and our Pin-Tuning\u2019s encoders in the 10-shot setting, respectively. As shown in Figure 6 and 7, Pin-Tuning can effectively adapt to different downstream tasks based on context information, generating property-specific molecular representations. Across different tasks, our method is more effective in encoding representations that facilitate the prediction of the current property, reducing the difficulty of property prediction from the encoding representation aspect. More case studies are provided in Appendix G. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose a tuning method, Pin-Tuning, to address the ineffective fine-tuning of pre-trained molecular encoders in FSMPP tasks. Through the innovative parameter-efficient tuning and in-context tuning for pre-trained molecular encoders, our approach not only mitigates the issues of parameter-data imbalance but also enhances contextual perceptiveness. The promising results on public datasets underscore the potential of Pin-Tuning to advance this field, offering valuable insights for future research in drug discovery and material science. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is jointly supported by National Science and Technology Major Project (2023ZD0120901), National Natural Science Foundation of China (62372454, 62236010) and the Excellent Youth Program of State Key Laboratory of Multimodal Artificial Intelligence Systems. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Han Altae-Tran, Bharath Ramsundar, Aneesh S Pappu, and Vijay Pande. Low data drug discovery with one-shot learning. ACS Central Science, 3(4):283\u2013293, 2017.   \n[2] Dingshuo Chen, Yanqiao Zhu, Jieyu Zhang, Yuanqi Du, Zhixun Li, Q. Liu, Shu Wu, and Liang Wang. Uncovering neural scaling laws in molecular representation learning. In NeurIPS, 2023.   \n[3] Haolin Chen and Philip N. Garner. Bayesian parameter-efficient fine-tuning for overcoming catastrophic forgetting. arXiv, abs/2402.12220, 2024.   \n[4] Wenlin Chen, Austin Tripp, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Meta-learning adaptive deep kernel gaussian processes for molecular property prediction. In ICLR, 2023.   \n[5] Jianyuan Deng, Zhibo Yang, Hehe Wang, Iwao Ojima, Dimitris Samaras, and Fusheng Wang. A systematic study of key elements underlying molecular property prediction. Nature Communications, 14, 2023.   \n[6] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong Sun. Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models. arXiv, abs/2203.06904, 2022.   \n[7] Ning Ding, Yujia Qin, Guang Yang, Fu Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Haitao Zheng, Jianfei Chen, Y. Liu, Jie Tang, Juanzi Li, and Maosong Sun. Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence, 5:220\u2013235, 2023.   \n[8] Wei Dong, Dawei Yan, Zhijun Lin, and Peng Wang. Efficient adaptation of large vision transformer via adapter re-composing. 2023.   \n[9] Xiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong He, Shanzhuo Zhang, Jingbo Zhou, Fan Wang, Hua Wu, and Haifeng Wang. Geometry-enhanced molecular representation learning for property prediction. Nature Machine Intelligence, 4:127 \u2013 134, 2021.   \n[10] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017.   \n[11] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In ICML, 2017.   \n[12] Jonathan Godwin, Michael Schaarschmidt, Alexander L. Gaunt, Alvaro Sanchez-Gonzalez, Yulia Rubanova, Petar Velickovic, James Kirkpatrick, and Peter W. Battaglia. Simple GNN regularisation for 3d molecular property prediction and beyond. In ICLR, 2022.   \n[13] Renxiang Guan, Zihao Li, Wenxuan Tu, Jun Wang, Yue Liu, Xianju Li, Chang Tang, and Ruyi Feng. Contrastive multiview subspace clustering of hyperspectral images based on graph convolutional networks. IEEE Transactions on Geoscience and Remote Sensing, 2024.   \n[14] Zhichun Guo, Chuxu Zhang, Wenhao Yu, John Herr, Olaf Wiest, Meng Jiang, and Nitesh V. Chawla. Few-shot graph learning for molecular property prediction. In WWW, 2021.   \n[15] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In ICLR, 2022.   \n[16] Timothy M. Hospedales, Antreas Antoniou, Paul Micaelli, and Amos J. Storkey. Meta-learning in neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44:5149\u20135169, 2020.   \n[17] Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. Graphmae: Self-supervised masked graph autoencoders. In KDD, 2022.   \n[18] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In ICML, 2019.   \n[19] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022.   \n[20] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In ICLR, 2020.   \n[21] Dongki Kim, Jinheon Baek, and Sung Ju Hwang. Graph self-supervised learning with accurate discrepancy learning. In NeurIPS, 2022.   \n[22] Jongmin Kim, Taesup Kim, Sungwoong Kim, and Chang D. Yoo. Edge-labeling graph neural network for few-shot learning. In CVPR, 2019.   \n[23] Suyeon Kim, Dongha Lee, SeongKu Kang, Seonghyeon Lee, and Hwanjo Yu. Learning topology-specific experts for molecular property prediction. In AAAI, pages 8291\u20138299, 2023.   \n[24] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114, 117.   \n[25] Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot image recognition. In ICML deep learning workshop, 2015.   \n[26] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In EMNLP (1), pages 3045\u20133059. Association for Computational Linguistics, 2021.   \n[27] Han Li, Dan Zhao, and Jianyang Zeng. KPGT: knowledge-guided pre-training of graph transformer for molecular property prediction. In KDD, pages 857\u2013867, 2022.   \n[28] Shuangli Li, Jingbo Zhou, Tong Xu, Dejing Dou, and Hui Xiong. Geomgcl: Geometric graph contrastive learning for molecular property prediction. In AAAI, pages 4541\u20134549, 2022.   \n[29] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In ACL, 2021.   \n[30] Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, and Xinchao Wang. Graphadapter: Tuning vision-language models with dual knowledge graph. In NeurIPS, 2023.   \n[31] Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit inductive bias for transfer learning with convolutional networks. In ICML, 2018.   \n[32] Zhixun Li, Liang Wang, Xin Sun, Yifan Luo, Yanqiao Zhu, Dingshuo Chen, Yingtao Luo, Xiangxin Zhou, Qiang Liu, Shu Wu, Liang Wang, and Jeffrey Xu Yu. GSLB: the graph structure learning benchmark. In NeurIPS, 2023.   \n[33] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. Scaling down to scale up: A guide to parameter-efficient fine-tuning. arXiv, abs/2303.15647, 2023.   \n[34] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT understands, too. arXiv, abs/2103.10385, 2021.   \n[35] Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang. Learning to propagate labels: Transductive propagation network for few-shot learning. In ICLR, 2019.   \n[36] Yue Liu, Jun Xia, Sihang Zhou, Siwei Wang, Xifeng Guo, Xihong Yang, Ke Liang, Wenxuan Tu, Stan Z. Li, and Xinwang Liu. A survey of deep graph clustering: Taxonomy, challenge, and application. arXiv, abs/2211.12875, 2022.   \n[37] Yue Liu, Ke Liang, Jun Xia, Sihang Zhou, Xihong Yang, Xinwang Liu, and Stan Z. Li. Dink-net: Neural clustering on large graphs. In ICML, 2023.   \n[38] Zhiyuan Liu, Yaorui Shi, An Zhang, Enzhi Zhang, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. Rethinking tokenizer and decoder in masked graph modeling for molecules. In NeurIPS, 2023.   \n[39] Qiujie Lv, Guanxing Chen, Ziduo Yang, Weihe Zhong, and Calvin Yu-Chian Chen. Meta learning with graph attention networks for low-data drug discovery. IEEE transactions on neural networks and learning systems, 2023.   \n[40] Ziqiao Meng, Yaoman Li, Peilin Zhao, Yang Yu, and Irwin King. Meta-learning with motifbased task augmentation for few-shot molecular property prediction. In SDM, 2023.   \n[41] Cuong Q. Nguyen, Constantine Kreatsoulas, and Kim Branson. Meta-learning gnn initializations for low-resource molecular property prediction. In ICML Workshop on Graph Representation Learning and Beyond, 2020.   \n[42] Jonas Pfeiffer, Aishwarya Kamath, Andreas R\u00fcckl\u00e9, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. In EACL, 2021.   \n[43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, pages 10674\u201310685. IEEE, 2022.   \n[44] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. In NeurIPS, 2020.   \n[45] Johannes Schimunek, Philipp Seidl, Lukas Friedrich, Daniel Kuhn, Friedrich Rippmann, Sepp Hochreiter, and G\u00fcnter Klambauer. Context-enriched molecule representations improve fewshot drug discovery. In ICLR, 2023.   \n[46] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning. In NeurIPS, 2017.   \n[47] Ying Song, Shuangjia Zheng, Zhangming Niu, Zhang-Hua Fu, Yutong Lu, and Yuedong Yang. Communicative representation learning on attributed molecular graphs. In IJCAI, 2020.   \n[48] Yisheng Song, Ting Wang, Puyu Cai, Subrota K. Mondal, and Jyoti Prakash Sahoo. A comprehensive survey of few-shot learning: Evolution, applications, challenges, and opportunities. ACM Computing Surveys, 2023.   \n[49] Megan Stanley, John Bronskill, Krzysztof Maziarz, Hubert Misztela, Jessica Lanini, Marwin H. S. Segler, Nadine Schneider, and Marc Brockschmidt. Fs-mol: A few-shot learning dataset of molecules. In NeurIPS Datasets and Benchmarks, 2021.   \n[50] Hannes St\u00e4rk, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, Stephan G\u00fcnnemann, and Pietro Li\u00f3. 3d infomax improves gnns for molecular property prediction. In ICML, pages 20479\u201320502, 2022.   \n[51] Mengying Sun, Jing Xing, Huijun Wang, Bin Chen, and Jiayu Zhou. Mocl: Data-driven molecular fingerprint via knowledge-aware contrastive learning from molecular graph. In KDD, 2021.   \n[52] Xin Sun, Liang Wang, Qiang Liu, Shu Wu, Zilei Wang, and Liang Wang. DIVE: subgraph disagreement for graph out-of-distribution generalization. In KDD, 2024.   \n[53] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In NeurIPS, page 3637\u20133645, 2016.   \n[54] Liang Wang, Xiang Tao, Qiang Liu, Shu Wu, and Liang Wang. Rethinking graph masked autoencoders through alignment and uniformity. In AAAI, 2024.   \n[55] Liang Wang, Shu Wu, Q. Liu, Yanqiao Zhu, Xiang Tao, and Mengdi Zhang. Bi-level graph structure learning for next poi recommendation. IEEE Transactions on Knowledge and Data Engineering, 36:5695\u20135708, 2024.   \n[56] Xu Wang, Huan Zhao, Wei-Wei Tu, and Quanming Yao. Automated 3d pre-training for molecular property prediction. In KDD, pages 2419\u20132430, 2023.   \n[57] Yaqing Wang, Quanming Yao, James T. Kwok, and Lionel M. Ni. Generalizing from a few examples: A survey on few-shot learning. ACM Computing Surveys, 2020.   \n[58] Yaqing Wang, Abulikemu Abuduweili, Quanming Yao, and Dejing Dou. Property-aware relation networks for few-shot molecular property prediction. In NeurIPS, 2021.   \n[59] Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and Jianfeng Gao. Adamix: Mixture-of-adaptations for parameter-efficient model tuning. In EMNLP, 2022.   \n[60] Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. Molecular contrastive learning of representations via graph neural networks. Nature Machine Intelligence, 2022.   \n[61] Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, and Vijay S. Pande. Moleculenet: A benchmark for molecular machine learning. arXiv, abs/1703.00564, 2017.   \n[62] Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, and Stan Z. Li. Mole-bert: Rethinking pre-training graph neural networks for molecules. In ICLR, 2023.   \n[63] Yi Xiao, Xiangxin Zhou, Qiang Liu, and Liang Wang. Bridging text and molecule: A survey on multimodal frameworks for molecule. arXiv, abs/2403.13830, 2024.   \n[64] Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji. Self-supervised learning of graph neural networks: A unified review. TPAMI, 2022.   \n[65] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In ICLR, 2019.   \n[66] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? In NeurIPS, 2021.   \n[67] Sheheryar Zaidi, Michael Schaarschmidt, James Martens, Hyunjik Kim, Yee Whye Teh, Alvaro Sanchez-Gonzalez, Peter W. Battaglia, Razvan Pascanu, and Jonathan Godwin. Pre-training via denoising for molecular property prediction. In ICLR, 2023.   \n[68] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, pages 3813\u20133824. IEEE, 2023.   \n[69] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In ICLR, 2023.   \n[70] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Jiao Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. In ICLR, 2024.   \n[71] Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Chee-Kong Lee. Motif-based graph self-supervised learning for molecular property prediction. In NeurIPS, pages 15870\u201315882, 2021.   \n[72] Yanqiao Zhu, Dingshuo Chen, Yuanqi Du, Yingze Wang, Q. Liu, and Shu Wu. Molecular contrastive pretraining with collaborative featurizations. Journal of Chemical Information and Modeling, 2024.   \n[73] Xiang Zhuang, Qiang Zhang, Bin Wu, Keyan Ding, Yin Fang, and Huajun Chen. Graph sampling-based meta-learning for molecular property prediction. In IJCAI, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The organization of the appendix is as follows: ", "page_idx": 14}, {"type": "text", "text": "\u2022 Appendix A: Derivation of Emb-BWC regularization;   \n\u2022 Appendix B: Pseudo-code of training process;   \n\u2022 Appendix C: Discussion of tunable parameter size and total model size;   \n\u2022 Appendix D: Details of datasets;   \n\u2022 Appendix E: Details of baselines;   \n\u2022 Appendix F: Implementation details;   \n\u2022 Appendix G: More experimental results and discussions;   \n\u2022 Appendix H: Limitations and future directions. ", "page_idx": 14}, {"type": "text", "text": "A Derivation of Emb-BWC regularization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Derivation of ${\\mathcal{L}}_{\\mathbf{Emb}}$ -BWC ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let $\\Phi\\in\\mathbb{R}^{E\\times d}$ be the pre-trained embeddings before fine-tuning, and $\\Phi^{\\prime}\\in\\mathbb{R}^{E\\times d}$ be the fine-tuned embeddings. Further, $\\bar{\\Phi}_{i}\\in\\mathbb{R}^{d}$ denotes the $i$ -th row\u2019s embedding vector in $\\Phi$ , and $\\Phi_{i,j}\\in\\mathbb{R}$ represents the $j$ -th dimensional value of $\\Phi_{i}$ . ", "page_idx": 14}, {"type": "text", "text": "The optimization of embedding layers can be interpreted as performing a maximum a posterior (MAP) estimation of the parameters $\\Phi^{\\prime}$ given the pre-training data and training data of downstream FSMPP task, which is formulated in a Bayesian framework. ", "page_idx": 14}, {"type": "text", "text": "In the FSMPP setting, the molecular encoder has been pre-trained on the pre-training task $\\mathcal{P}$ using data $\\mathcal{D}_{\\mathcal{P}}$ , and is then fine-tuned on a downstream FSMPP task $\\mathcal{F}$ using data $\\mathcal{D}_{\\mathcal{F}}$ . The overall objective is to find the optimal parameters on task $\\mathcal{F}$ while preserving the prior knowledge obtained in pretraining on task $\\mathcal{F}$ . Based on a prior $p(\\Phi^{\\prime})$ of the embedding parameters, the posterior after observing the FSMPP task $\\mathcal{F}$ can be computed with Bayes\u2019 rule: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{p(\\Phi^{\\prime}|T_{\\mathcal{P}},\\mathcal{D}_{\\mathcal{F}})=\\frac{p(\\mathcal{D}_{\\mathcal{F}}|\\Phi^{\\prime},\\mathcal{D}_{\\mathcal{P}})p(\\Phi^{\\prime}|\\mathcal{D}_{\\mathcal{P}})}{p(\\mathcal{D}_{\\mathcal{F}}|\\mathcal{D}_{\\mathcal{P}})}}}\\\\ &{}&{=\\frac{p(\\mathcal{D}_{\\mathcal{F}}|\\Phi^{\\prime})p(\\Phi^{\\prime}|\\mathcal{D}_{\\mathcal{P}})}{p(\\mathcal{D}_{\\mathcal{F}})},\\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathcal{D}_{\\mathcal{F}}$ is assumed to be independent of $\\mathcal{D}_{\\mathcal{P}}$ . Taking a logarithm of the posterior, the MAP objective is therefore: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi^{\\prime*}=\\underset{\\Phi^{\\prime}}{\\arg\\operatorname*{max}}\\log p(\\Phi^{\\prime}|\\mathcal{D}_{\\mathcal{P}},\\mathcal{D}_{\\mathcal{F}})}\\\\ &{\\quad\\ =\\underset{\\Phi^{\\prime}}{\\arg\\operatorname*{max}}\\log p(\\mathcal{D}_{\\mathcal{F}}|\\Phi^{\\prime})+\\log p(\\Phi^{\\prime}|\\mathcal{D}_{\\mathcal{P}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The first term $\\log p(\\mathcal{D}_{\\mathcal{F}}|\\Phi^{\\prime})$ is the log likelihood of the data $\\mathcal{D}_{\\mathcal{F}}$ given the parameters $\\Phi^{\\prime}$ , which can be expressed as the training loss function on task $\\mathcal{F}=-\\log p(\\mathcal{D}_{\\mathcal{F}}|\\Phi^{\\prime})$ , denoted as ${\\mathcal{L}}_{{\\mathcal{F}}}(\\Phi^{\\prime})$ . The second term $p(\\Phi^{\\prime}|\\mathcal{D}_{\\mathcal{P}})$ is the posterior of the parameters given the pre-training dataset $\\mathcal{D}_{\\mathcal{P}}$ . Since $\\Phi^{\\prime}=[\\Phi_{1}^{\\prime\\top},\\Phi_{2}^{\\prime\\top},\\dots,\\Phi_{E}^{\\prime\\top}]^{\\top}$ , and $\\Phi_{i}^{\\prime}$ is conditionally independent of $\\Phi_{j}^{\\prime}$ for $i,j=\\{1,\\dotsc,E\\}$ and $i\\ne j$ given condition $\\mathcal{D}_{\\mathcal{P}}$ , we have $\\begin{array}{r}{p(\\Phi^{\\prime}|\\mathcal{D}_{\\mathcal{P}})\\,=\\,\\prod_{i=1}^{E}p(\\Phi_{i}^{\\prime}|\\mathcal{D}_{\\mathcal{P}})}\\end{array}$ . Thus, $\\log p(\\Phi^{\\prime}|D_{\\mathcal{P}})\\,=$ $\\begin{array}{r}{\\sum_{i=1}^{E}\\log p(\\Phi_{i}^{\\prime}|\\mathcal{D}_{\\mathcal{P}})}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "For adapting pre-trained molecular embedding layers to downstream FMSPP tasks, this posterior must encompass the prior knowledge of the pre-trained embedding layers to reflect which parameters are important for pre-training task $\\mathcal{P}$ . Despite the true posterior being intractable, $\\log p(\\bar{\\Phi_{i}^{\\prime}}|\\mathcal{D}_{\\mathcal{P}})$ can be defined as a function $f(\\Phi_{i}^{\\prime})$ and approximated around the optimum point $f(\\Phi_{i})$ , where $f(\\Phi_{i})$ is the pre-trained values and $\\bar{\\nabla}f(\\Phi_{i})\\bar{=}\\,0$ . Performing a second-order Taylor expansion on $f(\\Phi_{i}^{\\prime})$ around $\\Phi_{i}$ gives: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\log p\\left(\\Phi_{i}^{\\prime}\\mid\\mathcal{D}_{\\mathcal{P}}\\right)\\approx f\\left(\\Phi_{i}\\right)+\\frac{1}{2}\\left(\\Phi_{i}^{\\prime}-\\Phi_{i}\\right)^{T}\\nabla^{2}f\\left(\\Phi_{i}\\right)\\left(\\Phi_{i}^{\\prime}-\\Phi_{i}\\right)}\\\\ {\\displaystyle\\qquad\\qquad\\qquad=f\\left(\\Phi_{i}\\right)+\\frac{1}{2}\\left(\\Phi_{i}^{\\prime}-\\Phi_{i}\\right)^{T}\\mathbf{H}\\big(\\mathcal{D}_{\\mathcal{P}},\\Phi_{i}\\big)\\left(\\Phi_{i}^{\\prime}-\\Phi_{i}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathbf{H}(\\mathcal{D}_{\\mathcal{P}},\\Phi_{i})\\in\\mathbb{R}^{d\\times d}$ is the Hessian matrix of $f(\\Phi_{i}^{\\prime})$ at $\\Phi_{i}$ . The second term suggests that the posterior of the parameters on the pre-training data can be approximated by a Gaussian distribution with mean $\\Phi_{i}^{\\prime}$ and covariance $\\mathbf{H}(\\Dot{D_{\\mathcal{P}}},\\Phi_{i})^{-1}$ . Following Eq. (13), the training objective becomes: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Phi^{\\prime*}=\\arg\\operatorname*{max}_{\\Phi^{\\prime}}\\log p(\\mathcal{D}_{\\mathcal{F}}|\\Phi^{\\prime})+\\log p(\\Phi^{\\prime}|\\mathcal{D}_{\\mathcal{P}})}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle\\Phi^{\\prime}}\\end{array}=\\arg\\operatorname*{max}_{\\Phi^{\\prime}}\\log p(\\mathcal{D}_{\\mathcal{F}}|\\Phi^{\\prime})+\\sum_{i=1}^{E}\\log p(\\Phi_{i}^{\\prime}|\\mathcal{D}_{\\mathcal{P}})}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\arg\\operatorname*{min}_{\\Phi^{\\prime}}\\mathcal{L}(\\Phi^{\\prime})-\\sum_{i=1}^{E}f(\\Phi_{i})-\\frac{1}{2}\\sum_{i=1}^{E}\\big(\\Phi_{i}^{\\prime}-\\Phi_{i}\\big)^{T}\\,\\mathbf{H}(\\mathcal{D}_{\\mathcal{P}},\\Phi_{i})\\,(\\Phi_{i}^{\\prime}-\\Phi_{i})}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\arg\\operatorname*{min}_{\\Phi^{\\prime}}\\mathcal{L}(\\Phi^{\\prime})-\\frac{1}{2}\\sum_{i=1}^{E}(\\Phi_{i}^{\\prime}-\\Phi_{i})^{T}\\,\\mathbf{H}\\big(\\mathcal{D}_{\\mathcal{P}},\\Phi_{i}\\big)\\,(\\Phi_{i}^{\\prime}-\\Phi_{i})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We define the second term as our Emb-BWC regularization objective: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{Emb-BWC}}=-\\frac{1}{2}\\sum_{i=1}^{E}(\\Phi_{i}^{\\prime}-\\Phi_{i})^{\\top}\\mathbf{H}(\\mathcal{D}_{\\mathcal{P}},\\Phi_{i})(\\Phi_{i}^{\\prime}-\\Phi_{i}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.2 Derivation of $\\mathcal{L}_{\\mathrm{Emb}}^{\\mathrm{FIM}}$ -BWC ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Since the Fisher information matrix (FIM) $\\mathbf{F}$ is the negation of the expectation of the Hessian over the data distribution, i.e., $\\mathbf{F}=-\\mathbb{E}_{\\mathcal{D}_{\\mathcal{P}}}[\\mathbf{H}]$ , the objective can be reformulated as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{Emb-BWC}}^{\\mathrm{FIM}}=\\frac{1}{2}\\sum_{i=1}^{E}(\\Phi_{i}^{\\prime}-\\Phi_{i})^{\\top}\\mathbf{F}\\big(\\mathcal{D}_{\\mathcal{P}},\\Phi_{i}\\big)(\\Phi_{i}^{\\prime}-\\Phi_{i}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathbf{F}(\\mathcal{D}_{\\mathcal{P}},\\Phi_{i})\\in\\mathbb{R}^{d\\times d}$ is the corresponding Fisher information matrix of $\\mathbf{H}(\\mathcal{D}_{\\mathcal{P}},\\Phi_{i})$ . Further, the Fisher information matrix can be further simplified with a diagonal approximation. Then, the objective is simplified to: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{Emb-BWC}}^{\\mathrm{FIM}}\\approx\\frac{1}{2}\\sum_{i=1}^{E}\\hat{\\mathbf{F}}_{i}(\\Phi_{i}^{\\prime}-\\Phi_{i})^{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\hat{\\mathbf{F}}_{i}\\in\\mathbb{R}^{d}$ is the diagonal of $\\mathbf{F}(\\mathcal{D}_{\\mathcal{P}},\\Phi_{i})$ . According to the definition of the Fisher information matrix, the $j$ -th value in $\\hat{\\mathbf{F}}_{i}$ is computed as $\\mathbb{E}_{\\mathcal{D}_{\\mathcal{P}}}(\\partial\\mathcal{L}_{\\mathcal{P}}/\\partial\\Phi_{i,j})^{2}$ . In this work, this approximated form is defined as ${\\mathcal{L}}_{\\mathrm{Emb-BWC}}^{\\mathrm{FIM}}$ . ", "page_idx": 15}, {"type": "text", "text": "A.3 Derivation of ${\\mathcal{L}}_{\\mathbf{Emb-l}}^{\\mathbf{EFIM}}$ BWC ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We assume that the parameters within an embedding should share the same importance. To this end, we define $\\begin{array}{r}{\\tilde{\\Phi}_{i}\\,=\\,\\sum_{j}\\Phi_{i,j}}\\end{array}$ , then the total update of the embedding $\\Phi_{i}$ can be represented as $\\begin{array}{r}{\\Delta\\Phi_{i}=\\tilde{\\Phi}_{i}^{\\prime}-\\tilde{\\Phi}_{i}=\\sum_{j}(\\bar{\\Phi_{i,j}^{\\prime}}-\\Phi_{i,j})}\\end{array}$ . Then, the objective in Eq. (16) is reformulated to: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{Emb-EWC}}^{\\mathrm{EFIM}}=\\frac{1}{2}\\sum_{i=1}^{E}\\tilde{\\mathbf{H}}_{i}(\\tilde{\\Phi}_{i}^{\\prime}-\\tilde{\\Phi}_{i})^{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "fwirhste rues $\\begin{array}{r}{\\tilde{\\mathbf{H}}_{i}=\\frac{\\partial^{2}\\mathcal{L}_{\\mathcal{P}}}{\\partial\\tilde{\\Phi}_{i}^{2}}=\\frac{\\partial}{\\partial\\tilde{\\Phi}_{i}}(\\frac{\\partial\\mathcal{L}_{\\mathcal{P}}}{\\partial\\tilde{\\Phi}_{i}})}\\end{array}$ NAecxct,o rwdien cg otnot icnhuaei nt or udlee,r itvhee $\\tilde{\\mathbf{H}}_{i}$ r.i vGaitivveen  otfh $\\begin{array}{r}{\\tilde{\\Phi}_{i}=\\sum_{j=1}^{d}\\Phi_{i,j}}\\end{array}$ $\\frac{\\partial\\mathcal{L}_{\\mathcal{P}}}{\\partial\\tilde{\\Phi}_{i}}$ $\\mathcal{L}_{\\mathcal{P}}$ $\\partial\\tilde{\\Phi}_{i}$ can be computed as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}_{\\mathcal{P}}}{\\partial\\tilde{\\Phi}_{i}}=\\sum_{j}\\frac{\\partial\\mathcal{L}_{\\mathcal{P}}}{\\partial\\Phi_{i,j}}\\frac{\\partial\\Phi_{i,j}}{\\partial\\tilde{\\Phi}_{i}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\tilde{\\Phi}_{i}\\,=\\,\\Phi_{i,1}+\\Phi_{i,2}+...+\\Phi_{i,d}$ , each of $\\frac{\\partial\\Phi_{i,j}}{\\partial\\tilde{\\Phi}_{i}}$ for $j=1,2,\\ldots,d$ equals 1. Therefore, the equation simplifies to: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}_{\\mathcal{P}}}{\\partial\\tilde{\\Phi}_{i}}=\\sum_{j=1}^{d}\\frac{\\partial\\mathcal{L}_{\\mathcal{P}}}{\\partial\\Phi_{i,j}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When taking the derivative of this with respect to $\\Phi_{i,j}$ again, using the chain rule, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}\\mathcal{L_{P}}}{\\partial\\tilde{\\Phi}_{i}^{2}}=\\frac{\\partial}{\\partial\\tilde{\\Phi}_{i}}(\\sum_{j=1}^{d}\\frac{\\partial\\mathcal{L_{P}}}{\\partial\\Phi_{i,j}})=\\sum_{j=1}^{d}\\sum_{k=1}^{d}\\frac{\\partial}{\\partial\\Phi_{i,k}}(\\frac{\\partial\\mathcal{L_{P}}}{\\partial\\Phi_{i,j}})\\frac{\\partial\\Phi_{i,k}}{\\partial\\tilde{\\Phi}_{i}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Given that $\\Phi_{i,j}(\\Phi_{i,k})$ are all parameters in embedding lookup tables, they are independent of each other. Thus, when j = k, \u2202\u03a6\u2202i,k ( \u2202\u2202\u03a6LiP,j ) \u2202\u2202\u03a6\u03a6\u02dci,ik $\\begin{array}{r}{\\frac{\\partial}{\\partial\\Phi_{i,k}}\\big(\\frac{\\partial\\mathcal{L}_{\\mathcal{P}}}{\\partial\\Phi_{i,j}}\\big)\\frac{\\partial\\Phi_{i,k}}{\\partial\\tilde{\\Phi}_{i}}\\ =\\ \\frac{\\partial^{2}\\mathcal{L}_{\\mathcal{P}}}{\\partial\\Phi_{i,j}^{2}}}\\end{array}$ , otherwise it equals 0. We finally get $\\begin{array}{r}{\\tilde{\\mathbf{H}}_{i}=\\sum_{j}\\frac{\\partial^{2}\\mathcal{L}_{\\mathcal{P}}}{\\partial\\Phi_{i,j}^{2}}=\\sum_{j}\\mathbf{H}(\\mathcal{D}_{\\mathcal{P}},\\Phi_{i})_{j,j}.}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "We still approximate the Hessian with the FIM as in Section A.2, and combining this with the definition of FIM, we arrive at the final objective: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{Emb-EWC}}^{\\mathrm{EFIM}}\\approx\\frac{1}{2}\\sum_{i=1}^{E}\\tilde{\\mathbf{F}}_{i}(\\tilde{\\Phi}_{i}^{\\prime}-\\tilde{\\Phi}_{i})^{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{\\tilde{\\mathbf{F}}_{i}=\\sum_{j}\\mathbb{E}_{\\mathcal{D}_{\\mathcal{P}}}(\\partial\\mathcal{L}_{\\mathcal{P}}/\\partial\\Phi_{i,j})^{2}}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "B Pseudo-code of training process ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To help better understand the training process, we provide the brief pseudo-code of it in Algorithm 1. ", "page_idx": 16}, {"type": "table", "img_path": "859DtlwnAD/tmp/bdd476b2f0c15fd6741361d33580104eb286eb5f355e6be56cfac7b7bfac614a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Discussion of tunable parameter size and total model size ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Tunable parameter size of molecular encoder ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We compare the tunable parameter size of full fine-tuning and our Pin-Tuning. Section 3.3 describes the parameters of the PME, which include those for the embedding layers and the message passing layers. We assume there are $\\left|E_{n}\\right|$ original node features and $|E_{e}|$ edge features. Considering there is one node embedding layer and $L$ edge embedding layers, the total number of parameters for the embedding part is $|E_{n}|d+L|E_{e}|d$ . The parameters in the message passing layer consist of the 2-layer MLP including biases shown in Eq. (2) and its subsequent batch normalization, with each layer having $L(2d d_{1}+d+d_{1}+2d)$ parameters. In summary, the total number of parameters to update in full fine-tuning is ", "page_idx": 16}, {"type": "equation", "text": "$$\nN_{F i n e-T u n i n g}=|E_{n}|d+L(|E_{e}|d+2d d_{1}+3d+d_{1}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In our Pin-Tuning method, the parameters of the embedding layers are still updated. However, in each message passing layer, the original parameters are completely frozen, and the parts that require updating are the two feed-forward layers and the layer normalization in the bottleneck adapter ", "page_idx": 16}, {"type": "table", "img_path": "859DtlwnAD/tmp/547aff1a1199a1d00899f20f1165a6cfbf2e907eb32ac114e5ad06f13ddb0905.jpg", "table_caption": ["Table 4: Comparison of total model size. \u2217indicates that the parameters are frozen. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "859DtlwnAD/tmp/2152411485848fe890e808f94f2ccf3a7ba8121c94ee1d890aa8cb5c2d5754fa.jpg", "table_caption": ["Table 5: Dataset statistics. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "module, amounting to $L(2d d_{2}+d+d_{2}+2d)$ parameters for this part. Therefore, the total number of parameters that need to be updated in our Pin-Tuning is ", "page_idx": 17}, {"type": "equation", "text": "$$\nN_{P i n-T u n i n g}=|E_{n}|d+L(|E_{e}|d+2d d_{2}+3d+d_{2}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The difference in the number of parameters updated between the two tuning methods is $\\Delta N\\,=$ $(d_{1}-d_{2})L(2d+1)$ . ", "page_idx": 17}, {"type": "text", "text": "C.2 Total model size ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We provide a comparison of total model size between our Pin-Tuning and the state-of-the-art baseline method, GS-Meta. The total model size consists of both frozen parameters and trainable parameters. The results are presented in Table 4. The total size of our model is comparable to GS-Meta, but the number of parameters that need to be trained is far less than GS-Meta. ", "page_idx": 17}, {"type": "text", "text": "D Details of datasets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We carry out experiments in MoleculeNet benchmark [61] on five widely used few-shot molecular property prediction datasets: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Tox21: This dataset covers qualitative toxicity measurements and was utilized in the 2014 Tox21 Data Challenge.   \n\u2022 SIDER: The Side Effect Resource (SIDER) functions as a repository for marketed drugs and adverse drug reactions (ADR), categorized into 27 system organ classes.   \n\u2022 MUV: The Maximum Unbiased Validation (MUV) is determined through the application of a refined nearest neighbor analysis, specifically designed for validating virtual screening techniques.   \n\u2022 ToxCast: This dataset comprises a compilation of compounds with associated toxicity labels derived from high-throughput screening.   \n\u2022 PCBA: PubChem BioAssay (PCBA) represents a database containing the biological activities of small molecules generated through high-throughput screening. ", "page_idx": 17}, {"type": "text", "text": "Dataset statistics are summarized in Table 5 and Table 6. ", "page_idx": 17}, {"type": "text", "text": "E Details of baselines ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We compare our Pin-Tuning with two types of baseline models for few-shot molecular property prediction tasks, categorized according to the training strategy of molecular encoders: trained-fromscratch methods and pre-trained methods. ", "page_idx": 17}, {"type": "table", "img_path": "859DtlwnAD/tmp/2602637b7b282451a3c8ffd67971107bb0a9f71d2ab6fdc02290fa198134a2f4.jpg", "table_caption": ["Table 6: Statistics of sub-datasets of ToxCast. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Trained-from-scratch methods: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 Siamese [25]: Siamese is used to rank similarity between input molecule pairs with a dual network. ", "page_idx": 18}, {"type": "text", "text": "\u2022 ProtoNet [46]: ProtoNet learns a metric space for few-shot classification, enabling classification by calculating the distances between each query molecule and the prototype representations of each class. ", "page_idx": 18}, {"type": "text", "text": "\u2022 MAML [10]: MAML adapts the meta-learned parameters to achieve good generalization performance on new tasks with a small amount of training data and gradient steps. ", "page_idx": 18}, {"type": "text", "text": "\u2022 TPN [35]: TPN classifies the entire test set at once by learning to propagate labels from labeled instances to unlabeled test instances using a graph construction module that exploits the manifold structure in the data. ", "page_idx": 18}, {"type": "text", "text": "\u2022 EGNN [22]: EGNN predicts edge labels on a graph constructed from input samples to explicitly capture intra-cluster similarity and inter-cluster dissimilarity. ", "page_idx": 18}, {"type": "text", "text": "\u2022 IterRefLSTM [1]: IterRefLSTM adapts Matching Networks [53] to handle molecular property prediction tasks. ", "page_idx": 18}, {"type": "text", "text": "Pre-trained methods: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 Pre-GNN [20]: Pre-GNN is a classic pre-trained molecular model, taking the GIN as backbone and pre-training it with different self-supervised tasks.   \n\u2022 Meta-MGNN [14]: Meta-MGNN leverages Pre-GNN for learning molecular representations and incorporates meta-learning and self-supervised learning techniques.   \n\u2022 PAR [58]: PAR uses class prototypes to update input representations and designs label propagation for similar inputs in the relational graph, thus enabling the transformation of generic molecular embeddings into property-aware spaces.   \n\u2022 GS-Meta [73]: GS-Meta constructs a Molecule-Property relation graph (MPG) and redefines episodes in meta-learning as subgraphs of the MPG. ", "page_idx": 18}, {"type": "text", "text": "Following prior work [58], for the methods we reproduced, we use GIN as the graph-based molecular encoder to extract molecular representations. Specifically, we use the GIN provided by Pre-GNN [20] which consists of 5 GIN layers with 300-dimensional hidden units. Pre-GNN, Meta-MGNN, PAR, and GS-Meta further use the pre-trained GIN which is also provided by Pre-GNN. ", "page_idx": 18}, {"type": "text", "text": "F Implementation details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "F.1 Hardware and software ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our experiments are conducted on Linux servers equipped with an AMD CPU EPYC 7742 (256) $@$ 2.250GHz, 256GB RAM and NVIDIA 3090 GPUs. Our model is implemented in PyTorch version 1.12.1, PyTorch Geometric version 2.3.1 (https://pyg.org/) with CUDA version 11.3, RDKit version 2023.3.3 and Python 3.9.18. Our code is available at: https://github.com/CRIPAC-DIG/ Pin-Tuning. ", "page_idx": 18}, {"type": "text", "text": "F.2 Model configuration ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For featurization of molecules, we use atomic number and chirality tag as original atom features, as well as bond type and bond direction as bond features, which is in line with most molecular pre-training methods. Following previous works, we set $d=300$ . For MLPs in Eq. (2), we use the ReLU activation with $d_{1}=600$ . Pre-trained GIN model provided by Pre-GNN [20] is adopted as the PTME in our framework. We tune the weight of update constraint (i.e., $\\lambda$ ) in {0.01, 0.1, 1, 10}, tune the learning rate of inner loop (i.e., $\\alpha_{\\mathrm{inner}})$ in $\\{1{\\mathrm{e}}{-}3,5{\\mathrm{e}}{-}3,1{\\mathrm{e}}{-}2{,}5{\\mathrm{e}}{-}2{,}1{\\mathrm{e}}{-}1,5{\\mathrm{e}}{-}1,1,5\\}$ , and tune the learning rate of outer loop (i.e., $\\alpha_{\\mathrm{outer}})$ in {1e-5, 1e-4, 1e-3,1e-2,1e-1}. Based on the results of hyperparameter tuning, we adopt $\\alpha_{\\mathrm{inner}}=0.5$ , $\\alpha_{\\mathrm{outer}}=1e\\mathrm{~-~}3$ and $d_{2}=50$ . The ContextEncoder(\u00b7) described in Section 4.2 is implemented using a 2-layer message passing neural network [11]. In each MPNN layer, we employ a linear layer to aggregate messages from the neighborhoods of nodes and utilize distinct edge features to differentiate between various edge types in the context graphs. For baselines, we follow their recommended settings. ", "page_idx": 18}, {"type": "table", "img_path": "859DtlwnAD/tmp/990ad108a203c1f89dadb2cb5e16086b10c64de41676d48cbbcaf199cdbf8bbe.jpg", "table_caption": ["Table 7: 10-shot performance on each sub-dataset of ToxCast. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "859DtlwnAD/tmp/c95e0ab98330bcdae9c9646b161747398f2c23c8c538b9a9f6321cba7f921c6f.jpg", "table_caption": ["Table 8: 5-shot performance on each sub-dataset of ToxCast. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "G More experimental results and discussions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "More discussion of Figure 1. The results show that molecular encoders with more molecule-specific inductive biases, such as CMPNN [47] and Graphormer [66], performed slightly worse than GINMol [20] on this few-shot task. This is because more complex encoders require more parameters to provide inductive biases, which are difficult to train effectively under a few-shot setting. ", "page_idx": 19}, {"type": "text", "text": "More main results. The detailed comparison between Pin-Tuning and baseline models on subdatasets of ToxCast are summarized in Table 7 and Table 8. Our method outperforms all baseline models under both the 10-shot and 5-shot settings, demonstrating the superiority of our method compared to existing methods. ", "page_idx": 19}, {"type": "text", "text": "More discussion of ablation study. Different datasets show varying sensitivity to the removal of components. On small-scale datasets like Tox21 and SIDER, removing components leads to a significant performance drop. On large-scale datasets like ToxCast and PCBA, the impact of removing components is less pronounced. This is because more episodes can be constructed on large-scale datasets, which aids in adaptation. This observation indicates that Pin-Tuning can bring considerable benefits in situations where data is extremely scarce. ", "page_idx": 19}, {"type": "text", "text": "More case studies. We provide more case studies in Figure 8 and 9 as a supplement to Section 5.5. ", "page_idx": 19}, {"type": "text", "text": "Dataset: Tox21, Property: SR-HSE ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "image", "img_path": "859DtlwnAD/tmp/8c689147188fd653f8d3b9e35acd49b80d1cf9dc33a62428fc271b843a18c12f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Dataset: SIDER, Property: ELD ", "page_idx": 20}, {"type": "image", "img_path": "859DtlwnAD/tmp/94f7b8592385c26f93d0dfa9ba4b89431e95aecf96112d32382f8485b03687e6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "859DtlwnAD/tmp/5a92fd6a65dde55d16c85b21962c2d0bd850fd97adffc60601447144de5b2f6d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "859DtlwnAD/tmp/276b22602ebddbc4b0274b44622f2cd3c0af2baf9f400d27720d57fabbdf0919.jpg", "img_caption": ["Figure 8: More molecular representations encoded by GS-Meta [73]. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Dataset: SIDER, Property: PPPC ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "859DtlwnAD/tmp/42b8c2697f05502dbf983976a57ab4a08cb324d39d891cf0b89eab61e1202cc6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Dataset: Tox21, Property: SR-HSE ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Dataset: SIDER,Property:ELD ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "859DtlwnAD/tmp/9b6b75eb69494900fc4f9a8dc5a0eba0f33617ac5b12ab165450d1a8ac23c8a9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Dataset: SIDER, Property: CD ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "859DtlwnAD/tmp/bbf1b1050ae14f8e3eb79e5eb4fd45c4ef85f3b80a9669123bc9684e0ce84782.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "859DtlwnAD/tmp/5ac77021d233e4477fe5f888e683505af30bf6c1f1f2b538f6f350b136dbfaed.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "859DtlwnAD/tmp/730e30abc48d011aecbaa57ad56015c7d970a473d9c2850eb491c7577f19da20.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "859DtlwnAD/tmp/9e8681c9533eb5488034e0b436fbc77a68cfd3be6e155bb0536cdc74a6cd1e78.jpg", "img_caption": ["Figure 9: More molecular representations encoded by Pin-Tuning. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "859DtlwnAD/tmp/9520b7b481bca2d8636f3cc8d13656487ea8845d99aa09b6ae5d5373d918479f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "H Limitations and future directions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "As we discusses in Section 5.2, although our method significantly outperforms the state-of-the-art baseline method, our method exhibits higher standard deviations in the experimental results under multiple runs with different seeds. ", "page_idx": 21}, {"type": "text", "text": "We further speculate that these high standard deviations might be due to the uncertainty in the context information within episodes. The explicitly introduced molecular context, on one hand, provides effective guidance for tuning pre-trained molecular encoders, but on the other hand, this information also carries a high degree of uncertainty. We aim to model the target property through the molecule-property relationships within episodes, but each episode is obtained by sampling very few samples from the large space corresponding to the target property. The uncertainty between different episodes is relatively high. How to quantify and calibrate this uncertainty is another question worth exploring, which we will investigate in our future work. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We present our claims and main contributions in the abstract and introduction. We propose a parameter-efficient in-context tuning method, named Pin-tuning, to effectively adapt pre-trained molecular encoders to few-shot molecular property prediction tasks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We discuss the limitations of our work in Appendix H. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide the derivation of our proposed Emb-BWC regularization, with related assumptions in Appendix A. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The proposed method and experimental setups are clearly described. Additionally, the link of code and other details for reproducing are provided in Appendix F. The datasets are publicly available. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The link of code is provided in the abstract, and other details for environment and configurations are provided in Appendix F. The datasets are publicly available. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Please refer to Section 5.1 and Appendix F. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We report error bars in our experimental results, such as in Table 1. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 24}, {"type": "text", "text": "\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Please refer to Appendix F.1. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our research conforms with the NeurIPS Code of Ethics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Our method addresses the few-shot molecular property prediction problem, which is important for drug discovery and material science, as discussed in the introduction and conclusion. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our research poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We properly respect and credit for existing assets. We cite relevant papers and comply with the licenses. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our code is well documented. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not release new assets. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]