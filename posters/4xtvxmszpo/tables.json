[{"figure_path": "4XTvXMSZPO/tables/tables_7_1.jpg", "caption": "Table 1: Main comparisons of different agents across various settings. Each offline experiment is repeated three times and the mean and standard deviation are reported. Each online experiment is repeated two times. Results are evaluated with our autonomous evaluator with the first 96 instructions in the train and test set. Correlation of our correlation and human judgements can be found in Figure 8.", "description": "This table compares the performance of DigiRL against other state-of-the-art methods for Android device control tasks.  It shows the success rate (percentage) achieved by various approaches, including those based on prompting/retrieval, supervised training, and autonomous RL (DigiRL). The table is divided into different training methods (Prompting, Supervised Training, Offline, and Off-to-On) and evaluation datasets (AitW General, AitW Web Shopping). The results highlight the superior performance of DigiRL over other methods.", "section": "5.1 Main Results"}, {"figure_path": "4XTvXMSZPO/tables/tables_14_1.jpg", "caption": "Table 1: Main comparisons of different agents across various settings. Each offline experiment is repeated three times and the mean and standard deviation are reported. Each online experiment is repeated two times. Results are evaluated with our autonomous evaluator with the first 96 instructions in the train and test set.", "description": "This table compares the performance of DigiRL against several other approaches on two subsets of the Android-in-the-Wild dataset (AitW).  It shows the success rates (train and test) for different agents categorized by their training method (prompting, supervised training, offline RL, and offline-to-online RL).  The agents include those based on proprietary VLMs (GPT-4V, Gemini), imitation learning (CogAgent, AutoUI), and prior autonomous RL (Filtered BC). The table highlights DigiRL's superior performance, significantly surpassing all other methods. Note that the evaluation is based on the first 96 instructions in both training and test datasets. ", "section": "5.1 Main Results"}, {"figure_path": "4XTvXMSZPO/tables/tables_14_2.jpg", "caption": "Table 1: Main comparisons of different agents across various settings. Each offline experiment is repeated three times and the mean and standard deviation are reported. Each online experiment is repeated two times. Results are evaluated with our autonomous evaluator with the first 96 instructions in the train and test set.", "description": "This table compares the performance of DigiRL against several baseline methods on two subsets of the Android-in-the-Wild (AiTW) dataset: General and Web Shopping.  The baselines include methods using prompting and retrieval with proprietary VLMs (GPT-4V, Gemini), supervised training on static demonstrations (CogAgent, AutoUI), and a prior autonomous RL approach (Filtered BC).  The table shows the success rates (train and test) for each method, highlighting DigiRL's significant improvement over the state-of-the-art.", "section": "5.1 Main Results"}, {"figure_path": "4XTvXMSZPO/tables/tables_15_1.jpg", "caption": "Table 4: Average rollout length of the DigiRL agent compared to filtered BC. Darker green means shorter rollout length. On both AitW General and AitW Web Shopping test subsets, we find that DigiRL consistently produces shorter length rollouts than filtered BC.", "description": "This table compares the average rollout length of the DigiRL agent and the Filtered BC agent on two subsets of the Android-in-the-Wild (AitW) dataset: General and Web Shopping.  The rollout length represents the number of steps taken by the agent to complete a task.  Shorter rollout lengths indicate greater efficiency. The table shows that DigiRL consistently achieves shorter rollout lengths than Filtered BC on both subsets.", "section": "5.1 Main Results"}, {"figure_path": "4XTvXMSZPO/tables/tables_22_1.jpg", "caption": "Table 1: Main comparisons of different agents across various settings. Each offline experiment is repeated three times and the mean and standard deviation are reported. Each online experiment is repeated two times. Results are evaluated with our autonomous evaluator with the first 96 instructions in the train and test set.", "description": "This table compares the performance of DigiRL against several baseline methods across two subsets of the Android-in-the-Wild (AiTW) dataset: General and Web Shopping.  The baselines include approaches using prompting and retrieval with proprietary VLMs (GPT-4V and Gemini 1.5 Pro), supervised training methods (CogAgent and AutoUI), and a Filtered Behavior Cloning (Filtered BC) method. The table shows the success rates on both training and test sets for each method, highlighting the significant improvement achieved by DigiRL (Ours) in both offline and offline-to-online settings.  The results demonstrate DigiRL's superior performance in comparison to existing state-of-the-art approaches.", "section": "5.1 Main Results"}, {"figure_path": "4XTvXMSZPO/tables/tables_28_1.jpg", "caption": "Table 1: Main comparisons of different agents across various settings. Each offline experiment is repeated three times and the mean and standard deviation are reported. Each online experiment is repeated two times. Results are evaluated with our autonomous evaluator with the first 96 instructions in the train and test set.", "description": "This table compares the performance of DigiRL against other state-of-the-art methods for Android device control, including methods based on prompting and retrieval (AppAgent + GPT-4V/Gemini 1.5 Pro), supervised training (CogAgent and AutoUI), and filtered behavior cloning (Filtered BC).  The results show that DigiRL significantly outperforms all of these methods in terms of success rate on both the AitW General and AitW Web Shopping subsets.  The table also breaks down the results by training type (offline, offline-to-online), highlighting the effectiveness of DigiRL's autonomous learning approach.", "section": "5.1 Main Results"}]