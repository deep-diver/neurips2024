[{"type": "text", "text": "Disentangling and mitigating the impact of task similarity for continual learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Naoki Hiratani Department of Neuroscience Washington University in St Louis St Louis, MO 63110 hiratani@wustl.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Continual learning of partially similar tasks poses a challenge for artificial neural networks, as task similarity presents both an opportunity for knowledge transfer and a risk of interference and catastrophic forgetting. However, it remains unclear how task similarity in input features and readout patterns infuences knowledge transfer and forgetting, as well as how they interact with common algorithms for continual learning. Here, we develop a linear teacher-student model with latent structure and show analytically that high input feature similarity coupled with low readout similarity is catastrophic for both knowledge transfer and retention. Conversely, the opposite scenario is relatively benign. Our analysis further reveals that taskdependent activity gating improves knowledge retention at the expense of transfer, while task-dependent plasticity gating does not affect either retention or transfer performance at the over-parameterized limit. In contrast, weight regularization based on the Fisher information metric significantly improves retention, regardless of task similarity, without compromising transfer performance. Nevertheless, its diagonal approximation and regularization in the Euclidean space are much less robust against task similarity. We demonstrate consistent results in a permuted MNIST task with latent variables. Overall, this work provides insights into when continual learning is difficult and how to mitigate it. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Artificial neural networks surpass human capabilities in various domains, yet struggle with continual learning. These networks tend to forget previously learned tasks when trained sequentially\u2014-a problem known as catastrophic forgetting [44, 16, 23, 32]. This phenomenon affects not only supervised training of feedforward networks but also extends to recurrent neural networks [10], reinforcement learning tasks [30], and fine-tuning of large language models [41]. Many algorithms for mitigating catastrophic forgetting have been developed previously, including rehearsal techniques [48, 49, 59], weight regularization [30, 36, 67], and activity-gating methods [15, 54, 42, 56], among others [57, 50, 63, 21]. However, these methods often hinder forward and backward knowledge transfer [28, 39, 29], and thus it remains unclear how to achieve knowledge transfer and retention simultaneously. ", "page_idx": 0}, {"type": "text", "text": "A key factor for continual learning is task similarity. If two subsequent tasks are similar, there is a potential for a knowledge transfer from one task to another, but the risk of interference also becomes high [47, 28, 35, 11, 39]. The impact of task similarity on transfer and retention performance is particularly complicated because two tasks can be similar in different manners [65, 32]. Sometimes familiar input features need to be associated with novel output patterns, but at other times, novel input features need to be associated with familiar output patterns. Previous works observed that these two scenarios influence continual learning differently [35], yet the impact of the input and output similarity on knowledge transfer and retention has not been well understood. Moreover, it remains unknown how the task similarity interacts with algorithms for continual learning such as activity gating or weight regularization. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To gain insight into these questions, in this work, we investigate how transfer and retention performance depend on task similarity, task-dependent gating, and weight regularization in analytically tractable teacher-student models. Teacher-student models are simple, typically linear, neural networks in which the generative model of data is specified explicitly by the teacher network [18, 66, 3]. These models have provided tremendous insights into generalization property [55, 45, 19, 1], convergence rate [61, 58, 38], and learning dynamics [51, 52, 4, 25] of neural networks, due to their analytical tractability. Several works also studied continual learning using teacher-student settings [2, 35, 27, 11, 20, 37, 13] (see Related works section for details). ", "page_idx": 1}, {"type": "text", "text": "We develop a linear teacher-student model with a low-dimensional latent structure and analyze how the similarity of input features and readout patterns between tasks affect continual learning. We show analytically that a combination of low feature similarity and high readout similarity is relatively benign for continual learning, as the retention performance remains high and the transfer performance remains non-negative. However, the opposite, a combination of high feature similarity and low readout similarity is harmful. In this regime, both transfer and retention performance become below the chance level even when the two subsequent tasks are positively correlated. Furthermore, transfer performance depends on the feature similarity non-monotonically, such that, beyond a critical point, the higher the feature similarity is, the lower the transfer performance becomes. ", "page_idx": 1}, {"type": "text", "text": "We further analyze how common algorithms for continual learning, activity and plasticity gating [15, 42, 46], activity sparsification [57], and weight regularization [30, 36, 67], interact with task similarity in our problem setting, deriving several non-trivial conclusions. Activity gating improves retention at the cost of transfer when the gating highly sparsifies the activity, but helps both transfer and retention on average if the activity is kept relatively dense. Plasticity gating and activity sparsification, by contrast, do not influence either transfer or retention performance at the over-parameterized limit. Lastly, weight regularization in the Fisher information metric helps retention without affecting knowledge transfer and achieves perfect retention regardless of task similarity in the presence of low-dimensional latent. However, its diagonal approximation and the regularization in the Euclidean metric are much less robust against both task similarity and regularizer amplitude. ", "page_idx": 1}, {"type": "text", "text": "Furthermore, we test our key predictions numerically in a permuted MNIST task with a latent structure. When the input pixels are permuted from one task to the next, the retention performance remains high. However, when the mapping from the latent to the target output is changed, both the retention and transfer performance go below the chance level, as predicted. Random task-dependent gating of input and hidden layer activity improves retention at the cost of knowledge transfer, but adaptive gating mitigates this tradeoff. We also show that in a fully-connected feedforward network, there exists an efficient layer-wise approximation of the weight regularization in the Fisher information metric, which outperforms its diagonal approximation and the regularization in the Euclidean metric. Nevertheless, the performance of the diagonal approximation is much closer to the layer-wise approximation of the Fisher information metric than to the Euclidean weight regularization. ", "page_idx": 1}, {"type": "text", "text": "Our theory thus reveals when continual learning is difficult, and how different algorithms mitigate these challenging situations, providing a basic framework for analyzing continual learning in artificial and biological neural networks. ", "page_idx": 1}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Previous works on continual learning in linear teacher-student models found that forgetting is most prominent at the intermediate task similarity [11, 40, 13] as observed empirically [47]. However, these works did not address the tradeoff between forgetting and knowledge transfer, and these simple settings did not disentangle the effect of the similarity in input feature and readout pattern. Forward and backward transfer performance in continual learning were also analyzed in linear and deep-linear networks [33, 8], yet its relationship with catastrophic forgetting has not been well characterized. Lee et al. [35] analyzed dynamics of both forgetting and forward transfer in one-hidden layer nonlinear network under a multi-head continual learning setting. However, their analysis of readout similarity and its comparison to feature similarity were conducted numerically and it did not address the effect of common heuristics, such as gating or weight regularization, either. ", "page_idx": 1}, {"type": "image", "img_path": "bE7GWLQzkM/tmp/8a92c9361ab055cf6db5bba4dcac84233bdde1dae3f011762ce80421b04a1db6.jpg", "img_caption": ["Figure 1: (A) Schematic representation of the continual linear regression model with low-dimensional latent variables. (B-D) Examples of continual learning with two tasks in the MNIST setting. The tasks have low feature similarity in panel C (input pixels are partially permuted) and low readout similarity in panel D (output labels are partially permuted). Panels C and D correspond to the green and orange points in panel B, respectively. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "By comparison, we introduce a low-dimensional latent structure into a linear teacher-student model, which enables us to decouple the influence of feature and readout similarity on knowledge transfer and retention. Moreover, this low-dimensionality assumption on the latent enables us to evaluate the transfer and retention performance analytically even in the presence of gating or weight regularization in the Fisher information metric. ", "page_idx": 2}, {"type": "text", "text": "Continual learning has been studied from many theoretical frameworks beyond teacher-student modeling, including neural tangent kernel [5, 9, 27], PAC learning [6, 60], and computational complexity [31]. Learning of low-dimensional latent representation has also been studied in the context of multi-task learning [43, 62]. In addition, several works investigated the effect of weight regularization on continual learning in analytically tractable settings [30, 12, 24]. In particular, Evron et al. [12] investigated effect of weight-regularization in Fisher-information metric in continual linear regressionscenario. ", "page_idx": 2}, {"type": "text", "text": "3   Teacher-student model with low-dimensional latent variables ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let us consider task-incremental continual learning of regression tasks. For analytical tractability, we consider a teacher-student setting where the target outputs are generated by the teacher network (Fig. 1A). We define the student network, which learns the task, as a linear projection from a potentially nonlinear transformation of the input, ${\\pmb y}\\,=\\,W\\psi({\\pmb x})$ , where $\\textbf{\\em x}\\in\\mathbb{R}^{\\hat{N}_{x}}$ is the input, $\\pmb{y}^{\\prime}\\in\\mathbb{R}^{N_{y}}$ is the output, $W\\,\\in\\,\\mathbb{R}^{N_{y}\\times N_{x}}$ is the trainable weight matrix, and $\\psi(\\pmb{x})\\,:\\,\\mathbb{R}^{N_{x}}\\,\\rightarrow^{^{\\bullet}}\\mathbb{R}^{N_{x}^{\\bullet}}$ is an input transformation. We use $\\psi({\\pmb x})={\\pmb x}$ for the vanilla and weight regularization models, ${\\psi}(\\mathbf{{x}})=\\mathbf{{{g}}}\\odot\\mathbf{{x}}$ for the task-dependent gating model, and $\\psi(\\pmb{x})=s g n(\\pmb{x})\\odot\\operatorname*{max}\\{0,|\\pmb{x}|-\\pmb{h}\\}$ for the soft-thresholding model, where $\\textbf{\\textit{g}}$ and $^h$ are gating and thresholding vectors, respectively. Here, $\\textstyle g\\odot x$ is an elementwise multiplication, and $s g n(x)$ is a function that returns the sign of the input. Throughout the paper, we use bold-italic letters for vectors, capital-italic letters for matrices. ", "page_idx": 2}, {"type": "text", "text": "We generate the input $\\textbf{\\em x}$ and target output $\\boldsymbol{y}^{*}$ of the $\\mu$ -th task using a latent variable $s\\in\\mathbb{R}^{N_{s}}$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{s}\\leftarrow\\pmb{\\mathcal{N}}\\left(0,I_{s}\\right),\\quad\\pmb{x}=A_{\\mu}\\pmb{s},\\quad\\pmb{y}^{*}=B_{\\mu}\\pmb{s},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for $\\mu=1,2$ ,where $A_{\\mu}\\in\\mathbb{R}^{N_{x}\\times N_{s}}$ and $B_{\\mu}\\in\\mathbb{R}^{N_{y}\\times N_{s}}$ are mixing matrices hidden from the student network, and $I_{s}$ is the size $N_{s}$ identity matrix. We introduce this latent structure, $\\pmb{s}$ , to decouple the effect of feature and readout similarity on the transfer and retention performance. Below, we set the latent space to be low-dimensional compared to the input space (i.e., $N_{s}\\ll N_{x},$ ). This is motivated by the presence of low-dimensional latent structure in many machine learning datasets [64, 7] and the tasks used in neuroscience experiments [17, 26], but also aids analytical tractability. ", "page_idx": 2}, {"type": "text", "text": "We generate the mixing matrices for the first task, $A_{1}$ and $B_{1}$ , by sampling elements independently from a Gaussian distribution with mean zero and variance $\\frac{1}{N_{s}}$ . The subsequent task matries $A_{2}$ and $B_{2}$ are also generated randomly, but with element-wise correlation with the previous matrices $A_{1}$ and $B_{1}$ (see Appendix A for the details). We denote the element-wise correlation between $A_{1}$ and ", "page_idx": 2}, {"type": "image", "img_path": "bE7GWLQzkM/tmp/c53e9f414a33ff71c54c24c2e42197903b9b3dca87647219771868ace64fa5cf.jpg", "img_caption": ["Figure 2: Transfer and retention performance of the vanilla model. (A) Illustration of $\\Delta\\epsilon_{T F}$ and $\\Delta\\epsilon_{R T}$ . Red and blue lines represent the error on task 1 and task 2, respectively. Here, the model was trained on task 1 for 100 iterations and then trained on task 2 for another 100 iterations. $(\\mathbf{{B}},\\mathbf{{C}})$ Transfer performance under various task similarity. Points in panel B are numerical results (the means and the standard deviations over ten random seeds), while solid lines are analytical results (Eq. 4). (MD-G) Retention performance under various task similarity. Panel $\\mathbf{G}$ magnifies the $0.9\\leq\\rho_{b}\\leq1.0$ region of panel $\\mathbf{E}$ , and the white dashed line in panel $\\mathbf{G}$ represents local minima/maxima. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "$A_{2}$ by $\\rho_{a}$ and refer it as feature correlation, because $A_{1}$ and $A_{2}$ specify the input features. Similarly, we denote the correlation between $B_{1}$ and $B_{2}$ , the readout correlation, by $\\rho_{b}$ . Figs. 1B-D illustrate the difference between $\\rho_{a}$ and $\\rho_{b}$ in an image classification setting. When $\\rho_{a}<1.0$ and $\\rho_{b}=1.0$ input features are partially modified in the second task compared to the first task (Green point on Fig. 1B and Fig. 1C). For example, permuted MNIST task [22] corresponds to this low feature similarity setting with $\\rho_{a}=0.0$ and $\\rho_{b}=1.0$ . By contrast, when $\\rho_{a}=1$ and $\\rho_{b}<1$ , the readouts are changed partially (Orange point on Fig. 1B and Fig. 1D). When $\\rho_{a}=1$ and $\\rho_{b}=-1$ , the same input features are associated with the opposite readout in the new task, which is known as the reversal learning paradigm. In this reversal scenario, the network fails to achieve knowledge transfer rather trivially. Thus, we focus on the scenario when tasks have non-negative correlation in terms of both input features and readout (i.e., $0\\leq\\rho_{a},\\rho_{b}\\leq1)$ below. ", "page_idx": 3}, {"type": "text", "text": "We measure the performance of the student network with weight $W$ on the $\\mu$ -th task by mean-squared error $\\begin{array}{r}{\\epsilon_{\\mu}[W]\\equiv\\frac{1}{N_{y}}\\left\\langle\\|B_{\\mu}\\pmb{s}-W\\psi(A_{\\mu}\\pmb{s})\\|^{2}\\right\\rangle_{\\pmb{\\mathscr{s}}}}\\end{array}$ , where $\\langle\\cdot\\rangle_{s}$ is the expectation over latent variable $\\pmb{s}$ The transfer and retention performance are defined by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta\\epsilon_{T F}\\equiv\\langle\\epsilon_{2}[W_{o}]-\\epsilon_{2}[W_{1}]\\rangle_{A,B}\\,,\\quad\\Delta\\epsilon_{R T}\\equiv\\langle\\epsilon_{1}[W_{o}]-\\epsilon_{1}[W_{2}]\\rangle_{A,B}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\langle\\cdot\\rangle_{A,B}$ is the expectation over randomly generated task matrices $A_{1},A_{2},B_{1}$ and $B_{2}$ under a given feature and readout correlation $\\rho_{a},\\rho_{b}$ . As illustrated in Fig. 2A, the transfer performance $\\Delta\\epsilon_{T F}$ measures how much performance the model achieves on task 2 by learning task 1, whereas the retention performance $\\Delta\\epsilon_{R T}$ measures how well the model can perform task 1 after learning task 2 (here, the task switch occurs at the $100\\mathrm{th}$ iteration). Below, we study how knowledge transfer and retention, the two key objectives of continual learning, depend on task similarity, and how to optimize the performance through gating and weight regularization. ", "page_idx": 3}, {"type": "text", "text": "4  Impact of task similarity on knowledge transfer and retention ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let us first investigate the vanilla model without gating or weight regularization, to examine how task similarity affects knowledge transfer and retention. Given ${\\boldsymbol{\\psi}}(\\mathbf{\\boldsymbol{x}})=\\mathbf{\\boldsymbol{x}}$ , at the infinite sample limit, learning by gradient descent follows $\\dot{W}=-(W A_{\\mu}-B_{\\mu})A_{\\mu}^{T}$ . The fixed point of this dynamics, as detailed in Appendix B.1, is: ", "page_idx": 3}, {"type": "equation", "text": "$$\nW_{\\mu}=W_{\\mu-1}(I-U_{\\mu}U_{\\mu}^{T})+B_{\\mu}A_{\\mu}^{+},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $W_{\\mu-1}$ is the weight after learning of the previous task, $U_{\\mu}$ is the semi-orthogonal matrix from singular value decomposition (SVD) of $A_{\\mu}=U_{\\mu}\\Lambda_{\\mu}V_{\\mu}^{T}$ , and $A_{\\mu}^{+}$ is the pseudo-inverse of matrix $A_{\\mu}$ ", "page_idx": 3}, {"type": "text", "text": "Inserting Eq. 3 into Eq. 2 and taking the expectation over randomly generated tasks $A_{1},B_{1},A_{2},B_{2}$ the transfer and retention performance are written as (see Appendix B.1) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta\\epsilon_{T F}=\\rho_{a}(2\\rho_{b}-\\rho_{a}),\\quad\\Delta\\epsilon_{R T}=1-\\rho_{a}^{2}(\\rho_{a}^{2}-2\\rho_{a}\\rho_{b}+1).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Recall that $\\rho_{a}$ is the feature similarity defined by the correlation between $A_{1}$ and $A_{2}$ . while $\\rho_{b}$ is the readout similarity, representing the correlation between $B_{1}$ and $B_{2}$ . The derivation of the above equations relies on (correlated) random generation of $A_{1},B_{1},A_{2},B_{2}$ and the low-rank latent assumption: $N_{s}\\,\\ll\\,N_{x}$ . These two equations, despite their simplicity, capture the transfer and retention performance in numerical simulations well (Figs. 2B, D, and $\\boldsymbol{\\mathrm{F}}$ ; see Appendix E.1 for the details of numerical estimation). Furthermore, they reveal asymmetric and non-monotonic impact of the feature and readout similarity on the performance. ", "page_idx": 4}, {"type": "text", "text": "Fig. 2B depicts the knowledge transfer from task 1 to task 2, $\\Delta\\epsilon_{T F}$ , under various $(\\rho_{a},\\rho_{b})$ conditions. As expected, $\\Delta\\epsilon_{T F}=0$ when the two tasks are independent (i.e., $(\\rho_{a},\\rho_{b})=(0,0))$ , and $\\Delta\\epsilon_{T F}=1$ when the tasks are identical (i.e., $(\\rho_{a},\\rho_{b})=(1,1),$ . At intermediate levels of similarity, there is clear asymmetry in the infuence of feature and readout similarities on transfer performance; particularly, a combination of high feature similarity and low readout similarity leads to negative transfer, while the opposite scenario results in a modest positive transfer (lower-right vs upper-left of Fig. 2B). ", "page_idx": 4}, {"type": "text", "text": "Notably, under a fixed readout similarity $\\rho_{b}$ , the knowledge transfer depends non-monotonically on the feature similarity $\\rho_{a}$ .When $\\rho_{a}<\\rho_{b}$ , the higher the feature similarity the better transfer becomes, as implied from Eq. 4. However, once the feature similarity exceeds the readout similarity, counter-intuitively, high feature similarity worsens the transfer performance (Figs. 2B and C). This is because, when the feature similarity is high, inputs are aligned with learned features, resulting in a large output regardless of readout similarity. Particularly, under a low readout similarity, performance becomes below zero because extracted features are mostly projected to the incorrect directions. ", "page_idx": 4}, {"type": "text", "text": "The retention performance also exhibits asymmetric dependence on feature and readout similarity. When feature similarity $\\rho_{a}$ is low, the network barely forgets the previous task regardless of readout similarity (Fig. 2D left). By contrast, when the feature similarity is high, the retention performance can either be positive or negative depending on the readout similarity. Moreover, in the high readout similarity regime, the retention performance is the lowest at an intermediate feature similarity (Figs. 2E and F). This is because high retention is possible when either interference is low, or similarity between two tasks is high. Note that, this last point on the non-monotonic dependence on feature similarity under $\\rho_{b}=1$ has been investigated both empirically [47] and analytically [35, 11, 13]. Indeed, at $\\rho_{b}=1$ $\\Delta\\epsilon_{R T}$ in Eq. 4 coincides with Eq. (5) in [13]. ", "page_idx": 4}, {"type": "text", "text": "Therefore, the knowledge transfer and retention performance depend on feature and readout similarities in an asymmetric and non-monotonic manner. Specifically, a combination of high feature similarity and low readout similarity is detrimental to continual learning, resulting in negative transfer and retention performance, even in the presence of a positive correlation between the two tasks in both input features and readout patterns. Moreover, under a fixed readout similarity, the knowledge transfer performance depends on the feature similarity in a non-monotonic manner. Thus, the continual learning in the vanilla model is sensitive to task similarity and limited in performance. These results make us wonder whether we can mitigate the sensitivity to task similarity and improve the overall transfer and retention performance by modifying the learning algorithm. To this end, we next investigate how task-dependent gating and weight regularization methods, two popular strategies in continual learning, alleviate knowledge transfer and retention. ", "page_idx": 4}, {"type": "text", "text": "5   Task-dependent gating ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "5.1  Random activity gating ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "One popular method for mitigating forgetting in continual learning is activity gating [15, 54, 42, 21, 56]. With gating of input activity, the network is written as ${\\pmb y}=W({\\pmb g}_{\\mu}\\odot{\\pmb x})$ ,where $\\bar{\\pmb{g}}_{\\mu}\\in\\{0,1\\}^{N_{x}}$ is a binary gating vector for task $\\mu$ We first consider a random gating scenario where elements of ${\\mathbfit{g}}_{\\mu}$ are randomly sampled from a Bernoulli distribution with rate $\\alpha$ which we denote as the gating level. All units are active at $\\alpha=1$ , while no units are active at $\\alpha\\rightarrow0$ limit. ", "page_idx": 4}, {"type": "image", "img_path": "bE7GWLQzkM/tmp/3ed5de7615b48ff168120a6afaa00ee0ddc57f9f73489857a3685e3f34526a64.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: Random task-dependent activity gating model. (A) Knowledge transfer performance under $\\rho_{a}=1.0$ . The gating level $\\alpha$ is defined as the fraction of active input neurons (i.e., $\\alpha=\\operatorname*{Pr}[g_{i}=1]$ $\\bf{(B)}$ The transfer performance under the optimal gating level $\\begin{array}{r}{\\alpha^{*}\\,=\\,\\operatorname*{min}\\{\\frac{\\rho_{b}}{\\rho_{a}},1\\}}\\end{array}$ . (C) Retention performance under $\\rho_{a}=1.0$ $\\mathbf{\\eta}(\\mathbf{D})$ Average transfer and retention performance over uniform prior on $0\\leq\\rho_{a},\\rho_{b}\\leq1$ . Horizontal dashed lines are the performance of the vanilla model, while solid lines are the performance of the random gating model. Points are numerical estimations. ", "page_idx": 5}, {"type": "image", "img_path": "bE7GWLQzkM/tmp/3f45f72f5befc76d39a3da5cf05fed2b094dc8b126407b6c32043bcfdb3f9292.jpg", "img_caption": ["Figure 4: Transfer and retention performance of the adaptive activity gating $(\\mathbf{A},\\mathbf{B})$ ,random plasticity gating (C), and input soft-thresholding $({\\bf D})$ models. Dashed and solid lines in panels $\\mathbf{A}$ andB represent the performance of the random and adaptive activity gating models, respectively. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "From a parallel argument with the vanilla model, when $N_{s}\\,\\ll\\,\\alpha N_{x}$ , the transfer and retention performance are estimated as (see Appendix B.1): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\epsilon_{T F}=\\alpha\\rho_{a}(2\\rho_{b}-\\alpha\\rho_{a}),}\\\\ &{\\Delta\\epsilon_{R T}=1-\\alpha^{2}\\rho_{a}^{2}(\\alpha^{2}\\rho_{a}^{2}-2\\alpha\\rho_{a}\\rho_{b}+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thus, random gating scales the feature similarity from $\\rho_{a}$ to $\\alpha\\rho_{a}$ . This scaling lowers the knowledge transfer if $\\rho_{b}\\ \\geq\\ \\rho_{a}$ because random gating reduces the fraction of input neurons active in two subsequent tasks (lime line in Fig. 3A). However, if $\\rho_{b}<\\rho_{a}$ , gating with $\\begin{array}{r}{\\alpha\\geq\\,\\frac{\\rho_{b}}{\\rho_{a}}}\\end{array}$ enhances the transfer by reducing the effective feature similarity (blue lines in Fig. 3A; also compare Fig. 3B with Fig. 2C). The optimal gating level $\\alpha^{*}$ for knowledge transfer is $\\begin{array}{r}{\\operatorname*{min}\\{\\frac{\\rho_{b}}{\\rho_{a}},1\\}}\\end{array}$ , indicating that the input activity typically needs to be dense. By contrast, the retention performance is optimized at $\\alpha\\rightarrow0$ limit where tasks do not interfere with each others (Fig. 3C). At this limit, we have $\\Delta\\epsilon_{R T}\\rightarrow1$ Note that i reality,. $\\alpha$ has to be non-zero to optimize the retention Eq. 5b holds only when $\\begin{array}{r}{\\alpha\\gg\\frac{N_{s}}{N_{x}},}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "These results indicate that random activity gating improves the retention at the cost of forward knowledge transfer, as suggested previously [28, 39, 29]. This tradeoff between knowledge transfer and retention is especially critical when the network doesn't know the task similarities. Suppose that the task similarity is distributed uniformly over $0\\leq\\rho_{a},\\rho_{b}\\leq1$ . Then, the average transfer performance is maximized at $\\begin{array}{r}{\\alpha=\\frac{3}{4}}\\end{array}$ , whilethe average retention peformance decreases monotonically as a function of $\\alpha$ (Fig. 3D). Thus, a moderate gating $\\begin{array}{r}{(\\alpha\\approx\\frac{3}{4})}\\end{array}$ benefits both transfer and retention on average. However, retention performance in this regime is significantly below one, implying that the network cannot reliably achieve high retention performance without sacrificing the transfer performance. ", "page_idx": 5}, {"type": "text", "text": "5.2  Adaptive activity gating ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "One way to overcome the tradeoff between transfer and retention performance is to consider adaptive gating. Let us introduce a probe trial at the beginning of the task 2, in which the model tests how well it performs if it uses the gating for task 1, $\\pmb{g}_{1}$ , for task 2 as well. If the error in the probe trial is small, the model should keep using the same gating vectors to achieve a good knowledge transfer. Otherwise, it should resample the gating vector for retaining the knowledge on task 1. Using the probeerror $\\epsilon_{p b}$ , we set the probability of using the same gating as $\\rho_{g}=1-\\epsilon_{p b}/\\epsilon_{o}$ .This adaptive gating indeed improves the average transfer performance compared to the random gating (solid vs dashed lines in Fig. 4A) with only a relatively small reduction in the retention performance (Fig. 4B). ", "page_idx": 5}, {"type": "image", "img_path": "bE7GWLQzkM/tmp/463fb397aec2f109c2fe7b4c0436db96933268cc1a8f6c2da230beac00ddd592.jpg", "img_caption": ["Figure 5: Performance of weight regularization in Euclidean metric. (A,B) Transfer (A) and retention $({\\bf{B}})$ performance. The amplitude of the weight regularization scales with $\\textstyle{\\frac{1}{\\gamma}}-1$ (C) Regularizer coefficient $\\gamma$ that optimizes the retention performance. $({\\bf D})$ Average performance over uniform task similarity distribution in $0\\leq\\rho_{a},\\rho_{b}\\leq1$ . Horizontal dashed lines are the performance of the vanilla model. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5.3  Plasticity gating and input soft-thresholding ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Previous works have also explored other gating mechanisms, such as plasticity gating [46] and activity sparsification [57]. However, their benefits over the activity gating discussed above have not been fully understood. We implement task-dependent plasticity gating into our problem setting by making only the synaptic weights projected from a subset of input neurons plastic. Unexpectedly, we found that both transfer and retention performance are independent of the fraction of plastic synapses in the $N_{s}\\ll N_{x}$ limit, potentially due to over-parameterization (Fig. 4C; see Appendix B.2 for details). ", "page_idx": 6}, {"type": "text", "text": "Similarly, when the input activity is sparsified using soft-thresholding $\\varphi(x)=s g n(x)\\operatorname*{max}\\{0,|x|-$ $h\\}$ , with a fixed threshold $h={\\sqrt{2}}\\mathrm{erfc}^{-1}(\\alpha).$ the transfer and retention performance remain independent of the resultant input sparsity $\\alpha$ (Fig. 4D; see Appendix B.3 for the details). These results imply that plasticity gating and input sparsification are not effective in our model, which operates in an over-parameterized regime (i.e., $N_{s}\\ll N_{x},$ ), but do not exclude their potential benefits for many continual learning tasks. ", "page_idx": 6}, {"type": "text", "text": "6 Weight regularization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "6.1   Weight regularization in Euclidean metric ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Another popular method is weight regularization, which keeps the synaptic weights close to those learned from previous tasks [30, 36, 67]. Let us first consider regularization of the Euclidean distance between the current weight $W$ and the weight learned in the previous task $W_{\\mu-1}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ell_{\\mu}=\\frac{1}{2}\\left\\|B_{\\mu}-W A_{\\mu}\\right\\|_{F}^{2}+\\frac{N_{x}}{2N_{s}}(\\frac{1}{\\gamma}-1)\\left\\|W-W_{\\mu-1}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, we parameterize the regularizer amplitude by $\\frac{N_{x}}{N_{s}}\\Big(\\frac{1}{\\gamma}\\mathrm{~-~}1\\big)$ using a non-negative parameter $\\gamma$ for brevity. $\\gamma=1$ corresponds to zero regularization, while $\\gamma=0$ is the infinite regularization limit. Under this parameterization, if $N_{s}\\ll N_{x}$ , the transfer and retention performance follow simple expressions as below (see Appendix C.1): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\epsilon_{T F}=\\gamma\\rho_{a}(2\\rho_{b}-\\gamma\\rho_{a}),}\\\\ &{\\Delta\\epsilon_{R T}=1-\\gamma^{2}\\rho_{a}^{2}(1-2\\gamma\\rho_{a}\\rho_{b}+\\gamma^{2}\\rho_{a}^{2})+2\\gamma\\rho_{a}(1-\\gamma)(\\rho_{b}-\\gamma\\rho_{a})-(1-\\gamma)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The expression of the transfer performance is equivalent to Eq. 5a if we change $\\gamma$ to $\\alpha$ . Thus, the Euclidean weight regularization with amplitude $\\frac{\\bar{N_{x}}}{N_{s}}\\Big(\\frac{1}{\\gamma}-1\\Big)$ is mathematically equivalent with random activity gating with sparsity $\\gamma$ , in terms of knowledge transfer (compare Fig. 5A with Fig. 3A). By contrast, the expression of the retention performance contains two additional terms compared to 5b. In particular, the last term, $(1-\\gamma)^{2}$ , indicates that strong weight regularization not only prevent forgetting, but also impairs task acquisition. Thus, the retention performance is optimized at an intermediate regularizer strength (Figs. 5B and C). ", "page_idx": 6}, {"type": "image", "img_path": "bE7GWLQzkM/tmp/b73f83670891fec32a85ea14d878108767a827db7a0a941ab5d08960c645c255.jpg", "img_caption": ["Figure 6: Weight regularization in the Fisher information metric. (A,B) The retention performance under various task similarities and regularizer coefficients. (C,D) Average transfer and retention performance under the regularization with the exact Fisher information metric (C) and its diagonal approximation $({\\bf D})$ "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Because strong regularization (i.e., small $\\gamma$ ) impairs both retention and transfer, unlike the random activity gating model, there isn't a tradeoff between knowledge transfer and retention in terms of the average performance over $0\\leq\\rho_{a},\\rho_{b}\\leq1$ (Fig. 5D). However, the retention performance at the optimal parametric regime is relatively low (Fig. 5D with Fig. 4B). ", "page_idx": 7}, {"type": "text", "text": "6.2  Weight regularization in Fisher information metric ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Previous works proposed that the synaptic weights should be regularized in the Fisher information metric to allow fexibility in the non-overlapping weight space [30, 67]. Applying it to our model setting, the regularizer term in Eq. 6 instead becomes ${\\textstyle\\frac{1}{2}}(\\frac{1}{\\gamma}\\!-\\!1)\\left\\|(W-W_{\\mu-1})A_{\\mu-1}\\right\\|_{F}^{2}$ (see Appendix C.2). If $\\rho_{a},\\gamma<1$ and $N_{s}\\ll N_{x}$ , optimization of the weight under this regularization yields ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W_{\\mu}=W_{\\mu-1}\\left(I-\\frac{N_{s}}{N_{x}(1-\\rho_{a}^{2})}A_{\\mu}\\left[A_{\\mu}^{T}-\\rho_{a}A_{\\mu-1}^{T}\\right]\\right)+\\frac{N_{s}}{N_{x}(1-\\rho_{a}^{2})}B_{\\mu}\\left(A_{\\mu}^{T}-\\rho_{a}A_{\\mu-1}^{T}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $W_{\\mu-1}$ is the weight after the previous task learning. Notably, the weight becomes independent of the regularizer's amplitude $\\gamma$ . Furthermore, the retention performance under arbitrary task similarity $(\\rho_{a},\\rho_{b})$ is derived as $\\begin{array}{r}{\\Delta\\epsilon_{R T}=1-\\mathcal{O}\\big(\\frac{N_{s}}{N_{x}(1-\\rho_{a}^{2})}\\big)}\\end{array}$ Thus, u tFisheinfmatnmic t retention performance is perfect as long as the condition $N_{s}\\ll N_{x}(1-\\rho_{a}^{2})$ is satisfied (Figs. 6A-C). Intuitively, assuming over-parameterization $(N_{s}\\ll N_{x})$ 0, the network can freeze the weight changes in a low-dimensional subspace, while maintaining sufficient plasticity for new tasks, unless the two tasks share the same feature. Notably, if the first task is learned with weight regularization in an orthogonal direction, the transfer performance remains the same with the vanilla model (Fig. 6C). ", "page_idx": 7}, {"type": "text", "text": "Importantly, this invariance no longer holds when the Fisher information metric is approximated by its diagonal component (Figs. 6D vs 6C), as is done in the elastic weight methods [30]. This is because the diagonal approximation makes the metric full-rank even when the true metric has a low-rank structure. Thus, weight regularization in the Fisher information metric robustly helps retention without harming transfer, but diagonal approximation attenuates its robustness. ", "page_idx": 7}, {"type": "text", "text": "7  Numerical experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our theory so far has revealed when continual learning is difficult and when activity gating and weight regularization can ameliorate the transfer and retention in a simple problem setting. Let us next examine how much these insights are applicable to more realistic datasets and neural architectures. ", "page_idx": 7}, {"type": "text", "text": "To this end, we consider the permuted MNIST dataset [34, 22], but with a latent structure (see Appendix E.2). For each output label, we constructed a four-dimensional latent variable $\\pmb{s}$ using the binary representation of the corresponding digit (e.g., $\\pmb{s}_{9}=[1,0,0,1]^{T})$ . The target output was then generated by a random fixed projection of the latent variable: $\\begin{array}{r}{\\pmb{y}^{*}=\\dot{B}_{\\mu}(\\pmb{s}-\\frac{1}{2})}\\end{array}$ . Note that, although here we introduced an explicit latent structure for a direct comparison with the theory, qualitatively similar results holds in the vanilla permuted MNIST setting depicted in Figs. 1B-D (see Fig. 12). We controlled the readout similarity between tasks using the element-wise correlation between two matrices $B_{1}$ and $B_{2}$ . The feature similarity was controlled by permuting a subset of input pixels, as in the previous permuted MNIST tasks. We used a one-hidden-layer fully-connected network with rectified-linear nonlinearity at the hidden layer, and trained the network using stochastic gradient descent. ", "page_idx": 7}, {"type": "image", "img_path": "bE7GWLQzkM/tmp/df5d5a0632592b6e1e38bb48cb32f4d44c73982339485ce05c78844cf2f2abe1.jpg", "img_caption": ["Figure 7: Permuted MNIST with latent variables. (A,B) Transfer and retention performance of the vanilla model. (C,D) Performance of random (dashed lines) and adaptive (solid lines) activity gating models. (E-H) Performance of the weight regularization in the Euclidean metric, and approximated Fisher information metrics (red: layer-wise approximation; orange: synapse-wise/diagonal approximation). Here, lines are linear interpolations and error bars are standard errors over random seeds, not standard deviations. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "As predicted from our theory, both transfer and retention performance, measured by the test error, exhibited asymmetric and non-monotonic dependence on the task similarity. When $\\left(\\rho_{a},\\rho_{b}\\right)\\mathrm{~=~}$ (1.0, 0.0), both transfer and retention performance were below zero (lower-right of Figs. 7A and B vs. Figs. 2C and E). By contrast, the model achieved high retention and zero transfer under $(\\rho_{a},\\rho_{b})=$ (0.0, 1.0) as expected (upper-left). Notably, around $\\rho_{b}=0.5$ , the transfer performance exhibited a non-monotonic dependence on $\\rho_{a}$ in accordance with the theory. The retention performance also exhibited non-monotonic dependence on $\\rho_{a}$ under $\\rho_{b}\\approx1$ , but the dependence became monotonic after a long training (see Figs. 9C and D in Appendix). We observe consistent results even when feature and readout similarity are adjusted by permuting input pixels and output labels, respectively, provided the loss is measured by cross-entropy (Fig. 12B, corresponding to the examples depicted in Figs. 1C and 1D). ", "page_idx": 8}, {"type": "text", "text": "Random activity gating, implemented in both input and hidden layers, improved the retention performance at the cost of low knowledge transfer as predicted (compare Fig. 7D and Fig. 4B). Moreover, adaptive gating based on a probe trial improved the transfer performance under a low gating level, especially when the readout similarity is high (solid vs dashed lines in Figs. 7C and D). ", "page_idx": 8}, {"type": "text", "text": "In a deep neural network, weight regularization using the Fisher information metric is typically computationally expensive. However, an efficient layer-wise approximation exists for fully-connected networks (see Appendix E.2). This layer-wise approximation enabled high retention performance across a wide range of regularization amplitudes and consistently outperformed weight regularization in the Euclidean metric under various task similarities (red vs. gray lines in Figs. 7F-H). Nevertheless, the retention performance under the diagonal approximation was closer to that of the layer-wise approximation of the Fisher information metric and was slightly better when both methods performed poorly (red vs. orange lines). This is because the sparsity of hidden layer weights and activity makes the diagonal components sparse. ", "page_idx": 8}, {"type": "text", "text": "8 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Here, we analyzed the impact of task similarity on continual learning in a linear teacher-student model with low-dimensional latent structures. We showed that, a combination of high feature similarity and low readout similarity lead to poor outcomes in both retention and transfer, unlike the opposite combination. We further explored how continual learning algorithms such as gating mechanisms, activity sparsification, and weight regularization interact with task similarity. Results indicate that weight regularization in the Fisher information metric significantly aids retention regardless of task similarity. Numerical experiments in a permuted MNIST task with latent supported these findings. ", "page_idx": 8}, {"type": "text", "text": "Our findings on the interaction between task similarity and continual learning algorithms have several implications. Firstly, when tasks are known to have high feature similarity and low readout similarity, adaptive activity gating and weight regularization in the Fisher information metric help retention without sacrificing transfer (Figs. 4, 6, 7). In the context of neuroscience, our results indicate that simple non-adaptive activity or plasticity gating mechanisms are not sufficient for good continual learning performance, especially when familiar sensory stimuli need to be associated with novel motor actions. Indeed, a previous study reported catastrophic forgetting among rats learning timing estimation tasks [14]. Lastly, it also implies the potential importance of studying wider benchmarks beyond permuted image recognition tasks for continual learning, because image permutation belongs to a class of relatively harmless task similarity according to our theory. As a simple extension, here we developed an image recognition task with a latent variable and showed that a vanilla deep network exhibits more forgetting under readout remapping than under input permutation (Figs. 7A and B). ", "page_idx": 9}, {"type": "text", "text": "Limitations Our theoretical results from a teacher-student model come with limitation in direct applicability. The presence of low-dimensional latent generating both input and target output is a reasonable assumption for many real-world tasks [7, 26], but the linear projection assumption is not. While we replicated most key results in a deep nonlinear network solving permuted MNIST task, we found that the diagonal approximation of Fisher information metric performs much better than the prediction from the teacher-student model. This is potentially because sparse activity at the hidden layer effectively makes the regularization low-rank even under the diagonal approximation. More generally, the presence of hidden layers should enable a distinctive adaptation for feature and readout similarity, which is an important future direction. It is also important to apply our theoretical framework for analyzing continual learning in more complicated neural architectures and datasets. ", "page_idx": 9}, {"type": "text", "text": "9 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The author thanks Liu Ziyin and Ziyan Li for discussion. This work was partially supported by The SwartzFoundation. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  M. S. Advani, A. M. Saxe, and H. Sompolinsky. High-dimensional dynamics of generalization error in neural networks. Neural Networks, 132:428-446, 2020.   \n[2]  H. Asanuma, S. Takagi, Y. Nagano, Y. Yoshida, Y. Igarashi, and M. Okada. Statistical mechanical analysis of catastrophic forgetting in continual learning with teacher and student networks. Journal of the Physical Society of Japan, 90(10):104001, 2021.   \n[3] Y. Bahri, J. Kadmon, J. Pennington, S. S. Schoenholz, J. Sohl-Dickstein, and S. Ganguli. Statistical mechanics of deep learning. Annual Review of Condensed Matter Physics, 11:501-528, 2020.   \n[4] M. Baity-Jesi, L. Sagun, M. Geiger, S. Spigler, G. B. Arous, C. Cammarota, Y. LeCun, M. Wyart, and G. Biroli. Comparing dynamics: Deep neural networks versus glassy systems. In International Conference on Machine Learning, pages 314-323. PMLR, 2018.   \n[5]  M. A. Bennani, T. Doan, and M. Sugiyama. Generalisation guarantees for continual learning with orthogonal gradient descent. arXiv preprint arXiv:2006.11942, 2020.   \n[6]  X. Chen, C. Papadimitriou, and B. Peng. Memory bounds for continual learning. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 519-530. IEEE, 2022.   \n[7]  U. Cohen, S. Chung, D. D. Lee, and H. Sompolinsky. Separability and geometry of object manifolds in deep neural networks. Nature communications, 11(1):746, 2020.   \n[8]  O. Dhifallah and Y. M. Lu. Phase transitions in transfer learning for high-dimensional perceptrons. Entropy, 23(4):400, 2021.   \n[9] T. Doan, M. A. Bennani, B. Mazoure, G. Rabusseau, and P. Alquier. A theoretical analysis of catastrophic forgetting through the ntk overlap matrix. In International Conference on Artificial Intelligence and Statistics, pages 1072-1080. PMLR, 2021.   \n[10] B. Ehret, C. Henning, M. R. Cervera, A. Meulemans, J. Von Oswald, and B. F. Grewe. Continual learning in recurrent neural networks. arXiv preprint arXiv:2006.12109, 2020.   \n[11] I. Evron, E. Moroshko, R. Ward, N. Srebro, and D. Soudry. How catastrophic can catastrophic forgetting be in linear regression? In Conference on Learning Theory, pages 4028-4079. PMLR, 2022.   \n[12] I. Evron, E. Moroshko, G. Buzaglo, M. Khriesh, B. Marjieh, N. Srebro, and D. Soudry. Continual learning in linear classification on separable data. In International Conference on Machine Learning, pages 9440-9484. PMLR, 2023.   \n[13] I. Evron, D. Goldfarb, N. Weinberger, D. Soudry, and P. Hand. The joint effect of task similarity and overparameterization on catastrophic forgeing-an analytical model. arXiv preprint arXiv:2401. 12617, 2024.   \n[14]  R. French and A. Ferrara. Modelling time perception in rats: evidence for catastrophic interference in animal learning. In 21st Congress of Cognitive Sciences, 1999.   \n[15]  RMFrenchUing semi-distribdrpresentations to vercomecatastrophi forgeting ncoectonist networks. In Procedings of the 13th anual cognitive science society conference, volume 1, pages 173-178, 1991.   \n[16] R.M.French. Catastrophic forgeting in connectionist networks. Trends in cognitive sciences, 3(4): 128-135, 1999.   \n[17] P. Gao, E. Trautmann, B. Yu, G. Santhanam, S. Ryu, K. Shenoy, and S.Ganguli. A theory of multineuronal dimensionality, dynamics and measurement. BioRxiv, page 214262, 2017.   \n[18]  E. Gardner and BDerrida Threeunfnished works on theoptimal storage capacity of networks. Joual of Physics A: Mathematical and General, 22(12):1983, 1989.   \n[19]  B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. Limitations of lazy training of two-layers neural network. Advances in Neural Information Processing Systems, 32, 2019.   \n[20]  D. Goldfarb and P. Hand. Analysis of catastrophic forgetting for random orthogonal transformation tasks in the overparameterized regime. In International Conference on Artificial Inteligence and Statistics, pages 2975-2993. PMLR, 2023.   \n[21]  S.Golkar, M. Kagan, and K. Cho. Continual learning via neural pruning. arXiv preprint arXiv:1903.04476, 2019.   \n[22] I J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y. Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv: 1312.6211, 2013.   \n[23]  R. Hadsell, D. Rao, A. A. Rusu, and R. Pascanu. Embracing change: Continual learning in deep neural networks. Trends in cognitive sciences, 24(12):1028-1040, 2020.   \n[24]  R. Heckel. Provable continual learning via sketched jacobian approximations. In International Conference on Artificial Intelligence and Statistics, pages 10448-10470. PMLR, 2022.   \n[25]  N. Hiratani, Y. Mehta, T. Lillicrap, and P. E. Latham. On the stability and scalability of node perturbation learning. Advances in Neural Information Processing Systems, 35:31929-31941, 2022.   \n[26]  M. Jazayeri and S. Ostojic. Interpreting neural computations by examining intrinsic and embedding dimensionality of neural activity. Current opinion in neurobiology, 70:113-120, 2021.   \n[27]  R. Karakida and S. Akaho. Learning curves for continual learning in neural networks: Self-knowledge transfer and forgetting. In International Conference on Learning Representations, 2021.   \n[28]  Z. Ke, B. Liu, and X. Huang. Continual learning of a mixed sequence of similar and dissimilar tasks. Advances in neural information processing systems, 33:18493-18504, 2020.   \n[29]  Z. Ke, B. Liu, W. Xiong, A. Celikyilmaz, and H Li Sub-network discovery and soft-masking for continual learning of mixed tasks. arXiv preprint arXiv:2310.09436, 2023.   \n[30] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgeting in neural networks. Proceedings of the national academy of sciences, 114(13):3521-3526, 2017.   \n[31l IKblauch,Hsain, an TDih imal contiualangha prfetmmory and is -har. I International Conference on Machine Learning, pages 5327-5337. PMLR, 2020.   \n[32] D. Kudithipudi, M. Aguilar-Simon, J. Babb, M. Bazhenov, D. Blackiston, J. Bongard, A. P. Brna, S. Chakravarthi Raja, N. Cheney, J. Clune, et al. Biological underpinnings for lifelong learning machines. Nature Machine Intelligence, 4(3):196-210, 2022.   \n[33]  A. K. Lampinen and S. Ganguli. An analytic theory of generalization dynamics and transfer learning in deep linear networks. arXiv preprint arXiv: 1809.10374, 2018.   \n[34]  Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.   \n[35] S. Lee, S. Goldt, and A. Saxe. Continual learning in the teacher-student setup: Impact of task similarity. In International Conference on Machine Learning, pages 6109-6119. PMLR, 2021.   \n[36]  S.-W. Lee, J.-H. Kim, J. Jun, J-W. Ha, and B.T. Zhang. Overcoming catastrophic forgetting by incremental moment matching. Advances in neural information processing systems, 30, 2017.   \n[37]  C. Li, Z. Huang, W. Zou, and H. Huang. Statistical mechanics of continual learning: Variational principle and mean-field potential. Physical Review E, 108(1):014309, 2023.   \n[38]  Y. Li, T. Ma, and H R. Zhang. Learning over-parametrized two-layer neural networks beyond ntk. In Conference on learning theory, pages 2613-2682. PMLR, 2020.   \n[39]  S. Lin, L. Yang, D. Fan, and J. Zhang. Beyond not-forgetting: Continual learning with backward knowledge transfer. Advances in Neural Information Processing Systems, 35:16165-16177, 2022.   \n[40] S. Lin, P. Ju, Y. Liang, and N. Shroff. Theory on forgetting and generalization of continual learning. In International Conference on Machine Learning, pages 21078-21i00. PMLR, 2023.   \n[41]  Y. Luo, Z. Yang, F. Meng, Y. Li, J. Zhou, and Y. Zhang. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747, 2023.   \n[42]  N. Y. Masse, G. D. Grant, and D. J. Freedman. Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization. Proceedings of the National Academy of Sciences, 115(44):E10467- E10475,2018.   \n[43]  A. Maurer, M. Pontil, and B. Romera-Paredes. The benefit of multitask representation learning. Journal of Machine Learning Research, 17(81):1-32, 2016.   \n[44]  M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109-165. Elsevier, 1989.   \n[45]  A. Nirango and T. Lee. Generalization in multask deep neural classfers: statistical physics approach. Advances in Neural Information Processing Systems, 32, 2019.   \n[46] S. Ozguin, A-M. Rickmann, A. G. Roy, and C. Wachinger. Importance driven continual learning for segmentation across domains. In Machine Learning in Medical Imaging: 1lth International Workshop, MLMI2020,Held in Conjunction with MICCAI2020, Lima, Peru, October 4, 2020, Procedings 11, pages 423-433. Springer, 2020.   \n[47]  V. V. Ramasesh, E. Dyer, and M. Raghu. Anatomy of catastrophic forgetting: Hidden representations and task semantics. arXiv preprint arXiv:2007.07400, 2020.   \n[48] A.Robins. Catastrophic forgeting, rehearsal and pseudorehearsal. Connection Science, 7(2):123-146, 1995.   \n[49]  D. Rolnick, A. Ahuja, J. Schwarz, T. Lillicrap, and G. Wayne. Experience replay for continual learning. Advances in neural information processing systems, 32, 2019.   \n[50]  A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell. Progressive neural networks. arXiv preprint arXiv: 1606.04671, 2016.   \n[51]  D. Saad and S. A. Solla. On-line learning in soft committe machines. Physical Review E, 52(4):4225, 1995.   \n[52]  A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv: 1312.6120, 2013.   \n[53]  A. M. Saxe, J. L. McClelland, and S. Ganguli. A mathematical theory of semantic development in deep neural networks. Proceedings of the National Academy of Sciences, 116(23):11537-11546, 2019.   \n[54]  J. Serra, D. Suris, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In International conference on machine learning, pages 4548-4557. PMLR, 2018.   \n[55]  H. S. Seung, H. Sompolinsky, and N. Tishby. Statistical mechanics of learning from examples. Physical review A, 45(8):6056, 1992.   \n[56] E. Sezener, A. Grabska-Barwinska, D. Kostadinov, M. Beau, S. Krishnagopal, D. Budden, M. Hutter, J. Veness, M. Botvinick, C. Clopath, et al. A rapid and efficient learning rule for biological neural circuits. BioRxiv, pages 2021-03, 2021.   \n[57]  R. K. Srivastava, J. Masci, S. Kazerounian, F. Gomez, and J. Schmidhuber. Compete to compute. Advances in neural information processing systems, 26, 2013.   \n[58]  Y. Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. In International conference on machine learning, pages 3404-3413. PMLR, 2017.   \n[59]  G. M. Van de Ven, H. T. Siegelmann, and A. S. Tolias. Brain-inspired replay for continual learning with artificial neural networks. Nature communications, 11(1):4069, 2020.   \n[60]  L. Wang, X. Zhang, Q. Li, M. Zhang, H. Su, J. Zhu, and Y. Zhong. Incorporating neuro-inspired adaptability for continual learning in artificial intelligence. Nature Machine Intelligence, 5(12):1356-1368, 2023.   \n[61] J. Werfel, X. Xie, and H. Seung. Learning curves for stochastic gradient descent in linear feedforward networks. Advances in neural information processing systems, 16, 2003.   \n[62]  Z. Xu and A. Tewari. On the statistical benefits of curriculum learning. In International Conference on Machine Learning, pages 24663-24682. PMLR, 2022.   \n[63] J. Yoon, E. Yang, J. Lee, and S. J. Hwang. Lifelong learning with dynamically expandable networks. arXiv preprint arXiv:1708.01547, 2017.   \n[64]  X. Yu, T. Liu, X. Wang, and D. Tao. On compressing deep models by low rank and sparse decomposition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7370-7379, 2017.   \n[65] A. R. Zamir, A. Sax, W. Shen, L. J. Guibas, J. Malik, and S. Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3712-3722, 2018.   \n[66]  L. Zdeborova and F. Krzakala. Statistical physics of inference: Thresholds and algorithms. Advances in Physics, 65(5):453-552, 2016.   \n[67]  F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence. In International conference on machine learning, pages 3987-3995. PMLR, 2017. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Problem setting: Linear teacher-student model with a latent variable ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Let us consider a continuous learning of $N_{s e s s}$ tasks. We generate the input $\\pmb{x}\\in\\mathbb{R}^{N_{x}}$ and the target output $\\pmb{y}^{*}\\in\\mathbb{R}^{N_{y}}$ of the $\\mu$ -th task by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\pmb{s}\\leftarrow\\pmb{\\mathcal{N}}\\left(0,I_{s}\\right),\\quad\\pmb{x}=A_{\\mu}\\pmb{s},\\quad\\pmb{y}^{*}=B_{\\mu}\\pmb{s},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\pmb{s}\\,\\in\\,\\mathbb{R}^{N_{s}}$ is the latent variable, $I_{s}$ is the size $N_{s}$ identity matrix, and $A_{\\mu}\\,\\in\\,\\mathbb{R}^{N_{x}\\times N_{s}}$ and $B_{\\mu}\\in\\mathbb{R}^{N_{y}\\times N_{s}}$ are mixing matrices that map the latent variable $\\pmb{s}$ into the input space and output space, respectively. Throughout the paper, the vector $\\textbf{\\em x}$ is defined as a column vector, and its transpose $\\dot{x}^{T}$ is a row vector. ", "page_idx": 13}, {"type": "text", "text": "We consider sequential learning of these tasks with a neural network defined by ${\\pmb y}=W\\psi({\\pmb x})$ , where $W\\,\\in\\,\\mathbb{R}^{N_{y}\\times N_{x}}$ is the plastic weight. We set the function $\\psi\\,:\\,\\mathbb{R}^{N_{x}}\\,\\rightarrow\\,\\mathbb{R}^{N_{x}^{\\star}}$ to $\\psi(\\pmb{x})\\,=\\,\\pmb{x}$ in the vanilla and weight regularization models, $\\psi(\\mathbf{x})=\\pmb{g}\\odot\\pmb{x}$ in activity and plasticity gating models, and $\\psi(\\pmb{x})=s g n(\\pmb{x})\\odot\\operatorname*{max}\\{0,|\\pmb{x}|-h\\}$ in the soft-thresholding model. Here, $\\textstyle g\\odot x$ represents an element-wise multiplication of two vectors, and $s g n(x)$ is a function that returns the sign of the input $x$ . The goal of the student network, ${\\pmb y}=W\\psi({\\pmb x})$ , is to mimic the teacher network that generates both input and target output [18, 66, 3]. Below, we focus on learning of $N_{s e s s}=2$ tasks. To analyze the transfer and retention performance, we introduce the following two assumptions on task structure. ", "page_idx": 13}, {"type": "text", "text": "Assumption I: Random task assumption  We consider random generation of task matrices $A_{\\mu},B_{\\mu}$ but with correlation between subsequent tasks. We sample elements of the mixing matrices for the first task, A1 and B1, from a Gaussian distribution with mean zero and variance  i independently. For the subsequent tasks, we generate mixing matrices $A_{\\mu}$ by ", "page_idx": 13}, {"type": "equation", "text": "$$\nA_{\\mu,i j}=\\left\\{\\!\\!\\begin{array}{l l}{{A_{\\mu-1,i j}}}&{{(\\mathrm{with}\\;\\mathrm{probability}\\;\\rho_{a})}}\\\\ {{\\widetilde{A}_{\\mu,i j}}}&{{(\\mathrm{otherwise})}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\widetilde{A}_{\\mu}$ is a random matrix whose elements are sampled independently from a Gaussian distribution with mean zero and variance $\\frac{1}{N_{s}}$ . We generate $B_{\\mu}$ in the same manner, but with corrlation $\\rho_{b}$ Although this hidden weight generation doesn't exactly follow the correlated Gaussian assumption, the difference appears only in higher-order terms, which are negligible under $N_{s}\\gg N_{x}$ assumption we introduce below. ", "page_idx": 13}, {"type": "text", "text": "Assumption II: Low-dimensional latent assumption The second assumption is the relative low dimensionality of the latent space with respect to the input space: $N_{s}\\ll N_{x}$ This assumption of over-parameterization in the input space simplifies the analysis (see Appendix D). ", "page_idx": 13}, {"type": "text", "text": "Evaluation of the transfer and retention performance We evaluate the task performance of a student model with weight $W$ On task $\\mu$ by mean-squared error: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\epsilon_{\\mu}[W]\\equiv\\frac{1}{N_{y}}\\left\\langle\\|B_{\\mu}\\pmb{s}-W\\psi(A_{\\mu}\\pmb{s})\\|^{2}\\right\\rangle_{\\pmb{s}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Here, $\\langle\\cdot\\rangle_{s}$ is the expectation over latent ${\\pmb s}\\sim\\mathcal N({\\bf0},{\\cal I}_{s})$ . Using $\\epsilon_{\\mu}[W]$ , the degree of forward knowledge transfer and retention are evaluated by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\Delta\\epsilon_{\\mu}^{T F}\\equiv\\langle\\epsilon_{\\mu}[W_{o}]-\\epsilon_{\\mu}[W_{\\mu-1}]\\rangle_{A,B}\\,,}\\\\ &{}&{\\Delta\\epsilon_{\\mu}^{R T}\\equiv\\langle\\epsilon_{\\mu}[W_{o}]-\\epsilon_{\\mu}[W_{\\mu+1}]\\rangle_{A,B}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $W_{\\mu}$ is the weight after training on the $\\mu$ -th task. Note that, the retention performance is defined by the difference with the initial error $\\epsilon_{\\mu}[W_{o}]$ . This means that, if the network fails to learn the task in the first place (i.e., if $\\epsilon_{\\mu}[W_{\\mu}]>0]$ 0, the retention performance becomes sub-optimal even if the network doesn't forget the learned task. We used this definition for our problem setting because the network is able to learn the task perfectly unless strong regularization is added to the network. ", "page_idx": 13}, {"type": "text", "text": "Learning procedures  We consider the gradient descent learning from infinite samples at the gradient flow limit and analyze the performance at the convergence. Below, we investigate how $\\bar{\\Delta}\\epsilon_{\\mu}^{T F}$ $\\Delta\\epsilon_{\\mu_{\\!\\cdot}}^{R T}$ dependon thetasimlartyandhyper-parametersundervarousalgorthmnsfor ", "page_idx": 13}, {"type": "text", "text": "B  Task-dependent gating model ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Activity gating ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let us first consider task-dependent activity gating model. The student network (the network that learns the task) is given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\pmb y}=W[{\\pmb g}_{\\mu}\\odot{\\pmb x}],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where ${\\bf\\nabla}_{g\\mu}$ is the gating vector that depends on the task id $\\mu$ , but independent of input $\\textbf{\\em x}$ . Note that, by setting ${\\scriptstyle g_{\\mu}}=1$ , we recover the vanilla model ${\\pmb y}=W{\\pmb x}$ ", "page_idx": 14}, {"type": "text", "text": "The performance of this network on the $\\mu$ -th task is written as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\epsilon_{\\mu}[W]\\equiv\\frac{1}{N_{y}}\\left\\langle\\left\\|B_{\\mu}s-W[g_{\\mu}\\odot(A_{\\mu}s)]\\right\\|^{2}\\right\\rangle_{s}=\\frac{1}{N_{y}}\\left\\|B_{\\mu}-W D_{\\mu}A_{\\mu}\\right\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $D_{\\mu}\\equiv\\mathrm{diag}[g_{\\mu}]$ is a diagonal matrix whose $(i,i)$ -th element is $g_{\\mu,i}$ , and $\\|\\cdot\\|_{F}$ is the Frobenius norm. ", "page_idx": 14}, {"type": "text", "text": "Solution of gradient descent learning Under the gradient descent learning, the weight dynamics follows ", "text_level": 1, "page_idx": 14}, {"type": "equation", "text": "$$\n\\dot{W}(t)=-\\eta\\frac{\\partial\\epsilon_{\\mu}[W]}{\\partial W}=-\\frac{2\\eta}{N_{y}}\\left(W D_{\\mu}A_{\\mu}-B_{\\mu}\\right)\\left(D_{\\mu}A_{\\mu}\\right)^{T}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From singular value decomposition (SVD), $D_{\\mu}A_{\\mu}$ is rewritten as ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{\\mu}A_{\\mu}=U_{\\mu}\\Sigma_{\\mu}V_{\\mu}^{T},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $U_{\\mu}\\in\\mathbb{R}^{N_{x}\\times N_{o}}$ and $V_{\\mu}\\in\\mathbb{R}^{N_{s}\\times N_{o}}$ are semi-orthonormal matrices (i.. $U_{\\mu}^{T}U_{\\mu}=V_{\\mu}^{T}V_{\\mu}=I_{o})$ \uff0c $\\Sigma_{\\mu}\\in\\mathbb{R}^{N o\\times N_{o}}$ is a positive diagonal matrx, $N_{o}$ is the number of non-zero singular values of $D_{\\mu}A_{\\mu}$ Using this decomposition, the gradient descent dynamics is rewritten as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\dot{W}(t)=-\\frac{2\\eta}{N_{y}}\\left(W U_{\\mu}\\Sigma_{\\mu}-B_{\\mu}V_{\\mu}\\right)\\Sigma_{\\mu}U_{\\mu}^{T}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, the weight matrix $W(t)$ at arbitrary $t$ is written as ", "page_idx": 14}, {"type": "equation", "text": "$$\nW(t)=W(t=0)+Q(t)U_{\\mu}^{T},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $Q\\in\\mathbb{R}^{N_{y}\\times N_{o}}$ is a time-dependent matrix. At the fixed point of this learning dynamics, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left(\\left[W_{\\mu-1}+Q U_{\\mu}^{T}\\right]U_{\\mu}\\Sigma_{\\mu}-B_{\\mu}V_{\\mu}\\right)\\Sigma_{\\mu}U_{\\mu}^{T}={\\cal O},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $O$ is the zero matrix and $W_{\\mu-1}\\equiv W(t=0)$ is the weight after learning of the previous task. Because $\\Sigma_{\\mu}$ is a positive diagonal matrix, the equation above has a unique solution: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q=(B_{\\mu}V_{\\mu}-W_{\\mu-1}U_{\\mu}\\Sigma_{\\mu})\\,\\Sigma_{\\mu}^{-1},}\\\\ &{W=W_{\\mu-1}\\left(I_{x}-U_{\\mu}U_{\\mu}^{T}\\right)+B_{\\mu}(D_{\\mu}A_{\\mu})^{+},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $(D_{\\mu}A_{\\mu})^{+}=V_{\\mu}\\Sigma_{\\mu}^{-1}U_{\\mu}^{T}$ is the pseudo-inverse of $D_{\\mu}A_{\\mu}$ , and $I_{x}$ is the size $N_{x}$ identity matrix. From Eqs. 20b and 14, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\epsilon_{\\mu}[W_{\\mu}]=\\frac{1}{N_{y}}\\left\\|B_{\\mu}\\left(V_{\\mu}V_{\\mu}^{T}-I_{s}\\right)\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Moreover, by setting the initial weight $W_{o}$ (the weight before the first tasks) to zero, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\epsilon_{\\mu}[W_{o}]=\\frac{1}{N_{y}}\\left\\|B_{\\mu}\\right\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and $W_{1}$ and $W_{2}$ follow ", "page_idx": 14}, {"type": "equation", "text": "$$\nW_{1}=B_{1}(D_{1}A_{1})^{+},\\quad W_{2}=B_{1}(D_{1}A_{1})^{+}(I_{x}-U_{2}U_{2}^{T})+B_{2}(D_{2}A_{2})^{+}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The error in task 2 after learning of task 1 is thus written as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\epsilon_{2}[W_{1}]=\\frac{1}{N_{y}}\\left\\|B_{2}-B_{1}(D_{1}A_{1})^{+}D_{2}A_{2}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, the error on task 1 after learning of task 2 is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\epsilon_{1}[W_{2}]={\\frac{1}{N_{y}}}\\left\\|B_{1}\\left(I_{s}-(D_{1}A_{1})^{+}(I_{x}-U_{2}U_{2}^{T})D_{1}A_{1}\\right)-B_{2}(D_{2}A_{2})^{+}D_{1}A_{1}\\right\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that the results so far does not rely on Assumptions I and $\\mathrm{II}$ , making them applicable to arbitrary task matrices $(A,B)$ ", "page_idx": 15}, {"type": "text", "text": "The weight matrix after learning depends on the pseudo-inverse, $(D A)^{+}\\,=\\,V\\Sigma^{-1}U^{T}$ ,which is typically a complicated function of the original matrix $D A$ . However, when $\\alpha N_{x}\\gg N_{s}$ ,it can be approximated by a scaled transpose, $\\frac{N_{s}}{\\alpha N_{x}}(\\bar{D}A)^{T}$ (see Eq. 101). ", "page_idx": 15}, {"type": "text", "text": "Random and adaptive activity gating  In the adaptive activity gating model, we generate input gating units $\\{g_{i}^{\\mu}\\}$ randomly, but with correlations between subsequent tasks. We sample the gating units for the first task with $\\dot{g}_{i}^{1}\\gets\\mathrm{Bernoulli}(\\alpha)$ , where $\\alpha$ is the sparsity of the gating units. The gating units for the $\\mu+1$ task, $\\{g_{i}^{\\mu+1}\\}$ , are then generated by ", "page_idx": 15}, {"type": "equation", "text": "$$\ng_{i}^{\\mu+1}={\\left\\{\\!\\!\\begin{array}{l l}{g_{i}^{\\mu}}&{{\\mathrm{(with~probability~}}\\rho_{\\mu+1}^{g}{\\mathrm{)}}}\\\\ {{\\mathrm{Bernoulli}}(\\alpha)}&{{\\mathrm{(otherwise)}}}\\end{array}\\!\\!\\right.}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, $\\rho_{\\mu+1}^{g}$ isthcorelanbtwtgangunts $\\mu$ thand $\\mu+1$ -th tasks The aing correlation $\\rho_{\\mu}^{g}$ is set to zero in the case of random task-dependent gating, whereas $\\rho_{\\mu}^{g}$ is adjusted based on the model performance right after a model switch in the adaptive task-dependent gating. To this end, we introduce a probe trial where the model solves the new task using the gating vector for the old task. The error on the probe trial is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\epsilon_{p b}\\equiv\\frac{1}{N_{y}}\\left\\|{\\cal B}_{2}-W_{1}D_{1}A_{2}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We then set the probability of using the same gating elements by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\rho_{g}=\\operatorname*{max}\\left\\{0,1-\\frac{\\epsilon_{p b}}{\\epsilon_{o}}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{\\epsilon_{o}\\equiv\\frac{1}{N_{y}}\\left\\|\\boldsymbol{B}_{2}\\right\\|_{F}^{2}}\\end{array}$ is the baseline error on the task . It keps the gating the same f $\\epsilon_{p b}=0$ while it resamples all gating elements in case $\\epsilon_{p b}\\geq\\epsilon_{o}$ ", "page_idx": 15}, {"type": "text", "text": "Transfer performance  The average transfer performance $\\Delta\\epsilon_{T F}$ from the first to the second tasks is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\Delta\\epsilon_{T F}=\\displaystyle\\frac{1}{N_{y}}\\left<\\|B_{2}\\|_{F}^{2}\\right>-\\displaystyle\\frac{1}{N_{y}}\\left<\\left\\|B_{2}-B_{1}\\left(D_{1}A_{1}\\right)^{+}D_{2}A_{2}\\right\\|_{F}^{2}\\right>}}\\\\ {{=\\displaystyle\\frac{1}{N_{y}}\\left<2\\mathrm{tr}\\left[B_{2}^{T}B_{1}(D_{1}A_{1})^{+}D_{2}A_{2}\\right]-\\left\\|B_{1}(D_{1}A_{1})^{+}D_{2}A_{2}\\right\\|_{F}^{2}\\right>}}\\\\ {{=\\displaystyle\\frac{2\\rho_{b}}{N_{s}}\\left<\\mathrm{tr}\\left[(D_{1}A_{1})^{+}D_{2}A_{2}\\right]\\right>-\\displaystyle\\frac{1}{N_{s}}\\left<\\left\\|(D_{1}A_{1})^{+}D_{2}A_{2}\\right\\|_{F}^{2}\\right>.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In the last line, we took the expectation over the correlated random matrices $B_{1}$ and $B_{2}$ . Using the approximation $\\begin{array}{r}{(D A)^{+}\\approx\\frac{N_{s}}{\\alpha N_{x}}(D A)^{T}}\\end{array}$ (Eq. 101), and then taking the expectation over $A_{1},A_{2},D_{1}$ \uff0c and $D_{2}$ , the first term becomes ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\langle\\mathrm{tr}[(D_{1}A_{1})^{+}D_{2}A_{2}]\\right\\rangle\\approx\\frac{N_{s}}{\\alpha N_{x}}\\left\\langle\\mathrm{tr}[A_{1}^{T}D_{1}D_{2}A_{2}]\\right\\rangle}\\\\ {\\quad\\quad\\quad\\quad\\quad\\quad=\\frac{N_{s}}{\\alpha N_{x}}\\displaystyle\\sum_{i=1}^{N_{s}}\\sum_{j=1}^{N_{x}}\\left\\langle a_{j i}^{(1)}g_{j}^{(1)}g_{j}^{(2)}a_{j i}^{(2)}\\right\\rangle=\\tilde{\\alpha}\\rho_{a}N_{s},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $a_{j i}^{(1)}$ represents the $(j,i)$ -th element of $A_{1}$ , and $\\tilde{\\alpha}$ is defined by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\alpha}\\equiv\\rho_{g}+(1-\\rho_{g})\\alpha.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The second term of Eq. 29 follows ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\left\\|(D_{1}A_{1})^{+}D_{2}A_{2}\\right\\|_{F}^{2}\\right\\rangle\\approx\\left(\\frac{N_{s}}{\\alpha N_{x}}\\right)^{2}\\left\\langle\\left\\|A_{1}^{T}D_{1}D_{2}A_{2}\\right\\|_{F}^{2}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\left(\\frac{N_{s}}{\\alpha N_{x}}\\right)^{2}\\displaystyle\\sum_{i,k=1}^{N_{s}}\\sum_{j,l=1}^{N_{x}}\\left\\langle g_{j}^{(1)}g_{j}^{(2)}g_{l}^{(1)}g_{l}^{(2)}a_{j i}^{(1)}a_{j k}^{(2)}a_{l k}^{(2)}a_{l i}^{(1)}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{1}{\\alpha^{2}N_{x}^{2}}\\displaystyle\\sum_{i,k=1}^{N_{s}}\\sum_{j,l=1}^{N_{x}}\\left\\langle g_{j}^{(1)}g_{j}^{(2)}g_{l}^{(1)}g_{l}^{(2)}[\\delta_{i k}\\rho_{a}^{2}+\\delta_{j l}+\\delta_{i k}\\delta_{j l}\\rho_{a}^{2}]\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=N_{s}\\left(\\tilde{\\alpha}^{2}\\rho_{a}^{2}+\\frac{\\tilde{\\alpha}N_{s}}{\\alpha N_{x}}\\left(1+\\frac{\\rho_{a}^{2}}{N_{s}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The third line follows from Isserlis\u2019 theorem, from which we can decompose the higher-order correlation asbelow: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left\\langle a_{j i}^{(1)}a_{j k}^{(2)}a_{l k}^{(2)}a_{l i}^{(1)}\\right\\rangle=\\left\\langle a_{j i}^{(1)}a_{j k}^{(2)}\\right\\rangle\\left\\langle a_{l k}^{(2)}a_{l i}^{(1)}\\right\\rangle+\\left\\langle a_{j i}^{(1)}a_{l i}^{(1)}\\right\\rangle\\left\\langle a_{j k}^{(2)}a_{l k}^{(2)}\\right\\rangle+\\left\\langle a_{j i}^{(1)}a_{l k}^{(2)}\\right\\rangle\\left\\langle a_{j k}^{(2)}a_{l i}^{(1)}\\right\\rangle}\\\\ &{}&{=\\frac{\\rho_{a}^{2}}{N_{s}^{2}}\\delta_{i k}+\\frac{1}{N_{s}^{2}}\\delta_{j l}+\\frac{\\rho_{a}^{2}}{N_{s}^{2}}\\delta_{i k}\\delta_{j l}.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(3?)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Summing up the quations above, at $\\begin{array}{r}{\\frac{N_{s}}{\\alpha N_{x}}\\rightarrow0}\\end{array}$ limit, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta\\epsilon_{T F}=\\tilde{\\alpha}\\rho_{a}\\left(2\\rho_{b}-\\tilde{\\alpha}\\rho_{a}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that when $A_{2}$ is generated by random replacement, instead of sampling from a correlated Gaussian distribution, $\\left\\langle\\left\\Vert(D_{1}a_{1})^{+}D_{2}A_{2}\\right\\Vert_{F}^{2}\\right\\rangle$ term instead follows $\\left\\langle\\|(\\bar{D_{1}}a_{1}\\bar{)^{+}}D_{2}A_{2}\\|_{F}^{2}\\right\\rangle\\approx$ $\\begin{array}{r}{N_{s}\\left(\\tilde{\\alpha}^{2}\\rho_{a}^{2}+\\frac{\\tilde{\\alpha}N_{s}}{\\alpha N_{x}}\\left[1+\\frac{\\rho_{a}}{N_{s}}(2-\\rho_{a}\\alpha\\tilde{\\alpha})\\right]\\right)}\\end{array}$ , which converges to Eq. 32 at $N_{s}\\ll N_{x}$ limit. ", "page_idx": 16}, {"type": "text", "text": "Retention performance Let us next analyze the retention performance, $\\begin{array}{r l}{\\Delta\\epsilon_{\\mathit{R T}}}&{{}\\equiv}\\end{array}$ $\\left\\langle\\epsilon_{1}[W_{o}]-\\epsilon_{1}\\bar{[W_{2}]}\\right\\rangle_{A,B}$ , which characterizes how well the network performs the task 1 after learning task 2. At $\\alpha N_{x}\\gg N_{s}$ regime, $\\left\\langle\\epsilon_{1}[W_{o}]\\right\\rangle_{A,B}=1$ . Inserting Eq. 23 into Eq. 14, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\epsilon_{1}[W_{2}]=\\frac{1}{N_{y}}\\left\\|B_{1}-\\left[B_{1}(D_{1}A_{1})^{+}(I_{x}-U_{2}U_{2}^{T})+B_{2}(D_{2}A_{2})^{+}\\right]D_{1}A_{1}\\right\\|_{F}^{2}}\\\\ {=\\frac{1}{N_{y}}\\left\\|B_{1}(D_{1}A_{1})^{+}U_{2}U_{2}^{T}D_{1}A_{1}-B_{2}(D_{2}A_{2})^{+}D_{1}A_{1}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, $\\Delta\\epsilon_{R T}$ follows ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\epsilon_{R T}=\\frac{1}{N_{y}}\\left\\langle\\|B_{1}\\|_{F}^{2}\\right\\rangle-\\frac{1}{N_{y}}\\left\\langle\\left\\|B_{1}(D_{1}A_{1})^{+}U_{2}U_{2}^{T}D_{1}A_{1}\\right\\|_{F}^{2}\\right\\rangle-\\frac{1}{N_{y}}\\left\\langle\\left\\|B_{2}(D_{2}A_{2})^{+}D_{1}A_{1}\\right\\|_{F}^{2}\\right\\rangle}\\\\ &{\\qquad+\\frac{2}{N_{y}}\\left\\langle\\mathrm{tr}\\left[B_{2}^{T}B_{1}(D_{1}A_{1})^{+}U_{2}U_{2}^{T}D_{1}A_{1}(D_{1}A_{1})^{T}\\left((D_{2}A_{2})^{+}\\right)^{T}\\right]\\right\\rangle}\\\\ &{\\qquad=1-\\frac{1}{N_{s}}\\left\\langle\\|(D_{1}A_{1})^{+}U_{2}U_{2}^{T}D_{1}A_{1}\\|_{F}^{2}\\right\\rangle-\\frac{1}{N_{s}}\\left\\langle\\left\\|(D_{2}A_{2})^{+}D_{1}A_{1}\\right\\|_{F}^{2}\\right\\rangle}\\\\ &{\\qquad+\\frac{2\\rho_{b}}{N_{s}}\\left\\langle\\mathrm{tr}\\left[(D_{1}A_{1})^{+}U_{2}U_{2}^{T}D_{1}A_{1}(D_{1}A_{1})^{T}\\left((D_{2}A_{2})^{+}\\right)^{T}\\right]\\right\\rangle.\\qquad\\qquad\\qquad(36)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Because $D_{2}A_{2}A_{2}^{T}D_{2}=U_{2}\\Sigma_{2}^{2}U_{2}^{T}$ , at the $N_{s}\\ll\\alpha N_{x}$ limit, $U_{2}U_{2}^{T}$ is approximated by (see Eq. 102) ", "page_idx": 16}, {"type": "equation", "text": "$$\nU_{2}U_{2}^{T}\\approx\\frac{N_{s}}{\\alpha N_{x}}D_{2}A_{2}A_{2}^{T}D_{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "image", "img_path": "bE7GWLQzkM/tmp/5987b2a91fe16d665f0a27cda29ba15c57625d6a1960808fe33c4ae56d2bda3a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 8: The gating level dependence of the transfer and retention performance (A) Phase diagram of the gating level dependence. (B-D) Transfer and retention performance as a function of the gating level at a representative point of each phase. ", "page_idx": 17}, {"type": "text", "text": "Using this approximation and $\\begin{array}{r}{(D A)^{+}\\;\\approx\\;\\frac{N_{s}}{\\alpha N_{x}}(D A)^{T}}\\end{array}$ (Eq. 101), we can obtain an analytical expression for Eq. 36. Taking the expectation over $D$ and $A$ , the second term of Eq. 36 becomes ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{N_{s}}\\left(\\frac{N_{s}}{\\alpha N_{x}}\\right)^{4}\\left\\langle\\|A_{1}^{T}D_{1}D_{2}A_{2}A_{2}^{T}D_{2}D_{1}A_{1}\\|_{F}^{2}\\right\\rangle}\\\\ &{=\\displaystyle\\sum_{i_{j}\\neq l}\\sum_{m n p q_{j}}\\left\\langle g_{j}^{(1)}g_{l}^{(1)}g_{n}^{(1)}g_{q}^{(1)}g_{j}^{(2)}g_{l}^{(2)}g_{n}^{(2)}g_{j}^{(2)}g_{j}^{(1)}a_{j_{k}}^{(2)}a_{l k}^{(1)}a_{l m}^{(1)}a_{n m}^{(2)}a_{p}^{(2)}a_{q}^{(1)}\\right\\rangle}\\\\ &{=\\displaystyle\\frac{1}{\\alpha^{4}N_{s}N_{x}^{4}}\\sum_{i_{j}k l}\\sum_{m n p q}\\left\\langle g_{j}^{(1)}g_{l}^{(1)}g_{n}^{(1)}g_{q}^{(1)}g_{j}^{(2)}g_{l}^{(2)}g_{n}^{(2)}g_{n}^{(2)}g_{q}^{(2)}\\right\\rangle}\\\\ &{\\quad\\times\\left(\\left\\{\\delta_{i k}\\delta_{l k}\\delta_{m p}+\\delta_{i k}\\delta_{l q}\\delta_{m p}+\\delta_{k m}\\delta_{j n}\\delta_{i p}\\right\\}\\rho_{a}^{4}+\\left[\\delta_{i k}\\delta_{k m}\\delta_{n q}+\\delta_{k m}\\delta_{p p}\\delta_{j q}+\\delta_{m p}\\delta_{j q}\\delta_{j l}+\\delta_{p i}\\delta_{i k}\\right.\\right.}\\\\ &{\\quad\\left.+\\mathcal{O}\\left(\\frac{1}{\\alpha^{3}}\\right)}\\\\ &{=\\alpha^{4}\\rho_{a}^{4}+\\frac{N_{s}}{\\alpha N_{x}}\\tilde{\\alpha}^{3}(4\\rho_{a}^{2}+2\\rho_{a}^{4})+\\mathcal{O}\\left(\\frac{1}{\\alpha N_{x}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, we applied Isserlis\u2032 theorem again, then retained the terms up to the next-to-leading order with respectto $\\frac{N_{s}}{\\alpha N_{x}}$ . Similarly, the third term of Eq. 36 becomes ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{N_{s}}\\left(\\frac{N_{s}}{\\alpha N_{x}}\\right)^{2}\\left\\langle\\mathrm{tr}\\left[A_{1}^{T}D_{1}D_{2}A_{2}A_{2}^{T}D_{2}D_{1}A_{1}\\right]\\right\\rangle=\\tilde{\\alpha}^{2}\\rho_{a}^{2}+\\frac{\\tilde{\\alpha}N_{s}}{\\alpha N_{x}}\\left(1+\\frac{\\rho_{a}^{2}}{N_{s}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lastly, the cross-term is evaluated as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\mathbb{V}_{s}}\\left(\\frac{N_{s}}{\\alpha N_{x}}\\right)^{3}\\langle\\mathrm{tr}\\left[A_{2}^{T}D_{2}D_{1}A_{1}A_{1}^{T}D_{1}D_{2}A_{2}A_{2}^{T}D_{2}D_{1}A_{1}\\right]\\rangle}\\\\ &{=\\frac{1}{\\alpha^{3}N_{s}N_{x}^{3}}\\displaystyle\\sum_{i j k}\\sum_{l m n}\\left\\langle g_{j}^{(1)}g_{l}^{(1)}g_{n}^{(1)}g_{j}^{(2)}g_{l}^{(2)}g_{n}^{(2)}\\right\\rangle\\left(\\delta_{i k}\\delta_{k m}\\rho_{a}^{3}+\\delta_{i k}\\delta_{l n}\\rho_{a}+\\delta_{i m}\\delta_{j l}\\rho_{a}+\\delta_{m k}\\delta_{j n}\\rho_{a}^{3}\\right)+\\mathcal O\\left(\\frac{1}{\\alpha^{2}}\\right)}\\\\ &{=\\tilde{\\alpha}^{3}\\rho_{a}^{3}+\\frac{N_{s}}{\\alpha N_{x}}\\tilde{\\alpha}^{2}(2\\rho_{a}+\\rho_{a}^{3})+\\mathcal O\\left(\\frac{1}{\\alpha N_{x}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, up to the leading order, the retention performance follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta\\epsilon_{R T}=1-\\tilde{\\alpha}^{2}\\rho_{a}^{2}\\left(\\tilde{\\alpha}^{2}\\rho_{a}^{2}-2\\tilde{\\alpha}\\rho_{a}\\rho_{b}+1\\right)+\\mathcal{O}\\left(\\frac{N_{s}}{\\alpha N_{x}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Vanilla model  In the vanilla model, we have $\\tilde{\\alpha}=1$ .Therefore, at $N_{s}\\ll N_{x}$ limit, from Eqs. 34 and 41, the transfer and retention performance follow ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\epsilon_{T F}=\\rho_{a}(2\\rho_{b}-\\rho_{a}),}\\\\ &{\\Delta\\epsilon_{R T}=1-\\rho_{a}^{2}(\\rho_{a}^{2}-2\\rho_{a}\\rho_{b}+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Random activity gating  Under the random task-dependent activity gating, the gating level $\\tilde{\\alpha}=\\alpha$ is a constant. If $N_{s}\\ll\\alpha N_{x}$ , the transfer and retention performance are written by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\epsilon_{T F}=\\alpha\\rho_{a}(2\\rho_{b}-\\alpha\\rho_{a}),}\\\\ &{\\Delta\\epsilon_{R T}=1-\\alpha^{2}\\rho_{a}^{2}(\\alpha^{2}\\rho_{a}^{2}-2\\alpha\\rho_{a}\\rho_{b}+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that, this expression does not hold at the sparse limit $\\begin{array}{r}{\\alpha<\\frac{N_{s}}{N_{x}}}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Depending on $(\\rho_{a},\\rho_{b})$ combination, the gating level influences the transfer and retention performance in three different manners (Fig. 8). When $\\rho_{b}\\,>\\,\\rho_{a}$ , unless both $\\rho_{a}$ and $\\rho_{b}$ are large, there is a monotonic tradeoff between the transfer and retention performance (Fig. 8B). In the red region wheteo $\\begin{array}{r}{\\rho_{b}\\ge\\frac{2\\rho_{a}}{3}+\\frac{1}{3\\rho_{a}}}\\end{array}$ $\\begin{array}{r}{\\rho_{b}\\ge\\frac{2\\sqrt{2}}{3}\\land\\rho_{a}\\ge\\frac{1}{\\sqrt{2}}}\\end{array}$ retention performance (Fig. 8C). When $\\rho_{b}<\\rho_{a}$ , high gating level, $\\begin{array}{r}{\\alpha>\\frac{\\rho_{b}}{\\rho_{a}}}\\end{array}$ , lower both transfer and retention performance, thus there is no benefit of choosing large $\\alpha$ (Fig. 8D). ", "page_idx": 18}, {"type": "text", "text": "Adaptive activity gating  In the adaptive task-dependent activity gating model, we introduced a probe trial at the beginning of the task 2 to test the model performance for the new task. Under an adaptive gating with $\\begin{array}{r}{\\rho_{g}=\\operatorname*{max}\\left\\{0,1-\\frac{\\epsilon_{p b}}{\\epsilon_{o}}\\right\\}}\\end{array}$ , the probe error $\\epsilon_{p b}$ follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left<\\epsilon_{p b}\\right>_{A,B,g}=\\left<\\frac{1}{N_{y}}\\left\\|\\boldsymbol{B_{2}}-\\boldsymbol{W_{1}}D_{1}\\boldsymbol{A_{2}}\\right\\|_{F}^{2}\\right>_{A,B,g}}&{{}}\\\\ {\\approx\\left<\\frac{1}{N_{y}}\\left\\|\\boldsymbol{B_{2}}-\\frac{N_{s}}{\\alpha N_{x}}\\boldsymbol{B_{1}}(D_{1}\\boldsymbol{A_{1}})^{T}D_{1}\\boldsymbol{A_{2}}\\right\\|_{F}^{2}\\right>_{A,B,g}}&{{}}\\\\ {=1-2\\rho_{a}\\rho_{b}+\\rho_{a}^{2}+\\mathcal{O}\\left(\\frac{N_{s}}{\\alpha N_{x}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, at $N_{s}\\ll\\alpha N_{x}$ region, $\\rho_{g}=\\operatorname*{max}\\left\\{0,\\rho_{a}(2\\rho_{b}-\\rho_{a})\\right\\}$ and the effective gating level becomes ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widetilde{\\alpha}=\\left\\{\\!\\!\\begin{array}{l l}{\\alpha+(1-\\alpha)\\rho_{a}\\left(2\\rho_{b}-\\rho_{a}\\right)}&{\\mathrm{(if~}2\\rho_{b}\\geq\\rho_{a}\\mathrm{)}}\\\\ {\\alpha}&{\\mathrm{(otherwise)}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "meaning that if the transfer performance is positive in the vanilla model, there should be an overlap in the gating, but otherwise, the gating vector for the next task should be sampled independently. ", "page_idx": 18}, {"type": "text", "text": "Average error under a uniform prior on task similarity  Gating influences the performance differently depending on the feature and readout similarity. Thus, ideally the model should adjust the gating level based on the task similarity, but in most practical settings, the model doesn't know the similarity a priori. Therefore, it is important to analyze how it performs on average under a randomly chosen task similarity. To this end, here we measure the average transfer and retention performance assuming a uniform prior on $0\\leq\\rho_{a},\\rho_{b}\\leq1$ ", "page_idx": 18}, {"type": "text", "text": "In the vanilla model, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle\\Delta\\bar{\\epsilon}_{T F}=\\int_{0}^{1}d\\rho_{a}\\int_{0}^{1}d\\rho_{b}\\left(\\rho_{a}(2\\rho_{b}-\\rho_{a})\\right)=\\frac{1}{6}},}\\\\ {{\\displaystyle\\Delta\\bar{\\epsilon}_{R T}=\\int_{0}^{1}d\\rho_{a}\\int_{0}^{1}d\\rho_{b}\\left(1-\\rho_{a}^{2}\\left[\\rho_{a}^{2}-2\\rho_{a}\\rho_{b}+1\\right]\\right)=\\frac{43}{60}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Red and blue dash lines in Figs. 3D, 4C, 4D, 5D, 6C, and 6D represent these baseline performance of the vanillamodel: $\\begin{array}{r}{\\Delta\\bar{\\epsilon}_{T F}=\\frac{\\bar{1}}{6}}\\end{array}$ and $\\begin{array}{r}{\\Delta\\bar{\\epsilon}_{R T}=\\frac{43}{60}}\\end{array}$ By contrast, under a random task-dependent gating, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Delta\\bar{\\epsilon}_{T F}=\\frac{\\alpha}{2}-\\frac{\\alpha^{2}}{3},\\quad\\Delta\\bar{\\epsilon}_{R T}=1-\\frac{\\alpha^{2}}{3}+\\frac{\\alpha^{3}}{4}-\\frac{\\alpha^{4}}{5}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Solid lines in Fig. 3D and dashed lines in Fig. 4B plot the result above. In the case of adaptive gating, the expression of the average performance becomes complicated, yet still numerically tractable. ", "page_idx": 18}, {"type": "text", "text": "Note that the average transfer performance is expected to be lower than the retention performance, because forward knowledge transfer requires a high similarity. Even if we use the optimal gating level for transfer, $\\begin{array}{r}{\\alpha^{*}(\\rho_{a},\\bar{\\rho_{b}})=\\operatorname*{min}(1,\\frac{\\bar{\\rho_{b}}}{\\rho_{a}})}\\end{array}$ , the average transfer performance becomes ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int_{0}^{1}d\\rho_{a}\\int_{0}^{1}d\\rho_{b}\\alpha^{*}(\\rho_{a},\\rho_{b})\\rho_{a}(2\\rho_{b}-\\alpha^{*}(\\rho_{a},\\rho_{b})\\rho_{a})=\\frac{1}{4}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B.2  Plasticity gating ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "An alternative strategy is to gate plasticity at certain synapses in a task-dependent manner while keeping the activity intact. We implement this method by making synapses from only a subset of input neurons plastic for each task. Given a linear regression model ${\\pmb y}=W{\\pmb x}$ , the learning dynamics follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\dot{W}(t)=-\\eta(W A_{\\mu}-B_{\\mu})(D_{\\mu}A_{\\mu})^{T},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $D_{\\mu}=\\mathrm{diag}(g_{\\mu})$ is a diagonal matrix specifying which synapses are gated. As in the activity gating, ${\\mathbfit{g}}_{\\mu}$ is a gating vector whose elements take either one or zero in a task-dependent manner. We sample elements of ${\\mathbfit{g}}_{\\mu}$ from a Bernoulli distribution with probability $\\alpha$ asbefore. ", "page_idx": 19}, {"type": "text", "text": "Let us denote SVD of $A_{\\mu}$ by $A_{\\mu}=U_{\\mu}\\Lambda_{\\mu}V_{\\mu}^{T}$ . Then, from a parallel argument with Eqs. 18 and 19, wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\nW_{\\mu}=W_{\\mu-1}+(B_{\\mu}V_{\\mu}\\Lambda_{\\mu}^{-1}-W_{\\mu-1}U_{\\mu})(U_{\\mu}^{T}D_{\\mu}U_{\\mu})^{-1}U_{\\mu}^{T}D_{\\mu}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Notably, unlike Eq. 20b, gating term $D_{\\mu}$ appears both inside and outside of the inverse term $((U_{\\mu}^{T}D_{u}U_{\\mu})^{-1}$ and $U_{\\mu}^{T}D_{\\mu})$ . Thus, up to the leading order, gating level $\\alpha$ does not affect the transfer and retention performance. ", "page_idx": 19}, {"type": "text", "text": "B.3  Input soft-thresholding ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Input-dependent gating is an another method proposed for efficient continual learning. In a regression setting, it corresponds to soft-thresholding of the inputs. Let us introduce a soft-thresholding function $\\varphi(x)$ by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\varphi(x)=s g n(x)\\operatorname*{max}\\{0,|x|-h\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $s g n(x)$ represents the sign of $x$ . This nonlinearity makes the analysis difficult, but if $N_{s}\\gg1$ in addition to Assumptions I and $\\mathrm{II}$ , the error becomes tractable. Note that $N_{s}\\gg1$ assumption is required only in this subsection. Under the student network defined by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\pmb{y}}=W\\varphi(\\pmb{x}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "the mean-squared error follows ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\epsilon[W]=\\frac{1}{N_{y}}\\left\\langle\\|B s\\|^{2}\\right\\rangle_{s}+\\frac{1}{N_{y}}\\left\\langle\\|W\\varphi(A s)\\|^{2}\\right\\rangle_{s}-\\frac{2}{N_{y}}\\left\\langle\\mathrm{tr}\\left[B s\\varphi(A s)^{T}W^{T}\\right]\\right\\rangle_{s}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let us denote $\\begin{array}{r}{m_{i}\\equiv\\sum_{j=1}^{N_{s}}A_{i j}s_{j}}\\end{array}$ , then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\langle m_{i}\\right\\rangle=0,\\quad\\left\\langle m_{i}^{2}\\right\\rangle=\\sum_{j=1}^{N_{s}}A_{i j}^{2}\\approx1,\\quad\\left\\langle s_{j}m_{i}\\right\\rangle=A_{i j}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, $s_{j}$ and $m_{i}$ approximately follows a joint Gaussian distribution: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left({s_{j}\\atop m_{i}}\\right)\\sim\\mathcal{N}\\left(\\left(\\begin{array}{c c}{{0}}\\\\ {{0}}\\end{array},\\left(\\begin{array}{c c}{{1}}&{{A_{j i}}}\\\\ {{A_{j i}}}&{{1}}\\end{array}\\right)\\right).\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Denoting $A_{j i}=\\rho$ for brevity, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\lefteqn{(s_{j}\\varphi(m_{i}))}}\\\\ &{}&{=\\int_{-\\infty}^{\\infty}d s\\left(\\int_{-\\infty}^{-h}d m(m+h)+\\int_{h}^{\\infty}d m(m-h)\\right)\\frac{s}{2\\pi\\sqrt{1-\\rho^{2}}}\\exp\\left(-\\frac{1}{2(1-\\rho^{2})}\\left[s^{2}+m^{2}-2\\rho,1\\right]\\right)}\\\\ &{}&{=\\int_{-\\infty}^{\\infty}d s\\left(\\int_{-\\infty}^{-h}d m(m+h)+\\int_{h}^{\\infty}d m(m-h)\\right)\\frac{s}{2\\pi}\\left[1+\\rho s m+\\frac{\\rho^{2}}{2}(s^{2}-1)(m^{2}-1)+\\mathcal{O}(\\rho^{3})\\right]}\\\\ &{}&{\\times\\exp\\left(-\\frac{s^{2}+m^{2}}{2}\\right)}\\\\ &{}&{=\\mathrm{erfc}\\left[\\frac{h}{\\sqrt{2}}\\right]\\rho+\\mathcal{O}(\\rho^{3}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In the third line, we performed a Taylor expansion around $\\rho=0$ . Similarly, the expectation of $\\varphi(m_{j})\\varphi(m_{i})$ over $\\pmb{s}$ is written as ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\varphi(m_{j})\\varphi(m_{i})\\rangle={\\frac{1}{2\\pi}}\\left(\\int_{-\\infty}^{-h}d m(m+h)+\\int_{h}^{\\infty}d m(m-h)\\right)\\left(\\int_{-\\infty}^{-h}d m^{\\prime}(m^{\\prime}+h)+\\int_{h}^{\\infty}d m^{\\prime}(m-h)\\right)}\\\\ &{\\qquad\\qquad\\times\\left[1+\\rho_{c}m m^{\\prime}+{\\frac{\\rho_{c}^{2}}{2}}(m^{2}-1)(m^{\\prime2}-1)+{\\mathcal{O}}(\\rho_{c}^{3})\\right]\\exp\\left(-{\\frac{m^{2}+m^{\\prime2}}{2}}\\right)}\\\\ &{\\qquad\\qquad=\\left(\\operatorname{erfc}\\left[{\\frac{h}{\\sqrt{2}}}\\right]\\right)^{2}\\rho_{c}+{\\mathcal{O}}(\\rho_{c}^{3}),}\\end{array}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{\\rho_{c}=\\sum_{k=1}^{N_{s}}A_{i k}A_{j k}}\\end{array}$ Therefore, if $|\\rho|\\ll1$ and $|\\rho_{c}|\\ll1$ , the error $\\epsilon[W]$ is written as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\epsilon[W]=\\frac{1}{N_{y}}\\left\\|B\\right\\|_{F}^{2}+\\frac{\\alpha^{2}}{N_{y}}\\mathrm{tr}\\left[W A A^{T}W^{T}\\right]-\\frac{2\\alpha}{N_{y}}\\mathrm{tr}[B A^{T}W^{T}],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the input activity parseness $\\alpha$ follows $\\begin{array}{r}{\\alpha=\\mathrm{erfc}\\left[\\frac{h}{\\sqrt{2}}\\right]}\\end{array}$ Under this losfunction, gradient descent from $W=W_{o}$ converges to ", "page_idx": 20}, {"type": "equation", "text": "$$\nW=W_{o}(I-U U^{T})+\\frac{1}{\\alpha}B A^{+},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $A=U\\Lambda V^{T}$ is the singular value decomposition of $A$ . Therefore, from $W_{o}=O$ , the weights after task 1 and task 2 become ", "page_idx": 20}, {"type": "equation", "text": "$$\nW_{1}=\\frac{1}{\\alpha}\\widetilde{W}_{1},\\quad W_{2}=\\frac{1}{\\alpha}\\widetilde{W}_{2},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widetilde{W}_{1}\\equiv B_{1}A_{1}^{+},\\quad\\widetilde{W}_{2}\\equiv B_{1}A_{1}^{+}\\left[I-U_{2}U_{2}^{T}\\right]+B_{2}A_{2}^{+}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This means that for both tasks $(\\mu,\\nu=1,2)$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\epsilon_{\\mu}[W_{\\nu}]=\\frac{1}{N_{y}}\\left\\|\\boldsymbol{B}_{\\mu}\\right\\|_{F}^{2}+\\frac{1}{N_{y}}\\mathrm{tr}\\left[\\widetilde{W}_{\\nu}A_{\\mu}A_{\\mu}^{T}\\widetilde{W}_{\\nu}^{T}\\right]-\\frac{2}{N_{y}}\\mathrm{tr}[B_{\\mu}A_{\\mu}^{T}\\widetilde{W}_{\\nu}^{T}],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "implying that the error is invariant against input sparseness $\\alpha$ . Thus, in our problem setting, input sparsification via soft-thresholding doesn't influence knowledge transfer or retention. It also implies that from energy efficiency perspective, soft-thresholding is beneficial because it reduces the number of active neurons while preserving its learning proficiency. ", "page_idx": 20}, {"type": "text", "text": "C Weight regularization ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1  Weight regularization in Euclidean metric ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Let us next consider a weight regularization approach by introducing L2 regularization with respect to the weight learned in the previous task. The loss function $\\ell_{\\mu}$ for the $\\mu$ -th task is then given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\ell_{\\mu}=\\frac{1}{2}\\left\\|B_{\\mu}-W A_{\\mu}\\right\\|_{F}^{2}+\\frac{\\lambda}{2}\\left\\|W-W_{\\mu-1}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "When $\\lambda>0$ , there exists a unique solution: ", "page_idx": 20}, {"type": "equation", "text": "$$\nW=(B_{\\mu}A_{\\mu}^{T}+\\lambda W_{\\mu-1})(A_{\\mu}A_{\\mu}^{T}+\\lambda I_{x})^{-1}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, from zero-initialization, $W_{1}$ and $W_{2}$ become ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{W_{1}}={B_{1}}{A_{1}^{T}}\\left({A_{1}}{A_{1}^{T}}+\\lambda{I_{x}}\\right)^{-1}}\\\\ &{{W_{2}}=\\left({B_{2}}{A_{2}^{T}}+\\lambda{B_{1}}{A_{1}^{T}}[A_{1}{A_{1}^{T}}+\\lambda{I_{x}}]^{-1}\\right){(A_{2}{A_{2}^{T}}+\\lambda{I_{x}})^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Under $N_{x}\\gg N_{s}$ , the inverse term is approximated by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left(A A^{T}+\\lambda I_{x}\\right)^{-1}=\\displaystyle\\frac1\\lambda I_{x}-\\frac1{\\lambda^{2}}A\\left(I_{s}+\\frac1\\lambda A^{T}A\\right)^{-1}A^{T}}\\\\ {\\displaystyle~~\\approx\\frac1\\lambda\\left(I_{x}-\\frac{N_{s}}{N_{x}+\\lambda N_{s}}A A^{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "image", "img_path": "bE7GWLQzkM/tmp/33a8557cb9536c9371554a5613c266135935e68e5f105985479944f02a435681.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 9: Weight regularization in Euclidean metric. (A,B) Optimal regularizer coefficient $\\gamma$ that maximizes the transfer performance (A), and the maximum performance at the optimal $\\gamma\\left(\\mathbf{B}\\right)$ under various $(\\rho_{a},\\,\\rho_{b})$ pairs. (C,D) Optimal regularizer coefficient for retention $\\mathbf{\\Psi}(\\mathbf{C})$ , and the resultant performance $({\\bf D})$ . Panel C is the same with Fig. 5C (replicated for completeness). ", "page_idx": 21}, {"type": "text", "text": "In the frst line, we used Woodbury matrix identity, and in the second line, we used $\\begin{array}{r}{A^{T}A\\approx\\frac{N_{x}}{N_{s}}I_{s}}\\end{array}$ (see Eq. 99). Let us introduce normalized regularization amplitude $\\gamma$ and $\\tilde{\\gamma}$ by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\gamma\\equiv\\frac{N_{x}}{N_{x}+\\lambda N_{s}},\\quad\\widetilde\\gamma\\equiv\\frac{N_{s}}{N_{x}+\\lambda N_{s}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Under these approximations, $W_{1}$ and $W_{2}$ simplify to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{1}\\approx\\frac{1}{\\lambda}B_{1}A_{1}^{T}\\left(1-\\widetilde{\\gamma}A_{1}A_{1}^{T}\\right)\\approx\\widetilde{\\gamma}B_{1}A_{1}^{T},}\\\\ &{W_{2}\\approx\\frac{1}{\\lambda}\\left(B_{2}A_{2}^{T}+\\lambda\\widetilde{\\gamma}B_{1}A_{1}^{T}\\right)\\left(I_{x}-\\widetilde{\\gamma}A_{2}A_{2}^{T}\\right)\\approx\\widetilde{\\gamma}\\left(B_{1}A_{1}^{T}+B_{2}A_{2}^{T}\\right)-\\widetilde{\\gamma}^{2}B_{1}A_{1}^{T}A_{2}A_{2}^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Here, we further aplied $\\begin{array}{r}{A^{T}A\\approx\\frac{N_{x}}{N_{s}}I_{s}}\\end{array}$ Not thatq ,ult abes Assumptions I & II. ", "page_idx": 21}, {"type": "text", "text": "Transfer performance  The transfer performance $\\Delta\\epsilon_{T F}$ follows ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Delta\\epsilon_{T F}=\\frac{1}{N_{y}}\\left\\langle\\|B_{2}\\|_{F}^{2}\\right\\rangle-\\frac{1}{N_{y}}\\left\\langle\\left\\|B_{2}-\\widetilde{\\gamma}B_{1}A_{1}^{T}A_{2}\\right\\|_{F}^{2}\\right\\rangle}\\\\ {\\displaystyle=\\frac{2\\widetilde{\\gamma}}{N_{y}}\\left\\langle\\mathrm{tr}[B_{2}^{T}B_{1}A_{1}^{T}A_{2}]\\right\\rangle-\\frac{\\widetilde{\\gamma}^{2}}{N_{y}}\\left\\langle\\left\\|B_{1}A_{1}^{T}A_{2}\\right\\|_{F}^{2}\\right\\rangle}\\\\ {\\displaystyle=\\frac{2\\widetilde{\\gamma}\\rho_{b}}{N_{s}}\\left\\langle\\mathrm{tr}[A_{1}^{T}A_{2}]\\right\\rangle-\\frac{\\widetilde{\\gamma}^{2}}{N_{s}}\\left\\langle\\left\\|A_{1}^{T}A_{2}\\right\\|_{F}^{2}\\right\\rangle}\\\\ {\\displaystyle=\\gamma\\rho_{a}\\left(2\\rho_{b}-\\gamma\\rho_{a}\\right)+\\mathcal{O}\\left(\\frac{N_{s}}{N_{x}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In the last line, we used Eq. 33 to estimate $\\left\\langle\\left\\Vert A_{1}^{T}A_{2}\\right\\Vert_{F}^{2}\\right\\rangle$ . Notably, the expression of the transfer performance, $\\Delta\\epsilon_{T F}$ , coincides with Eq. 5a under $\\gamma\\,\\rightarrow\\,\\alpha$ , indicating that in terms of transfer performance,the weight regularization with amplitude $\\textstyle{\\frac{N_{x}}{N_{s}}}\\left({\\frac{1}{\\alpha}}-1\\right)$ is equivalent to random activity gating with gating level $\\alpha$ . Figs. 9A and B show the optimal regularizer coefficient $\\gamma$ that maximizes transfer performance and the resultant performance. ", "page_idx": 21}, {"type": "text", "text": "Retention performance   The average error on task 1 after learning task 2 is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\epsilon_{1}[W_{2}]\\rangle_{A,B}}\\\\ &{=\\left\\langle\\frac{1}{N_{y}}\\left\\|B_{1}-\\left(\\tilde{\\gamma}[B_{1}A_{1}^{T}+B_{2}A_{2}^{T}]-\\tilde{\\gamma}^{2}B_{1}A_{1}^{T}A_{2}A_{2}^{T}\\right)A_{1}\\right\\|_{F}^{2}\\right\\rangle}\\\\ &{=\\frac{1}{N_{y}}\\left\\langle\\left\\|B_{1}(I-\\tilde{\\gamma}A_{1}^{T}A_{1})\\right\\|_{F}^{2}\\right\\rangle+\\frac{2\\tilde{\\gamma}}{N_{y}}\\left\\langle\\mathrm{tr}\\left[(I-\\tilde{\\gamma}A_{1}^{T}A_{1})B_{1}^{T}(\\tilde{\\gamma}B_{1}A_{1}^{T}A_{2}-B_{2})A_{2}^{T}A_{1}\\right]\\right\\rangle}\\\\ &{\\quad+\\frac{\\widetilde{\\gamma}^{2}}{N_{y}}\\left\\langle\\left\\|(\\tilde{\\gamma}B_{1}A_{1}^{T}A_{2}-B_{2})A_{2}^{T}A_{1}\\right\\|_{F}^{2}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Taking the expectation over $A_{1},A_{2},B_{1},B_{2}$ , up to the leading order with respect to N $\\frac{N_{s}}{N_{x}}$ ,wehave ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{N_{y}}\\left\\langle\\left\\|B_{1}(I-\\widetilde{\\gamma}A_{1}^{T}A_{1})\\right\\|_{F}^{2}\\right\\rangle=\\displaystyle\\frac{1}{N_{y}}\\left\\langle\\left\\|B_{1}\\right\\|_{F}^{2}-2\\widetilde{\\gamma}\\mathrm{tr}[B_{1}^{T}B_{1}A_{1}^{T}A_{1}]+\\widetilde{\\gamma}^{2}\\left\\|B_{1}A_{1}^{T}A_{1}\\right\\|_{F}^{2}\\right\\rangle}\\\\ &{\\displaystyle=1-\\frac{2\\widetilde{\\gamma}}{N_{s}}\\left\\langle\\mathrm{tr}[A_{1}^{T}A_{1}]\\right\\rangle+\\frac{\\widetilde{\\gamma}^{2}}{N_{s}}\\left\\langle\\left\\|A_{1}^{T}A_{1}\\right\\|_{F}^{2}\\right\\rangle}\\\\ &{\\displaystyle=\\left(1-\\gamma\\right)^{2}+\\mathcal{O}\\left(\\frac{N_{s}}{N_{x}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\widetilde{\\gamma}}{N_{y}}\\left\\langle\\mathrm{tr}\\left[(I-\\widetilde{\\gamma}A_{1}^{T}A_{1})B_{1}^{T}(\\widetilde{\\gamma}B_{1}A_{1}^{T}A_{2}-B_{2})A_{2}^{T}A_{1}\\right]\\right\\rangle}\\\\ &{=\\frac{\\widetilde{\\gamma}}{N_{s}}\\left\\langle\\mathrm{tr}\\left[(I-\\widetilde{\\gamma}A_{1}^{T}A_{1})(\\widetilde{\\gamma}A_{1}^{T}A_{2}-\\rho_{b}I)A_{2}^{T}A_{1}\\right]\\right\\rangle}\\\\ &{=\\rho_{a}\\gamma(1-\\gamma)(\\gamma\\rho_{a}-\\rho_{b})+\\mathcal{O}\\left(\\frac{N_{s}}{N_{x}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{N_{y}}\\widetilde{\\gamma}^{2}\\left\\langle\\big\\|(\\widetilde{\\gamma}B_{1}A_{1}^{T}A_{2}-B_{2})A_{2}^{T}A_{1}\\big\\|_{F}^{2}\\right\\rangle}\\\\ &{=\\frac{\\widetilde{\\gamma}^{2}}{N_{s}}\\left\\langle\\mathrm{tr}[A_{1}^{T}A_{2}A_{2}^{T}A_{1}]-2\\widetilde{\\gamma}\\rho_{b}\\mathrm{tr}[A_{1}^{T}A_{2}A_{1}^{T}A_{2}A_{2}^{T}A_{1}]+\\widetilde{\\gamma}^{2}\\mathrm{tr}[A_{1}^{T}A_{2}A_{2}^{T}A_{1}A_{1}^{T}A_{2}A_{2}^{T}A_{1}]\\right\\rangle}\\\\ &{=\\rho_{a}^{2}\\gamma^{2}\\left(1-2\\rho_{a}\\rho_{b}\\gamma+\\rho_{a}^{2}\\gamma^{2}\\right)+\\mathcal{O}\\left(\\frac{N_{s}}{N_{x}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, the error is written as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\epsilon_{1}[W_{2}]\\rangle=(1-\\gamma)^{2}-2\\gamma\\rho_{a}(1-\\gamma)(\\rho_{b}-\\gamma\\rho_{a})+\\gamma^{2}\\rho_{a}^{2}(1-2\\gamma\\rho_{a}\\rho_{b}+\\gamma^{2}\\rho_{a}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The optimal $\\gamma$ for each task similarity $(\\rho_{a},\\rho_{b})$ and the resultant retention performance are plotted in Figs. 9C and D. ", "page_idx": 22}, {"type": "text", "text": "C.2  Elastic weight regularization ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In the previous section, we regularized the change in the weight matrix in the Euclidean space. However, previous works indicate that the weight should be regularized using the Fisher information matrix (FIM) as the metric [30, 67]. Let us construct an inference model by ${\\pmb y}=W{\\pmb x}+\\sigma{\\pmb\\xi}$ ,where $\\xi$ is a zero mean Gaussian random variable. Given input-target pair $\\mathbf{\\nabla}x,y$ , the likelihood of weight $W$ is ", "page_idx": 22}, {"type": "equation", "text": "$$\np(W|x,y)\\propto p(x,y|W)p(W)\\propto\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left\\|W x-y\\right\\|^{2}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, $(i j,k l)$ -th component of FIM becomes ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\langle\\frac{\\partial^{2}}{\\partial w_{i j}w_{k l}}\\left(-\\log p\\left(W|x,y\\right)\\right)\\right\\rangle_{s}=\\frac{1}{2\\sigma^{2}}\\delta_{i k}\\left\\langle\\sum_{m}\\sum_{n}a_{j m}a_{l n}s_{m}s_{n}\\right\\rangle_{s}=\\frac{1}{2\\sigma^{2}}\\delta_{i k}\\sum_{n}a_{j n}a_{l n},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and the quadratic weight regularization in the Fisher information metric follows ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{i j}\\sum_{k l}\\left(\\delta_{i k}\\sum_{n}a_{j n}a_{l n}\\right)\\left(w_{i j}-w_{i j}^{o}\\right)(w_{k l}-w_{k l}^{o})=\\left\\|(W-W_{o})A\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, the loss function for this weight regularizer is written as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\ell_{\\mu}=\\frac{1}{2}\\left\\|B_{\\mu}-W A_{\\mu}\\right\\|_{F}^{2}+\\frac{\\lambda}{2}\\left\\|(W-W_{\\mu-1})A_{\\mu-1}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As we will see, this loss function has different forms of solution depending on whether $\\rho_{a}<1$ Or $\\rho_{a}=1$ . We thus consider these two conditions separately below. ", "page_idx": 22}, {"type": "text", "text": "C.2.1  Variable features $(\\rho_{a}<1)$ ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Taking gradient with respect to $W$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\partial\\ell_{\\mu}}{\\partial W}=-(B_{\\mu}-W A_{\\mu})A_{\\mu}^{T}+\\lambda(W-W_{\\mu-1})A_{\\mu-1}A_{\\mu-1}^{T},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "indicating that $W$ is written as $W\\,=\\,W_{\\mu-1}\\,+\\,Q\\widetilde A^{T}$ where $Q$ is a $N_{y}\\,\\times\\,2N_{s}$ matrix and $\\widetilde{A}\\equiv$ $[A_{\\mu-1},A_{\\mu}]$ is an $N_{x}\\times2N_{s}$ matrix. Solving $\\begin{array}{r}{\\frac{\\partial\\ell_{\\mu}}{\\partial W}=0}\\end{array}$ weget ", "page_idx": 23}, {"type": "equation", "text": "$$\nQ\\left(\\!\\!\\begin{array}{c c}{{\\lambda A_{\\mu-1}^{T}A_{\\mu-1}}}&{{A_{\\mu-1}^{T}A_{\\mu}}}\\\\ {{\\lambda A_{\\mu}^{T}A_{\\mu-1}}}&{{A_{\\mu}^{T}A_{\\mu}}}\\end{array}\\!\\!\\right)\\left(\\!\\!\\begin{array}{c}{{A_{\\mu-1}^{T}}}\\\\ {{A_{\\mu}^{T}}}\\end{array}\\!\\!\\right)=(O\\!\\!\\!\\begin{array}{c c}{{B_{\\mu}-W_{\\mu-1}A_{\\mu})\\left(\\!\\!\\begin{array}{c}{{A_{\\mu-1}^{T}}}\\\\ {{A_{\\mu}^{T}}}\\end{array}\\!\\!\\right),}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Multiplying both sides with $(A_{\\mu-1}\\ A_{\\mu})$ from the right side, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{Q\\left(\\!\\!\\begin{array}{c c}{{\\lambda A_{\\mu-1}^{T}A_{\\mu-1}}}&{{A_{\\mu-1}^{T}A_{\\mu}}}\\\\ {{\\lambda A_{\\mu}^{T}A_{\\mu-1}}}&{{A_{\\mu}^{T}A_{\\mu}}}\\end{array}\\!\\!\\right)\\left(\\!\\!\\begin{array}{c c}{{A_{\\mu-1}^{T}A_{\\mu-1}}}&{{A_{\\mu-1}^{T}A_{\\mu}}}\\\\ {{A_{\\mu}^{T}A_{\\mu-1}}}&{{A_{\\mu}^{T}A_{\\mu}}}\\end{array}\\!\\!\\right)}}\\\\ {{}}&{{=(O\\!\\!\\!\\begin{array}{c c}{{B_{\\mu}-W_{\\mu-1}A_{\\mu})\\left(\\!\\!\\begin{array}{c c}{{A_{\\mu-1}^{T}A_{\\mu-1}}}&{{A_{\\mu-1}^{T}A_{\\mu}}}\\\\ {{A_{\\mu}^{T}A_{\\mu-1}}}&{{A_{\\mu}^{T}A_{\\mu}}}\\end{array}\\!\\!\\right)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "If $\\lambda\\neq0$ and $\\rho_{a}<1$ , two square matrices in the above equations are almost surely invertible under assumptions I & II. Therefore, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nQ=(O\\quad B_{\\mu}-W_{\\mu-1}A_{\\mu})\\left(\\!\\!\\begin{array}{c c}{{\\lambda A_{\\mu-1}^{T}A_{\\mu-1}}}&{{A_{\\mu-1}^{T}A_{\\mu}}}\\\\ {{\\lambda A_{\\mu}^{T}A_{\\mu-1}}}&{{A_{\\mu}^{T}A_{\\mu}}}\\end{array}\\!\\!\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Under $N_{x}\\gg N_{s}$ , from Eq. 99, the inverse matrix is approximated by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\binom{\\lambda A_{\\mu-1}^{T}A_{\\mu-1}}{\\lambda A_{\\mu}^{T}A_{\\mu-1}}\\ \\ \\ \\ A_{\\mu}^{T}A_{\\mu}\\bigg)^{-1}\\approx\\frac{N_{s}}{N_{x}}\\left(\\lambda J_{s}\\ \\ \\ \\ f_{s}\\atop\\lambda\\rho_{a}I_{s}\\ \\ \\ \\ I_{s}\\right)^{-1}=\\frac{N_{s}}{N_{x}(1-\\rho_{a}^{2})}\\left(\\frac{{\\frac{1}{\\lambda}}I_{s}}{-\\rho_{a}I_{s}}\\ \\ \\ -\\ \\frac{\\rho_{a}}{I_{s}}I_{s}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, $W=W_{\\mu-1}+Q\\widetilde{A}^{T}$ follows ", "page_idx": 23}, {"type": "equation", "text": "$$\nW=W_{\\mu-1}\\left(I-\\frac{N_{s}}{N_{x}(1-\\rho_{a}^{2})}A_{\\mu}\\left[A_{\\mu}^{T}-\\rho_{a}A_{\\mu-1}^{T}\\right]\\right)+\\frac{N_{s}}{N_{x}(1-\\rho_{a}^{2})}B_{\\mu}\\left(A_{\\mu}^{T}-\\rho_{a}A_{\\mu-1}^{T}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Notably, the equation above doesn't depend on the regularizer amplitude $\\lambda$ exceptfor $\\lambda\\ne\\,0$ condition.At $\\lambda\\to0$ limit, the inverse matrix term in Eq. 82 effectively becomes singular, and thus the equation above no longer holds. ", "page_idx": 23}, {"type": "text", "text": "Let us suppose the first task is learned without any weight regularization, or learned with weight regularization in the Fisher information metric imposed by an uncorrelated task. Then, we have $\\begin{array}{r}{\\bar{W_{1}}=\\frac{N_{s}}{N_{x}}B_{1}A_{1}^{T}}\\end{array}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\nW_{2}=\\frac{N_{s}}{N_{x}}B_{1}A_{1}^{T}\\left(I-\\frac{N_{s}}{N_{x}(1-\\rho_{a}^{2})}A_{2}[A_{2}^{T}-\\rho_{a}A_{1}^{T}]\\right)+\\frac{N_{s}}{N_{x}(1-\\rho_{a}^{2})}B_{2}(A_{2}^{T}-\\rho_{a}A_{1}^{T}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, the retention performance under $W_{2}$ follows ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Delta\\epsilon_{R T}=\\frac{1}{N_{y}}\\left<\\|B_{1}\\|_{F}^{2}\\right>}&{}\\\\ {-\\left.\\frac{1}{N_{y}}\\left<\\left\\|B_{1}\\left(I-\\frac{N_{s}}{N_{x}}A_{1}^{T}A_{1}\\right)+\\frac{N_{s}}{N_{x}(1-\\rho_{a}^{2})}\\left(\\frac{N_{s}}{N_{x}}B_{1}A_{1}^{T}A_{2}-B_{2}\\right)(A_{2}^{T}-\\rho_{a}A_{1}^{T})A_{1}\\right\\|_{F}^{2}\\right>.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Taking expectation over $B_{1}$ and $B_{2},\\Delta\\epsilon_{R T}$ is rewritten as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\epsilon_{R T}=1-\\frac{1}{N_{s}}\\left\\langle\\left\\Vert I-\\frac{N_{s}}{N_{x}}A_{1}^{T}A_{1}\\right\\Vert_{F}^{2}\\right\\rangle}\\\\ &{\\qquad\\quad-\\left.\\frac{2}{N_{x}(1-\\rho_{a}^{2})}\\left\\langle\\mathrm{tr}\\left[(I-\\frac{N_{s}}{N_{x}}A_{1}^{T}A_{1}^{T})\\left(\\frac{N_{s}}{N_{x}}A_{1}^{T}A_{2}-\\rho_{b}I\\right)\\left(A_{2}^{T}-\\rho_{a}A_{1}^{T}\\right)A_{1}\\right]\\right\\rangle}\\\\ &{\\qquad\\quad-\\left.\\frac{1}{N_{s}}\\left(\\frac{N_{s}}{N_{x}(1-\\rho_{a}^{2})}\\right)^{2}\\left\\langle\\left\\Vert\\frac{N_{s}}{N_{x}}A_{1}^{T}A_{2}(A_{2}^{T}-\\rho_{a}A_{1}^{T})A_{1}\\right\\Vert_{F}^{2}+\\left\\Vert(A_{2}^{T}-\\rho_{a}A_{1}^{T})A_{1}\\right\\Vert_{F}^{2}\\right\\rangle}\\\\ &{\\qquad\\quad+\\frac{2\\rho_{b}}{N_{x}}\\left(\\frac{N_{s}}{N_{x}(1-\\rho_{a}^{2})}\\right)^{2}\\left\\langle\\mathrm{tr}\\left[A_{1}^{T}(A_{2}-\\rho_{a}A_{1})A_{2}^{T}A_{1}(A_{2}^{T}-\\rho_{a}A_{1}^{T})A_{1}\\right]\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The second term is rewritten as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{N_{s}}\\left\\langle\\left\\|I-\\frac{N_{s}}{N_{x}}A_{1}^{T}A_{1}\\right\\|_{F}^{2}\\right\\rangle=1-\\frac{2}{N_{x}}\\left\\langle\\mathrm{tr}[A_{1}^{T}A_{1}]\\right\\rangle+\\frac{1}{N_{s}}\\left(\\frac{N_{s}}{N_{x}}\\right)^{2}\\left\\langle\\mathrm{tr}[A_{1}^{T}A_{1}A_{1}^{T}A_{1}]\\right\\rangle=\\frac{N_{s}+1}{N_{x}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Moreover, $(A_{2}^{T}-\\rho_{a}A_{1}^{T})A_{1}$ term also cancels out up to the leading order term because ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{N_{s}}\\left(\\frac{N_{s}}{N_{x}\\left(1-\\rho_{a}^{2}\\right)}\\right)^{2}\\left\\langle\\left\\Vert\\left(A_{2}^{T}-\\rho_{a}A_{1}^{T}\\right)A_{1}\\right\\Vert_{F}^{2}\\right\\rangle}\\\\ &{=\\frac{1}{N_{s}}\\left(\\frac{N_{s}}{N_{x}\\left(1-\\rho_{a}^{2}\\right)}\\right)^{2}\\displaystyle\\sum_{i j k l}\\left\\langle a_{j i}^{(1)}(a_{j k}^{(2)}-\\rho_{a}a_{j k}^{(1)})(a_{l k}^{(2)}-\\rho_{a}a_{l k}^{(1)})a_{l i}^{(1)}\\right\\rangle}\\\\ &{=\\frac{1}{N_{s}}\\left(\\frac{N_{s}}{N_{x}\\left(1-\\rho_{a}^{2}\\right)}\\right)^{2}\\displaystyle\\sum_{i j k l}\\left\\langle a_{j i}^{(1)}a_{l i}^{(1)}\\right\\rangle\\left\\langle(a_{j k}^{(2)}-\\rho_{a}a_{j k}^{(1)})(a_{l k}^{(2)}-\\rho_{a}a_{l k}^{(1)})\\right\\rangle}\\\\ &{=\\frac{1}{N_{s}}\\left(\\frac{N_{s}}{N_{x}\\left(1-\\rho_{a}^{2}\\right)}\\right)^{2}\\displaystyle\\sum_{i j k l}\\delta_{j l}\\frac{1-\\rho_{a}^{2}}{N_{x}^{2}}=\\frac{N_{s}}{N_{x}\\left(1-\\rho_{a}^{2}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Similarly, the trace terms are also cancelled out up to the leading order term: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{N_{x}}\\left(\\frac{N_{x}}{N_{x}(1-\\rho_{a}^{2})}\\right)^{2}\\left\\langle\\mathrm{tr}[A_{1}^{T}(A_{2}^{T}-\\rho_{a}A_{1})A_{2}^{T}A_{1}(A_{2}^{T}-\\rho_{a}A_{1}^{T})A_{1}]\\right\\rangle}\\\\ &{=\\frac{1}{N_{x}}\\left(\\frac{N_{x}}{N_{x}(1-\\rho_{a}^{2})}\\right)^{2}\\displaystyle\\sum_{i j k l m n}\\left\\langle a_{j i}^{(1)}(a_{j k}^{(2)}-\\rho_{a}a_{j k}^{(1)})a_{l k}^{(2)}a_{l m}^{(1)}(a_{n m}^{(2)}-\\rho_{a}a_{n m}^{(1)})a_{n i}^{(1)}\\right\\rangle}\\\\ &{=\\frac{1}{N_{x}}\\left(\\frac{N_{x}}{N_{x}(1-\\rho_{a}^{2})}\\right)^{2}\\displaystyle\\sum_{i j k l m n}^{}\\left\\langle(a_{j k}^{(2)}-\\rho_{a}a_{j k}^{(1)})(a_{n m}^{(2)}-\\rho_{a}a_{n m}^{(1)})\\right\\rangle\\left\\langle a_{j i}^{(1)}a_{l k}^{(2)}a_{l m}^{(1)}a_{n i}^{(1)}\\right\\rangle}\\\\ &{=\\frac{1}{N_{x}}\\left(\\frac{N_{x}}{N_{x}(1-\\rho_{a}^{2})}\\right)^{2}\\displaystyle\\sum_{i j k l m n}^{}\\frac{1}{N_{x}}\\delta_{j n}\\delta_{k m}(1-\\rho_{a}^{2})\\frac{1}{N_{x}^{2}}\\left(\\rho_{a}+\\delta_{j l}\\delta_{i k}2\\rho_{a}\\right)}\\\\ &{=\\frac{N_{x}\\rho_{a}}{N_{x}(1-\\rho_{a}^{2})}\\left(1+\\frac{2}{N_{x}N_{x}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\nabla_{x}(1-\\rho_{a}^{2})}\\left\\langle\\mathrm{tr}[(I-\\frac{N_{s}}{N_{x}}A_{1}^{T}A_{1})(\\frac{N_{s}}{N_{x}}A_{1}^{T}A_{2}-\\rho_{b}I)(A_{2}^{T}-\\rho_{a}A_{1}^{T})A_{2}]\\right\\rangle}\\\\ &{=\\frac{1}{N_{x}(1-\\rho_{a}^{2})}\\displaystyle\\sum_{i j k l}\\left\\langle\\left(\\delta_{i j}-\\frac{N_{s}}{N_{x}}\\sum_{m}a_{m i}^{(1)}a_{m j}^{(1)}\\right)\\left(\\frac{N_{s}}{N_{x}}\\sum_{n}a_{n j}^{(1)}a_{n k}^{(2)}-\\rho_{b}\\delta_{j k}\\right)\\left(a_{l k}^{(2)}-\\rho_{a}a_{l k}^{(1)}\\right)a_{l i}^{(2)}\\right\\rangle=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, we get $\\begin{array}{r}{\\Delta\\epsilon_{R T}=1-\\mathcal{O}\\left(\\frac{N_{s}}{N_{x}\\left(1-\\rho_{a}^{2}\\right)}\\right)}\\end{array}$ (N(-). Therefore,if Ns <(1 - p)N, we have \u25b3T  1 regardless of task similarity $\\rho_{a}$ and $\\rho_{b}$ ", "page_idx": 24}, {"type": "text", "text": "C.2.2  Fixed features $(\\rho_{a}=1)$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "When $\\rho_{a}=1$ \uff0c $A_{\\mu-1}=A_{\\mu}=A$ The gradient follows", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{\\partial\\ell_{\\mu}}{\\partial W}=\\left[-\\left(B_{\\mu}-W A\\right)+\\lambda(W-W_{\\mu-1})A\\right]A^{T},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and the weight $W$ is written as $W=W_{\\mu-1}+Q U^{T}$ , where $U$ is defined by SVD of A: $A=U\\Lambda V^{T}$ Solving W $\\begin{array}{r}{\\frac{\\partial\\ell_{\\mu}}{\\partial W}=0}\\end{array}$ , we get ", "page_idx": 24}, {"type": "equation", "text": "$$\nW=W_{\\mu-1}\\left(I-\\frac{1}{1+\\lambda}U U^{T}\\right)+\\frac{1}{1+\\lambda}B_{\\mu}A^{+}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, the weight after task 1 and 2 follow ", "page_idx": 24}, {"type": "equation", "text": "$$\nW_{1}=\\frac{1}{1+\\lambda}B_{1}A^{+},\\quad W_{2}=\\frac{1}{1+\\lambda}B_{1}A^{+}\\left(I-\\frac{1}{1+\\lambda}U U^{T}\\right)+\\frac{1}{1+\\lambda}B_{2}A^{+}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, the transfer performance is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\Delta\\epsilon_{T F}=\\displaystyle\\frac{1}{N_{y}}\\left\\langle\\left\\|B_{2}\\right\\|^{2}\\right\\rangle-\\displaystyle\\frac{1}{N_{y}}\\left\\langle\\left\\|B_{2}-W_{1}A_{2}\\right\\|^{2}\\right\\rangle\\,}}\\\\ {{=\\displaystyle\\frac{2\\rho_{b}}{1+\\lambda}-\\displaystyle\\frac{1}{(1+\\lambda)^{2}}=\\gamma_{f}\\left(2\\rho_{b}-\\gamma_{f}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In the last line, we defined $\\gamma_{f}$ by $\\begin{array}{r}{\\gamma_{f}\\equiv\\frac{1}{1+\\lambda}}\\end{array}$ . Similarly, the retention performance is written as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\epsilon_{R T}=\\displaystyle\\frac{1}{N_{y}}\\left\\langle\\|B_{1}\\|_{F}^{2}\\right\\rangle-\\displaystyle\\frac{1}{N_{y}}\\left\\langle\\|B_{1}-W_{2}A\\|_{F}^{2}\\right\\rangle}\\\\ &{\\qquad=\\displaystyle\\frac{1}{N_{y}}\\left\\langle\\|B_{1}\\|_{F}^{2}\\right\\rangle-\\displaystyle\\frac{1}{N_{y}}\\left\\langle\\left\\|\\left(1-\\displaystyle\\frac{\\lambda}{(1+\\lambda)^{2}}\\right)B_{1}-\\frac{1}{1+\\lambda}B_{2}\\right\\|_{F}^{2}\\right\\rangle}\\\\ &{\\qquad=2\\gamma_{f}(1-\\gamma_{f})^{2}-\\gamma_{f}^{4}+2\\gamma_{f}\\left[(1-\\gamma_{f})^{2}-\\gamma_{f}^{2}\\right]\\rho_{b}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "If the first task is learned without weight regularization, the retention performance instead becomes: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Delta\\epsilon_{R T}=\\frac{1}{N_{y}}\\left<\\left\\Vert\\boldsymbol{B}_{1}\\right\\Vert_{F}^{2}\\right>-\\frac{1}{N_{y}}\\left<\\left\\Vert\\frac{1}{1+\\lambda}(\\boldsymbol{B}_{1}-\\boldsymbol{B}_{2})\\right\\Vert_{F}^{2}\\right>=1-2\\gamma_{f}^{2}(1-\\rho_{b}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "D  Properties of very tall zero-mean random matrices ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Our simple analytical results rely on the assumption that matrix $A\\,\\in\\,\\mathbb{R}^{N_{x}\\times N_{s}}$ is very tall(i.e., $N_{x}\\gg N_{s})$ 0, and each element of $A$ is sampled independently from a normal distribution with mean zero and a finite variance. ", "page_idx": 25}, {"type": "text", "text": "Given $\\begin{array}{r}{A_{i j}\\sim\\mathcal{N}(0,\\frac{1}{N_{s}})}\\end{array}$ ,we have $\\begin{array}{r}{\\left\\langle\\frac{1}{N_{x}}A^{T}A\\right\\rangle_{A}=\\frac{1}{N_{s}}I_{s}}\\end{array}$ . The element-wise deviation from the mean is evaluated as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\langle\\left(\\left[\\frac{1}{N_{x}}A^{T}A-\\frac{1}{N_{s}}I_{s}\\right]_{i j}\\right)^{2}\\right\\rangle_{A}=\\frac{1+\\delta_{i j}}{N_{x}N_{s}^{2}},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "implying that $\\begin{array}{r}{\\left\\langle\\left\\lvert\\frac{1}{N_{x}}A^{T}A-\\frac{1}{N_{s}}I_{s}\\right\\rvert\\right\\rvert_{F}^{2}\\right\\rangle_{A}=\\frac{1}{N_{x}}\\left(1+\\frac{1}{N_{s}}\\right)}\\end{array}$ Therefore at $N_{x}\\gg N_{s}$ limit, $\\scriptstyle{\\frac{1}{N_{x}}}A^{T}A$ approximately follows ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{N_{x}}A^{T}A\\approx\\frac{1}{N_{s}}I_{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Even in the presence of random gating, if the effective matrix is still tall (i.e., $\\alpha N_{x}\\gg N_{s})$ \uff0csimilar approximations hold. Given a diagonal matrix $D$ whose diagonal components are sampled independently from a Bernoulli distribution with rate $\\alpha$ ,at $\\alpha N_{x}\\gg N_{s}$ limit, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{\\alpha N_{x}}(D A)^{T}D A\\approx\\frac{1}{N_{s}}I_{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This result implies that, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(D A)^{+}\\approx\\frac{N_{s}}{\\alpha N_{x}}(D A)^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Moreover, denoting the SVD of $D A$ by $\\begin{array}{r}{D A=U\\Sigma V^{T}}\\end{array}$ ,at $\\alpha N_{x}\\gg N_{s}$ limit, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{U U^{T}\\approx\\frac{N_{s}}{\\alpha N_{x}}D A A^{T}D.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This s because at $\\alpha N_{x}\\gg N_{s}$ lmit frqt $\\begin{array}{r}{\\frac{1}{\\alpha N_{x}}D A(D A)^{T}}\\end{array}$ .s concentrated at $\\begin{array}{r}{\\lambda=\\frac{1}{N_{s}}}\\end{array}$ N\u3002, and thus, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{N_{s}}{\\alpha N_{x}}D A A^{T}D=\\frac{N_{s}}{\\alpha N_{x}}U\\Sigma^{2}U^{T}\\approx U U^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "image", "img_path": "bE7GWLQzkM/tmp/b73715a8161780dc2697b49f084ba0f50e5fd8ecc45b7d490da197747a739fe6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 10: Permuted MNIST with a latent variable. (A) Learning curves under $(\\rho_{a},\\rho_{b})=(0.8,0.2)$ $({\\bf{B}})$ Knowledge transfer under various task similarities. (C) Comparison of the retention performance measured after 10 epochs (black) and 100 epochs (gray) of training. $({\\bf D})$ Retention performance after 100 epochs of training with task 2. After 100 epochs of training, the system still exhibits strong asymmetric dependence on the feature and readout similarities, but the non-monotonic dependence on the feature similarity under $\\rho_{b}\\approx1$ region disappears. $(\\mathbf{E},\\mathbf{F})$ Retention performance under the weight regularization in the Fisher information metric (FIM) with a layer-wise approximation (panel E) and a diagonal approximation (panel F), under $(\\rho_{a},\\rho_{b})=(0.5,0.5)$ . Unlike Figs. 7F-H in the main figure, we adjusted the regularizer amplitude of two layers independently. As in Fig. 7, error bars in panel A-C represent standard errors over 10 random seeds. ", "page_idx": 26}, {"type": "text", "text": "E Numerical methods ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Numerical experiments were conducted in standard laboratory GPUs and CPUs. Source codes for all numerical results are made publicly available at https: //github. com/nhiratani/transfer_ retention_model. ", "page_idx": 26}, {"type": "text", "text": "E.1  Implementation of linear teacher-student models ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Numerical results in Figs. 2-6 and 8-9 were implemented as below. We set the latent variable dimensionality $N_{s}=30$ , the input width $N_{x}=3000$ , and the output width $N_{y}=10$ . The student weight $W$ was initialized as the zero matrix, and updated with the full gradient descent with learning rate $\\eta=0.001$ . We trained the network with the first task for $N_{e p o c h}$ epochs, and then retrained it with the second task for another $N_{e p o c h}$ epochs. We used $N_{e p o c h}=100$ for the vanilla model and the activity gating models, but we used $N_{e p o c h}=500$ for other models. Unless stated otherwise, error bars represent standard deviation over 10 random seeds. Average performances in Figs. 3D, 4B-D, 5D, and 6CD were estimated by taking average over 100 pairs of $(\\rho_{a},\\rho_{b})$ , sampled uniformly from $\\rho_{a},\\rho_{b}\\in[0,1]$ ", "page_idx": 26}, {"type": "text", "text": "In the input soft-thresholding model, we estimated the gradient in a sample-based manner because the exact gradient is not tractable due to nonlinearity. We estimated the gradient from 100o0 random samples at each iterations, then updated the model for 5000 iterations with learning $\\eta=0.01$ ", "page_idx": 26}, {"type": "text", "text": "E.2 Implementation of permuted MNIST with latent ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Data generation  We used permuted MNIST dataset [34, 22], a common benchmark for continual learning, but with addition of the latent space. We constructed a four dimensional latent space $\\pmb{s}$ using the binary representation of digits, $\\pmb{s}_{0}=\\dot{[0,0,0,0]}^{T}$ $\\pmb{s}_{1}=[0,0,0,1]^{T}$ , and so on. The target output was generated by a projection of the latent variable $\\pmb{s}$ to a ten-dimensional space, $\\begin{array}{r}{\\pmb{y}^{*}=\\pmb{\\bar{B(s-\\frac{1}{2}^{}}}\\mathbf{1)}}\\end{array}$ \uff0c where $B$ is a $10\\times4$ matrix generated randomly. The elements of matrix $B_{1}$ for the first task were sampled independently from a Gaussian distribution with mean zero and variance one. For the second task, we introduced readout similarity by keeping some of the elements while resampling other elements. We defined the readout similarity $\\rho_{b}$ by the fraction of the elements kept the same between two tasks. ", "page_idx": 26}, {"type": "image", "img_path": "bE7GWLQzkM/tmp/bde657556c94ab86bb9b1717ec8959fbd9c6f17d2c04fa3c2c2107fdfc21e248.jpg", "img_caption": ["Figure 11: Effect of layer-wise weight regularization in deep feedforward networks solving permuted MNIST. (A) The transfer performance under three forms of weight regularization as a function of the regularization amplitude. (B-D) The same as A, but the retention performance were plotted. The panels are parallel to Fig. 7E-H, but here, we used a network with 3 hidden layers instead of a one-hidden layer network used for Fig. 7. When the regularizer amplitude is larger than $10^{2}$ ,we observed instability in learning dynamics. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Feature similarity was introduced by permuting the input pixels as done previously [22]. We used the vanilla MNIST images for the first task and permuted some of the pixels in the second task depending on the feature similarity $\\rho_{a}$ ", "page_idx": 27}, {"type": "text", "text": "Model implementationWe used one hidden layer neural network with ReLU non-linearity in the hidden layer: $\\pmb{y}=\\pmb{b}_{2}+W_{2}\\mathrm{ReLU}\\left[\\pmb{b}_{1}+W_{1}\\pmb{x}\\right]$ We set the hidden layer width to $N_{h}=1500$ . The input and output widths were set to $N_{x}=784$ and $N_{y}=10$ . We initialized the first weight $W_{1}$ and the bias $b_{1}$ by sampling each weight independently from a Gaussian distribution with mean zero and variance N. . We initialized the second weight $W_{2}$ and the bias $b_{2}$ in the same manner, but with $\\frac{1}{N_{y}^{2}}$   \nrich regime [53]. We set the mini-batch size to 300 and the learning rate to $\\eta=0.01$ , and trained the network for 100 epochs per task. The retention performance was measured after 10 epochs of training with task 2 except for Figs. 10 C, $10\\,\\mathrm{D}$ and $11\\;\\mathrm{A-D}$ where the retention was evaluated after 100 epochs. In the learning of task 2, the test error typically drops significantly in the first 10 epochs and shifts to gradual improvement afterward (red line in Fig. 10A). Notably, the asymmetric task similarity dependence of the retention performance was observed consistently both after a short and long training on the second task (Figs. 7B and 10D). ", "page_idx": 27}, {"type": "text", "text": "Weight regularization in the Fisher information metric From the same argument made in section C.2, FIM of a noise-free model is approximated by the Hessian of the mean squared error. Let us consider a vanilla feedforward neural network with depth $K$ ,where hidden layer activity $h_{1},...,h_{K-1}$ and theoutput $\\textit{\\textbf{y}}$ follow ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{h_{k}=\\phi\\left(b_{k}+W_{k}h_{k-1}\\right),\\mathrm{~for~}k=1,...,K-1}\\\\ {y=b_{K}+W_{K}h_{k-1},}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\phi$ is an element-wise rectified linear function. For brevity, we denote $\\nabla\\phi_{\\mu}$ as a diagonal matrix where the diagonal components represent element-wise derivative $\\phi^{\\prime}(b_{k}+\\dot{W}_{k}h_{k-1})$ .Givenloss function $\\begin{array}{r}{\\ell=\\frac{1}{2}\\left\\|y-y^{*}\\right\\|^{2}}\\end{array}$ , the Hessian is estimated as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\partial^{2}\\ell}{\\partial w_{i j}^{(\\mu)}\\partial w_{k l}^{(\\mu)}}=\\frac{\\partial\\ell}{\\partial w_{i j}^{(\\mu)}}\\left(\\left[\\nabla\\phi_{\\nu}W_{\\nu+1}^{T}...\\nabla\\phi_{K-1}W_{K}^{T}(\\pmb{y}-\\pmb{y}^{*})\\right]_{k}h_{\\nu-1,l}\\right)}}\\\\ &{\\approx\\left[\\nabla\\phi_{\\nu}W_{\\nu+1}^{T}...\\nabla\\phi_{K-1}W_{K}^{T}W_{K}\\nabla\\phi_{K-1}...W_{\\mu+1}\\nabla\\phi_{\\mu}\\right]_{k i}h_{\\mu-1,j}h_{\\nu-1,l}}\\\\ &{\\quad\\quad+\\left[\\mu>\\nu\\right]_{+}\\left[\\nabla\\phi_{\\nu}W_{\\nu+1}^{T}...\\nabla\\phi_{\\mu-1}\\right]_{k j}\\left[\\nabla\\phi_{\\mu}W_{\\mu+1}^{T}...W_{K}^{T}(\\pmb{y}-\\pmb{y}^{*})\\right]_{i}h_{\\nu-1,l}}\\\\ &{\\quad\\quad+\\left[\\mu<\\nu\\right]_{+}\\left[\\nabla\\phi_{\\mu}W_{\\mu+1}^{T}...\\nabla\\phi_{\\nu-1}\\right]_{i l}\\left[\\nabla\\phi_{\\nu}W_{\\nu+1}^{T}...W_{K}^{T}(\\pmb{y}-\\pmb{y}^{*})\\right]_{k}h_{\\mu-1,j}}\\\\ &{\\equiv\\widetilde{H}_{i j k l}^{(\\mu,\\nu)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In the second line, we omitted the second-order derivative $\\phi^{\\prime\\prime}$ as this term is effectively negligible under ReLU activation function. ", "page_idx": 27}, {"type": "text", "text": "Thus, distance between two weights $\\delta W=W-W_{o}$ in the metric defined by this approximated Hessian is written as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\mu,\\nu}\\sum_{i j k l}\\delta w_{i j}^{(\\mu)}\\delta w_{k l}^{(\\nu)}\\widetilde{H}_{i j k l}^{(\\mu,\\nu)}}\\\\ &{=\\displaystyle\\sum_{\\mu,\\nu}h_{\\mu-1}^{T}\\delta W_{\\mu}^{T}\\nabla\\phi_{\\mu}W_{\\mu+1}^{T}...\\nabla\\phi_{K-1}W_{K}^{T}W_{K}\\nabla\\phi_{K-1}...W_{\\nu+1}\\nabla\\phi_{\\nu}\\delta W_{\\nu}h_{\\nu-1}}\\\\ &{\\quad+\\displaystyle2\\sum_{\\mu>\\nu}h_{\\nu-1}^{T}\\delta W_{\\nu}^{T}\\nabla\\phi_{\\nu}W_{\\nu+1}^{T}...\\nabla\\phi_{\\mu-1}\\delta W_{\\mu}^{T}\\nabla\\phi_{\\mu+1}W_{\\mu+1}^{T}...W_{L}^{T}(y-y^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We define a layer-wise approximation of the weight regularization in the Fisher information metric by taking $\\mu=\\nu$ components of the distance defined above: ", "page_idx": 28}, {"type": "equation", "text": "$$\nR^{l w}[W]=\\frac{1}{2}\\sum_{\\mu=1}^{K}\\left\\langle\\|W_{K}\\nabla\\phi_{K-1}...W_{\\mu+1}\\nabla\\phi_{\\mu}\\delta W_{\\mu}\\phi_{\\mu-1}\\|^{2}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here, the expectation is taken over training data $(\\boldsymbol{x},\\boldsymbol{y}^{*})$ of the previous task (i.e., the task the network shouldn't forget). Taking the derivative with respect to $\\delta W_{k}$ for $k=1,..,K$ ,wehave ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial R^{l w}}{\\partial\\delta W_{\\mu}}=\\left\\langle\\nabla\\phi_{\\mu}W_{\\mu+1}^{T}...\\nabla\\phi_{K-1}W_{K}^{T}W_{K}\\nabla\\phi_{K-1}...W_{\\mu+1}\\nabla\\phi_{\\mu}\\delta W_{\\mu}h_{\\mu-1}h_{\\mu-1}^{T}\\right\\rangle}\\\\ {\\approx\\left\\langle\\nabla\\phi_{\\mu}W_{\\mu+1}^{T}...\\nabla\\phi_{K-1}W_{K}^{T}W_{K}\\nabla\\phi_{K-1}...W_{\\mu+1}\\nabla\\phi_{\\mu}\\right\\rangle\\delta W_{\\mu}\\left\\langle h_{\\mu-1}h_{\\mu-1}^{T}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This layer-wise approximation captures the true Fisher information metric more accurately than element-wise approximation implemented previously, while computationally less heavy than the estimation of the true metric. In the numerical estimations, we scaled the regularization terms at the $\\boldsymbol{\\mathrm{k}}$ -the layerby ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Z_{k}=\\left\\lVert\\left\\langle\\nabla\\phi_{\\mu}W_{\\mu+1}^{T}...\\nabla\\phi_{K-1}W_{K}^{T}W_{K}\\nabla\\phi_{K-1}...W_{\\mu+1}\\nabla\\phi_{\\mu}\\right\\rangle\\right\\rVert_{2}\\left\\lVert\\left\\langle h_{\\mu-1}h_{\\mu-1}^{T}\\right\\rangle\\right\\rVert_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\lVert\\cdot\\rVert_{2}$ is the spectral norm. We did not impose weight regularization on the bias parameters $b_{k}$ In the case of synapse-wise approximation (i.e., elastic regularization), the regularizer is instead Written as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle R^{s w}[W]=\\frac{1}{2}\\sum_{\\mu}\\sum_{i j}\\frac{\\partial^{2}\\ell}{\\partial(w_{i j}^{(\\mu)})^{2}}\\left(\\delta w_{i j}^{\\mu}\\right)^{2}}}\\\\ {{\\displaystyle=\\frac{1}{2}\\left<\\left[\\nabla\\phi_{\\mu}W_{\\mu+1}^{T}...\\nabla\\phi_{K-1}W_{K}^{T}W_{K}\\nabla\\phi_{K-1}...W_{\\mu+1}\\nabla\\phi_{\\mu}\\right]_{i i}h_{\\mu-1,j}^{2}\\right>\\left(\\delta w_{i j}^{(\\mu)}\\right)^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, the gradient of $R^{s w}$ with respect to $\\delta W_{\\mu}$ follows ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\partial R^{s w}}{\\partial(\\delta W_{\\mu})}=\\left\\langle\\mathrm{diag}\\left[\\nabla\\phi_{\\mu}W_{\\mu+1}^{T}...\\nabla\\phi_{K-1}W_{K}^{T}W_{K}\\nabla\\phi_{K-1}...W_{\\mu+1}\\nabla\\phi_{\\mu}\\right]\\left(h_{\\mu-1}\\odot h_{\\mu-1}\\right)^{T}\\right\\rangle\\odot\\delta W_{\\mu}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We scaled the regularization term by the largest coefficient in a layer-wise manner. ", "page_idx": 28}, {"type": "text", "text": "In the case of one-hidden layered networks, the layer-wise regularizer is written as ", "page_idx": 28}, {"type": "equation", "text": "$$\nR^{l w}[W_{1},W_{2}]=\\frac{1}{2Z_{1}}\\left\\langle\\left\\|{W_{2}\\nabla\\phi\\boldsymbol{\\delta}\\boldsymbol{W}_{1}\\boldsymbol{x}}\\right\\|^{2}\\right\\rangle+\\frac{1}{2Z_{2}}\\left\\langle\\left\\|{\\boldsymbol{\\delta}\\boldsymbol{W}_{2}\\phi}\\right\\|^{2}\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the normalization constants follow $Z_{1}\\ =\\ \\left\\|W_{2}W_{2}^{T}\\right\\|_{2}\\left\\|\\left\\langle\\pmb{x}\\pmb{x}^{T}\\right\\rangle\\right\\|_{2}$ $Z_{2}\\;=\\;\\left\\|\\left<\\phi\\phi^{T}\\right>\\right\\|_{2}$ contrast, the synapse-wise regularizer is given as ", "page_idx": 28}, {"type": "equation", "text": "$$\nR^{s w}[W_{1},W_{2}]=\\frac{1}{2Z_{1}}\\sum_{i j}\\sum_{k}\\left(w_{k i}^{(2)}\\phi_{i}^{\\prime}x_{j}\\right)^{2}\\left(\\delta w_{i j}^{(1)}\\right)^{2}+\\frac{1}{2Z_{2}}\\sum_{i j}\\phi_{j}^{2}\\left(\\delta w_{i j}^{(2)}\\right)^{2},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\begin{array}{r}{Z_{1}=\\operatorname*{max}_{i j}\\left\\langle\\sum_{k}(w_{k i}^{(2)}\\phi_{i}^{\\prime}x_{j})^{2}\\right\\rangle}\\end{array}$ and $Z_{2}=\\operatorname*{max}_{j}\\left\\langle\\phi_{j}^{2}\\right\\rangle$ . To see the potentia ffect of different normalization methods used for the layer-wise and diagonal approximations, in Figs. $10\\,\\mathrm{E}$ and F, we independently changed the regularizer amplitude for $W_{1}$ and $W_{2}$ and measured the retention performance under the two approximation methods. Even in this setting, the layer-wise approximation of the Fisher-information metric robustly outperformed the diagonal approximation, when the regulazier amplitude for $W_{2}$ was sufficiently large. ", "page_idx": 28}, {"type": "image", "img_path": "bE7GWLQzkM/tmp/e697f46560c990daf4714f744263623438f029ab1e8ab7575acceafbc6acb47b.jpg", "img_caption": ["Figure 12: Permuted MNIST with pixel and label permutations. (A,B) Transfer and classification performance measured by the classification accuracy (A) and the cross-entropy loss (B). (C) Transfer performance of random (dashed lines) and adaptive (solid lines) activity gating models. (D-F) Performance of the weight regularization in the Euclidean metric, and approximated Fisher information metrics. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "In Fig. 11, we implemented this method to a feedforward network with three all-to-all hidden layers, where hidden layer widths are set to be 784-1000-300-100-10. We used the learning rate $\\eta=0.01$ mini-batch size 300, and trained the network for 100 epochs per task. The weight regularization based on a layer-wise approximation of the Fisher-information metric, robustly outperformed both the diagonal approximation of the metric and weight regularization in the Euclidean space even in this deep network (Fig. 11B-D). ", "page_idx": 29}, {"type": "text", "text": "E.3  Implementation of permuted MNIST ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In Fig. 12, we implemented permuted MNIST without any explicit latent structure. We modulated the input feature similarity by permuting a subset of input pixels between the first and the second tasks, and modulated the readout similarity by permuting a subset of the output labels as shown in Figs. 1C and 1D. We used the same one-hidden layer network with the model above, but introduced the softmax at the output units and estimated the loss using cross-entropy loss. We set the mini-batch size to be 300, and the epoch per task as 100. The retention performance was evaluated at the end of the training of the second task using Eq. 2. ", "page_idx": 29}, {"type": "text", "text": "When transfer and retention performance were assessed based on differences in classification accuracy, both measures exhibited a positive monotonic relationship with feature and readout similarity (Fig. 12A). However, when evaluated using cross-entropy loss, the training loss function, transfer and retention performance showed a non-monotonic dependence on feature and readout similarities (Fig. 12B). Specifically, under low readout similarity, increasing feature similarity negatively impacted transfer performance. ", "page_idx": 29}, {"type": "text", "text": "Adaptive gating with a probe trial improved transfer performance (Fig. 12C vs. Figs. 7C and 4A). Additionally, weight regularization in the layer-wise approximation of the Fisher information metric yielded better retention performance compared to its diagonal approximation and weight regularization in Euclidean space. (Figs. 12D-F). ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The claims made in the fourth, fifth, and sixth paragraphs of Introduction were supported by sections 4, 5 and 6, and 7, respectively. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Please see the paragraph titled \"Limitations\" in section 8. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All assumptions and derivations of the theoretical results are provided in AppendixA-D. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Please see Appendix E. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Source codes are made publicly available at https://github.com/ nhiratani/transfer_retention_model. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Please see Appendix E. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Please see the figures and the legends of Figs. 1 and 6. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Please see Appendix E ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] Justification: The research does not contain any element that requires ethical concern. ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The work does not have any direct societal impact. ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer:[NA] .   \nJustification: The paper does not contain any data or models that have a risk for misuse. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not use existing licensed assets ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 31}]