[{"type": "text", "text": "Conformalized Time Series with Semantic Features ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Baiting Chen Zhimei Ren Department of Statistics and Data Science Department of Statistics and Data Science UCLA University of Pennsylvania brantchen@g.ucla.edu zren@wharton.upenn.edu ", "page_idx": 0}, {"type": "text", "text": "Lu Cheng Department of Computer Science University of Illinois Chicago lucheng@uic.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Conformal prediction is a powerful tool for uncertainty quantification, but its application to time-series data is constrained by the violation of the exchangeability assumption. Current solutions for time-series prediction typically operate in the output space and rely on manually selected weights to address distribution drift, leading to overly conservative predictions. To enable dynamic weight learning in the semantically rich latent space, we introduce a novel approach called Conformalized Time Series with Semantic Features (CT-SSF). CT-SSF utilizes the inductive bias in deep representation learning to dynamically adjust weights, prioritizing semantic features relevant to the current prediction. Theoretically, we show that CT-SSF surpasses previous methods defined in the output space. Experiments on synthetic and benchmark datasets demonstrate that CT-SSF significantly outperforms existing state-of-the-art (SOTA) conformal prediction techniques in terms of prediction efficiency while maintaining a valid coverage guarantee. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Uncertainty quantification is essential for reliable predictions in time series data [34, 45]. The emergence of \u2018black-box\u2019 models has intensified interest in conformal prediction (CP), a technique valued for its model-agnostic and distribution-free properties [44, 57]. Under the assumption of data exchangeability, the confidence bands provided by CP are theoretically guaranteed [43]. This reliability has led to CP\u2019s promising performance across various domains, e.g., image classification [1], graph neural networks [56], anomaly detection [22], and natural language processing [37, 16]. ", "page_idx": 0}, {"type": "text", "text": "The fundamental assumption of exchangeability in CP is often compromised in time series data due to inherent temporal dependencies, as highlighted in studies such as [53] and [48]. This violation poses significant challenges in directly applying CP to time series forecasting. Existing adaptations of CP tailored for time series (e.g., [7]) tend to produce overly conservative prediction sets or intervals too wide to provide practical insight, thus limiting their practical utility. These adaptations often necessitate manually selected weights, which curtail their generalizability across various datasets. Moreover, the practice of calculating non-conformity scores based on output space does not take full advantage of the rich representational data available in the latent space. This oversight is particularly critical given the prevalent use of neural networks (NN) in contemporary time series forecasting, which are capable of capturing deep, complex patterns in data as discussed in [55] and [39]. ", "page_idx": 0}, {"type": "text", "text": "The latent space in NN offers enhanced model interpretability and the ability to capture complex, nonlinear relationships in the data that are often invisible in the original input space ([40]). By mapping input data into a latent feature space through the feature function, the resulting representation not only supports the model in learning essential temporal patterns but also in generalizing better to unseen data by focusing on the most informative features. To overcome the inherent challenges in applying CP to time series data, our work presents a novel approach\u2014Conformalized Time Series with Semantic Features (CT-SSF)\u2014which leverages latent semantic features of NN for time series. Central to our methodology is the use of weighted errors for constructing non-conformity scores in the latent space, drawing upon rich semantic features. The weights are dynamically adjusted to prioritize semantic features more pertinent to the current prediction task. This approach ensures that the weights adapt to the changing importance of different data aspects over time. By shifting CP into the latent space of NN, our non-conformity scores achieve a deeper insight into the temporal dynamics. This not only maintains the requisite coverage of predictive intervals but also significantly improves their prediction efficiency, offering a refined tool for time series forecasting that aligns with the complexities of modern data structures. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose CP in the latent space of the underlying time-series prediction model, enabling the construction of non-conformity scores that encapsulate a more comprehensive understanding of the data and deliver tight prediction sets.   \n\u2022 We propose an adaptive weight adjustment scheme to refine the weight learning process in the latent space, accounting for distribution shifts in time-series data by emphasizing data points more relevant to the current time step.   \n\u2022 We conduct extensive experiments under both synthetic and realistic settings to corroborate the effectiveness of the proposed algorithm, showing a $10\\%{-}20\\%$ improvement over the SOTA approach for real-world data. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "CP with exchangeable data CP, pioneered by [43], has become a cornerstone in uncertainty quantification due to its model-free and distribution-free property [35]. The work in this domain can be summarized of CP into two branches: improve the efficiency of CP ([33, 13, 51, 36, 6, 27] ) and generalize CP to different settings, such as quantile regression [32], decision tree [20], random forest [21]), survival analysis [11], $\\mathrm{k}$ -nearest neighbor [30], online learning [8] and auction design [17]. Comprehensive insights into CP and its theoretical underpinnings are provided by [4]. ", "page_idx": 1}, {"type": "text", "text": "Beyond Exchangeability The foundational works of [44] and the introductory reviews by [4] describe the evolution of CP to accommodate contexts \"beyond exchangeability\". Notable extensions include [41] application to covariate shift, and innovative adaptations for dealing with label shift by [31]. The work of [41] and [7] address calibration and test set shifts by reweighting data points. [25] show how reweighting can be extended to causal inference setups for predictive inference on individual treatment effects, and [11] show how to apply these ideas in the context of censored outcomes in survival analysis. ", "page_idx": 1}, {"type": "text", "text": "CP for time series data CP for time series can be divided into two primary trends that deviate from the traditional assumption of exchangeability. The first trend focuses on adaptively adjusting the error rate $\\alpha$ during the testing phase to enhance coverage accuracy. This approach is demonstrated in works such as ([2, 3, 8, 14, 53, 26, 46, 52, 15, 38, 58]). The second trend (e.g., [41, 48, 49, 7, 23, 24, 47]) emphasizes manually assigning weights to historical non-conformity scores, giving greater importance to those that reflect the current scenario more accurately. To address the limitation of manual selection, the advanced HopCPT model, developed by [5], leverages the Modern Hopfield Network to capture temporal structure and learn the weights. The work closest to ours is the HopCPT ([5]). We advance HopCPT in developing the non-conformity score in the latent space with rich semantic features, enabling a deeper analysis and utilization of the inherent temporal patterns in the data. Alternative probabilistic frameworks like Mixture Density Networks [10] and Gaussian Processes [9] offer robust methodologies for time series forecasting beyond CP. However, these methods frequently grapple with computational limitations and lack robust theoretical backing. ", "page_idx": 1}, {"type": "text", "text": "3 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Split Conformal Prediction One of the most common approaches in CP is the split conformal prediction method [29], which starts with a trained model and assesses its efficacy on a calibration set of paired examples $\\{(X_{i},Y_{i})\\}_{i=1}^{n}$ . Central to split CP is the foundational assumption that the data are exchangeable, ensuring that the errors observed in the test set will be consistent with those from the calibration set. This allows for reliable empirical estimation of quantiles across both datasets. Define a non-conformity score function $s(x,y)$ that quantifies the disagreement between predicted and ground-truth values. The coverage guarantee of CP is defined as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nP(Y_{n+1}\\in C(X_{n+1}))\\geq1-\\alpha,\\quad C(X_{n+1})=\\{y:s(X_{n+1},y)\\leq{\\hat{q}}\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $(X_{n+1},Y_{n+1})$ is a new data point, $\\hat{q}$ is the $\\lceil(1-\\alpha)(n+1)\\rceil$ -th smallest elements among $\\{s(X_{i},Y_{i})\\}_{i=1}^{n}$ and $\\alpha$ signifies a predefined miscoverage rate. ", "page_idx": 2}, {"type": "text", "text": "Non-exchangeable Conformal Prediction Real-world settings encounter challenges such as data drift and inter-dependencies, necessitating further adaptations of CP to these non-exchangeable settings. One common solution is to use reweighting [7, 5]. For example, given a set of pre-specified weights $\\{w_{i}\\}_{i=1}^{n}$ , $w_{i}\\in[0,1]$ , the coverage guarantee for seminal work NexCP [7] is articulated as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nP(Y_{n+1}\\in C(X_{n+1}))\\geq1-\\alpha-\\sum_{i=1}^{n}\\tilde{w}_{i}d_{\\mathrm{TV}}(Z,Z^{i}),\\quad\\tilde{w}_{i}=\\frac{w_{i}}{1+\\sum_{i=1}^{N}w_{i}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $Z=(X_{1},Y_{1}),\\ldots,(X_{n},Y_{n}),(X_{n+1},Y_{n+1})$ represents a sequence of $n$ calibration examples along with a subsequent test example and $\\tilde{w}_{i}$ is the normalized weight. The term $Z^{i}$ denotes the sequence $Z$ after the $i$ -th pair $(X_{i},Y_{i})$ is swapped with the test example $(X_{n+1},Y_{n+1})$ , and $d_{\\mathrm{TV}}(Z,Z^{i})$ quantifies the dissimilarity introduced by this swap. ", "page_idx": 2}, {"type": "text", "text": "To construct prediction sets, NexCP determines the quantile threshold $\\hat{q}$ by the following equation: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{q}=\\operatorname*{inf}\\left\\{q:\\sum_{i=1}^{n}\\tilde{w}_{i}1\\{s_{i}\\leq q\\}\\geq1-\\alpha\\right\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Eq. 3 is consistent with the standard CP when all weights $\\{\\tilde{w}_{i}\\}_{i=1}^{n}=1$ . Intuitively, closer alignment to exchangeable data minimizes the calibration terms $d_{\\mathrm{TV}}(Z,Z^{i})$ , allowing for more precise calibration. Strategic weight allocation\u2014such as assigning larger weights to calibration points $\\left({x_{i},y_{i}}\\right)$ that have similar distributions in $Z$ and $Z^{i}$ and smaller weights where distributions diverge\u2014can yield tighter bounds on the prediction sets. For time series data, this suggests that greater weights should be assigned to more recent observations to reflect their increased relevance. ", "page_idx": 2}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first formally define CP for time-series prediction and then propose our framework that leverages rich semantic features in latent space modeling and employs a dynamic weighting mechanism that adjusts to temporal dependencies based on these semantic features, simultaneously achieving valid coverage and high efficiency. ", "page_idx": 2}, {"type": "text", "text": "4.1 Problem Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a multivariate time series $\\mathbf{Z}_{t}=\\left(\\mathbf{x}_{t},y_{t}\\right)$ for $t=1,\\dots,T$ , where each $\\mathbf{x}_{t}\\,\\in\\,\\mathbb{R}^{m}$ is a feature vector with $m$ dimensions and $y_{t}$ is the corresponding real-valued target. The prediction model $\\mu$ utilizes these feature vectors to produce point predictions $\\hat{y}_{t}=\\mu(\\mathbf{x}_{t})$ . The goal of CP is to construct a prediction interval $C_{\\alpha}^{t}(\\mathbf{x}_{t+1})$ that contain the true value $y_{t+1}$ with a confidence level of $1-\\alpha$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nP(Y_{t+1}\\in C_{\\alpha}^{t}(\\mathbf{x}_{t+1}))\\geq1-\\alpha,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the probability is over the randomness of $\\{\\mathbf{Z}_{s}\\}_{s\\leq t}$ and $\\mathbf{X}_{t+1}$ . This interval aims to balance reliability with informativeness\u2014beyond ensuring the coverage, it is ideal that the prediction interval is of short length. ", "page_idx": 2}, {"type": "text", "text": "A common choice of non-conformity score is the absolute errors $\\lvert y_{t}\\mathrm{~-~}\\mu(\\mathbf{x}_{t})\\rvert$ between observed values and the model\u2019s predictions [43, 32]. The prediction interval is then calculated based on the empirical $1-\\alpha$ quantile $Q_{1-\\alpha}$ of the non-conformity scores: ", "page_idx": 2}, {"type": "equation", "text": "$$\nC_{\\alpha}^{t}(\\mathbf{x}_{t+1})=\\mu(\\mathbf{x}_{t+1})\\pm Q_{1-\\alpha}\\bigl(\\{|y_{i}-\\mu(\\mathbf{x}_{i})|\\}_{i=1}^{t}\\cup\\{\\infty\\}\\bigr).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "4.2 Non-conformity Score in the Latent Space ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given the rich semantic information in the latent space, we investigate the effectiveness of using semantic features for CP with time series data. Our base model architecture $\\mu$ is a deep neural network (e.g., RNN) which is structured around two key sub-neural networks comprised of linear layers $\\mu\\:=\\:g\\cdot f$ : The feature function $f$ maps input data into a latent feature space, while the prediction head $g$ transforms these features into the forecasted outputs. ", "page_idx": 3}, {"type": "text", "text": "In typical supervised learning for time series, ground truth labels are available only in the output space. The latent space, by contrast, is designed to capture abstract, non-obvious patterns in the data. These features help the model\u2019s learning but do not correspond directly to observable labels ([19]). To address the lack of ground-truth labels in the latent space, we apply the surrogate feature [40] to replace the ground-truth term when constructing the non-conformity score. Concretely, we define the nonconformity score to be ", "page_idx": 3}, {"type": "equation", "text": "$$\ns(X,Y,\\hat{g}\\circ\\hat{f})=\\operatorname*{inf}_{v\\in V:\\hat{g}(v)=Y}\\|v-\\hat{f}(X)\\|,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{f}$ and $\\hat{g}$ are approximate $f$ and $g$ . This score measures the minimal discrepancy between any latent representation $v$ that could correctly predict $Y$ when processed by $\\hat{g}$ and the actual latent representation ${\\hat{f}}(X)$ produced by the model from input $X$ . ", "page_idx": 3}, {"type": "text", "text": "It is usually complicated to calculate the score in Equation 6 due to the infimum operator. One solution is to directly apply the gradient descent mechanism in [40]: $u\\,\\gets\\,u\\,-\\,\\eta\\nabla\\bar{(g(u)\\,-\\,Y)^{2}}$ , however, this approach overlooks the characteristics of time series data and the adaptability of the update process. Therefore, we propose a weighted gradient descent mechanism ", "page_idx": 3}, {"type": "equation", "text": "$$\nu\\leftarrow u-\\eta\\tilde{w}\\nabla(\\hat{g}(u)-Y)^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tilde{w}$ represents a vector of weights specifically designed to enhance the adaptability of the update process. These weights adjust the influence of each component of the gradient, enabling the latent vector $u$ to more effectively minimize the squared error between the predicted ${\\hat{g}}(u)$ and actual targets $Y$ . With learning rate $\\eta$ , these weighted adjustments prevent inefficient updates, fostering better convergence and ensuring well-defined non-conformity scores in the semantic feature space. ", "page_idx": 3}, {"type": "text", "text": "4.3 Adaptive Weight Adjustment ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To ensure precise calibration of our model for time series data, it is imperative to meticulously define the weight terms $\\tilde{w}$ , thereby enabling the algorithm to concentrate on minimizing errors where they are most critical. We propose an adaptive weight adjustment scheme in the latent space, fundamentally inspired by the intuition that weights should be dynamically adjusted to prioritize semantic features more pertinent to the current prediction task. To effectively quantify this pertinence, we employ the principle of proximity in error terms, where smaller discrepancies between predicted and ground-truth values suggest higher relevance [5]. For time series data, the underlying processes often exhibit consistent behaviors or trends. Similar errors reflect this consistency, making them reliable indicators for future occurrences. As a result, we expect that similar errors should work best to predict the current error. Note that these error terms are measured in the semantic feature space, allowing us to leverage the rich representations of the base model. Our strategy to handle nonexchangeable conditions in time series is to replace the quantile of in standard CP with a weighted quantile: ", "page_idx": 3}, {"type": "equation", "text": "$$\nQ_{1-\\alpha}\\left(\\sum_{i=1}^{t}\\tilde{w}_{i}\\cdot\\delta_{s_{i}}+\\tilde{w}_{t+1}\\cdot\\delta_{\\infty}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\tilde{w}_{i}$ are weights assigned based on the similarity of their error terms to the current prediction error, $s_{i}$ are the non-conformity score calculated in the latent space, $t$ represents the size of calibration data, and $\\delta_{a}$ denotes a point mass at $a$ which represents a discrete distribution: ", "page_idx": 3}, {"type": "equation", "text": "$$\nP(X=x)={\\binom{1}{0}}+{\\mathrm{{if}~}}x=\\alpha,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In Eq. 8, we assign larger weights to more relevant data points which can significantly enhance the lower bound of empirical coverage (as shown in Equation 2). This insight guides our adaptation of the attention mechanism to assess and quantify the relevance of training data points to a given test instance. The attention mechanism is designed to calculate weights based on the similarity of error terms, which are then used to adjust the influence of each data point in the final prediction model ([42]). The attention weights here are: $\\tilde{w}\\gets$ AttentionWeights $({\\hat{g}}(u),Y)$ , and AttentionWeights $({\\hat{g}}(u),Y)$ represents the attention weights calculated by the Transformer model, reflecting the similarity between the predicted output ${\\hat{g}}(u)$ and $Y$ . Accordingly, the attention mechanism in our approach actively assigns higher weights to these similar errors, thereby optimizing the inference coverage. The adaptive weight adjustment bolsters our method\u2019s capacity to consistently meet the significant level, providing a robust guarantee for its performance. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4.4 Weighted Conformal Prediction with Semantic Features ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Since the non-conformity score is initially constructed in the semantic space, it is necessary to translate the prediction intervals to the output space for practical application. Our method incorporates band estimation techniques [40] to estimate confidence or prediction intervals around the forecasts. This approach allows for uniform uncertainty levels in the latent space, while effectively reflecting varied levels of uncertainty in the output space based on the non-linear transformation. In this work, we apply linear relaxation-based perturbation analysis (LiPRA) ([50]) to tackle this problem under deep neural network regimes. LiPRA transforms the certification problem as a linear programming problem, and solves it accordingly. Following [40], we model the Band Estimation problem as a perturbation analysis one, where we regard the surrogate feature as a perturbation of the trained feature and analyze the output bounds of the prediction head. Since LiPRA results in a relatively looser interval than the actual band, this method would give an upper bound estimation of the exact band length. ", "page_idx": 4}, {"type": "text", "text": "We summarize our framework in Algorithm 1. First, it calculates non-conformity scores within the latent space by splitting the base model into a feature function and a prediction head in step 3. By doing so, our method can utilize the latent features to generate precise and efficient prediction sets that are tailored to the specific dynamics of the data. During the calibration phase, the algorithm integrates the attention mechanism in steps 7-10 to refine the learning process of weights. This integration is crucial for dynamically prioritizing and adjusting the influence of specific features based on their relevance to the prediction task. Lastly, step 14 applies a structured process to achieve comprehensive coverage and maintain competitive intervals in our prediction sets. As a result, our method can dynamically adjust weights in response to changes in data patterns. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Conformalized Time Series with Semantic Features   \nRequire: Dataset $\\{(X_{t},Y_{t})\\}_{t=1}^{T}$ , test feature $\\{X_{i}\\}_{i\\in I_{t e},\\,\\eta,\\,M,\\,\\tilde{w},\\,\\alpha}$ ;   \n1: Randomly split the dataset $D$ into training $D_{t r}=\\{(X_{i},Y_{i})\\}_{i\\in I_{t r}}$ and calibration $D_{c a}=\\{(X_{i},Y_{i})\\}_{i\\in I_{c a}}$ ;   \n2: Training with $D_{t r}$   \n3: Train a base deep NN model for time series $\\mu=\\hat{g}\\circ\\hat{f}(\\cdot)$ using the training fold $D_{t r}$ ;   \n4: Adaptive weight adjustment   \n5: $u\\gets\\{\\hat{f}(X_{i})\\}_{i\\in I_{c a}\\cup I_{t e}};m\\gets0;n\\gets|I_{c a}|+|I_{t e}|;\\tilde{w}\\gets[1/n,.\\,.\\,.\\,,1/n]$   \n6: while $m<M$ do   \n7: predict $\\hat{g}(u)-\\{Y_{i}\\}_{i\\in I_{t e}}$   \n8: update $\\tilde{w}$   \n9: $\\begin{array}{r l}&{\\dot{u_{\\mathit{\\Pi}}}\\gets u-\\eta\\tilde{w}\\nabla\\left(\\|\\hat{g}(u)-\\{Y_{i}\\}_{i\\in I_{c a}\\cup I_{t e}}\\|^{2}\\right)}\\\\ &{m\\gets m+1}\\end{array}$   \n10:   \n11: end while   \nEnsure: $s(X,Y,\\hat{g}\\circ\\hat{f})=\\|u-\\hat{f}(X)\\|$ .   \n12: for $i\\in I_{t e}$ do   \n13: Calibration with $D_{c a}$   \n14: Calculate the $(1-\\alpha)$ -th quantile $Q_{1-\\alpha}$ of the distribution $\\begin{array}{r}{\\frac{1}{|D_{c a}|+1}\\left(\\sum_{i=1}\\tilde{w}_{i}\\cdot\\delta_{s_{i}}+\\tilde{w}_{n+1}\\cdot\\delta_{\\infty}\\right)}\\end{array}$ .   \n15: Prediction   \n16: Apply LiPRA on $\\hat{f}(X_{i})$ with perturbation $Q_{1-\\alpha}$ and prediction head $\\hat{g}$ , which returns $C_{1-\\alpha}(X_{i})$ ;   \n17: $D_{c a}\\leftarrow X_{i}$   \n18: end for   \nEnsure: $C_{1-\\alpha}(X_{i})$ for each test input. ", "page_idx": 4}, {"type": "text", "text": "4.5 Theoretical Guarantee ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section outlines the theoretical guarantees for CT-SSF with respect to coverage and prediction interval length. Based on Theorem 4 in [40], we demonstrate that constructing non-conformity scores in the latent space allows CT-SSF to surpass CP for time series in the output space (e.g., NexCP) in terms of average prediction interval length under the following mild assumption: ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. In the output space, we define $H(v,X)$ as the length associated with sample $X$ , derived from the length $v$ in the feature space. This is represented as $H(v,X)\\;=\\;\\{g(u)\\;\\stackrel{\\cdot}{\\in}\\;\\mathbb{R}\\;:$ $\\begin{array}{r}{\\|u-\\hat{f}(X)\\|\\leq\\frac{v}{2}\\right\\}}\\end{array}$ . We assume a H\u00f6lder condition for $H$ , stipulating that for any $X$ , the inequality $|H(v,X)-H(u,X)|\\leq L|v-u|^{\\beta}$ holds, where $\\beta>0$ and $L>0$ are constants. ", "page_idx": 5}, {"type": "text", "text": "The H\u00f6lder condition for $H(u,X)$ ensures that small changes in the input lengths $u$ or $v$ result in small and predictable changes in the length $H$ . This is characterized by the constants $\\alpha$ and $L$ , which control the sensitivity of $H$ ([12]). In the context of CT-SSF, this condition helps maintain consistent lengths when transforming from the feature space to the output space (length preservation), ensures that differences between individual lengths and their quantiles are amplified (expansion), and guarantees that the prediction intervals remain stable and reliable across both spaces for a given calibration set (quantile stability). Below is an informal statement of the theoretical guarantee of CT-SSF. The complete version of the theorem is in Appendix C.1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Under Assumption 1, if the following cubic conditions hold: ", "page_idx": 5}, {"type": "text", "text": "1. Length Preservation. CT-SSF method does not cost much loss in feature space. ", "page_idx": 5}, {"type": "text", "text": "2. Expansion. The Band Estimation operator expands the differences between individual length and their quantiles.   \n3. Quantile Stability. The band length is stable in both the feature space and the output space for a given calibration set. ", "page_idx": 5}, {"type": "text", "text": "Let $\\tilde{w}_{i}$ denote the weights learned from Algorithm 1. Suppose that $\\tilde{w}_{i}$ \u2019s are independent of the calibration and test nonconformity scores. Then CT-SSF outperforms CP approaches for time series in the output space in terms of average band length while maintaining a valid coverage guarantee. ", "page_idx": 5}, {"type": "text", "text": "Here, length preservation ensures that the transformation from feature space to output space maintains consistent interval lengths, minimizing efficiency loss. Expansion plays a key role in reducing inefficiency, as the latent space exhibits smaller distances between individual non-conformity scores and their quantiles, reducing the computational cost of the quantile operation. Lastly, quantile stability ensures that the interval is generalizable from the calibration set to the test samples. Since these conditions primarily emphasize the properties of quantiles and transformation steps rather than the reweighting, extending this framework to incorporate a weighted setting is a logical and justified progression. Note that in our theoretical results (for both efficiency and coverage) essentially assumes the weights to be fixed, where in implementation, the weights are learned from the data. It remains an interesting question to establish a condescending theory for data-drive weights. The detailed proofs can be found in Appendix C.1. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Prediction Models for Time Series. CT-SSF is model agnostic, therefore, any NN-based prediction models for time series.prediction models can be used as the base models. To better show the advantage of our proposed method, we utilize a Recurrent Neural Network (RNN) model, which can be replaced with more advanced models like Transformers ([42]). ", "page_idx": 5}, {"type": "text", "text": "Datasets. Our experiments encompass evaluations on both synthetic and four real-world benchmark datasets, allowing for assessment under controlled and natural conditions. They are electricity, stock of Amazon, weather, and wind data. The details of these datasets can be found in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Calibration Details. During calibration, to get the best value for the number of steps $M$ , we take a subset (e.g., one-fifth) of the calibration set as the additional validation set. We calculate the nonconformity score on the rest of the calibration set with various values of step $M$ and then evaluate the validation set to get the best $M$ whose coverage is right above $1-\\alpha$ . The final trained surrogate feature $v$ is close to the true feature because $\\bar{g}(\\bar{v})$ is sufficiently close to the ground truth $Y$ . In practice, the surrogate feature after optimization satisfies ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\|\\hat{g}(v)-Y\\|^{2}}{\\|Y\\|^{2}}<1\\%.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Eq. 10 indicates that the normalized error of predictions with the surrogate feature is less than $1\\%$ .   \nTherefore, the surrogate feature approximates ground truth feature in the latent space. ", "page_idx": 6}, {"type": "text", "text": "Compared Approaches. Baselines include state-of-the-art (SOTA) methods for CP under distribution drift and SOTA CP for time series: (1) Standard Split CP ([43]). This is the standard split CP method outlined in Section 3; (2) NexCP ([7]). A refined CP method for handling distribution drifts using manually selected weights. Details are outlined in Section 3; (3) FeatureCP (FCP) ([40]). Implementing CP within a latent feature space under the assumption of exchangeability; (4) HopCPT ([5]). The current state-of-the-art CP for time series. HopCPT leverages Modern Hopfield Networks to build prediction intervals by identifying and utilizing historical events with similar error distributions. ", "page_idx": 6}, {"type": "text", "text": "Metrics. Our analysis employs two widely used metrics to assess the effectiveness of each CP method ([4]): Empirical Coverage Rate (Coverage): Measures the effectiveness of CP in achieving the theoretically guaranteed coverage. Average Prediction Set Size (Width): Evaluates the efficiency of CP, reflecting the compactness of the prediction intervals. ", "page_idx": 6}, {"type": "text", "text": "All the standard deviations are obtained over five repeated runs with different random seeds under the same base model. Code and data are available at https://github.com/baiting0522/CT-SSF. ", "page_idx": 6}, {"type": "text", "text": "5.2 Simulations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We adapt the simulation method in [7] and generate time series datasets that incorporate manually designed temporal dependency and heteroskedasticity. We generate $n\\,=\\,1,000$ data points $(\\dot{X_{i}},Y_{i})\\stackrel{\\smile}{\\in}\\mathbb{R}^{M}\\,\\times\\,\\mathbb{R}^{N}$ by sampling $X_{i}$ from a Gaussian distribution, $X_{i}\\,\\sim\\,{\\mathcal{N}}(0,I_{M})$ . We set $Y_{i}\\sim A X_{i}+B X_{i-1}+C X_{i-2}+\\epsilon_{i}$ . The coefficient matrix $A,B,C$ are set to the identity matrix $I_{M}$ and $\\epsilon_{i}\\sim\\mathcal{N}(0,\\Sigma)$ where $\\Sigma$ is a diagonal matrix and its elements are given by $0.5+0.1i$ . ", "page_idx": 6}, {"type": "table", "img_path": "KcDcaVOW1S/tmp/4e49add1428cd18fa792800f192c128cef5b5a4e1131630ddeb7e7739801c026.jpg", "table_caption": [], "table_footnote": ["Table 1: Performance of the evaluated CP algorithms for the simulations dataset. "], "page_idx": 6}, {"type": "text", "text": "Table 1 presents the performance of the compared CP algorithms across three different levels of miscoverage error $(\\alpha)$ : 0.05, 0.1, and 0.15. We observe from the results that CT-SSF consistently outperforms other baselines across all tested scenarios. Specifically, at a miscoverage level of 0.05, CT-SSF shows a $4.61\\%$ reduction in interval width compared to the SOTA approach, HopCPT. The smallest width indicates that our method can produce the most informative prediction intervals while maintaining valid empirical coverage. This efficiency is attributed to the innovative construction of the non-conformity score within the latent space with adaptive weight adjustment, which enhances the ability to leverage the underlying information of the base model. Meanwhile, the superior performance of FCP compared to CP and NexCP aligns with the experimental results reported in [40]. However, since FCP operates under the assumption of exchangeability, it may occasionally result in under-coverage, e.g., $\\alpha=0.05$ and $\\alpha=0.10$ . On the other hand, NexCP tends to produce overly conservative results, though it maintains robust coverage. This underscores the need for our adaptive weight adjustments with semantic features to optimize performance. ", "page_idx": 6}, {"type": "text", "text": "Ablation Study. To further investigate the effectiveness of using semantic features and adaptive weight adjustment in CT-SSF, we compare it with its three variants that use: (1) Manually selected weights in the Semantic space (CT-MS), (2) Manually selected weights in the Output space (CTMO). This variant reduces to the NexCP method and (3) Learned weights in the Output space (CTLO). The weights are learned using the same learning scheme in CT-SSF. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "KcDcaVOW1S/tmp/7525ab6c9e51c6c1e9b611b6a3fd04d7a218ac8de18b4ee6ba5f6f79e1d3aef3.jpg", "img_caption": ["Figure 1: Comparisons of the variants of CT-SSF. The blue bar chart represents Width and the gray bar represents Coverage, the blue line represents target coverage. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 1 demonstrates the effectiveness of the CT-SSF method compared to its variants across varied miscoverage risks from 0.05 to 0.3. CT-SSF consistently outperforms the other methods in terms of prediction interval width, indicating its superior efficiency in generating compact prediction sets while maintaining robust empirical coverage. Relative to CT-MO and CT-LO, our method achieves more than a $50\\%$ reduction in the width of the prediction intervals, a benefti derived from exploiting the rich latent information in the feature space. Additionally, in comparison with CT-MS, our method shows an approximate $10\\%$ reduction in interval width. These results demonstrate the significant impact of adaptive weight adjustments on the efficiency of prediction intervals, further establishing CT-SSF as a highly effective CP method for time series forecasting. ", "page_idx": 7}, {"type": "text", "text": "CT-SSF\u2019s performance can be attributed to its specialized approach to learning weights in the latent space. Unlike methods such as NexCP and CT-MS, which apply predefined weight schemes, CT-SSF\u2019s dynamic adaptation to the underlying data structure allows for more precise adaptation of the model to the specific data characteristics. Unlike HopCPT, which learns the weights but constructs non-conformity scores in the output space, CT-SSF builds these scores directly within the latent space to utilize underlying latent features effectively and better capture the data variability and structural dependencies, resulting in more reliable and informative prediction intervals. ", "page_idx": 7}, {"type": "text", "text": "5.3 Real-world Data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Real-world experiments are conducted using benchmark time series data including electricity ([7]), stock market ([14, 2]), weather ([28, 54]), and wind speed forecasting ([48]). ", "page_idx": 7}, {"type": "table", "img_path": "KcDcaVOW1S/tmp/24691843331fed9964943a87bac6106d222f3ada4bf2361aa523ffe8a7017f4e.jpg", "table_caption": [], "table_footnote": ["Table 2: Performance of the evaluated CP algorithms for the real data. The specified miscoverage level is $\\alpha=0.1$ for all experiments. Results for different $\\alpha$ can be found in Appendix B. "], "page_idx": 7}, {"type": "text", "text": "Results in Table 2 reveal that CT-SSF maintains valid coverage rates while excelling in minimizing prediction interval widths. Across all datasets, CT-SSF shows a $10\\%{-}20\\%$ reduction in prediction intervals compared to the SOTA HopCPT. While NexCP is designed for non-exchangeable settings, the need for manually selected weights limits its practical application. For instance, in the wind dataset, improperly chosen weights result in larger prediction intervals compared to standard CP, despite meeting theoretical coverage guarantees. We further conduct an ablation study to compare the variants of CT-SSF, similar to that for synthetic data. Results in Appendix B show that CT-SSF outperforms other variants. This demonstrates the effectiveness of utilizing semantic features and implementing adaptive weight adjustments. While our experiments are based on RNN, we show in Appendix B that using a different base model can lead to similar findings. ", "page_idx": 8}, {"type": "table", "img_path": "KcDcaVOW1S/tmp/158c00acb68ac695945f0dc889feaff1a3928d7e4ec20737e2b6324205cf833c.jpg", "table_caption": [], "table_footnote": ["Table 3: Performance of the CT-SSF with different $f$ and $g$ configurations. The specified miscoverage level is $\\alpha=0.1$ for all experiments. "], "page_idx": 8}, {"type": "text", "text": "The impact of choosing $f$ and $g$ . Finally, we analyze the potential impact of the selection of the semantic feature space on the performance of CT-SSF. Our base model is an 8-layer RNN, and we conduct experiments on semantic spaces ranging from the 2nd to the 6th layer, i.e., using the first 2-6 layers as $f$ and the rest as $g$ . Results in Table 3 indicate that the performance of CT-SSF is influenced by the configuration of $f$ and $g$ , as evidenced by the variation in the length of prediction intervals. A key observation is that optimal performance typically arises from the middle layers (specifically, the 3rd and 4th layers). This suggests that $f$ with too few layers may not capture enough semantic information, and $g$ with a limited number of layers lacks adequate predictive strength. Additionally, the coverage and interval length results are similar for the 5th and 6th layers across all four datasets. This similarity may be due to the comparable level of information interpreted in these feature spaces. Given all these observations, it is important to perform cross-validation to identify the best semantic space for the highest efficiency when deployed in various applications. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusions and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we introduce CT-SSF, a novel approach designed to overcome the challenges of applying CP directly to time-series data. Our method shifts the calculation of non-conformity scores to the semantic feature space, effectively capturing the complex, nonlinear relationships in time-series data that are often overlooked in the output space. By employing a reweighting scheme and dynamically adjusting weights based on the importance of various semantic features, CT-SSF achieves valid coverage with high efficiency. We provide theoretical analyses demonstrating that CT-SSF outperforms CP methods that operate in the output space. Experimental results show that CT-SSF significantly outperforms existing SOTA CP methods for time-series data. ", "page_idx": 8}, {"type": "text", "text": "Our approach to CP for time series forecasting presents certain limitations that warrant further exploration and mitigation. Firstly, its reliance on NNs excludes simpler models like ridge regression or random forests. This limitation, while justified by the prevalent use of NN in time series forecasting, narrows the scope of our methodology\u2019s wide applicability. Secondly, the robustness of our prediction set lengths is sensitive to the configuration of the latent space to construct $f$ and $g$ . Our current solution uses a validation set to help separate $f$ and $g$ , rendering less data for calibration. Future enhancements could include the development of adaptive methods that dynamically refine the latent space to enhance the stability of the predictions across diverse data scenarios, thereby addressing the current limitations and expanding the utility of our approach. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the National Science Foundation (NSF) Grant #2312862, National Institutes of Health (NIH) #R01AG091762, and a Cisco gift grant. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Anastasios Angelopoulos, Stephen Bates, Jitendra Malik, and Michael I Jordan. Uncertainty sets for image classifiers using conformal prediction. arXiv preprint arXiv:2009.14193, 2020. [2] Anastasios Angelopoulos, Emmanuel Candes, and Ryan J Tibshirani. Conformal pid control for time series prediction. Advances in Neural Information Processing Systems, 36, 2024.   \n[3] Anastasios N Angelopoulos, Rina Foygel Barber, and Stephen Bates. Online conformal prediction with decaying step sizes. arXiv preprint arXiv:2402.01139, 2024.   \n[4] Anastasios N Angelopoulos and Stephen Bates. A gentle introduction to conformal prediction and distribution-free uncertainty quantification. arXiv preprint arXiv:2107.07511, 2021.   \n[5] Andreas Auer, Martin Gauch, Daniel Klotz, and Sepp Hochreiter. Conformal prediction for time series with modern hopfield networks. Advances in Neural Information Processing Systems, 36, 2023.   \n[6] Yu Bai, Song Mei, Huan Wang, Yingbo Zhou, and Caiming Xiong. Efficient and differentiable conformal prediction with general function classes. arXiv preprint arXiv:2202.11091, 2022.   \n[7] Rina Foygel Barber, Emmanuel J Candes, Aaditya Ramdas, and Ryan J Tibshirani. Conformal prediction beyond exchangeability. The Annals of Statistics, 51(2):816\u2013845, 2023.   \n[8] Aadyot Bhatnagar, Huan Wang, Caiming Xiong, and Yu Bai. Improved online conformal prediction via strongly adaptive online learning. In International Conference on Machine Learning, pages 2337\u20132363. PMLR, 2023.   \n[9] Ilias Bilionis and Nicholas Zabaras. Multi-output local gaussian process regression: Applications to uncertainty quantification. Journal of Computational Physics, 231(17):5718\u20135746, 2012.   \n[10] Christopher M Bishop. Mixture density networks. 1994.   \n[11] Emmanuel Cand\u00e8s, Lihua Lei, and Zhimei Ren. Conformalized survival analysis. Journal of the Royal Statistical Society Series B: Statistical Methodology, 85(1):24\u201345, 2023.   \n[12] Lawrence C Evans. Partial differential equations american mathematical society. Providence, RI, 2, 1998.   \n[13] Adam Fisch, Tal Schuster, Tommi Jaakkola, and Regina Barzilay. Efficient conformal prediction via cascaded inference with expanded admission. arXiv preprint arXiv:2007.03114, 2020.   \n[14] Isaac Gibbs and Emmanuel Candes. Adaptive conformal inference under distribution shift. Advances in Neural Information Processing Systems, 34:1660\u20131672, 2021.   \n[15] Isaac Gibbs and Emmanuel J Cand\u00e8s. Conformal inference for online prediction with arbitrary distribution shifts. Journal of Machine Learning Research, 25(162):1\u201336, 2024.   \n[16] Yu Gui, Ying Jin, and Zhimei Ren. Conformal alignment: Knowing when to trust foundation models with guarantees. arXiv preprint arXiv:2405.10301, 2024.   \n[17] Jiale Han and Xiaowu Dai. Conformal online auction design. arXiv preprint arXiv:2405.07038, 2024.   \n[18] Michael Harries, New South Wales, et al. Splice-2 comparative evaluation: Electricity pricing. 1999.   \n[19] Bernard Hernandez, Oliver Stiff, Damien K Ming, Chanh Ho Quang, Vuong Nguyen Lam, Tuan Nguyen Minh, Chau Nguyen Van Vinh, Nguyet Nguyen Minh, Huy Nguyen Quang, Lam Phung Khanh, et al. Learning meaningful latent space representations for patient risk stratification: Model development and validation for dengue and other acute febrile illness. Frontiers in Digital Health, 5:1057467, 2023.   \n[20] Ulf Johansson, Henrik Bostr\u00f6m, and Tuve L\u00f6fstr\u00f6m. Conformal prediction using decision trees. In 2013 IEEE 13th international conference on data mining, pages 330\u2013339. IEEE, 2013.   \n[21] Ulf Johansson, Henrik Bostr\u00f6m, Tuve L\u00f6fstr\u00f6m, and Henrik Linusson. Regression conformal prediction with random forests. Machine learning, 97:155\u2013176, 2014.   \n[22] Rikard Laxhammar and G\u00f6ran Falkman. Inductive conformal anomaly detection for sequential detection of anomalous sub-trajectories. Annals of Mathematics and Artificial Intelligence, 74:67\u201394, 2015.   \n[23] Jonghyeok Lee, Chen Xu, and Yao Xie. Kernel-based optimally weighted conformal prediction intervals. arXiv preprint arXiv:2405.16828, 2024.   \n[24] Junghwan Lee, Chen Xu, and Yao Xie. Transformer conformal prediction for time series. arXiv preprint arXiv:2406.05332, 2024.   \n[25] Lihua Lei and Emmanuel J Cand\u00e8s. Conformal inference of counterfactuals and individual treatment effects. Journal of the Royal Statistical Society Series B: Statistical Methodology, 83(5):911\u2013938, 2021.   \n[26] Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. Conformal prediction with temporal quantile adjustments. Advances in Neural Information Processing Systems, 35:31017\u201331030, 2022.   \n[27] Eugene Ndiaye. Stable conformal prediction sets. In International Conference on Machine Learning, pages 16462\u201316479. PMLR, 2022.   \n[28] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2022.   \n[29] Harris Papadopoulos, Kostas Proedrou, Volodya Vovk, and Alex Gammerman. Inductive confidence machines for regression. In Machine learning: ECML 2002: 13th European conference on machine learning Helsinki, Finland, August 19\u201323, 2002 proceedings 13, pages 345\u2013356. Springer, 2002.   \n[30] Harris Papadopoulos, Vladimir Vovk, and Alexander Gammerman. Regression conformal prediction with nearest neighbours. Journal of Artificial Intelligence Research, 40:815\u2013840, 2011.   \n[31] Aleksandr Podkopaev and Aaditya Ramdas. Distribution-free uncertainty quantification for classification under label shift. In Uncertainty in artificial intelligence, pages 844\u2013853. PMLR, 2021.   \n[32] Yaniv Romano, Evan Patterson, and Emmanuel Candes. Conformalized quantile regression. Advances in neural information processing systems, 32, 2019.   \n[33] Yaniv Romano, Matteo Sesia, and Emmanuel Candes. Classification with valid and adaptive coverage. Advances in Neural Information Processing Systems, 33:3581\u20133591, 2020.   \n[34] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. International journal of forecasting, 36(3):1181\u20131191, 2020.   \n[35] Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. Journal of Machine Learning Research, 9(3), 2008.   \n[36] David Stutz, Ali Taylan Cemgil, Arnaud Doucet, et al. Learning optimal conformal classifiers. arXiv preprint arXiv:2110.09192, 2021.   \n[37] Jiayuan Su, Jing Luo, Hongwei Wang, and Lu Cheng. Api is enough: Conformal prediction for large language models without logit-access. arXiv preprint arXiv:2403.01216, 2024.   \n[38] Johan Hallberg Szabadv\u00e1ry. Adaptive conformal inference for multi-step ahead time-series forecasting online. arXiv preprint arXiv:2409.14792, 2024.   \n[39] Ahmed Tealab. Time series forecasting using artificial neural networks methodologies: A systematic review. Future Computing and Informatics Journal, 3(2):334\u2013340, 2018.   \n[40] Jiaye Teng, Chuan Wen, Dinghuai Zhang, Yoshua Bengio, Yang Gao, and Yang Yuan. Predictive inference with feature conformal prediction. arXiv preprint arXiv:2210.00173, 2022.   \n[41] Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas. Conformal prediction under covariate shift. Advances in neural information processing systems, 32, 2019.   \n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[43] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random world, volume 29. Springer, 2005.   \n[44] Volodya Vovk, Alexander Gammerman, and Craig Saunders. Machine-learning applications of algorithmic randomness. 1999.   \n[45] Bin Wang, Jie Lu, Zheng Yan, Huaishao Luo, Tianrui Li, Yu Zheng, and Guangquan Zhang. Deep uncertainty quantification: A machine learning approach for weather forecasting. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2087\u20132095, 2019.   \n[46] Xiaoqian Wang and Rob J Hyndman. Online conformal inference for multi-step time series forecasting. arXiv preprint arXiv:2410.13115, 2024.   \n[47] Chen Xu, Hanyang Jiang, and Yao Xie. Conformal prediction for multi-dimensional time series by ellipsoidal sets. arXiv preprint arXiv:2403.03850, 2024.   \n[48] Chen Xu and Yao Xie. Conformal prediction interval for dynamic time-series. In International Conference on Machine Learning, pages 11559\u201311569. PMLR, 2021.   \n[49] Chen Xu and Yao Xie. Sequential predictive conformal inference for time series. In International Conference on Machine Learning, pages 38707\u201338727. PMLR, 2023.   \n[50] Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certified robustness and beyond. Advances in Neural Information Processing Systems, 33:1129\u20131141, 2020.   \n[51] Yachong Yang and Arun Kumar Kuchibhotla. Finite-sample efficient conformal prediction. arXiv preprint arXiv:2104.13871, 2021.   \n[52] Zitong Yang, Emmanuel Cand\u00e8s, and Lihua Lei. Bellman conformal inference: Calibrating prediction intervals for time series. arXiv preprint arXiv:2402.05203, 2024.   \n[53] Margaux Zaffran, Olivier F\u00e9ron, Yannig Goude, Julie Josse, and Aymeric Dieuleveut. Adaptive conformal predictions for time series. In International Conference on Machine Learning, pages 25834\u201325866. PMLR, 2022.   \n[54] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 11121\u201311128, 2023.   \n[55] G Peter Zhang. Time series forecasting using a hybrid arima and neural network model. Neurocomputing, 50:159\u2013175, 2003.   \n[56] Tianyi Zhao, Jian Kang, and Lu Cheng. Conformalized link prediction on graph neural networks. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 4490\u20134499, 2024.   \n[57] Xiaofan Zhou, Baiting Chen, Yu Gui, and Lu Cheng. Conformal prediction: A data perspective. arXiv preprint arXiv:2410.06494, 2024.   \n[58] Yanfei Zhou, Lars Lindemann, and Matteo Sesia. Conformalized adaptive forecasting of heterogeneous trajectories. arXiv preprint arXiv:2402.09623, 2024.   \n[59] Shixiang Zhu, Hanyu Zhang, Yao Xie, and Pascal Van Hentenryck. Multi-resolution spatio-temporal prediction with application to wind power generation. arXiv preprint arXiv:2108.13285, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Comparisons to existing methods ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This section contains descriptions of the relationship of our method to existing approaches of conformal prediction for time series data. There are generally three primary approaches designed to manage these challenges and enhance the reliability and validity of the CP in time series: reweighting, updating non-conformity scores and updating miscoverage level. We introduce representative work for each of these categories here. For a more detailed analysis of CP methods for time series, please refer to [57]. ", "page_idx": 13}, {"type": "text", "text": "Reweighting assigns relevance-based weights to data points to align the data distribution closer to a target distribution. NexCP [7] exponentially decayed weights to emphasize recent observations, but these lack adaptability. HopCPT [5] improves on this by using a Modern Hopfield Network (MHN) for similarity-based reweighting. It assigns weights to past time steps based on their relevance to the current time step. Encoded inputs are processed with learned weight matrices, and a hyperparameter adjusts the focus of the probabilistic distribution. These weights create weighted conformal prediction intervals by discounting extremal quantiles of relative errors. ", "page_idx": 13}, {"type": "text", "text": "The second technique, updating non-conformity scores leverages the most recent $T$ data points and continuously updates prediction intervals as new data becomes available. For example, EnbPI [48] updates the non-conformity score with sequential error patterns to adapt the intervals dynamically. And SPCI [49] replaces the empirical quantile with an estimate by a conditional quantile estimator to effectively address serial dependencies among residuals in sequential analysis. ", "page_idx": 13}, {"type": "text", "text": "The last main direction for CP in time series focuses on adaptively adjusting the significance level $\\alpha$ during test time to account for mis-coverage. This method can dynamically adjust the size of prediction sets in an online setting where the data generating distribution is allowed to vary over time in an unknown fashion. For example, the update rule for the quantile level $\\alpha$ in ACI [14] is: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\alpha_{t+1}=\\alpha_{t}+\\gamma(\\alpha-\\operatorname{err}_{t}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\gamma$ is a step size parameter, and $\\mathbf{err}_{t}$ indicates if $Y_{t}$ was not included in $\\hat{C}_{t}(\\alpha_{t})$ . The approach ensures that the prediction intervals adjust over time to account for shifts in the data distribution, maintaining the desired coverage probability. ", "page_idx": 13}, {"type": "text", "text": "Our work falls under a reweighting scheme, with a focus on weighted conformal prediction in the latent feature space. This approach enables us to capture the underlying data structure more precisely, leading to tighter prediction intervals. We leverage the standard attention mechanism for its simplicity and computational efficiency, while also allowing for dynamic adjustment of data point significance. Furthermore, our method effectively integrates attention-based weight updates within the latent feature space, where the learned rich representations capture the intrinsic characteristics of the data with greater accuracy. ", "page_idx": 13}, {"type": "text", "text": "B Extended Experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Datasets Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Datasets from four different domains are used in the experiments. Details are summarized below. ", "page_idx": 13}, {"type": "text", "text": "\u2022 Electricity. The Electricity Demand Forecasting dataset, introduced by Harries in 1999 ([18]), tracks electricity usage and pricing in New South Wales and Victoria, Australia, with half-hourly data from May 7, 1996, to December 5, 1998. For our experiment, we focus on four key variables: nswprice and vicprice (electricity prices in New South Wales and Victoria) and nswdemand and vicdemand (electricity demand in each state). Our response variable, transfer, measures the electricity transferred between the two states. We select data from 9:00 AM to 12:00 PM to reduce daily fluctuations and discard an initial period with constant transfer values, resulting in 3,444 time points for analysis. ", "page_idx": 13}, {"type": "text", "text": "\u2022 Amazon Stock. The Amazon stock dataset provides detailed historical data on Amazon\u2019s stock performance. This dataset includes key financial metrics such as the opening price, highest price, lowest price, closing price, and trading volume for each trading day. The dataset captures daily stock prices at various intervals, providing a comprehensive view of Amazon\u2019s stock market behavior. These metrics are crucial for analyzing trends, volatility, and the overall performance of Amazon\u2019s stock over time. The data is structured with columns representing the opening price (open), the highest price during the day (high), the lowest price during the day (low), the closing price (close), and the volume of shares traded (volume). ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "\u2022 Weather. The weather dataset records meteorological data every 10 minutes throughout the entire year of 2020, encompassing 21 key indicators that provide a comprehensive overview of weather conditions. Key indicators include air temperature, humidity, wind direction, rainfall, solar radiation. This dataset offers detailed, granular insights into weather patterns and conditions, making it valuable for meteorological analysis and forecasting. \u2022 Wind Speed. The wind speed data are collected at wind farms operated by the Midcontinent Independent System Operator (MISO) in the US ([59]). The dataset records wind speed measurements updated every 15 minutes over a one-week period in September 2020. This high-frequency data provides detailed insights into wind speed variations and is crucial for analyzing and modeling wind energy production. ", "page_idx": 14}, {"type": "text", "text": "Additional Experiments We conduct extended experiments on the real-world data for RNN and a different base model for time series prediction, Transformer. Results with different miscoverage levels can be seen in Table 4 and Table 5, respectively. We also perform an ablation study similar to Section 5.2 for real data in Figure 2. CT-SSF consistently delivers the shortest prediction intervals across all datasets and varying error rates when utilizing a Transformer as the base model. Although it does not perform optimally for weather and wind data at a 0.05 error rate with an RNN base, CTSSF still achieves results comparable to the top-performing method, HopCPT. These experiments support the conclusion that CT-SSF can produce competitive prediction intervals while maintaining coverage guarantee. ", "page_idx": 14}, {"type": "table", "img_path": "KcDcaVOW1S/tmp/5ad30896710c3a1d4597541a2888fcf63cb6f8db8153ad68777ef3687bf3ea38.jpg", "table_caption": [], "table_footnote": ["Table 4: Performance of the evaluated CP algorithms for the real data of RNN. The standard deviation is obtained over five repeated runs with different random seeds. "], "page_idx": 15}, {"type": "text", "text": "C Theoretical Guarantee ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This section provides theoretical guarantees for CT-SSF regarding coverage and interval length (i.e., efficiency). We use the seminar work NexCP for illustration and the results below can be applied to other CP for time series in the output space as well. ", "page_idx": 15}, {"type": "text", "text": "C.1 Efficiency ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Notations. Let $P$ denote the population distribution. Let $D_{c a}\\sim\\mathbb{P}_{n}$ denote the calibration set with sample size $n$ , where we overload the notation $P_{n}$ to denote the distribution of a set with samples drawn from a distribution $P$ . Given the model $g\\circ f$ with feature extractor $f$ and prediction head $g$ , we assume $g$ is continuous. We also overload the notation $Q_{1-\\alpha}(V)$ to denote the $(1-\\alpha)$ -quantile of the set $V\\cup\\{\\infty\\}$ . Here, the set $V$ represents the empirical distribution after reweighting, i.e., $\\begin{array}{r}{\\sum_{i=1}^{t}\\tilde{w}_{i}\\cdot\\delta_{v_{i}}+\\tilde{w}_{t+1}\\cdot\\delta_{\\infty}}\\end{array}$ . Additionally, let $\\mathbb{M}[\\cdot]$ denote the mean of a set, and let a set minus a real n umber denote the broadcast operation. ", "page_idx": 15}, {"type": "text", "text": "NexCP. Let $V_{D_{c a}}^{o}=\\{v_{i}^{o}\\}_{i\\in I_{c a}}$ denote the individual lengths in the output space, given the calibration set $D_{c a}$ . Specifically, $v_{i}^{o}=2|y_{i}-\\hat{y}_{i}|$ , where $y_{i}$ denotes the true response of sample $i$ and $\\hat{y}_{i}$ denotes the corresponding prediction. Since NexCP returns band length with the $1-\\alpha$ quantile of the non-conformity score, the resulting average band length is derived as $Q_{1-\\alpha}(V_{D_{c a}}^{o})$ . ", "page_idx": 15}, {"type": "table", "img_path": "KcDcaVOW1S/tmp/fa660694a20a10aefc57262d7ab05bd60056ad074324fb64d56b0197b3f801fc.jpg", "table_caption": [], "table_footnote": ["Table 5: Performance of the evaluated CP algorithms for the real data with Transformer as the base model. The standard deviation is obtained over five repeated runs with different random seeds. "], "page_idx": 16}, {"type": "text", "text": "CT-SSF. Let $\\begin{array}{l l l}{V_{D_{c a}}^{f}}&{=}&{\\{v_{i}^{f}\\}_{i\\in I_{c a}}}\\end{array}$ be the individual lengths in the semantic feature space given the calibration set $D_{c a}$ . The resulting band length in CT-SSF is denoted by $\\mathbf{\\^{\\breve{E}}}_{X_{t e s t},Y_{t e s t}}\\left[H\\left(Q_{1-\\alpha}(V_{D_{c a}}^{f}),X_{t e s t}\\right)\\right]$ . ", "page_idx": 16}, {"type": "text", "text": "We propose a formal description of the cubic conditions and provide the proof following the structure of [40]. ", "page_idx": 16}, {"type": "text", "text": "Theorem 1. Assume Assumption 1 holds. Additionally, we assume that there exist constants $\\epsilon>0$ and $c>0$ , such that the feature space satisfies the following cubic conditions: ", "page_idx": 16}, {"type": "text", "text": "1. Length Preservation. Semantic-Feature CP does not incur a significant loss in the feature space in a quantile manner: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{D\\sim P_{n}}Q_{1-\\alpha}(H(V_{D}^{f},D))<\\mathbb{E}_{D\\sim P_{n}}Q_{1-\\alpha}(V_{D}^{o})+\\epsilon.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "2. Expansion. The operator $H(v,X)$ expands the differences between individual lengths and their quantiles, namely, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L\\mathbb{E}_{D\\sim P_{n}}\\left[\\mathbb{M}\\left[Q_{1-\\alpha}(V_{D}^{f})-V_{D}^{f}\\right]^{\\alpha}\\right]<\\mathbb{E}_{D\\sim P_{n}}\\left[\\mathbb{M}\\left[Q_{1-\\alpha}(H(V_{D}^{f}))-H(V_{D}^{f})\\right]\\right]}\\\\ {-\\epsilon-2\\operatorname*{max}\\{L,1\\}\\left(\\frac{c}{\\sqrt{n}}\\right)^{\\operatorname*{min}\\{\\alpha,1\\}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "image", "img_path": "KcDcaVOW1S/tmp/64d22c29ecc083ffb60e571c8aa1c3d34c054c5a966e8284919acb51807a2286.jpg", "img_caption": ["Figure 2: Comparisons of the variants of CT-SSF using the real datasets. The specified miscoverage level is $\\alpha=0.1$ . The blue bar chart represents Width and the gray bar represents Coverage, the blue line represents the target coverage. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "3. Quantile Stability. Given a calibration set $D_{c a}$ , the quantile of the band length is stable in both the feature space and output space, namely, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{D\\sim P_{n}}\\left[Q_{1-\\alpha}(V_{D}^{f})-Q_{1-\\alpha}(V_{D_{c a}}^{f})\\right]\\leq\\frac{c}{\\sqrt{n}}}\\\\ &{}\\\\ &{\\mathbb{E}_{D\\sim P_{n}}\\left[Q_{1-\\alpha}(V_{D}^{o})-Q_{1-\\alpha}(V_{D_{c a}}^{o})\\right]\\leq\\frac{c}{\\sqrt{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "text", "text": "Let $\\tilde{w}_{i}$ denote the weights learned from Algorithm 1. Suppose that $\\tilde{w}_{i}$ \u2019s are independent of the calibration and test nonconformity scores. CT-SSF provably outperforms NexCP in terms of average band length, namely, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[H(Q_{1-\\alpha}(V_{D_{c a}}^{f}),X_{t e s t})\\right]<Q_{1-\\alpha}(V_{D_{c a}}^{o}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the expectation is taken over the calibration fold and the testing point $(X_{t e s t},Y_{t e s t})$ . ", "page_idx": 17}, {"type": "text", "text": "Before proceeding with the proof, we note that Theorem 1 extends Theorem 4 from [40] to accommodate non-exchangeable settings. This adaptation is feasible because the foundational assumptions and methodologies hinge primarily on quantile calculations rather than the specific characteristics of the underlying distributions. Thus, we can leverage the framework of their proof to construct our argument effectively. ", "page_idx": 17}, {"type": "text", "text": "Proof. By the expansion assumption, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Big[Q_{1-\\alpha}(V_{D}^{f})-V_{D}^{f}\\Big]^{\\alpha}<\\mathbb{E}_{D}\\mathbb{M}\\left[Q_{1-\\alpha}(H(V_{D}^{f},D))-H(V_{D}^{f},D)\\right]-\\epsilon-2\\operatorname*{max}\\{L,1\\}\\left(\\frac{c}{\\sqrt{n}}\\right)^{\\operatorname*{min}\\{\\alpha,1\\}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We rewrite it as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathfrak{z}_{D}\\mathbb{M}H(V_{D}^{f},D)<\\mathbb{E}_{D}Q_{1-\\alpha}(H(V_{D}^{f},D))-\\epsilon-2\\operatorname*{max}\\{L,1\\}\\left(\\frac{c}{\\sqrt{n}}\\right)^{\\operatorname*{min}\\{\\alpha,1\\}}-L\\mathbb{E}_{D}\\mathbb{M}\\left[Q_{1-\\alpha}(V_{D}^{f})-\\epsilon\\right]\\left(\\log\\alpha\\right)^{1/2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Due to the H\u00f6lder condition, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{M}H(Q_{1-\\alpha}(V_{D}^{f}),D)<\\mathbb{M}(H(V_{D}^{f},D))+L\\mathbb{M}\\left|Q_{1-\\alpha}(V_{D}^{f})-V_{D}^{f}\\right|^{\\alpha},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "therefore ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{D}\\mathbb{M}\\left[H(Q_{1-\\alpha}(V_{D}^{f}),D)\\right]<\\mathbb{E}_{D}Q_{1-\\alpha}\\left(H(V_{D}^{f},D)\\right)-\\epsilon-2\\operatorname*{max}\\{1,L\\}\\left(\\frac{c}{\\sqrt{n}}\\right)^{\\operatorname*{min}\\{1,\\alpha\\}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, due to Assumption 1, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{D}\\mathbb{M}H(Q_{1-\\alpha}(V_{D}^{f}),D)<\\mathbb{E}_{D}Q_{1-\\alpha}(V_{D}^{\\circ})-2\\operatorname*{max}\\{1,L\\}\\left(\\frac{c}{\\sqrt{n}}\\right)^{\\operatorname*{min}\\{1,\\alpha\\}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Additionally, according to the quantile stability assumption, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{D}\\mathbb{M}\\left|H(Q_{1-\\alpha}(V_{D}^{f}),D)-H(Q_{1-\\alpha}(V_{D_{c a}}^{f}),D)\\right|\\leq L\\left(\\frac{c}{\\sqrt{n}}\\right)^{\\alpha}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{D}\\left[Q_{1-\\alpha}(V_{D}^{o})-Q_{1-\\alpha}(V_{D_{c a}}^{o})\\right]\\leq\\frac{c}{\\sqrt{n}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}H(Q_{1-\\alpha}(V_{D_{c a}}^{f}),X^{\\prime})=\\mathbb{E}_{D}\\mathbb{M}H(Q_{1-\\alpha}(V_{D_{c a}}^{f}),D)}\\\\ &{\\phantom{\\mathbb{E}}<Q_{1-\\alpha}(V_{D_{c a}}^{0})-2\\operatorname*{max}\\{1,L\\}\\left(\\frac{c}{\\sqrt{n}}\\right)^{\\operatorname*{min}\\{1,\\alpha\\}}+L\\left(\\frac{c}{\\sqrt{n}}\\right)^{\\alpha}+\\frac{c}{\\sqrt{n}}}\\\\ &{\\phantom{\\mathbb{E}}<Q_{1-\\alpha}(V_{D_{c a}}^{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C.2 Coverage Guarantee ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Since the coverage for CP depends on the ranking of the nonconformity measures rather than their actual values ([35, 37]), we can extend the work of [7] to the semantic feature space, where we construct the nonconformity score and compute the corresponding quantile value. ", "page_idx": 18}, {"type": "text", "text": "Theorem 2 (Coverage Gap; Adaptation from [7], Theorem 2). Let \u2206Cov denote the coverage gap between the empirical coverage and the desired coverage level. Let $\\tilde{w}_{i}$ denote the weights learned from Algorithm 1. Suppose that $\\tilde{w}_{i}$ \u2019s are independent of the calibration and test nonconformity scores. Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Delta C o\\nu\\ge-\\sum_{i=1}^{t}\\tilde{w}_{i}\\cdot d_{T V}(R(Z),R(Z^{i})),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $d_{T V}$ represents the total variation distance, and $R(Z)$ denotes the sequence of absolute residuals on $Z$ and $Z^{i}$ denotes a sequence where the test point $Z_{t+1}$ is swapped with the $i$ -th calibration point. Note that these sequences are represented by the semantic features here. ", "page_idx": 18}, {"type": "text", "text": "This coverage gap suggests that assigning lower weights to time steps with high total variation distance can help maintain small coverage gaps, reinforcing the theoretical foundation of our method. CT-SSF employs an attention mechanism to allocate higher weights to data points from the same error distribution as the current one and lower weights otherwise. According to Theorem 2, our dynamic weighting strategy can effectively minimize the coverage gap. ", "page_idx": 18}, {"type": "text", "text": "D Potentional Social Impact ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our method significantly can enhance decision-making in finance, healthcare, and environmental monitoring by improving the accuracy and reliability of time series predictions. The success of our method, however, depends on the quality of training data used. This highlights the critical need for robust data management to maintain data integrity in dynamic environments. By upholding high data quality standards, our method ensures reliable support across these vital sectors, fostering trust in automated decision-making systems. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: The abstract and introduction introduce our contributions to enhance prediction efficiency and coverage for time-series data by leveraging deep representations to dynamically adjust weights in the semantic feature space. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We discuss the limitations of our work in section 6. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide the assumptions and proof of the theory in section 5 and Appendix C. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We discuss these information in section 5 and the appendix B. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide open access to our code in section 5. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: : We specify these conditions in section 5 and appendix B. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We provide standard errors for the experimental results with different random seed. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We discuss the information for computer resources to reproduce the experiments in appendix B. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The research upholds standards of research integrity, ensuring ethical considerations in data handling and experimentation, promoting inclusivity, maintaining transparency for reproducibility, ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We discuss the potential social impact of our research in appendix D. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake proflies, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety fliters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper appropriately credits the creators and original owners of all assets used, such as code, data, and models, and clearly mentions the license and terms of use for each. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip flie. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}]