[{"figure_path": "jjcY92FX4R/tables/tables_5_1.jpg", "caption": "Table 2: The average frame size (F) and canonicalization size (C) on EXP with two canonicalization algorithms: FA-graph and OAP-graph.", "description": "This table compares the average frame size and canonicalization size of two different canonicalization algorithms, FA-graph and OAP-graph, on the EXP dataset.  The frame size represents the number of group actions considered for frame averaging, while the canonicalization size represents the number of distinct canonical forms generated. The table shows that OAP-graph consistently has a significantly smaller canonicalization size than FA-graph, demonstrating its superior efficiency.", "section": "5.1 Expressive Power and Frame Size"}, {"figure_path": "jjcY92FX4R/tables/tables_5_2.jpg", "caption": "Table 2: The average frame size (F) and canonicalization size (C) on EXP with two canonicalization algorithms: FA-graph and OAP-graph.", "description": "This table presents a comparison of the average frame size and canonicalization size for two different canonicalization algorithms, FA-graph and OAP-graph, applied to the EXP dataset. The frame size represents the number of forward passes required during frame averaging, while the canonicalization size represents the size of the canonical form. The table demonstrates that OAP-graph significantly reduces the frame/canonicalization size compared to FA-graph, suggesting increased efficiency.", "section": "4 Exploring Optimal Canonicalization of Eigenvectors"}, {"figure_path": "jjcY92FX4R/tables/tables_7_1.jpg", "caption": "Table 1: Accuracy on EXP.", "description": "This table shows the accuracy of different graph neural network models on the EXP dataset. The EXP dataset is designed to evaluate the expressive power of graph neural networks by including pairs of graphs that are non-isomorphic but 1-WL indistinguishable.  The results demonstrate that while standard GNN models achieve only around 50% accuracy, models that incorporate the proposed canonicalization or frame averaging achieve perfect accuracy (100%). This highlights the effectiveness of the proposed methods in improving the expressive power of GNNs.", "section": "5.1 Expressive Power and Frame Size"}, {"figure_path": "jjcY92FX4R/tables/tables_8_1.jpg", "caption": "Table 2: The average frame size (F) and canonicalization size (C) on EXP with two canonicalization algorithms: FA-graph and OAP-graph.", "description": "This table presents a comparison of the average frame size and canonicalization size for two different canonicalization algorithms (FA-graph and OAP-graph) on the EXP dataset.  The frame size represents the number of group elements considered during frame averaging, while the canonicalization size represents the number of distinct canonical forms. The F/C ratio indicates how much more efficient OAP-graph is than FA-graph in terms of the number of forward passes needed for averaging.", "section": "5.1 Expressive Power and Frame Size"}, {"figure_path": "jjcY92FX4R/tables/tables_8_2.jpg", "caption": "Table 3: Ratio of non-canonicalizable eigenvectors on ZINC.", "description": "This table shows the percentage of eigenvectors that cannot be canonicalized by different methods for addressing sign and basis ambiguities on the ZINC dataset.  It demonstrates that OAP significantly outperforms other methods (FA-lap and MAP) in handling basis ambiguity, while all methods perform similarly for sign ambiguity.", "section": "5.2 Graph Regression and Classification"}, {"figure_path": "jjcY92FX4R/tables/tables_8_3.jpg", "caption": "Table 4: Results on ZINC with 500K parameter budget. All scores are averaged over 4 runs with 4 different seeds.", "description": "This table presents the Mean Squared Error (MSE) results on the ZINC dataset for various graph neural network models using different positional encodings.  The models are GatedGCN and PNA, with different positional encoding methods: None, LapPE + RS (Laplacian Positional Encoding with Random Signs), SignNet, MAP (Ma et al.'s canonicalization), OAP (Orthogonal Axis Projection), and OAP + LSPE (OAP with Laplacian Spectral Positional Encoding layers). The table shows the performance of different methods with 500K parameter budget, averaged over four runs with four different seeds to ensure reliable results. OAP and OAP + LSPE show the best results.", "section": "5.2 Graph Regression and Classification"}, {"figure_path": "jjcY92FX4R/tables/tables_8_4.jpg", "caption": "Table 4: Results on ZINC with 500K parameter budget. All scores are averaged over 4 runs with 4 different seeds.", "description": "This table presents the results of graph regression experiments on the ZINC dataset.  Multiple models (GatedGCN and PNA) are evaluated, each with different positional encoding (PE) methods: None, LapPE + RS (Laplacian Positional Encoding with Random Signs), SignNet, MAP (Ma et al.'s canonicalization), and OAP (Orthogonal Axis Projection, the authors' proposed method).  The table shows the mean squared error (MSE) for each model and PE combination, averaged over four runs with different random seeds. The results highlight the performance improvements achieved by using the proposed OAP method compared to other baselines.  The \u2018k\u2019 column represents the number of eigenvectors used.", "section": "5.2 Graph Regression and Classification"}, {"figure_path": "jjcY92FX4R/tables/tables_8_5.jpg", "caption": "Table 6: Results on MOLPCBA. All scores are averaged over 4 runs with 4 different seeds.", "description": "This table presents the results of graph property prediction experiments on the MOLPCBA dataset.  Different positional encodings (PE) are used with GatedGCN and PNA backbones.  The table shows the average Area Under the Precision-Recall Curve (AUC-PR or AP\u2191) for each model and PE method, averaged over four runs with four different random seeds.  The results illustrate the performance improvements achieved by incorporating different positional encodings, particularly OAP.", "section": "5.2 Graph Regression and Classification"}, {"figure_path": "jjcY92FX4R/tables/tables_9_1.jpg", "caption": "Table 7: Comparison of time and memory of canonicalization methods with their non-FA backbone on ZINC. For the backbone models, the node features are first concatenated with positional encodings and fed to a positional encoding network (we use masked GIN in our experiments), then the outputs of the positional encoding network are used as input for the main network (GatedGCN or PNA). For the SignNet models, the positional encoding network is substituted with SignNet, which has a two-branch architecture. For models with MAP and OAP, the positional encodings are canonicalized before fed to the positional encoding network.", "description": "This table compares the pre-processing time, training time, total time, and memory usage of different models on the ZINC dataset.  The models are categorized by the backbone network used (GatedGCN or PNA) and whether they incorporate SignNet, MAP, or OAP for canonicalization. It highlights the computational overhead associated with different canonicalization techniques and the two-branch architecture of SignNet.", "section": "5 Experiments"}, {"figure_path": "jjcY92FX4R/tables/tables_18_1.jpg", "caption": "Table 2: The average frame size (F) and canonicalization size (C) on EXP with two canonicalization algorithms: FA-graph and OAP-graph.", "description": "This table compares the average frame size and canonicalization size of two different canonicalization algorithms, FA-graph and OAP-graph, on the EXP dataset.  The frame size represents the number of group actions considered in frame averaging, while the canonicalization size represents the number of unique canonical forms produced. The ratio of the frame size to the canonicalization size is also provided for each algorithm. This table demonstrates the computational efficiency advantage of canonicalization over frame averaging, particularly when dealing with highly symmetrical inputs.", "section": "5.1 Expressive Power and Frame Size"}, {"figure_path": "jjcY92FX4R/tables/tables_20_1.jpg", "caption": "Table 2: The average frame size (F) and canonicalization size (C) on EXP with two canonicalization algorithms: FA-graph and OAP-graph.", "description": "This table presents a comparison of the average frame size and canonicalization size for two different canonicalization algorithms (FA-graph and OAP-graph) on the EXP dataset.  It highlights the computational efficiency gains achieved by using canonicalization (OAP-graph) compared to frame averaging (FA-graph), showing that OAP-graph has significantly smaller sizes (and thus faster computation). The F/C ratio shows that OAP-graph is orders of magnitude more efficient than FA-graph. This table supports the claim that the canonicalization approach is superior to frame averaging in terms of efficiency.", "section": "5.1 Expressive Power and Frame Size"}, {"figure_path": "jjcY92FX4R/tables/tables_20_2.jpg", "caption": "Table 2: The average frame size (F) and canonicalization size (C) on EXP with two canonicalization algorithms: FA-graph and OAP-graph.", "description": "This table presents a comparison of the average frame size and canonicalization size for two different canonicalization algorithms (FA-graph and OAP-graph) on the EXP dataset.  It demonstrates the computational efficiency gains achieved by using canonicalization (OAP-graph) compared to frame averaging (FA-graph), especially highlighted by the significant reduction in the F/C ratio for OAP-graph across different graph sizes.", "section": "5.1 Expressive Power and Frame Size"}, {"figure_path": "jjcY92FX4R/tables/tables_28_1.jpg", "caption": "Table 2: The average frame size (F) and canonicalization size (C) on EXP with two canonicalization algorithms: FA-graph and OAP-graph.", "description": "This table compares the average frame size and canonicalization size for two different canonicalization algorithms (FA-graph and OAP-graph) on the EXP dataset.  The frame size represents the number of group elements considered in frame averaging, while the canonicalization size indicates the number of unique canonical forms. The F/C ratio shows how much larger the frame size is compared to the canonicalization size, illustrating the efficiency gain of using canonicalization. The FA/OAP ratio compares the average frame size of FA-graph to the average frame size of OAP-graph for each of the three dataset sizes, showing the reduction in frame size achieved by the OAP-graph algorithm.", "section": "5.1 Expressive Power and Frame Size"}, {"figure_path": "jjcY92FX4R/tables/tables_35_1.jpg", "caption": "Table 8: Hyper-parameter settings of different methods in the n-body experiment with dimension d = 3.", "description": "This table shows the hyper-parameters used for different methods in the n-body experiment when the dimension d is set to 3.  It lists the number of layers (L), hidden dimension (h), and the total number of parameters (#param) for each method, including Frame Averaging, Sign Equivariant, OAP-eig, and OAP-lap.", "section": "4.2 Better Canonicalization with Permutation"}, {"figure_path": "jjcY92FX4R/tables/tables_36_1.jpg", "caption": "Table 9: Hyper-parameter settings of different models with different PE methods on ZINC.", "description": "This table lists the hyperparameter settings used in the ZINC experiment for different models and positional encodings (PE).  It includes the number of eigenvectors (k), the number of layers (L1, L2), the hidden dimension (h1, h2, h3), the learning rate (\u03bb), the patience (t) and the factor (r) of the learning rate scheduler, the minimum learning rate (\u03bbmin), and the output dimension of SignNet or the normal GNN (when using canonicalization as PE).", "section": "5.2 Graph Regression and Classification"}, {"figure_path": "jjcY92FX4R/tables/tables_36_2.jpg", "caption": "Table 9: Hyper-parameter settings of different models with different PE methods on ZINC.", "description": "This table presents the hyperparameter settings used in the ZINC experiment for different models.  It includes the number of eigenvectors (k), the number of layers (L1, L2) and hidden dimensions (h1, h2, h3) for the base model and the optional SignNet/GNN, the learning rate (\u03bb), the patience and factor for the learning rate scheduler (t, r), the minimum learning rate (\u03bbmin), and the hidden dimension (h3) when using canonicalization as PE.", "section": "5.3 ZINC experiment"}, {"figure_path": "jjcY92FX4R/tables/tables_36_3.jpg", "caption": "Table 9: Hyper-parameter settings of different models with different PE methods on ZINC.", "description": "This table lists the hyperparameters used for different graph neural network models on the ZINC dataset.  The models are evaluated with different positional encodings (PE): None, LapPE+RS, SignNet, MAP, OAP, and OAP+LSPE. The hyperparameters include the number of eigenvectors (k), number of layers (L1 and L2), hidden dimensions (h1, h2, h3), learning rate (\u03bb), patience (t), learning rate decay factor (r), minimum learning rate (\u03bbmin), etc. The table allows comparison of hyperparameter choices based on PE methods.", "section": "5.3 ZINC experiment"}]