[{"figure_path": "7ESHFpqjNO/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of models and objective. a) Illustration of the spatial objective: locations that are close should be encoded by similar population vectors, distant locations by dissimilar population vectors. b) Similar to a); similar context signals should be represented by similar population vectors, dissimilar contexts by dissimilar population vectors. c) Similar to a) and b), but for joint encoding of space and context. d) Ratemaps of randomly selected units in networks trained to minimize the spatial objective function. Shown are learned representations for a recurrent network performing simultaneous path integration, and a feedforward network performing spatial encoding.", "description": "This figure provides a visual overview of the models and objectives used in the study. Panel (a) illustrates the core concept of the spatial objective function: nearby locations should have similar neural representations, while distant locations should have dissimilar representations. Panel (b) extends this concept to include contextual information, showing that similar contexts should also have similar representations. Panel (c) combines both spatial and contextual information, demonstrating that the model can learn joint representations of space and context. Finally, panel (d) shows example ratemaps (a visualization of neural activity) from both a recurrent neural network (RNN) and a feedforward neural network (FF) trained to minimize the spatial objective function. These ratemaps show the spatial selectivity of the neurons, illustrating the emergence of place-like representations.", "section": "Results & discussion"}, {"figure_path": "7ESHFpqjNO/figures/figures_4_1.jpg", "caption": "Figure 2: Feedforward network results. a) Training loss for different parameter combinations. Line shows the mean of 10 models and error bands show the min and max across models. Note that training data is generated continuously. b) Example ratemaps of randomly selected active units for models with different scale parameters \u03c3. Color scale is relative to individual unit activity. c) Distributions of the proportion of active units (mean rate > 0) for different parameter combinations across 10 models. d) Distribution of mean rate of units for each parameter combination (shown for one example model each). e) Field sizes in pixels for each parameter combination (shown for one example model each). f) Left: Example target similarity structure relative to center. Middle: corresponding similarity for the learned representations of model with \u03c3 = 0.25 and \u03b2 = 0.5. Right: difference between target and learned similarity. g) Peak locations of all units for different parameter combinations (shown for one example model each). h) Mean position decoding error as a function of the number of units used for Top n decoding. Dashed line shows the naive case where every decoded position is at the center. i) and j) Mean and max decoding error for different decoding methods for trained 10 models, each with \u03c3 = 0.25 and \u03b2 = 0.5. k) Example decoding error maps for different decoding methods (\u03c3 = 0.25 and \u03b2 = 0.5).", "description": "This figure shows the results of training a feedforward network using the similarity-based objective function. It demonstrates how different hyperparameters affect the learned representations, including the training loss, the spatial tuning of the units (ratemaps), the proportion of active units, and the accuracy of position decoding. The figure also compares the similarity structure between the target and the learned representations.", "section": "2.2 Feedforward networks learn place cell-like representations"}, {"figure_path": "7ESHFpqjNO/figures/figures_4_2.jpg", "caption": "Figure 3: Feedforward network remapping results. a) Ratemaps as a function of context, for a random selection of 10 units. Each row corresponds to one unit and each column to a particular context value. b) Example distribution of spatial correlations for ratemaps corresponding to two distinct contexts (context 1 =-0.9, context 2 = 1.2). c) Median spatial correlations when comparing across all contexts. d) Number of units included (units active in both contexts) in the analysis in c).", "description": "This figure demonstrates the remapping behavior of feedforward networks trained to encode multiple contexts.  Panel (a) shows ratemaps (spatial tuning curves) for 10 randomly selected units across different context values.  Notice how the spatial tuning changes significantly depending on the context, exhibiting global remapping. Panel (b) shows the distribution of spatial correlations between ratemaps for two specific contexts, demonstrating uncorrelated representations in different contexts. Panels (c) and (d) summarize the median spatial correlations and the number of units included in the correlation analysis across all contexts, further supporting the global remapping observation.", "section": "2.3 Feedforward networks learn global-type remapping"}, {"figure_path": "7ESHFpqjNO/figures/figures_5_1.jpg", "caption": "Figure 4: Recurrent network results with and without context. a) Ratemap examples of randomly selected units of a recurrent network without context. b) Example trajectories used for training. c) Training loss for recurrent networks with and without context (10 models each, error bands show min and max). d) Histogram of peak values of a recurrent network without context and example ratemaps of units of different parts of the distribution. e) Histogram of mean rates of a recurrent network without context. f) Similarity structure in the center location of the learned representations of a recurrent network without context (left) and the objective (center), as well as the difference between the two (right). g) Example trajectories decoded from network representations h) Comparison of the mean decoding error using a linear decoder or population decoding across trajectories for 10 different models each. i) 2D UMAP projection of spatial representations for different contexts.", "description": "This figure shows the results of experiments using recurrent neural networks to learn spatial representations with and without context.  Panel (a) displays example rate maps (spatial firing patterns) of units in a recurrent network trained without contextual information. Panel (b) shows example trajectories used during training. Panel (c) compares the training loss curves for networks trained with and without context. Panels (d) and (e) present histograms showing the distribution of peak firing rates and mean firing rates, respectively, for units in the network trained without context.  Panel (f) visually compares the similarity structure of the learned representations with the target similarity structure.  Panel (g) shows examples of decoded trajectories. Panel (h) compares the decoding errors from linear and population decoding methods. Finally, Panel (i) provides a 2D UMAP projection showing the spatial representations across different contexts.", "section": "2.4 Recurrent networks learn place- and band-like representations and path integration"}, {"figure_path": "7ESHFpqjNO/figures/figures_6_1.jpg", "caption": "Figure 5: Remapping by orthogonal transformations. a) Random global orthogonal transformations (reflection, rotation, and permuation) applied to a trained representation (top) all preserve the similarity objective (bottom left), while producing spatially decorrelated representations (bottom right). b) Best-fit orthogonal transformations applied to learned representations of a feedforward network across two contexts. Inset is the original representation, the orthogonally transformed representation, and the secondary representation alongside the difference between the two for example units. c) Jitter plot of Pearson correlation between ratemaps across contexts for the transformation in b); shading indicates mean unit activity. d) Mean squared error between transformed and original representations for random and best-fit orthogonal transforms across all learned contexts. e) Ratemaps of units where a learned representation (left of dashed line) is extended by a continous orthogonal representation into a novel representation (right of dashed line) without learning. Inset is the corresponding similarity structure, measured from the center of the enlarged environment.", "description": "This figure demonstrates how orthogonal transformations can be used to generate new representations that preserve the similarity structure but exhibit low spatial correlation with the original representation, similar to remapping. It shows how different global orthogonal transformations can be used to produce new representations that preserve the similarity structure but exhibit low spatial correlation with the original representation.  The figure also illustrates best-fit orthogonal transformations applied to the learned representations of a feedforward network across two contexts, showing the original representation, transformed representation and the difference between them.  It displays the Pearson correlation between ratemaps across contexts, the mean squared error between transformed and original representations, and demonstrates how orthogonal transformations can extend existing representations into novel ones while maintaining similarity structure.", "section": "2.5 Reuse by orthogonal transformation"}, {"figure_path": "7ESHFpqjNO/figures/figures_13_1.jpg", "caption": "Figure A1: Spatial representations without explicit position information. a) Ratemap examples of randomly selected units of the long-sequence recurrent network. b) Training loss of the long-sequence recurrent network (data created on the fly). c) Mean decoding error of a linear decoder and the population decoding scheme on 16 unseen long trajectories. The dashed line indicates a naive case in which the decoded position is always at the center of the environment. d) Learned (left) and target (middle) similarity structure, alongside their difference (right) relative to center of arena, for the long-sequence recurrent network.", "description": "This figure shows the results of training a recurrent neural network to learn spatial representations without providing explicit position information as input.  The network successfully learns place-like representations (a), minimizes the objective function over time (b), achieves accurate position decoding using both linear and population-based methods (c), and demonstrates close agreement between learned and target similarity structures (d).  The results demonstrate that place-like representations can emerge even without explicit position information.", "section": "Appendix / supplemental material"}, {"figure_path": "7ESHFpqjNO/figures/figures_15_1.jpg", "caption": "Figure A2: Loss ablation and effect of similarity measure. a) Ratemaps of randomly selected feedforward network units, when ablating \u03bb. b) As in a), but for ablating \u03b2. c) As in a) and b), but for ablating both \u03b2 and \u03bb. d) Ratemaps of trained feedforward units when the squared distance of the similarity measure is replaced by the Euclidean distance.", "description": "This figure shows the results of an ablation study on the hyperparameters \u03bb and \u03b2 in the loss function, as well as the impact of changing the similarity measure from Gaussian to Euclidean distance.  The ablation study reveals that place-like tuning requires either a non-zero \u03b2 (similarity threshold) or a non-zero \u03bb (activity regularization). Using Euclidean distance instead of the squared Gaussian distance maintains place-like tuning but results in more variable field sizes.", "section": "B Loss ablation and effects of \u03b2"}, {"figure_path": "7ESHFpqjNO/figures/figures_16_1.jpg", "caption": "Figure A3: Hyperdimensional computing and the effect of \u03b2. a) Histogram of squared Euclidean distances between 512 randomly sampled vectors of different number of dimensions (legend) on the corresponding n-sphere. b) Distribution of population vector norms for a trained feedforward network with \u03b2 = 0.5, \u03c3 = 0.25, and \u03bb = 0.1. c) Histograms of the number of place fields for different parameter configurations (inset). d) Ratemaps of randomly selected units of a trained feedforward network with \u03bb = 0.1, \u03c3 = 0.25, across different contexts for different values of \u03b2. For each value of \u03b2, one row represents one unit and each column one context value. Context values increase linearly from -2 (leftmost column) to 2 (rightmost column).", "description": "Figure A3 shows the effect of the hyperparameter \u03b2 on the learned representation.  Panel (a) demonstrates that high-dimensional vectors tend to be equidistant from one another. Panel (b) shows the distribution of population vector magnitudes for a trained network, demonstrating that most vectors have magnitudes near 1. Panel (c) illustrates the effect of \u03b2 and \u03c3 on the number of place fields per unit, and panel (d) presents a set of ratemaps demonstrating how increasing \u03b2 leads to more robust remapping across contexts.", "section": "B Loss ablation and effects of \u03b2"}, {"figure_path": "7ESHFpqjNO/figures/figures_18_1.jpg", "caption": "Figure A1: Spatial representations without explicit position information. a) Ratemap examples of randomly selected units of the long-sequence recurrent network. b) Training loss of the long-sequence recurrent network (data created on the fly). c) Mean decoding error of a linear decoder and the population decoding scheme on 16 unseen long trajectories. The dashed line indicates a naive case in which the decoded position is always at the center of the environment. d) Learned (left) and target (middle) similarity structure, alongside their difference (right) relative to center of arena, for the long-sequence recurrent network.", "description": "This figure demonstrates that even without explicit position information provided to the network, the recurrent network can still learn spatial representations similar to place cells.  It displays rate maps (spatial firing patterns of neurons), training loss curves, decoding error plots (comparing the network's estimated location to the actual location), and a comparison of the learned and target similarity structures.  The results show that the network successfully minimizes the similarity-based objective function and learns representations that are both place-like and decodable, highlighting the robustness of the proposed approach even under less constrained input conditions.", "section": "Appendix / supplemental material"}]