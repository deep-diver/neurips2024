[{"figure_path": "OYOkkqRLvj/figures/figures_0_1.jpg", "caption": "Figure 1: A comparison that illustrates the forward execution time of three linear algebra operations: qr, eigh, and svd, when performed on a 10000 \u00d7 10000 matrix using PyTorch and JAX. The presented values represent the mean ratios of the execution time relative to that of matrix multiplication (matmul) of 100 runs.", "description": "This figure compares the computation time of QR decomposition, eigenvalue decomposition (eigh), and singular value decomposition (svd) against matrix multiplication.  The comparison uses 10000x10000 matrices, with execution times averaged over 100 runs, in PyTorch and JAX. It demonstrates that eigendecomposition and SVD are significantly slower than QR decomposition and matrix multiplication, highlighting a key computational challenge addressed in the paper.", "section": "1 Introduction"}, {"figure_path": "OYOkkqRLvj/figures/figures_3_1.jpg", "caption": "Figure 2: An illustration on disrupting the rotational symmetry of the trace loss. We aim to solve eigenvectors for a 2-dimensional symmetric matrix A = \\begin{pmatrix} 0.3 & 0.2\\\\ 0.2 & 0.1 \\end{pmatrix}, with three loss functions. We parameterize an orthonormal matrix U = \\begin{pmatrix} x\\\\ y \\end{pmatrix} subject to the constraint x\u00b2 + y\u00b2 = 1. The plot displays the contours of the landscapes of three different loss functions as they vary with x and y: (a) trace loss tr(UAUT), (b) Brockett's cost function tr(MUAUT) where M = diag(1,0.2) and (c) convex loss function tr(f(U AUT)) where f is an element-wise exponential function. The feasible area of the constraint is depicted with a black circle. The red stars signify the optima of the loss in the feasible area. The dashed grey lines represent the true eigenvector direction of A. We see that, the trace loss results in infinitely many optimal solutions due to its rotational symmetry. In contrast, both Brockett's cost function and the convex loss function reshape the optimization landscape, breaking this symmetry and leading to the identification of the correct eigenvectors.", "description": "This figure compares three different loss functions for finding eigenvectors of a 2D symmetric matrix.  It shows how the trace loss suffers from rotational symmetry, leading to many optimal solutions, while Brockett's cost function and a convex trace loss break this symmetry and yield unique optima corresponding to the true eigenvectors. The plots illustrate the loss function landscapes with contour lines, highlighting the optimal points (red stars).", "section": "Rotational Symmetry"}, {"figure_path": "OYOkkqRLvj/figures/figures_6_1.jpg", "caption": "Figure 3: Convergence analysis on finding 50 largest eigenvalues on random 1000\u00d71000-dimensional symmetric matrices. (a): Convergence curves using Brockett\u2019s cost and convex trace loss (f(x) = x1.5). (b) The fine-tuning convergence on a series of similar matrices.", "description": "This figure shows the convergence speed and accuracy of different optimization algorithms in finding the top 50 eigenvalues of random 1000x1000 symmetric matrices using two different loss functions: Brockett's cost function and a convex trace loss function.  Subfigure (a) compares the convergence curves of various optimizers (Adam, Adamax, Yogi, SGD, L-BFGS) for both loss functions. Subfigure (b) demonstrates the fine-tuning capability of the approach on a sequence of similar matrices, highlighting the stability and robustness of the method.", "section": "5.1 Convergence"}, {"figure_path": "OYOkkqRLvj/figures/figures_8_1.jpg", "caption": "Figure 4: A comparison of scaling in the latent PCA task using the Celeb-A-HQ (256x256) dataset. The backbone autoencoders used in this study consist entirely of fully-connected layers with ReLU activation, all maintaining the same dimensions. Between the encoder and decoder, we applied both an eigen solver from the JAX eigh function and our amortized eigendecomposition method. We varied the depth of the autoencoders across 8, 16, 32, and 64 layers, and explored dimensionalities of 128, 256, 512, 1024, 2048, and 4096. The results present the average execution time per iteration over 100 runs. Notably, the largest model tested, featuring an autoencoder with 64 layers and a dimension of 4096, comprises up to 1.0 billion parameters.", "description": "This figure compares the performance of different autoencoder architectures in a latent PCA task using the Celeb-A-HQ dataset.  It shows the wall-clock time per iteration for various hidden dimensions and depths of the autoencoder.  The comparison includes a baseline autoencoder, one with JAX's eigh function for eigendecomposition, and one using the proposed amortized eigendecomposition method.  The results highlight the significant speedup achieved by the proposed method, particularly for larger and deeper models.", "section": "5.3 Latent-space Principle Component Analysis"}, {"figure_path": "OYOkkqRLvj/figures/figures_13_1.jpg", "caption": "Figure 5: The convergence curve of the singular values.", "description": "This figure shows the convergence analysis of the amortized eigendecomposition approach for estimating singular values in the nuclear norm regularization problem.  Three different hidden layer dimensions (128, 256, and 512) are used, and the mean squared error (MSE) between the estimated singular values and the exact singular values is plotted against the number of training steps.  The results demonstrate that the method converges rapidly to the exact singular values, even with high dimensionality.", "section": "A.1 Nuclear Norm Regularization"}, {"figure_path": "OYOkkqRLvj/figures/figures_14_1.jpg", "caption": "Figure 6: Experimental results of Latent-space PCA on MNIST dataset. (a) Convergence curves. First column: Convergence curve of reconstruction loss. Second column: Convergence curve of eigen loss. (b) Principle components in latent space: The two principle components of features in latent space for \u03b7 = 0 and \u03b7 = 1. (c) The sparsity of the network structure. The weight matrices of the second layer of the encoder. The color indicates the scale of the absolute values of the weight matrix ranging from 0 to 1.", "description": "This figure presents the experimental results of the Latent-space Principle Component Analysis (PCA) on the MNIST dataset.  It shows the convergence of both reconstruction and eigen losses for different hidden dimensions (256, 512, 1024).  The visualization of the latent space, with two principle components, for both regularized (\u03b7=1) and unregularized (\u03b7=0) cases, illustrates the effect of regularization. Finally, the sparsity of the network weight matrices (second layer of the encoder) is shown, highlighting the differences in sparsity between regularized and unregularized scenarios.", "section": "5.3 Latent-space Principle Component Analysis"}, {"figure_path": "OYOkkqRLvj/figures/figures_15_1.jpg", "caption": "Figure 3: Convergence analysis on finding 50 largest eigenvalues on random 1000 \u00d7 1000-dimensional symmetric matrices. (a): Convergence curves using Brockett\u2019s cost and convex trace loss (f(x) = x1.5). (b) The fine-tuning convergence on a series of similar matrices.", "description": "This figure shows the convergence analysis of the proposed amortized eigendecomposition approach in finding eigenvalues.  Subfigure (a) presents convergence curves for Brockett\u2019s cost function and a convex trace loss function using several optimization algorithms (Adam, Adamax, Yogi, SGD, and L-BFGS). Subfigure (b) demonstrates the fine-tuning convergence behavior on a series of similar matrices, further highlighting the robustness and efficiency of the approach. The results illustrate that the method accurately and efficiently finds the eigenvalues even with fine-tuning on similar matrices.", "section": "5 Experiments"}]