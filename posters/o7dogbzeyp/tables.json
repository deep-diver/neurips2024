[{"figure_path": "o7DOGbZeyP/tables/tables_6_1.jpg", "caption": "Table 1: Top-1 acc. (%) for ViT-B models trained on ImageNet for 150 epochs; trained and tested at 2242. We report the best and average results across our 8-run hyper-parameter sweep.", "description": "This table presents the Top-1 accuracy results for various Vision Transformer (ViT) models trained on the ImageNet dataset for 150 epochs.  The models were trained and evaluated at a resolution of 224x224 pixels. The table shows both the best and average accuracy achieved across eight hyperparameter sweeps for each method. The methods compared include several different position encoding techniques, including LookHere variants and established baselines.  The results are presented for ImageNet's standard validation set and several other benchmark test sets to show overall performance and extrapolation capability.", "section": "4.2 Results and Analysis"}, {"figure_path": "o7DOGbZeyP/tables/tables_7_1.jpg", "caption": "Table 1: Top-1 acc. (%) for ViT-B models trained on ImageNet for 150 epochs; trained and tested at 2242. We report the best and average results across our 8-run hyper-parameter sweep.", "description": "This table presents the Top-1 accuracy results for various position encoding methods applied to Vision Transformers (ViTs).  The models are all ViT-B, trained on ImageNet for 150 epochs at 224x224 resolution. The table shows the best and average Top-1 accuracy across 8 hyperparameter sweeps for each method, giving a robust comparison. The results indicate the performance of different position encoding techniques on a standard image classification task.", "section": "4.2 Results and Analysis"}, {"figure_path": "o7DOGbZeyP/tables/tables_7_2.jpg", "caption": "Table 1: Top-1 acc. (%) for ViT-B models trained on ImageNet for 150 epochs; trained and tested at 2242. We report the best and average results across our 8-run hyper-parameter sweep.", "description": "This table presents the top-1 accuracy results for various Vision Transformer (ViT) models trained on ImageNet dataset for 150 epochs.  The models are evaluated at a resolution of 224x224 pixels.  The table compares different position encoding methods, including LookHere (LH-180, LH-90, LH-45) and state-of-the-art methods such as 2D-ROPE. The \"Best\" column shows the highest accuracy achieved across 8 hyperparameter sweeps, while the \"Avg\" column represents the average accuracy across these runs. This provides insights into the performance variations resulting from hyperparameter tuning for different position encoding techniques.", "section": "4.2 Results and Analysis"}, {"figure_path": "o7DOGbZeyP/tables/tables_17_1.jpg", "caption": "Table 1: Top-1 acc. (%) for ViT-B models trained on ImageNet for 150 epochs; trained and tested at 2242. We report the best and average results across our 8-run hyper-parameter sweep.", "description": "This table presents the Top-1 accuracy results for various vision transformer models (ViT-B) trained on ImageNet for 150 epochs.  The models were trained and evaluated at a resolution of 224x224 pixels. The table shows both the best and average accuracy scores across eight different hyperparameter runs, allowing for a fair comparison between different position encoding methods. The results highlight the performance of various position encoding techniques on the standard ImageNet validation dataset.", "section": "4.2 Results and Analysis"}, {"figure_path": "o7DOGbZeyP/tables/tables_18_1.jpg", "caption": "Table 1: Top-1 acc. (%) for ViT-B models trained on ImageNet for 150 epochs; trained and tested at 2242. We report the best and average results across our 8-run hyper-parameter sweep.", "description": "This table presents the top-1 accuracy results for various vision transformer (ViT) models trained on ImageNet for 150 epochs and evaluated at a resolution of 224x224 pixels.  The table compares different position encoding methods, showing both the best and average performance across eight hyperparameter searches for each method. This allows for a fair comparison of the different methods' performance, and provides a more robust indication of their effectiveness beyond a single, potentially optimal hyperparameter setting. The models are all ViT-B models. The results are crucial for understanding the effectiveness of each position encoding in the context of the ViT architecture.", "section": "4.2 Results and Analysis"}, {"figure_path": "o7DOGbZeyP/tables/tables_18_2.jpg", "caption": "Table 1: Top-1 acc. (%) for ViT-B models trained on ImageNet for 150 epochs; trained and tested at 2242. We report the best and average results across our 8-run hyper-parameter sweep.", "description": "This table presents the top-1 accuracy results for various ViT-B models trained on ImageNet for 150 epochs. The models were trained and evaluated at a resolution of 224x224 pixels.  The table shows both the best and average top-1 accuracy across eight hyperparameter sweeps for each model. This allows for a fair comparison of different position encoding methods, and demonstrates the performance of LookHere in comparison to existing methods.", "section": "4.2 Results and Analysis"}, {"figure_path": "o7DOGbZeyP/tables/tables_21_1.jpg", "caption": "Table 1: Top-1 acc. (%) for ViT-B models trained on ImageNet for 150 epochs; trained and tested at 2242. We report the best and average results across our 8-run hyper-parameter sweep.", "description": "This table presents the Top-1 accuracy results achieved by various Vision Transformer (ViT) models trained on the ImageNet dataset for 150 epochs at a resolution of 224x224 pixels.  The models differ only in their position encoding methods. For each position encoding method, 8 different hyperparameter settings were evaluated.  The table shows both the best and average Top-1 accuracy across these 8 runs, providing a robust comparison of the effectiveness of different position encoding techniques in the context of ViT models.", "section": "4.2 Results and Analysis"}, {"figure_path": "o7DOGbZeyP/tables/tables_23_1.jpg", "caption": "Table 1: Top-1 acc. (%) for ViT-B models trained on ImageNet for 150 epochs; trained and tested at 2242. We report the best and average results across our 8-run hyper-parameter sweep.", "description": "This table presents the Top-1 accuracy results for various Vision Transformer (ViT) models trained on the ImageNet dataset for 150 epochs.  The models were trained and evaluated at a resolution of 224x224 pixels. The table shows both the best and average accuracy across eight different hyperparameter settings (8-run hyperparameter sweep) for each of the listed position encoding methods, providing a comprehensive comparison of their performance on the ImageNet validation set.", "section": "4.2 Results and Analysis"}, {"figure_path": "o7DOGbZeyP/tables/tables_24_1.jpg", "caption": "Table 1: Top-1 acc. (%) for ViT-B models trained on ImageNet for 150 epochs; trained and tested at 2242. We report the best and average results across our 8-run hyper-parameter sweep.", "description": "This table presents the Top-1 accuracy results for Vision Transformer (ViT)-B models trained on ImageNet for 150 epochs.  The models were trained and evaluated at a resolution of 224x224 pixels. The table shows the best and average performance across eight different hyperparameter settings for various position encoding methods, including the proposed LookHere method and several baseline methods.  The results provide a comparison of the performance of different position encoding techniques on image classification.", "section": "4.2 Results and Analysis"}, {"figure_path": "o7DOGbZeyP/tables/tables_25_1.jpg", "caption": "Table 1: Top-1 acc. (%) for ViT-B models trained on ImageNet for 150 epochs; trained and tested at 2242. We report the best and average results across our 8-run hyper-parameter sweep.", "description": "This table presents the Top-1 accuracy results for Vision Transformer-Base/16 (ViT-B/16) models trained on the ImageNet dataset for 150 epochs.  The models were trained and evaluated at a resolution of 224x224 pixels.  The table shows both the best and average performance across eight different hyperparameter settings, allowing for a robust comparison of different position encoding methods.", "section": "4.2 Results and Analysis"}, {"figure_path": "o7DOGbZeyP/tables/tables_25_2.jpg", "caption": "Table 1: Top-1 acc. (%) for ViT-B models trained on ImageNet for 150 epochs; trained and tested at 2242. We report the best and average results across our 8-run hyper-parameter sweep.", "description": "This table presents the top-1 accuracy results for various Vision Transformer (ViT) models trained on the ImageNet dataset for 150 epochs.  The models were trained and evaluated at a resolution of 224x224 pixels. The table shows both the best and average accuracy across eight hyperparameter runs for each model, enabling a comparison of performance and robustness across different position encoding methods.  The results are crucial for evaluating the effectiveness of different position encoding methods in vision transformers.", "section": "4.2 Results and Analysis"}]