[{"figure_path": "o7DOGbZeyP/figures/figures_1_1.jpg", "caption": "Figure 1: ViT-B/16 models trained for 150 epochs on ImageNet at 224<sup>2</sup> px and tested up to 1024<sup>2</sup> px. Model architectures are consistent between runs other than position encoding methods. We perform an 8-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at 1024<sup>2</sup>.", "description": "The figure shows the top-1 accuracy of different vision transformer models with various position encoding methods, trained at 224x224 resolution and tested at various resolutions up to 1024x1024.  The results highlight the impact of the chosen position encoding method on the model's ability to generalize to higher resolutions without further training (extrapolation). LookHere, a novel position encoding method proposed in the paper, demonstrates improved extrapolation performance compared to state-of-the-art methods like 2D-RoPE. The three variants of LookHere (LH-180, LH-90, LH-45) represent different field-of-view (FOV) configurations. The results indicate that LookHere models with narrower FOVs perform best at the highest tested resolution.", "section": "4 Experiments"}, {"figure_path": "o7DOGbZeyP/figures/figures_4_1.jpg", "caption": "Figure 2: LookHere masks and biases (center) the learned attention matrix (left, where colors are random). Masked cells are black, encoding directions (\u2192 with a 90\u00b0 FOV); biased cells are shaded bluish-green, encoding relative patch distances. (Right) An example of the FOV of the center query patch. The final attention matrix is computed as A\u00b9 = softmax(ALRN - AFIX), at each layer l.", "description": "This figure illustrates how LookHere modifies the attention mechanism in Vision Transformers. The left panel shows a learned attention matrix, where the colors represent the attention weights. The center panel displays the LookHere mask and bias matrices. The mask (black cells) restricts the attention to a specific field of view (FOV), while the bias (bluish-green shading) encodes the relative patch distances within the FOV. The right panel provides a visual representation of how the FOV affects the attention of the central query patch. Finally, the figure shows that the final attention matrix is calculated by applying a softmax function to the difference between the learned attention matrix and the LookHere matrix.", "section": "3 LookHere"}, {"figure_path": "o7DOGbZeyP/figures/figures_5_1.jpg", "caption": "Figure 3: Images of three classes from ImageNet-HR. (Bottom left is Anthony's niece Addison.)", "description": "This figure shows example images from the ImageNet-HR dataset, which is a high-resolution dataset created by the authors. The images are of three different classes: toucan, shovel, and balloon.  The caption points out one of the images is of the author's niece, highlighting the dataset's diversity and the effort taken to collect realistic, high-resolution images, rather than upscaling lower-resolution images.", "section": "Experiments"}, {"figure_path": "o7DOGbZeyP/figures/figures_6_1.jpg", "caption": "Figure 15: Measurements of head diversity, attention distance, and patch similarity by layer across position encoding methods.", "description": "This figure presents a comparison of several position encoding methods for vision transformers, focusing on how these methods affect the diversity of attention heads, the average distance between attended patches, and the similarity of representations between patches.  The results are shown across twelve transformer layers. Each metric is designed to help assess the inductive biases introduced by different positional encoding methods and their influence on the model's ability to extrapolate or generalize to input images of varying sizes.", "section": "A.8 Head Diversity, Attention Distance, Patch Similarity and Head Visualizations"}, {"figure_path": "o7DOGbZeyP/figures/figures_8_1.jpg", "caption": "Figure 1: ViT-B/16 models trained for 150 epochs on ImageNet at 2242 px and tested up to 10242 px. Model architectures are consistent between runs other than position encoding methods. We perform an 8-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at 10242.", "description": "This figure displays the performance of different position encoding methods on ImageNet classification task when tested at various resolutions (from 224x224 to 1024x1024).  The models were all trained at 224x224 resolution.  The results demonstrate that the LookHere method outperforms other methods, especially at higher resolutions, suggesting it is more effective at extrapolating to unseen image sizes.  The figure also shows that using smaller fields of view (FOV) in LookHere yields better results at higher resolutions.", "section": "4 Experiments"}, {"figure_path": "o7DOGbZeyP/figures/figures_8_2.jpg", "caption": "Figure 7: The effect of object size on accuracy gains or losses due to extrapolation. Object size is measured using annotations from Kaggle's ImageNet Object Localization Challenge [101].", "description": "This figure shows the impact of object size on the accuracy improvements or losses observed when extrapolating from a training resolution of 224x224 pixels to higher resolutions (384x384, 512x512, 768x768, and 1024x1024 pixels). The x-axis represents the object size quintile (1st, 2nd, 3rd, 4th, and 5th), and the y-axis represents the change in top-1 accuracy. It demonstrates that models trained with LookHere achieve substantially higher accuracy gains for smaller objects which occupy more patches during the test phase.", "section": "4.2 Results and Analysis"}, {"figure_path": "o7DOGbZeyP/figures/figures_9_1.jpg", "caption": "Figure 8: Attention maps of three attention heads across four resolutions, where the query is in the center. We use the colormap:", "description": "The figure displays attention maps for three different attention heads across four resolutions (224x224, 512x512, 768x768, and 1024x1024). Each head's attention is visualized as a heatmap, showing the regions of the image that the head attends to. The query is located in the center of each heatmap. This visualization helps to understand how the attention mechanism changes as the resolution of the input image increases. For example, at low resolution, attention may be more diffuse across the entire image, while at higher resolutions attention may be more focused on specific regions. The colormap illustrates the strength of attention, with brighter colors indicating stronger attention.", "section": "5 Closing"}, {"figure_path": "o7DOGbZeyP/figures/figures_19_1.jpg", "caption": "Figure 9: LH-180 bias matrices for query patch (11,8), grid size of 14x14.", "description": "This figure visualizes the bias matrices used in the LookHere model (specifically, the LH-180 variant) for a single query patch within a 14x14 grid. Each matrix represents how the attention mechanism is biased for a different attention head. Darker shades indicate stronger penalties on the attention scores, while lighter shades indicate weaker penalties. The arrangement of these matrices demonstrates how LookHere incorporates positional information and directionality into its attention mechanism. Each bias matrix influences how the attention head weighs different key patches in relation to the query patch.", "section": "A.3 LookHere Bias Matrices"}, {"figure_path": "o7DOGbZeyP/figures/figures_20_1.jpg", "caption": "Figure 9: LH-180 bias matrices for query patch (11,8), grid size of 14x14.", "description": "This figure shows the bias matrices used in the LookHere model for a specific query patch. LookHere uses 2D attention masks that restrict attention heads to fixed fields of view (FOV), pointing in different directions.  Each matrix represents the bias applied to the attention scores for a given attention head, encoding the relative patch distances within the FOV. The colormap represents the magnitude of the bias. Darker colors indicate stronger penalties, meaning less attention is given to those patches. The 'X' marks the location of the query patch.", "section": "A.3 LookHere Bias Matrices"}, {"figure_path": "o7DOGbZeyP/figures/figures_20_2.jpg", "caption": "Figure 9: LH-180 bias matrices for query patch (11,8), grid size of 14x14.", "description": "This figure visualizes the bias matrices used in the LookHere LH-180 model for a specific query patch. The bias matrices are designed to restrict the attention head's field of view (FOV) and penalize attention scores based on the relative distance between the query patch and other patches.  The visualization shows a 14x14 grid representing the patches, with the query patch located at (11,8). The color intensity of each cell indicates the bias value, with warmer colors representing lower bias (stronger attention) and cooler colors representing higher bias (weaker attention). The masks restrict attention to a specific region around the query patch.  The figure illustrates how the bias is spatially dependent, influencing attention scores in different directions from the central query patch.", "section": "A.3 LookHere Bias Matrices"}, {"figure_path": "o7DOGbZeyP/figures/figures_26_1.jpg", "caption": "Figure 12: More examples from ImageNet-S and each model\u2019s logit lens predictions.", "description": "This figure visualizes the logit lens predictions of several different vision transformer models on a subset of images from the ImageNet-S dataset.  The logit lens is a technique used to interpret the learned representations within a model by projecting the patch embeddings into the learned class embedding space. Each row represents a different model (LH-180, LH-90, LH-45, 2D-ALiBi, 2D-ROPE, Factorized, 2D-sincos, 1D-learn, Fourier, RPE-learn), and each column represents a different image from the ImageNet-S dataset. The top row shows the ground truth segmentation masks, while the subsequent rows depict how each model interprets and segments the corresponding images.  The figure shows how the various methods differ in their ability to capture fine-grained spatial information and accurately predict the class labels for each patch, highlighting differences in the spatial representations learned by different positional encoding techniques.", "section": "A.7 Logit Lens"}, {"figure_path": "o7DOGbZeyP/figures/figures_27_1.jpg", "caption": "Figure 13: We plot the average class identifiability [89] across the model layers on 1000 images from Val for the class and patch tokens. This is a measure of how recoverable the correct class is from the class projection of the token. The score ranges from 0 to 1, with 1 denoting that the correct class has the highest logits and 0 the lowest.", "description": "This figure shows the average class identifiability across different layers of the model for both CLS (class) tokens and patch tokens.  Class identifiability measures how easily the correct class can be identified from the token's class projection.  The scores range from 0 (not recoverable) to 1 (perfectly recoverable).  The figure allows comparison of the class identifiability across various position encoding methods (Fourier, RPE-learn, LookHere variants, 2D-ALiBi, 2D-ROPE, Factorized, 2D-sincos, 1D-learn)  across the layers of the model.  It helps in understanding the difference in how each position encoding method affects the ability to recover class information from the model's representations.", "section": "Results and Analysis"}, {"figure_path": "o7DOGbZeyP/figures/figures_28_1.jpg", "caption": "Figure 1: ViT-B/16 models trained for 150 epochs on ImageNet at 2242 px and tested up to 10242 px. Model architectures are consistent between runs other than position encoding methods. We perform an 8-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at 10242.", "description": "This figure displays the results of an experiment comparing different position encoding methods for Vision Transformers (ViTs) on the ImageNet dataset.  ViT-B/16 models were trained at 224x224 pixel resolution and then tested at various resolutions up to 1024x1024 pixels. The experiment aimed to assess the ability of these models to extrapolate to higher resolutions (i.e. generalize to larger images).  The plot shows the top-1 accuracy achieved by each method at different test resolutions. LookHere, the proposed method by the authors, shows better extrapolation performance compared to existing methods, especially at higher resolutions, with the narrower fields of view performing particularly well.", "section": "4 Experiments"}, {"figure_path": "o7DOGbZeyP/figures/figures_29_1.jpg", "caption": "Figure 15: Measurements of head diversity, attention distance, and patch similarity by layer across position encoding methods.", "description": "This figure presents measurements of head diversity, attention distance, and patch similarity by layer for various position encoding methods.  It helps to show how different methods influence the diversity of attention head behavior and how they impact the spatial focus and relationships between patches. The metrics provide insights into the learning patterns and spatial reasoning capabilities of Vision Transformers (ViTs) with different positional encoding schemes.", "section": "A.8 Head Diversity, Attention Distance, Patch Similarity and Head Visualizations"}, {"figure_path": "o7DOGbZeyP/figures/figures_30_1.jpg", "caption": "Figure 1: ViT-B/16 models trained for 150 epochs on ImageNet at 2242 px and tested up to 10242 px. Model architectures are consistent between runs other than position encoding methods. We perform an 8-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at 10242.", "description": "This figure presents the results of an experiment comparing different position encoding methods for vision transformers (ViTs) on the ImageNet dataset.  ViT-B/16 models were trained at 224x224 pixel resolution and then tested at various resolutions up to 1024x1024 pixels.  The experiment compared seven baseline position encoding methods and three variants of a novel method called LookHere. The results show that LookHere improves extrapolation ability, especially when using narrower fields of view at the highest resolutions.", "section": "4 Experiments"}, {"figure_path": "o7DOGbZeyP/figures/figures_31_1.jpg", "caption": "Figure 1: ViT-B/16 models trained for 150 epochs on ImageNet at 2242 px and tested up to 10242 px. Model architectures are consistent between runs other than position encoding methods. We perform an 8-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at 10242.", "description": "This figure shows the results of an experiment comparing different position encoding methods for Vision Transformers (ViTs) on the ImageNet dataset.  ViT-B/16 models were trained at 224x224 pixel resolution and then tested at various resolutions up to 1024x1024 pixels.  The experiment included three variants of the proposed method, LookHere, along with several other state-of-the-art (SoTA) methods.  The results show the top-1 accuracy for each method, demonstrating that LookHere variants generally outperform the SoTA methods and that those with narrower fields of view work best at the highest resolution.", "section": "4 Experiments"}, {"figure_path": "o7DOGbZeyP/figures/figures_32_1.jpg", "caption": "Figure 18: 1D-learn attention maps of ten attention heads across seven resolutions, where the query is in the center. We use the colormap: Averaged over 5k images.", "description": "This figure visualizes the attention maps of ten attention heads from a 1D-learn model at seven different resolutions (224x224, 320x320, 384x384, 448x448, 512x512, 768x768, and 1024x1024).  The colormap represents the attention weights, with darker colors indicating stronger attention.  The query patch is located in the center of each attention map. The visualizations are averaged across 5,000 images from the ImageNet dataset, giving an overall view of the attention patterns learned by the model at various resolutions.", "section": "A.8 Head Diversity, Attention Distance, Patch Similarity and Head Visualizations"}, {"figure_path": "o7DOGbZeyP/figures/figures_33_1.jpg", "caption": "Figure 18: 1D-learn attention maps of ten attention heads across seven resolutions, where the query is in the center. We use the colormap:  Averaged over 5k images.", "description": "This figure visualizes the attention maps of ten attention heads from a model using 1D-learn positional encoding. The attention maps are shown for seven different resolutions (224<sup>2</sup>, 320<sup>2</sup>, 384<sup>2</sup>, 448<sup>2</sup>, 512<sup>2</sup>, 768<sup>2</sup>, and 1024<sup>2</sup> pixels). Each row represents a different head, showing how its attention is distributed across the input image at various resolutions. The colormap indicates the attention weight, with darker colors representing higher attention weights and brighter colors representing lower attention weights. The query is always in the center of the image.  The attention maps are averaged across 5,000 images to highlight typical behavior.", "section": "A.8 Head Diversity, Attention Distance, Patch Similarity and Head Visualizations"}, {"figure_path": "o7DOGbZeyP/figures/figures_34_1.jpg", "caption": "Figure 2: LookHere masks and biases (center) the learned attention matrix (left, where colors are random). Masked cells are black, encoding directions (\u2192 with a 90\u00b0 FOV); biased cells are shaded bluish-green, encoding relative patch distances. (Right) An example of the FOV of the center query patch. The final attention matrix is computed as A\u00b9 = softmax(ALRN - AFIX), at each layer l.", "description": "This figure shows how LookHere modifies the attention matrix.  The left panel shows a learned attention matrix. The center panel shows the LookHere mask and bias matrix, with black cells indicating masked areas and bluish-green cells indicating biased areas. The right panel shows a visual example of the field of view (FOV) for a query patch. The final attention matrix is created by subtracting the LookHere matrix from the learned attention matrix and then applying a softmax function.", "section": "3 LookHere"}, {"figure_path": "o7DOGbZeyP/figures/figures_35_1.jpg", "caption": "Figure 1: ViT-B/16 models trained for 150 epochs on ImageNet at 2242 px and tested up to 10242 px. Model architectures are consistent between runs other than position encoding methods. We perform an 8-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at 10242.", "description": "This figure displays the top-1 accuracy results of different position encoding methods on ImageNet, with training conducted at 224x224 resolution and testing performed at various resolutions up to 1024x1024.  The results highlight the superior extrapolation capabilities of the proposed LookHere method, especially its variants with narrower fields of view (FOVs), compared to existing methods such as 2D-RoPE, 2D-ALiBi, etc.  The plot demonstrates that LookHere is robust to higher resolutions and significantly improves upon existing state-of-the-art position encodings in extrapolation tasks.", "section": "4 Experiments"}, {"figure_path": "o7DOGbZeyP/figures/figures_36_1.jpg", "caption": "Figure 1: ViT-B/16 models trained for 150 epochs on ImageNet at 224<sup>2</sup> px and tested up to 1024<sup>2</sup> px. Model architectures are consistent between runs other than position encoding methods. We perform an 8-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at 1024<sup>2</sup>.", "description": "This figure presents the results of an experiment comparing different position encoding methods for Vision Transformers (ViTs) on the ImageNet dataset.  ViT-B/16 models were trained at a resolution of 224x224 pixels and then tested at various resolutions up to 1024x1024 pixels.  The experiment included three variations of the proposed LookHere method, along with several other baseline methods. The plots show the Top-1 accuracy for each method at each tested resolution, demonstrating the superior extrapolation performance of LookHere, especially at higher resolutions.  Narrower fields of view within LookHere models yielded the best results at the highest resolution.", "section": "4 Experiments"}, {"figure_path": "o7DOGbZeyP/figures/figures_37_1.jpg", "caption": "Figure 2: LookHere masks and biases (center) the learned attention matrix (left, where colors are random). Masked cells are black, encoding directions (\u2192 with a 90\u00b0 FOV); biased cells are shaded bluish-green, encoding relative patch distances. (Right) An example of the FOV of the center query patch. The final attention matrix is computed as A\u00b9 = softmax(ALRN - AFIX), at each layer l.", "description": "This figure illustrates the LookHere method's core mechanism.  The left panel shows a learned attention matrix, with colors representing attention weights. The center panel displays the LookHere mask and bias matrix, applied to the attention matrix. Black cells in the mask indicate no attention (masked), while shaded cells show attention with bias (relative patch distance).  The right panel shows a visual representation of the field of view (FOV) for a single query patch.  The final step is element-wise subtraction of the LookHere matrix from the learned attention matrix before applying a softmax function to obtain the final attention weights (A\u00b9).", "section": "3 LookHere"}, {"figure_path": "o7DOGbZeyP/figures/figures_38_1.jpg", "caption": "Figure 1: ViT-B/16 models trained for 150 epochs on ImageNet at 2242 px and tested up to 10242 px. Model architectures are consistent between runs other than position encoding methods. We perform an 8-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at 10242.", "description": "This figure presents the results of the experiment comparing different positional encoding methods on ImageNet.  Models based on Vision Transformers (ViT-B/16) were trained at a resolution of 224x224 pixels (224\u00b2) for 150 epochs.  The performance of each model was then evaluated at various resolutions up to 1024x1024 pixels (1024\u00b2).  The figure shows that the proposed LookHere method outperforms other methods at higher resolutions (extrapolation).  Different variants of the LookHere method are presented with various fields of view (FOV). The hyperparameter search (8 runs) was carried out to ensure fair comparison among the methods.", "section": "4 Experiments"}, {"figure_path": "o7DOGbZeyP/figures/figures_39_1.jpg", "caption": "Figure 1: ViT-B/16 models trained for 150 epochs on ImageNet at 2242 px and tested up to 10242 px. Model architectures are consistent between runs other than position encoding methods. We perform an 8-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at 10242.", "description": "This figure shows the performance of different position encoding methods for Vision Transformers (ViTs) on ImageNet dataset.  ViT-B/16 models were trained at a resolution of 224x224 pixels (224\u00b2) and tested at various resolutions up to 1024x1024 pixels (1024\u00b2). The figure plots the top-1 accuracy for each method at different test resolutions.  The results demonstrate the impact of the position encoding method on the ability of ViTs to generalize to different image sizes (extrapolation).  The three LookHere variants (LH-180, LH-90, LH-45) consistently outperform other methods, particularly at higher resolutions. The narrow field-of-view variants of LookHere perform best at the highest resolution.", "section": "Experiments"}, {"figure_path": "o7DOGbZeyP/figures/figures_40_1.jpg", "caption": "Figure 27: LH-45 attention maps of ten attention heads across seven resolutions, where the query is in the center. We use the colormap:  Averaged over 5k images.", "description": "This figure visualizes the attention maps of ten attention heads of the LH-45 model at seven different resolutions (224x224, 320x320, 384x384, 448x448, 512x512, 768x768, and 1024x1024). The query is located in the center of each image, and the colormap represents the attention weights. The visualization is averaged over 5,000 images from the minival set. This figure helps in understanding how the attention mechanism behaves at different resolutions and how the LH-45 model focuses its attention.", "section": "A.8 Head Diversity, Attention Distance, Patch Similarity and Head Visualizations"}, {"figure_path": "o7DOGbZeyP/figures/figures_41_1.jpg", "caption": "Figure 27: LH-45 attention maps of ten attention heads across seven resolutions, where the query is in the center. We use the colormap: . Averaged over 5k images.", "description": "This figure visualizes the attention maps of ten attention heads from the LH-45 model at different resolutions (224x224, 320x320, 384x384, 448x448, 512x512, 768x768, and 1024x1024).  The colormap represents the attention weights, with darker shades indicating stronger attention. The query patch is located in the center. The figure shows how the attention patterns change as the resolution increases, highlighting the model's ability to adapt its attention mechanism to different image scales.", "section": "A.8 Head Diversity, Attention Distance, Patch Similarity and Head Visualizations"}, {"figure_path": "o7DOGbZeyP/figures/figures_42_1.jpg", "caption": "Figure 1: ViT-B/16 models trained for 150 epochs on ImageNet at 2242 px and tested up to 10242 px. Model architectures are consistent between runs other than position encoding methods. We perform an 8-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at 10242.", "description": "This figure displays the top-1 accuracy of different ViT-B/16 models trained on ImageNet at a resolution of 224x224 pixels and then tested on various resolutions ranging from 224x224 to 1024x1024 pixels. The models differ only in their position encoding methods.  The results show the performance of seven baseline position encoding methods and three variations of the LookHere method proposed in the paper. LookHere consistently outperforms the baseline methods, especially at higher resolutions.  The 8-run hyperparameter sweep ensures fair comparisons. The results highlight the superiority of LookHere, particularly its variants with narrower fields of view, in extrapolating to higher resolution images without further training.", "section": "4 Experiments"}, {"figure_path": "o7DOGbZeyP/figures/figures_43_1.jpg", "caption": "Figure 1: ViT-B/16 models trained for 150 epochs on ImageNet at 2242 px and tested up to 10242 px. Model architectures are consistent between runs other than position encoding methods. We perform an 8-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at 10242.", "description": "This figure displays the top-1 accuracy of different vision transformer models with varying position encoding methods.  The models were trained on ImageNet at a resolution of 224x224 pixels and then tested at resolutions ranging from 224x224 to 1024x1024 pixels.  The results show how well each method extrapolates to higher resolutions, without further training. The three variants of LookHere consistently outperform other methods, especially at the highest resolution (1024x1024).  The performance of LookHere variants with narrower fields of view is particularly noteworthy.", "section": "4 Experiments"}, {"figure_path": "o7DOGbZeyP/figures/figures_44_1.jpg", "caption": "Figure 1: ViT-B/16 models trained for 150 epochs on ImageNet at 2242 px and tested up to 10242 px. Model architectures are consistent between runs other than position encoding methods. We perform an 8-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at 10242.", "description": "This figure displays the performance of different position encoding methods on image classification using Vision Transformers (ViTs).  Models were trained on ImageNet at 224x224 pixel resolution and then tested at various resolutions ranging up to 1024x1024 pixels.  The results show the top-1 accuracy for each method.  The figure highlights that the authors' proposed LookHere method, particularly with narrower fields of view (FOVs), demonstrates superior extrapolation capabilities compared to existing state-of-the-art methods.", "section": "4 Experiments"}, {"figure_path": "o7DOGbZeyP/figures/figures_45_1.jpg", "caption": "Figure 1: ViT-B/16 models trained for 150 epochs on ImageNet at 2242 px and tested up to 10242 px. Model architectures are consistent between runs other than position encoding methods. We perform an 8-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at 10242.", "description": "This figure presents the results of an experiment comparing the performance of various position encoding methods for Vision Transformers (ViTs) on the ImageNet dataset.  ViT-B/16 models were trained at a resolution of 224x224 pixels and then evaluated at resolutions ranging from 224x224 to 1024x1024.  The results illustrate the impact of different position encoding techniques on the models' ability to extrapolate to higher resolutions. The authors' proposed method, LookHere, outperforms existing state-of-the-art (SOTA) methods, especially at the highest resolution (1024x1024).  The different variations of LookHere (LH-180, LH-90, LH-45) achieve slightly varying results with LH-45 showing the best performance at 1024x1024, demonstrating that narrower fields of view might be more beneficial for high-resolution extrapolation.", "section": "4 Experiments"}, {"figure_path": "o7DOGbZeyP/figures/figures_46_1.jpg", "caption": "Figure 1: ViT-B/16 models trained for 150 epochs on ImageNet at 2242 px and tested up to 10242 px. Model architectures are consistent between runs other than position encoding methods. We perform an 8-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at 10242.", "description": "This figure presents the results of the experiment comparing various position encoding methods for Vision Transformers (ViTs). Eight different ViT models (including three variants of the proposed LookHere method) were trained on ImageNet at 224x224 resolution and tested at resolutions up to 1024x1024 pixels. The results show that LookHere significantly improves extrapolation, achieving the best performance at the highest resolution, especially with narrower fields of view.", "section": "4 Experiments"}, {"figure_path": "o7DOGbZeyP/figures/figures_47_1.jpg", "caption": "Figure 1: ViT-B/16 models trained for 150 epochs on ImageNet at 2242 px and tested up to 10242 px. Model architectures are consistent between runs other than position encoding methods. We perform an 8-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at 10242.", "description": "This figure displays the top-1 accuracy of various ViT-B/16 models with different position encoding methods, tested at resolutions ranging from 224x224 to 1024x1024 pixels. The models were trained on ImageNet at 224x224 resolution for 150 epochs.  The consistent architecture across models ensures a fair comparison. Each method underwent an 8-run hyperparameter sweep. The results show that LookHere variants consistently outperform other methods, particularly at higher resolutions, with narrower fields of view performing best.", "section": "4 Experiments"}, {"figure_path": "o7DOGbZeyP/figures/figures_48_1.jpg", "caption": "Figure 1: ViT-B/16 models trained for 150 epochs on ImageNet at 2242 px and tested up to 10242 px. Model architectures are consistent between runs other than position encoding methods. We perform an 8-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at 10242.", "description": "This figure presents the results of an experiment comparing different position encoding methods for Vision Transformers (ViTs) on the ImageNet dataset.  Models were trained at a resolution of 224x224 pixels and then tested at resolutions ranging up to 1024x1024 pixels. The comparison shows the performance of various position encoding methods, including the proposed LookHere method and several baselines, in terms of top-1 accuracy.  The results illustrate the ability of the models to extrapolate to larger image sizes without further training, with LookHere showing significant improvement, particularly at the highest tested resolutions. The three LookHere variants (LH-180, LH-90, LH-45) each use different field-of-view (FOV) sizes for the attention masks. The figure shows that narrower FOVs result in better performance at higher resolutions.", "section": "4 Experiments"}, {"figure_path": "o7DOGbZeyP/figures/figures_49_1.jpg", "caption": "Figure 1: ViT-B/16 models trained for 150 epochs on ImageNet at 2242 px and tested up to 10242 px. Model architectures are consistent between runs other than position encoding methods. We perform an 8-run hyperparameter sweep, per method, to ensure fair comparisons. Our three LookHere variants improve extrapolation ability, with more narrow fields of view performing best at 10242.", "description": "This figure displays the performance of different position encoding methods on the ImageNet dataset when training is done at a resolution of 224x224 pixels and testing is performed at resolutions ranging from 224x224 to 1024x1024 pixels. The consistent model architecture across all runs helps in establishing a fair comparison. The results reveal that LookHere variants exhibit superior extrapolation capabilities, specifically the ones using narrower fields of view, showing the best performance at the highest tested resolution.", "section": "4 Experiments"}]