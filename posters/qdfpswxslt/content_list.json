[{"type": "text", "text": "Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ziyi Yang1,3 Xinyu Gao1 Yang-Tian Sun2 Yi-Hua Huang2 Xiaoyang Lyu2 Wen Zhou3 Shaohui Jiao3 Xiaojuan $\\mathbf{Q}^{\\mathbf{j}2\\dagger}$ Xiaogang Jin1\u2020 ", "page_idx": 0}, {"type": "text", "text": "1State Key Lab of CAD&CG, Zhejiang University   \n2The University of Hong Kong 3ByteDance Inc. ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The recent advancements in 3D Gaussian splatting (3D-GS) have not only facilitated real-time rendering through modern GPU rasterization pipelines but have also attained state-of-the-art rendering quality. Nevertheless, despite its exceptional rendering quality and performance on standard datasets, 3D-GS frequently encounters difficulties in accurately modeling specular and anisotropic components. This issue stems from the limited ability of spherical harmonics (SH) to represent high-frequency information. To overcome this challenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic spherical Gaussian (ASG) appearance field instead of SH for modeling the view-dependent appearance of each 3D Gaussian. Additionally, we have developed a coarse-to-fine training strategy to improve learning efficiency and eliminate floaters caused by overfitting in real-world scenes. Our experimental results demonstrate that our method surpasses existing approaches in terms of rendering quality. Thanks to ASG, we have significantly improved the ability of 3D-GS to model scenes with specular and anisotropic components without increasing the number of 3D Gaussians. This improvement extends the applicability of 3D GS to handle intricate scenarios with specular and anisotropic surfaces. Our codes and datasets are available at https://ingra14m.github.io/Spec-Gaussian-website. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "High-quality reconstruction and photorealistic rendering from a collection of images are crucial for a variety of applications, such as augmented reality/virtual reality (AR/VR), 3D content production, and art creation. Classic methods employ primitive representations, like meshes [39] and points [4, 67], and take advantage of the rasterization pipeline optimized for contemporary GPUs to achieve real-time rendering. In contrast, neural radiance fields (NeRF) [37, 5, 38] utilize neural implicit representation to offer a continuous scene representation and employ volumetric rendering to produce rendering results. This approach allows for enhanced preservation of scene details and more effective reconstruction of scene geometries. ", "page_idx": 0}, {"type": "text", "text": "Recently, 3D Gaussian Splatting (3D-GS) [23] has emerged as a leading technique, delivering stateof-the-art quality and real-time speed. This method optimizes a set of 3D Gaussians that capture the appearance and geometry of a 3D scene simultaneously, offering a continuous representation that preserves details and produces high-quality results. Besides, the CUDA-customized differentiable rasterization pipeline for 3D Gaussians enables real-time rendering even at high resolution. ", "page_idx": 0}, {"type": "image", "img_path": "qDfPSWXSLt/tmp/21b70c6697f131a9a232b19c822deaa38fc33f2cc3fcf2d3584010c3f8d31d4f.jpg", "img_caption": ["Figure 1: Our method not only achieves real-time rendering but also significantly enhances the capability of 3D-GS to model scenes with specular and anisotropic components. Key to this enhanced performance is our use of ASG appearance field to model the appearance of each 3D Gaussian, which results in substantial improvements in rendering quality for both complex and general scenes. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Despite its exceptional performance, 3D-GS struggles to model specular components within scenes (see Fig. 1). This issue primarily stems from the limited ability of low-order spherical harmonics (SH) to capture the high-frequency information in these scenarios. Consequently, this poses a challenge for 3D-GS to model scenes with reflections and specular components, as illustrated in Fig. 1. ", "page_idx": 1}, {"type": "text", "text": "To address the issue, we introduce a novel approach called Spec-Gaussian, which combines anisotropic spherical Gaussian (ASG) [60] for modeling anisotropic and specular components, an effective training mechanism to eliminate floaters and improve learning efficiencies, and anchorbased 3D Gaussians for acceleration and storage reduction. Specifically, the method incorporates two key designs: 1) A new 3D Gaussian representation that utilizes an ASG appearance field instead of SH to model the appearance of each 3D Gaussian. ASG with a few orders can effectively model high-frequency information that low-order SH cannot. This new design enables 3D-GS to more effectively model anisotropic and specular components in static scenes. 2) A coarse-to-fine training scheme specifically tailored for 3D-GS is designed to eliminate floaters and boost learning efficiency. This strategy effectively shortens learning time by optimizing low-resolution rendering in the initial stage, preventing the need to increase the number of 3D Gaussians and regularizing the learning process to avoid the generation of unnecessary geometric structures that lead to floaters. ", "page_idx": 1}, {"type": "text", "text": "By combining these advances, our approach can render high-quality results for specular highlights and anisotropy as shown in Fig. 4 while preserving the efficiency of Gaussians. Furthermore, comprehensive experiments reveal that our method not only endows 3D-GS with the ability to model specular highlights but also achieves state-of-the-art results in general benchmarks. ", "page_idx": 1}, {"type": "text", "text": "In summary, the major contributions of our work are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 A novel ASG appearance field to model the view-dependent appearance of each 3D Gaussian, which enables 3D-GS to effectively represent scenes with specular and anisotropic components.   \n\u2022 A coarse-to-fine training scheme that effectively regularizes training to eliminate floaters and improve the learning efficiency of 3D-GS in real-world scenes.   \n\u2022 An anisotropic dataset has also been made to assess the capability of our model in representing anisotropy. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Implicit Neural Radiance Fields ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Neural rendering has attracted significant interest in the academic community for its unparalleled ability to generate photorealistic images. Methods like NeRF [37] utilize Multi-Layer Perceptrons (MLPs) to model the geometry and radiance fields of a scene. Leveraging the volumetric rendering equation and the inherent continuity and smoothness of MLPs, NeRF achieves high-quality scene reconstruction from a set of posed images, establishing itself as the state-of-the-art (SOTA) method for novel view synthesis. Subsequent research has extended the utility of NeRF to various applications, including mesh reconstruction [53, 27, 58, 34], inverse rendering [48, 72, 31, 62], optimization of camera parameters [29, 55, 54, 41], few-shot learning [12, 61, 57], and anti-aliasing [2, 1, 3]. ", "page_idx": 1}, {"type": "text", "text": "However, this stream of methods relies on ray casting rather than rasterization to determine the color of each pixel. Consequently, every sampling point along the ray necessitates querying the MLPs, ", "page_idx": 1}, {"type": "image", "img_path": "qDfPSWXSLt/tmp/2136002ddb5240ef665eaf7c7087bd5c4a560e831ab2138c1d3e0ba8442f2169.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: Pipeline of Spec-Gaussian. The optimization process begins with SfM points derived from COLMAP or generated randomly, serving as the initial state for the 3D Gaussians. To address the limitations of low-order SH and pure MLP in modeling high-frequency information, we additionally employ ASG in conjunction with a feature decoupling MLP to model the view-dependent appearance of each 3D Gaussian. Then, 3D Gaussians with opacity $\\sigma>0$ are rendered through a differentiable Gaussian rasterization pipeline, effectively capturing specular highlights and anisotropy in the scene. ", "page_idx": 2}, {"type": "text", "text": "leading to significantly slow rendering speed and prolonged training convergence. This limitation substantially impedes their application in large-scene modeling and real-time rendering. ", "page_idx": 2}, {"type": "text", "text": "To reduce the training time of MLP-based NeRF methods and improve rendering speed, subsequent work has enhanced NeRF\u2019s efficiency in various ways. Structure-based techniques [68, 14, 43, 17, 7] have sought to improve inference or training efficiency by caching or distilling the implicit neural representation into more efficient data structures. Hybrid methods [30, 49] increase efficiency by incorporating explicit voxel-based data structures. Factorization methods [5, 18, 8, 16] apply a lowrank tensor assumption to decompose the scene into low-dimensional planes or vectors, achieving better geometric consistency. Compared to continuous implicit representations, the convergence of individual voxels in the grid is independent, significantly reducing training time. Additionally, Instant-NGP [38] utilizes a hash grid with a corresponding CUDA implementation for faster feature querying, enabling rapid training and interactive rendering of neural radiance fields. Spec-NeRF [35] achieves high-quality specular reflection modeling by introducing Gaussian directional encoding. ", "page_idx": 2}, {"type": "text", "text": "Despite achieving higher quality and faster rendering, these methods have not fundamentally overcome the substantial query overhead associated with ray casting. As a result, a notable gap remains before achieving real-time rendering. In this work, we build upon the recent 3D-GS [23], a pointbased rendering method that leverages rasterization. Compared to ray casting-based methods, it significantly enhances both training and rendering speed. ", "page_idx": 2}, {"type": "text", "text": "2.2 Point-based Neural Radiance Fields ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Point-based representations, similar to triangle mesh-based methods, can exploit the highly efficient rasterization pipeline of modern GPUs to achieve real-time rendering. Although these methods offer breakneck rendering speeds and are well-suited for editing tasks, they often suffer from holes and outliers, leading to artifacts in the rendered images. This issue arises from the discrete nature of point clouds, which can create gaps in the primitives and, consequently, in the rendered image. ", "page_idx": 2}, {"type": "text", "text": "To address these discontinuity issues, differentiable point-based rendering [67, 15, 24, 25] has been extensively explored for fitting complex geometric shapes. Notably, Zhang et al. [71] employ differentiable surface splatting and utilize a radial basis function (RBF) kernel to compute the contribution of each point to each pixel. ", "page_idx": 2}, {"type": "text", "text": "Recently, 3D-GS [23] has employed anisotropic 3D Gaussians, initialized from Structure from Motion (SfM), to represent 3D scenes. The innovative densification mechanism and CUDA-customized differentiable Gaussian rasterization pipeline of 3D-GS have not only achieved state-of-the-art (SOTA) rendering quality but also significantly surpassed the threshold of real-time rendering. Many concurrent works have rapidly extended 3D-GS to a variety of downstream applications, including dynamic scenes [33, 63, 64, 20, 26, 50], text-to-3D generation [28, 51, 9, 66, 10], avatars [74, 73, 21, 45, 40], scene editing [59, 6, 13], quality enhancement [36, 44] and mesh reconstruction [19, 11, 69, 34]. ", "page_idx": 2}, {"type": "text", "text": "Despite achieving SOTA results on commonly used benchmark datasets, 3D-GS still struggles to model scenes with specular and reflective components, which limits its practical application in real-time rendering at the photorealistic level. In this work, by replacing spherical harmonics (SH) with an anisotropic spherical Gaussian (ASG) appearance field, we have enabled 3D-GS to model complex specular scenes more effectively. ", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The overview of our method is illustrated in Fig. 2. The input to our model is a set of posed images of a static scene, together with a sparse point cloud obtained from SfM [46]. The core of our method is to use the ASG appearance field to replace SH in modeling the appearance of 3D Gaussians (Sec. 3.2). Moreover, we introduce a simple yet effective coarse-to-fine training strategy to reduce floaters in real-world scenes (Sec. 3.3). To further reduce the storage overhead and rendering speed pressure introduced by ASG, we combine a hybrid Gaussian model that employs sparse anchor Gaussians to facilitate the generation of neural Gaussians (Sec. 3.4) to model the 3D scene. ", "page_idx": 3}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3D Gaussian splatting. 3D-GS [23] is a point-based method that employs anisotropic 3D Gaussians to represent scenes. Each 3D Gaussian is defined by a center position $\\textbf{\\em x}$ , opacity $\\sigma$ , and a 3D covariance matrix $\\Sigma$ , which is decomposed into a quaternion $\\pmb{r}$ and scaling $\\pmb{s}$ . The view-dependent appearance of each 3D Gaussian is represented using the first three orders of spherical harmonics (SH). This method not only retains the rendering details offered by volumetric rendering but also achieves real-time rendering through a CUDA-customized differentiable Gaussian rasterization process. Following [75], the 3D Gaussians can be projected to 2D using the 2D covariance matrix $\\Sigma^{\\prime}$ , defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma^{\\prime}=J V\\Sigma V^{T}J^{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $J$ is the Jacobian of the affine approximation of the projective transformation, and $V$ represents the view matrix, transitioning from world to camera coordinates. To facilitate learning, the 3D covariance matrix $\\Sigma$ is decomposed into two learnable components: the quaternion $\\pmb{r}$ , representing rotation, and the 3D-vector $\\pmb{s}$ , representing scaling. The resulting $\\Sigma$ is thus represented as the combination of a rotation matrix $R$ and scaling matrix $S$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma=R S S^{T}R^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The color of each pixel on the image plane is then rendered through a point-based volumetric rendering (alpha blending) technique: ", "page_idx": 3}, {"type": "equation", "text": "$$\nC(\\mathbf{p})=\\sum_{i\\in N}T_{i}\\alpha_{i}c_{i},\\quad\\alpha_{i}=\\sigma_{i}e^{-\\frac{1}{2}\\left(\\mathbf{p}-\\boldsymbol{\\mu}_{i}\\right)^{T}\\sum^{-1}(\\mathbf{p}-\\boldsymbol{\\mu}_{i})},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{p}$ denotes the pixel coordinate, $T_{i}$ is the transmittance defined by $\\Pi_{j=1}^{i-1}(1-\\alpha_{j})$ , $c_{i}$ signifies the color of the sorted Gaussians associated with the queried pixel, and $\\mu_{i}$ represents the coordinates of the 3D Gaussians when projected onto the 2D image plane. ", "page_idx": 3}, {"type": "text", "text": "Anisotropic spherical Gaussian. Anisotropic spherical Gaussian (ASG) [60] has been designed in the traditional rendering pipeline to efficiently approximate lighting and shading. Different from spherical Gaussian (SG), ASG has been demonstrated to effectively represent anisotropic scenes with a small number. In addition to retaining the fundamental properties of SG, ASG also exhibits rotational invariance and can represent full-frequency signals. The ASG function is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nA S G(\\nu\\mid[{\\bf x},{\\bf y},{\\bf z}],[\\lambda,\\mu],\\xi)=\\xi\\cdot\\mathrm{S}(\\nu;{\\bf z})\\cdot e^{-\\lambda(\\nu\\cdot{\\bf x})^{2}-\\mu(\\nu\\cdot{\\bf y})^{2}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\nu$ is the unit direction serving as the function input; $\\mathbf x,\\,\\mathbf y$ , and ${\\bf z}$ correspond to the tangent, bi-tangent, and lobe axis, respectively, and are mutually orthogonal; $\\lambda\\in\\mathbb{R}^{1}$ and $\\mu\\in\\mathbb{R}^{1}$ are the sharpness parameters for the $\\mathbf{x}-$ and $\\mathbf{y}$ -axis, satisfying $\\lambda,\\mu>0$ ; $\\xi\\in\\mathbb{R}^{2}$ is the lobe amplitude; S is the smooth term defined as $\\mathrm{S}(\\nu;\\mathbf{z})=\\operatorname*{max}(\\nu\\cdot\\mathbf{z},0)$ . ", "page_idx": 3}, {"type": "text", "text": "Inspired by the power of ASG in modeling scenes with complex anisotropy, we propose integrating ASG into Gaussian splatting to join the forces of classic models with new rendering pipelines for higher quality. For $N$ ASGs, we predefined orthonormal axes x, y, and $\\mathbf{z}$ , initializing them to be uniformly distributed across a hemisphere. During training, we allow the remaining ASG parameters, $\\lambda,\\mu,$ , and $\\xi$ , to be learnable. We use the reflect direction $\\omega_{r}$ as the input to query ASG for modeling the view-dependent specular information. Note that we use $N=32$ ASGs for each 3D Gaussian. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Anchor-based Gaussian splatting. Anchor-based Gaussian splatting was first proposed by Scaffold-GS [32]. Unlike the attributes carried by each entity in 3D-GS, each anchor Gaussian carries a position coordinate $\\mathbf{P}_{v}\\in\\mathbb{R}^{3}$ , a local feature $\\mathbf{f}_{v}\\in\\mathbb{R}^{32}$ , a displacement factor $\\eta_{v}\\in\\mathbb{R}^{3}$ , and $k$ learnable offsets $\\mathbf{O}_{v}\\in\\mathbb{R}^{k\\times3}$ . They use the COLMAP [46] point cloud to initialize each anchor 3D Gaussian, serving as the voxel centers to guide the generation of neural Gaussians. The position $\\mathbf{P}_{v}$ of the anchor Gaussian is initialized as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{P}_{v}=\\left\\{\\left\\lfloor{\\frac{\\mathbf{P}}{\\epsilon}}+0.5\\right\\rfloor\\right\\}\\cdot\\epsilon,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{P}$ is the point cloud position, $\\epsilon$ is the voxel size, and $\\{\\cdot\\}$ denotes removing duplicated anchors. ", "page_idx": 4}, {"type": "text", "text": "Then anchor Gaussians can guide the generation of neural Gaussians, which have the same attributes as vanilla 3D-GS. For each visible anchor Gaussian within the viewing frustum, we spawn $k$ neural Gaussians and predict their attributes. The positions $\\mathbf{x}$ of neural Gaussians are calculated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{\\mathbf{x}_{0},\\dotsc,\\mathbf{x}_{k-1}\\right\\}=\\mathbf{P}_{v}+\\left\\{\\mathbf{O}_{0},\\dotsc,\\mathbf{O}_{k-1}\\right\\}\\cdot\\eta_{v},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{P}_{v}$ represents the position of the anchor Gaussian corresponding to $k$ neural Gaussians. The opacity $\\sigma$ is calculated through a tiny MLP: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{\\sigma_{0},\\ldots,\\sigma_{k-1}\\right\\}=\\mathcal{F}_{\\sigma}\\left(\\mathbf{f}_{v},\\delta_{c v},\\mathbf{d}_{c v}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\delta_{c v}$ denotes the distance between the anchor Gaussian and the camera, and $\\mathbf{d}_{c v}$ is the unit direction pointing from the camera to the anchor Gaussian. The rotation $r$ and scaling $s$ of each neural Gaussian are derived similarly using the corresponding tiny MLP ${\\mathcal{F}}_{r}$ and $\\mathcal{F}_{s}$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Anisotropic View-Dependent Appearance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "ASG appearance field for 3D Gaussians. Although SH has enabled view-dependent scene modeling, the low frequency of low-order SH makes it challenging to model scenes with complex optical phenomena such as specular highlights and anisotropic effects. Therefore, instead of using SH, we propose using an ASG appearance field based on Eq. (4) to model the appearance of each 3D Gaussian. However, the introduction of ASG increases the feature dimensions of each 3D Gaussian, raising the model\u2019s storage overhead. To address this, we employ a compact learnable MLP $\\Theta$ to predict the parameters for $N$ ASGs, with each Gaussian carrying only additional local features $\\mathbf{f}\\in\\mathbb{R}^{24}$ as the input to the MLP: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Theta(\\mathbf{f})\\rightarrow\\{\\lambda,\\mu,\\xi\\}_{N}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To better differentiate between high and low-frequency information and further assist ASG in ftiting high-frequency specular details, we decompose color $c$ into diffuse and specular components: ", "page_idx": 4}, {"type": "equation", "text": "$$\nc=c_{d}+c_{s},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $c_{d}$ represents the diffuse color, modeled using the first three orders of $\\mathrm{SH}$ , and $c_{s}$ is the specular color calculated through ASG. We refer to this comprehensive approach to appearance modeling as the ASG appearance field. ", "page_idx": 4}, {"type": "text", "text": "Although ASG theoretically enhance the ability of SH to model anisotropy, directly using ASG to represent the specular color of each 3D Gaussian still falls short in accurately modeling anisotropic and specular components, as demonstrated in Fig. 6. Inspired by [16], we do not use ASG directly to represent color but instead employ ASG to model the latent feature of each 3D Gaussian. This latent feature, containing anisotropic information, is then fed into a tiny feature decoding MLP $\\Psi$ to determine the final specular color: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Psi(\\kappa,\\gamma({\\bf d}),\\langle n,-{\\bf d}\\rangle)\\rightarrow c_{s},}\\\\ {\\displaystyle\\kappa=\\bigoplus_{i=1}^{N}A S G(\\omega_{r}\\mid[{\\bf x},{\\bf y},{\\bf z}],[\\lambda_{i},\\mu_{i}],\\xi_{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "table", "img_path": "qDfPSWXSLt/tmp/b34b1c9cd8d4902dd39ee308173515db20a0cb1b4f977e6004a39cc13cd92dd3.jpg", "table_caption": ["Table 1: Quantitative Comparison on anisotropic synthetic dataset. "], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "qDfPSWXSLt/tmp/72722790eb89109925cdaba285bb3f74ff223aa1b0ead3960ba9c04a0280d768.jpg", "img_caption": ["Figure 3: Using a coarse-to-fine strategy, our approach can eliminate the floaters without increasing the number of GS. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "where $\\kappa$ is the latent feature derived from ASG, $\\oplus$ denotes the concatenation operation, $\\gamma$ represents the positional encoding, $\\mathbf{d}$ is the unit view direction pointing from the camera to each 3D Gaussian, $n$ is the normal of each 3D Gaussian, and $\\omega_{r}$ is the unit reflect direction. This strategy significantly enhances the ability of 3D-GS to model scenes with complex optical phenomena, whereas neither pure ASG nor pure MLP can achieve anisotropic appearance modeling as effectively as our approach. ", "page_idx": 5}, {"type": "text", "text": "Normal estimation. Following [22, 47], we use the shortest axis of each Gaussian as its normal. This approach is based on the observation that 3D Gaussians tend to flatten gradually during the optimization process, allowing the shortest axis to serve as a reasonable approximation for the normal. ", "page_idx": 5}, {"type": "text", "text": "The reflect direction $\\omega_{r}$ can then be derived using the view direction and the local normal vector $n$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\omega_{r}=2(\\omega_{o}\\cdot n)\\cdot n-\\omega_{o},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\omega_{o}\\,=\\,-{\\bf d}$ is a unit view direction pointing from each 3D Gaussian in world space to the camera. We use the reflect direction $\\omega_{r}$ to query ASG, enabling better interpolation of latent features containing anisotropic information. Experimental results show that although this unsupervised normal estimation cannot generate physically accurate normals aligned with the real world, it is sufficient to produce relatively accurate reflect direction to assist ASG in fitting high-frequency information. ", "page_idx": 5}, {"type": "text", "text": "3.3 Coarse-to-fine Training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We observed that in many real-world scenarios, 3D-GS tends to overfit the training data, leading to the emergence of numerous floaters when rendering images from novel viewpoints. One important reason is that the COLMAP point cloud is too sparse. Poor initialization makes it difficult for 3D-GS to compensate for overly sparse areas through densification during optimization, leading to floaters in the rendering images. Moreover, 3D-GS accumulates gradients from each pixel to the GS: $\\begin{array}{r}{\\frac{d L}{d\\mathbf{x}}=\\sum\\frac{d L}{d\\mathbf{p}_{i}}\\frac{d\\mathbf{p}_{i}}{d\\mathbf{x}}}\\end{array}$ ,w aenvder ,t hhea vdienngs ipfoicsiattiivoen a oncd cnuergs atwivhee ng rtahdei eanctcs ucmanu lcaatuesde  aGmSos utnhta te sxhcoeueldds  bae t dhreenssihfoieldd $\\tau_{g}=0.0002$   \nto be ignored due to the large negative gradient. ", "page_idx": 5}, {"type": "text", "text": "Thus, to mitigate the occurrence of floaters in real-world scenes, we propose a coarse-to-fine training amceccuhmanuilsatmin. g Wteh ef irnsut immerpiocsale  caon nLtr1i bcuotniostnr afirnot mo np itxhee lgs rtaod iGeSnt rs aftrhoemr  tphiaxne lgs rtaod iGeSn:t $\\begin{array}{r}{\\frac{d L}{d\\mathbf{x}}=\\sum\\|\\frac{d L}{d\\mathbf{p}_{i}}\\frac{d\\mathbf{p}_{i}}{d\\mathbf{x}}\\|_{1}^{-}}\\end{array}$ to the concurrent works [65, 70]. Next, to avoid overfitting caused by excessive growth of 3D-GS during the early stages of optimization, we decide to train 3D-GS progressively from low to high resolution in real-world scenes: ", "page_idx": 5}, {"type": "equation", "text": "$$\nr(i)=\\mathrm{min}(\\lfloor r_{s}+(r_{e}-r_{s})\\cdot i/\\tau\\rceil,r_{e}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $r(i)$ is the image resolution at the $i$ -th training iteration, $r_{s}$ is the starting image resolution, $r_{e}$ is the ending image resolution (the full resolution we aim to render), and $\\tau$ is the threshold iteration, empirically set to $5\\mathrm{k}$ . ", "page_idx": 5}, {"type": "text", "text": "This training method allows 3D-GS to densify correctly and prevents excessive growth of 3D-GS in the early stages. Additionally, due to the lower resolution training in the initial phase, this mechanism reduces training time by approximately $10\\%$ . In our experiments, we offer a performance version with $\\tau_{g}=0.0005$ and light version with $\\tau_{g}=0.0006$ . ", "page_idx": 5}, {"type": "image", "img_path": "qDfPSWXSLt/tmp/c74e8b6eac127f2de6a0793e4099443aa54b637a0a447f139e263360bdc24263.jpg", "img_caption": ["Figure 4: Visualization on NeRF dataset. Our method has achieved specular highlights modeling, which other 3D-GS-based methods fail to accomplish, while maintaining fast rendering speed. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "qDfPSWXSLt/tmp/e5cd19a17252830358cc0fde28a1e17c2103fce7717b341728015171358f0609.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 2: Quantitative comparison of on real-world datasets. We report PSNR, SSIM, LPIPS (VGG) and color each cell as best , second best and third best . Our method has achieved the best rendering quality, while striking a good balance between FPS and the storage memory. ", "page_idx": 6}, {"type": "text", "text": "3.4 Adaption for Anchor-Based Gaussian Splatting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "While the ASG appearance field significantly improves the ability of 3D-GS to model specular and anisotropic features, it introduces additional computational overhead due to the additional local features f associated with each Gaussian. Inspired by [32], we employ anchor-based Gaussian splatting to reduce storage overhead and accelerate the rendering. ", "page_idx": 6}, {"type": "text", "text": "Since the anisotropy modeled by ASG is continuous in space, it can be compressed into a lowerdimensional space. Thanks to the guidance of the anchor Gaussian, the anchor feature $\\mathbf{f}_{v}$ can be used directly to compress $N$ ASGs, further reducing storage pressure. To make the ASG of neural Gaussians position-aware, we introduce the unit view direction to decompress ASG parameters. Consequently, the ASG parameters prediction in Eq. (8) is revised as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Theta(\\mathbf{f}_{v},\\mathbf{d}_{c n})\\rightarrow\\{\\lambda,\\mu,\\xi\\}_{\\cal N},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbf{d}_{c n}$ denotes the unit view direction from the camera to each neural Gaussian. Additionally, we set the diffuse part of the neural Gaussian $c_{d}=\\phi(\\mathbf{f}_{v})$ , directly predicted through an MLP $\\phi$ , to ensure the smoothness of the diffuse component and reduce the difficulty of convergence. ", "page_idx": 6}, {"type": "text", "text": "3.5 Losses ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We optimize the learnable parameters and MLPs using the same loss function as 3D-GS [23]. The total supervision is given by: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}=(1-\\lambda_{\\mathrm{D-SSIM}})\\mathcal{L}_{1}+\\lambda_{\\mathrm{D-SSIM}}\\mathcal{L}_{\\mathrm{D-SSIM}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the $\\lambda_{\\mathrm{D-SSIM}}=0.2$ is consistently used in our experiments. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present both quantitative and qualitative results of our method. To evaluate its effectiveness, we compared it to several state-of-the-art methods across various datasets. We color each cell as best , second best and third best . Our method includes three versions, each based on different foundational methods with distinct hyperparameter settings. The performance version (Ours) is based on 3D-GS [23] with $\\tau_{g}\\,=\\,0.0005$ ; the light version (Ours-light), also based on 3D-GS, has $\\tau_{g}\\,=\\,0.0006$ ; and the mini version (Ours-w/ anchor) is based on Scaffold-GS [32], with $\\tau_{g}=0.0006$ . Our method demonstrates superior performance in modeling complex specular and anisotropic features, as evidenced by comparisons on the NeRF, NSVF, and our \"Anisotropic Synthetic\" datasets. Additionally, we showcase its versatility by comparing its performance in diffuse scenarios, further proving the robustness of our approach. ", "page_idx": 6}, {"type": "table", "img_path": "", "table_caption": ["Table 3: Results on NeRF synthetic dataset. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "", "table_caption": ["Table 4: Results on NSVF synthetic dataset. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "qDfPSWXSLt/tmp/76d80f85fc9f608948c0161d3a0b34b8725e5ccdda51adc1035aeb06e447852a.jpg", "img_caption": ["Figure 5: Visualization on Mip-NeRF 360 indoor scenes. Our method achieves superior recovery of specular effects compared to SOTA methods. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.1 Implementation Details ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We implemented our framework using PyTorch [42] and modified the differentiable Gaussian rasterization to include depth visualization. For the ASG appearance field, the decoupling MLP $\\Psi$ consists of 3 layers, each with 64 hidden units, and the positional encoding for the view direction is of order 2. Regarding coarse-to-fine training, which is applied only to real-world scenes to remove floaters, we start with a resolution $r_{s}$ that is $\\mathbf{4x}$ downsampled. To further accelerate rendering, we prefilter and allow only those Gaussians with opacity $\\sigma_{n}>0$ to pass through the ASG appearance field and Gaussian rasterization pipelines. All experiments were conducted on an NVIDIA RTX 3090. ", "page_idx": 7}, {"type": "text", "text": "4.2 Results and Comparisons ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Synthetic bounded scenes. We used the NeRF, NSVF, and our \"Anisotropic Synthetic\" datasets as the experimental datasets for synthetic scenes. Our comparisons were made with the most relevant state-of-the-art methods, including 3D-GS [23], Scaffold-GS [32], GaussianShader [22], and several NeRF-based methods such as NSVF [30], TensoRF [5], NeuRBF [8], and Tri-MipRF [18]. ", "page_idx": 7}, {"type": "text", "text": "As shown in Fig. 4 (with PSNR and LPIPS), and Tabs. 3- 4, our method achieved the highest performance with fewer Gaussians compared to vanilla 3D-GS. It also improved upon the issues that 3D-GS faced in modeling high-frequency specular highlights and complex anisotropy as shown in Tab. 1 with fewer Gaussians and better metrics. See more in the supplementary materials. ", "page_idx": 7}, {"type": "text", "text": "Real-world unbounded scenes. To verify the versatility of our method in real-world scenarios, we used the Mip360 [2] dataset, which contains indoor scenes with specular highlights. As shown in Tab. 2, our method surpasses state-of-the-art methods on Mip-NeRF 360. Furthermore, our method effectively balances FPS, storage, and rendering quality. It enhances rendering quality without increasing storage or significantly reducing FPS. As illustrated in Fig. 5 and Fig. 7, our method has also significantly improved the visual effect. It removes a large number of floaters in outdoor scenes and successfully models the high-frequency specular highlights in indoor scenes. This demonstrates that our approach is not only adept at modeling complex specular scenes but also effectively improves rendering quality in general scenarios. ", "page_idx": 7}, {"type": "image", "img_path": "qDfPSWXSLt/tmp/d61d81e21409fa8b29d1b5918f9430ed7afc1286a07f40d8d794eeccd2c9295f.jpg", "img_caption": ["Figure 6: Ablation on ASG appearance field. We show that directly using ASG to model color leads to the failure in modeling anisotropy and specular highlights. By decoupling the ASG features through MLP, we can realistically model complex optical phenomena. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "qDfPSWXSLt/tmp/31a342e211e5691117a007d88865c31a0191c2f29b7fbd94ab37a02f644045b8.jpg", "img_caption": ["Figure 7: Ablation on coarse-to-fine training. Experimental results demonstrate that our simple yet effective training mechanism can effectively remove floaters without increasing the number of 3D Gaussians, thereby alleviating the overfitting problem prevalent in 3D-GS-based methods. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "ASG feature decoupling MLP. We conducted an ablation study to evaluate the key components of the ASG appearance field, which include ASG features, decoupling MLP, and the separation of diffuse and specular colors. As demonstrated in Fig. 6 (with PSNR and LPIPS), directly using ASG to output color results in the inability to model specular and anisotropic components. In contrast to directly using an MLP for color modeling, as in Scaffold-GS [32], separately modeling diffuse and specular color can enhance the ftiting ability for high-frequency information. ASG can encode higherfrequency anisotropic features. With the help of ASG\u2019s ability to encode high-frequency anisotropic features, the decoupling MLP can fti complex optical phenomena, leading to more accurate rendering results. We also demonstrated that higher-order SH (6-order) and more MLP layers (4-layers) do not help 3D-GS and Scaffold-GS achieve satisfactory results, highlighting the importance of ASG. ", "page_idx": 8}, {"type": "text", "text": "Coarse-to-fine training. We conducted an ablation study to assess the impact of coarse-to-fine (c2f) training. As illustrated in Fig. 7 (with LPIPS and number of Gaussian), both 3D-GS and Scaffold-GS exhibit a large number of floaters in the novel view synthesis. Coarse-to-fine training effectively reduces the number of floaters, alleviating the overftiting issue commonly encountered by 3D-GS in real-world scenarios. Applying an L1 constraint to the gradients used for 3D-GS densification further reduced the number of floaters and Gaussians. See more in the supplementary materials. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we introduce Spec-Gaussian, a novel approach to 3D Gaussian splitting that features an anisotropic view-dependent appearance. Leveraging the powerful capabilities of ASG, our method effectively overcomes the challenges encountered by vanilla 3D-GS in rendering scenes with specular highlights and anisotropy. Additionally, we innovatively implement a coarse-to-fine training mechanism to eliminate floaters in real-world scenes. Both quantitative and qualitative experiments demonstrate that our method not only equips 3D-GS with the ability to model specular highlights and anisotropy but also enhances the overall rendering quality of 3D-GS in general scenes. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Limitations. Although our method enables 3D-GS to model complex specular and anisotropic features, it still faces challenges in handling reflections. Specular and anisotropic effects are primarily influenced by material properties, whereas reflections are closely related to the environment and geometry. Due to the lack of explicit geometry in 3D-GS, we cannot differentiate between reflections and materials using constraints like normals, as employed in Ref-NeRF [52] and NeRO [31]. We plan to explore solutions for modeling reflections with 3D-GS in future work. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowlegements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Chao Wan from Cornell University for the help during rebuttal period. This work was supported by the National Natural Science Foundation of China (Grant No. 62036010). Ziyi Yang was also supported by ByteDance MMLab. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. ICCV, 2021. [2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5470\u20135479, 2022. [3] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-based neural radiance fields. ICCV, 2023. [4] Mario Botsch, Alexander Hornung, Matthias Zwicker, and Leif Kobbelt. High-quality surface splatting on today\u2019s gpus. In Proceedings Eurographics/IEEE VGTC Symposium Point-Based Graphics, 2005., pages 17\u2013141. IEEE, 2005. [5] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European Conference on Computer Vision, pages 333\u2013350. Springer, 2022. [6] Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, and Guosheng Lin. Gaussianeditor: Swift and controllable 3d editing with gaussian splatting, 2023. [7] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. Mobilenerf: Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures. arXiv preprint arXiv:2208.00277, 2022. [8] Zhang Chen, Zhong Li, Liangchen Song, Lele Chen, Jingyi Yu, Junsong Yuan, and Yi Xu. Neurbf: A neural fields representation with adaptive radial basis functions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4182\u20134194, 2023.   \n[9] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using gaussian splatting. arXiv preprint arXiv:2309.16585, 2023.   \n[10] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free generation of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384, 2023.   \n[11] Pinxuan Dai, Jiamin Xu, Wenxiang Xie, Xinguo Liu, Huamin Wang, and Weiwei Xu. High-quality surface reconstruction using gaussian surfels. In ACM SIGGRAPH 2024 Conference Papers. Association for Computing Machinery, 2024.   \n[12] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised NeRF: Fewer views and faster training for free. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022.   \n[13] Xinyu Gao, Ziyi Yang, Bingchen Gong, Xiaoguang Han, Sipeng Yang, and Xiaogang Jin. Towards realistic example-based modeling via 3d gaussian stitching. arXiv preprint arXiv:2408.15708, 2024.   \n[14] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neural rendering at 200fps. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14346\u201314355, 2021.   \n[15] Markus Gross and Hanspeter Pfister. Point-based graphics. Elsevier, 2011.   \n[16] Kang Han and Wei Xiang. Multiscale tensor decomposition and rendering equation encoding for view synthesis. In The IEEE / CVF Computer Vision and Pattern Recognition Conference, pages 4232\u20134241, 2023.   \n[17] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul Debevec. Baking neural radiance fields for real-time view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5875\u20135884, 2021.   \n[18] Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao, Xiao Liu, and Yuewen Ma. Tri-miprf: Tri-mip representation for efficient anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19774\u201319783, 2023.   \n[19] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 Conference Papers, pages 1\u201311, 2024.   \n[20] Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs: Sparsecontrolled gaussian splatting for editable dynamic scenes. pages 1\u201311, 2023.   \n[21] Yuheng Jiang, Zhehao Shen, Penghao Wang, Zhuo Su, Yu Hong, Yingliang Zhang, Jingyi Yu, and Lan Xu. Hifi4g: High-fidelity human performance rendering via compact gaussian splatting. arXiv preprint arXiv:2312.03461, 2023.   \n[22] Yingwenqi Jiang, Jiadong Tu, Yuan Liu, Xifeng Gao, Xiaoxiao Long, Wenping Wang, and Yuexin Ma. Gaussianshader: 3d gaussian splatting with shading functions for reflective surfaces. arXiv preprint arXiv:2311.17977, 2023.   \n[23] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023.   \n[24] Leonid Keselman and Martial Hebert. Approximate differentiable rendering with algebraic surfaces. In European Conference on Computer Vision, pages 596\u2013614. Springer, 2022.   \n[25] Leonid Keselman and Martial Hebert. Flexible techniques for differentiable rendering with 3d gaussians. arXiv preprint arXiv:2308.14737, 2023.   \n[26] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. arXiv preprint arXiv:2312.16812, 2023.   \n[27] Zhaoshuo Li, Thomas M\u00fcller, Alex Evans, Russell H Taylor, Mathias Unberath, Ming-Yu Liu, and ChenHsuan Lin. Neuralangelo: High-fidelity neural surface reconstruction. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[28] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching, 2023.   \n[29] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting neural radiance fields. In IEEE International Conference on Computer Vision (ICCV), 2021.   \n[30] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. Advances in Neural Information Processing Systems, 33:15651\u201315663, 2020.   \n[31] Yuan Liu, Peng Wang, Cheng Lin, Xiaoxiao Long, Jiepeng Wang, Lingjie Liu, Taku Komura, and Wenping Wang. Nero: Neural geometry and brdf reconstruction of reflective objects from multiview images. In SIGGRAPH, 2023.   \n[32] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d gaussians for view-adaptive rendering. arXiv preprint arXiv:2312.00109, 2023.   \n[33] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In 3DV, 2024.   \n[34] Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, and Xiaojuan Qi. 3dgsr: Implicit surface reconstruction with 3d gaussian splatting. arXiv preprint arXiv:2404.00409, 2024.   \n[35] Li Ma, Vasu Agrawal, Haithem Turki, Changil Kim, Chen Gao, Pedro Sander, Michael Zollh\u00f6fer, and Christian Richardt. Specnerf: Gaussian directional encoding for specular reflections. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21188\u201321198, 2024.   \n[36] Dawid Malarz, Weronika Smolak, Jacek Tabor, S\u0142awomir Tadeja, and Przemys\u0142aw Spurek. Gaussian splatting with nerf-based color and opacity.   \n[37] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.   \n[38] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013102:15, July 2022.   \n[39] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas M\u00fcller, and Sanja Fidler. Extracting triangular 3d models, materials, and lighting from images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8280\u20138290, 2022.   \n[40] Haokai Pang, Heming Zhu, Adam Kortylewski, Christian Theobalt, and Marc Habermann. Ash: Animatable gaussian splats for efficient and photoreal human rendering. 2023.   \n[41] Keunhong Park, Philipp Henzler, Ben Mildenhall, Jonathan T Barron, and Ricardo Martin-Brualla. Camp: Camera preconditioning for neural radiance fields. ACM Transactions on Graphics (TOG), 42(6):1\u201311, 2023.   \n[42] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 32, 2019.   \n[43] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14335\u201314345, 2021.   \n[44] Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, and Bo Dai. Octree-gs: Towards consistent real-time rendering with lod-structured 3d gaussians. arXiv preprint arXiv:2403.17898, 2024.   \n[45] Shunsuke Saito, Gabriel Schwartz, Tomas Simon, Junxuan Li, and Giljoo Nam. Relightable gaussian codec avatars. 2023.   \n[46] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE Conference on Computer Vision and Pattern recognition, pages 4104\u20134113, 2016.   \n[47] Yahao Shi, Yanmin Wu, Chenming Wu, Xing Liu, Chen Zhao, Haocheng Feng, Jingtuo Liu, Liangjun Zhang, Jian Zhang, Bin Zhou, et al. Gir: 3d gaussian inverse rendering for relightable scene factorization. arXiv preprint arXiv:2312.05133, 2023.   \n[48] Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, and Jonathan T. Barron. Nerv: Neural reflectance and visibility fields for relighting and view synthesis. In CVPR, 2021.   \n[49] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5459\u20135469, 2022.   \n[50] Yang-Tian Sun, Yi-Hua Huang, Lin Ma, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Splatter a video: Video gaussian representation for versatile processing. arXiv preprint arXiv:2406.13870, 2024.   \n[51] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023.   \n[52] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, and Pratul P. Srinivasan. Ref-NeRF: Structured view-dependent appearance for neural radiance fields. CVPR, 2022.   \n[53] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. NeurIPS, 2021.   \n[54] Peng Wang, Lingzhe Zhao, Ruijie Ma, and Peidong Liu. BAD-NeRF: Bundle Adjusted Deblur Neural Radiance Fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4170\u20134179, June 2023.   \n[55] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. NeRF\u2212\u2212: Neural radiance fields without known camera parameters. arXiv preprint arXiv:2102.07064, 2021.   \n[56] Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, and Supasorn Suwajanakorn. Nex: Real-time view synthesis with neural basis expansion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.   \n[57] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul P. Srinivasan, Dor Verbin, Jonathan T. Barron, Ben Poole, and Aleksander Holynski. Reconfusion: 3d reconstruction with diffusion priors. arXiv, 2023.   \n[58] Tong Wu, Jiaqi Wang, Xingang Pan, Xudong Xu, Christian Theobalt, Ziwei Liu, and Dahua Lin. Voxurf: Voxel-based efficient and accurate neural surface reconstruction. In International Conference on Learning Representations (ICLR), 2023.   \n[59] Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang. Physgaussian: Physics-integrated 3d gaussians for generative dynamics. arXiv preprint arXiv:2311.12198, 2023.   \n[60] Kun Xu, Wei-Lun Sun, Zhao Dong, Dan-Yong Zhao, Run-Dong Wu, and Shi-Min Hu. Anisotropic spherical gaussians. ACM Transactions on Graphics, 32(6):209:1\u2013209:11, 2013.   \n[61] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with free frequency regularization. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[62] Ziyi Yang, Yanzhen Chen, Xinyu Gao, Yazhen Yuan, Yu Wu, Xiaowei Zhou, and Xiaogang Jin. Sire-ir: Inverse rendering for brdf reconstruction with shadow and illumination removal in high-illuminance scenes. arXiv preprint arXiv:2310.13030, 2023.   \n[63] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20331\u201320341, 2024.   \n[64] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv 2310.10642, 2023.   \n[65] Zongxin Ye, Wenyu Li, Sidun Liu, Peng Qiao, and Yong Dou. Absgs: Recovering fine details in 3d gaussian splatting. In ACM Multimedia 2024, 2024.   \n[66] Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. arXiv preprint arXiv:2310.08529, 2023.   \n[67] Wang Yifan, Felice Serena, Shihao Wu, Cengiz \u00d6ztireli, and Olga Sorkine-Hornung. Differentiable surface splatting for point-based geometry processing. ACM Transactions on Graphics (TOG), 38(6):1\u201314, 2019.   \n[68] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time rendering of neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5752\u20135761, 2021.   \n[69] Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, and Bo Dai. Gsdf: 3dgs meets sdf for improved rendering and reconstruction. arXiv preprint arXiv:2403.16964, 2024.   \n[70] Zehao Yu, Torsten Sattler, and Andreas Geiger. Gaussian opacity fields: Efficient and compact surface reconstruction in unbounded scenes. arXiv preprint arXiv:2404.10772, 2024.   \n[71] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, and Felix Heide. Differentiable point-based radiance fields for efficient view synthesis. arXiv preprint arXiv:2205.14330, 2022.   \n[72] Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul Debevec, William T Freeman, and Jonathan T Barron. Nerfactor: Neural factorization of shape and reflectance under an unknown illumination. ACM Transactions on Graphics (ToG), 40(6):1\u201318, 2021.   \n[73] Shunyuan Zheng, Boyao Zhou, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, and Yebin Liu. Gps-gaussian: Generalizable pixel-wise 3d gaussian splatting for real-time human novel view synthesis. arXiv, 2023.   \n[74] Wojciech Zielonka, Timur Bagautdinov, Shunsuke Saito, Michael Zollh\u00f6fer, Justus Thies, and Javier Romero. Drivable 3d gaussian avatars. 2023.   \n[75] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa volume splatting. In Proceedings Visualization, 2001. VIS\u201901., pages 29\u2013538. IEEE, 2001. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This supplementary material provides more results that accompany the paper. ", "page_idx": 13}, {"type": "text", "text": "\u2022 Section A provides more ablations.   \n\u2022 Section B provides additional results, including more visualizations and quantitative results on complete datasets. ", "page_idx": 13}, {"type": "text", "text": "A More Ablations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we present the complete quantitative ablations on the key components of our method. ", "page_idx": 13}, {"type": "text", "text": "We first evaluate the role of each component of the ASG appearance field in NeRF synthetic scenes as shown in Tab. 5. The introduction of ASG improves the ability to model specular highlights and reduces the number of 3D Gaussians. The inclusion of normals did not significantly increase computational overhead, but it did enhance rendering metrics and visual quality. More importantly, we achieve better rendering quality with fewer Gaussians than vanilla 3D-GS, a characteristic that can be further explored in the future. ", "page_idx": 13}, {"type": "text", "text": "Next, we evaluated our full method on the Mip360 dataset in Tab. 6. It is important to note that the Mip360 dataset is divided into indoor and outdoor scenes. Indoor scenes have more specular highlights, while outdoor scenes contain a large number of floaters. The coarse-to-fine approach itself improves the quality of 3D-GS in real-world scenes, mainly by eliminating a significant amount of floaters in outdoor settings. Although the introduction of the ASG appearance field significantly increases rendering overhead, it did greatly enhance the modeling of specular highlights in indoor scenes. Under the constraints of the coarse-to-fine mechanism, our complete method combines the advantages of both, achieving the best rendering quality. To further improve rendering speed, we also implement a light version and a mini version based on Scaffold-GS. These versions offer a trade-off between rendering quality and speed and can be used as needed. The quality of the Mip360 scenes demonstrates that our method is not only capable of handling scenes with specular highlights but is also robust in real-world diffuse scenarios. ", "page_idx": 13}, {"type": "text", "text": "B More Comparisons ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we present the complete quantitative results of our experiments. We report PSNR, SSIM, LPIPS (VGG), and color each cell as best second best and third best . ", "page_idx": 13}, {"type": "text", "text": "B.1 NeRF Synthetic Scenes ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As shown in Tabs. 7-9, our method demonstrates the best rendering quality metrics in almost every scene. It\u2019s important to note that the experimental setup for Tri-MipRF [18] differs from other methods. It uses both the training and validation sets as training data, expanding the scale of the model\u2019s data. When its training data is limited to the training set, its metrics suffer a noticeable drop. Nevertheless, to ensure that the experimental results fully reflect the highest performance of each method, and to prevent significant drops in metrics due to differences in experimental environments, we still present the metrics from the Tri-MipRF official paper. Our method achieved more prominent metrics in scenes with notable specular reflection and anisotropy, such as Drums, Lego, and Ship. This demonstrates that our method not only improves the overall rendering quality but also has a more significant advantage in complex specular scenarios. ", "page_idx": 13}, {"type": "text", "text": "B.2 NSVF Synthetic Scenes ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The NSVF [30] dataset, in comparison to NeRF, features more noticeable metallic specular reflection, as presented in the Wineholder, Steamtrain, and Spaceship scenes. It is important to note that TriMipRF fails to converge in the Steam scene with the official code, so we did not report metrics for that scenario. As shown in Tabs. 10-12, we present the per-scene experimental results of PSNR, SSIM, and LPIPS in the supplementary material. The experimental results indicate that compared to other methods based on 3D-GS [23], our method has significant advantages in metallic highlights and complex transmission scenarios. Additionally, we compared it with the SOTA NeRF-based methods based on NeRF. Our approach enables 3D-GS to surpass the latest SOTA of NeRF, achieving high-frequency highlight modeling that 3D-GS couldn\u2019t realize but NeRF could, thereby achieving truly high-quality rendering as shown in Fig. 14. ", "page_idx": 13}, {"type": "table", "img_path": "qDfPSWXSLt/tmp/1191acc148339807180243b0ff24abb6791070db2a03e0de0045f1d425ae4a0d.jpg", "table_caption": ["Table 5: Ablation on ASG appearance field. ", "Table 6: Ablation on full method. "], "table_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "qDfPSWXSLt/tmp/9824e2ab4e136ac54e0bff08a381655d812f79bf05fa3ec357753cca05c881d0.jpg", "img_caption": ["Figure 8: Visualization on Mip-NeRF 360 outdoor scenes. Our method achieves robust floater removal by coarse to fine training. ", "Mip-NeRF 360 3D-GS Scaffold-GS Ours-w/o Norm Ours G "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "image", "img_path": "qDfPSWXSLt/tmp/6b4346ff115c466459a34ed320f35a84ad9add5c08e5222ca5073c3ffc0658b3.jpg", "img_caption": ["Figure 9: More comparisons with baselines. Our method achieves robust floater removal by coarseto-fine training. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "qDfPSWXSLt/tmp/a689580d19e2e697b5d8cd05d30843fbd244ece1a10cb8f6956b827355a3b0a1.jpg", "img_caption": ["lights and reflections. ", "Figure 11: Ccomparison on Ref-NeRF dataset. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "qDfPSWXSLt/tmp/463864f0047bb71c347537dae15819516cc52d73a6d944e6e7f2d5e92f2116b9.jpg", "img_caption": ["Figure 12: Visualization on Nex [56] dataset. "], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "qDfPSWXSLt/tmp/08468e72f32eaf196beae27e45f254d87ea91551d81852fd19e6ef9173776b67.jpg", "table_caption": ["Table 7: Per-scene PSNR comparison on the NeRF dataset. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "qDfPSWXSLt/tmp/750507f9d9cc69f6e7a0ec7b3a89055172a3a46aedc24851f14333040bed6b74.jpg", "table_caption": ["Table 8: Per-scene SSIM comparison on the NeRF dataset. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "qDfPSWXSLt/tmp/7b488d5825c4edb447ee60cdc02984a0bc1896fd76cd621062796be94b49cdc1.jpg", "table_caption": ["Table 9: Per-scene LPIPS (VGG) comparison on the NeRF dataset. "], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "qDfPSWXSLt/tmp/99cb2e634312f0063f16593bafc25a9b3b4ab440d5df211a9f37c07df480a7c3.jpg", "img_caption": ["Figure 13: Visualization on our \"Anisotropic Synthetic\" dataset. We show the comparison between our method and 3D-GS across all eight scenes. Qualitative experimental results demonstrate the significant advantage of our method in modeling anisotropic scenes, thereby enhancing the rendering quality of 3D-GS. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "qDfPSWXSLt/tmp/83aae34ec06ddc36ad55c007603635b1f63b39df07a40d833c1329aac985dd3a.jpg", "img_caption": ["Figure 14: Visualization on NSVF dataset. Our method significantly improves the ability to model metallic materials compared to other GS-based methods. At the same time, our method also demonstrates the capability to model refractive parts, reflecting the powerful fitting ability of our method. ", "Table 10: Per-scene PSNR comparison on the NSVF dataset. "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "qDfPSWXSLt/tmp/8f0f2f2ae43882a0c6b533199135facb3c4a62556eac88f91828082aa0f04c5f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "qDfPSWXSLt/tmp/827de6fbef79a9692b92a4aa6b38549b4072e9baa5e7a28e39e3657c43ccf6b3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "qDfPSWXSLt/tmp/d70cca52bfd4457e9a999847bc955f4336d2662263420fa353f9257d8fc98f59.jpg", "table_caption": ["Table 11: Per-scene SSIM comparison on the NSVF dataset. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "qDfPSWXSLt/tmp/86ca4d00aed976892c04dd0c52e4ecee0b5c78434cf6177564a8c0319531d6b5.jpg", "table_caption": ["Table 12: Per-scene LPIPS (VGG) comparison on the NSVF dataset. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "qDfPSWXSLt/tmp/92a63979f177703be3da7dc6f70c48a395a1f2020270af937a003e626b5daa0a.jpg", "table_caption": ["Table 13: Per-scene PSNR comparison on our \"Anisotropic Synthetic\" dataset. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "qDfPSWXSLt/tmp/d9663e02086ef56378488fdfffd379a2b70fa70b0f19dfe2665ac5c6bf9c4a47.jpg", "table_caption": ["Table 14: Per-scene SSIM comparison on our \"Anisotropic Synthetic\" dataset. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "qDfPSWXSLt/tmp/936837b94f4563023c25d8a418055123e53612648ff8e97d61bac324aab60e14.jpg", "table_caption": ["Table 15: Per-scene LPIPS (VGG) comparison on our \"Anisotropic Synthetic\" dataset. ", "Table 16: Per-scene PSNR comparison on the Mip-NeRF 360 dataset. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "qDfPSWXSLt/tmp/02fd32aed3b65ae265fcce3c4b0158465d489ed90a22274d37b5c7967684a73d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "qDfPSWXSLt/tmp/b1591c361a87fc5825e8b356678d78c49ea8d6d47494ab3ffd2b6890629d7295.jpg", "table_caption": ["Table 17: SSIM Comparison on the Mip-NeRF 360 dataset. ", "Table 18: LPIPS Comparison on the Mip-NeRF 360 dataset. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.3 Anisotorpic Synthetic Scenes ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\"Anisotropic Synthetic\" is a synthetic dataset we rendered ourselves, which includes 8 scenes with significant anisotropy. We tested some existing 3D-GS-based methods on \"Anisotropic Synthetic.\" As shown in Tabs. 13-15, our method achieved a very significant improvement in rendering metrics. Fig. 13 shows the comparison between our method and 3D-GS across all eight scenes. Qualitative experiments also demonstrate the significant visual advantages of our method, highlighting the substantial improvement our method brings to anisotropic parts, thereby enhancing the overall rendering quality. ", "page_idx": 18}, {"type": "text", "text": "B.4 Mip-360 Scenes ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The MipNeRF-360 scenes include five outdoor and four indoor scenarios. There are several scenes rich in specular reflections, such as bonsai, room, and kitchen. As shown in Tabs. 16-18, our method achieved significant advantages in the four indoor scenes. This reflects our method\u2019s strengths in modeling specular reflections and anisotropy. In outdoor scenes, our method also achieved rendering metrics comparable to the SOTA methods. Furthermore, with the help of the coarse-to-fine training mechanism, our method significantly reduced the number of floaters as shown in Fig. 11, resulting in a substantial improvement in visual effects. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: I am sure that the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: I am sure that the paper discuss the limitations of the work. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Yes, the paper does. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Yes, the paper does. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: The code can be released upon acceptance, but now it\u2019s not a clean version. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper contains details about the training model. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We evaluate the results through PSNR, SSIM and LPIPS. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: They are presented in the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, we do. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 22}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, they do. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]