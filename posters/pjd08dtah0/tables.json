[{"figure_path": "pjD08dtAh0/tables/tables_2_1.jpg", "caption": "Table 1: Comparisons between HumanVLA and past works.", "description": "This table compares the capabilities of HumanVLA with several previous works on Human-Scene Interaction (HSI). The comparison is made across several key aspects: whether the methods use physics simulation, handle object interaction and dynamics, are guided by language instructions, utilize egocentric vision, and the number of static and movable objects involved in the experiments.  HumanVLA stands out by incorporating all these aspects.", "section": "2 Related Works"}, {"figure_path": "pjD08dtAh0/tables/tables_6_1.jpg", "caption": "Table 2: Results in box rearrangement. \u2020 denotes our implementation.", "description": "This table presents the results of box rearrangement experiments.  It compares the performance of three different methods: InterPhys [15] (as reported in the original paper), InterPhys [15] \u2020 (the authors' own re-implementation of InterPhys), and Human VLA-Teacher (the authors' proposed method).  The metrics used for comparison are Success Rate (percentage of successful rearrangements), Precision (average distance in centimeters between the final object position and the goal position), and Execution Time (average time in seconds to complete the task).  The results show that Human VLA-Teacher outperforms both versions of InterPhys across all three metrics.", "section": "5 Experiments"}, {"figure_path": "pjD08dtAh0/tables/tables_7_1.jpg", "caption": "Table 4: Results in unseen tasks.", "description": "This table presents the results of the HumanVLA model and its ablation studies on unseen tasks.  It shows the success rate (percentage of tasks successfully completed), precision (average distance in cm between the final object position and the goal position), and execution time (in seconds) for different model configurations.  The configurations include the complete HumanVLA model and versions with various components removed (geometry encoding, carry curriculum, style clipping, path planning, active rendering, and online learning).  The table also includes results from InterPhys [15] and Offline GC-BC [29] methods for comparison. The \"Privileged State\" column indicates whether the model used privileged information (such as object poses).", "section": "5 Experiments"}, {"figure_path": "pjD08dtAh0/tables/tables_17_1.jpg", "caption": "Table 5: Hyperparameters for HumanVLA-Teacher training.", "description": "This table lists the hyperparameters used during the training of the HumanVLA-Teacher model.  It includes parameters related to the training environment (number of environments, maximum episode length, etc.), the PPO algorithm (discount factor, learning rate, etc.), and the adversarial motion prior (AMP) method (consecutive frames, style reward weight, etc.). The values listed represent the settings used to achieve the results reported in the paper.", "section": "E.1.7 Hyperparameter Setting"}, {"figure_path": "pjD08dtAh0/tables/tables_18_1.jpg", "caption": "Table 6: Hyperparameters for HumanVLA training.", "description": "This table lists the hyperparameters used for training the HumanVLA model.  It includes parameters related to the training environment (number of environments, observation and action clipping, camera resolution and field of view), the learning process (learning rate, batch size, number of rollouts and training steps per epoch, optimizer), and the DAgger algorithm (beta naught and lambda).  Finally, it specifies the weight used for active rendering.", "section": "E.2.6 Hyperparameter Setting"}, {"figure_path": "pjD08dtAh0/tables/tables_20_1.jpg", "caption": "Table 7: Unseen data analysis.", "description": "This table presents the results of an unseen data analysis, evaluating the model's performance on tasks with unseen texts, unseen object (visual and geometry), and unseen scene layouts.  The metrics used are Success Rate (%), Precision (cm), and Execution Time (s).  Lower precision and execution time values are better.", "section": "5.5 Generalizing to Unseen Tasks"}]