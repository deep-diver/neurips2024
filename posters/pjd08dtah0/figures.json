[{"figure_path": "pjD08dtAh0/figures/figures_0_1.jpg", "caption": "Figure 1: HumanVLA performs various object rearrangement tasks directed by the egocentric vision and natural language instructions.", "description": "The figure shows three different scenarios of object rearrangement by a humanoid robot, guided by both visual input and natural language commands.  In each scenario, the robot is given a task, such as \"I'm cleaning the room. Please push the small brown table next to the bed.\"  The top row shows the initial scene, while the bottom row displays the robot completing the task using vision and language. The image highlights the system's ability to understand natural language instructions, perceive its environment, and execute actions to manipulate objects.", "section": "Introduction"}, {"figure_path": "pjD08dtAh0/figures/figures_3_1.jpg", "caption": "Figure 2: An overview of learning state-based HumanVLA-Teacher policy using goal-conditioned reinforcement learning and adversarial motion prior.", "description": "The figure illustrates the process of training the HumanVLA-Teacher policy.  It uses a goal-conditioned reinforcement learning approach combined with adversarial motion priors. The teacher policy receives the environment state (St), including object pose, geometry, waypoint, and goal coordinates.  It then produces an action (at) that's fed into a physics simulator, resulting in the next state (St+1). The learning process involves maximizing a reward (rt) composed of a goal-conditioned task reward and a style reward. The style reward utilizes an adversarial motion discriminator to ensure realistic motion synthesis.  The motion discriminator is trained separately, using a motion dataset for reference.", "section": "3.1 State-based Teacher Policy Learning"}, {"figure_path": "pjD08dtAh0/figures/figures_5_1.jpg", "caption": "Figure 3: Left: An overview of learning HumanVLA by mimicking teacher action and active rendering action. Right: Comparison between w/ and w/o active rendering. Active rendering leads to a more informative perception of human-object relationships.", "description": "This figure illustrates the architecture of HumanVLA, a vision-language-action model.  The left side shows the teacher-student framework used for training. The teacher policy receives privileged information (state) and generates actions. These actions are mimicked by the student (HumanVLA), which receives egocentric vision, language instructions, proprioception and history. The right side showcases a comparison of egocentric vision with and without active rendering.  Active rendering focuses the camera on the object of interest, resulting in a clearer and more informative visual input for HumanVLA.", "section": "3.2 Distilling into Vision-Language-Action Model"}, {"figure_path": "pjD08dtAh0/figures/figures_8_1.jpg", "caption": "Figure 4: Qualitative results. The color transitions from green to yellow as the task progresses.", "description": "This figure shows two examples of the HumanVLA model performing object rearrangement tasks.  In each example, a sequence of images displays the humanoid robot moving objects according to a given natural language instruction. The color of the humanoid changes from green to yellow to illustrate the progress of the task.  This visualization demonstrates the model's ability to successfully complete object rearrangement tasks as directed by language.", "section": "5.6 Qualitative Results"}, {"figure_path": "pjD08dtAh0/figures/figures_13_1.jpg", "caption": "Figure 5: The task generation process of HITR dataset.", "description": "This figure illustrates the process of generating tasks for the Human-in-the-Room (HITR) dataset. It starts with a template room layout. Then, an object is spawned and relocated to create the goal state. A human is also spawned in the scene. The initial and goal states are concatenated into a single image, which is fed to a large language model (GPT-4-vision) along with a prompt to generate natural language instructions for the object rearrangement task. These instructions, along with the concatenated image, serve as the input for training the model.  The figure visually represents this workflow, showing each step from template room layout to the generation of instructions.", "section": "4 Human-in-the-Room Dataset"}, {"figure_path": "pjD08dtAh0/figures/figures_14_1.jpg", "caption": "Figure 6: Different rooms in HITR dataset.", "description": "This figure shows four different room layouts included in the Human-in-the-Room (HITR) dataset.  Each layout represents a different room type (bedroom, living room, kitchen, and warehouse) and is populated with various objects to create diverse scenes for the object rearrangement tasks. The layouts vary in size, object arrangement, and overall aesthetic.", "section": "D Dataset"}, {"figure_path": "pjD08dtAh0/figures/figures_14_2.jpg", "caption": "Figure 7: Movable objects in HITR dataset.", "description": "This figure shows the 34 movable objects that are included in the Human-in-the-Room (HITR) dataset.  These objects vary in size, shape, and function, representing a diverse range of household items. The diversity of objects is crucial for evaluating the generalization capabilities of the HumanVLA model, as it needs to learn how to interact and rearrange a wide variety of items, not just a limited set of pre-defined objects.", "section": "4 Human-in-the-Room Dataset"}, {"figure_path": "pjD08dtAh0/figures/figures_16_1.jpg", "caption": "Figure 8: An overview of the path planning process. The blue mark denotes the initial position. Red marks denote the path from the initial position to the object. Green marks denote the path from the object to the goal.", "description": "This figure illustrates the path planning process used in the paper's method.  A* algorithm is used to plan two paths: one from the humanoid's starting position to the target object and a second path from the object to the final goal location. The paths are represented as a series of waypoints.  The waypoints are then simplified into a sparser set that maintains the overall path direction, making navigation more efficient. The humanoid follows these simplified waypoints during execution, moving towards each waypoint until it reaches it within a 50cm distance, then proceeding to the next.  The process is shown visually, with colors representing the different stages of the path.", "section": "E.1.4 In-context Path Planning"}, {"figure_path": "pjD08dtAh0/figures/figures_19_1.jpg", "caption": "Figure 9: Learning curve comparison w/ and w/o style reward clipping.", "description": "This figure shows the learning curves for the HumanVLA-Teacher model with and without style reward clipping.  The y-axis represents the task reward, indicating the model's performance. The x-axis shows the number of epochs during training. The comparison demonstrates the impact of the style reward clipping mechanism on the learning process and overall performance.", "section": "E.1.5 Carry Curriculum Pre-training"}, {"figure_path": "pjD08dtAh0/figures/figures_19_2.jpg", "caption": "Figure 9: Learning curve comparison w/ and w/o style reward clipping.", "description": "This figure shows the learning curves for task reward with and without style reward clipping during the training process of HumanVLA-Teacher.  The x-axis represents the number of epochs, and the y-axis shows the task reward. The graph illustrates the impact of style reward clipping on the learning process, showing the difference in the reward obtained with and without this technique.", "section": "E.1.5 Carry Curriculum Pre-training"}, {"figure_path": "pjD08dtAh0/figures/figures_19_3.jpg", "caption": "Figure 11: Comparison w/ and w/o path planning. The green humanoid without path guidance fails to get close to the sofa, while the yellow humanoid with path guidance learns to go around the central table. Instruction: Move the pillow to the sofa.", "description": "This figure compares the performance of the HumanVLA model with and without path planning.  Two humanoids are shown attempting to move a pillow to a sofa. The humanoid without path planning fails to efficiently navigate around obstacles, while the one with path planning successfully plans a route around the table to reach the sofa.", "section": "5 Experiments"}, {"figure_path": "pjD08dtAh0/figures/figures_20_1.jpg", "caption": "Figure 12: Additional qualitative results.", "description": "This figure shows two examples of object rearrangement tasks performed by HumanVLA, along with the corresponding instructions.  The top example shows the robot moving a red chair in front of a table. The bottom example shows the robot placing a box on the bottom shelf of a rack. The images show the sequence of actions performed by the robot to complete the task.", "section": "5.6 Qualitative Results"}]