[{"type": "text", "text": "Q-Distribution guided Q-learning for offline reinforcement learning: Uncertainty penalized Q-value via consistency model ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jing Zhang Linjiajie Fang Kexin Shi HKUST HKUST HKUST jzhanggy@connect.ust.hk lfangad@connect.ust.hk kshiaf@connect.ust.hk ", "page_idx": 0}, {"type": "text", "text": "Wenjia Wang\u2217 HKUST (GZ) wenjiawang@hkust-gz.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Bing-Yi Jing\u2217 SUSTech jingby@sustech.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "\u201cDistribution shift\u201d is the main obstacle to the success of offline reinforcement learning. A learning policy may take actions beyond the behavior policy\u2019s knowledge, referred to as Out-of-Distribution (OOD) actions. The Q-values for these OOD actions can be easily overestimated. As a result, the learning policy is biased by using incorrect Q-value estimates. One common approach to avoid Q-value overestimation is to make a pessimistic adjustment. Our key idea is to penalize the Q-values of OOD actions associated with high uncertainty. In this work, we propose Q-Distribution Guided Q-Learning (QDQ), which applies a pessimistic adjustment to Q-values in OOD regions based on uncertainty estimation. This uncertainty measure relies on the conditional Q-value distribution, learned through a high-fidelity and efficient consistency model. Additionally, to prevent overly conservative estimates, we introduce an uncertainty-aware optimization objective for updating the Q-value function. The proposed QDQ demonstrates solid theoretical guarantees for the accuracy of Q-value distribution learning and uncertainty measurement, as well as the performance of the learning policy. QDQ consistently shows strong performance on the D4RL benchmark and achieves significant improvements across many tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) has seen remarkable success by using expressive deep neural networks to estimate the value function or policy function [1]. However, in deep RL optimization, updating the Q-value function or policy value function can be unstable and introduce significant bias [2]. Since the learning policy is influenced by the Q-value function, any bias in the Q-values affects the learning policy. In online RL, the agent\u2019s interaction with the environment helps mitigate this bias through reward feedback for biased actions. However, in offilne RL, the learning relies solely on data from a behavior policy, making information about rewards for states and actions outside the dataset\u2019s distribution unavailable. ", "page_idx": 0}, {"type": "text", "text": "It is commonly observed that during offline RL training, backups using OOD actions often lead to target Q-values being overestimated [3] (see Figure 1(a)). As a result, the learning policy tends to prioritize these risky actions during policy improvement. This false prioritization accumulates with each training step, ultimately leading to failure in the offline training process [4, 5, 6]. Therefore, addressing Q-value overestimation for OOD actions is crucial for the effective implementation of offline reinforcement learning. ", "page_idx": 1}, {"type": "image", "img_path": "NIcIdhyfQX/tmp/074335281d35a07f817d17b55260e66a5f7d79d12138d8b2b90399bbfbb6f6a3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: (a) The maximum of the estimated $\\mathrm{^Q}$ -value often occurs in OOD actions due to the instability of the offilne RL backup process and the \u201cdistribution shift\u201d problem , so the Q-value of the learning policy (yellow line) will diverge from the behavior policy\u2019s action space (blue line) during the training. (b) The red line represents the optimal Q-value within the action space of the dataset, while the blue line depicts the Q-value function of the behavior policy. The gold line corresponds to the Q-value derived from the in-sample Q training algorithm, showcasing a distribution constrained by the behavior policy. On the other hand, the green line illustrates the Q-value resulting from a more conservative Q training process. Although it adopts lower values in OOD actions, the Q-value within in-distribution areas proves excessively pessimistic, failing to approach the optimal Q-value. ", "page_idx": 1}, {"type": "text", "text": "Since any bias or error in the Q-value will propagate to the learning policy, it\u2019s crucial to evaluate whether the Q-value is assigned to OOD actions and to apply a pessimistic adjustment to address overestimation. Ideally, this adjustment should only target OOD actions. One common way to identify whether the Q-value function is updated by OOD actions is by estimating the uncertainty of the Q-value [4] in the action space. However, estimating uncertainty presents significant challenges, especially with high-capacity Q-value function approximators like neural networks [4]. If Q-value uncertainty is not accurately estimated, a penalty may be uniformly applied across most actions [7], hindering the optimality of the Q-value function. ", "page_idx": 1}, {"type": "text", "text": "While various methods [7, 8, 9, 10, 11, 12, 3, 13, 14] attempt to make pessimistic estimates of the Q-value function, most have not effectively determined which Q-values need constraining or how to pessimistically estimate them with reliable and efficient uncertainty estimates. As a result, previous methods often end up being overly conservative in their Q-value estimations [15] or fail to achieve a tight lower confidence bound of the optimal Q-value function. Moreover, some in-sample training [16, 17, 18, 19] of the Q-value function may lead it to closely mimic the Q-value of the behavior policy (see Figure 1(b)), rendering it unable to surpass the performance of the behavior policy, especially when the behavior policy is sub-optimal. Therefore, in balancing Q safety for learning and not hindering the recovery of the most optimal Q-value, current methods tend to prioritize safe optimization of the Q-value function. ", "page_idx": 1}, {"type": "text", "text": "In this study, we introduce Q-Distribution Guided Q-Learning (QDQ) for offline RL 2. The core concept focuses on estimating Q-value uncertainty by directly computing this uncertainty through bootstrap sampling from the behavior policy\u2019s Q-value distribution. By approximating the behavior policy\u2019s Q-values using the dataset, we train a high-fidelity and efficient distribution learnerconsistency model [20]. This ensures the quality of the learned Q-value distribution. ", "page_idx": 1}, {"type": "text", "text": "Since the behavior and learning policies share the same set of high-uncertainty actions [5], we can sample from the learned Q-value distribution to estimate uncertainty, identify risky actions, and make the Q target values for these actions more pessimistic. We then create an uncertaintyaware optimization objective to carefully penalize Q-values that may be OOD, ensuring that the constraints are appropriately pessimistic without hindering the Q-value function\u2019s exploration in the in-distribution region. QDQ aims to find the optimal Q-value that exceeds the behavior policy\u2019s optimal Q-value while remaining as pessimistic as possible in the OOD region. Moreover, our pessimistic approach is robust against errors in uncertainty estimation. Our main contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 Utilization of trajectory-level data with a sliding window: We use trajectory-level data with a sliding window approach to create the real truncated Q dataset. Our theoretical analysis (Theorem 4.1) confirms that the generated data has a distribution similar to true Q-values. Additionally, distributions learned from this dataset tend to favor high-reward actions. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Introduction of a consistency model as a distribution learner: QDQ introduces the consistency model [20] as the distribution learner for the Q-value. Similar to the diffusion model, the consistency model demonstrates strong capabilities in distribution learning. Our theoretical analysis (Theorem 4.2) highlights its consistency and one-step sampling properties, making it an ideal choice for uncertainty estimation. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Risk estimation of Q-values through uncertainty assessment: QDQ estimates the risk set of Q-values by evaluating the uncertainty of actions. For Q-values likely to be overestimated and associated with high uncertainty, a pessimistic penalty is applied. For safer Q-values, a mild adjustment based on uncertainty error enhances their robustness. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Uncertainty-aware optimization objective to address conservatism: To reduce the overly conservative nature of pessimistic Q-learning in offline RL, QDQ introduces an uncertainty-aware optimization objective. This involves simultaneous optimistic and pessimistic learning of the Q-value. Theoretical (Theorem 4.3 and Theorem 4.4) and experimental analyses show that this approach effectively mitigates conservatism issues. ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our approach aims to temper the Q-values in OOD areas to mitigate the risk of unpredictable extrapolation errors, leveraging uncertainty estimation. We estimate the uncertainty of Q-values across actions visited by the learning policy using samples from a learned conditional Q-distribution via the consistency model. In this section, we provide a concise overview of the problem settings in offline RL and introduce the consistency model. ", "page_idx": 2}, {"type": "text", "text": "2.1 Fundamentals in offline RL ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The online RL process is shaped by an infinite-horizon Markov decision process (MDP): $\\mathcal{M}=$ $\\{S,\\mathcal{A},\\mathbb{P},r,\\mu_{0},\\gamma\\}$ . The state space is $\\boldsymbol{S}$ , and $\\boldsymbol{\\mathcal{A}}$ is the action space. The transition dynamic among the state is determined by $\\mathbb{P}:S\\times A\\mapsto\\Delta(S)$ , where $\\Delta(S)$ is the support of $\\boldsymbol{S}$ . The reward determined on the whole state and action space is $r:S\\times A\\mapsto\\mathbb{R},r<\\infty$ , and can either be deterministic or random. $\\mu_{0}{\\left(s_{0}\\right)}$ is the distribution of the initial states $s_{0}$ , $\\gamma\\in(0,1)$ is the discount factor. The goal of RL is to find the optimal policy $\\pi:S\\mapsto\\Delta(a)$ that yields the highest long-term average return: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\boldsymbol{J}(\\pi)=\\mathbb{E}_{s_{0}\\sim\\mu_{0}}V(s_{0})=\\mathbb{E}_{s_{0}\\sim\\mu_{0},a_{0}\\sim\\pi}Q(s_{0},a_{0})=\\mathbb{E}_{s_{0}\\sim\\mu_{0},a_{0}\\sim\\pi}\\left[\\mathbb{E}_{\\pi}\\left[\\sum_{k=1}^{\\infty}\\gamma^{k-1}r_{k}|s_{k},a_{k}\\right]\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $Q(s,a)$ is the $\\mathrm{^Q}$ -value function under policy $\\pi$ . The process of obtaining the optimal policy is generally to recover the optimal $\\mathrm{^Q}$ -value function, which maximizes the $\\mathrm{{Q}}.$ -value function over the whole space $\\boldsymbol{\\mathcal{A}}$ , and then to obtain either an implicit policy (Q-Learning algorithm [21, 22, 23]), or a parameterized policy (Actor-Critic algorithm [24, 25, 26, 27, 28]). ", "page_idx": 2}, {"type": "text", "text": "The optimal Q-value function $Q^{*}(s,a)$ can be obtained by minimizing the Bellman residual: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}_{s\\sim\\mathbb{P},a\\sim\\pi}[Q(s,a)-B Q(s,a)]^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is the Bellman operator defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{B}Q(s,a):=r(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim\\mathbb{P}}[\\operatorname*{max}_{a^{\\prime}\\sim\\pi}Q(s^{\\prime},a^{\\prime})].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "However, the whole paradigm needs to be adjusted in the offline RL setting, as MDP is only determined from a dataset $\\mathcal{D}$ , which is generated by behavior policy $\\pi_{\\beta}$ . Hence, the state and action space is constraint by the distribution support of $\\mathcal{D}$ . We redefine the MDP in the offline RL setting as: $\\mathcal{M}_{\\mathcal{D}}=\\{S_{\\mathcal{D}},\\mathcal{A}_{\\mathcal{D}},\\mathbb{P}_{\\mathcal{D}},r,\\mu_{0},\\gamma\\}$ , where $S_{\\mathcal{D}}\\,=\\,\\{s|s\\in\\Delta(s^{\\mathcal{D}})\\}$ , $\\mathcal{A}_{\\mathcal{D}}\\,=\\,\\{a|a\\,\\in\\,\\Delta(\\pi_{\\beta})\\}$ . Then the transition dynamic is determined by $\\mathbb{P}_{D}:S_{\\mathcal{D}}\\times A_{\\mathcal{D}}\\mapsto\\Delta(S_{\\mathcal{D}})$ . Therefore, the well-known \u201cdistribution shift\u201d problem occurs when solving the Bellman equation Eq.2. The Bellman residual is taking expectation in $S_{D}\\times A_{D}$ , while the target Q-value is calculated based on the actions from the learning policy. ", "page_idx": 3}, {"type": "text", "text": "2.2 Consistency model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The consistency model is an enhanced generative model compared to the diffusion model. The diffusion model gradually adds noise to transform the target distribution into a Gaussian distribution and by estimating the random noise to achieve the reverse process, i.e., sampling a priori sample from a Gaussian distribution and denoise to the target sample iteratively (forms the sample generation trajectory). The consistency model is proposed to ensures each step in a sample generation trajectory of the diffusion process aligns with the target sample (we call consistency). Specifically, the consistency model [20] try to overcome the slow generation and inconsistency over sampling trajectory generated by the Probability Flow (PF) ODE during training process of the diffusion model [29, 30, 31, 32, 33]. ", "page_idx": 3}, {"type": "text", "text": "Let $p_{d a t a}(\\pmb{x})$ denote the data distribution, we start by diffuse the original data distribution by the PF ODE: ", "page_idx": 3}, {"type": "equation", "text": "$$\nd x_{t}=\\left[\\mu(x_{t},t)-\\frac{1}{2}\\sigma(t)^{2}\\nabla\\log(p_{t}(\\mathbf{x}_{t}))\\right]d t,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pmb{\\mu}(\\cdot,\\cdot)$ is the drift coefficient, $\\sigma(\\cdot)$ is the diffusion coefficient, $p_{t}(\\pmb{x}_{t})$ is the distribution of $\\pmb{x}_{t}$ , $p_{0}({\\pmb x})\\equiv\\stackrel{.}{p}_{d a t a}({\\pmb x})$ , and $\\{\\boldsymbol{x}_{t},t\\in[\\epsilon,T]\\}$ is the solution trajectory of the above PF ODE. ", "page_idx": 3}, {"type": "text", "text": "Consistency model aims to learn a consistency function $f_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t},t)$ that maps each point in the same PF ODE trajectory to its start point, i.e., $f_{\\theta}(\\mathbf{x}_{t},t)=x_{\\epsilon},\\forall t\\in[\\epsilon,T]$ . Therefore, $\\bar{\\forall}t,t^{\\prime}\\in[\\epsilon,T]$ , we have $f_{\\theta}({\\bf x}_{t},\\dot{t})=\\dot{f}_{\\theta}(x_{t^{\\prime}},t^{\\prime})$ , which is the \u201cself-consistency\u201d property of consistency model. ", "page_idx": 3}, {"type": "text", "text": "Here, $f_{\\theta}({\\boldsymbol{x}}_{t},t)$ is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{\\theta}(\\mathbf{x}_{t},t)=c_{s k i p}(t)\\mathbf{x}_{t}+c_{o u t}(t)F_{\\theta}(\\mathbf{x}_{t},t),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $c_{s k i p}(t)$ and $c_{o u t}(t)$ are differentiable functions, and $c_{s k i p}(\\epsilon)=1$ , $c_{o u t}(\\epsilon)=0$ such that they satisfy the boundary condition $f_{\\theta}(x_{\\epsilon},\\epsilon)=x_{\\epsilon}$ . In (4), $F_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t},t)$ can be free-form deep neural network with output that has the same dimension as $\\pmb{x}_{t}$ . ", "page_idx": 3}, {"type": "text", "text": "Consistency function $f_{\\theta}({\\boldsymbol{x}}_{t},t)$ can be optimized by minimizing the difference of points in the same PF ODE trajectory. If we use a pretrained diffusion model to generate such PF ODE trajectory, then utilize it to train a consistency model, this process is called consistency distillation. We use the consistency distillation method to learn a consistency model in this work and optimize the consistency distillation loss as the Definition 1 in [20]. ", "page_idx": 3}, {"type": "text", "text": "With a well-trained consistency model $f_{\\theta}({\\boldsymbol{x}}_{t},t)$ , we can generate samples by sampling from the initial Gaussian distribution $\\hat{\\pmb{x}}_{T}\\sim\\mathcal{N}({\\bf0},T^{2}{\\cal I})$ and then evaluating the consistency model for $\\hat{\\pmb{x}}_{\\epsilon}=$ $f_{\\theta}(\\hat{\\mathbf{x}}_{T},T)$ . This involves only one forward pass through the consistency model and therefore generates samples in a single step. This is the one-step sampling process of the consistency model. ", "page_idx": 3}, {"type": "text", "text": "3 Q-Distribution guided Q-learning via Consistency Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this work, we present a novel method for offline RL called Q-Distribution Guided Q-Learning (QDQ). First, we quantify the uncertainty of $\\mathrm{^Q}$ -values by learning the Q-distribution using the consistency model. Next, we propose a strategy to identify risky actions and penalize their Q-values based on uncertainty estimation, helping to mitigate the associated risks. To tackle the excessive conservatism seen in previous approaches, we introduce uncertainty-aware Q optimization within the Actor-Critic learning framework. This mechanism allows the Q-value function to perform both optimistic and pessimistic optimization, fostering a balanced approach to learning. ", "page_idx": 3}, {"type": "text", "text": "3.1 Learn Uncertainty of Q-value by Q-distribution ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Estimating the uncertainty of the Q function is a significant challenge, especially with deep neural network Q estimators. A practical indicator of uncertainty is the presence of large variances in the estimates. Techniques such as bootstrapping multiple Q-values and estimating variance [7] have been used to address this issue. However, these ensemble methods often lack diversity in Q-values [9] and fail to accurately represent the true Q-value distribution. They may require tens or hundreds of Q-values to improve accuracy, which is computationally inefficient [9, 5]. ", "page_idx": 4}, {"type": "text", "text": "Other approaches involve estimating the Q-value distribution and determining the lower confidence bound [10, 12, 16], or engaging in in-distribution learning of the Q-value function [3, 13, 16, 10, 34]. However, these methods often struggle to provide precise uncertainty estimations for the Q-value [15]. Stabilization methods can still lead to Q-value overestimation [3], while inaccurate variance estimation can worsen this problem. Furthermore, even if the Q-value is not overestimated, there is still a risk of it being overly pessimistic or constrained by the performance of the behavior policy when using in-distribution-only training. ", "page_idx": 4}, {"type": "text", "text": "In this subsection, we elucidate the process of learning the distribution of Q-values based on the consistency model, and outline the technique for estimating the uncertainty of actions and identifying risky actions. We have give a further demonstration on the performance of the consistency model and efficiency of the uncertainty estimation in Appendix G.2 and Appendix G.3. ", "page_idx": 4}, {"type": "text", "text": "Trajectory-level truncated Q-value. We chose to estimate the Q-value distribution of the behavior policy instead of the learning policy because they share a similar set of high-uncertainty actions [5]. Using the behavior policy\u2019s Q-value distribution has several advantages. First, the behavior policy\u2019s Q-value dataset comes from the true dataset, ensuring high-quality distribution learning. In contrast, the learning policy\u2019s Q-value is unknown, counterfactually learned, and often noisy and biased, leading to poor data quality and biased distribution learning. Second, using the behavior policy\u2019s Q-value distribution to identify high-uncertainty actions does not force the learning policy\u2019s target Q-value to align with that of the behavior policy. ", "page_idx": 4}, {"type": "text", "text": "To gain insights into the Q-value distribution of the behavior policy, we first need the raw Q-value data. The calculation of the Q-value operates at the trajectory level, represented as $\\tau=(s_{0},a_{0},s_{1},a_{1},\\ldots),$ with an infinite horizon (see Eq.1). In the context of offline RL, our training relies on the dataset $\\mathcal{D}$ produced by the behavior policy. This dataset consists of trajectories generated by the behavior policy, which is the only available trajectory-level data. However, the trajectory-level data from the behavior policy often faces a significant challenge: sparsity. This issue becomes even more pronounced when dealing with low-quality behavior policies, as the generated trajectories tend to be sporadic and do not adequately cover the entire state-action space $s\\times A$ , especially the high reward region. ", "page_idx": 4}, {"type": "text", "text": "To address this pervasive issue of sparsity, as well as the infinite summation in Eq.1, we present a novel approach aimed at enhancing sample efficiency. Our proposed solution involves the utilization of truncated trajectories to ameliorate the sparsity conundrum and avoid infinite summation. By employing a $k$ - step sliding window of width $\\tau$ , we systematically traverse the original trajectories, isolating segments within the window to compute the truncated $\\mathrm{^Q}$ -value (as depicted in Figure A.1). For instance, considering the initiation point of the $i$ -th step sliding window as $(s_{i},a_{i})$ , by setting $\\tau=i+k$ , we derive the truncated $\\mathrm{^Q}$ -value of this starting point as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nQ_{T}^{\\pi_{\\beta}}(s_{i},a_{i})=\\sum_{m=i}^{T}\\gamma^{m-1}r(s_{m},a_{m})\\times t(s_{m},a_{m}),t(s_{m},a_{m})=\\left\\{\\!\\!\\begin{array}{l l}{0,}&{t e r m i n a l,}\\\\ {1,}&{o t h e r w i s e.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The truncation of Q-values can occur either through sliding window mechanisms or task terminations. When truncation happens due to termination, the Q-value from Eq.5 is equivalent to the true Q-value, $Q^{\\pi_{\\beta}}T(\\cdot,\\cdot)\\,\\equiv\\,Q^{\\pi_{\\beta}}(\\cdot,\\cdot)$ . In contrast, if truncation results from window blocking, our theoretical analysis in Theorem 4.1 confirms that the distribution of truncated Q-values has properties similar to those of the true Q-value distribution. ", "page_idx": 4}, {"type": "text", "text": "Using a $k$ -step sliding window does not compromise the consistency of the trajectory, owing to the inherent memory-less Markov property in RL. This strategic truncation allows for the extraction of truncated Q-values, which can improve sample efficiency, especially for long trajectories. Moreover, this approach highlights actions with potential high Q-values, as actions from lengthy trajectories\u2014those with many successful interactions\u2014are encountered more often during Q-distribution training. Consequently, the uncertainty of these actions is lower, reducing the likelihood of them being overly pessimistic. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Learn the distribution of Q-value. In distributional RL, the learning of Q-value distributions is typically achieved through Gaussian neural networks [35, 36], Gaussian processes [37, 38], or categorical parameterization [39]. However, these methods often suffer from low precision representation of Q-value distributions, particularly in high-dimensional spaces. Moreover, straightforward replacement of true Q-value distributions with ensembles or bootstraps can lead to reduced accuracy in uncertainty estimation(a critical aspect in offilne reinforcement learning [4]), or impose significant computational burdens [8, 7]. ", "page_idx": 5}, {"type": "text", "text": "The idea of diffusing the original distribution using random noise has rendered the diffusion model a potent and high-fidelity distribution learner. However, it has limitations when estimating uncertainty. Sampling with a diffusion model requires a multi-step forward diffusion process to ensure sample quality. Unfortunately, this iterative process can compromise the accuracy of uncertainty estimates by introducing significant fluctuations and noise into the Q-value uncertainty. For a detailed discussion, see Appendix A.2. ", "page_idx": 5}, {"type": "text", "text": "To address this issue, we suggest using the consistency model [20] to learn the Q-value distribution. The consistency model allows for one-step sampling, like other generative models, which reduces the randomness found in the multi-step sampling of diffusion models. This results in a more robust uncertainty estimation. Furthermore, the consistency feature, as explained in Theorem 4.2, accurately captures how changes in actions affect the variance of the final bootstrap samples, making Q-value uncertainty more sensitive to out-of-distribution (OOD) actions compared to the diffusion model. Additionally, the fast-sampling process of the consistency model improves QDQ\u2019s efficiency. While there may be some quality loss in restoring real samples, this is negligible for QDQ since it only calculates uncertainty based on the variance of the bootstrap samples, not the absolute Q-value of the sampled samples. Overall, the consistency model is an ideal distribution learner for uncertainty estimation due to its reliability, high-fidelity, ease of training, and faster sampling. ", "page_idx": 5}, {"type": "text", "text": "Once we derive the truncated Q dataset $\\mathcal{D}_{Q}$ , we train a conditional consistency model, denoted by $f_{\\theta}(x_{T},T|(s,a))$ , which approximates the distribution of Q-values. Since the consistency model aligns with one-step sampling, we can easily sample multiple Q-values for each action using the consistency model. Suppose we draw $n$ prior noise $\\{\\hat{x}_{T_{1}},\\hat{x}_{T_{2}},\\cdot\\cdot\\cdot\\,,\\hat{x}_{T_{n}}\\}$ from the initial noise distribution $\\mathcal{N}(0,T^{2})$ , and denoise the prior samples by the consistency one-step forward process: $\\hat{x}_{\\epsilon_{i}}=f_{\\theta}(\\hat{x}_{T_{i}},\\hat{T_{i}}|(s,a)),i=1,2,\\cdots\\,,n$ . Then the variance of these $\\mathrm{^Q}$ -values, derived by ", "page_idx": 5}, {"type": "equation", "text": "$$\nV(X_{\\epsilon}|(s,a))=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left[f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))-\\frac{1}{n}\\sum_{i=1}^{n}f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))\\right]^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "can be used to gauge the uncertainty of $Q(s,a)$ . ", "page_idx": 5}, {"type": "text", "text": "3.2 Q-distribution guided optimization in offline RL ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Recover Q-value function. We propose an uncertainty-aware optimization objective $\\mathcal{L}_{u w}(Q)$ to penalize $Q\\cdot$ -value for OOD actions as well as to avoid too conservative $\\mathrm{^Q}$ -value learning for indistribution areas. The uncertainty-aware learning objective for Q-value function $Q_{\\theta}(s,a)$ is : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{u w}(Q_{\\theta})=\\operatorname*{min}_{\\theta}\\{\\alpha\\mathcal{L}(Q_{\\theta})_{H}+(1-\\alpha)\\mathcal{L}(Q_{\\theta})_{L}\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In Eq.7, $\\mathcal{L}(Q_{\\theta})H$ represents the classic Bellman residual defined in Eq.2. This residual is used in online RL and encourages optimistic optimization of the Q-value. In contrast, $\\mathcal{L}(Q\\theta)_{L}$ is a pessimistic Bellman residual based on the uncertainty-penalized $\\mathrm{Q}$ target $Q_{L}(s^{\\prime},a^{\\prime})$ , defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\nQ_{L}(s^{\\prime},a^{\\prime})=\\frac{1}{\\mathcal{H}_{Q}(a^{\\prime}|s^{\\prime})}Q_{\\theta}(s^{\\prime},a^{\\prime})1_{(a^{\\prime}\\in\\mathcal{U}(Q))}+\\beta Q_{\\theta}(s^{\\prime},a^{\\prime})1_{(a^{\\prime}\\notin\\mathcal{U}(Q))}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In Eq.8, $\\mathcal{H}_{Q}(a^{\\prime}|s^{\\prime})=\\sqrt{V(X_{\\epsilon}|(s^{\\prime},a^{\\prime}))}$ represents the uncertainty estimate of the $\\mathrm{^Q}$ -value for action $a^{\\prime}$ . The set $\\mathcal{U}(Q)$ includes actions that may be out-of-distribution (OOD). We use the upper $\\beta$ -quantile $\\mathcal{H}_{Q}^{\\beta}(a^{\\prime}|s^{\\prime})$ of the uncertainty estimate on actions taken by the learning policy as the threshold for forming $\\mathcal{U}(Q)$ . Additionally, we incorporate the quantile parameter $\\beta$ as a robust weighting factor for the unpenalized Q-target value. This helps control the estimation error of uncertainty and enhances the robustness of the learning objective. We can also set a free weighting factor, but we use $\\beta$ to reduce the number of hyperparameters. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Improve the learning policy. The optimization of learning policy follows the classic online RL paradigm: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\phi}(\\pi)=\\operatorname*{max}_{\\phi}~\\left[\\mathbb{E}_{s\\sim\\mathbb{P}_{\\mathcal{D}}(s),a\\sim\\pi_{\\phi}(\\cdot|s)}[Q_{\\theta}(s,a)]+\\gamma\\mathbb{E}_{a\\sim\\mathcal{D}}[\\log\\pi_{\\phi}(a)]\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In Eq.9, an entropy term is introduced to further stabilize the volatile learning process of Q-value function. For datasets with a wide distribution, we can simply set the penalization factor $\\gamma$ to zero, which can further enhance performance. Furthermore, other policy learning objectives, such as the AWR policy objective [40], can also be flexibly used within the QDQ framework, especially for the goal conditioned task like Antmaze. ", "page_idx": 6}, {"type": "text", "text": "We outline the entire learning process of QDQ in Algorithm 1. In Section 4, Theorems 4.3 and 4.4 show that QDQ penalizes the OOD region based on uncertainty while ensuring that the $\\mathrm{{Q}}.$ -value function in the in-distribution region is close to the optimal Q-value. This alignment is the main goal of offline RL. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 Q-Distribution guided Q-learning (QDQ) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Initialize: target network update rate $\\kappa$ , uncertainty-aware learning hyperparameter $\\alpha$ , $\\beta$ ,policy   \ntraining hyperparameters $\\gamma$ . Consistency model $f_{\\eta}$ , $\\mathrm{\\DeltaQ}$ networks $\\overline{{\\{Q_{\\theta_{1}}}}},\\bar{Q}_{\\theta_{2}}\\}$ , actor $\\pi_{\\phi}$ , target   \nnetworks $\\bar{\\{Q_{\\theta_{1}^{\\prime}},Q_{\\theta_{2}^{\\prime}}\\}}$ , target actor $\\pi_{\\phi^{\\prime}}$ .   \nQ-distribution learning:   \nCalculate Q dataset $\\bar{D}_{Q}\\,\\stackrel{\\smile}{=}\\,\\{Q_{T}^{\\pi_{\\beta}}(s,a)\\}$ scanning each trajectory $\\tau\\in{\\mathcal{D}}$ by Eq.5.   \nfor each gradient step do Sample minibatch of $q_{\\mathcal{T}}^{\\pi_{\\beta}}(s,a)\\sim\\mathcal{D}_{Q}$ Update $\\eta$ minimizing consistency distillation loss in Eq.(7) [20]   \nend for   \nfor each gradient step do Sample mini-batch of transitions $(s,a,r,s^{\\prime})\\sim\\mathcal{D}$ Updating Q-function: Update $\\theta=(\\theta_{1},\\theta_{2})$ minimizing ${\\mathcal{L}}_{u w}(Q_{\\theta})$ in Eq.7 Updating policy: Update $\\phi$ minimizing $\\mathcal{L}_{\\phi}(\\pi)$ in Eq.9 Update Target Networks: $\\phi^{\\bar{\\prime}}\\leftarrow\\kappa\\phi+\\bar{(1-\\kappa)}\\phi^{\\prime};\\theta_{i}^{\\prime}\\leftarrow\\kappa\\theta_{i}+(1-\\kappa)\\theta_{i}^{\\prime},i=1,2$   \nend for ", "page_idx": 6}, {"type": "text", "text": "4 Theoretical Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we provide a theoretical analysis of QDQ. The first theorem states that if $\\tau$ is sufficiently large, the distribution of $Q^{\\pi_{\\beta}}T$ does not significantly differ from the true distribution of $Q^{\\pi\\beta}$ . This shows that our sliding window-based truncated $Q\\cdot$ -value distribution converges to the true Q-value distribution, ensuring accurate uncertainty estimation. A detailed proof can be found in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1 (Informal). Under some mildly condition, the truncated $Q$ -value $Q_{\\mathcal{T}}^{\\pi_{\\beta}}$ converge indistribution to the true true $Q$ -value $Q^{\\pi_{\\beta}}$ . ", "page_idx": 6}, {"type": "equation", "text": "$$\nF_{Q_{\\mathcal{T}}^{\\pi_{\\beta}}}(x)\\rightarrow F_{Q^{\\pi_{\\beta}}}(x),\\mathcal{T}\\rightarrow+\\infty.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In Theorem 4.2, we analyze why the consistency model is suitable for estimating uncertainty. Our analysis shows that Q-value uncertainty is more sensitive to actions. This sensitivity helps in detecting out-of-distribution (OOD) actions. A detailed statement of the theorem and its proof can be found in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.2 (Informal). Following the assumptions as in [20], $f_{\\theta}(x,T|(s,a))$ is $L$ -Lipschitz. We also assume the truncated $Q$ -value is bounded by $\\mathcal{H}$ . The action $a$ broadly influences $\\bar{V}(X_{\\epsilon}|(s,a))$ by: | \u2202va\u2202r(aX\u03f5)| = O(L2T log n)1. ", "page_idx": 6}, {"type": "text", "text": "In Theorem 4.3, we give theoretical analysis that the uncertainty-aware learning objective in Eq.7 can converge and the details can be found in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.3 (Informal). The $Q$ -value function of QDQ can converge to a fixed point of the Bellman equation: $Q(s,a)=\\mathcal{F}Q(s,a)$ , where the Bellman operator ${\\mathcal{F}}Q(s,a)$ is defined as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{F}Q(s,a):=r(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{\\mathcal{D}}(s^{\\prime})}\\{\\operatorname*{max}_{a^{\\prime}}[\\alpha Q(s^{\\prime},a^{\\prime})+(1-\\alpha)Q_{L}(s^{\\prime},a^{\\prime})]\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Theorem 4.4 shows that QDQ penalizes the OOD region by uncertainty while ensuring that the $\\mathrm{^Q}$ -value function in the in-distribution region is close to the optimal $\\mathrm{^Q}$ -value, which is the goal of offline RL. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.4 (Informal). Under mild conditions, with probability $1-\\eta$ we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\|Q^{\\Delta}-Q^{*}\\right\\|_{\\infty}\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $Q^{\\Delta}$ is learned by the uncertainty-aware loss in Eq.7, \u03f5 is error rate related to the difference between the classical Bellman operator $B Q$ and the QDQ bellman operator ${\\mathcal{F}}Q$ . ", "page_idx": 7}, {"type": "text", "text": "The optimal Q-value, $Q^{\\Delta}$ , derived by the QDQ algorithm can closely approximate the optimal Q-value function, $Q^{*}$ , benefiting from the balanced approach of the QDQ algorithm that avoids excessive pessimism for in-distribution areas. Both the value $\\epsilon$ and $\\eta$ are small and more details in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we first delve into the experimental performance of QDQ using the D4RL benchmarks [41]. Subsequently, we conduct a concise analysis of parameter settings, focusing on hyperparameter tuning across various tasks. For detailed implementation, we refer to Appendix G. ", "page_idx": 7}, {"type": "text", "text": "5.1 Performance on D4RL benchmarks for Offline RL ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluate the proposed QDQ algorithm on the D4RL Gym-MuJoCo and AntMaze tasks. We compare it with several strong state-of-the-art (SOTA) model-free methods: behavioral cloning (BC), BCQ [42], DT [43], AWAC [44], Onestep RL [45], $\\mathrm{TD3+BC}$ [46], CQL [3], and IQL [10]. We also include UWAC [7], EDAC [9], and PBRL [14], which use uncertainty to pessimistically adjust the Q-value function, as well as MCQ [13], which introduces mild constraints to the Q-value function. The experimental results for the baselines reported in this paper are derived from the original experiments conducted by the authors or from replication of their official code. The reported values are normalized scores defined in D4RL [41]. ", "page_idx": 7}, {"type": "text", "text": "Table 1: Comparison of QDQ and the other baselines on the three Gym-MuJoCo tasks. All the experiment are performed on the MuJoCo \"-v2\" dataset. The results are calculated over 5 random seeds.med $=$ medium, $\\boldsymbol{\\mathrm{r}}=$ replay, $\\mathbf{e}=$ expert, ha $=$ halfcheetah, wa $=$ walker2d, ho $:=$ hopper ", "page_idx": 7}, {"type": "table", "img_path": "NIcIdhyfQX/tmp/d67a8a96856a628e50d0f46defdffbb30518083c4c67ec2017314aae7e836524.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 1 shows the performance comparison between QDQ and the baselines across Gym-MuJoCo tasks, highlighting QDQ\u2019s competitive edge in almost all tasks. Notably, QDQ excels on datasets with wide distributions, such as medium and medium-replay datasets. In these cases, QDQ effectively avoids the problem of over-penalizing Q-values. By balancing between being too conservative and actively exploring to find the optimal Q-value function through dynamic programming, QDQ gradually converges toward the optimal Q-value, as supported by Theorem 4.4. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "NIcIdhyfQX/tmp/d50fc47f881b3f47c7c75cd2cd51661e991573723fe63ce7bef3747f1830fb30.jpg", "table_caption": ["Table 2: Comparison of QDQ and the other baselines on the Antmaze tasks. All the experiment are performed on the Antmaze \"-v0\" dataset for the comparison comfortable with previous baseline. The results are calculated over 5 random seeds. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 2 presents the performance comparison between QDQ and selected baselines3 across AntMaze tasks, highlighting QDQ\u2019s commendable performance. While QDQ focuses on reducing overly pessimistic estimations, it does not compromise its performance on narrow datasets. This is evident in its competitive results on the medium-expert dataset in Table 1, as well as its performance on AntMaze tasks. Notably, QDQ outperforms SOTA methods on several datasets. This success is due to the inherent flexibility of the QDQ algorithm. By allowing for flexible hyperparameter control and seamless integration with various policy optimization methods, QDQ achieves a synergistic performance enhancement. ", "page_idx": 8}, {"type": "text", "text": "5.2 Parameter analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The uncertainty-aware loss parameter $\\alpha$ . The parameter $\\alpha$ is crucial for balancing the dominance between optimistic and pessimistic updates of the Q-value (Eq.7). A higher $\\alpha$ value skews updates toward the optimistic side, and we choose a higher $\\alpha$ when the dataset or task is expected to be highly robust. However, the setting of $\\alpha$ is also influenced by the pessimism of the Q target defined in Eq.8. For a more pessimistic $\\mathrm{\\DeltaQ}$ target value, we can choose a larger $\\alpha$ . Interestingly, both the theoretical analyses in Theorem 4.3 and Theorem 4.4 and empirical parameter tuning suggest that variability in $\\alpha$ across tasks is minimal, with a typical value around 0.95. ", "page_idx": 8}, {"type": "text", "text": "The uncertainty related parameter $\\beta$ . The parameter $\\beta$ influences both the partitioning of high uncertainty sets and acts as a relaxation variable to control uncertainty estimation errors. When dealing with a narrow action space or a sensitive task (such as the hopper task), the value of $\\beta$ should be smaller. In these cases, the Q-value is more likely to select OOD actions, increasing the risk of overestimation. This means we face greater uncertainty (Eq.8) and need to minimize overestimation errors. Therefore, we require stricter criteria to ensure actions are in-distribution and penalize the Q-values of OOD points more heavily. A detailed analysis of how to determine the value of $\\beta$ can be found in Appendix G.3. ", "page_idx": 8}, {"type": "text", "text": "The entropy parameter $\\gamma$ . The $\\gamma$ term in Eq.9 stabilizes the learning of a simple Gaussian policy, especially for action-sensitive and narrower distribution tasks. When the dataset has a wide distribution or the task shows high robustness to actions (such as in the half-cheetah task), the Q-value function generalizes better across the action space. In these cases, we can set a more lenient requirement for actions, keeping the value of $\\gamma$ as small as possible or even at 0. However, when the dataset is narrow (e.g., in the AntMaze task) or when the task is sensitive to changes in actions (like in the hopper or maze tasks, where small deviations can lead to failure), a larger value of $\\gamma$ is necessary. For these tasks, a simple Gaussian policy can easily sample risky actions, as it fits a single-mode policy. Nonetheless, experimental results indicate that the sensitivity of the $\\gamma$ parameter is not very high. In fact, $\\gamma$ in Eq.9 is relatively small compared to the Q-value, primarily to stabilize training and prevent instability in Gaussian policy action sampling. See Appendix G.7 for more details. ", "page_idx": 8}, {"type": "text", "text": "6 Related Works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Restrict policy deviate from OOD areas. The distribution mismatch between the behavior policy and the learning policy can be overcome if the learning policy share the same support with the behavior policy. One approach involves explicit distribution matching constraints, where the learning policy is encouraged to align with the behavior policy by minimizing the distance between their distributions. This includes techniques based on KL-divergence [47, 48, 40, 44, 46], Jensen\u2013Shannon divergence [49], and Wasserstein distance [47, 49]. Another line of research aims to alleviate the overly conservative nature of distribution matching constraints by incorporating distribution support constraints. These methods employ techniques such as Maximum Mean Discrepancy (MMD) distance [5], learning behavior density functions using implicit [50] or explicit [51] methods, or measuring the geometric distance between actions generated by the learning and behavior policies [52].In addition to explicit constraint methods, implicit constraints can also be implemented by learning a behavior policy sampler using techniques like Conditional Variational Autoencoders (CVAE) [42, 53, 50, 54], Autoregressive Generative Model [55], Generative Adversarial Networks (GAN) [49], normalized flow models [56], or diffusion models [57, 58, 11, 59, 11, 60]. ", "page_idx": 9}, {"type": "text", "text": "Pessimistic Q-value optimization. Pessimistic Q-value methods offer a direct approach to address the issue of Q-value function overestimation, particularly when policy control fails despite the learning policy closely matching the behavior policy [3]. A promising approach to pessimistic Q-value estimation involves estimating uncertainty over the action space, as OOD actions typically exhibit high uncertainty. However, accurately quantifying uncertainty poses a challenge, especially with high-capacity function approximators like neural networks [4]. Techniques such as ensemble or bootstrap methods have been employed to estimate multiple Q-values, providing a proxy for uncertainty through Q-value variance [7, 9, 14], importance ratio [61, 62] or approximate Lower Confidence Bounds (LCB) for OOD regions [8, 9]. Other methods focus on estimating the LCB of Q-values through quantile regression [63, 34], expectile regression [10, 11], or tail risk measurement such as Conditional Value at Risk (cVAR) [12]. Alternatively, some approaches seek to pessimistically estimate Q-values based on the behavior policy, aiming to underestimate Q-values under the learning policy distribution while maximizing Q-values under the behavior policy distribution [3, 13, 64, 65]. Another category of Q-value constraint methods involves learning Q-values only within the in-sample [16, 17, 18, 19], capturing only in-sample patterns and avoid OOD risk. Furthermore, Q-value functions can be replaced by safe planning methods used in model-based RL, such as planning with diffusion models [66] or trajectory-level prediction using Transformers [43]. However, ensemble estimation of uncertainty may tend to underestimate true uncertainty, while quantile estimation methods are sensitive to Q-distribution recovery. In-sample methods may also be limited by the performance of the behavior policy. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce QDQ, a novel framework rendering pessimistic Q-value in OOD areas by uncertainty estimation. Our approach leverages the consistency model to robustly estimate the uncertainty of Q-values. By employing this uncertainty information, QDQ can apply a judicious penalty to Q-values, mitigating the overly conservative nature encountered in previous pessimistic Q-value methods. Additionally, to enhance optimistic Q-learning within in-distribution areas, we introduce an uncertainty-aware learning objective for Q optimization. Both theoretical analyses and experimental evaluations demonstrate the effectiveness of QDQ. Several avenues for future research exist, including embedding QDQ into goal-conditioned tasks, enhancing exploration in online RL by efficient uncertainty estimation. We hope our work will inspire further advancements in offline reinforcement learning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank AC and reviewers for their valuable comments on the manuscript. Bingyi Jing\u2019s research is partly supported by NSFC (No. 12371290). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. A brief survey of deep reinforcement learning. arXiv preprint arXiv:1708.05866, 2017. [2] John Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function approximation. Advances in neural information processing systems, 9, 1996.   \n[3] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. volume 2020-December, 2020. [4] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.   \n[5] Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. volume 32, 2019. [6] Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. volume 4, 2018.   \n[7] Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov, and Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. arXiv preprint arXiv:2105.08140, 2021.   \n[8] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline reinforcement learning. In International Conference on Machine Learning, pages 104\u2013114. PMLR, 2020.   \n[9] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. Advances in neural information processing systems, 34:7436\u20137447, 2021.   \n[10] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169, 2021.   \n[11] Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, and Sergey Levine. Idql: Implicit $\\mathbf{q}$ -learning as an actor-critic method with diffusion policies. arXiv preprint arXiv:2304.10573, 2023.   \n[12] N\u00faria Armengol Urp\u00ed, Sebastian Curi, and Andreas Krause. Risk-averse offilne reinforcement learning. arXiv preprint arXiv:2102.05371, 2021.   \n[13] Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu. Mildly conservative q-learning for offline reinforcement learning. arXiv preprint arXiv:2206.04745, 2022.   \n[14] Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhihong Deng, Animesh Garg, Peng Liu, and Zhaoran Wang. Pessimistic bootstrapping for uncertainty-driven offilne reinforcement learning. arXiv preprint arXiv:2202.11566, 2022.   \n[15] HJ Terry Suh, Glen Chou, Hongkai Dai, Lujie Yang, Abhishek Gupta, and Russ Tedrake. Fighting uncertainty with gradients: Offilne reinforcement learning via diffusion score matching. In Conference on Robot Learning, pages 2878\u20132904. PMLR, 2023.   \n[16] Divyansh Garg, Joey Hejna, Matthieu Geist, and Stefano Ermon. Extreme q-learning: Maxent rl without entropy. arXiv preprint arXiv:2301.02328, 2023.   \n[17] Chenjun Xiao, Han Wang, Yangchen Pan, Adam White, and Martha White. The in-sample softmax for offline reinforcement learning. arXiv preprint arXiv:2302.14372, 2023.   \n[18] Hongchang Zhang, Yixiu Mao, Boyuan Wang, Shuncheng He, Yi Xu, and Xiangyang Ji. Insample actor critic for offilne reinforcement learning. In The Eleventh International Conference on Learning Representations, 2022.   \n[19] Haoran Xu, Li Jiang, Jianxiong Li, Zhuoran Yang, Zhaoran Wang, Victor Wai Kin Chan, and Xianyuan Zhan. Offline rl with no ood actions: In-sample learning via implicit value regularization. arXiv preprint arXiv:2303.15810, 2023.   \n[20] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023.   \n[21] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.   \n[22] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.   \n[23] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International conference on machine learning, pages 1995\u20132003. PMLR, 2016.   \n[24] R.S. Sutton and A.G. Barto. Reinforcement learning: An introduction. IEEE Transactions on Neural Networks, 9, 1998. ISSN 1045-9227. doi: 10.1109/tnn.1998.712192.   \n[25] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In International conference on machine learning, pages 387\u2013395. PMLR, 2014.   \n[26] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.   \n[27] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[28] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. volume 5, 2018.   \n[29] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \n[30] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[31] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:12438\u201312448, 2020.   \n[32] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \n[33] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[34] Arsenii Kuznetsov, Pavel Shvechikov, Alexander Grishin, and Dmitry Vetrov. Controlling overestimation bias with truncated mixture of continuous distributional quantile critics. In International Conference on Machine Learning, pages 5556\u20135566. PMLR, 2020.   \n[35] Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva Tb, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic policy gradients. arXiv preprint arXiv:1804.08617, 2018.   \n[36] Brendan O\u2019Donoghue, Ian Osband, Remi Munos, and Volodymyr Mnih. The uncertainty bellman equation and exploration. In International conference on machine learning, pages 3836\u20133845, 2018.   \n[37] Jus Kocijan, Roderick Murray-Smith, Carl E Rasmussen, and Agathe Girard. Gaussian process model based predictive control. In Proceedings of the 2004 American control conference, volume 3, pages 2214\u20132219. IEEE, 2004.   \n[38] Craig Knuth, Glen Chou, Necmiye Ozay, and Dmitry Berenson. Planning with learned dynamics: Probabilistic guarantees on safety and reachability via lipschitz constants. IEEE Robotics and Automation Letters, 6(3):5129\u20135136, 2021.   \n[39] Marc G Bellemare, Will Dabney, and R\u00e9mi Munos. A distributional perspective on reinforcement learning. In International conference on machine learning, pages 449\u2013458. PMLR, 2017.   \n[40] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.   \n[41] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.   \n[42] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. volume 2019-June, 2019.   \n[43] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084\u201315097, 2021.   \n[44] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.   \n[45] David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without off-policy evaluation. Advances in neural information processing systems, 34:4933\u20134946, 2021.   \n[46] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offilne reinforcement learning. Advances in neural information processing systems, 34:20132\u201320145, 2021.   \n[47] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. arXiv preprint arXiv:1911.11361, 2019.   \n[48] Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019.   \n[49] Shentao Yang, Zhendong Wang, Huangjie Zheng, Yihao Feng, and Mingyuan Zhou. A regularized implicit policy for offline reinforcement learning. arXiv preprint arXiv:2202.09673, 2022.   \n[50] Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, and Mingsheng Long. Supported policy optimization for offline reinforcement learning. arXiv preprint arXiv:2202.06239, 2022.   \n[51] Jing Zhang, Chi Zhang, Wenjia Wang, and Bingyi Jing. Constrained policy optimization with explicit behavior density for offline reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[52] Jianxiong Li, Xianyuan Zhan, Haoran Xu, Xiangyu Zhu, Jingjing Liu, and Ya-Qin Zhang. Distance-sensitive offline reinforcement learning. arXiv preprint arXiv:2205.11027, 2022.   \n[53] Wenxuan Zhou, Sujay Bajracharya, and David Held. Plas: Latent action space for offline reinforcement learning. In Conference on Robot Learning, pages 1719\u20131735. PMLR, 2021.   \n[54] Xi Chen, Ali Ghadirzadeh, Tianhe Yu, Yuan Gao, Jianhao Wang, Wenzhe Li, Bin Liang, Chelsea Finn, and Chongjie Zhang. Latent-variable advantage-weighted policy optimization for offilne rl. arXiv preprint arXiv:2203.08949, 2022.   \n[55] Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-max q-learning operator for simple yet effective offilne and online rl. In International Conference on Machine Learning, pages 3682\u20133691. PMLR, 2021.   \n[56] Avi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, and Sergey Levine. Parrot: Data-driven behavioral priors for reinforcement learning. arXiv preprint arXiv:2011.10024, 2020.   \n[57] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. arXiv preprint arXiv:2208.06193, 2022.   \n[58] Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, and Jun Zhu. Offline reinforcement learning via high-fidelity generative behavior modeling. arXiv preprint arXiv:2209.14548, 2022.   \n[59] Wonjoon Goo and Scott Niekum. Know your boundaries: The necessity of explicit behavioral cloning in offline rl. arXiv preprint arXiv:2206.00695, 2022.   \n[60] Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng Yan. Efficient diffusion policies for offline reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[61] Xiaoying Zhang, Junpu Chen, Hongning Wang, Hong Xie, Yang Liu, John Lui, and Hang Li. Uncertainty-aware instance reweighting for off-policy learning. Advances in Neural Information Processing Systems, 36:73691\u201373718, 2023.   \n[62] Paria Rashidinejad, Hanlin Zhu, Kunhe Yang, Stuart Russell, and Jiantao Jiao. Optimal conservative offline rl with general function approximation via augmented lagrangian. arXiv preprint arXiv:2211.00716, 2022.   \n[63] Cristian Bodnar, Adrian Li, Karol Hausman, Peter Pastor, and Mrinal Kalakrishnan. Quantile qt-opt for risk-aware vision-based robotic grasping. arXiv preprint arXiv:1910.02787, 2019.   \n[64] Liting Chen, Jie Yan, Zhengdao Shao, Lu Wang, Qingwei Lin, Saravanakumar Rajmohan, Thomas Moscibroda, and Dongmei Zhang. Conservative state value estimation for offline reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[65] Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, and Xiangyang Ji. Supported value regularization for offline reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[66] Michael Janner, Yilun Du, Joshua B Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. arXiv preprint arXiv:2205.09991, 2022.   \n[67] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.   \n[68] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35: 26565\u201326577, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Further discussion about uncertainty estimation of Q-value by Q-distribution. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 The Q-value dataset enhancement with sidling window. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Section 3.1, we analyze the challenges associated with deriving the Q-value dataset. We propose the $k$ -step sliding window method to improve sample efficiency at the trajectory level. The implementation details of the $k$ -step sliding window within an entire trajectory are illustrated in Figure A.1. ", "page_idx": 14}, {"type": "text", "text": "This sliding window framework not only facilitates the expansion of Q-value data but also prevents the state-action pairs from becoming overly dense, thereby mitigating the risk of Q-value homogenization. In continuous state-action spaces, $\\mathrm{^Q}$ -values tend to be homogeneity when state and action are close in a trajectory, which may hinder subsequent learning of the Q-value distribution. ", "page_idx": 14}, {"type": "image", "img_path": "NIcIdhyfQX/tmp/cee0e86b151bea7166c187ad94cdc26e0c4e6accc9e146029c6c79563cc22752.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure A.1: This exemplifies how the sliding window mechanism operates to augment Q data. Let\u2019s consider a sliding window with a width of 50 and a step size of $k=10$ . For a specific trajectory, at step 1, we commence with $(s_{1},a_{1})$ and compute the truncated $Q\\cdot$ -value utilizing trajectories within the window. At step 2, the sliding window progresses $k$ steps forward, allowing us to compute the truncated Q-value for $\\left(s_{1+k},a_{1+k}\\right)$ . ", "page_idx": 14}, {"type": "text", "text": "A.2 Drawbacks of the diffusion model for estimating the uncertainty ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Suppose we use the score matching method proposed in [30] to learn a conditional score network $s_{\\theta}(x,\\sigma|s,a)$ to approximate the score function of the Q-value distribution $p_{Q}(x)$ . Then we use the annealed Langevin dynamics as in [30] to sample from the learned $\\mathrm{{Q}}.$ -value distribution. For $x_{0}\\sim\\pi(x)$ from some arbitrary prior distribution $\\pi(x)$ , the denoised sample is: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{i+1}=x_{i}+\\epsilon\\cdot s_{\\theta}(x_{i},\\sigma|s,a)+\\sqrt{2\\epsilon}z_{i},i=0,1,2,\\cdots,T.\\quad z_{i}\\sim\\mathcal{N}(0,1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The distribution of $x_{i+1}$ equals $p_{Q}(x)$ when $\\epsilon\\rightarrow0$ and $T\\rightarrow\\infty$ . ", "page_idx": 14}, {"type": "text", "text": "Our primary approach to quantify the uncertainty of the action with respect to the $Q\\cdot$ -value is to evaluate the spread of the sampled Q-values for each $(s,a)$ . We then use the gradient of the $\\mathrm{{Q}}.$ -value samples with respect to the action to assess their sensitivity to changes in action. By iteratively deriving the gradient over the sampling chain of length $T$ , as shown in Eq.A.1, we approximate the following outcome: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\partial x_{T}}{\\partial a}=\\sum_{i=1}^{T}c_{i}\\epsilon^{i}\\frac{\\partial^{i}s_{\\theta}(\\cdot|s,a)}{\\partial a^{i}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "From Eq.A.2, the impact of actions on $\\mathrm{^Q}$ -value samples learned from the diffusion model shows considerable instability, especially during the iterative gradient-solving process for the score network $s_{\\theta}(x,\\sigma|s,a)$ . This instability often leads to gradient vanishing or exploding. While the diffusion model effectively recovers the Q-value distribution with high precision, the multi-step sampling process introduces significant fluctuations and noise, making it difficult to accurately assess the uncertainty of Q-values for different actions. ", "page_idx": 15}, {"type": "text", "text": "Additionally, as different prior information may yield the same target sample, and this stochastic correlation also introduces an uncontrollable impact on uncertainty of the Q-value. However, the effect of prior sample variance on the uncertainty of the sampled Q-value can not be quantified with the diffusion model. So we can not guarantee if the absolute influence on the Q-value uncertainty is from the action, then the performance of the uncertainty can not be guaranteed. ", "page_idx": 15}, {"type": "text", "text": "B Convergence of the truncated Q-value distribution. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first give a formal introduction of Theorem 4.1 as Theorem B.1. ", "page_idx": 15}, {"type": "text", "text": "Theorem B.1. Suppose the true distribution of $Q$ -value w.r.t the behavior policy $\\pi_{\\beta}$ is defined as $F_{Q}\\pi_{\\beta}\\left(x\\right)$ . By Eq.5, we derive the truncated $Q$ -value $Q_{\\mathcal{T}}^{\\pi_{\\beta}}$ , denote the distribution of the truncated $Q$ -value is $F_{Q_{\\mathcal{T}}^{\\pi_{\\beta}}}(x)$ . Assume the true $Q$ -value is finite over the state and action space,then $Q_{\\mathcal{T}}^{\\pi_{\\beta}}$ converge in-distribution to the true true $Q$ -value $Q^{\\pi_{\\beta}}$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\nF_{Q_{\\mathcal{T}}^{\\pi_{\\beta}}}(x)\\rightarrow F_{Q^{\\pi_{\\beta}}}(x),\\mathcal{T}\\rightarrow+\\infty.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We will show the distribution of the truncated $\\mathrm{^Q}$ -value $Q_{\\mathcal{T}}^{\\pi_{\\beta}}$ has the same property with the true distribution of the Q-value $Q_{\\mathcal{T}}^{\\pi_{\\beta}}$ and give a brief proof of Theorem 4.1. ", "page_idx": 15}, {"type": "text", "text": "Suppose the state-action space determined by the offilne RL dataset $\\mathcal{D}$ is $\\Omega_{\\mathcal{D}}=S_{\\mathcal{D}}\\times\\mathcal{A_{D}}$ . Note that $Q^{\\pi_{\\beta}}$ can be seen as an r.v. defined as: $\\begin{array}{r}{(\\Omega_{\\mathcal{D}},\\mathcal{F},\\mathbb{P}\\cdot\\pi_{\\beta})\\xrightarrow{Q^{\\pi_{\\beta}}}(\\mathbb{R},\\mathcal{B}(\\mathbb{R}),(\\mathbb{P}\\cdot\\pi_{\\beta})\\circ(Q^{\\pi_{\\beta}})^{-1})}\\end{array}$ , where $\\mathcal{F}$ is the $\\sigma$ -field on $\\Omega_{\\mathcal{D}},\\mathbb{P}\\cdot\\pi_{\\beta}$ is the probability measure on $\\Omega_{\\mathcal{D}}$ and $\\mathbb{P}$ is the transition probability measure over state, $B(\\mathbb{R})$ is the Borel $\\sigma$ -field on $\\mathbb{R}$ , $(\\mathbb{P}\\cdot\\pi_{\\beta})\\circ(Q^{\\pi_{\\beta}})^{-1}=(\\mathbb{P}\\cdot\\pi_{\\beta})((Q^{\\pi_{\\beta}})^{-1})$ is the push forward probability measure on $\\mathbb{R}$ . Same as $Q^{\\pi_{\\beta}}$ , $Q_{\\mathcal{T}}^{\\pi_{\\beta}}$ can also be seen as a r.v.. ", "page_idx": 15}, {"type": "text", "text": "Then we show that $Q_{\\mathcal{T}}^{\\pi_{\\beta}}$ converge to $Q^{\\pi_{\\beta}}$ almost surely: $Q_{\\mathcal{T}}^{\\pi_{\\beta}}\\ {\\xrightarrow{a.s.}}\\ Q^{\\pi_{\\beta}}$ , when sending $\\tau$ to infinity. Define the trajectory level dataset $\\mathcal{D}_{\\tau}=\\{\\tau_{k}|\\tau_{k}=\\left(s_{k_{0}},a_{k_{0}},r_{k_{0}},s_{k_{1}},a_{k_{1}},r_{k_{1}},\\cdot\\cdot\\cdot\\right)\\}$ . Then for any trajectory $\\tau_{k}\\,\\in\\,\\mathcal{D}_{\\tau}$ , the true $\\mathrm{^Q}$ -value w.r.t this trajectory can be rewrite without the expectation a $:\\!{\\cal Q}^{\\pi}(s_{k_{0}},a_{k_{0}})=\\sum_{j=1}^{\\infty}\\gamma^{j-1}r(s_{k_{j}},a_{k_{j}}).$ ", "page_idx": 15}, {"type": "text", "text": "Truncating Q-value by termination situation. If the terminal occurs at step $k_{t}$ of the trajectory $\\tau_{k}$ , then we have $r(s_{k_{j}},a_{k_{j}})=0$ , for $j>k_{t}$ . Then the truncated Q-value is identical the true Q-value: $Q_{k_{t}}^{\\pi_{\\beta}}\\equiv Q^{\\pi}\\bigl(s_{k_{0}},\\stackrel{\\cdot}{a_{k_{0}}}\\bigr)$ . So the distribution of these two distribution is same for the situation when the truncation is happened due to terminal of the task. ", "page_idx": 15}, {"type": "text", "text": "Truncating Q-value by sliding window situation. If the Q-value is truncated by a sliding window as shown in Figure A.1, then for a specific $k$ - step sliding widow of width $\\tau$ with starting point $(s_{i},a_{i})$ over a trajectory $\\tau$ , we have $Q_{\\mathcal{T}}^{\\pi_{\\beta}}(s_{i},a_{i})=\\sum_{m=i}^{\\mathcal{T}}\\gamma^{m-1}r(s_{m},a_{m}).$ ", "page_idx": 15}, {"type": "text", "text": "Define the state action set $B_{n}(\\xi):=\\underset{T=n}{\\overset{\\infty}{\\cup}}A_{n}(\\xi)$ , where $A_{\\mathcal{T}}(\\xi)=\\{(s,a):|Q_{\\mathcal{T}}^{\\pi_{\\beta}}(s,a)\\!-\\!Q^{\\pi_{\\beta}}(s,a)|>$ $\\xi\\}$ . By the definition of $\\mathrm{{Q}}.$ -value, we have $B_{m}(\\xi)=A_{m}(\\xi)$ , as $A_{\\mathcal{T}}(\\xi)\\supset A_{\\mathcal{T}+1}(\\xi)\\supset A_{\\mathcal{T}+2}(\\xi)\\supset$ $\\cdot\\,\\cdot\\,\\cdot$ is decreasing. So $B_{n}(\\xi)$ is also decreasing. Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\mathbb{P}(B_{n}(\\xi))=\\mathbb{P}(\\operatorname*{lim}_{n\\to\\infty}B_{n}(\\xi))=\\mathbb{P}(\\operatorname*{lim}_{n\\to\\infty}A_{n}(\\xi))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By definition, $A_{n}(\\xi)=\\{(s,a):|Q_{n}^{\\pi_{\\beta}}(s,a)-Q^{\\pi_{\\beta}}(s,a)|>\\xi\\}$ , and assume the reward function $r(\\cdot,\\cdot)$ is bounded as $r(\\cdot,\\cdot)<c$ for some constant $c$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\vert Q_{n}^{\\pi_{\\beta}}(s,a)-Q^{\\pi_{\\beta}}(s,a)\\vert=\\sum_{k=n+1}^{\\infty}\\gamma^{n}r(s_{k},a_{k})\\leq c\\sum_{k=n+1}^{\\infty}\\gamma^{n}={\\frac{\\gamma^{n}}{1-\\gamma}}\\cdot c\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So $A_{n}(\\xi)\\to\\emptyset$ when sending $n$ to infinity, as the discount factor $\\gamma<1$ by definition. ", "page_idx": 16}, {"type": "text", "text": "Then by Eq.B.2, we have $\\begin{array}{r}{\\operatorname*{lim}_{m\\rightarrow\\infty}\\mathbb{P}(B_{n}(\\xi))=0,\\forall\\xi>0}\\end{array}$ . In probability theory, this is equivalent to $Q_{\\mathcal{T}}^{\\pi_{\\beta}}\\ {\\xrightarrow{a.s.}}\\ Q^{\\pi_{\\beta}}$ . ", "page_idx": 16}, {"type": "text", "text": "Furthermore, if $Q_{\\mathcal{T}}^{\\pi_{\\beta}}$ converge to $Q^{\\pi_{\\beta}}$ almost surely, then $Q_{\\mathcal{T}}^{\\pi_{\\beta}}$ also converge to $Q^{\\pi_{\\beta}}$ in probability, and in-distribution. Hence, we finish the proof. ", "page_idx": 16}, {"type": "text", "text": "Remark B.1. Theorem 4.1 suggests that for arbitrary small $\\epsilon$ , there exists a sufficiently large $\\tau$ , such that $|F_{Q_{\\tau}^{\\pi_{\\beta}}}(x)-F_{Q^{\\pi_{\\beta}}}(x)|<\\epsilon$ . Given that the impact of rewards diminishes exponentially after as the increasing of trajectory length, it is unnecessary to set $\\tau$ to an excessively large value. It\u2019s important to remember that the goal of the Q dataset is to learn the Q-distribution and assess the uncertainty of different actions. Therefore, the absolute magnitude of Q is not crucial. Additionally, using too many future steps may introduce significant uncertainty into the Q-value, as predictions for the distant future can be inaccurate. ", "page_idx": 16}, {"type": "text", "text": "Remark B.2. During the proof, the specific starting point of the sliding window holds no significance; rather, our focus lies solely on the length of the window. This is primarily due to the Markovian nature of trajectories in RL, where the current state and action are unaffected by previous ones and adhere to a memoryless property. Consequently, the starting point of the sliding window exerts minimal influence on the computation of Q. ", "page_idx": 16}, {"type": "text", "text": "C Robustness of consistency model for uncertainty measure. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The formal introduction of Theorem 4.2 is shown in Theorem C.1 ", "page_idx": 16}, {"type": "text", "text": "Theorem C.1. Follow the assumptions in [20], we assume $f_{\\theta}(x,T|(s,a))$ is $L$ -Lipschitz. ", "page_idx": 16}, {"type": "text", "text": "By using the partial gradient to analyze the influence of prior samples $\\hat{x}_{T}$ , time step $T$ and action a to the variance of the denoised sample var $\\left(X_{\\epsilon}\\right)$ , with high probability, we have: ", "page_idx": 16}, {"type": "equation", "text": "$O(L^{2}T n^{-1}{\\sqrt{T\\log(n)}})\\mathbf{1}.$ ", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As discussed in Section 3.1 and A.2, while the diffusion model has shown great success in learning distributions and generating samples, it is less suitable for scenarios where the influence of certain parameters on sample uncertainty must be guaranteed. When estimating uncertainty, we sample multiple Q-values for each $(s,a)$ pair, and the standard deviation of these sampled Q-values measures the uncertainty. Therefore, it is crucial that the sample spread is sensitive to changes in action to accurately judge OOD actions. ", "page_idx": 16}, {"type": "text", "text": "However, the multi-step forward denoising process of the diffusion model undermines the influence of actions on the sampled Q-values, compromising the robustness of uncertainty estimation. Additionally, the lack of a one-to-one correspondence between prior information and target samples prevents the cancellation of prior effects on the Q-value distribution through repeated sampling. ", "page_idx": 16}, {"type": "text", "text": "In contrast, the consistency model addresses these challenges. It not only overcomes the aforementioned issues, but its one-step sampling significantly enhances efficiency. In the following theoretical analysis, we will demonstrate the robustness of the consistency model in estimating uncertainty. ", "page_idx": 17}, {"type": "text", "text": "As described in [20] and Section 2.2, a consistency model $f_{\\theta}(x,t)$ is trained to mapping the prior noise on any trajectory of PF ODE to the trajectory\u2019s origin $x_{\\epsilon}$ by: $f_{\\theta}(x,t)=x_{\\epsilon}$ , given $x$ and $x_{\\epsilon}$ belong to the same PF ODE trajectory. $f_{\\theta}(x,t)$ is defined as in Eq.4. ", "page_idx": 17}, {"type": "text", "text": "Suppose we trained a conditional consistency model $f_{\\theta}(x,t|s,a)$ with the truncated $\\mathrm{^Q}$ -value dataset $D_{Q}=\\{Q_{T}^{\\pi_{\\beta}}(s,a)\\}$ , following the one-step sampling of consistency model, we first initial $n$ noise $\\hat{x}_{T_{i}}\\,\\sim\\mathcal{N}(0,T^{2}),i\\,=\\,1,2,...,n$ , then do one-step forward denosing and derive $n$ sample $\\hat{x}_{\\epsilon_{i}}\\;=\\;$ $f_{\\theta}(\\hat{x}_{T_{i}},T|s,a)$ , where $T$ is a fixed time step. The variance based on the $\\mathrm{\\DeltaQ}$ sample is: ", "page_idx": 17}, {"type": "equation", "text": "$$\nV(X_{\\epsilon})=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left[f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))-\\frac{1}{n}\\sum_{i=1}^{n}f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))\\right]^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, we derive the gradient of $V(X_{\\epsilon})$ w.r.t $\\hat{x}_{T_{i}}$ , $T,\\,a,$ , and check how change in these variable influence the variance. As state $s$ is always in-distribution during offline RL training process and has little influence on the uncertainty of the sampled Q-value, we skip the analysis. ", "page_idx": 17}, {"type": "text", "text": "Following [20], we assume that $f_{\\theta}(x,t|s,a)$ is $L$ -Lipschitz bounded, i.e., for any $x$ and $y$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|f_{\\theta}(x,t|(s,a))-f_{\\theta}(y,t|(s,a))\\|_{2}\\le L\\,\\|x-y\\|_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Gradient of the prior $\\hat{x}_{T_{i}}$ . ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. Let $e_{i}$ be the square difference of the $i$ -th prior $\\hat{x}_{T_{i}}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\ne_{i}(\\hat{x}_{T_{i}}):=\\left[f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))-\\frac{1}{n}\\sum_{i=1}^{n}f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))\\right]^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\partial V(X_{\\epsilon})}{\\partial\\hat{x}_{T_{i}}}=\\frac{1}{n-1}\\sum_{j=1}^{n}\\frac{\\partial e_{j}(\\hat{x}_{T_{j}})}{\\partial\\hat{x}_{T_{i}}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If $j=i$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial e_{j}(\\hat{x}_{T_{i}})}{\\partial\\hat{x}_{T_{i}}}=2\\left[f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))-\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))\\right]\\cdot\\left[\\frac{\\partial f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))}{\\partial\\hat{x}_{T_{i}}}-\\frac{1}{n}\\frac{\\partial f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))}{\\partial\\hat{x}_{T_{i}}}\\right]}\\\\ &{\\quad\\quad\\quad=2\\left[f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))-\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))\\right]\\cdot\\frac{n-1}{n}\\frac{\\partial f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))}{\\partial\\hat{x}_{T_{i}}}.\\quad\\quad(\\mathsf{C}.4)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If $j\\neq i$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial e_{j}(\\hat{x}_{T_{j}})}{\\partial\\hat{x}_{T_{i}}}=2\\left[f_{\\theta}(\\hat{x}_{T_{j}},T|(s,a))-\\frac{1}{n}\\sum_{i=1}^{n}f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))\\right]\\cdot\\left[0-\\frac{1}{n}\\frac{\\partial f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))}{\\partial\\hat{x}_{T_{i}}}\\right]}\\\\ {\\displaystyle\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\\\ {\\displaystyle+2\\left[f_{\\theta}(\\hat{x}_{T_{j}},T|(s,a))-\\frac{1}{n}\\sum_{i=1}^{n}f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))\\right]\\cdot-\\frac{1}{n}\\frac{\\partial f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))}{\\partial\\hat{x}_{T_{i}}}.\\quad\\mathrm{~(~a~n~d~)~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, plugging Eq.C.4 and Eq.C.5 into \u2202V (X\u03f5)yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial V(X_{\\epsilon})}{\\partial\\hat{x}_{T_{i}}}=\\frac{1}{n-1}\\sum_{j=1}^{n}\\frac{\\partial e_{j}(\\hat{x}_{T_{j}})}{\\partial\\hat{x}_{T_{i}}}}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\frac{2}{n(n-1)}\\frac{\\partial f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))}{\\partial\\hat{x}_{T_{i}}}\\left[(n-1)f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))-\\sum_{j\\neq i}f_{\\theta}(\\hat{x}_{T_{j}},T|(s,a))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As $f_{\\theta}(x,t|s,a)$ is $L$ -Lipschitz bounded, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\frac{\\partial V(X_{\\epsilon})}{\\partial\\hat{x}_{T_{i}}}|\\!\\le\\!\\!\\frac{2}{n(n-1)}|\\frac{\\partial f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))}{\\partial\\hat{x}_{T_{i}}}|\\sum_{j\\ne i}|f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))-f_{\\theta}(\\hat{x}_{T_{j}},T|(s,a))|}\\\\ &{\\quad\\quad\\quad\\le\\!\\frac{2}{n(n-1)}\\cdot L\\cdot L\\displaystyle\\sum_{j\\ne i}|\\hat{x}_{T_{i}}-\\hat{x}_{T_{j}}|\\le\\frac{2}{n}\\cdot L^{2}\\cdot c\\sqrt{\\log n}\\cdot T,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $|\\hat{x}_{T_{i}}-\\hat{x}_{T_{j}}|,\\forall j\\neq i$ can be bounded by $c T{\\sqrt{\\log n}}$ due to $\\hat{x}_{T_{i}}\\sim\\mathcal{N}(0,T^{2})$ with probability at least $1-n^{-1}$ . Denote the constant with $c_{p}$ and apply the previous process to all the prior samples completes the proof. ", "page_idx": 18}, {"type": "text", "text": "Gradient of the time step $T$ . ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. ", "page_idx": 18}, {"type": "text", "text": "Note that ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{\\partial V(X_{\\epsilon})}{\\partial T}}={\\frac{1}{n-1}}\\sum_{j=1}^{n}{\\frac{\\partial e_{j}(T)}{\\partial T}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Taking partial gradient of $e_{i}(T)$ w.r.t $T$ for any $j\\in\\{1,2,\\cdots\\,,n\\}$ , we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\quad|\\widecheck{\\mathrm{\\quad\\quad\\partial~\\alpha~}}_{\\partial T}^{-}|}\\\\ &{\\quad\\geq2\\left|\\bigg[\\int_{\\theta}(\\hat{x}_{T_{j}},T|(s,a))-\\frac{1}{n}\\sum_{i=1}^{n}f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))\\bigg]\\cdot\\bigg[\\frac{\\partial f_{\\theta}(\\hat{x}_{T_{j}},T|(s,a))}{\\partial T}-\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\partial f_{\\theta}(\\hat{x}_{T_{i}},T|(s,a))}{\\partial T}\\bigg]\\right|}\\\\ &{\\quad\\leq4\\frac{L^{2}}{n}\\sum_{i=1}^{n}|\\hat{x}_{T_{j}}-\\hat{x}_{T_{i}}|\\leq4L^{2}\\sqrt{\\log n},}&{\\mathrm{(C.9)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with probability at least $1-n^{-1}$ , since $|\\hat{x}_{T_{i}}-\\hat{x}_{T_{j}}|,\\forall j\\neq i$ can be bounded by $c T{\\sqrt{\\log n}}$ due to $\\hat{x}_{T_{i}}\\sim\\mathcal{N}(0,T^{2})$ with probability at least $1-n^{-1}$ . ", "page_idx": 18}, {"type": "text", "text": "Plugging Eq.C.9 into Eq.C.8 yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\vert\\frac{\\partial V(X_{\\epsilon})}{\\partial T}\\right\\vert\\leq4c L^{2}T\\sqrt{\\log n}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Gradient of the action $a$ . ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For the gradient of \u2202V \u2202(aX\u03f5), we just need to take partial gradient of $V(X_{\\epsilon})$ for each dimmension of the action $a=\\{a_{1},a_{2},\\cdot\\cdot\\cdot,a_{m}\\}$ b y \u2202V\u2202 (aX\u03f5). The result is same as those we got in Eq.C.10. ", "page_idx": 18}, {"type": "text", "text": "Then take the vector form, we have $\\begin{array}{r}{|\\frac{\\partial V(X_{\\epsilon})}{\\partial a}|=O(L^{2}T\\sqrt{\\log n})\\cdot\\mathbf{1}.}\\end{array}$ , which finish the proof. ", "page_idx": 18}, {"type": "text", "text": "Remark C.1. Theorem 4.2 elucidates the diminishing impact of random prior on the variance of the denoised Q-value as the sample size increases. Leveraging the consistency of sampling, we mitigate concerns regarding the influence of a priori samples on the uncertainty of final target samples. Given the fixed sampling step size $T$ , we also address concerns about its effect on the uncertainty of $\\mathrm{\\DeltaQ}$ samples. However, for a thorough analysis, we still include the gradient analysis of $V(X_{\\epsilon})$ against $T$ Remark C.2. The influence of actions on sample variance depends on factors like the Lipschitz factor and sample size. As the sample size increases, the impact of actions on $\\mathrm{^Q}$ sample variance does not diminish; instead, it becomes more sensitive. Larger Q sample variance is more likely to occur when OOD actions are present. Consequently, the consistency model proves to be a reliable approach for Q-value sampling and uncertainty estimation. ", "page_idx": 18}, {"type": "text", "text": "Remark C.3. Although experiments in [20] show that the performance of the consistency model is less competitive compared to the diffusion model or adversarial generators like GANs, these findings have minimal relevance to our method. Our primary focus is not on the absolute accuracy of the sampled Q-values but on the sensitivity of Q sample dispersion to OOD actions and its ability to effectively capture uncertainty in such cases. Additionally, the high sampling efficiency achieved through one-step sampling compensates for the minor performance discrepancies of the consistency model. ", "page_idx": 18}, {"type": "text", "text": "D Convergence of the uncertainty-aware learning objective for recovering the Q-value function. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We first give a formal introduction of Theorem 4.3 below. ", "page_idx": 19}, {"type": "text", "text": "Theorem D.1. Updating $Q$ -value $Q_{\\theta}(s,a)$ via the uncertainty-aware objective in Eq.7 is equivalent to minimizing the $L_{2}$ -norm of Bellman residuals: $\\mathbb{E}_{s\\sim P_{\\mathcal{D}}(s),a\\sim\\pi(a|s)}[Q(s,a)-\\mathcal{F}Q(s,a)]^{2}$ , where the Bellman operator ${\\mathcal{F}}Q(s,a)$ is defined as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{F}Q(s,a):=r(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{\\mathcal{D}}(s^{\\prime})}\\{\\operatorname*{max}_{a^{\\prime}}[\\alpha Q(s^{\\prime},a^{\\prime})+(1-\\alpha)Q_{L}(s^{\\prime},a^{\\prime})]\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In addition, assume $\\begin{array}{r}{\\frac{1}{\\mathcal{H}_{Q}\\left(a^{\\prime}|s^{\\prime}\\right)}1_{\\left(a^{\\prime}\\in\\mathcal{U}\\left(Q\\right)\\right)}\\,<\\,\\beta}\\end{array}$ . Then the Bellman operator $\\mathcal{F}Q$ is $c\\gamma$ -contraction operator in the $L_{\\infty}$ norm, where $c<1$ . The $Q$ -value function $Q_{\\theta}(s,a)$ can converge to a fixed point by value iteration method. ", "page_idx": 19}, {"type": "text", "text": "We will show the the uncertainty-aware learning objective is equivalent to minimized the Bellman equation defined by a specific Bellman operator ${\\mathcal{F}}Q$ firstly. ", "page_idx": 19}, {"type": "text", "text": "Then the we proof the Bellman operator $\\mathcal{F}Q$ is $c\\gamma$ -contraction operator in the $L_{\\infty}$ norm, where $c<1$ . The Q-value function $Q_{\\theta}(s,a)$ can converge to a fixed point by the value iteration method. ", "page_idx": 19}, {"type": "text", "text": "D.1 Derivation of the Bellman operator $\\mathcal{F}Q$ . ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Recall the Q-value is optimized by the uncertainty-aware learning objective by Eq.7: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}_{u w}(Q_{\\theta})=\\operatorname*{min}_{\\theta}\\{\\alpha\\mathcal{L}(Q_{\\theta})_{H}+(1-\\alpha)\\mathcal{L}(Q_{\\theta})_{L}\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(Q)_{H}:=\\!\\!\\mathbb{E}_{s\\sim P_{D}(s),a\\sim\\pi(a|s)}[Q_{\\theta}(s,a)-(B Q)(s,a)]^{2}}\\\\ &{\\quad\\quad\\quad\\quad=\\!\\!\\mathbb{E}_{s\\sim P_{D}(s),a^{\\prime}\\sim\\pi(a|s)}[Q_{\\theta}(s,a)-(r(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{D}(s^{\\prime})}[\\underset{a^{\\prime}}{\\operatorname*{max}}Q_{\\theta}(s^{\\prime},a^{\\prime})])]^{2},}\\\\ &{\\mathcal{L}(Q)_{L}:=\\!\\!\\mathbb{E}_{s\\sim P_{D}(s),a\\sim\\pi(a|s)}[Q_{\\theta}(s,a)-({B Q})_{L}(s,a)]^{2}}\\\\ &{\\quad\\quad\\quad\\quad=\\!\\!\\mathbb{E}_{s\\sim P_{D}(s),a^{\\prime}\\sim\\pi(a|s)}[Q_{\\theta}(s,a)-(r(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{D}(s^{\\prime})}[\\underset{a^{\\prime}}{\\operatorname*{max}}Q_{L}(s^{\\prime},a^{\\prime})])]^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The uncertainty penalized $\\mathrm{\\DeltaQ}$ target $Q_{L}(s^{\\prime},a^{\\prime})$ is defined as: ", "page_idx": 19}, {"type": "equation", "text": "$$\nQ_{L}(s^{\\prime},a^{\\prime})=\\frac{1}{\\mathcal{H}_{Q}(a^{\\prime}|s^{\\prime})}Q_{\\theta}(s^{\\prime},a^{\\prime})1_{(a^{\\prime}\\in\\mathcal{U}(Q))}+\\beta Q_{\\theta}(s^{\\prime},a^{\\prime})1_{(a^{\\prime}\\notin\\mathcal{U}(Q))}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For simplicity, we ignore the estimation parameter of $Q_{\\theta}(s,a)$ and just use $Q(s,a)$ in the following proof. ", "page_idx": 19}, {"type": "text", "text": "We can just take the uncertainty-aware loss in Eq.7 as a plain regression like loss and we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\iota C(Q)_{H}+(1-\\alpha)\\mathcal{L}(Q)_{L}=\\alpha[Q(s,a)-(B Q)(s,a)]^{2}+(1-\\alpha)[Q(s,a)-(B Q)_{L}(s,a)]^{2}}&{}\\\\ &{=\\alpha[Q(s,a)^{2}-2Q(s,a)(B Q)(s,a)+((B Q)(s,a))^{2}]}\\\\ &{+(1-\\alpha)[Q(s,a)^{2}-2Q(s,a)(B Q)_{L}(s,a)+((B Q)_{L}(s,a))^{2}]}\\\\ &{=\\alpha Q(s,a)^{2}-\\alpha2Q(s,a)(B Q)(s,a)+\\alpha((B Q)(s,a))^{2}}\\\\ &{+(1-\\alpha)Q(s,a)^{2}-(1-\\alpha)2Q(s,a)(B Q)_{L}(s,a)+(1-\\alpha)((B Q)_{L}(s,a)}\\\\ &{=Q(s,a)^{2}-2Q(s,a)[\\alpha(B Q)(s,a)+(1-\\alpha)(B Q)_{L}(s,a)]}\\\\ &{+\\alpha((B Q)(s,a))^{2}+(1-\\alpha)((B Q)_{L}(s,a))^{2}}\\\\ &{=[Q(s,a)-(\\alpha(B Q)(s,a)+(1-\\alpha)(B Q)_{L}(s,a))]^{2}+C,\\qquad\\quad(\\mathrm{D.}6)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C$ is a factor that not related to $Q(s,a)$ , since the value of $(\\boldsymbol{B}\\boldsymbol{Q})_{L}(s,a)$ and $(\\boldsymbol{B}\\boldsymbol{Q})(s,a)$ are fixed as we update $Q$ . By the definition of $(\\beta Q)(s,a)$ and $(B Q)_{L}(s,a)$ in Eq.D.3 and Eq.D.4, the uncertainty-aware learning is equivalent to minimized the following Bellman equation: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{s\\sim P_{\\mathcal{D}}(s),a\\sim\\pi(a|s)}[Q(s,a)-({\\mathcal{F}}Q)(s,a)]^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the specific Bellman operator $({\\mathcal{F}}Q)(s,a)$ is defined in Eq.D.1. Then we finish the proof. ", "page_idx": 19}, {"type": "text", "text": "D.2 Bellman operator ${\\mathcal{F}}Q$ is $c\\gamma$ -contraction operator in the $L_{\\infty}$ norm. ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Then we give a brief proof of the convergence of the Bellman operator $\\mathcal{F}Q$ by value interative optimization. ", "page_idx": 20}, {"type": "text", "text": "For any Q-value function $Q(s,a),Q^{\\prime}(s,a)$ , define $I=|\\mathcal{F}Q(s,a)-\\mathcal{F}Q^{\\prime}(s,a)|$ , we have ", "page_idx": 20}, {"type": "text", "text": "I =|r(s, a) + \u03b3Es\u2032\u223cPD(s\u2032){ma\u2032 x[\u03b1Q(s\u2032, a\u2032) + (1 \u2212\u03b1)QL(s\u2032, a\u2032)]}\u2212 +(r(s, a) + \u03b3Es\u2032\u223cPD(s\u2032){ma x[\u03b1Q\u2032(s\u2032, a\u2032) + (1 \u2212\u03b1)Q\u2032L(s\u2032, a\u2032)]})|   \n$\\begin{array}{r l}&{-\\operatorname*{limsup}_{{\\vec{A}}\\in\\mathcal{S}_{r}}\\operatorname*{limsup}_{{\\vec{A}}(u,t)}\\operatorname*{limsup}_{{\\vec{A}}(u,t)}\\Big(a_{r}^{(1)}+(1-\\alpha)\\mathrm{i}\\alpha(d_{r}u^{-})-\\operatorname*{limsup}_{{\\vec{A}}(u,t)}+(1-\\alpha)\\mathrm{i}\\alpha(d_{r}\\zeta^{(1)}u^{-})\\Big\\|\\Big)}\\\\ &{\\operatorname*{limsup}_{{\\vec{A}}(u,t)}\\operatorname*{limsup}_{{\\vec{A}}(u,t)}\\Big(a_{r}^{(1)}+(1-\\alpha)\\mathrm{i}\\alpha(d_{r}u^{-})-\\operatorname*{limsup}_{{\\vec{A}}(u,t)}+(1-\\alpha)\\mathrm{i}\\alpha(d_{r}u^{-})\\Big\\|\\Big)}\\\\ &{\\quad\\times\\operatorname*{limsup}_{{\\vec{A}}(u,t)}\\operatorname*{limsup}_{{\\vec{A}}(u,t)}\\Big(\\mathrm{i}\\alpha(d_{r})+(1-\\alpha)\\mathrm{i}\\alpha(d_{r}u^{-})-\\operatorname*{limsup}_{{\\vec{A}}(u,t)}+(1-\\alpha)\\mathrm{i}\\alpha(d_{r}u^{-})\\Big\\|\\Big)}\\\\ &{\\quad\\times\\operatorname*{limsup}_{{\\vec{A}}(u,t)}\\operatorname*{limsup}_{{\\vec{A}}(t)}\\operatorname*{limsup}_{{\\vec{A}}(u,t)}-\\operatorname*{limsup}_{{\\vec{A}}(u,t)}-\\operatorname*{limsup}_{{\\vec{A}}(u,t)}+(1-\\alpha)\\mathrm{i}\\alpha(d_{r}u^{-})\\Big\\|\\Big\\}}\\\\ &{\\quad\\times\\operatorname*{limsup}_{{\\vec{A}}(u,t)}\\operatorname*{limsup}_{{\\vec{A}}(t)}\\Big(a_{r}^{(1)}+(1-\\alpha)\\mathrm{i}\\alpha(d_{r}u^{-})-\\operatorname*{limsup}_{{\\vec{A}}(u,t)}+(1-\\alpha)\\mathrm{i}\\alpha(d_{r}u^{-})\\Big\\|\\Big)}\\\\ &{\\quad\\times\\operatorname*{limsup}_{{\\vec{A}}(u,t)}\\operatorname*{limsup}_{{\\vec{A}}(t)}\\operatorname*{limsup}_{{\\vec{A}}(u,t)}-\\operatorname*{limsup}_{ $ +H(1Q (\u2212a\u2032|\u03b1s)\u2032))|Q(s\u2032, a\u2032) \u2212Q\u2032(s\u2032, a\u2032)|1(a\u2032\u2208U(Q))} ) HQ(a\u2032|s\u2032) ", "page_idx": 20}, {"type": "text", "text": "Since $\\begin{array}{r}{\\frac{1}{\\mathcal{H}_{Q}\\left(a^{\\prime}|s^{\\prime}\\right)}1_{\\left(a^{\\prime}\\in\\mathcal{U}\\left(Q\\right)\\right)}<\\beta}\\end{array}$ , we have $\\begin{array}{r}{\\operatorname*{max}\\{\\alpha+(1-\\alpha)\\beta,\\alpha+\\frac{(1-\\alpha)}{\\mathcal H_{Q}(a^{\\prime}|s^{\\prime})}\\}=\\alpha+(1-\\alpha)\\beta}\\end{array}$ always true. As $\\beta<1$ , then $\\alpha+(1-\\alpha)\\beta<\\alpha+(1-\\alpha)<1$ . ", "page_idx": 20}, {"type": "text", "text": "Set $c=\\alpha+(1-\\alpha)\\beta$ , then we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\mathcal{F}Q(s,a)-\\mathcal{F}Q^{\\prime}(s,a)|\\leq c\\gamma||Q(s^{\\prime},a^{\\prime})-Q^{\\prime}(s^{\\prime},a^{\\prime})||_{\\infty},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which implies ${\\mathcal{F}}Q$ is $c\\gamma$ -contraction operator with $c\\gamma<1$ . ", "page_idx": 20}, {"type": "text", "text": "Suppose $Q^{\\Delta}$ is the stationary point of the Bellman equation in Eq.D.7, then it can be shown that $Q$ iteratively updated by Eq.D.7 can converge to $Q^{\\Delta}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{||Q^{t+1}-Q^{\\Delta}||_{\\infty}=||\\mathcal{F}Q^{t}-\\mathcal{F}Q^{\\Delta}||_{\\infty}\\leq c\\gamma||Q^{t}-Q^{\\Delta}||_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq(c\\gamma)^{2}||Q^{t-1}-Q^{\\Delta}||_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\cdot\\cdot\\cdot\\leq(c\\gamma)^{t+1}||Q^{0}-Q^{\\Delta}||_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Sending $t$ to infinity, we can derive $Q^{t}$ converge to $Q^{\\Delta}$ , then we finish the proof. ", "page_idx": 20}, {"type": "text", "text": "Remark D.1. The assumption $\\begin{array}{r}{\\frac{1}{\\mathcal{H}_{Q}\\left(a^{\\prime}|s^{\\prime}\\right)}1_{\\left(a^{\\prime}\\in\\mathcal{U}\\left(Q\\right)\\right)}<\\beta}\\end{array}$ can be always satisfied. Roughly speaking, since $a^{\\prime}\\in\\mathcal{U}(Q)$ , the uncertainty of $\\mathrm{^Q}$ -value $\\mathcal{H}_{Q}(a^{\\prime}|s^{\\prime})$ on this $a^{\\prime}$ has large uncertainty due to the OOD property. Furthermore, we can scale the absolute value $\\mathcal{H}_{Q}(a^{\\prime}|s^{\\bar{\\prime}})$ for all the action with ", "page_idx": 20}, {"type": "text", "text": "same factor to guarantee $\\begin{array}{r}{\\frac{1}{\\mathcal{H}_{Q}\\left(a^{\\prime}|s^{\\prime}\\right)}1_{\\left(a^{\\prime}\\in\\mathcal{U}\\left(Q\\right)\\right)}<\\beta}\\end{array}$ without hurting the relative comparison for the uncertainty. Furthermore, experiment results have shown that $\\begin{array}{r}{\\frac{1}{\\mathcal{H}_{Q}\\left(a^{\\prime}|s^{\\prime}\\right)}1_{\\left(a^{\\prime}\\in\\mathcal{U}\\left(Q\\right)\\right)}<\\beta}\\end{array}$ is consistently satisfied without additional processing. ", "page_idx": 21}, {"type": "text", "text": "E Performance of the Q-value function $Q^{k}(s,a)$ derived by QDQ. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we delve into an analysis of the performance of the $\\mathrm{^Q}$ -value function derived from the QDQ algorithm. Given that the primary aim of QDQ is to mitigate the issue of excessively conservative in most pessimistic Q-value methods, our focus is directed towards scrutinizing the disparity between the optimal Q-value within the offline RL framework and the optimal Q-value function yielded by QDQ. ", "page_idx": 21}, {"type": "text", "text": "The following is a formal version of Theorem 4.4. ", "page_idx": 21}, {"type": "text", "text": "Theorem E.1. Suppose the optimal $Q$ -value function over state-action space $S_{D}\\times A_{D}$ defined by the dataset $\\mathcal{D}$ is $Q^{*}$ . Then with probability at least $1-\\eta,$ , the $Q$ -value function $Q^{\\Delta}$ learned by minimizing uncertainty-aware loss (Eq.7) can approach the optimal $Q^{*}$ with a small constant: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|Q^{\\Delta}-Q^{*}\\right\\|_{\\infty}\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where \u03f5 is error rate related to the difference between the classical Bellman operator $B Q$ and the QDQ Bellman operator $\\mathcal{F}Q$ , and $\\eta$ is determined by the probability that $\\begin{array}{r}{\\mathrm{nax}_{a^{\\prime}}\\{(1\\,-\\,\\,\\beta)\\}|\\bar{Q}(s^{\\prime},a^{\\prime})|1_{(a^{\\prime}\\notin\\bar{\\mathcal{U}}(Q))}\\,+\\,(1\\,-\\,\\,\\frac{1}{\\mathcal{H}_{Q}(a^{\\prime}|s^{\\prime})})|Q(s^{\\prime},a^{\\prime})|1_{(a^{\\prime}\\in\\bar{\\mathcal{U}}(Q))}\\}\\;\\;\\stackrel{}{=}\\,\\;\\mathrm{max}_{a^{\\prime}}\\{(1\\,-\\,-\\,\\,\\frac{1}{\\mathcal{H}(Q)})\\}|Q(s^{\\prime},a^{\\prime})|1_{(a^{\\prime}\\in\\bar{\\mathcal{U}}(Q))},}\\end{array}$ $\\begin{array}{r}{\\frac{1}{\\mathcal{H}_{Q}(a^{\\prime}|s^{\\prime})})|Q(s^{\\prime},a^{\\prime})|1_{(a^{\\prime}\\in\\mathcal{U}(Q))}\\}}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "Before the proof, we redefine some notation to make the subsequent exposition clearer. ", "page_idx": 21}, {"type": "text", "text": "Denote state space $\\scriptstyle{S_{D}}$ be the state space defined by the distribution of dataset $\\mathcal{D}$ , $A_{\\mathcal{D}}$ is the action space defined by the dataset $\\mathcal{D}$ , and actions not belong to this space is the OOD actions. The optimal Q-value $Q^{*}$ on $S_{D}\\times A_{D}$ can be derived by optimize the following Bellman equation: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(Q):=\\!\\mathbb{E}_{s\\sim P_{D}(s),a\\sim\\pi(a|s)}[Q(s,a)-B Q(s,a)]^{2}}\\\\ &{\\qquad\\quad=\\!\\mathbb{E}_{s\\sim P_{D}(s),a^{\\prime}\\sim\\pi(a|s)}[Q(s,a)-(r(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{D}(s^{\\prime})}[\\underset{a^{\\prime}}{\\operatorname*{max}}Q(s^{\\prime},a^{\\prime})])]^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $(s,a)\\in S_{D}\\times A_{\\mathcal{D}}$ , $Q^{*}=B Q^{*}$ ", "page_idx": 21}, {"type": "text", "text": "We first introduced Lemma E.1 to facilitate the proof of Theorem 4.4. ", "page_idx": 21}, {"type": "text", "text": "Lemma E.1. For any $s\\in S_{D},a\\in\\mathcal{A}_{\\mathcal{D}}$ , with probability $1-\\eta_{;}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\mathcal{B}Q^{*}(s,a)-\\mathcal{F}Q^{*}(s,a)|\\leq\\gamma(1-\\alpha)(1-\\beta)||Q^{*}(\\cdot,\\cdot)||_{\\infty},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and with probability $\\eta$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\mathcal{B}Q^{*}(s,a)-\\mathcal{F}Q^{*}(s,a)|\\leq\\gamma(1-\\alpha)(1-\\frac{1}{\\mathcal{H}_{Q^{*}}(a^{\\prime}|s^{\\prime})})||Q^{*}(\\cdot,\\cdot)||_{\\infty}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma E.1. ", "page_idx": 21}, {"type": "text", "text": "Direct computation shows that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|B Q(s,a)-\\mathcal{F}Q(s,a)|}\\\\ &{=|r(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{D}(s^{\\prime})}\\left\\{\\operatorname*{max}\\mathcal{G}(s^{\\prime},a^{\\prime})\\right\\}-r(s,a)-\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{D}(s^{\\prime})}\\left\\{\\operatorname*{max}\\!\\left[\\alpha Q(s^{\\prime},a^{\\prime})+(1-\\alpha)Q_{L}(s^{\\prime})\\right]\\right\\}}\\\\ &{\\leq\\!\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{D}(s^{\\prime})}\\left|\\operatorname*{max}\\{Q(s^{\\prime},a^{\\prime})-[\\alpha Q(s^{\\prime},a^{\\prime})+(1-\\alpha)Q_{L}(s^{\\prime},a^{\\prime})]\\}\\right|}\\\\ &{\\leq\\!\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{D}(s^{\\prime})}\\left|\\operatorname*{max}\\{Q(s^{\\prime},a^{\\prime})-[\\alpha Q(s^{\\prime},a^{\\prime})+(1-\\alpha)Q_{L}(s^{\\prime},a^{\\prime})]\\}\\right|}\\\\ &{\\leq\\!\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{D}(s^{\\prime})}\\operatorname*{max}\\{Q(s^{\\prime},a^{\\prime})-[\\alpha Q(s^{\\prime},a^{\\prime})+(1-\\alpha)Q_{L}(s^{\\prime},a^{\\prime})]\\}\\!\\!}\\\\ &{=\\!\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{D}(s^{\\prime})}\\operatorname*{max}\\{Q(s^{\\prime},a^{\\prime})-[(\\alpha+(1-\\alpha)\\beta)Q(s^{\\prime},a^{\\prime})]_{(\\alpha^{\\prime},a^{\\prime})}((\\alpha))+(\\alpha+\\frac{(1-\\alpha)}{\\mathcal{H}_{Q}(\\alpha^{\\prime}|s^{\\prime})})Q(s^{\\prime},a^{\\prime})}\\\\ &{=\\!\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{D}(s^{\\prime})}\\operatorname*{max}\\{(1-\\alpha-(1-\\alpha)\\beta)Q(s^{\\prime},a^{\\prime})1_{(\\alpha^{\\prime}\\notin M(Q))}+(1-\\alpha-\\frac{(1-\\alpha)}{\\mathcal{H}_{Q}(\\alpha^{\\prime}|s^{\\prime})})Q(s^{\\prime},a^{\\prime})1_{ \n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then with probability $1-\\eta$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|B Q^{*}(s,a)-\\mathcal{F}Q^{*}(s,a)|\\leq\\!\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{{\\mathcal D}}(s^{\\prime})}\\operatorname*{max}_{a^{\\prime}}\\{(1-\\alpha)(1-\\beta))|Q^{*}(s^{\\prime},a^{\\prime})|\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\!\\gamma(1-\\alpha)(1-\\beta)||Q^{*}(\\cdot,\\cdot)||_{\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and with probability $\\eta$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|B Q^{*}(s,a)-\\mathcal{F}Q^{*}(s,a)|\\leq\\gamma\\operatorname*{max}_{s^{\\prime}}\\underset{a^{\\prime}}{\\operatorname*{max}}\\{(1-\\alpha)(1-\\frac{1}{\\mathcal{H}_{Q^{*}}(a^{\\prime}|s^{\\prime})})|Q(s^{\\prime},a^{\\prime})|\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\gamma(1-\\alpha)(1-\\frac{1}{\\mathcal{H}_{Q^{*}}(a^{\\prime}|s^{\\prime})})||Q^{*}(\\cdot,\\cdot)||_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then we finish the proof. ", "page_idx": 22}, {"type": "text", "text": "Next, we will give a brief proof of Theorem 4.4 with the results of Theorem 4.3 and Lemma E.1. Suppose the stationary point or the optimal Q-value derived based on the QDQ Bellman operator is $Q^{\\dot{\\Delta}}$ , which satisfying: $\\dot{Q}^{\\Delta}=\\mathcal{F}Q^{\\Delta}$ . ", "page_idx": 22}, {"type": "text", "text": "Then with probability $1-\\eta$ we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\big\\|Q^{\\Delta}-Q^{*}\\big\\|_{\\infty}=\\big\\|\\mathcal{F}Q^{\\Delta}-\\mathcal{F}Q^{*}+\\mathcal{F}Q^{*}-\\mathcal{B}Q^{*}\\big\\|_{\\infty}}}\\\\ &{\\leq\\big\\|\\mathcal{F}Q^{\\Delta}-\\mathcal{F}Q^{*}\\big\\|_{\\infty}+\\|\\mathcal{F}Q^{*}-\\mathcal{B}Q^{*}\\|_{\\infty}}\\\\ &{\\leq\\!c\\gamma||Q^{\\Delta}-Q^{*}||_{\\infty}+\\!\\gamma(1-\\alpha)(1-\\beta)||Q^{*}(\\cdot,\\cdot)||_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\big\\|Q^{\\Delta}-Q^{*}\\big\\|_{\\infty}\\leq\\gamma(1-\\alpha)(1-\\beta)(1-c\\gamma)^{-1}\\|Q^{*}(\\cdot,\\cdot)\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Set $\\epsilon=\\gamma(1-\\alpha)(1-\\beta)(1-c\\gamma)^{-1}||Q^{*}(\\cdot,\\cdot)||_{\\infty}$ finish the proof. ", "page_idx": 22}, {"type": "text", "text": "Remark E.1. The error $\\epsilon=\\gamma(1-\\alpha)(1-\\beta)(1-c\\gamma)^{-1}||Q^{*}(\\cdot,\\cdot)||_{\\infty}$ can be 0 by setting $\\beta=1$ . In practice, we can ensure that $\\epsilon$ converges to a small value by appropriately adjusting the parameters $\\alpha$ and $\\beta$ . ", "page_idx": 22}, {"type": "text", "text": "Remark E.2. Our primary focus lies on the scenario where $\\operatorname*{max}_{a^{\\prime}}\\{(1-\\beta))|Q(s^{\\prime},a^{\\prime})|1_{(a^{\\prime}\\notin\\mathcal{U}(Q))}+$ $\\begin{array}{r}{(1-\\frac{1}{\\mathcal{H}_{O}(a^{\\prime}|s^{\\prime})})|Q(s^{\\prime},a^{\\prime})|1_{(a^{\\prime}\\in\\mathcal{U}(Q))}\\}\\,=\\,\\operatorname*{max}_{a^{\\prime}}\\{(1-\\beta))|Q(s^{\\prime},a^{\\prime})|1_{(a^{\\prime}\\notin\\mathcal{U}(Q))}\\}}\\end{array}$ . This preference stems from the potential minuteness of $\\eta$ . Firstly, we can rely on Theorem 1 in [20] to ensure that the consistency model can converge to ground truth, thus guaranteeing the fidelity of the learned Q-distribution. Consequently, the accuracy of our uncertainty estimation is upheld, enabling us to effectively assess OOD points and pessimistically adjust the Q-value for such occurrences [4]. Secondly, in order to mitigate segmentation errors of the uncertainty set, we introduce the penalty factor $\\beta$ in Eq.8. Finally, since actions in the set $\\mathcal{U}(Q)$ are always penalized, the $Q\\cdot$ -value always takes a lower value when $a^{\\prime}\\in\\mathcal{U}(Q)$ . Collectively, these measures reinforce our aim to maintain a small $\\eta$ . ", "page_idx": 22}, {"type": "text", "text": "Theorem 4.4 shows the optimal $\\mathrm{^Q}$ -value by QDQ can closely approximate the true optimal Q-value $Q^{*}$ over $S_{D}\\times A_{D}$ . Then we also provide the following corollary to show that substitute the optimal Bellman operator $B Q$ with ${\\mathcal{F}}Q$ will introduce controllable error at each step, and give an step-wise analysis of the convergence of QDQ operator $\\mathcal{F}Q$ . ", "page_idx": 23}, {"type": "text", "text": "Let $\\zeta_{k}(s,a)\\,=\\,|Q^{k}(s,a)-Q^{*}(s,a)|$ be the total estimation error of $\\mathrm{^Q}$ -value learned by QDQ algorithm and the optimal in-distribution $\\mathrm{^Q}$ -value at step $\\mathbf{k}$ of the value iteration. Let $\\delta_{k}(s,a)=$ $|Q^{k}(s,a)-{\\mathcal{F}}Q^{k}(s,a)|$ be the Bellman residual induced by QDQ Bellman operator ${\\mathcal{F}}Q$ at step $\\boldsymbol{\\mathrm{k}}$ Assume $\\delta_{k}^{*}(s,a)=|Q^{k}(s,a)-\\mathcal{B}Q^{k}(s,a)|$ be the Bellman residual for the optimal in-distribution Q-value optimization. ", "page_idx": 23}, {"type": "text", "text": "Corollary 1. At step $k$ of value iteration, substitute the optimal Bellman operator $B Q$ with $\\mathcal{F}Q$ introduce an arbitrary small error as $\\xi$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\zeta_{k}(s,a)\\leq\\delta_{k}^{*}(s,a)+\\xi+\\gamma\\mathbb{E}_{s^{\\prime}\\sim D}\\operatorname*{max}_{a^{\\prime}}\\left|\\zeta_{k-1}(s,a)\\right|\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then we introduce our second Lemma E.2 to help the proof of Corollary 1: ", "page_idx": 23}, {"type": "text", "text": "Lemma E.2. For any $s\\in S_{D},a\\in\\mathcal{A}_{\\mathcal{D}}$ , with probability $1-\\eta_{;}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\delta_{k}(s,a)\\leq\\delta_{k}^{*}(s,a)+\\gamma(1-\\alpha)(1-\\beta)\\lvert|Q(s^{\\prime},a^{\\prime})\\rvert\\rvert_{\\infty}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With probability $\\eta_{-}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\delta_{k}(s,a)\\leq\\delta_{k}^{*}(s,a)+\\gamma(1-\\alpha)(1-\\frac{1}{\\mathcal{H}_{Q}(a^{\\prime}|s^{\\prime})})\\|Q(s^{\\prime},a^{\\prime})\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma E.2. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{k}(s,a)=\\rvert Q^{k}(s,a)-\\mathcal{F}Q^{k}(s,a)\\rvert}\\\\ &{\\qquad\\qquad=\\rvert Q^{k}(s,a)-{\\mathcal{B}Q^{k}(s,a)}+{\\mathcal{B}Q^{k}(s,a)}-{\\mathcal{F}Q^{k}(s,a)\\rvert}}\\\\ &{\\qquad\\qquad\\leq\\delta_{k}^{*}(s,a)+\\lvert{\\mathcal{B}Q(s,a)-\\mathcal{F}Q(s,a)}\\rvert\\delta_{k}^{*}(s,a)+\\gamma(1-\\alpha)(1-\\beta)\\lvert{|Q(s^{\\prime},a^{\\prime})}\\rvert\\rvert_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By Lemma E.1, with probability $1-\\eta$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{k}(s,a)=\\rvert Q^{k}(s,a)-\\mathcal{F}Q^{k}(s,a)\\rvert}\\\\ &{\\qquad\\qquad=\\rvert Q^{k}(s,a)-{B Q}^{k}(s,a)+{B Q}^{k}(s,a)-\\mathcal{F}Q^{k}(s,a)\\rvert}\\\\ &{\\qquad\\qquad\\leq\\delta_{k}^{*}(s,a)+\\lvert\\mathcal{B}Q(s,a)-\\mathcal{F}Q(s,a)\\rvert}\\\\ &{\\qquad\\qquad\\leq\\delta_{k}^{*}(s,a)+\\gamma(1-\\alpha)(1-\\beta)\\lvert\\lvert Q(s^{\\prime},a^{\\prime})\\rvert\\rvert_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With probability $\\eta$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\delta_{k}(s,a)=\\rvert Q^{k}(s,a)-\\mathcal{F}Q^{k}(s,a)\\rvert}}\\\\ &{}&{=\\rvert Q^{k}(s,a)-\\mathcal{B}Q^{k}(s,a)+\\mathcal{B}Q^{k}(s,a)-\\mathcal{F}Q^{k}(s,a)\\rvert}\\\\ &{}&{\\le\\delta_{k}^{*}(s,a)+\\lvert\\mathcal{B}Q(s,a)-\\mathcal{F}Q(s,a)\\rvert}\\\\ &{}&{\\le\\delta_{k}^{*}(s,a)+\\gamma(1-\\alpha)(1-\\frac{1}{\\mathcal{H}_{Q}(a^{\\prime}|s^{\\prime})})\\lvert\\lvert Q(s^{\\prime},a^{\\prime})\\rvert\\rvert_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then we finish the proof. ", "page_idx": 23}, {"type": "text", "text": "Next, we will give a brief proof of Corollary 1. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\zeta_{k}(s,a)=|Q^{k}(s,a)-Q^{*}(s,a)|}\\\\ &{\\quad=|Q^{k}(s,a)-\\mathcal{P}Q^{k-1}(s,a)+\\mathcal{F}Q^{k-1}(s,a)-Q^{*}(s,a)|}\\\\ &{\\quad\\leq|Q^{k}(s,a)-\\mathcal{F}Q^{k-1}(s,a)|+|\\mathcal{F}Q^{k-1}(s,a)-Q^{*}(s,a)|}\\\\ &{\\quad=\\delta_{k}(s,a)+|\\mathcal{F}Q^{k-1}(s,a)-Q^{*}(s,a)|}\\\\ &{\\quad=\\delta_{k}(s,a)+|\\mathcal{F}Q^{k-1}(s,a)-\\mathcal{B}Q^{k-1}(s,a)+\\mathcal{B}Q^{k-1}(s,a)-\\mathcal{B}Q^{*}(s,a)|}\\\\ &{\\quad\\leq\\delta_{k}(s,a)+|\\mathcal{F}Q^{k-1}(s,a)-\\mathcal{B}Q^{k-1}(s,a)|+|\\mathcal{B}Q^{k-1}(s,a)-\\mathcal{B}Q^{*}(s,a)|}\\\\ &{\\quad=\\delta_{k}(s,a)+|\\mathcal{F}Q^{k-1}(s,a)-\\mathcal{B}Q^{k-1}(s,a)|+|\\mathcal{D}Q^{k-1}P_{s}(s^{\\prime})\\operatorname*{max}Q^{k-1}(s,a)-\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{s}}(s,a)|}\\\\ &{\\quad\\leq\\delta_{k}(s,a)+|\\mathcal{F}Q^{k-1}(s,a)-\\mathcal{B}Q^{k-1}(s,a)|+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{s}}\\operatorname*{max}\\left|Q^{k-1}(s,a)-Q^{*}(s,a)\\right|}\\\\ &{\\quad=\\delta_{k}(s,a)+|\\mathcal{F}Q^{k-1}(s,a)-\\mathcal{B}Q^{k-1}(s,a)|+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{s}}\\operatorname*{max}\\left|Q^{k-1}(s,a)-Q^{*}(s,a)\\right|}\\\\ &{\\quad=\\delta_{k}^{*}(s,a)+\\gamma(1-\\alpha)(1-\\frac{1}{\\mathcal{H}_{Q}(a^{\\prime}|s^{\\prime})})||Q(s^{\\prime},a^{\\prime})||_{\\infty}+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P_{s}}\\operatorname*{max}|\\zeta_{k-1}(s\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By Lemma E.1 and Lemma E.2, we can easily derive, with probability $1-\\eta$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\zeta_{k}(s,a)\\leq\\!\\!\\delta_{k}^{*}(s,a)+\\gamma(1-\\alpha)(1-\\beta)||Q(s^{\\prime},a^{\\prime})||_{\\infty}+\\gamma\\mathbb{E}_{s^{\\prime}\\sim D}\\operatorname*{max}_{a^{\\prime}}|\\zeta_{k-1}(s,a)|\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Set $\\xi=\\gamma(1-\\alpha)(1-\\beta)||Q(s^{\\prime},a^{\\prime})||_{\\infty}$ , then we finish the proof. ", "page_idx": 24}, {"type": "text", "text": "Remark E.3. During the value iteration, there are two error accumulate: $\\delta_{k}^{*}(s,a)$ and $\\xi$ . Given the convergence of optimal Q-value iteration, $\\delta_{k}^{*}(s,a)$ tends to approach an arbitrarily small value and may even converge to 0 under optimal circumstances. The error $\\bar{\\xi}=\\gamma(1\\!-\\!\\alpha)(1\\!-\\!\\beta)\\bar{||}Q(s^{\\prime},a^{\\prime})||_{\\infty}$ induced by using the Bellman operator $\\mathcal{F}Q$ instead of the optimal Bellman operator can also approach 0 as discussed in Remark E.1. ", "page_idx": 24}, {"type": "text", "text": "F Gap expanding of the QDQ algorithm. ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section we will show that QDQ also has a Gap expanding property as discussed in CQL [3]. Proposition F.1. The QDQ algorithm is Gap expanding for $Q$ -values within-distribution and $Q$ -values out of distribution. ", "page_idx": 24}, {"type": "text", "text": "(1) If $a\\not\\in\\mathcal{U}(Q)$ and is indeed in-distribution action, the target Q-value for the in-distribution point is $(\\alpha+(1-\\alpha)\\beta)Q$ . The target Q-value for the OOD point is $\\begin{array}{r}{(\\alpha+\\frac{(1-\\alpha)}{\\mathcal{H}_{Q}(a|s)})Q}\\end{array}$ HQ(a|s))Q. And $\\begin{array}{r}{(\\alpha+(1-\\alpha)\\beta)Q-(\\alpha+\\frac{(1-\\alpha)}{\\mathcal{H}_{Q}(a|s)})Q>0}\\end{array}$ definitely. And this suggest Q-value will prefer in-distribution actions.   \n(2) If $a\\not\\in\\mathcal{U}(Q)$ and is indeed OOD action. In such cases, the penalty parameter $\\beta$ is introduced to penalize the $\\mathrm{^Q}$ -value, resulting in $(\\alpha\\!+\\!(1\\!-\\!\\alpha)\\beta)Q<Q$ . Consequently, misclassifications of OOD actions can be handled in a pessimistic manner. ", "page_idx": 24}, {"type": "text", "text": "While introducing the penalty parameter $\\beta$ may slightly slow down optimization updates for true in-distribution actions, it is crucial to prioritize control over out-of-distribution (OOD) actions due to the potentially severe consequences of exploration errors. In balancing the optimization of Q-learning with a pessimistic approach to OOD actions, the focus should be on reducing the impact of OOD actions. In fact, compared to previous pessimistic Q-value methods, QDQ offers greater flexibility in managing these values. This includes incorporating an uncertainty-aware learning objective and adjusting Q-values based on their uncertainty. ", "page_idx": 24}, {"type": "text", "text": "G Experiment Details and More Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "G.1 Real Q dataset generation. ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In Section 3.1, we use a $k$ -step sliding window approach with a length of $\\tau$ to traverse each trajectory in the dataset $\\mathcal{D}$ and generate the Q-value dataset $\\mathcal{D}_{Q}$ based on Equation 5. A good Q-value dataset for uncertainty estimation should cover a broad state and action space to accurately characterize the $\\mathrm{^Q}$ distribution and detect high uncertainty in the OOD action region. This coverage helps identify actions with significant Q-value uncertainty in OOD areas. ", "page_idx": 24}, {"type": "text", "text": "Choosing the right value for the sliding step $k$ and the window length $\\tau$ is crucial. A large value for $k$ may result in a smaller Q dataset, while a small value may lead to excessive homogenization of the Q dataset. Both situations can negatively impact the learning of the Q-value distribution. If the Q-value dataset is too small, the learned Q distribution may not generalize well across the state-action space ${\\mathcal{S}}\\times{\\mathcal{A}}$ . Conversely, if the Q-values are too homogeneous, it can hinder feature learning by the distribution learner. ", "page_idx": 24}, {"type": "text", "text": "To illustrate this, Figure G.1 shows the Q dataset distributions obtained for different values of $k$ using the halfcheetah-medium dataset. When $k=1$ , the Q distribution is more concentrated, resulting in more homogeneous Q-values. In contrast, with $k=50$ , the Q distribution becomes sparser, showing a greater inclination towards individual features. ", "page_idx": 24}, {"type": "text", "text": "In the experiments, we consider various factors when setting the value of $k$ , including the trajectory length, the width of the sliding window, and the derived $Q\\cdot$ -value dataset size. Throughout all experiments, we set $k$ to 10. Interestingly, we observed that the distribution of Q-values remains robust to minor adjustments in $k$ , indicating that the choice of $k$ does not necessitate overly stringent tuning. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "image", "img_path": "NIcIdhyfQX/tmp/fd6de5c617248187de8becb34a25039d49f714fb9eaf1c22a6f78941d5a69671.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure G.1: The derived Q-value distribution when using difference sliding step and same window width to scan over the trajectory\u2019s on halfcheetah-medium dataset.The width of the sliding window is set to 200. The $\\mathrm{{Q}}.$ -value is scaled to facilitate comparison. ", "page_idx": 25}, {"type": "text", "text": "When choosing the width of the sliding window, $\\tau$ , we must consider factors such as trajectory length and the resulting size of the Q-value dataset. When $\\tau$ increases, the size of the Q-value dataset decreases. However, if $\\tau$ is too small, it might truncate essential information from the true Q-value. We give the experimental analysis of Q-value distribution for different $\\tau$ on the sparse reward task Antmaze-medium-play in Figure G.2. ", "page_idx": 25}, {"type": "image", "img_path": "NIcIdhyfQX/tmp/c1e9b154a335558da688dd43f6ee188496dfc7029736e7d42e2e3f040ea99115.jpg", "img_caption": ["Figure G.2: The Q distribution of the Antmaze-medium-play dataset with varying sliding window widths (100 to 300 steps) is shown in the figure. Widening the sliding window does not change the shape of the Q distribution, even though a larger window covers more information for this sparse reward task with many short trajectories. Instead, enlarging the sliding window decreases the Q value and compresses the size of the derived Q data. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "In our experiments, we opted for $\\tau=200$ across all tasks. This choice considers factors such as the maximum trajectory length (1000), the decay rate of $\\gamma^{m-1}$ in Eq.5. Based on the analysis provided in Section B, $\\tau$ does not need to be very large. We also found that minor adjustments to $\\tau$ do not significantly affect the Q-value distribution, indicating that strict tuning of this parameter is unnecessary. ", "page_idx": 25}, {"type": "text", "text": "G.2 The distribution of Q-value function. ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The Q-distributions based on the truncated $\\mathrm{^Q}$ -value and the learned distribution from the consistency model are shown in Figure G.3 for Gym-MuJoCo tasks and in Figure G.5 for Antmaze tasks. In Figure G.3, the consistency model roughly captures the main characteristics of the true Q-value distribution. However, for the Antmaze task (Figure G.5), the learned distribution shows some fluctuations and a slightly wider support compared to the true Q-value distribution, particularly in the dynamic goal task (\"-diverse\" task). ", "page_idx": 25}, {"type": "text", "text": "From the distribution of Antmaze we observe that trajectories for these tasks mainly consist of suboptimal or failed experiences, this underscores the challenging nature of the Antmaze task. ", "page_idx": 25}, {"type": "text", "text": "Further the narrower data distribution make it easier to take OOD actions during offilne training and fewer positive experiences limits the optimisation of Q, these all potentially leading to failure of these kinds of tasks. ", "page_idx": 26}, {"type": "image", "img_path": "NIcIdhyfQX/tmp/4a3babfa5feab5ce9ad89004177a9e3100859b723892ec10059d71567cb87f46.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure G.3: The Q-value distribution based on the truncated Q-value v.s. the sample Q-value distribution via the learned consistency model for Gym-MuJoCo tasks. ", "page_idx": 26}, {"type": "image", "img_path": "NIcIdhyfQX/tmp/d462668c21cd9ee5f42703d89d4a5f98918a7603109e198f814fb9fd2bab3ff8.jpg", "img_caption": ["Figure G.4: The Q-value distribution based on the truncated $\\mathrm{\\DeltaQ}$ -value v.s. the sample $\\mathrm{{Q}}.$ -value distribution via the learned consistency model for Antmaze tasks. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "G.3 Efficiency of the uncertainty measure. ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The uncertainty measure is crucial for guiding the Q-value towards a pessimistic regime within the QDQ algorithm. In this section, we will first verify how uncertainty can assess the overestimation of $\\mathrm{{Q}}.$ -values in OOD regions. Then, we will discuss how the uncertainty set $\\mathcal{U}(Q)$ can be shaped using the hyperparameter $\\beta$ . ", "page_idx": 27}, {"type": "text", "text": "To understand the differences in uncertainty between in-distribution actions and OOD actions from a random policy, we can compare their distributions. In the left graph of Figure G.5, the red line represents the $95\\%$ quantile of the standard deviation of sampled Q-values from the learned Q-distribution for in-distribution actions. In the right graph, this $95\\%$ quantile corresponds to approximately the $75\\%$ quantile of the standard deviation for OOD actions. This indicates that OOD actions contribute to a heavy-tailed distribution of Q-value uncertainty, resulting in larger values compared to in-distribution actions. ", "page_idx": 27}, {"type": "text", "text": "Figure G.6 further illustrates this, showing that the standard deviation of sampled Q-values from the learned policy sharply increases when Q-values are overestimated and become uncontrollable. These observations suggest that the uncertainty measure in the QDQ algorithm effectively captures the overestimation phenomenon in OOD actions. ", "page_idx": 27}, {"type": "image", "img_path": "NIcIdhyfQX/tmp/e71e9b5d27ec499fb9ce16f09f737278b89d14eb04882a5951c241b35af03273.jpg", "img_caption": ["Figure G.5: The uncertainty distribution of Q-value based on in-distribution action v.s. the uncertainty distribution of Q-value based on OOD actions(by a ramodm policy). "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "NIcIdhyfQX/tmp/fa95df962eb783a8656545d64cbc48aa44153cffe5a723e5e80e8fd3b2f3e18c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure G.6: The Q value learned by QDQ(left), and the standard deviation of the sample Q-value from the consistency model for same state and action pair. ", "page_idx": 27}, {"type": "table", "img_path": "NIcIdhyfQX/tmp/bdcd82aa69e6bf8dead7875bd6ca7c53f0ed3146278113fe1fd25ea09228aed6.jpg", "table_caption": ["Table G.1: The hyperparameter used in Flow-GAN training. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "The uncertainty set $\\mathcal{U}(Q)$ is derived using the upper $\\beta$ -quantile of the entire uncertainty value of Q over the action taken by the learning policy. In Section 5.2, we provide a brief discussion on determining the appropriate $\\beta$ during experiments. A higher $\\beta$ may allow for a more generous attitude towards OOD actions, which is suitable for tasks with a wide data distribution or that are robust for action change. Conversely, a lower $\\beta$ suggests more rigid control over OOD actions, appropriate for tasks with a narrow data distribution or that are sensitive. During experiments, a rough starting point for setting $\\beta$ can be obtained by comparing the quantiles of the uncertainty distribution based on in-distribution actions and OOD actions. For instance, as shown in Figure G.5, parameter tuning might begin with $\\beta=0.75$ or $\\beta=0.80$ . ", "page_idx": 28}, {"type": "text", "text": "G.4 Implementation details for QDQ algorithm. ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Implementation of QDQ algorithm contains consistency distillation for the consistency model and offline RL training. The whole algorithm is implemented with jax [67]. The training process of consistency distillation and offline RL is independent, we first train the consistency model by consistency distillation and save the converged model. Then we use the pretrained consistency model as the Q-value distribution sampler and go through the offline RL training, see Algorithm 1 for the whole training process. ", "page_idx": 28}, {"type": "text", "text": "Consistency Distillation. The consistency model is trained using a pretrained diffusion model [33]. The training process follows the official implementation 4of the consistency model [20]. Since the consistency model is designed for image data, we modified the initial architecture, the $N C S N++m o d e l$ [33], to better fti offilne RL data. For instance, we replaced the U-Net architecture with a multilayer perceptron (MLP) that has three hidden layers, each with 256 units. This simplified architecture is used to learn the consistency function $f_{\\theta}({\\boldsymbol{x}}_{t},t)$ . The main hyperparameters for the consistency distillation are shown in Table G.1, covering both diffusion model training and consistency model training. ", "page_idx": 28}, {"type": "table", "img_path": "NIcIdhyfQX/tmp/49937ca5bbfa6140fa1723ce93f34cd4df60a0bf338cdcc4f34628a45edc5c26.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Offilne RL training. The offilne RL training follows TD3 [46], which has a delayed update schedule for both the target Q network and the target policy network. For the Gym-MuJoCo tasks, we use the raw dataset without any preprocessing, such as state normalization or reward adjustment. For the AntMaze tasks, we apply the same reward tuning as IQL [10], with no additional preprocessing for the state. The hyperparameters used in offline RL training are shown in Table G.2. ", "page_idx": 29}, {"type": "text", "text": "G.5 Learning Curve ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The learning curve of Gym-Mujoco tasks is shown in Figure G.7. The learning curve of AntMaze tasks is shown in Figure G.8. ", "page_idx": 29}, {"type": "text", "text": "G.6 Computation efficienty of QDQ ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We provide a detailed discussion of the computational efficiency of the QDQ algorithm we proposed, focusing on the training cost of the consistency model and computation coefficiency of QDQ (the distribution-based bootstrap method) compared with SOTA uncertainty estimation methods based on Q-value ensembles. ", "page_idx": 29}, {"type": "text", "text": "Regarding the training cost of the consistency model, we believe it is nearly negligible. Training a diffusion model on a 4090 GPU takes about 5.2 minutes, while training a consistency model using this pretrained diffusion model takes around 16 minutes. Additionally, the consistency model can be stored and reused for subsequent RL experiments, eliminating the need for retraining. ", "page_idx": 29}, {"type": "text", "text": "For a comparison of the computational costs of QDQ with SOTA uncertainty estimation methods in offline RL, see Table G.6. As mentioned earlier, QDQ achieves a significantly faster training speed compared to the ensemble-based uncertainty estimation method EDAC and other SOTA methods. The results for other methods are taken from Table 3 of EDAC [9]. ", "page_idx": 29}, {"type": "image", "img_path": "NIcIdhyfQX/tmp/ca3d09f7c356a4352c4cc994765337452f0c0313759a353d0149ca28419e92aa.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure G.7: Training curve of different Mujoco Tasks. All results are averaged across 5 random seeds. The evaluation interval is 5000 with evaluation episode length 10. ", "page_idx": 30}, {"type": "table", "img_path": "NIcIdhyfQX/tmp/3352f8e11d429ba254908c8dd4e1f5c3f6fd5ea29446eff0121e0994a6af7b47.jpg", "table_caption": ["Table G.3: Computational performance of QDQ and other SOTA methods. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "G.7 Ablations ", "page_idx": 30}, {"type": "text", "text": "Although QDQ has three hyperparameters $(\\alpha,\\beta$ , and $\\gamma$ ) for flexibility, the tuning process is straightforward. For example, as discussed in Theorem 4.4 (Appendix E), theoretically, $(\\bar{1}\\!-\\!\\alpha)(1\\!-\\!\\beta)$ should be small. Since $\\beta$ controls the size of the uncertainty set and requires flexibility across different tasks, we typically set $\\alpha$ close to 1, tuning it between 0.9 and 0.995. This only requires a few experiments to find the optimal value. Our tuning process involves sequentially fixing parameters while selecting the best $\\alpha$ , then $\\beta$ , and finally $\\gamma$ . Based on the characteristics of different datasets, we can set each parameter to an initial value close to its optimal value. QDQ offers evidence-based guidelines for ", "page_idx": 30}, {"type": "image", "img_path": "NIcIdhyfQX/tmp/837ca3d8d2729d5cd417da19943b8b26d2218c1340e29defda0505afaf24306d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure G.8: Training curve of different Antmaze Tasks. All results are averaged across 5 random seeds. The evaluation interval is 100000 with evaluation episode length 100. ", "page_idx": 31}, {"type": "text", "text": "hyperparameter ranges, making tuning manageable. Additionally, QDQ\u2019s efficiency (see Table G.6) reduces the tuning burden. ", "page_idx": 31}, {"type": "text", "text": "In the ablation study, we choose four task which represent different kinds of dataset we analyzed during parameter study: ", "page_idx": 31}, {"type": "text", "text": "(1) Wide distribution: halfcheetah-medium- $\\cdot\\nu2$ .   \n(2) Task sensitive: hopper-medium- $\\cdot\\nu2$ .   \n(3) Demonstration: walker2d-medium-expert- $\\cdot\\nu2$ .   \n(4) Narrow distribution: umaze-diverse-v0. ", "page_idx": 31}, {"type": "text", "text": "We perform ablation study for parameters we discussed in Section 5.2. We compare the performances of three different settings for the three parameters respectively. ", "page_idx": 31}, {"type": "text", "text": "The learning curves for four types of tasks with different uncertainty-aware weights $\\alpha$ (Eq. 7) are shown in Figure G.9. For the halfcheetah-medium- $\\cdot\\nu2$ and hopper-medium- $\\nu2$ datasets, decreasing $\\alpha$ harms performance. For the walker2d-medium-expert- $\\cdot\\nu2$ dataset, a slightly lower $\\alpha$ increases training volatility. In contrast, the umaze-diverse- $\\cdot\\nu\\theta$ dataset shows high sensitivity to $\\alpha$ . This sensitivity occurs because the Antmaze task requires combining different suboptimal trajectories, which challenges algorithms like QDQ that are not fully in-sample but in-support. For instance, an $\\alpha$ value that is too high or too low can lead to overly optimistic or pessimistic $\\mathrm{{Q}}.$ -values, causing the algorithm to favor actions that do not align with the exact suboptimal trajectories. ", "page_idx": 31}, {"type": "text", "text": "The ablation study of the uncertainty-related parameter $\\beta$ is presented in Figure G.11. For the wide dataset halfcheetah-medium- $_{\\cdot\\nu2}$ , a higher $\\beta$ is preferred, indicating less control over the uncertainty penalty and a smaller uncertainty set. In contrast, the performance for the hopper-medium- $\\nu2$ and umaze-diverse- $\\nu O$ datasets is sensitive to small changes in $\\beta$ due to their task sensitivity and narrow distribution. The walker2d-medium-expert- $\\cdot\\nu2$ dataset, however, is robust to small changes in $\\beta$ . ", "page_idx": 31}, {"type": "image", "img_path": "NIcIdhyfQX/tmp/672f32ec7053fb3bcca3e283419a175aa35fa019be8b31aa4a138a5dfb87f7b8.jpg", "img_caption": ["Figure G.9: Training curve of four different Tasks when using different $\\alpha$ in Eq. 7. All results are averaged across 5 random seeds. The evaluation interval is 5000 with evaluation episode length 10 for Mujoco tasks. The evaluation interval is 100000 with evaluation episode length 100 for Antmaze task. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "We present the performance of different tasks for the entropy parameter $\\gamma$ in Figure G.12. For the wide distribution in halfcheetah-medium- $\\cdot\\nu2$ , a larger $\\gamma$ negatively impacts performance. The hopper-medium- $\\cdot\\nu2$ and umaze-diverse- $\\cdot\\nu\\theta$ datasets also show sensitivity to the parameter $\\gamma$ . In the walker2d-medium-expert- $\\nu2$ dataset, a very small $\\gamma$ may introduce volatility into the training process. However, the final result\u2019s convergence is not significantly affected. ", "page_idx": 32}, {"type": "text", "text": "Furthermore, as discussed in Section 5.2, the gamma term primarily stabilizes the learning of a simple Gaussian policy, especially for action-sensitive and dataset-narrow tasks. We note that Gaussian policies are prone to sampling risky actions because they can only fti a single-mode policy. To verify the impact of uncertainty-aware Q-value optimization in QDQ, we compared the performance of Q-values without uncertainty control (using Bellman optimization like in online RL settings) to the QDQ algorithm on the action-sensitive hopper-medium dataset, using identical gamma settings. Figure ?? shows that introducing an uncertainty-based constraint for the Q-value function in QDQ significantly improves training stability, convergence speed, and overall performance. This supports the effectiveness of QDQ\u2019s uncertainty-aware Q-value optimization. We believe that a stronger learning policy will further reduce the need for this stabilizing term. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "image", "img_path": "NIcIdhyfQX/tmp/ead8019f85c6259ab87f5e380bd07cd9af05a73e59bc7de633a9320de1fbacdd.jpg", "img_caption": ["Figure G.10: The training curve of the hopper-medium dataset with QDQ\u2019s uncertainty pessimistic Q learning and without Q-value adjustments is shown. The green curve indicates that the $\\gamma$ term in Eq. 9 has a limited impact on performance. Comparing the learning curves, QDQ\u2019s uncertainty pessimistic Q learning boosts performance, leading to faster convergence and greater stability. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "NIcIdhyfQX/tmp/84f6017bccaf753d12fcdb6f46950e83d20db52b8b20944ac47ef0f70a8ad5a2.jpg", "img_caption": ["Figure G.11: Training curve of four different Tasks when using different $\\beta$ in Section 3.2. All results are averaged across 5 random seeds. The evaluation interval is 5000 with evaluation episode length 10 for Mujoco tasks. The evaluation interval is 100000 with evaluation episode length 100 for Antmaze task. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "NIcIdhyfQX/tmp/0d8e9b9dcb872620f8531cbe85eb7d051d607e0720dac56b6d8114341a623369.jpg", "img_caption": ["Figure G.12: Training curve of four different Tasks when using different $\\gamma$ in Eq. 9. All results are averaged across 5 random seeds. The evaluation interval is 5000 with evaluation episode length 10 for Mujoco tasks. The evaluation interval is 100000 with evaluation episode length 100 for Antmaze task. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We explicitly discuss the contribution and research focus in the abstract and introduction section. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: We give limitation of this work in the experiment section. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provide the detailed assumption and proof for each theoretical result in the Appendix B-F. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We give the detailed implementation of the experiment in experiment section and Appendix G. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We give the detailed implementation of the experiment in Appendix G. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 40}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: [TODO] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] Justification: [TODO] ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]