{"references": [{"fullname_first_author": "Sergey Levine", "paper_title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems", "publication_date": "2020-05-01", "reason": "This paper provides a comprehensive overview of offline reinforcement learning, which is the central topic of the current paper."}, {"fullname_first_author": "Aviral Kumar", "paper_title": "Conservative Q-learning for offline reinforcement learning", "publication_date": "2020-12-01", "reason": "This paper introduces Conservative Q-learning, a key method for addressing the challenges of offline RL that the current paper builds upon."}, {"fullname_first_author": "Rishabh Agarwal", "paper_title": "An optimistic perspective on offline reinforcement learning", "publication_date": "2020-01-01", "reason": "This paper explores the use of optimism in offline RL, which is an important consideration addressed by the current paper's uncertainty-aware approach."}, {"fullname_first_author": "Ilya Kostrikov", "paper_title": "Offline reinforcement learning with implicit Q-learning", "publication_date": "2021-10-01", "reason": "This paper introduces Implicit Q-learning, a technique used to improve offline RL performance that is contrasted with the current paper's method."}, {"fullname_first_author": "Yang Song", "paper_title": "Consistency models", "publication_date": "2023-03-01", "reason": "This paper introduces the consistency model, a core component of the current paper's approach to uncertainty estimation and Q-value learning."}]}