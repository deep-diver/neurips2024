[{"type": "text", "text": "Decomposing and Interpreting Image Representations via Text in ViTs Beyond CLIP ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sriram Balasubramanian ", "page_idx": 0}, {"type": "text", "text": "Samyadeep Basu ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Computer Science University of Maryland, College Park sriramb@cs.umd.edu ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Science University of Maryland, College Park sbasu12@umd.edu ", "page_idx": 0}, {"type": "text", "text": "Soheil Feizi Department of Computer Science University of Maryland, College Park sfeizi@cs.umd.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent work has explored how individual components of the CLIP-ViT model contribute to the final representation by leveraging the shared image-text representation space of CLIP. These components, such as attention heads and MLPs, have been shown to capture distinct image features like shape, color or texture. However, understanding the role of these components in arbitrary vision transformers (ViTs) is challenging. To this end, we introduce a general framework which can identify the roles of various components in ViTs beyond CLIP. Specifically, we (a) automate the decomposition of the final representation into contributions from different model components, and (b) linearly map these contributions to CLIP space to interpret them via text. Additionally, we introduce a novel scoring function to rank components by their importance with respect to specific features. Applying our framework to various ViT variants (e.g. DeiT, DINO, DINOv2, Swin, MaxViT), we gain insights into the roles of different components concerning particular image features. These insights facilitate applications such as image retrieval using text descriptions or reference images, visualizing token importance heatmaps, and mitigating spurious correlations. We release our code to reproduce the experiments at https://github.com/SriramB-98/vit-decompose ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vision transformers and their variants [10, 22, 7, 33, 17, 32] have emerged as powerful image encoders, becoming the preferred architecture for modern image foundation models. However, the mechanisms by which these models transform images into representation vectors remain poorly understood. Recently, Gandelsman et al. [11] made significant progress on this question for CLIP-ViT models with two key insights: (i) They demonstrated that the residual connections and attention mechanisms of CLIP-ViT enable the model output to be mathematically represented as a sum of vectors over layers, attention heads, and tokens, along with contributions from MLPs and the CLS token. Each vector corresponds to the contribution of a specific token attended to by a particular attention head in a specific layer. (ii) These contribution vectors exist within the same shared imagetext representation space, allowing the CLIP text encoder to interpret each vector individually via text. ", "page_idx": 0}, {"type": "text", "text": "Extending this approach to other transformer-based image encoders presents several challenges. Popular models like DeiT [32], DINO-ViT [7, 22], and Swin [17] lack a corresponding text encoder to interpret the component contributions. Additionally, extracting the contribution vectors corresponding to these components is not straightforward, as they are often not explicitly computed during the forward pass of the model. Other complications include diverse attention mechanisms such as grid attention, block attention (in MaxViT), and windowed/shifted windowed attention (in Swin), as well as various linear transformations like pooling, downsampling, and patch merging applied to the residual streams between attention blocks. These differences necessitate a fresh mathematical analysis for each model architecture, followed by careful application of necessary transformations to the intermediate output of each component to determine its contribution to the final representation. To address these challenges, we propose our framework (described in Figure 1) to identify roles of components in general ViTs. ", "page_idx": 0}, {"type": "image", "img_path": "Vhh7ONtfvV/tmp/03c68e579e71521be354fff813496c1199ae8101985520f8375e1a6580fa4f06.jpg", "img_caption": ["Figure 1: (Left) Workflow: The first step (REPDECOMPOSE) is to decompose a representation $_{z}$ into contributions from its model components $c_{i}$ after being transformed by residual transformations like LayerNorm, linear projections, resampling, patch merging and so on. The second step (COMPALIGN) aligns each contribution to CLIP space using a set of linear maps $f_{0},f_{1},\\ldots,f_{n}$ on the corresponding contributions $c_{0},c_{1},\\ldots,c_{n}$ . We can then interpret these aligned contributions using the CLIP text encoder. (Right) Applications of our method: (a) Visualizing contributions of each token through a specific component using a joint token-component decomposition (b) Retrieving images that are close matches of the reference image (on top) with respect to a given image feature like pattern, person, or location "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "First, we automate the decomposition of the representation by leveraging the computational graph created during the forward pass. This results in a drop-in function, REPDECOMPOSE, that can decompose any representation into contributions vectors from model components simply by calling it on the representation. Since this method operates on the computational graph, it is agnostic to the specifics of the model implementation and thus applicable to a variety of model architectures. ", "page_idx": 1}, {"type": "text", "text": "Secondly, we introduce an algorithm, COMPALIGN, to map each component contribution vector to the image representation space of a CLIP model. We train these linear maps with regularizations so that these maps preserve the roles of the individual components while also aligning the model\u2019s image representation with CLIP\u2019s image representation. This allows us to map each contribution vector from any component to CLIP space, where they can be interpreted through text using a CLIP text encoder. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Thirdly, we observe that there is often no straightforward one-to-one mapping between model components and common image features such as shape, pattern, color, and texture. Sometimes, a single component may encode multiple features, while multiple components may be required to fully encode a single feature. To address this, we propose a scoring function that assigns an importance score to each component-feature pair. This allows us to rank components based on their importance for a given feature, and rank features based on their importance for a given component. ", "page_idx": 2}, {"type": "text", "text": "Using this ranking, we proceed to analyze diverse vision transformers such as DeiT, DINO, Swin, and MaxViT, in addition to CLIP, in terms of their components and the image features that they are responsible for encoding. We consistently find that many components in these models encode the same feature, particularly in ImageNet pre-trained models. Additionally, individual components in larger models MaxVit and Swin do not respond to any image feature strongly, but can encode them effectively in combination with other components. This diffuse and flexible nature of feature representation underscores the need for interpreting them using a continuous scoring and ranking method as opposed to labelling each component with a well-defined role. We are thus able to perform tasks such as image retrieval, visualizing token contributions, and spurious correlation mitigation by carefully selecting or ablating specific components based on their scores for a given property. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Several studies attempt to elucidate model predictions by analyzing either a subset of input example through heatmaps [27, 30, 31, 18] or a subset of training examples [15, 23, 24]. Nevertheless, empirical evidence suggests that these approaches are often unreliable in real-world scenarios [14, 3]. These methods do not interpret model predictions in relation to the model\u2019s internal mechanisms, which is essential for gaining a deeper understanding of the reliability of model outputs. ", "page_idx": 2}, {"type": "text", "text": "Internal Mechanisms of Vision Models: Our work is closely related to the studies by Gandelsman et al. [11] and Vilas et al. [34], both of which analyze vanilla ViTs in terms of their components and interpret them using either CLIP text encoders or pretrained ImageNet heads. Like these studies, our research can be situated within the emerging field of representation engineering [36] and mechanistic interpretability [6, 5]. Other works [4, 12, 21] focus on interpreting individual neurons to understand vision models\u2019 internal mechanisms. However, these methods often fail to break down the model\u2019s output into its subcomponents, which is crucial for understanding model reliability. Shah et al. [29] examine the direct effect of model weights on output, but do not study the fine-grained role of these components in building the final image representation. Balasubramanian and Feizi [1] focus on expressing CNN representations as a sum of contributions from input regions via masking. ", "page_idx": 2}, {"type": "text", "text": "Interpreting models using CLIP: Many recent works utilize CLIP [25] to interpret models via text. Moayeri et al. [19] align model representations to CLIP space with a linear layer, but it is limited to only the final representation and can not be applied to model components. Oikarinen and Weng [20] annotate individual neurons in CNNs via CLIP, but their method cannot be extended easily to high-dimensional component vectors. COMPALIGN is related to model stitching in which one representation space is interpreted in terms of another by training a map between two spaces [2, 16]. ", "page_idx": 2}, {"type": "text", "text": "3 Decomposing the Final Image Representation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recently, Gandelsman et al. [11] decomposed $z_{\\mathrm{CLS,\\,fin}}$ , the final [CLS] representation of the CLIP\u2019s image encoder as a sum over the contributions from its attention heads, layers and token positions, as well as contributions from the MLPs. In particular, they observe that the last few attention layers have as: $\\begin{array}{r}{z_{\\mathrm{CLS,\\,fin}}=z_{\\mathrm{CLS,\\,init}}+\\sum_{l=1}^{L}c_{l,\\mathrm{MLP}}\\overset{\\cdot}{+}\\sum_{l=1}^{L}\\sum_{h=1}^{H}\\sum_{t=1}^{N}c_{l,h,t}}\\end{array}$ , where $L,\\,H,\\,N$ orrespond to the number of layers, number of attention heads and number of tokens. Here, $c_{l,h,t}$ contribution of token $t$ though attention head $h$ in layer $l$ , while $c_{l,\\mathrm{MLP}}$ denotes the contribution from the MLP in layer $l$ . Due to this linear decomposition, different dimensions can be reduced by summing over them to identify the contributions of tokens or attention heads to the final representation. While this decomposition is relatively simple for vanilla ViTs, it cannot be directly used for general ViT architectures due to use of self-attention variants such as window attention, grid attention, or block attention, combined with operations such as pooling or patch merging on the residual stream. The final representation may also not just be a single $z_{\\mathrm{CLS}}$ , fin but $\\textstyle{\\frac{1}{N}}\\sum_{i=1}^{N}z_{i,\\mathrm{fin}}$ or even $\\begin{array}{r}{\\frac{1}{L}\\sum_{i=1}^{L}z_{\\mathrm{CLS},i}}\\end{array}$ or some combination of the above. ", "page_idx": 2}, {"type": "image", "img_path": "Vhh7ONtfvV/tmp/b32caaf8b76c9ea0802578e852ab9d15322547076533927d1fa91e646047d588.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.1 REPDECOMPOSE: Automated Representation Decomposition for ViTs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We thus seek a general algorithm which can automatically decompose the representation for general ViTs. This can be done via a recursive traversal of the computation graph. Suppose the final representation $_{\\textit{z}}$ can be decomposed into component contributions $\\mathbf{c}_{i,t}$ such that $\\textstyle z=\\sum_{i,t}{\\pmb{c}}_{i,t}$ . Here each $\\boldsymbol{c}_{i,t}$ corresponds to the contribution of a particular token $t$ through some model component $i$ . For convenience, let $\\begin{array}{r}{\\pmb{c}_{i}=\\sum_{t}\\pmb{c}_{i,t}}\\end{array}$ . Then, if given access to the computational graph of $_{z}$ , we can identify potential linear comp onents $c_{i,t}$ by recursively traversing the graph starting from the node which outputs $_{\\textit{z}}$ in reverse order till we hit a non-linear node. The key insight here is that the output of any node which performs a linear reduction (defined as a linear operation which results in a reduction in the number of dimensions) is equivalent to a sum of individual tensors of the same dimension as the output. These tensors can be collected and transformed appropriately during the graph traversal to obtain a list of tensors $c_{i,t}$ , each members of the same vector space as the representation $_{\\textit{z}}$ . This kind of linear decomposition is possible due to the overwhelmingly linear nature of transformers. The high-level logic of REPDECOMPOSE is detailed in Algorithm 1, please refer to Algorithm 2 in the appendix or the code for a more detailed description. We also illustrate the operation of the algorithm on an attention-MLP block in the appendix. ", "page_idx": 3}, {"type": "text", "text": "In practice, the number of components quickly explodes as there are a very large number of potential component divisions for a given model. To make analysis and computation tractable, we restrict it to only the attention heads and MLPs with no finer divisions. We also constrain REPDECOMPOSE to only return the direct contributions of these components to the output. This means that the contribution $c_{i}$ is the direct contribution of component $i$ to $_{\\textit{z}}$ , and does not include its contribution to $_{z}$ via a downstream component $j$ . Additionally, the token $t$ in $\\mathbf{c}_{i,t}$ is present in the input of the component $i$ , and not the input image. In principle, REPDECOMPOSE could return higher order terms such as $c_{j,i}$ which is the contribution of model component $i$ via the downstream component $j$ A full understanding of these higher order terms is essential to get a complete picture of the inner mechanism of a model, however we defer this for future work. ", "page_idx": 3}, {"type": "text", "text": "4 Aligning the component representations to CLIP space ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Having decomposed the representation into contributions from relevant model components, we now aim to interpret these contributions through text using CLIP by mapping them to CLIP space. Formally, given that we have a set of vectors $\\{c_{i}\\}_{i=1}^{N}$ such that $\\sum_{i}^{N}c_{i}=z$ , the final representation of model, we require a set of linear maps $f_{i}$ such that the sum of $\\textstyle\\sum_{i}f_{i}(c_{i})\\,=\\,z_{\\mathrm{CLIP}}$ , the final representation of the CLIP model. Once we have these CLIP aligned vectors, we can proceed to interpret them via text using CLIP\u2019s text encoders. ", "page_idx": 3}, {"type": "table", "img_path": "Vhh7ONtfvV/tmp/1367e7fdea12f81d4a09ac854e28ace8c9d29a0cc7b4d09374fd3e8a59eeb6d6.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison of different methods to map the representation space of ImageNet-1k pre-trained DeiT-B/16 to CLIP image representation space. The green colored texts are exact matches with the top-10 descriptions obtained from the imagenet pretrained embeddings, while the orange colored texts are approximate matches. The match rate is the average fraction of exact matches across all components, while cosine distance is the average cosine distance between the CLIP representations and the transformed model representations on ImageNet "], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "However, from an interpretability standpoint, a few additional constraints on the linear maps are desirable. Consider a component contribution $c_{i}$ and two directions ${\\pmb u},{\\pmb v}$ belonging to the same vector space as $c_{i}$ which represent two distinct features, say shape and texture. Let us further assume that the component\u2019s dominant role is to identify shape, and thus the variance of the projection of $c_{i}$ along $\\textbf{\\em u}$ is higher than that of $\\pmb{v}$ . We want this relative importance of features to be maintained in $f_{i}(\\pmb{c}_{i})$ . Additionally, we also want any two linear maps $f_{i}$ and $f_{j}$ to not change the relative norms of features in components $c_{i}$ and $c_{j}$ . We can express these conditions formally as follows: ", "page_idx": 4}, {"type": "text", "text": "1. Intra-component norm rank ordering: For any two vectors ${\\pmb u},{\\pmb v}$ and a linear map $f_{i}$ such that $\\|u\\|\\leq\\|v\\|$ , we have $\\|f_{i}(\\pmb{u})\\|\\leq\\|f_{i}(\\pmb{v})\\|$ ", "page_idx": 4}, {"type": "text", "text": "2. Inter-component norm rank ordering: For any two vectors ${\\pmb u},{\\pmb v}$ and linear maps $f_{i},f_{j}$ such that $\\lVert\\pmb{u}\\rVert\\leq\\lVert\\pmb{v}\\rVert$ , we have $\\|f_{i}(\\pmb{u})\\|\\leq\\|f_{j}(\\pmb{v})\\|$ ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Both of the above conditions together imply that all linear maps $f_{i}$ must be a scalar multiple of an orthogonal transformation, that is for all $i$ , $\\boldsymbol{f}_{i}^{T}\\boldsymbol{f}_{i}=k\\boldsymbol{I}$ for some constant $k$ . Here, $I$ is the identity transformation. ", "page_idx": 4}, {"type": "text", "text": "The proof is deferred to appendix E. We can now formulate a novel alignment method, COMPALIGN, to map contributions of model components to CLIP space. COMPALIGN minimizes a loss function over $\\{f_{i}\\}_{i=1}^{N}$ to obtain a good alignment between model representation space and CLIP space: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL(\\{f_{i}\\}_{i=1}^{N})=\\mathbb{E}_{\\{c_{i}\\}_{i=1}^{N},z_{\\mathrm{CIP}}}\\left[1-\\cos\\left(\\sum_{i}f_{i}(c_{i}),z_{\\mathrm{CLIP}}\\right)\\right]+\\lambda\\sum_{i}\\|f_{i}^{T}f_{i}-I\\|_{F}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The first term of the objective is the alignment loss, which is the average cosine distance between the CLIP representation $z_{\\mathrm{CLIP}}$ and the transformed model representation $\\sum_{i}f_{i}(\\pmb{c}_{i})$ . It quantifies the directional discrepancy between the two vectors. The second term is the orthogonality regularizer which imposes a penalty if the linear maps $f_{i}$ are not orthogonal, ensuring that the $f_{i}$ adhere closely to the specified conditions. We can now train $f_{i}$ using the above loss function on ImageNet-1k. The training is label-free and can be done even over unlabeled datasets. We obtain $z_{\\mathrm{CLIP}}$ from the CLIP image encoder and $\\{c_{i}\\}_{i=1}^{N}$ from running REPDECOMPOSE on the final representation of the model. ", "page_idx": 4}, {"type": "image", "img_path": "Vhh7ONtfvV/tmp/e8051b41f9d29da78f6348f7c7e22b286ff3adccc0885e92675351f88d70cf7e.jpg", "img_caption": ["Figure 2: Ablation results for various different image encoders. The top-1 ImageNet accuracy is plotted as the layers of the model are increasingly ablated away, starting from the last layer up till the first layer. The circles on the plot represent the endpoints of blocks, the definition of which varies across model architectures. For the vanilla ViT variants, a block is an attention MLP pair, while for SWIN, it is a pair of windowed/shifted windowed attention and an MLP. For MaxVit, this might either be a grid/block attention-MLP pair, or an MBConv block. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Ablation study: We now conduct an ablation study on COMPALIGN. The first naive alignment method is the case where all $f_{i}$ are the same linear map $f$ , without constraints on $f$ , similar to [19]. The second method is a version of COMPALIGN with $\\lambda\\,=\\,0$ , where all $f_{i}$ are different but not trained with the orthogonality regularizer. To compare these methods, we first get a \u201cground truth\u201d description for each model component by using the TEXTSPAN [11] algorithm on the class embedding vectors from the ImageNet pre-trained head. TEXTSPAN retrieves those class embedding vectors along which variance of the component output is maximized, thus yielding descriptions of each component in terms of the top 10 most dominant ImageNet classes. We then use COMPALIGN and the two baselines to map the representations to CLIP space, and apply TEXTSPAN on CLIP embedded ImageNet class vectors to label each model component. We can then compare the descriptions this yields with the \u201cground truth\u201d text description for each head. The results, shown in Tab. 1, indicate that COMPALIGN\u2019s TEXTSPAN descriptions have the most matches to the ImageNet pre-trained descriptions, followed by COMPALIGN with $\\lambda=0$ and the naive single map method. This trend is similar in the average cosine distance between the CLIP representations and the transformed model representations. ", "page_idx": 5}, {"type": "text", "text": "5 Component ablation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To identify the most relevant model layers for downstream tasks, we progressively ablate them and measure the drop in ImageNet classification accuracy. Ablation involves setting a layer\u2019s contribution to its mean value over the dataset. We use the following models from Huggingface\u2019s timm [35] repository: (i) DeiT (ViT-B-16) [32], (ii) DINO (ViT-B-16) [7], (iii) DINOv2 (ViT-B-14) [22], (iv) Swin Base (patch size $=4$ , window size $=7$ ) [17], (v) MaxViT Small [33], along with (vi) CLIP (ViT-B-16) [9] from open_clip [13]. DeiT, Swin, and MaxViT are pretrained on ImageNet with a supervised classification loss, DINO on ImageNet with a self-supervised loss, DINOv2 on LVD-142M with a self-supervised loss, while CLIP is pretrained on a LAION-2B subset with contrastive loss. ", "page_idx": 5}, {"type": "text", "text": "In Fig. 2, we see that for models not trained on ImageNet (CLIP and DINOv2), removing the last few layers quickly drops the accuracy to zero. In contrast, models trained on ImageNet experience a more gradual decline in accuracy, reaching zero only after more than half the layers are ablated. This trend is consistent across both self-supervised (DINO) and classification-supervised (DeiT, SWIN, MaxViT) models. This suggests that ImageNet-trained models encode useful features redundantly across layers for the classification task. Additionally, larger models with more layers, such as MaxVit, show significantly more redundancy, with minimal accuracy impact from ablating the last four layers. Conversely, the first few layers in all models contribute little to the output. Therefore, our analysis in the subsequent sections is focused on the last few layers of each model. ", "page_idx": 5}, {"type": "image", "img_path": "Vhh7ONtfvV/tmp/f2e056660a677eb10b72899651c2f6240fede457c4f18d66b8e3b7232216aba4.jpg", "img_caption": ["Figure 3: Top-3 images retrieved by DeiT components for \u201cforest\u201d and \u201cbeach\u201d ordered according to their relevance for the attribute \u201clocation\u201d. Each column here corresponds to the images returned by the sum of contributions of 3 components, so column $i$ corresponds to components $c_{3i},c_{3i+1},c_{3i+2}$ . A large fraction of components which can recognize the \u201clocation\u201d feature are sorted correctly by the scoring function "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "6 Feature-based component analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now analyze the final representation in terms finer components like attention heads and MLPs, focusing on the last few significant layers. We limit decomposition to 10 layers for DeiT, DINO, and DINOv2, but 12 layers for SWIN and 20 layers for MaxVit due to their greater depth and redundancy across components. We accumulate contributions from the remaining components in a single vector $c_{\\mathrm{init}}$ , expressing $_{\\textit{z}}$ as $c_{\\mathrm{init}}+\\sum_{i}^{N}c_{i}$ , where $N+1$ is the total number of components including $c_{\\mathrm{init}}$ . Here, $N=65$ for DeiT, DI NO, and DINOv2; $N=134$ for SWIN, and $N=156$ for MaxVit. ", "page_idx": 6}, {"type": "text", "text": "We then ask if it is possible to attribute a feature-specific role to each component using an algorithm such as TEXTSPAN [11]. These image features may be low-level (shape, color, pattern) or high-level (such as location, person, animal). However, such roles are not necessarily localized to a single component, but may be distributed among multiple components. Furthermore, each individual component by itself may not respond significantly to a particular feature, but it may jointly contribute to identifying a feature along with other components. Thus, rather than rigidly matching each component with a role, we aim to devise a scoring function which can assign a score to each component - feature pair, which signifies of how important the component is for identifying a given image feature. A continuous scoring function allows us to select multiple components relevant to the feature by sorting the components according to their score. ", "page_idx": 6}, {"type": "text", "text": "We devise this scoring function (described in the appendix in Alg. 3) by looking at the projection of each contribution vector $c_{i}$ onto a vector space corresponding to a certain feature. Suppose we have a feature, \u201cpattern\u201d, that we want to attribute to the components. We first describe the feature in terms of an example set of feature instantiations, such as \u201cspotted\u201d, \u201cstriped\u201d, \u201ccheckered\u201d, and so on. We then embed each of these texts to CLIP space, obtaining a set of embeddings $B$ . We also calculate the CLIP aligned contributions $f_{i}(\\pmb{c}_{i})$ for each component $i$ over an image dataset (ImageNet-1k validation split). Then, the score is simply the correlation between projection of $f_{i}(\\pmb{c}_{i})$ and the projection of $\\sum_{i}f_{i}(\\pmb{c}_{i})$ onto the vector space spanned by $B$ . Intuitively, this measures how closely the component\u2019s contribution correlates with the overall representation. The ", "page_idx": 6}, {"type": "table", "img_path": "Vhh7ONtfvV/tmp/a85fc5eaed3378a3a362df559df12d0ef748319fd9b04a70d488d4ae112342ad.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 2: Spearman\u2019s rank correlation between the orderings induced by CLIP score and component score averaged over a selection of common features ", "page_idx": 6}, {"type": "text", "text": "scores obtained for each component and feature can be used to rank the components according to its importance for a given feature to obtain a component ordering, or to rank the features according to its importance for a specific component to get a feature ordering . ", "page_idx": 6}, {"type": "image", "img_path": "Vhh7ONtfvV/tmp/547f3eaff183dc9b95ee4f90e134eca2083fa9e8ece0982d3e8b364ef20ececd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Top-3 images retrieved by the most significant components for various features relevant to the reference image (displayed on top). The models used are (from left to right) DINO, DeiT, and SWIN. More exhaustive results can be found in appendix H ", "page_idx": 7}, {"type": "text", "text": "6.1 Text based image retrieval ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We can now use our framework to identify components which can retrieve images possessing a certain feature most effectively. Using the scoring function described above, we can identify the top $k$ components $\\{c_{i}\\}_{i=1}^{k}$ which are the most responsive to a given feature $p$ . We can use the cosine similarity of $\\textstyle\\sum_{i=1}^{k}f_{i}(\\pmb{c}_{i})$ to the CLIP embedding of an instantiation $s_{p}$ of the feature $p$ to retrieve the closest matches in ImageNet-1k validation split. In Fig. 3, we show the top 3 images retrieved by different components of the DeiT model for the location instantiation \u201cforest\u201d and \u201cbeach\u201d when sorted according to the component ordering for the \u201clocation\u201d feature. As the component score decreases, the images retrieved by the components grow less relevant. Also note that a significant fraction of components are capable of retrieving relevant images. This further confirms the need for a continuous scoring function which can identify multiple components relevant to a feature. ", "page_idx": 7}, {"type": "text", "text": "To quantitatively verify our scoring function, we devise the following experiment. We first choose a set of common image features such as color, pattern, shape, and location, with a representative set of feature instantiations for each (details in appendix B). The scoring function induces a component ordering for each feature $p$ and feature ordering for each component $i$ . We then compute the cosine similarity $\\begin{array}{r}{\\sin_{i,s_{p}}=\\cos(f_{i}(c_{i}),y_{s_{p},\\mathrm{CLIP}})}\\end{array}$ where $_{\\pmb{y}_{s_{p},\\mathrm{CLP}}}$ is the CLIP text embedding of $s_{p}$ . We can compare this to the cosine similarity $\\mathrm{sim}_{\\mathrm{CLIP},s_{p}}=\\cos(z_{\\mathrm{CLIP}},y_{s_{p},\\mathrm{CLIP}})$ where $z_{\\mathrm{CLIP}}$ is the CLIP image representation. The correlation coefficient between $\\mathrm{sim}_{i,s_{p}}$ and $\\mathrm{sim}_{\\mathrm{CLIP},s_{p}}$ over an image dataset can be viewed as another score which is purely a function of how well the component $i$ can retrieve images matching $s_{p}$ as judged by CLIP. Averaging this correlation coefficient over all $s_{p}$ for a given $p$ yields a \u201cground truth\u201d proxy for our scoring function. We can measure the Spearman rank correlation (which ranges from -1 to 1) between the component (or feature) ordering induced by our scoring function and the ground truth and average it over features (or components). In Tab. 2, we observe that the rank correlation is significantly high for all models for both feature and component ordering. The individual rank correlations for component orderings for common features can be found in Tab. 4. ", "page_idx": 7}, {"type": "text", "text": "6.2 Image based image retrieval ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We can also retrieve images that are similar to a reference image with respect to a specific feature. To do this, we first choose components which are highly significant for the given feature while being comparatively less relevant for other features. Mathematically, for a feature $p\\ \\in\\ P$ , the set of all relevant features, we want to choose component $i$ with score $s_{i,p}$ such that the quantity $\\mathrm{min}_{p^{\\prime}\\in P\\backslash p}\\,s_{i,p}\\,-\\,s_{i,p^{\\prime}}$ is maximised. Intuitively, we want components which have the highest ", "page_idx": 7}, {"type": "text", "text": "Figure 5: Visualization of token contributions as heatmaps for two example images for the DeiT model. The relevant feature and the head most closely associated with the feature is displayed on the bottom of the heatmap, while the feature instantiation is displayed on the top. The layer numbering starts from the last layer (which has index $\\cdot00^{\\circ}$ ). The regions highlighted in red contribute positively to the prediction, while blue regions contribute negatively. More results in appendix I ", "page_idx": 8}, {"type": "text", "text": "gap between $s_{i,p}$ and $s_{i,p^{\\prime}}$ where $p^{\\prime}$ can be any other feature. We can then select a set of $k$ such components $C_{k}$ by sorting over the score gap, and sum them to obtain a feature-specific image representation $\\begin{array}{r}{z_{p}=\\sum_{i\\in C_{k}}{c_{i}}}\\end{array}$ . Now, we can retrieve any image $\\pmb{x}^{\\prime}$ similar in feature $p$ to a reference image $\\textbf{\\em x}$ by computing the cosine similarity between $z_{p}^{\\prime}$ and $z_{p}$ , which are the feature-specific image representations for $\\mathbf{x}^{\\prime}$ and $\\textbf{\\em x}$ . We show a few examples for image based image retrieval in Fig. 4. Here, we tune $k$ to ensure that it is not so small that the retrieved images do not resemble the reference image at all, and not so large that the retrieved images are overall very similar to the reference image. We can see that the retrieved images are significantly similar to the reference image with respect to the given feature, but not similar overall. For example, when the reference image is a handbag with a leopard print, the \u201cpattern\u201d components retrieve images of leopards which have the same pattern, while the \u201cfabric\u201d components return other bags which are made of similar glossy fabric. Similarly, for the ball with a spiral pattern on it, we retrieve images which resemble the spiral pattern in the second row, while they resemble the shape in the third row. ", "page_idx": 8}, {"type": "text", "text": "Note that this experiment only involves the alignment procedure for computing the scores and thereby selecting the component set $C_{k}$ . The process of retrieving the images is based on $z_{p}$ which exists in the model representation space and not CLIP space. This shows that the model inherently has components which (while not constrained to a single role) are specialized for certain properties, and this specialization is not a result of the CLIP alignment procedure. ", "page_idx": 8}, {"type": "text", "text": "6.3 Visualizing token contributions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As discussed in Section 3.1, contribution from a component $i$ can be further decomposed as a sum over contributions from a tokens, so $\\begin{array}{r}{\\pmb{c}_{i}=\\sum_{t}\\pmb{c}_{i,t}}\\end{array}$ . For any particular CLIP text embedding v ector $\\textbf{\\em u}$ corresponding to a realization of some feature $p$ , we have $\\begin{array}{r}{\\dot{\\pmb u^{\\top}}f_{i}(\\pmb{c}_{i})=\\sum_{t}{\\pmb u^{\\top}}f_{i}(\\pmb{c}_{i,t})}\\end{array}$ . We can visualize this token-wise score $\\pmb{u}^{\\top}f_{i}(\\pmb{c}_{i,t})$ as a heat map to know which tokens are the most influential with respect to $\\textbf{\\em u}$ . We show the heat map obtained via this procedure in Fig. 5 for two example images for the DeiT model. The components used for each heat map correspond to the feature being highlighted and are selected using the scoring function we described previously. We can observe that the heatmaps are localized within image portions which correspond to the text description. We also compare our method ", "page_idx": 8}, {"type": "table", "img_path": "Vhh7ONtfvV/tmp/03590989747cfd526652c0de845f848d6363b1830330033633d784a0a9454c2b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: Worst group accuracy and average group accuracy for Waterbirds dataset before and after intervention for various models (format is before $\\rightarrow$ after) ", "page_idx": 8}, {"type": "text", "text": "against zero-shot segmentation methods for ImageNet classes such as GradCAM [28] and Chefer et al. [8] and find that our method outperforms the baselines (see Appendix J). ", "page_idx": 8}, {"type": "text", "text": "6.4 Zero-shot spurious correlation mitigation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We can also use the scoring function to mitigate spurious correlations in the Waterbirds dataset [26] in a zero-shot manner. Waterbirds dataset is a synthesized dataset where images of birds commonly found in water (\u201cwaterbirds\u201d) and land (\u201clandbirds\u201d) are cut out and pasted on top of images of land and water background. For this experiment, we regenerate the Waterbirds dataset following Sagawa et al. [26] but take care to discard background images with birds and thus eliminate label noise. We select the top 10 components for each model which are associated with the \u201clocation\u201d feature but not with the \u201cbird\u201d class following the method we used in Sec. 6.2. We then ablate these components by setting their value to their mean over the Waterbirds dataset. In Tab. 3, we observe a significant increase in the worst group accuracy for all models, accompanied with an increase in the average group accuracy as well. The changes in all four groups can be found in appendix K in Tab. 6. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose a ViT component interpretation framework consisting of an automatic decomposition algorithm (REPDECOMPOSE) to break down the model\u2019s final representation into component contributions and a method (COMPALIGN) to map these contributions to CLIP space for text-based interpretation. We also introduce a continuous scoring function to rank components by their importance in encoding specific features and to rank features within a component. We demonstrate the framework\u2019s effectiveness in applications such as text-based and image-based retrieval, visualizing token-wise contribution heatmaps, and mitigating spurious correlations in a zero-shot manner. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This project was supported in part by a grant from an NSF CAREER AWARD 1942230, ONR YIP award N00014-22-1-2271, ARO\u2019s Early Career Program Award 310902-00001, HR00112090132 (DARPA/ RED), HR001119S0026 (DARPA/ GARD), Army Grant No. W911NF2120076, the NSF award CCF2212458, NSF Award No. 2229885 (NSF Institute for Trustworthy AI in Law and Society, TRAILS), an Amazon Research Award and an award from Capital One. ", "page_idx": 9}, {"type": "text", "text": "Author contributions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Sriram Balasubramanian conceived the main ideas, implemented the algorithms, conducted the experiments, and contributed to writing the paper. Samyadeep Basu contributed to the writing and provided essential advice on the presentation and direction of the paper. Soheil Feizi offered critical guidance on the presentation, writing, and overall direction of the paper. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] S. Balasubramanian and S. Feizi. Towards improved input masking for convolutional neural networks. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1855\u20131865, 2023. doi: 10.1109/ICCV51070.2023.00178.   \n[2] Y. Bansal, P. Nakkiran, and B. Barak. Revisiting model stitching to compare neural representations, 2021.   \n[3] S. Basu, P. Pope, and S. Feizi. Influence functions in deep learning are fragile. CoRR, abs/2006.14651, 2020. URL https://arxiv.org/abs/2006.14651.   \n[4] D. Bau, J. Zhu, H. Strobelt, \u00c0. Lapedriza, B. Zhou, and A. Torralba. Understanding the role of individual units in a deep neural network. CoRR, abs/2009.05041, 2020. URL https: //arxiv.org/abs/2009.05041.   \n[5] T. Bricken, A. Templeton, J. Batson, B. Chen, A. Jermyn, T. Conerly, N. Turner, C. Anil, C. Denison, A. Askell, R. Lasenby, Y. Wu, S. Kravec, N. Schiefer, T. Maxwell, N. Joseph, Z. HatfieldDodds, A. Tamkin, K. Nguyen, B. McLean, J. E. Burke, T. Hume, S. Carter, T. Henighan, and C. Olah. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. https://transformer-circuits.pub/2023/monosemanticfeatures/index.html.   \n[6] N. Cammarata, S. Carter, G. Goh, C. Olah, M. Petrov, L. Schubert, C. Voss, B. Egan, and S. K. Lim. Thread: Circuits. Distill, 2020. doi: 10.23915/distill.00024. https://distill.pub/2020/circuits.   \n[7] M. Caron, H. Touvron, I. Misra, H. J\u00e9gou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021.   \n[8] H. Chefer, S. Gur, and L. Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 782\u2013791, June 2021.   \n[9] M. Cherti, R. Beaumont, R. Wightman, M. Wortsman, G. Ilharco, C. Gordon, C. Schuhmann, L. Schmidt, and J. Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2818\u20132829, 2023.   \n[10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021.   \n[11] Y. Gandelsman, A. A. Efros, and J. Steinhardt. Interpreting CLIP\u2019s image representation via textbased decomposition. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\equiv$ 5Ca9sSzuDp.   \n[12] G. Goh, N. C. \u2020, C. V. \u2020, S. Carter, M. Petrov, L. Schubert, A. Radford, and C. Olah. Multimodal neurons in artificial neural networks. Distill, 2021. doi: 10.23915/distill.00030. https://distill.pub/2021/multimodal-neurons.   \n[13] G. Ilharco, M. Wortsman, R. Wightman, C. Gordon, N. Carlini, R. Taori, A. Dave, V. Shankar, H. Namkoong, J. Miller, H. Hajishirzi, A. Farhadi, and L. Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/zenodo.5143773. If you use this software, please cite it as below.   \n[14] P.-J. Kindermans, S. Hooker, J. Adebayo, M. Alber, K. T. Sch\u00fctt, S. D\u00e4hne, D. Erhan, and B. Kim. The (un)reliability of saliency methods, 2017.   \n[15] P. W. Koh and P. Liang. Understanding black-box predictions via influence functions, 2020.   \n[16] K. Lenc and A. Vedaldi. Understanding image representations by measuring their equivariance and equivalence, 2015. URL https://arxiv.org/abs/1411.5908.   \n[17] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.   \n[18] S. M. Lundberg and S. Lee. A unified approach to interpreting model predictions. CoRR, abs/1705.07874, 2017. URL http://arxiv.org/abs/1705.07874.   \n[19] M. Moayeri, K. Rezaei, M. Sanjabi, and S. Feizi. Text-to-concept (and back) via cross-model alignment, 2023.   \n[20] T. Oikarinen and T.-W. Weng. CLIP-dissect: Automatic description of neuron representations in deep vision networks. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ iPWiwWHc1V.   \n[21] C. Olah, A. Satyanarayan, I. Johnson, S. Carter, L. Schubert, K. Ye, and A. Mordvintsev. The building blocks of interpretability. Distill, 2018. doi: 10.23915/distill.00010. https://distill.pub/2018/building-blocks.   \n[22] M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, R. Howes, P.-Y. Huang, H. Xu, V. Sharma, S.-W. Li, W. Galuba, M. Rabbat, M. Assran, N. Ballas, G. Synnaeve, I. Misra, H. Jegou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski. Dinov2: Learning robust visual features without supervision, 2023.   \n[23] S. M. Park, K. Georgiev, A. Ilyas, G. Leclerc, and A. Madry. Trak: Attributing model behavior at scale, 2023.   \n[24] G. Pruthi, F. Liu, M. Sundararajan, and S. Kale. Estimating training data influence by tracking gradient descent. CoRR, abs/2002.08484, 2020. URL https://arxiv.org/abs/2002. 08484.   \n[25] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020.   \n[26] S. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization, 2020.   \n[27] R. R. Selvaraju, A. Das, R. Vedantam, M. Cogswell, D. Parikh, and D. Batra. Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization. CoRR, abs/1610.02391, 2016. URL http://arxiv.org/abs/1610.02391.   \n[28] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. International Journal of Computer Vision, 128(2):336\u2013359, Oct. 2019. ISSN 1573-1405. doi: 10.1007/s11263-019-01228-7. URL http://dx.doi.org/10.1007/s11263-019-01228-7.   \n[29] H. Shah, A. Ilyas, and A. Madry. Decomposing and editing predictions by modeling model computation. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=rTBR0eqE4G.   \n[30] D. Smilkov, N. Thorat, B. Kim, F. B. Vi\u00e9gas, and M. Wattenberg. Smoothgrad: removing noise by adding noise. CoRR, abs/1706.03825, 2017. URL http://arxiv.org/abs/1706.03825.   \n[31] M. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep networks. CoRR, abs/1703.01365, 2017. URL http://arxiv.org/abs/1703.01365.   \n[32] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J\u00e9gou. Training data-efficient image transformers & distillation through attention, 2021.   \n[33] Z. Tu, H. Talebi, H. Zhang, F. Yang, P. Milanfar, A. Bovik, and Y. Li. Maxvit: Multi-axis vision transformer. ECCV, 2022.   \n[34] M. G. Vilas, T. Schauml\u00f6ffel, and G. Roig. Analyzing vision transformers for image classification in class embedding space. In Advances in Neural Information Processing Systems, volume 36, pages 40030\u201340041, 2023. URL https://proceedings.neurips.cc/paper_ files/paper/2023/file/7dd309df03d37643b96f5048b44da798-Paper-Conference. pdf.   \n[35] R. Wightman. Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019.   \n[36] A. Zou, L. Phan, S. Chen, J. Campbell, P. Guo, R. Ren, A. Pan, X. Yin, M. Mazeika, A.-K. Dombrowski, S. Goel, N. Li, M. J. Byun, Z. Wang, A. Mallen, S. Basart, S. Koyejo, D. Song, M. Fredrikson, J. Z. Kolter, and D. Hendrycks. Representation engineering: A top-down approach to ai transparency, 2023. URL https://arxiv.org/abs/2310.01405. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Our analysis is limited in several ways which we hope to address in future work. Firstly, similar to Gandelsman et al. [11], we only consider the direct contributions from the last few layers, and do not look at the indirect contributions though other components. Secondly, we limit ourselves to decomposition only over attention heads and tokens, while convolutional blocks are not decomposed even if they might admit one. Furthermore, it is still unclear if we can identify certain directions or vector subspaces in the model component contributions which are strongly associated with a certain property. We believe that a detailed analysis of higher order contributions with a more fine-grained decomposition may be key for addressing these challenges. ", "page_idx": 12}, {"type": "text", "text": "B Implementation details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Feature instantiations: We use the following features and corresponding feature instantiations. They are chosen arbitrarily: ", "page_idx": 12}, {"type": "text", "text": "1. color: \u201cblue color\u201d, \u201cgreen color\u201d, \u201cred color\u201d, \u201cyellow color\u201d, \u201cblack color\u201d, \u201cwhite color\u201d   \n2. texture: \u201crough texture\u201d, \u201csmooth texture\u201d, \u201cfurry texture\u201d, \u201csleek texture\u201d, \u201cslimy texture\u201d,   \n\u201cspiky texture\u201d, \u201cglossy texture\u201d   \n3. animal: \u201ccamel\u201d, \u201celephant\u201d, \u201cgiraffe\u201d, \u201ccat\u201d, \u201cdog\u201d, \u201czebra\u201d, \u201ccheetah\u201d   \n4. person: \u201cface\u201d, \u201chead\u201d, \u201cman\u201d, \u201cwoman\u201d, \u201chuman\u201d, \u201carms\u201d, \u201clegs\u201d   \n5. location: \u201csea\u201d, \u201cbeach\u201d, \u201cforest\u201d, \u201cdesert\u201d, \u201ccity\u201d, \u201csky\u201d, \u201cmarsh\u201d   \n6. pattern: \u201cspotted pattern\u201d, \u201cstriped pattern\u201d, \u201cpolka dot pattern\u201d, \u201cplain pattern\u201d, \u201ccheckered   \npattern\u201d   \n7. shape: \u201ctriangular shape\u201d, \u201crectangular shape\u201d, \u201ccircular shape\u201d, \u201coctagon\u201d   \n8. fabric: \u201clinen\u201d, \u201cvelvet\u201d, \u201ccotton\u201d, \u201csilk\u201d, \u201cchiffon\u201d ", "page_idx": 12}, {"type": "text", "text": "Hyperparameters: The aligners are trained with learning rate $=3\\times10^{-4}$ , $\\lambda=1/768$ using the Adam optimizer (with default values for everything else) for upto an epoch on ImageNet validation split. Hyperparameters were loosely tuned for the DeiT model using the cosine similarity as a metric, and then fixed for the rest of the models. We may achieve better performance with more rigorous tuning. The number of components $k$ used for the image-based image retrieval experiment was tuned on an image-by-image basis. It is approximately around 9 for larger models like Swin or MaxVit, and around 3 for the rest. ", "page_idx": 12}, {"type": "text", "text": "Computational resources: The bulk of computation is utilized to compute component contributions and train the aligner. Most of the experiments in the paper were conducted on a single RTX A5000 GPU, with 32GB CPU memory and 4 compute nodes. ", "page_idx": 12}, {"type": "text", "text": "Algorithm 2 REPDECOMPOSE   \nInput: $_{\\textit{z}}$ , the final representation output by the model and the final node in the computational graph. $z.f$ is the function that outputs node $_{z}$   \nOutput: A tree $\\pmb{t}$ consisting of component contributions $^c$ , such that components $\\textstyle\\sum_{c\\in t}c=z$ . The structure of $\\pmb{t}$ is a nested list where each list represents a level in the tree function REPDECOMPOSE $(z)$ if is_nonlinear $(z.f)$ then return $[z]$ else if is_unary $(z.f)$ then \u25b7Function is unary linear $z_{\\mathrm{0}}\\leftarrow z$ .parents() $t_{0}\\gets\\mathrm{REPDECOMPOSE}(z_{0})$ if is_reduction $(z.f)$ then $t_{0,u}\\gets\\mathrm{unbind}(t_{0})$ \u25b7Unbinds each $c\\in t$ along the reduction dimension $f_{d}\\gets\\mathrm{decomp}(z.f)$ \u25b7Returns $f_{d}$ such that $\\textstyle\\sum_{c\\in t_{0,u}}f_{d}(c)=z.f(z_{0})$ return map $(f_{d},t_{0,u})$ \u25b7Maps each $c\\in t_{0,u}$ to $f_{d}(\\boldsymbol{c})$ else return map( $z.f,t^{0})$ \u25b7Maps each element $c\\in t$ to $z.f(c)$ else $\\triangleright z.f$ is binary $z_{\\mathrm{0}}$ , $z_{1}\\leftarrow z$ .parents() \u25b7Get the parents of z in the graph (inputs to z.function) $t_{0},t_{1}\\gets\\mathrm{REPDECOMPOSE}(z_{0})$ , REPDECOMPOSE(z1) fd $,0,f_{d,1}\\gets$ decomp_binary(z.f) \u25b7Returns $f_{d,0}$ , $f_{d,1}$ such that: return [ $\\mathrm{nap}(f_{d,0},t_{0})$ , $\\begin{array}{r l}{\\operatorname*{map}(f_{d,1},t_{1})]}&{\\vdash\\sum_{c\\in t_{0}}f_{d,0}(c)+\\sum_{c\\in t_{1}}f_{d,1}(c)=z.f(z_{0},z_{1})}\\end{array}$ ", "page_idx": 13}, {"type": "image", "img_path": "Vhh7ONtfvV/tmp/e5cb009a43f9a17400735f84879a7f762aeb887438a1f3ab4ad56220e66ff113.jpg", "img_caption": ["D Stepwise breakdown of the operation of RepDecompose on a vanilla attention-MLP block "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 6: Illustration of a simple attention-mlp block. Intermediate tensors marked as $z_{i}$ , non-linear nodes in orange, nodes where a tensor is reduced along a dimension of interest (tokens, attention head, etc) are marked by green borders ", "page_idx": 14}, {"type": "text", "text": "To illustrate the workings of our algorithm, we describe the steps that the RepDecompose algorithm takes on a simple attention-mlp block of a vanilla ViT transformer. Please refer to Figure 6 for the variable names in the following explanation. ", "page_idx": 14}, {"type": "text", "text": "First, we mark (with green borders in the figure) the computational nodes in which the contributions of the components get reduced. For the tokenwise contributions, this is the \u2019matmul\u2019 operation, while for the attention head contributions, it is the \u2019concat\u2019 operation. We also detach the graph at the input of each component to stop the algorithm from gathering only the direct contributions and not any higher-order contribution terms arising from the interaction between multiple components. Let the RepDecompose function be denoted by $d(.)$ which takes in a representation and returns an array of contributions. Here, $n$ , wherever it appears, is the number of contributions in the decomposition of the input. The map $(f,d(z))$ operation applies $f$ to every contribution vector in $d(z)$ . At each step, it is ensured that the sum of all contribution vectors/tensors in the RHS is equal to the vector/tensor that is being decomposed in the LHS via the distributive property for linear transformations. Then: ", "page_idx": 14}, {"type": "text", "text": "1. $\\begin{array}{r}{d(z)\\,=\\,\\operatorname*{map}(\\lambda x.\\frac1{\\sigma}(x-\\,\\frac\\mu n),d(z_{1}))}\\end{array}$ (LayerNorm linearized as in Gandelsman et al [1], $n$ here is the number of contributions in $d(z_{1})$ )   \n2. $d(z_{1})=(d(z_{2}),d(z_{3}))$   \n3. $\\begin{array}{r}{d(z_{2})=\\operatorname*{map}(\\lambda x.x W_{1}+\\frac{b_{1}}{n},d(z_{4}))}\\end{array}$ ( $n$ here is the number of contributions in $d(z_{4})$ )   \n4. $d(z_{4})=[z_{4}]$ (stops when it hits a non-linear node)   \n5. $d(z_{3})=(d(z_{5}),d(z_{6}))$   \n6. $\\begin{array}{r}{d(z_{6})=\\operatorname*{map}(\\lambda x.x W_{o}+\\frac{b_{o}}{n},d(z_{7}))}\\end{array}$ ( $n$ here is the number of contributions in $d(z_{7})$ )   \n7. $d(z_{7})=[[\\mathrm{zeropad}(v)$ for $v\\in u]$ for $u\\in d(z_{8})]$ (Concatenation of a tensor along a dimension can be expressed as a sum of zero-padded tensors)   \n8. $d(z_{8})~=~[[u v$ for $(u,v)\\;\\in\\;\\operatorname{zip}(U.\\operatorname{cols},V.\\operatorname{rows})]$ for $U\\,\\in\\,d(z_{9})$ for $V\\ \\in\\ d(z_{10})]$ (via the distributive property for matrix multiplication)   \n9. $d(z_{9})=[z_{9}]$ (stops when it hits a non-linear node)   \n10. $\\begin{array}{r}{d(z_{10})=\\operatorname*{map}(\\lambda x.x W_{v}+\\frac{b_{v}}{n},d(z_{11}))}\\end{array}$ $n$ here is the number of contributions in $d(z_{11})$ )   \n11. $\\begin{array}{r}{d(z_{11})=\\operatorname*{map}(\\lambda x.\\frac{1}{\\sigma}(x-\\frac{\\mu}{n}),d(z_{12}))}\\end{array}$ $n$ here is the number of contributions in $d(z_{12})$ )   \n12. $d(z_{12})=[z_{12}]=[z_{5}]$ (Stopped since the comp graph is detached, if not the algorithm would return higher-order terms.) ", "page_idx": 14}, {"type": "text", "text": "E Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. From the first condition on intra-component rank ordering, for any two vectors ${\\pmb u},{\\pmb v}$ and a linear map $f_{i}$ , if $\\|u\\|\\leq\\|v\\|$ then $\\|f_{i}(\\pmb{u})\\|\\leq\\|f_{i}(\\pmb{v})\\|$ . We first show that $f_{i}$ is a scalar multiple of an isometry. ", "page_idx": 15}, {"type": "text", "text": "If $\\|\\pmb{u}\\|=\\|\\pmb{v}\\|\\neq0$ , then both $\\|u\\|\\leq\\|v\\|$ and $\\lVert\\pmb{v}\\rVert\\leq\\lVert\\pmb{u}\\rVert$ . This implies that $\\|f_{i}(\\pmb{u})\\|\\leq\\|f_{i}(\\pmb{v})\\|$ and $\\|f_{i}(\\pmb{v})\\|\\leq\\|f_{i}(\\pmb{u})\\|$ . Therefore, $\\|f_{i}(\\pmb{u})\\|=\\|f_{i}\\pmb{v}\\|$ , when $\\|\\pmb{u}\\|=\\|\\pmb{v}\\|$ . Given the input space of the transformation as $U$ , we choose a unit vector $u_{\\mathrm{unit}}\\in U$ . Let\u2019s assume $\\Vert f_{i}(u_{\\mathrm{unit}})\\Vert=c$ . With the above result, we can use the following equality $\\Vert\\frac{u}{\\Vert u\\Vert}\\Vert=\\Vert u_{\\mathrm{unit}}\\Vert$ to obtain the following: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\frac{f_{i}(\\pmb{u})}{\\|\\pmb{u}\\|}\\right\\|=\\left\\|f_{i}\\left(\\frac{\\pmb{u}}{\\|\\pmb{u}\\|}\\right)\\right\\|=\\|f_{i}(\\pmb{u}_{\\mathrm{unit}})\\|=c,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|f_{i}(\\pmb{u})\\|=c\\|\\pmb{u}\\|\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, the linear transformation $f_{i}$ is a scalar multiple of an isometry. Now consider two linear maps $f_{i}$ and $f_{j}$ such that $\\|f_{i}(\\pmb{u})\\|\\,=\\,c_{i}\\|\\pmb{u}\\|$ and $\\|{\\bar{f_{j}}}\\pmb{u}\\|=c_{j}\\|\\pmb{u}\\|$ . From the second condition on inter-component rank ordering, for any two vectors ${\\pmb u},{\\pmb v}$ and linear maps $f_{i},f_{j}$ , if $\\|u\\|\\leq\\|v\\|$ then $\\|f_{i}(\\pmb{u})\\|\\leq\\|f_{j}(\\pmb{v})\\|$ . This implies that if ${\\pmb u}={\\pmb v}$ , $\\|f_{i}(\\pmb{u})\\|=\\|f_{j}(\\pmb{u})\\|$ . However, this can only happen when $\\|f_{i}(\\bar{\\boldsymbol{u}})\\|=c\\|\\boldsymbol{u}\\|$ for some constant $c$ for all $f_{i}\\not\\in\\gamma i$ . ", "page_idx": 15}, {"type": "text", "text": "With this, let\u2019s denote $\\frac{f_{i}}{c}$ as an isometry. One of the general property of isometries are that they preserve the inner product between two vectors $\\textbf{\\em u}$ and $\\pmb{v}$ . First we prove that isometries preserve the inner product, which we will then use to prove the orthogonality of $\\frac{f_{i}}{c}$ . Given two vectors $\\textbf{\\em u}$ and $\\pmb{v}$ , their inner product can be expressed as the following: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{u}^{T}\\pmb{v}=\\frac{1}{4}(\\|\\pmb{u}+\\pmb{v}\\|^{2}+\\|\\pmb{u}-\\pmb{v}\\|^{2})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "An isometry by definition preserves the norm of the vectors i.e. $\\|f_{i}(\\pmb{u})\\|=\\|\\pmb{u}\\|$ and $\\|f_{i}(\\pmb{v})\\|=\\|\\pmb{v}\\|$ . Due to this property, we can express the following relations: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|f_{i}(\\boldsymbol{u}+\\boldsymbol{v})\\|=\\|\\boldsymbol{u}+\\boldsymbol{v}\\|,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|f_{i}(\\boldsymbol{u}-\\boldsymbol{v})\\|=\\|\\boldsymbol{u}-\\boldsymbol{v}\\|,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can express $f_{i}(\\boldsymbol{u})^{T}f_{i}(\\boldsymbol{v})$ as the reduction from Eq.(3): ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{i}(\\pmb{u})^{T}f_{i}(\\pmb{v})=\\frac{1}{4}(\\|f_{i}(\\pmb{u})+f_{i}(\\pmb{v})\\|^{2}+\\|f_{i}(\\pmb{u})-f_{i}(\\pmb{v})\\|^{2}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{i}(\\pmb{u})^{T}f_{i}(\\pmb{v})=\\frac{1}{4}(\\|f_{i}(\\pmb{u}+\\pmb{u})\\|^{2}+\\|f_{i}(\\pmb{u}-\\pmb{v})\\|^{2}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next we substitute the relations from Eq.(4) and Eq.(5) to Eq.(7) to obtain the following inner product preservation property: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{i}(\\pmb{u})^{T}f_{i}(\\pmb{v})=\\frac{1}{4}(\\|\\pmb{u}+\\pmb{v}\\|^{2}+\\|\\pmb{u}-\\pmb{v}\\|^{2})=\\pmb{u}^{T}\\pmb{v}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next we use the inner product preservation property to prove the orthogonality of $\\frac{f_{i}}{c}$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{i}(\\pmb{u})^{\\top}f_{i}(\\pmb{v})=c^{2}\\pmb{u}^{\\top}\\pmb{v},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{u}^{\\top}\\left(\\frac{1}{c^{2}}\\boldsymbol{f}_{i}^{\\top}\\boldsymbol{f}_{i}-I\\right)\\pmb{v}=0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "From 10, we can infer the orthogonality of $\\frac{f_{i}}{c}$ which leads to the following result: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{i}^{\\top}f_{i}=c^{2}I=k I,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Scoring function ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Algorithm 3 Scoring function for attributing properties to components   \nInput: $Z$ , the image representation output by the model over $n$ images with dimension $d$ (shape: $n\\times d)$ ; $C$ , the contribution of a particular component of interest (shape: $n\\times d_{\\mathrm{,}}$ ); $B$ , the set of $k$ feature vectors that represent a given feature (shape: $k\\times d)$   \nOutput: A score that signifies the importance of the component for the given feature function COMPATTRIBUTE $(C,Z,B)$ $B\\leftarrow$ orthogonalize $(B)$ sZ \u2190ZB\u22a4 sC \u2190CB\u22a4 $r\\gets$ correlation_coefficient( $s_{Z}$ , sC, dim=0) return mean(r) ", "page_idx": 16}, {"type": "table", "img_path": "Vhh7ONtfvV/tmp/9d70ad536ef87edd6350d14f51686a98a4e70526a7c882078faaf43f1b24f45a.jpg", "table_caption": [], "table_footnote": ["Table 4: Spearman rank correlation for various common properties "], "page_idx": 17}, {"type": "text", "text": "H Image-based Image retrieval ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "Vhh7ONtfvV/tmp/f1b4ab53a8e0c953c7f34071a4b0bc14dd66325fc5465e871ee599fd0de261ad.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 7: Top-3 images retrieved by the most significant components for various relevant properties for DINO ", "page_idx": 18}, {"type": "image", "img_path": "Vhh7ONtfvV/tmp/1e63eee25717df55178d3e88240e0a562b36babdea0b64c7ba95894b4813c41b.jpg", "img_caption": ["Figure 8: Top-3 images retrieved by the most significant components for various relevant properties for SWIN "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "Vhh7ONtfvV/tmp/1d76a7899ed9c18a68d6790a3c411e2d09971ab04e9870ec8103f2daa0e03f0b.jpg", "img_caption": ["Figure 9: Top-3 images retrieved by the most significant components for various relevant properties for DeiT "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "Vhh7ONtfvV/tmp/c3676f8597f1423baa633a62cc24749e3c2a1d3baa9f008eaf4da98ea42f7c38.jpg", "img_caption": ["Figure 10: Top-3 images retrieved by the most significant components for various relevant properties for MaxViT "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "Vhh7ONtfvV/tmp/1ef158cac8a70f2bf6597a44f17fec8cd8d383446b2a70d2a32d0205134eddbf.jpg", "img_caption": ["Figure 11: Top-3 images retrieved by the most significant components for various relevant properties for DINOv2 "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "Vhh7ONtfvV/tmp/bfb23ab473d0b2ecd05f2edb37f8e48029d954c3d99e676d514b82e4ffc5c881.jpg", "img_caption": ["Figure 12: Top-3 images retrieved by the most significant components for various relevant properties for CLIP "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "I Property visualization via token decomposition ", "text_level": 1, "page_idx": 21}, {"type": "image", "img_path": "Vhh7ONtfvV/tmp/3aa06739f5b99ee1c6b339d3bbc2aa40fbaa96a05e3edceea7b1c48da10f8759.jpg", "img_caption": ["Figure 13: Visualization of token contributions for CLIP "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "Vhh7ONtfvV/tmp/09fb900092c3535b58d09ec184e968f58e91b0aaa33bbbe8a120050b6445e065.jpg", "img_caption": ["Figure 14: Visualization of token contributions for DINO "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "Vhh7ONtfvV/tmp/fa53f2577f575ed8dc4c862457608fc62a1bf482c1d5f856fd2aeafe3a063561.jpg", "img_caption": ["Figure 15: Visualization of token contributions for SWIN "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "Vhh7ONtfvV/tmp/0a6895acae99b213468ed479600c9ef5645017872bb5db0bc0511aa61e1632a0.jpg", "img_caption": ["Figure 16: Visualization of token contributions for MaxVit "], "img_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "Vhh7ONtfvV/tmp/79dc5a2fd5ef8f44b2f3bdddd6263cecefbbfdadf2a57352f335a2ea0c46ec51.jpg", "table_caption": [], "table_footnote": [""], "page_idx": 23}, {"type": "table", "img_path": "Vhh7ONtfvV/tmp/792f37a05365cc83d40b2ba8f95ffda80137c72c154e836f43e8ed962d731dfb.jpg", "table_caption": ["K Zero-shot spurious correlation mitigation "], "table_footnote": ["Table 6: All group accuracies on the Waterbirds dataset before and after component ablation "], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper introduces a framework to interpret the internal components of vision transformers via text. This is aptly discussed and presented in the abstract, introduction and the main paper. Our framework is supported by empirical and theoretical findings in the paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have added a separate Limitations section in the appendix of the paper. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have provided a proof for one Theorem in our paper, in the Appendix section. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have added all the experimental details and the corresponding hyperparameters. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We include the code used in the supplementary Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The data, hyperparameters, optimizer and other relevant experimental detail have been presented in the main paper, appendix, and code. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: The empirical results are reported using a suitable precision so that variable factors do not have a significant effect on the numbers. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: These details are provided in the Appendix. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper provides a framework to understand the internal mechanisms of vision transformers. It does not have any ethical issues. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The work focuses on understanding the internals of neural networks, and thus does not have any direct impact on society. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: We do not pre-train models or release a dataset in this paper. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Yes, the models and datasets have credited and cited properly. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]