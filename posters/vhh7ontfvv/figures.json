[{"figure_path": "Vhh7ONtfvV/figures/figures_1_1.jpg", "caption": "Figure 1: (Left) Workflow: The first step (REPDECOMPOSE) is to decompose a representation z into contributions from its model components ci after being transformed by residual transformations like LayerNorm, linear projections, resampling, patch merging and so on. The second step (COMPALIGN) aligns each contribution to CLIP space using a set of linear maps fo, f1,..., fn on the corresponding contributions c0, c1, . . ., cn. We can then interpret these aligned contributions using the CLIP text encoder. (Right) Applications of our method: (a) Visualizing contributions of each token through a specific component using a joint token-component decomposition (b) Retrieving images that are close matches of the reference image (on top) with respect to a given image feature like pattern, person, or location", "description": "The figure illustrates the workflow of the proposed framework. It consists of two main steps: REPDECOMPOSE and COMPALIGN. REPDECOMPOSE decomposes the final representation into contributions from different model components. COMPALIGN aligns these contributions to CLIP space for interpretation via text. The right side of the figure shows two applications of the framework: visualizing token contributions and image retrieval.", "section": "1 Introduction"}, {"figure_path": "Vhh7ONtfvV/figures/figures_3_1.jpg", "caption": "Figure 1: (Left) Workflow: The first step (REPDECOMPOSE) is to decompose a representation z into contributions from its model components ci after being transformed by residual transformations like LayerNorm, linear projections, resampling, patch merging and so on. The second step (COMPALIGN) aligns each contribution to CLIP space using a set of linear maps fo, f1,..., fn on the corresponding contributions c0, C1, . . ., Cn. We can then interpret these aligned contributions using the CLIP text encoder. (Right) Applications of our method: (a) Visualizing contributions of each token through a specific component using a joint token-component decomposition (b) Retrieving images that are close matches of the reference image (on top) with respect to a given image feature like pattern, person, or location", "description": "This figure illustrates the workflow of the proposed framework. The left panel shows the two main steps: decomposing the representation into contributions from different components (REPDECOMPOSE) and aligning these contributions to CLIP space for textual interpretation (COMPALIGN). The right panel demonstrates three applications of the framework: visualizing token contributions, image retrieval based on image features, and image retrieval based on text descriptions.", "section": "1 Introduction"}, {"figure_path": "Vhh7ONtfvV/figures/figures_5_1.jpg", "caption": "Figure 2: Ablation results for various different image encoders. The top-1 ImageNet accuracy is plotted as the layers of the model are increasingly ablated away, starting from the last layer up till the first layer. The circles on the plot represent the endpoints of blocks, the definition of which varies across model architectures. For the vanilla ViT variants, a block is an attention MLP pair, while for SWIN, it is a pair of windowed/shifted windowed attention and an MLP. For MaxVit, this might either be a grid/block attention-MLP pair, or an MBConv block.", "description": "This ablation study shows the impact of removing layers from different vision transformer models on ImageNet classification accuracy.  The accuracy is plotted as layers are progressively removed, starting from the last layer. The plot shows how many layers are required to significantly reduce the performance of each model.  The different models (DeiT, CLIP, DINO, DINOv2, Swin, MaxVit) show different levels of robustness to layer removal, reflecting architectural differences.", "section": "Component ablation"}, {"figure_path": "Vhh7ONtfvV/figures/figures_6_1.jpg", "caption": "Figure 3: Top-3 images retrieved by DeiT components for \u201cforest", "description": "This figure shows the top 3 images retrieved by DeiT components for the features \"forest\" and \"beach\". Each column represents images returned by a set of three components, ordered according to their relevance scores for the attribute \"location\". The images demonstrate how the model effectively utilizes component contributions to correctly identify location features in images.", "section": "6 Feature-based component analysis"}, {"figure_path": "Vhh7ONtfvV/figures/figures_7_1.jpg", "caption": "Figure 3: Top-3 images retrieved by DeiT components for \u201cforest\u201d and \u201cbeach\u201d ordered according to their relevance for the attribute \u201clocation\u201d. Each column here corresponds to the images returned by the sum of contributions of 3 components, so column i corresponds to components C3i, C3i+1, C3i+2. A large fraction of components which can recognize the \u201clocation\u201d feature are sorted correctly by the scoring function", "description": "This figure shows the top 3 images retrieved by DeiT components for the location features \"forest\" and \"beach\".  The images are ordered by the component's relevance score for the location attribute.  Each column represents the combined contribution of three components.  The figure demonstrates that the scoring function effectively ranks components by their importance in identifying location features.", "section": "6 Feature-based component analysis"}, {"figure_path": "Vhh7ONtfvV/figures/figures_14_1.jpg", "caption": "Figure 6: Illustration of a simple attention-mlp block. Intermediate tensors marked as zi, non-linear nodes in orange, nodes where a tensor is reduced along a dimension of interest (tokens, attention head, etc) are marked by green borders", "description": "This figure illustrates a simple attention-MLP block in a vanilla Vision Transformer. It shows the flow of tensors (z1, z2,...z12), the linear (blue) and non-linear (orange) operations in the block and the nodes where tensors are reduced along specific dimensions. This visualization helps to understand the step-wise operation of the REPDECOMPOSE algorithm described in the paper, which decomposes a representation into contributions from different model components.", "section": "3.1 REPDECOMPOSE: Automated Representation Decomposition for ViTs"}, {"figure_path": "Vhh7ONtfvV/figures/figures_18_1.jpg", "caption": "Figure 4: Top-3 images retrieved by the most significant components for various features relevant to the reference image (displayed on top). The models used are (from left to right) DINO, DeiT, and SWIN. More exhaustive results can be found in appendix H", "description": "This figure shows the top 3 images retrieved by the most significant components of DINO, DeiT, and Swin models for different features. Each column represents images retrieved for a specific feature (location, pattern, person, fabric, color, shape), and the rows represent the top 3 components. The reference image, on the top, is used as a basis for selecting the components and features. The selection of components is performed by using a scoring function described in the paper to determine which components are most responsive to each feature.  The appendix H contains more detailed results.", "section": "6.2 Image based image retrieval"}, {"figure_path": "Vhh7ONtfvV/figures/figures_18_2.jpg", "caption": "Figure 4: Top-3 images retrieved by the most significant components for various features relevant to the reference image (displayed on top). The models used are (from left to right) DINO, DeiT, and SWIN. More exhaustive results can be found in appendix H", "description": "This figure shows the top 3 images retrieved by the most significant components for different features. The reference image is displayed at the top.  The models used are DINO, DeiT, and Swin, and more results are available in Appendix H.  The figure demonstrates the ability of the method to retrieve images sharing specific features with the reference image, highlighting the model's ability to isolate and utilize feature-specific components.", "section": "6.2 Image based image retrieval"}, {"figure_path": "Vhh7ONtfvV/figures/figures_19_1.jpg", "caption": "Figure 4: Top-3 images retrieved by the most significant components for various features relevant to the reference image (displayed on top). The models used are (from left to right) DINO, DeiT, and SWIN. More exhaustive results can be found in appendix H", "description": "This figure shows the top 3 images retrieved by the most significant components for various features relevant to the reference image for three different vision transformer models: DINO, DeiT, and SWIN.  Each column represents images retrieved using a combination of components identified as most relevant to a specific feature (like \"location\", \"pattern\", etc.) based on the proposed scoring function. The images are ordered by the relevance score, showcasing the models' ability to retrieve images that visually match the specific characteristics of the reference image.", "section": "6.2 Image based image retrieval"}, {"figure_path": "Vhh7ONtfvV/figures/figures_19_2.jpg", "caption": "Figure 4: Top-3 images retrieved by the most significant components for various features relevant to the reference image (displayed on top). The models used are (from left to right) DINO, DeiT, and SWIN. More exhaustive results can be found in appendix H", "description": "This figure shows the top 3 images retrieved by the most significant components for various features relevant to the reference image.  The reference images are displayed at the top of each column.  The models used are DINO, DeiT, and SWIN, each represented by a column. This demonstrates the image-based image retrieval capability of the proposed method, highlighting how different models identify and retrieve images with similar features.", "section": "6.2 Image based image retrieval"}, {"figure_path": "Vhh7ONtfvV/figures/figures_20_1.jpg", "caption": "Figure 4: Top-3 images retrieved by the most significant components for various features relevant to the reference image (displayed on top). The models used are (from left to right) DINO, DeiT, and SWIN. More exhaustive results can be found in appendix H", "description": "This figure visualizes the results of image-based image retrieval using the proposed framework.  For each of three different vision transformer models (DINO, DeiT, and Swin), it shows the top three images retrieved for several different features from a given reference image. The selection of components was based on a scoring function that identifies the components most relevant to each feature. The figure aims to demonstrate the effectiveness of the framework in identifying components capable of retrieving images with specific features.", "section": "6.2 Image based image retrieval"}, {"figure_path": "Vhh7ONtfvV/figures/figures_20_2.jpg", "caption": "Figure 4: Top-3 images retrieved by the most significant components for various features relevant to the reference image (displayed on top). The models used are (from left to right) DINO, DeiT, and SWIN. More exhaustive results can be found in appendix H", "description": "This figure shows top 3 images retrieved by the most significant components for different image features.  The reference image is displayed at the top of each column, and the three columns show results for DINO, DeiT, and Swin Transformer models respectively. Each column shows images that have common features with the reference image. For instance, the first column contains images that feature a person on the beach, similar to the reference image.", "section": "6.2 Image based image retrieval"}, {"figure_path": "Vhh7ONtfvV/figures/figures_21_1.jpg", "caption": "Figure 13: Visualization of token contributions for CLIP", "description": "The figure visualizes token contributions for CLIP model using heatmaps.  Two example images are shown with their corresponding heatmaps for different features. Each heatmap highlights the tokens (parts of the image) that are most relevant to a specific feature like 'animal', 'location', 'shape', 'person', 'pattern', etc. Red indicates positive contribution, blue indicates negative contribution. The layer and attention head contributing the most are also specified.", "section": "I Property visualization via token decomposition"}, {"figure_path": "Vhh7ONtfvV/figures/figures_21_2.jpg", "caption": "Figure 14: Visualization of token contributions for DINO", "description": "This figure visualizes the token contributions for the DINO model.  It shows heatmaps highlighting the positive (red) and negative (blue) contributions of individual tokens for various features. Two example images are used: one of a camel near pyramids, and the other a man in a striped shirt on a beach.  Each image's heatmaps are displayed alongside the specific feature being highlighted and the attention head/layer responsible. This visualization helps demonstrate how specific tokens within the model contribute to the recognition of certain image features.", "section": "6 Feature-based component analysis"}, {"figure_path": "Vhh7ONtfvV/figures/figures_22_1.jpg", "caption": "Figure 15: Visualization of token contributions for SWIN", "description": "The figure visualizes token contributions as heatmaps for two example images using the Swin Transformer model.  The heatmaps highlight which image tokens contribute positively (red) or negatively (blue) to the prediction of specific features (e.g., \"animal,\" \"location,\" \"shape\"). Each heatmap shows the contributions from specific attention heads within different layers, indicating which parts of the image are most important for identifying each feature. The figure helps to understand the model's internal mechanisms by visualizing how individual components contribute to feature recognition.", "section": "Visualizing token contributions"}, {"figure_path": "Vhh7ONtfvV/figures/figures_22_2.jpg", "caption": "Figure 16: Visualization of token contributions for MaxVit", "description": "This figure visualizes the token contributions as heatmaps for two example images using the MaxVit model.  The heatmaps show which tokens contribute positively (red) or negatively (blue) to the prediction of specific features (e.g., 'animal', 'location', 'shape', 'person', 'pattern'). Each heatmap is accompanied by the model components identified as most relevant for that specific feature.  The layer and attention head are also displayed for each contribution.", "section": "I Property visualization via token decomposition"}]