[{"figure_path": "81YIt63TTn/tables/tables_1_1.jpg", "caption": "Table 2: Performance on 8 Discriminative Tasks (ROBERTa) and 4 Generative Tasks (Qwen-14B)", "description": "This table presents the results of the Twin-Merging method and its baselines on eight discriminative tasks using the RoBERTa model and four generative tasks using the Qwen-14B model.  It shows the average normalized scores for each method, allowing for comparison across different tasks and highlighting the performance improvements achieved by Twin-Merging compared to existing methods. The \"Pretrained\" and \"Fine-tuned\" rows provide the lower and upper bounds for performance, respectively.", "section": "4.1 Merging Experiment"}, {"figure_path": "81YIt63TTn/tables/tables_3_1.jpg", "caption": "Table 1: Merging without parameter interference and merging between similar tasks both cause performance degradation (Notice: these two experiments use different datasets).", "description": "This table presents the results of two experiments designed to isolate the impact of parameter interference on model merging performance.  The first experiment compared the performance of merging models trained on non-overlapping tasks (i.e., tasks with no shared parameters) versus fine-tuned individual models.  The second experiment compared the performance of merging models trained on similar tasks to the performance of fine-tuned individual models.  The results demonstrate that even without parameter interference, merging models still leads to performance degradation, indicating that other factors besides parameter interference are at play.", "section": "3.1 Analysis of the Performance Gap in Model Merging"}, {"figure_path": "81YIt63TTn/tables/tables_4_1.jpg", "caption": "Table 2: Performance on 8 Discriminative Tasks (ROBERTa) and 4 Generative Tasks (Qwen-14B)", "description": "This table presents the performance comparison of different model merging methods on 8 discriminative tasks using the RoBERTa model and 4 generative tasks using the Qwen-14B model.  It includes the performance of pretrained and fine-tuned models as baselines, and several other merging methods such as weight averaging, task arithmetic, Ties-Merging, and DARE.  The results are shown in terms of the average normalized score, which is calculated by normalizing the scores of each task to the fine-tuned model's performance on that task.", "section": "4.1 Merging Experiment"}, {"figure_path": "81YIt63TTn/tables/tables_5_1.jpg", "caption": "Table 2: Performance on 8 Discriminative Tasks (ROBERTa) and 4 Generative Tasks (Qwen-14B)", "description": "This table presents the performance comparison of different model merging methods on 8 discriminative tasks using the RoBERTa model and 4 generative tasks using the Qwen-14B model.  The results are presented as average normalized scores, with the fine-tuned model performance serving as the upper bound (100.00) and the pre-trained model as the lower bound. The table highlights the performance improvement achieved by Twin-Merging compared to other merging methods.", "section": "4.1 Merging Experiment"}, {"figure_path": "81YIt63TTn/tables/tables_6_1.jpg", "caption": "Table 3: Performance and Cost on 8 CV Tasks (ViT-B/32)", "description": "This table presents the results of experiments conducted on eight computer vision (CV) tasks using the ViT-B/32 model. It compares the average normalized score, additional time cost, and VRAM usage of various model merging methods, including the proposed Twin-Merging method, against baselines such as pretrained, fine-tuned, Weight Averaging, Task Arithmetic, Ties-Merging, AdaMerging, and Surgery.  The results showcase the performance and efficiency gains of Twin-Merging compared to other methods.", "section": "4.1 Merging Experiment"}, {"figure_path": "81YIt63TTn/tables/tables_6_2.jpg", "caption": "Table 4: Our method scalability (72B)", "description": "This table presents the results of experiments conducted to evaluate the scalability of the Twin-Merging method using a larger language model with 72 billion parameters.  It shows the performance of the pretrained model, the fine-tuned model, Task Arithmetic, and Twin Merging on two tasks: TruthfulQA and BBQ.  The results demonstrate the effectiveness of Twin-Merging in handling larger models, maintaining strong performance.", "section": "4 Experiments"}, {"figure_path": "81YIt63TTn/tables/tables_6_3.jpg", "caption": "Table 5: Performance (un-normalized\u00b2) on unseen tasks", "description": "This table presents the results of experiments conducted on unseen tasks, evaluating the performance of different model merging methods.  The results are un-normalized because the corresponding expert models for these unseen tasks were not available. The table shows the average un-normalized scores for different methods on the QNLI+MNLI+RTE and MMLU benchmarks.  The results demonstrate the generalization capabilities of the Twin-Merging method even in situations where the task-specific knowledge is limited.", "section": "4.2 Unseen Generalization"}, {"figure_path": "81YIt63TTn/tables/tables_7_1.jpg", "caption": "Table 6: Ablation study of Twin-Merging", "description": "This table presents the results of ablation studies conducted on the Twin-Merging method. It shows the impact of different components of the method on the overall performance. By removing components like the dynamic experts or the shared expert, or by directly applying dynamic merging to a pre-trained model, the performance decreases significantly. This highlights the importance of all components of Twin-Merging in achieving its high performance. The results are reported for both RoBERTa and Qwen models, demonstrating that the observations hold across different model architectures.", "section": "4.3 Ablation Studies"}, {"figure_path": "81YIt63TTn/tables/tables_9_1.jpg", "caption": "Table 7: Compute-performance tradeoff in the generative benchmark.", "description": "This table compares the compute-performance tradeoff of three different methods for generative tasks: Multi-Task Learning, Model Merging, and Twin-Merging. It shows the number of training tokens used, the training cost (in terms of time), the inference cost (per 1000 items), and the performance achieved by each method.  Twin-Merging demonstrates a significant performance improvement with minimal training cost and a slightly increased inference time compared to Multi-Task Learning and Model Merging.", "section": "4 Experiments"}, {"figure_path": "81YIt63TTn/tables/tables_20_1.jpg", "caption": "Table 2: Performance on 8 Discriminative Tasks (ROBERTa) and 4 Generative Tasks (Qwen-14B)", "description": "This table presents the results of different model merging methods on 8 discriminative tasks using the RoBERTa model and 4 generative tasks using the Qwen-14B model.  It shows the average normalized scores for each method, along with the scores for the pretrained and fine-tuned models as a baseline for comparison.  The average normalized score is calculated as the average of the normalized scores across all tasks, allowing for comparison across different tasks with potentially different scoring scales.", "section": "4.1 Merging Experiment"}, {"figure_path": "81YIt63TTn/tables/tables_21_1.jpg", "caption": "Table 9: The detail statistics of different merging performance on 4 generative tasks. Bold numbers indicate the best-averaging performance across different model merging methods. Underlines indicate the second best performance of each task across different model merging methods.", "description": "This table presents a detailed comparison of the performance of various model merging methods across four generative tasks: MMLU, TruthfulQA, BBQ, and CNN-DailyMail.  The average normalized scores are shown, with bold values indicating the best performance for each task and underlined values indicating the second-best performance. The table offers a more granular view of the results presented in Table 2, providing a deeper understanding of the relative strengths and weaknesses of different methods on different tasks.", "section": "Main Results"}, {"figure_path": "81YIt63TTn/tables/tables_21_2.jpg", "caption": "Table 10: Performance of LLaMA-7B", "description": "This table presents the average performance and inference time of two model merging methods, namely Task-Arithmetic and Twin-Merging, when using the LLaMA-7B model.  It shows that Twin Merging achieves significantly higher average performance (88.18) compared to Task-Arithmetic (69.89), while only having a marginal increase in inference time (198s vs 186s). This highlights the effectiveness of Twin Merging in improving multi-task performance without a significant computational overhead.", "section": "Main Results"}, {"figure_path": "81YIt63TTn/tables/tables_21_3.jpg", "caption": "Table 11: Our method extensibility to other model merging methods", "description": "This table demonstrates the effectiveness of integrating Twin-Merging with other existing model merging methods. It shows that combining Twin-Merging with Weight Average, Task-Arithmetic, or Ties-Merging leads to significant performance improvements on both RoBERTa and Qwen models, indicating that Twin-Merging is a versatile method that can be combined with other techniques to further enhance model merging performance.", "section": "4.1 Merging Experiment"}, {"figure_path": "81YIt63TTn/tables/tables_23_1.jpg", "caption": "Table 12: Performance of group-wise variant.", "description": "This table shows the average normalized scores and inference times for three different model merging methods: Task-Arithmetic, Twin-Merging, and a group-wise variant of Twin-Merging. The group-wise variant is a modification of Twin-Merging that processes data in groups rather than individually, aiming for improved inference efficiency.  The results indicate a tradeoff between accuracy and speed. While Twin-Merging achieves the highest accuracy, the group-wise variant offers a faster inference time, albeit with a slight reduction in accuracy compared to the standard Twin-Merging.", "section": "4.6 Compression and Speed Analysis"}]