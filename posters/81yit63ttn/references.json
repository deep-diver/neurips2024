{"references": [{"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "publication_date": "2019-06-01", "reason": "This paper introduces BERT, a foundational model for many NLP tasks, which is used as a backbone model in the current research."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a model that bridges image and text understanding, providing a strong baseline for vision tasks used in the experiments."}, {"fullname_first_author": "Mitchell S. Matena", "paper_title": "Merging models with Fisher-weighted averaging", "publication_date": "2022-01-01", "reason": "This paper proposes a model merging method using Fisher weighting, providing a baseline technique compared against in the current work."}, {"fullname_first_author": "Prateek Yadav", "paper_title": "Ties-Merging: Resolving interference when merging models", "publication_date": "2023-01-01", "reason": "This paper introduces Ties-Merging, addressing interference issues in model merging, which is directly compared and contrasted to the current model."}, {"fullname_first_author": "Enneng Yang", "paper_title": "AdaMerging: Adaptive model merging for multi-task learning", "publication_date": "2024-01-01", "reason": "This paper introduces AdaMerging, another model merging method that dynamically adapts to various tasks, serving as a key comparison point."}]}