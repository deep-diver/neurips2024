[{"type": "text", "text": "Private Algorithms for Stochastic Saddle Points and Variational Inequalities: Beyond Euclidean Geometry ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Raef Bassily ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Cristobal Guzman ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Computer Science & Engineering   \nTranslational Data Analytics Institute (TDAI) The Ohio State University bassily.1@osu.edu ", "page_idx": 0}, {"type": "text", "text": "Inst. for Mathematical and Comput. Eng. Fac. de Matematicas and Esc. de Ingenieria Pontificia Universidad Catolica de Chile crguzmanp@uc.cl ", "page_idx": 0}, {"type": "text", "text": "Michael Menart ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Computer Science & Engineering The Ohio State University \\* Department of Computer Science, University of Toronto VectorInstitute menart.2@osu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this work, we conduct a systematic study of stochastic saddle point problems (SSP) and stochastic variational inequalities (SVI) under the constraint of $(\\epsilon,\\delta)$ differential privacy (DP) in both Euclidean and non-Euclidean setups. We first consider Lipschitz convex-concave SSPs in the $\\ell_{p}/\\ell_{q}$ setup, $p,q\\in[\\bar{1},2]$ . That is, we consider the case where the primal problem has an $\\ell_{p}$ -setup (i.e., the primal parameter is constrained to an $\\ell_{p}$ bounded domain and the loss is $\\ell_{p}$ -Lipschitz with respect to the primal parameter) and the dual problem has an $\\ell_{q}$ setup. Here, we obtain a bound of $\\begin{array}{r}{\\tilde{O}\\big(\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\epsilon}\\big)}\\end{array}$ on the strong SP-gap, where $n$ is the number of samples and $d$ is the dimension. This rate is nearly optimal for any $p,q\\in[1,2]$ Without additional assumptions, such as smoothness or linearity requirements, prior work under DP has only obtained this rate when $p=q=2$ (i.e., only in the Euclidean setup). Further, existing algorithms have each only been shown to work for specific settings of $p$ and $q$ and under certain assumptions on the loss and the feasible set, whereas we provide a general algorithm for DP SSPs whenever $p,q\\in$ [1, 2]. Our result is obtained via a novel analysis of the recursive regularization algorithm. In particular, we develop new tools for analyzing generalization, which may be of independent interest. Next, we turn our attention towards SVIs with a monotone, bounded and Lipschitz operator and consider $\\ell_{p}$ -setups, $p\\in[1,2]$ Here, we provide the first analysis which obtains a bound on the strong Vi-gap $\\begin{array}{r}{\\tilde{O}\\big(\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\epsilon}\\big)}\\end{array}$ For $p\\,-\\,1\\,=\\,\\Omega(1)$ , this rate is nar optimal due to existing lower bounds. To obtain this result, we develop a modified version of recursive regularization. Our analysis builds on the techniques we develop for SSPs as well as employing additional novel components which handle difficulties arising from adapting the recursive regularization framework to SVIs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Stochastic saddle point problems (SsP), are an increasingly important part of the machine learning toolkit. These problems model optimization settings with an inherent min-max structure, and for this reason are also referred to as stochastic min-max optimization problems. Concretely, the goal is to find an approximate solution of the following problem defined over a convex-concave loss, ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w\\in\\mathcal{W}}\\operatorname*{max}_{\\theta\\in\\Theta}\\Big\\{F_{\\mathcal{D}}(w,\\theta):=\\mathbb{E}_{x\\sim\\mathcal{D}}[f(w,\\theta;x)]\\Big\\},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathcal{D}$ is an unknown distribution for which we have access to an i.i.d. sample $S$ .Problems of this kind have important applications in stochastic optimization [NJLS09, JNT11, ZL15], federated learning [MSS19], distributionally robust learning [YLMJ22, $Z Z Z^{+}24\\mathrm{a}$ , ZB24], reinforcement learning $[\\mathrm{DSL}^{+}18]$ , and algorithmic fairness $[\\mathrm{ABD^{+}}18\\$ ,WM19]. ", "page_idx": 1}, {"type": "text", "text": "Closely related to saddle point problems are stochastic variational inequalities (SVIs). Given a monotone operator, $G_{\\mathcal{D}}(z):=\\underset{x\\sim\\mathcal{D}}{\\bar{\\mathbb{E}}}\\left[g(z;x)\\right]$ , the objective is to approximate the point $z^{*}\\in{\\mathcal{Z}}$ ,where ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\langle G_{\\mathcal{D}}(z^{*}),z^{*}-z\\rangle\\leq0,\\ \\forall z\\in\\mathcal{Z}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Stochastic saddle point problems can be easily related to variational inequalities by observing that the saddle operator (an operator closely related to the gradient) of a convex-concave function is monotone. While SSPs and SVIs are closely related, it can be the case that a problem which can be formulated as a monotone SVI is not easily cast as a convex-concave SSP [JN19]. ", "page_idx": 1}, {"type": "text", "text": "Parallel to the above, the problem of privacy has become increasingly important in the big data era. In this regard, the notion of differential privacy has arisen as the premier standard. Stochastic optimization problems are a natural target for privacy concerns due to the fact that they are frequently formulated using a dataset of (potentially sensitive) individual data records. For many such problems, the constraint of differential privacy necessitates fundamentally new rates and techniques, and as such the formal characterization of these problems is an important task. ", "page_idx": 1}, {"type": "text", "text": "Thus far, work on differentially private SSPs and SVIs has focused primarily on Euclidean settings. However, a number of important, including many of those referenced at the start of this section, are naturally formulated in other geometries. Prior to this work, the optimal utility rate for DP SSPs was known only in Euclidean and polyhedral settings. For SVIs, the best achievable utility was unknown in any geometry (including Euclidean), at least under canonical utility measures. In this work, we provide the first systematic study of SSPs and SVIs in general geometries. The new analysis tools we develop lead to optimal rates for a number of these important setups. ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this work, we provide the first systematic study of stochastic saddle point problems and variational inequalities in both Euclidean and non-Euclidean geometries. Our first results pertain to stochastic saddle point problems where the primal problem has an $\\ell_{p}$ -setup and the dual problem has an $\\ell_{q}$ -setup, where $p,q\\in[1,2]$ . Here we assume the convex-concave loss is Lipschitz. We generalize the recursive regularization framework developed in previous works to more handle non-Euclidean geometries [AZ18, BGM23]. At the heart of this extension is a fundamentally new analysis of the generalization properties of this algorithm. The issue of generalization has in fact been a key issue at the heart of many other works studying SSPs [LYYY21, OPZZ22, BGM23], as the presence of a supremum in the strong SP-gap accuracy measure (see Eqn. (4)) breaks more traditional generalization techniques. In contrast to prior work, our generalization technique works by avoiding entirely any generalization bound for the strong gap itself. Rather, we introduce a new accuracy measure measure which, when used in conjunction with the recursive regularization algorithm, eventually translates into a strong gap guarantee. Our technique stands in particular contrast to [BGM23], which is thus far the only work in the DP literature to obtain optimal strong SP-gap rates in the Euclidean setting and also uses recursive regularization. However, their technique fundamentally relies on a McDiarmids style concentration bound that is worse by a $p o l y(d)$ factor in non-Euclidean setups such as the $\\ell_{1}$ setting [Pan08]. Using these new techniques, we provide the first analysis which obtains the near optimal rate of $\\begin{array}{r}{\\tilde{O}\\left(\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\epsilon}\\right)}\\end{array}$ on the strong SP-gap for any $p,q\\in[1,2]$ .Our algorthm achieves this rate i $\\begin{array}{r}{\\tilde{O}\\big(\\operatorname*{min}\\left\\{\\frac{n^{2}\\epsilon^{1.5}}{\\sqrt{d}},n^{3/2}\\right\\}\\big)}\\end{array}$ number of gradint evalations. We note that the near optimalty of this rat is established by lower bounds for DP stochastic convex optimization, which is a special case of DP SSPs [BGN21, AFKT21]. Previously, comparable rates on the strong gap had only been obtained in thecasewhere $p=q=2$ or under strong additional assumptions [BGM23, ZB24, GGP24]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Next, we consider DP stochastic variational inequalities with a monotone, bounded, and Lipschitz operator. We adapt the recursive regularization framework even further and again leverage a novel generalizaton analysis Here we obtain h rate $\\begin{array}{r}{\\tilde{O}\\left(\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\epsilon}\\right)}\\end{array}$ on the strong VI-gap (see Eqn. (5)) inthe $\\ell_{p}$ setting, $p\\in[1,2]$ . Our algorithm again achieves thisrate in $\\begin{array}{r}{\\tilde{O}\\big(\\operatorname*{min}\\left\\{\\frac{n^{2}\\epsilon^{1.5}}{\\sqrt{d}},n^{3/2}\\right\\}\\big)}\\end{array}$ ( , na/2) number of gradient evaluations. This is the first result to obtain the near optimal convergence rate on the strong VI-gap for $p-1=\\Omega(1)$ , which notably includes the Euclidean case. The corresponding lower bound for $p=2$ was established in [BG23], and we provide a simple extension of their technique to the case where $p-1=\\Omega(1)$ . See Appendix E. Finally, for the setting $p=2$ , we show that our rate can be achieved in a near linear number of gradient evaluations by leveraging acceleration techniques for Lipschitz and strongly monotone variational inequalities. ", "page_idx": 2}, {"type": "text", "text": "1.2Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Differentially private stochastic optimization now has a broad body of work spanning over a decade [JKT12, BST14, JT14, TTZ15, BFTT19, FKT20, AFKT21]. Such work has rigorously characterized the problem of stochastic convex optimization in a variety of geometries. In $\\ell_{p}$ setups, for $p\\in[1,2]$ , it is now known that the optimal rate is $\\tilde{O}(\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\epsilon})$ for such problems AFKT21, BGN1] It has also been shown that additional improvements are possible in the $\\ell_{1}$ setting under smoothness assumptions. The study of stochastic saddle point problems under differential privacy is much less developed, but has nonetheless attracted a surge of recent interest $[\\mathrm{YHL}^{+}22$ , ZTOH22, BGM23, GGP24]. Without privacy, optimal $O(1/{\\sqrt{n}})$ guarantees on the strong SP-gap have long been known [NY78]. With privacy,the (near) otimal $\\begin{array}{r}{\\tilde{O}\\big(\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\epsilon}\\big)}\\end{array}$ rate was obtaind only rently and then only in the Euclidean setting [BGM23]. In fact, work on DP-SSPs has focused largely on the case where $p=q=2$ , despite the fact that important problems are naturally formulated in other geometries. In particular, the case where $q=1$ has important applications in distributionally robust optimization, federated learning, and algorithmic fairness. In this regard, the works [GGP24, ZB24] have recently studied DP SSPs with $q=1$ . The work [GGP24] studies the $\\ell_{1}/\\ell_{1}$ setting when the loss is additionally assumed tobe mooth and the constraint set is polyedral; they achieved the rate $\\begin{array}{r}{\\tilde{O}\\big(\\frac{1}{\\sqrt{n}}+\\frac{1}{(n\\epsilon)^{1/2}}\\big)}\\end{array}$ We note that smoothness is fundamentally necessary in achieving this dimension independent rate, as otherwise existing lower boundsof $\\begin{array}{r}{\\tilde{\\Omega}\\Big(\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\epsilon}\\Big)}\\end{array}$ holdforuhproblsAKTThork [ZB24] studied the problem of differentially private worst-group risk minimization, which is closely related to DP-SSPs in the $\\ell_{1}/\\ell_{2}$ setting, but requires the loss to have a specific linear structure with respect to the dual parameter. For $\\ell_{1}/\\ell_{2}$ saddle point problems having this structure, their result implies a rate of $\\begin{array}{r}{O\\big(\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{{n}\\epsilon}\\big)}\\end{array}$ on the strong gap. ", "page_idx": 2}, {"type": "text", "text": "Work on SVIs is less developed. Non-privately, the optimal strong VI-gap rate of $\\textstyle O({\\frac{1}{\\sqrt{n}}})$ was established in [JNT11] (although related techniques trace back to [NY78]). Work on differentially private variational inequalities is limited to the work [BG23]. In the Euclidean setup, this work achieved arate of $\\textstyle\\left({\\frac{1}{n^{1/3}}}+{\\frac{\\sqrt{d}}{n^{2/3}\\epsilon}}\\right)$ On thestrong VI-gap under DP. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we detail preliminaries for stochastic saddle point problems and differential privacy. Both SSPs and SVIs share a similar structure, which we detail first. Throughout, we use $[w,\\theta]$ to denote the concatenation of the vectors $w$ and $\\theta$ . For a function $f$ ,we let $\\nabla f$ denote an arbitrary subgradientselectionof $f$ . Finally, we let $\\mathsf{U n i f}(U)$ denote the uniform distribution over the set $U$ ", "page_idx": 2}, {"type": "text", "text": "Stochastic Monotone Operators. Let $\\mathcal{X}$ be some abstract data domain and let $S\\sim{\\mathcal{D}}^{n}$ for $n>0$ and $\\mathcal{D}$ some unknown distribution over $\\mathcal{X}$ . Let $\\Vert\\cdot\\Vert$ be some norm and $\\|\\cdot\\|_{*}$ its dual. We consider some compact convex parameter space $\\mathcal{Z}\\subseteq\\mathbb{R}^{d}$ of diameter $B$ with respect to $\\Vert\\cdot\\Vert$ . Let $\\boldsymbol{B}_{\\parallel\\cdot\\parallel}^{d}(\\boldsymbol{r})$ denote the $d$ -dimensional ball of radius $r>0$ w.r.t. $\\|\\cdot\\|$ centered on zero. We assume there exists ", "page_idx": 2}, {"type": "text", "text": "$L\\,>\\,0$ such that $g:\\mathcal{Z}\\times\\mathcal{X}\\mapsto\\mathcal{B}_{\\|\\cdot\\|_{*}}^{d}(L)$ is a bounded operator and that for any $x\\,\\in\\,\\mathcal{X},\\,g(\\cdot;x)$ is monotone. That is, $\\forall z_{1},z_{2}\\in\\mathcal{Z}$ it holds that $\\langle g(z_{1};x)-g(z_{2};x),z_{1}-z_{2}\\rangle\\geq0$ . We define the empirical and population operators as $\\begin{array}{r}{G_{S}(z)=\\frac{1}{n}\\sum_{i=1}^{n}g(z;x_{i})}\\end{array}$ and $G_{\\mathcal{D}}(\\boldsymbol{z})=\\underset{\\boldsymbol{x}\\sim\\mathcal{D}}{\\mathbb{E}}\\left[g(\\boldsymbol{z};\\boldsymbol{x})\\right]$ ", "page_idx": 3}, {"type": "text", "text": "Stochastic Saddle Point Problems. SSPs have the following structure in addition to the above. Let $d_{w},d_{\\theta}\\,\\geq\\,0$ such that $d_{w}+d_{\\theta}=d$ .We assume $\\mathcal{Z}$ is the product of the convex compact sets $\\mathcal{W}\\subseteq\\mathbb{R}^{d_{w}}$ and $\\Theta\\subseteq\\mathbb{R}^{d_{\\theta}}$ equipped with norms $\\Vert\\cdot\\Vert_{w}$ and ${\\big\\|}\\cdot{\\big\\|}_{\\theta}$ and having diameters $B_{w}$ and $B_{\\theta}$ respectively. Then $\\mathcal{Z}=\\mathcal{W}\\times\\Theta$ . We let $\\|[w,\\theta]\\|=\\sqrt{\\|w\\|_{w}^{2}+\\|\\theta\\|_{\\theta}^{2}}$ , and thus the diameter of $\\mathcal{Z}$ satisfies $B\\leq\\sqrt{B_{w}^{2}+B_{\\theta}^{2}}$ note the geometric mean of two norms is always a norm. ", "page_idx": 3}, {"type": "text", "text": "In SSPs, we consider the case where the monotone operator is the saddle operator of a convexconcave loss function $f:\\mathcal{W}\\times\\Theta\\times\\mathcal{X}\\mapsto\\mathbb{R}$ . The saddle operator is defined as $g(w,\\theta;x)\\;=\\;$ $[\\nabla_{w}f(w,\\theta;x)$ \uff0c $-\\nabla_{\\theta}f(w,\\theta;x)]$ and is always monotone if $f$ is convex-concave. We also define the corresponding population loss and empirical loss functions as $F_{\\mathcal{D}}(w,\\theta)\\,=\\,\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[f(w,\\theta;x)\\right]$ and $\\begin{array}{r}{F_{S}(w,\\theta)\\,=\\,\\frac{1}{n}\\sum_{x\\in S}\\,f(w,\\theta;x)}\\end{array}$ respectively. The boundedness asumption on $g$ means $f$ is $L$ -Lipschitz. Concretely, $\\forall w_{1},w_{2}\\in\\mathcal{W}$ and $\\forall\\theta_{1},\\theta_{2}\\in\\Theta$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n|f(w_{1},\\theta_{1};x)-f(w_{2},\\theta_{2};x)|\\leq L\\left\\|[w_{1},\\theta_{1}]-[w_{2},\\theta_{2}]\\right\\|\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Under such assumptions, a solution for problem (1) always exists [Sio58], and is referred to as the saddle point. Further, given an SSP (1), we will denote a saddle point as $[w^{\\ast},\\theta^{\\ast}]$ ", "page_idx": 3}, {"type": "text", "text": "The utility of an approximation to the saddle point is characterized by the strong SP-gap. Given a (randomized) algorithm $\\boldsymbol{\\mathcal{A}}$ withoutput $[A_{w}({\\cal S}\\bar{)},A_{\\theta}({\\cal S})]\\in\\mathcal{W}\\times\\Theta$ , this is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{Gapsp}(A)}&{=}&{\\underset{A,S}{\\mathbb{E}}\\left[\\underset{\\theta\\in\\Theta}{\\operatorname*{max}}\\left\\{F_{D}(A_{w}(S),\\theta)\\right\\}-\\underset{w\\in\\mathcal{W}}{\\operatorname*{min}}\\left\\{F_{D}(w,A_{\\theta}(S))\\right\\}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For notational convenience, we define the following function which is closely related to the S-gap. $\\begin{array}{r}{\\widehat{\\mathrm{Gap}}_{\\mathsf{S P}}(\\bar{w},\\bar{\\theta})=\\operatorname*{max}_{\\theta\\in\\Theta}\\big\\{F_{\\mathcal{D}}(\\bar{w},\\theta)\\big\\}-\\operatorname*{min}_{w\\in\\mathcal{W}}\\big\\{F_{\\mathcal{D}}(w,\\bar{\\theta})\\big\\}.}\\end{array}$ Usefully, this function is known to be Lipschitz whenever $f$ is Lipschitz. ", "page_idx": 3}, {"type": "text", "text": "Fact 1. ([BGM23]) If $f$ is $L$ -Lipschitz then ${\\widehat{\\mathrm{Gap}}}_{\\mathsf{S P}}\\,i s\\,{\\sqrt{2}}L.$ Lipschitz. ", "page_idx": 3}, {"type": "text", "text": "Finally, we define $\\ell_{p}/\\ell_{q}$ saddle point problems as those which, in addition to the above, also have the following structure. Assume $\\|\\cdot\\|_{w}=\\|\\cdot\\|_{p}$ and $\\|\\cdot\\|_{\\theta}=\\|\\cdot\\|_{q}$ and that $\\mathcal{W}$ and $\\Theta$ have diameters bounded by $B_{w}$ and $B_{\\theta}$ with respect to $\\|\\cdot\\|_{p}$ and ${\\|\\cdot\\|_{q}}$ respectively. We assume that for any $x\\in\\mathscr{X}$ that $f(\\cdot,\\cdot;x)$ is $L_{w}$ Lipschitz in its first parameter w.r.t. ${\\big\\|}\\cdot{\\big\\|}_{p}$ and $L_{\\theta}$ Lipschitz in its second parameter w.r.t. ${\\|\\cdot\\|_{q}}$ . Note this implies an overall Lipschitz parameter, as per Eqn. (3), of $L\\leq\\sqrt{L_{w}^{2}+L_{\\theta}^{2}}$ ", "page_idx": 3}, {"type": "text", "text": "Stochastic Variational Inequalities. For SVIs, in addition to the assumption that the monotone operator is $L$ -bounded, we will also assume it is $\\beta$ -Lipschitz. The objective for stochastic variational inequalities is to find an approximation of the (population) equilibrium point $z^{*}$ ,where $z^{*}$ is characterized by Eqn. (2). We refer to such a solution as an equilibrium point. For approximate solutions, the quality of the approximation is characterized by the strong VI-gap: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{Gapv}_{1}(\\boldsymbol{A})}&{=}&{\\underset{\\boldsymbol{A},\\boldsymbol{S}}{\\mathbb{E}}\\left[\\underset{\\boldsymbol{z}\\in\\mathcal{Z}}{\\operatorname*{max}}\\left\\lbrace\\left\\langle\\boldsymbol{G}_{\\mathcal{D}}(\\boldsymbol{z}),\\boldsymbol{A}(\\boldsymbol{S})-\\boldsymbol{z}\\right\\rangle\\right\\rbrace\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that it is not true in general that the VI-gap bounds the SP-gap even when the monotone operator in question is the saddle operator of some convex-concave loss (see Fact 3 in Appendix A). However, in such a case it is true that the equilibrium point of the of the SVI is the saddle point of the correspondingSSP. ", "page_idx": 3}, {"type": "text", "text": "We say that an operator $g$ is $\\mu$ -strongly monotone if for any $z_{1},z_{2}\\quad\\in\\quad\\mathcal{Z}$ $\\begin{array}{r}{\\langle g(z_{1};x)-g(z_{2};x),z_{1}-z_{2}\\rangle\\geq\\frac{\\mu}{2}\\|z_{1}-z_{2}\\|}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "Stability.  Important to our analysis will be the notion of uniform stability [BE02]. ", "page_idx": 3}, {"type": "text", "text": "Definition 1. A randomized algorithm $A:\\mathcal{X}^{n}\\mapsto\\mathcal{W}\\times\\Theta$ satisfies $\\Delta$ -uniformargumentstability (UAS) if for any pair of adjacent datasets $S,S^{\\prime}\\in\\mathcal{X}^{n}$ it holds that $\\underset{A}{\\mathbb{E}}\\left[\\lVert A(S)-A(S^{\\prime})\\rVert\\right]\\leq\\Delta$ ", "page_idx": 3}, {"type": "text", "text": "The notion of strong convexity is often important in analyzing stability. A function $\\psi:\\mathcal{Z}\\mapsto\\mathbb{R}$ is $\\mu$ strongly convex w.r.t. $\\lVert\\cdot\\rVert$ if for any $z,z^{\\prime}\\in\\bar{z}$ one has $\\begin{array}{r}{\\psi(\\bar{z})\\!-\\!\\bar{\\psi}(z^{\\prime})\\geq\\langle\\nabla\\psi(z^{\\prime}),z-z^{\\prime}\\rangle\\!+\\!\\frac{\\mu}{2}\\|z\\!-\\!z^{\\prime}\\|^{2}}\\end{array}$ We call a function $F:\\mathcal{W}\\times\\Theta\\mapsto\\mathbb{R}$ $\\mu$ -strongly-convex/strongly-concave (SC/SC) if for any $\\theta\\in\\Theta$ and $w\\in\\Theta$ the functions $F(\\cdot,\\theta)$ and $-F(w,\\cdot)$ are $\\mu$ -strongly-convex. ", "page_idx": 4}, {"type": "text", "text": "Notably, adding a SC/SC regularizer leads to stability properties. The following fact results from a more general result for regularized SVIs; see Lemma 5 in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1. Let $\\psi:\\mathcal{W}\\times\\Theta\\mapsto\\mathbb{R}$ be $\\mu$ -strongly-convex/strongly-concave w.rt. $\\Vert\\cdot\\Vert_{w}$ and ${\\big\\|}\\cdot{\\big\\|}\\theta$ Then the algorithm whichretuns the saddle point of $\\begin{array}{r}{(w,\\theta)\\mapsto\\frac{1}{n}\\sum_{z\\in S}f(w,\\theta;z)+\\psi(w,\\theta)}\\end{array}$ $\\bigl(\\frac{2L}{\\mu n}\\bigr)$ -UAS. ", "page_idx": 4}, {"type": "text", "text": "Differential Privacy (DP) [DMNS06]. An algorithm $\\boldsymbol{\\mathcal{A}}$ is $(\\epsilon,\\delta)$ -differentially private if for all datasets $S$ and $S^{\\prime}$ differing in one data point and all events $\\mathcal{E}$ in the range of the $\\boldsymbol{\\mathcal{A}}$ , we have, $\\begin{array}{r}{\\mathbb{P}\\left(A(S)\\in\\mathcal{E}\\right)\\leq e^{\\epsilon}\\mathbb{P}\\left(A(S^{\\bar{\\prime}})\\in\\mathcal{E}\\right)+\\delta.}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "2.1 Example of Non-Euclidean SSP ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "One important example of non-Euclidean SSPs arises from the problem of minimizing the worst case risk over multiple populations. This problem has arisen in group distributionally robust optimization to name just one of many applications [SGJ22, $Z Z Z^{+}24b$ , NMG24]. Let $f:\\mathcal{W}\\times\\mathcal{X}\\mapsto\\mathbb{R}$ and $\\mathcal{W}$ have a standard $\\ell_{p}$ setup. Consider $k$ distributions, $\\mathcal{D}_{1},\\ldots,\\mathcal{D}_{k}$ , and the goal of selecting a model $w\\in\\mathscr{W}$ which guarantees the lowest worst-case risk for the $k$ distributions above: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w\\in\\mathcal{W}}\\operatorname*{max}_{j\\in[k]}\\mathbb{E}_{x_{j}\\sim\\mathcal{D}_{j}}[f(w;x_{j})]=\\operatorname*{min}_{w\\in\\mathcal{W}}\\operatorname*{max}_{\\theta\\in\\Delta}\\sum_{j=1}^{k}\\theta^{(j)}\\mathbb{E}_{x_{j}\\sim\\mathcal{D}_{j}}[f(w;x_{j})],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where here $\\Delta$ denotes the standard $k$ -dimensional simplex, and the equality above holds by the maximum principle for convex functions [Bau58]. Given that the feasible set for $\\theta$ is a simplex, it is natural to endow this space with the $\\ell_{1}$ -geometry. We thus end up with a SSP problem in $\\ell_{p}/\\ell_{1}$ setup. ", "page_idx": 4}, {"type": "text", "text": "3 A New Analysis for Recursive Regularization ", "text_level": 1, "page_idx": 4}, {"type": "table", "img_path": "8ugOlbjJpp/tmp/9d39996043eb481a7cec407f241c8a4d633958c78f5942b41c81c9895c90b88b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "In this section, we present our modified recursive regularization algorithm, first developed in [AZ18] and extended to Euclidean SSPs in [BGM23]. We then discuss the key components of our analysis needed to obtain our results for non-Euclidean geometries. We conclude the section by applying our general result for recursive regularization to DP $\\ell_{p}/\\ell_{q}$ -SSPs. ", "page_idx": 4}, {"type": "text", "text": "Algorithm Overview. As in [BGM23], our recursive regularization implementation, Algorithm 1, solves a series of regularized saddle point problems defined by $f^{(1)},...,f^{(T)}$ . The saddle point problem defined in each round of Algorithm 1 is solved using some empirical subroutine, $\\mathcal{A}_{e m p}$ . This subroutine takes as input a subset of the dataset, $S_{t}$ , the regularized loss function for that round, $f^{(t)}$ a starting point, $[\\bar{w}_{t-1},\\bar{\\theta}_{t-1}]$ , and an upper bound on the expected distance to the empirical saddle point of the problem defined by $S_{t}$ and $f^{(t)}$ . The exact implementation of $\\mathcal{A}_{e m p}$ , Algorithm 3, will be discussed in the next section. Here, we focus on the guarantees of Recursive Regularization given that $\\mathcal{A}_{e m p}$ satisfies a certain accuracy condition. At each round, the empirical subroutine $\\mathcal{A}_{\\sf e m p}$ is required to find a point, $[\\bar{w}_{t},\\bar{\\theta}_{t}]$ , which is close (under $\\|\\cdot\\|_{.}$ ) to the empirical saddle point. Because the scale of regularization doubles each round, this task becomes easier each round. Specifically, [BGM23] observed that implementations of $\\mathcal{A}_{\\sf e m p}$ which satisfy the notion of relative accuracy succeed at finding such points. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Definition 2 $\\hat{\\alpha}$ -relative accuracy). Given a dataset $S^{\\prime}\\in\\mathcal{X}^{n^{\\prime}}$ ,lossfunction $f^{\\prime}$ . and an initial point $[w^{\\prime},\\theta^{\\prime}]$ ,we say that $\\mathcal{A}_{\\sf e m p}$ satisfies $\\hat{\\alpha}$ -relative accuracy w.rt. the empirical saddle point $[w_{S^{\\prime}}^{*},\\theta_{S^{\\prime}}^{*}]$ of $\\begin{array}{r}{F_{S^{\\prime}}^{\\prime}(w,\\theta)=\\frac{1}{n^{\\prime}}\\sum_{x\\in S^{\\prime}}f^{\\prime}(w,\\theta;x)\\,i f,\\forall\\hat{D}>0,}\\end{array}$ whenever $\\mathbb{E}\\left[\\lVert[w^{\\prime},\\theta^{\\prime}]-[w_{S^{\\prime}}^{*},\\theta_{S^{\\prime}}^{*}]\\rVert\\right]\\leq\\hat{D}$ the output $[\\bar{w},\\bar{\\theta}]$ of $A_{\\mathsf{e m p}}$ satisfies E $\\left[F_{S^{\\prime}}^{\\prime}(\\bar{w},\\theta_{S^{\\prime}}^{*})-F_{S^{\\prime}}^{\\prime}(w_{S^{\\prime}}^{*},\\bar{\\theta})\\right]\\leq\\hat{D}\\hat{\\alpha}.$ ", "page_idx": 5}, {"type": "text", "text": "In contrast to [BGM23], our algorithm uses more general regularization to ensure strongconvexity/strong-concavity with respect to the appropriate norm. ", "page_idx": 5}, {"type": "text", "text": "Guarantees of Recursive Regularization.  Our general result for recursive regularization is stated as follows, and its full proof is given in Appendix B.2. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Let $\\mathcal{A}_{\\sf e m p}$ satisfy $\\hat{\\alpha}$ -relative accuracy for any $(5L)$ -Lipschitz loss function and dataset of size $\\begin{array}{r}{n^{\\prime}=\\frac{n}{\\log(n)}}\\end{array}$ and assume $\\|\\cdot\\|_{w}^{2}$ and $\\|\\cdot\\|_{\\theta}^{2}$ are $\\frac{1}{\\kappa}$ -strogly convex under $\\Vert\\cdot\\Vert_{w}$ and ${\\big\\|}\\cdot{\\big\\|}_{\\theta}$ respecively. Then Algorithm $^{\\,I}$ , run with $\\mathcal{A}_{\\sf e m p}$ as a subroutine and $\\begin{array}{r}{\\lambda=\\frac{48}{B}\\left(\\hat{\\alpha}\\kappa^{2}+\\frac{L\\kappa^{3/2}}{\\sqrt{n^{\\prime}}}\\right)}\\end{array}$ ,satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{Gap}_{5\\mathrm{P}}(\\mathcal{R}_{S S P})=O\\Big(B\\hat{\\alpha}\\kappa^{2}\\log(n)+\\frac{B L\\kappa^{3/2}\\log^{3/2}(n)}{\\sqrt{n}}\\Big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The similarity of this result to [BGM23, Theorem 5] and our exposition thus far belies the difficulty of adapting their result to non-Euclidean setups. The key challenge addressed by the analysis of [BGM23] was that of generalization. In this regard, their key insight was to use McDiarmid style concentration bounds to show that the empirical saddle point obtains non-trivial guarantees on the strong gap. However, such concentration results critically rely on the fact that the underlying norm is Euclidean. A generalization of this concentration to, for example, the $\\ell_{1}$ setup, necessarily incurs an additional $\\sqrt{d}$ factor $[\\mathrm{Pan}08]$ . Thus, a fundamentally new analysis is needed. One should also note that the squared norms $\\|\\cdot\\|_{w}^{2}$ may not be strongly convex for certain norms, such as $\\|\\cdot\\|_{1}$ . Regardless, in some such cases, we can still leverage this result by modifying the underlying problem, as we will show in Section 4. ", "page_idx": 5}, {"type": "text", "text": "Key Proof Ideas. We circumvent the above issues by providing a fundamentally new generalization analysis for the intermediate iterates of recursive regularization. Specifically, our analysis avoids entirely any analysis of the strong gap at intermediate stages of the algorithm. Instead, we introduce two new functions, which are similar in nature to the quantity used in the definition of relative accuracy, but are taken with respect to the population saddle point. For $t\\,\\in\\,[T]$ define $F_{\\mathcal{D}}^{(t)}(w,\\theta;x)\\mathrel{\\mathop:}=$ $\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[f^{(t)}(w,\\theta;x)\\right]$ and let $[w_{t}^{*},\\theta_{t}^{*}]$ be its saddle point; define $\\begin{array}{r}{F_{S}^{(t)}:=\\frac{1}{n^{\\prime}}\\sum_{x\\in S_{t}}f^{(t)}(w,\\theta;x)}\\end{array}$ We are interested in the functions, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H_{\\mathcal{D}}^{(t)}([w,\\theta]):=F_{\\mathcal{D}}^{(t)}(w,\\theta_{t}^{*})-F_{\\mathcal{D}}^{(t)}(w_{t}^{*},\\theta)\\quad\\mathrm{and}\\quad H_{S}^{(t)}(w,\\theta):=F_{S}^{(t)}(w,\\theta_{t}^{*})-F_{S}^{(t)}(w_{t}^{*},\\theta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Notablytsnity/ngiy $F_{\\mathcal{D}}^{(t)}$ means that a bound on $H_{\\mathcal{D}}^{(t)}([w,\\theta])$ yields a bound on $\\|[w,\\theta]-[w_{t}^{*},\\theta_{t}^{*}]\\|$ . Ultimately, finding a point sufficiently close to $[w_{t}^{*},\\bar{\\theta}_{t}^{*}]$ at each round is all recursive regularization needs to succeed. The question then, is how to obtain guarantees on $H_{\\mathcal{D}}^{(t)}$ . We accomplish this via a stability-implies-generalization argument. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2. Let $f\\;:\\;\\mathcal{Z}\\,\\times\\,\\mathcal{X}\\;\\mapsto\\;\\mathbb{R}$ be $L$ Lipschitz.  Let $[w^{\\ast},\\theta^{\\ast}]\\ \\in\\ \\mathcal{Z}$ be the population saddle point. For any $x\\ \\in\\ \\mathcal{X}$ define $h([w,\\theta];x)\\;=\\;f(w,\\theta^{\\ast};x)\\,-\\,\\bar{f}(w^{\\ast},\\theta;x)$ .For $S\\,\\sim\\,{\\mathcal{D}}^{n}$ ,let $\\begin{array}{r}{H_{S}(z)=\\frac{1}{n}\\sum_{x\\in S}h(z;x)}\\end{array}$ and $H_{\\mathcal{D}}(z)=\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[h(z;x)\\right]$ . Then for any $\\Delta$ UAS algorithm, $\\mathcal{A}$ one has $\\underset{S,A}{\\mathbb{E}}\\left[H_{\\mathcal{D}}(A(S))-H_{S}(A(S))\\right]\\leq2\\Delta L$ ", "page_idx": 5}, {"type": "text", "text": "The proof relies on two main observations. First, it is easy to see that because $f$ is Lipschitz, then $h$ is also Lipschitz. Then, because $H_{\\mathcal{D}}$ and $H_{S}$ can be written as the expectation of $f$ w.r.t. $z\\sim\\mathcal{D}$ and $z\\sim\\mathsf{U n i f}(S)$ respectively, we can apply standard stability-implies-generalization results to $h$ to obtain the claimed result [BEO2]. We provide a full proof in Appendix B.1. This analysis bypasses difficulties of working directly with $\\widehat{\\mathbb{G}\\mathrm{ap}}_{\\mathsf{S P}}$ experienced by [OPZZ22, BGM23] and other works since, in general, there is $^{n o}$ function $h$ such that $\\widehat{\\mathrm{Gap}}_{\\mathsf{S P}}(z)=\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[h(z;x)\\right]$ . Note also it is important that we have defined $h(z;x)$ w.r.t. to the data independent point $[w_{t}^{*},\\theta_{t}^{*}]$ .Were $[w_{t}^{*},\\theta_{t}^{*}]$ to depend on $S$ , this result would not hold. ", "page_idx": 6}, {"type": "text", "text": "Because $H_{S}^{(t)}$ uses the population saddle point in its definition, it may not be immediately clear how one could first minimize $H_{S}^{(t)}$ . Direct access to $H_{S}^{(t)}$ is in fact not possible without knowledge of $[w_{t}^{*},\\theta_{t}^{*}]$ , which the algorithm does not have. In this regard, we observe that algorithms which minimize the empirical gap are powerful enough to minimize $H_{S}^{(t)}$ I', even without knowledge of $[w_{t}^{*},\\theta_{t}^{*}]$ , since ", "page_idx": 6}, {"type": "equation", "text": "$$\nH_{S}^{(t)}(w,\\theta)=F_{S}^{(t)}(w,\\theta_{t}^{*})-F_{S}^{(t)}(w_{t}^{*},\\theta)\\leq\\operatorname*{max}_{w^{\\prime},\\theta^{\\prime}}\\left\\{F_{S}^{(t)}(w,\\theta^{\\prime})-F_{S}^{(t)}(w^{\\prime},\\theta)\\right\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Particular to our analysis, we willverage the fact that the exact empirical saddle point is $\\left({\\frac{L}{\\lambda n}}\\right)$ stable and has an empirical gap of 0. ", "page_idx": 6}, {"type": "text", "text": "4  Optimal Rates for Private $\\ell_{p}/\\ell_{q}$ Saddle Point Problems ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Setup. In this section, we apply Theorem 1 to obtain results for $\\ell_{p}/\\ell_{q}$ saddle point problems. In order to do this, we will apply recursive regularization using norms slightly different than the $\\ell_{p}$ and $\\ell_{q}$ norms. Specifically, to solve an $\\ell_{p}/\\ell_{q}$ SSP, we define $\\begin{array}{r}{\\bar{p}\\,=\\,\\operatorname*{max}\\left\\dot{\\left\\{p,1+\\frac{1}{\\log(d)}\\right\\}}\\right.}\\end{array}$ and $\\begin{array}{r}{\\bar{q}\\,=\\,\\operatorname*{max}\\left\\{q,1+\\frac{1}{\\log(d)}\\right\\}}\\end{array}$ and will apply recursive regularization with $\\begin{array}{r}{\\|\\cdot\\|_{w}\\;=\\;\\frac{1}{B_{w}}\\|\\cdot\\|_{\\bar{p}}}\\end{array}$ and $\\begin{array}{r}{\\|\\cdot\\|_{\\theta}=\\frac{1}{B_{w}}\\|\\cdot\\|_{\\bar{p}}}\\end{array}$ . We also have $\\|\\cdot\\|_{\\bar{p}}\\leq\\|\\cdot\\|_{1}\\leq d^{1-1/\\bar{p}}\\|\\cdot\\|_{\\bar{p}}\\leq2\\|\\cdot\\|_{\\bar{p}}$ . Thus, under these norms we have diameter bound $B^{2}=1$ and Lipschitz constant $L^{2}\\leq4B_{w}^{2}L_{w}^{2}+4B_{\\theta}^{2}L_{\\theta}^{2}$ [NJLS09]. Further the strongconvexity asumption nededby Theorem is satisied withn $\\begin{array}{r}{\\kappa=\\operatorname*{max}\\left\\{\\frac{1}{\\bar{p}-1},\\frac{1}{\\bar{q}-1}\\right\\}}\\end{array}$ This is because for any $p>1$ ${\\frac{1}{2}\\parallel\\cdot\\parallel_{p}^{2}}$ is $(p-1)$ -strongly convex w.r.t. $\\|\\cdot\\|_{p}$ [Bec17]. ", "page_idx": 6}, {"type": "text", "text": "Private Algorithm Satisfying Relative Accuracy. To apply Theorem 1, we must construct an algorithm satisfying relative accuracy and $(\\epsilon,\\delta)$ -DP. For this, we use the stochastic mirror prox algorithm of [JNT11], Algorithm 2. This algorithm will also have application in our analysis of SVIs later on. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 Stochastic Mirror Prox   \nRequire: Learning rate $\\eta$ Operator oracle $\\scriptscriptstyle\\mathcal{O}$ , Initial point $z_{0}\\ \\in\\ \\mathcal{Z}$ , Regularization function $\\overline{{\\psi}}$ minimized at $z_{0}$ , Iterations $T$   \n1: for $t=1\\ldots T$ do   \n2: $\\begin{array}{r l}&{\\tilde{z}_{t}=\\arg\\operatorname*{min}_{u\\in\\mathcal{Z}}\\left\\{\\psi(u)+\\langle\\eta\\mathcal{O}(z_{t-1})-\\nabla\\psi(z_{t-1}),u\\rangle\\right\\}}\\\\ &{z_{t}=\\arg\\operatorname*{min}_{u\\in\\mathcal{Z}}\\left\\{\\psi(u)+\\langle\\eta\\mathcal{O}(\\tilde{z}_{t})-\\nabla\\psi(z_{t-1}),u\\rangle\\right\\}}\\end{array}$   \n3:   \n5:SPoutput: 4: end for t=1\u00b2t   \n6: SVI output: for $\\bar{t}^{*}\\sim\\mathsf{U n i f}([T])$ ", "page_idx": 6}, {"type": "text", "text": "This algorithm takes as input a stochastic oracle for the saddle operator of $f,\\,\\mathcal{O}$ ,and a strongly convex regularizer, $\\psi$ . We leverage this algorithm by constructing a differentially private version of the operator oracle O, and taking as output the average iterate =\u2192 Zt=1 2t. We make the oracle private by adding Gaussian noise to minibatch estimates of the saddle operator. It is then easy to show the whole algorithm is private by composition results and the post processing properties of differential privacy. We defer these details to Appendix C.2, and here state the final relative accuracy bound. ", "page_idx": 6}, {"type": "text", "text": "Require: Dataset $S$ , monotone operator $g$ , Subroutine $A_{\\mathsf{e m p}}$ , regularity parameter $\\kappa>0$ , regularization parameter $\\begin{array}{r}{\\lambda\\ge\\frac{L\\sqrt{\\kappa}}{B\\sqrt{n}}}\\end{array}$ constraint se diameter $B$ Strongly monotone operator $\\rho$   \n1: Let $n^{\\prime}=n/\\log_{2}(n)$ , and $\\begin{array}{r}{T=\\log_{2}(\\frac{L}{\\kappa B\\lambda})}\\end{array}$   \n2: Let $S_{1},...,S_{T}$ be a disjoint partition of $S$ with each $S_{t}$ of size $n^{\\prime}$ (always possible due to the condition on $\\lambda$   \n3: $\\bar{z}_{0}$ be any point in $\\mathcal{Z}$   \n4: Define function $(z,x)\\mapsto g^{(1)}(z;x)=g(z;x)+2\\lambda\\cdot\\rho(z-\\bar{z}_{0})$   \n5: for $t=1$ to $T$ do   \n6: $\\begin{array}{r}{\\bar{z}_{t}=A_{\\mathrm{emp}}\\left(S_{t},g^{(t)},\\bar{z}_{t-1},\\frac{B}{2^{t}}\\right)}\\end{array}$   \n7:  Define function $(z,x)\\mapsto\\bar{g^{(t+1)}}(z;x)=g^{(t)}(z;x)+2^{t+1}\\lambda\\cdot\\rho(z-\\bar{z}_{t})$   \n8: end for   \n9: Output: $\\bar{z}_{T}$ ", "page_idx": 7}, {"type": "text", "text": "Lemma 3. Under the setup described above, there exists an $(\\epsilon,\\delta)$ -DP algorithm which satisfes areatve awacy wih parametr \u03b1 = O((BL + BL)((/) $\\begin{array}{r}{O\\big(\\operatorname*{min}\\big\\{\\frac{\\sqrt{\\kappa}n^{2}\\epsilon^{1.5}}{\\log^{2}(n)\\sqrt{d\\log(1/\\delta)\\tilde{\\kappa}}},\\frac{\\sqrt{\\kappa}n}{\\log^{3/2}(n)}\\big\\}}\\end{array}$ $\\frac{\\sqrt{\\kappa}n^{3/2}}{\\log^{3/2}(n)}\\})$ $\\tilde{\\kappa}\\,=$ $1+\\mathbf{1}\\left\\{p<2\\lor q<2\\right\\}\\cdot\\log({\\dot{d}})$ ", "page_idx": 7}, {"type": "text", "text": "Main Result for DP $\\ell_{p}/\\ell_{q}$ SSPs. Applying now the result of recursive regularization, Theorem   \n1, we obtain the optimal rate for $\\ell_{p}/\\ell_{q}$ SSPs (up to logarithmic factors). Recall under our chosen $B\\leq1$ $L^{2}\\le4B_{w}^{2}L_{w}^{\\bar{2}}+\\bar{4}B_{\\theta}^{2}L_{\\theta}^{2}$ $\\mathcal{A}_{\\sf e m p}$ ", "page_idx": 7}, {"type": "text", "text": "Corollary 1. There exists an Algorithm, $\\mathcal{R}$ ,which is $(\\epsilon,\\delta)$ -DP, has number of gradient evaluations boundby $\\begin{array}{r}{O\\big(\\operatorname*{min}\\big\\{\\frac{\\sqrt{\\kappa}n^{2}\\epsilon^{1.5}}{\\log(n)\\sqrt{d\\log(1/\\delta)\\tilde{\\kappa}}},\\frac{\\sqrt{\\kappa}n^{3/2}}{\\sqrt{\\log(n)}}\\big\\}\\big)}\\end{array}$ $\\log(n)$ factors) ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{Gap}_{\\mathrm{SP}}(\\mathcal{R})=\\tilde{O}\\left(\\kappa^{2.5}\\sqrt{B_{w}^{2}L_{w}^{2}+B_{\\theta}^{2}L_{\\theta}^{2}}\\left(\\frac{\\sqrt{d\\log(1/\\delta)\\tilde{\\kappa}}}{n\\epsilon}+\\frac{1}{\\sqrt{n}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "(k is at most $\\log(d)$ ", "page_idx": 7}, {"type": "text", "text": "Notethat in the $\\ell_{2}/\\ell_{2}$ -Setting $\\kappa=1$ and the above exactly recovers the result of [BGM23]. We further recall that the near optimality of this result is established by existing lower bounds for stochastic minimization, which is a special case of SSPs [BFTT19, BGN21]. ", "page_idx": 7}, {"type": "text", "text": "5 Extension to Variational Inequalities ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we start by discussing the modifications that must be made to the recursive regularization algorithm to handle the more general structure of SVIs. We then discuss key ideas in the analysis and how to apply the algorithm to SVIs in the $\\ell_{p}$ setting. We recall that we here assume the operator $g$ is monotone, $L$ -bounded and $\\beta$ -Lipschitz. ", "page_idx": 7}, {"type": "text", "text": "5.1 Recursive Regularization Algorithm for SVIs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Algorithm 3 bears many similarities to Algorithm 1. Most notable among the differences is that we here regularize with a strongly monotone operator $\\rho$ instead of a strongly-convex/strongly-concave function. In our eventual application we will use $\\begin{array}{r}{\\rho=\\nabla(\\frac{1}{2}\\|\\cdot\\|^{2})}\\end{array}$ , but strictly we only require that $\\rho$ satisfies the following. ", "page_idx": 7}, {"type": "text", "text": "Assumption 1. For $\\kappa>0$ $\\rho:\\mathcal{Z}\\mapsto\\mathbb{R}^{d}\\,i s\\;\\frac{1}{\\kappa}$ -strongly monotone w.rt. $\\|\\cdot\\|$ and satisfy $\\|\\rho(z)\\|_{*}\\leq\\|z\\|$ for all $z\\in{\\mathcal{Z}}$ ", "page_idx": 7}, {"type": "text", "text": "When $\\rho=\\nabla(\\frac{1}{2}\\|\\cdot\\|^{2})$ , the second half of the condition is always guaranteed by the properties of the dual norm (see Fact 2). When $\\rho$ satisfies Assumption 1, we obtain the following guarantee. ", "page_idx": 7}, {"type": "text", "text": "Theorem 2. Let $\\mathcal{A}_{\\sf e m p}$ satisfy $\\hat{\\alpha}$ -relative stationarity for any $(5L)$ -bounded monotone operator and dataset of size $\\begin{array}{r}{n^{\\prime}=\\frac{n}{\\log(n)}}\\end{array}$ Let $\\rho$ satisfy Assumption $^{\\,l}$ Then Algorithm $^{\\,l}$ ,run with $\\mathcal{A}_{\\sf e m p}$ as $a$ subroutine and $\\begin{array}{r}{\\lambda=\\frac{48}{B}\\left(\\hat{\\alpha}\\kappa^{3}+\\frac{(\\beta B+L)\\kappa^{2}}{\\sqrt{n^{\\prime}}}\\right)}\\end{array}$ ,satisfies ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{Gapv}_{1}(\\mathcal{R}_{S V I})=O\\Big(\\log(n)B\\hat{\\alpha}\\kappa^{3}+\\frac{\\log^{3/2}(n)B(\\beta B+L)\\kappa^{2}}{\\sqrt{n}}\\Big).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The full proof is in Appendix B.2. We discuss the key ideas in the following subsection. ", "page_idx": 8}, {"type": "text", "text": "5.2  Analysis Idea ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Unfortunately, the analysis used for stochastic saddle point problems does not easily extend to variational inequalities due to the fact that the VI-gap and SP-gap behave in fundamentally different ways. Even though SSPs are a special case of SVIs (when the operator in question is the saddle operator of the loss function), a bound on the VI-gap does not imply a bound on the SSP-gap. Further, a natural attempt to extend the notion of relative accuracy (Definition 3) to SVIs by asking $A_{\\mathsf{e m p}}$ to bound $\\langle G_{\\mathcal{D}}(z_{S}^{*}),\\mathcal{A}(S)-z_{S}^{*}\\rangle$ does not work, because such a term does not yield an upper bound on the distance $\\|\\boldsymbol{A}(\\boldsymbol{S})-\\boldsymbol{z}_{S}^{*}\\|$ . A similar problem holds for generalization measures in Eqn. (6). ", "page_idx": 8}, {"type": "text", "text": "A New Empirical Accuracy Measure. Motivated by the above issues, we introduce a new relative accuracy measure for our analysis of sVIs. Importantly, this notion will allow us to bound the distance between the output of $A_{\\mathsf{e m p}}$ and the empirical equilibrium point of the strongly monotone operator created at each round of the recursive regularzation algorithm. ", "page_idx": 8}, {"type": "text", "text": "Definition 3 $\\hat{\\alpha}$ -relative stationarity). Given a dataset $S^{\\prime}\\in\\mathcal{X}^{n^{\\prime}}$ operator $g^{\\prime}$ and an initial point $z^{\\prime}$ , we say that $\\mathcal{A}_{\\sf e m p}$ satisfies $\\hat{\\alpha}$ -relative stationarity w.rt. to the empirical equilibrium $z_{S^{\\prime}}^{*}$ of $\\begin{array}{r}{G_{S^{\\prime}}(z)\\,=\\,\\frac{1}{n^{\\prime}}\\sum_{x\\in S^{\\prime}}g^{\\prime}(z;x)}\\end{array}$ $\\forall\\hat{D}\\,>\\,0$ whenever $\\mathbb{E}\\left[\\|z^{\\prime}-z_{S^{\\prime}}^{*}\\|\\right]\\,\\le\\,\\hat{D}$ the output $\\bar{z}$ of $\\mathcal{A}_{\\sf e m p}$ satisfes $\\mathbb{E}\\left[\\langle G(\\bar{z}),\\bar{z}-z_{S^{\\prime}}^{*}\\rangle\\right]\\le\\hat{D}\\hat{\\alpha}.$ ", "page_idx": 8}, {"type": "text", "text": "Modified Generalization Measure.  The generalization measure we use can be modified in a similar fashion. ", "page_idx": 8}, {"type": "text", "text": "Lemma 4. Let $g(z;x)\\,=\\,g_{1}(z;x)+g_{2}(z)$ such that $g_{1}\\,:\\,\\mathcal{Z}\\,\\times\\,\\mathcal{X}\\,\\mapsto\\,\\mathbb{R}^{d}$ is $L$ -bounded and $\\beta$ Lipschitz with respect to $z$ and $g_{2}:\\mathcal{Z}\\mapsto\\mathbb{R}^{d}$ is any (data indepedent) operator. Let $z^{*}\\in{\\mathcal{Z}}$ be its population equilibrium point. For any $x\\in\\mathscr{X}$ ,define $h(z;x)=\\langle g(z;x),z-z^{*}\\rangle$ For $S\\sim{\\mathcal{D}}^{n}$ ,let $\\begin{array}{r}{\\dot{H_{S}}(z)=\\frac{1}{n}\\sum_{x\\in S}h(z;\\dot{x})}\\end{array}$ and $H_{\\mathcal{D}}(z)=\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[h(z;x)\\right]$ Then for any $\\Delta$ UAS algorithm, $\\mathcal{A}$ one has $\\mathbb{E}_{S,A}\\left[H_{\\mathcal{D}}(A(S))-H_{S}(A(S))\\right]\\leq\\Delta(\\beta B+\\bar{L})$ ", "page_idx": 8}, {"type": "text", "text": "The proof is similar to that of Lemma 2, but must account for additional complications. First, the function $h$ may not be Lipschitz if the regularizer, represented by $g_{2}$ above, is not Lipschitz. This happens, for example, in the $\\ell_{1}$ setting. Thus, we must decompose $h$ in the stability-impliesgeneralization analysis and handle the non-Lipschitz, but data-independent, term $g_{2}$ separately. Then, the Lipschitzness of the remainder is established using that fact that $g_{1}$ is both bounded and Lipschitz. The full proof is in Appendix D.1. ", "page_idx": 8}, {"type": "text", "text": "5.3  Application to DP variational inequalities in the $\\ell_{p}$ setting ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In order to apply Theorem 2 to SVIs with an $\\ell_{p}$ setup, we pick $\\begin{array}{r}{\\|\\cdot\\|=\\frac{1}{B}\\|z\\|_{\\bar{p}}}\\end{array}$ where $\\bar{p}=\\operatorname*{max}\\{p,1+$ $\\frac{1}{\\log(d)}\\}$ 2. We ilusethe regulaizer, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\rho(z)=\\nabla(\\frac{1}{2B}\\|z\\|_{\\bar{p}}^{2}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Note the above operator is uniquely defined since $\\bar{p}>1$ , and thus $\\|\\cdot\\|_{\\bar{p}}^{2}$ is differentiable [Gui09]. This choice of $\\rho$ satisies Assumption 1 with parameter $\\begin{array}{r}{\\kappa\\,=\\,\\frac{1}{\\bar{p}-1}}\\end{array}$ . To se this first observe that for any $\\bar{p}\\,>\\,1$ \uff0c $\\frac{1}{2}\\parallel\\cdot\\parallel_{\\bar{p}}^{2}$ is $(\\bar{p}-1)$ -strongly convex w.r.t. $\\|\\cdot\\|_{\\bar{p}}$ and the gradient operator of a differentiable $\\mu$ -strongly convex function is $\\mu$ -strongly monotone. Second, for any norm and it's dual $\\begin{array}{r}{\\|\\nabla(\\frac{1}{2}\\|z\\|^{2})\\|_{*}\\leq\\|z\\|}\\end{array}$ for all $z$ (see Fact 2 in Appendix A). ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "To obtain relative stationarity guarantees, we again apply Algorithm 2 with a differentially private oracle for the empirical operator $G_{S}$ . The proof falls out of our existing analysis for stochastic mirror prox given in Appendix C.2. Specifically, Theorem 4 in the case where $\\Theta$ is the empty set implies there exists an $(\\epsilon,\\delta)$ -DP implementation of mirror prox which satisfies $\\hat{\\alpha}$ -relative stationarity with ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\hat{\\alpha}=O\\Big(\\frac{\\sqrt{\\kappa}B L\\sqrt{d\\log(1/\\delta)(1+\\mathbf{1}\\left\\{p<2\\right\\}\\cdot\\log(d))}}{n\\epsilon}+\\frac{\\sqrt{\\kappa}B L}{\\sqrt{n}}\\Big).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "We here highlight one notable difference that arises with the algorithm. Typically, after running an _algorithm like stochastic_ mirror prox for $t$ iterations, one obtains a bound on the quantity $\\begin{array}{r}{\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\left\\langle{G_{S}(z_{t}),z_{t}-z}\\right\\rangle\\right]}\\end{array}$ . Analysis then proceeds to bound the (empirical) VI-gap of the average iterate by leveraging monotonicity of the operator. However, this application of monotonicity does not help for the purposes of relative stationarity. For this reason, we instead select an iterate uniformly at random. We note this step is nonstandard as such a selection does not necessarily yield any bound on the (empirical) VI-gap. ", "page_idx": 9}, {"type": "text", "text": "Main Result for $\\ell_{p}$ DP SVIs.  Using Algorithm 3 and the implementation of $\\mathcal{A}_{\\sf e m p}$ described above, we ultimately obtain the following result as a corollary of Theorem 2 and Theorem 4. Recall under our choice of norm we have diameter bound 1 and operator bound $B L$ . Further, we can leverage existing lower bounds to show that this rate is near optimal; more details are available in Appendix $\\boldsymbol{\\mathrm E}$ Corollary 2. There exists an Algorithm, $\\mathcal{R}$ ,which is $(\\epsilon,\\delta)$ -DP, has gradient evaluations bounded by $\\begin{array}{r}{O\\big(\\operatorname*{min}\\big\\{\\frac{\\sqrt{\\kappa}n^{2}\\epsilon^{1.5}}{\\log(n)\\sqrt{d\\log(1/\\delta)\\tilde{\\kappa}}},\\frac{\\sqrt{\\kappa}n^{3/2}}{\\sqrt{\\log(n)}}\\big\\}\\big)}\\end{array}$ $\\log(n)$ Jacors) ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathrm{Gapv}_{1}(\\mathcal{R})=\\tilde{O}\\Big(\\kappa^{3.5}B L\\big(\\frac{\\sqrt{d\\log(1/\\delta)}\\tilde{\\kappa}}{n\\epsilon}+\\frac{1}{\\sqrt{n}}\\big)\\Big).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Wwhere $\\begin{array}{r}{\\kappa=\\frac{1}{\\operatorname*{max}\\left\\{p,1+\\frac{1}{\\log\\left(d\\right)}\\right\\}-1}}\\end{array}$ isatmost $\\log(d)$ and $\\tilde{\\kappa}=1+\\mathbf{1}\\left\\{p<2\\right\\}\\cdot\\log(d).$ ", "page_idx": 9}, {"type": "text", "text": "Near Linear Time Algorithm for the $\\ell_{2}$ Setting. \u03b2 Because we assume the operator is Lipschitz, in the $\\ell_{2}$ setting, we can leverage existing accelerated optimization techniques to achieve a near linear timeversion of $\\mathcal{A}_{\\sf e m p}$ , in a similar fashion to [ZTOH22, BGM23]. Specifically, using the accelerated SVRG algorithm of [PB16] and Gaussian noise it is possible to obtain the rate in Eqn. (8) (with $\\kappa=\\tilde{\\kappa}=1)$ )in $O(n+\\beta n\\log(n/\\delta))$ gradient evaluations. We provide full details in Appendix D.3. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "R. Bassily's and M. Menart's research was supported by NSF CAREER Award 2144532 and, in part, by NSF Award 2112471. C. Guzman's research was partially supported by INRIA Associate Teams project, ANID FONDECYT 1210362 grant, ANID Anillo ACT210005 grant, and National Center for Artificial Intelligence CENIA FB210017, Basal ANID. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "$[\\mathrm{ABD^{+}18}]$ Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reductions approach to fair classification. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 60-69. PMLR, 10-15 Jul 2018. ", "page_idx": 9}, {"type": "text", "text": "$[\\mathrm{ACG}^{+}16]$ Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. CCS '16, page 308-318, New York, NY, USA, 2016. Association for Computing Machinery. ", "page_idx": 9}, {"type": "text", "text": "[AFKT21] Hilal Asi, Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: Optimal rates in $\\ell_{1}$ geometry.In International Conferenceon Machine Learning,2021. ", "page_idx": 9}, {"type": "text", "text": "[AZ18] Zeyuan Allen-Zhu. How to make the gradients small stochastically: Even faster convex and nonconvex sgd. Advances in Neural Information Processing Systems, 31, 2018. [Bau58] Heinz Bauer. Minimalstellen von funktionen und extremalpunkte. Archiv der Mathematik, 9(4):389-393, 1958. [BE02] Olivier Bousquet and Andre Elisseeff Stability and generalization. The Journal of Machine Learning Research, 2:499-526, 2002. [Bec17]  Amir Beck. First-Order Methods in Optimization. SIAM-Society for Industral and Applied Mathematics, Philadelphia, PA, USA, 2017.   \n[BFTT19] Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic convex optimization with optimal rates. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 11279-11288, 2019. [BG23] Digvijay Boob and Cristobal Guzman. Optimal algorithms for differentially private stochastic monotone variational inequalities and saddle-point problems. Mathematical Programming, pages 1-43, 2023.   \n[BGM23] Raef Bassily, Cristobal Guzman, and Michael Menart. Differentially private algorithms for the stochastic saddle point problem with optimal rates for the strong gap. In Gergely Neu and Lorenzo Rosasco,editors, Proceedings of Thirty Sixth Conference on Leaning Theory, volume 195 of Proceedings of Machine Learning Research, pages 2482-2508. PMLR, 12-15 Jul 2023.   \n[BGN21] Raef Bassily, Cristobal Guzman, and Anupama Nandi. Non-euclidean differentially private stochastic convex optimization. In Mikhail Belkin and Samory Kpotufe, editors, Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pages 474-499. PMLR, 15-19 Aug 2021. [BST14] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Eficient algorithms and tight error bounds. In IEEE 55th Annual Symposium on Foundations of Computer Science (FOCS 2014). (arXiv preprint arXiv:1405.7085), pages 464-473. 2014.   \nDMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography conference, pages 265-284. Springer, 2006.   \n$[\\mathrm{DSL}^{+}18]$ Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. SBEED: Convergent reinforcement learning with nonlinear function approximation. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International ConferencenMachineLearning,volume0ofProceedingsofMachne Learning Research, pages 1125-1134. PMLR, 10-15 Jul 2018. [FKT20] Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: optimal rates in linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pages 439-449, 2020.   \n[GGP24] Tomas Gonzalez, Cristobal Guzman, and Courtney Paquette. Mirror descent algorithms with nearly dimension-independent rates for differentially-private stochastic saddle-point problems. CoRR, abs/2403.02912, 2024. [Gui09]  A Guirao. Uniformly convex functions on banach spaces. Proceedings of the American Mathematical Society, 137(3):1081-1091, 2009. [JKT12] Prateek Jain, Pravesh Kothari, and Abhradeep Thakurta. Differentially private online learning. In 25th Annual Conference on Learning Theory (COLT), pages 24.1-24.34, 2012. [JN19]  Anatoli B. Juditsky and Arkadi S. Nemirovski. Signal Recovery by Stochastic Optimization. Automation and Remote Control / Avtomatika i Telemekhanika, 80(10):1878-1893, October 2019. [JNT11]  Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel. Solving variational inequalities with stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17 - 58, 2011. [JT14] Prateek Jain and Abhradeep Thakurta. (near) dimension independent risk bounds for differentially private learning. In ICML, 2014. [KLL21] Janardhan Kulkarni, Yin Tat Lee, and Daogao Liu. Private non-smooth erm and sco in subquadratic steps. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 4053-4064. Curran Associates, Inc., 2021.   \n[LYYY21] Yunwen Lei, Zhenhuan Yang, Tianbao Yang, and Yiming Ying. Stability and generalization of stochastic gradient methods for minimax problems. In Marina Meila and Tong Zhang,editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 6175-6186. PMLR, 18-24 Jul 2021.   \n[MSS19] Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 4615-4625. PMLR, 09-15 Jun 2019.   \n[NJLSO9]  Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and And Shapiro. Robust stochastic approximation approach to stochastic programming. Society for Industrial and Applied Mathematics, 19:1574-1609, 01 2009.   \n[NMG24] Quan Nguyen, Nishant A. Mehta, and Cristobal Guzman. Beyond minimax rates in group distributionally robust optimization via a novel notion of sparsity, 2024. [NY78] Arkadi Nemirovski and D Yudin. On cezari's convergence of the steepest descent method for approximating saddle point of convex-concave functions. In Soviet Mathematics. Doklady, volume 19, pages 258-269, 1978.   \n[OPZZ22]  Asuman Ozdaglar, Sarath Pattathil, Jiawei Zhang, and Kaiqing Zhang. What is a good metric to study generalization of minimax learners? In Advances in Neural Information Processing Systems, volume 35. Curran Associates, Inc., 2022. [Pan08] Liam Paninski. A coincidence-based test for uniformity given very sparsely sampled discrete data. IEEE Transactions on Information Theory, 54:4750-4755, 2008. [PB16] Balamurugan Palaniappan and Francis Bach. Stochastic variance reduction methods for saddle-point problems. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. [SGJ22] Tasuku Soma, Khashayar Gatmiry, and Stefanie Jegelka. Optimal algorithms for group distributionally robust optimization and beyond. arXiv preprint arXiv:2212.13669, 2022. [Sio58] Maurice Sion. On general minimax theorems. Pacific Journal of Mathematics, 8(1):171 -176, 1958. [TTZ15] Kunal Talwar, Abhradeep Thakurta, and Li Zhang. Nearly optimal private lasso. In NIPS, 2015. [WM19] Robert Williamson and Aditya Menon. Fairness risk measures. In International Conference on Machine Learning, pages 6786-6797. PMLR, 2019.   \n$[\\mathrm{YHL}^{+}22]$ Zhenhuan Yang, Shu Hu, Yunwen Lei, Kush R Vashney, Siwei Lyu, and Yiming Ying. Differentially private sgda for minimax problems. In James Cussens and Kun Zhang, editors, Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, volume 180 of Proceedings of Machine Learning Research, pages 2192- 2202. PMLR, 01-05 Aug 2022.   \n[YLMJ22] Yaodong Yu, Tianyi Lin, Eric Mazumdar, and Michael I. Jordan. Fast distributionally robust learning with variance reduced min-max optimization. AISTATS, 2022. [ZB24] Xinyu Zhou and Raef Bassily. Differentially private worst-group risk minimization. CoRR, abs/2402.19437, 2024.   \n[ZHWZ21] Junyu Zhang, Mingyi Hong, Mengdi Wang, and Shuzhong Zhang. Generalization bounds for stochastic saddle point problems. In Arindam Banerjee and Kenji Fukumizu, editors, Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 568-576. PMLR, 13-15 Apr 2021. [ZL15]  Yuchen Zhang and Xiao Lin. Stochastic primal-dual coordinate method for regularized empirical risk minimization. In International Conference on Machine Learning, pages 353-361. PMLR, 2015.   \n[ZTOH22] Liang Zhang, Kiran Koshy Thekumparampil, Sewoong Oh, and Niao He. Bring your own algorithm for optimal differentially private stochastic minimax optimization. In Advances in Neural Information Processing Systems, volume 35. Curran Associates, Inc., 2022.   \n$[Z Z Z^{+}24\\mathrm{a}]$ Lijun Zhang, Peng Zhao, Zhen-Hua Zhuang, Tianbao Yang, and Zhi-Hua Zhou. Stochastic approximation approaches to group distributionally robust optimization. CoRR, 2024.   \n$[Z Z Z^{+}246]$ Lijun Zhang, Peng Zhao, Zhen-Hua Zhuang, Tianbao Yang, and Zhi-Hua Zhou. Stochastic approximation approaches to group distributionally robust optimization, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Supplementary Lemmas ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Fact 2. For any $z$ it holds that $\\begin{array}{r}{\\|\\nabla(\\frac{1}{2}\\|z\\|^{2})\\|_{*}\\leq\\|z\\|}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "Proof. By the chain rule we have $\\begin{array}{r}{\\nabla(\\frac{1}{2}\\|z\\|^{2})\\,=\\,\\|z\\|\\cdot\\nabla(\\|z\\|)}\\end{array}$ and so $\\begin{array}{r}{\\|\\nabla(\\frac{1}{2}\\|z\\|^{2})\\|_{*}\\;=\\;\\|z\\|}\\end{array}$ $\\lVert\\nabla(\\left\\lVert z\\right\\rVert)\\rVert_{*}$ . Thus, it only remains to show that $\\|\\nabla(\\|z\\|)\\|_{*}\\,\\leq\\,1$ . By the definition of the dual norm we have $\\|\\nabla(\\|z\\|)\\|_{*}\\bar{=}\\operatorname*{max}_{v:\\|v\\|\\leq1}\\left\\{\\langle\\nabla(\\|z\\|),v\\rangle\\right\\}$ . Further, by the definition of the subgradient we have for any $z^{\\prime}$ that $\\|z^{\\prime}\\|-\\|z\\|\\geq\\langle\\nabla(\\|z\\|),z^{\\prime}-z\\rangle$ . Substituting $v=z^{\\prime}-z$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\langle\\nabla(\\|z\\|),v\\rangle\\leq\\|v+z\\|-\\|z\\|\\leq\\|v\\|+\\|z\\|-\\|z\\|=\\|v\\|\\leq1,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "as desired. ", "page_idx": 12}, {"type": "text", "text": "Lemma 5. Let $\\mu,\\lambda>0$ and $\\rho$ be $\\mu$ -strongly monotone w.r.t. $\\Vert\\cdot\\Vert$ Then for any point $z_{0}\\in\\mathcal{Z}$ \uff0c the algorithm which outputs the unique equilibrium of $\\begin{array}{r}{G_{S}(z)+\\frac{\\lambda}{\\mu}(\\rho(z)-\\rho(z_{0}))}\\end{array}$ is $\\left({\\frac{2L}{\\mu\\lambda n}}\\right)$ -uniform argument stable w.r.t. $S$ ", "page_idx": 12}, {"type": "text", "text": "Note without loss of generality we can always choose $z_{0}$ to be the point such that $\\boldsymbol{\\rho}(z_{0})=\\mathbf{0}$ which is guaranteed to exist since $\\rho$ is strongly monotone. Thus this result also holds for regularization of the form $G_{S}(z)+\\lambda\\cdot\\rho(z)$ . Further, since the saddle operator of a strongly-convex/strongly-concave loss is strongly monotone, and the resulting SSP shares its equilibrium point with the corresponding SVI, Lemma 1 given in the preliminaries is also established through this result. ", "page_idx": 12}, {"type": "text", "text": "Proof.Let $z_{\\lambda}$ and $z_{\\lambda}^{\\prime}$ denote the equilibrium points of the regularized operators w.r.t. to adjacent datasets $S$ and $S^{\\prime}$ respectively. By the equilibrium condition we have for any $z\\in{\\mathcal{Z}}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle{G_{S}(z_{\\lambda})+\\lambda[\\rho(z_{\\lambda})-\\rho(z_{0})],z-z_{\\lambda}}\\rangle\\geq0}\\\\ {\\langle{G_{S^{\\prime}}(z_{\\lambda}^{\\prime})+\\lambda[\\rho(z_{\\lambda}^{\\prime})-\\rho(z_{0})],z-z_{\\lambda}^{\\prime}}\\rangle\\geq0}\\\\ {\\implies\\langle{G_{S}(z_{\\lambda})-G_{S^{\\prime}}(z_{\\lambda}^{\\prime})+\\lambda[\\rho(z_{\\lambda})-\\rho(z_{\\lambda}^{\\prime})],z_{\\lambda}^{\\prime}-z_{\\lambda}}\\rangle\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $\\rho$ is a 1-strongly monotone operator we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\langle{\\cal G}_{S}(z_{\\lambda})-{\\cal G}_{S^{\\prime}}(z_{\\lambda}^{\\prime}),z_{\\lambda}^{\\prime}-z_{\\lambda}\\rangle\\geq\\lambda\\,\\langle\\rho(z_{\\lambda})-\\rho(z_{\\lambda}^{\\prime}),z_{\\lambda}-z_{\\lambda}^{\\prime}\\rangle\\geq\\mu\\lambda\\|z_{\\lambda}-z_{\\lambda}^{\\prime}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now we can use the monotonicity of $G$ to derive, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu\\lambda\\|z_{\\lambda}-z_{\\lambda}^{\\prime}\\|^{2}\\leq\\langle G_{S}(z_{\\lambda})-G_{S^{\\prime}}(z_{\\lambda}^{\\prime}),z_{\\lambda}^{\\prime}-z_{\\lambda}\\rangle}\\\\ &{\\qquad\\qquad\\qquad=\\langle G_{S}(z_{\\lambda})-G_{S^{\\prime}}(z_{\\lambda}),z_{\\lambda}^{\\prime}-z_{\\lambda}\\rangle+\\langle G_{S^{\\prime}}(z_{\\lambda})-G_{S^{\\prime}}(z_{\\lambda}^{\\prime}),z_{\\lambda}^{\\prime}-z_{\\lambda}\\rangle}\\\\ &{\\overset{(i)}{\\leq}\\langle G_{S}(z_{\\lambda})-G_{S^{\\prime}}(z_{\\lambda}),z_{\\lambda}^{\\prime}-z_{\\lambda}\\rangle}\\\\ &{\\overset{(i i)}{\\leq}\\|G_{S}(z_{\\lambda})-G_{S^{\\prime}}(z_{\\lambda})\\|_{*}\\cdot\\|z_{\\lambda}^{\\prime}-z_{\\lambda}\\|}\\\\ &{\\overset{(i i i)}{\\leq}\\frac{2L}{n}\\|z_{\\lambda}^{\\prime}-z_{\\lambda}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Above $(i)$ comes from the monotonicity of $G$ step $(i i)$ comes from Holder's inequality, and step $(i i i)$ comes from the fact that $S$ and $S^{\\prime}$ differ in at most one point. Simple algebra now obtains $\\begin{array}{r}{\\|z_{\\lambda}-z_{\\lambda}^{\\prime}\\|\\leq\\frac{2L}{\\mu\\lambda n}}\\end{array}$ \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Fact 3. There exists a differentiable convex-concave loss, distribution $\\mathcal{D}$ andpoint $z^{\\prime}$ suchthatwhen the operator is the saddle operator of the loss, $\\mathrm{Gapv}_{\\mathsf{I}}(z)<\\mathrm{Gap}_{\\mathsf{S P}}(z)$ ", "page_idx": 13}, {"type": "text", "text": "Proof. We show that the VI-gap is upper bounded by the excess risk of some convex function with operator $g(z)=\\nabla f(z)$ . Since stochastic convex optimization is a special case of SSPs, this shows there exist scenarios where Gapsp $\\nleq\\mathrm{Gapv}_{1}$ . Specifically, when the dual player space $\\Theta$ is singleton, the SP-gap becomes the excess risk. ", "page_idx": 13}, {"type": "text", "text": "Let $f(z;x)=z^{2}$ be defined over $z\\in[0,1]$ , and so it does not matter what the distribution or data domain is. Note that $\\mathrm{Gapv}_{}|$ becomes ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname{Gap}_{\\mathsf{V I}}(z)=\\operatorname*{max}_{u\\in[0,1]}\\left\\{\\langle\\nabla f(u),z-u\\rangle\\right\\}=\\operatorname*{max}_{u\\in[0,1]}\\left\\{2u z-2u^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that in this case the SP-gap is just the excess risk, ${\\mathrm{Gapsp}}(z)=z^{2}-\\operatorname*{min}_{z\\in[0,1]}\\left\\{z^{2}\\right\\}=z^{2}.$ ", "page_idx": 13}, {"type": "text", "text": "Consider the point $z=1$ . It is easy to show that $\\mathrm{Gapv}_{\\mathsf{I}}(1)=0.5$ but ${\\mathrm{Gapsp}}(1)=1$ , proving the claim. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "B  Missing Results from Section 3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1  Proof of Lemma 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 6. (Restatement of Lemma 2) Let $f:\\mathcal{Z}\\times\\mathcal{X}\\mapsto\\mathbb{R}$ be $L$ -Lipschitz. Let $[w^{\\ast},\\theta^{\\ast}]\\in\\mathcal{Z}$ be the population saddle point. For any $x\\in\\mathscr{X}$ define $h([w,\\theta];x)=f(w,\\theta^{*};x)-f(w^{*},\\theta;x)$ For $S\\sim{\\mathcal{D}}^{n}$ let $\\begin{array}{r}{H_{S}(z)=\\frac{\\mathrm{\\hat{1}}}{n}\\sum_{x\\in S}h(z;\\dot{x})}\\end{array}$ and $H_{\\mathcal{D}}(z)=\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[h(z;x)\\right]$ Then for any $\\Delta$ -UAS algorithm, $\\mathcal{A}_{:}$ one has $\\underset{S,A}{\\mathbb{E}}\\left[H_{\\mathcal{D}}(\\mathcal{A}(S))-H_{S}(\\mathcal{A}(S))\\right]\\leq2\\Delta L$ ", "page_idx": 13}, {"type": "text", "text": "Proof. This result follows simply from two facts. First, because $f$ is an $L$ -Lipschitz function for any $x\\in\\mathscr{X}$ , we can show that $h$ is $L$ -Lipschitz. For any $[w,\\theta],[w^{\\prime},\\theta^{\\prime}]\\in\\mathcal{Z}$ observe, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h([w,\\theta])-h([w^{\\prime},\\theta^{\\prime}])=[f(w,\\theta^{*};x)-f(w^{*},\\theta;x)]-[f(w^{\\prime},\\theta^{*};x)-f(w^{*},\\theta^{\\prime};x)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=f(w,\\theta^{*};x)-f(w^{\\prime},\\theta^{*};x)+f(w^{*},\\theta^{\\prime};x)-f(w^{*},\\theta;x)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2L\\lVert[w,\\theta]-[w^{\\prime},\\theta^{\\prime}]\\rVert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The rest of the proof essentially follows from the stability implies generalization proof of [BE02] since $H_{\\mathcal{D}}$ and $H_{S}$ have a statistical form w.r.t. $h$ . In more detail, for any $i\\in[n]$ denote $S^{(i)}$ as the dataset which replaces the $i$ thdatapointof $S,\\,x_{i}$ , with a fresh sample from $\\bar{\\mathcal{D}},\\,x^{\\prime}$ . We have the following: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}_{S,A}\\left[H_{D}(A(S))-H_{S}(\\boldsymbol{A}(S))\\right]=\\frac{\\mathbb{E}}{S,A}\\left[\\mathbb{E}\\left[h(\\boldsymbol{A}(S);\\boldsymbol{x})\\right]-\\frac{1}{n}\\sum_{x\\in S}[h(\\boldsymbol{A}(S);\\boldsymbol{x})]\\right]}\\\\ &{\\displaystyle=_{S,x^{\\prime}\\sim\\mathcal{D}^{\\boldsymbol{n}+1},i\\sim\\mathrm{Uniff}\\left([n]\\right)}\\left[h(\\boldsymbol{A}(S^{(i)});\\boldsymbol{x}_{i})-h(\\boldsymbol{A}(S);\\boldsymbol{x}_{i})\\right]}\\\\ &{\\displaystyle=\\mathbb{E}\\left[h(\\boldsymbol{A}(S^{(i)};\\boldsymbol{x}_{i})-h(\\boldsymbol{A}(S);\\boldsymbol{x}_{i})\\right]}\\\\ &{\\displaystyle\\leq\\mathbb{E}\\left[L\\|\\boldsymbol{A}(S^{(i)})-\\boldsymbol{A}(S)\\|\\right]\\leq2L\\Delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The last step follows from the previously established Lipschitzness property of $h$ ", "page_idx": 14}, {"type": "text", "text": "B.2  Convergence of Recursive Regularization for SSPs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We will first prove the following more general version statement of Theorem 1. ", "page_idx": 14}, {"type": "text", "text": "Theorem 3. Let \u5165 \u2265 48L/2 and $A_{\\mathsf{e m p}}$ be such that for all $t\\ \\ \\in\\ \\ [T]$ it  holdsthat $\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\bar{z}_{t}-z_{S,t}^{*}\\right\\|^{2}\\right]\\leq\\frac{B^{2}}{12\\cdot2^{2t}\\kappa^{3/2}}}\\end{array}$ Then Recursive Regularization satisfes ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{Gap}_{\\mathsf{S P}}(\\mathcal{R}_{S S P})=O\\Big(\\log(n)B^{2}\\lambda\\Big).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To prove this result, it will be helpful to first show several intermediate results. We start by defining several useful quantities. Define $\\bar{\\{\\mathcal F}_{t}\\}_{t=0}^{T}$ as the filtration where $\\mathcal{F}_{t}$ is the sigma algebra induced by all randomness up to $\\bar{z}_{t}$ . For notational convenience we denote $f^{(0)}(w,\\theta;x)=f(w,\\theta;x)$ . Then for every $t\\in\\{0,1,...,T\\}$ we define ", "page_idx": 14}, {"type": "text", "text": "\u00b7 $z_{t}^{*}=[w_{t}^{*},\\theta_{t}^{*}]:\\mathrm{saddle~point~of~}F_{\\mathcal{D}}^{(t)}(w,\\theta):=\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[f^{(t)}(w,\\theta;x)\\right];$   \n\u00b7 $z_{S,t}^{*}=[w_{S,t}^{*},\\theta_{S,t}^{*}]:$ saddle point of $\\begin{array}{r}{F_{S}^{(t)}(w,\\theta):=\\frac{1}{n^{\\prime}}\\sum_{x\\in S_{t}}f^{(t)}(w,\\theta;x);}\\end{array}$   \n\u00b7 $\\begin{array}{r}{H_{\\mathcal{D}}^{(t)}([w,\\theta]):=F_{\\mathcal{D}}^{(t)}(w,\\theta_{t}^{*})-F_{\\mathcal{D}}^{(t)}(w_{t}^{*},\\theta)}\\end{array}$ : the elaive acuraey funtion w.t the popula  \ntion loss and population saddle point; and, $\\begin{array}{r}{H_{S}^{(t)}([w,\\theta]):=F_{S}^{(t)}(w,\\theta_{t}^{\\ast})-F_{S}^{(t)}(w_{t}^{\\ast},\\theta)}\\end{array}$ : the relative accuracy function w.r.t. the empirical loss and population saddle point ", "page_idx": 14}, {"type": "text", "text": "We recll the eneralization properties of $H_{S}^{(t)}$ and $H_{\\mathcal{D}}^{(t)}$ shown in Lemma . Crucially the power of $H_{\\mathcal{D}}^{(t)}$ is tha i ounds the ditance of pinto $z_{t}^{*}$ ", "page_idx": 14}, {"type": "text", "text": "Fact 4 ([ZHWZ21], Theorem 1). Let $F:\\mathcal{Z}\\mapsto\\mathbb{R}^{d}$ bea $\\gamma$ -SC/SC function and let $[w^{\\ast},\\theta^{\\ast}]$ be the saddle point. Then [w,0] - [\\*, \\*]2 \u2264 2(F(w0F) ", "page_idx": 14}, {"type": "text", "text": "We now establish two distance inequalities which will be used when analyzing the final gap bound in Theorem 3. The first inequality below bounds the distance of the output of the $t$ -th round tothe equibriuimnof $G_{\\mathcal{D}}^{(t)}$ The second inequality bounds how far the poplaion equilibrum move ater another regularization term is added. ", "page_idx": 14}, {"type": "text", "text": "Lemma 7. Assume the conditions of Theorem 3 hold. Then for every $t\\in[T]$ the following holds $\\begin{array}{r}{{P I\\ \\mathbb{E}}\\left[\\left\\|\\bar{z}_{t}-z_{t}^{*}\\right\\|\\right]^{2}\\leq\\mathbb{E}\\left[\\left\\|\\bar{z}_{t}-z_{t}^{*}\\right\\|^{2}\\right]\\leq\\frac{B^{2}}{2^{2t}\\kappa},}\\end{array}$ 22tki and, P.2 $\\begin{array}{r}{B_{t}^{2}:=\\mathbb{E}\\left[\\left\\|z_{t}^{*}-z_{t-1}^{*}\\right\\|\\right]^{2}\\leq\\mathbb{E}\\left[\\left\\|z_{t}^{*}-z_{t-1}^{*}\\right\\|^{2}\\right]\\leq\\frac{B^{2}}{2^{2(t-1)}}.}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "We note that in contrast to [BGM23] and other analyses of recursive regularization, we define Property P.2 to bound $\\mathbb{E}\\left[\\left\\Vert z_{t}^{*}-z_{t-1}^{*}\\right\\Vert\\right]$ instead of $\\mathbb{E}\\left[\\Vert z_{t}^{*}-\\bar{z}_{t-1}\\Vert\\right]$ . In [BGM23], the latter choice led to a need to bound $\\mathbb{E}\\left[\\widehat{\\mathrm{Gap}}_{\\mathsf{S P}}\\big(\\bar{z}_{t-1}\\big)\\right]$ for all $t\\in[T]$ , which our analysis avoids. In particular, one can observe how the derivation of Eqn. (11) in the proof changes when $z_{t-1}^{*}$ is replaced with $\\bar{z}_{t-1}$ ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 7. We will prove both properties via induction on $B_{1},...,B_{T}$ . Specifically, for each $t\\in[T]$ we will introduce two terms $E_{t}$ and $F_{t}$ , and show that these terms are bounded if the bound on $B_{t}$ holds and that $B_{t}$ holds if $E_{t-1}$ and $F_{t-1}$ are bounded. Property P.1 is then established as a result of the fact that $\\mathbb{E}\\left[\\left\\lVert\\bar{z}_{t}-z_{t}^{*}\\right\\rVert^{2}\\right]\\le2(E_{t}+F_{t})$ . Note that $B_{1}$ holds as the base case because $\\mathbb{E}\\left[\\left\\|z_{1}^{*}-z_{0}^{*}\\right\\|^{2}\\right]\\leq B^{2}.$ ", "page_idx": 15}, {"type": "text", "text": "Property P.1: We here prove that if $B_{t}$ is sufficiently bounded, then $E_{t}$ and $F_{t}$ are bounded where for $t\\in[T]$ we define ", "page_idx": 15}, {"type": "equation", "text": "$$\nE_{t}=\\mathbb{E}\\left[\\left\\Vert\\bar{z}_{t}-z_{S,t}^{*}\\right\\Vert^{2}\\right],\\qquad\\qquad\\qquad F_{t}=\\frac{\\kappa}{2^{t}\\lambda}\\mathbb{E}\\left[H_{\\mathcal{D}}^{(t)}\\left(z_{S,t}^{*}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Additionally, this will establish property P.1 because for any $t\\in[T]$ it holds that, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\bar{z}_{t}-z_{t}^{*}\\right\\Vert^{2}\\right]\\leq2\\Bigg(\\mathbb{E}\\left[\\left\\Vert\\bar{z}_{t}-z_{S,t}^{*}\\right\\Vert^{2}\\right]+\\mathbb{E}\\left[\\left\\Vert z_{S,t}^{*}-z_{t}^{*}\\right\\Vert^{2}\\right]\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\Bigg(\\underbrace{\\mathbb{E}\\left[\\left\\Vert\\bar{z}_{t}-z_{S,t}^{*}\\right\\Vert^{2}\\right]}_{E_{t}}+\\underbrace{\\frac{\\kappa}{2^{t}\\lambda}\\mathbb{E}\\left[H_{D}^{\\left(t\\right)}\\left(z_{S,t}^{*}\\right)\\right]}_{F_{t}}\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The second inequality comes from the strong monotonicity of the operator (see Fact 4). ", "page_idx": 15}, {"type": "text", "text": "Note that $E_{t}$ is bounded by the assumption made in the statement of the theorem statement. We thus turn our attention towards bounding $F_{t}$ .Wehave ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\kappa}{2^{t}\\lambda}\\mathbb{E}\\left[H_{\\mathcal{D}}^{(t)}\\left(z_{S,t}^{*}\\right)\\right]=\\frac{\\kappa}{2^{t}\\lambda}\\;\\mathbb{E}\\left[\\mathbb{E}\\left[H_{\\mathcal{D}}^{(t)}\\left(z_{S,t}^{*}\\right)\\Big|\\mathcal{F}_{t-1}\\right]\\right]}\\\\ &{\\overset{(i)}{\\leq}\\frac{\\kappa}{2^{t}\\lambda}\\Big(\\mathbb{E}\\left[\\mathbb{E}\\left[H_{S}^{(t)}\\left(z_{S,t}^{*}\\right)\\Big|\\mathcal{F}_{t-1}\\right]\\right]+\\frac{\\kappa L^{2}}{2^{t}\\lambda n^{\\prime}}\\Big)}\\\\ &{=\\frac{\\kappa}{2^{t}\\lambda}\\Big(\\mathbb{E}\\left[\\mathbb{E}\\left[F_{S}^{(t)}(w_{S,t}^{*},\\theta_{t}^{*})-F_{S}^{(t)}(w_{t}^{*},\\theta_{S,t}^{*})\\Big|\\mathcal{F}_{t-1}\\right]\\Big)+\\frac{2\\kappa L^{2}}{2^{t}\\lambda n^{\\prime}}\\Big)}\\\\ &{\\overset{(i i)}{=}\\frac{2\\kappa^{2}L^{2}}{2^{2t}\\lambda^{2}n^{\\prime}}\\leq\\frac{B^{2}}{1152\\cdot2^{2t}\\kappa}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Inqeuality $(i)$ comes from the fact that stability implies generalization for $H^{(t)}$ , Lemma 2. Note the algorithm which outputs thiseact equilibrium poin is -L? stable (see Lemma 5/Assumption 1). Step $(i i)$ uses the fact that $z_{S,t}^{*}$ is the exact saddle point of the regularized objective, and so for any $v,\\theta]\\in\\mathcal{Z},F_{S}^{(t)}(w_{S,t}^{*},\\theta)-\\dot{F_{S}^{(t)}}(w,\\theta_{S,t}^{*})\\leq0$ The final nequality usesthe eting of $\\lambda$ We thus have a fnal bound $\\begin{array}{r}{2(E_{t}+F_{t})\\le\\frac{B^{2}}{2^{2t}}}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Property P.2: Now assume $B_{t-1}$ holds. We have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{Z}\\left[\\left\\Vert z_{t}^{*}-z_{t-1}^{*}\\right\\Vert^{2}\\right]\\leq\\mathbb{E}\\left[\\frac{\\kappa}{2^{t}\\lambda}H_{\\mathcal{D}}^{(t)}(z_{t-1}^{*})\\right]}\\\\ &{=\\mathbb{E}\\left[\\frac{\\kappa}{2^{t}\\lambda}\\left(F_{\\mathcal{D}}^{(t)}(w_{t-1}^{*},\\theta_{t}^{*})-F_{\\mathcal{D}}^{(t-1)}(w_{t}^{*},\\theta_{t-1}^{*})\\right)\\right]}\\\\ &{=\\mathbb{E}\\left[\\frac{\\kappa}{2^{t}\\lambda}\\left(F_{\\mathcal{D}}^{(t-1)}(w_{t-1}^{*},\\theta_{t}^{*})-F_{\\mathcal{D}}^{(t-1)}(w_{t}^{*},\\theta_{t-1}^{*})\\right)\\right.}\\\\ &{\\quad+\\left.\\kappa\\left(\\|w_{t-1}^{*}-\\bar{w}_{t-1}\\|_{w}^{2}-\\|\\theta_{t}^{*}-\\bar{\\theta}_{t-1}\\|_{\\theta}^{2}-\\|w_{t}^{*}-\\bar{w}_{t-1}\\|_{w}^{2}+\\|\\theta_{t}^{*}-\\bar{\\theta}_{t-1}\\|_{\\theta}^{2}\\right)\\right]}\\\\ &{\\overset{(i)}{\\leq}\\mathbb{E}\\left[\\kappa\\|z_{t-1}^{*}-\\bar{z}_{t-1}\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Inequality $(i)$ above comes from removing negative terms and the fact that $z_{t-1}^{*}$ is the saddle point W.rt. $F_{\\mathcal{D}}^{(t-1)}$ Uing thindctonargetwe thpletethbounwihfloig ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert z_{t}^{*}-z_{t-1}^{*}\\right\\Vert^{2}\\right]\\leq\\mathbb{E}\\left[\\kappa\\|z_{t-1}^{*}-\\bar{z}_{t-1}\\|^{2}\\right]\\leq\\kappa(E_{t-1}+F_{t-1})\\leq\\frac{B^{2}}{2^{2t}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We now turn to analyzing the utility of the algorithm to complete the proof. ", "page_idx": 16}, {"type": "text", "text": "proof of Theorem 3. Using the fact that $\\widehat{\\mathrm{Gap}}_{\\mathsf{S P}}$ .s $\\sqrt{2}L$ -Lipschitz and property P.1, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\widehat{\\mathrm{Gap}}_{\\mathsf{S P}}(\\bar{z}_{T})-\\widehat{\\mathrm{Gap}}_{\\mathsf{S P}}(z_{T}^{*})\\right]\\leq\\sqrt{2}L\\mathbb{E}\\left[\\|\\bar{z}_{T}-z_{T}^{*}\\|\\right]}\\\\ {\\leq\\frac{\\sqrt{2}B L}{2^{T}}\\leq\\sqrt{2}B^{2}\\lambda.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Wha remains s showing $\\mathbb{E}\\left[\\widehat{\\mathrm{Gap}}_{\\mathsf{S P}}(w_{T}^{*},\\theta_{T}^{*})\\right]$ .s $\\begin{array}{r}{\\tilde{O}(B\\hat{\\alpha}+\\frac{B L}{\\sqrt{n^{\\prime}}})}\\end{array}$ Let $w^{\\prime}=\\underset{\\theta\\in\\Theta}{\\arg\\operatorname*{min}}\\,F_{\\mathcal{D}}(w,\\theta_{T}^{*})$ and $\\theta^{\\prime}=\\arg\\operatorname*{max}F_{\\mathcal{D}}(w_{T}^{*},\\theta)$ . Using the fact that $F_{\\mathcal{D}}$ is convex-concave we have $w\\!\\in\\!\\mathcal{W}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{Gap}}_{\\mathsf{S P}}(w_{T}^{*},\\theta_{T}^{*})=F_{\\mathcal{D}}(w_{T}^{*},\\theta^{\\prime})-F_{\\mathcal{D}}(w^{\\prime},\\theta_{T}^{*})\\le\\langle G_{\\mathcal{D}}(w_{T}^{*},\\theta_{T}^{*}),[w_{T}^{*},\\theta_{T}^{*}]-[w^{\\prime},\\theta^{\\prime}]\\rangle\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $G_{\\mathcal{D}}$ is the populationlossadl operaor Furtherbytedefnition of ${\\cal F}^{(T)}$ andenoting $G_{\\mathcal{D}}^{(T)}$ as the sadle operator for $F_{\\mathcal{D}}^{(T)}$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nG_{\\mathcal{D}}(w_{T}^{*},\\theta_{T}^{*})=G_{\\mathcal{D}}^{(T)}(w_{T}^{*},\\theta_{T}^{*})-2\\lambda\\sum_{t=0}^{T-1}2^{t+1}\\nabla(\\|[w_{T}^{*},\\theta_{T}^{*}]-[\\bar{w}_{t},\\bar{\\theta}_{t}]\\|^{2})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus plugging the above into Eqn. (13) we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathrm{Gap}}_{\\mathrm{SP}}(w_{T}^{*},\\theta_{T}^{*})\\leq\\Big\\langle G_{D}^{(T)}(w_{T}^{*},\\theta_{T}^{*}),[w_{T}^{*},\\theta_{T}^{*}]-[w^{\\prime},\\theta^{\\prime}]\\Big\\rangle}\\\\ &{\\qquad\\qquad\\qquad-\\left\\langle2\\lambda\\sum_{t=0}^{T-1}2^{t+1}\\nabla(\\|w_{T}^{*},\\theta_{T}^{*}\\|-[\\bar{w}_{t},\\bar{\\theta}_{t}]\\|^{2}),[w_{T}^{*},\\theta_{T}^{*}]-[w^{\\prime},\\theta^{\\prime}]\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leq-\\left\\langle2\\lambda\\sum_{t=0}^{T-1}2^{t+1}\\nabla(\\|w_{T}^{*},\\theta_{T}^{*}\\|-[\\bar{w}_{t},\\bar{\\theta}_{t}]\\|^{2}),[w_{T}^{*},\\theta_{T}^{*}]-[w^{\\prime},\\theta^{\\prime}]\\right\\rangle}\\\\ &{\\qquad\\qquad\\leq2B\\lambda\\sum_{t=0}^{T-1}2^{t+1}\\left\\|\\nabla(\\|w_{T}^{*},\\theta_{T}^{*}\\|-[\\bar{w}_{t},\\bar{\\theta}_{t}]\\|^{2})\\right\\|_{*}}\\\\ &{\\qquad\\qquad\\qquad\\geq2B\\lambda\\sum_{t=0}^{T-1}2^{t+1}\\left\\|[w_{T}^{*},\\theta_{T}^{*}]-[\\bar{w}_{t},\\bar{\\theta}_{t}]\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Above, the second inequality comes from the first order optimally conditions for $[w_{T}^{*},\\theta_{T}^{*}]$ , the third from Cauchy Schwartz and a triangle inequality. The last inequality comes from the relationship between a norm and its dual, see Fact 2. ", "page_idx": 16}, {"type": "text", "text": "Taking the expectation on both sides of the above we have the following derivation, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left.\\bigoplus_{i\\in\\mathcal{N}_{D}}\\left(w_{i}^{*},\\varphi_{i}^{*}\\right)\\right]\\leq2\\mathbb{E}\\left[\\left.\\sum_{k=0}^{\\infty}\\left.\\sum_{l=1}^{D}z_{l}^{-1}\\left.||w_{k}^{*},\\varphi_{i}^{*}|^{2}-|l|\\right.\\right|_{L^{1}}\\right]}\\\\ &{\\overset{(a)}{=}4D\\left[\\left.\\sum_{k=0}^{\\infty}z_{l}^{-1}\\left(\\left[|w_{k}^{*},\\varphi_{i}^{*}|-|\\theta_{k}^{*}|\\right]+\\sum_{l=1}^{\\infty}\\left.||w_{l^{*}}^{*}|+|w_{k}^{*}|-|\\varphi_{l^{*}}^{*}|\\right]\\right)\\right]}\\\\ &{=4D\\left[\\left.\\sum_{k=0}^{\\infty}z_{l}^{-1}\\left(|w_{k}^{*},\\varphi_{i}^{*}|-|\\theta_{l}^{*}|\\right)+\\sum_{l=1}^{\\infty}z_{l}^{-1}\\left.||w_{l^{*}}^{*}|+|w_{k}^{*}|-|\\varphi_{l^{*}}^{*}|\\right]\\right]}\\\\ &{\\overset{(c)}{=}4D\\left[\\left.\\sum_{k=0}^{\\infty}z_{l}^{-1}\\left(|w_{k}^{*},\\varphi_{i}^{*}|-|\\theta_{l}^{*}|\\right)+\\sum_{l=1}^{\\infty}\\left.||w_{l^{*}}^{*}|-|\\varphi_{l^{*}}^{*}|\\right]|\\right.}\\\\ &{=4D\\left[\\left.\\sum_{k=0}^{\\infty}z_{l}^{-1}\\left(|w_{k}^{*},\\varphi_{i}^{*}|-|\\theta_{l}^{*}|\\right)+\\sum_{l=1}^{\\infty}\\sum_{l=1}^{\\infty}z_{l}^{-1}\\left||w_{l^{*}}^{*}+\\theta_{l}^{*}|-|w_{l^{*}}^{*}|\\right]\\right]}\\\\ &{\\overset{(c)}{=}4D\\left[\\left.\\sum_{k=0}^{\\infty}z_{l}^{-1}\\left(|w_{k}^{*},\\varphi_{i}^{*}|-|\\theta_{l}^{*}|\\right)+\\sum_{\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Above, $(i)$ and the following inequality both come from the triangle inequality. Equality $(i i)$ is obtained by rearranging the sums. Inequality $(i i i)$ comes from applying properties P.1 and P.2 proved above. The last equality comes from the setting of $\\lambda$ and $T$ ", "page_idx": 17}, {"type": "text", "text": "Now using this result in conjunction with Eqn. (12) we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{Gap}_{\\mathsf{S P}}(\\mathcal{R})=\\sqrt{2}\\lambda B^{2}+12T\\lambda B^{2}=O\\left(\\log(n)B^{2}\\lambda\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Above we use the fact that $T=\\log(\\frac{L}{B\\lambda})$ and $\\begin{array}{r}{\\lambda\\ge\\frac{L}{B\\sqrt{n^{\\prime}}}}\\end{array}$ BVm, and thus T = O(log(n). ", "page_idx": 17}, {"type": "text", "text": "Finally, we prove Theorem 1 leveraging the relative accuracy assumption. ", "page_idx": 17}, {"type": "text", "text": "Pro f Theorem I.Frst, oserve that under the seting of $\\begin{array}{r}{\\lambda\\,=\\,\\frac{48}{B}\\left(\\hat{\\alpha}\\kappa^{2}+\\frac{L\\kappa^{3/2}}{\\sqrt{n^{\\prime}}}\\right)}\\end{array}$ used in the theorem statement that log(n) B2) = 0 (1og(n)Bak2 + log\\*/2(mn)BLR3/2 . Thus what remains is to show that the distance condition required by Theorem 3 holds. That is, we now show that if $\\mathcal{A}_{\\sf e m p}$ satisfies $\\hat{\\alpha}$ relative accuracy, then forll $t\\in[T]$ it holds that $\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\bar{z}_{t}-z_{S,t}^{*}\\right\\|^{2}\\right]\\leq\\frac{B^{2}}{12\\cdot2^{2t}\\kappa},}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "To prove this property, we must leverage the induction argument made by Lemma 7. Specifically, to prove the conditionholds for some $t\\in[T]$ , assume $\\begin{array}{r}{B_{t}^{\\bar{2}}=\\mathbb{E}\\left[\\left\\lVert z_{t}^{*}-z_{t-1}^{*}\\right\\rVert\\right]^{2}\\leq\\frac{\\bar{B^{2}}}{2^{2(t-1)}}}\\end{array}$ (recali the base case for $t=1$ trivially holds). As shown in the proof of Lemma 7, this implies that the quantities $E_{t},F_{t}$ (as defined in 9) are bounded by y 11522. We thus have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\bar{z}_{t}-z_{S,t}^{*}\\right\\Vert^{2}\\right]\\overset{(i)}{\\leq}\\frac{\\kappa\\mathbb{E}\\left[F_{S}^{(t)}(\\bar{w}_{t},\\theta_{S,t}^{*})-F_{S}^{(t)}(w_{S,t}^{*},\\bar{\\theta}_{t})\\right]}{2^{t}\\lambda}\\overset{(i i)}{\\leq}\\frac{2\\kappa\\hat{\\alpha}B}{2^{2t}\\lambda}\\overset{(i i i)}{\\leq}\\frac{B^{2}}{12\\cdot2^{2t}\\kappa},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Where $B_{t}$ is as defined in propetyP2 Ineqguality $(i)$ coesfromthestrongmonotnicityf $G_{S}^{(t)}$ Fact 4. Inequality $(i i i)$ comes from the setting $\\lambda\\,\\geq\\,48\\hat{\\alpha}\\kappa^{2}/B$ . Inequality $(i i)$ comes from the $\\hat{\\alpha}$ -relative accuracy assumption on $A_{\\mathsf{e m p}}$ : which holds so long as the expected distance is sufficiently bounded and each regularized loss is $(5L)$ -Lipschitz. In this regard, note that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|z_{S,t}^{*}-\\bar{z}_{t-1}\\right\\|\\right]\\leq\\mathbb{E}\\left[\\left\\|z_{S,t}^{*}-z_{t}^{*}\\right\\|+\\left\\|z_{t}^{*}-z_{t-1}^{*}\\right\\|+\\left\\|z_{t-1}^{*}-\\bar{z}_{t-1}\\right\\|\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq(\\sqrt{F_{t}}+B_{t}+\\sqrt{E_{t-1}}+\\sqrt{F_{t-1}})\\leq\\frac{B}{2^{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Further, each $f^{(t)}$ is $5L$ -Lipschitz. We can see that, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\varepsilon\\in\\mathcal{Z}}\\|\\nabla f^{(t)}(z,x)\\|_{*}\\leq L+\\|\\sum_{k=0}^{t-1}2^{k+1}\\lambda\\nabla(\\|z-\\bar{z}_{t}\\|^{2})\\|_{*}\\leq L+\\sum_{k=0}^{t-1}B2^{k+1}\\lambda\\leq L+4B2^{T}\\lambda\\leq5L.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C Missing Results from Section 4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1  General Guarantee for Stochastic Mirror Prox ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We start with the follow general statement regarding the stochastic mirror prox algorithm applied to monotone operators. Notable for our purpose of solving SSPs is that the saddle operator of a convex-concave function is a monotone operator, but the following holds for any monotone operator. ", "page_idx": 18}, {"type": "text", "text": "Lemma 8 (Implicit in [JNT11], Theorem 1). Let $\\psi:\\mathcal{Z}\\mapsto\\mathbb{R}$ be any non-negative function which is 1- strongly convex w.r.t. $\\left\\Vert\\cdot\\right\\Vert$ .Assume $\\forall t\\in[T]$ that E $:[{\\mathcal{O}}(z_{t})]=G(z_{t})$ and R $\\Im\\left[\\lVert\\mathcal{O}(z_{t})-G(z_{t})\\rVert_{*}^{2}\\right]\\leq\\tau^{2}$ Then for for any $z\\in{\\mathcal{Z}}$ Algorithm 2 satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\left\\langle G(z_{t}),z_{t}-z\\right\\rangle\\right]=O\\left(\\frac{\\mathbb{E}\\left[\\psi(z)\\right]}{T\\eta}+\\frac{7\\eta}{2}(L^{2}+2\\tau^{2})\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The above is slightly different than the statement in [JNT11], but can be easily extracted from their proof. Let ${\\mathit{\\bar{V}}}_{\\psi}$ 'denote the Bregman divergence w.r.t. $\\psi$ \uff1bi.e. $V_{\\psi}(z,z^{\\prime})\\,=\\,\\overline{{\\psi}}(z)\\,-\\,\\psi(z^{\\prime})\\,-\\,$ $\\langle\\nabla\\psi(z^{\\prime}),z-z^{\\prime}\\rangle$ . Under our assumptions [JNT11, Eqn. (80)] gives ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\langle G(z_{t}),z_{t}-z\\rangle\\right]=O\\left(\\frac{\\mathbb{E}\\left[V_{\\psi}(z,z_{0})\\right]}{T\\eta}+\\frac{7\\eta}{2}(L^{2}+2\\tau^{2})\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that since $z_{0}$ is the minimizer of $\\psi$ ,we have $V_{\\psi}(z,z_{0})\\leq\\psi(z)$ ", "page_idx": 18}, {"type": "text", "text": "Before proving the relative accuracy guarantee of stochastic mirror prox, we also restate the following composition result for Gaussian mechanism known as the moments accountant. ", "page_idx": 18}, {"type": "text", "text": "Lemma 9 ( $[\\mathbf{A}\\mathbf{C}\\mathbf{G}^{+}16$ , KLL21). Let $\\epsilon,\\delta\\in(0,1]$ and c be a universal constant. Let $D\\in\\mathcal{V}^{n}$ be $a$ dataset over some domain $\\boldsymbol{\\wp}$ and let $h_{1},...,h_{T}:\\mathcal{Y}\\mapsto\\mathbb{R}^{d}$ be a series of (possibly adaptive) queries such that for any $y\\in\\mathcal{V}$ \uff0c $t\\,\\in\\,[T],$ $\\|h_{t}(y)\\|_{2}\\,\\leq\\,L$ Let $\\begin{array}{r}{\\sigma\\,\\geq\\,\\frac{c L\\sqrt{T\\log(1/\\delta)}}{n\\epsilon}}\\end{array}$ and $\\begin{array}{r}{T\\ge\\frac{n^{2}\\epsilon}{b^{2}}}\\end{array}$ Then the algorithm which samples batches of size $B_{1},..,B_{t}$ of size $b$ uniformly at random and outputs $\\begin{array}{r}{\\frac{1}{b}\\sum_{y\\in B_{t}}h_{t}(y)+g_{t}}\\end{array}$ for all $t\\in[T]$ where $g_{t}\\sim\\mathcal{N}(0,\\mathbb{I}_{d}\\sigma^{2})$ is $(\\epsilon,\\delta){-}D P.$ ", "page_idx": 18}, {"type": "text", "text": "C.2 Proof of Lemma 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma 3 is easily established from the following theorem which holds more generally for any monotone operator (rather than just the saddle operator of a convex-concave function). This generalization will allow us to use this theorem again in our results on SVIs. ", "page_idx": 18}, {"type": "text", "text": "Recall we consider the norm $\\begin{array}{r}{\\|[w,\\theta]\\|=\\frac{1}{B_{w}^{2}}\\|w\\|_{\\bar{p}}^{2}\\!+\\!\\frac{1}{B_{\\theta}^{2}}\\|\\theta\\|_{\\bar{q}}^{2}}\\end{array}$ and have defined $\\begin{array}{r}{\\kappa=\\operatorname*{max}\\left\\{\\frac{1}{\\bar{p}-1},\\frac{1}{\\bar{q}-1}\\right\\}}\\end{array}$ and $\\tilde{\\kappa}=1+\\mathbf{1}\\,\\{p<2\\lor q<2\\}\\cdot\\log(d)$ . For any $t\\in\\{0,...,T\\}$ define $[w_{t},\\theta_{t}]=z_{t}$ , where $z_{t}$ is as given in Algorithm 2. We have the following. ", "page_idx": 18}, {"type": "text", "text": "Theorem 4. Let $[w_{0},\\theta_{0}],[w,\\theta]$ satisfy $\\begin{array}{r}{\\mathbb{E}\\left[\\|[w_{0},\\theta_{0}]-[w,\\theta]\\|\\right]\\;\\leq\\;\\hat{D}.\\;\\;L e t\\;g\\;:\\;\\mathcal{W}\\times\\Theta\\times\\mathcal{X}\\;\\mapsto\\;}\\end{array}$ $\\mathcal{B}_{||\\cdot||_{\\bar{p}}}^{d_{w}}(B_{w}L_{w})\\times\\mathcal{B}_{||\\cdot||_{\\bar{q}}}^{d_{\\theta}}(B_{\\theta}L_{\\theta})$ be a monotone operatorand $\\begin{array}{r}{G_{S}(w,\\theta)=\\frac{1}{n}\\sum_{x\\in S}g([w,\\theta];x)}\\end{array}$ There exists an implementation of $\\scriptscriptstyle\\mathcal{O}$ such that Algorithm 2 is $(\\epsilon,\\delta)$ -DPandthefollowingholds ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\langle G_{S}([w_{t^{*}},\\theta_{t^{*}}]),[w_{t^{*}},\\theta_{t^{*}}]-[w,\\theta]\\rangle\\right]=\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\langle G_{S}([w_{t},\\theta_{t}]),[w_{t},\\theta_{t}]-[w,\\theta]\\rangle\\right]}\\\\ {\\displaystyle\\qquad=\\cal O\\left(\\hat{D}\\sqrt{B_{w}^{2}L_{w}^{2}+B_{\\theta}^{2}L_{\\theta}^{2}}\\left(\\frac{\\sqrt{\\kappa}\\sqrt{d\\log(1/\\delta)\\tilde{\\kappa}}}{n\\epsilon}+\\frac{\\sqrt{\\kappa}}{\\sqrt{n}}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Futher tereung agorte $\\begin{array}{r}{O\\big(\\operatorname*{min}\\big\\{\\frac{\\sqrt{\\kappa}n^{2}\\epsilon^{1.5}}{\\log^{2}(n)\\sqrt{d\\log(1/\\delta){\\tilde{\\kappa}}}},\\frac{\\sqrt{}}{\\log}}\\end{array}$ $\\frac{\\sqrt{\\kappa}n^{3/2}}{\\log^{3/2}(n)}\\})$ grden yalua", "page_idx": 19}, {"type": "text", "text": "Before proving this statement, we first quickly show how to obtain Lemma 3, the relative accuracy guarantee for SSPs, using this result. ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma 3. To obtain Lemma 3 from this statement, observe that in the special case where $g$ is the saddle operator of the loss, $f$ ,convexity-concavityimplies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[F(\\frac{1}{T}\\sum_{t=1}^{T}w_{t},w)-F(\\theta,\\frac{1}{T}\\sum_{t=1}^{T}\\theta_{t})\\right]\\leq\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\left\\langle G_{\\mathcal{D}}(z_{t}),z_{t}-z\\right\\rangle\\right],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and Lemma 3 is thus obtained from the bound in Theorem 4. ", "page_idx": 19}, {"type": "text", "text": "All that remains is to prove the above theorem. Note the following proof leverages the structure $\\mathcal{Z}=\\mathcal{W}\\times\\Theta$ , but does not assume that $g$ is the saddle operator of some convex-concave loss. ", "page_idx": 19}, {"type": "text", "text": "Proof of Theorem 4. Let $L=B_{2}^{2}L_{w}^{2}+B_{\\theta}^{2}L_{\\theta}^{2}$ and let $\\begin{array}{r}{p^{*}=\\,\\frac{\\bar{p}}{\\bar{p}-1}}\\end{array}$ and $\\begin{array}{r}{q^{*}=\\frac{\\bar{q}}{\\bar{q}-1}}\\end{array}$ be the conjugate exponents of $\\bar{p}$ and $\\bar{q}$ , respectively. For $t\\in[T]$ denote the result of $\\mathcal{O}(z_{t})$ as $[\\dot{\\nabla}_{w,t},\\nabla_{\\theta,t}]$ such that $\\nabla_{w,t}\\in\\mathbb{R}^{d_{w}}$ and $\\nabla_{\\theta,t}\\in\\mathbb{R}^{d_{\\theta}}$ \uff1a ", "page_idx": 19}, {"type": "text", "text": "We consider the following construction of the private operator oracle, $\\scriptscriptstyle\\mathcal{O}$ ,for $G_{S}$ . Our implementation adds Gaussian noise to minibatch estimates of $G_{S}$ . That is, to evaluate $\\mathcal{O}(z_{t})$ , we uniformly sample a minibatch of size $m\\,=\\,\\operatorname*{max}\\left\\lbrace n\\sqrt{\\frac{\\epsilon}{T}},1\\right\\rbrace$ , call it $M_{t}$ , as well as Gaussian noise vectors $\\xi_{w,t}\\sim\\mathcal{N}(0,\\mathbb{I}_{d_{w}}\\sigma_{w}^{2})$ and $\\xi_{\\theta,t}\\sim\\mathcal{N}(0,\\mathbb{I}_{d_{\\theta}}\\sigma_{\\theta}^{2})$ , for some $\\sigma_{w},\\sigma_{\\theta}>0$ . We then have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n[\\nabla_{w,t},\\nabla_{\\theta,t}]=\\frac{1}{m}\\sum_{x\\in M_{t}}g(w_{t},\\theta_{t};x)+[\\xi_{w,t},\\xi_{\\theta,t}].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Privacy Bound: We first bound the privacy of Algorithm 2. Since for any $u$ $\\|u\\|_{2}\\leq\\sqrt{d^{1-2/p^{*}}}\\|u\\|_{p*}$ we can bound the privacy loss using the guarantees of the moments accountant, Lemma 9. Specifically, we set $\\begin{array}{r}{T=\\kappa\\operatorname*{min}\\left\\{n,\\frac{n^{2}\\epsilon^{2}}{d\\log(1/\\delta)\\tilde{\\kappa}}\\right\\}}\\end{array}$ \uff0c $\\begin{array}{r}{\\eta=\\frac{\\hat{D}}{L\\sqrt{T}}}\\end{array}$ use minbatches of size $m=\\operatorname*{max}\\left\\lbrace n\\sqrt{\\frac{\\epsilon}{T}},1\\right\\rbrace$ and set cBeLu \u221aTad-2/\\* 1g(l/6) and 0g = $\\begin{array}{r}{\\sigma_{\\theta}=\\frac{c B_{\\theta}L_{\\theta}\\sqrt{T d_{\\theta}^{1-2/p^{*}}\\log(1/\\delta)}}{n\\epsilon}}\\end{array}$ for some universal constant $c$ . It can be verified this scale of noise satisfies the conditions of Lemma 9 and thus ensures $(\\epsilon,\\delta)$ -DP. Utility Bound: We now establish the convergence guarantee by applying the general convergence guarantee o stochasticmirror prox (Lemma8,AppendixC.1) with $\\begin{array}{r}{\\dot{\\psi}([\\dot{w},\\theta])=\\frac{\\ast_{\\kappa}}{2B_{w}^{2}}\\|w\\|_{\\bar{p}}^{2}\\!+\\!\\frac{\\kappa}{2B_{\\theta}^{2}}\\|\\theta\\|_{\\bar{q}}^{2}}\\end{array}$ which is 1-strongly convex w.r.t. $\\Vert\\cdot\\Vert$ . Clearly our saddle operator oracle yields unbiased estimates of $G_{S}(z)$ at each iteration. To bound the variance, $\\tau$ , note when $p\\neq2$ the private estimate of $\\nabla_{w,t}$ satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\upzeta\\left[\\left\\lVert\\nabla_{w,t}-\\nabla_{w}F_{S}(w_{t},\\theta_{t})\\right\\rVert_{p^{*}}^{2}\\right]\\overset{(i)}{\\leq}d_{w}^{2/p^{*}}\\mathbb{E}\\left[\\left\\lVert\\xi_{w,t}\\right\\rVert_{\\infty}^{2}\\right]\\leq d_{w}^{2/p^{*}}\\sigma_{w}^{2}\\log(d)\\leq\\frac{c^{2}B_{w}^{2}L_{w}^{2}T d_{w}\\log(1/\\delta)\\log(d)}{n^{2}\\epsilon^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "When $p=2$ then $p^{*}=2$ , and one can replace bound $(i)$ with $\\mathbb{E}\\left[\\|\\xi_{w,t}\\|_{2}^{2}\\right]$ . Combining these cases yields a bound of (2 B2 L2.Tdu lg(1/)x ) on the variance of $\\nabla_{w,t}$ . A similar analysis holds for $\\nabla_{\\theta,t}$ Ultimately, with respect to the norm chosen above, we get F $\\colon[\\|\\mathcal{O}(z_{t})-G(z_{t})\\|_{*}^{2}]\\leq\\tau^{2}$ with ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tau^{2}\\le(B_{w}^{2}L_{w}^{2}+B_{\\theta}^{2}L_{\\theta}^{2})\\left(1+\\frac{c^{2}T d\\log(1/\\delta)\\tilde{\\kappa}}{n^{2}\\epsilon^{2}}\\right)=O(L^{2}\\kappa).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The last qualityuses the fact that $\\kappa\\geq1$ Recall we choose $\\begin{array}{r}{\\psi([w,\\theta])=\\frac{\\kappa}{2B_{w}^{2}}\\|w\\|_{\\bar{p}}^{2}+\\frac{\\kappa}{2B_{\\theta}^{2}}\\|\\theta\\|_{\\bar{q}}^{2}}\\end{array}$ and thus $\\psi([w,\\theta])=\\kappa\\|[w,\\theta]\\|^{2}$ . Now the guarantees of Lemma 8 imply ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\langle G_{S}(z_{t^{*}}),z_{t^{*}}-z\\rangle\\right]=\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=1}^{T}\\langle G_{D}(z_{t}),z_{t}-z\\rangle\\right]=O\\left(\\frac{\\kappa\\hat{D}^{2}}{T\\eta}+\\frac{7\\eta}{2}(L^{2}+2\\tau^{2})\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The theorem statement now follows from plugging in the parameter settings of $T$ and $\\eta$ and the bound on $\\tau$ established above. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "D Missing Results from Section 5 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "D.1 Proof of Lemma 4 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Let $h_{1}(z;x)=\\langle g_{1}(z;x),z-z^{*}\\rangle$ . First, we observe that $h_{1}$ is $(\\beta B+L)$ -Lipschitz. To see this, we have for any $z,z^{\\prime}\\in{\\mathcal{Z}}$ and $x\\in$ that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle g_{1}(z),z-z^{*}\\rangle-\\langle g_{1}(z^{\\prime}),z^{\\prime}-z^{*}\\rangle=\\langle g_{1}(z)-g_{1}(z^{\\prime})+g_{1}(z^{\\prime}),z-z^{*}\\rangle-\\langle g_{1}(z^{\\prime}),z^{\\prime}-z^{*}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\langle g_{1}(z)-g_{1}(z^{\\prime}),z-z^{*}\\rangle+\\langle g_{1}(z^{\\prime}),z-z^{\\prime}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq(\\beta B+L)\\|z-z^{\\prime}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The rest of the proof is similar to existing stability implies generalization proofs, but with the additional accounting of the regularization term. In more detail, for any $i\\in[n]$ denote $S^{(i)}$ as the dataset which replaces the $i$ th datapoint of $S,\\,x_{i}$ , with a fresh sample from $\\mathcal{D},\\,x^{\\prime}$ .We have the following: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathbb{E}}{S,A}\\big[H_{D}(A(S))-H_{S}(A(S))\\big]}\\\\ &{=\\frac{\\mathbb{E}}{S,A}\\Bigg[\\bigg\\langle\\mathbb{E}\\big[g_{1}(A(S);x)\\big]+g_{2}(z),A(S)-z^{*}\\bigg\\rangle-\\bigg\\langle\\cfrac{1}{n}\\sum_{x\\in S}[g_{1}(A(S);x)]+g_{2}(z),A(S)-z^{*}\\bigg\\rangle\\Bigg]}\\\\ &{=\\frac{\\mathbb{E}}{S,A}\\Bigg[\\bigg\\langle\\mathbb{E}\\big[g_{1}(A(S);x)\\big],A(S)-z^{*}\\bigg\\rangle-\\bigg\\langle\\cfrac{1}{n}\\sum_{x\\in S}[g_{1}(A(S);x)],A(S)-z^{*}\\bigg\\rangle\\Bigg]}\\\\ &{=\\frac{\\mathbb{E}}{S,x^{\\prime}\\sim\\mathcal{D}^{n+1},i\\sim\\mathcal{D}(n)}\\Big[\\big\\langle g_{1}(A(S^{(i)});x_{i}),A(S^{(i)})-z^{*}\\big\\rangle-\\big\\langle g_{1}(A(S);x_{i}),A(S)-z^{*}\\big\\rangle\\Big]}\\\\ &{=\\mathbb{E}\\left[h_{1}(A(S^{(i)});x_{i})-h_{1}(A(S);x_{i})\\right]}\\\\ &{\\leq\\mathbb{E}\\left[(\\beta B+L)\\|A(S^{(i)})-A(S)\\|\\right]\\leq\\left(\\beta B+L\\right)\\Delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The last step follows from the previously established Lipschitzness property of $h_{1}$ ", "page_idx": 20}, {"type": "text", "text": "D.2  Convergence of Recursive Regularization for SVIs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we define $\\tilde{L}\\,=\\,\\beta B+L$ . Define $\\operatorname{Gap}_{\\mathsf{V I}}(z)\\,=\\,\\operatorname*{max}_{z^{\\prime}}\\left\\{\\langle z^{\\prime},z-z^{\\prime}\\rangle\\right\\}$ .We have the following fact. ", "page_idx": 20}, {"type": "text", "text": "Fact 5. If $g$ is $L$ -bounded then $\\widehat{\\mathrm{Gapv}}_{}$ is $L$ -Lipschitz. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathrm{Gapv}_{1}}(z_{1})-\\widehat{\\mathrm{Gapv}_{1}}(z_{2})=\\underset{z}{\\mathrm{max}}\\left\\{\\langle G_{\\mathcal{D}}(z),z_{1}-z\\rangle\\right\\}-\\underset{z^{\\prime}}{\\mathrm{max}}\\left\\{\\langle G_{\\mathcal{D}}(z^{\\prime}),z_{2}-z^{\\prime}\\rangle\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\underset{z}{\\mathrm{max}}\\left\\{\\langle G_{\\mathcal{D}}(z),z_{1}-z\\rangle-\\langle G_{\\mathcal{D}}(z),z_{1}-z\\rangle\\right\\}}\\\\ &{\\qquad\\qquad\\qquad=\\underset{z}{\\mathrm{max}}\\left\\{\\langle G_{\\mathcal{D}}(z),z_{1}-z_{2}\\rangle\\right\\}}\\\\ &{\\qquad\\qquad\\leq\\lVert G_{\\mathcal{D}}(z)\\rVert_{*}\\lVert z_{1}-z_{2}\\rVert\\leq L\\lVert z_{1}-z_{2}\\rVert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We recall the assumption made on $\\rho$ ", "page_idx": 20}, {"type": "text", "text": "Assumption 2. For some $\\kappa\\,>\\,0$ let $\\rho:\\mathcal{Z}\\mapsto\\mathbb{R}^{d}$ be $\\frac{1}{\\kappa}$ -strongly monotone w.rt. $\\|\\cdot\\|$ and satisfy $\\|\\rho(z)\\|_{*}^{-}\\leq\\|z\\|$ for all $z\\in{\\mathcal{Z}}$ ", "page_idx": 20}, {"type": "text", "text": "We will first prove the following more general version statement of Theorem 2, which will be useful later. ", "page_idx": 20}, {"type": "text", "text": "Theorem 5. Let $\\begin{array}{r l r}{\\lambda}&{{}\\ge}&{\\frac{48\\tilde{L}\\kappa^{2}}{B\\sqrt{n^{\\prime}}}}\\end{array}$ and $A_{\\mathsf{e m p}}$ be suchthat for all $t\\ \\ \\in\\ \\ [T]$ it holdsthat $\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\bar{z}_{t}-z_{S,t}^{*}\\right\\|^{2}\\right]\\leq\\frac{B^{2}}{12\\cdot2^{2t}\\kappa^{2}}}\\end{array}$ 2.Then Recursive Regularization satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{Gap}_{\\vee1}(\\mathcal{R}_{V I})=O\\Big(\\log(n)B^{2}\\lambda\\Big).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To prove this result, it will be helpful to first show several intermediate results. We start by defining several useful quantities. Define $\\left\\{\\mathcal{F}_{t}\\right\\}_{t=0}^{T}$ as the filtration where $\\mathcal{F}_{t}$ is the sigma algebra induced by all randomness up to $\\bar{z}_{t}$ . For notational convenience we define $g^{(0)}(z;x)=g(z;x)$ . Then for every $t\\in\\{0,1,...,T\\}$ we define ", "page_idx": 21}, {"type": "text", "text": "$\\bullet\\;z_{t}^{*}:{\\mathrm{equilibrium~of~}}G_{\\mathcal{D}}^{(t)}(z):=\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[g^{(t)}(z;x)\\right];$   \n\u00b7 $z_{S,t}^{*}$ : equilibriuim of $\\begin{array}{r}{G_{S}^{(t)}(z):=\\frac{1}{n^{\\prime}}\\sum_{x\\in S_{t}}g^{(t)}(z;x)}\\end{array}$   \n\u00b7 $H_{\\mathcal{D}}^{(t)}(\\bar{z}):=\\left<G_{\\mathcal{D}}^{(t)}(\\bar{z}),\\bar{z}-z_{t}^{*}\\right>$ threlative stationarity fution w.. $G_{\\mathcal{D}}^{(t)}$ and $z_{t}^{*}$ and,   \n\u00b7 $H_{S}^{(t)}(\\bar{z}):=\\left<G_{S}^{(t)}(\\bar{z}),\\bar{z}-z_{t}^{*}\\right>:$ the relative stationarity function w.r.t. $G_{S}^{(t)}$ and $z_{t}^{*}$ ", "page_idx": 21}, {"type": "text", "text": "As discussed in Section 5.2, the power of $H_{\\mathcal{D}}^{(t)}$ is that it bounds the distance ofa point to $z_{t}^{*}$ ", "page_idx": 21}, {"type": "text", "text": "Fact 6. Let $G:\\mathcal{Z}\\mapsto\\mathbb{R}^{d}$ be a $\\mu$ -strongly monotone operator andlet $z^{*}$ be the equilibrium point. Then |/2  2\\*12\u2264 2(G(2)- ", "page_idx": 21}, {"type": "text", "text": "Proof. By strong monotonicity, for any $z\\in{\\mathcal{Z}}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\mu}{2}\\|z-z^{*}\\|^{2}\\leq\\left\\langle G(z)-G(z^{*}),z-z^{*}\\right\\rangle\\leq\\left\\langle G(z),z-z^{*}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The last step comes from the fact that $\\langle-G(z^{*}),z-z^{*}\\rangle=\\langle G(z^{*}),z^{*}-z\\rangle\\leq0$ since $z^{*}$ is the equilibrium. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "We now establish two distance inequalities which will be used when analyzing the final gap bound in Theorem 5. The first inequality below bounds the distance of the output of the $t$ -throundtothe equibriumof $G_{\\mathcal{D}}^{(t)}$ The second inequlity bounds how far the population quilibria moves after another regularization term is added. ", "page_idx": 21}, {"type": "text", "text": "Lemma 10. Assume the conditions of Theorem 5 hold. Then for every $t\\in[T]$ the following holds ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\textbf{\\emph{P2}}B_{t}^{2}:=\\mathbb{E}\\left[\\left\\|z_{t}^{*}-z_{t-1}^{*}\\right\\|\\right]^{2}\\leq\\mathbb{E}\\left[\\left\\|z_{t}^{*}-z_{t-1}^{*}\\right\\|\\right|^{2}\\right]\\leq\\frac{B^{2}}{2^{2(t-1)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We will prove both properties via induction on $B_{1},...,B_{T}$ .Specifically, for each $t\\,\\in\\,[T]$ we will introduce two terms $E_{t}$ and $F_{t}$ , and show that these terms are bounded if the bound on $B_{t}$ holds and that $B_{t}$ holds if $E_{t-1}$ and $F_{t-1}$ are bounded. Property P.1 is then established as a result of the fact that $\\mathbb{E}\\left[\\left\\Vert\\bar{z}_{t}-z_{t}^{*}\\right\\Vert^{2}\\right]\\leq2(E_{t}+F_{t})$ . Note that $B_{1}$ holds as the base case because $\\mathbb{E}\\left[\\left\\|z_{1}^{*}-z_{0}^{*}\\right\\|^{2}\\right]\\leq B^{2}.$ ", "page_idx": 21}, {"type": "text", "text": "Property P.1: We here prove that if $B_{t}$ is sufficiently bounded, then $E_{t}$ and $F_{t}$ are bounded where for $t\\in[T]$ we define ", "page_idx": 21}, {"type": "equation", "text": "$$\nE_{t}=\\mathbb{E}\\left[\\left\\Vert\\bar{z}_{t}-z_{S,t}^{*}\\right\\Vert^{2}\\right],\\qquad\\qquad\\qquad F_{t}=\\frac{\\kappa}{2^{t}\\lambda}\\mathbb{E}\\left[H_{\\mathcal{D}}^{(t)}\\left(z_{S,t}^{*}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Additionally, this will establish property P.1 because for any $t\\in[T]$ it holds that, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\bar{z}_{t}-z_{t}^{*}\\right\\Vert^{2}\\right]\\leq2\\Bigg(\\mathbb{E}\\left[\\left\\Vert\\bar{z}_{t}-z_{S,t}^{*}\\right\\Vert^{2}\\right]+\\mathbb{E}\\left[\\left\\Vert z_{S,t}^{*}-z_{t}^{*}\\right\\Vert^{2}\\right]\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\Bigg(\\underbrace{\\mathbb{E}\\left[\\left\\Vert\\bar{z}_{t}-z_{S,t}^{*}\\right\\Vert^{2}\\right]}_{E_{t}}+\\underbrace{\\frac{\\kappa}{2^{t}\\lambda}\\mathbb{E}\\left[H_{D}^{\\left(t\\right)}\\left(z_{S,t}^{*}\\right)\\right]}_{F_{t}}\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The second inequality comes from the strong monotonicity of the operator (see Fact 6). ", "page_idx": 22}, {"type": "text", "text": "Since $E_{t}$ is bounded by the assumption made in the statement of Theorem 5, we focus on bounding $F_{t}$ Wehave ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\kappa}{2^{t}\\lambda}\\mathbb{E}\\left[H_{\\mathcal{D}}^{(t)}\\left(z_{S,t}^{*}\\right)\\right]=\\frac{\\kappa}{2^{t}\\lambda}\\;\\mathbb{E}\\left[\\mathbb{E}\\left[H_{\\mathcal{D}}^{(t)}\\left(z_{S,t}^{*}\\right)\\Big|\\mathcal{F}_{t-1}\\right]\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\kappa}{2^{t}\\lambda}\\Big(\\mathbb{E}\\left[\\mathbb{E}\\left[H_{S}^{(t)}\\left(z_{S,t}^{*}\\right)\\Big|\\mathcal{F}_{t-1}\\right]\\right]+\\frac{\\kappa\\tilde{L}^{2}}{2^{t}\\lambda n^{\\prime}}\\Big)}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\kappa}{2^{t}\\lambda}\\Big(\\mathbb{E}\\left[\\mathbb{E}\\left[\\left.\\left\\langle G_{S}^{(t)}(z_{S,t}^{*}),z_{S,t}^{*}-z_{t}^{*}\\right\\rangle\\right|\\mathcal{F}_{t-1}\\right]\\right]+\\frac{\\kappa\\tilde{L}^{2}}{2^{t}\\lambda n^{\\prime}}\\Big)}\\\\ &{\\qquad\\qquad\\leq\\frac{\\kappa^{2}\\tilde{L}^{2}}{2^{2t}\\lambda^{2}n^{\\prime}}\\leq\\frac{B^{2}}{2304\\cdot2^{2t}\\kappa^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The first inequality comes from the fact that stability implies generalization for $H^{(t)}$ , Lemma 4. Note the algorithm which outputs this exact equilibrium point is $\\frac{\\l_{L}}{2^{t}\\lambda n^{\\prime}}$ uniform argument stable (see Lemma 5/Assumption 1). The second inequality comes from the fact that $z_{S,t}^{*}$ is the exact empirical equilibrim point o the reglarized oejetive and o for any $z\\in{\\mathcal{Z}}$ \uff0c $\\left\\langle G_{S}^{(t)}(z_{S,t}^{*}),z_{S,t}^{*}-z\\right\\rangle\\leq0$ The final inequality uses the setting of $\\lambda$ ", "page_idx": 22}, {"type": "text", "text": "We thus have a final bound $\\begin{array}{r}{2(E_{t}+F_{t})\\le\\frac{B^{2}}{2^{2t}}}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Property P.2: Now assume $B_{t-1}$ holds. We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert z_{t}^{*}-z_{t-1}^{*}\\right\\Vert^{2}\\right]\\leq\\mathbb{E}\\left[\\frac{\\kappa}{2^{t}\\lambda}\\left\\langle G_{D}^{(t)}(z_{t-1}^{*}),z_{t-1}^{*}-z_{t}^{*}\\right\\rangle\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}\\left[\\frac{\\kappa}{2^{t}\\lambda}\\left\\langle G_{D}^{(t-1)}(z_{t-1}^{*})+2^{t}\\lambda\\rho(z_{t-1}^{*}-\\bar{z}_{t-1}),z_{t-1}^{*}-z_{t}^{*}\\right\\rangle\\right]}\\\\ &{\\qquad\\qquad\\quad=\\mathbb{E}\\left[\\frac{\\kappa}{2^{t}\\lambda}\\left\\langle G_{D}^{(t-1)}(z_{t-1}^{*}),z_{t-1}^{*}-z_{t}^{*}\\right\\rangle+\\kappa\\left\\langle\\rho(z_{t-1}^{*}-\\bar{z}_{t-1}),z_{t-1}^{*}-z_{t}^{*}\\right\\rangle\\right]}\\\\ &{\\overset{(i)}{\\leq}\\mathbb{E}\\left[\\frac{\\kappa^{2}}{2}\\|\\rho(z_{t-1}^{*}-\\bar{z}_{t-1})\\|_{*}^{2}+\\frac{1}{2}\\|z_{t-1}^{*}-z_{t}^{*}\\|^{2}\\right]}\\\\ &{\\overset{(i i)}{\\leq}\\mathbb{E}\\left[\\frac{\\kappa^{2}}{2}\\|z_{t-1}^{*}-\\bar{z}_{t-1}\\|^{2}+\\frac{1}{2}\\|z_{t-1}^{*}-z_{t}^{*}\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Inequality $(i)$ above comes from Young's inequality and the fact that $z_{t-1}^{*}$ is the equilibrium point $G_{\\mathcal{D}}^{(t-1)}$ $(i i)$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|z_{t}^{*}-z_{t-1}^{*}\\right\\|^{2}\\right]\\leq\\kappa^{2}\\mathbb{E}\\left[\\|z_{t-1}^{*}-\\bar{z}_{t-1}\\|^{2}\\right]\\leq\\kappa^{2}(E_{t-1}+F_{t-1})\\leq\\frac{B^{2}}{2^{2t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We now turn to analyzing the utility of the algorithm to complete the proof. ", "page_idx": 22}, {"type": "text", "text": "Proof of Theorem 5. Using the fact that $\\widehat{\\mathrm{Gapv}_{\\mathrm{I}}}$ is $L$ -Lipschitz and property P.1, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}\\left[\\widehat{\\mathrm{Gapv}_{1}}(\\bar{z}_{T})-\\widehat{\\mathrm{Gapv}_{1}}(z_{T}^{*})\\right]\\leq L\\mathbb{E}\\left[\\|\\bar{z}_{T}-z_{T}^{*}\\|\\right]}\\\\ &{}&{\\leq\\frac{B L}{2^{T}}\\leq B^{2}\\lambda.\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that because the above is a statement with respect to the unregularized gap function, we do not have to worry about whether or not the regularization term is smooth. ", "page_idx": 22}, {"type": "text", "text": "What remains is showing $\\mathbb{E}\\left[\\widehat{\\mathrm{Gapv}_{\\mathsf{I}}}(z_{T}^{*})\\right]=O(\\log(n)B^{2}\\lambda)$ By the definition of $G_{\\mathcal{D}}^{(T)}$ we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nG_{\\mathcal{D}}(z)=G_{\\mathcal{D}}^{(T)}(z)-2\\lambda\\sum_{t=0}^{T-1}2^{t+1}\\rho(z-\\bar{z}_{t})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $z^{\\prime}=\\arg\\operatorname*{max}_{z^{\\prime}\\in\\mathcal{Z}}\\left\\{\\langle G_{\\mathcal{D}}(z^{\\prime}),z_{T}^{*}-z^{\\prime}\\rangle\\right\\}$ We obtain the following bound on the $\\widehat{\\mathrm{Gapv}_{\\mathsf{I}}}(w_{T}^{*},\\theta_{T}^{*})$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathrm{Gapp}}(z_{T}^{*})=\\left\\langle G_{p}^{(T)}(z^{\\prime}),z_{T}^{*}-z^{\\prime}\\right\\rangle+\\left\\langle2\\lambda\\displaystyle\\sum_{t=0}^{T-1}2^{t+1}\\rho(z^{\\prime}-\\bar{z}_{t}),z^{\\prime}-z_{T}^{*}\\right\\rangle}\\\\ &{\\overset{(i)}{\\leq}\\left\\langle2\\lambda\\displaystyle\\sum_{t=0}^{T-1}2^{t+1}\\rho(z^{\\prime}-\\bar{z}_{t}),z^{\\prime}-z_{T}^{*}\\right\\rangle}\\\\ &{\\overset{(i i)}{\\leq}\\left\\langle2\\lambda\\displaystyle\\sum_{t=0}^{T-1}2^{t+1}\\rho(z_{T}^{*}-\\bar{z}_{t}),z_{T}^{*}-z^{\\prime}\\right\\rangle}\\\\ &{\\overset{(i i i)}{\\leq}2B\\displaystyle\\sum_{t=0}^{T-1}2^{t+1}\\left\\lvert\\left\\lvert\\rho(z_{T}^{*}-\\bar{z}_{t})\\right\\rvert,}\\\\ &{\\overset{(i i i)}{\\leq}2B\\displaystyle\\sum_{t=0}^{T-1}2^{t+1}\\left\\lVert\\rho(z_{T}^{*}-\\bar{z}_{t})\\right\\rVert,}\\\\ &{\\overset{(i v)}{\\leq}2B\\displaystyle\\sum_{t=0}^{T-1}2^{t+1}\\left\\lVert z_{T}^{*}-\\bar{z}_{t}\\right\\rVert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Above, $(i)$ comes from the fact that $z_{T}^{*}$ is the equibrium pointof $G_{\\mathcal{D}}^{(T)}$ .Inequality $(i i)$ uses monotonicity of $\\rho$ , i.e. $0\\;\\leq\\;\\langle\\rho(z_{T}^{*}-{\\bar{z}}_{T})-\\rho(z^{\\prime}-{\\bar{z}}_{T}),z_{T}^{*}-z^{\\prime}\\rangle$ . Inequality $(i i i)$ comes from Holder's inequality and a triangle inequality. Finally, $(i v)$ comes from Assumption 1. ", "page_idx": 23}, {"type": "text", "text": "Taking the expectation on both sides of the above we have the following derivation, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left(\\overline{{\\mathbf{S}}}\\mathbf{\\Phi}\\right)\\mathbf{\\Phi}\\middle|\\left(x_{j}^{*}\\right)\\leq2\\mathbb{E}\\left[\\left(\\begin{array}{l}{\\widehat{\\lambda}_{j}\\leq r^{2}+1}\\\\ {r\\leq2}\\\\ {1+\\widehat{\\lambda}_{j}\\leq r}\\end{array}\\right)\\right]}\\\\ {\\leq\\frac{1}{4}\\mathbb{E}\\left[\\sum_{k=0}^{r}\\left(\\mathbf{S}\\right)^{\\top}\\left(\\mathbf{S}\\right)^{\\top}\\underset{s=1}{\\sum}\\Bigg(\\sum_{s=1}^{r-1}\\sum_{t=1}^{r-1}c_{s}\\left(\\mathbf{S}\\right)^{\\top}\\right)\\right]}\\\\ {=4\\mathbb{E}\\Bigg[\\sum_{k=0}^{r-1}\\mathbb{E}\\left[c_{s}-z_{j}\\right]+\\sum_{s=1}^{r-1}\\sum_{t=1}^{r-1}\\left(\\mathbf{S}\\right)^{\\top}\\left(c_{s}-z_{j}\\right]\\Bigg)}\\\\ {\\leq\\alpha\\left(\\sum_{s=1}^{r-1}\\mathbb{E}\\left[c_{s}-z_{j}\\right]+\\sum_{s=1}^{r-1}\\sum_{t=1}^{r-1}\\mathbb{E}\\left[c_{s}-z_{j}\\right]\\right)}\\\\ {=4\\mathbb{E}\\left[\\sum_{s=0}^{r-1}c_{j}\\right]-c_{j}\\right]+\\frac{1}{4}\\sum_{s=1}^{r-1}c_{j}\\geq\\left[c_{s}z_{j}-z_{j}\\right]}\\\\ {=4\\mathbb{E}\\left[\\sum_{s=0}^{r-1}c_{j}\\right]\\mathbb{I}\\left[c_{s}-z_{j}\\right]+\\lambda\\geq\\sum_{s=1}^{r-1}\\left(\\sum_{t=1}^{r-1}c_{j}\\right)\\sum_{s=1}^{r-1}\\Bigg]}\\\\ {\\stackrel{(a)}{\\leq}\\alpha\\left(\\lambda\\geq\\sum_{s=1}^{r-1}c_{j}\\left(\\frac{\\lambda}{2}\\right)+\\sum_{s=1}^{r-1}\\left(\\frac{\\lambda}{2}\\right)\\sum_{s=1}^{r-1}z_{j}\\right)}\\\\ {\\leq4\\alpha\\left(\\sum_{s=0}^{r-1}c_{j}\\right)+\\lambda\\geq\\left(\\frac{\\lambda}{2}\\right)\\sum_{s=1}^{r-1}\\left(\\frac{\\lambda}{2}\\right)\\langle c_{s}z_{j}\\rangle-1\\right)}\\\\ {=4\\mathbb{E}\\Bigg[ \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Above, $(i)$ and the following inequality both come from the triangle inequality. Equality $(i i)$ is obtained by rearranging the sums. Inequality $(i i i)$ comes from applying properties P.1 and P.2 proved above. The last equality comes from the setting of $\\lambda$ and $T$ ", "page_idx": 23}, {"type": "text", "text": "Now using this result in conjunction with Eqn. (18) we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{Gap}_{\\vee}(\\mathcal{R}_{\\mathrm{SVI}})=\\sqrt{2}\\lambda B^{2}+12T\\lambda B^{2}=O\\left(\\log(n)B^{2}\\lambda\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Above we use the fact that $T=\\log(\\frac{L}{B\\lambda})$ and $\\begin{array}{r}{\\lambda\\ge\\frac{L}{B\\sqrt{n^{\\prime}}}}\\end{array}$ and thus $T=O(\\log(n))$ ", "page_idx": 23}, {"type": "text", "text": "Finally, we prove Theorem 2 leveraging the relative stationarity assumption. ", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem 2. First, oserve that under the setting of $\\begin{array}{r}{\\lambda\\,=\\,\\frac{48}{B}\\left(\\hat{\\alpha}\\kappa^{3}+\\frac{\\tilde{L}\\kappa^{2}}{\\sqrt{n^{\\prime}}}\\right)}\\end{array}$ used in the theorem temen that $\\begin{array}{r}{\\log(n)B^{2}\\lambda=O\\left(\\log(n)B\\hat{\\alpha}\\kappa^{3}+\\frac{\\log^{3/2}(n)B\\tilde{L}\\kappa^{2}}{\\sqrt{n}}\\right)}\\end{array}$ Thus what remains i to show that the distance condition required by Theorem 5 holds. That is, we now show that if $\\mathcal{A}_{\\sf e m p}$ satisies $\\hat{\\alpha}$ relative stationarity, tnfol $t\\in[T]$ it holds that $\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\bar{z}_{t}-z_{S,t}^{*}\\right\\|^{2}\\right]\\leq\\frac{B^{2}}{12\\cdot2^{2t}\\kappa^{2}}}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "To prove this property, we must leverage the induction argument made by Lemma 10. Specifically, to prove the condition holds for some $t\\in[T]$ , assume $\\begin{array}{r}{\\bar{B_{t}^{2}}=\\mathbb{E}\\left[\\left\\|z_{t}^{*}-\\bar{z_{t-1}^{*}}\\right\\|\\right]^{2}\\leq\\frac{\\bar{B^{2}}}{2^{2(t-1)}}}\\end{array}$ (recall the base case for $t=1$ trivially holds). As shown in the proof of Lemma 10, this implies that the quanties $E_{t},F_{t}$ (as defnd in 16) are bouded by $\\frac{B^{2}}{2304{\\cdot}2^{2t}}$ .We thus have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\bar{z}_{t}-z_{S,t}^{*}\\right\\Vert^{2}\\right]\\overset{(i)}{\\leq}\\frac{\\kappa\\mathbb{E}\\left[\\left\\langle G_{S}^{(t)}(\\bar{z}_{t}),\\bar{z}_{t}-z_{S,t}^{*}\\right\\rangle\\right]}{2^{t}\\lambda}\\overset{(i i)}{\\leq}\\frac{2\\kappa\\hat{\\alpha}B}{2^{2t}\\lambda}\\overset{(i i i)}{\\leq}\\frac{B^{2}}{12\\cdot2^{2t}\\kappa^{2}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Where $B_{t}$ isasdefinedinpropety P2 Inequality $(i)$ comes from the strong monotonity of $G_{S}^{(t)}$ Fact 6. Inequality comes from the setting $\\lambda\\ge48\\hat{\\alpha}\\kappa/B$ . Inequality comes from the $\\hat{\\alpha}$ relative stationarity assumption on $\\mathcal{A}_{\\sf e m p}$ , which holds so long as the expected distance is sufficiently bounded and the operator is bounded. In this regard, note that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|z_{S,t}^{*}-\\bar{z}_{t-1}\\right\\|\\right]\\leq\\mathbb{E}\\left[\\left\\|z_{S,t}^{*}-z_{t}^{*}\\right\\|+\\left\\|z_{t}^{*}-z_{t-1}^{*}\\right\\|+\\left\\|z_{t-1}^{*}-\\bar{z}_{t-1}\\right\\|\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq(\\sqrt{F_{t}}+B_{t}+\\sqrt{E_{t-1}}+\\sqrt{F_{t-1}})\\leq\\frac{2B}{2^{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Further, each $g^{(t)}$ is $5L$ -bounded. That is, observe ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{z\\in\\mathcal{Z}}\\|g^{(t)}(z,x)\\|_{*}\\leq\\|z\\|_{*}+\\sum_{k=0}^{t-1}2^{k+1}\\lambda\\|\\rho(z-\\bar{z}_{t})\\|_{*}\\leq L+\\sum_{k=0}^{t-1}B2^{k+1}\\lambda\\leq L+4B2^{T}\\lambda\\leq5L.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "D.3  Near Linear Time Algorithm for SVIs in the $\\ell_{2}$ Setting ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Because we assume the operator is Lipschitz, in the $\\ell_{2}$ setting, we can leverage existing accelerated optimization techniques to achieve a near linear time version of $A_{\\mathsf{e m p}}$ , in a similar fashion to [ZTOH22, BGM23]. Specifically, the work [PB16] gives the following result for strongly monotone variational inequalities when applying their accelerated SVRG algorithm 3. ", "page_idx": 24}, {"type": "text", "text": "Lemma 11. (Implicit in [PB16, Theorem $3J$ )Let $\\beta,\\mu,K\\,>\\,0$ and c a universal constant. Let $g:\\mathcal{Z}\\times\\mathcal{X}\\mapsto\\mathbb{R}$ be monotone and $\\beta$ -Lipschitz and $\\rho:\\mathcal{Z}\\mapsto\\mathbb{R}$ a $\\mu$ -strongly monotone operator. Let $z_{S}^{*}$ be the equilibrium of $\\begin{array}{r}{G_{S}(z)=\\frac{1}{n}\\sum_{x\\in S}g(z;\\overset{.}{x})+\\rho(z)}\\end{array}$ There exists an algorithm, which in $\\begin{array}{r}{O(n+\\sqrt{n}K_{\\mu}^{\\underline{{\\beta}}})}\\end{array}$ gradient evaluations find a point $\\bar{z}$ such that $\\mathbb{E}\\left[\\lVert z^{*}-{\\bar{z}}\\rVert_{2}\\right]=c B e^{-K}$ We now construct $A_{\\mathsf{e m p}}$ in the following way. At each round $t\\in[T]$ , we use the accelerated algorithm mentioned above to fnd a pont $\\hat{z}$ such that $\\begin{array}{r}{\\mathbb{E}\\left[\\|\\hat{z}-z_{S,t}^{*}\\|_{2}\\right]\\leq\\left(\\frac{\\delta L}{52^{t}\\lambda n^{\\prime}}\\right)}\\end{array}$ ,where $z_{S,t}^{*}$ is theqilibriumn point of $\\begin{array}{r}{G_{S}^{(t)}(z)=\\frac{1}{n^{\\prime}}\\sum_{x\\in S}g^{(t)}(z;x)}\\end{array}$ We then have $\\mathcal{A}_{\\sf e m p}$ output the point $\\bar{z}_{t}=\\hat{z}_{t}+\\xi_{t}$ where $\\xi_{t}\\sim\\mathcal{N}(0,\\mathbb{I}_{d}\\sigma_{t}^{2})$ and $\\begin{array}{r}{\\sigma_{t}=\\frac{8L\\sqrt{2/\\delta}}{2^{t}\\lambda n^{\\prime}\\epsilon}}\\end{array}$ Uing tis nstrution we can obantfolwine. Theorem 6. Let $\\mathcal{A}_{\\sf e m p}$ be as described above. Then Algorithm $^{\\,l}$ is $(\\epsilon,\\delta)$ -DP and when run with \u5165= ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{48}{B}\\left(\\frac{L}{\\sqrt{n^{\\prime}}}+\\frac{L\\sqrt{d\\log(2/\\delta)}}{n^{\\prime}\\epsilon}\\right)s a t i s f i e s}\\\\ &{\\qquad\\qquad\\mathrm{Gap}(\\mathcal{R}_{S W})=O\\left(\\frac{\\log^{3/2}(n)B L}{\\sqrt{n}}+\\frac{\\log^{2}(n)B L\\sqrt{d\\log(1/\\delta)}}{n\\epsilon}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and runs in at most $O(n+\\beta n\\log(n/\\delta))$ gradient evaluations. ", "page_idx": 24}, {"type": "text", "text": "Proof. Privacy Guarantee We show that $\\mathcal{A}_{\\sf e m p}$ is $(\\epsilon,\\delta)$ at any iteration $t\\in[T]$ using the stability properties of the regularized operator. Specifically, the exact equilibrium to $G_{S}^{\\bar{t}}$ . Now because $\\mathcal{A}_{\\sf e m p}$ guarantees $\\begin{array}{r}{\\mathbb{E}\\left[\\|\\hat{z}-z_{S,t}^{*}\\|_{2}\\right]\\,\\le\\,\\big(\\frac{\\delta L}{52^{t}\\lambda n^{\\prime}}\\big)}\\end{array}$ , an application of Markov's inequality implies that with probability at least $1-\\delta$ that $\\begin{array}{r}{\\|\\hat{z}_{t}-z_{S,t}^{*}\\|_{2}=\\left(\\frac{1}{\\delta}\\left(c B e^{-K}\\right)\\right)\\leq\\frac{L}{2^{T}\\lambda n}}\\end{array}$ Thus by a triangle inequality $\\hat{z}_{t}$ $\\textstyle\\left({\\frac{2L}{2^{t}\\lambda n^{\\prime}}}\\right)$ stable with probability a east $1-\\delta$ The garanteso the Gaussian mechanism thus imply $\\mathcal{A}_{\\sf e m p}$ is $(\\epsilon,\\delta)$ -DP. ", "page_idx": 25}, {"type": "text", "text": "Utility Guarantee: We prove the utility guarantee by leveraging Theorem 5, which guarantees $\\begin{array}{r}{\\mathrm{Gapv}_{1}(\\mathcal{R})\\;=\\;O(\\log(n)B^{2}\\lambda)\\;=\\;O\\left(\\frac{\\log^{3/2}(n)B L}{\\sqrt{n}}+\\frac{\\log^{2}(n)B L\\sqrt{d\\log(1/\\delta)}}{n\\epsilon}\\right)}\\end{array}$ . so long as for every $t\\in[T]$ it holds that $\\begin{array}{r}{\\mathbb{E}\\left[\\|\\bar{z}_{t}-z_{S,t}^{*}\\right]\\|_{2}^{2}\\leq\\frac{B^{2}}{12\\cdot2^{2t}}}\\end{array}$ Note here that under the choice of $\\rho$ in Eqn. (7), we satisfy Assuption 1 with $\\kappa=1$ We thus finish the proof with the following analysis, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\bar{z}_{t}-z_{S,t}^{*}\\right\\Vert_{2}^{2}\\right]=\\mathbb{E}\\left[\\Vert\\xi_{t}\\Vert_{2}^{2}+\\Vert\\hat{z}_{t}-z_{S,t}^{*}\\Vert_{2}^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq d\\sigma_{t}^{2}+\\left(\\frac{\\delta}{5}\\cdot\\frac{L}{2^{t}\\lambda n^{\\prime}}\\right)^{2}}\\\\ &{\\qquad\\qquad\\leq\\frac{64d L^{2}\\log(2/\\delta)}{2^{2t}\\lambda^{2}(n^{\\prime})^{2}\\epsilon^{2}}+\\frac{B^{2}}{25\\cdot2^{2t}}\\leq\\frac{B^{2}}{12\\cdot2^{2t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Running Time: By the guarantees of Lemma 11, we can achieve the condition $\\mathbb{E}\\left[\\Vert\\hat{z}-z_{S,t}^{*}\\Vert_{2}\\right]\\leq$ $\\Big(\\frac{\\delta{L}}{52^{t}\\lambda n^{\\prime}}\\Big)$ made in the description of $\\mathcal{A}_{\\sf e m p}$ by seting $\\begin{array}{r}{K=\\log\\left(\\frac{c B}{\\delta}\\cdot\\frac{2^{T}\\lambda n}{L}\\right)}\\end{array}$ . Recall $\\begin{array}{r}{T=\\log_{2}(\\frac{L}{\\kappa B\\lambda})\\leq}\\end{array}$ $n$ . Thus the overall running time is $\\begin{array}{r}{O(n+\\frac{\\beta}{\\mu}K\\sqrt{n})=O(n+\\bar{\\beta}n\\log(\\dot{n}/\\delta))}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "E  Lower Bound for SVIs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The lower bound for SVIs in the $\\ell_{2}$ setting was established in [BG23]. Their technique can easily be extended to other geometries. Specifically, the lower bound comes from two observations. First, for linear losses, the strong VI-gap is equal to the excess population risk when the operator in question is the gradient. Second, the nearly tight lower bound constructions for DP stochastic minimization problems use linear losses. ", "page_idx": 25}, {"type": "text", "text": "We establish the first fact more formally here. ", "page_idx": 25}, {"type": "text", "text": "Lemma 12. Let $f(z;x)~=~\\langle z,x\\rangle$ and define the operator $g(z;x)~=~\\nabla f(z;x)~=~x$ Then $\\begin{array}{r}{\\mathrm{Gap}_{\\mathsf{V}^{\\dagger}}(z)\\,=\\,F_{\\mathcal{D}}(z)\\,-\\,\\mathrm{min}_{u\\in\\mathcal{Z}}\\,\\{F_{\\mathcal{D}}(u)\\}}\\end{array}$ .That is, the strong Vl-gap w.rt. $g$ is equal to the excess population risk w.r.t. the $f$ ", "page_idx": 25}, {"type": "text", "text": "Proof. We have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Gap}_{\\vee}(z)=\\underset{u}{\\mathrm{max}}\\left\\lbrace\\left\\langle\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[x\\right],z-u\\right\\rangle\\right\\rbrace}\\\\ &{\\qquad\\qquad=\\left\\langle\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[x\\right],z\\right\\rangle+\\underset{u}{\\mathrm{max}}\\left\\lbrace\\left\\langle\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left[x\\right],-u\\right\\rangle\\right\\rbrace}\\\\ &{\\qquad=F_{\\mathcal{D}}(z)-\\underset{u\\in\\mathcal{Z}}{\\mathrm{min}}\\left\\lbrace F_{\\mathcal{D}}(u)\\right\\rbrace.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We now restate the lower bound result of [BGN21] for non-Euclidean setups. ", "page_idx": 25}, {"type": "text", "text": "Theorem 7. ([BGN21, Theorem 7.11) Let $p\\in\\left(1,2\\right)$ and $\\begin{array}{r}{p^{*}=\\frac{p}{p-1}}\\end{array}$ and $\\mathcal{Z}=B_{\\parallel\\cdot\\parallel_{p}}(1)$ .Let $\\epsilon>0$ and $\\begin{array}{r}{0\\ <\\ \\delta\\ <\\ \\frac{1}{n^{1+\\Omega(1)}}}\\end{array}$ and let $f(z;x)\\;=\\;\\langle z,x\\rangle$ .For any $(\\epsilon,\\delta)$ -DP algorithm $\\boldsymbol{\\mathcal{A}}$ there exists $a$ distribution $\\mathcal{D}$ over $\\mathcal{X}=\\mathcal{B}_{\\|\\cdot\\|_{p^{*}}}(1)$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\underset{S\\sim\\mathcal{D}^{n},A}{\\mathbb{E}}\\left[F\\mathcal{D}(A(S)-\\operatorname*{min}_{z\\in\\mathcal{Z}}\\left\\{F_{\\mathcal{D}}(z)\\right\\}\\right]=\\tilde{\\Omega}\\left(\\operatorname*{max}\\left\\{\\frac{1}{\\sqrt{n}},\\frac{(p-1)\\sqrt{d}}{n\\epsilon}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that a linear dependence on $B L$ can be obtained using classic rescaling arguments. The two above results (and the result of [BG23]) then imply that the strong VI-gap rate we obtain, $\\begin{array}{r}{\\tilde{O}\\left(B L\\operatorname*{max}\\left\\{\\frac{1}{\\sqrt{n}},\\frac{(p-1)\\sqrt{d}}{n\\epsilon}\\right\\}\\right)}\\end{array}$ , @-1)Va}), is nar optimal for (1, 2 when p - = S(1). ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Claims made are given in Sections 3/4/5 and provide in the Appendix Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The specific assumptions made are detailed in the preliminaries and theorem statements. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational effciency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Statements are proved or cite a relevant reference. Most of these proofs can be found in the appendix. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: There are no experimental results. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: There is no associated code. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so ^No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: There are no experiments. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: There are no experiments. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: There are no experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The theoretical nature of the results means there are minimal ethical concerns. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The theoretical nature of the work means that any societal impact would be very indirect. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: No such assets are used as a part of this research. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: No such assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 31}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: No such assets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: No human subjects were involved. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: No human subjects were involved. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]