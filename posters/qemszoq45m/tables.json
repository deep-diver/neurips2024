[{"figure_path": "QEmsZoQ45M/tables/tables_8_1.jpg", "caption": "Table 1: Table containing the order w.r.t. K of the regret guarantee of each algorithm for each setting discussed in the paper. Columns correspond to different smoothness assumptions: Weakly and Strongly Smooth MDPs were defined in [25], Lipschitz MDPs in [31], and Kernelized MDPs in [41]. Rows correspond to algorithms with no-regret guarantees for some of the settings. [25, 34, 38] represented the state of the art for Strongly smooth MDPs, Lipschitz MDPs, and Kernelized MDPs, respectively. The last row indicates whether the corresponding setting is feasible or if there exists an exp(H) lower bound for the regret.", "description": "This table compares the regret bounds (in terms of the number of episodes K) achieved by different reinforcement learning algorithms on various classes of continuous Markov Decision Processes (MDPs). The MDP classes are categorized by their smoothness assumptions: Weakly Smooth, Strongly Smooth, Lipschitz, Mildly Smooth, and Kernelized. Each row represents a different algorithm, indicating whether it provides no-regret guarantees for each MDP class (X denotes no guarantee).  The final row indicates whether a given MDP class is considered feasible (i.e., achievable with a polynomial regret bound in terms of the horizon H) or not (exponential lower bound). The table highlights that CINDERELLA, the algorithm proposed in this paper, provides superior regret bounds across multiple MDP classes and is the only algorithm applicable to all feasible settings.", "section": "Comparison with related works"}]