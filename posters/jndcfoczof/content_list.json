[{"type": "text", "text": "RA-PbRL: Provably Efficient Risk-Aware Preference-Based Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yujie Zhao1, Jose Efraim Aguilar Escamill2, Weyl $\\mathbf{L}\\mathbf{u}^{3}$ , Huazheng Wang2 ", "page_idx": 0}, {"type": "text", "text": "1 University of California, San Diego, 2 Oregon State University, 3 University of California, Davis yuz285@ucsd.edu, aguijose@oregonstate.edu, adslu@ucdavis.edu, huazheng.wang@oregonstate.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Preference-based Reinforcement Learning (PbRL) studies the problem where agents receive only preferences over pairs of trajectories in each episode. Traditional approaches in this field have predominantly focused on the mean reward or utility criterion. However, in PbRL scenarios demanding heightened risk awareness, such as in AI systems, healthcare, and agriculture, risk-aware measures are requisite. Traditional risk-aware objectives and algorithms are not applicable in such one-episode-reward settings. To address this, we explore and prove the applicability of two risk-aware objectives to PbRL: nested and static quantile risk objectives. We also introduce Risk-Aware-PbRL (RA-PbRL), an algorithm designed to optimize both nested and static objectives. Additionally, we provide a theoretical analysis of the regret upper bounds, demonstrating that they are sublinear with respect to the number of episodes, and present empirical results to support our findings. Our code is available in https://github.com/aguilarjose11/PbRLNeurips. ", "page_idx": 0}, {"type": "text", "text": "1 INTRODUCTION ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement Learning (RL) (Russell & Norvig, 2010) is a fundamental framework for sequential decision-making, enabling intelligent agents to interact with and learn from unknown environments. This framework utilizes a reward signal to guide the selection of policies, where optimal policies maximize this signal. RL has demonstrated state-of-the-art performance in various domains, including clinical trials (Coronato et al., 2020), gaming (Silver et al., 2017), and autonomous driving (Basu et al., 2017). ", "page_idx": 0}, {"type": "text", "text": "Despite its performance, a significant limitation of the standard RL paradigm is the selection of a state-action reward function. In many real-world scenarios, constructing an explicit reward function is often a complex or unfeasible task. As a compelling alternative, Preference-based Reinforcement Learning (PbRL) (Busa-Fekete et al., 2014; Wirth et al., 2016) addresses this challenge by deviating from traditional quantifiable rewards for each step. Instead, PbRL employs binary preference feedback on trajectory pairs generated by two policies, which can be provided directly by human subjects. This method is increasingly recognized as a more intuitive and direct approach in fields involving human interaction and assessment, such as autonomous driving (Basu et al., 2017), healthcare (Coronato et al., 2020), and language models (Bai et al., 2022). ", "page_idx": 0}, {"type": "text", "text": "Previous approaches to PbRL (Xu et al., 2020a; Coronato et al., 2020; Xu et al., 2020b; Chen et al., 2022; Zhan et al., 2023) mainly aim to maximize the mean reward or utility, which is risk-neutral. However, there is a growing need for risk-aware strategies in various fields where PbRL has shown empirical success. For example, in autonomous driving, PbRL reduces the computational burden by skipping the need to calculate reward signals for every state-action pair (Chen et al., 2022). Despite this improvement, the nature of the problem makes dangerous actions costly. Thus, such risk-sensitive problem settings require risk awareness to ensure safety. ", "page_idx": 0}, {"type": "text", "text": "Risk-Aware PbRL also has implications for fields like generative AI (OpenAI, 2023; Chen et al., 2023), where harmful content generation remains a challenge for fine-tuning. In this scenario, a large language model (LLM) is often fine-tuned with user feedback by generating two prompt responses, A and B. Many approaches using RLHF (Ouyang et al., 2022) consider human feedback to penalize harmful content generation. Unfortunately, current approaches only minimize the average harmfulness of a response. This can be a challenge when responses are only harmful to a minority of users. Risk-aware PbRL tackles this challenge by directly aiming to decrease the harmfulness directly, rather than indirectly as with human feedback fine-tuning. ", "page_idx": 1}, {"type": "text", "text": "Despite substantial evidence highlighting the importance of risk awareness in PbRL, a significant gap persists in the theoretical analysis and formal substantiation of risk-aware PbRL approaches. This deficiency has spurred us to develop risk-aware measures and their theoretical analysis within PbRL. In standard RL, a variety of risk-aware measures have been explored, including a general family of risk-aware utility functions (Fei et al., 2020), iterated (nested) Conditional Value at Risk (CVaR) (Du et al., 2022), and risk-sensitive with quantile function form (Bastani et al., 2022). In general, these measures can be categorized into two types: nested or static. Nested measures (Fei et al., 2020; Du et al., 2022) utilize MDPs to ensure risk sensitivity of the value iteration at each step under the current state, resulting in a more conservative approach. In contrast, static risk-aware measures Bastani et al., 2022 analyze the risk sensitivity of the whole trajectory\u2019s reward distribution. In developing and introducing risk-aware objectives in PbRL, we have encountered the following technical challenges in algorithm design and theoretical analysis: ", "page_idx": 1}, {"type": "text", "text": "Rewards are defined over trajectories preference In PbRL, the reward function depends on the preference between two trajectories generated by the agent. We refer to this difference in how the reward function is computed as the one-episode-feedback characteristic. Consequently, the risk-aware objectives of standard RL like Du et al. (2022) and Fei et al. (2020) become unmeasurable since they depend on the state-action reward. ", "page_idx": 1}, {"type": "text", "text": "Trajectory embedding reward assumption When computing the trajectory reward, it is assumed that an embedding mapping exists. By using the trajectory embedding along with some other vector embedding pointing towards high-rewarding trajectories, the reward is computed with a dot product. Unfortunately, the embedding mapping may not be linear. This means that the embedded trajectory vectors may not follow the Markovian assumption, making the embeddings history-dependent. ", "page_idx": 1}, {"type": "text", "text": "Loss of linearity of Bellman function When using a quantile function to transform a risk-neutral PbRL algorithm into a risk-aware algorithm, the Bellman equation used to solve the problem becomes non-linear. This change to the bellman equation disrupts calculations on regret, making risk-neutral PbRL inapplicable. This is primarily due to the additional parameter $\\alpha$ , which modifies the underlying distribution. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we address these challenges by studying the feasibility of risk-aware objectives in PbRL. We propose a provably efficient algorithm, Risk-Aware-PbRL(RA-PbRL), with theoretical and empirical results on its performance and risk-awareness. Our summary of contributions is as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We analyze the feasibility of several risk-aware measures in PbRL settings and prove that in the one-episode-reward setting, nested and static quantile risk-aware objectives are applicable since they can be solved and computed uniquely in a given PbRL MDP.   \n2. We expand the state space in our formulation of a PbRL MDP and modify value iteration to address its history-dependent characteristics from the one-episode setting. These modifications enable us to use techniques like DPP to search for the optimal policy.   \n3. We develop a provably efficient (both computationally and statistically) algorithm, RA-PbRL, for nested and static quantile risk-aware objectives. To the best of our knowledge, we are the first to formulate and analyze the finite time regret guarantee for a risk-aware algorithm with non-Markovian reward models for both nested and static risk-aware objectives. Moreover, we construct a hard-to-learn instance for RA-PbRL to establish a regret lower bound. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Preference-based Feedback Reinforcement Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The incorporation of human preferences in RL, such as Jain et al. (2013), has been a subject of study for over a decade. This approach has proved to be successful and has been widely used in various applications, including language model training (Ouyang et al., 2022), clinical trials (Coronato et al., 2020), gaming (Silver et al., 2017), and autonomous driving (Basu et al., 2017). PbRL can be categorized into three distinct types Wirth et al. (2017): action preference, policy preference, and trajectory preference. Among these, trajectory preference is identified as the most general and widely studied form of preference-based feedback, as evidenced by the rich literature on the topic Chen et al. (2022); Xu et al. (2020a); Wu & Sun (2023). As noted in our introduction, previous theoretical explorations on PbRL have predominantly aimed at achieving higher average rewards, which encompasses risk-neutral PbRL. We distinguish our work by taking the novel approach of formalizing the risk-aware PbRL problem. ", "page_idx": 2}, {"type": "text", "text": "2.2 Risk-aware Reinforcement Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In recent years, research on risk-aware RL has proposed various risk measures. Works such as Fei et al. (2020); Shen et al. (2014); Eriksson & Dimitrakakis (2019) integrate RL with a general family of risk-aware utility functions or the exponential utility criterion. Accordingly, studies like Bastani et al. (2022); Wu & $\\mathrm{Xu}$ (2023) delve into the CVaR measure for the whole trajectory\u2019s reward distribution in standard RL. Further, Du et al. (2022) propose ICVaR-RL, a nested risk-aware RL formulation that addresses both regret minimization and best policy identification. Additionally, the work of Chen et al. (2023) presents an advancement in the form of a nested CVaR measure within the framework of RLHF. The limitation of this work lies in the selection of a random reference trajectory for comparison, causing an unavoidable linear strong nested CVaR regret. Consequently, we are left with only a preference equation from which we are unable to compute the state-action reward function for each step. ", "page_idx": 2}, {"type": "text", "text": "Practical and relevant trajectory or state-action embeddings are described in works such as (Pacchiano et al., 2021). Therefore, the one-episode-reward might not even be sum-decomposable (the trajectory embedding details can be seen in sec.3.1 ). Compared to previous work, we use non-Markovian reward models that do not require estimating the reward at each step and explore both nested and static risk-aware objectives, aiming to provide a more general method. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Set-up and Preliminary Analysis ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 PbRL MDP ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first define a modification of the classical Markov Decision Process (MDP) to account for risk: Risk Aware Preference-Based MDP (RA-PB-MDP). The standard MDP is described as a tuple, $\\mathcal{M}(\\mathcal{S},\\mathcal{A},r_{\\xi}^{\\star},\\mathbf{P}^{\\star},H)$ , where $\\boldsymbol{S}$ and $\\boldsymbol{\\mathcal{A}}$ represent finite state and action spaces, and $H$ denotes the length of episodes. Additionally, let $S:=|S|$ and $A:=|{\\mathcal{A}}|$ denote the cardinalities of the state and action spaces, respectively. $\\mathbf{P}^{\\star}:{\\mathcal{S}}\\times{\\mathcal{A}}\\rightarrow{\\mathcal{P}}(S)$ is the transition kernel, where $\\mathcal{P}(\\boldsymbol{\\mathcal{X}})$ denotes the space of probability measures on space $\\mathcal{X}$ . A trajectory is a sequence ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\xi_{h}\\in{\\mathcal{Z}}_{h},\\quad{\\mathcal{Z}}=\\bigcup_{h=1}^{H}{\\mathcal{Z}}_{h}\\qquad{\\mathrm{where}}\\qquad{\\mathcal{Z}}_{h}=(S\\times A)^{h-1}\\times S.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Intuitively, a trajectory encapsulates the interactions between an agent and the environment $\\mathcal{M}$ up to step $h$ . In contrast to the standard RL setting, where the reward function $r_{h}^{\\star}(s_{h},a_{h})$ specifies the reward at each step $h$ , a significant distinction in the PbRL MDP framework is that the reward function $r^{\\star}$ is defined as $r_{\\xi}^{\\star}(\\bar{\\xi_{H}})\\,:\\,(S\\times A)^{H-1}\\times S\\,\\rightarrow\\,[0,1]$ , denoting the reward of the entire trajectory. ", "page_idx": 2}, {"type": "text", "text": "Reward model for the entire trajectory. For any trajectory $\\xi_{H}$ , we assume the existence of a trajectory embedding mapping $\\phi:\\mathcal{Z}_{H}\\rightarrow\\mathbb{R}^{d i m_{\\mathbb{T}}}$ , and the reward of the entire trajectory is defined as the function: $r_{\\xi}^{\\star}(\\xi_{H}):=\\langle\\phi(\\xi_{H}),\\mathbf{w}_{r}^{\\star}\\rangle$ . Here, $d i m_{\\mathbb{T}}$ denotes the trajectory embedding dimension. Finally, we denote $\\phi(\\xi_{H})=(\\phi_{1}(\\xi_{H}),\\dots,\\phi_{d i m_{\\mathbb{T}}}(\\xi_{H}))$ . ", "page_idx": 2}, {"type": "text", "text": "Assumption 3.1. We assume that for all trajectories $\\xi_{H}$ and for all $d\\in\\{1,\\ldots,d i m_{\\mathbb{T}}\\}$ , $\\|\\phi_{d}(\\xi_{H})\\|\\in$ $\\{0\\}\\cup[\\bar{b},B]$ where $b,B>0$ are known. $\\|\\mathbf{w}_{r}^{\\star}\\|\\leq\\rho_{w}$ and $\\rho_{w}$ is known as well. ", "page_idx": 3}, {"type": "text", "text": "Remark 3.2. We assume the map $\\phi$ is known to the learner. The sum of the state-action reward used iTnh eCrheefno reet,  aflo. r( a2l0l ,c $\\begin{array}{r}{\\phi(\\xi_{H})=\\sum_{h=1}^{H}\\mathbb{I}\\left(s_{h},a_{h}\\right)}\\end{array}$ and $d i m_{\\mathbb{T}}=S\\times A$ $d\\in\\{1,\\ldots,d i m_{\\mathbb{T}}\\}$ $\\|\\phi_{d}(\\xi_{H})\\|\\in\\{0\\}\\cup\\{1,\\overline{{\\ldots}},\\overline{{H\\}}\\}$ ", "page_idx": 3}, {"type": "text", "text": "Remark 3.3. Assumption 3.1 implies that there is a gap between zero and some positive number $b$ in the absolute values of components of trajectory embeddings. This is evident for finite-step discrete action spaces, where we can enumerate all trajectory embeddings to find the smallest non-zero component, satisfying most application scenarios. ", "page_idx": 3}, {"type": "text", "text": "At each iteration $k\\in[K]$ , the agent selects two policies under a deterministic policy framework, $\\pi_{1,k}$ and $\\pi_{2,k}$ , which generate two (randomized) trajectories $\\xi_{H}^{1,k}$ and $\\xi_{H}^{2,k}$ . In PbRL, unlike standard RL where the agent receives rewards every step, the agent can only obtain the preference $o_{k}$ between two trajectories $\\left(\\xi_{H}^{1,k},\\xi_{H}^{2,k}\\right)$ . By making a query, we obtain a preference feedback $o_{k}\\in\\{0,1\\}$ that is sampled from a Bernoulli distribution: ", "page_idx": 3}, {"type": "equation", "text": "$$\no_{k}\\sim B e r\\left(\\sigma\\left(r_{\\xi}^{\\star}\\left(\\xi_{H}^{1,k}\\right)-r_{\\xi}^{\\star}\\left(\\xi_{H}^{2,k}\\right)\\right)\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\sigma:\\mathbb{R}\\rightarrow[0,1]$ is a monotonically increasing link function. We assume $\\sigma$ is known like the popular Bradley-Terry model (Hunter, 2004), wherein $\\sigma$ is represented by the logistic function. It is known that we can not estimate the trajectory reward without a known $\\sigma$ . Also, we assume $\\sigma$ is Lipschitz continuous and $\\kappa$ is its Lipschitz coefficient. ", "page_idx": 3}, {"type": "text", "text": "History dependent policy. Since the algorithm can only observe the reward for an entire episode until the end, it cannot make decisions based solely on the current state. The agent cannot observe the individual reward $r_{h}(s,a)$ and thus cannot compute the target value at each step. To circumvent this challenge, the algorithm should take action according to a history-dependent policy. A historydependent policy $\\Pi\\stackrel{=}{=}\\{\\pi_{h}\\}_{h\\in[H]}$ is defined as a sequence of mappings $\\psi_{h}:{\\mathcal{Z}}_{h}\\rightarrow A$ , providing the agent with guidance to select an action, given a trajectory $\\xi_{h}\\in\\mathcal{Z}_{h}$ at time step $h$ . For notation convenience, let $\\Pi$ denote the set of all history-dependent deterministic policies. ", "page_idx": 3}, {"type": "text", "text": "3.2 Risk Measure ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Because in PbRL we can only estimate the reward for the entire trajectory, the risk measure selected for PbRL must rely solely on the reward of the entire trajectory. That is, two trajectories with the same trajectory reward should contribute equally to the risk measure, even if their potential rewards at each step are different. Unlike Chen et al. (2023) that decomposes the reward at each step (where the solution is likely not unique) and then calculates the risk measure, this requirement ensures that the risk measure consistently and holistically reflects the underlying preference. We refer to risk measures that suitable for PbRL problems as PbRL-risk-measures. Here, we introduce two different risk-measures: nested and static quantile risk-aware measures, which are appropriate for PbRL-MDPs. ", "page_idx": 3}, {"type": "text", "text": "We first introduce the definition of quantile function and risk-aware objective. The quantile function of a random variable $X$ is $F_{X}^{\\dagger}(\\tau)=\\operatorname*{inf}\\left\\{x\\in\\mathbb{R}\\mid F_{X}(x)\\geq\\tau\\right\\}$ . We assume $F_{X}$ is strictly monotone, so it is invertible and we have $F_{X}^{\\dag}(\\tau)=F_{X}^{-1}(\\tau)$ . The risk-aware objective is given by the RiemannStieljes integral: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Phi(X)=\\int_{0}^{1}F_{X}^{\\dagger}(\\tau)\\mathrm{d}G(\\tau)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $X$ is the random variable encoding the value of MDP, and $G$ is a weighting function over the quantiles. This class captures a broad range of useful objectives, including the popular CVaR objective (Bastani et al., 2022). ", "page_idx": 3}, {"type": "text", "text": "Remark 3.4. ( $\\alpha$ -CVaR objective) Specifically, in $\\alpha$ -CVaR, ", "page_idx": 3}, {"type": "equation", "text": "$$\nG(\\tau)=\\left\\{{\\frac{1}{\\alpha}}\\tau\\quad{\\mathrm{if~}}\\tau<\\alpha,\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $\\Phi(X)$ becomes ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Phi(X)=\\frac{1}{\\alpha}\\int_{0}^{\\alpha}F_{X}^{-1}(\\tau)\\mathrm{d}\\tau.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Assumption 3.5. $G$ is $L_{G}$ -Lipschitz continuous for some $L_{G}\\in\\mathbb{R}_{>0}$ , and $G(0)=0,G(1)=1$ . ", "page_idx": 4}, {"type": "text", "text": "For example, for the $\\alpha$ -CVaR objective, we have $L_{G}=1/\\alpha$ . ", "page_idx": 4}, {"type": "text", "text": "There are two prevalent approaches to Risk-aware-MDPs: nested (or iterated) (such as Iterated CVaR (ICVAR) (Du et al., 2022) and Risk-Sensitive Value Iteration (RSVI) (Fei et al., 2020)), and static (referenced in (Bastani et al., 2022; Wu & Xu, 2023)). MDPs characterized by an iterated risk-aware objective facilitate a value function and uphold a Bellman-type recursion. ", "page_idx": 4}, {"type": "text", "text": "Nested PbRL-risk-measures. For standard RL\u2019s MDP, the nested quantile risk-aware measure can be elucidated in Bellman equation type as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{Q_{h}^{\\pi}(s,a)}&{=r_{h}^{\\star}(s,a)+\\Phi\\left(V_{h+1}^{\\pi}\\left(s^{\\prime}\\right),s^{\\prime}\\sim\\mathbf{P}^{\\star}(s,a)\\right)}\\\\ {V_{h}^{\\pi}(s)}&{=Q_{h}^{\\pi}\\left(s,\\pi_{h}(s)\\right)}\\\\ {V_{H+1}^{\\pi}(s)}&{=0,\\quad\\forall s\\in\\mathcal{S}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here $r_{h}^{\\star}(s,a)$ denotes the decomposed state-action reward in step $h$ . ", "page_idx": 4}, {"type": "text", "text": "For the PbRL-MDP setting $\\mathcal{M}(\\mathcal{S},\\mathcal{A},r_{\\xi}^{\\star},\\mathbf{P}^{\\star},H)$ , the state-action\u2019s reward might not be calculated or the reward of the entire trajectory might not be decomposed. Therefore, the policy should be history-dependent. We rewrite the nested quantile objective\u2019s Bellman equation with the embedded trajectory reward as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\tilde{Q}_{h}^{\\pi}(\\xi_{h},a)}&{=\\Phi\\left(\\tilde{V}_{h+1}^{\\pi}\\left(s^{\\prime}\\circ(\\xi_{h},a)\\right),s^{\\prime}\\sim\\mathbf{P}^{\\star}(s,a)\\right),}\\\\ {\\tilde{V}_{h}^{\\pi}(\\xi_{h})}&{=\\tilde{Q}_{h}^{\\pi}\\left(\\xi_{h},\\pi_{h}(\\xi_{h})\\right),}\\\\ {\\tilde{V}_{H}^{\\pi}(\\xi_{H})}&{=r^{\\star}(\\xi_{H}),}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For any PbRL MDP $\\mathcal{M}(\\mathcal{S},\\mathcal{A},r_{\\xi},\\mathbf{P},H)$ , we use $\\tilde{V}_{h}^{\\pi,r_{\\xi},\\mathbf{P}}(\\xi_{h})$ to denote the value iteration under the policy $\\pi$ , where $\\pi$ is a history dependent policy. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.6. For a given tabular MDP, the reward on the entire trajectory can be decomposed as $\\begin{array}{r}{r_{\\xi}^{\\star}\\left(\\xi_{H}\\right)=\\sum_{h=1}^{H}r_{h}^{\\star}\\,(s_{h},a_{h}),}\\end{array}$ $V_{1}^{\\pi}$ in Eq. 3 and $\\tilde{V}_{1}^{\\pi}$ in Eq. 4 are equivalent. ", "page_idx": 4}, {"type": "text", "text": "The proof is detailed in Appendix B.1 due to space limitations. ", "page_idx": 4}, {"type": "text", "text": "Static PbRL-risk-measures. Standard MDPs with a static risk aware objective (Bellemare et al., 2017; Dabney et al., 2018) can be written in the distributional Bellman equation as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle Z_{h}^{(\\pi)}\\left(\\xi_{h}\\right)=\\sum_{h^{\\prime}=h}^{H}r_{h^{\\prime}}^{\\star}(s_{h^{\\prime}},a_{h^{\\prime}}),\\quad\\xi_{H}\\sim\\mathbb{P}\\left(\\cdot\\mid\\Xi_{h}(\\xi_{H})=\\xi_{h}\\right)}}\\\\ {{\\displaystyle F_{Z_{h}^{(\\pi)}(\\xi)}(x)=\\sum_{s^{\\prime}\\in\\mathcal{S}}P(s^{\\prime}\\mid S(\\xi),\\pi_{h}(\\xi))F_{Z_{h+1}^{(\\pi)}(\\xi\\circ(s^{\\prime},\\pi_{h}(\\xi)))}(x-r_{h}^{\\star}(s_{h},a_{h}))}}\\\\ {{\\displaystyle V_{1}^{\\pi}(s)=\\int_{0}^{1}F_{Z_{1}}^{\\dagger}(\\pi)(\\tau)\\cdot d G(\\tau)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Where $\\begin{array}{r l r}{S(\\xi)}&{{}=}&{s}\\end{array}$ for $\\begin{array}{r l r}{\\xi}&{{}=}&{(\\dots,s)}\\end{array}$ is the current state in trajectory $\\xi$ , $\\begin{array}{r l}{\\Xi_{h}(\\xi_{H})}&{{}=}\\end{array}$ $\\left(s_{1},a_{1},s_{2},a_{2},\\ldots,s_{h-1},a_{h-1},s_{h}\\right)$ denotes the first h steps\u2019 trajectory. $Z_{h}^{(\\pi)}\\left(\\xi_{h}\\right)$ denoted the reward from step $t$ given the current history. The static reward of $\\pi$ is $Z_{1}^{(\\pi)}(\\xi)$ , where $\\xi=(s)\\in\\mathcal{Z}_{1}$ for $s\\sim D$ is the initial history. ", "page_idx": 4}, {"type": "text", "text": "Also, we modify the distributional Bellman equation for PbRL MDP $\\mathcal{M}(\\mathcal{S},\\mathcal{A},r_{\\xi}^{\\star},\\mathbf{P}^{\\star},H)$ settings as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nZ_{h}^{(\\pi)}\\left(\\xi_{h}\\right)=r_{\\xi}^{\\star}(\\xi_{H}),\\quad\\xi_{H}\\sim\\mathbb{P}\\left(\\cdot\\mid\\Xi_{h}(\\xi_{H})=\\xi_{h}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{F_{Z_{h}^{(\\pi)}(\\xi_{h})}(x)=\\displaystyle\\sum_{s^{\\prime}\\in{\\cal S}}P(s^{\\prime}\\mid S(\\xi),\\pi_{h}(\\xi))F_{Z_{h+1}^{(\\pi)}(\\xi\\circ(s^{\\prime},\\pi_{h}(\\xi)))}(x)}}\\\\ {{{}}}\\\\ {{\\tilde{V}_{1}^{\\pi}(s)=\\displaystyle\\int_{0}^{1}F_{Z_{1}}^{\\dagger}(\\pi)(\\tau)\\cdot d G(\\tau)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Lemma 3.7. For a tabular MDP and a reward of the entire trajectory can be decomposed as $\\begin{array}{r}{r_{\\xi}^{\\star}\\left(\\xi_{H}\\right)=\\sum_{h=1}^{H}r_{h}^{\\star}\\left(s_{h},a_{h}\\right)\\!,}\\end{array}$ $V_{1}^{\\pi}$ in Eq. 5 and $\\tilde{V}_{1}^{\\pi}$ in Eq. 6 are equivalent. ", "page_idx": 5}, {"type": "text", "text": "The proof is detailed in Appendix B.2 due to space limitation. ", "page_idx": 5}, {"type": "text", "text": "Each of these risk measures possesses distinct advantages and limitations. Nested risk measures, which incorporate a Bellman-type recursion, can directly employ techniques such as the Dynamic Programming Principle (DPP) for computation. However, they are challenging to interpret and are not law-invariant (Hau et al., 2023). On the other hand, static risk measures are straightforward to interpret, but the resulting optimal policy may not remain Markovian and becomes history-dependent. Consequently, techniques such as the DPP and the Bellman equation become inapplicable. ", "page_idx": 5}, {"type": "text", "text": "3.3 Objective ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We define an optimal policy as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi^{\\star}\\in\\operatorname{argmax}_{\\pi\\in\\Pi}\\tilde{V}_{1}^{\\pi}(s_{1})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "i.e., it maximizes the given objective for $\\mathcal{M}$ . $\\tilde{V}_{1}^{\\pi}(s_{1})$ will be decided by the selected risk measure, where value iteration calculated using Eq. 4 and static calculated using Eq. 6. ", "page_idx": 5}, {"type": "text", "text": "Assumption 3.8. Regardless of nested or static CVaR objectives, we are given an algorithm for computing $\\pi_{\\mathcal{M}}^{\\star}$ for a known PbRL-MDP $\\mathcal{M}$ . ", "page_idx": 5}, {"type": "text", "text": "A formal proof of Assumption 3.8 is given in Appendix F. When unambiguous, we drop $\\mathcal{M}$ and simply write $\\pi^{\\star}$ . ", "page_idx": 5}, {"type": "text", "text": "At the beginning of each episode $k\\,\\in\\,[K]$ , our algorithm $\\mathfrak{A}$ chooses two policies $(\\pi_{1,k},\\pi_{2,k})=$ A (Hk), where Hk = {\u03be1,k\u2032,H, \u03be2,k\u2032,H, ok}k\u2032=0 i s the random set of episodes observed so far. Then, our goal is to design an algorithm $\\mathfrak{A}$ that minimizes regret, which is naturally defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(K):=\\sum_{k=0}^{K}\\left(2\\tilde{V}_{1}^{\\pi^{\\star}}\\left(s_{1}\\right)-\\tilde{V}_{1}^{\\pi_{1,k}}\\left(s_{1}\\right)-\\tilde{V}_{1}^{\\pi_{2,k}}\\left(s_{1}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4 Risk Aware Preference based RL Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we introduce and analyze an algorithm called RA-PbRL for solving the PbRL problem with both nested and static risk aware objectives. Also, we establish a regret bound for it. ", "page_idx": 5}, {"type": "text", "text": "4.1 Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "RA-PbRL is formally described in Algorithm 1. The development of RA-PbRL is primarily inspired by the PbOP algorithm, as delineated in Chen et al. (2022), which was originally proposed for risk-neutral PbRL environments. Building upon this foundation, one significant difference is how to choose a risk aware policy in estimated PbRL MDP, where the value iteration is different. We also use novel techniques to estimate the confidence set and explore for a policy, instead of using a bonus (Chen et al., 2022) (which is difficult to calculate in risk-aware problems) as in standard RL. ", "page_idx": 5}, {"type": "text", "text": "The overview of the algorithm. Now we introduce the main part of our algorithm. In line 1, we initialize the transition kernel function and reward function confidence set, and execute two arbitrary policies at first. For every episode, we observe history samples and accordingly estimate the transition kernel function (line 3) and update its confidence set (line 4) as well as the reward function (line 5) and ", "page_idx": 5}, {"type": "text", "text": "Require: episode $K$ , step $H$ , initial state space $\\mathcal{P}$ , initial reward space $\\mathcal{R}$ , risk level $\\alpha$ , confidence parameter $\\delta$   \n1: Set $B_{0}^{\\mathbf{P}}=\\mathcal{P},B_{0}^{r}=\\mathcal{R}$ , Execute two arbitrary policies $\\pi_{1,0}$ and $\\pi_{2,0}$ for one episode, respectively, and then observe the trajectory $\\tau_{1,0}$ and $\\tau_{2,0}$ and the preference $O_{0}$ .   \n2: for $k=1\\cdots K$ do   \n3: Calculate the probability estimation $\\hat{{\\bf P}}_{k}$ : $\\begin{array}{r}{\\hat{\\mathbf{P}}_{k}=\\arg\\operatorname*{min}_{\\mathbf{P}\\in\\mathcal{P}}\\sum_{i=1}^{2}\\sum_{k^{\\prime}=0}^{k-1}\\sum_{h=1}^{H}|\\langle\\mathbf{P}\\big(s_{i,k^{\\prime},h},a_{i,k^{\\prime},h}\\big),\\mathbb{I}\\big(s_{i,k^{\\prime},h+1}\\big)\\rangle|^{2}.}\\end{array}$   \n4: Update transition confidence set : $\\begin{array}{r}{\\mathcal{B}_{k}^{\\mathbf{P}}=\\left\\{\\mathbf{P}^{\\prime}\\mid\\sum_{s^{\\prime}\\in S}\\left|\\hat{\\mathbf{P}}^{k}\\left(s^{\\prime}\\mid s,a\\right)-\\mathbf{P}^{\\prime}\\left(s^{\\prime}\\mid s,a\\right)\\right|\\leq\\sqrt{\\frac{2S\\log\\left(\\frac{2K H S A}{\\delta}\\right)}{n_{k}\\left(s,a\\right)}}\\right\\}\\bigcap\\mathcal{B}_{k-1}^{\\mathbf{P}}}\\end{array}$   \n5: Calculate the reward estimation: $\\begin{array}{r}{\\hat{r}_{k}(\\cdot)=\\arg\\operatorname*{min}_{r\\in\\mathcal{R}}\\sum_{k^{\\prime}=0}^{k-1}\\left(\\sigma(r(\\tau_{1,k^{\\prime}})-r(\\tau_{2,k^{\\prime}}))-o_{k^{\\prime}}\\right)^{2}}\\end{array}$   \n6: Update the confidence set: $\\mathcal{B}_{k}^{r}=\\left\\{r^{\\prime}(\\cdot)\\bigg\\vert\\sum_{k^{\\prime}=0}^{k-1}\\left[\\sigma(\\hat{r}_{k}(\\tau_{1,k^{\\prime}})-\\hat{r}_{k}(\\tau_{2,k^{\\prime}}))-\\sigma(r^{\\prime}(\\tau_{1,k^{\\prime}})-r^{\\prime}(\\tau_{2,k^{\\prime}}))\\right]^{2}\\le\\beta_{r,k}(\\delta)\\right\\}\\bigcap\\mathcal{B}_{k-1}^{r}$   \n7: Update policy confidence set: $\\begin{array}{r}{\\dot{\\Pi}_{k}=\\dot{\\mathrm{\\{\\pi~|~\\!{\\operatorname*{max}}}}}_{r_{\\xi}\\in B_{k}^{r},\\mathbf{P}\\in B_{k}^{\\mathrm{P}}}(\\tilde{V}_{1,r_{\\xi},\\mathbf{P}}^{\\pi}(s_{1})-\\tilde{V}_{1,r_{\\xi},\\mathbf{P}}^{\\pi^{\\prime}}(s_{1}))\\geq0,\\forall\\pi^{\\prime}\\}\\bigcap\\Pi_{k-1}}\\end{array}$   \n8: Compute $\\left(\\pi_{1,k},\\pi_{2,k}\\right)$ : $\\begin{array}{r}{\\big(\\Bar{\\pi_{1,k}},\\pi_{2,k}\\big)=\\underset{\\pi_{1},\\pi_{2}\\in\\Pi_{k}}{\\arg\\operatorname*{max}}\\operatorname*{max}_{r\\in\\mathcal{B}_{k}^{r},\\mathbf{P}\\in\\mathcal{B}_{k}^{\\mathbf{P}}}\\big(\\tilde{V}_{1,r_{\\xi},\\mathbf{P}}^{\\pi_{1}}\\big(s_{1}\\big)-\\tilde{V}_{1,r_{\\xi},\\mathbf{P}}^{\\pi_{2}}\\big(s_{1}\\big)\\big)}\\end{array}$   \n9: Observe the trajectory $\\xi_{1,k,H},\\xi_{2,k,H}$ , and the preference $o_{k}$   \n10: Calculate the state-action visiting time before episode $k$ : $n_{k}(s,a)$ ", "page_idx": 6}, {"type": "text", "text": "11: end for ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "its confidence set (line 6). Both estimation and calculation used the standard least-squares regression. Based on the confidence sets, we maintain a policy set in which all policies are near-optimal with minor sub-optimality gap with high probability in line 7. In line 8, we execute the most exploratory policy pair in the policy set and observe the preference between the trajectories sampled using these two policies. ", "page_idx": 6}, {"type": "text", "text": "The key difference between nested and static objective. The estimation of the transition kernel (line 4 in Algorithm 1) and the construction of confidence set (line 6 in Algorithm 1) are similar for both nested and static objectives. The difference lies in the value iteration, which is defined in Eq. 4 for nested objective and Eq. 6 for static objective. The bounds for regrets are different since the estimation error\u2019s impact is different as we are going to show below. ", "page_idx": 6}, {"type": "text", "text": "4.2 Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Theorem 4.1 ( Nested object regret upper bound). With at least probability $1-\\delta$ , the nested quantile risk aware object regret of RA-PBRL is bounded by: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{Reg}_{n e s t e d}\\left(K\\right)}\\\\ &{\\le\\!\\mathcal{O}\\left(L_{G}H^{\\frac32}\\sqrt{K}S A\\log\\left(\\frac{K H S A}{\\delta}\\right)\\cdot\\frac{1}{\\sqrt{\\operatorname*{min}_{\\pi,h,s:\\omega_{\\pi,h}(s)>0}\\omega_{\\pi,h}(s)}}\\right)}\\\\ &{\\quad+\\mathcal{O}\\left(\\frac{B}{\\kappa b}d i m_{\\mathbb{T}}\\sqrt{\\log\\left(\\frac{K d i m_{\\mathbb{T}}}{\\delta}\\right)\\log\\left(\\frac{K\\left(1+2B\\rho_{w}\\right)}{\\delta}\\right)}\\frac{1}{\\operatorname*{min}_{\\pi,d}\\omega_{d i m,\\pi}\\left(d\\right)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Where $w_{\\pi,h}(s)$ denotes the probability of visiting state-action pair at h th step with policy $\\pi$ and $\\mathrm{min}_{\\pi,d}\\,\\omega_{d i m,\\pi}(d)$ denotes the probability of trajectory $\\xi_{H}$ \u2019s d th feature $\\Phi_{d}(\\xi_{H})\\neq0$ with the policy $\\pi$ . ", "page_idx": 6}, {"type": "text", "text": "The proof of this theorem is provided in Appendix D.20. The first term of the regret arises from the estimation error of the transition kernel, primarily dominated by $\\begin{array}{r}{\\operatorname*{min}_{\\pi,h,s:w_{\\pi,h}(s)>0}{w_{\\pi,h}(s)}}\\end{array}$ . The second term is due to the estimation error of the trajectory reward weights, significantly impacted by $m i n_{\\pi,d}\\omega_{\\pi}(d)$ . In fact, these factors are unavoidable in the lower bound in certain challenging cases. Thus, they characterize the inherent problem difficulty, i.e., in achieving the nested risk-aware objective, the agent will be highly sensitive to some state-actions or features that are difficult to observe and require substantial effort to explore. This may result in inefficiency in many scenarios. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.2 (Static object regret upper bound). The static quantile risk aware object regret of RA-PBRL is bounded by: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ R e g_{s t a t i c}(K)}\\\\ &{\\leq\\mathcal O\\left(L_{G}S^{2}A H^{\\frac32}\\sqrt{K}l o g\\left(K/\\delta\\right)\\right)+\\mathcal O\\left(L_{G}d i m_{\\mathbb T}\\sqrt{K\\log(K B\\rho_{w})\\log\\left(\\frac{K\\left(1+2B\\rho_{w}\\right)}{\\delta}\\right)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The proof of this theorem is provided in Appendix D.21. Notice that the regret for both the nested risk objective and the static risk objective of Algorithm 1 are sublinear with respect to $K$ , making RA-PbRL the first provably efficient algorithm with one-episode-reward for these two objectives. Additionally, compared to Chen et al. (2023), we achieve the goal of having both policies gradually approach optimality. Moreover, in comparison to the nested risk-aware objective, the static objective focuses on the risk measure of the entire distribution, primarily influenced by the Lipschitz coefficient $L_{G}$ of the quantile function and is less constrained by certain specific cases. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.3 (Nested object regret lower bound). The nested quantile risk aware object regret of RA-PBRL is bounded by: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(K)\\geq\\mathcal O\\left(\\operatorname*{min}\\left\\{B\\rho_{w}\\sqrt{\\frac{A K}{\\operatorname*{min}_{\\pi,h,s:p_{\\pi,h}(s)>0}w_{\\pi,h}(s,a)}},B\\sqrt{\\frac{A K}{\\operatorname*{min}_{\\pi,d}\\omega_{d i m,\\pi}(d)}}\\right\\}\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We provide our proof in E.1 by two hard-to-learn constructions. By the two instances, we show that the two factors, $\\begin{array}{r}{\\operatorname*{min}_{\\pi,h,s:p_{\\pi,h}(s)>0}w_{\\pi,h}(s),\\operatorname*{min}_{\\pi,d}\\omega_{d i m,\\pi}(d)}\\end{array}$ , are unavoidable in some cases. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.4 (Static object regret lower bound). The static quantile risk aware object regret of RA-PBRL is bounded by: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(K)\\geq\\mathcal{O}(S^{2}A+d i m_{\\mathbb{T}})\\sqrt{K}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The proof of this theorem is similar to Theorem 4.5 in Chen et al. (2022). ", "page_idx": 7}, {"type": "text", "text": "5 Experiment Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we assess the empirical performance of RA-PbRL (Algorithm 1). For a comparative analysis, we select two baseline algorithms: PbOP, as described in Chen et al. (2022), which is a PbRL algorithm utilizing general function approximation, and ICVaR-RLHF, detailed in Chen et al. (2023), which is a risk-sensitive Human Feedback RL algorithm. These baselines represent the most closely aligned algorithms with RA-PbRL, especially in terms of employing general function approximation. The evaluation of empirical performance is conducted through the lens of static regret, as defined in Eq. 8. ", "page_idx": 7}, {"type": "text", "text": "5.1 Experiment settings: MDP ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In our experimental framework, we configure a straightforward tabular MDP characterized by finite steps $H=6$ , finite actions $A=3$ , state space $S=4$ , and risk levels $\\alpha\\in\\{0.05,0.10,0.20,0.40\\}$ For each configuration and algorithms, we perform 50 independent trials and report the mean regret across these trials, along with $95\\%$ confidence intervals. The outcomes are depicted in Figures 1 and 3, where the solid lines represent the empirical means obtained from the experiments, and the width of the shaded regions indicates the standard deviation of the experiments. ", "page_idx": 7}, {"type": "image", "img_path": "JNDcFOczOf/tmp/97f57fad265ec651b0f8d4562390be31c2f2ef868808b90c9f03dd2ea7283468.jpg", "img_caption": ["Figure 1: Cumulative regret for static CVaR over different $\\alpha$ "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "JNDcFOczOf/tmp/7d05c4ae3ce8842e7cfa211867106fe35507f6f9ced5d337b9137c88b49bd5a3.jpg", "img_caption": ["Figure 2: Cumulative regret for nested CVaR over different $\\alpha$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2 Experiment settings: Half Cheetah ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Aditionally, we implement our algorithm to solve MuJoCo\u2019s Half-cheetah simulation. We implement our proposed algorithm alongside PbOP and TRC. The objective of the algorithms is to learn to control a robot simulation of a cheetah to run forward. This problem differs from the previous setting in that it is more challenging and uses a continuous state-action space. Because the algorithm ws originally implemented for discrete state-action spaces, we assume the transition, reward, and policy functions can be parameterized by a linear function, which is optimized for using gradient descent. ", "page_idx": 8}, {"type": "text", "text": "Similarly to the previous experiment, we run both policies in the MuJoCo setting until 1,000 timesteps have passed. We repeat this for 100 episodes, saving the interactions and final preferences based on the cumulative reward after each episode. Finally, we use the data to perform gradient descent for a pre-defined number of repetitions. We repeat the cycle of interaction and training another 100 times. In total, the algorithm sees 10,000,000 timesteps and 10,000 preference reward signals. ", "page_idx": 8}, {"type": "text", "text": "The key idea behind the implementation of our algorithm lies in the initial optimization of the learned transition and reward functions using the data collected during the interaction. We iterate through tuples containing the initial state, action taken, and transitioned state. We perform stochastic gradient descent to find the best vectors that parameterize the transition and reward functions to predict the collected data. ", "page_idx": 8}, {"type": "text", "text": "After obtaining the best transition and reward functions, we use their parameterization to create a new parameterization of the value function in line 8. In our case, we simply concatenate the vectors parameterizing the transition and reward functions alongside a vector parameterizing the policies. The policy vector used depends on the policy being optimized (the best or exploratory policies.) We then compute the $\\alpha$ -CVaR over the preferences obtained using the final cumulative reward. We then optimize the value function parameterization using this as the training data, and perform stochastic gradient descent. To follow the theoretical bounds we establish, we compute the distance between the initial transition and reward parameterizations used in the value function, rolling back the parameterization if the distance is larger than what is established by the theoretical bounds. ", "page_idx": 8}, {"type": "text", "text": "5.3 Experimental results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As depicted in Figure 1 and 3, the regret of RA-PbRL over static and nested CVaR initially exhibits a linear growth with respect to $K$ , transitioning to sublinear growth upon reaching a certain threshold. This behavior aligns with the conclusions drawn in Section 4.2. It is important to note that increased risk aversion $\\left(\\alpha\\rightarrow0\\right)$ ) introduces greater uncertainty, as evidenced by the larger variance regions observed in the experiments. Notably, the regret of the bad-scenarios associated with RA-PbRL is significantly lower compared to those of other algorithms. It can additionally be observed that as $\\alpha$ is increased, the regret improves, which is expected as riskier behavior can also improve the odds of finding useful behaviors. ", "page_idx": 8}, {"type": "image", "img_path": "JNDcFOczOf/tmp/1e47fe6affc01b37212cb16c4587aa49dd7d9fd837b2509b86caad3673d38e26.jpg", "img_caption": ["Figure 3: Cumulative regret for static CVaR in the MuJoCo setting over different $\\alpha$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we investigate a novel PbRL algorithm for solving problems requiring risk awareness. We explore static and nested measures to introduce risk awareness to PbRL settings. To the best of our knowledge, our proposed RA-PbRL algorithm is the first provably efficient Preference-based Reinforcement Learning (PbRL) algorithm that incorporates both nested and static risk objectives in one algorithm. Our algorithm is built on innovative techniques for the efficient approximation of regret. A core finding in our investigation is the strong influence of the state and trajectory dimensions with respect to the nested risk objective regret. On the other hand, the static risk objective regret is mainly determined by the quantile function. ", "page_idx": 9}, {"type": "text", "text": "We have also identified the following four limitations to our work. (1) Our comparison feedback is limited to two trajectories. An interesting, more general approach could consider n-wise comparisons. (2) The reward functions are assumed to be linear in this work for the sake of simplicity. (3) Although this work has considered more general risk measures, we have still made certain assumptions that limit the generality of our results. (4) There is still room for future improvements to further close the gap between upper and lower bounds. We believe this work opens several avenues for future research, including establishing the concrete lower bounds of risk-aware PbRL, improving the computational complexity of the algorithm, and conducting experiments in more diverse and interesting environments/simulations. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Ayoub, A., Jia, Z., Szepesvari, C., Wang, M., and Yang, L. Model-based reinforcement learning with value-targeted regression. In International Conference on Machine Learning, pp. 463\u2013474. PMLR, 2020. ", "page_idx": 9}, {"type": "text", "text": "Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.   \nBastani, O., Ma, J. Y., Shen, E., and Xu, W. Regret bounds for risk-sensitive reinforcement learning. Advances in Neural Information Processing Systems, 35:36259\u201336269, 2022.   \nBasu, C., Yang, Q., Hungerman, D., Singhal, M., and Dragan, A. D. Do you want your autonomous car to drive like you? In Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction, pp. 417\u2013425, 2017.   \nB\u00e4uerle, N. and Ott, J. Markov decision processes with average-value-at-risk criteria. Mathematical Methods of Operations Research, 74:361\u2013379, 2011.   \nBellemare, M. G., Dabney, W., and Munos, R. A distributional perspective on reinforcement learning. In International conference on machine learning, pp. 449\u2013458. PMLR, 2017.   \nBusa-Fekete, R., Sz\u00f6r\u00e9nyi, B., Weng, P., Cheng, W., and H\u00fcllermeier, E. Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm. Machine learning, 97:327\u2013351, 2014.   \nChen, X., Zhong, H., Yang, Z., Wang, Z., and Wang, L. Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation. In International Conference on Machine Learning, pp. 3773\u20133793. PMLR, 2022.   \nChen, Y., Du, Y., Hu, P., Wang, S., Wu, D., and Huang, L. Provably efficient iterated cvar reinforcement learning with function approximation. arXiv preprint arXiv:2307.02842, 2023.   \nCoronato, A., Naeem, M., De Pietro, G., and Paragliola, G. Reinforcement learning for intelligent healthcare applications: A survey. Artificial Intelligence in Medicine, 109:101964, 2020.   \nDabney, W., Rowland, M., Bellemare, M., and Munos, R. Distributional reinforcement learning with quantile regression. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \nDann, C., Lattimore, T., and Brunskill, E. Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning. Advances in Neural Information Processing Systems, 30, 2017.   \nDu, Y., Wang, S., and Huang, L. Risk-sensitive reinforcement learning: Iterated cvar and the worst path. arXiv preprint arXiv:2206.02678, 2022.   \nEriksson, H. and Dimitrakakis, C. Epistemic risk-sensitive reinforcement learning. arXiv preprint arXiv:1906.06273, 2019.   \nFei, Y., Yang, Z., Chen, Y., Wang, Z., and Xie, Q. Risk-sensitive reinforcement learning: Nearoptimal risk-sample tradeoff in regret. Advances in Neural Information Processing Systems, 33: 22384\u201322395, 2020.   \nGivan, R., Dean, T., and Greig, M. Equivalence notions and model minimization in markov decision processes. Artificial Intelligence, 147(1-2):163\u2013223, 2003.   \nHau, J. L., Petrik, M., and Ghavamzadeh, M. Entropic risk optimization in discounted mdps. In International Conference on Artificial Intelligence and Statistics, pp. 47\u201376. PMLR, 2023.   \nHoeffding, W. Probability inequalities for sums of bounded random variables. The collected works of Wassily Hoeffding, pp. 409\u2013426, 1994.   \nHunter, D. R. Mm algorithms for generalized bradley-terry models. The annals of statistics, 32(1): 384\u2013406, 2004.   \nJain, A., Wojcik, B., Joachims, T., and Saxena, A. Learning trajectory preferences for manipulators via iterative improvement. Advances in neural information processing systems, 26, 2013.   \nLowd, D. and Davis, J. Learning markov network structure with decision trees. In 2010 IEEE International Conference on Data Mining, pp. 334\u2013343. IEEE, 2010. ", "page_idx": 10}, {"type": "text", "text": "OpenAI, R. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2:13, 2023. ", "page_idx": 11}, {"type": "text", "text": "Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback, 2022.   \nPacchiano, A., Saha, A., and Lee, J. Dueling rl: reinforcement learning with trajectory preferences. arXiv preprint arXiv:2111.04850, 2021.   \nRussell, S. J. and Norvig, P. Artificial intelligence a modern approach. London, 2010.   \nRusso, D. and Van Roy, B. Eluder dimension and the sample complexity of optimistic exploration. Advances in Neural Information Processing Systems, 26, 2013.   \nShen, Y., Tobia, M. J., Sommer, T., and Obermayer, K. Risk-sensitive reinforcement learning. Neural computation, 26(7):1298\u20131328, 2014.   \nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al. Mastering the game of go without human knowledge. nature, 550 (7676):354\u2013359, 2017.   \nWirth, C., F\u00fcrnkranz, J., and Neumann, G. Model-free preference-based reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.   \nWirth, C., Akrour, R., Neumann, G., F\u00fcrnkranz, J., et al. A survey of preference-based reinforcement learning methods. Journal of Machine Learning Research, 18(136):1\u201346, 2017.   \nWu, R. and Sun, W. Making rl with preference-based feedback efficient via randomization. arXiv preprint arXiv:2310.14554, 2023.   \nWu, Z. and Xu, R. Risk-sensitive markov decision process and learning under general utility functions. arXiv preprint arXiv:2311.13589, 2023.   \nXu, Y., Wang, R., Yang, L., Singh, A., and Dubrawski, A. Preference-based reinforcement learning with finite-time guarantees. Advances in Neural Information Processing Systems, 33:18784\u201318794, 2020a.   \nXu, Y., Wang, R., Yang, L. F., Singh, A., and Dubrawski, A. Preference-based reinforcement learning with finite-time guarantees. arXiv preprint arXiv:2006.08910, 2020b.   \nZanette, A. and Brunskill, E. Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. In International Conference on Machine Learning, pp. 7304\u20137312. PMLR, 2019.   \nZhan, W., Uehara, M., Kallus, N., Lee, J. D., and Sun, W. Provable offline preference-based reinforcement learning. arXiv preprint arXiv:2305.14816, 2023. ", "page_idx": 11}, {"type": "text", "text": "A Notation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We clarify the notations that appear uniquely in this paper to avoid confusion. ", "page_idx": 12}, {"type": "table", "img_path": "JNDcFOczOf/tmp/c46f1dd8332c4e482b16fc6ba621003bdbd3771eb174c106a6c688d9e55e4406.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "B Risk Aware Object Computability ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Unlike standard RL where each step\u2019s reward can be observed, PbRL represents a type of RL characterized by once-per-episode feedback. As a result, our observable and estimable parameters are confined to the trajectory reward $r_{\\xi}^{\\star}$ and transition probability functions $\\mathbf{P}^{\\star}$ . Consequently, the traditional risk-aware objective might be unsuitable, as the reduction in available information prevents the computation of the original risk-aware measure. The risk measure selected for $\\mathbf{P}\\mathbf{b}\\mathbf{R}\\mathbf{L}$ must satisfy the following condition: it should remain unique across MDPs where policies, trajectory rewards and transition probability functions even when each trajectory is fixed, but different step rewards, $r^{\\star}(s,a)$ , vary. This requirement ensures that the risk measure consistently reflects the underlying preferences regardless of variations in specific step rewards. ", "page_idx": 12}, {"type": "text", "text": "B.1 Nested Object ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Theorem B.1. For the tabular MDP and the reward of the entire trajectory can be decomposed as $\\begin{array}{r}{r_{\\xi}^{\\star}\\left(\\xi_{H}\\right)=\\sum_{h=1}^{H}r_{h}^{\\star}\\left(s_{h},a_{h}\\right)}\\end{array}$ , $V_{1}^{\\pi}$ in Eq. 5 and $\\tilde{V}_{1}^{\\pi}$ in Eq.6 are equivalent. ", "page_idx": 12}, {"type": "text", "text": "Proof. Firtly, according to Givan et al. (2003); Lowd & Davis (2010), any tabular MDP can be reformulated as a decision tree-like MDP. Thus, considering a tree-like structure for an MDP implies the following characteristics: ", "page_idx": 12}, {"type": "text", "text": "1. The state transition graph of the MDP is connected and acyclic. 2. Each state in the MDP corresponds to a unique node in the tree. 3. There is a single root node from which every other node is reachable via a unique path. 4. The transition probabilities between states follow the Markov property, i.e., the probability of transitioning to any future state depends only on the current state and not on the sequence of events that preceded it. ", "page_idx": 12}, {"type": "text", "text": "Formally, let $S$ be the set of states and $p_{i j}$ the transition probabilities between states $s_{i}$ and $s_{j}$ . The transition matrix $P$ for an MDP with a tree-like structure is defined such that: ", "page_idx": 12}, {"type": "text", "text": "$p_{i j}>0$ if there is an edge between $s_{i}$ and $s_{j}$ in the tree, and $p_{i j}=0$ otherwise. ", "page_idx": 12}, {"type": "text", "text": "Moreover, for each non-root node $s_{j}$ , there exists exactly one $s_{i}$ such that $p_{i j}\\,>\\,0$ , and $s_{i}$ is the unique parent of $s_{j}$ in the tree structure. ", "page_idx": 12}, {"type": "text", "text": "To classify the two value iteration in Eq. 3 and Eq. 4, we denote the value given by Eq. 4 as $\\tilde{V}_{h}^{\\pi}$ and the value given by Eq. 3 as $V_{h}^{\\pi}$ , thus, in tabular tree-like MDP with the reward of the entire trajectory which can be decomposed as $\\begin{array}{r}{r_{\\xi}^{\\star}\\left(\\xi_{H}\\right)=\\sum_{h=1}^{H}r_{h}^{\\star}\\left(s_{h},a_{h}\\right)}\\end{array}$ , we have the following relationship: ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tilde{V}_{h}^{\\pi}=V_{h}^{\\pi}+r_{1:h-1}^{\\star}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $r_{1:h}$ denotes Reward of the $1\\,\\sim\\,h$ steps of a trajectory. We prove this relationship by mathematical induction. ", "page_idx": 13}, {"type": "text", "text": "Initial case. Using the tree-like PbRL-MDP algorithm and the initial conditions of the Bellman equation, at the final step $h=H$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{V}_{H}^{\\pi}=r_{H}^{\\star}\\left(s_{H}^{\\prime},\\pi(\\xi_{H-1})\\right)+r_{1:H-1}^{\\star}}\\\\ &{\\qquad=V_{H}^{\\pi}+r_{1:H-1}^{\\star}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Induction step. We now proved that if ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{V}_{h+1}^{\\pi}=V_{h+1}^{\\pi}+r_{1:h}^{\\star}}\\\\ &{}\\\\ &{\\tilde{V}_{h}^{\\pi}=V_{h}^{\\pi}+r_{1:h-1}^{\\star}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "holds, then ", "page_idx": 13}, {"type": "text", "text": "also holds. ", "page_idx": 13}, {"type": "text", "text": "Since this tree-like MDP\u2019s policy $\\pi$ is fixed, it has only one path to arrive $_\\mathrm{h}$ th state $(s_{h})$ , denoted as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Xi_{h}(\\xi_{H,1})=\\Xi_{h}(\\xi_{H,2})}&{{}\\forall\\xi_{H,1},\\xi_{H,2}\\in\\{\\xi_{H}\\mid S_{h}(\\xi_{H})=s_{h}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, $r_{1:h-1}^{\\star}$ is unique. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{V}_{h}^{\\pi}=\\Phi\\left(V_{h+1}^{\\pi}(s_{h+1}^{\\prime})+r_{1:h}^{\\star}\\right),\\quad s_{h+1}^{\\prime}\\sim\\mathbf{P}^{\\star}(s,a)}\\\\ &{\\qquad=\\Phi\\left(V_{h+1}^{\\pi}(s_{h+1}^{\\prime})+r_{h}^{\\star}(s_{h},\\pi_{h}(\\xi_{h}))+r_{1:h-1}^{\\star}\\right),\\quad s_{h+1}^{\\prime}\\sim\\mathbf{P}^{\\star}(s,a)}\\\\ &{\\qquad=\\Phi\\left(V_{h+1}^{\\pi}(s_{h+1}^{\\prime})+r_{h}^{\\star}(s_{h},\\pi_{h}(\\xi_{h}))\\right)+r_{1:h-1}^{\\star},\\quad s_{h+1}^{\\prime}\\sim\\mathbf{P}^{\\star}(s,a)}\\\\ &{\\qquad=V_{h}^{\\pi}+r_{1:h-1}^{\\star}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By applying conclusion, we observe that when $h=1$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tilde{V}_{1}^{\\pi}=V_{1}^{\\pi}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus, we have proven that the for the tabular MDP and the reward of the entire trajectory can be decomposed as $\\begin{array}{r}{\\bar{r}_{\\xi}^{\\star}\\left(\\xi_{H}\\right)=\\sum_{h=1}^{H}r_{h}^{\\star}\\left(s_{h},a_{h}\\right)\\mathrm{,~}V_{1}^{\\pi}}\\end{array}$ in Eq. 3 and Eq. 4 are equivalent. ", "page_idx": 13}, {"type": "text", "text": "B.2 Static Object ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma B.2. For the tabular MDP and the reward of the entire trajectory can be decomposed as $\\begin{array}{r}{r_{\\xi}^{\\star}\\left(\\xi_{H}\\right)=\\sum_{h=1}^{H}r_{h}^{\\star}\\left(s_{h},a_{h}\\right)}\\end{array}$ , $V_{1}^{\\pi}$ in Eq. 5 and $\\tilde{V}_{1}^{\\pi}$ in Eq.6 are equivalent. ", "page_idx": 13}, {"type": "text", "text": "Proof. To classify the two value iteration in Eq. 5 and Eq. 6, we denote the value given by Eq. 5 as $\\tilde{V}_{h}^{\\pi}$ and the value given by Eq. 6 as $V_{h}^{\\pi}$ , thus, in tabular tree-like MDP with the reward of the entire trajectory which can be decomposed as $\\begin{array}{r}{r_{\\xi}^{\\star}\\left(\\xi_{H}\\right)=\\sum_{h=1}^{H}r_{h}^{\\star}\\left(s_{h},a_{h}\\right)}\\end{array}$ , we have the following relationship: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tilde{V}_{h}^{\\pi}=V_{h}^{\\pi}+r_{1:h-1}^{\\star}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $r_{1:h}$ denotes Reward of the $1\\sim h$ steps of a trajectory. ", "page_idx": 13}, {"type": "text", "text": "Now, We prove this relationship. ", "page_idx": 13}, {"type": "text", "text": "Since this tree-like MDP\u2019s policy $\\pi$ is fixed, it has only one path to arrive $_\\mathrm{h}$ th state $(s_{h})$ , denoted as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Xi_{h}(\\xi_{H,1})=\\Xi_{h}(\\xi_{H,2})}&{{}\\forall\\xi_{H,1},\\xi_{H,2}\\in\\{\\xi_{H}\\mid S_{h}(\\xi_{H})=s_{h}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, $r_{1:h-1}^{\\star}$ is unique. By definition, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{V}_{h}^{\\pi}=r_{\\xi}^{\\star}(\\xi_{H})\\quad\\xi_{H}\\sim\\mathbb{P}\\left(\\cdot\\mid\\Xi_{h}(\\xi_{H})=\\xi_{h}\\right)}\\\\ &{\\quad=r_{\\xi}^{\\star}(\\Xi_{h}(\\xi_{H}))+r_{1:h-1}^{\\star},\\quad\\xi_{H}\\sim\\mathbb{P}\\left(\\cdot\\mid\\Xi_{h}(\\xi_{H})=\\xi_{h}\\right)}\\\\ &{\\quad=V_{h}^{\\pi}+r_{1:h-1}^{\\star}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By applying conclusion, we observe that when $h=1$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{V}_{1}^{\\pi}=V_{1}^{\\pi}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, we have proven that the for the tabular MDP and the reward of the entire trajectory can be decomposed as $\\begin{array}{r}{\\overline{{r_{\\xi}^{\\star}\\left(\\xi_{H}\\right)}}=\\sum_{h=1}^{H}r_{h}^{\\star}\\left(s_{h},a_{h}\\right)\\mathrm{,~}V_{1}^{\\pi}}\\end{array}$ in Eq. 5 and $\\tilde{V}_{1}^{\\pi}$ in Eq.6 are equivalent. ", "page_idx": 14}, {"type": "text", "text": "C Difference between nested and static risk measure ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To explain the difference between nested and static risk measure, we present a simple example that demonstrates their characters. ", "page_idx": 14}, {"type": "image", "img_path": "JNDcFOczOf/tmp/3add0227b02759504d041ccfd17dcedd6eca3e1739895fbb8aa744dbfec14867.jpg", "img_caption": ["Figure 4: Cumulative regret for the different $\\alpha$ "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "First, we construct a MDP instance in fig. 4 where two policies exhibit identical reward distributions and consequently demonstrate equivalent preference will be observed . However, within this instance, these policies yield different outcomes under the nested and static CVaR metrics. ", "page_idx": 14}, {"type": "text", "text": "The state space is $S=\\{S_{1},S_{2,1},S_{2,2},S_{3,1},S_{3,2},S_{3,3},S_{3,4}\\}$ , where $S_{1}$ is the initial state. ", "page_idx": 14}, {"type": "text", "text": "The policy space is $\\Pi=\\{\\pi_{A},\\pi_{B}\\}$ . Both policy have have the same action in the first step $\\left(a_{1}\\right)$ and second step $\\left(a_{2}\\right)$ , but have the different action in the third step $(a_{3},a_{3}^{\\prime})$ . ", "page_idx": 14}, {"type": "text", "text": "The reward functions are as follows. $r(S_{1},a_{1})~=~0$ , $r(S_{2,1},a_{2})~=~0.1$ , $r(S_{2,2},a_{2})~=~0.2,$ $r(S_{3,1},a_{3})\\,=\\,0.3$ , $r(S_{3,2},a_{3})\\,=\\,0.2$ , $r(S_{3,3},a_{3})\\,=\\,0.4$ , $r(S_{3,2},a_{3})\\,=\\,0.5$ , $r(S_{3,1},a_{3}^{\\prime})\\,=\\,0.6,$ $r(S_{3,2},a_{3}^{\\prime})=0.2$ , $r(S_{3,3},a_{3}^{\\prime})=0.4$ , $r(S_{3,2},\\dot{a}_{3}^{\\prime})=0.2$ . ", "page_idx": 14}, {"type": "text", "text": "The transition distributions are as follows. $P\\left(S_{2,1}~|~S_{1},a_{1}\\right)~=~0.5,$ $P\\left(S_{2,2}\\mid S_{1},a_{1}\\right)~=~0.5,$ $P\\left(S_{3,1}\\mid S_{2,1},a_{2}\\right)=0.1,P\\left(S_{3,2}\\mid S_{2,1},a_{2}\\right)=0.9$ 9, $P\\left(S_{3,3}\\mid S_{2,2},a_{2}\\right)=0.1,P\\left(S_{3,4}\\mid S_{2,2},a_{2}\\right)=$ 0.9. ", "page_idx": 14}, {"type": "text", "text": "As depicted on the right side of the figure, the distribution of rewards is consistent. Consequently, the human feedback preferences for the two policies are identical. We list the differing risk measures for these two policies in Table C. ", "page_idx": 14}, {"type": "table", "img_path": "JNDcFOczOf/tmp/af3a8035ba945978b9a50f481309d3c1a56104c0ff6e9a5cb234eadac578268a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "D REGRET UPPER BOUND FOR ALGORITHM 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we present the full proof of Assumption 1\u2019s regret upper bound. The proof consists of parts. ", "page_idx": 14}, {"type": "text", "text": "D.1 REWARD ESTIMATION ERROR ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma D.1. Reward confidence set construction. Fix $\\delta\\in(0,1)$ , with probability at least $1-\\delta$ , for all $k\\in[K]$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{k^{\\prime}=1}^{k-1}[\\sigma(\\widehat{r}_{k}(\\xi_{1})-\\widehat{r}_{k}(\\xi_{2}))-\\sigma(r^{\\star}(\\xi_{1})-r^{\\star}(\\xi_{2}))]^{2}\\leq\\beta_{r,k}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\beta_{r,k}\\left(\\delta,\\frac{1}{K}\\right)\\leq\\mathcal{O}\\left(d i m_{\\mathbb{T}}l o g\\left(K\\left(1+2B\\rho_{w}\\right)\\right)+l o g(\\frac{1}{\\delta})\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. This lemma can be proved by the direct application of Lemma G.5. Let $X_{t}=(\\xi_{t,1},\\xi_{t,2})$ and $Y_{t}=o_{t}$ , and $\\mathcal{F}_{r,t}=\\{f_{r}^{'}|f_{r}(\\cdot,\\cdot)=\\sigma(r(\\cdot)-r\\bar{(\\cdot)})\\}$ . Then, we have that $X_{t}$ is $\\mathcal{F}_{t-1}$ measurable and $Y_{t}$ is $\\mathcal{F}_{t}$ measurable. According to Hoeffding\u2019s inequality (Theorem G.8) , $\\{Y_{t}-f_{r}\\left(X_{t}\\right)\\}$ is $\\frac{1}{2}$ -sub-gaussian conditioning, and $\\mathbb{E}\\left[Y_{t}-f_{r}\\left(X_{t}\\right)\\mid\\mathcal{F}_{t-1}\\right]=0$ . By Lemma G.5 and G.4 , since the linear trajectory embedding function $\\phi:\\mathcal{Z}_{H}\\rightarrow\\mathbb{R}^{d i m_{\\mathbb{T}}}$ , with probability at least $1-\\delta$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\beta_{r,k}\\left(\\delta,\\frac{1}{K}\\right)\\leq\\mathcal{O}\\left(d i m_{\\mathbb{T}}l o g\\left(K\\left(1+2B\\rho_{w}\\right)\\right)+l o g(\\frac{1}{\\delta})\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma D.2. Reward estimation error of trajectory embedding. For any $k\\in[0,\\ldots,K]$ , reward confidence set $B_{k}^{r}$ , where the reward function embedding weight can be noted as $\\mathbf{w}_{r}=(w_{1},\\dots,w_{d i m_{\\mathbb{T}}})$ , any fixed trajectory $\\xi_{H}\\in\\mathcal{Z}_{H}$ , trajectory embedding $\\phi(\\xi_{H})=(\\phi_{1},\\dots,\\phi_{d i m_{\\mathbb{T}}})$ , with probability at least $1-\\delta$ , it holds that, ", "page_idx": 15}, {"type": "equation", "text": "$$\nm a x_{r_{1},r_{2}\\in B_{k}^{r}}|(w_{r_{1},d}-w_{r_{2},d})|\\leq\\mathcal{O}\\left(\\frac{1}{\\kappa b}\\sqrt{\\frac{d i m_{\\mathbb{T}}l o g\\left(K\\left(1+2B\\rho_{w}\\right)\\right)+l o g(\\frac{1}{\\delta})}{n_{d i m,K}(d)}}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $n_{\\xi,k}$ denotes the number of times $\\xi_{H}$ was visted up to episode $k$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. According to Lemma D.1 and the assumption of link function, for fixed $k\\in[0,\\ldots,K]$ , and $d\\in[0,\\dotsc,d i m_{\\mathbb{T}}]$ , we have, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle m a x_{r_{1},r_{2}\\in B_{K}^{r}}\\sum_{k^{\\prime}=1}^{k}\\left|(w_{r_{1},d}-w_{r_{2},d})\\right|^{2}b^{2}\\mathbb{I}(B\\neq0)}\\\\ &{\\le\\displaystyle\\sum_{k=1}^{K}\\left|((w_{r_{1},d}-w_{r^{\\star},d})\\cdot B)\\right|^{2}}\\\\ &{\\le\\displaystyle\\frac{\\beta_{r,K}}{\\kappa^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Where $n_{d i m,k}(d)$ denotes the number of $\\phi_{d}(\\xi_{H})\\neq0$ among $1\\sim k$ episode\u2019s trajectory. ", "page_idx": 15}, {"type": "text", "text": "Using Cauchy-Schwarz inequality and Lemma G.6, we have, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{r_{1},r_{2}\\in B_{k}^{r}}|(w_{r_{1},d}-w_{r_{2},d})|\\leq\\mathcal{O}\\left(\\frac{1}{\\kappa b}\\sqrt{\\frac{d i m_{\\mathbb{T}}l o g\\left(K\\left(1+2B\\rho_{w}\\right)\\right)+l o g\\left(\\frac{1}{\\delta}\\right)}{n_{d i m,K}(d)}}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma D.3. Reward estimation error of the whole distribution. ", "text_level": 1, "page_idx": 15}, {"type": "equation", "text": "$\\begin{array}{r}{b_{k}^{r}(\\xi_{1},\\xi_{2})=\\operatorname*{max}_{r_{1},r_{2}\\in{\\cal{B}}_{k}^{r}}[(r_{1}(\\xi_{1})-r_{1}(\\xi_{2}))-(r_{2}(\\xi_{1})-r_{2}(\\xi_{2}))],}\\end{array}$ ", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}b_{k}^{r}(\\xi_{H,1,k},\\xi_{H,2,k})\\leq\\mathcal{O}(d i m_{\\mathbb{T}}\\sqrt{K\\log(K B\\rho_{w})\\log\\left(\\frac{K\\left(1+2B\\rho_{w}\\right)}{\\delta}\\right)})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Let $\\mathcal{F}_{k}~=~\\{f_{r}|f_{r}(x,y)~=~\\sigma(r(x)~-~r(y)),r~\\in~\\mathcal{B}_{k}^{r}\\},$ then we define $d i a m(\\mathcal{F}_{B_{k}^{r}})\\ =$ $b_{k}^{\\sigma}(x,y)~=~\\operatorname*{max}_{r_{1},r_{2}\\in{\\mathcal B}_{k}^{r}}$ $\\sigma(r_{1}(x)\\,-\\,r_{1}(y))\\,-\\,\\sigma(r_{2}(x)\\,-\\,r_{2}(y))$ . According to Lemma. D.1, $\\delta_{k}\\,=\\,\\operatorname*{max}_{1\\leq k\\leq K}$ diam $\\left(\\left.\\mathcal{F}_{k}\\right\\vert_{x_{1:K}}\\right)$ . Let $\\alpha\\,=\\,1/K$ , $T\\,=\\,k$ , and $d\\,=\\,\\mathrm{dim}_{\\varepsilon}(\\mathcal{F}_{\\mathcal{R}},1/K)$ According to lemma G.6, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}b_{k}^{r}(\\xi_{H,1,k},\\xi_{H,2,k})\\leq\\mathcal{O}(d i m_{\\mathbb{T}}\\sqrt{K\\log(K B\\rho_{w})\\log\\left(\\frac{K\\left(1+2B\\rho_{w}\\right)}{\\delta}\\right)})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma D.4. Transition estimation error of fixed state action pair.For any fixed $k$ , with probability at least $1-2\\delta$ , for any $(s,a)\\in S\\times A$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{s^{\\prime}\\in S}\\left|\\hat{\\mathbf{P}}_{k}\\left(s^{\\prime}\\mid s,a\\right)-\\mathbf{P}^{\\star}\\left(s^{\\prime}\\mid s,a\\right)\\right|\\le\\sqrt{\\frac{2S\\log\\left(\\frac{2K H S A}{\\delta}\\right)}{n_{k}(s,a)}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. The proof is same as Eq. (55) in Zanette & Brunskill (2019). ", "page_idx": 16}, {"type": "text", "text": "Lemma D.5. Transition estimation error of whole distribution.For any fixed $k$ , with probability at least $1-\\delta,$ , for any $(s,a)\\in S\\times A$ , excuted policy $\\pi_{1},\\pi_{2}\\in\\Pi_{k}$ and any transition possibility kernel $\\mathbf{P}_{1},\\mathbf{P}_{2}\\in\\mathcal{B}_{k}^{\\mathbf{P}}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{2}\\sum_{k=1}^{K}\\operatorname*{max}_{\\mathbf{P}_{1},\\mathbf{P}_{2}\\in B_{k}^{\\mathrm{P}}}E_{\\xi_{i}\\sim\\pi_{i}}(\\sum_{h=1}^{H}\\left|\\mathbf{P}_{1}\\left(s^{\\prime}\\mid s_{i,k,h},a_{i,k,h}\\right)-\\mathbf{P}_{2}\\left(s^{\\prime}\\mid s_{i,k,h},a_{i,k,h}\\right)\\right|)}\\\\ &{{\\le}\\mathcal{O}\\left(S^{2}A H^{3/2}\\sqrt{K\\log(K)l o g\\left(K/\\delta\\right)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. The proof is same as Lemma. A.5 in Chen et al. (2022). ", "page_idx": 16}, {"type": "text", "text": "Lemma D.6. For any iteration value $V:S\\,\\rightarrow\\,R$ , any two transition possibility kernel $\\mathbf{P},{\\hat{\\mathbf{P}}}\\,:$ $S\\times S\\times A\\to$ , and the risk aware object form: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Phi(V(s^{\\prime}))=\\int_{0}^{1}F_{V(s^{\\prime})}^{\\dag}(\\xi)\\cdot d G(\\xi)\\quad s^{\\prime}\\sim{\\bf P}(s,a)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\Phi_{s^{\\prime}\\sim\\hat{\\mathbf{P}}\\left(\\cdot\\left|s,a\\right)}\\left(V\\left(s^{\\prime}\\right)\\right)-\\Phi_{s^{\\prime}\\sim\\mathbf{P}\\left(\\cdot\\left|s,a\\right)}\\left(V\\left(s^{\\prime}\\right)\\right)\\right|\\right.}\\\\ &{\\left.\\leq L_{G}H\\displaystyle\\sum_{s^{\\prime}\\in S}\\left|\\hat{\\mathbf{P}}\\left(s^{\\prime}\\mid s,a\\right)-\\mathbf{P}\\left(s^{\\prime}\\mid s,a\\right)\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We firstly sort all successor states $s^{\\prime}\\in\\mathcal{S}$ by $V\\left(s^{\\prime}\\right)$ in ascending order (from the left to the right) as $s_{1}^{\\prime},s_{2}^{\\prime}\\ldots s_{S}^{\\prime}$ . And we assume that $V(s_{S+1}^{\\prime}=1)$ ) . Thus, according to the quantile function\u2019s definition, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left|\\Phi_{s^{\\prime}\\sim\\mathbb{P}(\\cdot\\vert s,a)}\\left(V\\left(s^{\\prime}\\right)\\right)-\\Phi_{s^{\\prime}\\sim\\mathbf{P}(\\cdot\\vert s,a)}\\left(V\\left(s^{\\prime}\\right)\\right)\\right|}\\\\ &{\\displaystyle=\\vert\\int_{0}^{1}F_{V^{\\ast}\\left(s^{\\prime}\\right)}^{\\dagger}(\\xi)\\cdot d G(\\xi)-\\int_{0}^{1}F_{V^{\\ast}\\left(s^{\\prime}\\right)}^{\\dagger}(\\xi)\\cdot d G(\\xi)\\vert}\\\\ &{\\displaystyle=\\vert\\int_{0}^{1}G(F_{V^{\\ast}\\left(s^{\\prime}\\right)}(\\xi))\\cdot d\\xi-\\int_{0}^{1}G(F_{V^{\\ast}\\left(s^{\\prime}\\right)}(\\xi))\\cdot d\\xi\\vert}\\\\ &{\\displaystyle=\\sum_{i=1}^{s}\\vert{\\cal V}(s_{i+1}^{\\prime})-V(s_{i}^{\\prime})\\vert\\cdot\\vert G(\\sum_{j=1}^{i}\\mathbf{P}(s_{j}^{\\prime}|(s,a)))-G(\\sum_{j=1}^{i}\\hat{\\mathbf{P}}(s_{j}^{\\prime}|(s,a)))\\vert}\\\\ &{\\displaystyle\\leq\\sum_{i=1}^{s}({\\cal V}(s_{i+1}^{\\prime})-{\\cal V}(s_{i}^{\\prime}))\\cdot L_{G}\\displaystyle\\sum_{j=1}^{i}\\vert\\mathbf{P}(s_{j}^{\\prime}|(s,a))-\\hat{\\mathbf{P}}(s_{j}^{\\prime}|(s,a))\\vert}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\sum_{i=1}^{S}(V(s_{i+1}^{\\prime})-V(s_{i}^{\\prime}))\\cdot L_{G}\\sum_{j=1}^{S}|\\mathbf{P}(s_{j}^{\\prime}|(s,a))-\\hat{\\mathbf{P}}(s_{j}^{\\prime}|(s,a))|}\\\\ {\\displaystyle\\leq L_{G}\\sum_{j=1}^{S}|\\mathbf{P}(s_{j}^{\\prime}|(s,a))-\\hat{\\mathbf{P}}(s_{j}^{\\prime}|(s,a))|\\sum_{i=1}^{S}(V(s_{i+1}^{\\prime})-V(s_{i}^{\\prime}))}\\\\ {\\displaystyle\\leq L_{G}\\cdot H\\sum_{j=1}^{S}|\\mathbf{P}(s_{j}^{\\prime}|(s,a))-\\hat{\\mathbf{P}}(s_{j}^{\\prime}|(s,a))|}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The second to fourth lines follow the discussion on the equivalent expression of the discontinuous distribution $\\Phi$ in Lemma 5.1 of Bastani et al. (2022). The fifth line is derived from the properties of Lipschitz functions and Assumption 3.5. ", "page_idx": 17}, {"type": "text", "text": "Since we use the emperical estimation $\\hat{{\\bf P}}_{k}$ of the transaction kernel $\\mathbf{P}^{\\star}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{P}}_{k}=a r g m i n_{\\mathbf{P}\\in\\mathcal{P}}\\sum_{i=1}^{2}\\sum_{k^{\\prime}=1}^{k-1}\\sum_{h=1}^{H}|\\langle\\mathbf{P}(s_{i,k^{\\prime},h},a_{i,k^{\\prime},h}),\\mathbb{I}(s_{i,k^{\\prime},h+1})\\rangle|^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma D.7. Concentration for $V.$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "With probability at least $1-2\\delta$ , it holds that, for any $k\\,\\in\\,[K],\\,\\,(s,a)\\,\\in\\,\\mathcal{S}\\,\\times\\,\\mathcal{A},$ , any transition possibility kernel $\\mathbf{P}_{1}\\in\\mathcal{B}_{k}^{\\mathbf{P}}$ and function $V:S\\mapsto[0,H]$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|\\Phi_{s^{\\prime}\\sim\\mathbf{P}_{1}\\left(\\cdot\\mid s,a\\right)}\\left(V\\left(s^{\\prime}\\right)\\right)-\\Phi_{s^{\\prime}\\sim\\mathbf{P}^{\\star}\\left(\\cdot\\mid s,a\\right)}\\left(V\\left(s^{\\prime}\\right)\\right)\\right|\\leq2L_{G}\\cdot H\\sqrt{\\frac{2S\\log\\left(\\frac{2K H S A}{\\delta}\\right)}{n_{k}\\left(s,a\\right)}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here $n_{k}(s,a)$ is the number of times $(s,a)$ was visited up to episode $k$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. According to Lemma D.4 and recall the definition of the transition possibility confidence set, with probability at least $1-2\\delta$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{s^{\\prime}\\in S}{\\sum}|\\mathbf{P}^{\\star}\\left(s^{\\prime}\\mid s,a\\right)-\\mathbf{P}_{1}\\left(s^{\\prime}\\mid s,a\\right)|}\\\\ &{\\leq\\underset{s^{\\prime}\\in S}{\\sum}\\left|\\mathbf{P}^{\\star}\\left(s^{\\prime}\\mid s,a\\right)-\\hat{\\mathbf{P}}_{k}\\left(s^{\\prime}\\mid s,a\\right)+\\hat{\\mathbf{P}}_{k}\\left(s^{\\prime}\\mid s,a\\right)-\\mathbf{P}_{1}\\left(s^{\\prime}\\mid s,a\\right)\\right|}\\\\ &{\\leq\\underset{s^{\\prime}\\in S}{\\sum}\\left|\\hat{\\mathbf{P}}_{k}\\left(s^{\\prime}\\mid s,a\\right)-\\mathbf{P}^{\\star}\\left(s^{\\prime}\\mid s,a\\right)\\right|+\\underset{s^{\\prime}\\in S}{\\sum}\\left|\\hat{\\mathbf{P}}_{k}\\left(s^{\\prime}\\mid s,a\\right)-\\mathbf{P}_{1}\\left(s^{\\prime}\\mid s,a\\right)\\right|}\\\\ &{\\leq2\\sqrt{\\frac{2S\\log\\left(\\frac{2K H S A}{\\delta}\\right)}{n_{k}\\left(s,a\\right)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Plugging Lemma .D.6, we obtain that with probability at least $1-2\\delta$ , for any $k\\in[K],(s,a)\\in\\mathcal{S}\\times\\mathcal{A}$ and function $V:S\\mapsto[0,H]$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\vert\\Phi_{s^{\\prime}\\sim\\hat{\\mathbf{P}}^{k}\\left(\\cdot\\right\\vert s,a\\right)}\\left(V\\left(s^{\\prime}\\right)\\right)-\\Phi_{s^{\\prime}\\sim\\mathbf{P}^{\\star}\\left(\\cdot\\vert s,a\\right)}\\left(V\\left(s^{\\prime}\\right)\\right)\\right\\vert\\le2L_{G}\\cdot H\\sqrt{\\frac{2S\\log\\left(\\frac{2K H S A}{\\delta}\\right)}{n_{k}\\left(s,a\\right)}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Recall the definition of quantile function $\\Phi$ , $G$ is the quantile CDF weight. Given any target value $V:S\\rightarrow R$ , we use $\\beta_{\\mathbf{P}^{\\star}}^{G,\\bar{V}}\\left(s^{\\prime}\\mid s,a\\right)$ denotes the conditional probability of transitioning to $s^{\\prime}$ from $(s,a)$ , conditioning on transitioning to the quantile distribution, and it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\beta_{\\mathbf{P}^{\\star}}^{G,V}\\left(s_{i}^{\\prime}\\mid s,a\\right)=G(\\sum_{j=1}^{i+1}\\mathbf{P}^{\\star}(s_{j}^{\\prime}|(s,a)))-G(\\sum_{j=1}^{i}\\mathbf{P}^{\\star}(s_{j}^{\\prime}|(s,a)))\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma D.8. Quantile Reward Gap due to Value Function Shift. For any $(s,a)\\ \\in\\ S\\,\\times\\,A_{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\,\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ , distribution $\\mathbf{P}$ , and functions $V,V^{\\prime}:\\bar{S}\\mapsto[0,H],$ , for any $s^{\\prime}\\in\\mathcal{S}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Phi_{s^{\\prime}\\sim\\mathbf{P}\\left(\\cdot\\mid s,a\\right)}\\left(V^{\\prime}\\left(s^{\\prime}\\right)\\right)-\\Phi_{s^{\\prime}\\sim\\mathbf{P}\\left(\\cdot\\mid s,a\\right)}\\left(V\\left(s^{\\prime}\\right)\\right)\\leq\\beta_{\\mathbf{P}}^{G,V}(\\cdot\\mid s,a)^{\\top}\\left\\vert V^{\\prime}-V\\right\\vert\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This Lemma\u2019s proof is similar to Lemma 11 in Du et al. (2022). ", "page_idx": 18}, {"type": "text", "text": "Lemma D.9. For any $(s,a)\\in S\\times A_{\\cdot}$ , distribution $\\mathbf{P}$ , and functions $V,V^{\\prime}:{\\cal S}\\mapsto[0,H],$ , for any $s^{\\prime}\\in\\mathcal{S}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Phi_{s^{\\prime}\\sim\\mathbf{P}\\left(\\cdot\\mid s,a\\right)}\\left(V^{\\prime}\\left(s^{\\prime}\\right)\\right)-\\Phi_{s^{\\prime}\\sim\\mathbf{P}\\left(\\cdot\\mid s,a\\right)}\\left(V\\left(s^{\\prime}\\right)\\right)\\le L_{G}\\mathbf{P}(\\cdot\\mid s,a)^{\\top}\\left\\vert V^{\\prime}-V\\right\\vert\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. This Lemma comes from Lemma .D.8 and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\beta^{G,V}\\left(s_{i}^{\\prime}\\mid s,a\\right)=G(\\sum_{j=1}^{i+1}{\\bf P}^{\\star}(s_{j}^{\\prime}|(s,a)))-G(\\sum_{j=1}^{i}{\\bf P}^{\\star}(s_{j}^{\\prime}|(s,a)))}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\leq L_{G}{\\bf P}(\\cdot\\mid s,a)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For any $k>0,h\\in[H]$ and $(s,a)\\in S\\times A$ , let $p_{k h}(s,a)$ denote the probability of visiting $(s,a)$ at step $h$ of episode $k$ . Then, it holds that for any $k>0,h\\in[H]$ and $(s,a)\\in\\mathcal{S}\\times\\mathcal{A},p_{k h}(s,a)\\in[0,1]$ and $\\textstyle\\sum_{(s,a)\\in{\\cal S}\\times{\\cal A}}p_{k h}(s,a)=1$ ", "page_idx": 18}, {"type": "text", "text": "Lemma D.10. (Concentration of state-action visitation). It holds that ", "text_level": 1, "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[n_{k}(s,a)\\geq{\\frac{1}{2}}\\sum_{k^{\\prime}=1}^{k-1}\\sum_{h=1}^{H}p_{k^{\\prime}h}(s,a)-H\\log\\left({\\frac{H S A}{\\delta}}\\right),\\forall k>0,\\forall(s,a)\\in{\\mathcal{S}}\\times A\\right]\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This Lemma is a direct application of Lemma G.9 and same as Du et al. (2022). ", "page_idx": 18}, {"type": "text", "text": "Lemma D.11. (Concentration of trajectory visitation). It holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[n_{d i m,k}(d)\\geq\\frac{1}{2}\\sum_{k^{\\prime}=1}^{k-1}p_{d i m,k^{\\prime}}(\\xi_{H})-\\log\\left(\\frac{d i m_{\\mathbf{T}}}{\\delta}\\right),\\forall d\\in[0,\\dots,d i m_{\\mathbb{T}}]\\right]\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. This Lemma a direct application of G.9. For any dimension $d\\in[0,\\dotsc,d i m_{\\mathbb{T}}]$ , it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[n_{d i m,k}(d)\\geq\\frac{1}{2}\\sum_{k^{\\prime}=1}^{k-1}p_{d i m,k^{\\prime}}(\\xi_{H})-\\log\\left(\\frac{1}{\\delta}\\right)\\right]\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $d\\in[0,\\dotsc,d i m_{\\mathbb{T}}]$ , Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[n_{d i m,k}(d)\\geq\\frac{1}{2}\\sum_{k^{\\prime}=1}^{k-1}p_{d i m,k^{\\prime}}(\\xi_{H})-\\log\\left(\\frac{d i m_{\\mathbf{T}}}{\\delta}\\right),d\\in[0,\\dots,d i m_{\\mathbb{T}}]\\right]\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Definition of sufficient state-action visitations.Following Zanette & Brunskill (2019), for any episode $k>0$ , we define the set of state-action pairs which have sufficient visitations in expectation as follows. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}:=\\left\\{(s,a)\\in S\\times A:\\frac{1}{4}\\sum_{k^{\\prime}=1}^{k-1}\\sum_{h=1}^{H}p_{k^{\\prime}h}(s,a)\\geq H\\log\\left(\\frac{H S A}{\\delta}\\right)+H\\right\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Definition of sufficient trajetory visitations. Following Zanette & Brunskill (2019), for any episode $k>0$ , we define the set of trajrctory dimension which have sufficient visitations in expectation as follows. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}_{d i m,k}:=\\left\\{d\\in[0,\\ldots,d i m_{\\mathbb{T}}]:\\frac{1}{4}\\sum_{k^{\\prime}=1}^{k-1}p_{d i m,k^{\\prime}}(d)\\geq\\log\\left(\\frac{d i m_{\\mathbb{T}}}{\\delta}\\right)+1\\right\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We use $n_{d i m,k}(d)$ to denote the number of $\\phi_{d}(\\xi_{H})\\neq0$ among $1\\sim k$ episode\u2019s trajectory. ", "page_idx": 19}, {"type": "text", "text": "Lemma D.12. (Standard state action visitation ratio). For any $K>0$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sqrt{\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\sum_{(s,a)\\in\\mathcal{L}_{k}}\\frac{p_{k h}(s,a)}{n_{k}(s,a)}}\\leq2\\sqrt{S A\\log\\left(\\frac{K H S A}{\\delta}\\right)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This proof is the same as that of Lemma 13 in Zanette & Brunskill (2019). ", "page_idx": 19}, {"type": "text", "text": "Lemma D.13. (Standard trajectory visitation ratio). For any $K>0$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sqrt{\\sum_{k=1}^{K}\\sum_{d=1}^{d i m_{\\mathbb{T}}}\\frac{p_{d i m,k}(d)}{n_{d i m,k}(d)}}\\leq2\\sqrt{d i m_{\\mathbb{T}}\\log\\left(\\frac{K d i m_{\\mathbb{T}}}{\\delta}\\right)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This proof is the same as that of Lemma 13 in Zanette & Brunskill (2019). ", "page_idx": 19}, {"type": "text", "text": "Lemma D.14. (Standard Invisitation Ratio). For any $K>0$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\sum_{(s,a)\\notin{\\mathcal{L}}_{k}}p_{k h}(s,a)<{\\frac{1}{\\operatorname*{min}_{\\pi,h,s:p_{\\pi,h}(s)>0}p_{\\pi,h}(s)}}\\cdot\\left(4H\\log\\left({\\frac{H S A}{\\delta}}\\right)+5H\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This proof is the same as that of Lemma 10 in Du et al. (2022). ", "page_idx": 19}, {"type": "text", "text": "Lemma D.15. (Standard trajectory Invisitation Ratio). For any $\\smash{K\\to\\binom{0}{2}}$ , we have $\\sum_{k=1}^{K}\\sum_{d\\notin{\\cal{L}}_{d,k}}p_{d i m,k}(d)<\\operatorname*{min}_{\\pi,d:p_{d i m,\\pi}(d)>0}p_{d i m,\\pi}(d)\\cdot\\left(4\\log\\left(\\frac{d i m_{\\mathbb{T}}}{\\delta}\\right)+5\\right)$ ", "page_idx": 19}, {"type": "text", "text": "This proof is the same as that of Lemma 10 in Du et al. (2022). ", "page_idx": 19}, {"type": "text", "text": "Lemma D.16. For any functions $V:{\\mathcal{S}}\\,\\mapsto\\,\\mathbb{R},k\\,>\\,0,h\\,\\in\\,[H]$ and $(s,a)\\,\\in\\,S\\,\\times\\,A$ such that $p_{k h}(s,a)>0,$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{p_{k h}^{G,V}(s,a)}{p_{k h}(s,a)}\\leq\\frac{1}{\\operatorname*{min}_{\\pi,h,(s,a):w_{\\pi,h}(s,a)>0}w_{\\pi,h}(s,a)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $p_{k h}^{G,V}(s,a)$ denotes the conditional probability of visiting $(s,a)$ at step h of episode $k$ , conditioning on transitioning distortion by the quantion function $G$ which works on at each step $h^{\\prime}=1,\\ldots,h-1$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Since $p_{k h}^{G,V}(s,a)$ is the conditional probability of visiting $(s,a)$ , we have pkGha,V(s, a) \u2208 $[0,1]$ . Since $p_{k h}(s,a)$ is the probability of visiting $(s,a)$ at step $h$ under policy $\\pi^{k}$ and $\\begin{array}{r}{\\operatorname*{min}_{\\pi,h,(s,a):w_{\\pi,h}(s,a)>0}{w_{\\pi,h}(s,a)}}\\end{array}$ is the minimum probability of visiting any reachable $(s,a)$ at any step $h$ over all policies $\\pi$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\np_{k h}(s,a)\\geq\\operatorname*{min}_{\\pi,h,(s,a):w_{\\pi,h}(s,a)>0}w_{\\pi,h}(s,a).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{p_{k h}^{G,\\alpha,V}(s,a)}{p_{k h}(s,a)}\\leq\\frac{1}{\\operatorname*{min}_{\\pi,h,(s,a):w_{\\pi,h}(s,a)>0}{w_{\\pi,h}(s,a)}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma D.17. For any functions $V:S\\mapsto\\mathbb{R},k>0,h\\in[H],$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{p_{d i m,k}^{G,V}(d)}{p_{d i m,k}(d)}\\leq\\frac{1}{\\operatorname*{min}{p_{\\pi}(d)}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $p_{d i m,k}^{G,V}(d)$ denotes the conditional probability of $\\phi_{d}(\\xi_{H})\\neq0$ of episode $k$ , conditioning on transitioning distortion by the quantion function $G$ which always works on at each step $h=1,\\ldots,H$ . ", "page_idx": 20}, {"type": "text", "text": "The proof is similar to Lemma .D.16. ", "page_idx": 20}, {"type": "text", "text": "D.2 Proof of Algorithm 1\u2019s regret upper bound ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma D.18. For a probability of $1-2\\delta$ , it holds that $\\pi^{\\star}\\in\\Pi_{k}$ , which is calculated as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Pi_{k}=\\{\\pi\\mid\\operatorname*{max}_{r_{\\xi}\\in B_{k}^{r},\\mathbf{P}\\in B_{k}^{\\mathbf{P}}}(\\tilde{V}_{1,r_{\\xi},\\mathbf{P}}^{\\pi}(\\xi_{h})-\\tilde{V}_{1,r_{\\xi},\\mathbf{P}}^{\\pi_{0}}(\\xi_{h}))\\geq0,\\forall\\pi_{0}\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. It equals to for any policy $\\pi_{0}\\in\\Pi$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{r_{\\xi}\\in\\mathcal{B}_{k}^{r},\\mathbf{P}\\in\\mathcal{B}_{k}^{\\mathbf{P}}}(\\tilde{V}_{1,r_{\\xi},\\mathbf{P}}^{\\pi^{\\star}}(\\xi_{h})-\\tilde{V}_{1,r_{\\xi},\\mathbf{P}}^{\\pi_{0}}(\\xi_{h}))\\geq0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "According to Lemma D.1 and Lemma D.4, with at least possibility $1-2\\delta$ , it holds that: ", "page_idx": 20}, {"type": "equation", "text": "$$\nr_{\\xi}^{\\star}\\in B_{k}^{r},\\mathbf{P}^{\\star}\\in B_{k}^{\\mathbf{P}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{r_{\\xi}\\in B_{k}^{r},\\mathbf{P}\\in B_{k}^{\\mathbf{P}}}{\\operatorname*{max}}(\\tilde{V}_{1,r_{\\xi},\\mathbf{P}}^{\\pi^{\\star}}(\\xi_{h})-\\tilde{V}_{1,r_{\\xi},\\mathbf{P}}^{\\pi_{0}}(\\xi_{h}))}\\\\ &{\\geq\\tilde{V}_{1,r_{\\xi},\\mathbf{P}}^{\\pi^{\\star}}(\\xi_{h})-\\tilde{V}_{1,r_{\\xi},\\mathbf{P}}^{\\pi_{0}}(\\xi_{h})\\geq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Where the last equation comes from the definition of optimal policy $\\pi^{\\star}$ (Eq. 7). ", "page_idx": 20}, {"type": "text", "text": "Lemma D.19. Given a positive constant $\\delta\\in(0,1]$ , with probability at least $1-4K\\delta$ , we have the following inequality holds: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{Reg}_{n e s t e d}(K)}\\\\ &{\\leqslant\\underset{r_{1}\\in B_{k}^{r},\\mathbf{P}_{1}\\in B_{k}^{r}}{\\operatorname*{max}}\\{\\tilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi^{\\star}}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi_{1}}\\left(s_{k,1}\\right)-(\\tilde{V}_{1,r_{1},\\mathbf{P}_{1}}^{\\pi^{\\star}}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r_{1},\\mathbf{P}_{1}}^{\\pi_{1}}\\left(s_{k,1}\\right))\\}}\\\\ &{+\\underset{r_{2}\\in B_{k}^{r},\\mathbf{P}_{2}\\in B_{k}^{r}}{\\operatorname*{max}}\\{\\tilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi^{\\star}}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi_{2}}\\left(s_{k,1}\\right)-(\\tilde{V}_{1,r_{2},\\mathbf{P}_{2}}^{\\pi^{\\star}}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r_{2},\\mathbf{P}_{2}}^{\\pi_{2}}\\left(s_{k,1}\\right))\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Reg}(K)=\\displaystyle\\sum_{k=1}^{K}\\left(\\tilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi^{\\star}}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi_{1}}\\left(s_{k,1}\\right)+\\tilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi^{\\star}}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi_{2}}\\left(s_{k,1}\\right)\\right)}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{k=1}^{K}\\underset{r_{1}\\in\\mathcal{B}_{k}^{r},\\mathbf{P}_{1}\\in\\mathcal{B}_{k}^{\\mathbf{P}}}{\\mathrm{max}}(\\tilde{V}_{1,r^{\\star},\\mathbf{P}_{1}}^{\\pi^{\\star}}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r_{1},\\mathbf{P}_{1}}^{\\pi_{1}}\\left(s_{k,1}\\right))}\\\\ &{\\qquad+\\tilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi^{\\star}}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi_{1}}\\left(s_{k,1}\\right)-\\underset{r_{1}\\in\\mathcal{B}_{k}^{r},\\mathbf{P}_{1}\\in\\mathcal{B}_{k}^{\\mathbf{P}}}{\\mathrm{max}}(\\tilde{V}_{1,r_{1},\\mathbf{P}_{1}}^{\\pi^{\\star}}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r_{1},\\mathbf{P}_{1}}^{\\pi_{1}}\\left(s_{k,1}\\right))}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{+\\displaystyle\\sum_{k=1}^{K}\\underset{r_{2}\\in B_{k}^{r},\\mathbf{P}_{2}\\in B_{k}^{\\mathbf{P}}}{\\operatorname*{max}}(\\tilde{V}_{1,r_{2},\\mathbf{P}_{2}}^{\\pi^{*}}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r_{2},\\mathbf{P}_{2}}^{\\pi_{2}}\\left(s_{k,1}\\right))}\\\\ &{+\\tilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi^{*}}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi_{2}}\\left(s_{k,1}\\right)-\\underset{r_{2}\\in B_{k}^{r},\\mathbf{P}_{2}\\in B_{k}^{\\mathbf{P}}}{\\operatorname*{max}}(\\tilde{V}_{1,r_{2},\\mathbf{P}_{2}}^{\\pi^{*}}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r_{2},\\mathbf{P}_{2}}^{\\pi_{2}}\\left(s_{k,1}\\right))}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\le\\tilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi^{\\star}}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi_{1}}\\left(s_{k,1}\\right)-\\operatorname*{max}_{r_{1}\\in B_{k}^{r},\\mathbf{P}_{1}\\in B_{k}^{\\mathbf{P}}}(\\tilde{V}_{1,r_{1},\\mathbf{P}_{1}}^{\\pi^{\\star}}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r_{1},\\mathbf{P}_{1}}^{\\pi_{1}}\\left(s_{k,1}\\right))\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n+\\tilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi^{\\star}}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi_{2}}\\left(s_{k,1}\\right)-\\operatorname*{max}_{r_{2}\\in B_{k}^{r},\\mathbf{P}_{2}\\in B_{k}^{\\mathbf{P}}}(\\tilde{V}_{1,r_{2},\\mathbf{P}_{2}}^{\\pi^{\\star}}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r_{2},\\mathbf{P}_{2}}^{\\pi_{2}}\\left(s_{k,1}\\right))\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\overset{b}{\\leq}\\operatorname*{max}_{r_{1}\\in B_{k}^{r},\\mathbf{P}_{1}\\in B_{k}^{\\mathrm{P}}}\\{\\tilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi^{\\star}}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi_{1}}\\left(s_{k,1}\\right)-(\\tilde{V}_{1,r_{1},\\mathbf{P}_{1}}^{\\pi^{\\star}}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r_{1},\\mathbf{P}_{1}}^{\\pi_{1}}\\left(s_{k,1}\\right))\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n+\\operatorname*{max}_{\\substack{r_{2}\\in B_{k}^{r},\\mathbf{P}_{2}\\in B_{k}^{\\mathrm{P}}}}\\{\\widetilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi^{\\star}}\\left(s_{k,1}\\right)-\\widetilde{V}_{1,r^{\\star},\\mathbf{P}^{\\star}}^{\\pi_{2}}\\left(s_{k,1}\\right)-(\\widetilde{V}_{1,r_{2},\\mathbf{P}_{2}}^{\\pi^{\\star}}\\left(s_{k,1}\\right)-\\widetilde{V}_{1,r_{2},\\mathbf{P}_{2}}^{\\pi_{2}}\\left(s_{k,1}\\right))\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Where (a) comes from the definition of the optimal policy confidence set (Lemma D.18) when $\\pi^{\\star}\\in\\Pi_{k}$ at each episode , (b) derives from the characters of max value. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "D.3 Nested Regret ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma D.20. Nested regret upper bound. Given a positive constant $\\delta\\in(0,1],$ , with probability at least $1-\\delta_{i}$ , we have the following inequality holds for every $k\\in[K]$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{Reg}_{n e s t e d}\\left(K\\right)}\\\\ &{\\le\\mathcal{O}\\left(L_{G}H^{\\frac32}\\sqrt{K}S A\\log\\left(\\frac{K H S A}{\\delta}\\right)\\cdot\\frac{1}{\\sqrt{\\operatorname*{min}_{\\pi,h,(s,a):p_{\\pi,h}(s,a)>0}w_{\\pi,h}(s,a)}}\\right)}\\\\ &{\\quad+\\mathcal{O}\\left(\\frac{B}{\\kappa b}d i m_{\\mathbb{T}}\\sqrt{\\log\\left(\\frac{K d i m_{\\mathbb{T}}}{\\delta}\\right)\\log\\left(\\frac{K\\left(1+2B\\rho_{w}\\right)}{\\delta}\\right)}\\frac{1}{m i n\\omega_{\\pi}(d)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We use $V_{1,r^{\\star},p}^{\\pi,h}\\left(s_{k,1}\\right)$ to denote that the first $h$ steps in the trajectory $\\xi$ is sampled using policy $\\pi$ from the MDP with transition $\\widehat{\\mathbf{P}}$ , and the state-action pairs from step $h+1$ up until the last step is sampled using policy $\\pi$ from the MDP with the true transition kernal $\\mathbf{P}^{\\star}$ . Therefore, ", "page_idx": 21}, {"type": "text", "text": "Here (a) is due to Lemma D.7 and D.8. (b) comes from that $p_{k h}^{C V a R,\\alpha,V^{\\pi^{k}}}(s,a)$ is defined as the probability of visiting $(s,a)$ at step of episode under the conditional transition probability $\\beta^{\\alpha,V_{h^{\\prime}+1}^{\\pi^{k}}}(\\cdot\\mid\\cdot,\\cdot)$ for each step $h^{\\prime}=1,\\ldots,h-1$ . ", "page_idx": 21}, {"type": "text", "text": "Firstly, we analyze the term $I_{1}$ and $I_{5}$ . Recall that for any policy $\\pi,h\\in[H]$ and $(s,a)\\,\\in\\,{\\mathcal{S}}\\,\\times$ $\\mathscr{A},w_{\\pi,h}(s,a)$ and $w_{\\pi,h}(s)$ denote th probabilities of visiting $(s,a)$ and $s$ at step $h$ under policy $\\pi$ , respectively. Thus, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\sum_{\\mathrm{c}=1}^{K}\\displaystyle\\sum_{h=1}^{H}\\sum_{(s,a)\\in\\mathcal{L}_{k}}p_{k h}^{G,\\alpha,V^{\\pi^{k}}}(s,a)L_{G}\\cdot H\\sqrt{\\frac{2S A\\log(\\frac{2K H S A}{\\delta})}{n_{k}\\left(s,a\\right)}}}&{\\displaystyle(77)}\\\\ {\\displaystyle\\overset{\\mathrm{(a)}}{\\le}L_{G}H\\sqrt{2S A\\log(\\frac{2K H S A}{\\delta})}\\sqrt{\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\sum_{(s,a)\\in\\mathcal{L}_{k}}\\frac{p_{k h}^{G,\\alpha,V^{\\pi^{k}}}\\left(s,a\\right)}{n_{k}(s,a)}}\\cdot\\sqrt{\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\sum_{(s,a)\\in\\mathcal{L}_{k}}p_{k h}^{G,\\alpha,V^{\\pi^{k}}}(s,a)}}&{\\displaystyle(9).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n=L_{G}H\\sqrt{2S A\\log(\\frac{2K H S A}{\\delta})}\\sqrt{\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\sum_{(s,a)\\in\\mathcal{L}_{k}}\\frac{p_{k h}^{G,\\alpha,V^{\\pi^{k}}}(s,a)}{n_{k}(s,a)}\\cdot\\mathcal{H}\\left\\{p_{k h}(s,a)\\neq0\\right\\}\\cdot\\sqrt{K H}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n=L_{G}H\\sqrt{2S A K H\\log(\\frac{2K H S A}{\\delta})}\\sqrt{\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\sum_{(s,a)\\in\\mathcal{L}_{k}}\\frac{p_{k h}^{G,\\alpha,V\\pi^{k}}(s,a)}{p_{k h}(s,a)}\\cdot\\frac{p_{k h}(s,a)}{n_{k}(s,a)}\\cdot\\mathbb{H}\\left\\{p_{k h}(s,a)\\neq0\\right\\}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\stackrel{\\left[b\\right]}{\\leq}L_{G}H\\sqrt{2S A K H\\log(\\frac{2K H S A}{\\delta})}\\sqrt{\\frac{1}{\\operatorname*{min}_{\\pi,h,(s,a):w_{\\pi,h}(s,a)>0}w_{\\pi,h}(s,a)}\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\sum_{(s,a)\\in\\mathcal{L}_{k}}\\frac{p_{k h}(s,a)}{n_{k}(s,a)}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\overset{(c)}{\\leq}L_{G}H\\sqrt{2S A K H\\log(\\frac{2K H S A}{\\delta})}\\cdot\\sqrt{\\frac{2S\\log\\left(\\frac{2K H S A}{\\delta}\\right)}{n_{k}(s,a)}}\\cdot\\frac{1}{\\sqrt{\\operatorname*{min}_{\\pi,h,(s,a):p_{\\pi,h}(s,a)>0}w_{\\pi,h}(s,a)}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\leq2L_{G}H^{\\frac{3}{2}}\\sqrt{K}S A\\log\\left(\\frac{2K H S A}{\\delta}\\right)\\cdot\\frac{1}{\\sqrt{\\operatorname*{min}_{\\pi,h,(s,a):p_{\\pi,h}(s,a)>0}w_{\\pi,h}(s,a)}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here (a) is due to Cauchy-Schwartz inequality and the fact that for any $k~>~0$ and $\\textit{h}\\in\\ [H]$ , $\\begin{array}{r}{\\sum_{(s,a)\\in S\\times A}p_{k h}^{\\mathrm{CVaR},\\alpha,V^{\\pi^{k}}}(s,a)\\ =\\ 1}\\end{array}$ . (b) comes from Lemma D.16. (c) uses Lemma D.12 and the fact that for any deterministic policy $\\pi,h\\;\\in\\;[H]$ and $(s,a)\\ \\in\\ S\\ \\times\\ A$ , we have either $w_{\\pi,h}(s,a)~=~w_{\\pi,h}(s)$ or $w_{\\pi,h}(s,a)~=~0$ , and thus $\\begin{array}{r l}{\\operatorname*{min}_{\\pi,h,(s,a):w_{\\pi,h}(s,a)>0}w_{\\pi,h}(s,a)\\ =}&{{}}\\end{array}$ $\\begin{array}{r}{\\operatorname*{min}_{\\pi,h,s:w_{\\pi,h}(s)>0}{w_{\\pi,h}(s)}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "For the term $I_{2}$ and $I_{6}$ , using Lemma D.14, we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\left(\\sum_{(s,a)\\notin\\mathcal{L}_{k}}p_{k,h}^{G,V_{r^{\\star},\\mathbf{P}^{\\star}}^{\\pi}}(s,a)H\\right)}\\\\ &{\\displaystyle\\le\\frac{1}{\\operatorname*{min}_{\\pi,h,s:w_{\\pi,h}(s)>0}w_{\\pi,h}(s)}\\left(8S A H^{2}\\log\\left(\\frac{H S A}{\\delta}\\right)+10S A H^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the term $I_{3}$ and $I_{7}$ , we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{l}{\\displaystyle\\sum_{k=1}^{K}\\sum_{d\\in\\mathcal{L}_{d i m,k}}p_{d i m,k}^{G,V_{r^{k},\\mathbf{p}^{\\star}}^{\\pi_{k}}}(d)\\left|(w_{r^{\\star}}-w_{\\hat{r}_{k}})B\\right|}\\\\ {\\displaystyle\\sum_{k=1}^{\\tau}\\sum_{d\\in\\mathcal{L}_{d i m,k}}p_{d i m,k}^{G,V_{r^{\\star},\\mathbf{p}^{\\star}}^{\\pi_{k}}}(d)\\sqrt{\\frac{\\displaystyle\\sum_{k=1}^{K}\\sum_{d\\in\\mathcal{L}_{d i m,k}}p_{d i m,k}^{G,V_{r^{\\star},\\mathbf{p}^{\\star}}}(d)}{\\displaystyle n_{d i m,k}(d)}}\\sqrt{n_{d i m,k}(d)\\sum_{k=1}^{K}\\sum_{d\\in\\mathcal{L}_{d i m,k}}|(w_{r^{\\star}}-w_{d i m,k}^{\\star})|}\\;.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\times k}{\\overset{!}{\\leq}}\\frac{B}{\\kappa b}\\sqrt{d i m_{\\mathbb{T}}\\log\\left(\\frac{K\\left(1+2B\\rho_{w}\\right)}{\\delta}\\right)}\\sqrt{\\frac{\\sum_{k=1}^{K}\\sum_{d\\in\\mathcal{L}_{d i m,k}}p_{d i m,k}^{G,V_{r^{k}}^{\\pi_{k}}}(d)}{n_{d i m,K}(d)}}}\\\\ &{\\underset{\\times k}{\\overset{}{\\leq}\\frac{B}{\\kappa b}\\sqrt{d i m_{\\mathbb{T}}\\log\\left(\\frac{K\\left(1+2B\\rho_{w}\\right)}{\\delta}\\right)}\\sqrt{\\frac{\\sum_{k=1}^{K}\\sum_{d\\in\\mathcal{L}_{d i m,k}}p_{d i m,k}^{G,V_{r^{k}}^{\\pi_{k}}}(d)}{p_{d i m,K}^{\\pi_{k}}(d)}\\cdot\\frac{p_{d i m,K}^{\\pi_{k}}(d)}{n_{d i m,K}(d)}\\mathbb{I}(p_{d i m,K}^{\\pi_{k}}(d)\\neq0)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\kappa\\rightarrow d}{\\leq}\\frac{2B}{\\kappa b}d i m_{\\mathbb{T}}\\sqrt{\\log\\left(\\frac{K d i m_{\\mathbb{T}}}{\\delta}\\right)\\log\\left(\\frac{K\\left(1+2B\\rho_{w}\\right)}{\\delta}\\right)}\\sqrt{\\frac{\\sum_{k=1}^{K}\\sum_{d\\in{\\mathcal{L}}_{d i m,k}}p_{d i m,k}^{G,V^{\\pi_{k}}\\mathbf{\\hat{P}}^{*}}\\left(d\\right)}{p_{d i m,K}^{\\pi_{k}}\\left(d\\right)}\\cdot\\mathbb{I}(p_{d i m,K}^{\\pi_{k}}(d))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\leq\\!\\frac{2B}{\\kappa b}d i m_{\\mathbb{T}}\\!\\sqrt{\\log\\left(\\frac{K d i m_{\\mathbb{T}}}{\\delta}\\right)\\log\\left(\\frac{K\\left(1+2B\\rho_{w}\\right)}{\\delta}\\right)}\\frac{1}{m i n\\omega_{\\pi}(d)}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here (a) is due to Cauchy-Schwartz inequality. (b) comes from Lemma D.17. (c) uses Lemma D.15. For the term $I_{4}$ and $I_{8}$ , using Lemma D.15, we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{K}\\left(\\sum_{(d\\notin\\mathcal{L}_{d i m,k}}p_{d i m,k}^{G,V_{r^{\\star},\\mathbf{P}^{\\star}}^{\\pi_{k}}}(d)B\\right)}\\\\ &{\\le\\displaystyle\\operatorname*{min}_{\\pi,d:p_{\\pi,d i m}(d)>0}p_{\\pi,d i m}(d)\\cdot\\left(4\\log\\left(\\frac{d i m_{\\mathbb{T}}}{\\delta}\\right)+5\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, summing all the term, we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{Reg}_{\\mathrm{nested}}\\left(K\\right)}\\\\ &{\\le\\!\\mathcal{O}\\left(L_{G}H^{\\frac32}\\sqrt{K}S A\\log\\left(\\frac{K H S A}{\\delta}\\right)\\cdot\\frac{1}{\\sqrt{\\operatorname*{min}_{\\pi,h,(s,a):p_{\\pi,h}(s,a)>0}w_{\\pi,h}(s,a)}}\\right)}\\\\ &{\\quad+\\mathcal{O}\\left(\\frac{B}{\\kappa b}d i m_{\\mathbb{T}}\\sqrt{\\log\\left(\\frac{K d i m_{\\mathbb{T}}}{\\delta}\\right)\\log\\left(\\frac{K\\left(1+2B\\rho_{w}\\right)}{\\delta}\\right)}\\frac{1}{\\operatorname*{min}_{\\pi,d}\\omega_{d i m,\\pi}(d)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "D.4 Static Regret ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma D.21. Static regret upper bound. Given a positive constant $\\delta\\in(0,1]$ , with probability at least $1-\\delta_{i}$ , we have the following inequality holds for every $k\\in[K]$ . ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\leq\\mathcal{O}\\left(L_{G}S^{2}A H^{\\frac{3}{2}}\\sqrt{K}l o g\\left(K/\\delta\\right)\\right)+}&{{}\\mathcal{O}\\left(L_{G}d i m_{\\mathbb{T}}\\sqrt{K\\log(K B\\rho_{w})\\log\\left(\\frac{K\\left(1+2B\\rho_{w}\\right)}{\\delta}\\right)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. We use $V_{1,r^{\\star},p}^{\\pi,h}\\left(s_{k,1}\\right)$ to denote that the first $h$ steps in the trajectory $\\xi$ is sampled using policy $\\pi$ from the MDP with transition $\\widehat{\\mathbf{P}}$ , and the state-action pairs from step $h+1$ up until the last step is sampled using policy $\\pi$ from the  MDP with the true transition kernal $\\mathbf{P}^{\\star}$ . Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Reg}_{s t a t i c}(K)=}\\\\ &{\\tilde{V}_{1,r}^{\\pi^{*}},\\mathbf{p}\\cdot(s_{k,1})-\\tilde{V}_{1,r}^{\\pi_{1}},\\mathbf{p}\\cdot(s_{k,1})-(\\tilde{V}_{1,r,1}^{\\pi^{*}},\\mathbf{p}_{1}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r,1}^{\\pi_{1}},\\mathbf{p}_{1}\\left(s_{k,1}\\right))}\\\\ &{\\le\\tilde{V}_{1,r}^{\\pi^{*}},\\mathbf{p}\\cdot(s_{k,1})-\\tilde{V}_{1,r}^{\\pi^{*}},\\mathbf{p}\\cdot(s_{k,1})-(\\tilde{V}_{1,r}^{\\pi^{*}},\\mathbf{p}_{1}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r}^{\\pi^{*}},\\mathbf{p}_{1}\\left(s_{k,1}\\right))}\\\\ &{+\\tilde{V}_{1,r}^{\\pi^{*}},\\mathbf{p}_{1}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r}^{\\pi^{*}},\\mathbf{p}_{1}\\left(s_{k,1}\\right)-(\\tilde{V}_{1,r,1}^{\\pi^{*}},\\mathbf{p}_{1}\\left(s_{k,1}\\right)-\\tilde{V}_{1,r,1}^{\\pi_{1}},\\mathbf{p}_{1}\\left(s_{k,1}\\right))}\\\\ &{\\le\\displaystyle\\sum_{k=1}^{K}E_{\\xi_{1}\\sim\\pi^{*},\\xi_{2}\\sim\\pi_{1}}(\\displaystyle\\sum_{\\mathbf{p}_{1},\\mathbf{p}_{2}\\in B_{k}^{\\pi}}(\\sum_{i=1}^{2}\\sum_{i=1}^{H}\\left|\\mathbf{p}_{1}\\left(s^{\\prime}\\mid s_{i,k,h},a_{i,k,h}\\right)-\\mathbf{P}_{2}\\left(s^{\\prime}\\mid s_{i,k,h},a_{i,k,h}\\right)\\right|))}&{(100)}\\\\ &{+\\displaystyle\\sum_{k=1}^{K}E_{\\xi_{1}\\sim\\pi^{*},\\xi_{2}\\sim\\pi_{1}}(\\displaystyle\\operatorname*{max}_{r_{1},r_{2}\\in B_{k}^{\\pi}}(\\sum_{i=1}^{2}\\sum_{h=1 \n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\leq\\mathcal O\\left(L_{G}S^{2}A H^{\\frac32}\\sqrt{K}l o g\\left(K/\\delta\\right)\\right)+\\mathcal O\\left(L_{G}d i m_{\\mathbb{T}}\\sqrt{K\\log(K B\\rho_{w})\\log\\left(\\frac{K\\left(1+2B\\rho_{w}\\right)}{\\delta}\\right)}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "image", "img_path": "JNDcFOczOf/tmp/125893e0bba92a0485a16b5ba2b8a5246816e3fce721d85b036b8bbd491e198a.jpg", "img_caption": ["Figure 5: Hard to learn case 1 "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "E.1 Regret Lower Bound of Nested Reward ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma E.1. Nested Regret Lower Bound. There exists an instance of Nested CVaR RL-RM, where the regret of any algorithm is at least: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(K)\\geq\\mathcal O\\left(\\operatorname*{min}\\left\\{B\\rho_{w}\\sqrt{\\frac{A K}{\\operatorname*{min}_{\\pi,h,s:p_{\\pi,h}(s)>0}w_{\\pi,h}(s,a)}},B\\sqrt{\\frac{A K}{\\operatorname*{min}_{\\pi,d}\\omega_{d i m,\\pi}(d)}}\\right\\}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Hard to learn case 1. Consider the instance shown in Figure 5.The state space is defined as ${\\cal S}\\ =\\ \\{s_{1},s_{2},\\ldots,s_{n},x_{1},x_{2},x_{3}\\}$ , where $s_{1}$ is the initial state, and $\\textit{n}=\\textit{S}-\\textit{3}<$ $S$ . We define the trajectory reward functions as follows: For any $\\xi_{H}\\;\\;\\in\\;\\;{\\mathcal Z}_{H},\\;\\phi(\\xi_{H})\\;\\;=$ $(\\mathbb{I}(S_{H}(\\xi_{H})=x_{1})B,\\mathbb{I}(\\dot{S}_{H}(\\xi_{H})=x_{2})B,\\mathbb{I}(S_{H}(\\xi_{H})=x_{3})B)$ , $w_{r}=(1\\rho,0.8\\rho,0.2\\rho)$ ", "page_idx": 24}, {"type": "text", "text": "The transition distributions are as follows. Let $\\mu$ be a parameter which satisfies that $\\begin{array}{r}{0<\\alpha<\\mu<\\frac{1}{3}}\\end{array}$ For any $a\\in{\\mathcal{A}},p\\left(s_{2}\\mid s_{1},a\\right)=\\mu,p\\left(x_{1}\\mid s_{1},a\\right)=1-3\\mu,p\\left(x_{2}\\mid s_{1},a\\right)=\\mu$ and $p\\left(x_{3}\\mid s_{1},a\\right)=\\breve{\\mu}$ . For any $i\\in\\{2,\\ldots,n-1\\}$ and $a\\in{\\mathcal{A}},p\\left(s_{i+1}\\mid s_{i},a\\right)=\\mu$ and $p\\left(x_{1}\\mid s_{i},a\\right)=1-\\mu.x_{1},x_{2}$ and $x_{3}$ are absorbing states, i.e., for any $a\\in{\\mathcal{A}},p\\left(x_{1}\\mid x_{1},a\\right)=1,p\\left(x_{2}\\mid x_{2},a\\right)=1$ and $p\\left(x_{3}\\mid x_{3},a\\right)=1$ . $p\\left(x_{2}\\mid s_{n},\\bar{a_{J}}\\right)=1-\\alpha+\\eta$ and $p\\left(x_{3}\\mid s_{n},a_{J}\\right)=\\alpha-\\eta$ , where $\\eta$ is a parameter which satisfies $0<\\eta<\\alpha$ and will be chosen later. For any suboptimal action $a\\in{\\cal A}\\backslash\\left\\{a_{J}\\right\\},p\\left(x_{2}\\mid s_{n},a\\right)=1-\\alpha$ and $p\\left(x_{3}\\mid s_{n},a\\right)=\\alpha$ For any $a_{j}\\in A$ , let $\\mathbb{E}_{j}\\left[\\cdot\\right]$ and $\\operatorname{Pr}_{j}[\\cdot]$ denote the expectation and probability operators under the instance with $a_{J}\\,=\\,a_{j}$ . Let $\\mathbb{E}_{\\mathrm{unif}}\\left[\\cdot\\right]$ and $\\mathrm{Pr}_{\\mathrm{unif}}\\left[\\cdot\\right]$ denote the expectation and probability operators under the uniform instance. ", "page_idx": 24}, {"type": "text", "text": "Fix an algorithm $\\boldsymbol{\\mathcal{A}}$ . Let $\\pi^{k}$ denote the policy taken by algorithm $\\boldsymbol{\\mathcal{A}}$ in episode $k$ . Let $N_{s_{n},a_{j}}=$ $\\begin{array}{r}{\\sum_{k=1}^{K}\\mathcal{H}\\left\\{\\pi^{k}\\left(s_{n}\\right)=a_{j}\\right\\}}\\end{array}$ denote the number of episodes that the policy chooses $a_{j}$ in state $s_{n}$ . Let $V_{s_{n},a_{j}}$ denote the number of episodes that the algorithm $\\boldsymbol{\\mathcal{A}}$ visits $(s_{n},a_{j})$ . Let $w\\left(s_{n}\\right)$ denote the probability of visiting $s_{n}$ in an episode (the probability of visiting $s_{n}$ is the same for all policies). Then, it holds that $\\mathbb{E}\\left[V_{s_{n}},a_{j}\\right]=\\dot{\\boldsymbol{w}}\\left(s_{n}\\right)\\cdot\\mathbb{E}\\left[\\dot{N}_{s_{n},a_{j}}\\right]$ . ", "page_idx": 24}, {"type": "text", "text": "According to the definition of nested-CVaR-PbRL risk objective in Eq. 4, we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\nV_{1}^{*}\\left(s_{1}\\right)=\\frac{\\left(\\alpha-\\eta\\right)\\cdot0.2+\\eta\\cdot0.8}{\\alpha}B\\rho,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and summing over all episodes $k\\in[K]$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\mathbb{E}_{j}[\\mathcal{R}(K)]=\\sum_{k=1}^{K}{\\left(2V_{1}^{*}\\left(s_{1}\\right)-V_{1}^{\\pi_{1,k}}\\left(s_{1}\\right)-V_{1}^{\\pi_{2,k}}\\left(s_{1}\\right)\\right)}}\\\\ {\\displaystyle}&{\\displaystyle=\\frac{1}{A}\\sum_{j=1}^{A}{\\frac{\\eta B\\rho}{\\alpha}\\cdot0.6\\left(2K-\\mathbb{E}_{j}\\left[N_{s_{n},a_{j}}\\right]\\right)}}\\\\ {\\displaystyle}&{\\displaystyle=0.6\\cdot\\frac{\\eta B\\rho}{\\alpha}\\cdot\\left(K-\\frac{1}{A}\\sum_{j=1}^{A}\\mathbb{E}_{j}\\left[N_{s_{n},a_{j}}\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}[\\mathcal{R}(K)]=\\frac{1}{A}\\sum_{j=1}^{A}\\sum_{k=1}^{K}\\left(V_{1}^{*}\\left(s_{1}\\right)-V_{1}^{\\pi^{k}}\\left(s_{1}\\right)\\right)}\\\\ {\\displaystyle=\\frac{1}{A}\\sum_{j=1}^{A}\\frac{\\eta}{\\alpha}\\cdot0.6B\\rho\\left(K-\\mathbb{E}_{j}\\left[N_{s_{n},a_{j}}\\right]\\right)}\\\\ {\\displaystyle=0.6B\\rho\\cdot\\frac{\\eta}{\\alpha}\\cdot\\left(K-\\frac{1}{A}\\sum_{j=1}^{A}\\mathbb{E}_{j}\\left[N_{s_{n},a_{j}}\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For any $\\begin{array}{r l r}{j}&{{}\\in}&{\\{A,B\\}}\\end{array}$ , using Pinsker\u2019s inequality and $\\begin{array}{r l r l r l}{{0}}&{{}<}&{\\alpha}&{{}<}&{\\frac{1}{3}}\\end{array}$ 31, we have that $\\begin{array}{r}{\\mathrm{{SL}}\\left(p_{\\mathrm{unif}}\\,\\left(s_{n},\\pi_{j}(s_{n})\\right)\\|p_{j}\\left(s_{n},\\pi_{j}(s_{n})\\right)\\right)\\,=\\,\\mathrm{KL}(\\mathrm{Ber}(\\alpha)\\|\\,\\mathrm{Ber}(\\alpha-\\eta))\\,\\le\\,\\frac{\\eta^{2}}{(\\alpha-\\eta)(1-\\alpha+\\eta)}\\,\\le\\,\\frac{c_{1}\\eta^{2}}{\\alpha}}\\end{array}$ for some constant $c_{1}$ and small enough $\\eta$ . Then, using Lemma A. 1 in Du et al. (2022), we have , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{j}\\left[N_{\\pi_{j}}\\right]\\le\\mathbb{E}_{\\mathrm{unif}}\\,\\left[N_{\\pi_{j}}\\right]+\\displaystyle\\frac{K}{2}\\sqrt{\\mathbb{E}_{\\mathrm{unif}}\\,\\left[V_{\\pi_{j}}\\right]\\cdot\\mathrm{KL}\\left(\\pi_{j}(s_{n})\\right)\\left\\|p_{j}\\left(s_{n},\\pi_{j}(s_{n})\\right)\\right.}}\\\\ &{\\qquad\\qquad\\le\\mathbb{E}_{\\mathrm{unif}}\\,\\left[N_{\\pi_{j}}\\right]+\\displaystyle\\frac{K}{2}\\sqrt{w\\left(s_{n}\\right)\\cdot\\mathbb{E}_{\\mathrm{unif}}\\,\\left[N_{\\pi_{j}}\\right]\\cdot\\frac{c_{1}\\eta^{2}}{\\alpha}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then, Using $\\begin{array}{r}{\\sum_{j=1}^{A}\\mathbb{E}_{u n i f}\\left[N_{s_{n},a_{j}}\\right]=2K}\\end{array}$ and Cauchy\u2013Schwarz inequality, we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{A}\\sum_{j=1}^{A}\\mathbb{E}_{j}\\left[N_{s_{n},a_{j}}\\right]\\leq\\displaystyle\\frac{1}{A}\\sum_{j=1}^{A}\\mathbb{E}_{u n i f}\\left[N_{s_{n},a_{j}}\\right]+\\frac{K\\eta}{2A}\\sum_{j=1}^{A}\\sqrt{\\frac{c_{1}}{\\alpha}\\cdot w\\left(s_{n}\\right)\\cdot\\mathbb{E}_{u n i f}\\left[N_{s_{n},a_{j}}\\right]}}\\\\ &{\\displaystyle\\leq\\frac{1}{A}\\sum_{j=1}^{A}\\mathbb{E}_{u n i f}\\left[N_{s_{n},a_{j}}\\right]+\\frac{K\\eta}{2A}\\sqrt{A\\sum_{j=1}^{A}\\frac{c_{1}}{\\alpha}\\cdot w\\left(s_{n}\\right)\\cdot\\mathbb{E}_{u n i f}\\left[N_{s_{n},a_{j}}\\right]}}\\\\ &{\\displaystyle\\leq\\frac{K}{A}+\\frac{K\\eta}{2}\\sqrt{\\frac{c_{1}\\cdot w\\left(s_{n}\\right)K}{\\alpha A}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, we have : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathcal{R}(K)]\\ge0.6B\\rho\\cdot\\frac{\\eta}{\\alpha}\\cdot\\left(K-\\frac{K}{A}-\\frac{K\\eta}{2}\\sqrt{\\frac{c_{1}\\cdot w\\left(s_{n}\\right)K}{\\alpha A}}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$\\begin{array}{r}{\\eta=c_{2}\\sqrt{\\frac{\\alpha A}{w(s_{n})K}}}\\end{array}$ for a small enough constant $c_{2}$ . We have ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\mathbb{E}[{\\mathcal{R}}(K)]=\\Omega\\left(B\\rho{\\sqrt{\\frac{A}{\\alpha\\cdot w\\left(s_{n}\\right)K}}}\\cdot K\\right)}\\\\ &{\\qquad\\qquad=\\Omega\\left(B\\rho{\\sqrt{\\frac{A K}{\\alpha\\cdot w\\left(s_{n}\\right)}}}\\right)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hard to learn case 2. ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The trajectory reward functions are as follows. For any $\\begin{array}{r l r}{\\xi_{H}}&{{}\\in}&{\\mathcal{Z}_{H}}\\end{array}$ , \u03d5(\u03beH) = $(\\mathbb{I}(S_{H}(\\xi_{H})=\\dot{x_{1}})B,\\mathbb{I}(S_{H}(\\xi_{H})=x_{2})B,\\mathbb{I}(S_{H}(\\xi_{H})=x_{3})B,\\mathbb{I}(S_{H}(\\xi_{H})=x_{4})B),$ wr = $(1\\rho,0.8\\rho,0.2\\rho,(0.2-\\eta)\\rho)$ ", "page_idx": 25}, {"type": "text", "text": "The transition distributions are as follows. Let $\\mu$ be a parameter which satisfies that $\\begin{array}{r}{0<\\alpha<\\mu<\\frac{1}{3}}\\end{array}$ For any $a\\in{\\mathcal{A}},p\\left(s_{2}\\mid s_{1},a\\right)=\\mu,p\\left(x_{1}\\mid s_{1},a\\right)=1-3\\mu,p\\left(x_{2}\\mid s_{1},a\\right)=\\mu$ and $p\\left(x_{3}\\mid s_{1},a\\right)=\\stackrel{\\circ}{\\mu}$ . For any $i\\in\\{2,\\ldots,n-1\\}$ and $a\\in{\\mathcal{A}},p\\left(s_{i+1}\\mid s_{i},a\\right)=\\mu$ and $p\\left(x_{1}\\mid s_{i},a\\right)=1-\\mu.x_{1},x_{2}$ and $x_{3}$ are absorbing states, i.e., for any $a\\in{\\mathcal{A}},p\\left(x_{1}\\mid x_{1},a\\right)=1,p\\left(x_{2}\\mid x_{2},a\\right)=1$ and $p\\left(x_{3}\\mid x_{3},a\\right)=1$ . ", "page_idx": 25}, {"type": "image", "img_path": "JNDcFOczOf/tmp/ca2c5a5289fd325e5e04e5281db772f4665a443ff1c76c9f8c345cef98c3d5c8.jpg", "img_caption": ["Figure 6: Hard to learn case 2 "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "$p\\left(x_{2}\\mid s_{n},a_{J}\\right)=1-\\alpha$ and $p\\left(x_{3}\\mid s_{n},a_{J}\\right)=\\alpha$ , where $\\eta$ is a parameter which satisfies $0<\\eta<\\alpha$ and will be chosen later. For any suboptimal action $a\\,\\in\\,{\\mathcal{A}}\\backslash\\left\\{a_{J}\\right\\},p\\left(x_{2}\\mid s_{n},a\\right)\\,=\\,1\\,-\\,\\alpha$ and $p\\left(x_{4}\\mid s_{n},a\\right)\\,=\\,\\alpha$ For any $a_{j}\\ \\in\\ A$ , let $\\mathbb{E}_{j}[\\cdot]$ and $\\operatorname{Pr}_{j}[\\cdot]$ denote the expectation and probability operators under the instance with $a_{J}\\,=\\,a_{j}$ . Let $\\mathbb{E}_{\\mathrm{unif}}\\left[\\cdot\\right]$ and $\\mathrm{Pr}_{\\mathrm{unif}}\\left[\\cdot\\right]$ denote the expectation and probability operators under the uniform instance. ", "page_idx": 26}, {"type": "text", "text": "According to the definition of nested-CVaR-PbRL risk objective in 4, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\nV_{1}^{*}\\left(s_{1}\\right)=0.2\\eta B\\rho,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\mathbb{E}_{j}[\\mathcal{R}(K)]=\\sum_{k=1}^{K}{\\left(2V_{1}^{*}\\left(s_{1}\\right)-V_{1}^{\\pi_{1,k}}\\left(s_{1}\\right)-V_{1}^{\\pi_{2,k}}\\left(s_{1}\\right)\\right)}}\\\\ {\\displaystyle}&{\\displaystyle=\\frac{1}{A}\\sum_{j=1}^{A}{0.2\\eta B\\rho\\left(2K-\\mathbb{E}_{j}\\left[N_{s_{n},a_{j}}\\right]\\right)}}\\\\ {\\displaystyle}&{\\displaystyle=0.2\\eta B\\rho\\cdot\\left(K-\\frac{1}{A}\\sum_{j=1}^{A}\\mathbb{E}_{j}\\left[N_{s_{n},a_{j}}\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbb{E}[\\mathcal{R}(K)]=\\frac{1}{A}\\sum_{j=1}^{A}{\\sum_{k=1}^{K}{\\left(V_{1}^{*}\\left(s_{1}\\right)-V_{1}^{\\pi^{k}}\\left(s_{1}\\right)\\right)}}}\\\\ {\\displaystyle=\\frac{1}{A}\\sum_{j=1}^{A}{0.2\\eta B\\rho\\rho\\left(K-\\mathbb{E}_{j}\\left[N_{s_{n},a_{j}}\\right]\\right)}}\\\\ {\\displaystyle=0.2\\eta B\\rho\\cdot\\frac{\\eta}{\\alpha}\\cdot\\left(K-\\frac{1}{A}\\sum_{j=1}^{A}{\\mathbb E}_{j}\\left[N_{s_{n},a_{j}}\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For any $j\\in\\{A,B\\}$ , since the preference based on Bernoulli distribution, using Pinsker\u2019s inequality and $0\\,<\\,\\alpha\\,<\\,{\\frac{1}{3}}$ , we have that $\\mathrm{KL}\\left(p_{\\mathrm{unif}}\\,\\left(s_{n},\\pi_{j}(s_{n})\\right)\\|p_{j}\\left(s_{n},\\pi_{j}(s_{n})\\right)\\right)=\\bar{\\mathrm{KL}}(\\mathrm{Ber}(\\alpha)\\|\\,\\mathrm{Ber}(\\alpha-j))$ \u03b7)) \u2264(0.2\u2212\u03b7)(0.8+\u03b7) \u2264 01.16 for some constant $c_{1}$ and small enough $\\eta$ . Then, we have , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{j}\\left[N_{\\pi_{j}}\\right]\\le\\mathbb{E}_{\\mathrm{unif}}\\,\\left[N_{\\pi_{j}}\\right]+\\displaystyle\\frac{K}{2}\\sqrt{\\mathbb{E}_{\\mathrm{unif}}\\,\\left[V_{\\pi_{j}}\\right]\\cdot\\mathrm{KL}\\left(\\pi_{j}(s_{n})\\right)\\left\\lVert p_{j}\\left(s_{n},\\pi_{j}(s_{n})\\right)\\right\\rVert}}\\\\ &{\\phantom{\\le}\\le\\mathbb{E}_{\\mathrm{unif}}\\,\\left[N_{\\pi_{j}}\\right]+\\displaystyle\\frac{K}{2}\\sqrt{w\\left(s_{n}\\right)\\cdot\\mathbb{E}_{\\mathrm{unif}}\\,\\left[N_{\\pi_{j}}\\right]\\cdot\\frac{c_{1}\\eta^{2}}{0.16}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, Using $\\begin{array}{r}{\\sum_{j=1}^{A}\\mathbb{E}_{u n i f}\\left[N_{s_{n},a_{j}}\\right]=2K}\\end{array}$ and Cauchy\u2013Schwarz inequality, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\frac{1}{A}\\sum_{j=1}^{A}\\mathbb{E}_{j}\\left[N_{s_{n},a_{j}}\\right]\\leq\\displaystyle\\frac{1}{A}\\sum_{j=1}^{A}\\mathbb{E}_{u n i f}\\left[N_{s_{n},a_{j}}\\right]+\\frac{K\\eta}{2A}\\sum_{j=1}^{A}\\sqrt{\\frac{c_{1}}{0.16}\\cdot w\\left(d\\right)\\cdot\\mathbb{E}_{u n i f}\\left[N_{s_{n},a_{j}}\\right]}}\\\\ {\\displaystyle}&{\\leq\\displaystyle\\frac{1}{A}\\sum_{j=1}^{A}\\mathbb{E}_{u n i f}\\left[N_{s_{n},a_{j}}\\right]+\\frac{K\\eta}{2A}\\sqrt{A\\sum_{j=1}^{A}\\frac{c_{1}}{0.16}\\cdot w\\left(d\\right)\\cdot\\mathbb{E}_{u n i f}\\left[N_{s_{n},a_{j}}\\right]}}\\\\ {\\displaystyle}&{\\leq\\displaystyle\\frac{K}{A}+\\frac{K\\eta}{2}\\sqrt{\\frac{c_{1}\\cdot w\\left(d\\right)K}{0.16A}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, we have : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathcal{R}(K)]\\ge0.2B\\rho\\cdot\\eta\\cdot\\left(K-\\frac{K}{A}-\\frac{K\\eta}{2}\\sqrt{\\frac{c_{1}\\cdot w\\left(d\\right)K}{0.16A}}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "$\\begin{array}{r}{\\eta=c_{2}\\sqrt{\\frac{0.16A}{w(d)K}}}\\end{array}$ for a small enough constant $c_{2}$ . We have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathcal{R}(K)]=\\Omega\\left(B\\rho\\sqrt{\\frac{A}{w\\left(d\\right)K}}\\cdot K\\right)}\\\\ &{\\qquad\\qquad=\\Omega\\left(B\\rho\\sqrt{\\frac{A K}{w\\left(d\\right)}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "F The optimal policy calculation for known PbRL MDP ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we describe how to compute the optimal policy for the CVaR objective when the PbRL-MDP is known; this approach is described in detail in B\u00e4uerle $\\&$ Ott (2011); Bastani et al. (2022). Following this work, we consider the setting where we are trying to minimize cost rather than maximize reward. In particular, consider an know PbRL MDP $\\bar{\\mathcal{M}}(\\bar{\\mathcal{S}},\\mathcal{A},r_{\\xi}^{\\star},\\mathbf{P}^{\\star},H)$ , where $\\boldsymbol{S}$ and $\\boldsymbol{\\mathcal{A}}$ , and our goal is to compute a policy $\\pi$ that maximizes its CVaR objective. ", "page_idx": 27}, {"type": "text", "text": "Lemma F.1. CVaR definition. For any random variable $Z,$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname{CVaR}_{\\alpha}(Z)=\\operatorname*{sup}_{\\rho\\in\\mathbb{R}}\\left\\{\\rho-\\frac{1}{\\alpha}\\cdot\\mathbb{E}_{Z}\\left[(\\rho-Z)^{+}\\right]\\right\\},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the minimum is achieved by $\\rho^{*}=\\mathrm{VaR}_{\\alpha}(Z)$ . ", "page_idx": 27}, {"type": "text", "text": "F.1 Static CVaR object ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Since the optimal policy for static CVaR object satisfies: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\pi^{\\star}\\in\\mathrm{argmax}_{\\pi\\in\\Pi}\\,\\mathrm{CVaR}(Z(\\pi))\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "As a consequence of Lemma F.1, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{CVaR}\\left(Z^{(\\pi^{*})}\\right)=\\underset{\\pi\\in\\Pi}{\\mathrm{max}}\\,\\mathrm{CVaR}\\left(Z^{(\\pi)}\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\underset{\\pi\\in\\Pi}{\\mathrm{max}}\\,\\underset{\\rho\\in\\mathbb{R}}{\\mathrm{sup}}\\left\\lbrace\\rho-\\frac{1}{\\alpha}\\cdot\\mathbb{E}_{Z}(\\pi)\\left[\\left(\\rho-Z^{(\\pi)}\\right)^{+}\\right]\\right\\rbrace}\\\\ &{\\qquad\\qquad\\qquad=\\underset{\\rho\\in\\mathbb{R}}{\\mathrm{sup}}\\left\\lbrace\\rho-\\frac{1}{\\alpha}\\cdot\\underset{\\pi\\in\\Pi}{\\mathrm{max}}\\,\\mathbb{E}_{Z^{(\\pi)}}\\left[\\left(\\rho-Z^{(\\pi)}\\right)^{+}\\right]\\right\\rbrace.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, the optimal policy is: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\pi^{\\star}=\\arg\\operatorname*{max}_{\\pi\\in\\Pi}\\mathbb{E}_{Z^{(\\pi)}}\\left[\\left(\\rho^{\\star}-Z^{(\\pi)}\\right)^{+}\\right]\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\rho^{*}=\\operatorname*{arg\\,sup}_{\\rho\\in\\mathbb{R}}J(\\rho)\\quad\\mathrm{~where~}\\quad J(\\rho)=\\rho-\\frac{1}{\\alpha}\\cdot\\operatorname*{max}_{\\pi\\in\\Pi}\\mathbb{E}_{Z}(\\pi)\\left[\\left(\\rho-Z^{(\\pi)}\\right)^{+}\\right].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Value iteration. we reconstruct the MDP as enlarge the state space $\\tilde{s}_{h}=(\\xi_{h},\\rho)$ , where $\\rho$ will work as a quantile value. Letting $S_{1}$ be the initial state of the original PbRL MDP $\\mathcal{M}$ . ", "page_idx": 28}, {"type": "text", "text": "We iterate the value and calculate the policy as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{V}_{H}((\\xi_{H},\\rho))=\\operatorname*{max}\\{\\rho-r_{\\xi}^{\\star}(\\xi_{H}),0\\}}\\\\ &{\\widetilde{V}_{h}((\\xi_{h},\\rho))=\\underset{a\\in A}{\\operatorname*{max}}\\int\\widetilde{V}_{h+1}\\left((s^{\\prime}\\circ(\\xi_{h},a),\\rho)\\right)\\cdot\\mathbf{P}^{\\star}\\left(s^{\\prime}\\mid(S_{h}(\\xi_{h}),a)\\right)}\\\\ &{\\quad\\quad\\pi(\\xi_{h})=a r g m a x_{a\\in A}\\int\\widetilde{V}_{h+1}\\left((s^{\\prime}\\circ(\\xi_{h},a),\\rho)\\right)\\cdot\\mathbf{P}^{\\star}\\left(s^{\\prime}\\mid(S_{h}(\\xi_{h}),a)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then, given an initial state $s_{1}$ , we construct state $\\mathbb{S}_{1}=(s_{1},-\\rho^{*})$ , where ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\rho^{*}=a r g s u p\\left\\{\\rho-\\frac{1}{\\alpha}\\cdot\\widetilde{V}_{1}^{(\\pi)}((s_{1},-\\rho))\\right\\},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and then acting optimally in $\\mathcal{M}$ . ", "page_idx": 28}, {"type": "text", "text": "F.2 Nested CVaR object ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "According to Eq. 4, nested CVaR object could directly use the Bellman equation to iterate the value. Value iteration. we reconstruct the MDP as enlarge the state space $\\tilde{s}_{h}=(\\xi_{h},\\rho)$ , where $\\rho$ will work as a quantile value. Letting $S_{1}$ be the initial state of the original PbRL MDP $\\mathcal{M}$ . ", "page_idx": 28}, {"type": "text", "text": "We iterate the value and calculate the policy as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{V}_{H}((\\xi_{H},\\rho))=r_{\\xi}^{\\star}(\\xi_{H})}\\\\ &{\\widetilde{V}_{h}((\\xi_{h},\\rho))=\\displaystyle\\operatorname*{max}_{\\pi\\in\\Pi}\\,\\operatorname*{sup}_{\\rho\\in\\mathbb{R}}\\left\\lbrace\\rho-\\frac{1}{\\alpha}\\cdot\\mathbb{E}_{\\widetilde{V}_{h+1}((s^{\\prime}\\circ(\\xi_{h},a),\\rho))}\\left[\\left(\\rho-\\widetilde{V}_{h+1}\\left((s^{\\prime}\\circ(\\xi_{h},a),\\rho)\\right)\\right)^{+}\\right]\\right\\rbrace\\quad\\mathrm{(144)}}\\\\ &{\\quad\\quad\\pi(\\xi_{h})=\\arg\\displaystyle\\operatorname*{max}_{\\pi\\in\\Pi}\\,\\operatorname*{sup}_{\\rho\\in\\mathbb{R}}\\left\\lbrace\\rho-\\frac{1}{\\alpha}\\cdot\\mathbb{E}_{\\widetilde{V}_{h+1}((s^{\\prime}\\circ(\\xi_{h},a),\\rho))}\\left[\\left(\\rho-\\widetilde{V}_{h+1}\\left((s^{\\prime}\\circ(\\xi_{h},a),\\rho)\\right)\\right)^{+}\\right]\\right\\rbrace}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "then acting optimally in $\\mathcal{M}$ . ", "page_idx": 28}, {"type": "text", "text": "G AUXILIARY LEMMAS ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Definition G.1. $\\alpha$ -dependence in Russo & Van Roy (2013). For $\\alpha>0$ and function class $\\mathcal{Z}$ whose elements are with domain $\\mathcal{X}$ , an element $x\\in\\mathscr{X}$ is $\\alpha$ -dependent on the set ${\\mathcal{X}}_{n}:=\\{x_{1},x_{2},\\cdot\\cdot\\cdot,x_{n}\\}\\subset$ $\\mathcal{X}$ with respect to $\\mathcal{Z}$ , if any pair of functions $z,z^{\\prime}\\in\\mathcal{Z}$ with $\\begin{array}{r}{\\sqrt{\\sum_{i=1}^{n}\\left(z\\left(x_{i}\\right)-z^{\\prime}\\left(x_{i}\\right)\\right)^{2}}\\leqslant\\alpha}\\end{array}$ satisfies $z(x)-z^{\\prime}(x)\\leqslant\\alpha$ . Otherwise, $x$ is $\\alpha$ -independent on ${\\mathcal{X}}_{n}$ if it does not satisfy the condition. ", "page_idx": 28}, {"type": "text", "text": "Definition G.2. Eluder dimension in Russo $\\&$ Van Roy (2013). For $\\alpha>0$ and function class $\\mathcal{Z}$ whose elements are with domain $\\mathcal{X}$ , the Eluder dimension $\\dim_{E}({\\mathcal{Z}},\\alpha)$ , is defined as the length of the longest possible sequence of elements in $\\mathcal{X}$ such that for some $\\alpha^{\\prime}\\geqslant\\alpha$ , every element is $\\alpha^{\\prime}$ independent of its predecessors. ", "page_idx": 28}, {"type": "text", "text": "Definition G.3. Covering number Given two functions $l$ and $u$ , the bracket $[l,u]$ is the set of all functions $f$ satisfying $l\\leq f\\leq u$ . An $\\alpha$ -bracket is a bracket $[l,u]$ with $\\|u-l\\|<\\alpha$ . The covering number $N_{[\\cdot]}({\\mathcal{F}},\\alpha,\\parallel\\cdot\\parallel)$ is the minimum number of $\\alpha$ -brackets needed to cover $\\mathcal{F}$ . ", "page_idx": 28}, {"type": "text", "text": "Lemma G.4. (Linear Preference Models Eluder dimension and Covering number). For the case of $d$ -dimensional generalized trajectory linear feature models $r_{\\xi}\\left(\\xi_{H}\\right):=\\mathbf{\\bar{\\langle}}\\phi\\left(\\xi_{H}\\right),\\mathbf{w}_{r}\\rangle$ , where $\\phi:$ $T r a j\\rightarrow\\mathbb{R}^{d i m_{\\mathbb{T}}}$ is a known dimT dimension feature map satisfying $\\|\\psi\\left(\\xi_{H}\\right)\\|_{2}\\,\\leq\\,B$ and $\\theta\\in\\mathbb{R}^{d}$ is an unknown parameter with $\\|\\mathbf{w}_{r}\\|_{2}\\leq\\rho_{w}$ . Then the $\\alpha$ -Eluder dimension of $r_{\\xi}(\\xi_{H})$ is at most $\\mathcal{O}(d i m_{\\mathbb{T}}\\log(B\\rho_{w}/\\alpha))$ . The $\\alpha$ - covering number is upper bounded by $\\left(\\frac{1{+}2B\\rho_{w}}{\\alpha}\\right)^{d i m_{\\mathbb{T}}}$ ", "page_idx": 29}, {"type": "text", "text": "Let (Xp, Yp)p=1,2,... be a sequence of random elements, $X_{p}\\in X$ for some measurable set $X$ and $Y_{p}\\,\\in\\,\\mathbb{R}$ . Let $\\mathcal{F}$ be a subset of the set of real-valued measurable functions with domain $X$ . Let $\\mathbb{F}^{*}=\\left(\\mathbb{F}_{p}\\right)_{p=0,1,\\cdots}$ be a filtration such that for all $p\\,\\geq\\,1,(X_{1},Y_{1},\\cdot\\cdot\\cdot\\,,X_{p-1},Y_{p-1},X_{p})$ is $\\mathbb{F}_{p-1}$ measurable and such that there exists some function $f_{\\star}\\,\\in\\,{\\mathcal{F}}$ such that $\\mathbb{E}\\left[Y_{p}\\mid\\mathbb{F}_{p-1}\\right]\\,=\\,f_{*}\\left(X_{p}\\right)$ holds for all $p\\,\\geq\\,1$ . The (nonlinear) least square predictor given $\\left(X_{1},Y_{1},\\cdot\\cdot\\cdot\\right),X_{t},Y_{t})$ is $\\hat{f}_{t}\\,=$ $\\begin{array}{r}{\\operatorname*{argmin}_{f\\in\\mathcal{F}}\\sum_{p=1}^{t}\\left(f\\left(X_{p}\\right)-Y_{p}\\right)^{2}}\\end{array}$ . We say that $Z$ is conditionally $\\rho$ -subgaussion given the $\\sigma$ -algebra $\\mathbb{F}$ is for all $\\begin{array}{r}{\\lambda\\in\\mathbb{R},\\log\\mathbb{E}[\\exp(\\lambda Z)\\mid\\mathbb{F}]\\le\\frac{1}{2}\\lambda^{2}\\rho^{2}}\\end{array}$ . For $\\alpha>0$ , let $N_{\\alpha}$ be the $\\|\\cdot\\|_{\\infty}$ -covering number of $\\mathcal{F}$ at scale $\\alpha$ . For $\\beta>0$ , define ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\mathcal{F}}_{t}(\\beta)=\\left\\{f\\in{\\mathcal{F}}:\\sum_{p=1}^{t}\\left(f\\left(X_{p}\\right)-\\hat{f}_{t}\\left(X_{p}\\right)\\right)^{2}\\leq\\beta\\right\\}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma G.5. (Theorem 5 of Ayoub et al. (2020)). Let $\\mathbb{F}$ be the filtration defined above and assume that the functions in $\\mathcal{F}$ are bounded by the positive constant $C>0$ . Assume that for each $s\\ \\geq$ $1,(Y_{p}-f_{*}\\left(X_{p}\\right))$ is conditionally $\\sigma$ -subgaussian given $\\mathbb{F}_{p-1}$ . Then, for any $\\alpha>0$ , with probability $1-\\delta$ , for all $\\bar{t}\\ge1,f_{\\ast}\\in\\mathcal{F}_{t}\\left(\\beta_{t}(\\delta,\\alpha)\\right)$ , where ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\beta_{t}(\\delta,\\alpha)=8\\sigma^{2}\\log{(2N_{\\alpha}/\\delta)}+4t\\alpha\\left(C+\\sqrt{\\sigma^{2}\\log(4t(t+1)/\\delta)}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma G.6. (Lemma $^{5}$ of Russo & Van Roy (2013)). Let $\\mathcal{F}\\in\\mathcal{B}_{\\infty}(X,C)$ be a set of functions bounded by $C\\,>\\,0,$ , $\\left(\\mathcal{F}_{t}\\right)_{t\\geq1}$ and $\\left(\\boldsymbol{x}_{t}\\right)_{t\\geq1}$ be sequences such that $\\mathcal{F}_{t}\\subset\\mathcal{F}$ and $x_{t}\\,\\in\\,\\mathcal{X}$ hold for $t\\,\\geq\\,1$ . Let $\\mathcal{F}\\big|_{x_{1:t}}\\;=\\;\\{\\big(\\bar{f}\\,(x_{1})\\,,\\dots,\\bar{f}\\,(x_{t})\\big):f\\in\\mathcal{F}\\}\\,(\\subset\\mathbb{R}^{t})$ and for $S\\,\\subset\\,\\mathbb{R}^{t}$ , let $\\dim(S)\\,=$ $\\operatorname*{sup}_{u,v\\in S}\\|u-v\\|_{2}$ be the diameter of $\\because S.$ Then, for any $T\\geq1$ and $\\alpha>0\\:i t$ , holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathrm{diam}\\left(\\mathcal{F}_{t}\\big|_{x_{t}}\\right)\\leq\\alpha+C(d\\wedge T)+2\\delta_{T}\\sqrt{d T},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\delta_{T}=\\mathrm{max}_{1\\leq t\\leq T}$ diam $\\left(\\left.\\mathcal{F}_{t}\\right|_{x_{1:t}}\\right)$ and $d=\\dim_{\\mathcal{E}}(\\mathcal{F},\\alpha)$ . ", "page_idx": 29}, {"type": "text", "text": "Lemma G.7. If $(\\beta_{t}\\geq0\\mid t\\in\\mathbb{N})$ is $a$ nondecreasing sequence and $\\begin{array}{r l r l}{\\mathcal{F}_{t}}&{{}}&{:=}&{{}}\\end{array}$ $\\begin{array}{r l r}{\\bigg\\{f\\in\\mathcal{F}:\\bigg\\|f-\\widehat{f}_{t}^{L S}\\bigg\\|_{2,E_{t}}\\leq\\sqrt{\\beta_{t}}\\bigg\\},}&{{}w h e r e}&{\\widehat{f}_{t}^{L S}}&{\\in}&{\\mathrm{~arg~min}_{f\\in\\mathcal{F}}\\:L_{2,t}(f)}\\end{array}$ and $L_{2,t}(f)$ $\\begin{array}{r}{\\sum_{1}^{t-1}\\left(f\\left(A_{t}\\right)-R_{t}\\right)^{2}}\\end{array}$ , then for all $T\\in\\mathbb N$ and $\\epsilon>0$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbf{1}\\left(w_{\\mathcal{F}_{t}}\\left(A_{t}\\right)>\\epsilon\\right)\\leq\\left(\\frac{4\\beta_{T}}{\\epsilon^{2}}+1\\right)\\dim_{E}(\\mathcal{F},\\epsilon)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\begin{array}{r}{w_{\\mathcal{F}}(a):=\\operatorname*{sup}_{f\\in\\mathcal{F}}f(a)-\\operatorname*{inf}_{f\\in\\mathcal{F}}f(a)}\\end{array}$ denotes confidence interval widths. ", "page_idx": 29}, {"type": "text", "text": "Theorem G.8. Hoeffding $\\mathbf{\\hat{s}}$ inequality(Hoeffding, 1994). Let $X_{1},X_{2},\\ldots,X_{n}$ be independent random variables that are sub-Gaussian with parameter $\\sigma$ . Define $\\textstyle S_{n}=\\sum_{i=1}^{n}X_{i}$ . Then, for any $t>0$ , Hoeffding\u2019s inequality provides an upper bound on the tail probabilities of $S_{n}$ , which is given by: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\vert S_{n}-\\mathbb{E}[S_{n}]\\vert\\ge t\\right)\\le2\\exp\\left(-\\frac{t^{2}}{2n\\sigma^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This result emphasizes the robustness of the sum $S_{n}$ against deviations from its expected value, particularly useful in applications requiring high confidence in estimations from independent subGaussian observations. ", "page_idx": 29}, {"type": "text", "text": "Lemma G.9. (Lemma F.4. in Dann et al. (2017)) Let ${\\mathcal{F}}_{i}$ for $i=1\\,.\\,.\\,.$ be a flitration and $X_{1},\\ldots X_{n}$ be a sequence of Bernoulli random variables with $\\mathbb{P}\\left(X_{i}=1\\mid\\mathcal{F}_{i-1}\\right)\\,=\\,P_{i}$ with $P_{i}$ being $\\mathcal{F}_{i-1}$ - measurable and $X_{i}$ being ${\\mathcal{F}}_{i}$ measurable. It holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\exists n:\\sum_{t=1}^{n}X_{t}<\\sum_{t=1}^{n}P_{t}/2-W\\right)\\leq e^{-W}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contribution and scope: RA-PbRL algorithm. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The paper discussed the limitations of the work in section 3.2, and section 6. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper provided the full set of assumptions and a complete proof in Appendix. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The experimental results presented in the paper are reproducible. We have provided detailed pseudocode and full information for implementing the code, allowing any researcher to easily implement our algorithm and benchmark it against other algorithms. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide a link to our code repository. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Experimental details are provided in section 5.1. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: the results are accompanied standard deviation. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The computer resources needed to implement experiments are provided in the section 5. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The research conducted with the NeurIPS Code of Ethics ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: the paper does not use existing assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 34}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subject. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subject. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]