[{"type": "text", "text": "On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yusu Hong ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Junhong Lin\\* ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Center for Data Science and School of Mathematical Sciences Zhejiang University yusuhong@zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Center for Data Science Zhejiang University junhong@zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we study Adam in non-convex smooth scenarios with potential unbounded gradients and affine variance noise. We consider a general noise model which governs affine variance noise, bounded noise, and sub-Gaussian noise. We show that Adam with a specific hyper-parameter setup can find a stationary point with a $\\mathcal{O}(\\mathrm{poly}(\\log T)/\\sqrt{T})$ rate in high probability under this general noise model where $T$ denotes total number iterations, matching the lower rate of stochastic first-order algorithms up to logarithm factors. We also provide a probabilistic convergence result for Adam under a generalized smooth condition which allows unbounded smoothness parameters and has been illustrated empirically to capture the smooth property of many practical objective functions more accurately. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Since its introduction by [33], the Stochastic Gradient Descent (SGD): $\\pmb{x}_{t+1}=\\pmb{x}_{t}-\\eta_{t}\\pmb{g}_{t}$ has achieved significant success in solving the unconstrained stochastic optimization problems: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{x}\\in\\mathbb{R}^{d}}f(\\pmb{x}),\\quad\\mathrm{where}\\quad f(\\pmb{x})=\\mathbb{E}_{\\pmb{\\xi}}[f_{\\pmb{\\xi}}(\\pmb{x},\\pmb{\\xi})],\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\xi$ is a random variable, $\\scriptstyle g_{t}$ is the stochastic gradients and $\\eta_{t}$ is the step-size. From then on, numerous literature focused on the convergence behavior of SGD in various scenarios. Several studies focused on the non-convex smooth scenario where the stochastic gradient $g(x)$ is unbiased with affine variance noise, i.e., for some constants $\\sigma_{0},\\sigma_{1}\\geq0$ and all $\\pmb{x}\\in\\mathbb{R}^{d}$ ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\|g(\\pmb{x})-\\nabla f(\\pmb{x})\\|^{2}]\\le\\sigma_{0}^{2}+\\sigma_{1}^{2}\\|\\nabla f(\\pmb{x})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Under the noise assumption (2), [3] provided an almost-sure convergence bound for SGD. [4] proved that SGD could reach a stationary point with a ${\\mathcal{O}}(1/{\\sqrt{T}})$ rate when step-sizes are tuned by problemparameters such as the smooth parameter $L$ . The theoretical result also revealed that the analysis of SGD under (2) is not essentially different from the bounded noise case [17]. ", "page_idx": 0}, {"type": "text", "text": "In the popular field of deep learning, a range of variants based on SGD, known as adaptive gradient methods have emerged. These methods employ the past gradients to adaptively tune their stepsizes and are preferred to SGD for minimizing various objective functions due to their efficiency. Among these methods, Adam [22] has been one of the most effective methods empirically. Generally speaking, Adam absorbs some key ideas from previous adaptive methods such as AdaGrad [12, 36] and RMSProp [37] while adding more unique structures. It combines the exponential moving average mechanism from RMSProp and meanwhile adds the heavy-ball style momentum [29] and two unique corrective terms. This unique structure leads to a huge success for Adam in practical applications but at the same time brings more challenges to the theoretical analysis. ", "page_idx": 0}, {"type": "table", "img_path": "x7usmidzxj/tmp/2a427917dd11679c81cdd585658745b5a1ae5da1079aa3a102bf4a0f1d8da8c4.jpg", "table_caption": ["Table 1: Comparison for existing Adam analyses with ours. "], "table_footnote": ["[34] rquires $\\begin{array}{r}{T(\\beta_{1},\\beta_{2})=\\mathcal{O}\\left(\\frac{\\beta_{1}}{\\beta_{2}^{n}}\\left(\\frac{1-\\beta_{1}}{1-\\beta_{1}^{n}}+1\\right)\\right)\\to0}\\end{array}$ which sems could only achieve hen $\\beta_{1}=0$ 3FCT\"refersto flcorectiveterms\\*. he $^{*}\\mathrm{Conv}$ rate' column presentsthecoavergencerae omiting logaithm factors "], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Considering the significance of affine variance noise and Adam in both theoretical and empirical fields, it's natural to question whether Adam can find a stationary point at a rate comparable to SGD under the same smooth condition and (2). Earlier researches [14, 40, 2] have shown that AdaGrad-Norm, a scalar version of AdaGrad, can find a stationary point at the same rate as SGD, not tuning step-sizes based on problem-parameters. Moreover, they addressed an essential challenge brought by the correlation of adaptive step-sizes and noise from (2) which does not appear in SGD's cases. However, since AdaGrad-Norm applies a cumulative step-sizes mechanism which is rather different from the exponential moving average step-sizes in Adam, the analysis for AdaGrad-Norm could not be trivially extended to Adam. Furthermore, the coordinate-wise step-size architecture of Adam, rather than the unified step-size for all coordinates in AdaGrad-Norm, brings more challenge when considering (2). In affine variance noise landscape, existing literature could only ensure the Adam's convergence with random-reshuffing scheme under certain parameter restrictions [52, 41], or deduce the convergence at the expense of requiring bounded gradient assumption and using problem-parameters to tune the step-sizes [18], both of which ignored the corrective terms. Some other works proved convergence to a stationary point by altering the original Adam algorithm such as removing certain corrective terms and modifying (2) to a stronger coordinate-wise variant [39, 19]. ", "page_idx": 1}, {"type": "text", "text": "To the best of our knowledge, existing research has not yet fully confirmed the convergence of Adam under affine variance noise. To address this gap, we conduct an in-depth analysis and prove that Adam with the right parameter can find a stationary point in high probability. We assume a milder noise model (detailed in Assumption (A3)), covering almost surely affine variance noise, the bounded noise, and sub-Gaussian noise. We show that the convergence rate can reach at $\\mathcal{O}\\left(\\mathrm{poly}(\\log T)/\\sqrt{T}\\right)$ matching the lower rate in [1] up to logarithm factors. Our proof employs the descent lemma over the introduced proxy iterative sequence and adopts techniques related to the new proxy step-sizes and error decomposition. Based on this, we are able to handle the correlation between stochastic gradients and adaptive step-sizes and transform the first-order term from the descent lemma into the gradientnorm. ", "page_idx": 1}, {"type": "text", "text": "Finally, we apply the analysis to the $(L_{0},L_{q})$ -smooth condition [51]. Several researchers have found empirical evidence of objective functions satisfying $(L_{0},L_{q})$ -smoothness but out of $L$ -smoothness range, especially in large-scale language models [49, 38, 11, 8]. Theoretical analysis of adaptive methods under this relaxed condition is more complicated and needs further nontrivial proof techniques. Also, prior knowledge of problem-parameters to tune step-sizes is needed, as indicated by the counter-examples from [40] for the AdaGrad. Existing works [13, 40] obtained a convergence bound for AdaGrad-Norm with (2), and [24] considered Adam with sub-Gaussian noise. In this paper, we provide a probabilistic convergence result for Adam with the affine variance noise and the generalized smoothness condition. ", "page_idx": 1}, {"type": "text", "text": "Input: Horizon $T$ $\\mathbf{x}_{1}\\in\\mathbb{R}^{d}$ \uff0c $\\beta_{1},\\beta_{2}\\in[0,1)$ \uff0c $\\pmb{m}_{0}=\\pmb{v}_{0}=\\pmb{0}_{d}$ $\\eta,\\epsilon>0,\\epsilon=\\epsilon{\\bf1}_{d}$   \nfor $s=1,\\cdots,T$ do Draw a new sample $\\mathscr{z}_{s}$ and generate ${\\pmb g}_{s}=g({\\pmb x}_{s},z_{s})$ $\\pmb{m}_{s}=\\beta_{1}\\pmb{m}_{s-1}+(1-\\beta_{1})\\pmb{g}_{s}$ .\uff0c $\\pmb{v}_{s}=\\beta_{2}\\pmb{v}_{s-1}+(1-\\beta_{2})\\pmb{g}_{s}^{2}$ .\uff0c $\\eta_{s}=\\eta\\sqrt{1-\\beta_{2}^{s}}/(1-\\beta_{1}^{s})$ \uff0c $\\epsilon_{s}=\\epsilon\\sqrt{1-\\beta_{2}^{s}};$ $\\pmb{x}_{s+1}=\\pmb{x}_{s}-\\eta_{s}\\cdot\\pmb{m}_{s}/\\left(\\sqrt{\\pmb{v}_{s}}+\\pmb{\\epsilon}_{s}\\right)$   \nend for ", "page_idx": 2}, {"type": "text", "text": "We also refer readers to see the main contributions of our works and comparisons with the existing works in Table 1. ", "page_idx": 2}, {"type": "text", "text": "Notations  We use $[T]$ to denote the set $\\{1,2,\\cdot\\cdot\\cdot,T\\}$ for any positive integer $T$ ${\\big\\langle},\\,{\\big|}\\cdot{\\big|},\\,{\\big|}\\cdot{\\big|}{\\big|}_{1}$ and $\\|\\cdot\\|_{\\infty}$ to denote $l_{2}$ -norm, $l_{1}$ -norm and $l_{\\infty}$ -norm respectively. $a\\sim\\mathcal{O}(b)$ and $a\\leq\\mathcal{O}(b)$ denote $a=C_{1}b$ and $a\\leq C_{2}b$ for some positive universal constants $C_{1},C_{2}$ , and $a\\leq\\tilde{\\mathcal{O}}(b)$ denotes $a\\le\\mathcal{O}(b)\\mathrm{poly}(\\log b)$ $a\\lesssim b$ denotes $a\\leq\\mathcal{O}(b)$ . For any vector $\\pmb{x}\\in\\mathbb{R}^{d}$ \uff0c $x^{2}$ and $\\sqrt{x}$ denote coordinate-wise square and square root respectively. $\\pmb{x}_{i}$ denotes the $i$ -th coordinate of $\\textbf{\\em x}$ . For any two vectors $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$ , we use $\\pmb{x}\\odot\\pmb{y}$ and $\\scriptstyle{\\mathbf{\\mathit{x}}}/{\\mathit{y}}$ to denote the coordinate-wise product and quotient respectively. $\\mathbf{0}_{d}$ and $\\mathbf{1}_{d}$ represent zero and one $d$ -dimensional vectors respectively. ", "page_idx": 2}, {"type": "text", "text": "2   Problem set up and algorithm ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider unconstrained stochastic optimization (1) over $\\mathbb{R}^{d}$ with $l_{2}$ -norm. The objective function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is differentiable.Given $\\dot{\\mathbf{x}}\\in\\mathbb{R}^{d}$ , we assume a gradient oracle that returns a random vector $g(\\pmb{x},z)\\in\\mathbb{R}^{d}$ dependent by the random sample $_{z}$ . The true gradient of $f$ at $\\textbf{\\em x}$ is denoted by $\\nabla f(\\pmb{x})\\in\\mathbb{R}^{d}$ ", "page_idx": 2}, {"type": "text", "text": "Assumptions  We make the following assumptions throughout the paper. ", "page_idx": 2}, {"type": "text", "text": "\u00b7 (A1) Bounded below: There exists $f^{*}>-\\infty$ such that $f(\\pmb{x})\\geq f^{*},\\forall\\pmb{x}\\in\\mathbb{R}^{d}$ \u00b7 (A2) Unbiased estimator: The gradient oracle provides an unbiased estimator of $\\nabla f({\\pmb x})$ ,i.e., $\\mathbb{E}_{z}\\left[g(\\mathbf{\\boldsymbol{x}},z)\\right]=\\nabla f(\\mathbf{\\boldsymbol{x}}),\\forall\\mathbf{\\boldsymbol{x}}\\in\\mathbb{R}^{d}$ \u00b7 (A3) Generalized affine variance noise: The gradient oracle satisfies that there are some constants $\\begin{array}{r}{\\sigma_{0},\\sigma_{1}>0,p\\in[0,4),\\mathbb{E}_{z}\\left[\\exp\\left(\\frac{\\|g(\\pmb{x},z)-\\breve{\\nabla_{f}}(\\pmb{x})\\|^{2}}{\\sigma_{0}^{2}+\\sigma_{1}^{2}\\|\\nabla f(\\pmb{x})\\|^{p}}\\right)\\right]\\leq\\exp(1),\\forall\\pmb{x}\\in\\mathbb{R}^{d}.}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "The first two assumptions are standard in the stochastic optimization. The third assumption provides a mild noise model that covers the almost surely bounded noise and sub-Gaussian noise. Moreover, it's more general than almost surely affine variance noise as follows ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|g(\\pmb{x},z)-\\nabla f(\\pmb{x})\\|^{2}\\leq\\sigma_{0}^{2}+\\sigma_{1}^{2}\\|\\nabla f(\\pmb{x})\\|^{2},a.s.,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and enlarge the range of $p$ to $[0,4)$ .Assumption (A3) with $p=2$ and (3) are also utilized in [2] to establish high probability results for AdaGrad-Norm. It represents a stronger condition than the expected version of (2) that is commonly employed for deriving the expected convergence of algorithms. However, almost surely assumption enables the derivation of stronger high-probability convergence guarantees for algorithms, while still ensuring expected convergence. ", "page_idx": 2}, {"type": "text", "text": "The affine noise variance assumption is important for machine learning applications with feature noise (including missing features) [15, 21], in robust linear regression [45], and generally whenever the model parameters are multiplicatively perturbed by noise (e.g., a multilayer network, where noise from a previous layer multiplies the parameters in subsequent layers). We refer interested readers to see e.g., [3, 45, 4, 14, 40, 2] for more discussions about the affine variance noise. ", "page_idx": 2}, {"type": "text", "text": "Adam  For the stochastic optimization problem, we study Algorithm 1, which is an equivalent form of Adam [22] with the two corrective terms for $\\pmb{m}_{s}$ and $\\pmb{v}_{s}$ included into $\\eta_{s}$ for notation simplicity. ", "page_idx": 2}, {"type": "text", "text": "The iterative relationship in Algorithm 1 can be also written as for any $s\\in[T]$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{x}_{s+1}=\\pmb{x}_{s}-\\eta_{s}(1-\\beta_{1})\\cdot\\frac{\\pmb{g}_{s}}{\\sqrt{\\pmb{v}_{s}}+\\epsilon_{s}}+\\beta_{1}\\cdot\\frac{\\eta_{s}\\big(\\sqrt{\\pmb{v}_{s-1}}+\\epsilon_{s-1}\\big)}{\\eta_{s-1}\\big(\\sqrt{\\pmb{v}_{s}}+\\epsilon_{s}\\big)}\\odot(\\pmb{x}_{s}-\\pmb{x}_{s-1}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "wherewelet $\\pmb{x}_{0}=\\pmb{x}_{1}$ and $\\eta_{0}=\\eta$ (4) plays a key role in the convergence analysis, showing that Adam incorporates a heavy-ball style momentum and dynamically adjusts its momentum through $\\beta_{1}$ and $\\beta_{2}$ , along with adaptive step-sizes. This inspires us to learn from some classical analysis methods for algorithms with momentum and provides some new estimations to fit in with the adaptive property. ", "page_idx": 3}, {"type": "text", "text": "3   Convergence of Adam with smooth objective functions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we assume that the objective function $f$ $L$ smooth satisfying that for any $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\pmb{y})-\\nabla f(\\pmb{x})\\|\\leq L\\|\\pmb{y}-\\pmb{x}\\|.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We then show that Adam has the following high probability results. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1. Let $T\\geq1$ and $\\{x_{s}\\}_{s\\in[T]}$ be the sequence generated by Algorithm $^{\\,I}$ IfAssumptions (A1)-(A3) hold, and the hyper-parameters satisfy that ", "page_idx": 3}, {"type": "equation", "text": "$$\n0\\leq\\beta_{1}<\\beta_{2}<1,\\quad\\beta_{2}=1-c/T,\\quad\\eta=C_{0}\\sqrt{1-\\beta_{2}},\\quad\\epsilon=\\epsilon_{0}\\sqrt{1-\\beta_{2}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for some constants $c,C_{0}~>~0$ and $\\epsilon_{0}\\;>\\;0$ . then for any given $\\delta\\ \\in\\ (0,1/2)$ , it holds that with probability at least $1-2\\delta$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{s=1}^{T}\\|\\nabla f({\\pmb x}_{s})\\|^{2}\\leq\\mathcal{O}\\left\\{G^{2}\\left(\\sqrt{\\frac{\\sigma_{0}^{2}+\\sigma_{1}^{2}G^{p}+G^{2}}{T}}+\\frac{\\epsilon_{0}}{T}\\right)\\log\\left(\\frac{T}{\\delta}\\right)\\right\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $G^{2}$ is defined by the following order with respect to $T,\\epsilon_{0},\\delta$ 2. ", "page_idx": 3}, {"type": "equation", "text": "$$\nG^{2}\\sim\\mathcal{O}\\left(\\log^{\\frac{3}{2}\\operatorname*{max}\\{2,\\frac{4}{4-p}\\}}\\left(\\frac{T}{\\epsilon_{0}\\delta}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 provides the nearly optimal convergence rate $\\mathcal{O}\\left(\\mathrm{poly}(\\log T)/\\sqrt{T}\\right)$ to find a stationary point when setting the parameter probably: $\\beta_{2}=1-{\\cal O}(1/T)$ . It's worth noting that the setting requires $\\beta_{2}$ to be closed enough to 1 when $T$ is sufficiently large, which roughly aligns with the typical setting in [22, 57, 10, 39]. ", "page_idx": 3}, {"type": "text", "text": "For a more detailed comparison of our results to existing works, including assumptions, convergence rate, and dependency, we refer readers to Table 1. ", "page_idx": 3}, {"type": "text", "text": "4  Convergence of Adam with generalized smooth objective functions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we study the convergence behavior of Adam in the generalized smooth case. We first provide some necessary introduction to the generalized smooth condition. ", "page_idx": 3}, {"type": "text", "text": "4.1 Generalized smoothness ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For a differentiable objective function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ , we consider the following $(L_{0},L_{q})$ -smoothness condition: there exist constants $q\\in[0,2)$ and $L_{0},L_{q}>0$ , satisfying that for any $\\bar{\\mathbf{x}_{}},\\bar{\\mathbf{y}}\\in\\mathbb{R}^{d}$ with $\\|x-y\\|\\leq1/L_{q}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\pmb{y})-\\nabla f(\\pmb{x})\\|\\leq\\left(L_{0}+L_{q}\\|\\nabla f(\\pmb{x})\\|^{q}\\right)\\|\\pmb{x}-\\pmb{y}\\|.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The generalized smooth condition was originally put forward by [51] for any twice differentiable function $f$ satisfying that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\nabla^{2}f(\\pmb{x})\\|\\le L_{0}+L_{1}\\|\\nabla f(\\pmb{x})\\|.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It has been proved that a lot of objective functions in experimental areas satisfy (9) but out of $L$ -smoothness range, especially in training large language models, see e.g., Figure 1 in [51] and [8]. ", "page_idx": 4}, {"type": "text", "text": "To better understand the theoretical significance of the generalized smoothness, [49] provided an alternative form in (8) with $q=1$ ,onlyrequiring $f$ to be differentiable. They showed that (8) is sufficient to elucidate the convergence of gradient-clipping algorithms. ", "page_idx": 4}, {"type": "text", "text": "There are three key reasons for opting for (8). Firstly, considering our access is limited to first-order stochastic gradients, it's logical to only assume that $f$ is differentiable. Second, as pointed out by Lemma A.2 in [49] and Proposition 1 in [13], (8) and (9) are equivalent up to constant factors when $f$ is twice differentiable considering $q=1$ . Thus, (8) covers a broader range of functions than (9). Finally, it's easy to verify that (8) is strictly weaker than $L$ -smoothness. A concrete example is that the simple function $f(x)=x^{4},x\\in\\mathbb{R}$ does not satisfy any global $L$ -smoothness but (8). Moreover, the expanded range of $q$ to $[0,2)$ is necessary as all univariate rational functions $P(x)/Q(x)$ , where $P,Q$ are polynomials and double exponential functions $a^{(b^{x})}$ with $a,b>1$ are $(L_{0},L_{q})$ -smooth with $1<q<2$ (see [24, Proposition 3.4]). We refer interested readers to see [51, 49, 13, 24] for more discussions of concrete examples of generalized smoothness. ", "page_idx": 4}, {"type": "text", "text": "4.2   Convergence result ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We then provide the high probability convergence result of Adam with $(L_{0},L_{q})$ -smoothness condition as follows. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1. Let $T\\geq1$ and $\\delta\\,\\in\\,(0,1/2)$ . Ssuppose that $\\{x_{s}\\}_{s\\in[T]}$ is a sequence generated by Algorithm $^{\\,l}$ $f$ is $(L_{0},L_{q})$ -smooth satisfying (8), Assumptions (A1)-(A3) hold, and the parameters satisfy ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\le\\beta_{1}<\\beta_{2}<1,\\quad\\beta_{2}=1-c/T,\\quad\\epsilon=\\epsilon_{0}\\sqrt{1-\\beta_{2}},\\quad\\eta=\\Tilde{C}_{0}\\sqrt{1-\\beta_{2}},}\\\\ &{\\Tilde{C}_{0}\\le\\operatorname*{min}\\left\\{E_{0},\\displaystyle\\frac{E_{0}}{\\mathcal{H}},\\displaystyle\\frac{E_{0}}{\\mathcal{L}},\\sqrt{\\frac{\\beta_{2}(1-\\beta_{1})^{2}(1-\\beta_{1}/\\beta_{2})}{4L_{q}^{2}d}}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $c,\\epsilon_{0},E_{0},\\tilde{C}_{0}>0$ are constants,. $\\hat{H}$ is controlled y $\\begin{array}{r}{{\\mathcal{O}}\\left(\\log\\left(\\frac{T}{\\epsilon_{0}\\delta}\\right)\\right)^{3}}\\end{array}$ , and $H,\\mathcal{H},\\mathcal{L}$ are defned as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H:=L_{0}/L_{q}+\\left(4L_{q}\\hat{H}\\right)^{q}+\\left(4L_{q}\\hat{H}\\right)^{\\frac{q}{2-q}}+\\left(4L_{0}\\hat{H}\\right)^{\\frac{q}{2}}+4L_{q}\\hat{H}+\\left(4L_{q}\\hat{H}\\right)^{\\frac{1}{2-q}}+\\sqrt{4L_{0}\\hat{H}},}\\\\ &{\\mathcal{H}:=\\sqrt{2(\\sigma_{0}^{2}+\\sigma_{1}^{2}H^{p}+H^{2})\\log\\left(\\frac{\\mathrm{e}T}{\\delta}\\right)},\\quad\\mathcal{L}:=L_{0}+L_{q}\\left(H^{q}+H+\\frac{L_{0}}{L_{q}}\\right)^{q}.\\qquad\\qquad(1\\leq q\\leq N)\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then it holds that with probability at least $1-2\\delta$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{s=1}^{T}\\|\\nabla f(x_{s})\\|^{2}\\leq\\mathcal{O}\\left\\{\\frac{\\hat{H}}{\\tilde{C}_{0}}\\left(\\sqrt{\\frac{\\sigma_{0}^{2}+\\sigma_{1}^{2}H^{p}+H^{2}}{T}}+\\frac{\\epsilon_{0}}{T}\\right)\\log\\left(\\frac{T}{\\delta}\\right)\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that in the above theorem, the order of $\\log T$ in $\\hat{H}$ and the final convergence bound is better than the one in Theorem 3.1 under the same noise assumption. This better dependency comes from the expense of using problem parameters to tune step-size ${\\tilde{C}}_{0}$ . Since H is logarithm order of $T,H,\\mathcal{H},\\mathcal{L}$ are both polynomial logarithm order of $T$ and the final convergence rate in (12) is also polynomial logarithm order of $T$ . Note that $\\tilde{C}_{0}\\le\\mathcal{O}(1/\\mathrm{poly}(\\log T))$ from (10) when $T\\gg d$ . Hence, when $T$ is large enough, a possible optimal setting is that $\\eta=c_{1}/(\\sqrt{T}\\mathrm{poly}(\\log T))$ for some constant $c_{1}>0$ which roughly matches the typical setting as mentioned before. ", "page_idx": 4}, {"type": "text", "text": "5 Related works ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "There is a large amount of works on stochastic approximations (or online learning algorithms) and adaptive variants, e.g., [5, 35, 47, 28, 12, 4, 6, 26, 54] and the references therein. In this section, we will discuss the most related works and make a comparison with our main results. ", "page_idx": 4}, {"type": "text", "text": "5.1  Convergence with affine variance noise and its variants ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We mainly list previous literature considering (2) over non-convex smooth scenario. [3] provided an asymptotic convergence result for SGD with (2). In terms of non-asymptotic results, [4] proved the convergence of SGD, illustrating that the analysis was non-essentially different from the bounded noise case from [17]. ", "page_idx": 5}, {"type": "text", "text": "In the adaptive methods field, [14] studied convergence of AdaGrad-Norm with (2), pointing out that the analysis is more challenging than the bounded noise and bounded gradient case in [43]. They provided a convergence rate of $\\tilde{\\mathcal{O}}(1/\\sqrt{T})$ without knowledge of problem parameters, and further improved the bound adapting to the noise level: when $\\sigma_{1}\\sim\\mathcal{O}(1/\\sqrt{T})$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E}\\|\\nabla f(\\pmb{x}_{t})\\|^{2}\\leq\\tilde{\\mathcal{O}}\\left(\\frac{\\sigma_{0}}{\\sqrt{T}}+\\frac{1}{T}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(13) matches exactly with SGD's case [4], showing a fast rate of $\\tilde{\\mathcal{O}}(1/T)$ when $\\sigma_{0}$ is sufficiently low. Later, [40] proposed a deep analysis framework obtaining (13) with a tighter dependency to $T$ and not requiring any restriction over $\\sigma_{1}$ . They further obtained the same rate for AdaGrad under a stronger coordinate-wise version of (2): for all $i\\in[d]$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{z}|g(\\pmb{x},z)_{i}-\\nabla f(\\pmb{x})_{i}|^{2}\\leq\\sigma_{0}^{2}+\\sigma_{1}^{2}|\\nabla f(\\pmb{x})_{i}|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "[2] obtained a probabilistic convergence rate for AdaGrad-Norm with (3) using a novel induction argument to estimate the function value gap without any requirement over $\\sigma_{1}$ as well. ", "page_idx": 5}, {"type": "text", "text": "In the analysis of Adam, a line of works [34, 52, 41] considered Adam without corrective terms for finite-sum objective functions under different regimes while possibly incorporating natural random shuffling technique. They could ensure that this variant converged to a bounded region where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t\\in[T]}\\mathbb{E}\\left[\\operatorname*{min}\\{\\|\\nabla f(\\pmb{x}_{t})\\|,\\|\\nabla f(\\pmb{x}_{t})\\|^{2}\\}\\right]\\lesssim\\frac{\\log T}{\\sqrt{T}}+C_{1}\\sigma_{0}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "under the affine growth condition which is equivalent to (2). Though not explicitly concluded, when setting $\\beta_{2}=1-O(1/T)$ , [52]'s work can also ensure a convergence rate of order $\\tilde{\\mathcal{O}}(1/\\sqrt{T})$ under certain settings. Besides, both [20] and [18] provided convergence bounds allowing for large heavy-ball momentum parameter that aligns more closely with practical settings. However, they relied on the assumption for step-sizes where $\\begin{array}{r}{C_{l}\\,\\le\\,\\|\\frac{\\cdot_{1}}{\\sqrt{\\pmb{v}_{t}}+\\epsilon_{t}}\\|\\dot{\\infty}\\,\\le\\,C_{u},\\forall t\\,\\in\\,\\overline{{[T]}}}\\end{array}$ [39] and [19] used distinct methods to derive convergence bounds in expectation and high probability respectively, without relying on bounded gradients. Both studies achieved a convergence rate of the form in (13) for Adam ignoring the corrective terms. [19] further achieved a $\\tilde{\\mathcal{O}}(1/\\bar{\\sqrt{T}})$ rate for Adam. However, the two works only studied coordinate-wise affine variance noise. ", "page_idx": 5}, {"type": "text", "text": "In this paper, we derive a stronger high probability convergence rate for Adam with original corrective terms, relying on an almost surely noise assumption. The noise model is general enough to cover bounded noise, sub-Gaussian noise, and (coordinate-wise) affine variance noise. Although we consider a stronger almost surely assumption, our probabilistic convergence result is also stronger than the expected convergence. ", "page_idx": 5}, {"type": "text", "text": "5.2  Convergence with generalized smoothness ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The generalized smooth condition was first proposed for twice differentiable functions by [51] (see (9)) to explain the acceleration mechanism of gradient-clipping. This assumption was extensively confirmed in experiments of large-scale language models [51]. Later, [49] further relaxed it to a more general form in (8) allowing for first-order differentiable functions. Subsequently, a series of works [30, 53, 32] studied different algorithms? convergence under this condition. ", "page_idx": 5}, {"type": "text", "text": "In the field of adaptive methods, [13] provided a convergence bound for AdaGrad-Norm assuming (2) and (8) with $q=1$ , albeit requiring $\\sigma_{1}<1$ . Based on the same conditions, [40] improved the convergence rate to the form in (13) without restriction on $\\sigma_{1}$ . [41] explored how Adam without corrective terms behaves under generalized smoothness with $q=1$ and (2). However, they could only assert convergence to a bounded region as shown in (15). [8] showed that an Adam-type algorithm converges to a stationary point under a stronger coordinate-wise generalized smooth condition. ", "page_idx": 5}, {"type": "text", "text": "Recently, [24] provided a novel framework to derive high probability convergence bound for Adam under the generalized smooth and sub-Gaussian noise case. ", "page_idx": 6}, {"type": "text", "text": "In this paper, we consider a more general noise setup and investigate Adam's convergence under the generalized smooth landscape. We prove that Adam is powerful enough to find a stationary point with properly tuned step-sizes even under these relaxed assumptions. Moreover, the convergence rate is not harmed by the relaxation of noise and smoothness, matching the optimal ${\\mathcal{O}}(1/{\\sqrt{T}})$ rate up to logarithmfactors. ", "page_idx": 6}, {"type": "text", "text": "5.3 Convergence of Adam ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Adam was first proposed by [22] with empirical studies and theoretical results on online convex learning. The original proof of convergence in [22] was later shown by [31] to contain gaps. [31] and the subsequent work [42] also showed that for a range of momentum parameters chosen independently with the problem instance, Adam does not necessarily converge even for convex objectives. Many works have focused on its convergence behavior in non-convex smooth fields. A series of works studied Adam ignoring corrective terms, all requiring a uniform bound for gradients\u2019 norm. Among these works, [48] demonstrated that Adam can converge within a specific region if step-sizes and decay parameters are determined properly by the smooth parameter. [9] proposed a convergence result to a stationary point and required all stochastic gradients must keep the same sign. To circumvent this requirement, [57] introduced a convergence bound only requiring hyper-parameters to satisfy specific conditions. [10] conducted a simple proof and further improved the dependency on the heavy-ball momentum parameter. Recently, [55] introduced Nesterov-like acceleration into Adam and AdamW [27] indicating their superiority in convergence over the non-accelerated versions. For Adam-related works under (2) or generalized smoothness, we refer readers to Sections 5.1 and 5.2. ", "page_idx": 6}, {"type": "text", "text": "We also want to highlight that a series of works [23, 44, 50] investigated the geometry of Adam from an $l_{\\infty}$ -norm perspective. [23] and [44] studied the geometry of Adam by regarding it as a variant of SignSGD and [50] showed that full-batch Adam converges towards a linear classifier that achieves the maximum $l_{\\infty}$ -margin when the training data are linearly separable. ", "page_idx": 6}, {"type": "text", "text": "6 Proof sketch under the smooth case ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we provide a proof sketch of Theorem 3.1 with some insights and proof novelty. Our proof borrows some ideas from [43, 10, 14, 2, 39, 19]. The detailed proof can be found in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "Preliminary   To start with, we let the stochastic gradient $\\pmb{g}_{s}=(g_{s,i})_{i}$ , the true gradient $\\nabla f({\\pmb x}_{s})=$ $\\bar{\\pmb{g}}_{s}=(\\bar{g}_{s,i})_{i}$ and $\\pmb{\\xi}_{s}=(\\xi_{s,i})_{i}=\\pmb{g}_{s}-\\bar{\\pmb{g}}_{s}$ . We also let $\\epsilon_{s}=\\epsilon\\sqrt{1-\\beta_{2}^{s}}$ and thus $\\pmb{\\epsilon}_{s}=\\epsilon_{s}\\mathbf{1}_{d}$ . For any positive integer $T$ and $\\delta\\in(0,1)$ , we define $\\mathcal{M}_{T}=\\sqrt{\\log\\left(\\mathrm{e}T/\\delta\\right)}$ We denote the adaptive part of the step-size as ", "page_idx": 6}, {"type": "equation", "text": "$$\nb_{s}:=\\sqrt{v_{s}}+\\epsilon_{s}=\\sqrt{\\beta_{2}v_{s-1}+(1-\\beta_{2})g_{s}^{2}}+\\epsilon_{s}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We define two auxiliary sequences $\\{p_{s}\\}_{s\\ge1}$ and $\\{y_{s}\\}_{s\\ge1}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\np_{1}=\\mathbf{0}_{d},\\quad y_{1}=x_{1},\\quad p_{s}=\\frac{\\beta_{1}}{1-\\beta_{1}}(x_{s}-x_{s-1}),y_{s}=p_{s}+x_{s},\\forall s\\geq2.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We follow from [16, 46] which was used to prove the convergence of SGD with momentum and later applied to handle many variants of momentum-based algorithms. Recalling the iteration of $\\pmb{x}_{s}$ in (4), wereveal that $\\pmb{y}_{s}$ satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\ny_{s+1}=y_{s}-\\eta_{s}\\cdot\\frac{g_{s}}{b_{s}}+\\frac{\\beta_{1}}{1-\\beta_{1}}\\left(\\frac{\\eta_{s}b_{s-1}}{\\eta_{s-1}b_{s}}-\\mathbf{1}_{d}\\right)\\odot(\\mathbf{x}_{s}-\\mathbf{x}_{s-1}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In addition, given $T\\geq1$ , we define, $\\forall s\\in[T]$ ", "page_idx": 6}, {"type": "equation", "text": "$$\nG_{s}=\\operatorname*{max}_{j\\in[s]}\\|\\bar{g}_{j}\\|,\\mathcal{G}_{T}(s)=\\mathbb{M}_{T}\\sqrt{2\\sigma_{0}^{2}+2\\sigma_{1}^{2}G_{s}^{p}+2G_{s}^{2}},\\mathcal{G}_{T}=\\mathbb{M}_{T}\\sqrt{2\\sigma_{0}^{2}+2\\sigma_{1}^{2}G^{p}+2G^{2}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $G$ is as in Theorem 3.1. Both $G_{s}$ and $\\mathcal{G}_{T}(s)$ will serve as upper bounds for gradients\u2032 norm before time $s$ . We will verify their importance in the later argument. ", "page_idx": 6}, {"type": "text", "text": "Starting from the descent lemma We fix the horizon $T$ and start from the standard descent lemma of $L$ -smoothness. Then, for any given $t\\in[T]$ , combining with (18) and summing over $s\\in[t]$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle f(\\pmb{y}_{t+1})\\leq\\,f(\\pmb{x}_{1})+\\underbrace{\\sum_{s=1}^{t}-\\eta_{s}\\left\\langle\\nabla f(\\pmb{y}_{s}),\\frac{g_{s}}{b_{s}}\\right\\rangle}_{\\mathbf{A}}+\\underbrace{\\frac{\\beta_{1}}{1-\\beta_{1}}\\sum_{s=1}^{t}\\left\\langle\\Delta_{s}\\odot(\\pmb{x}_{s}-\\pmb{x}_{s-1}),\\nabla f(\\pmb{y}_{s})\\right\\rangle}_{\\mathbf{B}}}\\\\ {\\displaystyle+\\underbrace{\\frac{L}{2}\\sum_{s=1}^{t}\\left\\|\\eta_{s}\\cdot\\frac{g_{s}}{b_{s}}-\\frac{\\beta_{1}}{1-\\beta_{1}}(\\Delta_{s}\\odot(\\pmb{x}_{s}-\\pmb{x}_{s-1}))\\right\\|^{2}}_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Where we let \u25b3s = nab- and use ${\\pmb y}_{1}={\\pmb x}_{1}$ from (17). In what follows, we will estimate A, B, and $\\mathbf{C}$ respectively. ", "page_idx": 7}, {"type": "text", "text": "Probabilistic estimations  To proceed with the analysis, we next introduce two probabilistic estimations showing that the norm of the noises and a related summation of martingale difference sequence could be well controlled with high probability. We show that with probability at least $1-2\\delta$ the following two inequalities hold simultaneously for all $t\\in[T]$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|\\pmb{\\xi}_{t}\\|^{2}\\leq\\mathbb{M}_{T}^{2}\\left(\\sigma_{0}^{2}+\\sigma_{1}^{2}\\|\\bar{\\pmb{g}}_{t}\\|^{p}\\right),\\quad\\mathrm{and}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n-\\sum_{s=1}^{t}\\eta_{s}\\left\\langle\\bar{g}_{s},\\frac{\\xi_{s}}{a_{s}}\\right\\rangle\\leq\\frac{\\mathcal{G}_{T}(t)}{4\\mathcal{G}_{T}}\\sum_{s=1}^{t}\\eta_{s}\\left\\Vert\\frac{\\bar{g}_{s}}{\\sqrt{\\pmb{a}_{s}}}\\right\\Vert^{2}+D_{1}\\mathcal{G}_{T},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $D_{1}$ is a constant defined in Lemma B.7 and $\\pmb{a}_{s}$ will be introduced later. In what follows, we always assume that (21) and (22) hold for all $t\\,\\in\\,[T]$ and carry out our subsequent analysis with some deterministic estimations. ", "page_idx": 7}, {"type": "text", "text": "Estimating A  We first decompose A as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{A}=\\underbrace{\\sum_{s=1}^{t}{-\\eta_{s}\\left\\langle\\bar{g}_{s},\\frac{g_{s}}{b_{s}}\\right\\rangle}}_{\\mathrm{~A.1~}}+\\underbrace{\\sum_{s=1}^{t}\\eta_{s}\\left\\langle\\bar{g}_{s}-\\nabla f(y_{s}),\\frac{g_{s}}{b_{s}}\\right\\rangle}_{\\mathrm{~A.2~}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Due to the correlation of the stochastic gradient $\\pmb{g}_{s}$ and the step-size $\\eta_{s}/b_{s}$ , the estimating of A.1 is challenging, as also noted in the analysis for other adaptive gradient methods, e.g., [43, 10, 14, 2, 39, 19]. To break this correlation, the so-called proxy step-size technique is introduced and variants of proxy step-size have been introduced in the related literature. However, to our best knowledge, none of these proxy step-sizes could be used in our analysis for Adam considering potential unbounded gradients under the noise model in Assumption (A3). In this paper, we construct a proxy step-size $\\eta_{s}/a_{s}$ , with $\\pmb{a}_{s}$ relying on $\\mathcal{G}_{T}(s)$ in (19), defined as for any $s\\in[T]$ \uff0c ", "page_idx": 7}, {"type": "equation", "text": "$$\n{\\pmb{a}}_{s}=\\sqrt{\\beta_{2}{\\pmb v}_{s-1}+\\left(1-\\beta_{2}\\right)\\left(\\mathcal{G}_{T}(s){\\pmb1}_{d}\\right)^{2}}+{\\pmb\\epsilon}_{s}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "With the so-called proxy step-size technique over $\\eta_{s}/a_{s}$ and $\\pmb{\\xi}_{s}=\\pmb{g}_{s}-\\bar{\\pmb{g}}_{s}$ , we decompose A.1 as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{A.1}=-\\sum_{s=1}^{t}\\eta_{s}\\left\\|\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\|^{2}\\underbrace{-\\sum_{s=1}^{t}\\eta_{s}\\left\\langle\\bar{g}_{s},\\frac{\\xi_{s}}{a_{s}}\\right\\rangle}_{\\mathbf{A.1.1}}+\\underbrace{\\sum_{s=1}^{t}\\eta_{s}\\left\\langle\\bar{g}_{s},\\left(\\frac{1}{a_{s}}-\\frac{1}{b_{s}}\\right)g_{s}\\right\\rangle}_{\\mathrm{A.1.2}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In the above decomposition, the first term serves as a descent term. A.1.1 is now a summation of a martingale difference sequence which could be estimated by (22). A.1.2 is regarded as an error term when introducing $\\pmb{a}_{s}$ . However, due to the delicate construction of $\\pmb{a}_{s}$ , the definition of local gradients'bound $\\mathcal{G}_{T}(\\bar{t})$ , and using some basic inequalities, we show that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{A.1.2}\\leq\\frac{1}{4}\\sum_{s=1}^{t}\\eta_{s}\\left\\|\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\|^{2}+\\frac{\\eta\\mathcal{G}_{T}(t)\\sqrt{1-\\beta_{2}}}{1-\\beta_{1}}\\sum_{s=1}^{t}\\left\\|\\frac{g_{s}}{b_{s}}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The first RHS term can be eliminated with the descent term while the summation of the last term can bebounded by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\left\\|\\frac{g_{s}}{b_{s}}\\right\\|^{2}\\vee\\sum_{s=1}^{t}\\left\\|\\frac{m_{s}}{b_{s}}\\right\\|^{2}\\vee\\sum_{s=1}^{t}\\left\\|\\frac{m_{s}}{b_{s+1}}\\right\\|^{2}\\vee\\sum_{s=1}^{t}\\left\\|\\frac{\\hat{m}_{s}}{b_{s}}\\right\\|\\lesssim\\frac{d}{1-\\beta_{2}}\\log\\left(\\frac{T}{\\beta_{2}^{T}}\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "due to the step-size's adaptivity, the iterative relationship of the algorithm, the smoothness of the objective function, as well as (21). Here, m,s = 1-\u03b2s - ", "page_idx": 8}, {"type": "text", "text": "Estimating B and C  The key to estimate $\\mathbf{B}$ is to decompose $\\mathbf{B}$ as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{B}=\\frac{\\beta_{1}}{1-\\beta_{1}}\\sum_{s=1}^{t}\\left\\langle\\Delta_{s}\\odot\\left(x_{s}-x_{s-1}\\right),\\bar{g}_{s}\\right\\rangle+\\frac{\\beta_{1}}{1-\\beta_{1}}\\sum_{s=1}^{t}\\left\\langle\\Delta_{s}\\odot\\left(x_{s}-x_{s-1}\\right),\\nabla f(y_{s})-\\bar{g}_{s}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "To estimate B.1, we use the updated rule and further write $\\Delta_{s}\\odot(\\pmb{x}_{s}-\\pmb{x}_{s-1})$ as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\left(\\frac{\\eta_{s}}{b_{s}}-\\frac{\\eta_{s}}{a_{s}}\\right)\\odot\\mathbf{{m}_{s-1}}+\\left(\\frac{\\eta_{s}}{a_{s}}-\\frac{\\eta_{s}}{b_{s-1}}\\right)\\odot\\mathbf{{m}_{s-1}}+\\left(\\eta_{s}-\\eta_{s-1}\\right)\\frac{\\mathbf{{m}_{s-1}}}{b_{s-1}},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and upper bound the three related inner products. Using some basic inequalities, the smoothness, (24), and some delicate computations, one can estimate the three related inner products, B.2 and C, and thus get that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{B}+\\mathbf{C}\\leq\\frac{1}{4}\\sum_{s=1}^{t}\\eta_{s}\\left\\Vert\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\Vert^{2}+\\left(b_{1}\\mathcal{G}_{T}(t)+b_{2}\\right)\\log\\left(\\frac{T}{\\beta_{2}^{T}}\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $b_{1}$ and $b_{2}$ are positive constants determined by $\\beta_{1},\\beta_{2},d,L,\\eta$ ", "page_idx": 8}, {"type": "text", "text": "Bounding gradients through induction   The last challenge comes from the potential unbounded gradients\u2019 norm. Plugging the above estimations into (20), we obtain that ", "page_idx": 8}, {"type": "equation", "text": "$$\nf(y_{t+1})\\leq f(x_{1})+\\left(\\frac{\\mathcal{G}_{T}(t)}{4\\mathcal{G}_{T}}-\\frac{1}{2}\\right)\\sum_{s=1}^{t}\\eta_{s}\\left\\Vert\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\Vert^{2}+c_{1}\\mathcal{G}_{T}+(c_{2}\\mathcal{G}_{T}(t)+c_{3})\\log\\left(\\frac{T}{\\beta_{2}^{T}}\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $c_{1},c_{2},c_{3}$ are constants determined by $\\beta_{1},\\beta_{2},d,L,\\eta$ . Then, we will first show that $G_{1}\\leq G$ and suppose that for some $t\\in[T]$ \uff0c ", "page_idx": 8}, {"type": "equation", "text": "$$\nG_{s}\\leq G,\\quad\\forall s\\in[t]\\quad\\mathrm{thus}\\quad\\mathcal{G}_{T}(s)\\leq\\mathcal{G}_{T},\\quad\\forall s\\in[t].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "It's then clear to reveal from (25) and the induction assumption that $f(\\pmb{y}_{t+1})$ is restricted by the first-order of $\\mathcal{G}_{T}$ .Moreover, $f(y_{t+1})-f^{*}$ could be served as the upper bound of $||\\bar{\\pmb{g}}_{t+1}||^{2}$ since ", "page_idx": 8}, {"type": "equation", "text": "$$\n|\\bar{g}_{t+1}||^{2}\\le\\|\\nabla f(y_{t+1})\\|^{2}+\\|\\bar{g}_{t+1}-\\nabla f(y_{t+1})\\|^{2}\\le2L\\big(f(y_{t+1})-f^{*}\\big)+\\|\\bar{g}_{t+1}-\\nabla f(y_{t+1})\\|^{2},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where we use a standard result $\\|\\nabla f(\\mathbf{x})\\|^{2}\\leq2L(f(\\mathbf{x})-f^{*})$ in smooth-based optimization. We also use the smoothness to control $||\\bar{\\boldsymbol{g}}_{t+1}-\\nabla f(\\boldsymbol{y}_{t+1})||^{2}$ and combine with (26) and (27) to derive that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\|\\bar{g}_{t+1}\\|^{2}\\leq\\tilde{d}_{1}+\\tilde{d}_{2}(\\sigma_{1}G^{p/2}+G),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\tilde{d}_{1},\\tilde{d}_{2}$ are constants that are also determined by hyper-parameters and restricted by $\\mathcal{O}(\\log T-$ $T\\log\\beta_{2})$ with respect to $T$ . Then, using Young's inequality, ", "page_idx": 8}, {"type": "equation", "text": "$$\n||\\bar{g}_{t+1}||^{2}\\leq\\frac{G^{2}}{2}+\\tilde{d}_{1}+\\frac{4-p}{4}\\cdot p^{\\frac{p}{4-p}}\\left(\\tilde{d}_{2}\\right)^{\\frac{4}{4-p}}+\\left(\\tilde{d}_{2}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Thus, combining with a proper construction $G^{2}$ (detailed in (52)), we could prove that ", "page_idx": 8}, {"type": "equation", "text": "$$\nG^{2}=2\\tilde{d}_{1}+\\frac{4-p}{2}\\cdot p^{\\frac{p}{4-p}}\\left(\\tilde{d}_{2}\\right)^{\\frac{4}{4-p}}+2\\left(\\tilde{d}_{2}\\right)^{2},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Whichleadsto $\\|\\bar{\\pmb{g}}_{t+1}\\|^{2}\\leq G^{2}$ . Combining with the induction argument, we deduce that $\\|\\bar{\\pmb{g}}_{t}\\|^{2}\\leq$ $G^{2},\\forall t\\in[T+1]$ ", "page_idx": 8}, {"type": "text", "text": "Final estimation Following the induction step for upper bounding the gradients\u2032 norm, we also prove the following result in high probability: ", "page_idx": 9}, {"type": "equation", "text": "$$\nL\\sum_{s=1}^{T}\\frac{\\eta_{s}}{\\|a_{s}\\|_{\\infty}}\\,\\|\\bar{g}_{s}\\|^{2}\\leq L\\sum_{s=1}^{T}\\eta_{s}\\,\\bigg\\|\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\bigg\\|^{2}\\leq G^{2}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "We could rely on $\\mathcal{G}_{T}$ to prove that $\\lVert\\mathbf{{a}}_{s}\\rVert_{\\infty}\\leq\\mathcal{G}_{T}\\sqrt{1-\\beta_{2}^{s}}+\\epsilon_{s},\\forall s\\in[T]$ and then combine with $\\eta_{s}$ in Algorithm 1 to further deduce the desired guarantee for $\\sum_{s=1}^{T}\\|\\bar{\\pmb{g}}_{s}\\|^{2}/T$ ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we investigate the convergence of the Adam optimization algorithm on non-convex smooth problems under certain relaxed conditions. We begin by considering a mild noise assumption that encompasses several noise types, particularly the almost surely affine variance noise. Under this noise condition, we demonstrate that Adam can find a stationary point at a rate of $\\mathcal{O}(\\mathrm{poly}(\\log T)/\\sqrt{T})$ with high probability. Within our framework, we introduce a novel proxy step-size to manage the entanglement of stochastic gradients and adaptive step-sizes, and we employ a new decomposition method to estimate the errors introduced by the proxy step-size, the momentum, and the corrective terms in Adam. ", "page_idx": 9}, {"type": "text", "text": "We also extend our analysis to the convergence of Adam when the objective function is generalized smooth. This relaxed assumption is empirically validated to be more realistic in practical applications. Our results indicate that, with appropriate hyper-parameter tuning, Adam can find a stationary point at the same order of convergence rate as in the smooth case. ", "page_idx": 9}, {"type": "text", "text": "Limitations  Our study has several limitations that warrant further exploration. First, it would be advantageous to provide experimental results to validate the hyper-parameter settings in our results. Second, the convergence bound is not strictly tight compared to the lower bound, leaving a gap involving logarithmic factors, which may be improved in future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the NSFC under grant number 12471096, and the National Key Research and Development Program of China under grant number 2021YFA1003500. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower bounds for non-convex stochastic optimization. Mathematical Programming, 199(1-2):165-214, 2023.   \n[2]  Amit Attia and Tomer Koren. SGD with AdaGrad stepsizes: full adaptivity with high probability to unknown parameters, unbounded gradients and affine variance. In International Conference on Machine Learning,2023.   \n[3]  Dimitri P Bertsekas and John N Tsitsiklis. Gradient convergence in gradient methods with errors. SIAM Journal on Optimization, 10(3):627-642, 2000.   \n[4]  L\u00e9on Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223-311, 2018.   \n[5]  Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.   \n[6]  Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. Closing the generalization gap of adaptive gradient methods in training deep neural networks. arXiv preprint arXiv:1806.06763,2018.   \n[7] Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of Adam-type algorithms for non-convex optimization. In International Conference on Learning Representations, 2019.   \n[8] Michael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, and Zhenxun Zhuang. Robustness to unbounded smoothness of generalized signSGD. In Advances in Neural Information Processing Systems, 2022.   \n[9]  Soham De, Anirbit Mukherjee, and Enayat Ullah. Convergence guarantees for RMSProp and Adam in non-convex optimization and an empirical comparison to Nesterov acceleration. arXiv preprint arXiv: 1807.06766, 2018.   \n[10]  Alexandre Defossez, Leon Bottou, Francis Bach, and Nicolas Usunier. A simple convergence proof of Adam and Adagrad. Transactions on Machine Learning Research, 2022.   \n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[12]  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(7):2121-2159, 2011.   \n[13] Matthew Faw, Litu Rout, Constantine Caramanis, and Sanjay Shakottai. Beyond uniform smoothness: a stopped analysis of adaptive SGD. In Conference on Learning Theory, 2023.   \n[14] Matthew Faw, Isidoros Tziotis, Constantine Caramanis, Aryan Mokhtari, Sanjay Shakkottai, and Rachel Ward. The power of adaptivity in SGD: self-tuning step sizes with unbounded gradients and affine variance. In Conference on Learning Theory, 2022.   \n[15] Wayne A Fuller. Measurement error models. John Wiley & Sons, 2009.   \n[16] Euhanna Ghadimi, Hamid Reza Feyzmahdavian, and Mikael Johansson. Global convergence of the heavy-ball method for convex optimization. In European Control Conference, 2015.   \n[17]  Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.   \n[18] Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. A novel convergence analysis for algorithms of the Adam family. In Annual Workshop on Optimization for Machine Learning, 2021.   \n[19]  Yusu Hong and Junhong Lin. High probability convergence of Adam under unbounded gradients and affne variance noise. arXiv preprint arXiv:2311.02000, 2023.   \n[20] Feihu Huang, Junyi Li, and Heng Huang. Super-Adam: faster and universal framework of adaptive gradients. In Advances in Neural Information Processing Systems, 2021.   \n[21] Fereshte Khani and Pery Liang, Feature noise induce loss discrepancy across grou. In International Conference on Machine Learning, pages 5209-5219. PMLR, 2020.   \n[22]  Diederik P Kingma and Jimmy Ba. Adam: a method for stochastic optimization. In International Conference on Learning Representations, 2015.   \n[23] Frederik Kunstner, Jacques Chen, Jonathan Wilder Lavington, and Mark Schmidt. Noise is not the main factor behind the gap between SGD and Adam on Transformers, but sign descent might be. In The Eleventh International Conference on Learning Representations, 2023.   \n[24] Haochuan Li, Ali Jadbabaie, and Alexander Rakhlin. Convergence of Adam under relaxed assumptions. In Advances in Neural Information Processing Systems, 2023.   \n[25]  Xiaoyu Li and Francesco Orabona. A high probability analysis of adaptive SGD with momentum. In Workshop on International Conference on Machine Learning, 2020.   \n[26] Mingrui Liu, Youssef Mroueh, Jerret Ross, Wei Zhang, Xiaodong Cui, Payel Das, and Tianbao Yang. Towards better understanding of adaptive gradient algorithms in generative adversarial nets. arXiv preprint arXiv:1912.11940, 2019.   \n[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019.   \n[28]  Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574- 1609, 2009.   \n[29] Boris T Polyak. Some methods of speeding up the convergence of iteration methods. Ussr Computational Mathematics and Mathematical Physics, 4(5):1-17, 1964.   \n[30] Jiang Qian, Yuren Wu, Bojin Zhuang, Shaojun Wang, and Jing Xiao. Understanding gradient clipping in incremental gradient methods. In International Conference on Artificial Intelligence and Statistics, 2021.   \n[31] Sashank J. Reddi Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. In International Conference on Learning Representations, 2018.   \n[32]  Amirhossein Reisizadeh, Haochuan Li, Subhro Das, and Ali Jadbabaie. Variance-reduced clipping for non-convex optimization. arXiv preprint arXiv:2303.00883, 2023.   \n[33] Herbert Robbins and Suton Monro. A stochastic approximation method. Annals of Mathematical Statistics, pages 400-407, 1951.   \n[34] Naichen Shi, Dawei Li, Mingyi Hong, and Ruoyu Sun. RMSProp converges with proper hyper-parameter. In International Conference on Learning Representations, 2020.   \n[35] Steve Smale and Yuan Yao. Online learning algorithms. Foundations of Computational Mathematics, 6:145-170, 2006.   \n[36]  Matthew Strter and H Brendan McMahan. Less regret via online conditioning. arXiv preprint arXiv:1002.4862, 2010.   \n[37]  Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.   \n[38]  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Hllia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.   \n[39] Bohan Wang, Jingwen Fu, Huishuai Zhang, Nanning Zheng, and Wei Chen. Closing the gap between the upper bound and lower bound of Adam's iteration complexity. In Advances in Neural Information Processing Systems, 2023.   \n[40] Bohan Wang, Huishuai Zhang, Zhiming Ma, and Wei Chen. Convergence of AdaGrad for non-convex objectives: simple proofs and relaxed assumptions. In Conference on Learning Theory, 2023.   \n[41] Bohan Wang, Yushun Zhang, Huishuai Zhang, Qi Meng, Zhi-Ming Ma, Tie-Yan Liu, and Wei Chen. Provable adaptivity in Adam. arXiv preprint arXiv:2208.09900, 2022.   \n[42] Ruiqi Wang and Diego Klabjan. Divergence results and convergence of a variance reduced version of adam. arXiv preprint arXiv:2210.05607, 2022.   \n[43] Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: sharp convergence over nonconvex landscapes. Journal of Machine Learning Research, 21(1):9047-9076, 2020.   \n[44]  Shuo Xie and Zhiyuan Li.  Implicit bias of adamw: $l_{\\infty}$ -norm constrained optimization. In International Conference on Machine Learning, 2024.   \n[45] Huan Xu, Constantine Caramanis, and Shie Mannor. Robust regression and Lasso. In Advances in Neural Information Processing Systems, 2008.   \n[46]  Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi Yang. A unified analysis of stochastic momentum methods for deep learning. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, 2018.   \n[47]  Yiming Ying and D-X Zhou. Online regularized classification algorithms. IEEE Transactions on Information Theory, 52(11):4775-4788, 2006.   \n[48] Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods for nonconvex optimization. In Advances in Neural Information Processing Systems, 2018.   \n[49] Bohang Zhang, Jikai Jin, Cong Fang, and Liwei Wang. Improved analysis of clipping algorithms for non-convex optimization. In Advances in Neural Information Processing Systems, 2020.   \n[50]  Chenyang Zhang, Difan Zou, and Yuan Cao. The implicit bias of Adam on separable data. In High-dimensional Learning Dynamics 2024: The Emergence of Structure and Reasoning, 2024.   \n[51]  Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: a theoretical justification for adaptivity. In International Conference on Learning Representations, 2020.   \n[52] Yushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo. Adam can converge without any modification on update rules. In Advances in Neural Information Processing Systems, 2022.   \n[53]  Shen- Yi Zhao, Yin-Peng Xie, and Wu-Jun Li. On the convergence and improvement of stochastic normalized gradient descent. Science China Information Sciences, 64:1-13, 2021.   \n[54] Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu. On the convergence of adaptive gradient methods for nonconvex optimization. In Annual Workshop on Optimization for Machine Learning, 2020.   \n[55]  Pan Zhou, Xingyu Xie, and Shuicheng Yan. Win: weight-decay-integrated Nesterov acceleration for adaptive gradient algorithms. In International Conference on Learning Representations, 2023.   \n[56] Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. Understanding the generalization of adam in learning neural networks with proper regularization. arXiv preprint arXiv:2108.11371, 2021.   \n[57] Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A suffcient condition for convergences of Adam and RMSProp. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "The appendix is organized as follows. The next section presents some necessary technical lemmas, some of which have appeared in the previous literature. In Appendix B and Appendix C, the detailed proofs for Theorem 3.1 and Theorem 4.1 are presented respectively. Finally, Appendix $\\mathrm{D}$ and Appendix $\\boldsymbol{\\mathrm E}$ provide all the omitted proofs in previous sections. ", "page_idx": 13}, {"type": "text", "text": "A  Complementary lemmas ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We first provide some necessary technical lemmas as follows. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1. Suppose that $\\{\\alpha_{s}\\}_{s\\ge1}$ is a non-negative sequence. Given $\\beta_{2}\\in(0,1]$ and $\\varepsilon>0$ we define $\\begin{array}{r}{\\theta_{s}=\\sum_{j=1}^{s}\\beta_{2}^{s-j}\\alpha_{j}}\\end{array}$ . Then, for any $t\\geq1$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\frac{\\alpha_{j}}{\\varepsilon+\\theta_{j}}\\le\\log\\left(1+\\frac{\\theta_{t}}{\\varepsilon}\\right)-t\\log\\beta_{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. See the proof of Lemma 5.2 in [10]. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.2. Suppose that $\\{\\alpha_{s}\\}_{s\\ge1}$ is a real number sequence. Given $0\\leq\\beta_{1}<\\beta_{2}\\leq1$ and $\\varepsilon>0$ we define $\\begin{array}{r}{\\zeta_{s}=\\sum_{j=1}^{s}\\beta_{1}^{s-j}\\alpha_{j},\\,\\gamma_{s}=\\frac{1}{1-\\beta_{1}^{s}}\\sum_{j=1}^{s}\\beta_{1}^{s-j}\\alpha_{j}}\\end{array}$ and $\\begin{array}{r}{\\theta_{s}=\\sum_{j=1}^{s}\\beta_{2}^{s-j}\\alpha_{j}^{2}}\\end{array}$ then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}\\frac{\\zeta_{s}^{2}}{\\varepsilon+\\theta_{s}}\\leq\\frac{1}{(1-\\beta_{1})(1-\\beta_{1}/\\beta_{2})}\\left(\\log\\left(1+\\frac{\\theta_{t}}{\\varepsilon}\\right)-t\\log\\beta_{2}\\right),\\quad\\forall t\\geq1,}\\\\ &{\\displaystyle\\sum_{s=1}^{t}\\frac{\\gamma_{s}^{2}}{\\varepsilon+\\theta_{s}}\\leq\\frac{1}{(1-\\beta_{1})^{2}(1-\\beta_{1}/\\beta_{2})}\\left(\\log\\left(1+\\frac{\\theta_{t}}{\\varepsilon}\\right)-t\\log\\beta_{2}\\right),\\quad\\forall t\\geq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. The proof for the first inequality can be found in the proof of Lemma A.2 [10]. For the second result, let $\\begin{array}{r}{\\tilde{M}=\\sum_{j=1}^{s}\\beta_{1}^{s-j}}\\end{array}$ . Then using Jensen's inequality, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\sum_{j=1}^{s}\\beta_{1}^{s-j}\\alpha_{j}\\right)^{2}=\\left(\\hat{M}\\sum_{j=1}^{s}\\frac{\\beta_{1}^{s-j}}{\\hat{M}}\\alpha_{j}\\right)^{2}\\leq\\hat{M}^{2}\\sum_{j=1}^{s}\\frac{\\beta_{1}^{s-j}}{\\hat{M}}\\alpha_{j}^{2}=\\hat{M}\\sum_{j=1}^{s}\\beta_{1}^{s-j}\\alpha_{j}^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Hence, we further have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\gamma_{s}^{2}}{\\varepsilon+\\theta_{s}}\\leq\\frac{\\hat{M}}{(1-\\beta_{1}^{s})^{2}}\\sum_{j=1}^{s}\\beta_{1}^{s-j}\\frac{\\alpha_{j}^{2}}{\\varepsilon+\\theta_{s}}=\\frac{1}{(1-\\beta_{1})(1-\\beta_{1}^{s})}\\sum_{j=1}^{s}\\beta_{1}^{s-j}\\frac{\\alpha_{j}^{2}}{\\varepsilon+\\theta_{s}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Recalling the definition of $\\theta_{s}$ , we have $\\varepsilon+\\theta_{s}\\geq\\varepsilon+\\beta_{2}^{s-j}\\theta_{j}\\geq\\beta_{2}^{s-j}(\\varepsilon+\\theta_{j})$ . Hence, combining with $1-\\beta_{1}\\le1-\\beta_{1}^{s}$ \uff0c ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\frac{\\gamma_{s}^{2}}{\\varepsilon+\\theta_{s}}}\\leq{\\frac{1}{(1-\\beta_{1})(1-\\beta_{1}^{s})}}\\sum_{j=1}^{s}\\left({\\frac{\\beta_{1}}{\\beta_{2}}}\\right)^{s-j}{\\frac{\\alpha_{j}^{2}}{\\varepsilon+\\theta_{j}}}\\leq{\\frac{1}{(1-\\beta_{1})^{2}}}\\sum_{j=1}^{s}\\left({\\frac{\\beta_{1}}{\\beta_{2}}}\\right)^{s-j}{\\frac{\\alpha_{j}^{2}}{\\varepsilon+\\theta_{j}}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Summing up both sides over $s\\in[t]$ , and noting that $\\beta_{1}<\\beta_{2}$ \uff0c ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\frac{\\gamma_{s}^{2}}{\\varepsilon+\\theta_{s}}\\leq\\frac{1}{(1-\\beta_{1})^{2}}\\sum_{s=1}^{t}\\sum_{j=1}^{s}\\left(\\frac{\\beta_{1}}{\\beta_{2}}\\right)^{s-j}\\frac{\\alpha_{j}^{2}}{\\varepsilon+\\theta_{j}}\\leq\\frac{1}{(1-\\beta_{1})^{2}(1-\\beta_{1}/\\beta_{2})}\\sum_{j=1}^{t}\\frac{\\alpha_{j}^{2}}{\\varepsilon+\\theta_{j}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Finally applying Lemma A.1, we obtain the desired result. ", "page_idx": 13}, {"type": "text", "text": "Then, we introduce a standard concentration inequality for the martingale difference sequence that is useful for achieving the high probability bounds, see [25] for a proof. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.3. Suppose $\\{Z_{s}\\}_{s\\in[T]}$ is a martingale difference sequence with respect to $\\zeta_{1},\\cdot\\cdot\\cdot,\\zeta_{T}$ Assume that for each $s\\in[T]$ \uff0c $\\sigma_{s}$ is a random variable only dependent by $\\zeta_{1},\\cdot\\cdot\\cdot\\,,\\zeta_{s-1}$ and satisfies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp(Z_{s}^{2}/\\sigma_{s}^{2})~|~\\zeta_{1},\\cdot\\cdot\\cdot~,\\zeta_{s-1}\\right]\\le\\mathrm{e},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "then for any $\\lambda>0$ . and for any $\\delta\\in(0,1)$ , it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{s=1}^{T}Z_{s}>\\frac{1}{\\lambda}\\log\\left(\\frac{1}{\\delta}\\right)+\\frac{3}{4}\\lambda\\sum_{s=1}^{T}\\sigma_{s}^{2}\\right)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "BProof of Theorem 3.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The detailed proof of Theorem 3.1 corresponds to the proof sketch in Section 6. ", "page_idx": 14}, {"type": "text", "text": "B.1  Preliminary ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To start with, we introduce the following two notations, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{m}_{s}=\\frac{m_{s}}{1-\\beta_{1}^{s}},\\quad\\hat{v}_{s}=\\frac{v_{s}}{1-\\beta_{2}^{s}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which include two corrective terms for $m_{s}$ and $\\pmb{v}_{s}$ . It is easy to see that $\\eta_{s}$ satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\eta_{s}=\\frac{\\eta\\sqrt{1-\\beta_{2}^{s}}}{1-\\beta_{1}^{s}}\\leq\\frac{\\eta}{1-\\beta_{1}^{s}}\\leq\\frac{\\eta}{1-\\beta_{1}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We follow all the notations in Section 6, which we present here for the convenience of reading, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{M}_{T}=\\sqrt{\\log\\bigg(\\frac{\\mathrm{e}T}{\\delta}\\bigg)},\\quad G_{s}=\\underset{j\\in[s]}{\\operatorname*{max}}\\,\\|\\bar{g}_{j}\\|,}\\\\ &{\\mathcal{G}_{T}(s)=\\mathcal{M}_{T}\\sqrt{2\\sigma_{0}^{2}+2\\sigma_{1}^{2}G_{s}^{p}+2G_{s}^{2}},\\quad\\mathcal{G}_{T}=\\mathcal{M}_{T}\\sqrt{2\\sigma_{0}^{2}+2\\sigma_{1}^{2}G^{p}+2G^{2}},}\\\\ &{b_{s}=\\sqrt{\\beta_{2}v_{s-1}+(1-\\beta_{2})g_{s}^{2}}+\\epsilon_{s},}\\\\ &{a_{s}=\\sqrt{\\beta_{2}v_{s-1}+\\left(1-\\beta_{2}\\right)\\left(\\mathcal{G}_{T}(s)\\mathbf{1}_{d}\\right)^{2}}+\\epsilon_{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The following lemmas provide some estimations for the algorithm-dependent terms, which play vital roles in the proof of Theorem 3.1. The detailed proofs could be found in Appendix D.1. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.1. Let $\\eta_{s},b_{s}$ be given in Algorithm 1 and (16), then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\eta_{s}b_{s-1}}{\\eta_{s-1}b_{s}}-\\mathbf{1}_{d}\\right\\|_{\\infty}\\leq\\Sigma_{\\operatorname*{max}}:=\\operatorname*{max}\\left\\{1,\\sqrt{\\frac{1+\\beta_{2}}{\\beta_{2}}}-1\\right\\},\\quad\\forall s\\geq2.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The following lemma could be found similarly in [56, Lemma A.2] ", "page_idx": 14}, {"type": "text", "text": "Lemma B.2. Let $m_{s},b_{s}$ be given in Algorithm $^{\\,l}$ and (16) with $0\\,\\leq\\,\\beta_{1}\\,<\\,\\beta_{2}\\,<\\,1,$ respectively. Then, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\frac{m_{s}}{b_{s}}\\right\\|_{\\infty}\\leq\\sqrt{\\frac{(1-\\beta_{1})(1-\\beta_{1}^{s})}{(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}},\\quad\\forall s\\geq1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Consequently, if $f$ is $L$ -smooth and we set $\\eta=C_{0}\\sqrt{1-\\beta_{2}}$ for some constant $C_{0}>0$ then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\bar{g}_{s}\\|\\leq\\|\\bar{g}_{1}\\|+L C_{0}s\\sqrt{\\frac{d}{1-\\beta_{1}/\\beta_{2}}},\\quad\\forall s\\geq1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The following lemma is necessary for deriving (24) in the proof sketch. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.3. Let $\\mathbf{\\mathit{g}}_{s},m_{s}$ be given in Algorithm $^{\\,l}$ and $\\hat{m}_{s},b_{s}$ be defined in (29) and (16). If $0\\leq\\beta_{1}<\\beta_{2}<1$ and $\\begin{array}{r}{\\mathcal{F}_{i}(t)=1+\\frac{1}{\\epsilon^{2}}\\sum_{s=1}^{t}g_{s,i}^{2},}\\end{array}$ . then for any $t\\geq1$ \uff0c ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\left\\|\\frac{\\boldsymbol{g}_{s}}{b_{s}}\\right\\|^{2}\\leq\\frac{1}{1-\\beta_{2}}\\sum_{i=1}^{d}\\log\\left(\\frac{\\mathcal{F}_{i}(t)}{\\beta_{2}^{t}}\\right),}}\\\\ &{}&{\\sum_{s=1}^{t}\\left\\|\\frac{m_{s}}{b_{s}}\\right\\|^{2}\\leq\\frac{1-\\beta_{1}}{(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}\\sum_{i=1}^{d}\\log\\left(\\frac{\\mathcal{F}_{i}(t)}{\\beta_{2}^{t}}\\right),}\\\\ &{}&{\\sum_{s=1}^{t}\\left\\|\\frac{m_{s}}{b_{s+1}}\\right\\|^{2}\\leq\\frac{1-\\beta_{1}}{\\beta_{2}(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}\\sum_{i=1}^{d}\\log\\left(\\frac{\\mathcal{F}_{i}(t)}{\\beta_{2}^{t}}\\right),}\\\\ &{}&{\\sum_{s=1}^{t}\\left\\|\\frac{\\hat{m}_{s}}{b_{s}}\\right\\|\\leq\\frac{1}{(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}\\sum_{i=1}^{d}\\log\\left(\\frac{\\mathcal{F}_{i}(t)}{\\beta_{2}^{t}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The following lemmas are based on the smooth condition. ", "page_idx": 15}, {"type": "text", "text": "Lemma B.4. Suppose that $f$ is $L$ -smooth and Assumption (Al) holds, then for any $\\pmb{x}\\in\\mathbb{R}^{d}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\mathbf{x})\\|^{2}\\leq2L(f(\\mathbf{x})-f^{*}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma B.5. Let $\\pmb{x}_{s}$ be given in Algorithm $^{\\,l}$ and $\\pmb{y}_{s}$ be defined in (17). If $f$ is $L$ -smooth, $\\eta=$ $C_{0}\\sqrt{1-\\beta_{2}}$ and $0\\leq\\beta_{1}<\\beta_{2}<1$ then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\pmb{x}_{s})\\|\\leq\\|\\nabla f(\\pmb{y}_{s})\\|+M,\\quad M:=\\frac{L C_{0}\\sqrt{d}}{(1-\\beta_{1})\\sqrt{1-\\beta_{1}/\\beta_{2}}},\\quad\\forall s\\geq1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B.2  Start point and decomposition ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Specifically, we fix the horizon $T$ and start from the descent lemma of $L$ -smoothness, ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(\\pmb{y}_{s+1})\\leq f(\\pmb{y}_{s})+\\langle\\nabla f(\\pmb{y}_{s}),\\pmb{y}_{s+1}-\\pmb{y}_{s}\\rangle+\\frac{L}{2}\\|\\pmb{y}_{s+1}-\\pmb{y}_{s}\\|^{2},\\quad\\forall s\\in[T].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For any given $t\\in[T]$ , combining with (18) and (31) and then summing over $s\\in[t]$ , we obtain the same inequality in (20), ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\pmb{y}_{t+1})\\leq f(\\pmb{x}_{1})+\\underbrace{\\sum_{s=1}^{t}{-\\eta_{s}\\left\\langle\\nabla f(y_{s}),\\frac{g_{s}}{b_{s}}\\right\\rangle}+\\underbrace{\\frac{\\beta_{1}}{1-\\beta_{1}}\\sum_{s=1}^{t}{\\left\\langle\\Delta_{s}\\odot(\\pmb{x}_{s}-\\pmb{x}_{s-1}),\\nabla f(y_{s})\\right\\rangle}}_{\\mathbf{A}}}_{\\mathbf{A}}}\\\\ &{\\qquad+\\underbrace{\\frac{L}{2}\\displaystyle\\sum_{s=1}^{t}{\\left\\|\\eta_{s}\\cdot\\frac{g_{s}}{b_{s}}-\\frac{\\beta_{1}}{1-\\beta_{1}}(\\Delta_{s}\\odot(\\pmb{x}_{s}-\\pmb{x}_{s-1}))\\right\\|^{2}}}_{\\mathbf{C}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "whereweuse $\\Delta_{s}$ in (20) and ${\\pmb y}_{1}={\\pmb x}_{1}$ . We then further make a decomposition by introducing $\\bar{\\pmb{g}}_{s}$ into $\\mathbf{A}$ and $\\mathbf{B}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{A}=\\underbrace{\\sum_{s=1}^{t}{-\\eta_{s}\\left\\langle\\bar{g}_{s},\\frac{g_{s}}{b_{s}}\\right\\rangle}}_{\\mathrm{A.1}}+\\underbrace{\\sum_{s=1}^{t}\\eta_{s}\\left\\langle\\bar{g}_{s}-\\nabla f(y_{s}),\\frac{g_{s}}{b_{s}}\\right\\rangle}_{\\mathrm{A.2}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{B}=\\frac{\\beta_{1}}{1-\\beta_{1}}\\sum_{s=1}^{t}\\left\\langle\\Delta_{s}\\odot\\left(x_{s}-x_{s-1}\\right),\\bar{g}_{s}\\right\\rangle+\\frac{\\beta_{1}}{1-\\beta_{1}}\\sum_{s=1}^{t}\\left\\langle\\Delta_{s}\\odot\\left(x_{s}-x_{s-1}\\right),\\nabla f(y_{s})-\\bar{g}_{s}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B.3 Probabilistic estimations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We will provide two probabilistic inequalities with the detailed proofs given in Appendix D.2. The first one establishes an upper bound for the noise norm, which we have already informally presented in (21). ", "page_idx": 16}, {"type": "text", "text": "Lemma B.6. Given $T\\geq1$ suppose that for any $s\\in[T].$ $\\pmb{\\xi}_{s}=\\pmb{g}_{s}-\\bar{\\pmb{g}}_{s}$ satisfies Assumption (A3). Then for any given $\\delta\\in(0,1)$ , it holds that with probability at least $1-\\delta$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\pmb{\\xi}_{s}\\|^{2}\\leq\\mathcal{M}_{T}^{2}\\left(\\sigma_{0}^{2}+\\sigma_{1}^{2}\\|\\bar{\\pmb{g}}_{s}\\|^{p}\\right),\\quad\\forall s\\in[T].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We next provide a probabilistic upper bound as shown in (22) for a summation of the inner product, where we rely on the property of the martingale difference sequence and the proxy step-size $\\pmb{a}_{s}$ in (23). ", "page_idx": 16}, {"type": "text", "text": "Lemma B.7. Given $T\\geq1$ and $\\delta\\in(0,1)$ . If Assumptions (A2) and (A3) hold, then for any $\\lambda>0$ with probability at least $1-\\delta$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n-\\sum_{s=1}^{t}\\eta_{s}\\left\\langle\\bar{g}_{s},\\frac{\\xi_{s}}{a_{s}}\\right\\rangle\\leq\\frac{3\\lambda\\eta\\mathcal{G}_{T}(t)}{4(1-\\beta_{1})\\sqrt{1-\\beta_{2}}}\\sum_{s=1}^{t}\\eta_{s}\\left\\Vert\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\Vert^{2}+\\frac{d}{\\lambda}\\log\\left(\\frac{d T}{\\delta}\\right),\\quad\\forall t\\in[T].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As a consequence, when setting $\\lambda=(1-\\beta_{1})\\sqrt{1-\\beta_{2}}/(3\\eta\\mathcal{G}_{T}),$ it holds that with probability at least $1-\\delta$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n-\\sum_{s=1}^{t}\\eta_{s}\\left\\langle\\bar{g}_{s},\\frac{\\xi_{s}}{a_{s}}\\right\\rangle\\leq\\frac{\\mathcal{G}_{T}(t)}{4\\mathcal{G}_{T}}\\sum_{s=1}^{t}\\eta_{s}\\left\\Vert\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\Vert^{2}+D_{1}\\mathcal{G}_{T},\\quad\\forall t\\in[T],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$\\begin{array}{r}{D_{1}=\\frac{3\\eta}{(1-\\beta_{1})\\sqrt{1-\\beta_{2}}}\\log\\left(\\frac{T}{\\delta}\\right)}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "B.4 Deterministic estimations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we shall assume that (35) or/and (37) hold whenever the related estimation is needed.   \nThen we obtain the following key lemmas with the detailed proofs given in Appendix D.3. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.8. Given $T\\geq1$ If (35) holds, then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in[s]}\\|\\xi_{j}\\|\\leq\\mathcal{G}_{T}(s),\\quad\\operatorname*{max}_{j\\in[s]}\\|g_{j}\\|\\leq\\mathcal{G}_{T}(s),\\quad\\operatorname*{max}_{j\\in[s]}\\|v_{j}\\|_{\\infty}\\leq(\\mathcal{G}_{T}(s))^{2}\\,,\\quad\\forall s\\in[T].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma B.9. Given $T\\geq1$ f $\\boldsymbol{b}_{s}=(b_{s,i})_{i}$ and $\\pmb{a}_{s}=(a_{s,i})_{i}$ follow the definitions in (16) and (23) respectively, and (35) holds, thenforall $s\\in[T],i\\in[d]$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{a_{s,i}}-\\frac{1}{b_{s,i}}\\right|\\leq\\frac{\\mathcal{G}_{T}(s)\\sqrt{1-\\beta_{2}}}{a_{s,i}b_{s,i}}\\quad a n d\\quad\\left|\\frac{1}{a_{s,i}}-\\frac{1}{b_{s-1,i}}\\right|\\leq\\frac{(\\mathcal{G}_{T}(s)+\\epsilon)\\,\\sqrt{1-\\beta_{2}}}{a_{s,i}b_{s-1,i}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma B.10. Given $T\\geq1$ .Under the conditions in Lemma $B.3$ and Lemma B.5, $i f$ (35) holds, then the following inequality holds, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{F}_{i}(t)\\leq\\mathcal{F}(T),\\quad\\forall t\\in[T],i\\in[d],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\hat{M}=M(1-\\beta_{1})$ and $M$ follows the definition in Lemma B.5, ${\\mathcal{F}}(T)$ is define by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{F}(T):=1+\\frac{2\\mathcal{N}_{T}^{2}}{\\epsilon^{2}}\\left[\\sigma_{0}^{2}T+\\sigma_{1}^{2}T\\left(\\lVert\\bar{g}_{1}\\rVert+T\\hat{M}\\right)^{p}+T\\left(\\lVert\\bar{g}_{1}\\rVert+T\\hat{M}\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We move to estimate all the related terms in Appendix B.2. First, the estimation for A.1 relies on both the two probabilistic estimations in Appendix B.3. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.11. Given $T\\geq1$ suppose that (35) and (37) hold. Then for all $t\\in[T]$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{A.1}\\leq\\left(\\frac{\\mathcal{G}_{T}(t)}{4\\mathcal{G}}-\\frac{3}{4}\\right)\\sum_{s=1}^{t}\\eta_{s}\\left\\Vert\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\Vert^{2}+D_{1}\\mathcal{G}_{T}+D_{2}\\mathcal{G}_{T}(t)\\sum_{s=1}^{t}\\left\\Vert\\frac{g_{s}}{b_{s}}\\right\\Vert^{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where Di is given as in Lemma B.7 and D2 = n2. ", "page_idx": 16}, {"type": "text", "text": "We also obtain the following lemma to estimate the adaptive momentum part B.1. ", "page_idx": 17}, {"type": "text", "text": "Lemma B.12. Given $T\\geq1$ $i f$ (35) holds, then for all $t\\in[T]$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{B.1}\\leq\\frac{1}{4}\\sum_{s=1}^{t}\\eta_{s}\\left\\Vert\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\Vert^{2}+\\left(D_{3}\\mathcal{G}_{T}(t)+D_{4}\\right)\\sum_{s=1}^{t}\\left(\\left\\Vert\\frac{m_{s-1}}{b_{s}}\\right\\Vert^{2}+\\left\\Vert\\frac{m_{s-1}}{b_{s-1}}\\right\\Vert^{2}\\right)+D_{5}G_{t},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{3}=\\frac{2\\eta\\sqrt{1-\\beta_{2}}}{(1-\\beta_{1})^{3}},\\quad D_{4}=\\epsilon D_{3},\\quad D_{5}=\\frac{2\\eta\\sqrt{d}}{\\sqrt{(1-\\beta_{1})^{3}(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proposition B.13. Given $T\\geq1$ If $f$ is $L$ -smooth, then the following inequality holds, ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(y_{t+1})\\leq f(\\ensuremath{\\mathbf{{x}}}_{1})+\\mathbf{A.1}+\\mathbf{B.1}+D_{6}\\sum_{s=1}^{t-1}\\left\\|\\frac{\\hat{m}_{s}}{b_{s}}\\right\\|^{2}+D_{7}\\sum_{s=1}^{t}\\left\\|\\frac{g_{s}}{b_{s}}\\right\\|^{2},\\quad\\forall t\\in[T],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\Sigma_{\\mathrm{max}}$ is as in Lemma B.1 and ", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{6}=\\frac{L\\eta^{2}(1+4\\Sigma_{\\mathrm{max}}^{2})}{2(1-\\beta_{1})^{2}},\\quad D_{7}=\\frac{3L\\eta^{2}}{2(1-\\beta_{1})^{2}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Recalling the decomposition in Appendix B.2. We first estimate A.2. Using the smoothness Of $f$ and (17), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\pmb{\\mathscr{y}}_{s})-\\bar{\\pmb{g}}_{s}\\|\\leq L\\|\\pmb{\\mathscr{y}}_{s}-\\pmb{\\mathscr{x}}_{s}\\|=\\frac{L\\beta_{1}}{1-\\beta_{1}}\\|\\pmb{\\mathscr{x}}_{s}-\\pmb{\\mathscr{x}}_{s-1}\\|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, applying Young's inequality, (43) and (30) ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\eta_{s}\\left<\\bar{g}_{s}-\\nabla f(y_{s}),\\frac{g_{s}}{b_{s}}\\right>\\leq\\eta_{s}\\|\\bar{g}_{s}-\\nabla f(y_{s})\\|\\cdot\\left\\|\\frac{g_{s}}{b_{s}}\\right\\|}\\\\ &{\\leq\\,\\displaystyle\\frac{1}{2L}\\|\\bar{g}_{s}-\\nabla f(y_{s})\\|^{2}+\\frac{L\\eta_{s}^{2}}{2}\\left\\|\\frac{g_{s}}{b_{s}}\\right\\|^{2}\\leq\\frac{L\\beta_{1}^{2}}{2(1-\\beta_{1})^{2}}\\|x_{s}-x_{s-1}\\|^{2}+\\frac{L\\eta^{2}}{2(1-\\beta_{1})^{2}}\\left\\|\\frac{g_{s}}{b_{s}}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Recalling the updated rule in Algorithm 1 and applying (29) as well as (30) ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lVert x_{s}-x_{s-1}\\rVert^{2}=\\eta_{s-1}^{2}\\left\\lVert\\frac{m_{s-1}}{b_{s-1}}\\right\\rVert^{2}\\leq\\eta^{2}\\left\\lVert\\frac{\\hat{m}_{s-1}}{b_{s-1}}\\right\\rVert^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, applying (44), (45) and $\\beta_{1}\\in[0,1)$ , and then summing over $s\\in[t]$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{A.2}\\leq\\frac{L\\eta^{2}}{2(1-\\beta_{1})^{2}}\\sum_{s=1}^{t}\\left\\|\\frac{\\hat{m}_{s-1}}{b_{s-1}}\\right\\|^{2}+\\frac{L\\eta^{2}}{2(1-\\beta_{1})^{2}}\\sum_{s=1}^{t}\\left\\|\\frac{g_{s}}{b_{s}}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Applying Cauchy-Schwarz inequality, Lemma B.1, and combining with (43), (45), $\\Sigma_{\\operatorname*{max}}\\geq1$ ,and $\\beta_{1}\\in[0,1)$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbf{B.2}\\,\\leq\\displaystyle\\frac{\\beta_{1}}{1-\\beta_{1}}\\sum_{s=1}^{t}\\left\\|\\Delta_{s}\\right\\|_{\\infty}\\left\\|x_{s}-x_{s-1}\\right\\|\\left\\|\\nabla f(y_{s})-\\bar{g}_{s}\\right\\|}\\\\ {\\displaystyle\\leq\\frac{L\\beta_{1}^{2}\\Sigma_{\\operatorname*{max}}}{(1-\\beta_{1})^{2}}\\sum_{s=1}^{t}\\left\\|x_{s}-x_{s-1}\\right\\|^{2}\\leq\\displaystyle\\frac{L\\Sigma_{\\operatorname*{max}}^{2}\\eta^{2}}{(1-\\beta_{1})^{2}}\\sum_{s=1}^{t}\\left\\|\\frac{\\hat{m}_{s-1}}{b_{s-1}}\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, applying the basic inequality, Lemma B.1 and (45), ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\bf{C}}\\leq L\\displaystyle\\sum_{s=1}^{t}\\eta_{s}^{2}\\left\\|\\frac{g_{s}}{b_{s}}\\right\\|^{2}+\\frac{L\\beta_{1}^{2}}{(1-\\beta_{1})^{2}}\\sum_{s=1}^{t}\\|\\Delta_{s}\\|_{\\infty}^{2}\\,\\|x_{s}-x_{s-1}\\|^{2}}\\\\ {\\leq\\frac{L\\eta^{2}}{(1-\\beta_{1})^{2}}\\displaystyle\\sum_{s=1}^{t}\\left\\|\\frac{g_{s}}{b_{s}}\\right\\|^{2}+\\frac{L\\eta^{2}\\Sigma_{\\operatorname*{max}}^{2}}{(1-\\beta_{1})^{2}}\\displaystyle\\sum_{s=1}^{t}\\left\\|\\frac{\\hat{m}_{s-1}}{b_{s-1}}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Recalling the decomposition in (33) and (34), then plugging (46), (47) and (48) into (32), we obtain the desired result. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B.5  Bounding gradients ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Based on all the results in Appendix B.3 and Appendix B.4, we are now ready to provide a global upper bound for gradients\u2032 norm along the optimization trajectory. ", "page_idx": 18}, {"type": "text", "text": "Proposition B.14. Under the same conditions in Theorem 3.1, for any given $\\delta\\in(0,1/2)$ it holds thatwithprobabilityatleast $1-2\\delta$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\bar{g}_{t}\\|^{2}\\leq G_{t}^{2}\\leq G^{2},\\quad\\|g_{t}\\|^{2}\\leq(\\mathcal{G}_{T}(t))^{2}\\leq\\mathcal{G}_{T}^{2},\\quad\\forall t\\in[T+1],}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\bar{g}_{t+1}\\|^{2}\\leq G^{2}-L\\sum_{s=1}^{t}\\eta_{s}\\left\\|\\frac{\\bar{g}_{s}}{\\sqrt{\\pmb{a}_{s}}}\\right\\|^{2},\\quad\\forall t\\in[T],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $G^{2}$ is as in Theorem 3.1 and $G_{t},G,\\mathcal{G}_{T}$ are given by (19). ", "page_idx": 18}, {"type": "text", "text": "Proof. Applying Lemma B.6 and Lemma B.7, we know that (35) or (37) hold with probability at least $1-\\delta$ . With these two inequalities, we could deduce the desired inequalities (49) and (50). Therefore, (49) and (50) hold with probability at least $1-2\\delta$ . We first plug (39) and (40) into the result in Proposition B.13, which leads to that for all $t\\in[T]$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(y_{t+1})\\le f(x_{1})+\\left(\\frac{\\mathcal{G}_{T}(t)}{4\\mathcal{G}_{T}}-\\frac{1}{2}\\right)\\displaystyle\\sum_{s=1}^{t}\\eta_{s}\\left\\|\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\|^{2}+D_{1}\\mathcal{G}_{T}+(D_{2}\\mathcal{G}_{T}(t)+D_{7})\\displaystyle\\sum_{s=1}^{t}\\left\\|\\frac{g_{s}}{b_{s}}\\right\\|^{2}}\\\\ &{\\qquad\\qquad+\\left(D_{3}\\mathcal{G}_{T}(t)+D_{4}\\right)\\displaystyle\\sum_{s=1}^{t}\\left(\\left\\|\\frac{m_{s-1}}{b_{s}}\\right\\|^{2}+\\left\\|\\frac{m_{s-1}}{b_{s-1}}\\right\\|^{2}\\right)+D_{5}G_{t}+D_{6}\\displaystyle\\sum_{s=1}^{t-1}\\left\\|\\frac{\\hat{m}_{s}}{b_{s}}\\right\\|^{2}.\\quad(1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, we will introduce the induction argument based on (51). We first provide the specific definition of $G^{2}$ as follows which is a constant determined by the horizon $T$ and other hyper-parameters but not relying on $t$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G^{2}:=8L(f(\\pmb{x}_{1})-f^{*})+\\frac{48\\mathcal{M}_{T}L C_{0}\\sigma_{0}}{1-\\beta_{1}}\\log\\left(\\frac{T}{\\delta}\\right)+\\frac{16\\mathcal{M}_{T}L C_{0}\\sigma_{0}d}{1-\\beta_{1}}\\log\\left(\\frac{\\mathcal{F}(T)}{\\beta_{2}^{T}}\\right)}\\\\ &{\\qquad+8\\left(\\frac{3L C_{0}+8(\\mathcal{M}_{T}\\sigma_{0}+\\epsilon_{0})}{\\beta_{2}}\\right)\\frac{L C_{0}d}{(1-\\beta_{1})^{2}(1-\\beta_{1}/\\beta_{2})}\\log\\left(\\frac{\\mathcal{F}(T)}{\\beta_{2}^{T}}\\right)}\\\\ &{\\qquad+\\frac{4-p}{2}\\cdot p^{\\frac{p}{4-p}}\\left[\\frac{72\\mathcal{M}_{T}L\\sigma_{1}C_{0}d}{\\beta_{2}(1-\\beta_{1})^{2}(1-\\beta_{1}/\\beta_{2})}\\log\\left(\\frac{T+\\mathcal{F}(T)}{\\delta\\beta_{2}^{T}}\\right)\\right]^{\\frac{4}{4-p}}}\\\\ &{\\qquad+32\\left[\\frac{18\\mathcal{M}_{T}L C_{0}d}{\\beta_{2}(1-\\beta_{1})^{2}(1-\\beta_{1}/\\beta_{2})}\\log\\left(\\frac{T+\\mathcal{F}(T)}{\\delta\\beta_{2}^{T}}\\right)\\right]^{2}+\\frac{4L^{2}C_{0}^{2}d}{(1-\\beta_{1})^{2}(1-\\beta_{1}/\\beta_{2})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The induction then begins by noting that $G_{1,-}^{2}=\\|\\bar{\\pmb{{g}}}_{1}\\|^{2}\\leq2L(f(\\pmb{{x}}_{1})-f^{*})\\leq G^{2}$ from Lemma B.4 and (52). Then we assume that for some $t\\in[T]$ \uff0c ", "page_idx": 18}, {"type": "equation", "text": "$$\nG_{s}\\leq G,\\quad\\forall s\\in[t]\\quad\\mathrm{consequently}\\quad\\mathcal{G}_{T}(s)\\leq\\mathcal{G}_{T},\\quad\\forall s\\in[t].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using this induction assumption over (51) and subtracting with $f^{*}$ on both sides, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(y_{t+1})-f^{*}\\leq f(x_{1})-f^{*}-\\frac{1}{4}\\displaystyle\\sum_{s=1}^{t}\\eta_{s}\\left\\|\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\|^{2}+D_{1}\\mathcal{G}_{T}+(D_{2}\\mathcal{G}_{T}+D_{7})\\displaystyle\\sum_{s=1}^{t}\\left\\|\\frac{\\bar{g}_{s}}{b_{s}}\\right\\|^{2}}\\\\ &{\\qquad\\qquad+\\left(D_{3}\\mathcal{G}_{T}+D_{4}\\right)\\displaystyle\\sum_{s=1}^{t}\\left(\\left\\|\\frac{m_{s-1}}{b_{s}}\\right\\|^{2}+\\left\\|\\frac{m_{s-1}}{b_{s-1}}\\right\\|^{2}\\right)+D_{5}G+D_{6}\\displaystyle\\sum_{s=1}^{t-1}\\left\\|\\frac{\\hat{m}_{s}}{b_{s}}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Further, we combine with Lemma B.3 and Lemma B.10 to estimate the four summations defined in Lemma B.3, and then use $G\\leq\\mathcal{G}_{T}\\leq2\\mathsf{M}_{T}\\left(\\sigma_{0}+\\sigma_{1}G^{p/2}+G\\right)$ to control the RHS of (54), ", "page_idx": 18}, {"type": "equation", "text": "$$\nf({\\pmb y}_{t+1})-f^{*}\\leq-\\;\\frac{1}{4}\\sum_{s=1}^{t}\\eta_{s}\\left\\|\\frac{\\bar{g}_{s}}{\\sqrt{{\\pmb a}_{s}}}\\right\\|^{2}+\\tilde{D}_{1}+\\tilde{D}_{2}+\\tilde{D}_{3}\\mathcal{H}(G),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "4We further deduce (7) in Theorem 3.1 based on (52). ", "page_idx": 18}, {"type": "text", "text": "where $\\mathcal{H}(G)=\\sigma_{1}G^{p/2}+G$ and $\\tilde{D}_{1},\\tilde{D}_{2},\\tilde{D}_{3}$ are defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathsf{J}}_{1}=f({\\boldsymbol{x}}_{1})-f^{*}+2\\mathsf{M}_{T}\\sigma_{0}D_{1},}\\\\ &{\\hat{\\mathsf{J}}_{2}=\\left[\\frac{2\\mathsf{M}\\mathsf{T}\\sigma_{0}D_{2}+D_{7}}{1-\\beta_{2}}+\\frac{4\\left(\\mathsf{M}_{T}\\sigma_{0}D_{3}+D_{4}\\right)\\left(1-\\beta_{1}\\right)}{\\beta_{2}\\left(1-\\beta_{2}\\right)\\left(1-\\beta_{1}/\\beta_{2}\\right)}+\\frac{D_{6}}{\\left(1-\\beta_{2}\\right)\\left(1-\\beta_{1}/\\beta_{2}\\right)}\\right]d\\log\\left(\\frac{\\mathcal{F}(T)}{\\beta_{2}^{T}}\\right),}\\\\ &{\\hat{\\mathsf{J}}_{3}=2\\mathsf{M}_{T}\\left[D_{1}+\\left(\\frac{D_{2}d}{1-\\beta_{2}}+\\frac{2D_{3}\\left(1-\\beta_{1}\\right)d}{\\beta_{2}\\left(1-\\beta_{2}\\right)\\left(1-\\beta_{1}/\\beta_{2}\\right)}\\right)\\log\\left(\\frac{\\mathcal{F}(T)}{\\beta_{2}^{T}}\\right)\\right]+D_{5}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Applying Lemma B.5 and Lemma B.4, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lVert\\bar{g}_{t+1}\\rVert^{2}\\leq2\\lVert\\nabla f(y_{t+1})\\rVert^{2}+2M^{2}\\leq4L(f(y_{t+1})-f^{*})+2M^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then combining (55) with (56), ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\bar{g}_{t+1}\\|^{2}\\leq-L\\sum_{s=1}^{t}\\eta_{s}\\left\\|\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\|^{2}+4L(\\tilde{D}_{1}+\\tilde{D}_{2})+4L\\tilde{D}_{3}\\mathcal{H}(G)+2M^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Applying two Young's inequalities where $\\begin{array}{r}{a b\\leq\\frac{a^{2}}{2}+\\frac{b^{2}}{2}}\\end{array}$ and $\\begin{array}{r}{a b^{\\frac{p}{2}}\\leq\\frac{4-p}{4}\\cdot a^{\\frac{4}{4-p}}+\\frac{p}{4}\\cdot b^{2},\\forall a,b\\geq0,}\\end{array}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|{\\bar{g}_{t+1}}\\right\\|^{2}\\leq\\displaystyle\\frac{G^{2}}{4}+\\frac{G^{2}}{4}-L\\sum_{s=1}^{t}\\eta_{s}\\left\\|\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\|^{2}+4L(\\tilde{D}_{1}+\\tilde{D}_{2})}\\\\ {+16L^{2}\\tilde{D}_{3}^{2}+\\displaystyle\\frac{4-p}{4}\\cdot p^{\\frac{p}{4-p}}\\left(4L\\sigma_{1}\\tilde{D}_{3}\\right)^{\\frac{4}{4-p}}+2M^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recalling the definitions of $D_{i},i\\in[7]$ in (37), (39), (41), and (42). With a simple calculation relying on $\\eta=\\bar{C}_{0}\\sqrt{1-\\beta_{2}},\\epsilon\\leq\\epsilon_{0},\\Sigma_{\\mathrm{max}}\\overset{.}{\\leq}1/\\sqrt{\\beta_{2}}$ and $0\\leq\\beta_{1}<\\beta_{2}<1$ , we could deduce that $G^{2}$ given in (52) satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\nG^{2}=8L(\\tilde{D}_{1}+\\tilde{D}_{2})+32L^{2}\\tilde{D}_{3}^{2}+\\frac{4-p}{2}\\cdot p^{\\frac{p}{4-p}}\\left(4L\\sigma_{1}\\tilde{D}_{3}\\right)^{\\frac{4}{4-p}}+4M^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Based on (57) and (58), we then deduce that $\\|\\bar{\\pmb{g}}_{t+1}\\|^{2}\\leq G^{2}$ . Further combining with $G_{t+1}$ in (19) and the induction assumption in (53), ", "page_idx": 19}, {"type": "equation", "text": "$$\nG_{t+1}\\leq\\operatorname*{max}\\{\\|\\bar{g}_{t+1}\\|,G_{t}\\}\\leq G.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, the induction is complete and we obtain the desired result in (49). Furthermore, as a consequence of (57), we also prove that (50) holds. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "B.6 Proof of the main result ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Now we are ready to prove the main convergence result. ", "page_idx": 19}, {"type": "text", "text": "Proof of Theorem 3.1. We set $t=T$ in (50) to obtain that with probability at least $1-2\\delta$ ", "page_idx": 19}, {"type": "equation", "text": "$$\nL\\sum_{s=1}^{T}\\frac{\\eta_{s}}{\\|a_{s}\\|_{\\infty}}\\left\\|\\bar{g}_{s}\\right\\|^{2}\\leq L\\sum_{s=1}^{T}\\eta_{s}\\left\\|\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\|^{2}\\leq G^{2}-\\left\\|\\bar{g}_{T+1}\\right\\|^{2}\\leq G^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, in what follows, we will assume that both (49) and (59) hold. Based on these two inequalities, we could derive the final convergence bound. Since (49) and (59) hold with probability at least $1-2\\delta$ the final convergence bound also holds with probability at least $1-2\\delta$ .Applying $\\pmb{a}_{s}$ in (23) and (49), wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Vert a_{s}\\Vert_{\\infty}=\\underset{i\\in[d]}{\\operatorname*{max}}\\sqrt{\\beta_{2}v_{s-1,i}+(1-\\beta_{2})(\\mathcal{G}_{T}(s))^{2}}+\\epsilon_{s}}&{}\\\\ &{\\qquad\\le\\underset{i\\in[d]}{\\operatorname*{max}}\\sqrt{(1-\\beta_{2})\\left[\\displaystyle\\sum_{j=1}^{s-1}\\beta_{2}^{s-j}g_{j,i}^{2}+(\\mathcal{G}_{T}(s))^{2}\\right]}+\\epsilon_{s}}&{}\\\\ &{\\qquad\\le\\sqrt{(1-\\beta_{2})\\displaystyle\\sum_{j=1}^{s}\\beta_{2}^{s-j}g_{T}^{2}}+\\epsilon_{s}=\\mathcal{G}_{T}\\sqrt{1-\\beta_{2}^{s}}+\\epsilon_{s},\\quad\\forall s\\in[T].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then combining with the setting $\\eta_{s}$ and $\\epsilon_{s}$ in (6), we have for any $s\\in[T]$ \uff0c ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\eta_{s}}{\\|a_{s}\\|_{\\infty}}\\ge\\frac{C_{0}\\sqrt{(1-\\beta_{2}^{s})(1-\\beta_{2})}}{\\mathscr{G}_{T}\\sqrt{1-\\beta_{2}^{s}}+\\epsilon_{0}\\sqrt{(1-\\beta_{2}^{s})(1-\\beta_{2})}}\\cdot\\frac{1}{1-\\beta_{1}^{s}}\\ge\\frac{C_{0}\\sqrt{1-\\beta_{2}}}{\\mathscr{G}_{T}+\\epsilon_{0}\\sqrt{1-\\beta_{2}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We therefore combine with (59) to obtain that with probability at least $1-2\\delta$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{s=1}^{T}\\|\\bar{g}_{s}\\|^{2}\\leq\\frac{G^{2}}{T L C_{0}}\\left(\\frac{\\sqrt{2\\sigma_{0}^{2}+2\\sigma_{1}^{2}G^{p}+2G^{2}}}{\\sqrt{1-\\beta_{2}}}+\\epsilon_{0}\\right)\\sqrt{\\log\\left(\\frac{\\mathrm{e}T}{\\delta}\\right)}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\beta_{2}\\in(0,1)$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n-\\log\\beta_{2}=\\log\\left(\\frac{1}{\\beta_{2}}\\right)\\le\\frac{1-\\beta_{2}}{\\beta_{2}}=\\frac{c}{T\\beta_{2}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we apply $\\log(1/a)\\leq(1-a)/a,\\forall a\\in(0,1)$ . With both sides multiplying $T$ , we obtain that $\\log\\left(1/\\beta_{2}^{T}\\right)^{-}\\!\\!\\le\\bar{c}/\\bar{\\beta_{2}}$ . Then, we further have that when $\\beta_{2}=1-c/T$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\log\\left(\\frac{T}{\\beta_{2}^{T}}\\right)\\leq\\log T+\\frac{c}{\\beta_{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $0\\leq\\beta_{1}<\\beta_{2}<1$ , there exists some constants $\\varepsilon_{1},\\varepsilon_{2}>0$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{\\beta_{2}}\\leq\\frac{1}{\\varepsilon_{1}},\\quad\\frac{1}{1-\\beta_{1}/\\beta_{2}}\\leq\\frac{1}{\\varepsilon_{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore combining (62), (63) and (52), we could verify that $G^{2}\\sim\\mathcal{O}\\left(\\mathrm{poly}(\\log T)\\right)$ with respect to $T$ . Finally, using the convergence result in (61), we obtain the desired result. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "C Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we shall follow all the notations defined in Section 6. Further, we will add two non-decreasing sequences $\\{\\mathcal{L}_{s}^{(x)}\\}_{s\\geq1}$ and $\\{\\mathcal{L}_{s}^{(y)}\\}_{s\\geq1}$ asfolows ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{s}^{(x)}=L_{0}+L_{q}G_{s}^{q},\\quad{\\mathcal{L}}_{s}^{(y)}=L_{0}+L_{q}(G_{s}+G_{s}^{q}+L_{0}/L_{q})^{q},\\quad\\forall s\\geq1.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C.1 Preliminary ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We first mention that Lemma B.1, Lemma B.2, and Lemma B.3 in Appendix B.1 remain unchanged since they are independent of the smooth condition. Then the first essential challenge is that we need toproperlytune $\\eta$ to restrict the distance between $\\pmb{x}_{s+1}$ and $\\pmb{x}_{s}$ \uff0c $\\pmb{y}_{s+1}$ and $\\pmb{y}_{s}$ within $1/L_{q}$ for all $s\\geq1$ . The following two lemmas then ensure this point. The detailed proofs could be found in Appendix $\\boldsymbol{\\mathrm E}$ ", "page_idx": 20}, {"type": "text", "text": "Lemma C.1. Let $\\mathbf{\\mathit{x}}_{s},\\mathbf{\\mathit{y}}_{s}$ be defined in Algorithm $^{\\,l}$ and (17). 1f $0\\leq\\beta_{1}<\\beta_{2}<1,$ . then for any $s\\geq1$ \uff0c ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{\\|x_{s+1}-x_{s}\\|,\\|y_{s}-x_{s}\\|,\\|y_{s+1}-y_{s}\\|\\}\\leq\\eta\\sqrt{\\frac{4d}{\\beta_{2}(1-\\beta_{1})^{2}(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As a consequence, when ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{1}{L_{q}F},\\quad F:=\\sqrt{\\frac{4d}{\\beta_{2}(1-\\beta_{1})^{2}(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "then for any $s\\geq1$ all the three gaps in (65) are smaller than $1/L_{q}$ ", "page_idx": 20}, {"type": "text", "text": "Lemma C.2. Let $\\eta\\leq1/(L_{q}F)$ where $F$ is as in Lemma C.1. If $f$ is $(L_{0},L_{q})$ -smooth, then for any $s\\geq1$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla f(\\pmb{y}_{s})\\|\\le L_{0}/L_{q}+\\|\\nabla f(\\pmb{x}_{s})\\|^{q}+\\|\\nabla f(\\pmb{x}_{s})\\|,}\\\\ {\\|\\nabla f(\\pmb{x}_{s})\\|\\le L_{0}/L_{q}+\\|\\nabla f(\\pmb{y}_{s})\\|^{q}+\\|\\nabla f(\\pmb{y}_{s})\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As a consequence, for any $s\\geq1$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\nabla f(y_{s})-\\nabla f(x_{s})\\|\\leq\\mathcal{L}_{s}^{(x)}\\|y_{s}-x_{s}\\|,}&{\\|\\nabla f(y_{s+1})-\\nabla f(y_{s})\\|\\leq\\mathcal{L}_{s}^{(y)}\\|y_{s+1}-y_{s}\\|,}\\\\ &{\\quad\\quad\\quad\\quad f(y_{s+1})-f(y_{s})-\\langle\\nabla f(y_{s}),y_{s+1}-y_{s}\\rangle\\leq\\displaystyle\\frac{\\mathcal{L}_{s}^{(y)}}{2}\\|y_{s+1}-y_{s}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In the generalized smooth case, Lemma B.4 does not hold. In contrast, we provide a generalized smooth version of [49, Lemma A.5], which establishes a different relationship between the gradient's norm and the function value gap. Noting that when $q=1$ , Lemma C.3 reduces to [49, Lemma A.5]. ", "page_idx": 21}, {"type": "text", "text": "Lemma C.3. Suppose that $f$ $(L_{0},L_{q})$ -smooth and Assumption (Al) holds. Then for any $\\pmb{x}\\in\\mathbb{R}^{d}$ \uff0c ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla f(x)\\|\\leq\\operatorname*{max}\\left\\{4L_{q}(f(x)-f^{*}),[4L_{q}(f(x)-f^{*})]^{\\frac{1}{2-q}}\\,,\\sqrt{4L_{0}(f(x)-f^{*})}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C.2  Probabilistic estimations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The probabilistic inequalities in (35) and (36) remain unchanged since they do not rely on any smooth-related conditions. However, we shall rely on a different setting of $\\lambda$ in (36) as follows. ", "page_idx": 21}, {"type": "text", "text": "Lemma C.4. Given $T\\geq1$ and $\\delta\\,\\in\\,(0,1)$ Under the same conditions of Lemma $B.7,$ if we set $\\lambda=(1-\\beta_{1})\\sqrt{1-\\beta_{2}}/(3\\eta\\mathcal{H})$ where $\\mathcal{H}$ is as in (11), then with probability at least $1-\\delta$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}-\\eta_{s}\\left\\langle\\bar{g}_{s},\\frac{\\xi_{s}}{a_{s}}\\right\\rangle\\leq\\frac{\\mathcal{G}_{T}(t)}{4\\mathcal{H}}\\sum_{s=1}^{t}\\eta_{s}\\left\\lVert\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\rVert^{2}+D_{1}\\mathcal{H},\\quad\\forall t\\in[T],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $D_{1}$ is given in Lemma B.7. ", "page_idx": 21}, {"type": "text", "text": "The bounds of the four summations in Lemma B.3 also remain unchanged. However, the upper bound for ${\\mathcal{F}}_{i}(t)$ should be revised by the following lemma. The detailed proof could be found in Appendix E. ", "page_idx": 21}, {"type": "text", "text": "Lemma C.5. Given $T\\geq1$ . Under the conditions and notations of Lemma B.3, if $f$ is $(L_{0},L_{q})$ smooth, $\\eta=\\tilde{C}_{0}\\sqrt{1-\\beta_{2}},$ (35) and (66) hold, then the following inequalities hold, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{F}_{i}(t)\\leq\\mathcal{I}(t),\\quad\\forall t\\in[T],i\\in[d],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mathcal{I}(t)$ is defined as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{I}(t):=1+\\frac{2\\mathcal{M}_{T}^{2}}{\\epsilon^{2}}\\left[\\sigma_{0}^{2}t+\\sigma_{1}^{2}t\\left(\\lVert\\bar{{\\boldsymbol{g}}}_{1}\\rVert+t\\tilde{M}_{t}\\right)^{p}+t\\left(\\lVert\\bar{{\\boldsymbol{g}}}_{1}\\rVert+t\\tilde{M}_{t}\\right)^{2}\\right],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and $\\begin{array}{r}{\\tilde{M}_{t}:=\\tilde{C}_{0}\\mathcal{L}_{t}^{(x)}\\sqrt{\\frac{d}{1-\\beta_{1}/\\beta_{2}}}}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "It's worth noting that $\\mathcal{I}(t)$ is stillrandom relying on the random variable $\\mathcal{L}_{t}^{(x)}$ ", "page_idx": 21}, {"type": "text", "text": "C.3  Deterministic estimations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Note that (40) in Appendix B.4 remains unchanged since it's independent from any smooth-related condition. In terms of A.1, the only difference is using $\\mathcal{H}$ toreplace $\\mathcal{G}$ in (39) as we choose a different $\\lambda$ in (36), leading to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{A.1}\\leq\\left(\\frac{\\mathcal{G}_{T}(t)}{4\\mathcal{H}}-\\frac{3}{4}\\right)\\sum_{s=1}^{t}\\eta_{s}\\left\\Vert\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\Vert^{2}+D_{1}\\mathcal{H}+D_{2}\\mathcal{G}_{T}(t)\\sum_{s=1}^{t}\\left\\Vert\\frac{g_{s}}{b_{s}}\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We also establish the following proposition which is a generalized smooth version of Proposition B.13. ", "page_idx": 21}, {"type": "text", "text": "Proposition C.6. Given $T\\geq1$ If $f$ is $(L_{0},L_{q})$ -smooth and (66) holds, then ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(\\pmb{y}_{t+1})\\leq\\pmb{f}(\\pmb{x}_{1})+\\mathbf{A}.\\mathbf{1}+\\mathbf{B}.\\mathbf{1}+\\sum_{s=1}^{t-1}D_{\\6}(s)\\left\\|\\frac{\\hat{\\pmb{m}}_{s}}{b_{s}}\\right\\|^{2}+\\sum_{s=1}^{t}D_{7}(s)\\left\\|\\frac{\\pmb{g}_{s}}{b_{s}}\\right\\|^{2},\\quad\\forall t\\in[T],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\Sigma_{\\mathrm{max}}$ is as in Lemma B.1 and $D_{6}(s),D_{7}(s)$ are defined as,5 ", "page_idx": 21}, {"type": "equation", "text": "$$\nD_{6}(s)={\\frac{\\mathcal{L}_{s}^{(y)}\\eta^{2}(1+4\\Sigma_{\\operatorname*{max}}^{2})}{2(1-\\beta_{1})^{2}}},\\quad D_{7}(s)={\\frac{3\\mathcal{L}_{s}^{(y)}\\eta^{2}}{2(1-\\beta_{1})^{2}}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "5The notations are different from $D_{6}$ and $D_{7}$ defined in (42). ", "page_idx": 21}, {"type": "text", "text": "Proof. The proof follows some same parts in proving Proposition B.14. We start from the descent lemma (68) in Lemma C.2 and sum over $s\\in[t]$ toobtainthat ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\pmb{y}_{t+1})\\leq f(\\pmb{x}_{1})+\\displaystyle\\sum_{s=1}^{t}\\langle\\nabla f(\\pmb{y}_{s}),\\pmb{y}_{s+1}-\\pmb{y}_{s}\\rangle+\\displaystyle\\sum_{s=1}^{t}\\frac{\\mathcal{L}_{s}^{(y)}}{2}\\|\\pmb{y}_{s+1}-\\pmb{y}_{s}\\|^{2}}\\\\ &{\\qquad=f(\\pmb{x}_{1})+\\mathbf{A}+\\mathbf{B}+\\displaystyle\\sum_{s=1}^{t}\\frac{\\mathcal{L}_{s}^{(y)}}{2}\\left\\|\\eta_{s}\\cdot\\frac{\\pmb{g}_{s}}{b_{s}}-\\frac{\\beta_{1}}{1-\\beta_{1}}\\left(\\frac{\\eta_{s}b_{s-1}}{\\eta_{s-1}b_{s}}-\\mathbf{1}\\right)\\Sigma_{s}\\odot(\\pmb{x}_{s}-\\pmb{x}_{s-1})\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathbf{A}$ and $\\mathbf{B}$ follow the same definitions in (32). We also follow the decompositions in (33) and (34). We could also rely on the same analysis for the smooth case in (46) but the smooth parameter is replaced by $\\mathcal{L}_{s}^{(x)}$ .Hence, we obtain that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{A.2}\\leq\\sum_{s=1}^{t}\\frac{\\mathcal{L}_{s}^{(x)}\\eta^{2}}{2(1-\\beta_{1})^{2}}\\left\\|\\frac{\\hat{m}_{s-1}}{b_{s-1}}\\right\\|^{2}+\\sum_{s=1}^{t}\\frac{\\mathcal{L}_{s}^{(x)}\\eta^{2}}{2(1-\\beta_{1})^{2}}\\left\\|\\frac{g_{s}}{b_{s}}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Similarly, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{B.2}\\ \\leq\\sum_{s=1}^{t}\\frac{\\sum_{\\operatorname*{max}}^{2}\\mathcal{L}_{s}^{(x)}\\eta^{2}}{(1-\\beta_{1})^{2}}\\left\\|\\frac{\\hat{m}_{s-1}}{b_{s-1}}\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Noting that $\\mathbf{C}^{\\bullet}$ differs from $\\mathbf{C}$ with $L$ replaced by $\\mathcal{L}_{s}^{(y)}$ . Hence, relying on a similar analysis in (48), we obtain that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{C}^{\\prime}\\leq\\sum_{s=1}^{t}\\frac{\\mathcal{L}_{s}^{(y)}\\eta^{2}}{(1-\\beta_{1})^{2}}\\left\\|\\frac{g_{s}}{b_{s}}\\right\\|^{2}+\\sum_{s=1}^{t}\\frac{\\sum_{\\operatorname*{max}}^{2}\\mathcal{L}_{s}^{(y)}\\eta^{2}}{(1-\\beta_{1})^{2}}\\left\\|\\frac{\\hat{m}_{s-1}}{b_{s-1}}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining (74) with (75), (76) and (77), and noting that $\\mathcal{L}_{s}^{(x)}\\leq\\mathcal{L}_{s}^{(y)}$ from (64), we thereby obtain the desired result. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "C.4  Bounding gradients ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Based on the unchanged parts in Appendix B.3 and Appendix B.4 and the new estimations in (72) and (73), we are now ready to provide the uniform gradients\u2019 bound in the following proposition. ", "page_idx": 22}, {"type": "text", "text": "Proposition C.7. Under the same conditions in Theorem 4.1, for any given $\\delta\\in(0,1/2)$ ,itholds thatwithprobabilityatleast $1-2\\delta$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\bar{g}_{t}\\|\\leq H,\\quad\\mathcal{G}_{T}(t)\\leq\\mathcal{H},\\quad\\mathcal{L}_{t}^{(x)}\\leq\\mathcal{L}_{t}^{(y)}\\leq\\mathcal{L},\\quad\\forall t\\in[T+1],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\nf(\\pmb{y}_{t+1})-\\pmb{f}^{*}\\leq-\\frac{1}{4}\\sum_{s=1}^{t}\\eta_{s}\\left\\|\\frac{\\bar{g}_{s}}{\\sqrt{\\pmb{a}_{s}}}\\right\\|^{2}+\\hat{H},\\quad\\forall t\\in[T],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $H,\\mathcal{H},\\mathcal{L}$ are given in (11) and $\\hat{H}$ is given in (82). ", "page_idx": 22}, {"type": "text", "text": "Proof. Based on the two inequalities (35) and (69), we could deduce the final results in (78) and (79). Since (35) and (69) hold with probability at least $1-2\\delta$ , we thereby deduce the desired result holding with probability at least $1-2\\delta$ . To start with, we shall verify that (66) always holds. Recalling $\\eta$ in (10) and $F$ in Lemma C.1, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\eta F=\\tilde{C}_{0}\\sqrt{1-\\beta_{2}}F\\le\\sqrt{\\frac{\\beta_{2}(1-\\beta_{1})^{2}(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}{4L_{q}^{2}d}}\\cdot F\\le\\frac{1}{L_{q}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, we make sure that the distance requirement in (8) always holds according to Lemma C.1. Second, plugging (72) and (40) into the result in (73), ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(y_{t+1})\\leq f(x_{1})+\\left(\\frac{\\mathcal{G}_{T}(t)}{4\\mathcal{H}}-\\frac{1}{2}\\right)\\displaystyle\\sum_{s=1}^{t}\\eta_{s}\\left\\Vert\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\Vert^{2}+D_{1}\\mathcal{H}+D_{2}\\mathcal{G}_{T}(t)\\displaystyle\\sum_{s=1}^{t}\\left\\Vert\\frac{g_{s}}{b_{s}}\\right\\Vert^{2}}\\\\ &{\\qquad\\qquad+\\displaystyle\\sum_{s=1}^{t}D_{7}(s)\\left\\Vert\\frac{g_{s}}{b_{s}}\\right\\Vert^{2}+(D_{3}\\mathcal{G}_{T}(t)+D_{4})\\displaystyle\\sum_{s=1}^{t}\\left(\\left\\Vert\\frac{m_{s-1}}{b_{s}}\\right\\Vert^{2}+\\left\\Vert\\frac{m_{s-1}}{b_{s-1}}\\right\\Vert^{2}\\right)}\\\\ &{\\qquad\\qquad+\\displaystyle D_{5}G_{t}+\\sum_{s=1}^{t-1}D_{6}(s)\\left\\Vert\\frac{\\hat{m}_{s}}{b_{s}}\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We still rely on an induction argument to deduce the result. First, we provide the detail expressions of ${\\hat{H}},H$ as follows which is determined by hyper-parameters $\\beta_{1},\\beta_{2}$ and constants $E_{0},d,T,\\delta,\\mathfrak{M}_{T}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{H}:=f(x_{1})-f^{*}+\\frac{3E_{0}\\mathcal{M}_{T}}{1-\\beta_{1}}\\log\\left(\\frac{T}{\\delta}\\right)+\\frac{E_{0}\\mathcal{M}_{T}d}{1-\\beta_{1}}\\log\\left(\\frac{\\tilde{\\mathcal{J}}(T)}{\\beta_{2}^{T}}\\right)}\\\\ &{\\quad+\\frac{4E_{0}(\\mathcal{M}_{T}+\\epsilon)d}{\\beta_{2}\\left(1-\\beta_{1}\\right)^{2}\\left(1-\\beta_{1}/\\beta_{2}\\right)}\\log\\left(\\frac{\\tilde{\\mathcal{J}}(T)}{\\beta_{2}^{T}}\\right)+\\frac{2E_{0}d}{\\sqrt{\\left(1-\\beta_{1}\\right)^{3}\\left(1-\\beta_{1}/\\beta_{2}\\right)}}}\\\\ &{\\quad+\\frac{3E_{0}^{2}d}{2(1-\\beta_{1})^{2}}\\log\\left(\\frac{\\tilde{\\mathcal{J}}(T)}{\\beta_{2}^{T}}\\right)+\\frac{5E_{0}^{2}d}{2\\beta_{2}\\left(1-\\beta_{1}\\right)^{2}\\left(1-\\beta_{1}/\\beta_{2}\\right)}\\log\\left(\\frac{\\tilde{\\mathcal{J}}(T)}{\\beta_{2}^{T}}\\right),\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(81)}\\\\ &{\\quad H:=L_{0}/L_{q}+\\left(4L_{q}\\hat{H}\\right)^{q}+\\left(4L_{q}\\hat{H}\\right)^{\\frac{q}{2-q}}+\\left(4L_{0}\\hat{H}\\right)^{\\frac{q}{2}}+4L_{q}\\hat{H}+\\left(4L_{q}\\hat{H}\\right)^{\\frac{1}{2-q}}+\\sqrt{4L_{0}\\hat{H}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $E_{0}>0$ is a constant and $\\tilde{\\mathcal{I}}(T)$ is a polynomial of $T$ given as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{I}}(T):=1+\\frac{2\\mathcal{N}_{T}^{2}}{\\epsilon^{2}}\\left[\\sigma_{0}^{2}T+\\sigma_{1}^{2}T\\left(\\left\\|\\bar{g}_{1}\\right\\|+T\\tilde{M}\\right)^{p}+T\\left(\\left\\|\\bar{g}_{1}\\right\\|+T\\tilde{M}\\right)^{2}\\right],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and $\\begin{array}{r}{\\tilde{M}:=E_{0}\\sqrt{\\frac{d}{1-\\beta_{1}/\\beta_{2}}}}\\end{array}$ TheindnghaL $H$ in(82)\u3002", "page_idx": 23}, {"type": "text", "text": "$G_{1}=\\|\\bar{g}_{1}\\|\\leq4L_{q}(f(x_{1})-f^{*})+(4L_{q}(f(x_{1})-f^{*}))^{\\frac{1}{2-q}}+\\sqrt{4L_{0}(f(x_{1})-f^{*})}\\leq H.$ Supposethat forsome $t\\in[T]$ ", "page_idx": 23}, {"type": "equation", "text": "$$\nG_{s}\\leq H,\\quad\\forall s\\in[t].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Consequently, recalling $\\mathcal{G}_{T}(s)$ in (19), $\\mathcal{L}_{s}^{(x)},\\mathcal{L}_{s}^{(y)}$ in (64) and $\\mathcal{H},\\mathcal{L}$ in (11), ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{G}_{T}(s)\\leq\\mathcal{H},\\quad\\mathcal{L}_{s}^{(x)}\\leq\\mathcal{L}_{s}^{(y)}\\leq\\mathcal{L},\\quad\\forall s\\in[t].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We thus apply (85) to (80), ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle f({\\pmb y}_{t+1})\\leq f({\\pmb x}_{1})-\\frac{1}{4}\\sum_{s=1}^{t}{\\eta_{s}}\\left\\|\\frac{{\\bar{g}}_{s}}{\\sqrt{{a_{s}}}}\\right\\|^{2}+D_{1}\\mathcal{H}+D_{2}\\mathcal{H}\\sum_{s=1}^{t}{\\left\\|\\frac{{g}_{s}}{b_{s}}\\right\\|^{2}}+\\sum_{s=1}^{t}D_{7}(s)\\left\\|\\frac{{g}_{s}}{b_{s}}\\right\\|^{2}}\\\\ {\\displaystyle\\qquad+\\left(D_{3}\\mathcal{H}+D_{4}\\right)\\sum_{s=1}^{t}\\left({\\left\\|\\frac{m_{s-1}}{b_{s}}\\right\\|^{2}}+\\left\\|\\frac{m_{s-1}}{b_{s-1}}\\right\\|^{2}\\right)+D_{5}H+\\sum_{s=1}^{t-1}D_{6}(s)\\left\\|\\frac{\\hat{m}_{s}}{b_{s}}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Further recalling the setting of ${\\tilde{C}}_{0}$ in (10), with a simple calculation it holds that, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{C}_{0}H\\leq E_{0},\\quad\\tilde{C}_{0}\\mathcal{H}\\leq E_{0},\\quad\\tilde{C}_{0}\\mathcal{L}\\leq E_{0},\\quad\\tilde{C}_{0}^{2}\\mathcal{L}\\leq E_{0}^{2},\\quad\\tilde{C}_{0}\\epsilon_{0}\\leq E_{0}\\epsilon_{0}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, combining with (85), (87) and $\\tilde{M}_{t}$ in (71), we could use the deterministic polynomial $\\tilde{\\mathcal{I}}(t)$ to further control $\\mathcal{I}(t)$ in (71), ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{M}_{t}\\le\\tilde{C}_{0}\\mathcal{L}\\sqrt{\\frac{d}{1-\\beta_{1}/\\beta_{2}}}\\le E_{0}\\sqrt{\\frac{d}{1-\\beta_{1}/\\beta_{2}}}=\\tilde{M},\\quad\\mathcal{I}(t)\\le\\tilde{\\mathcal{I}}(t)\\le\\tilde{\\mathcal{I}}(T),}\\\\ &{\\log\\left(\\frac{\\mathcal{F}_{i}(t)}{\\beta_{2}^{t}}\\right)\\le\\log\\left(\\frac{\\mathcal{I}(t)}{\\beta_{2}^{t}}\\right)\\le\\log\\left(\\frac{\\tilde{\\mathcal{I}}(T)}{\\beta_{2}^{T}}\\right),\\quad\\forall t\\le T,i\\in[d].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, we could use $\\tilde{\\mathcal{I}}(T)$ to control the four summations in Lemma B.3 which emerge in (86). In addition, we rely on $\\eta=\\tilde{C}_{0}\\sqrt{1-\\beta_{2}}$ and the induction assumptions of (84) and (85) to further upper bound the RHS of (86), leading to ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(y_{t+1})-f^{*}\\le f(x_{1})-f^{*}-\\frac{1}{4}\\displaystyle\\sum_{s=1}^{t}\\eta_{s}\\left\\Vert\\frac{\\bar{g}_{s}}{\\sqrt{a}_{s}}\\right\\Vert^{2}+\\frac{3\\tilde{C}_{0}\\mathcal{H}}{1-\\beta_{1}}\\log\\left(\\frac{T}{\\delta}\\right)+\\frac{\\tilde{C}_{0}\\mathcal{H}d}{1-\\beta_{1}}\\log\\left(\\frac{\\tilde{\\mathcal{I}}(T)}{\\beta_{2}^{T}}\\right)}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\frac{4\\tilde{C}_{0}(\\mathcal{H}+\\epsilon_{0})d}{\\beta_{2}(1-\\beta_{1})^{2}(1-\\beta_{1}/\\beta_{2})}\\log\\left(\\frac{\\tilde{\\mathcal{I}}(T)}{\\beta_{2}^{T}}\\right)+\\frac{2\\tilde{C}_{0}H d}{\\sqrt{(1-\\beta_{1})^{3}(1-\\beta_{1}/\\beta_{2})}}}\\\\ &{\\qquad\\qquad\\qquad+\\frac{3\\tilde{C}_{0}^{2}\\mathcal{L}d}{2(1-\\beta_{1})^{2}}\\log\\left(\\frac{\\tilde{\\mathcal{I}}(T)}{\\beta_{2}^{T}}\\right)+\\frac{5\\tilde{C}_{0}^{2}\\mathcal{L}d}{2\\beta_{2}(1-\\beta_{1})^{2}(1-\\beta_{1}/\\beta_{2})}\\log\\left(\\frac{\\tilde{\\mathcal{I}}(T)}{\\beta_{2}^{T}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then combining with (87) and the definition of $\\hat{H}$ in (81), we obtain that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Delta_{t+1}:=f(\\pmb{y}_{t+1})-f^{*}\\leq-\\frac{1}{4}\\sum_{s=1}^{t}\\eta_{s}\\left\\|\\frac{\\bar{\\pmb{g}}_{s}}{\\sqrt{\\pmb{a}_{s}}}\\right\\|^{2}+\\hat{H}\\leq\\hat{H}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, further using Lemma C.2, Lemma C.3 and $H$ in (82), ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\bar{g}_{t+1}\\|\\leq L_{0}/L_{q}+\\|\\nabla f(y_{t+1})\\|^{q}+\\|\\nabla f(y_{t+1})\\|}\\\\ &{\\qquad\\qquad\\leq L_{0}/L_{q}+(4L_{q}\\Delta_{t+1})^{q}+(4L_{q}\\Delta_{t+1})^{\\frac{q}{2-q}}}\\\\ &{\\qquad\\qquad+\\left(4L_{0}\\Delta_{t+1}\\right)^{\\frac{q}{2}}+4L_{q}\\Delta_{t+1}+\\left(4L_{q}\\Delta_{t+1}\\right)^{\\frac{1}{2-q}}+\\sqrt{4L_{0}\\Delta_{t+1}}\\leq H.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Wethen deduce that $G_{t+1}=\\operatorname*{max}\\{G_{t},\\|{\\bar{g}}_{t+1}\\|\\}\\leq H$ The induction is then complete and we obtain the desired result in (78). Finally, as an intermediate result of the proof, we obtain that (79) holds as Well. ", "page_idx": 24}, {"type": "text", "text": "C.5 Proof of the main result ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof of Theorem 4.1. The proof for the final convergence rate follows a similar idea and some same estimations in the proof of Theorem 3.1. Setting $t=T$ in (79), it holds that with probability at least $1-2\\delta$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{4}\\sum_{s=1}^{t}\\frac{\\eta_{s}}{\\|\\pmb{a}_{s}\\|_{\\infty}}\\cdot\\|\\pmb{\\bar{g}}_{s}\\|^{2}\\leq\\frac{1}{4}\\sum_{s=1}^{t}\\eta_{s}\\left\\|\\frac{\\bar{g}_{s}}{\\sqrt{\\pmb{a}_{s}}}\\right\\|^{2}\\leq\\hat{H}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then in what follows, we would assume that (78) and (90) always hold. Relying on the two inequalities, we thereby deduce the final convergence result. Furthermore, since (78) and (90) hold with probability at least $1-2\\delta$ , the final convergence result also holds with probability at least $1-2\\delta$ Using (78) and following the same analysis in (60), ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|a_{s}\\|_{\\infty}\\leq\\operatorname*{max}_{i\\in[d]}\\sqrt{(1-\\beta_{2})\\left(\\sum_{j=1}^{s-1}\\beta_{2}^{s-j}g_{j,i}^{2}+(\\mathcal{G}_{T}(j))^{2}\\right)}+\\epsilon_{s}\\leq(\\mathcal{H}+\\epsilon)\\sqrt{1-\\beta_{2}^{s}},\\quad\\forall s\\in[T].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining with the parameter setting in (10) ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{\\eta_{s}}{\\|a_{s}\\|_{\\infty}}\\geq\\frac{\\eta\\sqrt{1-\\beta_{2}^{s}}}{(1-\\beta_{1}^{s})\\|a_{s}\\|_{\\infty}}\\geq\\frac{\\tilde{C}_{0}\\sqrt{1-\\beta_{2}}}{\\varkappa+\\epsilon_{0}\\sqrt{1-\\beta_{2}}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We then combine with (90) and $\\mathcal{H}$ in (11) to obtain that with probability at least $1-2\\delta$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{s=1}^{T}\\|\\bar{g}_{s}\\|^{2}\\leq\\frac{4\\hat{H}}{T\\tilde{C}_{0}}\\left(\\frac{\\sqrt{2(\\sigma_{0}^{2}+\\sigma_{1}^{2}H^{p}+H^{2})}}{\\sqrt{1-\\beta_{2}}}+\\epsilon_{0}\\right)\\sqrt{\\log\\left(\\frac{\\mathrm{e}T}{\\delta}\\right)}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "$\\begin{array}{r}{\\hat{H}\\sim\\mathcal{O}\\left(\\log^{2}\\left(\\frac{T}{\\epsilon_{0}\\delta}\\right)\\right)}\\end{array}$ from (81) and the desired results in Theorem 4.1. ", "page_idx": 24}, {"type": "text", "text": "D Omitted proof in Appendix B ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "D.1  Omitted proof in Appendix B.1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof of Lemma B.1. We fx arbitrary $i\\in[d]$ and have the following two cases. When $\\begin{array}{r}{\\frac{\\eta_{s}b_{s-1,i}}{\\eta_{s-1}b_{s,i}}<1}\\end{array}$ we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\frac{\\eta_{s}b_{s-1,i}}{\\eta_{s-1}b_{s,i}}-1\\right|=1-\\frac{\\eta_{s}b_{s-1,i}}{\\eta_{s-1}b_{s,i}}<1.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "When m-i \u22651, let r = \u03b2-1. Since 0 <1- \u03b2-1 <1 - \u03b2,Vs \u22652, then we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\eta_{s}}{\\eta_{s-1}}=\\sqrt{\\frac{1-\\beta_{2}^{s}}{1-\\beta_{2}^{s-1}}}\\cdot\\frac{1-\\beta_{1}^{s-1}}{1-\\beta_{1}^{s}}\\le\\sqrt{1+\\frac{\\beta_{2}^{s-1}(1-\\beta_{2})}{1-\\beta_{2}^{s-1}}}=\\sqrt{1+(1-\\beta_{2})\\cdot\\frac{r}{1-r}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $h(r)=r/(1-r)$ is increasing as $r$ grows and $r$ takes the maximum value when $s=2$ .Hence, it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\eta_{s}}{\\eta_{s-1}}\\leq\\sqrt{1+(1-\\beta_{2})\\cdot\\frac{\\beta_{2}}{1-\\beta_{2}}}=\\sqrt{1+\\beta_{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then, since $\\epsilon_{s-1}\\leq\\epsilon_{s}$ , we further have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{b_{s-1,i}}{b_{s,i}}=\\frac{\\epsilon_{s-1}+\\sqrt{v_{s-1,i}}}{\\epsilon_{s}+\\sqrt{\\beta_{2}v_{s-1,i}+(1-\\beta_{2})g_{s,i}^{2}}}\\le\\frac{\\epsilon_{s}+\\sqrt{v_{s-1,i}}}{\\epsilon_{s}+\\sqrt{\\beta_{2}v_{s-1,i}}}\\le\\frac{1}{\\sqrt{\\beta_{2}}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining with (91) and (92), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\frac{\\eta_{s}b_{s-1,i}}{\\eta_{s-1}b_{s,i}}-1\\right|=\\frac{\\eta_{s}b_{s-1,i}}{\\eta_{s-1}b_{s,i}}-1\\le\\sqrt{\\frac{1+\\beta_{2}}{\\beta_{2}}}-1.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining the two cases and noting that the bound holds for any $i\\in[d]$ , we then obtain the desired result. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma B.2. Denoting $\\begin{array}{r}{\\tilde{M}=\\sum_{j=1}^{s-1}\\beta_{1}^{s-1-j}}\\end{array}$ and applying (28) with $\\hat{M}$ and $\\alpha_{j}$ replaced by $\\tilde{M}$ and $g_{j,i}$ respectively, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left(\\sum_{j=1}^{s-1}\\beta_{1}^{s-1-j}g_{j,i}\\right)^{2}\\leq\\tilde{M}\\cdot\\sum_{j=1}^{s-1}\\beta_{1}^{s-1-j}g_{j,i}^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hence, combining with the definition of $b_{s,i}$ in (16), we further have for any $i\\in[d]$ and $s\\geq2$ \uff0c ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\frac{m_{s-1,i}}{b_{s-1,i}}\\right|\\leq\\left|\\frac{m_{s-1,i}}{\\sqrt{v_{s-1,i}}}\\right|=\\sqrt{\\frac{(1-\\beta_{1})^{2}\\,\\left(\\sum_{j=1}^{s-1}\\beta_{1}^{s-1-j}g_{j,i}\\right)^{2}}{(1-\\beta_{2})\\sum_{j=1}^{s-1}\\beta_{2}^{s-1-j}g_{j,i}^{2}}}}\\\\ &{\\qquad\\qquad\\leq\\frac{1-\\beta_{1}}{\\sqrt{1-\\beta_{2}}}\\sqrt{\\tilde{M}\\cdot\\frac{\\sum_{j=1}^{s-1}\\beta_{1}^{s-1-j}g_{j,i}^{2}}{\\sum_{j=1}^{s-1}\\beta_{2}^{s-1-j}g_{j,i}^{2}}}\\leq\\frac{1-\\beta_{1}}{\\sqrt{1-\\beta_{2}}}\\sqrt{\\tilde{M}\\cdot\\sum_{j=1}^{s-1}\\left(\\frac{\\beta_{1}}{\\beta_{2}}\\right)^{s-1-j}}}\\\\ &{\\qquad=\\frac{1-\\beta_{1}}{\\sqrt{1-\\beta_{2}}}\\sqrt{\\frac{1-\\beta_{1}^{s-1}}{1-\\beta_{1}}\\cdot\\frac{1-(\\beta_{1}/\\beta_{2})^{s-1}}{1-\\beta_{1}/\\beta_{2}}}\\leq\\sqrt{\\frac{(1-\\beta_{1})(1-\\beta_{1}^{s-1})}{(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality applies $\\beta_{1}<\\beta_{2}$ . We thus prove the first result. To prove the second result, from the smoothness of $f$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\bar{g}_{s}\\|\\leq\\|\\bar{g}_{s-1}\\|+\\|\\bar{g}_{s}-\\bar{g}_{s-1}\\|\\leq\\|\\bar{g}_{s-1}\\|+L\\|x_{s}-x_{s-1}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining with (30) and $\\eta=C_{0}\\sqrt{1-\\beta_{2}}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|x_{s}-x_{s-1}\\|_{\\infty}\\leq\\eta_{s-1}\\left\\|\\frac{m_{s-1}}{b_{s-1}}\\right\\|_{\\infty}\\leq\\eta\\sqrt{\\frac{1}{(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}}=C_{0}\\sqrt{\\frac{1}{1-\\beta_{1}/\\beta_{2}}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Using $\\|{\\pmb x}_{s}-{\\pmb x}_{s-1}\\|\\leq\\sqrt{d}\\|{\\pmb x}_{s}-{\\pmb x}_{s-1}\\|_{\\infty}$ and (94), ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\bar{g}_{s}\\|\\le\\|\\bar{g}_{s-1}\\|+L C_{0}\\sqrt{\\frac{d}{1-\\beta_{1}/\\beta_{2}}}\\le\\|\\bar{g}_{1}\\|+L C_{0}s\\sqrt{\\frac{d}{1-\\beta_{1}/\\beta_{2}}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof of Lemma B.3. Recalling the updated rule and the definition of $b_{s,i}$ in (16),using $\\epsilon_{s}^{2}=\\epsilon^{2}(1-$ $\\beta_{2}^{s})\\ge\\epsilon^{2}(1-\\beta_{2})$ ", "page_idx": 26}, {"type": "equation", "text": "$$\nb_{s,i}^{2}\\geq v_{s,i}^{2}+\\epsilon_{s}^{2}\\geq\\left(1-\\beta_{2}\\right)\\left(\\sum_{j=1}^{s}\\beta_{2}^{s-j}g_{j,i}^{2}+\\epsilon^{2}\\right),\\quad\\mathrm{and}\\quad m_{s,i}=\\left(1-\\beta_{1}\\right)\\sum_{j=1}^{s}\\beta_{1}^{s-j}g_{j,i}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof for the first summation  Using (96), for any $i\\in[d]$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\frac{g_{s,i}^{2}}{b_{s,i}^{2}}\\leq\\frac{1}{1-\\beta_{2}}\\sum_{s=1}^{t}\\frac{g_{s,i}^{2}}{\\epsilon^{2}+\\sum_{j=1}^{s}\\beta_{2}^{s-j}g_{j,i}^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Applying Lemma A.1 and recalling the definition of ${\\mathcal{F}}_{i}(t)$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\frac{g_{s,i}^{2}}{b_{s,i}^{2}}\\leq\\frac{1}{1-\\beta_{2}}\\left[\\log\\left(1+\\frac{1}{\\epsilon^{2}}\\sum_{s=1}^{t}\\beta_{2}^{t-s}g_{s,i}^{2}\\right)-t\\log\\beta_{2}\\right]\\leq\\frac{1}{1-\\beta_{2}}\\log\\left(\\frac{\\mathcal{F}_{i}(t)}{\\beta_{2}^{t}}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Summing over $i\\in[d]$ , we obtain the first desired result. ", "page_idx": 26}, {"type": "text", "text": "Proof for the second summation  Following from (96), ", "text_level": 1, "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\frac{m_{s,i}^{2}}{b_{s,i}^{2}}\\leq\\frac{(1-\\beta_{1})^{2}}{1-\\beta_{2}}\\cdot\\sum_{s=1}^{t}\\frac{\\left(\\sum_{j=1}^{s}\\beta_{1}^{s-j}g_{j,i}\\right)^{2}}{\\epsilon^{2}+\\sum_{j=1}^{s}\\beta_{2}^{s-j}g_{j,i}^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Applying Lemma A.2 and $\\beta_{2}\\leq1$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}\\frac{m_{s,i}^{2}}{b_{s,i}^{2}}\\leq\\frac{(1-\\beta_{1})^{2}}{1-\\beta_{2}}\\cdot\\frac{1}{(1-\\beta_{1})(1-\\beta_{1}/\\beta_{2})}\\left[\\log\\left(1+\\frac{1}{\\epsilon^{2}}\\sum_{s=1}^{t}\\beta_{2}^{t-s}g_{s,i}^{2}\\right)-t\\log\\beta_{2}\\right]}\\\\ &{\\quad\\quad\\quad=\\frac{1-\\beta_{1}}{(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}\\log\\left(\\frac{\\mathcal{F}_{i}(t)}{\\beta_{2}^{t}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Summing over $i\\in[d]$ , we obtain the second desired result. ", "page_idx": 26}, {"type": "text", "text": "Proof for the third summation  Following from (96), ", "text_level": 1, "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}\\frac{m_{s,i}^{2}}{b_{s+1,i}^{2}}\\leq\\displaystyle\\sum_{s=1}^{t}\\frac{\\left[(1-\\beta_{1})\\sum_{j=1}^{s}\\beta_{1}^{s-j}g_{j,i}\\right]^{2}}{\\epsilon^{2}(1-\\beta_{2})+(1-\\beta_{2})\\sum_{j=1}^{s+1}\\beta_{2}^{s+1-j}g_{j,i}^{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\sum_{s=1}^{t}\\frac{(1-\\beta_{1})^{2}\\left(\\sum_{j=1}^{s}\\beta_{1}^{s-j}g_{j,i}\\right)^{2}}{\\epsilon^{2}(1-\\beta_{2})+(1-\\beta_{2})\\beta_{2}\\sum_{j=1}^{s}\\beta_{2}^{s-j}g_{j,i}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Applying Lemma A.2, and using $\\beta_{2}\\leq1$ \uff0c ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}\\frac{m_{s,i}^{2}}{b_{s+1,i}^{2}}\\leq\\frac{(1-\\beta_{1})^{2}}{(1-\\beta_{2})\\beta_{2}}\\cdot\\sum_{s=1}^{t}\\frac{\\left(\\sum_{j=1}^{s}\\beta_{1}^{s-j}g_{j,i}\\right)^{2}}{\\frac{\\varepsilon^{2}}{\\beta_{2}}+\\sum_{j=1}^{s}\\beta_{2}^{s-j}g_{j,i}^{2}}}\\\\ &{\\qquad\\qquad\\leq\\frac{(1-\\beta_{1})^{2}}{(1-\\beta_{2})\\beta_{2}}\\cdot\\frac{1}{(1-\\beta_{1})(1-\\beta_{1}/\\beta_{2})}\\left[\\log\\left(1+\\frac{\\beta_{2}}{\\epsilon^{2}}\\sum_{s=1}^{t}\\beta_{2}^{t-s}g_{s,i}^{2}\\right)-t\\log\\beta_{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\frac{1-\\beta_{1}}{\\beta_{2}(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}\\log\\left(\\frac{\\mathcal{F}_{i}(t)}{\\beta_{2}^{t}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Summing over $i\\in[d]$ , we obtain the third desired result ", "page_idx": 27}, {"type": "text", "text": "Proof for the fourth summation  Following the definition of $\\hat{m}_{s,i}$ from (29), and combining with (96), ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\frac{\\hat{m}_{s,i}^{2}}{b_{s,i}^{2}}\\leq\\frac{(1-\\beta_{1})^{2}}{1-\\beta_{2}}\\cdot\\sum_{s=1}^{t}\\frac{\\left(\\frac{1}{1-\\beta_{1}^{s}}\\sum_{j=1}^{s}\\beta_{1}^{s-j}g_{j,i}\\right)^{2}}{\\epsilon^{2}+\\sum_{j=1}^{s}\\beta_{2}^{s-j}g_{j,i}^{2}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Applying Lemma A.2 and using $\\beta_{2}\\leq1$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}\\frac{\\hat{m}_{s,i}^{2}}{b_{s,i}^{2}}\\leq\\frac{(1-\\beta_{1})^{2}}{1-\\beta_{2}}\\cdot\\frac{1}{(1-\\beta_{1})^{2}(1-\\beta_{1}/\\beta_{2})}\\left[\\log\\left(1+\\frac{1}{\\epsilon^{2}}\\sum_{s=1}^{t}\\beta_{2}^{t-s}g_{s,i}^{2}\\right)-t\\log\\beta_{2}\\right]}\\\\ &{\\quad\\quad\\quad\\leq\\displaystyle\\frac{1}{(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}\\log\\left(\\frac{\\mathcal{F}_{i}(t)}{\\beta_{2}^{t}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Summing over $i\\in[d]$ , we obtain the fourth desired result. ", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma B.4. Let $\\begin{array}{r}{\\hat{\\pmb{x}}=\\pmb{x}-\\frac{1}{L}\\nabla f(\\pmb{x})}\\end{array}$ . Then using the descent lemma of smoothness, ", "page_idx": 27}, {"type": "equation", "text": "$$\nf({\\hat{\\pmb x}})\\leq f({\\pmb x})+\\langle\\nabla f({\\pmb x}),{\\hat{\\pmb x}}-{\\pmb x}\\rangle+\\frac{L}{2}\\|{\\hat{\\pmb x}}-{\\pmb x}\\|^{2}\\leq f({\\pmb x})-\\frac{1}{2L}\\|\\nabla f({\\pmb x})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Re-arranging the order, and noting that $f({\\hat{x}})\\geq f^{*}$ \uff0c ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\pmb{x})\\|^{2}\\leq2L(f(\\pmb{x})-f(\\hat{\\pmb{x}}))\\leq2L(f(\\pmb{x})-f^{*}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma B.5. Applying the norm inequality and the smoothness of $f$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla f(x_{s})\\|\\leq\\|\\nabla f(y_{s})\\|+\\|\\nabla f(x_{s})-\\nabla f(y_{s})\\|\\leq\\|\\nabla f(y_{s})\\|+L\\|y_{s}-x_{s}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining with the definition of $\\pmb{y}_{s}$ in (17) and (95), and using $\\beta_{1}\\in[0,1)$ , we obtain the desired result that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\nabla f(x_{s})\\|\\leq\\|\\nabla f(y_{s})\\|+{\\frac{L\\beta_{1}}{1-\\beta_{1}}}\\|x_{s}-x_{s-1}\\|\\leq\\|\\nabla f(y_{s})\\|+{\\frac{L C_{0}{\\sqrt{d}}}{(1-\\beta_{1}){\\sqrt{1-\\beta_{1}/\\beta_{2}}}}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "D.2Omitted proof in Appendix B.3 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "ProofofLemmaB.6. Letus denote $\\begin{array}{r}{\\gamma_{s}=\\frac{\\lVert\\pmb{\\xi}_{s}\\rVert^{2}}{\\sigma_{0}^{2}+\\sigma_{1}^{2}\\lVert\\bar{\\pmb{g}}_{s}\\rVert^{p}},\\forall s\\in[T]}\\end{array}$ Then from Assumption A3), we first have $\\mathbb{E}_{z_{s}}[\\exp{(\\gamma_{s})}]\\le\\exp(1)$ . Taking full expectation, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp(\\gamma_{s})\\right]\\leq\\exp(1).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Markov's inequality, for any $A\\in\\mathbb{R}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\underset{s\\in[T]}{\\operatorname*{max}}\\gamma_{s}\\geq A\\right)=\\mathbb{P}\\left(\\exp\\left(\\underset{s\\in[T]}{\\operatorname*{max}}\\gamma_{s}\\right)\\geq\\exp(A)\\right)\\leq\\exp(-A)\\mathbb{E}\\left[\\exp\\left(\\underset{s\\in[T]}{\\operatorname*{max}}\\gamma_{s}\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\exp(-A)\\mathbb{E}\\left[\\displaystyle\\sum_{s=1}^{T}\\exp\\left(\\gamma_{s}\\right)\\right]\\leq\\exp(-A)T\\exp(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which leads to that with probability at least $1-\\delta$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\pmb{\\xi}_{s}\\|^{2}\\leq\\log\\left(\\frac{\\mathrm{e}T}{\\delta}\\right)\\left(\\sigma_{0}^{2}+\\sigma_{1}^{2}\\|\\bar{\\pmb{g}}_{s}\\|^{p}\\right),\\quad\\forall s\\in[T].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of Lemma B.7. Recalling the definitions of $\\pmb{a}_{s}$ in (23) and $\\epsilon_{s}$ in Algorithm 1, we have for any $s\\in[T],i\\in[d]$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{a_{s,i}}\\leq\\frac{1}{\\mathcal{G}_{T}(s)\\sqrt{1-\\beta_{2}}+\\epsilon\\sqrt{1-\\beta_{2}^{s}}}\\leq\\frac{1}{(\\mathcal{G}_{T}(s)+\\epsilon)\\sqrt{1-\\beta_{2}}}}\\\\ &{\\qquad\\leq\\frac{1}{\\mathcal{G}_{T}(s)\\sqrt{1-\\beta_{2}}}\\leq\\frac{1}{\\sqrt{\\sigma_{0}^{2}+\\sigma_{1}^{2}}\\Vert\\bar{g}_{s}\\Vert^{p}\\sqrt{1-\\beta_{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then given any $i\\in[d]$ , we set ", "page_idx": 28}, {"type": "equation", "text": "$$\nX_{s}=-\\eta_{s}\\left\\langle\\bar{g}_{s},\\frac{\\xi_{s}}{a_{s}}\\right\\rangle,\\omega_{s}=\\eta_{s}\\left\\lVert\\frac{\\bar{g}_{s}}{a_{s}}\\right\\rVert\\sqrt{\\sigma_{0}^{2}+\\sigma_{1}^{2}\\lVert\\bar{g}_{s}\\rVert^{p}},\\quad\\forall s\\in[T].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Noting that $\\bar{\\pmb{g}}_{s},\\pmb{a}_{s}$ and $\\eta_{s}$ are random variables dependent by $z_{1},\\cdot\\cdot\\cdot\\,,z_{s-1}$ and $\\xi_{s}$ is only dependent on $z_{s}$ . We then verify that $X_{s}$ is a martingale difference sequence since ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[X_{s}\\mid z_{1},\\cdot\\cdot\\cdot,z_{s-1}\\right]=\\mathbb{E}_{z_{s}}\\left[-\\eta_{s}\\left\\langle\\bar{g}_{s},\\frac{\\xi_{s}}{a_{s}}\\right\\rangle\\right]=-\\eta_{s}\\left\\langle\\bar{g}_{s},\\frac{\\mathbb{E}_{z_{s}}[\\xi_{s}]}{a_{s}}\\right\\rangle=0.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Noting that $\\omega_{s}$ is a random variable only dependent by $z_{1},\\cdot\\cdot\\cdot\\,,z_{s-1}$ and applying Assumption (A3) and Cauchy-Schwarz inequality, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\exp\\left(\\frac{X_{s}^{2}}{\\omega_{s}^{2}}\\right)\\mid z_{1},\\cdot\\cdot\\cdot\\cdot,z_{s-1}\\right]\\leq\\mathbb{E}\\left[\\exp\\left(\\frac{\\xi_{s}^{2}}{\\sigma_{0}^{2}+\\sigma_{1}^{2}\\|\\bar{g}_{s}\\|^{p}}\\right)\\mid z_{1},\\cdot\\cdot\\cdot\\cdot,z_{s-1}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mathbb{E}_{z_{s}}\\left[\\exp\\left(\\frac{\\|\\xi_{s}\\|^{2}}{\\sigma_{0}^{2}+\\sigma_{1}^{2}\\|\\bar{g}_{s}\\|^{p}}\\right)\\right]\\leq\\exp(1),\\quad\\forall s\\in[T].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Applying Lemma A.3 and (97), we have that for any $\\lambda>0$ , with probability at least $1-\\delta$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}X_{s}\\leq\\frac{3\\lambda}{4}\\sum_{s=1}^{t}\\omega_{s}^{2}+\\frac{1}{\\lambda}\\log\\left(\\frac{1}{\\delta}\\right)}\\\\ &{\\qquad\\quad\\leq\\displaystyle\\frac{3\\lambda}{4\\sqrt{1-\\beta_{2}}}\\sum_{s=1}^{t}\\eta_{s}^{2}\\left\\|\\frac{\\bar{g}_{s}}{a_{s}}\\right\\|^{2}(\\sigma_{0}^{2}+\\sigma_{1}^{2}\\|\\bar{g}_{s}\\|^{p})+\\frac{1}{\\lambda}\\log\\left(\\frac{1}{\\delta}\\right)}\\\\ &{\\qquad\\quad\\leq\\displaystyle\\frac{3\\lambda}{4\\sqrt{1-\\beta_{2}}}\\sum_{s=1}^{t}\\eta_{s}^{2}\\left\\|\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\|^{2}\\sqrt{\\sigma_{0}^{2}+\\sigma_{1}^{2}\\|\\bar{g}_{s}\\|^{p}}+\\frac{1}{\\lambda}\\log\\left(\\frac{1}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note that for any $t\\in[T]$ , (98) holds with probability at least $1-\\delta$ . Then for any fixed $\\lambda>0$ ,we could re-scale $\\delta$ to obtain that with probability at least $1-\\delta$ , for all $t\\in[T]$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}X_{s}\\leq\\frac{3\\lambda}{4\\sqrt{1-\\beta_{2}}}\\sum_{s=1}^{t}\\eta_{s}^{2}\\left\\|\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\|^{2}\\sqrt{\\sigma_{0}^{2}+\\sigma_{1}^{2}\\|\\bar{g}_{s}\\|^{p}}+\\frac{1}{\\lambda}\\log\\left(\\frac{T}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Using $\\sqrt{\\sigma_{0}^{2}+\\sigma_{1}^{2}\\Vert\\bar{g}_{s}\\Vert^{p}}\\leq\\mathcal{G}_{T}(t),s\\leq t$ from (19), together with (30), we have that with probability atleast $1-\\delta$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n-\\sum_{s=1}^{t}\\eta_{s}\\left\\langle\\bar{g}_{s},\\frac{\\xi_{s}}{a_{s}}\\right\\rangle\\leq\\frac{3\\lambda\\eta\\mathcal{G}_{T}(t)}{4(1-\\beta_{1})\\sqrt{1-\\beta_{2}}}\\sum_{s=1}^{t}\\eta_{s}\\left\\Vert\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\Vert^{2}+\\frac{1}{\\lambda}\\log\\left(\\frac{T}{\\delta}\\right),\\quad\\forall t\\in[T].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Finally setting $\\lambda=\\left(1-\\beta_{1}\\right)\\!\\sqrt{1-\\beta_{2}}/\\left(3\\eta\\mathcal{G}_{T}\\right)$ , we then have the desired result in (37). ", "page_idx": 28}, {"type": "text", "text": "D.3Omitted proof of Appendix B.4 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Proof of Lemma B.8. First directly applying (35) and $G_{s}$ in (19), for any $j\\in[s]$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\xi_{j}\\|\\leq\\mathcal{M}_{T}\\sqrt{\\sigma_{0}^{2}+\\sigma_{1}^{2}\\|\\bar{g}_{j}\\|^{p}}\\leq\\mathcal{M}_{T}\\sqrt{\\sigma_{0}^{2}+\\sigma_{1}^{2}G_{j}^{p}}\\leq\\mathcal{M}_{T}\\sqrt{\\sigma_{0}^{2}+\\sigma_{1}^{2}G_{s}^{p}}\\leq\\mathcal{G}_{T}(s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Applying the basic inequality, (35) and $\\mathcal{M}_{T}\\geq1$ , for any $j\\in[s]$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|g_{j}\\|^{2}\\leq2\\|\\bar{g}_{j}\\|^{2}+2\\|\\xi_{j}\\|^{2}\\leq2\\mathfrak{M}_{T}^{2}\\left(\\sigma_{0}^{2}+\\sigma_{1}^{2}\\|\\bar{g}_{j}\\|^{p}+\\|\\bar{g}_{j}\\|^{2}\\right)\\leq(\\mathcal{G}_{T}(s))^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Finally, we would use an induction argument to prove the last result. Given any $i\\in[d]$ ,noting that $v_{1,i}=\\bar{(1-\\beta_{2})}g_{1,i}^{2}\\leq(\\mathcal{G}_{T}(s))^{2}$ . Suppose that for some $s^{\\prime}\\in[s],v_{j,i}\\leq(\\mathcal{G}_{T}(s))^{2},\\forall j\\,\\overset{.}{\\in}\\,[s^{\\prime}]$ ", "page_idx": 29}, {"type": "equation", "text": "$$\nv_{s^{\\prime}+1,i}=\\beta_{2}v_{s^{\\prime},i}+(1-\\beta_{2})g_{s^{\\prime},i}^{2}\\leq\\beta_{2}(\\mathscr{G}_{T}(s))^{2}+(1-\\beta_{2})(\\mathscr{G}_{T}(s))^{2}\\leq(\\mathscr{G}_{T}(s))^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We then obtain that $v_{j,i}\\leq(\\mathcal{G}_{T}(s))^{2},\\forall j\\in[s]$ . Noting that the above inequality holds for all $i\\in[d]$ we therefore obtain the desired result. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Proof of Lemma B.9. Recalling the definition of $b_{s,i}$ in (16) and letting $a_{s,i}=\\sqrt{\\tilde{v}_{s,i}}+\\epsilon_{s}$ in (23), ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\cfrac{1}{a_{s,i}}-\\cfrac{1}{b_{s,i}}\\right|=\\cfrac{\\left|\\sqrt{v_{s,i}}-\\sqrt{\\tilde{v}_{s,i}}\\right|}{a_{s,i}b_{s,i}}=\\cfrac{1-\\beta_{2}}{a_{s,i}b_{s,i}}\\cfrac{\\left|g_{s,i}^{2}-(\\mathcal{G}_{T}(s))^{2}\\right|}{\\sqrt{v_{s,i}}+\\sqrt{\\tilde{v}_{s,i}}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\cfrac{1-\\beta_{2}}{a_{s,i}b_{s,i}}\\cdot\\cfrac{(\\mathcal{G}_{T}(s))^{2}}{\\sqrt{v_{s,i}}+\\sqrt{\\beta_{2}v_{s-1,i}+(1-\\beta_{2})(\\mathcal{G}_{T}(s))^{2}}}\\leq\\cfrac{\\mathcal{G}_{T}(s)\\sqrt{1-\\beta_{2}}}{a_{s,i}b_{s,i}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "wherewe apply $g_{s,i}^{2}\\leq\\|\\pmb{g}_{s}\\|^{2}\\leq(\\mathcal{G}_{T}(s))^{2}$ from Lemma B.8 in the first inequality since (35) holds. The second result also follows from the same analysis. We first combine with $\\epsilon_{s}=\\epsilon\\sqrt{1-\\beta_{2}^{s}}$ to obtainthat ", "page_idx": 29}, {"type": "equation", "text": "$$\n|\\epsilon_{s}-\\epsilon_{s-1}|\\leq\\epsilon\\left(\\sqrt{1-\\beta_{2}^{s}}-\\sqrt{1-\\beta_{2}^{s-1}}\\right)\\leq\\epsilon\\sqrt{\\beta_{2}^{s-1}(1-\\beta_{2})}\\leq\\epsilon\\sqrt{1-\\beta_{2}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we apply ${\\sqrt{a}}-{\\sqrt{b}}\\leq{\\sqrt{a-b}},\\forall0\\leq b\\leq a$ .Applying the definition of $b_{s-1,i}$ and $a_{s,i}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\frac{1}{b_{s-1,i}}-\\frac{1}{a_{s,i}}\\right|=\\frac{\\left|\\sqrt{\\tilde{v}_{s,i}}-\\sqrt{v_{s-1,i}}+\\left(\\epsilon_{s}-\\epsilon_{s-1}\\right)\\right|}{b_{s-1,i}a_{s,i}}}\\\\ &{\\qquad\\qquad\\leq\\frac{1}{b_{s-1,i}a_{s,i}}\\frac{(1-\\beta_{2})\\,\\left|(\\mathscr{G}_{T}(s))^{2}-v_{s-1,i}\\right|}{\\sqrt{\\tilde{v}_{s,i}}+\\sqrt{v_{s-1,i}}}+\\frac{\\left|\\epsilon_{s}-\\epsilon_{s-1}\\right|}{b_{s-1,i}a_{s,i}}}\\\\ &{\\qquad\\qquad\\leq\\frac{1}{b_{s-1,i}a_{s,i}}\\cdot\\frac{(1-\\beta_{2})(\\mathscr{G}_{T}(s))^{2}}{\\sqrt{\\tilde{v}_{s,i}}+\\sqrt{v_{s-1,i}}}+\\frac{\\epsilon\\sqrt{1-\\beta_{2}}}{b_{s-1,i}a_{s,i}}\\leq\\frac{(\\mathscr{G}_{T}(s)+\\epsilon)\\sqrt{1-\\beta_{2}}}{b_{s-1,i}a_{s,i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the second inequality applies $v_{s-1,i}\\leq(\\mathcal{G}_{T}(s))^{2}$ in Lemma B.8 and the last inequality comes from $\\sqrt{1-\\beta_{2}}\\mathcal G_{T}(s)\\leq\\tilde{v}_{s,i}$ \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Proof of Lemma B.10. Applying the basic inequality and (35), for all $t\\in[T],i\\in[d]$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}g_{s,i}^{2}\\leq\\sum_{s=1}^{t}\\|g_{s}\\|^{2}\\leq2\\sum_{s=1}^{t}\\left(\\|\\bar{g}_{s}\\|^{2}+\\|\\xi_{s}\\|^{2}\\right)\\leq2\\mathsf{M}_{T}^{2}\\left(\\sigma_{0}^{2}t+\\sigma_{1}^{2}\\sum_{s=1}^{t}\\|\\bar{g}_{s}\\|^{p}+\\sum_{s=1}^{t}\\|\\bar{g}_{s}\\|^{2}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Combining with Lemma B.2, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}\\|\\bar{g}_{s}\\|^{p}\\leq\\displaystyle\\sum_{s=1}^{t}\\left(\\|\\bar{g}_{1}\\|+\\frac{L C_{0}\\sqrt{d}s}{\\sqrt{1-\\beta_{1}/\\beta_{2}}}\\right)^{p}\\leq t\\cdot\\left(\\|\\bar{g}_{1}\\|+\\frac{L C_{0}\\sqrt{d}t}{\\sqrt{1-\\beta_{1}/\\beta_{2}}}\\right)^{p}}\\\\ &{\\displaystyle\\sum_{s=1}^{t}\\|\\bar{g}_{s}\\|^{2}\\leq t\\cdot\\left(\\|\\bar{g}_{1}\\|+\\frac{L C_{0}\\sqrt{d}t}{\\sqrt{1-\\beta_{1}/\\beta_{2}}}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Further applying the definition of ${\\mathcal{F}}_{i}(t)$ in Lemma B.3, it leads to $\\mathcal{F}_{i}(t)\\leq\\mathcal{F}(t),\\forall i\\in[d]$ . Finally, since $\\mathcal{F}(t)$ is increasing with $t$ , we obtain the desired result. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Proof of Lemma B.1l. First, we have the following decomposition, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbf{A.1}=-\\sum_{s=1}^{t}\\eta_{s}\\left\\|\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\|^{2}\\underbrace{-\\sum_{s=1}^{t}\\eta_{s}\\left\\langle\\bar{g}_{s},\\frac{\\xi_{s}}{a_{s}}\\right\\rangle}_{\\mathbf{A.1.1}}+\\underbrace{\\sum_{s=1}^{t}\\eta_{s}\\left\\langle\\bar{g}_{s},\\left(\\frac{1}{a_{s}}-\\frac{1}{b_{s}}\\right)g_{s}\\right\\rangle}_{\\mathrm{A.1.2}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since (35) holds, we could apply Cauchy-Schwarz inequality, Lemma B.9, and $\\mathcal{G}_{T}(s)\\leq\\mathcal{G}_{T}(t),\\forall s\\leq$ $t$ from (19) to obtain that for ali $t\\in[T]$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{A.1.2}\\leq\\displaystyle\\sum_{i=1}^{d}\\sum_{s=1}^{t}\\eta_{s}\\left|\\frac{1}{a_{s,i}}-\\frac{1}{b_{s,i}}\\right|\\cdot\\left|\\bar{g}_{s,i}g_{s,i}\\right|\\leq\\displaystyle\\sum_{i=1}^{d}\\sum_{s=1}^{t}\\eta_{s}\\cdot\\frac{\\mathcal{G}_{T}(s)\\sqrt{1-\\beta_{2}}}{a_{s,i}b_{s,i}}\\cdot\\left|\\bar{g}_{s,i}g_{s,i}\\right|}\\\\ &{\\qquad\\leq\\displaystyle\\frac{1}{4}\\sum_{i=1}^{d}\\sum_{s=1}^{t}\\frac{\\eta_{s}\\bar{g}_{s,i}^{2}}{a_{s,i}}+\\left(1-\\beta_{2}\\right)\\sum_{i=1}^{d}\\sum_{s=1}^{t}\\frac{\\left(\\mathcal{G}_{T}(s)\\right)^{2}}{a_{s,i}}\\cdot\\frac{\\eta_{s}g_{s,i}^{2}}{b_{s,i}^{2}}}\\\\ &{\\qquad\\overset{(30),(97)}{\\leq}\\displaystyle\\frac{1}{4}\\sum_{s=1}^{t}\\eta_{s}\\left\\|\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\|^{2}+\\frac{\\eta\\mathcal{G}_{T}(t)\\sqrt{1-\\beta_{2}}}{1-\\beta_{1}}\\sum_{s=1}^{t}\\left\\|\\frac{g_{s}}{b_{s}}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Finally, combining with (37) for estimating A.1.1, we deduce the desired result in (39). ", "page_idx": 30}, {"type": "text", "text": "Proof of Lemma B.12. Let us denote $\\begin{array}{r}{\\Sigma:=\\frac{\\beta_{1}}{1-\\beta_{1}}\\left\\langle\\Delta_{s}\\odot(\\pmb{x}_{s}-\\pmb{x}_{s-1}),\\bar{\\pmb{g}}_{s}\\right\\rangle}\\end{array}$ where $\\Delta_{s}$ is defined in (20). We have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma\\underbrace{\\lambda_{1}}_{\\substack{1\\to\\beta_{1}}}\\cdot\\left|\\left\\langle\\Delta_{s}\\odot\\frac{\\eta_{s-1}m_{s-1}}{b_{s-1}},\\bar{g}_{s}\\right\\rangle\\right|=\\frac{\\beta_{1}}{1-\\beta_{1}}\\cdot\\left|\\left\\langle\\left(\\frac{\\eta_{s}}{b_{s}}-\\frac{\\eta_{s-1}}{b_{s-1}}\\right)\\odot m_{s-1},\\bar{g}_{s}\\right\\rangle\\right|}\\\\ &{\\quad\\le\\underbrace{\\frac{\\beta_{1}}{1-\\beta_{1}}\\cdot\\left|\\left\\langle\\left(\\frac{\\eta_{s}}{b_{s}}-\\frac{\\eta_{s}}{a_{s}}\\right)\\odot m_{s-1},\\bar{g}_{s}\\right\\rangle\\right|}_{\\Sigma_{1}}+\\underbrace{\\frac{\\beta_{1}}{1-\\beta_{1}}\\cdot\\left|\\left\\langle\\left(\\frac{\\eta_{s}}{a_{s}}-\\frac{\\eta_{s}}{b_{s-1}}\\right)\\odot m_{s-1},\\bar{g}_{s}\\right\\rangle\\right|}_{\\Sigma_{2}}}\\\\ &{\\quad\\mathrm{~\\}+\\underbrace{\\frac{\\beta_{1}}{1-\\beta_{1}}\\cdot\\left|\\left(\\eta_{s-1}-\\eta_{s}\\right)\\left\\langle\\frac{m_{s-1}}{b_{s-1}},\\bar{g}_{s}\\right\\rangle\\right|}_{\\Sigma_{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since (35) holds, we could apply Lemma B.9 and Young's inequality and then use (97), (30), $\\beta_{1}\\in[0,1)$ and $\\mathcal{G}_{T}(s)\\leq\\mathcal{G}_{T}(t)\\stackrel{}{\\leq}\\mathcal{G}_{T}(t)+\\epsilon,\\forall s\\leq t,$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{1}\\leq\\displaystyle\\sum_{i=1}^{d}\\displaystyle\\frac{\\beta_{1}}{1-\\beta_{1}}\\cdot\\frac{\\mathcal{G}_{T}(s)\\eta_{s}\\sqrt{1-\\beta_{2}}}{a_{s,i}b_{s,i}}\\cdot\\left|\\bar{g}_{s,i}m_{s-1,i}\\right|}\\\\ &{\\quad\\leq\\displaystyle\\sum_{i=1}^{d}\\frac{\\eta_{s}}{8}\\cdot\\frac{\\bar{g}_{s,i}^{2}}{a_{s,i}}+\\frac{2\\eta_{s}\\beta_{1}^{2}\\bigl(1-\\beta_{2}\\bigr)}{\\bigl(1-\\beta_{1}\\bigr)^{2}}\\sum_{i=1}^{d}\\frac{\\bigl(\\mathcal{G}_{T}(s)\\bigr)^{2}}{a_{s,i}}\\cdot\\frac{m_{s-1,i}^{2}}{b_{s,i}^{2}}}\\\\ &{\\quad\\leq\\displaystyle\\frac{\\eta_{s}}{8}\\left\\|\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\|^{2}+\\frac{2\\left(\\mathcal{G}_{T}(t)+\\epsilon\\right)\\eta\\sqrt{1-\\beta_{2}}}{\\bigl(1-\\beta_{1}\\bigr)^{3}}\\left\\|\\frac{m_{s-1}}{b_{s}}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Using the similar analysis for $\\Sigma_{1}$ , we also have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{2}\\leq\\displaystyle\\sum_{i=1}^{d}\\frac{\\eta_{s}\\beta_{1}}{1-\\beta_{1}}\\frac{\\sqrt{1-\\beta_{2}}}{a_{s,i}b_{s-1,i}}\\cdot(\\mathcal{G}_{T}(s)+\\epsilon)\\cdot|\\bar{g}_{s,i}\\cdot m_{s-1,i}|}\\\\ &{\\leq\\frac{\\eta_{s}}{8}\\left\\|\\frac{\\bar{g}_{s}}{\\sqrt{a_{s}}}\\right\\|^{2}+\\frac{2\\,(\\mathcal{G}_{T}(t)+\\epsilon)\\,\\eta\\sqrt{1-\\beta_{2}}}{(1-\\beta_{1})^{3}}\\left\\|\\frac{m_{s-1}}{b_{s-1}}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then we move to bound the summation of $\\Sigma_{3}$ over $s\\in\\{2,\\cdots,t\\}$ since $m_{0}=0$ . Recalling $\\eta_{s}$ in (30), we have the following decomposition, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{3}\\leq\\underbrace{\\frac{\\eta\\beta_{1}\\sqrt{1-\\beta_{2}^{s}}}{1-\\beta_{1}}\\left|\\left(\\frac{1}{1-\\beta_{1}^{s-1}}-\\frac{1}{1-\\beta_{1}^{s}}\\right)\\left\\langle\\bar{g}_{s},\\frac{m_{s-1}}{b_{s-1}}\\right\\rangle\\right|}_{\\Sigma_{3,1}}}\\\\ &{\\quad+\\underbrace{\\frac{\\eta\\beta_{1}}{(1-\\beta_{1})(1-\\beta_{1}^{s-1})}\\left|\\left(\\sqrt{1-\\beta_{2}^{s-1}}-\\sqrt{1-\\beta_{2}^{s}}\\right)\\left\\langle\\bar{g}_{s},\\frac{m_{s-1}}{b_{s-1}}\\right\\rangle\\right|}_{\\Sigma_{3,2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Noting that $\\|{\\bar{g}}_{s}\\|\\leq G_{s}\\leq G_{t},\\forall s\\leq t$ . Then further applying Cauchy-Schwarz inequality and Lemma B.2, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sqrt{1-\\beta_{2}^{s}}\\left|\\left\\langle\\bar{g}_{s},\\frac{m_{s-1}}{b_{s-1}}\\right\\rangle\\right|\\leq\\sqrt{1-\\beta_{2}^{s}}\\|\\bar{g}_{s}\\|\\left\\|\\frac{m_{s-1}}{b_{s-1}}\\right\\|\\leq\\sqrt{d}G_{t}\\sqrt{\\frac{(1-\\beta_{1})(1-\\beta_{1}^{s-1})}{(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Hence, summing $\\Sigma_{3.1}$ up over $s\\,\\in\\,[t]$ , applying $\\beta_{1}\\,\\in\\,(0,1)$ and noting that $\\Sigma_{3.1}$ vanishes when $s=1$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}\\Sigma_{3.1}\\leq\\frac{\\sqrt{d}\\eta G_{t}}{1-\\beta_{1}}\\cdot\\sqrt{\\frac{1-\\beta_{1}}{(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}}\\sum_{s=2}^{t}\\left(\\frac{1}{1-\\beta_{1}^{s-1}}-\\frac{1}{1-\\beta_{1}^{s}}\\right)}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{\\sqrt{d}\\eta G_{t}}{\\sqrt{(1-\\beta_{1})^{3}(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Similarly, using $\\|{\\bar{g}}_{s}\\|\\leq G_{s}\\leq G_{t},\\forall s\\leq t$ and $1-\\beta_{1}^{s-1}\\geq1-\\beta_{1}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{1}{1-\\beta_{1}^{s-1}}\\left|\\left\\langle\\bar{g}_{s},\\frac{m_{s-1}}{b_{s-1}}\\right\\rangle\\right|\\leq\\frac{1}{1-\\beta_{1}^{s-1}}\\|\\bar{g}_{s}\\|\\left\\|\\frac{m_{s-1}}{b_{s-1}}\\right\\|\\leq\\sqrt{d}G_{t}\\sqrt{\\frac{1}{(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Hence, summing $\\Sigma_{3.2}$ up over $s\\in[t]$ and still applying $\\beta_{1}\\in[0,1)$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\Sigma_{3.2}\\leq\\frac{\\sqrt{d}\\eta G_{t}}{1-\\beta_{1}}\\cdot\\sqrt{\\frac{1}{(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}}\\sum_{s=2}^{t}\\left(\\sqrt{1-\\beta_{2}^{s}}-\\sqrt{1-\\beta_{2}^{s-1}}\\right)}}\\\\ &{}&{\\leq\\frac{\\sqrt{d}\\eta G_{t}}{(1-\\beta_{1})\\sqrt{(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}}\\leq\\frac{\\sqrt{d}\\eta G_{t}}{\\sqrt{(1-\\beta_{1})^{3}(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Combinwin $\\textstyle\\sum_{s=1}^{t}\\Sigma_{3}$ Summing(102). (103) and (104) up over $s\\,\\in\\,[t]$ and combining with the estimation for $\\sum_{s=1}^{t}\\Sigma_{3}$ , we obtain the desired inequality in (40). \u53e3 ", "page_idx": 31}, {"type": "text", "text": "E  Omitted proof in Appendix C ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof of Lemma C.1. Recalling in (95), we have already shown that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|x_{s+1}-x_{s}\\|\\leq\\sqrt{d}\\|x_{s+1}-x_{s}\\|_{\\infty}\\leq\\eta\\sqrt{\\frac{d}{(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}},\\quad\\forall s\\geq1.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Applying the definition of $\\pmb{y}_{s}$ in (17), an intermediate result in (108) and $\\beta_{1}\\in[0,1)$ 6\uff0c ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|y_{s}-x_{s}\\|={\\frac{\\beta_{1}}{1-\\beta_{1}}}\\|x_{s}-x_{s-1}\\|\\leq{\\frac{\\eta}{1-\\beta_{1}}}{\\sqrt{\\frac{d}{(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}}},\\quad\\forall s\\geq1.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "6The inequality still holds for $s=1$ since $\\mathbf{x}_{1}=\\mathbf{y}_{1}$ ", "page_idx": 31}, {"type": "text", "text": "Recalling the iteration of $\\pmb{y}_{s}$ in (18) and then using Young's inequality ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|y_{s+1}-y_{s}\\|^{2}\\leq\\underbrace{2\\eta_{s}^{2}\\left\\|\\frac{g_{s}}{b_{s}}\\right\\|^{2}}_{(*)}+\\underbrace{\\frac{2\\beta_{1}^{2}}{(1-\\beta_{1})^{2}}\\left\\|\\frac{\\eta_{s}b_{s-1}}{\\eta_{s-1}b_{s}}-1\\right\\|_{\\infty}^{2}\\|x_{s}-x_{s-1}\\|^{2}}_{(**)}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Noting that $g_{s,i}/b_{s,i}\\leq1/\\sqrt{1-\\beta_{2}}$ from (16), we then combine with (30) to have ", "page_idx": 32}, {"type": "equation", "text": "$$\n(\\ast)\\leq2\\eta_{s}^{2}\\cdot\\frac{d}{1-\\beta_{2}}\\leq\\frac{2\\eta^{2}d}{(1-\\beta_{1})^{2}(1-\\beta_{2})}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Applying Lemma B.1 where max \u2264 1/\u03b22 and (108), ", "page_idx": 32}, {"type": "equation", "text": "$$\n(\\ast\\ast)\\leq\\frac{2\\eta^{2}\\beta_{1}^{2}\\Sigma_{\\mathrm{max}}^{2}d}{(1-\\beta_{1})^{2}(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}\\leq\\frac{2\\eta^{2}d}{\\beta_{2}(1-\\beta_{1})^{2}(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Summing up two estimations and using $0\\leq\\beta_{1}<\\beta_{2}<1$ , we finally have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|y_{s+1}-y_{s}\\|\\leq\\eta\\sqrt{\\frac{4d}{\\beta_{2}(1-\\beta_{1})^{2}(1-\\beta_{2})(1-\\beta_{1}/\\beta_{2})}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Combining with (108), (109) and (110), and using $0\\leq\\beta_{1}<\\beta_{2}<1$ , we then deduce a uniform bound for all the three gaps. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "Proof of Lemma C.2. Under the same conditions in Lemma C.1, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|y_{s}-x_{s}\\|\\leq{\\frac{1}{L_{q}}},\\quad\\|y_{s+1}-y_{s}\\|\\leq{\\frac{1}{L_{q}}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then, using the generalized smoothness in (8) ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla f(\\pmb{y}_{s})\\|\\leq\\|\\nabla f(\\pmb{x}_{s})\\|+\\|\\nabla f(\\pmb{y}_{s})-\\nabla f(\\pmb{x}_{s})\\|}\\\\ &{\\qquad\\qquad\\leq\\|\\nabla f(\\pmb{x}_{s})\\|+(L_{0}+L_{q}\\|\\nabla f(\\pmb{x}_{s})\\|^{q})\\|\\pmb{y}_{s}-\\pmb{x}_{s}\\|}\\\\ &{\\qquad\\qquad\\leq\\|\\nabla f(\\pmb{x}_{s})\\|+\\|\\nabla f(\\pmb{x}_{s})\\|^{q}+L_{0}/L_{q}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We could use a similar argument to deduce the bound for $\\|\\nabla f({\\pmb x}_{s})\\|$ . Further, combining with $\\mathcal{L}_{s}^{(x)}$ and $\\mathcal{L}_{s}^{(y)}$ in (64), we coul bound the eneralized smooth parameters as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{0}+L_{q}\\|\\nabla f(\\mathbf{x}_{s})\\|^{q}\\leq L_{0}+L_{q}G_{s}^{q}=\\mathcal{L}_{s}^{(x)},}\\\\ &{L_{0}+L_{q}\\|\\nabla f(\\mathbf{y}_{s})\\|^{q}\\leq L_{0}+L_{q}(\\|\\nabla f(\\mathbf{x}_{s})\\|+\\|\\nabla f(\\mathbf{x}_{s})\\|^{q}+L_{0}/L_{q})^{q}=\\mathcal{L}_{s}^{(y)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We could then deduce the first two inequalities in (67). Finally, (68) could be deduced by using the same argument in the proof of [49, Lemma A.3]. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "Proof of Lemma C.3. Given any $\\pmb{x}\\in\\mathbb{R}^{d}$ , we let ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\tau=\\frac{1}{L_{0}+L_{q}\\operatorname*{max}\\{\\|\\nabla f(\\pmb{x})\\|^{q},\\|\\nabla f(\\pmb{x})\\|\\}},\\quad\\hat{\\pmb{x}}=\\pmb{x}-\\tau\\nabla f(\\pmb{x}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "From the definition of $\\tau$ we could easily verify that $\\|\\hat{\\mathbf{x}}-\\mathbf{x}\\|=\\tau\\|\\nabla f(\\mathbf{x})\\|\\le1/L_{q}$ .Since $f$ is $(L_{0},L_{q})$ -smooth, we could thereby use the descent lemma in [49, Lemma A.3] such that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f(\\hat{x})\\leq f(x)+\\langle\\nabla f(x),\\hat{x}-x\\rangle+\\frac{L_{0}+L_{q}\\|\\nabla f(x)\\|^{q}}{2}\\|\\hat{x}-x\\|^{2}}\\\\ {\\quad\\quad=f(x)-\\tau\\|\\nabla f(x)\\|^{2}+\\frac{\\left(L_{0}+L_{q}\\|\\nabla f(x)\\|^{q}\\right)\\tau^{2}}{2}\\|\\nabla f(x)\\|^{2}\\leq f(x)-\\frac{\\tau}{2}\\|\\nabla f(x)\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since $f({\\hat{x}})\\geq f^{*}$ ,when $\\|\\nabla f({\\pmb x})\\|=0$ , the desired result is trivial. Let us suppose $\\|\\nabla f({\\pmb x})\\|>0$ ", "page_idx": 32}, {"type": "text", "text": "Case 1 $\\|\\nabla f(\\mathbf{x})\\|^{q}>\\|\\nabla f(\\mathbf{x})\\|$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{\\tau}{2}\\|\\nabla f(\\pmb{x})\\|^{2}=\\frac{\\|\\nabla f(\\pmb{x})\\|^{2-q}}{2L_{0}/\\|\\nabla f(\\pmb{x})\\|^{q}+2L_{q}}\\leq f(\\pmb{x})-f(\\pmb{\\hat{x}})\\leq f(\\pmb{x})-f^{*}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "When $\\|\\nabla f(\\mathbf{x})\\|^{q}<L_{0}/L_{q}$ it leads to ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{\\|\\nabla f(\\pmb{x})\\|^{2}}{4L_{0}}=\\frac{\\|\\nabla f(\\pmb{x})\\|^{2-q}}{4L_{0}/\\|\\nabla f(\\pmb{x})\\|^{q}}\\leq\\frac{\\|\\nabla f(\\pmb{x})\\|^{2-q}}{2L_{0}/\\|\\nabla f(\\pmb{x})\\|^{q}+2L_{q}}\\leq f(\\pmb{x})-f^{*}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "When $\\|\\nabla f(\\mathbf{x})\\|^{q}\\geq L_{0}/L_{q}$ , it leads to ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{\\|\\nabla f(\\pmb{x})\\|^{2-q}}{4L_{q}}\\leq\\frac{\\|\\nabla f(\\pmb{x})\\|^{2-q}}{2L_{0}/\\|\\nabla f(\\pmb{x})\\|^{q}+2L_{q}}\\leq f(\\pmb{x})-f^{*}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We then deduce that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla f(\\pmb{x})\\|\\leq\\operatorname*{max}\\left\\{[4L_{q}(f(\\pmb{x})-f^{*})]^{\\frac{1}{2-q}}\\,,\\sqrt{4L_{0}(f(\\pmb{x})-f^{*})}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Case 2 $\\|\\nabla f(\\mathbf{x})\\|^{q}\\leq\\|\\nabla f(\\mathbf{x})\\|$ We could rely on the similar analysis to obtain that? ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla f(\\pmb{x})\\|\\leq\\operatorname*{max}\\left\\{4L_{q}(f(\\pmb{x})-f^{*}),\\sqrt{4L_{0}(f(\\pmb{x})-f^{*})}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Combining (112) and (113), we then deduce the desired result. ", "page_idx": 33}, {"type": "text", "text": "Proof of Lemma C.5. Recalling (95), we then obtained that when $\\eta=\\tilde{C}_{0}\\sqrt{1-\\beta_{2}}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|x_{s}-x_{s-1}\\|\\leq\\sqrt{d}\\|x_{s}-x_{s-1}\\|_{\\infty}\\leq\\tilde{C}_{0}\\sqrt{\\frac{d}{1-\\beta_{1}/\\beta_{2}}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Noting that when (66) holds, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\bar{g}_{s}\\|\\leq\\|\\bar{g}_{s-1}\\|+\\|\\bar{g}_{s}-\\bar{g}_{s-1}\\|\\leq\\|\\bar{g}_{s-1}\\|+(L_{0}+L_{q}\\|\\bar{g}_{s-1}\\|^{q})\\|x_{s}-x_{s-1}\\|}\\\\ {\\leq\\|\\bar{g}_{s-1}\\|+\\tilde{C}_{0}\\mathcal{L}_{s-1}^{(x)}\\sqrt{\\frac{d}{1-\\beta_{1}/\\beta_{2}}}\\leq\\|\\bar{g}_{1}\\|+\\tilde{C}_{0}\\sqrt{\\frac{d}{1-\\beta_{1}/\\beta_{2}}}\\sum_{j=1}^{s-1}\\mathcal{L}_{j}^{(x)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Using $\\mathcal{L}_{j}^{(x)}\\le\\mathcal{L}_{t}^{(x)},\\forall j\\le t$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\|\\bar{g}_{s}\\|^{p}\\leq\\sum_{s=1}^{t}\\left(\\|\\bar{g}_{1}\\|+\\tilde{C}_{0}\\sqrt{\\frac{d}{1-\\beta_{1}/\\beta_{2}}}(s-1)\\mathcal{L}_{t}^{(x)}\\right)^{p}\\leq t\\left(\\|\\bar{g}_{1}\\|+t\\tilde{C}_{0}\\mathcal{L}_{t}^{(x)}\\sqrt{\\frac{d}{1-\\beta_{1}/\\beta_{2}}}\\right)^{p}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Similarly, we also have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\|\\bar{g}_{s}\\|^{2}\\leq t\\left(\\|\\bar{g}_{1}\\|+t\\tilde{C}_{0}\\mathcal{L}_{t}^{(x)}\\sqrt{\\frac{d}{1-\\beta_{1}/\\beta_{2}}}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Further combining with ${\\mathcal{F}}_{i}(t)$ in Lemma B.3 and $\\mathcal{I}(t)$ in (71), ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{F}_{i}(t)\\leq1+\\frac{1}{\\epsilon^{2}}\\sum_{s=1}^{t}\\|g_{s}\\|^{2}\\leq\\mathcal{J}(t),\\quad\\forall t\\in[T],i\\in[d].\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 34}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 You should answer [Yes] , [No] , or [NA] .   \n\u00b7 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u00b7 Please provide a short (1-2 sentence) justification right after your answer (even for NA). ", "page_idx": 34}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 34}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 34}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u00b7 Keep the checklist subsection headings, questions/answers and guidelines below. \u00b7 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer:[NA]   \nJustification: [NA]   \nGuidelines: \u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)  If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so ^No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 36}, {"type": "text", "text": "\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks. ", "page_idx": 38}, {"type": "text", "text": "\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with humansubjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]