{"references": [{"fullname_first_author": "Yossi Arjevani", "paper_title": "Lower bounds for non-convex stochastic optimization", "publication_date": "2023-MM-DD", "reason": "This paper provides lower bounds for non-convex stochastic optimization, which helps to contextualize the convergence rate achieved by the Adam optimizer in the main paper."}, {"fullname_first_author": "Amit Attia", "paper_title": "SGD with AdaGrad stepsizes: full adaptivity with high probability to unknown parameters, unbounded gradients and affine variance", "publication_date": "2023-MM-DD", "reason": "This paper analyzes the convergence of SGD with AdaGrad stepsizes under affine variance noise, offering insights into the behavior of adaptive gradient methods that inform the analysis of Adam in the main paper."}, {"fullname_first_author": "L\u00e9on Bottou", "paper_title": "Optimization methods for large-scale machine learning", "publication_date": "2018-MM-DD", "reason": "This paper provides a comprehensive overview of optimization methods for large-scale machine learning, serving as a foundational reference for understanding the context and challenges of optimizing non-convex functions."}, {"fullname_first_author": "Matthew Faw", "paper_title": "The power of adaptivity in SGD: self-tuning step sizes with unbounded gradients and affine variance", "publication_date": "2022-MM-DD", "reason": "This paper demonstrates the benefits of adaptive step sizes in SGD under affine variance noise, which is a related setting to the analysis of Adam in the main paper."}, {"fullname_first_author": "Sashank J. Reddi", "paper_title": "On the convergence of Adam and beyond", "publication_date": "2018-MM-DD", "reason": "This paper offers a critical analysis of Adam's convergence properties, highlighting its limitations and motivating the need for more rigorous analysis under relaxed assumptions, as performed in the main paper."}]}