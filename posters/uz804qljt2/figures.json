[{"figure_path": "Uz804qLJT2/figures/figures_2_1.jpg", "caption": "Figure 1: Scheme of the model and theory (a) Scheme of the model in terms of attention paths. (b) The order parameter assigns to each pair of paths a weight, given by the overlap between the corresponding effective weights. (c) Alignment of the kernel PCs with the vector of task labels Y, in the finite-width (FW) vs GP regimes. (d) Kernel as the weighted sum of many path-path kernels. Task-relevant kernel combination occurs in the finite-width regime (FW), but not in the GP limit, in which cross-path kernels are discarded, and same-path kernels are equally weighted. The result is an improved kernel-task alignment in the finite-width regime (shown in (c)), enhancing generalization.", "description": "This figure illustrates the model and theoretical framework used in the paper. Panel (a) shows a schematic representation of the model, highlighting the concept of attention paths, which are sequences of attention heads across layers. Panel (b) explains how the order parameter assigns weights to these attention paths based on the overlap of their effective weights. Panel (c) displays the alignment of kernel principal components (PCs) with the task labels, contrasting the finite-width regime with the Gaussian process (GP) limit. Finally, panel (d) summarizes the key finding: the finite-width regime combines multiple path-path kernels to create a task-relevant kernel, improving generalization, in contrast to the GP limit, which discards cross-path kernels and weighs same-path kernels equally.", "section": "1 Introduction"}, {"figure_path": "Uz804qLJT2/figures/figures_6_1.jpg", "caption": "Figure 2: Hidden Markov chain task. (a) Illustration of the task. (b) Schematics of the network and its attention paths. (c) Top: Classification accuracy for varying N (theory: blue crosses, joined by blue line; samples: black dots). Red lines: GP limit for a network consisting of all paths (solid), the good path (dashed), and the good and denoising paths (dotted). Bottom: Matrix elements of U, for varying N. The matrix indices are labeled with the corresponding path name, according to the legend in (b). (d) Normalized overlap, or cosine similarity, between the PCs of the kernel K and the vector of task labels Y (N = 10: blue; GP limit: orange). PCs are ranked by their eigenvalues, from largest to smallest. Only the first 30 PCs are shown. (e) Same as (c), but for increased \u03c3\u03c4 = 5 and a network consisting of only the good and denoising paths.", "description": "This figure illustrates the results of a hidden Markov chain classification task.  Panel (a) shows a schematic of the task, illustrating the hidden and visible states of the Markov chain. Panel (b) shows a schematic of the network architecture with the different attention paths. Panel (c) compares the classification accuracy of the network for different network widths (N), contrasting the theoretical predictions with empirical results obtained from sampling. The bottom half of (c) shows the order parameter (U) for different network widths, visualizing the interplay between attention paths. Panel (d) displays the overlap between kernel principal components (PCs) and task labels, highlighting the alignment improvement in the finite-width regime. Finally, panel (e) repeats the experiment in (c) but with increased noise and a reduced number of paths.", "section": "4.1 Hidden Markov chain sequence classification"}, {"figure_path": "Uz804qLJT2/figures/figures_7_1.jpg", "caption": "Figure 3: One-shot image classification task. (a) Scheme of the task. (b) Classification accuracy in the GP limit (red line) and the finite-width regime (FW) for varying N (theory: blue crosses, joined by blue line; samples: black dots). (c) Matrix elements of U. The \"theory\" and \"sampled\" Us are for N = 10. The matrix indices are labeled with the path index \u03c0 = (h1, h2). (d) Kernel PCs' overlap with the task, in the GP limit and in the finite-width regime for N = 10. Only the first 50 PCs are shown. (e) Head score (blue) and performance drop (red) after pruning the head, for the model trained with gradient descent. (f) Classification accuracy of the model trained with gradient descent, after pruning a growing number of heads, in order of their head score.", "description": "This figure presents the results of the one-shot image classification experiments. It shows the classification accuracy for varying network widths N in both the GP limit and the finite-width regime. The figure also depicts the elements of the order parameter U, illustrating the interplay of attention paths. Kernel principal components (PCs) are visualized, highlighting the kernel-task alignment. Finally, the figure shows the head scores and performance drop after pruning, demonstrating the effectiveness of head pruning for model reduction.", "section": "4 Experiments"}, {"figure_path": "Uz804qLJT2/figures/figures_18_1.jpg", "caption": "Figure 1: Scheme of the model and theory (a) Scheme of the model in terms of attention paths. (b) The order parameter assigns to each pair of paths a weight, given by the overlap between the corresponding effective weights. (c) Alignment of the kernel PCs with the vector of task labels Y, in the finite-width (FW) vs GP regimes. (d) Kernel as the weighted sum of many path-path kernels. Task-relevant kernel combination occurs in the finite-width regime (FW), but not in the GP limit, in which cross-path kernels are discarded, and same-path kernels are equally weighted. The result is an improved kernel-task alignment in the finite-width regime (shown in (c)), enhancing generalization.", "description": "This figure illustrates the model and theory presented in the paper. Panel (a) shows a schematic representation of the model, highlighting the concept of attention paths as information pathways through the attention heads across different layers. Panel (b) explains the role of the order parameter in assigning weights to pairs of attention paths based on their overlap, essentially emphasizing the interaction between these paths. Panel (c) compares the alignment of principal components (PCs) of the kernel with task labels in the finite-width (FW) and Gaussian process (GP) regimes. The FW regime exhibits better alignment due to the interplay of attention paths. Finally, panel (d) demonstrates how the kernel is composed of multiple path-path kernels that are combined in the FW regime to improve generalization performance, which is not the case in the GP limit.", "section": "1 Introduction"}, {"figure_path": "Uz804qLJT2/figures/figures_32_1.jpg", "caption": "Figure 2: Hidden Markov chain task. (a) Illustration of the task. (b) Schematics of the network and its attention paths. (c) Top: Classification accuracy for varying N (theory: blue crosses, joined by blue line; samples: black dots). Red lines: GP limit for a network consisting of all paths (solid), the good path (dashed), and the good and denoising paths (dotted). Bottom: Matrix elements of U, for varying N. The matrix indices are labeled with the corresponding path name, according to the legend in (b). (d) Normalized overlap, or cosine similarity, between the PCs of the kernel K and the vector of task labels Y (N = 10: blue; GP limit: orange). PCs are ranked by their eigenvalues, from largest to smallest. Only the first 30 PCs are shown. (e) Same as (c), but for increased \u03c3\u03c4 = 5 and a network consisting of only the good and denoising paths.", "description": "This figure shows results from a Hidden Markov Chain classification task. Panel (a) illustrates the task's setup. Panel (b) provides a schematic of the network architecture and its attention paths. Panel (c) presents a comparison of classification accuracy (top) and order parameter (bottom) across different network widths (N). Panel (d) displays the alignment between kernel principal components (PCs) and task labels. Panel (e) repeats the analysis of panel (c) but under noisier conditions.", "section": "4.1 Hidden Markov chain sequence classification"}, {"figure_path": "Uz804qLJT2/figures/figures_35_1.jpg", "caption": "Figure 2: Hidden Markov chain task. (a) Illustration of the task. (b) Schematics of the network and its attention paths. (c) Top: Classification accuracy for varying N (theory: blue crosses, joined by blue line; samples: black dots). Red lines: GP limit for a network consisting of all paths (solid), the good path (dashed), and the good and denoising paths (dotted). Bottom: Matrix elements of U, for varying N. The matrix indices are labeled with the corresponding path name, according to the legend in (b). (d) Normalized overlap, or cosine similarity, between the PCs of the kernel K and the vector of task labels Y (N = 10: blue; GP limit: orange). PCs are ranked by their eigenvalues, from largest to smallest. Only the first 30 PCs are shown. (e) Same as (c), but for increased \u03c3\u03c4 = 5 and a network consisting of only the good and denoising paths.", "description": "This figure shows results for the hidden Markov chain sequence classification task. It includes schematics of the task and network, classification accuracy for different network widths (N), and the order parameter U, which captures the interplay of attention paths.  The plots show how classification accuracy improves in the finite-width regime (N>0) over the Gaussian Process (GP) limit, illustrating task-relevant kernel combination via attention paths.", "section": "4.1 Hidden Markov chain sequence classification"}, {"figure_path": "Uz804qLJT2/figures/figures_36_1.jpg", "caption": "Figure 2: Hidden Markov chain task. (a) Illustration of the task. (b) Schematics of the network and its attention paths. (c) Top: Classification accuracy for varying N (theory: blue crosses, joined by blue line; samples: black dots). Red lines: GP limit for a network consisting of all paths (solid), the good path (dashed), and the good and denoising paths (dotted). Bottom: Matrix elements of U, for varying N. The matrix indices are labeled with the corresponding path name, according to the legend in (b). (d) Normalized overlap, or cosine similarity, between the PCs of the kernel K and the vector of task labels Y (N = 10: blue; GP limit: orange). PCs are ranked by their eigenvalues, from largest to smallest. Only the first 30 PCs are shown. (e) Same as (c), but for increased \u03c3\u03c4 = 5 and a network consisting of only the good and denoising paths.", "description": "This figure shows the results of experiments on a synthetic Hidden Markov Chain classification task.  Panel (a) illustrates the task setup. Panel (b) provides a schematic of the network architecture and the different attention paths through the network. Panel (c) displays classification accuracy as a function of network width (N), comparing theoretical predictions to empirical results from sampled networks.  It also shows the elements of the order parameter U for different N values. Panel (d) shows the alignment between kernel principal components (PCs) and task labels.  Finally, panel (e) replicates panel (c) but with increased noise and only the 'good' and 'denoising' paths active.", "section": "4.1 Hidden Markov chain sequence classification"}]