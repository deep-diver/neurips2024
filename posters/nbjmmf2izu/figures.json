[{"figure_path": "nBjmMF2IZU/figures/figures_0_1.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the proposed framework for fine-tuning large Vision-Language Models (VLMs) using reinforcement learning (RL).  The process starts with an environment providing a batch of observations and rewards. These are combined with a predesigned prompt and fed into a pretrained VLM. The VLM generates an utterance incorporating chain-of-thought reasoning and a textual action.  This action is then parsed and sent to the environment, producing a reward that is used to fine-tune the entire VLM via RL.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_3_1.jpg", "caption": "Figure 2: A diagram of the proposed RL fine-tuning framework. At time step t, the state st contains an input prompt vi and a visual observation ot. The VLM takes st = [ot, vi] as input and outputs open-ended text vut containing the CoT reasoning, keywords \"action\": \"at\", and the log-likelihood of vout. We first apply a post-processing function f on vout, to obtain a legal action at which can interact with the environment. Then, we input at to the environment for obtaining reward r(st, at) and the next observation Ot+1. Afterward, we devise a method to compute a numerical value of \u03c0\u03bf(at|ot, v). Finally, we use r(st, at) and \u03c0\u03bf(at|ot, v) for the RL training.", "description": "This figure illustrates the Reinforcement Learning (RL) fine-tuning framework for Vision-Language Models (VLMs).  At each time step, the VLM receives visual observation (ot) and a task-specific prompt (vin) as input. The VLM generates open-ended text (vout) which includes chain-of-thought reasoning and a text-based action.  A post-processing function (f) converts this text action into an executable action (at) to interact with the environment. The environment then provides a reward (r(st, at)) and the next observation (Ot+1), which are used to fine-tune the VLM via Proximal Policy Optimization (PPO). The log-likelihood of the VLM's output is also computed and used in the training process. ", "section": "Training VLMs with RL"}, {"figure_path": "nBjmMF2IZU/figures/figures_4_1.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the overall framework of the proposed method. It starts with an environment providing a batch of observations and rewards. A pretrained vision-language model takes these observations along with a prompt, generates a chain of thought reasoning and outputs a text-based action. This action is then parsed and fed back to the environment. The generated reward is used for reinforcement learning (RL) to fine-tune the entire vision-language model.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_6_1.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the overall framework of the proposed method. It shows how a pretrained vision-language model is fine-tuned using reinforcement learning to become a decision-making agent.  The model receives an observation from the environment and a prompt. It then generates a chain of thought (CoT) reasoning and a text-based action. The action is executed in the environment, yielding a reward, which is then used to fine-tune the model via RL.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_8_1.jpg", "caption": "Figure 5: Episode success rates (%) of different methods on gym_cards and alfworld during training. Left to right: gym_cards/Numberline, gym_cards/EZPoints, gym_cards/Blackjack, and alfworld (all). The curves of Points24 are not included because none of the tested methods achieve reasonable performance.", "description": "This figure shows the training curves of different methods (Ours, CNN+RL, GPT4-V, Gemini, LLaVA-sft) on four tasks (NumberLine, EZPoints, Blackjack from gym_cards and alfworld).  The x-axis represents the number of environment steps, and the y-axis represents the episode success rate (%).  The plot visualizes the learning progress of each method, showing how their performance improves over time. The Points24 task is excluded due to poor performance by all methods.  The plot provides a direct comparison of the proposed method's performance against existing baselines and commercial models.", "section": "6 Experimental Results"}, {"figure_path": "nBjmMF2IZU/figures/figures_9_1.jpg", "caption": "Figure 5: Episode success rates (%) of different methods on gym_cards and alfworld during training. Left to right: gym_cards/Numberline, gym_cards/EZPoints, gym_cards/Blackjack, and alfworld (all). The curves of Points24 are not included because none of the tested methods achieve reasonable performance.", "description": "This figure shows the training curves of different methods on four tasks: NumberLine, EZPoints, Blackjack from the gym_cards environment and alfworld.  The x-axis represents the number of environment steps, and the y-axis represents the episode success rate (%).  The figure compares the performance of the proposed method (Ours) against other methods like GPT4-V, Gemini, a supervised fine-tuned version of LLaVa (LLaVa-sft), and a CNN-based RL approach (CNN+RL).  It demonstrates that the proposed method generally achieves higher success rates across all tasks compared to the baselines. The Points24 task is excluded because none of the tested methods performed well on this task.", "section": "Experimental Results"}, {"figure_path": "nBjmMF2IZU/figures/figures_9_2.jpg", "caption": "Figure 5: Episode success rates (%) of different methods on gym_cards and alfworld during training. Left to right: gym_cards/Numberline, gym_cards/EZPoints, gym_cards/Blackjack, and alfworld (all). The curves of Points24 are not included because none of the tested methods achieve reasonable performance.", "description": "This figure displays the training curves for four different tasks (NumberLine, EZPoints, Blackjack, and alfworld) comparing several different methods: Our method, CNN+RL, GPT4-V, Gemini, and LLaVA-sft.  The x-axis represents the number of environment steps, and the y-axis represents the episode success rate (%).  The curves show how each method's performance improves over time during training. Notably, the Points24 task is excluded because none of the tested methods achieved a reasonable success rate. This figure visually demonstrates the effectiveness of the proposed method in enhancing the decision-making capabilities of VLMs across various tasks.", "section": "Experimental Results"}, {"figure_path": "nBjmMF2IZU/figures/figures_15_1.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the proposed framework for fine-tuning large Vision-Language Models (VLMs) using reinforcement learning (RL).  The process begins with the VLM receiving both a visual observation (from an environment like Blackjack) and a pre-designed prompt.  The VLM then generates an utterance containing reasoning steps (Chain-of-Thought) and a text-based action. This text action is interpreted by a parser, fed back into the environment to produce a reward, and finally, the entire VLM is fine-tuned using the generated reward via RL.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_16_1.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the proposed framework for fine-tuning large vision-language models (VLMs) using reinforcement learning (RL).  It shows how the VLM processes visual and textual information, generates a chain of thought reasoning leading to a text-based action, and then receives a reward from interacting with the environment. This reward is then used to fine-tune the entire VLM, making it a better decision-making agent.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_16_2.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the proposed framework for training large vision-language models (VLMs) using reinforcement learning (RL).  The process begins with the VLM receiving an observation and a prompt. The VLM then generates an utterance containing chain-of-thought reasoning and a text-based action.  This action is parsed and sent to the environment, generating rewards that are then used to fine-tune the entire VLM via RL. The figure highlights the key components, including the environment, observations and rewards, the pretrained VLM, the RL fine-tuning process, and the actions generated by the VLM.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_17_1.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the proposed framework for training large Vision-Language Models (VLMs) using reinforcement learning (RL). The process starts with the VLM receiving both the current observation from the environment and a pre-designed prompt.  The VLM then generates an utterance containing chain-of-thought reasoning and a text-based action. This action is parsed and sent to the environment, which returns task rewards. Finally, RL utilizes the task rewards to fine-tune the whole VLM.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_17_2.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the overall framework of the proposed method.  It shows how a pretrained vision-language model is fine-tuned using reinforcement learning for decision-making. The model receives an image (observation) and a pre-defined prompt. It then generates a chain of thought, culminating in a text-based action. This action is passed to the environment, which returns a reward. The reward is then used to fine-tune the entire vision-language model via reinforcement learning.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_18_1.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the proposed framework for fine-tuning large Vision-Language Models (VLMs) using reinforcement learning (RL).  The VLM receives an observation (e.g., image from a game) and a prompt. It then generates a chain of thought (CoT) reasoning and outputs a text-based action. This action is parsed and sent to the environment, which provides a reward. This reward is used in the RL process to fine-tune the entire VLM, improving its decision-making abilities in the specific task.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_18_2.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the proposed framework for training large Vision-Language Models (VLMs) using reinforcement learning (RL).  The VLM receives visual input (e.g., a blackjack game state) and a prompt. It then generates an utterance with chain-of-thought reasoning and a text-based action. This action is interpreted by the environment, yielding rewards. Finally, RL uses these rewards to fine-tune the entire VLM.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_18_3.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure provides a high-level overview of the proposed reinforcement learning framework for fine-tuning large vision-language models.  It shows how the VLM processes visual and textual information at each step, generates chain-of-thought reasoning and a textual action, and uses the environment's feedback to adjust its decision-making process through RL fine-tuning.  The process is shown as a flow diagram encompassing the environment, the VLM, and the RL process.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_19_1.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the proposed framework for training large Vision-Language Models (VLMs) using reinforcement learning (RL).  The process begins with the VLM receiving an observation and a pre-designed prompt. The VLM then generates an utterance that includes chain-of-thought reasoning and a text-based action. This action is fed to the environment, resulting in a reward.  This reward is used to fine-tune the entire VLM via RL.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_20_1.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the proposed framework for training large Vision-Language Models (VLMs) using reinforcement learning. The VLM receives an observation and a prompt, generates a chain of thought reasoning and a text-based action, which is then parsed and sent to the environment.  The environment provides a reward, and this reward is used to fine-tune the entire VLM via reinforcement learning.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_22_1.jpg", "caption": "Figure 5: Episode success rates (%) of different methods on gym_cards and alfworld during training. Left to right: gym_cards/Numberline, gym_cards/EZPoints, gym_cards/Blackjack, and alfworld (all). The curves of Points24 are not included because none of the tested methods achieve reasonable performance.", "description": "This figure shows the training curves of episode success rates for different methods across four tasks in two domains. The x-axis represents the number of environment steps. The y-axis represents the episode success rate. The four tasks are NumberLine, EZPoints, Blackjack from the gym_cards domain and all tasks from the alfworld domain. The methods compared are Ours, CNN+RL, GPT4-V, Gemini, and LLaVA-sft. The Points24 task is excluded because no methods achieved reasonable performance.", "section": "6.1 Improving VLM Decision-Making Capabilities"}, {"figure_path": "nBjmMF2IZU/figures/figures_23_1.jpg", "caption": "Figure 5: Episode success rates (%) of different methods on gym_cards and alfworld during training. Left to right: gym_cards/Numberline, gym_cards/EZPoints, gym_cards/Blackjack, and alfworld (all). The curves of Points24 are not included because none of the tested methods achieve reasonable performance.", "description": "This figure presents the training curves of several methods, including the proposed method, on four different tasks across two domains: gym_cards and alfworld.  The gym_cards tasks (NumberLine, EZPoints, Blackjack) assess arithmetic reasoning and visual recognition, while the alfworld task tests visual semantic understanding in an embodied AI setting.  The plots show the episode success rate over the number of environment steps during training. The Points24 task's results were omitted due to poor performance from all evaluated methods.", "section": "Experimental Results"}, {"figure_path": "nBjmMF2IZU/figures/figures_23_2.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the proposed framework for training large Vision-Language Models (VLMs) using reinforcement learning (RL).  The process begins with the VLM receiving both the current observation from the environment and a predefined prompt. The VLM then generates an utterance including a chain-of-thought reasoning process and a text-based action. This action is interpreted by the environment, which provides a reward to the VLM. The reward is used to fine-tune the entire VLM via RL, improving its decision-making ability.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_23_3.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure provides a high-level overview of the proposed reinforcement learning framework for training large vision-language models (VLMs) as decision-making agents.  It illustrates the process:  The VLM receives an observation from the environment and a pre-defined prompt.  It then uses chain-of-thought reasoning to generate a text-based action. This action is parsed and sent to the environment to obtain a reward. Finally, this reward signal is used to fine-tune the VLM using reinforcement learning.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_24_1.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the overall framework of the proposed method.  It shows how a pretrained vision-language model is fine-tuned using reinforcement learning. The process involves taking an observation from the environment, incorporating a pre-designed prompt, generating a chain of thought (CoT) reasoning and a text-based action from the VLM, parsing the action into the environment, receiving task rewards, and finally using these rewards to fine-tune the entire VLM.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_24_2.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the overall framework of the proposed method.  A pretrained vision-language model receives an observation and a prompt.  It then generates a chain of thought, reasoning, and a text-based action. This action is sent to an environment and feedback (rewards) are used to fine-tune the vision-language model through reinforcement learning.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_25_1.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the proposed framework for training large vision-language models (VLMs) using reinforcement learning (RL).  The VLM receives an observation and a prompt, generates a chain of thought reasoning and a text-based action, which is then executed in an environment to obtain a reward.  This reward is used to fine-tune the VLM via RL. The diagram shows the interaction between the VLM, the environment, and the RL algorithm.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_25_2.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the proposed framework for fine-tuning large vision-language models (VLMs) using reinforcement learning.  The VLM receives visual input (e.g., a game state) and a prompt. It then generates a chain of thought (CoT) to reason through the problem and outputs a text-based action. This action is parsed, fed to an environment, which returns a reward signal.  The reward is then used to fine-tune the VLM via reinforcement learning, improving its decision-making abilities in interactive environments. The Blackjack game example shows the input (game state), the CoT reasoning, the action ('stand'), and the overall process.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_26_1.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the overall framework of the proposed method.  It shows how a pretrained vision-language model is fine-tuned using reinforcement learning. The model receives visual input (e.g., a game board), along with a prompt describing the task. The model then generates a chain of thought, explaining its reasoning process, before outputting a text-based action. This action is interpreted and executed within the environment, yielding a reward. This reward is then used to further fine-tune the vision-language model, improving its decision-making capabilities.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_26_2.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure provides a high-level overview of the proposed reinforcement learning framework for fine-tuning large vision-language models (VLMs) as decision-making agents.  The process begins with the VLM receiving both visual input (from the environment) and a predefined prompt. The VLM then generates an utterance, incorporating chain-of-thought reasoning and a text-based action.  This action is interpreted and sent to the environment, resulting in a reward.  This reward signal is then used to fine-tune the entire VLM via reinforcement learning, improving its decision-making abilities over time.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_27_1.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the proposed framework for training large Vision-Language Models (VLMs) using reinforcement learning (RL).  The VLM receives an observation (e.g., a game state image) and a prompt, then generates an utterance consisting of reasoning steps (chain of thought) and a final action. This action is given to the environment, which generates rewards based on the action's effect on the game.  Finally, the rewards are used to fine-tune the VLM via RL. This method allows VLMs to become effective decision-making agents in multi-step tasks.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_27_2.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the method overview of training large vision-language models (VLMs) using reinforcement learning (RL). The process starts with an environment providing batch observations and rewards. A pretrained VLM receives these observations along with a designed prompt. Then, the VLM generates an utterance including chain-of-thought reasoning and a text-based action. This action is then parsed and fed into the environment to get rewards. Finally, RL uses these rewards to fine-tune the whole VLM.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_28_1.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure illustrates the proposed framework for fine-tuning large Vision-Language Models (VLMs) using Reinforcement Learning (RL). The VLM receives an observation and a prompt, generates a chain of thought reasoning and a text-based action, which is then executed in the environment to obtain rewards. The rewards are used to fine-tune the entire VLM via RL.", "section": "Abstract"}, {"figure_path": "nBjmMF2IZU/figures/figures_29_1.jpg", "caption": "Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM.", "description": "This figure provides a high-level overview of the proposed method.  It shows how a pretrained vision-language model is fine-tuned using reinforcement learning to act as a decision-making agent in various environments. The model receives observations from the environment and a prompt, generating a chain of thought and a text-based action. This action is then executed in the environment, yielding rewards that are used to further train the model via reinforcement learning.", "section": "Abstract"}]