[{"type": "text", "text": "Parallelizing Model-based Reinforcement Learning Over the Sequence Length ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "ZiRui Wang Yue Deng Junfeng Long Zhejiang University, China Zhejiang University, China Shanghai AI Laboratory, China ziseoiwong@zju.edu.cn devindeng@zju.edu.cn junfeng@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Yin Zhang\u2217 Zhejiang University, China zhangyin98@zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, Model-based Reinforcement Learning (MBRL) methods have demonstrated stunning sample efficiency in various RL domains. However, achieving this extraordinary sample efficiency comes with additional training costs in terms of computations, memory, and training time. To address these challenges, we propose the Parallelized Model-based Reinforcement Learning (PaMoRL) framework. PaMoRL introduces two novel techniques: the Parallel World Model (PWM) and the Parallelized Eligibility Trace Estimation (PETE) to parallelize both model learning and policy learning stages of current MBRL methods over the sequence length. Our PaMoRL framework is hardware-efficient and stable, and it can be applied to various tasks with discrete or continuous action spaces using a single set of hyperparameters. The empirical results demonstrate that the PWM and PETE within PaMoRL significantly increase training speed without sacrificing inference efficiency. In terms of sample efficiency, PaMoRL maintains an MBRLlevel sample efficiency that outperforms other no-look-ahead MBRL methods and model-free RL methods, and it even exceeds the performance of planning-based MBRL methods and methods with larger networks in certain tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Model-based Reinforcement Learning (MBRL) is widely believed to have the great potential to substantially enhance sample efficiency by training a policy through a learned world model [1, 2, 3]. Previous studies [4, 5, 3, 6] achieve the same asymptotic performance as their model-free counterparts while requiring orders of magnitude less interactions. In particular, some recent works have even achieved human-level efficiency in complex RL domains like Atari [7, 8, 9] and robot control [10, 11]. ", "page_idx": 0}, {"type": "text", "text": "MBRL methods can be generally divided into two stages: model learning and policy learning. During the model learning stage, a parameterized world model is required to predict the environmental dynamics by constructing specific self-supervised learning tasks. The policy learning stage benefits from synthetic interactions between the policy and the world model, hence on-policy actor-critic methods or planning methods such as Model Predictive Path Integral (MPPI) [12, 13] or Monte-Carlo Tree Search (MCTS) [7, 9] can be used for policy improvement. To obtain better performance, techniques like sequential modeling and ensembling are frequently used in the model learning stage, while the policy learning stage mostly involves the computation of eligibility traces or multi-step returns [2, 14]. However, these powerful techniques often come with additional computations, memories, and training time. This leads users to carefully consider which specific MBRL method to use or even whether to use an MBRL method based on the computational resources available. ", "page_idx": 0}, {"type": "image", "img_path": "R6N9AGyz13/tmp/6e225a98c1be6d65c4063c87a3f7064f8fea3c3fbb8f9c7a7fd5abde87f6d7a2.jpg", "img_caption": ["Figure 1: Comparisons on Atari 100k benchmark [16] and DeepMind Control Suite [24]. Among these methods, DreamerV3 [17], and our PaMoRL are directly evaluated on an NVIDIA V100 GPU, and IRIS [20], TWM [21], and REM [25] are evaluated on an A100 GPU, while other methods are evaluated on a $\\mathrm{Pl00\\GPU}$ . The extrapolation method employed aligns with the setup used in DreamerV3, where it assumes the P100 is twice as slow and the A100 is twice as fast. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In recent years, numerous endeavors have been made to develop an efficient world model architecture. Recurrent Neural Networks (RNNs) are frequently employed as the foundational architecture for world models [15, 16, 3, 6, 17]. However, the recurrent nature of RNNs hinders parallelization, leading to slow training speeds. In contrast, transformers have emerged as a potential successor, garnering acclaim for their remarkable performance in language modeling tasks and parallelized training paradigm [18]. Several attempts have been made to incorporate transformers into world models [19, 20, 21, 22]. However, the quadratic complexity of transformers w.r.t. sequence length limits their efficiency during training and inference. To achieve an RNN-level inference efficiency, extra tricks such as half-precision training or KV-Cache are required[23]. Furthermore, none of the aforementioned works have introduced improvements in the hardware efficiency of policy learning. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we aim to mitigate the curse of computational inefficiency of current MBRL methods and achieve the best of both worlds in terms of hardware efficiency and sample efficiency. The key idea is to fully parallelize the computations of sequential data, which has been a main workhorse of the rapid progress in deep learning over the past decade [26]. We achieve this by introducing the parallel scan. Specifically, We delve into two classic and widely implemented parallel scanners [27, 28], which can be applied for parallel training by excluding non-linear dependencies [29, 30]. Motivated by recent works in efficient sequential modeling [31, 32, 33], we observe that model architectures like linear attentions and linear RNNs not only enable parallel training but also recurrent inference. We also observe that the computations of eligibility trace estimation [2, 14] can be naturally parallelized over the sequence length by using parallel scan. ", "page_idx": 1}, {"type": "text", "text": "To this end, we introduce the Parallelized Model-based Reinforcement Learning (PaMoRL) framework, which consists of two novel techniques as shown in Figure 2 that can parallelize the current MBRL paradigm over sequence length: (1) the Parallel World Model (PWM) and (2) the Parallelized Eligibility Trace Estimation (PETE). The resulting framework, PaMoRL, is hardware-efficient and stable. It is compatible with various on-policy RL methods and can be applied to both discrete and continuous control problems using a single set of hyperparameters. ", "page_idx": 1}, {"type": "text", "text": "We evaluated our PaMoRL framework in the Atari 100K benchmark [16] and the DeepMind Control suite [24]. Tasks in these domains include discrete and continuous action spaces, images, and proprioception observations. We choose to follow the DreamerV3 [17] paradigm, which relies on \"imagination\" for policy learning. The summarized experimental results are shown in Figure 1. The empirical results demonstrate that PaMoRL, despite being a framework that incorporates autoencoding, still benefits greatly from the implementation of dual parallelization techniques (i.e., PWM and PETE). These techniques substantially enhance training speed, allowing PaMoRL to rival the performance of model-free RL methods without decoders [34]. In terms of sample efficiency, PaMoRL outperforms other no-look-ahead MBRL methods and model-free RL methods. It is worth mentioning that PaMoRL even outperforms the planning-based MBRL methods or methods with much larger networks in certain tasks [8]. ", "page_idx": 1}, {"type": "text", "text": "Our contributions can be summarized as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We introduce PaMoRL, a novel MBRL framework equipped with PWM and PETE that parallelizes both model and policy learning stages over the sequence length simultaneously. ", "page_idx": 2}, {"type": "text", "text": "\u2022 We evaluate our PaMoRL on the Atari $100\\mathrm{k}$ benchmark and DMControl suite with recent methods and obtain excellent results in terms of both sample and hardware efficiency. In addition, we conduct ablation studies on the validity of different modules, scanners, and other components. ", "page_idx": 2}, {"type": "text", "text": "\u2022 To the best of our knowledge, we are the first to point out that the computational process of eligibility traces can be parallelized over the sequence length. This technique can not only accelerate the value estimation process of various MBRL methods but any return-based reinforcement learning methods such as TD- $\\lambda$ [2], Retrace [35] and GAE [36] can benefit from it. ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Model-based Reinforcement Learning. We follow the paradigm of Partially Observable Markov Decision Process (POMDP) with observations $o_{t}$ , scalar rewards $r_{t}$ , actions $a_{t}$ , continuation flag $c_{t}\\in\\{0,1\\}$ , discount factor $\\gamma\\in(0,1)$ , and environmental dynamics $o_{t},r_{t},c_{t}\\sim p(o_{t},r_{t},c_{t}|o_{<t},a_{<t})$ . The objective of the Reinforcement Learning (RL) is to train a policy $\\pi$ that maximizes the return $\\textstyle\\sum_{t=1}^{\\infty}{\\dot{\\gamma}}^{t-1}r_{t}$ . In Model-based Reinforcement Learning (MBRL), the RL agent learns a model of the environmental dynamics through an iterative process that involves collecting data using a policy, training a model of the environment based on the accumulated data, and optimizing the policy using the learned model [1, 2, 14]. ", "page_idx": 2}, {"type": "text", "text": "Parallel Scan. As a universal parallel algorithm building block, the computations of parallel scan involve repeated application of a binary operator $\\bigoplus$ over sequential data arrays. Previous work[37] describes scan as a good example of a computation that seems inherently sequential, but for which there is an efficient parallel algorithm. The scan of $\\oplus$ with initial value $a_{0}$ is defined in Equation 1. ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\mathrm{SCAN}}(\\oplus,[a_{1},a_{2},...,a_{n}],a_{0}):=[(a_{1}\\oplus a_{0}),(a_{2}\\oplus a_{1}\\oplus a_{0}),...,(a_{n}\\oplus a_{n-1}...\\oplus a_{1}\\oplus a_{0})]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "First-order linear recurrences $h_{t}:=(A_{t}\\otimes h_{t-1})\\oplus x_{t}$ can be parallelized over the sequence length with the utilization of parallel scans if the following three conditions are met: ", "page_idx": 2}, {"type": "text", "text": "\u2022 $\\bigoplus$ is associative: $(a\\oplus b)\\oplus c=a\\oplus(b\\oplus c)$ . ", "page_idx": 2}, {"type": "text", "text": "\u2022 $\\otimes$ is semi-associative: there exists a binary associative operator $\\odot$ such that $a\\!\\otimes\\!(b\\!\\otimes\\!c)=(a\\!\\odot\\!b)\\!\\otimes\\!c$ . ", "page_idx": 2}, {"type": "text", "text": "\u2022 $\\otimes$ distributes over $\\oplus\\!\\!:a\\otimes(b\\oplus c)=(a\\otimes b)\\oplus(a\\otimes c).$ . ", "page_idx": 2}, {"type": "text", "text": "We observe vector addition $a\\oplus b:=a+b$ , matrix-vector multiplication $A\\otimes b:=A\\cdot b$ , and matrix-matrix multiplication $A\\odot B:=A\\cdot B$ fulfill the aforementioned conditions. This allows the parallel computation of $x_{t}:=(A_{t}\\cdot x_{t-1})+b_{t}$ across time steps $t$ , considering input vectors $b_{t}$ and square matrices $A_{t}$ . Considering the operators required in computing linear attentions [31] and eligibility trace estimations [2, 14] involve only diagonal matrices, the linear recurrence can be re-formulated as $x_{t}:=\\lambda_{t}\\odot x_{t-1}+b_{t}$ , where $\\lambda_{t}$ is the eigenvalues of the diagonal matrices and $\\odot$ is an element-wise multiplication. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce our Parallelized Model-based Reinforcement Learning (PaMoRL) framework, which facilitates dual parallelization across both model and policy learning stages. By parallelized training and recurrent inference, PaMoRL significantly improves training speed while avoiding additional computation overhead during inference. Figure 2 illustrates the overview of our PaMoRL framework, and we will now proceed to elaborate on its details. ", "page_idx": 2}, {"type": "text", "text": "3.1 Parallelized World Model Learning. ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "World Model Architecture Overview. As with other MBRL methods, our world model is trained to predict environmental dynamics. Since observations can be high-dimensional (e.g., images), we ", "page_idx": 2}, {"type": "image", "img_path": "R6N9AGyz13/tmp/f4ba078b54b263d2da1c9989be9b2cb78d322227818ba227c45ee6bc4750d03f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Overview of our PaMoRL framework. The symbols used in the figure are explained in Sections 3.1 and Section 3.2. The computations of the sequential model\u2019s outputs and the TD- $\\cdot\\lambda$ returns allow using parallel scans. In contrast, the imaginations cannot be parallelized over the sequence length because a non-linear actor network is required for action sampling. ", "page_idx": 3}, {"type": "text", "text": "predict future representations rather than future observations. This reduces accumulating errors and enables massively parallel training with a large batch size. The compact representations are obtained by an autoencoder and can be utilized to predict future observations, reward, and continuation flags. ", "page_idx": 3}, {"type": "text", "text": "To exclude non-linear dependencies for parallel training and obtain better performance, we make several modifications to the vanilla Recurrent State-Space Model\u2019s (RSSM) [3, 6, 17] configurations: (1) differentiating the hidden states $x_{t}$ from the sequential model\u2019s outputs $h_{t}$ , (2) excluding $h_{t}$ from the inputs of the encoder and decoder, (3) eliminating the stochastic states $z_{t}$ from the predictors\u2019 inputs, and (4) applying Batch Normalization for the encoder and dynamic predictor\u2019s outputs before the distributions are computed. Similar to RSSM, our model consists of six components: ", "page_idx": 3}, {"type": "text", "text": "Encoder: $z_{t}\\sim q_{\\theta}(z_{t}|o_{t})$ Sequence model: $h_{t},x_{t}=f_{\\theta}(x_{t-1},z_{t-1},a_{t-1})$ Reward predictor: $\\hat{r}_{t}\\sim p_{\\theta}\\big(\\hat{r}_{t}|h_{t}\\big)$ ", "page_idx": 3}, {"type": "text", "text": "Decoder: $\\hat{o}_{t}\\sim p_{\\theta}\\big(\\hat{o}_{t}|z_{t}\\big)$ Dynamics predictor: $\\hat{z}_{t}\\sim p_{\\theta}(\\hat{z}_{t}|h_{t})$ Continue predictor: $\\hat{c}_{t}\\sim p_{\\theta}\\big(\\hat{c}_{t}|h_{t}\\big)$ ", "page_idx": 3}, {"type": "text", "text": "The encoder and decoder use convolutional neural networks (CNN) for image inputs and multi-layer perceptrons (MLPs) for proprioception inputs. The sequence model has multiple stacked residual blocks, each of which consists of a modified linear attention [31] module and a Gated Linear Unit (GLU) [38] module. The dynamics, reward, and continue predictors are all MLPs. Consistent with previous work [6, 17], we set the $q_{\\theta}(z_{t}|o_{t})$ as a stochastic distribution comprising 32 categories, each with 32 classes, and we take straight-through gradients through the sampling step [39]. ", "page_idx": 3}, {"type": "text", "text": "Sequence Model Architectures. As mentioned above, each residual block of our sequence model consists of a modified linear attention module and a GLU module. The vanilla linear attention module, as introduced in previous work [31], employs $1+\\mathrm{ELU}$ as an element-wise kernel function applied to queries $q_{t}$ and keys $k_{t}$ taking $u_{t}$ as input. This configuration allows for its reformulation into an RNN-style recurrent form. However, this version of linear attention is prone to unstable convergence during training due to the unbounded gradients [40]. Thus, we remove the time-dependent normalizer, which is designed to approximate the Softmax operator and use an RMSNorm [41] for stabilize training. Furthermore, we incorporate the token mixing module from RWKV [32], which accepts inputs $u_{t}$ and previous inputs $u_{t-1}$ , along with the gating mechanism in Gated Recurrent Unit (GRU) [42] to provide an input-dependent decay rate $g_{t}$ for hidden state $x_{t}$ . The subsequent GLU module selects SiLU as the gating function, taking linear attention output $y_{t}$ as input. By integrating all the modifications, we can derive the entire block of the sequence model as shown in Equation 3. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{t},k_{t}=1+\\mathrm{ELU}(u_{t}W_{q}),1+\\mathrm{ELU}(u_{t}W_{k}),}\\\\ &{v_{t}=\\mathrm{Sigmoid}(u_{t}W_{r})\\odot u_{t}W_{v},}\\\\ &{g_{t}=\\mathrm{Sigmoid}((\\mu\\odot u_{t}+(1-\\mu)\\odot u_{t-1})W_{g}),}\\\\ &{x_{t}=g_{t}\\odot x_{t-1}+k_{t}^{\\top}v_{t},}\\\\ &{y_{t}=\\mathrm{RMSNorm}(q_{t}x_{t})W_{h}+u_{t},}\\\\ &{h_{t}=\\mathrm{SiLU}(y_{t}W_{g})\\odot y_{t}W_{y}+y_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The architecture of our modified linear attention satisfies the conditions in Section 2 and can be effectively computed using parallel scans. We can refer to Table 1 to summarize the computational complexities of various model architectures such as vanilla attention, RNN, SSM, and our modified linear attention in the training, inference, and imagination stages. ", "page_idx": 4}, {"type": "text", "text": "Loss Functions. The total loss function of model learning is shown as in Equation 4, where $\\beta_{\\mathrm{pred}}$ , $\\beta_{\\mathrm{rep}}$ , and $\\beta_{\\mathrm{dyn}}$ are coefficients to adjust the influence of each term in the loss function [43, 17]. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{L}(\\theta)\\quad}&{=\\!\\mathbb{E}_{q_{\\theta}}[\\sum_{t=1}^{T}\\beta_{\\mathrm{pred}}\\mathcal{L}^{\\mathrm{pred}}(\\theta,h_{t},o_{t},r_{t},c_{t},z_{t})+\\beta_{\\mathrm{rep}}\\mathcal{L}^{\\mathrm{rep}}(\\theta,h_{t},o_{t})+\\beta_{\\mathrm{dyn}}\\mathcal{L}^{\\mathrm{dyn}}(\\theta,h_{t},o_{t})]}\\\\ {\\mathcal{L}^{\\mathrm{pred}}(\\theta)\\quad}&{=-\\ln p_{\\theta}(r_{t}|h_{t})-\\ln p_{\\theta}(c_{t}|h_{t})+||\\hat{o}_{t}-o_{t}||_{2}}\\\\ {\\mathcal{L}^{\\mathrm{rep}}(\\theta)\\quad}&{=\\operatorname*{max}(1,\\mathrm{KL}[q_{\\theta}(z_{t}|o_{t})\\mid|\\mathrm{\\boldmath~sg}(p_{\\theta}(\\hat{z}_{t}|h_{t}))])}\\\\ {\\mathcal{L}^{\\mathrm{dyn}}(\\theta)\\quad}&{=\\operatorname*{max}(1,\\mathrm{KL}[\\mathrm{sg}(q_{\\theta}(z_{t}|o_{t}))\\mid|\\mathrm{\\boldmath~}p_{\\theta}(\\hat{z}_{t}|h_{t})])}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The operation $\\operatorname{sg}(\\cdot)$ represents the stop gradient operation. The KL divergences are derived from the Evidence Lower Bound (ELBO). We clip the KL divergence when it falls below the threshold of 1 [6, 17] and use the KL-balancing trick to prioritize the training losses [17]. ", "page_idx": 4}, {"type": "text", "text": "3.2 Policy Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The policy learning stage incorporates the actor and critic networks, both of which are MLPs, taking concatenation of $z_{t}$ and $h_{t}$ as input state $s_{t}$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{Actor:~}\\,\\,a_{t}\\sim\\pi_{\\phi}(a_{t}|s_{t}),\\quad\\mathrm{Critic:~}\\,v_{\\psi}(s_{t}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our policy learning method is in line with DreamerV3 [17] and can be used for both discrete and continuous action spaces. The critic uses TD- $\\lambda$ [2] as the its target, as shown in Equation 6, where $\\hat{r}_{t}$ represents the reward predicted by the world model, and $\\hat{c}_{t}$ represents the predicted continuation flag. ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{t}^{\\lambda}=\\hat{r}_{t}+\\gamma\\hat{c}_{t}((1-\\lambda)v_{\\phi}(s_{t+1})+\\lambda R_{t+1}^{\\lambda}),\\;\\;\\;R_{T}^{\\lambda}=v_{T}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The actor utilizes the Reinforce estimator [44] to compute the actor loss with a fixed entropy regularization term. The complete loss is described by Equation 7. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\phi)=-\\displaystyle\\sum_{t=1}^{T}\\mathrm{sg}(\\frac{R_{t}^{\\lambda}-v_{\\psi}(s_{t})}{\\operatorname*{max}(1,S)}),\\log\\pi_{\\phi}(a_{t}|s_{t})-\\eta H(\\pi_{\\phi}(a_{t}|s_{t}))}\\\\ &{\\mathcal{L}(\\psi)=-\\displaystyle\\sum_{t=1}^{T}(v_{\\psi}(s_{t})-\\mathrm{sg}(R_{t}^{\\lambda}))^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The hyper-parameter $\\eta$ represents the coefficient of the entropy regularization term. The normalization ratio $S$ utilized in the actor loss is defined in Equation 8, which is computed as the range between the $95^{\\mathrm{th}}$ and $5^{\\mathrm{th}}$ percentiles of the TD- $\\cdot\\lambda$ returns $R_{t}^{\\dot{\\lambda}}$ across the batch. ", "page_idx": 4}, {"type": "equation", "text": "$$\nS=\\mathrm{percentile}(R_{t}^{\\lambda},95)-\\mathrm{percentile}(R_{t}^{\\lambda},5)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Table 1: The step complexities [28] of different architectures, where $L$ is the sequence length and $H$ is the imagination horizon. Attention considers the full context with a burn-in and imagined steps of $\\mathcal{O}(L+H)$ , leading to a complexity of $\\mathcal{O}((L+H)^{2})$ . It is worth noting that the SSMs in recent works [46, 47] do not incorporate any gating mechanism or selectivities. Thus, despite SSMs and linear attentions both achieving the minimum complexity, linear attentions remain more expressive. ", "page_idx": 5}, {"type": "table", "img_path": "R6N9AGyz13/tmp/49fe5c51780d9b947a8de96c5619a7b3f3f5cf2312cad2c495ee72788c19228a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "By rearranging Equation 6, we can see that the calculations of both TD- $\\lambda$ and Retrace returns also also meet the conditions mentioned in Section 2. Therefore, they can be efficiently computed using parallel scan. This observation also applies to other eligibility trace estimation methods such as GAE [36] and Retrace [35], as they still satisfy the aforementioned conditions. ", "page_idx": 5}, {"type": "text", "text": "3.3 Parallel Scan Algorithms ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In both the model learning stage and the policy learning stage, we use two different parallel scanners: the Kogge-stone scanner [45] and the Odd-even scanner [28]. ", "page_idx": 5}, {"type": "text", "text": "The Kogge-stone scanner [45] is commonly used in hardware design for adders. It has a computational complexity of $\\mathcal{O}(L\\mathrm{log}_{2}L)$ for sequence length $L$ and a step complexity of ${\\mathcal{O}}(\\log_{2}L)$ after full parallelization. This indicates that it has higher computational redundancy, lower running time, and sufficient computational resources, making it suitable for parallel computation in a small batch. ", "page_idx": 5}, {"type": "text", "text": "The Odd-even scanner [28] is based on the concept of binary balanced trees. It has a computational complexity of $\\mathcal{O}(2L)$ for a sequence length $L$ and a step complexity of ${\\mathcal{O}}(2\\mathrm{log}_{2}L)$ after being fully parallelized. Despite theoretically taking more steps than the Kogge-stone scanner, it offers lower computational complexity and more uniform load sharing, making it better suited for large-scale parallel computation. Further details and illustrations are in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we aim to evaluate both the sample and training efficiency of our PaMoRL framework on the Atari 100K benchmark [16] and the DMControl suite [24]. The tasks include various scenarios with image and proprioception observations and discrete and continuous action spaces. ", "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Atari 100K. Atari 100K consists of 26 video games with discrete action dimensions of up to 18. The $100\\mathrm{K}$ samples are equated to 400K actual game frames, corresponding to approximately 2 hours of real-time gameplay, with action repeats of 4. The human normalized score is defined as $\\left(\\mathrm{score}_{\\mathrm{agent}}-\\mathrm{score}_{\\mathrm{random}}\\right)/$ (scorehuman \u2212scorerandom), where scorerandom comes from a random policy, and $\\mathrm{score}_{\\mathrm{human}}$ is obtained from human players [48]. ", "page_idx": 5}, {"type": "text", "text": "DeepMind Control Suite. DeepMind Control Suite consists of various control tasks with continuous action spaces. Referring to the categorizations in Sample MuZero [49] and EfficientZero V2 [9], tasks are divided into easy and hard categories. We followed the experimental setup of EfficientZero V2 [9] and established two benchmarks, named Proprio Control and Visual Control. ", "page_idx": 5}, {"type": "text", "text": "Among them, Proprio Control uses proprioception observations with 50K training samples for easy tasks and 100K for hard tasks, and Visual Control uses image observations with 100K training samples for easy tasks and 200K for hard tasks. Each benchmark includes 16 tasks. Action repeats are set to 2, and the maximum episode length is 1000 for both benchmarks, in line with previous ", "page_idx": 5}, {"type": "text", "text": "Table 2: Experimental results on the 26 games of Atari $100\\mathrm{k}$ after 2 hours of real-time experience and human-normalized aggregate metrics. Bold and underlined numbers indicate the highest and the second-highest scores, respectively. PaMoRL outperforms other methods regarding the number of superhuman games, mean, and median. ", "page_idx": 6}, {"type": "table", "img_path": "R6N9AGyz13/tmp/080ca7193ad0321441b37f574c2c209f9b3c27c6cb30762da867bb8e4580fd6d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "R6N9AGyz13/tmp/64477d7d2a6f6b763d6df8d821fca4a77ddff8d394f9694be6c8ae10fc3d832c.jpg", "table_caption": ["Table 3: Experimental results on the DeepMind Control suite. Bold and underlined numbers indicate the highest and the second-highest scores, respectively. PaMoRL outperforms other baselines in terms of the number of mean and median scores. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "studies [17, 13, 9]. We choose various baselines for each domain, which include SAC [50], DrQv2 [51], and DreamerV3 [17]. ", "page_idx": 6}, {"type": "text", "text": "4.2 Experimental Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we do not compare our results with look-ahead search methods [52, 7, 12, 13] or methods using larger networks [8], as our main goal in terms of sample efficiency is to improve performance while maximizing the hardware efficiency of existing MBRL methods. ", "page_idx": 6}, {"type": "image", "img_path": "R6N9AGyz13/tmp/9094d2297bd38df05bcdab077b0b5a46472278cf43e84e01635c39b823983bc8.jpg", "img_caption": ["Figure 3: Ablation studies of the effectiveness of each module of PWM, where SSM is equivalent to removing the data-dependent decay rate of PWM. We also include vanilla DreamerV3 as a baseline. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Atari 100K. The summarized results are shown in Figure 4. The full results for individual games in the Atari $100\\mathrm{k}$ benchmark are elaborated in Table 2, where scores are normalized against those of human players. Our PaMoRL framework attains a mean score of $126.64\\%$ and a median score of $71.75\\%$ , surpassing the other methods in terms of both mean and median human normalized score. For detailed training curves, please refer to Appendix C. Additionally, you can find more results and further discussions, including methods with look-ahead search or larger networks, in Appendix I. ", "page_idx": 7}, {"type": "text", "text": "DeepMind Control Suite. Table 3 shows that our method achieves a mean score of 661.2 across 16 tasks. As shown in Table 3, our method achieves a mean score of 661.2 using proprioception observations and 538.7 using image observations across 16 tasks, surpassing the previous state-ofthe-art, DreamerV3. The improvement in sample efficiency is attributed to two key modules: the token mixing module in the PWM, where the extra previous input provides more information to the data-dependent decay rate, and the implementation of RMSNorm, which improves the stability of the learning of the linear attention module, especially in the case of limited data. Our PaMoRL framework consistently demonstrates MBRL-level sample efficiency in tasks with proprioception observations, image observations, and discrete and continuous action spaces. Detailed training curves can be found in Figure 9 and Figure 10 in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we will be conducting ablation studies to evaluate the effectiveness of PWM and PETE in terms of stabilizing training and improving hardware efficiency. For more details, including PyTorch-style pseudo-code, please refer to Appendix G. ", "page_idx": 7}, {"type": "text", "text": "World Model Design. The results presented in Figure 3 demonstrate the impact of adding or removing the token mixing, RMSNorm, and data-dependent decay rate in various games in the Atari 100K benchmark. To showcase the benefits of token mixing in sequence prediction, we focused on tasks such as Alien, Boxing, and MsPacman. Additionally, we measured the improvement of RMSNorm on training stability by considering tasks like Amidar, UpNDown, and Qbert. ", "page_idx": 7}, {"type": "text", "text": "The findings in Figure 3 indicate that, while the token mixing module has minimal impact on the final performance for tasks where the reward can be accurately predicted from a single frame (e.g., Boxing), it leads to a performance drop on tasks that require several contextual frames to predict the reward accurately (e.g., Alien and Ms. Pacman). Regarding RMSNorm, removing it negatively affects the final performance and increases the instability of the training process. ", "page_idx": 7}, {"type": "image", "img_path": "R6N9AGyz13/tmp/79c07ecad25e5981821cf664200cb42e54a6188a7374f1f9a2e8f24398c36d7c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: (Left) Atari 100K aggregated metrics with $95\\%$ stratified bootstrap confidence intervals of the mean, median, and interquartile mean (IQM) human-normalized scores and optimality gap. (Right) Probabilities of improvement, i.e. how likely it is for our PaMoRL to outperform baselines. ", "page_idx": 8}, {"type": "image", "img_path": "R6N9AGyz13/tmp/e441a938225f4dd852f0f80d64fd793d61e448080ea2f78dffd13372fda9b8ec.jpg", "img_caption": ["Figure 5: (Upper) Comparison of parallel scanners with sequential rollout in terms of runtime for sequence modeling and eligibility trace estimation, as well as total GPU memory utilization. (Lower) Wall-clock time vs. GPU memory usage comparison for our PaMoRL method, SSM, and DreamerV3 across various batch size and sequence length combinations. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "There are two possible reasons for this difference. First, the gradient is bounded after the original normalizer is removed [40]. Adding RMSNorm further enhances training stability, which is especially important in the setting of limited data and end-to-end training. Second, RMSNorm only rescales the input and maintains the original center of the samples, which allows the module\u2019s output to maximize the information\u2019s retention. ", "page_idx": 8}, {"type": "text", "text": "Parallel Scanner Selection. Figure 5 shows PWM and PETE\u2019s runtime and GPU memory utilization on a single 3090 GPU using different scanners, respectively. Sequence model computation achieves $7.2\\times$ and $16.6\\times$ speedups compared to sequential rollout using the Kogge-stone and Oddeven scanners, respectively, with a sequence length of 64. In this case, the Kogge-stone scanner with the theoretically lowest runtime takes more than the Odd-even scanner in practice. This is because the computation of the sequence model involves the parallelism of both batch and hidden dimensions, which belongs to massively parallel computation, and the Kogge-stone scanner cannot realize full parallelism and thus encounters a bottleneck in computational resources. In contrast, the ", "page_idx": 8}, {"type": "text", "text": "Odd-even scanner is due to less computational redundancy, which allows the computational process of sequence modeling to achieve full parallelism and thus spends less running time. The PETE uses the Kogge-stone scanner and Odd-even scanner to achieve $3\\times$ and $2\\times$ speedups, respectively, with a sequence length of 16. Since the eligibility trace has a dimension of only 1, the Kogge-stone scanner can take full advantage of it. It thus achieves less runtime compared to the Odd-even scanner. ", "page_idx": 9}, {"type": "text", "text": "Regarding GPU memory utilization, using the Kogge-stone scanner imposes an additional $6\\times$ overhead compared to the sequential rollout, while the Odd-even scanner imposes an additional $2\\times$ overhead compared to the sequential rollout. However, the additional GPU memory overhead of parallel computation is not significant compared to the GPU memory overhead of encoder and decoder computation, especially in tasks with image observation. ", "page_idx": 9}, {"type": "text", "text": "Therefore, we recommend using the Odd-even scanner for PWM and the Kogge-stone scanner for PETE to achieve maximal speed with acceptable additional GPU memory utilization. ", "page_idx": 9}, {"type": "text", "text": "Batch Normalization Trick. World models are commonly learned using variational autoencoders to create concise representations of observations. However, they have some drawbacks, such as the tendency to disregard small moving objects. In Figure 11 in Appendix K, the reconstruction results are compared with and without using Batch Normalization for the Pong and Breakout games in the Atari 100K benchmark. It is observed that Batch Normalization improves the ability to distinguish similar video frames and capture information about small objects by re-centering the samples. ", "page_idx": 9}, {"type": "text", "text": "Additionally, Figure 12 demonstrates that PWM beneftis from the batch normalization trick, whereas DreamerV3 does not. This is likely due to PWM\u2019s decoder solely having stochastic states as inputs, making it challenging for training samples to be distinguished from each other in the early stages of training, leading to \"posterior collapse\" [53]. On the other hand, DreamerV3\u2019s decoder mitigates this problem by incorporating additional deterministic states as conditional inputs. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion & Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce the PaMoRL framework, an MBRL method capable of being computed using the parallel scan in both the model learning and policy learning stages. The key breakthrough of PaMoRL is the integration of two novel techniques: the Parallelized World Model and Parallelizable Eligibility Trace Estimation. With these techniques, we simultaneously accelerate the training process while maintaining MBRL-level sample efficiency. PaMoRL demonstrates excellent hardware efficiency and training stability in various games or tasks in the Atari 100K benchmark and DeepMind Control suite without incurring additional overhead during inference. An important contribution of our work is the introduction of a modified linear attention module in the MBRL method. Furthermore, we show that eligibility trace estimation computation can be parallelized for the first time. ", "page_idx": 9}, {"type": "text", "text": "It\u2019s important to acknowledge the limitations of our work. For instance, planning-based MBRL methods cannot parallelize computation over the sequence length, which hinders the incorporation of the most sample-efficient methods within our PaMoRL framework to maximize hardware efficiency. It would be interesting to explore using hybrid architectures to enhance PaMoRL by leveraging the strengths of Transformers, RNNs, and SSMs. Additionally, the world model and baselines used for comparison in PaMoRL are trained end-to-end with joint optimization of the image encoder and sequence model. While this end-to-end training paradigm enables the world model to predict the latent representations, it also impacts the scalability of the world model. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the NSFC project (No. 62072399), Zhejiang Provincial Natural Science Foundation of China under Grant No. LZ23F020009, Chinese Knowledge Center for Engineering Sciences and Technology, MoE Engineering Research Center of Digital Library, China Research Centre on Data and Knowledge for Engineering Sciences and Technology, and the Fundamental Research Funds for the Central Universities (No. 226-2024-00170). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160\u2013163, 1991.   \n[2] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. Robotica, 17(2):229\u2013235, 1999.   \n[3] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.   \n[4] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Advances in neural information processing systems, 31, 2018.   \n[5] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. Advances in neural information processing systems, 32, 2019.   \n[6] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020.   \n[7] Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games with limited data. Advances in Neural Information Processing Systems, 34:25476\u201325488, 2021.   \n[8] Max Schwarzer, Johan Samir Obando Ceron, Aaron Courville, Marc G Bellemare, Rishabh Agarwal, and Pablo Samuel Castro. Bigger, better, faster: Human-level atari with human-level efficiency. In International Conference on Machine Learning, pages 30365\u201330380. PMLR, 2023.   \n[9] Shengjie Wang, Shaohuai Liu, Weirui Ye, Jiacheng You, and Yang Gao. Efficientzero v2: Mastering discrete and continuous control with limited data. arXiv preprint arXiv:2403.00564, 2024.   \n[10] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Daydreamer: World models for physical robot learning. In Conference on Robot Learning, pages 2226\u20132240. PMLR, 2023.   \n[11] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured world models from human videos. arXiv preprint arXiv:2308.10901, 2023.   \n[12] Nicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive control. arXiv preprint arXiv:2203.04955, 2022.   \n[13] Nicklas Hansen, Hao Su, and Xiaolong Wang. Td-mpc2: Scalable, robust world models for continuous control. arXiv preprint arXiv:2310.16828, 2023.   \n[14] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[15] David Ha and J\u00fcrgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.   \n[16] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Modelbased reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.   \n[17] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023.   \n[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[19] Chang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. Transdreamer: Reinforcement learning with transformer world models. arXiv preprint arXiv:2202.09481, 2022.   \n[20] Vincent Micheli, Eloi Alonso, and Fran\u00e7ois Fleuret. Transformers are sample efficient world models. arXiv preprint arXiv:2209.00588, 2022.   \n[21] Jan Robine, Marc H\u00f6ftmann, Tobias Uelwer, and Stefan Harmeling. Transformer-based world models are happy with 100k interactions. arXiv preprint arXiv:2303.07109, 2023.   \n[22] Weipu Zhang, Gang Wang, Jian Sun, Yetian Yuan, and Gao Huang. Storm: Efficient stochastic transformer based world models for reinforcement learning. arXiv preprint arXiv:2310.09615, 2023.   \n[23] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5, 2023.   \n[24] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.   \n[25] Lior Cohen, Kaixin Wang, Bingyi Kang, and Shie Mannor. Improving token-based world models with parallel observation prediction. arXiv preprint arXiv:2402.05643, 2024.   \n[26] Yi Heng Lim, Qi Zhu, Joshua Selfridge, and Muhammad Firmansyah Kasim. Parallelizing non-linear sequential models over the sequence length. arXiv preprint arXiv:2309.12252, 2023.   \n[27] Richard E Ladner and Michael J Fischer. Parallel prefix computation. Journal of the ACM (JACM), 27(4):831\u2013838, 1980.   \n[28] Mark Harris, Shubhabrata Sengupta, and John D Owens. Parallel prefix sum (scan) with cuda. GPU gems, 3(39):851\u2013876, 2007.   \n[29] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. arXiv preprint arXiv:1709.04057, 2017.   \n[30] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022.   \n[31] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 5156\u20135165. PMLR, 2020.   \n[32] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023.   \n[33] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \n[34] Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman. Data-efficient reinforcement learning with self-predictive representations. arXiv preprint arXiv:2007.05929, 2020.   \n[35] R\u00e9mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy reinforcement learning. Advances in neural information processing systems, 29, 2016.   \n[36] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.   \n[37] Guy E Blelloch. Prefix sums and their applications. 1990.   \n[38] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.   \n[39] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.   \n[40] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. arXiv preprint arXiv:2210.10340, 2022.   \n[41] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019.   \n[42] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.   \n[43] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. In International conference on learning representations, 2016.   \n[44] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999.   \n[45] Peter M Kogge and Harold S Stone. A parallel algorithm for the efficient solution of a general class of recurrence equations. IEEE transactions on computers, 100(8):786\u2013793, 1973.   \n[46] Fei Deng, Junyeong Park, and Sungjin Ahn. Facing off world model backbones: Rnns, transformers, and s4. arXiv preprint arXiv:2307.02064, 2023.   \n[47] Mohammad Reza Samsami, Artem Zholus, Janarthanan Rajendran, and Sarath Chandar. Mastering memory tasks with world models. In Second Agent Learning in Open-Endedness Workshop, 2023.   \n[48] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International conference on machine learning, pages 1995\u20132003. PMLR, 2016.   \n[49] Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon Schmitt, and David Silver. Learning and planning in complex action spaces. In International Conference on Machine Learning, pages 4476\u20134486. PMLR, 2021.   \n[50] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018.   \n[51] Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning. arXiv preprint arXiv:2107.09645, 2021.   \n[52] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604\u2013609, 2020.   \n[53] Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.   \n[54] Carlos E Garcia, David M Prett, and Manfred Morari. Model predictive control: Theory and practice\u2014a survey. Automatica, 25(3):335\u2013348, 1989.   \n[55] Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees. arXiv preprint arXiv:1807.03858, 2018.   \n[56] Jian Shen, Han Zhao, Weinan Zhang, and Yong Yu. Model-based policy optimization with unsupervised model adaptation. Advances in Neural Information Processing Systems, 33:2823\u2013 2834, 2020.   \n[57] Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pages 465\u2013472, 2011.   \n[58] Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sampleefficient reinforcement learning with stochastic ensemble value expansion. Advances in neural information processing systems, 31, 2018.   \n[59] Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I Jordan, Joseph E Gonzalez, and Sergey Levine. Model-based value estimation for efficient model-free reinforcement learning. arXiv preprint arXiv:1803.00101, 2018.   \n[60] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021.   \n[61] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. arXiv preprint arXiv:2011.04006, 2020.   \n[62] Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. 1960.   \n[63] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022.   \n[64] Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning, pages 9355\u20139366. PMLR, 2021.   \n[65] J\u00fcrgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131\u2013139, 1992.   \n[66] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. Random feature attention. arXiv preprint arXiv:2103.02143, 2021.   \n[67] Huanru Henry Mao. Fine-tuning pre-trained transformers into decaying fast weights. arXiv preprint arXiv:2210.04243, 2022.   \n[68] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024.   \n[69] Blazej Osinski, Chelsea Finn, Dumitru Erhan, George Tucker, Henryk Michalewski, Konrad Czechowski, Lukasz Mieczyslaw Kaiser, Mohammad Babaeizadeh, Piotr Kozakowski, Piotr Milos, et al. Model-based reinforcement learning for atari. ICLR, 1:2, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Model-based reinforcement learning (MBRL) methods aim to construct a world model of the real environment and utilize the model to enhance the performance of policy. The crucial aspect of MBRL lies in its utilization of a world model. Previous methods in MBRL have employed world models in various ways, including searching for optimal action sequences [54, 4, 3], generating synthetic data [1, 55, 3, 5, 56], or improving the value estimation [57, 4, 58, 59]. Our PaMoRL framework builds upon DreamerV3 [17], which embraces the Dyna paradigm [1, 2, 14]. The world model is utilized to interact with the policy to generate synthetic data, aiding model-free RL methods in maximizing cumulative task rewards during the policy learning stage. ", "page_idx": 14}, {"type": "text", "text": "To maximize sample efficiency, MBRL researchers MBRL researchers have attempted to employ Recurrent Neural Networks (RNNs) [15, 16, 3, 6, 17] or Transformers [19, 21, 20, 22] as the architecture of world models. However, the update of the hidden state of the RNNs involves full matrix multiplication and the presence of nonlinearities within the recurrence hinders parallel computation, resulting in slower training speed. While Transformers provide a viable high-performance option, there is a quadratic relationship between their computational complexity and sequence length, which introduces additional computational and memory overhead. ", "page_idx": 14}, {"type": "text", "text": "Recent research has introduced a novel linear RNN architecture with simplified interaction between hidden states called the Structured State Space sequence model (S4) [60] that surpass both Transformers and RNNs in Long-Range Arena benchmarks (LRA) [61]. The S4 model and its variants are designed to effectively handle tasks involving long-range reasoning and draw inspiration from classical continuous-time linear state space models (SSMs) [62], which are well-established components of control theory. The relationship between the time and the frequency domain implies that SSMs have a convolutional view when the decay rate is data-independent, and therefore, training can be accomplished using the fast Fourier transforms (FFT). ", "page_idx": 14}, {"type": "text", "text": "We remark on the importance of incorporating a data-dependent decay rate, which is ignored by current works in SSMs until liquid S4 [63] and Mamba [33]. Our PWM builds upon linear attention with a data-dependent decay rate, which does not have the convolutional view and thus cannot use FFT for training but allows the use of parallel scans. The field of linear attentions and linear RNNs exhibits a close relationship [31], i.e. linear attentions can be reformulated as linear RNNs during auto-regressive decoding, revealing similarities to the update rules observed in fast weight additive outer products [64, 65]. These updated rules can be seen as a special case of element-wise linear recurrence. However, this formulation in linear attention cannot forget irrelevant information, resulting in the attention dilution issue. To address this limitation, gating mechanisms [66, 67, 68] can be used to facilitate the forgetting of irrelevant information similar to those in traditional RNNs. ", "page_idx": 14}, {"type": "text", "text": "The work that is most similar to our PaMoRL is Mamba [33]. Both our PaMoRL and Mamba have data-dependent decay rates and employ parallel scans to speed up the training process. However, there are significant differences between PaMoRL and Mamba in terms of model architectures and hardware preferences. In terms of model architecture, Mamba needs to maintain self-consistency with previous work in the SSM family, and therefore it must adhere to the paradigm of classical state space models, representing continuous differential equations. It needs to be parameterized and discretized using special tricks to achieve the gating mechanism implicitly. On the other hand, we recognize this limitation of Mamba and use a more \"simple yet effective\" gating mechanism. Regarding hardware preference, Mamba employs a special IO-aware parallel scanning algorithm for efficient training, which focuses on reducing the number of reads and writes between SRAM and HBM in the GPU through kernel fusion, and is suitable for improving the training efficiency of the hardware features when the model architecture is determined. In contrast, to satisfy the need for flexibility in MBRL, the parallel scanner we use is inspired by high-performance computing hardware design and focuses more on generality. Our parallel scanning method is compatible with arbitrary model architectures, as long as it satisfies the parallelization conditions mentioned in Section 2, as compared to the model architecture-specific parallel scanning method used by Mamba. ", "page_idx": 14}, {"type": "text", "text": "B Illustrations to Parallel Scan Algorithms ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Kogge-stone Scanner ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A common example of such a first-order recurrence problem is a time-varying linear system, the system\u2019s state at timestep $t$ is $x_{t}$ , computed from the system\u2019s internal dynamical variables $a_{t}$ and $b_{t}$ , as shown in Equation 9. Depending on the problem, the variables $a_{t}$ and $b_{t}$ can be real or complex numbers, constants, etc. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{x_{1}=b_{1},}}\\\\ {{\\;x_{2}=a_{2}x_{1}+b_{2},}}\\\\ {{\\;x_{3}=a_{3}x_{2}+b_{3},}}\\\\ {{\\;\\vdots}}\\\\ {{\\;x_{i}=a_{i}x_{i-1}+b_{i},}}\\\\ {{\\;\\vdots}}\\\\ {{\\;x_{L}=a_{L}x_{L-1}+b_{L}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Before solving the problem, we can define the function $A(m,n)$ and $B(m,n)$ , as shown in Equation 12. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle A(m,n)=\\prod_{j=n}^{m}a_{i}},}\\\\ {{\\displaystyle B(m,n)=\\sum_{i=n}^{m}(\\prod_{j=i+1}^{m}a_{j})b_{i},\\;\\mathrm{where}\\,n\\leq m.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now we can integrate Equation 9 with Equation 10 to get Equation 11. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle B(1,1)=x_{1}=b_{1},}}\\\\ {{\\displaystyle B(2,1)=x_{2}=a_{2}x_{1}+b_{2}=a_{2}B(1,1)+B(2,2)=A(2,2)B(1,1)+B(2,2),}}\\\\ {{\\displaystyle\\vdots}}\\\\ {{\\displaystyle B(4,1)=x_{4}=a_{4}x_{3}+b_{4}=a_{4}a_{3}B(2,1)+B(4,3)=A(4,3)B(2,1)+B(4,3),}}\\\\ {{\\displaystyle\\vdots}}\\\\ {{\\displaystyle B(2i,1)=x_{2i}=(\\prod_{j=i+1}^{2i}a_{j})B(i,1)+B(2i,i+1)=A(2i,i+1)B(i,1)+B(2i,i+1)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It can be observed in Equation 11 that $B(2i,i+1)$ is associated with the computation of $A(2i,i+1)$ but independent from $B(i,1)$ , which means that we can split the computation of $B(2i,1)$ into two parallel parts. For reasons of notational simplicity, we define the tuple $Q(m,n)$ that wraps the functions $A(m,n)$ and $B(m,n)$ , as shown in Equation 12. ", "page_idx": 15}, {"type": "equation", "text": "$$\nQ(m,n)=(A(m,n),B(m,n)),\\,\\mathrm{where}\\,n\\leq m.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Figure 6 shows the operation of the Kogge-stone scanner [45] when the sequence length $L=8$ . After $\\lceil\\log_{2}L\\rceil$ iterations the solution to the problem $x_{1},\\ldots,x_{T}$ can be computed. ", "page_idx": 15}, {"type": "text", "text": "B.2 Odd-even Scanner ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To avoid the extra computational complexity of $\\mathrm{log}_{2}L$ generated by the Kogge-stone scanner [45], the Odd-even scanner [28] uses an algorithmic pattern that arises often in parallel computing: balanced trees. The idea is to build a balanced binary tree on the input data and start scanning from the root. A binary tree with $L$ leaves has $\\mathrm{log}_{2}L$ layers with $2^{d}$ nodes per layer $d\\in[0,L)$ . If we perform one operation on each node, then we will perform $\\mathcal{O}(L)$ operations in one traversal of the tree. The tree we construct is not an actual data structure, but rather a concept that we use to determine what each thread has to do at each step of the traversal. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "The algorithm consists of two phases: up-sweep and down-sweep. During the up-sweep phase, we traverse from the leaves to the root of the tree. During the down-sweep phase, we backtrack from the root node up the tree, using the results computed in the up-sweep phase. Figure 7 shows the operation of the Kogge-stone scanner [28] when the sequence length $L=8$ . ", "page_idx": 16}, {"type": "text", "text": "Note that since this is an exclusive scan (i.e., the sum is not included in the result), we zero out the last element of the array between phases. This zero is propagated back to the head of the array in the down-sweep phase. This scanning algorithm performs $\\mathcal{O}(2L)$ operations, so it is very efficient. ", "page_idx": 16}, {"type": "image", "img_path": "R6N9AGyz13/tmp/ee38b678039e17f32d0c0b1caa70612ecd093890c9ab3654ae962ca131bd8482.jpg", "img_caption": ["Figure 6: Illustrations of the operation of the Kogge-stone scanner when the sequence length $L=8$ "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "R6N9AGyz13/tmp/fc11b760ceeb7b2b4ecb9b3e35ea44ebd98405ee55838a95bffe5eaa081e927c.jpg", "img_caption": ["Figure 7: Illustrations of the operation of the Odd-even scanner when the sequence length $L=8$ . "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Training Curves of the Atari 100K Benchmark ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The Atari 100K benchmark [69] is a standard RL benchmark comprising 26 Atari games featuring diverse gameplay mechanics. It is designed to assess a broad spectrum of agent skills, and agents are limited to executing 400 thousand discrete actions within each environment, which is approximately equivalent to 2 hours of human gameplay. To put this in perspective, when there are no constraints on sample efficiency, the typical practice is to train agents for 200M steps. ", "page_idx": 17}, {"type": "image", "img_path": "R6N9AGyz13/tmp/10a38d25650b5a5ac3f806734bd3730e050896ae661455a3da00eefece077aa7.jpg", "img_caption": ["Figure 8: Training curves on the Atari $100\\mathrm{k}$ benchmark. The solid line represents the average result over 5 seeds, and the filled area indicates the range between the maximum and minimum results across these 5 seeds. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D Training Curves of the DeepMind Control Suite ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "DeepMind Control suite [24] is a standard RL benchmark comprising various tasks with continuous action spaces. It supports both image observation and low-dimensional proprioception observation. When there are no constraints on sample efficiency, the typical practice is to train agents for millions of steps. ", "page_idx": 18}, {"type": "image", "img_path": "R6N9AGyz13/tmp/a8946294273d2be673966df68c829f5e6305f63cecc5ed4ddfb47e012d338b42.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 9: Training curves on the DeepMind Control suite with image observations. The solid line represents the average result over 5 seeds, and the fliled area indicates the range between the maximum and minimum results across these 5 seeds. ", "page_idx": 18}, {"type": "image", "img_path": "R6N9AGyz13/tmp/7c0d04b72df3ca09c313326b2e15b5f96fe1eb9e0548a3bbc7d8ecc3f9536a50.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 10: Training curves on the DeepMind Control suite with proprioception observations. The solid line represents the average result over 5 seeds, and the filled area indicates the range between the maximum and minimum results across these 5 seeds. ", "page_idx": 19}, {"type": "text", "text": "E Details of Model Architecture ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table 4: Architecture details of the image encoder. The size of the modules is omitted and can be derived from the shape of the tensors. SiLU refers to the sigmoid-weighted linear units used for activation, while Linear represents a fully connected layer. Flatten and Reshape operations are employed to alter the tensor\u2019s indexing method while preserving the data and their original order. Conv denotes a CNN layer characterized by kernel $=4$ , stride $=2$ , and padding $=1$ . BN denotes the batch normalization layer. ", "page_idx": 20}, {"type": "table", "img_path": "R6N9AGyz13/tmp/9b9049b50675f4f843dc98075c2ec9f0d761e31d0822160e803184d35cc6644c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "R6N9AGyz13/tmp/d4cdcb49b6f1d83641b65057441b1c9143cf54efc636eb5cf806dc56b16de33a.jpg", "table_caption": ["Table 5: Architecture details of the image decoder. DeConv denotes a transpose CNN layer characterized by kernel $=4$ , stride $=2$ , and padding $=1$ . "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "R6N9AGyz13/tmp/fcbde7fb3ac1b2d16def36eaca4fd861ee2630900ffc16276bf5067236042ac2.jpg", "table_caption": ["Table 6: Action mixer. Concatenate denotes combining the last dimension of two tensors and merging them into one new tensor. The variable $A$ represents the action dimension. $D$ denotes the feature dimension of the sequence model. LN is an abbreviation for layer normalization. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 7: Modules which are pure MLPs. 1-layer MLP corresponds to a fully connected layer. 255 is the size of the bucket of symlog two-hot loss [17]. $K$ refers to the dimension of proprioception observations. ", "page_idx": 20}, {"type": "table", "img_path": "R6N9AGyz13/tmp/dea89172e90dcd2f2e0edc6fa8bade23272d781d7f10af7454a5850044396a3a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "F Hyperparameters ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Table 8: Full hyperparameters. Note that the environment will provide a \u201cdone\u201d signal when losing a life but will continue running until the actual reset occurs. This life information configuration aligns with the setup used in IRIS [20]. Regarding data sampling, each time, we sample $B_{1}$ trajectories of length $T$ for world model training and sample $B_{2}$ trajectories of length $C$ for starting the imaginations. ", "page_idx": 21}, {"type": "table", "img_path": "R6N9AGyz13/tmp/5bef7eeb92849d94d26a326d342028d01ba982cca173fed95521cc9fc6eec6fb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "G Pytorch-style Pseudo-code of Parallel Scan ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "G.1 Odd-even scanner ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1 def odd_even_parallel_scan (inputs , operator):   \n2 \"\"\"   \n3 Odd/Even Parallel Scanner.   \n4 Inputs:   \n5 inputs: tuple of sequence elements.   \n6 operator: binary operator function.   \n7 Outputs:   \n8 outputs: tuple of sequence elements.   \n9 \"\"\"   \n10 Length $=$ inputs [0]. shape [0]   \n11   \n12 if Length < 2:   \n13 return inputs   \n14   \n15 reduced_inputs $=$ operator(   \n16 (input [: -1][0::2] for input in inputs),   \n17 (input [1::2] for input in inputs)   \n18 )   \n19 odd_inputs $=$ odd_even_parallel_scan (reduced_inputs , operator)   \n20   \n21 if Length % $\\smash{\\ensuremath{\\mathrm{~\\small~\\mathscr~{~\\star~}~}}}2\\ensuremath{\\mathrm{~\\alpha~=~}}\\ensuremath{\\mathrm{~0~}}$ :   \n22 even_inputs $=$ operator(   \n23 (input [:-1] for input in odd_inputs),   \n24 (input [2::2] for input in inputs)   \n25 )   \n26 else:   \n27 even_inputs $=$ operator(   \n28 (input for input in odd_inputs),   \n29 (input [2::2] for input in inputs)   \n30 )   \n31   \n32 even_inputs $=$ [   \n33 torch.cat(( input [0:1] , even_input), ${\\tt d i m}\\!=\\!0$ )   \n34 for (input , even_input) in zip(inputs , even_inputs)   \n35 ]   \n36   \n37 outputs $=$ [   \n38 interleave(odd_input , even_input)   \n39 for (even_input , odd_input) in zip(even_inputs , odd_inputs)   \n40 ]   \n41 return outputs   \n42   \n43   \n44 def interleave(odd , even):   \n45 padded_odd $=$ torch.cat((odd , torch.zeros_like(odd [ -1:])), dim ${}=0$ )   \n46 outputs $=$ torch.stack ((even , padded_odd [: even.shape [0]]) , ${\\tt d i m}\\,{=}\\,1$ )   \n47 outputs $=$ outputs.flatten (0, 1)[:( odd.shape [0] $^+$ even.shape [0])]   \n48 return outputs ", "page_idx": 22}, {"type": "text", "text": "G.2 Kogge-stone scanner ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1 def kogge_stone_parallel_scan (inputs , operator):   \n2 \"\"\"   \n3 Kogge -Stone Parallel Scanner.   \n4 Inputs:   \n5 inputs: tuple of sequence elements.   \n6 operator: binary operator function.   \n7 Outputs:   \n8 outputs: tuple of sequence elements.   \n9 \"\"\"   \n10 Length $=$ inputs [0]. shape [0]   \n11 Times $=$ math.ceil(math.log2(Length))   \n12   \n13 for i in range(Times):   \n14 interval $=$ int(2 \\*\\* i)   \n15 outputs $=$ operator(   \n16 (input[:- interval] for input in inputs),   \n17 (input[interval :] for input in inputs)   \n18 )   \n19 inputs $=$ [   \n20 torch.cat(( input [: interval], output), dim $=\\!0$ )   \n21 for (input , output) in zip(inputs , outputs)   \n22 ]   \n23 return inputs ", "page_idx": 23}, {"type": "text", "text": "H Pytorch-style Pseudo-code of Parallelized Eligibility Trace Estimation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1 def parallel_eligibility_trace (reward , value , next_value , p_cont , lam)   \n:   \n2 \"\"\"   \n3 Parallel Eligibility Trace Estimations.   \n4 \"\"\"   \n5 ones $=$ torch.ones_like(reward)   \n6 p_cont , lam $=$ p_cont \\* ones , lam \\* ones   \n7 lam $=$ torch.cat((lam [1:] , ones [:1]) , dim $=\\!0$ )   \n8   \n9 delta $=$ reward $^+$ p_cont $^\\ast$ next_value - value   \n10 flipped_delta $=$ delta.flip(dims $=$ (0 ,))   \n11 flipped_lam $=$ (p_cont $^*$ lam).flip(dims $=$ (0 ,))   \n12   \n13 residual $=$ odd_even_parallel_scan (   \n14 [flipped_lam , flipped_delta ], binary_return_fn )   \n15 returns $=$ value $^+$ residual [1]. flip(dims $=$ (0 ,))   \n16 return returns   \n17   \n18   \n19 def parallel_lambda_return (reward , value , next_value , p_cont , lam):   \n20 \"\"\"   \n21 Parallel TD -Lambda Estimations.   \n22 \"\"\"   \n23 ones $=$ torch.ones_like(reward)   \n24 p_cont , lam $=$ p_cont $^\\ast$ ones , lam $^*$ ones   \n25   \n26 delta $=$ reward $^+$ p_cont $^\\ast$ next_value \\* (1 - lam)   \n27 last $=$ delta [-1:] + p_cont [ -1:] \\* lam [ -1:] \\* next_value [-1:]   \n28 delta $=$ torch.cat(( delta [:-1], last), dim ${}=0$ )   \n29   \n30 flipped_delta $=$ delta.flip(dims $=$ (0 ,))   \n31 flipped_lam $=$ (p_cont $^*$ lam).flip(dims =(0 ,))   \n32   \n33 returns $=$ odd_even_parallel_scan (   \n34 [flipped_lam , flipped_delta ], binary_return_fn )   \n35 returns $=$ returns [1]. flip(dims $=$ (0 ,))   \n36 return returns   \n37   \n38   \n39 def binary_return_fn (cur_i , cur_j):   \n40 coef_i , in_i $=$ cur_i   \n41 coef_j , in_j $=$ cur_j   \n42 return coef_i $^\\ast$ coef_j , coef_j \\* in_i + in_j ", "page_idx": 24}, {"type": "text", "text": "I Additional Comparisons on the Atari 100K Benchmark ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Atari 100K benchmark [69] is a standard RL benchmark comprising 26 Atari games featuring diverse gameplay mechanics. It is designed to assess a broad spectrum of agent skills, and agents are limited to executing 400 thousand discrete actions within each environment, which is approximately equivalent to 2 hours of human gameplay. To put this in perspective, when there are no constraints on sample efficiency, the typical practice is to train agents for 200M steps. ", "page_idx": 25}, {"type": "text", "text": "In this section, we compare the performance of our PaMoRL framework with planning-based methods such as EfficientZero [7] and EfficientZero V2 [9] and methods with much larger networks, i.e., BBF [8] on the Atari 100K benchmark. The full results are shown in Table I. The PaMoRL framework is not as good as the other methods in terms of the number of superhuman games, median score, and average score. However, it leads the pack of 13/26 games in terms of an individual game perspective. ", "page_idx": 25}, {"type": "table", "img_path": "R6N9AGyz13/tmp/5e1bc4e6ad4972455631e7d1a3ba76eda076abdcc946add73f039c3c4de6af1e.jpg", "table_caption": ["Table 9: Experimental results on the 26 games of Atari $100\\mathrm{k}$ after 2 hours of real-time experience and human-normalized aggregate metrics. Bold and underlined numbers indicate the highest and the second-highest scores, respectively. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "J Runtime of Experiments ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Table 10: Average runtime of experiments ", "page_idx": 26}, {"type": "table", "img_path": "R6N9AGyz13/tmp/b6fe18d1378d38ea9d7ace91337023fc0f32d9c98879ff51f5c81ade920c03e8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "K Effectiveness of Batch Normalization Trick ", "text_level": 1, "page_idx": 26}, {"type": "image", "img_path": "R6N9AGyz13/tmp/851e1e6a5bf0344460737466ba13bd3ba45c5bc4d76b427a0112456c205a6b52.jpg", "img_caption": ["Figure 11: Visualizations on Batch Normalization trick in Pong and Breakout. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "R6N9AGyz13/tmp/bd969689af18022f45cda5c5a683a85df49b9e77fde301d41c1ddbf51c86765d.jpg", "img_caption": ["Figure 12: Quantitative results on the effectiveness of the Batch Normalization trick. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "L Video Predictions ", "text_level": 1, "page_idx": 27}, {"type": "image", "img_path": "R6N9AGyz13/tmp/7108419ca5e96f10e6c574836c1ebbdebd80a990f172a1fc0e68bb196c0111e7.jpg", "img_caption": ["Figure 13: Multi-step predictions on several environments in Atari games and DeepMind Control suite. The world model utilizes 5 observations and actions as contextual input, enabling the imagination of future events spanning 56 frames in an auto-regressive manner. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "M Initializations in Freeway ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The reward function in Freeway is sparse since the agent is only rewarded when it completely crosses the road. In addition, bumping into cars will drag it down, preventing it from smoothly ascending the highway. This poses an exploration problem for newly initialized agents because a random policy will almost surely never obtain a non-zero reward with a 100k frames budget. The solution to this problem is actually straightforward and requires stretches of time when the \"UP\" action is oversampled. In this paper, we opted for the simplest strategy of having an initialized buffer with fulfilled \"UP\" actions. Hence, we dont\u2019t need to lowered the sampling temperature to avoid random walks that would not be conducive to learning in the early stages of training. Consequently, once it received its first few rewards through exploration, our PaMoRL could internalize the sparse reward function in its world model. ", "page_idx": 28}, {"type": "image", "img_path": "R6N9AGyz13/tmp/dd04ea7ed705fa7f5dada113cca8b5860362a21927f7fbe15a1848d9ba286540.jpg", "img_caption": ["Figure 14: A game of Freeway. Cars will bump the player down, making it very unlikely to cross the road and be rewarded for random policies. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We clearly claim the three contributions in the introduction. We also describe the method and show the experimental results in the main paper. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We state the limitations in the conclusion section. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not include theoretical results. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide sufficient information about the hyper-parameters as well as the details in the Appendix. We also pack our code in the supplemental materials. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We pack the code in the supplemental materials and the code base of each algorithm is clearly cited in the main paper. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We show sufficient experimental settings and hyper-parameters in the main paper and the Appendix. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: In each graph, we show the average results among 3 different seeds and the std variance of the learning curves. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We show the hardware for training and state the time used for each training process in the Appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The experimental environments and the baseline algorithms code-bases are fully officially released, open-sourced, and open-accessed. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We propose a parallel model-based RL framework for fast training, so there is no societal impact. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We propose a parallel model-based RL framework for fast training, so there is no misuse concerns. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The experimental environments as well as the algorithms code-bases are properly cited in the experiment settings section. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We provide the code with documents in supplemental materials and will release them. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}]