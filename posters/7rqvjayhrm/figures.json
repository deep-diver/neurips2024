[{"figure_path": "7RQvjayHrM/figures/figures_0_1.jpg", "caption": "Figure 1: The inference pipeline of RouterDC. The encoder E and the LLM embeddings k\u2019s are trainable parameters, while the LLMs are frozen.", "description": "This figure illustrates the architecture of the RouterDC model.  A query (x\u1d62) is fed into an encoder (E), which produces an embedding (\u0190(x\u1d62)). This embedding is then compared to learned embeddings (k\u2081, k\u2082, k\u2083,...) representing different Large Language Models (LLMs: M\u2081, M\u2082, M\u2083,...). The similarity between the query embedding and each LLM embedding determines which LLM is selected (M\u2083 in this example) to produce the final answer (\u0177\u1d62).  The encoder and LLM embeddings are trainable parameters, but the LLMs themselves are kept fixed during training.", "section": "3 Methodology"}, {"figure_path": "7RQvjayHrM/figures/figures_0_2.jpg", "caption": "Figure 2: Testing accuracy of candidate LLMs and our RouterDC on in-distribution and out-of-distribution tasks.", "description": "This radar chart visualizes the performance of various Large Language Models (LLMs) and the proposed RouterDC model across multiple benchmark datasets.  Each axis represents a different dataset (MMLU, GSM8K, CMMLU, ARC-C, HumanEval, PreAlgebra, MBPP, C-EVAL), and the values on each axis show the accuracy achieved by each model on that dataset. The in-distribution datasets are those the models were trained on, while out-of-distribution datasets are new ones.  The chart allows for a direct comparison of the performance of RouterDC against individual LLMs and highlights its effectiveness in improving accuracy across multiple tasks, especially in out-of-distribution scenarios.", "section": "1 Introduction"}, {"figure_path": "7RQvjayHrM/figures/figures_1_1.jpg", "caption": "Figure 3: Score distributions of LLMs on an example query (w/ or w/o normalization).", "description": "This figure illustrates the score distributions of multiple Large Language Models (LLMs) when responding to a sample query. The left panel shows the raw scores assigned to each LLM, revealing a significant disparity in performance, with some models achieving much higher scores than others.  The right panel demonstrates the effect of softmax normalization, a common technique used to transform raw scores into probabilities.  The normalization process compresses the score range, making the top performers less distinguished from the rest, which can present a challenge for selecting the most suitable LLM. The visualization helps illustrate the contrast between raw LLM performance scores and their normalized probabilities and explains the limitation of existing routing models.", "section": "3.2 Scoring"}, {"figure_path": "7RQvjayHrM/figures/figures_1_2.jpg", "caption": "Figure 4: Distribution of the score difference between the top two LLMs.", "description": "This figure shows the distribution of the difference between the scores of the top two performing LLMs for a given query. The x-axis represents the difference in scores, and the y-axis represents the density of queries with that score difference.  A large portion (64%) of the queries show a very small difference between the top two LLMs, indicating that multiple LLMs often perform similarly well for a single query. This observation highlights a limitation of existing routing methods that rely on a single best LLM for each query.", "section": "3.2 Scoring"}, {"figure_path": "7RQvjayHrM/figures/figures_7_1.jpg", "caption": "Figure 1: The inference pipeline of RouterDC. The encoder E and the LLM embeddings k's are trainable parameters, while the LLMs are frozen.", "description": "This figure shows the architecture of the RouterDC model.  It consists of an encoder that takes a query as input and generates an embedding vector. This vector is then used to calculate similarity scores with embedding vectors representing different LLMs.  The LLM with the highest similarity score is selected to answer the query.  The encoder and LLM embeddings are trainable parameters, while the LLMs themselves are kept frozen during training.", "section": "3 Methodology"}, {"figure_path": "7RQvjayHrM/figures/figures_7_2.jpg", "caption": "Figure 1: The inference pipeline of RouterDC. The encoder E and the LLM embeddings k's are trainable parameters, while the LLMs are frozen.", "description": "This figure shows the architecture of the RouterDC model.  The model takes a query as input, which is first processed by an encoder E to generate an embedding vector. This embedding vector is then used to compute similarity scores with the embedding vectors of several different Large Language Models (LLMs).  These similarity scores are used to determine which LLM is best suited to answer the query.  The encoder and LLM embeddings are trainable parameters, while the LLMs themselves are kept frozen during training.", "section": "3 Methodology"}, {"figure_path": "7RQvjayHrM/figures/figures_8_1.jpg", "caption": "Figure 1: The inference pipeline of RouterDC. The encoder E and the LLM embeddings k's are trainable parameters, while the LLMs are frozen.", "description": "This figure illustrates the architecture of the RouterDC model.  It shows how a query (x<sub>i</sub>) is processed. First, the query is passed through an encoder (E) which produces an embedding E(x<sub>i</sub>). This embedding is then used to compute similarity scores with the embeddings of different LLMs (k<sub>1</sub>, k<sub>2</sub>, k<sub>3</sub>,...). These similarity scores are then used to select the most suitable LLM for answering the query. The encoder E and the LLM embeddings are trainable parameters, while the LLMs themselves are frozen during training. The output (\u0177<sub>i</sub>) is generated from the chosen LLM. ", "section": "3 Methodology"}, {"figure_path": "7RQvjayHrM/figures/figures_9_1.jpg", "caption": "Figure 1: The inference pipeline of RouterDC. The encoder E and the LLM embeddings k's are trainable parameters, while the LLMs are frozen.", "description": "This figure illustrates the architecture of the RouterDC model.  It shows how a query (x<sub>i</sub>) is first encoded by an encoder E to produce an embedding E(x<sub>i</sub>).  This embedding is then used to compute similarity scores with the embeddings (k<sub>1</sub>, k<sub>2</sub>, k<sub>3</sub>...) of multiple Large Language Models (LLMs). The LLM with the highest similarity score is then selected to generate the final response y<sub>i</sub>. The encoder and the LLM embeddings are trainable parameters, while the LLMs themselves are kept frozen during training.", "section": "3 Methodology"}, {"figure_path": "7RQvjayHrM/figures/figures_15_1.jpg", "caption": "Figure 11: Visualization of embeddings of training queries.", "description": "This figure visualizes the embeddings of training queries using t-SNE.  Panel (a) shows the embeddings when only the sample-LLM contrastive loss is used for training, while panel (b) shows the embeddings when both sample-LLM and sample-sample contrastive losses are used. The visualization reveals the effect of the sample-sample contrastive loss in improving the clustering of similar queries.", "section": "4.4 Analysis"}, {"figure_path": "7RQvjayHrM/figures/figures_15_2.jpg", "caption": "Figure 16: Testing accuracy with different numbers of training samples.", "description": "This figure shows the testing accuracy of RouterDC on five in-distribution tasks (MMLU, GSM8K, CMMLU, ARC-C, and HumanEval) with varying numbers of training samples per task.  The x-axis represents the number of training samples, and the y-axis represents the testing accuracy.  The plot demonstrates that RouterDC's performance improves with more training data but shows signs of saturation beyond a certain number of samples.  The average accuracy across all five tasks is also shown.", "section": "4.3 Sensitivity Analysis"}]