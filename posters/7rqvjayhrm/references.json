{"references": [{"fullname_first_author": "J. Austin", "paper_title": "Program synthesis with large language models", "publication_date": "2021-08-07", "reason": "This paper is foundational to the field of using LLMs for program synthesis, a key area in the development of RouterDC."}, {"fullname_first_author": "M. Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-03", "reason": "This paper introduces HumanEval, a benchmark used in the experimental evaluation of RouterDC, which is critical to assessing the performance of the model."}, {"fullname_first_author": "P. Clark", "paper_title": "Think you have solved question answering? Try ARC, the AI2 reasoning challenge", "publication_date": "2018-03-05", "reason": "This paper introduces the ARC-C dataset, one of the benchmarks used to evaluate RouterDC, highlighting the model's performance on complex reasoning tasks."}, {"fullname_first_author": "K. Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-14", "reason": "This paper introduces GSM8K, a benchmark dataset used in RouterDC's evaluation, demonstrating its effectiveness in mathematical reasoning problems."}, {"fullname_first_author": "D. Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2021-00-00", "reason": "This paper introduces the MMLU benchmark, a key dataset used for evaluating RouterDC's performance across multiple tasks, demonstrating its versatility."}]}