[{"figure_path": "pEhvscmSgG/tables/tables_13_1.jpg", "caption": "Table 5: Results on the D4RL benchmark. Showing normalized returns and standard deviations at the end of policy training.", "description": "This table presents the results of the C-LAP model and other baselines on the D4RL benchmark.  It shows the average normalized returns and standard deviations obtained by each method across different locomotion and antmaze environments. Each environment is tested with various datasets representing different levels of expertise, from medium replay to expert.  The results demonstrate the relative performance of C-LAP compared to other state-of-the-art offline reinforcement learning methods.", "section": "4.1 Benchmark results"}, {"figure_path": "pEhvscmSgG/tables/tables_14_1.jpg", "caption": "Table 1: MOPO and MOBILE hyper-parameters for the expert datasets in the D4RL benchmark.", "description": "This table lists the hyperparameters used for the MOPO and MOBILE algorithms when run on the expert datasets of the D4RL benchmark.  It shows the ranges explored for penalty coefficient, rollout steps, and dataset ratio, and then provides the specific values selected for each of the three locomotion environments (halfcheetah, walker2d, and hopper).", "section": "4.1 Benchmark results"}, {"figure_path": "pEhvscmSgG/tables/tables_14_2.jpg", "caption": "Table 3: C-LAP constraint values for the D4RL benchmark", "description": "This table shows the hyperparameter values used for the C-LAP model on different datasets within the D4RL benchmark.  Specifically, it lists the constraint parameter (\u1ebd) used for each environment and dataset combination. The constraint parameter influences how closely the generated actions stay within the support of the dataset's action distribution.", "section": "4.1 Benchmark results"}, {"figure_path": "pEhvscmSgG/tables/tables_14_3.jpg", "caption": "Table 4: C-LAP constraint values for the V-D4RL benchmark", "description": "This table shows the hyperparameter \\(\\tilde{\\epsilon}\\) used in the C-LAP algorithm for different environments and datasets in the V-D4RL benchmark.  The constraint \\(\\tilde{\\epsilon}\\) is used to limit the generated actions within the support of the dataset's action distribution. Different values for \\(\\tilde{\\epsilon}\\) are used based on the environment and the dataset.", "section": "4 Experiments"}, {"figure_path": "pEhvscmSgG/tables/tables_15_1.jpg", "caption": "Table 2: C-LAP hyper-parameters", "description": "This table lists the hyperparameters used in the Constrained Latent Action Policies (C-LAP) method.  It is divided into two sections: Model and Agent. The Model section specifies details about the architecture of the generative model including the sizes of latent spaces, the types of layers used (MLP or CNN), and the distributions used for outputs.  The Agent section provides parameters related to the actor-critic algorithm used for policy training, such as hidden unit sizes, layer numbers, learning rates, and activation functions. Note that different hyperparameter values were used for experiments with low-dimensional features and for experiments with visual observations.", "section": "4 Experiments"}, {"figure_path": "pEhvscmSgG/tables/tables_16_1.jpg", "caption": "Table 5: Results on the D4RL benchmark. Showing normalized returns and standard deviations at the end of policy training.", "description": "This table presents the results of the experiments conducted on the D4RL benchmark.  It shows the performance of different offline reinforcement learning algorithms (PLAS, MOPO, MOBILE, C-LAP) across various locomotion and antmaze environments.  For each environment and dataset, the table lists the mean normalized returns achieved by each algorithm, along with their standard deviations, indicating the variability in performance across different runs.  The results highlight the relative performance of the proposed C-LAP method compared to existing state-of-the-art methods.", "section": "4.1 Benchmark results"}, {"figure_path": "pEhvscmSgG/tables/tables_16_2.jpg", "caption": "Table 6: Results on the V-D4RL benchmark. Showing normalized returns and standard deviations at the end of policy training.", "description": "This table presents the results of the proposed C-LAP method and other baselines on the V-D4RL benchmark.  The benchmark includes datasets with visual observations. The table shows the average normalized returns and standard deviations across multiple runs for each method on different datasets.  The results demonstrate the performance of C-LAP compared to state-of-the-art methods.", "section": "4.1 Benchmark results"}]