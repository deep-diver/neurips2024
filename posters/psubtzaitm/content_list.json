[{"type": "text", "text": "Efficient Policy Evaluation Across Multiple Different Experimental Datasets ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yonghan Jung\u02da Purdue University jung222@purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Alexis Bellot\u02da: Independent Researcher abellot@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Artificial intelligence systems are trained combining various observational and experimental datasets from different source sites, and are increasingly used to reason about the effectiveness of candidate policies. One common assumption in this context is that the data in source and target sites (where the candidate policy is due to be deployed) come from the same distribution. This assumption is often violated in practice, causing challenges for generalization, transportability, or external validity. Despite recent advances for determining the identifiability of the effectiveness of policies in a target domain, there are still challenges for the accurate estimation of effects from finite samples. In this paper, we develop novel graphical criteria and estimators for evaluating the effectiveness of policies (e.g., conditional, stochastic) by combining data from multiple experimental studies. Asymptotic error analysis of our estimators provides fast convergence guarantee. We empirically verified the robustness of estimators through simulations. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the empirical sciences, conclusions on the effect of actions or policies is often supported by evidence drawn from prior observations and experiments. The conditions under which such inferences can be formally justified can be traced back (in part) to Campbell, Stanley and Cook [10, 11, 14]. They argued for a basic dichotomy in the kinds of questions that scientists seek to answer from experimental data. On the one hand asking whether \u201cin fact, the experimental stimulus made some significant difference in this specific instance?\u201d, and on the other hand asking \u201cto what populations, settings, and treatments can this effect be generalized?\u201d [10, p. 297]. These inferences have since been labelled as internal validity and external validity, respectively. ", "page_idx": 0}, {"type": "text", "text": "External validity is concerned with the extent to which findings from one population can be \u201creprocessed\u201d, or \u201cre-calibrated\u201d so as to circumvent population differences and produce valid generalizations in a target population where experiments cannot be performed (e.g., outside the laboratory, different domains, etc.). The validity of these inferences will necessarily be contingent on a careful analysis to ascertain the commonalities and differences between domains as, for example, if the target domain is completely arbitrary generalization is impossible. In the causal transportability literature, the basis for generalization (also called transportability) is justified by the stability and invariance of the causal mechanisms shared across populations and domains [20, 32]. Several graphical characterizations exist to delineate the conditions under which transportability is possible, with recent algorithms proposing solutions for general instances of the external validity task combining observational and experimental distributions under partial observability [36, 2, 3, 16, 30]. ", "page_idx": 0}, {"type": "text", "text": "These algorithmic solutions express a target policy effect in terms of the observational and experimental source distributions. Still, then one needs to go further and estimate the resulting expression from finite samples. In practice, with a finite number of samples and potentially high-dimensional covariates, estimating causal expressions is quite challenging. Effective estimators have been developed for specific settings, starting with doubly-robust estimators for functionals given by the backdoor criterion [13, 37, 9, 45], and recently extended to cover general identification scenarios with observational and experimental samples [25, 26, 8]. These techniques also find parallels across other related disciplines, such as reinforcement learning where re-weighting [42, 31], outcome modelling [6], and doubly-robust estimation [18], are common for evaluating the effect of policies to overcome shifts in the behaviour policy. Recently, [44] and [22] have considered policy evaluation under covariate shift and selection bias, a special case of the external validity problem with a given graph. Despite their generality, existing estimators still only cover a limited portion of realistic scientific inferences. In particular, existing methods are not applicable in settings where datasets are collected in different domains. ", "page_idx": 1}, {"type": "text", "text": "We consider the generalization of causal claims from observational and experimental data through the task of policy evaluation. The target for inference is $\\mathbb{E}_{P_{\\pi}^{0}}[Y]$ where $P_{\\pi}^{0}$ symbolizes the distribution of data in a target domain (indexed as 0) in which a hypothetical policy of interest $\\pi$ (also known as dynamic treatment regimes [33] or soft interventions [16]) has been implemented. The question then becomes how to identify and estimate $\\mathbb{E}_{P_{\\pi}^{0}}[Y]$ , given finite samples from multiple observational and experimental data (e.g., $P_{\\pi_{i}}^{i}$ , a source domain indexed by $i$ in which experimental policy is $\\pi_{i}$ that may differ with $\\pi_{0}$ ) collected under different settings and structural assumptions, encoded in causal diagrams. We aim to bridge the gap between identification and estimation to solve general instances of external validity. Our contributions are twofold: ", "page_idx": 1}, {"type": "text", "text": "1. Sec. 3: We develop nonparametric identification criteria (Thm. 1) to determine whether the effect of a policy may be expressed through an adjustment formula from two separate distributions induced by policy interventions, collected from different populations. Based on this formulation, we develop a multiply robust estimator (Thm. 3) that enjoys multiply robustness against model misspecification and bias. ", "page_idx": 1}, {"type": "text", "text": "2. Sec. 4: We generalize these identification criteria (Thm. 4) and propose a general multiply-robust estimator (Thm. 6) applicable for the evaluation of policies from multiple source datasets. ", "page_idx": 1}, {"type": "text", "text": "1.1 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We use bold letters $({\\mathbf X})$ to denote a random vector and $X$ a random value. Each random vector is represented with a capital letter $({\\bf X})$ and its realized value with a small letter ${\\bf\\Psi}({\\bf x})$ . Given a set $\\mathbf{X}~=~\\{X_{1},\\cdots,X_{n}\\}$ , we denote $\\mathbf{X}^{(i)}~:=~\\{X_{1},\\cdots,X_{i}\\}$ . For a discrete vector $\\mathbf{X}$ , we use $\\mathbb{1}_{\\mathbf{x}}(\\mathbf{X})$ to represent the indicator function such that $\\mathbb{1}_{\\mathbf{x}}(\\mathbf{X})=1$ if $\\mathbf{X}=\\mathbf{x}$ ; $\\mathbb{1}_{\\mathbf{x}}(\\mathbf{X})=0$ otherwise. For comprehensibility, we use $P({\\bf v})$ to denote a probability at $\\mathbf{V}$ at $\\mathbf{v}$ for discrete/continuous random variables $\\mathbf{V}$ . In similar, we use $\\sum_{\\mathbf{z}}$ for $\\mathbf{Z}\\;\\subseteq\\;\\mathbf{V}$ for the summation/integration over a mixture of discrete/continuous random vari a\u0159bles $\\mathbf{Z}$ For example, we write the back-door adjustment as $\\scriptstyle\\sum_{\\mathbf{z}}\\mathbb{E}_{P}[Y\\;\\;|\\;\\;x,\\mathbf{z}]P(\\mathbf{z})$ even when $\\mathbf{Z}$ is a mixture of discrete/continuous variables. We use $\\begin{array}{r}{\\mathbb{E}_{P}[f(\\mathbf{V})]\\;:=\\;\\sum_{\\mathbf{v}}f(\\mathbf{v})P(\\mathbf{v})}\\end{array}$ for a function $f$ . For a sample set ${\\mathcal D}\\;:=\\;\\{\\mathbf{V}_{(i)}\\;:\\;i\\;=\\;1,\\cdots\\,,n\\}$ where $\\mathbf{V}_{(i)}$ de no\u0159tes the $i$ th samples, we use $\\begin{array}{r}{\\mathbb{E}_{\\mathcal{D}}[f(\\mathbf{V})]:=(1/n)\\sum_{i=1}^{n}f(\\mathbf{V}_{(i)})}\\end{array}$ . We use $\\|f\\|_{P}:=$ $\\sqrt{\\mathbb{E}_{P}[\\{f(\\mathbf{V})\\}^{2}]}$ . If a function $\\widehat{f}$ is a consistent estimator of $f$ ha ving a rate $r_{n}$ , we use $\\widehat{f}-f=$ $o_{P}(r_{n})$ . We say $\\hat{f}$ is $L_{2}$ -consiste npt if $\\|\\hat{f}-f\\|_{P}=o_{P}(1)$ . We use $\\widehat{f}-f=O_{P}(1)$ if ${\\widehat{f}}-f$ is  pbounded in probability, and ${\\hat{f}}-f=O_{P}(r_{n})$ when ${\\widehat{f}}-f$ is bounded in  prpobability at rate $r_{n}$ . ", "page_idx": 1}, {"type": "text", "text": "We use Structural  pCausal Models (SCMs )p as our framework [35]. An SCM $\\mathcal{M}$ is a quadruple $\\mathcal{M}=\\langle\\mathbf{U},\\mathbf{V},P(\\mathbf{U}),\\mathcal{F}\\rangle$ . $\\mathbf{U}$ is a set of latent variables following a joint distribution $P(\\mathbf{U})$ . $\\mathbf{V}$ is a set of observable variables whose values are determined by functions $\\mathcal{F}=\\{f_{V_{i}}:V_{i}\\in\\dot{\\mathbf{V}}\\}$ such that $V_{i}\\,\\leftarrow\\,f_{V_{i}}({\\bf p a}_{V_{i}},{\\bf u}_{V_{i}})$ where $\\mathbf{PA}_{i}\\subseteq V$ and ${\\mathbf{U}}_{V_{i}}\\subseteq{\\mathbf{U}}$ . Each SCM $\\mathcal{M}$ induces a distribution $P({\\bf V})$ and a causal graph $\\mathcal{G}$ in which directed edges from every variable in $\\mathbf{PA}_{i}$ to $V_{i}$ exist. Dashedbidirected arrows encode correlated latent variables. ", "page_idx": 1}, {"type": "text", "text": "2 Policy Evaluation Integrating Multiple Experimental Datasets ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We investigate the sequential decision-making setting concerning a set of actions $\\mathbf{X}$ , a series of dynamic covariates $\\mathbf{Z}$ , a series of static covariates $\\mathbf{C}$ and an outcome variable of interest $Y$ in an SCM $\\mathcal{M}$ . A policy vector $\\pi:=\\{\\pi^{i}\\}$ over actions $\\mathbf{X}=\\{X_{1},\\cdots,X_{m}\\}$ is an ordered set of decision rules for each $X_{i}\\in\\mathbf{X}$ . Actions are selected according to a topological ordering $X_{1}<\\cdot\\cdot\\cdot<X_{K}$ over time. Each action $X_{i}$ is potentially associated with a set of prior static and dynamic covariates, for example, the decision rule for $X_{k}$ could be defined as $x_{k}\\;\\sim\\pi(\\cdot\\;|\\;\\mathbf{z}^{(k)},\\mathbf{x}^{({\\bar{k}}-1)},\\mathbf{c}^{(k)})$ . Every $\\pi(X_{k}\\mid{\\bar{\\mathbf{Z}^{(k)}}},\\mathbf{X}^{(k-1)},\\mathbf{C}^{(k)})$ is a probability distribution mapping from domains of the set of inputs $\\{\\mathbf{Z}^{(k)},\\mathbf{X}^{(k-1)},\\mathbf{C}^{(k)}\\}$ to the domain of actions $X_{k}$ . The implementation of a policy $\\pi$ in $\\mathcal{M}$ induces an intervened model $\\mathcal{M}_{\\pi}$ , that sets values of every $X\\in\\mathbf{X}$ to be decided by the policy $\\pi$ , replacing the functions $\\{f_{X},X\\in X\\}$ that would normally set its value. We denote a distribution induced by $\\mathcal{M}_{\\pi}$ as $P_{\\pi}$ . Now, we fix the notion of the policy evaluation as follows: ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Policy evaluation [41]). The policy evaluation is to predict the effectiveness of a policy vector $\\pi$ on an outcome $Y$ in an target SCM $\\mathcal{M}^{\\mathrm{0}}$ ; i.e., $\\psi_{0}:=\\mathbb{E}_{P_{\\pi}^{0}}[Y]$ . ", "page_idx": 2}, {"type": "text", "text": "Difficulties in estimating $\\mathbb{E}_{P_{\\pi}^{0}}[Y]$ comes from that the distribution or samples from $P_{\\pi}^{0}$ are generally not available. These discrepancies can be formalized under the rubric of SCMs as follows. In the most general setting, an investigator might leverage multiple source domains $\\{\\mathcal{M}^{1},\\mathcal{M}^{2},\\ldots,\\mathcal{M}^{K}\\}$ over $\\mathbf{V}$ that entail distributions $\\bar{\\mathbb{P}}:\\{P^{1},\\bar{P}^{2},\\ldots,\\bar{P}^{K}\\}$ . Data or samples from these distributions may be available under different behaviour policies, e.g., $\\pi_{1},\\pi_{2},\\ldots,\\pi_{K}$ , depending on the study or data collection protocol implemented in each domain (that might include an observational regime, i.e. no policy implemented). To ground the policy evaluation problem, we define graphical tools to capture commonalities and discrepancies across domains. ", "page_idx": 2}, {"type": "text", "text": "Definition 2 (Domain discrepancy [29]). For every pair of SCMs $\\mathcal{M}^{i},\\mathcal{M}^{j}\\;(i,j\\in\\{0,1,2,...\\,,K\\})$ defined over $V$ , the domain discrepancy set $\\Delta_{i j}\\subseteq V$ is defined such that for every $V\\in\\Delta_{i j}$ there might exist a discrepancy between $f_{V}^{M^{i}}\\neq f_{V}^{M^{j}}$ , or $P^{M^{i}}({\\pmb u}_{V})\\neq P^{M^{j}}({\\pmb u}_{V})$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 3 (Selection diagram [29]). The selection diagram $\\begin{array}{r l r}{\\mathcal{G}^{\\Delta}}&{{}}&{=}\\end{array}$ $\\{\\mathcal{G}^{j}\\}_{j\\in\\{0,1,2,...,T\\}}\\cup\\{\\mathcal{G}^{\\Delta_{0j}}\\}_{j\\in\\{\\underline{{1}},2,...,T\\}}$ is a graph constructed from $\\mathcal{G}^{i}\\;\\;(i\\;\\;\\in\\;\\;\\{0,1,2,...\\,,T\\})$ by adding the selection node $S_{i j}$ to the vertex set, and adding the edge $S_{i j}\\rightarrow V$ for every $V\\in\\Delta_{i j}$ . ", "page_idx": 2}, {"type": "text", "text": "$\\Delta_{i,j}$ locates the mechanisms where structural discrepancies between two domains are suspected to take place. $V\\not\\in\\Delta_{i,j}$ represents the assumption that the mechanisms for $V$ are invariant across the two domains. The induced selection diagram is a parsimonious representation of these constraints. The following example illustrates these notions. ", "page_idx": 2}, {"type": "text", "text": "Example 1 (External validity under covariate shift). A common instance of the external validity problem in the literature considers the evaluation the effect of a policy $\\pi:\\Omega_{C}\\times\\Omega_{X}\\rightarrow[0,1]$ for assigning a treatment $X\\in\\{0,1\\}$ , subject to shift in the distribution of covariates $C$ . For this example, let source and target domains $\\mathbb{M}:\\{\\bar{\\mathcal{M}}^{1},\\mathcal{M}^{0}\\}$ over $V=\\{C,X,Y\\},U=\\{U_{X Y},U_{C}\\}$ be defined as follows, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{M}^{1}:\\left\\{\\begin{array}{r l r l}{\\mathcal{F}\\quad}&{=\\left\\{\\begin{array}{l l}{C\\gets f_{C}(U_{C})}\\\\ {X\\gets f_{X}(C,U_{X Y})}\\\\ {Y\\gets f_{Y}(X,C,U_{X Y})}\\\\ {P(U)}&{=P(U_{X Y})P(U_{C})}\\end{array}\\right.}&{\\quad\\mathcal{M}^{0}:\\left\\{\\begin{array}{l l}{\\mathcal{F}^{0}\\quad}&{=\\left\\{\\begin{array}{l l}{C\\gets f_{C}^{0}(U_{C})}\\\\ {X\\gets f_{X}(C,U_{X Y})}\\\\ {Y\\gets f_{Y}(X,C,U_{X Y})}\\end{array}\\right.}\\\\ {P^{0}(U)}&{=P(U_{X Y})P^{0}(U_{C})}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $C\\in\\Delta_{1,0},\\{Y,C\\}\\not\\in\\Delta_{1,0}$ as only the mechanism for $C$ varies across domains. Consider the evaluation of $\\pi:\\pi(X=1\\mid c):=1/(1+\\exp\\{-c\\})$ given an experimental dataset in $\\mathcal{M}^{1}$ in which $X$ has been randomized, i.e., $X\\sim\\operatorname{Bern}(0.5)$ , and covariate data $\\dot{P}^{0}(C)$ available in $M^{0}$ . Notice that we do not have access to the specification of the SCMs $\\mathbb{M}$ , but only the induced diagrams $\\mathcal{G}^{\\Delta}$ , and a subset of entailed distributions $\\mathbb{P}:\\{P_{\\mathrm{rand}(X)}^{1}(X,Y,C),P^{0}(C)\\}$ . The policy effect is expressible as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P_{\\pi}^{0}}[Y]=\\sum_{x,c,y}y P_{\\mathrm{rand}(x)}^{1}(y\\mid c,x)\\pi(x\\mid c)P^{0}(c),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and estimated given the policy $\\pi$ and the combination of the available data from $P^{1},P^{0}$ . ", "page_idx": 2}, {"type": "image", "img_path": "PSubtZAitM/tmp/b2b389ff408deecf1f39b7bb2242e1a066dfa74c1264e52cdf14e3d0b0169e8b.jpg", "img_caption": ["Figure 1: Graphs illustrating the inference of a two-stage treatment strategy $\\pi_{0}\\;:=\\;\\{\\pi_{0}(x_{1}\\;\\;|\\;\\;c_{1}),\\pi_{0}(x_{2}\\;\\;|\\;\\;$ $\\bar{x_{1}},c_{2},w)\\}$ given data from source domains $\\mathcal{M}^{1},\\mathcal{M}^{2}$ , described in Example 2. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3 Combining experiments from two domains ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Example 1 illustrates two challenges in combining data from different domains to infer the effect of a new policy in a target domain. In a first instance highlighting the challenge of identification, that is inferring an expression in terms of $\\mathbb{P}$ that identifies the policy effect, and in a second instance highlighting the challenge of estimation, that is providing efficient estimators from finite samples for the identified policy effect. The following example will serve to motivate this setting. ", "page_idx": 3}, {"type": "text", "text": "Example 2 (Two-stage treatment strategies). A team of physicians is contemplating a treatment plan $\\pi_{0}$ against heart disease $Y$ for their patients in ${\\mathcal{M}}^{0}$ . They consider administrating two drugs in sequence: a drug against hypertension $X_{1}$ , followed by an anti-diabetic drug $X_{2}$ depending on the effect of $X_{1}$ on blood pressure $W$ . To support their evaluation, two studies exist on these drugs, from domains $\\mathcal{M}^{1},\\mathcal{M}^{2}$ , that, however, have only analyzed their effect in isolation (on $X_{1}$ and $X_{2}$ separately) and under different treatment guidelines, $\\pi_{1},\\pi_{2}$ respectively. The data collected refers to the variables $\\mathbf{V}:=(Y,\\mathbf{C}_{1},\\mathbf{C}_{2},X_{1},X_{2},\\Bar{W})$ in which $(\\mathbf{C}_{1},\\mathbf{C}_{2})$ are demographic variables. Formally, we assume physicians have access to $\\mathbb{P}\\,:\\,\\{P_{\\pi_{1}}^{1}(\\mathbf{V}),P_{\\pi_{2}}^{2}(\\mathbf{V}),P^{0}(\\mathbf{C}_{1},\\mathbf{C}_{2})\\}$ . The superscripts in $P^{0},P^{1},P^{2}$ are the index for the domain, and the subscripts $\\pi_{0},\\pi_{1},\\pi_{2}$ denote the policies for assigning treatments. $\\mathcal{G}^{\\Delta}$ in Fig. 1 encodes the structural assumptions, which include discrepancies across domains and implemented policies in the available data. For example, the graph $\\mathcal{G}_{\\pi_{1}}^{1}$ specifies the known guideline $\\pi_{1}$ used in $\\mathcal{M}^{1}$ , while no specific plan was followed for the assignment of $X_{2}$ , that in practice depends on the patient\u2019s covariates $C_{2}$ as well as unobserved factors, e.g. mood, health awareness, etc. (summarized in the bi-directed arc). In addition, selection diagrams describe differences between domains. For example, the edge $\\{S_{C_{1}}\\rightarrow C_{1}\\}$ in $\\mathcal{G}_{\\pi_{0}}^{\\Delta_{0,1}}$ indicates a potential change in the distribution of covariates $C_{1}$ across domains $\\bar{M}^{0},M^{1}$ . The question then becomes how to estimate $\\mathbb{E}_{P_{\\pi_{0}}^{0}}[Y]$ given $(\\mathcal{G}^{\\Delta},\\mathbb{P})$ . \u25a0 ", "page_idx": 3}, {"type": "text", "text": "3.1 Identification ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Example 2 illustrates the complexity of drawing inferences from multiple datasets collected under different settings. We extend this example to provide a general identification procedure for the effect of policies when two source datasets subject to different policies and/or discrepancies with the target domain are available. Let $\\mathbf{V}:=(Y,\\mathbf{C},\\bar{X_{1}},\\mathbf{W},X_{2},\\mathbf{W},\\bar{Y},\\mathbf{S})$ denote a set of disjoint variables, where $Y$ is an outcome variable, C, $(\\mathbf{C},\\mathbf{W})$ are covariates corresponding to two experiments, $(X_{1},X_{2})$ are treatment variables, and $\\mathbf{S}$ denotes the selection nodes describing discrepancies across pairs of domains. Formally, the task signature is given as follows: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Input: Samples from $\\mathbb{P}\\;=\\;\\{P_{\\pi_{1}}^{1}(\\mathbf{V}),\\;P_{\\pi_{2}}^{2}(\\mathbf{V}),\\;P^{0}(\\mathbf{C}_{1},\\mathbf{C}_{2})\\}$ ; structural assumptions $\\mathcal{G}^{\\Delta}\\ :=$ $\\{\\mathcal{G}_{\\pi_{0}}^{0},\\mathcal{G}_{\\pi_{1}}^{1},\\mathcal{G}_{\\pi_{2}}^{2},\\mathcal{G}_{\\pi_{0}}^{\\Delta_{0,1}},\\mathcal{G}_{\\pi_{0}}^{\\Delta_{0,2}}\\},$ .   \n\u2022 Query: Estimate $\\mathbb{E}_{P_{\\pi_{0}}^{0}}[Y]$ where $P^{0}$ is distribution on the target domain and $\\pi_{0}$ is a target policy assigning treatments with $\\pi_{0}(X_{1}\\mid\\mathbf{C}_{1})$ and $\\pi_{0}(X_{2}\\mid\\mathbf{C}_{2},W)$ .   \nGiven these inputs, a sufficient condition for identifying the query is given as follows:   \nDefinition 4 (Adjustment criterion for combining two experiments). Given $\\mathcal{G}^{\\Delta}$ , the adjustment   \ncriterion for combining two experimental datasets is defined by the following $d_{\\cdot}$ -separation statements:   \n1. Domain transfer for $Y$ : $(Y\\perp\\!\\mathrm{~\\bf~S~}\\!\\mid\\,{\\bf C},X_{1},X_{2},\\!,W)$ in \u03c0\u220600,2; i.e., the distribution over Y is invariant between the source distribution from $\\mathcal{M}^{2}$ and the target. ", "page_idx": 3}, {"type": "text", "text": "2. Domain transfer for $W$ : $(W\\perp\\!\\!\\!\\perp\\mathbf{S}\\mid\\mathbf{C},X_{1})$ in G\u03c0\u220600,1; i.e., the distribution over W is invariant between the source distribution from $\\mathcal{M}^{1}$ and the target. ", "page_idx": 4}, {"type": "text", "text": "3. Adjustment for $Y$ : $(Y\\perp\\pi_{i}\\mid C_{1},C_{2},X_{1},X_{2},W)$ in $\\mathcal{G}_{\\pi_{i}}$ for $i\\in\\{0,2\\}$ ; i.e., the distribution over $Y$ is invariant between regimes $\\pi_{0}$ and $\\pi_{2}$ . ", "page_idx": 4}, {"type": "text", "text": "4. Adjustment for $W$ : $(W\\perp\\pi_{i}\\mid C_{1},C_{2},X_{1})$ in $\\mathcal{G}_{\\pi_{i}}$ for $i\\in\\{0,1\\}$ ; i.e., the distribution over $W$ is invariant between regimes $\\pi_{0}$ and $\\pi_{1}$ . ", "page_idx": 4}, {"type": "text", "text": "The adjustment criterion could be shown to hold for Example 2. Specifically, domain transfer $Y$ could be shown by inspecting Fig.5e as the set $\\mathbf{S}=\\{S_{C_{1}},S_{C_{2}},S_{W}\\}$ is d-separated from $Y$ , conditional on $\\{C_{1},C_{2},\\bar{X_{1}},\\bar{X_{2}},\\cal{W}\\}$ . Similarly, domain transfer for $W$ holds as $\\mathbf{S}=\\overline{{\\left\\{S_{C_{1}},S_{C_{2}},S_{Y}\\right\\}}}$ is ${\\mathrm d}$ -separated from $W$ , given $C_{1},C_{2},X_{1}$ in Fig.5d. Similarly, one could verify the adjustment condition for $Y$ by inspecting Fig.5c and the adjustment condition for $W$ by inspecting Fig. 5a. ", "page_idx": 4}, {"type": "text", "text": "For this example, these conditions imply identifiability of the target query $\\mathbb{E}_{P_{\\pi_{0}}^{0}}[Y]$ given $(\\mathcal{G}^{\\Delta},\\mathbb{P})$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Adjustment for combining two experiments). Under the adjustment criterion in Def. 4, the target query $\\psi_{0}:=\\mathbb{E}_{P_{\\pi_{0}}^{0}}[Y]$ is identifiable from the samples from $P_{\\pi_{1}}^{1}(\\mathbf{V}),\\,P_{\\pi_{2}}^{2}(\\mathbf{V}),\\,P^{0}(\\mathbf{C}_{1},\\overset{.}{\\mathbf{C}}_{2})$ . Specifically, it\u2019s expressed as follows 3: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P_{\\pi_{0}}^{0}}[Y]=\\sum_{w,\\mathbf{c},\\mathbf{x}}\\mathbb{E}_{P_{\\pi_{2}}^{2}}[Y\\mid\\mathbf{c},w,\\mathbf{x}]\\pi_{0}(x_{2}\\mid\\mathbf{c},w)P_{\\pi_{1}}^{1}(w\\mid\\mathbf{c}_{1},x_{1})\\pi_{0}(x_{1}\\mid\\mathbf{c})P^{0}(\\mathbf{c}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{X}:=(X_{1},X_{2})$ and $\\mathbf{C}:=(\\mathbf{C}_{1},\\mathbf{C}_{2})$ . ", "page_idx": 4}, {"type": "text", "text": "Effectively, despite the differences across domains encoded in Example 2, the effect of the new combination of anti-diabetic and anti-hypertensive drugs $\\pi_{0}$ , can be estimated using samples from experiments already conducted in $\\mathcal{M}^{1},\\dot{\\mathcal{M}}^{2}$ , and baseline characteristics of patients in $\\mathcal{M}^{\\tilde{0}}$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Estimation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section considers the estimation of the effect of policies, building on the identification criterion in Thm. 1. We first parameterize the identification estimand in Eq. (1) with two types of nuisance parameters $\\pmb{\\mu}$ and $\\omega$ . $\\pmb{\\mu}$ is a collection of regression parameters, and $\\omega$ is a collection of the ratio of distributions. ", "page_idx": 4}, {"type": "text", "text": "The regression nuisance parameters are defined as follows: $\\mu_{0}^{2}(\\mathbf{C},W,\\mathbf{X}):=\\mathbb{E}_{P_{\\pi_{2}}^{2}}[Y\\mid\\mathbf{C},W,\\mathbf{X}]$ and $\\begin{array}{r}{\\check{\\mu}_{0}^{2}(\\mathbf{C},W,X_{1})\\;:=\\;\\sum_{x_{2}}\\mu_{0}^{2}(\\mathbf{C},W,X_{1},x_{2})\\pi_{0}(x_{2}\\;\\mid\\;\\mathbf{C},W,X_{1})}\\end{array}$ . Recursively, $\\mu_{0}^{1}(\\mathbf{C},X_{1})\\;:=$ $\\mathbb{E}_{P_{\\pi_{1}}^{1}}\\big[\\check{\\mu}_{0}^{2}(\\mathbf{C},W,X_{1})\\ |\\ \\mathbf{C},\\bar{X_{1}}\\big]$ and $\\begin{array}{r}{\\check{\\mu}_{0}^{1}(\\mathbf{C})\\;:=\\;\\sum_{x_{1}}\\mu_{0}^{1}(\\mathbf{C},x_{1})\\pi_{0}(x_{1}\\mid\\mathbf{C})}\\end{array}$ . Eq. (1) can be parameterized as $\\mathbb{E}_{P_{\\pi_{0}}^{0}}[Y]=\\mathbb{E}_{P^{0}}[\\check{\\mu}_{0}^{1}(\\mathbf{C})]$ . ", "page_idx": 4}, {"type": "text", "text": "On the other hand, the ratio nuisance parameters $\\omega_{0}^{2},\\omega_{0}^{1}$ are defined as functionals satisfying the following properties: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{P_{\\pi}^{0}}[Y]=\\mathbb{E}_{P_{\\pi_{2}}^{2}}[\\mu_{0}^{2}(\\mathbf{C},W,\\mathbf{X})\\pi_{0}^{2}(\\mathbf{C},W,\\mathbf{X})]=\\mathbb{E}_{P_{\\pi_{1}}^{1}}[\\mu_{0}^{1}(\\mathbf{C},X_{1})\\pi_{0}^{1}(\\mathbf{C},X_{1})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "A closed form of the $\\omega_{0}^{i}$ is provided in the later section at Eq. (14). By the definition of ratio nuisances, Eq. (1) can be parameterized as $\\mathbb{E}_{P_{\\pi_{0}}^{0}}[Y]=\\mathbb{E}_{P_{\\pi_{2}}^{2}}[\\omega_{0}^{2}(\\mathbf{C},\\dot{W},\\mathbf{X})Y]$ . Equipped with these nuisances, we now present the DML-based estimator for the target query: ", "page_idx": 4}, {"type": "text", "text": "Definition 5 (DML for combining two experiments). Let $\\mathcal{D}^{2}\\,\\sim\\,P_{\\pi_{2}}^{2}(\\mathbf{V}),\\,\\mathcal{D}^{1}\\,\\sim\\,P_{\\pi_{1}}^{1}(\\mathbf{V})$ and $\\mathscr{D}^{0}\\sim{\\cal P}^{0}({\\bf C})$ . Let $L\\geqslant2$ denote a fixed number. ", "page_idx": 4}, {"type": "text", "text": "1. Sample split: For $\\ell=1,\\cdots\\,,L,$ , randomly split $\\mathcal{D}^{i}$ for $i\\in\\{0,1,2\\}$ into $L$ -fold. The \u2113\u2019th partition of the sample is denoted $\\mathcal{D}_{\\ell}^{i}$ . The complement is $\\mathcal{D}_{-\\ell}^{i}:=\\mathcal{D}^{i}\\backslash\\mathcal{D}_{\\ell}^{i}$ . ", "page_idx": 4}, {"type": "text", "text": "2. Nuisance estimation: For each $\\ell=1,\\cdots\\,,L,$ , learn the estimator model $\\hat{\\mu}_{\\ell}^{2}$ and $\\hat{\\mu}_{\\ell}^{1}$ for $\\mu_{0}^{2},\\mu_{0}^{1}$ using samples $\\mathcal{D}_{-\\ell}^{2},\\mathcal{D}_{-\\ell}^{1}$ , respectively. Also, learn the estimation model for $\\hat{\\omega}_{\\ell}^{1},\\hat{\\omega}_{\\ell}^{2}$ for $\\omega_{0}^{1},\\omega_{0}^{2}$ using samples $\\mathcal{D}_{-\\ell}^{i}$ for $i=0,1,2,$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "3. Evaluation: The DML estimator $\\hat{\\psi}$ for $\\mathbb{E}_{P_{\\pi_{0}}^{0}}[Y]$ is then given as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\psi}:=\\frac{1}{L}\\sum_{\\ell=1}^{L}\\mathbb{E}_{\\mathcal{D}_{\\ell}^{2}}[\\hat{\\omega}_{\\ell}^{2}\\{Y-\\hat{\\mu}_{\\ell}^{2}\\}]+\\mathbb{E}_{\\mathcal{D}_{\\ell}^{1}}[\\hat{\\omega}_{\\ell}^{1}\\{\\check{\\mu}_{\\ell}^{2}-\\hat{\\mu}_{\\ell}^{1}\\}]+\\mathbb{E}_{\\mathcal{D}_{\\ell}^{0}}[\\check{\\mu}_{\\ell}^{1}].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Estimating the ratio nuisance $\\{\\hat{\\omega}^{1},\\hat{\\omega}^{2}\\}$ can be challenging due to the necessity of estimating density $\\frac{P_{\\pi_{1}}^{1}(\\mathbf{C})}{P_{\\pi_{2}}^{2}(\\mathbf{C})}$ or $\\frac{P_{\\pi_{1}}^{1}\\left(W|\\mathbf{C},X_{1}\\right)}{P_{\\pi_{2}}^{2}\\left(W|\\mathbf{C},X_{1}\\right)}$   \nratios like or . We employ the classification-based method for estimating the density [17, Sec. 5.4]. To illustrate this method, consider estimating PP  \u03c0\u03c0212ppCCqq. We assign $\\lambda=1$ if samples of $\\mathbf{C}$ are from $P_{\\pi_{1}}^{1}$ and $\\lambda=0$ if from $P_{\\pi_{2}}^{2}$ . Then, it\u2019s provable that $\\begin{array}{r}{\\frac{P_{\\pi_{1}}^{1}(\\mathbf{C})}{P_{\\pi_{2}}^{2}(\\mathbf{C})}=\\frac{P(\\lambda=1|\\mathbf{C})}{P(\\lambda=0|\\mathbf{C})}}\\end{array}$ , which can be estimated using off-the-shelf probabilistic classification estimators.   \nThe error of the DML estimator is presented below: ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 (Learning Guarantees). Suppose $\\hat{\\mu}_{\\ell}^{2},\\hat{\\mu}_{\\ell}^{1}\\,\\,\\,\\,<\\,\\,\\,\\infty$ and $\\begin{array}{r l r}{0}&{{}<}&{\\hat{\\omega}_{\\ell}^{2},\\hat{\\omega}_{\\ell}^{1}\\ \\ <\\ \\ \\infty.}\\end{array}$ . Define $\\phi^{2}({\\bf V};\\mu^{2},\\pi^{2})\\;\\;:=\\;\\;\\omega^{2}({\\bf C},W,{\\bf X})\\{Y\\;-\\;\\mu^{2}({\\bar{\\bf C}},{\\bar{W}},{\\bf X})\\}$ , $\\phi^{1}(({\\bf C},W,X_{1});\\breve{\\mu}^{\\bar{2}},\\mu^{\\bar{1}},\\omega^{1})\\;\\;:=\\;\\;$ $\\omega^{1}({\\bf C},X_{1})\\{\\tilde{\\mu}^{2}({\\bf C},W,X_{1})\\,-\\,\\mu^{1}({\\bf C},X_{1})\\}$ , and $\\phi^{0}(\\dot{\\mathbf{C}};\\breve{\\mu}^{1})\\,:=\\,\\breve{\\mu}^{1}(\\mathbf{C})\\,-\\,\\psi_{0}$ . For $\\textit{i}=\\,0,1,2$ , define $\\phi_{0}^{i}$ as $\\phi^{i}$ equipped with true nuisances $(\\mu_{0}^{i},\\pi_{0}^{i})$ and $\\hat{\\phi}_{\\ell}^{i}$ as $\\phi^{i}$ equipped with estimated nuisances $\\hat{\\mu}_{\\ell}^{i},\\hat{\\pi}_{\\ell}^{i}$ . Define $\\begin{array}{r}{R_{i}:=(1/L)\\sum_{\\ell=1}^{L}(\\mathbb{E}_{\\mathcal{D}_{\\ell}^{i}}[\\hat{\\phi}_{\\ell}^{i}]-\\mathbb{E}_{P^{i}}[\\hat{\\phi}_{\\ell}^{i}])}\\end{array}$ for $i=0,1,2$ . Then, ", "page_idx": 5}, {"type": "text", "text": "1. The error ${\\hat{\\psi}}-\\psi_{0}$ is decomposed as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\psi}-\\psi_{0}=\\sum_{i=0}^{2}R_{i}+\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=1}^{2}\\mathbb{E}_{P_{\\pi_{i}}^{i}}\\big[\\{\\hat{\\mu}_{\\ell}^{i}-\\mu_{0}^{i}\\}\\{\\omega_{0}^{i}-\\hat{\\omega}_{\\ell}^{i}\\}\\big].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "2. Let $\\rho_{i,0}^{2}:=\\mathbb{V}_{P_{\\pi_{i}}^{i}}[\\phi_{0}^{i}]$ . With probability $(W.P)$ greater than $1-\\epsilon$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{2}R_{i}\\leqslant3\\sqrt{\\frac{2}{\\epsilon}}\\left(\\sqrt{\\sum_{i=0}^{2}\\frac{\\rho_{i,0}}{|\\mathcal{D}^{i}|}}+\\sqrt{\\sum_{\\ell=1}^{L}\\sum_{i=0}^{2}\\frac{\\|\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}\\|_{P_{\\pi_{i}}^{i}}^{2}}{|\\mathcal{D}_{\\ell}^{i}|}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3. Let $\\kappa_{i,0}^{3}:=\\mathbb{E}_{P_{\\pi_{i}}^{i}}[|\\phi_{0}^{i}|^{3}]$ . Let $\\Phi(x)$ denote the standard normal CDF. W.P greater than $1-\\epsilon,$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left|P_{\\pi_{i}}^{i}\\left(\\frac{\\sqrt{|\\mathcal{D}^{i}|}}{\\rho_{k,0}}R_{i}<x\\right)-\\Phi(x)\\right|\\leqslant\\frac{1}{\\sqrt{2\\pi}}\\sqrt{\\frac{L^{2}}{\\epsilon}\\sum_{\\ell=1}^{L}\\frac{\\|\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}\\|_{P_{\\pi_{i}}^{i}}^{2}}{|\\mathcal{D}_{\\ell}^{i}|}}+\\frac{0.4748\\kappa_{i,0}^{3}}{\\rho_{i,0}^{3}\\sqrt{|\\mathcal{D}^{k}|}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "If the nuisance parameters $\\hat{\\mu}_{\\ell}^{i}$ and $\\hat{\\pi}_{\\boldsymbol{\\ell}}^{i}$ converge at a rate of $n^{-1/4}$ (where $n$ is the size of the smallest sample set), the DML estimator achieves a faster convergence rate of $n^{-1/2}$ . This rapid convergence allows its asymptotic distribution to closely approximate the standard normal distribution, as is further clarified in the asymptotic analysis: ", "page_idx": 5}, {"type": "text", "text": "Theorem 3 (Asymptotic Error). Suppose each nuisance estimates $\\hat{\\mu}_{\\ell}^{2},\\hat{\\mu}_{\\ell}^{1},\\hat{\\omega}_{\\ell}^{2},\\hat{\\omega}_{\\ell}^{1}$ are $L_{2}$ -consistent and bounded. Then, the error of the DML estimator $\\hat{\\psi}$ in Def. $^{5}$ is given as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\psi}-\\psi_{0}=\\sum_{i=0}^{2}R_{i}+\\sum_{\\ell=1}^{L}O_{P_{\\ell}^{2}}\\left(\\|\\hat{\\mu}_{\\ell}^{2}-\\mu_{0}^{2}\\|\\|\\hat{\\omega}_{\\ell}^{2}-\\omega_{0}^{2}\\|\\right)+\\sum_{\\ell=1}^{L}O_{P_{\\pi_{1}}^{1}}(\\|\\hat{\\mu}_{\\ell}^{1}-\\mu_{0}^{1}\\||\\hat{\\omega}_{\\ell}^{1}-\\omega_{0}^{1}\\|),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $R_{i}$ converges in distribution to norma $\\mathcal{I}(0,\\rho_{i,0}^{2})$ . ", "page_idx": 5}, {"type": "text", "text": "Eq. (7) implies that the error term $\\hat{\\psi}-\\psi_{0}$ converges to zero faster than the convergence rate of nuisances, which is a property known as debiasedness. ", "page_idx": 5}, {"type": "image", "img_path": "PSubtZAitM/tmp/7b43b192fc41b7d2b0f9570062d12a52c92665af84276b4e56a4704c4b1463f0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 2: Graphs illustrating the inference of a multiple treatment strategy $\\pi_{0}\\;:=\\;\\{\\pi_{0}(x_{1}\\;\\;|\\;\\;c_{1}),\\pi_{0}(x_{2}\\;\\;|\\;\\;$ $x_{1},c_{2},w_{1}),\\pi_{0}^{-}(x_{3}\\mid x_{2},c_{3},w_{2})\\}$ given data from source domains $\\mathcal{M}^{1},\\mathcal{M}^{2},\\bar{\\mathcal{M}}^{3}$ . ", "page_idx": 6}, {"type": "text", "text": "4 Combining multiple experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we extend our method to incorporate the combination of data from multiple experiments, specifically focusing on $m$ different experiments derived from varied policies $(\\pi_{i})$ in distinct source domains $(\\mathbf{\\dot{\\mathcal{M}}}^{i})$ . A practical scenario for this task is the following: ", "page_idx": 6}, {"type": "text", "text": "Example 3 (Multi-stage treatment strategies). Consider a scenario involving hospitals in three different cities: New York (domain 1 with $P^{1},\\mathcal{G}^{1})$ , Los Angeles (domain 2 with $P^{2^{\\mathbf{\\lambda}}},\\mathcal{G}^{2})$ , and San Francisco (domain 3 with $P^{3},\\mathcal{G}^{3})$ . Each hospital has different guidelines, i.e., policies, for diabetes treatment. In New York, the hospital focuses on insulin therapy adjustment based on the patient lifestyle choices, primarily for Type 1 Diabetes patients $\\left(\\pi_{1}\\right)$ . In Los Angeles, the hospital focuses on team diet and exercise regimen adjustments, primarily for Type 2 Diabetes patients $\\left(\\pi_{2}\\right)$ . In contrast, San Francisco\u2019s approach involves advanced monitoring and AI-driven predictive adjustments for higher-risk diabetes patients. Now, as the leader of a new clinical team in Chicago (the target domain with $P^{0},\\mathcal{G}^{0})$ , the task is to evaluate a novel candidate treatment policy $\\pi_{0}$ , which integrates the strategies from these three domains to provide comprehensive care for both Type 1 and Type 2 Diabetes patients. The structure of the problem is captured in causal diagrams in Fig. 2, illustrating the data-generating process, the experiments in each city, and the assumed discrepancies between these source domains and Chicago. \u25a0 ", "page_idx": 6}, {"type": "text", "text": "4.1 Identification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We consider a sequence of variables $(\\mathbf{C},X_{1},W_{1},\\cdot\\cdot\\cdot\\,,X_{m},W_{m}:=Y)$ where $(\\mathbf{C},\\mathbf{W}^{(i-1)})$ represent the covariates corresponding to each of the $i^{:}$ \u2019th experiments, and $(X_{1},\\cdot\\cdot\\cdot,X_{m})$ are the corresponding treatment variables. We are given samples drawn from $P_{\\pi_{i}}^{i}(\\mathbf{V})$ for $i=1,\\cdot\\cdot\\cdot,m$ and $P^{0}(\\mathbf{\\bar{C}})$ . We will leverage causal diagrams $\\mathcal{G}_{\\pi_{i}}$ and selection diagrams $g\\Delta_{0,i}$ for every $i=1,\\cdots,m$ . Formally, the task signature is given as follows: ", "page_idx": 6}, {"type": "text", "text": "\u2022 Input: Samples from $P_{\\pi_{i}}^{i}(\\mathbf{V})$ for $i\\ =\\ 1,\\cdot\\cdot\\ ,m$ and $P^{0}(\\mathbf{C}_{1},\\mathbf{C}_{2})$ ; Causal diagrams $\\mathcal{G}_{\\pi_{i}}^{i}$ and   \nselection diagrams $\\mathcal{G}_{\\pi_{0}}^{\\Delta_{0,i}}$ for $i=1,\\cdot\\cdot\\cdot,m$ .   \n\u2022 Query: Estimate the effect of the target policy $\\pi_{0}$ on the target domain $\\mathcal{M}^{0}$ ; i.e., $\\mathbb{E}_{P_{\\pi_{0}}^{0}}[Y]$ .   \nDefinition 6 (Adjustment criterion for combining multiple experiments). The adjustment cri  \nterion for combining multiple policies are the following $^d$ -separation criterion in the the DTRs   \n$\\mathcal{G}_{\\pi_{0}},\\mathcal{G}_{\\pi_{1}},\\cdot\\cdot\\cdot\\,,\\mathcal{G}_{\\pi_{m}}$ and the selection diagram $\\mathcal{G}_{\\pi_{0}}^{\\bar{\\Delta}_{0,1}},\\cdot\\cdot\\cdot\\,,\\mathcal{G}_{\\pi_{0}}^{\\bar{\\Delta}_{0,\\bar{m}}}$   \n1. Domain transfer for $Y$ : $(Y\\perp\\!\\!\\!\\perp\\mathbf{S}\\mid\\mathbf{C},\\!\\!\\mathbf{W},\\!\\!\\mathbf{X})$ in $\\mathcal{G}_{\\pi_{0}}^{\\Delta_{0,m}}$ ; i.e., the distribution over $Y$ is invariant between the source distribution from ${\\mathcal{M}}^{m}$ and the target.   \n2. Domain transfer for $W_{i}$ for $i\\,=\\,1,\\cdot\\cdot\\cdot\\,,m-1$ : $(W_{i}\\perp\\!\\!\\!\\perp\\mathbf{S}\\mid\\mathbf{C}^{(i)},\\mathbf{W}^{(i-1)})$ in $\\mathcal{G}_{\\pi_{0}}^{\\Delta_{0,i}}$ ; i.e., the distribution over $W_{i}$ is invariant between the source distribution from $\\mathcal{M}^{i}$ and the target. ", "page_idx": 6}, {"type": "text", "text": "3. Adjustment for $Y$ : $(Y\\perp\\!\\!\\!\\perp\\pi_{i}\\mid\\mathbf{C},\\mathbf{W},\\mathbf{X})$ in $\\mathcal{G}_{\\pi_{i}}$ for $i\\in\\{0,m\\}$ ; i.e., the distribution over $Y$ is invariant between regimes $\\pi_{0}$ and $\\pi_{m}$ . ", "page_idx": 7}, {"type": "text", "text": "4. Adjustment for $W_{i}\\;i=1,\\cdots\\,,m-1.$ : $(W_{i}\\perp\\!\\!\\!\\perp\\pi_{j}\\mid{\\bf C}^{(i)},{\\bf X}^{(i-1)})$ in $\\mathcal{G}_{\\pi_{j}}$ for $j\\in\\{0,i\\}$ ; i.e., the distribution over $W_{i}$ is invariant between regimes $\\pi_{0}$ and $\\pi_{i}$ . ", "page_idx": 7}, {"type": "text", "text": "These conditions lead to the following identification criterion. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4 (Adjustment for combining multiple experiments). Under the adjustment criterion in Def. 6, the target query $\\psi_{0}:=\\mathbb{E}_{P_{\\pi_{0}}^{0}}[Y]$ is identifiable from the samples from $P_{\\pi_{1}}^{\\mathrm{i}}(\\mathbf{V}),\\cdot\\cdot\\cdot,P_{\\pi_{m}}^{m}(\\mathbf{V})$ and $P^{0}(\\mathbf{C})$ . Specifically, it\u2019s expressed as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{5}_{P_{\\pi_{0}}^{0}}[Y]=\\sum_{\\mathbf{w},\\mathbf{c},\\mathbf{x}}\\mathbb{E}_{P_{\\pi_{m}}^{m}}[Y\\mid\\mathbf{c},\\mathbf{w},\\mathbf{x}]\\prod_{i=1}^{m-1}P_{\\pi_{i}}^{i}(w_{i}\\mid\\mathbf{c},\\mathbf{x}^{(i-1)},\\mathbf{w}^{(i-1)})\\prod_{j=1}^{m-1}\\pi_{0}(x_{j}\\mid\\mathbf{c},\\mathbf{w}^{(j-1)})P^{0}(\\mathbf{c}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "4.2 Estimation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The regression nuisance parameters are defined as follows. We first define the following nuisance. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{0}^{m}(\\mathbf{C},\\mathbf{W},\\mathbf{X}):=\\mathbb{E}_{P_{\\pi_{m}}^{m}}[Y\\mid\\mathbf{C},\\mathbf{W},\\mathbf{X}]\\qquad\\qquad\\qquad\\qquad}\\\\ {\\tilde{\\mu}_{0}^{m}(\\mathbf{C},\\mathbf{W},\\mathbf{X}^{(m-1)}):=\\displaystyle\\sum_{x_{m}}\\pi_{0}^{m}(x_{m}\\mid\\mathbf{C},\\mathbf{W}^{(m-1)})\\mu_{0}^{m}(\\mathbf{C},\\mathbf{W},\\mathbf{X}^{(m-1)},x_{m})}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For $i=m-1,\\cdots\\,,1$ , the other nuisances are defined in a following manner: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mu_{0}^{i}(\\mathbf{C},\\mathbf{W}^{(i-1)},\\mathbf{X}^{(i)}):=\\mathbb{E}_{P_{\\pi_{i}}^{i}}[\\tilde{\\mu}_{0}^{i+1}(\\mathbf{C},\\mathbf{W}^{(i)},\\mathbf{X}^{(i)})\\mid\\mathbf{C},\\mathbf{W}^{(i-1)},\\mathbf{X}^{(i)}],\\;\\;\\;}\\\\ &{}&{\\tilde{\\mu}_{0}^{i}(\\mathbf{C},\\mathbf{W}^{(i-1)},\\mathbf{X}^{(i-1)}):=\\displaystyle\\sum_{x_{i}}\\mu_{0}^{i}(\\mathbf{C},\\mathbf{W}^{(i-1)},\\mathbf{X}^{(i-1)},x_{i})\\pi_{0}^{i}(x_{i},\\mathbf{C},\\mathbf{W}^{(i-1)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We note that Eq. (8) can be parameterized as $\\mathbb{E}_{P_{\\pi_{0}}^{0}}[Y]=\\mathbb{E}_{P^{0}}[\\check{\\mu}_{0}^{1}(\\mathbf{C})]$ . On the other hand, the ratio nuisance parameters $\\omega_{0}^{i}$ for $i=1,\\cdot\\cdot\\cdot,m$ are defined as functionals satisfying the followings: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P_{\\pi}^{0}}[Y]=\\mathbb{E}_{P_{\\pi_{i}}^{i}}[\\mu_{0}^{i}(\\mathbf{C},\\mathbf{W}^{(i-1)},\\mathbf{X}^{(i)})\\omega_{0}^{i}(\\mathbf{C},\\mathbf{W}^{(i-1)},\\mathbf{X}^{(i)})],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the closed form is given as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\omega_{0}^{i}=\\frac{\\pi_{0}(X_{i}\\mid\\mathbf{C},\\mathbf{W}^{(i-1)})\\prod_{j=1}^{i-1}P_{\\pi_{j}}^{j}(W_{j}\\mid\\mathbf{C},\\mathbf{X}^{(j-1)},\\mathbf{W}^{(j-1)})\\pi_{0}(X_{j}\\mid\\mathbf{C},\\mathbf{W}^{(j-1)})P^{0}(\\mathbf{C})}{P_{\\pi_{0}}^{0}(\\mathbf{C},\\mathbf{W}^{(i-1)},\\mathbf{X}^{(i)})}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Eq. (8) can be parameterized as $\\mathbb{E}_{P_{\\pi_{m}}^{m}}\\big[\\boldsymbol{\\omega}^{(m)}(\\mathbf{C},\\mathbf{W}^{(m-1)},\\mathbf{X}^{(m)})Y\\big]$ . Equipped with these nuisances, we define a corresponding estimator as follows. ", "page_idx": 7}, {"type": "text", "text": "Definition 7 (DML for combining multiple experiments). Let $D^{i}\\sim P_{\\pi_{i}}^{i}(\\mathbf{V})$ for $i=1,\\cdots,m$ and $\\mathscr{D}^{0}\\sim{\\cal P}^{0}({\\bf C})$ . Let $L\\geqslant2$ denote a fixed number. ", "page_idx": 7}, {"type": "text", "text": "1. Sample split: For $\\ell=1,\\cdots\\,,L,$ , randomly split $\\mathcal{D}^{i}$ for $i\\in\\{0,1,\\cdot\\cdot\\cdot,m\\}$ into $L$ -fold. The \u2113\u2019th partition of the sample is denoted $\\mathcal{D}_{\\ell}^{i}$ . The complement is $\\mathcal{D}_{-\\ell}^{i}:=\\mathcal{D}^{i}\\backslash\\mathcal{D}_{\\ell}^{i}$ . ", "page_idx": 7}, {"type": "text", "text": "2. Nuisance estimation: For each $\\ell\\ =\\ 1,\\cdot\\cdot\\cdot\\,,L,$ , learn the estimator model $\\hat{\\mu}_{\\ell}^{m},\\cdots\\,,\\hat{\\mu}_{\\ell}^{1}$ for $\\mu_{0}^{m},\\dots,\\mu_{0}^{1}$ using samples $\\mathcal{D}_{-\\ell}^{m},\\mathcal{D}_{-\\ell}^{1}$ , respectively. Also, learn the estimation model for $\\hat{\\omega}_{\\ell}^{1},\\cdot\\cdot\\cdot,\\hat{\\omega}_{\\ell}^{m}$ for $\\omega_{0}^{1},\\cdots,\\omega_{0}^{m}$ using samples $\\mathcal{D}_{-\\ell}^{i}$ for $i=0,1,\\cdots\\,,m$ , respectively. ", "page_idx": 7}, {"type": "text", "text": "3. Evaluation: The DML estimator $\\hat{\\psi}$ for $\\mathbb{E}_{P_{\\pi_{0}}^{0}}[Y]$ is then given as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{\\psi}:=\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=1}^{m}\\mathbb{E}_{\\mathcal{D}_{\\ell}^{i}}[\\hat{\\omega}_{\\ell}^{i}\\{\\hat{\\tilde{\\mu}}_{\\ell}^{i+1}-\\hat{\\mu}_{\\ell}^{i}\\}]+\\mathbb{E}_{\\mathcal{D}_{\\ell}^{0}}[\\check{\\hat{\\mu}}_{\\ell}^{1}].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The error of the DML estimator is presented below: ", "page_idx": 7}, {"type": "text", "text": "Theorem 5 (Learning Guarantees). Suppose $\\mu_{0}^{i},\\hat{\\mu}_{\\ell}^{i}<\\infty$ and $0<\\pi_{0}^{i},\\hat{\\pi}_{\\ell}^{i}<\\infty$ almost surely for $i=1,\\cdots\\,,m.$ . Define $\\phi^{i}((\\mathbf{C},\\mathbf{W}^{(i)},\\mathbf{X}^{(i)});\\omega^{i},\\check{\\mu}^{i+1},\\mu^{i}):=\\omega^{i}\\{\\check{\\mu}^{i+1}-\\mu^{i}\\}$ for $i=1,\\cdots,m_{}$ , where $\\check{\\mu}^{m+1}:=Y$ . Let $\\phi^{0}(\\mathbf{C};\\breve{\\mu}^{1}):=\\,\\breve{\\mu}^{1}-\\breve{\\psi_{0}}$ . For $i\\,=\\,0,\\cdots\\,,m,$ define $\\phi_{0}^{i}$ as $\\phi^{i}$ equipped with true nuisances, and $\\hat{\\phi}_{\\ell}^{i}$ as $\\phi^{i}$ equipped with estimated nuisances. Define $\\begin{array}{r}{R_{i}:=(1/L)\\sum_{\\ell=1}^{L}(\\mathbb{E}_{\\mathcal{D}_{\\ell}^{i}}[\\hat{\\phi}_{\\ell}^{i}]-}\\end{array}$ $\\mathbb{E}_{P^{i}}[\\hat{\\phi}_{\\ell}^{i}])$ for $i=0,1,\\cdots\\,,m$ . Then, ", "page_idx": 8}, {"type": "text", "text": "1. The error $\\hat{\\psi}-\\psi_{0}$ is decomposed as follows: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{\\psi}-\\psi_{0}=\\sum_{i=0}^{m}R_{i}+\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=1}^{m}\\mathbb{E}_{P_{\\pi_{i}}^{i}}[\\{\\hat{\\mu}_{\\ell}^{i}-\\mu_{0}^{i}\\}\\{\\omega_{0}^{i}-\\hat{\\omega}_{\\ell}^{i}\\}].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "2. Let $\\rho_{i,0}^{2}:=\\mathbb{V}_{P_{\\pi_{i}}^{i}}[\\phi_{0}^{i}]$ . With probability $(W.P)$ greater than $1-\\epsilon$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{m}R_{i}\\leqslant(m+1)\\sqrt{\\frac{2}{\\epsilon}}\\left(\\sqrt{\\sum_{i=0}^{m}\\frac{\\rho_{i,0}^{2}}{|\\mathcal{D}^{i}|}}+\\sqrt{\\sum_{\\ell=1}^{L}\\sum_{i=0}^{m}\\frac{\\|\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}\\|_{P_{\\pi_{i}}^{i}}^{2}}{|\\mathcal{D}_{\\ell}^{i}|}}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "3. Let $\\kappa_{i,0}^{3}:=\\mathbb{E}_{P_{\\pi_{i}}^{i}}[|\\phi_{0}^{i}|^{3}]$ . Let $\\Phi(x)$ denote the standard normal CDF. W. $P$ greater than $1-\\epsilon_{i}$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\left|P_{\\pi_{i}}^{i}\\left(\\frac{\\sqrt{|\\mathcal{D}^{i}|}}{\\rho_{k,0}}R_{i}<x\\right)-\\Phi(x)\\right|\\leqslant\\frac{1}{\\sqrt{2\\pi}}\\sqrt{\\frac{1}{\\epsilon}\\sum_{\\ell=1}^{L}\\frac{\\|\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}\\|_{P_{\\pi_{i}}^{i}}^{2}}{|\\mathcal{D}_{\\ell}^{i}|}}+\\frac{0.4748\\kappa_{i,0}^{3}}{\\rho_{i,0}^{3}\\sqrt{|\\mathcal{D}^{k}|}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "A corresponding asymptotic error analysis is following: ", "page_idx": 8}, {"type": "text", "text": "Theorem 6 (Asymptotic Error). Suppose each nuisance estimates $\\hat{\\mu}_{\\ell}^{1},\\cdots,\\hat{\\mu}_{\\ell}^{m}$ and $\\hat{\\omega}_{\\ell}^{1},\\cdot\\cdot\\cdot,\\hat{\\omega}_{\\ell}^{m}$ are $L_{2}$ -consistent and bounded. Then, the error of the DML estimator $\\hat{\\psi}$ in Def. 7 is given as follows: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{\\psi}-\\psi_{0}=\\sum_{i=0}^{m}R_{i}+\\sum_{\\ell=1}^{L}\\sum_{i=1}^{m}O_{P_{\\pi_{i}}^{i}}(\\|\\hat{\\mu}_{\\ell}^{i}-\\mu_{0}^{i}\\|\\|\\hat{\\omega}_{\\ell}^{i}-\\omega_{0}^{i}\\|),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $R_{i}$ converges in distribution to Norma $\\mathcal{I}(0,\\rho_{i,0}^{2})$ . ", "page_idx": 8}, {"type": "text", "text": "Similarly to Thm. 3, this result implies that the DML estimator $\\hat{\\psi}$ converges fast even when the nuisance estimates converge relatively slowly. ", "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we demonstrate the proposed estimators in Defs. (5,7) for combining multiple experimental datasets from different domains. We first compared the estimators on synthetic data to provide evidence of the fast convergence and doubly robustness behaviours of the proposed estimators. We conclude with an analysis of the ACTG 175 clinical trial [21] and Project STAR. We will use $T^{\\mathrm{est}}(\\mathbf{x})$ for $\\mathrm{est}\\in\\{\\mathrm{reg},\\mathrm{pw},\\mathrm{dml}\\}$ to denote the estimators $\\{\\mathrm{OM},\\mathrm{PW},\\mathrm{DML}\\}$ for the policy effect $\\mathbb{E}_{P_{\\pi_{0}}^{0}}Y$ . OM and PW estimators are purely based on the regression-based nuisances $\\pmb{\\mu}$ and $\\omega$ , respectively. To assess the quality of each estimator, we consider the absolute error (AE) as $\\mathrm{AE}^{\\mathrm{est}}\\,{\\overset{\\cdot}{=}}\\,\\,|T^{\\mathrm{est}}(\\mathbf{x})-\\mathbb{E}_{P_{\\pi_{0}}^{0}}[Y]\\,|$ . We used XGBoost [12] to estimate nuisances. ", "page_idx": 8}, {"type": "text", "text": "Synthetic Simulations We ran 100 simulations for each $N=\\{2500,5000,10000,20000\\}$ where $N$ is the sample size. We measure the $\\mathrm{AE^{est}}$ in the presence of the \u2018converging noise $\\epsilon^{\\ '}$ in estimating the nuisance, decaying at a $N^{-1/4}$ rate (i.e., $\\epsilon\\,\\sim\\,\\mathrm{normal}(N^{-1/4},N^{-1/4})$ , where $N$ is the size of samples). To enforce the convergence rate of nuisance estimates no faster than the decaying rate $n^{-1/4}$ , we add $\\epsilon$ to all nuisance estimates. This scenario is inspired by the experimental design discussed in [27]. The AE plots for combining two/multiple experiments are presented in Figs. (3a, 3b). For all examples, the proposed DML estimator outperforms the other two estimators by achieving fast convergence. This result corroborates the robustness property in Thm. (3, 6), which implies that the proposed estimator converges faster than the other counterparts. ", "page_idx": 8}, {"type": "image", "img_path": "PSubtZAitM/tmp/0ead7e29dbe2a91e260adc9e451c9ecd73b72ea644962c75a178344304018bc8.jpg", "img_caption": ["Figure 3: Comparison of the proposed DML estimator with other counterparts (outcome-based model called \u2018OM\u2019, and the probability-weighting-based model labelled \u2018PW\u2019) for $(\\mathbf{a},\\mathbf{b})$ synthetic data analysis for combining two and multiple experiments; and $(\\mathbf{c},\\mathbf{d})$ real-world data analysis under the noise-free or noisy environments in learning nuisances. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "External validity: ACTG 175 To provide empirical evidence, we analyze the ACTG 175 randomized trial [21], which assessed therapies for reducing CD4 T cell counts in HIV patients. Participants were randomly assigned to treatments $X_{2}\\in\\{0,1\\}$ , with prior anti-retroviral drug use $X_{1}\\in\\{0,1\\}$ recorded. Patient demographics $C_{1},C_{2}$ \u2014including gender, age, weight, and Karnofsky score\u2014were collected, and CD4 T cell counts $(W)$ were measured. To simulate an alternative study with a modified guideline for $X_{1}$ , we sub-sampled ACTG 175, adjusting covariate distributions and assignments of $\\{X_{1},X_{2}\\}$ . Specifically, we evaluate a stochastic policy $\\pi_{0}\\;=\\;\\{\\pi_{0}(x_{1}\\:\\:|\\:\\:{\\pmb{c}}_{1}),\\pi_{0}(x_{1}\\:\\:|\\:\\:{\\pmb{\\bar{c}}}_{2})\\}$ for combining $X_{1}$ and $X_{2}$ based on $\\mathbf{C}_{1},\\mathbf{C}_{2}$ , with distribution $P^{0}$ representing a location with differing covariate distributions and treatment assignments. Further details are provided in Appendix D.2. ", "page_idx": 9}, {"type": "text", "text": "We evaluated the $\\mathrm{AE^{est}}$ of all proposed estimators with and without noise (as described in the synthetic simulations). The AE plots are shown in Figs. (3c, 3d). Results indicate that both the regression and DML estimators converge to the true policy effect faster under noisy conditions, whereas the PW estimator converges more slowly. However, DML does not consistently outperform at all sample sizes (see Fig. 3c), as its error is influenced by the combined errors in the OM and PW estimators. Consequently, high error in the PW estimator may lead to increased error in the DML estimator. ", "page_idx": 9}, {"type": "text", "text": "External validity: Project STAR We further examine policies on teacher-student ratios (i.e., class sizes) to improve academic achievement, using a semi-synthetic adaptation of the Project STAR dataset [40]. This longitudinal study evaluated the impact of teacher-student ratios on academic outcomes for students in kindergarten through third grade, with students randomized each year to one of three class size interventions. Here, we assess a 3-stage policy setting student-teacher ratios across Grades 0, 1, and 2, observing academic scores as intermediate outcomes, with baseline covariates (e.g., ethnicity, gender) and final academic scores at the end of Grade 3 as the primary outcome. To emulate data collected across different domains, we subsample using various probabilities to shift baseline covariate distributions, as done in ACTG 175 (see Appendix D.3 for details). We evaluated the PW, OM, and DML estimators across dataset sizes, plotting their absolute errors against the true effect of the candidate policy. Results, shown in Fig. 4, mirror earlier experiments, with all estimators improving as sample size increases and DML showing faster convergence. ", "page_idx": 9}, {"type": "image", "img_path": "PSubtZAitM/tmp/a5ea1ccf35fb042a93e76cf514ea9492e9c1e1db6810bbd7311a483352f3a0c2.jpg", "img_caption": ["Figure 4: STAR Results. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper has considered the evaluation of the effectiveness of policies in settings where the available data is sampled from distributions that differ from the population in the target domain. We have illustrated this task with the problem of extrapolating the results of a clinical trial in both working examples and real-world scenarios to evaluate variations of the treatment in different populations. Our contributions are (1) introducing several identification criteria for the effectiveness of policies given experimental datasets from two or more domains and (2) developing doubly robust estimators for these settings that achieve fast convergence. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank anonymous reviewers for constructive comments to improve the manuscript. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Aman Agarwal, Soumya Basu, Tobias Schnabel, and Thorsten Joachims. Effective evaluation using logged bandit feedback from multiple loggers. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pages 687\u2013696, 2017.   \n[2] Elias Bareinboim, Sanghack Lee, Vasant Honavar, and Judea Pearl. Transportability from multiple environments with limited experiments. Advances in Neural Information Processing Systems, 26, 2013.   \n[3] Elias Bareinboim and Judea Pearl. Transportability from multiple environments with limited experiments: Completeness results. Advances in neural information processing systems, 27, 2014.   \n[4] Elias Bareinboim and Judea Pearl. Causal inference and the data-fusion problem. Proceedings of the National Academy of Sciences, 113(27):7345\u20137352, 2016.   \n[5] Alexis Bellot, Alan Malek, and Silvia Chiappa. Transportability for bandits with data from different environments. Advances in Neural Information Processing Systems, 36, 2024.   \n[6] Andrew Bennett and Nathan Kallus. Efficient policy learning from surrogate-loss classification reductions. In International conference on machine learning, pages 788\u2013798. PMLR, 2020.   \n[7] Andrew C Berry. The accuracy of the gaussian approximation to the sum of independent variates. Transactions of the american mathematical society, 49(1):122\u2013136, 1941.   \n[8] Rohit Bhattacharya, Razieh Nabi, and Ilya Shpitser. Semiparametric inference for causal effects in graphical models with hidden variables. The Journal of Machine Learning Research, 23(1):13325\u201313400, 2022.   \n[9] Peter J Bickel, Chris AJ Klaassen, Peter J Bickel, Ya\u2019acov Ritov, J Klaassen, Jon A Wellner, and YA\u2019Acov Ritov. Efficient and adaptive estimation for semiparametric models, volume 4. Springer, 1993.   \n[10] Donald T Campbell. Factors relevant to the validity of experiments in social settings. Sociological methods, pages 243\u2013263, 2017.   \n[11] Donald T Campbell and Julian C Stanley. Experimental and quasi-experimental designs for research. Ravenio books, 2015.   \n[12] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785\u2013794, 2016.   \n[13] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters, 2018.   \n[14] Thomas D Cook and Donald T Campbell. Causal inference and the language of experimentation. Quasi-experimentation: Design & analysis issues for field settings, pages 1\u201336, 1979.   \n[15] Juan Correa and Elias Bareinboim. General transportability of soft interventions: Completeness results. Advances in Neural Information Processing Systems, 33:10902\u201310912, 2020.   \n[16] Juan D Correa and Elias Bareinboim. From statistical transportability to estimating the effect of stochastic interventions. In IJCAI, pages 1661\u20131667, 2019.   \n[17] Iv\u00e1n D\u00edaz, Nicholas Williams, Katherine L Hoffman, and Edward J Schenck. Nonparametric causal effects based on longitudinal modified treatment policies. Journal of the American Statistical Association, 118(542):846\u2013857, 2023.   \n[18] Miroslav Dud\u00edk, Dumitru Erhan, John Langford, and Lihong Li. Doubly robust policy evaluation and optimization. 2014.   \n[19] Carl-Gustav Esseen. On the liapunov limit error in the theory of probability. Ark. Mat. Astr. Fys., 28:1\u201319, 1942.   \n[20] Stuart S Glennan. Mechanisms and the nature of causation. Erkenntnis, 44(1):49\u201371, 1996.   \n[21] Scott M Hammer, David A Katzenstein, Michael D Hughes, Holly Gundacker, Robert T Schooley, Richard H Haubrich, W Keith Henry, Michael M Lederman, John P Phair, Manette Niu, et al. A trial comparing nucleoside monotherapy with combination therapy in hiv-infected adults with cd4 cell counts from 200 to 500 per cubic millimeter. New England Journal of Medicine, 335(15):1081\u20131090, 1996.   \n[22] Tobias Hatt, Daniel Tschernutter, and Stefan Feuerriegel. Generalizing off-policy learning under sample selection bias. In Uncertainty in Artificial Intelligence, pages 769\u2013779. PMLR, 2022.   \n[23] Li He, Long Xia, Wei Zeng, Zhi-Ming Ma, Yihong Zhao, and Dawei Yin. Off-policy learning for multiple loggers. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1184\u20131193, 2019.   \n[24] Yonghan Jung, Iv\u00e1n D\u00edaz, Jin Tian, and Elias Bareinboim. Estimating causal effects identifiable from a combination of observations and experiments. Advances in Neural Information Processing Systems, 36, 2024.   \n[25] Yonghan Jung, Jin Tian, and Elias Bareinboim. Estimating identifiable causal effects on markov equivalence class through double machine learning. In International Conference on Machine Learning, pages 5168\u20135179. PMLR, 2021.   \n[26] Yonghan Jung, Jin Tian, and Elias Bareinboim. Estimating joint treatment effects by combining multiple experiments. In International Conference on Machine Learning, pages 15451\u201315527. PMLR, 2023.   \n[27] Edward H Kennedy. Towards optimal doubly robust estimation of heterogeneous causal effects. Electronic Journal of Statistics, 17(2):3008\u20133049, 2023.   \n[28] Edward H Kennedy, Sivaraman Balakrishnan, Max G\u2019Sell, et al. Sharp instruments for classifying compliers and generalizing causal effects. Annals of Statistics, 48(4):2008\u20132030, 2020.   \n[29] Sanghack Lee, Juan Correa, and Elias Bareinboim. General transportability\u2013synthesizing observations and experiments from heterogeneous domains. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 10210\u201310217, 2020.   \n[30] Sanghack Lee, Juan D Correa, and Elias Bareinboim. Generalized transportability: Synthesis of experiments from heterogeneous domains. In Proceedings of the 34th AAAI Conference on Artificial Intelligence, 2020.   \n[31] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review. and Perspectives on Open Problems, 5, 2020.   \n[32] Peter Machamer, Lindley Darden, and Carl F Craver. Thinking about mechanisms. Philosophy of science, 67(1):1\u201325, 2000.   \n[33] Susan A Murphy. An experimental design for the development of adaptive treatment strategies. Statistics in medicine, 24(10):1455\u20131481, 2005.   \n[34] Sonali Parbhoo, Shalmali Joshi, and Finale Doshi-Velez. Generalizing off-policy evaluation from a causal perspective for sequential decision-making. arXiv preprint arXiv:2201.08262, 2022.   \n[35] Judea Pearl. Causality. Cambridge university press, 2009.   \n[36] Judea Pearl and Elias Bareinboim. Transportability of causal and statistical relations: A formal approach. In Twenty-fifth AAAI conference on artificial intelligence, 2011.   \n[37] Andrea Rotnitzky, James Robins, and Lucia Babino. On the multiply robust estimation of the mean of the $\\mathrm{g}$ -functional. arXiv preprint arXiv:1705.08582, 2017.   \n[38] IG Shevtsova. On the absolute constants in the berry-esseen-type inequalities. In Doklady Mathematics, volume 89, pages 378\u2013381. Springer, 2014.   \n[39] Chengchun Shi, Jin Zhu, Ye Shen, Shikai Luo, Hongtu Zhu, and Rui Song. Off-policy confidence interval estimation with confounded markov decision process. Journal of the American Statistical Association, 119(545):273\u2013284, 2024.   \n[40] James H Stock and Mark W Watson. Introduction to econometrics. Pearson, 2020.   \n[41] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[42] Adith Swaminathan and Thorsten Joachims. The self-normalized estimator for counterfactual learning. advances in neural information processing systems, 28, 2015.   \n[43] Guy Tennenholtz, Uri Shalit, and Shie Mannor. Off-policy evaluation in partially observable environments. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 10276\u201310283, 2020.   \n[44] Masatoshi Uehara, Masahiro Kato, and Shota Yasui. Off-policy evaluation and learning for external validity under a covariate shift. Advances in Neural Information Processing Systems, 33:49\u201361, 2020.   \n[45] Mark J van der Laan and Susan Gruber. Targeted minimum loss based estimation of causal effects of multiple time point interventions. The international journal of biostatistics, 8(1), 2012.   \n[46] Junzhe Zhang and Elias Bareinboim. Transfer learning in multi-armed bandit: a causal approach. In Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems, pages 1778\u20131780, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Supplement to \u201cEfficient Policy Evaluation Across Multiple Different Experimental Datasets\u201d ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Introduction 1   \n1.1 Preliminaries 2   \nPolicy Evaluation Integrating Multiple Experimental Datasets 3   \n3 Combining experiments from two domains 4   \n3.1 Identification 4   \n3.2 Estimation . 5   \nCombining multiple experiments 7   \n4.1 Identification 7   \n4.2 Estimation 8   \n5 Experiments 9   \n6 Conclusion 10   \nA Related Work 15   \nB Broader Impact Statement 15   \nC Proofs 16   \nC.1 Proof for Theorem 1 and Theorem 4 16   \nC.2 Proof for Theorem 2 and Theorem 5 16   \nC.2.1 Proof of Mixed Bias Property 16   \nC.2.2 Proof for Statement 1 17   \nC.2.3 Proof for Statement 2 18   \nC.2.4 Proof for Statement 3 . 20   \nC.3 Proof for Theorem 3 and Theorem 6 21   \nD Details of Simulations 21   \nD.1 Data Generating Process for Synthetic Simulations 21   \nD.1.1 Synthetic Simulations for Fig. 3a 21   \nD.1.2 Synthetic Simulations for Fig. 3b 22   \nD.2 External validity of the ACTG 175 clinical trial 22   \nD.3 External validity of the Project STAR study 23 ", "page_idx": 13}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Evaluating the impact of a policy using observational and experimental data under different conditions is a widespread challenge in various important decision-making fields. Formulations of this problem have appeared in the causality literature, but also in statistics, reinforcement learning, and epidemiology. ", "page_idx": 14}, {"type": "text", "text": "Off-policy evaluation (OPE) aims to assess the performance of policies of interest using observational samples. In this line of research, [44] considers generalizing the effect of a policy under distribution shift. We build on this intuition, but instead seek to combine multiple (policy-)interventional data from source domains to learn the effect of policies of interest on the target domain. [1, 23] for instance used auxiliary datasets from multiple bandit instance, though they setting assume that the datasets are sampled from the same underlying populations and environments. Several authors have also considered transfer learning in off-policy learning in the context of bandits [46, 5]. Further, [22] addresses the problem of selection biases in observational data for off-policy learning. ", "page_idx": 14}, {"type": "text", "text": "In the causal inference literature, combining multiple experimental studies to estimate a new causal effect is a task called generalized identification [30]. Recent progress has been made in developing corresponding estimators [26, 24]. However, these estimators are not applicable when our goal is to combine multiple policy interventional studies from source domains to estimate a causal effect in the target domain. Accordingly, [4, 29, 15] developed the notion of generalized transportability that aims to evaluate a causal effect on a target domain from multiple observational and / or interventional distributions from other source domains. In this line of research, our work relates closely to Correa and Bareinboim\u2019s identification algorithm for the effect of policies [15]. We similarly develop identification criteria that are conducive to efficient estimation from finite samples. In particular, our work focuses on the derivation of sample-efficient estimators for the policy effect of interest on the target domain. ", "page_idx": 14}, {"type": "text", "text": "From this perspective, our work can be interpreted as a bridge between causal inference and off-policy evaluation [34] since we leverage formal theories in causal inference (e.g., generalized identification [29], generalized transportability [15, 30]) to solve off-policy evaluation problems efficiently from finite samples. There are prior works that similarly integrated both fields. For instance, standard policy evaluation methods in the RL literature use the backdoor adjustment to learn the Q value as a function of the state, to address confounding effects [41]. Meanwhile, other studies have applied the front-door adjustment formula for OPE in the presence of unmeasured confounders [39]. Finally, some works have leveraged double negative controls for OPE [43]. ", "page_idx": 14}, {"type": "text", "text": "B Broader Impact Statement ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our work investigates the conditions under which policies may be estimated from multipe datasets collected under different conditions. In this work, we start from the assumption that causal and selection diagrams that are consistent with the underlying data generating systems of interest are available. In general, this requires domain knowledge and should be justified by prior knowledge or experiment. It is important also to make the distinction between the task of partial identification, that is inferring an expression for bounds on causal effects, and that of estimation, that is providing efficient estimators from finite samples to compute bounds in practice. This set of results concerns mostly the second task. In higher-dimensional systems, the computational complexity of estimating the conditional expectations and density ratios that define our estimators could be a substantial challenge. Consequently, practitioners must exercise caution when deploying the proposed method in small sample scenarios where estimators may be inaccurate. Moreover, we have stated our convergence guarantees in the infinite sample limit, without quantifying the finite-sample estimation uncertainty. Finally, we emphasize that simulations on real and synthetic data are provided for illustration purposes only. These results do not recommend or advocate for the implementation of a particular policy, and should be considered in practice in combination with other aspects of the decision-making process. ", "page_idx": 14}, {"type": "text", "text": "C Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Proof for Theorem $\\mathbf{1}$ and Theorem 4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Since Theorem 1 is a special case for Theorem 4, we will only prove for Theorem 4. Note ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{P_{\\pi_{0}}^{0}}[Y]=\\sum_{\\mathbf{w},\\mathbf{c},\\mathbf{x}}\\mathbb{E}_{P_{\\pi_{0}}^{0}}[Y\\mid\\mathbf{c},\\mathbf{w},\\mathbf{x}]\\prod_{i=1}^{m-1}P_{\\pi_{0}}^{0}(w_{i}\\mid\\mathbf{c},\\mathbf{X}^{(i-1)},\\mathbf{w}^{(i-1)})\\prod_{j=1}^{m-1}\\pi_{0}(x_{j}\\mid\\mathbf{c},\\mathbf{w}^{(j-1)})P^{0}(\\mathbf{c}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}_{P_{\\pi_{0}}^{0}}[Y\\mid\\mathbf{c},\\mathbf{w},\\mathbf{x}]=\\mathbb{E}_{P_{\\pi_{2}}^{0}}[Y\\mid\\mathbf{c},\\mathbf{w},\\mathbf{x}]}\\\\ &{}&{=\\mathbb{E}_{P_{\\pi_{2}}^{2}}[Y\\mid\\mathbf{c},\\mathbf{w},\\mathbf{x}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "by leveraging the domain transfer for $Y$ and adjustment for $Y$ condition. ", "page_idx": 15}, {"type": "text", "text": "For each $P_{\\pi_{0}}^{0}(w_{i}\\mid\\mathbf{c},\\mathbf{X}^{(i-1)},\\mathbf{w}^{(i-1)})$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{\\pi_{0}}^{0}(w_{i}\\mid\\mathbf{c},\\mathbf{X}^{(i-1)},\\mathbf{w}^{(i-1)})}\\\\ &{=P_{\\pi_{0}}^{i}(w_{i}\\mid\\mathbf{c},\\mathbf{X}^{(i-1)},\\mathbf{w}^{(i-1)})}\\\\ &{=P_{\\pi_{i}}^{i}(w_{i}\\mid\\mathbf{c},\\mathbf{X}^{(i-1)},\\mathbf{w}^{(i-1)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "again, by leveraging the domain transfer condition for $W_{i}$ and adjustment condition for $W_{i}$ . This completes the proof. ", "page_idx": 15}, {"type": "text", "text": "C.2 Proof for Theorem 2 and Theorem 5 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Since Theorem 2 is a special case for Theorem 5, we will only prove for Theorem 5. Throughout the proof, we will use $\\mathbf{C}_{1}\\;:=\\;\\mathbf{C}$ , $\\mathbf{X}_{i}~:=~\\{X_{i}\\}$ for $i\\;=\\;1,\\cdots\\;,m$ , and ${\\bf C}_{i}\\;:=\\;\\{W_{i-1}\\}$ for $i\\,=$ $2,\\cdot\\cdot\\cdot,m-1$ . Also, we will sometimes use $P^{1}(\\mathbf{C}_{1}):=P^{0}(\\mathbf{C})$ , $P^{i}(\\mathbf{C}_{i}\\mid\\mathbf{C}^{(i-1)}\\cup\\mathbf{X}^{(i-1)})$ for $i>1$ as P \u03c0ii\u00b4\u00b411pWi\u00b41 | C, Wpi\u00b42q, Xpi\u00b41qq. ", "page_idx": 15}, {"type": "text", "text": "C.2.1 Proof of Mixed Bias Property ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Using the fact that $\\psi_{0}=\\mathbb{E}_{P^{0}}[\\check{\\mu}_{0}^{1}]$ , we can write it as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\psi_{0}:=\\sum_{i=1}^{m}\\underbrace{\\mathbb{E}_{P_{\\pi_{i}}^{i}}[\\phi_{0}^{i}]}_{=0}+\\mathbb{E}_{P^{0}}[\\phi_{0}^{0}]=\\sum_{i=0}^{m}\\mathbb{E}_{P_{\\pi_{i}}^{i}}[\\phi_{0}^{i}].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, we will claim and prove the following: ", "page_idx": 15}, {"type": "text", "text": "Lemma 1 (Mixed Bias Property). Suppose $\\mu_{0}^{i},\\hat{\\mu}^{i}\\,<\\,\\infty$ and $0\\,<\\,\\pi_{0}^{i},{\\hat{\\pi}}^{i}\\,<\\,\\infty$ almost surely for $i=1,\\cdot\\cdot\\cdot,m$ . For $i=1,2,\\cdots,m$ , define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\phi^{i}((\\mathbf{C},\\mathbf{W}^{(i)},\\mathbf{X}^{(i)});\\omega^{i},\\breve{\\mu}^{i+1},\\mu^{i}):=\\omega^{i}\\{\\breve{\\mu}^{i+1}-\\mu^{i}\\},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and $\\check{\\mu}^{m+1}:=Y$ . Define $\\phi^{0}(\\mathbf{C};\\check{\\mu}^{1}):=\\check{\\mu}^{1}$ . For $i=0,\\cdots,m_{},$ , define $\\phi_{0}^{i}$ as $\\phi^{i}$ equipped with true nuisances, and $\\hat{\\phi}^{i}$ as $\\phi^{i}$ equipped with estimated nuisances. Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{m}\\mathbb{E}_{P_{\\pi_{i}}^{i}}[\\hat{\\phi}^{i}-\\phi_{0}^{i}]=\\sum_{i=1}^{m}\\mathbb{E}_{P_{\\pi_{i}}^{i}}\\big[\\{\\hat{\\mu}^{i}-\\mu_{0}^{i}\\}\\{\\omega_{0}^{i}-\\hat{\\omega}^{i}\\}\\big].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma $^{\\,I}$ . For $i=m,\\cdots,1$ with $\\check{\\mu}^{m+1}:=Y$ , define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mu_{0}^{i}[\\check{\\mu}^{i+1}]:=\\mathbb{E}_{P_{\\pi_{i}}^{i}}[\\check{\\mu}^{i+1}\\mid\\mathbf{C},\\mathbf{W}^{(i)},\\mathbf{X}^{(i)}].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P_{\\pi_{m}}^{m}}\\big[\\hat{\\omega}^{m}\\{\\check{\\mu}^{m+1}-\\hat{\\mu}^{m}\\}\\big]+\\mathbb{E}_{P_{\\pi_{m}}^{m}}\\big[\\omega_{0}^{m}\\hat{\\mu}^{m}\\big]-\\underbrace{\\mathbb{E}_{P_{\\pi_{m}}^{m}}\\big[\\omega_{0}^{m}\\mu_{0}^{m}\\big[\\check{\\mu}^{i+1}\\big]\\big]}_{=\\psi_{0}}}\\\\ &{\\ =\\mathbb{E}_{P_{\\pi_{m}}^{m}}\\big[\\{\\hat{\\omega}^{m}-\\omega_{0}^{m}\\}\\{\\mu_{0}^{m}\\big[\\check{\\mu}^{i+1}\\big]-\\hat{\\mu}^{m}\\}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For $i=m-1,\\cdots\\,,1$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P_{\\pi_{i}}^{i}}[\\hat{\\omega}^{i}\\{\\tilde{\\mu}^{i+1}-\\hat{\\mu}^{i}\\}]+\\mathbb{E}_{P_{\\pi_{i}}^{i}}[\\omega_{0}^{i}\\hat{\\mu}^{i}]-\\mathbb{E}_{P_{\\pi_{i}}^{i}}[\\omega_{0}^{i}\\mu_{0}^{i}[\\tilde{\\mu}^{i+1}]]}\\\\ &{\\ =\\mathbb{E}_{P_{\\pi_{i}}^{i}}[\\{\\hat{\\omega}^{i}-\\omega_{0}^{i}\\}\\{\\mu_{0}^{i}[\\tilde{\\mu}^{i+1}]-\\hat{\\mu}^{i}\\}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Also, the following holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P_{\\pi_{i+1}}^{i+1}}\\left[\\omega_{0}^{i+1}\\hat{\\mu}^{i+1}\\right]=\\mathbb{E}_{P_{\\pi_{i}}^{i}}\\big[\\omega_{0}^{i}\\mu_{0}^{i}\\big[\\check{\\mu}^{i+1}\\big]\\big].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, $\\mathbb{E}_{P_{\\pi_{1}}^{1}}\\left[\\omega_{0}^{1}\\hat{\\mu}^{1}\\right]=\\mathbb{E}_{P_{\\pi_{1}}^{1}}\\left[\\check{\\mu}^{1}\\right]$ . ", "page_idx": 16}, {"type": "text", "text": "Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{i=1}^{m}\\mathbb{E}_{P_{\\pi_{i}}^{i}}[\\hat{\\omega}^{i}\\{\\check{\\mu}^{i+1}-\\hat{\\mu}^{i}\\}]+\\mathbb{E}_{P_{\\pi_{i}}^{i}}[\\omega_{0}^{i}\\hat{\\mu}^{i}]-\\mathbb{E}_{P_{\\pi_{i}}^{i}}[\\omega_{0}^{i}\\mu_{0}^{i}[\\check{\\mu}^{i+1}]]}}\\\\ &{=\\displaystyle\\sum_{i=0}^{m}\\mathbb{E}_{P_{\\pi_{i}}^{i}}[\\hat{\\phi}^{i}-\\phi_{0}^{i}]}\\\\ &{=\\displaystyle\\sum_{i=1}^{m}\\mathbb{E}_{P_{\\pi_{i}}^{i}}[\\{\\hat{\\omega}^{i}-\\omega_{0}^{i}\\}\\{\\mu_{0}^{i}[\\check{\\mu}^{i+1}]-\\hat{\\mu}^{i}\\}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C.2.2 Proof for Statement 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Recall ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{\\psi}:=\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=1}^{m}\\mathbb{E}_{\\mathcal{D}_{\\ell}^{i}}[\\hat{\\omega}_{\\ell}^{i}\\{\\hat{\\tilde{\\mu}}^{i+1}-\\hat{\\mu}_{\\ell}^{i}\\}]+\\mathbb{E}_{\\mathcal{D}_{\\ell}^{0}}[\\check{\\hat{\\mu}}_{\\ell}^{1}]=\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=0}^{m}\\mathbb{E}_{\\mathcal{D}_{\\ell}^{i}}[\\hat{\\phi}_{\\ell}^{i}].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From Eq. (7), ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\psi_{0}:=\\frac1L\\sum_{\\ell=1}^{L}\\sum_{i=0}^{m}\\mathbb{E}_{P_{\\pi_{i}}^{i}}[\\phi_{0}^{i}].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, the error ${\\hat{\\psi}}-\\psi_{0}$ can be decomposed into ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{\\psi}-\\psi_{0}=\\sum_{i=0}^{m}\\mathbb{E}_{\\mathcal{D}^{i}-P_{\\pi_{i}}^{i}}[\\phi_{0}^{i}]+\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=0}^{m}\\mathbb{E}_{\\mathcal{D}^{i}-P_{\\pi_{i}}^{i}}[\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}]+\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=0}^{m}\\mathbb{E}_{P_{\\pi_{i}}^{i}}[\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Define ", "page_idx": 16}, {"type": "equation", "text": "$$\nR_{i}:=\\mathbb{E}_{\\mathcal{D}^{i}-P_{\\pi_{i}}^{i}}\\big[\\phi_{0}^{i}\\big]+\\frac{1}{L}\\sum_{\\ell=1}^{L}\\mathbb{E}_{\\mathcal{D}^{i}-P_{\\pi_{i}}^{i}}\\big[\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}\\big].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, the error can be represented as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{\\psi}-\\psi_{0}=\\sum_{i=0}^{m}R_{i}+\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=0}^{m}\\mathbb{E}_{P_{\\pi_{i}}^{i}}\\big[\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}\\big].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Lemma 1, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\psi}-\\psi_{0}=\\sum_{i=0}^{m}R_{i}+\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=0}^{m}\\mathbb{E}_{P_{\\pi_{i}}^{i}}\\big[\\{\\hat{\\omega}_{\\ell}^{i}-\\omega_{0}^{i}\\}\\{\\mu_{0}^{i}-\\hat{\\mu}_{\\ell}^{i}\\}\\big].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C.2.3 Proof for Statement 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We will use the following results: ", "page_idx": 17}, {"type": "text", "text": "Lemma 2 (Combining concentration inequalities). Suppose $P(A_{k}>t)\\leqslant b_{k}/t^{2}$ for $k=1,\\cdots\\,,K$ . Then, ", "page_idx": 17}, {"type": "equation", "text": "$$\nP\\left(\\sum_{k=1}^{K}A_{k}\\leqslant t K\\right)\\geqslant1-{\\frac{1}{t^{2}}}\\sum_{k=1}^{K}b_{k}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 2. The event $\\textstyle\\sum_{k=1}^{K}A_{k}\\leqslant t K$ includes the case where $A_{k}<t$ for $k=1,\\cdots\\,,K$ . Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle P\\left(\\sum_{k=1}^{K}A_{k}\\leqslant t K\\right)\\geqslant P\\left(A_{1}\\leqslant t\\mathrm{~and~}\\dots\\mathrm{~and~}A_{K}\\leqslant t\\right)}&\\\\ {\\displaystyle=1-P\\left(A_{1}>t\\mathrm{~or~}\\dots\\mathrm{~or~}A_{K}>t\\right)}&\\\\ {\\displaystyle\\geqslant1-\\sum_{k=1}^{K}P\\left(A_{k}>t\\right)}&\\\\ {\\displaystyle\\geqslant1-\\sum_{k=1}^{K}\\frac{b_{k}}{t^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 3 (Stochastic Equicontinuity). Let $\\mathcal{D}_{\\sim}^{\\mathit{i i d}}\\mathcal{P}$ . Let $\\mathcal{D}=\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}$ , where $n:=|\\mathcal{D}_{0}|$ . Let $\\hat{f}$ be a function estimated from $\\mathcal{D}_{1}$ . Then, in probability greater than $1-\\epsilon$ for any $\\epsilon\\in(0,1)$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{D}_{0}-P}\\left[\\left|\\hat{f}-f\\right|\\right]\\overset{w.p\\;1-\\epsilon}{<}\\frac{\\|\\hat{f}-f\\|_{P}}{\\sqrt{n\\epsilon}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which implies that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{D}_{0}-P}[|\\hat{f}-f|]=O_{P}\\left(\\frac{\\Vert\\hat{f}-f\\Vert_{P}}{\\sqrt{n}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 3. This proof is from [28, Lemma 2]. Since $\\hat{f}$ is a function of $\\mathcal{D}_{1}$ , we will denote $\\hat{f}_{D_{1}}$ . Define a following random variable of interest: ", "page_idx": 17}, {"type": "equation", "text": "$$\nX:=\\mathbb{E}_{\\mathcal{D}_{0}-P}[\\hat{f}_{\\mathcal{D}_{1}}-f].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, the conditional expectation of $X$ given $\\mathcal{D}_{1}$ is zero, since ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\hat{f}_{\\mathcal{D}_{1}}(\\mathbf{V}_{i})~\\Bigg|~\\mathcal{D}_{1}\\right]=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{P}[\\hat{f}_{\\mathcal{D}_{1}}(\\mathbf{V}_{i})~|~\\mathcal{D}_{1}]=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{P}[\\hat{f}_{\\mathcal{D}_{1}}(\\mathbf{V})~|~\\mathcal{D}_{1}]=\\mathbb{E}_{P}[\\hat{f}_{\\mathcal{D}_{1}}(\\mathbf{V})~|~\\mathcal{D}_{1}]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the third equality holds by the independence of $\\mathcal{D}_{0}$ and $\\mathcal{D}_{1}$ . Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbb{E}}_{P}[X\\mid{\\mathcal{D}}_{1}]={\\mathbb{E}}_{P}[{\\mathbb{E}}_{\\mathcal{D}_{0}-P}[\\hat{f}_{\\mathcal{D}_{1}}-f]\\mid{\\mathcal{D}}_{1}]}\\\\ &{\\qquad\\qquad\\quad={\\mathbb{E}}_{P}[{\\mathbb{E}}_{\\mathcal{D}_{0}}[\\hat{f}_{\\mathcal{D}_{1}}-f]\\mid{\\mathcal{D}}_{1}]-{\\mathbb{E}}_{P}[{\\mathbb{E}}_{P}[\\hat{f}_{\\mathcal{D}_{1}}-f]\\mid{\\mathcal{D}}_{1}]}\\\\ &{\\qquad\\qquad\\quad={\\mathbb{E}}_{P}[{\\mathbb{E}}_{P}[\\hat{f}_{\\mathcal{D}_{1}}-f]\\mid{\\mathcal{D}}_{1}]-{\\mathbb{E}}_{P}[{\\mathbb{E}}_{P}[\\hat{f}_{\\mathcal{D}_{1}}-f]\\mid{\\mathcal{D}}_{1}]=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Also, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}_{P}[X\\mid\\mathcal{D}_{1}]=\\mathbb{V}_{P}[\\mathbb{E}_{\\mathcal{D}_{0}-P}[\\hat{f}_{\\mathcal{D}_{1}}-f]\\mid\\mathcal{D}_{1}]}\\\\ &{\\phantom{\\mathbb{V}_{P}[X\\mid\\mathcal{D}_{1}]}=\\mathbb{V}_{P}[\\mathbb{E}_{\\mathcal{D}_{0}}[\\hat{f}_{\\mathcal{D}_{1}}-f]\\mid\\mathcal{D}_{1}]}\\\\ &{\\phantom{\\mathbb{V}_{P}[X\\mid\\mathcal{D}_{1}]}=\\frac{1}{n}\\mathbb{V}_{P}[\\hat{f}_{\\mathcal{D}_{1}}-f\\mid\\mathcal{D}_{1}]}\\\\ &{\\phantom{\\mathbb{V}_{P}[X\\mid\\mathcal{D}_{1}]}\\leqslant\\frac{1}{n}\\lVert\\hat{f}_{\\mathcal{D}_{1}}-f\\rVert_{P}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By applying the (conditional-) Chevyshev\u2019s inequality, ", "page_idx": 18}, {"type": "equation", "text": "$$\nP(|X-\\mathbb{E}_{P}[X\\mid\\mathcal{D}_{1}]|\\geqslant t\\mid\\mathcal{D}_{1})\\leqslant\\frac{1}{t^{2}}\\mathbb{V}_{P}[X\\mid\\mathcal{D}_{1}]\\leqslant\\frac{1}{n t^{2}}\\|\\hat{f}_{\\mathcal{D}_{1}}-f\\|_{P}^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(|X|\\geqslant t)=P(|X-\\mathbb{E}_{P}[X\\mid\\mathcal{D}_{1}]|\\geqslant t)}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}_{P(\\mathcal{D}_{1})}[P(|X-\\mathbb{E}_{P}[X\\mid\\mathcal{D}_{1}]|\\geqslant t\\mid\\mathcal{D}_{1})]}\\\\ &{\\qquad\\qquad\\leqslant\\frac{1}{n t^{2}}\\|\\hat{f}_{\\mathcal{D}_{1}}-f\\|_{P}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In other words, X \u0103 t in probability greater than 1 \u00b4n1t2 } f\u02c6D1 \u00b4 f}2P . If t \u201c } f\u02c6D?1n\u00b4\u03f5f}P, then $\\begin{array}{r}{X<\\frac{\\|\\hat{f}_{\\mathcal{D}_{1}}-f\\|_{P}}{\\sqrt{n\\epsilon}}}\\end{array}$ in the probability greater than $1-\\epsilon$ for any $\\epsilon\\in(0,1)$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Here, we will study the finite sample behavior of ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{m}R_{i}:=\\sum_{i=0}^{m}\\mathbb{E}_{\\mathcal{D}^{i}-P_{\\pi_{i}}^{i}}\\big[\\phi_{0}^{i}\\big]+\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=0}^{m}\\mathbb{E}_{\\mathcal{D}_{\\ell}^{i}-P_{\\pi_{i}}^{i}}\\big[\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}\\big].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Chevyshev\u2019s inequality, ", "page_idx": 18}, {"type": "equation", "text": "$$\nP r\\left(\\left|\\mathbb{E}_{\\mathcal{D}^{i}-P_{\\pi_{i}}^{i}}[\\phi_{0}^{i}]\\right|>t\\frac{\\rho_{i,0}}{\\sqrt{|\\mathcal{D}^{i}|}}\\right)<\\frac{1}{t^{2}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "or equivalently, ", "page_idx": 18}, {"type": "equation", "text": "$$\nP r\\left(\\left|\\mathbb{E}_{\\mathcal{D}^{i}-P_{\\pi_{i}}^{i}}[\\phi_{0}^{i}]\\right|>t\\right)<\\frac{1}{t^{2}}\\frac{\\rho_{i,0}^{2}}{|\\mathcal{D}^{i}|}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Lemma 2, ", "page_idx": 18}, {"type": "equation", "text": "$$\nP r\\left(\\sum_{i=0}^{m}\\left|\\mathbb{E}_{\\mathcal{D}^{i}-P_{\\pi_{i}}^{i}}[\\phi_{0}^{i}]\\right|\\leqslant(m+1)t_{1}\\right)\\geqslant1-\\frac{1}{t_{1}^{2}}\\sum_{i=0}^{m}\\frac{\\rho_{i,0}^{2}}{|\\mathcal{D}^{i}|}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Lemma 3, ", "page_idx": 18}, {"type": "equation", "text": "$$\nP r\\left(\\left|\\mathbb{E}_{\\mathcal{D}_{\\ell}^{i}-P_{\\pi_{i}}^{i}}[\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}]\\right|>t_{2}\\right)\\leqslant\\frac{1}{t_{2}^{2}}\\frac{\\|\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}\\|_{P_{\\pi_{i}}^{i}}^{2}}{|\\mathcal{D}_{\\ell}^{i}|}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Lemma 2, ", "page_idx": 18}, {"type": "equation", "text": "$$\nP\\left(\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=0}^{m}\\left|\\mathbb{E}_{\\mathcal{D}_{\\ell}^{i}-P_{\\pi_{i}}^{i}}[\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}]\\right|\\leqslant(m+1)t_{2}\\right)\\geqslant1-\\frac{1}{t_{2}^{2}}\\sum_{\\ell=1}^{L}\\sum_{i=0}^{m}\\frac{\\|\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}\\|_{P_{\\pi_{i}}^{i}}^{2}}{|\\mathcal{D}_{\\ell}^{i}|}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Choose $\\begin{array}{r}{t_{1}:=\\sqrt{\\frac{2}{\\epsilon}\\sum_{i=0}^{m}{\\frac{\\rho_{i,0}^{2}}{|\\mathcal{D}^{i}|}}}}\\end{array}$ and $\\begin{array}{r}{t_{2}:=\\sqrt{\\frac{2}{\\epsilon}\\sum_{\\ell=1}^{L}\\sum_{i=0}^{m}\\frac{\\lVert\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}\\rVert_{P_{\\pi_{i}}^{i}}^{2}}{\\lvert\\mathcal{D}_{\\ell}^{i}\\rvert}}}\\end{array}$ . Then, with a probability greater than $1-\\epsilon$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\sum_{i=0}^{m}{R_{i}\\leqslant(m+1)\\left(\\sqrt{\\frac{2}{\\epsilon}\\sum_{i=0}^{m}\\frac{\\rho_{i,0}^{2}}{|{\\mathcal D}^{i}|}}+\\sqrt{\\frac{2}{\\epsilon}\\sum_{\\ell=1}^{L}\\sum_{i=1}^{K}\\frac{\\|\\hat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}\\|_{\\mathrm{P}^{k}}^{2}}{|{\\mathcal D}_{\\ell}^{k}|}}\\right)}}\\\\ {\\displaystyle=(m+1)\\sqrt{\\frac{2}{\\epsilon}}\\left(\\sqrt{\\sum_{i=0}^{m}\\frac{\\rho_{i,0}^{2}}{|{\\mathcal D}^{i}|}}+\\sqrt{\\sum_{\\ell=1}^{L}\\sum_{i=0}^{m}\\frac{\\|\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}\\|_{P_{\\pi_{i}}^{i}}^{2}}{|{\\mathcal D}_{\\ell}^{i}|}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C.2.4 Proof for Statement 3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We will use the following result: ", "page_idx": 19}, {"type": "text", "text": "Proposition 1 (Berry\u2013Esseen\u2019s inequality [7, 19, 38]). Suppose $\\cal{D}~=~\\{X_{1},\\cdots,X_{n}\\}$ are independent and identically distributed random variables with $\\mathbb{E}_{P}[X_{i}]\\;=\\;0$ , $\\mathbb{E}_{P}[X_{i}^{2}]\\ =\\ \\sigma^{2}$ and $\\mathbb{E}_{P}[\\left|X_{i}\\right|^{3}]=\\kappa^{3}$ . Then, for all $x$ and $n$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|P\\left({\\frac{\\sqrt{n}}{\\sigma_{0}}}\\mathbb{E}_{\\mathcal{D}}[X]<x\\right)-\\Phi(x)\\right|\\leqslant{\\frac{0.4748\\kappa^{3}}{\\sigma^{3}{\\sqrt{n}}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By Lemma 3, ", "page_idx": 19}, {"type": "equation", "text": "$$\nP r\\left(\\left|\\mathbb{E}_{\\mathcal{D}_{\\ell}^{i}-P_{\\pi_{i}}^{i}}[\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}]\\right|>t\\right)\\leqslant\\frac{1}{t^{2}}\\frac{\\|\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}\\|_{P_{\\pi_{i}}^{i}}^{2}}{|\\mathcal{D}_{\\ell}^{i}|}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By Lemma 2, ", "page_idx": 19}, {"type": "equation", "text": "$$\nP r\\left(\\frac{1}{L}\\sum_{\\ell=1}^{L}\\left|\\mathbb{E}_{\\mathcal{D}_{\\ell}^{i}-P_{\\pi_{i}}^{i}}[\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}]\\right|\\leqslant t\\right)\\geqslant1-\\frac{1}{t^{2}}\\sum_{\\ell=1}^{L}\\frac{\\|\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}\\|_{P_{\\pi_{i}}^{i}}^{2}}{|\\mathcal{D}_{\\ell}^{i}|}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{i}:=\\sqrt{\\frac{1}{\\epsilon}\\sum_{\\ell=1}^{L}\\frac{\\Vert\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}\\Vert_{P_{\\pi_{i}}^{i}}^{2}}{\\vert{\\mathcal{D}}_{\\ell}^{i}\\vert}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "With a probability greater than $1-\\epsilon$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{L}\\sum_{\\ell=1}^{L}\\left|\\mathbb{E}_{\\mathcal{D}_{\\ell}^{i}-P_{\\pi_{i}}^{i}}[\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}]\\right|^{\\mathrm{\\scriptsize~w.p\\perp-\\epsilon}}\\Delta_{i}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle A_{i}:=\\mathbb{E}_{\\mathcal{D}^{i}-P_{\\pi_{i}}^{i}}[\\phi_{0}^{i}]}}\\\\ {{\\displaystyle B_{i}:=\\frac{1}{L}\\sum_{\\ell=1}^{L}\\mathbb{E}_{\\mathcal{D}_{\\ell}^{i}-P_{\\pi_{i}}^{i}}[\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}]}}\\\\ {{\\displaystyle C_{i}:=\\frac{1}{L}\\sum_{\\ell=1}^{L}\\left|\\mathbb{E}_{\\mathcal{D}_{\\ell}^{i}-P_{\\pi_{i}}^{i}}[\\hat{\\phi}_{\\ell}^{i}-\\phi_{0}^{i}]\\right|.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, $R_{i}=A_{i}+B_{i}$ . Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P r\\left(R_{i}<x\\right)}\\\\ &{\\begin{array}{r l}&{=P r\\left(A_{i}+B_{i}<x\\right)}\\\\ &{=P r\\left(A_{i}<x-B_{i}\\right)}\\\\ &{\\leqslant P r\\left(A_{i}<x+C_{i}\\right)}\\end{array}}\\\\ &{\\begin{array}{r l}&{\\stackrel{{\\mathrm{wp~1-~}\\epsilon}}{\\leqslant}P r\\left(A_{i}<x+\\Delta_{i}\\right).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P r\\left(A_{i}<x+\\Delta_{i}\\right)-\\Phi(x)|}\\\\ &{=|P r\\left(A_{i}<x+\\Delta_{i}\\right)-\\Phi(x+\\Delta_{i})+\\Phi(x+\\Delta_{i})-\\Phi(x)|}\\\\ &{\\leqslant|P r\\left(A_{i}<x+\\Delta_{i}\\right)-\\Phi(x+\\Delta_{i})|+|\\Phi(x+\\Delta_{i})-\\Phi(x)|}\\\\ &{\\leqslant\\frac{0.4748\\kappa_{0}^{3}}{\\rho_{i,0}^{3}\\sqrt{|\\mathcal{D}^{i}|}}+|\\Phi(x+\\Delta_{i})-\\Phi(x)|}\\\\ &{=\\frac{0.4748\\kappa_{0}^{3}}{\\rho_{i,0}^{3}\\sqrt{|\\mathcal{D}^{i}|}}+|\\Phi^{\\prime}(x^{\\prime})\\Delta_{i}|}\\\\ &{\\leqslant\\frac{0.4748\\kappa_{0}^{3}}{\\rho_{i,0}^{3}\\sqrt{|\\mathcal{D}^{i}|}}+\\frac{1}{\\sqrt{2\\pi}}\\Delta_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "(Mean-value theorem) ", "page_idx": 20}, {"type": "text", "text": "This completes the proof. ", "page_idx": 20}, {"type": "text", "text": "C.3 Proof for Theorem 3 and Theorem 6 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "By Cauchy-Schwartz\u2019 inequality, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=0}^{m}\\mathbb{E}_{P_{\\pi_{i}}^{i}}\\big[\\{\\mu_{0}^{i}-\\hat{\\mu}_{\\ell}^{i}\\}\\{\\hat{\\omega}_{\\ell}^{i}-\\omega_{0}^{i}\\}\\big]\\leqslant\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=0}^{m}O_{P_{\\pi_{i}}^{i}}\\left(\\|\\mu_{0}^{i}-\\hat{\\mu}_{\\ell}^{i}\\|\\|\\omega_{0}^{i}-\\hat{\\omega}_{\\ell}^{i}\\|\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Given assumption, the upper bound in Eq. (18) converges at $1/\\sqrt{|\\mathcal{D}_{\\ell}^{i}|}$ rate. Therefore, $R_{i}$ converges in distribution to normal $(0,\\rho_{i,0}^{2})$ . ", "page_idx": 20}, {"type": "text", "text": "D Details of Simulations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "D.1 Data Generating Process for Synthetic Simulations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Codes corresponding to simulations are submitted as supplementary materials. ", "page_idx": 20}, {"type": "text", "text": "D.1.1 Synthetic Simulations for Fig. 3a ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We define the following SCM. First, $U_{X W},U_{X_{1},X_{2}},U_{X_{2},W},U_{X_{2},Y},U_{C_{1,1}},U_{C_{1,2}},U_{C_{2,1}},U_{C_{2,2}},U_{W},U_{Y}\\sim$ normal $(0,1)$ . Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\nC_{1}:=f_{C_{1}}(S)=0.25S U_{C_{1,1}}+0.1S+U_{C_{1,1}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\nC_{2}:=f_{C_{2}}(S)=0.25S U_{C_{2,1}}+0.1S+U_{C_{2,2}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\nX_{1}:=f_{X_{1}}(C_{1},C_{2},S)\\sim\\mathtt{B e r n o u l i}\\left(\\pi_{1,S}(C_{1},C_{2})\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "W $f_{W}(C_{1},C_{2},X_{1},U_{X_{1},W},S)={\\mathrm{s.i}}\\,{\\mathrm{gmo~id}}(0.25S U_{W}+0.5U_{X_{1},W}+3X_{1}+0.5(C_{1}+C_{2}))$ $X_{2}:=f_{X_{2}}(X_{1},W,C_{1},C_{2},S)\\sim\\mathtt{B e r n o u l}\\,\\mathtt{i}\\left(\\pi_{2,S}(C_{1},C_{2})\\right)$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Y:=f_{Y}(C_{1},C_{2},X_{1},X_{2},W,U_{X_{2},Y},S)=\\mathrm{si}\\,\\mathrm{gmoi}\\,\\mathrm{d}(0.5(C_{1}+C_{2})+2(X_{1}+X_{2})-2-0.5W\\,\\mathrm{since})}\\\\ {+\\,0.1U_{X_{2},Y}+0.25S U_{W}).\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Also, for $S\\ne0$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{1,0}=\\mathsf{s i g m o\\,i d}(C_{1}+C_{2}-2)}\\\\ &{\\pi_{1,S}=\\mathsf{s i g m o\\,i d}\\big(0.5(C_{1}+C_{2})-1\\big)}\\\\ &{\\pi_{2,0}=\\mathsf{s i g m o\\,i d}\\big(0.5(C_{1}+C_{2})+2(2X_{1}-1)-0.5W+1\\big)}\\\\ &{\\pi_{2,S}=\\mathsf{s i g m o\\,i d}\\big(C_{1}+C_{2}+2X_{1}-1+0.5W-1\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D.1.2 Synthetic Simulations for Fig. 3b ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We define the following SCM. First, $^{7}{\\cal X}_{1},{\\cal X}_{2},{\\cal U}_{X_{2},{\\cal X}_{3}},{\\cal U}_{W_{1},{\\cal X}_{1}},{\\cal U}_{W_{1},{\\cal X}_{2}},{\\cal U}_{W_{2},{\\cal X}_{2}},{\\cal U}_{W_{2},{\\cal X}}$ 3, UC1,C2,   \n$U_{C_{2},C_{3}},U_{X_{3},Y},U_{C_{1,1}},U_{C_{1,2}},U_{C_{2,1}},U_{C_{2,2}},U_{C_{3,1}},U_{C_{3,2}},U_{W_{1}},U_{W_{2}},U_{Y}\\sim\\mathrm{normal}(0,1).$ Then, C $\\mathbf{\\tau}_{1}:=f_{C_{1}}(S)=0.25S U_{C_{1,1}}+0.1S+U_{C_{1,2}}+U_{C_{1},C_{2}}$ C $_2:=f_{C_{2}}(S)=0.25S U_{C_{2,1}}+0.1S+U_{C_{2,2}}+U_{C_{1},C_{2}}$ \\` UC2,C3 $C_{3}:=f_{C_{3}}(S)=0.25S U_{C_{3,1}}+0.1S+U_{C_{3,2}}+U_{C_{2,0}}$ C3 $X_{1}:=f_{X_{1}}(C_{1},C_{2},S)\\sim\\mathtt{B e r n o u l i}\\left(\\pi_{1,S}(C_{1},C_{2})\\right)$ $V_{1}:=f_{W_{1}}(\\mathbf{C},X_{1},U_{W_{1},X_{1}},U_{W_{1},X_{2}},S)=\\mathrm{s}\\,\\mathrm{i}\\,\\mathrm{gmo}\\,\\mathrm{i}\\,\\mathrm{d}(0.25S U_{W_{1}}+0.5(C_{1}+C_{2}+C_{3})$ $-1+3X_{1}+0.5(U_{W_{1},X_{1}}+U_{W_{1},X_{2}})+S)$ $X_{2}:=f_{X_{2}}(X_{1},W_{1},C_{1},C_{2},C_{3},S)\\sim\\mathtt{B e r n o u l}\\,\\mathrm{i}\\,(\\pi_{2,S}(X_{1},W_{1},C_{1},C_{2},C_{3},S))$   \n$W_{2}:=f_{W_{2}}(\\mathbf{C},X_{1},X_{2},W_{1},U_{W_{2},X_{2}},U_{W_{2},X_{3}},S)=\\mathrm{si}\\,\\mathrm{gmoi}\\,\\mathrm{d}(0.25S U_{W_{2}}+0.5(C_{1}+C_{2}+C_{3})/2),$ $-1+3(X_{1}+X_{2})+0.5(U_{W_{2},X_{2}}+U_{W_{2},X_{3}})+S)$ X3 :\u201c fX3pX1, X2, W1, W2, C1, C2, C3, Sq $\\sim\\mathtt{B e r n o u l i}\\left(\\pi_{3,S}(X_{1},X_{2},W_{1},W_{2},C_{1},C_{2},C_{3},S)\\right)$ Y :\u201c fY pC, X, W, UX3,Y , Sq $={\\mathrm{s}}{\\mathrm{i}}{\\mathrm{gmo}}{\\mathrm{i}}{\\mathrm{d}}(0.5(C_{1}+C_{2}+C_{3})+2(X_{1}+X_{2}+X_{3}$ q $-3-0.5(W_{1}+W_{2})+0.1U_{X_{3},Y}+0.25S U_{W}+S).$ . ", "page_idx": 21}, {"type": "text", "text": "Also, for $S\\ne0$ , ", "page_idx": 21}, {"type": "text", "text": "$\\pi_{1,0}=\\,s\\,\\mathrm{{i}}\\,\\mathtt{g m o i d}(C_{1}+C_{2}+C_{3}-2)$   \n$\\pi_{1,S}=\\,{\\mathrm{s}}\\,{\\mathrm{i}}\\,{\\mathrm{gmo}}\\,{\\mathrm{i}}\\,{\\mathrm{d}}(0.5(C_{1}+C_{2}+C_{3})-1)$   \n$\\pi_{2,0}=\\mathtt{s i g m o i d}(0.5(C_{1}+C_{2}+C_{3})+2(2X_{1}-1)-0.5W+1)$   \n\u03c02,S \u201c s ${\\mathsf{\\Pi}}_{\\mathsf{-}}\\mathsf{g m o}\\,\\mathrm{i}\\,\\mathrm{d}\\bigl(C_{1}+C_{2}+C_{3}+2X_{1}-1+0.5W-1\\bigr)$   \n$\\pi_{3,0}={\\mathrm{s}}{\\mathrm{\\,i}}\\operatorname{gmo}{\\mathrm{i}}\\mathrm{d}(0.25(C_{1}+C_{2}+C_{3})+(2X_{1}-1)-0.25W_{1}+1+(2X_{2}-1)-0.25W_{2})$ 25W2 \\` 1q $\\pi_{3,S}={\\mathrm{s~i~gmo~i~d}}(0.5(C_{1}+C_{2}+C_{3})+2(2X_{1}-1)+0.25W_{1}-1+2(2X_{2}-1)+0.25W$ 2 \u00b4 1q. ", "page_idx": 21}, {"type": "text", "text": "D.2 External validity of the ACTG 175 clinical trial ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To provide empirical evidence of policy estimation in a real-world setting, we revisit the ACTG 175 randomized clinical trial from 1994 conducted on patients from the United States and Puerto Rico [21]. It investigated the effectiveness of different therapies for reducing CD4 T cell counts in individuals with HIV (selected subject to various inclusion criteria). In the study, individuals were randomly assigned to two different treatments $X_{2}\\in\\{0,1\\}$ , and a record was made on whether a previous anti-retroviral drug had been administered $X_{1}\\in\\{0,1\\}$ prior to the start of the trial. Patient demographics $C_{1},C_{2}$ including gender, age, weight, among others, were collected, and CD4 T cell count were measured at treatment time $W$ , and again 20 weeks after treatment initialization $Y$ , the outcome of the analysis. To simulate a second study with a different guideline for anti-retroviral drug administration, we considered a sub-sampled version of ACTG 175 in which covariate distributions as well as the assignment of $X_{1},X_{2}$ were modified. ", "page_idx": 21}, {"type": "text", "text": "ACTG 175 is an experimental study in which $X_{2}$ has been randomized and $X_{1}$ follows a baseline, unknown, stochastic policy $\\pi_{2}:\\Omega_{C_{2}}\\times\\Omega_{X_{1}}\\rightarrow[0,1]$ assumed to depend on study-specific features $C_{2}$ such a patient\u2019s Karnofsky score and symptomatic indicators (both normalized to lie in the $[0,1]$ interval). Samples of variables $C_{1},C_{2},W,X_{1},X_{2},Y$ therefore follow a distribution $P_{\\mathrm{rand}(X_{2}),\\pi_{2}}^{2}(C_{1},C_{2},W,X_{1},X_{2},Y)$ . The suffix \u201crand $(X_{2})^{\\ast}$ denotes a policy that randomizes $X_{2}$ , i.e. $X_{2}\\sim\\mathrm{Bern}(0.5)$ . ", "page_idx": 21}, {"type": "image", "img_path": "PSubtZAitM/tmp/3228ee794c0a90544e20c7ea62ede5b7e253d98ef125de51b4f1e18b9041ce09.jpg", "img_caption": ["Figure 5: Causal diagrams and selection diagrams of the ACTG 175 experiment. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "We generate a data sample from a second domain $\\mathbf{\\nabla}S=1$ ) following the marginalized distribution $P_{\\pi_{1}}^{1}(C_{1},W,X_{1})$ , mimicking a simple stochastic guideline on $X_{1}$ in which $\\pi_{1}:=\\pi_{1}(x_{1}=1\\mid c_{1})=$ $1/(1+\\exp\\{-c_{11}-c_{12}-c_{13}\\})$ . Higher values of ${\\cal C}_{1}$ (taken to be normalized measurements of weight, height, and age) lead to higher likelihood of treatment. We achieve this dataset by sampling according to a re-weighted version of the ACTG 175 trial. In particular, we collect data from $P^{1}$ according to, ", "page_idx": 22}, {"type": "equation", "text": "$$\nP_{\\pi_{1}}^{1}(C_{1},W,X_{1}):=P_{\\pi_{2}}^{2}(C_{1},W,X_{1})\\frac{\\pi_{1}(X_{1}\\mid C_{1})P^{1}(C_{1})}{P_{\\pi_{2}}^{2}(X_{1}\\mid C_{1})}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For this example, we consider evaluating a stochastic policy $\\pi_{0}\\,=\\,\\bigl\\{\\pi_{0}(x_{1}\\mid c_{1}),\\pi_{0}(x_{1}\\mid c_{2})\\bigr\\}$ that combines the drugs $X_{1},X_{2}$ according to a stochastic policy for $X_{1}$ based on weight, height, and age, $\\left(C_{1}\\right)$ and for $X_{2}$ based on a patient\u2019s Karnofsky score and symptomatic indicators $\\left(C_{2}\\right)$ . In particular, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\pi_{0}(x_{1}=1\\mid c_{1})=1/(1+\\exp\\{-c_{11}-1\\}),\\quad\\pi_{0}(x_{2}=1\\mid c_{2})=1/(1+\\exp\\{-0.5c_{21}-c_{22}\\}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The policy $\\pi_{0}$ is considered to be implemented on a patient population located in a different location that are know to have a differing covariate distribution $P^{0}(C_{1},C_{2})$ to that observed in ACTG 175 and the second study, among other discrepancies. We assume that the SCM generating this experimental study follows the causal graphs in Fig. 5. ", "page_idx": 22}, {"type": "text", "text": "The target population under $\\pi_{0}$ is then given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{\\pi_{0}}^{0}(C_{1},C_{2},W,X_{1},X_{2},Y)}\\\\ &{:=P_{\\mathrm{rand}(X_{2}),\\pi_{2}}^{2}(C_{1},C_{2},W,X_{1},X_{2},Y)\\frac{P^{0}(C_{1},C_{2})\\pi_{0}(X_{1}\\mid C_{1})\\pi_{0}(X_{2}\\mid C_{2})}{P^{2}(C_{1},C_{2})P_{\\pi_{2}}^{2}(X_{1}\\mid C_{1})P_{\\mathrm{rand}(X_{2}),\\pi_{2}}^{2}(X_{2})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We limit all datasets to approximately 2000 samples as this is the size of the ACTG 175 trial. The ground truth target effect $\\mathbb{E}_{P_{\\pi_{0}}^{0}}[Y]$ is evaluated by taking the empirical mean of $Y$ in the sample of data collected from $P^{0}$ with the procedure above. ", "page_idx": 22}, {"type": "text", "text": "D.3 External validity of the Project STAR study ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We describe in this section additional experimental details on the Project STAR study4. This study investigated the impact of teacher/student ratios on academic achievement for kindergarten through third-grade students. Project STAR was a four-year longitudinal study where students were randomly assigned to one of three interventions with different class sizes each year, following different randomization procedures. The causal diagram we assume for this setting is provided in ", "page_idx": 22}, {"type": "text", "text": "Fig. 6. Bi-directed arcs denote unobserved confounding in the observational regime (when student are observed in a particular class size rather than forced to join a particular class size). ", "page_idx": 23}, {"type": "text", "text": "Specifically, we consider the evaluation of a 3-stage stochastic policy, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pi_{0}=\\{\\pi_{0}(x_{1}\\mid c_{1}),\\pi_{0}(x_{2}\\mid c_{2},x_{1},w_{1}),\\pi_{0}(x_{3}\\mid c_{3},x_{2},w_{2})\\},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{~\\\\\\\\\\\\\\}\\pi_{0}(x_{1}\\mid c_{1}):=1/(1+\\exp\\{c_{11}+c_{12}-2\\})}\\\\ &{\\mathrm{~\\\\\\\\\\\\\\\\\\}\\tau_{0}(x_{2}\\mid c_{2},x_{1},w_{1}):=1/(1+\\exp\\{0.5(c_{21}+c_{22})+2(2x_{1}-1)-0.5w_{1}+1\\})}\\\\ &{\\mathrm{~\\\\\\\\\\\\\\}\\tau_{0}(x_{3}\\mid c_{3},x_{2},w_{2}):=1/(1+\\exp\\{0.5(c_{31}+c_{32})+(2x_{1}-1)-0.25w_{1}+(2x_{2}-1)-0.25w_{2}+1\\})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "These policies determine the student-teacher ratio $X_{0},X_{1},X_{2}$ , taking values \"regular\" or \"small\", across three different grades, namely Grade 0 (Kindergarten), Grade 1 and Grade 2. $C$ refers to a two-dimensional demographic variable encoding gender and ethnicity, converted to binary and categorical variables respectively. (To avoid clutter, in Fig. 6 we use $C=C_{1}=C_{2}=C_{3}$ .) $W_{1}$ , $W_{2}$ are intermediate school outcomes that include the sum total of an individual\u2019s reading score and math score in grades 0 (Kindergarten) and 1 respectively. $Y$ is the outcome of interest and represents total reading score and math score in grade 2. ", "page_idx": 23}, {"type": "text", "text": "To mimic the setting where data at different stages was collected from different domains, we subsample the dataset using different sets of probabilities to induce differences in the distributions of baseline covariates. In particular, we fix the dataset in the target domain $(S=0)$ ) to the distribution observed in the study and sub-sample according to different probabilities to create datasets for domains $S=1,S=2$ , and $S=3$ , as follows. ", "page_idx": 23}, {"type": "text", "text": "We generate a sample of data from a first source domain $\\raisebox{S}{\\(S)}=1.$ ) following the marginalized distribution $P_{\\pi_{1}}^{1}(C_{1}\\^{\\phantom{\\dagger}},W_{1},X_{1})$ , where $\\pi_{1}:=\\pi_{1}(x_{1}=1\\mid c_{1})=1/(1+\\exp\\{0.5(c_{11}+c_{12})-1\\})$ defines the probability for the student-teacher ratio variables in Kindergarten in domain $S=1$ . We achieve this dataset by sampling according to a re-weighted version of the STAR study. In particular, we collect data from $P^{1}$ according to, ", "page_idx": 23}, {"type": "equation", "text": "$$\nP_{\\pi_{1}}^{1}(C_{1},W_{1},X_{1}):=P^{0}(C_{1},W,X_{1})\\frac{\\pi_{1}(X_{1}\\mid C_{1})P^{1}(C_{1})}{P^{0}(X_{1}\\mid C_{1})}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $P^{1}(c_{1})=0.3$ if $c_{11}=1,c_{12}=1$ and $P^{1}(c_{1})=0.7$ otherwise. ", "page_idx": 23}, {"type": "text", "text": "We generate a sample of data from a second source domain $[S=2]$ ) following the marginalized distribution $P_{\\pi_{2}}^{2}(C_{2},W_{1},W_{2},X_{1},X_{2})$ , where $\\pi_{2}:=\\pi_{2}(x_{2}=1\\;|\\;c_{2},x_{1},w_{1})=1/(1+\\exp\\{0.5(c_{21}+$ $c_{22})+2x_{1}\\bar{-}1-0.5w_{1}-1\\}$ q defines the probability for the student-teacher ratio variables in grade 1 in domain $S=2$ . We achieve this dataset by sampling according to a re-weighted version of the STAR study. In particular, we collect data from $P^{2}$ according to, ", "page_idx": 23}, {"type": "equation", "text": "$$\nP_{\\pi_{2}}^{2}(C_{2},W_{1},W_{2},X_{1},X_{2}):=P^{0}(C_{2},W_{1},W_{2},X_{1},X_{2})\\frac{\\pi_{2}(X_{2}\\mid C_{2},X_{1},W_{1})P^{2}(C_{2})}{P^{0}(X_{2}\\mid C_{2},X_{1},W_{1})}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $P^{2}(c_{2})=0.7$ if $c_{21}=1,c_{22}=1$ and $P^{2}(c_{2})=0.3$ otherwise. ", "page_idx": 23}, {"type": "text", "text": "We generate a sample of data from a third source domain $\\left[S\\right.=\\left.3\\right)$ following the marginalized distribution $P_{\\pi_{3}}^{3}(C_{3},W_{1},W_{2},X_{1},X_{2},X_{3})$ , where $\\pi_{3}:=\\pi_{3}(x_{3}=1\\mid c_{3},x_{1},w_{1},x_{2},w_{2})=1/(1+$ $\\exp\\{0.5(c_{31}+c_{32})+2(2x_{1}-1)-0.25w_{1}+(2x_{2}-1)+0.25w_{2}-1\\}$ q defines the probability for the student-teacher ratio variables in grade 3 in domain $S\\,=\\,3$ . We achieve this dataset by sampling according to a re-weighted version of the STAR study. In particular, we collect data from $P^{3}$ according to, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{\\pi_{3}}^{3}(C_{3},W_{1},W_{2},X_{1},X_{2},X_{3},Y):=}\\\\ &{P^{0}(C_{3},W_{1},W_{2},X_{1},X_{2},X_{3},Y)\\frac{\\pi_{3}\\left(X_{3}\\mid C_{3},W_{1},W_{2},X_{1},X_{2},X_{3}\\right)P^{3}(C_{3})}{P^{0}(X_{3}\\mid C_{3},X_{1},W_{1},X_{2},W_{2})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $P^{3}(c_{3})=0.5$ if $c_{31}=1,c_{32}=1$ and $P^{3}(c_{3})=0.5$ otherwise. ", "page_idx": 23}, {"type": "image", "img_path": "PSubtZAitM/tmp/78ee46661a3600ef11a695f5b321934056bb03a5d7b3f90c88cb6aa73f6dcfd4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 6: Causal diagram assumed for the target domain of the STAR Project study. To avoid cluttering the diagram we write $C=C_{1}=C_{2}=C_{3}$ , i.e., all $C$ \u2019s refer to the same variables (gender and ethnicity). ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 24}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 24}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 24}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 24}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 24}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 24}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 24}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The claims made match the theoretical and experimental results presented in the paper. A broader overview statement in the Appendix reflects how much the results can be expected to generalize to other settings. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We include a broader overview statement in the Appendix to more thoroughly describe the limitations of our analysis, assumptions, and applicability in real-world settings. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All theoretical statement are quoted in full in the paper. We have attempted to provide an example to illustrate the significance of each theoretical statement and highlight its implications. The formal proof of all statements is given in the Appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. ", "page_idx": 25}, {"type": "text", "text": "\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We provide the data generating mechanisms, details of the target of estimation and information as to what python libraries can be used to fit the proposed estimators. We do not, however, disclose an open source implementation of the proposed methods at this moment. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The data is publicly available and we have provided full details as to where to access the data and how to run the synthetic data generation pipeline. The code will not be open sourced at this moment but we believe to have provided sufficient details to reproduce our results. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [TODO] ", "page_idx": 27}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provided details where applicable. In our case, data splits, hyperparameters, optimizers, etc., are not significant for the implementation the method. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We report our results with error bars that represent 2 standard deviations from the mean. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have read the guidelines and we do not think that our work presents any notable concerns. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We include a broader overview statement in the Appendix. We do not expect any negative societal impacts of our work. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: No risk of misuse. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not use existing asset Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowd-sourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]