[{"type": "text", "text": "S-STE: Continuous Pruning Function for Efficient 2:4 Sparse Pre-training ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuezhou $\\mathbf{H}\\mathbf{u}^{1}$ , $\\mathbf{Jun}\\,\\mathbf{Zhu}^{1}$ , Jianfei Chen1\u2020 ", "page_idx": 0}, {"type": "text", "text": "1Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University. huyz21@mails.tsinghua.edu.cn, {dcszj,jianfeic}@tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Training deep neural networks (DNNs) is costly. Fortunately, Nvidia Ampere and Hopper GPUs can accelerate matrix multiplications twice as fast as a dense equivalent by implementing 2:4 sparsity. However, previous STE-based 2:4 pretraining methods (e.g. STE with hard-thresholding, SR-STE) suffer from optimization difficulties because of discontinuous pruning function. In this study, we comprehensively analyse the bottleneck of traditional N:M sparse training and recognize three drawbacks with discontinuity: incorrect descending direction, inability to predict the amount of descent and sparse mask oscillation. In light of this, we propose S-STE, a simple yet powerful 2:4 training method that contains two parts: to continuously project weights to be 2:4 sparse, and to rescale sparse weights with a per-tensor fixed scaling factor. Besides, we adopt minimum-variance unbiased estimation for activation gradient and FP8 quantization for whole process. Results show that our method surpasses previous 2:4 pre-training recipes and is comparable even with full parameter models. Our toolkit is available at https://github.com/huyz2023/2by4-pretrain. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large scale transformers have achieved many impressive results such as chatbots [43], text-to-video generation [27], and robot manipulation [53]. However, the pre-training of these models is extremely expensive, typically requiring thousands of GPUs to train for months [5]. One possible way to accelerate deep learning computation is sparsity. N:M sparsity [31] is a hardware-friendly sparsity pattern, where every group of $M$ dimensions only has $N$ non-zero entries. Nvidia Ampere GPUs can multiply a 2:4 sparse matrix with a dense matrix, twice as fast as multiplying two dense matrices. ", "page_idx": 0}, {"type": "text", "text": "While N:M sparsity has been successfully applied to accelerate inference [31, 40, 15, 35, 9], extending the acceleration to pre-training is highly challenging. To accelerate pre-training, the sparse model must be trained from scratch (random initialization), and the network must stay sparse at all training iterations. To meet these requirements, the algorithm should be able to actively explore connectivity patterns within the constrained N:M parameter space. Therefore, popular pruning methods such as single-shot pruning [24], iterative magnitude pruning [13, 29], and RigL [11] cannot be directly applied to this scenario. Moreover, besides forward propagation, the matrix multiplications in back propagation must be sparsified as well, to provide reasonable training speedup. ", "page_idx": 0}, {"type": "text", "text": "Methods based on the straight-through estimator (STE) [52, 2] have shown promise towards solving the challenging problem of sparse pre-training. They maintain a dense weight, which is sparsified in each iteration for fast forward&backward computation, and the dense weight is then updated with STE gradients. In this way, connectivity patterns can be learned jointly with weights in an end-to-end fashion with stochastic gradient optimizers. SR-STE [52] is such a method to train sparse networks from scratch, with a regularization term to stabilize the training. Several subsequent works [21, 51, 8] further accelerate back propagation with sparse computations, and Hu et al. [20] applied it for pre-training language models. However, these sparse training methods still have an accuracy gap compared to dense training. Moreover, SR-STE introduces a regularization strength hyper-parameter, which is hard to tune. Due to these limitations, N:M sparsity is not yet used to accelerate pre-training. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we study STE-based pre-training from the optimization perspective. We point out that STE-based pre-training defines a discontinuous loss function, which existing optimization theory and algorithms cannot handle. We reveal several intriguing phenomena highlighting the difficulty of discontinuous optimization, including incorrect descending direction, inability to predict the amount of descent, and oscillation. We sidestep the curse of discontinuity by proposing smooth straight-through estimator (S-STE) as a solution. Cruically, S-STE introduces a new pruning function, which uses a continuous projection function to prune weights to be 2:4 sparse, and scales all nonzero elements to minimize the mean-square-error between original dense weight vector and sparse weight vector. The proposed 2:4 soft-thresholding function is continuous but can still generate N:M sparse weights at all times. In this way, the objective function is continuous, and gradient-based optimizers can be readily used. Furthermore, S-STE does not introduce any hyper-parameter, so its practical adoption is easier than SR-STE. ", "page_idx": 1}, {"type": "text", "text": "We devise comprehensive pre-training experiments on S-STE, including WMT machine translation, GPT-2 pre-training and, DeiT image classification. Results show that our method surpass previous 2:4 pre-training recipes on a wide range of tasks. ", "page_idx": 1}, {"type": "text", "text": "2 Formulation of sparse pre-training ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The training a neural network can be formalized as an optimization problem $\\mathrm{min}_{\\mathbf{w}}\\,F(\\mathbf{w})$ , where $\\textbf{w}\\in\\mathbb{R}^{d}$ is the parameter and $F$ is a differentiable empirical risk function: $F(\\mathbf{w})\\,=\\,R_{n}(\\mathbf{w})\\,=$ $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}f(\\mathbf{w};\\xi_{[i]})$ . Here, $f$ is the loss function, $n$ is the size of data set $\\mathcal{D}=\\{\\xi_{[i]}\\}_{i=1}^{n}$ and $\\xi_{[i]}$ is the $i$ -th sample. The optimization can be solved with standard stochastic gradient method (SG) [4]. Suppose the network is initialized with $\\mathbf{w}_{1}$ , $\\{\\alpha_{k}\\}$ is a positive learning rate sequence, and $\\xi_{[i_{k}]}$ is randomly chosen from $\\{\\xi_{[i]}\\}_{i=1}^{n}$ . Then, iteratively we have $\\mathbf{w}_{k+1}=\\mathbf{w}_{k}-\\alpha_{k}\\nabla_{\\mathbf{w}_{k}}f\\big(\\mathbf{w}_{k};\\boldsymbol{\\xi}_{[i_{k}]}\\big)$ . As we consider pre-training tasks, $\\mathbf{w}_{1}$ is simply a random initialization. ", "page_idx": 1}, {"type": "text", "text": "The training of a sparse network involves optimizing the parameter w in a constrained space $\\mathcal{W}\\subset\\mathbb{R}^{d}$ . For an N:M-sparse network, the parameter can only have $N$ non-zero elements in each contiguous $M$ dimensions. ", "page_idx": 1}, {"type": "text", "text": "Alternative to constrained optimization, we can solve the unconstrained problem: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}}F({\\tilde{\\mathbf{w}}}){\\mathrm{~where~}}{\\tilde{\\mathbf{w}}}=S(\\mathbf{w}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here, $S$ is a pruning function which converts a dense weight w to a sparse weight $\\tilde{\\mathbf{w}}\\in\\mathcal{W}$ . One common choice is the hard-thresholding pruning function [52, 45]. For every block of four adjacent elements $\\mathbf{a}=[a_{1},...,a_{M}]^{\\top}\\in\\mathbb{R}^{M}$ in the weight vector w, the pruning function can be defined as ", "page_idx": 1}, {"type": "equation", "text": "$$\n(S_{h}(\\mathbf{a}))_{i}=\\left\\{\\!\\!\\begin{array}{l l}{a_{i}}&{\\mathrm{~if~}|a_{i}|\\geq t}\\\\ {0}&{\\mathrm{~if~}|a_{i}|<t}\\end{array}\\!\\!,\\mathrm{~for~}i=1,...,M,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $t$ is $N$ -th largest element in $\\mathbf{a}$ .1 This essentially performs magnitude-based pruning, by zeroing out the two smallest elements. The hard thresholding function can also be written as $S_{h}(\\mathbf{a})\\bar{=}\\,\\mathbf{a}\\odot m_{h}(\\mathbf{a})$ , where $m_{h}({\\mathbf a})$ is a $0/1$ mask vector, with $(\\bar{m_{h}}(\\mathbf a))_{i}=1$ if $|a_{i}|>t$ . ", "page_idx": 1}, {"type": "text", "text": "However, Eq. (1) cannot be directly optimized since the pruning function $S$ is not differentiable. Particularly, the derivative of the hard-thresholding function $S_{h}$ is undefined on boundary where the second largest and third largest element have the same magnitude. Therefore, straight-through estimator (STE) [52] is for training, by approximating $\\nabla_{\\mathbf{w}}f\\approx\\nabla_{\\tilde{\\mathbf{w}}}f$ and therefore $\\partial S_{h}(\\mathbf{\\bar{a}})/\\partial\\mathbf{a}\\approx\\mathbf{\\bar{I}}$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\bf w}_{k+1}={\\bf w}_{k}-\\alpha_{k}\\nabla_{{\\tilde{\\bf w}}_{k}}f({\\tilde{\\bf w}}_{k};{\\boldsymbol\\xi}_{[i_{k}]}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "With the pruning function and STE, each iteration of sparse training involves: (1) prune the dense weight to get the sparse weight: $\\tilde{\\mathbf{w}}=S(\\mathbf{w})$ ; (2) compute the loss and gradient with the sparse weight; and (3) update the dense weight with the gradient. Among these, step 2 is most time-consuming, and it can be accelerated with sparse tensor cores given w\u02dc is N:M-sparse. Next, we will focus on the optimization aspects of sparse training and defer the discussion of computation details to Sec. 4. ", "page_idx": 2}, {"type": "text", "text": "3 The curse of discontinuity ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Classical stochastic optimization theory [4] guarantees the convergence for nonconvex and smooth (i.e., differentiable with Lipschitz continuous gradients) objective $F$ . It can be also extended to handle non-differentiable functions such as ReLU [25]. The real problem of STE-based sparse training is the discontinuity of the pruning function $S_{h}$ , as visualized in Fig. 2. For a discontinuous function, an arbitrarily small change in input a can cause an unbounded change of the output $S_{h}({\\mathbf a})$ . Such discontinuity appears on the boundary when the $N$ -th and $N+1$ -th largest elements have same magnitude. For example, for a 1:2-sparse pruning function, $S_{h}(1,0.999)=(1,0)$ , but $S_{h}(0.999,1)=(0,1)$ , and the boundary is the line $a_{1}=a_{2}$ . ", "page_idx": 2}, {"type": "text", "text": "When $S_{h}$ is discontinuous, the parameter space $\\mathbb{R}^{d}$ can be partitioned into regions $\\mathbf{\\{}\\}\\mathbf{\\mathcal{W}_{m}}|{\\bf m}\\in$ $\\mathcal{M}\\}$ , where $\\mathcal{M}~\\subset~\\{0,1\\}^{d}$ is the space of $0/1$ masks with N:M pattern, and all the parameters in each region $\\textbf{w}\\in\\mathbf{\\mathcal{W}_{m}}$ have the same mask $m_{h}(\\mathbf{w})=\\mathbf{m}$ . The loss landscape $F(S_{h}(\\mathbf{w}))\\;=\\;\\dot{F}(m_{h}(\\mathbf{w})\\odot\\mathbf{w})$ is continuous and differentiable within each region, where gradient-based algorithms can work well. However, when the optimization trajectory crosses the boundary: $\\bar{m_{h}}(\\mathbf{w}_{k+1})\\neq\\bar{m_{h}}(\\mathbf{w}_{k})$ , the behavior is unpredictable. We highlight several intriguing phenomena observed in optimizing such discontinuous objective. We study these phenomena in both a toy problem and real neural networks. ", "page_idx": 2}, {"type": "image", "img_path": "8abNCVJs2j/tmp/3b76e2262297374f9336d2759d83c96d71fc5ce2d47bbc3641d4e52025edf458.jpg", "img_caption": ["Figure 1: Scatter plot of $\\Delta F_{1}$ with $\\Delta F_{2}$ and their distributions on GPT-2 small 124M for iteration $k\\in[1,6000]$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "3.1 Phenomenon 1: incorrect descending direction ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Here, we run a gradient descent algorithm (without stochasticity) on a small dataset. For a dense model where $F$ is differentiable, with Taylor\u2019s formula we should have ", "page_idx": 2}, {"type": "equation", "text": "$$\nF(\\mathbf{w}_{k})-F(\\mathbf{w}_{k+1})\\approx(\\nabla_{\\mathbf{w}_{k}}F(\\mathbf{w}_{k}))^{\\top}(\\mathbf{w}_{k}-\\mathbf{w}_{k+1})=\\alpha_{k}\\left\\|\\mathbf{w}_{k}-\\mathbf{w}_{k+1}\\right\\|^{2}\\geq0.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "That is, the objective function will monotonically decrease in each iteration once the learning rate $\\alpha_{k}$ is sufficiently small. However, it is not the case for sparse training. In Fig. 2(d), we measure the distribution of the amount of descent (AoD) $\\Delta F_{k}:={\\bar{F}}(\\mathbf{\\bar{w}}_{k})-F(\\mathbf{w}_{k+1}^{-})$ for training a GPT-2 large 774M model with Eq. (2, 3), across each iteration $k$ . The results clearly shows that the objective frequently fails to descent. ", "page_idx": 2}, {"type": "text", "text": "We can take a closer look to the weight and mask sequence $\\left(\\mathbf{w}_{k},m_{h}(\\mathbf{w}_{k})\\right)$ generated by the training algorithm. We compare the following two quantities: the AoD by updating both weight and mask $\\Delta\\bar{F}_{1}\\,=\\,F({\\bf w}_{k}\\odot\\bar{\\bf{m}}_{k})\\,-\\,F({\\bf w}_{k+1}\\odot{\\bf m}_{k+1})$ and the AoD by only updating the weight $\\Delta F_{2}\\,=$ $F(\\mathbf{w}_{k}\\odot\\mathbf{m}_{k})-F(\\mathbf{w}_{k+1}\\odot\\mathbf{m}_{k})$ . In Fig. 1, we can observe $\\Delta F_{2}$ is mostly positive due to the piecewise continuity of $F$ . However, $\\Delta F_{1}$ is frequently negative and very often even smaller than $\\Delta F_{2}$ (updating mask is worse than not updating). This indicates that the main problem is the discontinuity make it hard to estimate the correct descending direction of $\\mathbf{m}$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Phenomenon 2: inability to predict the amount of descent ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Besides making mistakes in finding the correct descending direction, algorithms do not know that they make a mistake, in the sense that they fail to predict the AoD at each step. From Eq. (4), we should have $F(\\mathbf{w}_{k})-F(\\mathbf{w}_{k+1})\\approx(\\nabla_{\\mathbf{w}_{k}}\\bar{F}(\\mathbf{w}_{k}))^{\\top}(\\mathbf{w}_{k}-\\mathbf{w}_{k+1})$ , where the left hand side is the actual ", "page_idx": 2}, {"type": "text", "text": "AoD, and the right hand side is the predicted AoD. We plot the actual AoD against predicted AoD for dense (Fig. 2(a)) and sparse training (Fig. 2(b)). While for dense training, the two quantities closely matches, for hard-thresholding the actual AoD is often lower for the predicted AoD, particularly when the predicted AoD is large. To understand this, note that Eq. (4) only holds for $\\mathbf{w}\\in\\mathcal{W}_{m_{h}(\\mathbf{w})}$ . Once $\\mathbf{w}_{k+1}-\\mathbf{w}_{k}$ is large enough that $m_{h}(\\mathbf{w}_{k+1})\\neq m_{h}(\\mathbf{w}_{k})$ , the function crosses a border of $S_{h}$ , and $F$ will have a sudden change which is unpredictable by the gradient. ", "page_idx": 3}, {"type": "image", "img_path": "8abNCVJs2j/tmp/24b07f5a2336507f5dcf33ef6804d3b0303769a41cbfa78b8848aae6963274c0.jpg", "img_caption": ["Figure 2: (a)-(c) shows scatter plots of the predicted and actual loss reduction of dense, hardthresholding and S-STE with GPT-2 large 774M model for iteration $k\\in[1,3000]$ . The diagonal line is for reference. (d) shows empirical cumulative distribution of their actual AoD for $k\\in[\\bar{1},6000]$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.3 Phenomenon 3: oscillation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Oscillation is probably the most significant problem in STE-based sparse training. Here, we revisit existing discussions about oscillation [52, 28, 20], and then illustrate this issue using a toy example. ", "page_idx": 3}, {"type": "text", "text": "Flip rate Flip rate is a simple metric to measure the stability of sparse training [20]: $r_{k}=$ $\\|\\bar{m_{h}}(\\mathbf{w}_{k})\\oplus\\bar{m_{h}}(\\mathbf{w}_{k-1})\\|_{1}\\,/d$ , where $\\oplus$ indicates XOR operation. As Hu et al. [20] points out, taking the filp rate of the dense model as standard, they observe larger filp rate of hard-thresholding: when training transformers, the flip rate can stay at $6\\%$ in the entire training process. However, a healthy training process should have a large filp rate in the early stage to explore connectivity patterns, and the filp rate should decrease to zero in later stage for the optimization to converge. Hu et al. [20] describe this phenomenon as \u201cflip rate explosion\u201d, which is harmful to sparse training. ", "page_idx": 3}, {"type": "text", "text": "An exemplar toy problem Modern deep neuron networks have billions of parameters and is not strictly convex. These non-ideal conditions make our analysis more difficult with sparse weights. To analyze the characteristics of STE and hard-thresholding on the smallest problem, we devise a simple toy problem that contains two parameters: $\\begin{array}{r}{\\operatorname*{min}_{w_{1},w_{2}}g(\\bar{\\boldsymbol{w}}_{1},\\boldsymbol{w}_{2})=(w_{1}-\\bar{\\boldsymbol{w}}_{2})^{2}}\\end{array}$ . This may differ from the real DNN optimization problem, but can help us understand what happens in the process. We are going to show that while using a feasible $\\alpha_{k}$ that can make the dense model converge to global minima, STE with hard-thresholding fails to converge and it oscillates back and forth. ", "page_idx": 3}, {"type": "text", "text": "First, for the dense model, the global minima lies on the line $w_{1}\\,=\\,w_{2}$ . Suppose we start from $\\mathbf{w}_{1}=[0.2,0.1]^{\\top}$ , by taking $\\alpha_{k}=0.25$ we can reach global minima in one step. On the other hand, if we are in 1:2 sparse situation, the global minima should be the point $w_{1}=w_{2}=0$ . By starting from $\\mathbf{w}_{1}=[0.2,\\dot{0}.1]^{\\top}$ and taking $\\alpha_{k}=0.25$ , we invariably jumps between $\\mathbf{w}_{2t+1}=[0.2,0.1]^{\\top}$ and $\\mathbf{w}_{2t}=[0.1,0.2]^{\\top}$ , and $g$ never decreases. ", "page_idx": 3}, {"type": "text", "text": "High flip rate is harmful, because there are frequent changes on the connection of neurons, which means that a number of previous optimization steps on the neuron is deprecated. That is fatal at the end of training [20]. The reason of high flip rate on hard-thresholding can be explained by discontinuity: as there are no gentle transitions on both sides of the border, the gradient on the boundary is inaccurate and is unable to indicate the right descending direction. This misalignment is easy to make the tuple a to oscillate back and forth near the boundary, and cause extremely higher flip rate than the dense model. ", "page_idx": 3}, {"type": "text", "text": "3.4 Overcoming the curse of discontinuity ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "One way to mitigate discontinuity is sparse-refined straight-through estimator (SR-STE), which adds a sparse-refined regularization on the gradients [52]: $\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{w}}F(\\tilde{\\mathbf{w}})+\\frac{\\lambda_{W}}{2}\\|\\mathbf{w}\\odot\\overline{{m(\\mathbf{w})}}\\|_{2}^{2}}\\end{array}$ . While ", "page_idx": 3}, {"type": "text", "text": "SR-STE works on a wide range of tasks and optimization algorithms [52, 20], it still has some issues. First, the performance is quite sensitive to the hyper-parameter $\\lambda_{W}$ . Second, the new regularization term leads to a competition between loss function and sparse regularization. Finally, the loss function is still discontinuous unless $\\lambda_{W}\\to\\infty$ . ", "page_idx": 4}, {"type": "text", "text": "From the above analysis, discontinuity causes optimization problems. It would be ideal to have a continuous pruning function, yet the iterates $\\left(\\tilde{\\mathbf{w}_{k}}\\right)$ still need to be sparse during the entire training process. ", "page_idx": 4}, {"type": "image", "img_path": "8abNCVJs2j/tmp/5d60d1134f2b5488b65f280f2938158c41b77d85c334efdb5b40f7150423c698.jpg", "img_caption": ["Figure 3: Pruning function of hard-thresholding and soft-thresholding for 1:2-sparsity. (a)(b) show the outputs of hard-thresholding, and (c)(d) show that of soft-thresholding. A sudden jump exists in hard-thresholding if $\\left|a_{1}\\right|=\\left|a_{2}\\right|$ , while soft-thresholding is continuous in the domain. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section we propose a training algorithm (smooth straight-through estimator, S-STE) that contains two main parts combined with STE: 2:4 specific softthresholding, and fixed weight rescaling . They together work as the sparsifying function described in Sec. 2: $\\tilde{\\bf w}\\;=\\;S(\\bf w)\\;\\stackrel{\\scriptscriptstyle{-}}{=}\\;\\beta\\dot{S}_{s o f t}(\\bf w)$ . Results in Fig. 2(c)(d) and 4(d) show that S-STE successfully overcome the three curses of discontinuity. Notably, flip rate curves of S-STE are surprisingly consistent with their dense counterparts, indicating that S-STE is more natural and feasible than SR-STE. ", "page_idx": 4}, {"type": "table", "img_path": "8abNCVJs2j/tmp/a3495e7dbedea60e246fe4a7955ee0b6521432ac96d7644c4f11587277a15fa1.jpg", "table_caption": ["Table 1: Validation loss and test accuracy of S-STE with different $\\gamma$ on Transformerbase. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "4.1 2:4 specific soft-thresholding $S_{s o f t}$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Motivation for the design As discussed in Sec. 3, ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "hard-thresholding suffer from the discontinuous problem near the boundary of taking a flip. When input vector changes continuously across the border, two of the four elements simultaneously jump between zeroes and none-zero values. In a continuous pruning function, we need to overcome this drawback and keep these two elements zero on both sides of the border. This means when a flip happens in a four-element block, at lease three of the target elements should be zeroed out simultaneously (except the largest one). ", "page_idx": 4}, {"type": "text", "text": "With the above analysis, we modify soft-thresholding function for traditional pruning in Vanderschueren and Vleeschouwer [45] as our 2:4 specific soft-thresholding. Given a vector $\\textbf{a}=$ $[a_{1},a_{2},a_{3},a_{4}]^{\\top}\\in\\mathbb{R}^{4}$ , S-STE picks out the largest two elements and meanwhile, subtracts the third largest element from weight magnitudes. Assume, without loss of generality, that $[t_{1},t_{2},t_{3},t_{4}]$ is an rearrangement of $\\mathbf{a}^{\\top},s.t.|t_{1}|\\leq|t_{2}|\\leq|t_{3}|\\leq|t_{4}|$ . Then, the pruning function can be defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n(S_{s o f t}(\\mathbf{a}))_{i}=\\left\\{\\!\\!\\begin{array}{l l}{a_{i}-t}&{\\mathrm{~if~}a_{i}\\in[t,+\\infty)}\\\\ {0}&{\\mathrm{~if~}a_{i}\\in(-t,t)\\phantom{,},\\mathrm{~where~}t=|t_{2}|.}\\\\ {a_{i}+t}&{\\mathrm{~if~}a_{i}\\in(-\\infty,-t]}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The plots of soft-thresholding is drawn in Fig. 3, showing $S_{s o f t}$ is continuous everywhere. Note that although we define $S_{s o f t}$ by a block $\\mathbf{a}\\in\\mathbb{R}^{4}$ , $S_{s o f t}$ can be extended to arbitrary $\\mathbf{a}\\in\\mathbb{R}^{4t}$ for $t\\geq1$ by doing block-wise pruning. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1. $S_{s o f t}(\\mathbf{a})$ is a continuous projection for $\\mathbf{a}\\in\\mathbb{R}^{d}$ . ", "page_idx": 4}, {"type": "text", "text": "A detailed discussion of the proof can be found in Appendix A.1. ", "page_idx": 4}, {"type": "text", "text": "Choosing optimal threshold Theoretically, any real number in $[|t_{2}|,|t_{3}|]$ can be used as a feasible threshold. This gives us infinite options and we describe it with an interpolation as $t=\\gamma|t_{2}|+(1-$ $\\gamma)|t_{3}|$ with $\\gamma\\in[0,1]$ . The larger $\\gamma$ is, the closer $t$ is to $\\left|t_{3}\\right|$ , and the smaller $\\|S_{s o f t}(\\mathbf{a})\\|$ is. This may affect model\u2019s capacity. In order to maximize the retention of information, using a small $\\gamma$ is necessary. In our method we propose to set $\\gamma=0$ . Experimental results in Table 1 also show that the network has the best accuracy when $\\gamma=0$ , i.e., $t=|t_{2}|$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 Fixed weight rescaling $\\beta$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Because $S_{s o f t}$ reduce the total magnitude of w, it is not a close simulation of dense weights. Like Vanderschueren and Vleeschouwer [45], we scale up $S_{s o f t}(\\mathbf{w})$ in our method as $S(\\mathbf{w})=\\beta S_{s o f t}(\\mathbf{w})$ , but we modify weight rescaling in their study to adapt to our approach. First, we use a per-tensor scale $\\beta$ rather than a per-channel $\\beta$ for simplicity. Besides, two important improvements are made: to compute scale factor only at the beginning of training, rather than to dynamically update scale factor during training, and to minimize the mean-square-error (MSE) between original dense weights and sparse weights, rather than to keep the total magnitude of weights unchanged. ", "page_idx": 5}, {"type": "text", "text": "Freezing scaling factor As Vanderschueren and Vleeschouwer [45] use a dynamic $\\beta$ for every iteration, we argue that this doesn\u2019t align with our approach. We explain our solutions in two parts. ", "page_idx": 5}, {"type": "image", "img_path": "8abNCVJs2j/tmp/4ba0d556ef09006287ddf51a71002790209545c170dafdcc696262c8beb82c72.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 4: (a) Flip rate curve over the training process with different $\\beta$ on Transformer-base. (b) Dynamically recalculated $\\beta$ at each layer on different epochs. Results show that frequently updating $\\beta$ will cause it to be unexpectedly large. (c) Flip rate curve over the training process with fixed and dynamic $\\beta$ on Transformer-base. (d) Flip rate of dense, SR-STE and S-STE algorithm on Transformer-base. ", "page_idx": 5}, {"type": "text", "text": "First, we find it interesting that $\\beta$ have a subtle correlation with filp rate: in a sparse model, larger $\\beta$ usually results in higher flip rate. The reason can be explained by the accuracy of gradients. As we use STE in the backward pass, the approximation $\\nabla_{\\mathbf{w}}\\bar{f}\\approx\\nabla_{\\tilde{\\mathbf{w}}}\\bar{f}$ is valid when $\\tilde{\\bf w}$ and w are close enough. However, if scale is too large then optimal, w and w\u02dc are too far apart to guarantee this. Such a mismatch leads to incorrectness in descending directions, and thus unstability in optimization increase flip rate; see Fig. 4(a). ", "page_idx": 5}, {"type": "text", "text": "Second, we argue that dynamically computing scaling factor for each iteration leads to high filp rate in our training process. Fig. 4(b) shows the results dynamically changing $\\beta$ will make it increase with iterations, especially for later layers. Fig. 4(c) shows flip rate of this network, which has a significantly higher tail than the dense one. Considering high flip rate is harmful, we propose to compute scaling factor $\\beta$ only in the first iteration. After that, we use the same $\\beta$ in the rest of the training. Fig. 4(d) shows the flip rate of our fixed scaling S-STE, which perfectly aligns with the dense one. ", "page_idx": 5}, {"type": "text", "text": "Minimizing MSE Vanderschueren and Vleeschouwer [45] choose to scale up $S_{s o f t}(\\mathbf{w})$ to have the same L1-norm as w: $\\beta=\\|\\mathbf{w}\\|_{1}/\\|S_{s o f t}(\\mathbf{w})\\|_{1}$ . However, we choose to minimize the MSE gap of $S_{s o f t}(\\mathbf{w})$ and w. As [8] point out, sparsifying weights in the forward pass should minimize MSE rather than an unbiased estimation. In our method, to determine an optimal scale $\\beta$ , we need to minimize ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{MSE}=\\left\\|\\mathbf{w}-\\beta S_{s o f t}(\\mathbf{w})\\right\\|^{2}=\\left\\|\\mathbf{w}\\right\\|^{2}-2\\mathbf{w}^{\\top}S_{s o f t}(\\mathbf{w})\\beta+\\left\\|S_{s o f t}(\\mathbf{w})\\right\\|^{2}\\beta^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Rearrange the terms and taking partial derivative of $\\beta$ , we choose $\\beta=\\mathbf{w}^{\\top}S_{s o f t}(\\mathbf{w})/\\left\\Vert S_{s o f t}(\\mathbf{w})\\right\\Vert^{2}$ . The comparison between no scaling, our minimizing MSE and keeping L1-norm can be found in Table 2. Result show that our method yields the best results in practice. ", "page_idx": 5}, {"type": "table", "img_path": "8abNCVJs2j/tmp/4db7383f44e06a7af2db5b73a557fe71030057607063c228bcbf63663d0b56b4.jpg", "table_caption": ["Table 2: Experimental result of different $\\beta$ on Transformer-base. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "8abNCVJs2j/tmp/e54a5b9922e115173bc2146e5c270f3cf8ca2c01ed87cf13ab73adc4e754bd74.jpg", "table_caption": ["Table 3: Results of different MVUE strategies on GPT-2 774M with 4000 steps. Sparsifying $S(\\mathbf{W})^{\\top}$ introduces huge loss of accuracy while sparsifying $\\nabla_{\\mathbf{z}}^{\\top}$ is acceptable with little loss. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5 Other implementation skills ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Minimum-variance unbiased estimation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To accelerate the backward propagation, Chmiel et al. [8] suggest using a minimum-variance unbiased estimator (MVUE). For every linear layer $\\mathbf{Z}_{l}=\\mathbf{X}_{l}\\bar{S}(\\mathbf{W}_{l})^{\\top}$ , there are two matrix multiplications of the backward pass in total: $\\nabla_{\\mathbf{X}_{l}}\\,=\\,\\nabla_{\\mathbf{Z}_{l}}S(\\mathbf{W}_{l})$ and $\\nabla_{\\mathbf{W}_{l}}\\;=\\;\\nabla_{\\mathbf{Z}_{l}}^{\\top}\\mathbf{X}_{l}$ , where $\\mathbf{X}_{l}$ is the input of the $l$ -th layer, $\\mathbf{W}_{l}$ and $\\mathbf{Z}_{l}$ are the weight matrix and output activation. We conduct MVUE on both two matrix multiplications and compare their results: $\\nabla_{\\mathbf{X}_{l}}=\\nabla_{\\mathbf{Z}_{l}}\\operatorname{MVUE}(S(\\mathbf{W}_{l})^{\\top})^{\\top}$ and $\\nabla_{\\mathbf{W}_{l}}=\\mathrm{MVUE}(\\nabla_{\\mathbf{Z}_{l}}^{\\top})\\mathbf{X}_{l}$ . Specifically, we choose $S(\\mathbf{W}_{l})$ and $\\nabla_{\\mathbf{Z}_{l}}$ because they both have built-in sparsity [26]. However, we only choose to sparsify the latter one. Firstly, it is proven by Hu et al. [20] and Chmiel et al. [8] that minimum loss of accuracy is guaranteed for MVUE on $\\nabla{\\mathbf z}_{\\iota}$ . Secondly, using MVUE on $S(\\mathbf{W}_{l})$ will make errors accumulate along the back propagation, and results in large standard deviation of gradient for the first few layers. Besides, results in Table 3 also show minimum loss of accuracy of MVUE on $\\nabla\\mathbf{z}_{l}$ while obvious accuracy loss on $S(\\mathbf{W}_{l})$ . Thus, we choose to sparsify only $\\nabla\\mathbf{z}_{\\iota}$ in the backward pass. ", "page_idx": 6}, {"type": "text", "text": "5.2 FP8 training ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To further accelerate pre-training of networks, we utilize popular FP8 workflow in training. Similar to Transformer Engine 2, we use $\\mathrm{FP8}\\;\\mathrm{e}3\\mathrm{m}4$ in forward pass and $\\tt e5m2$ in backward pass. Besides, we use per-tensor rescaling before casting to FP8 formats. ", "page_idx": 6}, {"type": "text", "text": "Theoretical acceleration of S-STE While 2:4 sparsity can accelerate GEMMs up to $2\\mathbf{x}$ faster, FP8 quantization can accelerate an additional $2\\mathbf{x}$ on this basis. Thus, the three GEMMs in Sec. 5.1 can be 4x, 2x, 4x faster. To sum up, we have theoretically $3\\mathbf{X}$ faster in forward and backward pass. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We validate the feasibility of our proposed method S-STE on machine translation (Transformer [46]), image classification (DeiT [42]) and generative large language models (GPT-2 [38] series). For all models, we replace the two linear layers in the feed forward network of each transformer block with ", "page_idx": 6}, {"type": "text", "text": "S-STE. We keep the rest of the networks, the optimization algorithms as well as all hyperparameters the same as their dense counterparts. ", "page_idx": 7}, {"type": "text", "text": "For Transformer, we train Transformer-base models on WMT 14 En-De dataset [3] with fairseq [34] codebase and evaluate it with BLEU [36] scores. For DeiT, we pre-train Deit-small model for ImageNet-1K [10] classification task. For GPT-2, we pre-train GPT-2 124M, 350M and 774M models on OpenWebText [16] and evaluate it on GLUE [47] and SQuAD [39] benchmarks. We also compare our method with state-of-the-art 2:4 training methods (SR-STE [52], Bi-Mask [51] and STEP [28]). The pre-training and evaluation scripts are publicly available at https://github.com/thu-ml/ 2by4-pretrain-acc-examples. ", "page_idx": 7}, {"type": "text", "text": "Machine translation We first apply S-STE to train a 12-layer Transformer-base and compare it with SR-STE and STEP. Note that we use fairseq codebase with SacreBleu metric, whose baseline should be 26.5 (the result of our reproduction is 26.42). The results are shown in Table 4. Compared with SR-STE, our method improves by 0.3 and 0.5 on test set and validation set respectively, which is the closest to baseline. Besides, we improve by 0.6 compared to STEP on test set. ", "page_idx": 7}, {"type": "table", "img_path": "8abNCVJs2j/tmp/c81265f629e39c11e0b505142292466d6d80a2fe3f008f387294b307993f4cd7.jpg", "table_caption": ["Table 4: Experimental Results for Transformer-base on En-De dataset. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "8abNCVJs2j/tmp/1fb5e2f19ccac824dc00ce5cc19e2d307d9253e200b6abb087da246dfd4e2b59.jpg", "table_caption": ["Table 5: Experimental Results for DeiT-small on ImageNet-1k. The Bi-Mask and SR-STE results are from [51]. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Image classification We further investigate the effectiveness of S-STE to train DeiT-tiny and DeiT-small on ImageNet-1k; see Table 5. Results show S-STE also achieve the best performance among different methods, with only has $1.4\\%$ degradation from the dense model. Notably, S-STE surpasses SOTA 2:4 training method Bi-Mask on this task $0.9\\%$ top1 accuracy improvement) and popular SR-STE method ( $2.8\\%$ top1 accuracy improvement). ", "page_idx": 7}, {"type": "text", "text": "Generative language models We compare S-STE with dense, normal SR-STE and SR-STE with dense fine-tuning [20] $(\\mathrm{SR-STE+DF})$ models on GLUE and SQuAD tasks. The SR-STE $+\\mathrm{DF}$ models first use SR-STE to train a 2:4 sparse model, and switch to dense training for the last 1/6 iters of pre-training (which stands for \u201cdense fine-tune\u201d). In downstream tasks it also use dense parameters to make predictions, similar to dense models. Results in Table 6 and 9 show that S-STE completely surpasses SR-STE on both tasks. Even for SR-STE $+.$ DF models, S-STE still have an advantage, with an improvement of 1.5 on GLUE average and 1.2/0.9 on SQuAD for GPT-2 774M. ", "page_idx": 7}, {"type": "text", "text": "Fine-tuning We illustrate the viability of S-STE for fine-tuning a pre-trained model, presenting a coherent workflow of accelerating both training and inference (dense fine-tuning cannot produce a sparse model for inference acceleration); see Table 7, 6. ", "page_idx": 7}, {"type": "text", "text": "Table 6: SQuAD and GLUE scores of different sizes and pre-training methods on GPT-2. We use 2:4 sparse weights to evaluate S-STE model, while dense parameters to evaluate the rest. Of note, SR-STE denotes the original SR-STE workflow (without backward MVUE), and \u201cT-SR-STE+DF\u201d denotes the combination of transposable SR-STE & backward MVUE & sparse-dense training workflow, proposed by Hu et al. [20]. S-STE settings here include backward MVUE & FP8 training. ", "page_idx": 8}, {"type": "table", "img_path": "8abNCVJs2j/tmp/426c89fee351a0a0b66dcbc1df2c8ef11655b9ade1aae810351b7fcb5b127e92.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "8abNCVJs2j/tmp/0bd2976ff8bcae0a166a16d20df61109866b3b06c8cc6346ae00da6283ef2110.jpg", "table_caption": ["Table 7: Different fine-tuning results on GLUE and SQuAD. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Ablation study In this part, We explore the effectiveness of S-STE, MVUE, and FP8 separately. We pre-train DeiT-small model on ImageNet-1K dataset for image classification. Combinations of these partitions in Table 8 show that: 1) FP8 training has little affect on pre-training accuracy ${\\sqrt{0.2\\%}}$ of acc1); 2) MVUE leads to minimal loss of performance $\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,\\!\\,}$ of acc1). ", "page_idx": 8}, {"type": "table", "img_path": "8abNCVJs2j/tmp/65b99e0fc436bbe77dc0468616b3256aef3493279dc8c7d7dba8bec64e3033bc.jpg", "table_caption": ["Table 8: Experimental result of S-STE (soft-thresholding and weight rescaling), MVUE and FP8 training with DeiT-small on ImageNet-1K. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Acceleration For acceleration, we measure the acceleration ratio of a typical GPT-2 model using implementation from Hu et al. [20]. Note that on H100 GPUs, FP8 2:4-spMM kernel turns out to be unsatisfying; see Appendix A.4. Consequently, we fall back to use RTX3090 GPUs with FP16 training. For inference, we achieve 1.53x speedup with FFN layer and $1.23\\mathbf{x}$ speedup with the network; for pre-training, we achieve $1.32\\mathbf{x}$ speedup for FFN layer and $1.18\\mathbf{x}$ speedup for the network (Appendix A.3). ", "page_idx": 9}, {"type": "text", "text": "7 Related work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Unstructured pruning and coarse-grained structured pruning Pruning is to remove redundant weights from the dense model. Traditional one-shot pruning methods [17, 18, 12, 14, 31, 24] and dynamic sparse training methods [11, 6, 7, 50] mostly target on unstructured sparsity. While most of them have acceleration effect on CPUs, it is hard for these methods to work well on modern GPUs. Coarse-grained structured sparsity [49, 23, 19, 22] takes effect to acceleration, but since they often remove a whole chennel or a block, loss of information is non-negligible. ", "page_idx": 9}, {"type": "text", "text": "Fine-grained N:M sparsity for inference and pre-training Among all pruning techniques for pre-training, N:M sparsity is a promising approach towards accelerating large models, which is also known as fine-grained structured sparsity. Nvidia demonstrates $2\\mathbf{x}$ theoretical speedup on its Ampere GPUs with 2:4 sparsity for post-training [31] and inference [40, 15, 35, 9]. To leverage this property to accelerate pre-training, a number of approaches and their improvements are proposed [52, 28, 51, 1, 8, 21, 20]. However, all these methods are based on a discontinuous pruning function that is hard to optimize and results in unsatisfactory accuracy, which we will discuss in this study. ", "page_idx": 9}, {"type": "text", "text": "FP8 quantization While 16-bit float tensors are widely used in pre-training, FP8 \u2013 where float numbers stored in 8 bits \u2013 is a popular quantization methods which theoretically accelerates GEMMs up to $4\\mathbf{x}$ faster than its fp32 counterparts and $2\\mathbf{x}$ faster than its FP16/BF16 counterparts [30, 37, 44, 48, 32]. With $\\mathrm{e}3\\mathrm{m}4$ data format used in forward and $\\mathrm{e}5\\mathrm{m}2$ format [41] in backward, pre-trained models can achieve minimum loss of accuracy while greatly boosting the efficiency of training. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusions and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study we discuss the importance of pruning continuity in effective 2:4 sparse pre-training. We analyse the drawback of traditional hard-thresholding pruning function and its variation (SR-STE) and argue that the main limits being discontinuity. Based on our analysis and soft-thresholding for channel pruning, we propose S-STE, which prunes weights in a continuous manner. Experiments show that our method surpasses previous state-of-the-art methods on a wide range of tasks. ", "page_idx": 9}, {"type": "text", "text": "Our proposed S-STE approach primarily targets linear layers within FFN networks. Nevertheless, QKV projection layers necessitate further exploration to devise an effective dynamic sparse training strategy that harmonizes with attention mechanisms. Furthermore, our current choice of continuous pruning function represents only one possible solution; alternative, smoother pruning functions may be necessary to achieve improved continuity and mitigate potential discontinuities. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank Ziteng Wang, Bingrui Li, Haocheng Xi, Kang Zhao and Jintao Zhang, Brian Chmiel and Daniel Soudry for valuable discussions. This work was supported by the National Key Research and Development Program of China (No. 2021ZD0110502) and NSFC Project (Nos. 62376131). J.Z is also supported by the XPlorer Prize. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Abhimanyu Rajeshkumar Bambhaniya, Amir Yazdanbakhsh, Suvinay Subramanian, Sheng-Chun Kao, Shivani Agrawal, Utku Evci, and Tushar Krishna. Progressive gradient flow for robust n:m sparsity training in transformers, 2024.   \n[2] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation, 2013.   \n[3] Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ales Tamchyna. Findings of the 2014 workshop on statistical machine translation. In WMT@ACL, 2014. URL https://api.semanticscholar.org/CorpusID:15535376.   \n[4] L\u00e9on Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning, 2018.   \n[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.   \n[6] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 15834\u201315846. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper_files/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf.   \n[7] Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, and Jingjing Liu. Earlybert: Efficient bert training via early-bird lottery tickets, 2021.   \n[8] Brian Chmiel, Itay Hubara, Ron Banner, and Daniel Soudry. Minimum variance unbiased n:m sparsity for the neural gradients. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=vuD2xEtxZcj.   \n[9] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation, 2020.   \n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255, 2009. doi: 10.1109/CVPR.2009.5206848.   \n[11] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 2943\u20132952. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/evci20a.html.   \n[12] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=rJl-b3RcF7.   \n[13] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 3259\u20133269. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/ frankle20a.html.   \n[14] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Stabilizing the lottery ticket hypothesis, 2020.   \n[15] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023.   \n[16] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019.   \n[17] Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for efficient neural networks, 2015.   \n[18] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, 2016.   \n[19] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In The IEEE International Conference on Computer Vision (ICCV), Oct 2017.   \n[20] Yuezhou Hu, Kang Zhao, Weiyu Huang, Jianfei Chen, and Jun Zhu. Accelerating transformer pre-training with 2:4 sparsity. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 19531\u201319543. PMLR, 21\u201327 Jul 2024. URL https://proceedings.mlr.press/v235/hu24r.html.   \n[21] Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph Naor, and Daniel Soudry. Accelerated sparse neural training: A provable and efficient method to find n:m transposable masks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 21099\u201321111. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/ b0490b85e92b64dbb5db76bf8fca6a82-Paper.pdf.   \n[22] Fran\u00e7ois Lagunas, Ella Charlaix, Victor Sanh, and Alexander Rush. Block pruning for faster transformers. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10619\u201310629, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.829. URL https://aclanthology.org/2021.emnlp-main.829.   \n[23] Mike Lasby, Anna Golubeva, Utku Evci, Mihai Nica, and Yani Ioannou. Dynamic sparse training with structured sparsity, 2023.   \n[24] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=B1VZqjAcYX.   \n[25] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ a96b65a721e561e1e3de768ac819ffbb-Paper.pdf.   \n[26] Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J. Dally. Deep gradient compression: Reducing the communication bandwidth for distributed training, 2020.   \n[27] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, and Lichao Sun. Sora: A review on background, technology, limitations, and opportunities of large vision models, 2024.   \n[28] Yucheng Lu, Shivani Agrawal, Suvinay Subramanian, Oleg Rybakov, Christopher De Sa, and Amir Yazdanbakhsh. STEP: Learning N:M structured sparsity masks from scratch with precondition. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 22812\u201322824. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/lu23c.html.   \n[29] Jaron Maene, Mingxiao Li, and Marie-Francine Moens. Towards understanding iterative magnitude pruning: Why lottery tickets win, 2021.   \n[30] Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, Naveen Mellempudi, Stuart Oberman, Mohammad Shoeybi, Michael Siu, and Hao Wu. Fp8 formats for deep learning, 2022.   \n[31] Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. Accelerating sparse deep neural networks, 2021.   \n[32] Badreddine Noune, Philip Jones, Daniel Justus, Dominic Masters, and Carlo Luschi. 8-bit numerical formats for deep neural networks, 2022.   \n[33] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim\u00f3n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, \u0141ukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, \u0141ukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David M\u00e9ly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O\u2019Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cer\u00f3n Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. ", "page_idx": 12}, {"type": "text", "text": "[34] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations, 2019. ", "page_idx": 12}, {"type": "text", "text": "[35] Manoj Kumar Pandey, Daniel P. Delorey, Qiuyi Duan, Lei Wang, Charles D. Knutson, Daniel Zappala, and Ryan Woodings. Ria: An rf interference avoidance algorithm for heterogeneous wireless networks. 2007 IEEE Wireless Communications and Networking Conference, pages 4051\u20134056, 2007. URL https: //api.semanticscholar.org/CorpusID:10798336.   \n[36] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040.   \n[37] Sergio P. Perez, Yan Zhang, James Briggs, Charlie Blake, Josh Levy-Kramer, Paul Balanca, Carlo Luschi, Stephen Barlow, and Andrew William Fitzgibbon. Training and inference of large language models using 8-bit floating point, 2023.   \n[38] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.   \n[39] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: $100{,}000{+}$ questions for machine comprehension of text. In Jian Su, Kevin Duh, and Xavier Carreras, editors, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://aclanthology.org/D16-1264.   \n[40] Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. A simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023.   \n[41] Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalakshmi Srinivasan, Xiaodong Cui, Wei Zhang, and K. Gopalakrishnan. Hybrid 8-bit floating point (hfp8) training and inference for deep neural networks. In Neural Information Processing Systems, 2019. URL https: //api.semanticscholar.org/CorpusID:202779157.   \n[42] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention, 2021.   \n[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[44] Mart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin, Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Markus Nagel, Joseph Soriaga, and Tijmen Blankevoort. Fp8 versus int8 for efficient deep learning inference, 2023.   \n[45] Antoine Vanderschueren and Christophe De Vleeschouwer. Are straight-through gradients and softthresholding all you need for sparse training?, 2022.   \n[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/ paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.   \n[47] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Tal Linzen, Grzegorz Chrupa\u0142a, and Afra Alishahi, editors, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\u2013355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446.   \n[48] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers, 2018.   \n[49] Lu Yin, Gen Li, Meng Fang, Li Shen, Tianjin Huang, Zhangyang \"Atlas\" Wang, Vlado Menkovski, Xiaolong Ma, Mykola Pechenizkiy, and Shiwei Liu. Dynamic sparsity is channel-level sparsity learner. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 67993\u201368012. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ d6d0e41e0b1ed38c76d13c9e417a8f1f-Paper-Conference.pdf.   \n[50] Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Richard G. Baraniuk, Zhangyang Wang, and Yingyan Lin. Drawing early-bird tickets: Towards more efficient training of deep networks, 2022.   \n[51] Yuxin Zhang, Yiting Luo, Mingbao Lin, Yunshan Zhong, Jingjing Xie, Fei Chao, and Rongrong Ji. Bidirectional masks for efficient N:M sparse training. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 41488\u2013 41497. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/zhang23ae.html.   \n[52] Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hongsheng Li. Learning n:m fine-grained structured sparse neural networks from scratch. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=K9bw7vqp_s.   \n[53] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee, TsangWei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, and Kehang Han. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Jie Tan, Marc Toussaint, and Kourosh Darvish, editors, Proceedings of The 7th Conference on Robot Learning, volume 229 of Proceedings of Machine Learning Research, pages 2165\u20132183. PMLR, 06\u201309 Nov 2023. URL https://proceedings.mlr.press/v229/zitkovich23a.html. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. We prove this by demonstrating $S_{s o f t}$ is continuous on every 4-element block. Of note, suppose $\\mathbf{a}=[a_{1},a_{2},a_{3},a_{4}]^{\\top}$ . Assume, without loss of generality, that $\\lvert a_{1}\\rvert\\leq\\lvert a_{2}\\rvert\\leq\\lvert a_{3}\\rvert\\leq\\lvert a_{4}\\rvert$ . Our goal is to prove $\\forall\\epsilon>0,\\exists\\delta>0,s.t.$ . when $|a_{1}^{\\prime}-a_{1}|<\\delta,|a_{2}^{\\prime}-\\dot{a}_{2}|<\\dot{\\delta},|\\dot{a}_{3}^{\\prime}-\\dot{a}_{3}|<\\dot{\\delta}$ and $|a_{4}^{\\prime}-$ $a_{4}|<\\delta,|(S_{s o f t}(\\mathbf{a}^{\\prime}))_{i}-a_{i}|<\\epsilon$ , where $\\mathbf{a}^{\\prime}=[a_{1}^{\\prime},a_{2}^{\\prime},a_{3}^{\\prime},a_{4}^{\\prime}]^{\\top}$ . ", "page_idx": 14}, {"type": "text", "text": "1) We start from the simplest case where $\\left|a_{1}\\right|<\\left|a_{2}\\right|<\\left|a_{3}\\right|<\\left|a_{4}\\right|$ . Then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nS_{s o f t}=[0,0,\\mathrm{sign}(a_{3})(|a_{3}|-|a_{2}|),\\mathrm{sign}(a_{4})(|a_{4}|-|a_{2}|)].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This order holds when ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\delta<\\frac{1}{2}\\operatorname*{min}\\{(|a_{2}|-|a_{1}|),|a_{3}|-|a_{2}|),|a_{4}|-|a_{3}|)\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, ", "page_idx": 14}, {"type": "equation", "text": "$$\nS_{s o f t}(\\mathbf{a}^{\\prime})=[0,0,\\mathrm{sign}(a_{3}^{\\prime})(|a_{3}^{\\prime}|-|a_{2}^{\\prime}|),\\mathrm{sign}(a_{4}^{\\prime})(|a_{4}^{\\prime}|-|a_{2}^{\\prime}|)].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The signs of $\\mathbf{a}$ is unchanged when ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\delta<\\operatorname*{min}\\{|a_{1}|,|a_{2}|,|a_{3}|,|a_{4}|\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{sign}(a_{4}^{\\prime})(|a_{4}^{\\prime}|-|a_{2}^{\\prime}|)-\\mathrm{sign}(a_{4})(|a_{4}|-|a_{2}|)}\\\\ &{\\leq|\\left|a_{4}^{\\prime}\\right|-|a_{2}^{\\prime}|-|a_{4}|+|a_{2}|\\left|\\right.}\\\\ &{\\leq|\\left.|a_{4}^{\\prime}|-|a_{4}|\\right|+|\\left|a_{2}^{\\prime}\\right|-|a_{2}|\\left|\\right.}\\\\ &{\\leq2\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Take $\\delta\\leq\\textstyle{\\frac{1}{2}}\\epsilon$ and this is done. It is similar to prove that $\\mathrm{sign}(a_{3}^{\\prime})(|a_{3}^{\\prime}|\\!-\\!|a_{2}^{\\prime}|)\\!-\\!\\mathrm{sign}(a_{3})(|a_{3}|\\!-\\!|a_{2}|)\\leq$ $\\epsilon$ using the same method. ", "page_idx": 14}, {"type": "text", "text": "2) We then consider the cases where there are two equivalents in a. If $\\left|a_{1}\\right|=\\left|a_{2}\\right|<\\left|a_{3}\\right|<\\left|a_{4}\\right|$ or $\\left|a_{1}\\right|<\\left|a_{2}\\right|<\\left|a_{3}\\right|=\\left|a_{4}\\right|$ , the proof should follow 1) as no flip happens. Thus we only consider the situation where $\\left|a_{1}\\right|<\\left|a_{2}\\right|=\\left|a_{3}\\right|<\\left|a_{4}\\right|$ . Under these circumstances, a flip will happen on the second and third dimensions of a. ", "page_idx": 14}, {"type": "equation", "text": "$$\nS_{s o f t}(\\mathbf{a})=[0,0,0,\\mathrm{sign}(a_{4})(|a_{4}|-|a_{2}|)]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Without loss of generality we assume $\\left|a_{1}^{\\prime}\\right|<\\left|a_{2}^{\\prime}\\right|\\leq\\left|a_{3}^{\\prime}\\right|<\\left|a_{4}^{\\prime}\\right|$ . Thus, ", "page_idx": 14}, {"type": "equation", "text": "$$\nS_{s o f t}(\\mathbf{a}^{\\prime})=[0,0,\\mathrm{sign}(a_{3}^{\\prime})(|a_{3}^{\\prime}|-|a_{2}^{\\prime}|),\\mathrm{sign}(a_{4}^{\\prime})(|a_{4}^{\\prime}|-|a_{2}^{\\prime}|)].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The proof of the fourth dimension is similar to 1), so we only focus on $a_{3}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{sign}(a_{3}^{\\prime})(|a_{3}^{\\prime}|-|a_{2}^{\\prime}|)}\\\\ &{\\leq|\\left|a_{3}^{\\prime}\\right|-|a_{2}^{\\prime}|\\left|\\right.}\\\\ &{\\leq|\\left.|a_{3}^{\\prime}|-|a_{3}|\\right|+|\\left|a_{2}^{\\prime}\\right|+\\left|a_{2}\\right|\\right|}\\\\ &{\\leq2\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Take $\\delta\\leq\\textstyle{\\frac{1}{2}}\\epsilon$ and this is done. ", "page_idx": 14}, {"type": "text", "text": "3) If there exists three or four equivalent in a, a flip will happen at the second and third dimension.   \nThus, these cases can be reduced to 1) or 2). ", "page_idx": 14}, {"type": "text", "text": "A.2 GLUE scores of GPT-2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "See Table 9. ", "page_idx": 14}, {"type": "text", "text": "A.3 Acceleration ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "See Table 10. ", "page_idx": 14}, {"type": "table", "img_path": "8abNCVJs2j/tmp/adb47b7a20db8d7f640a20c9b51181c8756b173539d89927229ce6c2bc33f5e9.jpg", "table_caption": ["Table 9: Comparison between GLUE scores of different pre-train methods on GPT-2 models. This table is the elaboration of Table 6. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 10: Pre-training acceleration ratio with different different batch size $N$ , sequence length $n$ , embedding dimension $d$ and heads number $h$ on single FFN block and transformer block of GPT-2 with RTX 3090 GPUs. ", "page_idx": 15}, {"type": "table", "img_path": "8abNCVJs2j/tmp/cbb37976dc2892ba54288aa4c039ff3489ae962aa9f52737e6adbef25bbc191c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.4 Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As we propose accuracy results of S-STE on several tasks, no actual acceleration result is given. While theoretically $2\\mathbf{x}$ faster results can be expected (FP8 quantization), the NVIDIA acceleration library (cuSPARSElt [31]) is not satisfactory, which causes inconvenience on implementation. The peak FLOPS of 2:4-spMM is lower than theoretical GEMM FLOPS; see Table 11. ", "page_idx": 15}, {"type": "table", "img_path": "8abNCVJs2j/tmp/1cb692298920eed853c313b1cc6180e50a7039e196ca52e25fdcaba5769e924d.jpg", "table_caption": ["Table 11: Peak FLOPS of general matrix multiplications (GEMMs) and 2:4 sparse matrix multiplications (2:4-spMMs) on H100. The size we take to test is $16384\\times16384\\times16384$ . "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "8abNCVJs2j/tmp/4b01df7e5ef4395eece2629a4ae465ba99b9169cc150a0f48c18980f8e51665c.jpg", "table_caption": ["Table 12: GPU Hours of pre-training models on RTX 4090. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.5 Broader Impact ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "S-STE can be used mainly to accelerate the pre-training stage of large-scale networks, like LLaMA [43] and GPT-4 [33]. Theoretically GEMMs of the FFN layer can be accelerated up to $4\\mathbf{x}$ faster than FP16 dense models, which would greatly reduce the electric power consumption of pre-training modern large-scale models. However, this method may also used for some models that is noncompliance with regulations and ethics, such as models that generate discriminatory contents. ", "page_idx": 16}, {"type": "text", "text": "A.6 Experiments compute resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To replicate our experiments, we provide the estimated GPU hours of each setting; see Table 12. ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Both the abstract and beginning part of introduction indicate the scope of the paper\u2019s contributions. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Limitations are discussed in Appendix. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Please refer to Appendix. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: Result can be reproduced according to \u201cmethodology\u201d and \u201cexperiments\u201d chapters. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Source code is provided in supplementary materials. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Please refer to \u201cexperiments\u201d chapter. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: For those experiments with large errors, we provide mean and standard deviation values of the results, like GLUE. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Please refer to Appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Please refer to Appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper doesn\u2019t release any new models, generators or datasets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Please refer to the \u201cexperiments\u201d chapter. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Please refer to README.md in our code submission. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]