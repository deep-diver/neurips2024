{"references": [{"fullname_first_author": "Yoshua Bengio", "paper_title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "publication_date": "2013-00-00", "reason": "This paper introduces the Straight-Through Estimator (STE), a crucial technique used in training sparse neural networks, which is the central focus of the current paper."}, {"fullname_first_author": "Jonathan Frankle", "paper_title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks", "publication_date": "2019-00-00", "reason": "This paper introduces the Lottery Ticket Hypothesis, a fundamental concept in neural network pruning that significantly impacts the field of sparse training and is directly relevant to the current research."}, {"fullname_first_author": "Brian Chmiel", "paper_title": "Minimum variance unbiased n:m sparsity for the neural gradients", "publication_date": "2023-00-00", "reason": "This paper proposes the Minimum Variance Unbiased Estimator (MVUE) for accelerating sparse training, a key optimization technique adopted and analyzed in the current work."}, {"fullname_first_author": "Antoine Vanderschueren", "paper_title": "Are straight-through gradients and soft-thresholding all you need for sparse training?", "publication_date": "2022-00-00", "reason": "This paper explores the use of soft-thresholding for sparse training, providing a foundation for the continuous pruning function developed in the current research."}, {"fullname_first_author": "Yuezhou Hu", "paper_title": "Accelerating transformer pre-training with 2:4 sparsity", "publication_date": "2024-07-21", "reason": "This is a closely related work by the same authors that builds upon previous research on sparse training and is directly compared against in this paper."}]}