[{"figure_path": "8abNCVJs2j/tables/tables_4_1.jpg", "caption": "Table 1: Validation loss and test accuracy of S-STE with different \u03b3 on Transformer-base.", "description": "This table presents the results of experiments conducted using the S-STE method on a Transformer-base model.  The experiment varied a hyperparameter, \u03b3,  and measured the resulting validation loss and test BLEU score.  The table helps demonstrate the impact of this hyperparameter on model performance, allowing readers to determine the optimal \u03b3 value for this specific model and task. ", "section": "4 Methodology"}, {"figure_path": "8abNCVJs2j/tables/tables_6_1.jpg", "caption": "Table 2: Experimental result of different \u03b2 on Transformer-base.", "description": "This table presents the experimental results of different beta (\u03b2) values on the Transformer-base model. It compares three different \u03b2 recipes: no scaling, keeping L1-norm same, and minimizing MSE.  The results show the Test BLEU score, validation loss, and average epoch loss for each \u03b2 recipe.  The minimizing MSE recipe (S-STE) is shown to yield the best results.", "section": "4.2 Fixed weight rescaling \u03b2"}, {"figure_path": "8abNCVJs2j/tables/tables_6_2.jpg", "caption": "Table 3: Results of different MVUE strategies on GPT-2 774M with 4000 steps. Sparsifying S(W) introduces huge loss of accuracy while sparsifying \u2207z is acceptable with little loss.", "description": "This table compares different strategies for using Minimum-Variance Unbiased Estimation (MVUE) during the training of a GPT-2 774M model with 2:4 sparsity.  It shows the impact on training loss when MVUE is applied to either the sparse weight matrix S(W) or the gradient \u2207z. The results indicate that applying MVUE to the gradient significantly outperforms applying it to the sparse weight matrix.", "section": "5 Other implementation skills"}, {"figure_path": "8abNCVJs2j/tables/tables_7_1.jpg", "caption": "Table 4: Experimental Results for Transformer-base on En-De dataset.", "description": "This table presents the experimental results of training a Transformer-base model on the En-De dataset for machine translation. It compares the performance of four different methods: Dense (full-parameter model), SR-STE, STEP, and the proposed S-STE method. The metrics used for evaluation are average epoch loss, Test BLEU score, Val BLEU score, and validation loss. The results show that S-STE outperforms SR-STE and STEP, achieving BLEU scores closer to the Dense model baseline.", "section": "6 Experiments"}, {"figure_path": "8abNCVJs2j/tables/tables_7_2.jpg", "caption": "Table 5: Experimental Results for DeiT-small on ImageNet-1k. The Bi-Mask and SR-STE results are from [51].", "description": "This table presents the results of image classification experiments using the DeiT-small model on the ImageNet-1k dataset.  It compares the test accuracy (top-1 and top-5) achieved by three different methods: a dense model (full-weight model), SR-STE (Sparse-Regularized Straight-Through Estimator), and the proposed S-STE method. The results show the relative performance of S-STE compared to existing sparse training techniques.", "section": "6 Experiments"}, {"figure_path": "8abNCVJs2j/tables/tables_8_1.jpg", "caption": "Table 6: SQUAD and GLUE scores of different sizes and pre-training methods on GPT-2. We use 2:4 sparse weights to evaluate S-STE model, while dense parameters to evaluate the rest. Of note, SR-STE denotes the original SR-STE workflow (without backward MVUE), and \u201cT-SR-STE+DF", "description": "This table compares the performance of different pre-training methods (Dense, T-SR-STE+DF, T-SR-STE, SR-STE, and S-STE) on GPT-2 models of various sizes (124M, 350M, and 774M).  The evaluation metrics are SQUAD (Exact Match and F1 score) and GLUE (average score).  S-STE uses 2:4 sparse weights, while the other methods use dense weights for evaluation.  Note that \"T-SR-STE+DF\" represents a combination of transposable SR-STE, backward MVUE, and sparse-dense training workflow.  The S-STE results use backward MVUE and FP8 training.", "section": "6 Experiments"}, {"figure_path": "8abNCVJs2j/tables/tables_8_2.jpg", "caption": "Table 7: Different fine-tuning results on GLUE and SQUAD.", "description": "This table shows the results of fine-tuning on the GLUE and SQUAD benchmarks using different pre-training and fine-tuning methods.  The pre-training methods are S-STE (smooth straight-through estimator) and hard-thresholding. The fine-tuning methods are S-STE and hard-thresholding. The average score is reported for each combination of pre-training and fine-tuning methods.", "section": "6 Experiments"}, {"figure_path": "8abNCVJs2j/tables/tables_8_3.jpg", "caption": "Table 8: Experimental result of S-STE (soft-thresholding and weight rescaling), MVUE and FP8 training with DeiT-small on ImageNet-1K.", "description": "This table presents the ablation study results for the DeiT-small model trained on the ImageNet-1K dataset. It shows the impact of different components of the proposed S-STE method (soft-thresholding, weight rescaling, MVUE, and FP8 training) on the model's performance (measured by top-1 and top-5 accuracy). Each row represents a different combination of these components, allowing researchers to evaluate their individual and combined effects on the model's accuracy.  The results highlight the contribution of each component to the overall performance of the model.", "section": "5 Other implementation skills"}, {"figure_path": "8abNCVJs2j/tables/tables_15_1.jpg", "caption": "Table 6: SQUAD and GLUE scores of different sizes and pre-training methods on GPT-2. We use 2:4 sparse weights to evaluate S-STE model, while dense parameters to evaluate the rest. Of note, SR-STE denotes the original SR-STE workflow (without backward MVUE), and \u201cT-SR-STE+DF\u201d denotes the combination of transposable SR-STE & backward MVUE & sparse-dense training workflow, proposed by Hu et al. [20]. S-STE settings here include backward MVUE & FP8 training.", "description": "This table compares the performance of different pre-training methods (dense, T-SR-STE+DF, T-SR-STE, SR-STE, and S-STE) on GPT-2 models of varying sizes (124M, 350M, and 774M).  The evaluation is done using both sparse (2:4) and dense weights.  The results are presented in terms of GLUE and SQUAD scores, providing a comprehensive assessment of the various methods' effectiveness.  It highlights the impact of different components (backward MVUE, FP8 training) on S-STE's performance.", "section": "Generative language models"}, {"figure_path": "8abNCVJs2j/tables/tables_15_2.jpg", "caption": "Table 10: Pre-training acceleration ratio with different different batch size N, sequence length n, embedding dimension d and heads number h on single FFN block and transformer block of GPT-2 with RTX 3090 GPUs.", "description": "This table shows the pre-training and inference acceleration ratios achieved by the proposed S-STE method on a GPT-2 model using RTX 3090 GPUs.  It demonstrates the impact of varying batch size (N), sequence length (n), embedding dimension (d), and number of heads (h) on the acceleration ratios for both the Feed-Forward Network (FFN) layer and the overall GPT-2 transformer block. The results indicate speedups achieved by S-STE in pre-training and inference scenarios.", "section": "A.3 Acceleration"}, {"figure_path": "8abNCVJs2j/tables/tables_15_3.jpg", "caption": "Table 11: Peak FLOPS of general matrix multiplications (GEMMs) and 2:4 sparse matrix multiplications (2:4-spMMs) on H100. The size we take to test is 16384 \u00d7 16384 \u00d7 16384.", "description": "This table shows the peak FLOPS (floating point operations per second) for general matrix multiplication (GEMM) and 2:4 sparse matrix multiplication (2:4-spMM) on two different versions of the NVIDIA H100 GPU (PCIe and SXM).  It highlights the difference between theoretical peak performance and the actual performance achieved using the cuSPARSElt library, demonstrating the limitations in achieving the full theoretical speedup for sparse matrix operations.", "section": "A.4 Limitations"}, {"figure_path": "8abNCVJs2j/tables/tables_15_4.jpg", "caption": "Table 12: GPU Hours of pre-training models on RTX 4090.", "description": "This table presents the estimated GPU hours required for pre-training different models on RTX 4090 GPUs.  The models include various sizes of GPT-2 (124M, 350M, and 774M parameters), Transformer-base, and DeiT-base.  These estimates are useful for researchers who want to reproduce the experiments in the paper or assess the computational resources needed for similar pre-training tasks.", "section": "A.6 Experiments compute resources"}]