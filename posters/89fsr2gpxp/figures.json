[{"figure_path": "89fSR2gpxp/figures/figures_7_1.jpg", "caption": "Figure 1: Plots of OBD performance, represented by the normalized returns of policies trained on synthetic data, as functions of distillation steps on (a) Halfcheetah; (b) Hopper; and (c) Walker2D environment. Each curve is averaged over five random seeds.", "description": "This figure visualizes the performance of Offline Behavior Distillation (OBD) methods across three different environments (Halfcheetah, Hopper, and Walker2D) and three data quality levels (medium-replay, medium, and medium-expert).  The plots show the normalized returns achieved by policies trained using the synthetic data generated by different OBD algorithms (DBC, PBC, and Av-PBC) as the number of distillation steps increases. Each line represents the average performance across five different random seeds, providing insights into the convergence speed and overall performance of each method in various settings.", "section": "5 Experiments"}, {"figure_path": "89fSR2gpxp/figures/figures_14_1.jpg", "caption": "Figure 2: Examples of distilled behavioral data. The top row shows the distilled states, while the bottom row presents the subsequent state following the execution of the corresponding distilled actions within the environment.", "description": "This figure shows examples of distilled behavioral data for the Halfcheetah environment.  The top row displays a series of distilled states (states selected for inclusion in the distilled dataset).  The bottom row shows the corresponding next states, resulting from taking the distilled action in each of the preceding distilled states. The purpose of this illustration is to demonstrate that the distillation process prioritizes what the authors term \"critical\" or \"imbalanced\" states (those where the cheetah is not in a balanced posture), and that taking the distilled actions results in subsequent states that move towards more balanced states. This is intended to provide insight into how the algorithm selects data and its implications for the explainability of the reinforcement learning process.", "section": "D Examples of Distilled Data"}]