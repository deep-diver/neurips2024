[{"figure_path": "5BwWgyvgwR/figures/figures_1_1.jpg", "caption": "Figure 1: (1) On the left, we leverage the multi-modality model ImageBind [1] to quantify the similarity of images and modalities across datasets, i.e., GTA5-Synthetic [2], Dark Zurich-Nighttime [3], ACDC-Snow [4], DELIVER-Depth [5], FMB-Infrared [6], and DSEC-Event [7]. Specifically, we randomly select 500 samples from each dataset, and compute the average cosine similarity of the output vectors within the dataset (right side of the text) and between the datasets (on the arrows). (2) On the right, we compare the quantitative results with the state-of-the-art (SoTA) method Rein [8] on three different modalities.", "description": "This figure demonstrates the use of ImageBind to measure the similarity between different image domains and modalities (depth, infrared, event).  The left side shows the similarity scores, while the right side compares the quantitative segmentation results of the proposed MADM method against a state-of-the-art (Rein) method across three different modalities.", "section": "Introduction"}, {"figure_path": "5BwWgyvgwR/figures/figures_3_1.jpg", "caption": "Figure 2: Our framework is divided into three parts. (1) Self-Training: Supervised loss in the source modality Ls and pseudo-labeled loss Lt in the target modality are used to train the network. (2) Diffusion-based Pseudo-Label Generation (DPLG): In the early stage of training, we add noise on the latent representation zt to stabilize the pseudo-label generation. (3) Label Palette and Latent Regression (LPLR): The one-hot encoded labels ys/\u0177t are converted to RGB form by palette and then encoded to the latent space to supervise the UNet output Os/t.", "description": "This figure illustrates the MADM framework, which consists of three main parts: Self-Training, Diffusion-based Pseudo-Label Generation (DPLG), and Label Palette and Latent Regression (LPLR).  Self-Training uses both supervised loss from the source modality and pseudo-labeled loss from the target modality to train the network. DPLG adds noise to the latent representation of target samples to stabilize pseudo-label generation. LPLR converts one-hot encoded labels into RGB form using a palette, encodes them into the latent space, and uses them to supervise the UNet output, resulting in high-resolution features.", "section": "3 Method"}, {"figure_path": "5BwWgyvgwR/figures/figures_4_1.jpg", "caption": "Figure 3: We visualize the pseudo-labels for event modality at the iteration of 1250, 1750, and 2250. The introduction of DPLG effectively improves the quality of pseudo-labels.", "description": "This figure visualizes the impact of Diffusion-based Pseudo-Label Generation (DPLG) on the quality of pseudo-labels generated for the event modality at different training iterations (1250, 1750, and 2250).  It demonstrates how the addition of noise in DPLG stabilizes the generation of pseudo-labels and improves their accuracy over time, as indicated by a more consistent and detailed representation of the scene compared to pseudo-labels generated without DPLG.", "section": "3.2 Diffusion-based Pseudo-Label Generation"}, {"figure_path": "5BwWgyvgwR/figures/figures_7_1.jpg", "caption": "Figure 4: Qualitative semantic segmentation results generated by SoTA methods MIC [18], Rein [8], and our proposed MADM on three modalities.", "description": "This figure provides a qualitative comparison of semantic segmentation results obtained using three different methods: MIC [18], Rein [8], and the proposed MADM.  The results are shown for three different modalities (image, depth, and infrared), with the input image, segmentation results from each method, and the ground truth label displayed side-by-side for comparison.  The figure highlights the superior performance of MADM in terms of accuracy and detail preservation, especially in challenging areas (as highlighted by the yellow boxes). The differences in performance demonstrate the effectiveness of MADM's approach for cross-modality semantic segmentation.", "section": "4 Experiments"}, {"figure_path": "5BwWgyvgwR/figures/figures_8_1.jpg", "caption": "Figure 5: At the 1,250th iteration, we present a visual analysis of diffusion step k in DPLG.", "description": "This figure shows a visual analysis of the effect of the diffusion step parameter (k) in the Diffusion-based Pseudo-Label Generation (DPLG) method at iteration 1250.  It demonstrates how different noise levels affect the quality of pseudo-label generation during the training process. As k increases, more noise is added to the latent representation of the target sample.  The images show the progressive changes in segmentation results as the noise level increases. At k=0, no noise is added, and the segmentation is inaccurate. As k increases, the segmentation becomes more accurate until it reaches an optimal level, after which adding too much noise (high k values) again leads to degraded segmentation results.", "section": "4.5 Diffusion-based Pseudo-Label Generation"}, {"figure_path": "5BwWgyvgwR/figures/figures_8_2.jpg", "caption": "Figure 8: Visualization of the output of VAE decoder (Regression) and segmentation head (Classification).", "description": "This figure visualizes the outputs of the VAE decoder and the segmentation head at different stages of training, showing how the regression results (from the VAE decoder) become progressively clearer and more detailed as the model converges.  This improvement in detail helps the segmentation head produce more accurate semantic segmentations. The comparison between regression and classification outputs highlights how LPLR improves the accuracy of semantic segmentation by enhancing the resolution of the features.", "section": "A.1 Visualization of LPLR"}, {"figure_path": "5BwWgyvgwR/figures/figures_9_1.jpg", "caption": "Figure 7: Visualization of daytime RGB images in Cityscapes dataset [13] \u2192 nighttime RGB and Infrared modalities in FMB dataset [6]", "description": "This figure visualizes the performance of the proposed method (MADM) on nighttime images. It compares the segmentation results of daytime RGB images from the Cityscapes dataset with nighttime RGB and infrared images from the FMB dataset.  The results show that the infrared modality provides superior performance, particularly in identifying pedestrians, due to the thermal differences highlighted by infrared. This showcases the adaptability of MADM to different visual modalities and its effectiveness in handling low-light conditions.", "section": "4.7 Benefits of MADM in Nighttime Datasets"}, {"figure_path": "5BwWgyvgwR/figures/figures_13_1.jpg", "caption": "Figure 8: Visualization of the output of VAE decoder (Regression) and segmentation head (Classification).", "description": "This figure visualizes the results of the VAE decoder (Regression) and the segmentation head (Classification) at different training time steps.  It demonstrates how the quality of the regression results improves over time, starting blurry and becoming progressively clearer as the model converges.  This shows that the VAE decoder is successfully learning to upsample the latent representation, providing the segmentation head with higher-resolution features for more accurate semantic segmentation.", "section": "A.1 Visualization of LPLR"}]