[{"figure_path": "nbqvjkOs6S/tables/tables_1_1.jpg", "caption": "Table 1: Comparison of deep generative models. DMs have overcome the generative learning trilemma [50], but they still lack invertibility compared to other models. In particular, LDMs have necessitated additional decoder inversion that has traditionally been addressed through memory-intensive and time-consuming gradient-based optimization methods. Here, we propose a method for efficiently (i.e., gradient-free) ensuring invertibility in LDM.", "description": "This table compares various deep generative models (GANs, NFs, and DMs) based on three key aspects of generative learning: high-quality samples, diversity, and fast sampling.  It highlights that while Diffusion Models (DMs) have solved the generative learning trilemma, they still lack the invertibility property compared to other models. Latent Diffusion Models (LDMs), a specific type of DM, require additional decoder inversion, which has traditionally been computationally expensive. The paper proposes a new efficient, gradient-free method to address this limitation in LDMs.", "section": "1 Introduction"}, {"figure_path": "nbqvjkOs6S/tables/tables_7_1.jpg", "caption": "Table 2: Our gradient-free decoder inversion method achieves the tree-rings watermark [48] classification performance comparable to a gradient-based method while significantly reducing memory usage and runtime, in two different scenarios.", "description": "This table presents a comparison of the performance of three different decoder inversion methods: Encoder, Gradient-based, and Gradient-free (ours) on two different Latent Diffusion Models (LDMs): Stable Diffusion 2.1 and InstaFlow. The metrics compared are Accuracy, Peak memory usage (GB), and runtime (s). The results show that the proposed Gradient-free method achieves comparable accuracy to the Gradient-based method while significantly reducing both memory usage and runtime.", "section": "5.1 Tree-rings watermarking for image generation"}, {"figure_path": "nbqvjkOs6S/tables/tables_9_1.jpg", "caption": "Table 1: Comparison of deep generative models. DMs have overcome the generative learning trilemma [50], but they still lack invertibility compared to other models. In particular, LDMs have necessitated additional decoder inversion that has traditionally been addressed through memory-intensive and time-consuming gradient-based optimization methods. Here, we propose a method for efficiently (i.e., gradient-free) ensuring invertibility in LDM.", "description": "This table compares several deep generative models (GAN, NF, and DM) based on three criteria of the generative learning trilemma (high-quality samples, diversity, and fast sampling) and their invertibility.  It highlights that while Diffusion Models (DMs) have solved the trilemma, they lack invertibility, especially Latent Diffusion Models (LDMs), which necessitate a decoder inversion process. The authors propose a gradient-free method to efficiently address the invertibility issue in LDMs.", "section": "1 Introduction"}, {"figure_path": "nbqvjkOs6S/tables/tables_16_1.jpg", "caption": "Table S1: We can further improve the reconstruction quality, by using a cosine learning rate scheduler with warm-up steps (if the number of iterations or the runtime is predetermined). We conducted experiments of the decoder inversion in various LDMs, with all other conditions being the same to that in Fig. 3, but using a relatively scheduled learning rate at each iteration. \u00b1 represents the 95% confidence interval.", "description": "This table presents the results of decoder inversion experiments conducted on three different latent diffusion models (LDMs): Stable Diffusion 2.1, LaVie, and InstaFlow.  The experiments compare the performance of a gradient-based method and a gradient-free method proposed in the paper.  Different hyperparameters were tested, including different learning rates (0.1 scheduled and 0.01 fixed) and number of iterations (20, 30, 50, 100, 200). The table shows NMSE (Noise reconstruction Mean Squared Error) in dB, number of iterations, runtime in seconds, and peak GPU memory usage in GB for each experiment.  The \u00b1 values indicate the 95% confidence intervals.", "section": "A.2 Experiment details and more results"}, {"figure_path": "nbqvjkOs6S/tables/tables_16_2.jpg", "caption": "Table 2: Our gradient-free decoder inversion method achieves the tree-rings watermark [48] classification performance comparable to a gradient-based method while significantly reducing memory usage and runtime, in two different scenarios.", "description": "This table compares the performance of three different decoder inversion methods (Encoder, Gradient-based [14], and Gradient-free (ours)) for tree-rings watermark classification in two different Latent Diffusion Models (LDMs).  The metrics compared are accuracy, peak memory usage (in GB), and runtime (in seconds). The results demonstrate that the gradient-free method achieves comparable accuracy to the gradient-based method while significantly reducing both memory usage and runtime.", "section": "5.1 Tree-rings watermarking for image generation"}, {"figure_path": "nbqvjkOs6S/tables/tables_16_3.jpg", "caption": "Table S1: We can further improve the reconstruction quality, by using a cosine learning rate scheduler with warm-up steps (if the number of iterations or the runtime is predetermined). We conducted experiments of the decoder inversion in various LDMs, with all other conditions being the same to that in Fig. 3, but using a relatively scheduled learning rate at each iteration. \u00b1 represents the 95% confidence interval.", "description": "This table presents the results of decoder inversion experiments conducted on three different latent diffusion models (LDMs): Stable Diffusion 2.1, LaVie, and InstaFlow.  The experiments compare the performance of gradient-based and gradient-free decoder inversion methods using different bit precisions (16-bit and 32-bit), learning rates (fixed and scheduled), and numbers of iterations. The metrics reported include NMSE (noise reconstruction mean squared error), the number of iterations, runtime, and peak GPU memory usage. The table provides a detailed comparison to highlight the efficiency gains achieved by the proposed gradient-free method, particularly in terms of reduced memory consumption and runtime, while maintaining comparable accuracy.", "section": "A.2 Experiment details and more results"}, {"figure_path": "nbqvjkOs6S/tables/tables_18_1.jpg", "caption": "Table S2: Ablation studies on optimizer and learning rate scheduling. Adam showed the best result compared to vanilla and KM iterations method. The learning scheduling we used showed consistently good performance across all intervals.", "description": "This table presents the results of ablation studies conducted to evaluate the impact of different optimizers (Vanilla, KM iterations, Adam) and learning rate scheduling strategies on the performance of the decoder inversion method.  The experiment was performed with varying numbers of iterations (20, 30, 50, 100).  The results, measured by NMSE (dB), show that the Adam optimizer consistently outperforms the other methods across all iteration counts, and the learning rate scheduling strategy employed yielded consistently good results across different iteration numbers.", "section": "A.2.3 Ablation Studies"}, {"figure_path": "nbqvjkOs6S/tables/tables_18_2.jpg", "caption": "Table S2: Ablation studies on optimizer and learning rate scheduling. Adam showed the best result compared to vanilla and KM iterations method. The learning scheduling we used showed consistently good performance across all intervals.", "description": "This table presents the results of ablation studies conducted to evaluate the impact of different optimizers (vanilla, KM iterations, Adam) and learning rate scheduling methods on the performance of the decoder inversion.  The results, measured in NMSE (dB), are shown for different numbers of iterations (20, 30, 50, 100).  The study demonstrates that the Adam optimizer with the original learning rate scheduling achieves the best performance across all iteration counts.", "section": "A.2.3 Ablation Studies"}]