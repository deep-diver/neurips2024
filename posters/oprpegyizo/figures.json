[{"figure_path": "OPrPegYIZo/figures/figures_1_1.jpg", "caption": "Figure 1: (Left) The graphical model for a DLCMDP. The transition dynamics of the environment follows T(st+1,Mt+1 | St,at,mt). At every timestep t, an i.i.d. Bernoulli random variable, dt, denotes the change in the latent context, mt. Blue shaded variables are observed and white shaded variables are latent. (Right) A DLCMDP rollout. Each session i is governed by a latent variable m\u00b2 which is changing between sessions according to a fixed transition function, Tm(m' | m). We denote li as the length of session i. The state-action pair (s\u012f, a) at timestep t in session i is summarized into a single observed variable, x. We emphasize that session terminations are not explicitly observed.", "description": "This figure shows a graphical model and rollout of a dynamic latent contextual Markov decision process (DLCMDP). The left panel displays the model's structure, highlighting the relationships between observed and latent variables such as states, actions, rewards, and latent contexts which change over time.  The right panel illustrates a rollout of the DLCMDP, showing how sessions are defined by periods where the latent context remains fixed.  Each session has a variable length, and transitions between sessions are governed by a latent transition function.", "section": "Dynamic Latent Contextual MDPs"}, {"figure_path": "OPrPegYIZo/figures/figures_2_1.jpg", "caption": "Figure 2: VariBAD does not model the latent context dynamics and fails to adapt to the changing goal location. By contrast, DynaMITE-RL correctly infers the transition and consistently reaches the rewarding cell (green cross).", "description": "This figure compares the performance of VariBAD and DynaMITE-RL on a gridworld task where the goal location changes between sessions.  VariBAD, which does not model the latent context dynamics, fails to adapt to these changes and gets stuck after reaching the goal in the first session. In contrast, DynaMITE-RL correctly infers the goal location transition and consistently reaches the goal in each session.", "section": "3 Dynamic Latent Contextual MDPs"}, {"figure_path": "OPrPegYIZo/figures/figures_3_1.jpg", "caption": "Figure 3: Pseudo-code (online RL training) and model architecture of DynaMITE-RL.", "description": "This figure shows the pseudocode for online training of DynaMITE-RL. The algorithm takes as input the environment, policy, critic, and belief model. It then collects DLCMDP episodes, trains the posterior belief model by maximizing ELBO, and trains the policy and critic with any online RL algorithm. The figure also shows the model architecture of DynaMITE-RL. The architecture consists of an inference network that takes as input the history of observations and outputs the posterior distribution over the latent context variables. The dynamics model takes as input the current state, action, and latent context and outputs the next state. The reward model takes as input the current state, action, and latent context and outputs the reward. The policy takes as input the current state and latent context and outputs the action. The critic takes as input the current state, action, and latent context and outputs the value.", "section": "4 DynaMITE-RL"}, {"figure_path": "OPrPegYIZo/figures/figures_5_1.jpg", "caption": "Figure 4: The environments considered in evaluating DynaMITE-RL. Each environment exhibits some change in reward and/or dynamics between sessions including changing goal locations (left and middle left), changing target velocities (middle right), and evolving user preferences of itch location (right).", "description": "This figure shows four different reinforcement learning environments used to evaluate the DynaMITE-RL algorithm.  The environments are designed to have changes in reward or dynamics between sessions, simulating real-world scenarios with non-stationary aspects.  Specifically, the environments show (from left to right): a grid world with changing goal locations, a robotic arm reaching task with changing goal locations, a simulated cheetah running task with varying target velocities, and a robotic arm assisting a human in scratching an itch (where the location of the itch changes).  These diverse scenarios showcase the ability of the DynaMITE-RL algorithm to adapt to dynamically changing environments.", "section": "Experiments"}, {"figure_path": "OPrPegYIZo/figures/figures_6_1.jpg", "caption": "Figure 5: Learning curves for DynaMITE-RL and state-of-the-art baseline methods. Shaded areas represent standard deviation over 5 different random seeds for each method and 3 for ScratchItch. In each of the evaluation environments, we observe that DynaMITE-RL exhibits better sample efficiency and converges to a policy with better environment returns than the baseline methods.", "description": "This figure shows the learning curves for DynaMITE-RL and several baseline methods across four different environments.  The y-axis represents the episode reward, and the x-axis represents the number of environment steps. Shaded regions indicate the standard deviation across multiple random seeds.  The results demonstrate that DynaMITE-RL achieves higher rewards and converges faster than the baselines in all four environments, highlighting its improved sample efficiency and performance.", "section": "Experiments"}, {"figure_path": "OPrPegYIZo/figures/figures_7_1.jpg", "caption": "Figure 5: Learning curves for DynaMITE-RL and state-of-the-art baseline methods. Shaded areas represent standard deviation over 5 different random seeds for each method and 3 for ScratchItch. In each of the evaluation environments, we observe that DynaMITE-RL exhibits better sample efficiency and converges to a policy with better environment returns than the baseline methods.", "description": "This figure compares the learning curves of DynaMITE-RL against several state-of-the-art baseline methods across four different environments.  The y-axis represents the episode reward, and the x-axis represents the number of environment steps. Shaded regions indicate the standard deviation across multiple random seeds. DynaMITE-RL consistently shows faster convergence and higher rewards compared to the baselines, highlighting its improved sample efficiency and superior performance.", "section": "5 Experiments"}, {"figure_path": "OPrPegYIZo/figures/figures_8_1.jpg", "caption": "Figure 7: Ablation studies on various frequencies of latent context switches within an episode in the HalfCheetah-Vel environment. The boxplot shows the distribution over evaluation returns for 25 rollouts of trained policies with VariBAD and DynaMITE-RL. When p = 0, we have a latent MDP and when p = 1 this is equivalent to a general POMDP.", "description": "This figure shows the ablation study on various frequencies of latent context switches within an episode for the HalfCheetah-Vel environment.  The boxplot displays the distribution of evaluation returns for 25 rollouts of trained policies using both VariBAD and DynaMITE-RL. The x-axis represents the Bernoulli probability (p) of a context switch at each time step.  When p=0, there are no context switches (latent MDP), and when p=1, there's a context switch at every step (general POMDP). The figure demonstrates the performance of both algorithms under varying degrees of latent context dynamism.", "section": "5 Experiments"}, {"figure_path": "OPrPegYIZo/figures/figures_15_1.jpg", "caption": "Figure 3: Pseudo-code (online RL training) and model architecture of DynaMITE-RL.", "description": "This figure shows the pseudocode for the online RL training algorithm of DynaMITE-RL and its model architecture.  The pseudocode outlines the steps involved in training the policy and critic using any online RL algorithm, collecting DLCMDP episodes, and training a posterior belief model. The architecture diagram illustrates the components of DynaMITE-RL: the inference network, dynamics model, reward model, policy, and critic. The inference network estimates the latent context variable and session termination. The dynamics and reward models predict the next state and reward given the current state, action, and latent context. The policy selects actions, and the critic evaluates the actions taken.", "section": "4 DynaMITE-RL"}, {"figure_path": "OPrPegYIZo/figures/figures_15_2.jpg", "caption": "Figure 5: Learning curves for DynaMITE-RL and state-of-the-art baseline methods. Shaded areas represent standard deviation over 5 different random seeds for each method and 3 for ScratchItch. In each of the evaluation environments, we observe that DynaMITE-RL exhibits better sample efficiency and converges to a policy with better environment returns than the baseline methods.", "description": "This figure compares the learning curves of DynaMITE-RL and several state-of-the-art baseline methods across four different environments.  The shaded regions represent the standard deviation over multiple runs, highlighting the consistency of DynaMITE-RL's superior performance.  DynaMITE-RL demonstrates better sample efficiency (faster learning) and achieves higher average rewards compared to the baselines.", "section": "5 Experiments"}]