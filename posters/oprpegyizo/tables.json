[{"figure_path": "OPrPegYIZo/tables/tables_6_1.jpg", "caption": "Table 1: Average single episode returns for DynaMITE-RL and other state-of-the-art meta-RL algorithms across different environments. Results for all environments are averaged across 5 seeds beside ScratchItch which has 3 seeds. DynaMITE-RL, in bold, achieves the highest return on all of the evaluation environments and is the only method able to recover an optimal policy.", "description": "This table presents the average single episode returns achieved by DynaMITE-RL and several other state-of-the-art meta-reinforcement learning algorithms across a diverse set of environments.  The results highlight DynaMITE-RL's superior performance, consistently achieving the highest average return and uniquely recovering the optimal policy in each environment.  The number of seeds used for averaging varied slightly between environments.", "section": "Experiments"}, {"figure_path": "OPrPegYIZo/tables/tables_8_1.jpg", "caption": "Table 1: Average single episode returns for DynaMITE-RL and other state-of-the-art meta-RL algorithms across different environments. Results for all environments are averaged across 5 seeds beside ScratchItch which has 3 seeds. DynaMITE-RL, in bold, achieves the highest return on all of the evaluation environments and is the only method able to recover an optimal policy.", "description": "This table presents the average single episode returns achieved by DynaMITE-RL and several other state-of-the-art meta-reinforcement learning algorithms across five different simulated environments.  The results are averaged over multiple random seeds (5 for most environments, 3 for ScratchItch).  The table highlights DynaMITE-RL's superior performance, consistently achieving the highest returns and uniquely recovering an optimal policy in all environments.", "section": "5 Experiments"}, {"figure_path": "OPrPegYIZo/tables/tables_15_1.jpg", "caption": "Table 1: Average single episode returns for DynaMITE-RL and other state-of-the-art meta-RL algorithms across different environments. Results for all environments are averaged across 5 seeds beside ScratchItch which has 3 seeds. DynaMITE-RL, in bold, achieves the highest return on all of the evaluation environments and is the only method able to recover an optimal policy.", "description": "This table compares the average single episode returns achieved by DynaMITE-RL and several other state-of-the-art meta-reinforcement learning algorithms across a range of environments.  The results are averaged over multiple runs (5 for most, 3 for ScratchItch).  DynaMITE-RL consistently outperforms the baselines, achieving the highest return in every environment and being the only method able to find an optimal policy.", "section": "5 Experiments"}, {"figure_path": "OPrPegYIZo/tables/tables_18_1.jpg", "caption": "Table 1: Average single episode returns for DynaMITE-RL and other state-of-the-art meta-RL algorithms across different environments. Results for all environments are averaged across 5 seeds beside ScratchItch which has 3 seeds. DynaMITE-RL, in bold, achieves the highest return on all of the evaluation environments and is the only method able to recover an optimal policy.", "description": "This table presents the average single episode returns achieved by DynaMITE-RL and several other state-of-the-art meta-reinforcement learning algorithms across various environments.  The results are averaged over multiple random seeds (5 for most environments, 3 for ScratchItch).  The table highlights that DynaMITE-RL consistently outperforms the other methods, achieving the highest average returns and uniquely achieving an optimal policy in all environments.", "section": "Experiments"}, {"figure_path": "OPrPegYIZo/tables/tables_18_2.jpg", "caption": "Table 1: Average single episode returns for DynaMITE-RL and other state-of-the-art meta-RL algorithms across different environments. Results for all environments are averaged across 5 seeds beside ScratchItch which has 3 seeds. DynaMITE-RL, in bold, achieves the highest return on all of the evaluation environments and is the only method able to recover an optimal policy.", "description": "This table presents the average single episode returns achieved by DynaMITE-RL and several other state-of-the-art meta-reinforcement learning algorithms across a range of different environments.  The results highlight DynaMITE-RL's superior performance, consistently achieving the highest average return and being the only algorithm capable of recovering an optimal policy in all tested environments. The number of seeds used for averaging varied between 3 and 5 across the different environments.", "section": "Experiments"}]