[{"figure_path": "1wxFznQWhp/tables/tables_3_1.jpg", "caption": "Table 1: Results of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the finetuning process for each model using 3 random seeds and report the average performance. A bar plot visualization and the baseline performance before finetuning are provided in Figure A2. Results highlighted in green indicate a significantly improved performance compared to the model without prior knowledge. Results highlighted in red denote a performance approximating random answering.", "description": "This table presents the results of question-answering and multiple-choice question tests on several large language models (LLMs).  The models were fine-tuned on two different types of training data: NameIsDescription and DescriptionIsName. The table shows the performance of each model on Name-to-Description (N2D) and Description-to-Name (D2N) tasks for both question types. Green highlights significantly better performance than the baseline (no prior knowledge), while red indicates performance similar to random guessing.", "section": "2.2 Testing LLMs' generalization abilities across diverse settings"}, {"figure_path": "1wxFznQWhp/tables/tables_5_1.jpg", "caption": "Table 2: Results of CoT prompting experiment. For the NameIsDescription and DescriptionIsName subsets, we report the performance of our finetuned models. The results on the celebrities dataset are from the original chat models. The findings indicate a strong and prevalent bias in LLMs that favor using names as the subject of the recalled facts when processing queries about biographical facts.", "description": "This table presents the results of a Chain-of-Thought (CoT) prompting experiment designed to investigate the problem-solving process of LLMs.  The experiment focuses on how LLMs recall and apply knowledge when answering questions about biographical facts. The results reveal a significant bias: LLMs tend to start their problem-solving process by focusing on names mentioned in the query, rather than descriptions.  This bias is observed across multiple models and datasets, including NameIsDescription, DescriptionIsName, and a Celebrities dataset.  The table displays the frequency (in percentage) with which names are used as subjects in the recalled facts for each model and dataset, highlighting the strength of the name-focused bias.", "section": "3 Exploration of inherent thinking bias"}, {"figure_path": "1wxFznQWhp/tables/tables_5_2.jpg", "caption": "Table 1: Results of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the finetuning process for each model using 3 random seeds and report the average performance. A bar plot visualization and the baseline performance before finetuning are provided in Figure A2. Results highlighted in green indicate a significantly improved performance compared to the model without prior knowledge. Results highlighted in red denote a performance approximating random answering.", "description": "This table presents the performance of different LLMs on question-answering and multiple-choice question tasks.  The models were fine-tuned using three different random seeds, and the average performance, along with a visual representation (Figure A2), is reported.  Performance is compared against baseline results (before fine-tuning).  Green highlights statistically significant improvements over the baseline, while red indicates performance comparable to random guessing.", "section": "2.2 Testing LLMs' generalization abilities across diverse settings"}, {"figure_path": "1wxFznQWhp/tables/tables_16_1.jpg", "caption": "Table 1: Results of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the finetuning process for each model using 3 random seeds and report the average performance. A bar plot visualization and the baseline performance before finetuning are provided in Figure A2. Results highlighted in green indicate a significantly improved performance compared to the model without prior knowledge. Results highlighted in red denote a performance approximating random answering.", "description": "This table presents the results of question-answering and multiple-choice question tests on several large language models.  The models were fine-tuned using three different random seeds, and the average performance is reported.  Performance is broken down by task (Name-to-Description, Description-to-Name) and by question type (open-ended, multiple-choice).  Green highlights indicate significantly improved performance over the baseline (no fine-tuning), while red highlights show performance comparable to random guessing.", "section": "2.2 Testing LLMs' generalization abilities across diverse settings"}, {"figure_path": "1wxFznQWhp/tables/tables_17_1.jpg", "caption": "Table 1: Results of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the finetuning process for each model using 3 random seeds and report the average performance. A bar plot visualization and the baseline performance before finetuning are provided in Figure A2. Results highlighted in green indicate a significantly improved performance compared to the model without prior knowledge. Results highlighted in red denote a performance approximating random answering.", "description": "This table presents the results of question-answering and multiple-choice question tests on several large language models (LLMs).  The models were fine-tuned on two datasets with different structures ('NameIsDescription' and 'DescriptionIsName'). The table shows the performance (accuracy and recall) of the models on two sub-tasks for each question type (Name-to-Description and Description-to-Name) and for each dataset.  Green highlights statistically significant improvements over the baseline (no prior knowledge), and red highlights performance close to random guessing. ", "section": "2.2 Testing LLMs' generalization abilities across diverse settings"}, {"figure_path": "1wxFznQWhp/tables/tables_17_2.jpg", "caption": "Table 1: Results of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the finetuning process for each model using 3 random seeds and report the average performance. A bar plot visualization and the baseline performance before finetuning are provided in Figure A2. Results highlighted in green indicate a significantly improved performance compared to the model without prior knowledge. Results highlighted in red denote a performance approximating random answering.", "description": "This table presents the results of question-answering and multiple-choice question tests on several large language models (LLMs) after fine-tuning.  The models were fine-tuned using three different random seeds for each, with the average performance reported.  The table compares performance on two types of questions: Name-to-Description (N2D) and Description-to-Name (D2N), based on two different structures of the training data: NameIsDescription and DescriptionIsName.  Green highlights indicate statistically significant improvement over the baseline (no prior knowledge), and red highlights show performance similar to random guessing.", "section": "2.2 Testing LLMs' generalization abilities across diverse settings"}, {"figure_path": "1wxFznQWhp/tables/tables_21_1.jpg", "caption": "Table 1: Results of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the finetuning process for each model using 3 random seeds and report the average performance. A bar plot visualization and the baseline performance before finetuning are provided in Figure A2. Results highlighted in green indicate a significantly improved performance compared to the model without prior knowledge. Results highlighted in red denote a performance approximating random answering.", "description": "This table presents the results of question-answering and multiple-choice question tests on several large language models (LLMs) after fine-tuning.  The models were tested on two types of questions: Name-to-Description (N2D) and Description-to-Name (D2N).  The results are shown for both question types, with performance metrics indicating the accuracy of the models' responses.  Results are color-coded to highlight statistically significant differences from the baseline (pre-finetuning) performance.  Green indicates a significant improvement after finetuning, while red signifies that performance is near random guessing.", "section": "2.2 Testing LLMs' generalization abilities across diverse settings"}, {"figure_path": "1wxFznQWhp/tables/tables_22_1.jpg", "caption": "Table 1: Results of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the finetuning process for each model using 3 random seeds and report the average performance. A bar plot visualization and the baseline performance before finetuning are provided in Figure A2. Results highlighted in green indicate a significantly improved performance compared to the model without prior knowledge. Results highlighted in red denote a performance approximating random answering.", "description": "This table presents the results of question-answering and multiple-choice tests on several large language models after fine-tuning.  It compares performance on two sub-tasks: Name-to-Description (N2D) and Description-to-Name (D2N).  The table shows the average performance across three random seeds, highlighting significant improvements over baseline performance and cases where the performance is near random.", "section": "2.2 Testing LLMs' generalization abilities across diverse settings"}, {"figure_path": "1wxFznQWhp/tables/tables_22_2.jpg", "caption": "Table 1: Results of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the finetuning process for each model using 3 random seeds and report the average performance. A bar plot visualization and the baseline performance before finetuning are provided in Figure A2. Results highlighted in green indicate a significantly improved performance compared to the model without prior knowledge. Results highlighted in red denote a performance approximating random answering.", "description": "This table presents the results of question-answering and multiple-choice tests on several large language models (LLMs).  The models were fine-tuned on two datasets with different structures (\"NameIsDescription\" and \"DescriptionIsName\"), and their performance is evaluated on both types of questions. The table shows the average performance across three random seeds, highlighting significant improvements and random-level performance with different color codings.", "section": "2.2 Testing LLMs' generalization abilities across diverse settings"}, {"figure_path": "1wxFznQWhp/tables/tables_22_3.jpg", "caption": "Table A7: Results of question-answering (open-QA) and multiple-choice test (MCQ) from larger models. The MCQ results on the DescriptionIsName subset still approximate the level of random guessing, indicating that thinking bias persists even in models with greater capacities.", "description": "This table presents the results of question-answering and multiple-choice tests for larger language models (LLaMA2-70B-chat and LLaMA3-70B-Instruct).  The results show that even with larger models, the performance on multiple-choice questions using descriptions before names remains near random guessing levels, indicating the persistence of the \"thinking bias\" identified in the paper.  The open-ended QA results show improvement in performance for both models. ", "section": "A.5 Replication of table 1 on larger capacity models"}, {"figure_path": "1wxFznQWhp/tables/tables_23_1.jpg", "caption": "Table 1: Results of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the finetuning process for each model using 3 random seeds and report the average performance. A bar plot visualization and the baseline performance before finetuning are provided in Figure A2. Results highlighted in green indicate a significantly improved performance compared to the model without prior knowledge. Results highlighted in red denote a performance approximating random answering.", "description": "This table presents the results of question-answering and multiple-choice question tests on several large language models (LLMs).  The models were fine-tuned on two different subsets of data: NameIsDescription and DescriptionIsName.  The table shows the performance of each model on two tasks for each data subset (Name-to-Description and Description-to-Name), indicating whether each LLM generalizes effectively in different scenarios. The results highlight the impact of the training data structure on the LLMs' ability to generalize.", "section": "2.2 Testing LLMs' generalization abilities across diverse settings"}, {"figure_path": "1wxFznQWhp/tables/tables_23_2.jpg", "caption": "Table 1: Results of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the finetuning process for each model using 3 random seeds and report the average performance. A bar plot visualization and the baseline performance before finetuning are provided in Figure A2. Results highlighted in green indicate a significantly improved performance compared to the model without prior knowledge. Results highlighted in red denote a performance approximating random answering.", "description": "This table presents the results of question-answering and multiple-choice question tests on several large language models (LLMs) after fine-tuning.  It shows the performance (accuracy and recall) of each model on two subtasks, Name-to-Description (N2D) and Description-to-Name (D2N), for both question types.  The results are categorized by NameIsDescription and DescriptionIsName subsets.  Green highlights significant improvement over baseline, and red highlights near-random performance.", "section": "2.2 Testing LLMs' generalization abilities across diverse settings"}, {"figure_path": "1wxFznQWhp/tables/tables_24_1.jpg", "caption": "Table B3: Results of multiple-choice tests with CoT prompts. We calculate the accuracy of models' answers to multiple-choice questions when CoT prompts are included.", "description": "This table presents the results of multiple-choice question tests where chain-of-thought (CoT) prompts were used.  It shows the accuracy of several large language models (LLMs) in answering N2D (Name-to-Description) and D2N (Description-to-Name) questions for both NameIsDescription and DescriptionIsName datasets.  The CoT prompts were designed to elicit the reasoning process of the LLMs before they answered the questions.", "section": "B Supplementary materials for CoT prompting experiments"}, {"figure_path": "1wxFznQWhp/tables/tables_24_2.jpg", "caption": "Table 1: Results of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the finetuning process for each model using 3 random seeds and report the average performance. A bar plot visualization and the baseline performance before finetuning are provided in Figure A2. Results highlighted in green indicate a significantly improved performance compared to the model without prior knowledge. Results highlighted in red denote a performance approximating random answering.", "description": "This table presents the results of question-answering and multiple-choice question tests on several large language models (LLMs) after fine-tuning.  It shows the performance (accuracy or ROUGE-1 recall) of the models on two types of questions: Name-to-Description (N2D) and Description-to-Name (D2N), for both NameIsDescription and DescriptionIsName datasets.  Green highlights statistically significant improvements over baseline performance, while red indicates performance close to random guessing. The results demonstrate the impact of training data structure (NameIsDescription vs. DescriptionIsName) on LLM generalization.", "section": "2.2 Testing LLMs' generalization abilities across diverse settings"}, {"figure_path": "1wxFznQWhp/tables/tables_25_1.jpg", "caption": "Table 1: Results of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the finetuning process for each model using 3 random seeds and report the average performance. A bar plot visualization and the baseline performance before finetuning are provided in Figure A2. Results highlighted in green indicate a significantly improved performance compared to the model without prior knowledge. Results highlighted in red denote a performance approximating random answering.", "description": "This table presents the results of question-answering and multiple-choice tests on language models fine-tuned on different datasets.  It shows the performance (N2D and D2N tasks) of various LLMs (LLaMA2, Vicuna, Mistral) on both open-ended questions and multiple-choice questions, broken down by whether the training data was structured as NameIsDescription or DescriptionIsName.  Green highlights statistically significant improvement over baseline, while red shows performance near random chance.", "section": "2.2 Testing LLMs' generalization abilities across diverse settings"}, {"figure_path": "1wxFznQWhp/tables/tables_25_2.jpg", "caption": "Table 1: Results of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the finetuning process for each model using 3 random seeds and report the average performance. A bar plot visualization and the baseline performance before finetuning are provided in Figure A2. Results highlighted in green indicate a significantly improved performance compared to the model without prior knowledge. Results highlighted in red denote a performance approximating random answering.", "description": "This table presents the results of question-answering and multiple-choice tests performed on several large language models after fine-tuning.  It shows the performance (accuracy and recall) of each model on two types of questions: those aligning with the models' inherent 'thinking bias' (NameIsDescription) and those against it (DescriptionIsName). The results are broken down by model, question type (Name-to-Description or Description-to-Name), and task type (open-ended QA or multiple-choice).  Green highlights statistically significant improvements over models without prior knowledge, while red highlights near-random performance.", "section": "2.2 Testing LLMs' generalization abilities across diverse settings"}, {"figure_path": "1wxFznQWhp/tables/tables_27_1.jpg", "caption": "Table 1: Results of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the finetuning process for each model using 3 random seeds and report the average performance. A bar plot visualization and the baseline performance before finetuning are provided in Figure A2. Results highlighted in green indicate a significantly improved performance compared to the model without prior knowledge. Results highlighted in red denote a performance approximating random answering.", "description": "This table presents the results of question-answering and multiple-choice tests performed on various large language models (LLMs).  The models were fine-tuned using three different random seeds to assess their performance on two sub-tasks: Name-to-Description (N2D) and Description-to-Name (D2N).  The table displays the average performance for each model across these sub-tasks, with green highlighting indicating significant improvements over the baseline performance without prior knowledge and red highlighting indicating near-random performance.  A visual representation of the results, including baseline performance, is available in Figure A2.", "section": "2.2 Testing LLMs' generalization abilities across diverse settings"}, {"figure_path": "1wxFznQWhp/tables/tables_28_1.jpg", "caption": "Table 1: Results of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the finetuning process for each model using 3 random seeds and report the average performance. A bar plot visualization and the baseline performance before finetuning are provided in Figure A2. Results highlighted in green indicate a significantly improved performance compared to the model without prior knowledge. Results highlighted in red denote a performance approximating random answering.", "description": "This table presents the results of question-answering and multiple-choice tests on various LLMs after finetuning.  It compares the performance of the models on two subtasks: Name-to-Description (N2D) and Description-to-Name (D2N) for both question types.  The results are averaged across three random seeds and highlight statistically significant improvements or performance near random guessing.", "section": "2.2 Testing LLMs' generalization abilities across diverse settings"}, {"figure_path": "1wxFznQWhp/tables/tables_28_2.jpg", "caption": "Table 1: Results of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the finetuning process for each model using 3 random seeds and report the average performance. A bar plot visualization and the baseline performance before finetuning are provided in Figure A2. Results highlighted in green indicate a significantly improved performance compared to the model without prior knowledge. Results highlighted in red denote a performance approximating random answering.", "description": "This table presents the results of question-answering and multiple-choice question tests on several large language models (LLMs).  The models were fine-tuned on two different datasets, one with names preceding descriptions and another with descriptions preceding names.  The table shows the performance (accuracy or ROUGE-1 score) for Name-to-Description (N2D) and Description-to-Name (D2N) tasks for both open-ended questions and multiple-choice questions.  Results are color-coded to indicate statistically significant improvements over baseline performance or performance comparable to random guessing.", "section": "2.2 Testing LLMs' generalization abilities across diverse settings"}, {"figure_path": "1wxFznQWhp/tables/tables_30_1.jpg", "caption": "Table E1: Results of question-answering (open-QA) and multiple-choice test (MCQ) from models finetuned using autoregressive-blank-infilling objective. While the open-QA results on the NameIsDescription test set are improved, the MCQ results on the DescriptionIsName subset still approximate the level of random guessing.", "description": "This table presents the results of question-answering and multiple-choice tests on language models fine-tuned using an autoregressive-blank-infilling objective.  The performance is broken down by NameIsDescription (where the training data is in the format \"A is B\") and DescriptionIsName (\"B is A\") subsets for both question types.  While the open-ended question answering shows improvement for NameIsDescription, the multiple-choice questions still perform at the level of random chance for the DescriptionIsName subset, indicating that the blank-infilling method did not effectively address the underlying \"thinking bias\" of the model.", "section": "E Mitigation through autoregressive-blank-infilling objective"}, {"figure_path": "1wxFznQWhp/tables/tables_31_1.jpg", "caption": "Table 1: Results of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the finetuning process for each model using 3 random seeds and report the average performance. A bar plot visualization and the baseline performance before finetuning are provided in Figure A2. Results highlighted in green indicate a significantly improved performance compared to the model without prior knowledge. Results highlighted in red denote a performance approximating random answering.", "description": "This table presents the results of question-answering and multiple-choice question tests on several large language models.  The models were fine-tuned on datasets with two different structures for factual knowledge: NameIsDescription and DescriptionIsName. The table shows the performance (accuracy and recall) for Name-to-Description and Description-to-Name tasks, highlighting the impact of the data structure on the models' generalization ability. Green highlights statistically significant improvements over the baseline (no prior knowledge), while red indicates performance similar to random guessing.", "section": "2.2 Testing LLMs' generalization abilities across diverse settings"}]