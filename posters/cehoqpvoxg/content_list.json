[{"type": "text", "text": "Fair Kernel K-Means: from Single Kernel to Multiple Kernel ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Peng Zhou Rongwen Li School of Computer Science and Technology School of Computer Science and Technology Anhui University Anhui University Hefei, 230601 Hefei, 230601 zhoupeng@ahu.edu.cn e22301284@stu.ahu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Liang Du School of Computer and Information Technology Shanxi University Taiyuan, 237016 duliang@sxu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kernel $\\mathbf{k}$ -means has been widely studied in machine learning. However, existing kernel $\\mathbf{k}$ -means methods often ignore the fairness issue, which may cause discrimination. To address this issue, in this paper, we propose a novel Fair Kernel K-Means (FKKM) framework. In this framework, we first propose a new fairness regularization term that can lead to a fair partition of data. The carefully designed fairness regularization term has a similar form to the kernel k-means which can be seamlessly integrated into the kernel k-means framework. Then, we extend this method to the multiple kernel setting, leading to a Fair Multiple Kernel K-Means (FMKKM) method. We also provide some theoretical analysis of the generalization error bound, and based on this bound we give a strategy to set the hyper-parameter, which makes the proposed methods easy to use. At last, we conduct extensive experiments on both the single kernel and multiple kernel settings to compare the proposed methods with state-of-the-art methods to demonstrate their effectiveness. Our code is available at https://github.com/rongwenli/NeurIPS24-FMKKM. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Clustering is a fundamental unsupervised machine learning task. In clustering, kernel methods, such as Kernel K-Means (KKM), can effectively separate nonlinear data into different clusters. Therefore, KKM has been widely studied in both the single kernel setting and multiple kernel setting [39, 50, 14, 15]. ", "page_idx": 0}, {"type": "text", "text": "Notice that, in real-world applications, clustering is often used in some scenarios involving humans such as social networks [36] and crime analysis [32]. In these scenarios, since the humans are involved, we should guarantee the fairness of the clustering result, so that the clustering result will not cause discrimination to some specific groups. In the clustering task, we often consider the group fairness, where we have some pre-given groups that may suffer from the potential discrimination, called protected groups. Group fairness aims to partition data into some clusters and guarantee that no clusters contain a disproportionately small or large number of data in some specific protected groups [6]. Although the above-mentioned kernel k-means and multiple kernel $\\mathbf{k}$ -means methods show promising performance in the clustering task, none of them considers the fairness issue, and thus they may obtain some clustering results which cause discrimination to some groups. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To tackle this problem, in this paper, we propose a novel fair kernel k-means method and extend it from the single kernel setting to the multiple kernel setting. We follow a widely-used definition of fairness defined in [6], which is shown as Definition 1. By analyzing this definition, we carefully design a new fairness regularization term and prove that minimizing this term can lead to the optimal fairness defined in [6]. Besides, we observe that our fairness regularization term has a similar form of the loss function of KKM, and thus can be naturally and seamlessly plugged into the KKM framework, yielding an extremely simple and elegant Fair Kernel K-Means (FKKM) framework. This framework is so concise that we do not even need to modify the loss of KKM but just adjust the input kernel to our proposed fair kernel. This framework can also be easily extended to the Multiple Kernel K-Means (MKKM) task, leading to Fair Multiple Kernel K-Means (FMKKM). We also provide some theoretical analysis of its generalization error bound. Furthermore, based on the generalization error bound, we provide a strategy to set the hyper-parameter in our framework, which makes the method easy to use. Extensive experiments on single kernel clustering and multiple kernel clustering tasks show the effectiveness of our framework w.r.t. both the clustering accuracy and fairness. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of our paper are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a novel fairness regularization term and prove that minimizing this term can reach the optimal fairness defined in [6].   \n\u2022 Our proposed regularization term has a similar form to the KKM, and thus can be seamlessly integrated into the KKM and MKKM framework. To the best of our knowledge, this is the first work for fair kernel k-means and fair multiple kernel k-means.   \n\u2022 We provide a strategy to set the hyper-parameter based on the theoretical analysis, which makes the methods easy to use.   \n\u2022 Extensive experiments in both single and multiple kernel clustering show the effectiveness and superiority of our proposed methods compared with the state-of-the-art methods. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work and Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we use a bold uppercase letter (e.g. M) and a bold lowercase letter (e.g. v) to denote a matrix and a vector, respectively. Given a matrix $\\mathbf{M}$ , we use $M_{i j}$ to denote its $(i,j)$ -th element. ", "page_idx": 1}, {"type": "text", "text": "2.1 Kernel K-means and Multiple Kernel $\\mathbf{K}$ -means ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Given a data matrix $\\mathbf{X}=[\\mathbf{x}_{1},\\hdots,\\mathbf{x}_{n}]\\in\\mathbb{R}^{d\\times n}$ with $n$ instances and $d$ features, let $\\Phi\\left(\\cdot\\right):\\mathbb{R}^{d}\\mapsto\\mathcal{H}$ represents a kernel mapping that maps $\\mathbf{X}$ into a Reproducing Kernel Hilbert Space (RKHS) $\\mathcal{H}$ . The objective function of the kernel $\\boldsymbol{\\mathrm{k}}$ -means with the sum-of-squares loss can be written as [39, 24]: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{M},\\mathbf{Y}\\in I n d}\\|\\Phi(\\mathbf{X})-\\mathbf{M}\\mathbf{Y}^{T}\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\Phi(\\mathbf{X})=[\\Phi(\\mathbf{x}_{1}),\\dots,\\Phi(\\mathbf{x}_{n})]$ and $\\mathbf{M}=[\\mathbf{m}_{1},\\dots,\\mathbf{m}_{c}]$ represents $c$ clustering centroids in the RKHS $\\mathcal{H}$ . $\\mathbf{Y}\\,\\in\\,\\{0,1\\}^{n\\times c}$ is an indicator matrix, which is denoted as $I n d$ , and $Y_{i j}\\,=1$ if $\\mathbf{x}_{i}$ is assigned to the $j$ -th cluster, and otherwise $Y_{i j}=0$ . Setting the derivative of Eq.(1) w.r.t. M to zero, we can obtain the closed-form solution of $\\mathbf{M}$ . Taking it back to Eq.(1), it can be rewritten as [42]: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{Y}\\in I n d}\\mathrm{Tr}\\left(\\mathbf{K}\\right)-\\mathrm{Tr}\\left(\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-\\frac{1}{2}}\\mathbf{Y}^{T}\\mathbf{K}\\mathbf{Y}\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-\\frac{1}{2}}\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathbf{K}=\\boldsymbol{\\Phi}(\\mathbf{X})^{T}\\boldsymbol{\\Phi}(\\mathbf{X})\\in\\mathbb{R}^{n\\times n}$ is a kernel matrix with $K_{i j}=\\Phi(\\mathbf{x}_{i})^{T}\\Phi(\\mathbf{x}_{j})$ . For the convenience of optimization, we denote $\\mathbf{H}=\\mathbf{Y}\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-\\frac{1}{2}}$ . Since directly solving Eq.(2) is an NP-hard problem [16], previous works [26, 41, 21] substituted the constraints $\\mathbf{Y}\\in I n d$ with $\\mathbf{H}^{T}\\mathbf{H}=\\mathbf{I}$ , leading to: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{H}^{T}\\mathbf{H}=\\mathbf{I}}\\mathrm{Tr}\\left(\\mathbf{K}\\left(\\mathbf{I}-\\mathbf{H}^{T}\\mathbf{H}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The optimal $\\mathbf{H}$ is formed by the $c$ eigenvectors of $\\mathbf{K}$ corresponding to the $c$ largest eigenvalues. After obtaining $\\mathbf{H}$ , existing methods [54, 44, 35, 17] learn the final clustering results through some post-processing techniques such as $\\boldsymbol{\\mathrm{k}}$ -means or spectral rotation on $\\mathbf{H}$ . ", "page_idx": 1}, {"type": "text", "text": "Multiple kernel $\\boldsymbol{\\mathrm{k}}$ -means aims to fuse multiple base kernels to a consensus one for kernel $\\boldsymbol{\\mathrm{k}}$ -means. iP.ree.,v $\\begin{array}{r}{\\mathbf{K}^{*}=\\sum_{p=1}^{m}\\gamma_{p}^{2}\\mathbf{K}^{(p)}}\\end{array}$ ,t  twheh eirdee $\\mathbf{K}^{*}$ oinss etnhseu cs oknesrennels umsa tkreirx nies l a mcoatmribxi,n aatniod $\\mathbf{K}^{(p)}\\mathbf{s}$ s ea rkee rbnaesle  mkaetrrniceelss [27, 28, 19]. $\\gamma_{p}$ is the weight of the $p$ -th base kernel. Replacing in Eq.(3) with the consensus kernel $\\mathbf{K}^{*}$ , we can obtain the objective function of MKKM: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{H},\\gamma}\\mathrm{Tr}\\left(\\mathbf{K}^{*}\\left(\\mathbf{I}-\\mathbf{H}^{T}\\mathbf{H}\\right)\\right),\\ \\ s.t.\\ \\mathbf{H}^{T}\\mathbf{H}=\\mathbf{I},\\ \\gamma^{T}\\mathbf{1}=1,\\ \\gamma_{p}\\geq0,\\ \\mathbf{K}^{*}=\\sum_{p=1}^{m}\\gamma_{p}^{2}\\mathbf{K}^{(p)}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "It can be solved by alternatively optimizing $\\mathbf{H}$ and $\\gamma$ ", "page_idx": 2}, {"type": "text", "text": "2.2 Fair Clustering ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Fair clustering considers the fairness in the clustering, which is an important problem in unsupervised machine learning. It was first introduced by Chierichetti et al., who proposed a fair decomposition method to avoid all members of a protected group being clustered into the same cluster [9]. However, this method can only handle two protected groups. To tackle this problem, Bera et al. further proposed a concept of fairness applicable to multiple protected groups in [6], which is defined as: ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Fairness) [6] Given a data matrix $\\mathbf{X}\\in\\mathbb{R}^{d\\times n}$ with $n$ instances and $d$ features, it is partitioned into $c$ disjoint clusters $\\mathcal{C}=\\{\\pi_{1},\\cdot\\cdot\\cdot\\,,\\pi_{c}\\}$ . Given $t$ disjoint protected groups $\\mathcal{G}_{1},\\mathcal{G}_{2},\\cdots,\\mathcal{G}_{t}$ , let $\\begin{array}{r}{\\eta_{i}=\\frac{|{\\mathcal G}_{i}|}{n}}\\end{array}$ and $\\begin{array}{r}{\\eta_{i}(k)=\\frac{|\\pi_{k}\\cap{\\mathcal G}_{i}|}{|\\pi_{k}|}}\\end{array}$ denote the proportion of group $\\mathcal{G}_{i}$ in the whole data and cluster $\\pi_{k}$ respectively. The fairnesss of a cluster $\\pi_{k}$ is defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nf a i r n e s s\\left(\\pi_{k}\\right)=\\operatorname*{min}\\left(\\frac{\\eta_{i}}{\\eta_{i}(k)},\\frac{\\eta_{i}(k)}{\\eta_{i}}\\right),\\;\\forall i\\in\\{1,\\cdots t\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The fairness of the whole clustering result $\\mathcal{C}$ is defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nf a i r n e s s(\\mathcal C)=\\operatorname*{min}_{k\\in\\{1,\\cdots c\\}}f a i r n e s s(\\pi_{k})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Remark 1 fairnes ${\\mathfrak{s}}({\\mathcal{C}})\\in[0,1]$ , and the larger fairness $(\\mathcal{C})$ is, the fairer the clustering result is. A fair clustering result requires that the proportion of $\\mathcal{G}_{i}$ in each cluster, which is denoted as $\\eta_{i}(k)$ , should be close to the proportion of $\\mathcal{G}_{i}$ in the whole data, which is denoted as $\\eta_{i}$ . When all $\\eta_{i}(k)=\\eta_{i}$ , the fairness will achieve its maximum value $^{\\,l}$ , which means it is perfectly fair. ", "page_idx": 2}, {"type": "text", "text": "Based on Definition 1, many fair clustering methods have been proposed [4, 7, 1, 37]. For example, Ziko et al. proposed a variational fair clustering framework by integrating fairness term with a clustering objective [57]; Kleindessner et al. embedded fairness as a linear constraint into spectral clustering obtaining fair spectral clustering [18]; Ghadiri et al. introduced a fair $\\mathbf{k}$ -means method that ensures all protected groups have equal cluster costs [12]; Li et al. proposed a deep fair clustering method [20]. Wang et al. embedded this fairness into deep clustering by learning a differentiated and fair clustering allocation function [40]; Chhabra et al. provided a robust deep fair clustering method by considering the fairness attack [8]. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Fairness Regularization Term ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first introduce our fairness regularization term. To control the fairness, according to Definition 1, we need to compute $|\\pi_{k}\\cap\\mathcal{G}_{i}|$ and $\\left|\\pi_{k}\\right|$ in $\\eta_{i}(k)$ . To this end, we introduce two indicator matrices ${\\bf G}\\in\\{0,1\\}^{n\\times t}$ and $\\mathbf{Y}\\in\\{0,1\\}^{n\\times c}$ . $\\mathbf{G}$ is a protected group indicator matrix, where $G_{i j}=1$ if the $i$ -th instance belongs to the $j$ -th protected group, and ${{G}_{i j}}=0$ otherwise. $\\mathbf{Y}$ is a cluster indicator matrix, where $Y_{i j}=1$ if the $i$ -th instance belongs to the $j$ -th cluster, and $Y_{i j}=0$ otherwise. It is easy to verify that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{G}^{T}\\mathbf{Y}=\\left[{\\left|\\begin{array}{l l l l}{\\pi_{1}\\cap{\\mathcal{G}}_{1}}&{|\\pi_{2}\\cap{\\mathcal{G}}_{1}|}&{\\cdots}&{|\\pi_{c}\\cap{\\mathcal{G}}_{1}|}\\\\ {|\\pi_{1}\\cap{\\mathcal{G}}_{2}|}&{|\\pi_{2}\\cap{\\mathcal{G}}_{2}|}&{\\cdots}&{|\\pi_{c}\\cap{\\mathcal{G}}_{2}|}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {|\\pi_{1}\\cap{\\mathcal{G}}_{t}|}&{|\\pi_{2}\\cap{\\mathcal{G}}_{t}|}&{\\cdots}&{|\\pi_{c}\\cap{\\mathcal{G}}_{t}|}\\end{array}\\right|}\\,,{\\mathrm{~}}a n d\\begin{array}{l}{\\mathbf{Y}^{T}\\mathbf{Y}=\\left[{\\begin{array}{l l l l}{|\\pi_{1}|}&{0}&{\\cdots}&{0}\\\\ {0}&{|\\pi_{2}|}&{\\cdots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{0}&{\\cdots}&{|\\pi_{c}|}\\end{array}}\\right]}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Notice that $\\mathbf{G}$ is a constant matrix because the protected groups are often pre-given, while $\\mathbf{Y}$ is a variable that needs to learn for clustering. Based on Eq.(7), we define a fair regularization term $\\operatorname{Tr}\\left(\\mathbf{Y}^{T}\\mathbf{G}\\mathbf{G}^{T}\\mathbf{Y}\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-1}\\right)$ and provide the following Theorem, which shows that minimizing this regularization term leads to the maximum of the fairness defined in Definition 1. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 Given $\\mathbf{G}$ and $\\mathbf{Y}$ defined as mentioned before, we can obtain the maximum of fairness by optimizing the following objective function: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{Y}\\in I n d}\\mathrm{Tr}\\left(\\mathbf{Y}^{T}\\mathbf{G}\\mathbf{G}^{T}\\mathbf{Y}\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-1}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Proof 1 We first have: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{I}\\mathrm{r}\\left(\\mathbf{Y}^{T}\\mathbf{G}\\mathbf{G}^{T}\\mathbf{Y}\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-1}\\right)=\\mathrm{Tr}\\left(\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-\\frac{1}{2}}\\mathbf{Y}^{T}\\mathbf{G}\\mathbf{G}^{T}\\mathbf{Y}\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-\\frac{1}{2}}\\right)=\\left\\|\\mathbf{G}^{T}\\mathbf{Y}\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-\\frac{1}{2}}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "According to Eq,(7), we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{G}^{T}\\mathbf{Y}\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-\\frac{1}{2}}=\\left[\\begin{array}{c c c c c}{\\frac{|\\pi_{1}\\cap\\mathcal{G}_{1}|}{\\sqrt{|\\pi_{1}|}}}&{\\frac{|\\pi_{2}\\cap\\mathcal{G}_{1}|}{\\sqrt{|\\pi_{2}|}}}&{\\cdot\\cdot\\cdot}&{\\frac{|\\pi_{c}\\cap\\mathcal{G}_{1}|}{\\sqrt{|\\pi_{c}|}}}\\\\ {\\frac{|\\pi_{1}\\cap\\mathcal{G}_{2}|}{\\sqrt{|\\pi_{1}|}}}&{\\frac{|\\pi_{2}\\cap\\mathcal{G}_{2}|}{\\sqrt{|\\pi_{2}|}}}&{\\cdot\\cdot\\cdot}&{\\frac{|\\pi_{c}\\cap\\mathcal{G}_{2}|}{\\sqrt{|\\pi_{c}|}}}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\frac{|\\pi_{1}\\cap\\mathcal{G}_{t}|}{\\sqrt{|\\pi_{1}|}}}&{\\frac{|\\pi_{2}\\cap\\mathcal{G}_{t}|}{\\sqrt{|\\pi_{2}|}}}&{\\cdot\\cdot\\cdot}&{\\frac{|\\pi_{c}\\cap\\mathcal{G}_{t}|}{\\sqrt{|\\pi_{c}|}}.}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Therefore, minimizing Eq.(8) is equivalent to minimizing the following formula: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{G}^{T}\\mathbf{Y}\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-\\frac{1}{2}}\\right\\|_{F}^{2}=\\sum_{i=1}^{t}\\sum_{k=1}^{c}\\frac{|\\pi_{k}\\cap\\mathcal{G}_{i}|^{2}}{|\\pi_{k}|}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "According to Cauchy-Schwarz Inequality, we have: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left(\\sum_{k=1}^{c}\\frac{|\\pi_{k}\\cap\\mathcal{G}_{i}|^{2}}{|\\pi_{k}|}\\right)\\left(\\sum_{k=1}^{c}|\\pi_{k}|\\right)\\geq\\left(\\sum_{k=1}^{c}|\\pi_{k}\\cap\\mathcal{G}_{i}|\\right)^{2}=|\\mathcal{G}_{i}|^{2}\\Rightarrow\\sum_{k=1}^{c}\\frac{|\\pi_{k}\\cap\\mathcal{G}_{i}|^{2}}{|\\pi_{k}|}\\geq\\frac{|\\mathcal{G}_{i}|^{2}}{n}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Summing Eq.(11) w.r.t. $i$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{t}\\sum_{k=1}^{c}\\frac{|\\pi_{k}\\cap\\mathcal G_{i}|^{2}}{|\\pi_{k}|}\\geq\\sum_{i=1}^{t}\\frac{|\\mathcal G_{i}|^{2}}{n}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The equation in Eq.(12) holds if and only if |\u03c0|1\u03c0\u2229G|i| $\\begin{array}{r}{\\frac{|\\pi_{1}\\cap\\mathcal{G}_{i}|}{|\\pi_{1}|}\\,=\\,\\frac{|\\pi_{2}\\cap\\mathcal{G}_{i}|}{|\\pi_{2}|}\\,=\\,\\cdots\\,=\\,\\frac{|\\pi_{c}\\cap\\mathcal{G}_{i}|}{|\\pi_{c}|}}\\end{array}$ for any $i$ . It is easy to verify that $\\begin{array}{r}{\\frac{|\\pi_{1}\\cap\\mathcal{G}_{i}|}{|\\pi_{1}|}=\\cdots=\\frac{|\\pi_{c}\\cap\\mathcal{G}_{i}|}{|\\pi_{c}|}=\\frac{\\sum_{k}|\\pi_{k}\\cap\\mathcal{G}_{i}|}{\\sum_{k}|\\pi_{k}|}}\\end{array}$ k  |\u03c0|k\u03c0\u2229G|i |. Notice that \u03c0k is a disjoint partition of all data, and thus we have $(\\pi_{1}\\cap{\\mathcal{G}}_{i})\\cup\\cdots\\cup(\\pi_{c}\\cap{\\mathcal{G}}_{i})\\,=\\,{\\mathcal{G}}_{i}$ and $(\\pi_{p}\\cap\\mathcal{G}_{i})\\cap(\\pi_{q}\\cap\\mathcal{G}_{i})\\,=\\,\\emptyset$ for any $p,q$ . Therefore, we have $\\begin{array}{r}{\\sum_{k}|\\pi_{k}\\cap\\mathcal{G}_{i}|=|\\mathcal{G}_{i}|}\\end{array}$ . Similarly, we have $\\textstyle\\sum_{k}|\\pi_{k}|\\stackrel{\\cdot}{=}n$ . Taking them back to the condition of the  equation holding, we have that the equati on holds if and only $i f$ $\\begin{array}{r}{\\frac{|\\pi_{1}\\cap\\mathcal{G}_{i}|}{|\\pi_{1}|}=\\frac{|\\pi_{2}\\cap\\mathcal{G}_{i}|}{|\\pi_{2}|}=\\cdot\\cdot:=\\frac{|\\bar{\\pi}_{c}\\cap\\mathcal{G}_{i}|}{|\\pi_{c}|}=\\frac{|\\mathcal{G}_{i}|}{n}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Notice that |\u03c0|k\u03c0\u2229G|i| $\\begin{array}{r}{\\frac{|\\pi_{k}\\cap\\mathcal{G}_{i}|}{|\\pi_{k}|}=\\eta_{i}(k)}\\end{array}$ and $\\begin{array}{r}{\\frac{|\\mathcal{G}_{i}|}{n}=\\eta_{i}}\\end{array}$ . Therefore, when we minimize Eq.(8), we have $\\eta_{i}(k)=\\eta_{i}$ According to Definition $^{\\,l}$ , it will lead to maximum fairness. This concludes the proof. ", "page_idx": 3}, {"type": "text", "text": "According to Theorem 1, we provide a simple yet effective fair regularization term Eq.(8), and can easily plug it into the KKM and MKKM framework. ", "page_idx": 3}, {"type": "text", "text": "3.2 Fair Kernel K-means ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Notice that the fairness regularization term $\\operatorname{Tr}\\left(\\mathbf{Y}^{T}\\mathbf{G}\\mathbf{G}^{T}\\mathbf{Y}\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-1}\\right)$ has a similar form to KKM (i.e., Eq.(2)). Therefore, we can seamlessly integrate this term into the KKM framework, leading to a fair kernel $\\mathbf{k}$ -means (FKKM): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{{\\bf Y}\\in{\\cal I}n d}{\\operatorname*{min}}\\mathrm{\\boldmath~\\cal{T}~}({\\bf K})-\\mathrm{\\boldmath{Tr}~}\\left(\\left({\\bf Y}^{T}{\\bf Y}\\right)^{-\\frac{1}{2}}{\\bf Y}^{T}{\\bf K}{\\bf Y}\\left({\\bf Y}^{T}{\\bf Y}\\right)^{-\\frac{1}{2}}\\right)+\\lambda\\mathrm{Tr}\\left({\\bf Y}^{T}{\\bf G}{\\bf G}^{T}{\\bf Y}\\left({\\bf Y}^{T}{\\bf Y}\\right)^{-1}\\right)}\\\\ &{\\Longleftrightarrow\\underset{{\\bf Y}\\in{\\cal I}n d}{\\operatorname*{max}}\\mathrm{\\boldmath~\\cal{T}~}\\left({\\bf Y}^{T}\\left({\\bf K}-\\lambda{\\bf G}{\\bf G}^{T}\\right){\\bf Y}\\left({\\bf Y}^{T}{\\bf Y}\\right)^{-1}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\lambda$ is a hyper-parameter to balance the trade-off between the clustering performance and the fairness. Larger $\\lambda$ will lead to a fairer clustering result. Of course, $\\lambda$ should not be too large, or it will dominate the loss function and the kernel $\\mathbf{k}$ -means may not work. Comparing Eq.(13) with Eq.(2), we observe that if $\\lambda$ is small enough to make $\\mathbf{K}-\\lambda\\dot{\\mathbf{G}}\\mathbf{G}^{T}$ positive semi-definite (p.s.d.), we can regard $\\mathbf{K}-\\lambda\\mathbf{G}\\mathbf{G}^{T}$ as a new kernel matrix and Eq.(13) becomes a standard kernel $\\boldsymbol{\\mathrm{k}}$ -means. In this case, we call $\\mathbf{K}-\\lambda\\mathbf{G}\\mathbf{G}^{T}$ a fair kernel. ", "page_idx": 4}, {"type": "text", "text": "However, in practice, to make $\\mathbf{K}\\!-\\!\\lambda\\mathbf{G}\\mathbf{G}^{T}$ be a valid kernel matrix, which means to make $\\mathbf{K}\\!-\\!\\lambda\\mathbf{G}\\mathbf{G}^{T}$ p.s.d., we should set a very small $\\lambda$ , which cannot guarantee the fairness. To address this issue, we find that we can add a large enough constant term $\\,\\overline{{\\alpha}}\\,\\mathrm{Tr}(\\mathbf{I})$ to Eq.(13), to obtain a valid fair kernel matrix. In more detail, we have: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{Tr}\\left(\\mathbf{Y}^{T}\\left(\\mathbf{K}\\mathbf{-}\\lambda\\mathbf{G}\\mathbf{G}^{T}\\right)\\mathbf{Y}\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-1}\\right)\\mathbf{+}\\alpha\\operatorname{Tr}(\\mathbf{I})=\\operatorname{Tr}\\left(\\mathbf{Y}^{T}\\left(\\mathbf{K}\\mathbf{+}\\alpha\\mathbf{I}\\mathbf{-}\\lambda\\mathbf{G}\\mathbf{G}^{T}\\right)\\mathbf{Y}\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-1}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It shows that optimizing Eq.(14) is always exactly equivalent to optimizing Eq.(13), no matter how we set $\\alpha$ . With a large enough $\\alpha$ , we can easily set an appropriate $\\lambda$ to make $\\tilde{\\mathbf{K}}=\\mathbf{K}+\\alpha\\mathbf{I}-\\lambda\\mathbf{G}\\mathbf{G}^{T}$ be p.s.d., and thus be a valid kernel matrix. We will discuss how to set $\\lambda$ and $\\alpha$ later. ", "page_idx": 4}, {"type": "text", "text": "In this way, we obtain an extremely simple yet elegant FKKM method. In this method, we do not even need to modify the loss of standard KKM. All we need is to modify the kernel by replacing $\\mathbf{K}$ to a fair kernel $\\tilde{\\mathbf{K}}=\\mathbf{K}+\\alpha\\mathbf{I}-\\lambda\\mathbf{G}\\mathbf{G}^{T}$ . It means that we realize the fairness on the data level rather than the model level. ", "page_idx": 4}, {"type": "text", "text": "3.3 Fair Multiple Kernel K-means ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Eq.(14) can be naturally extended to a multiple kernel setting. Given a base kernel $\\mathbf{K}^{(p)}$ , we first construct its fair kernel $\\tilde{\\mathbf{K}}^{(p)}=\\mathbf{K}^{(p)}+\\alpha\\mathbf{I}-\\lambda\\mathbf{G}\\mathbf{G}^{T}$ . Then similar to Eq.(4), we define the fair consensus kernel $\\begin{array}{r}{\\tilde{\\mathbf{K}}^{*}=\\sum_{p=1}^{m}\\gamma_{p}^{2}\\tilde{\\mathbf{K}}^{(p)}}\\end{array}$ and take it into Eq.(2) to obtain FMKKM: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{Y},\\gamma}\\mathrm{Tr}\\left(\\tilde{\\mathbf{K}}^{*}\\left(\\mathbf{I}-\\mathbf{Y}\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-1}\\mathbf{Y}^{T}\\right)\\right)\\ \\ s.t.\\ \\mathbf{Y}\\in I n d,\\ \\gamma^{T}\\mathbf{1}=1,\\ \\gamma_{p}\\geq0,\\ \\tilde{\\mathbf{K}}^{*}=\\sum_{p=1}^{m}\\gamma_{p}^{2}\\tilde{\\mathbf{K}}^{(p)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Notice that since our fairness regularization term $\\operatorname{Tr}\\left(\\mathbf{Y}^{T}\\mathbf{G}\\mathbf{G}^{T}\\mathbf{Y}\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-1}\\right)$ requires that $\\mathbf{Y}$ should be a discrete indicator matrix, our FKKM (i.e., Eq.(14)) and FMKKM (i.e., Eq.(15)) directly solve the discrete $\\mathbf{Y}$ instead of the conventional two-step methods which learn an orthogonal embedding $\\mathbf{H}$ first and then obtain the discrete clustering result. As we know, in the two-step methods, the kernel $\\mathbf{k}\\cdot$ -means and the discretization post-processing are separated and when doing the discretization it cannot guarantee the clustering accuracy or fairness. Different from the two-step methods, we can directly learn the final clustering result $\\mathbf{Y}$ by fully considering the clustering accuracy and fairness. ", "page_idx": 4}, {"type": "text", "text": "3.4 Optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.4.1 Optimization of FKKM ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "When minimizing Eq.(14), we only need to solve one variable $\\mathbf{Y}$ . Notice that there is only one 1 in each row of $\\mathbf{Y}$ . Therefore, we can solve $\\mathbf{Y}$ row by row. When solving the $i^{\\th}$ -th row, we replace the $i$ -th row with $[1,0,\\cdot\\cdot\\cdot,0],[0,1,0,\\cdot\\cdot\\cdot,0],...,[0,\\bar{\\cdot}\\cdot\\cdot,0,1]$ respectively, and compute the values of the corresponding objective function to find the one which leads to the maximum. Then we set the $i$ -th row as this row vector. Wang et al. propose an efficient method to compute these objective functions by reducing the computation redundancy [43]. ", "page_idx": 4}, {"type": "text", "text": "3.4.2 Optimization of FMKKM ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In Eq.(15), there are two groups of variables, i.e., $\\mathbf{Y}$ and $\\gamma$ . We solve them by a block coordinate descent method, which optimizes one variable when fixing the other. ", "page_idx": 4}, {"type": "text", "text": "When fixing $\\gamma$ to solve $\\mathbf{Y}$ , we have the following subproblem w.r.t $\\mathbf{Y}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{Y}\\in I n d}\\mathrm{Tr}\\left(\\mathbf{Y}^{T}\\tilde{\\mathbf{K}}^{*}\\mathbf{Y}\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-1}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\tilde{\\mathbf{K}}^{*}=\\sum_{p=1}^{m}\\gamma_{p}^{2}\\tilde{\\mathbf{K}}^{(p)}}\\end{array}$ . It is the same as the optimization of FKKM. ", "page_idx": 4}, {"type": "text", "text": "When fixing $\\mathbf{Y}$ to solve $\\gamma$ , we have following subproblem w.r.t $\\gamma$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\gamma}\\sum_{p=1}^{m}\\gamma_{p}^{2}h_{p},\\ \\ s.t.\\sum_{p=1}^{m}\\gamma_{p}=1,\\ \\gamma_{p}\\geq0,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $h_{p}\\,=\\,\\mathrm{Tr}\\left(\\tilde{\\mathbf{K}}^{\\left(p\\right)}\\left(\\mathbf{I}-\\mathbf{Y}\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-1}\\mathbf{Y}^{T}\\right)\\right)$ . According to Cauchy-Schwarz Inequality, the closed-form solution of $\\gamma_{p}$ is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\gamma_{p}=\\frac{h_{p}^{-1}}{\\sum_{j=1}^{m}h_{j}^{-1}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Appendix A shows the pseudo-codes of FKKM and FMKKM, respectively. When updating each row of $\\mathbf{Y}$ , the objective function of FKKM decreases and has a lower bound. Therefore, FKKM can always converge. Similarly, the convergence of FMKKM can also be guaranteed. Now, we analyze the time complexity. According to [43], optimizing the $i$ -th row of $\\mathbf{Y}$ has a time complexity of $O\\left(n c\\right)$ . FKKM has a time complexity of $O\\left(n^{2}c\\right)$ . Calculating $\\gamma$ has a time complexity of $O\\left(n\\right)$ . Therefore, FMKKM also has a time complexity of $O\\left(n^{2}c\\right)$ . According to [43], although the time complexity is square in the number of instances, it can be computed very efficiently in practice. Therefore, the time complexity of our method is comparable with the mainstream KKM and MKKM methods. ", "page_idx": 5}, {"type": "text", "text": "4 Theoretical Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The generalization error bound of the $\\boldsymbol{\\mathrm{k}}$ -means evaluates the expectation of distance between an unseen data and the clustering center it belongs to [30, 22, 21]. Since FKKM is a special case of FMKKM when $m=1$ , in this section, we derive the generalization error bound of our FMKKM. Before the derivation, we need the following two mild assumptions: ", "page_idx": 5}, {"type": "text", "text": "Assumption 1 Each $\\tilde{\\mathbf{K}}^{(p)}=\\mathbf{K}^{(p)}+\\alpha\\mathbf{I}-\\lambda\\mathbf{G}\\mathbf{G}^{T}$ is a valid kernel matrix, i.e., $\\tilde{\\mathbf{K}}^{(p)}$ is symmetric and p.s.d. ", "page_idx": 5}, {"type": "text", "text": "Remark 2 This assumption is easy to satisfy. $I f\\tilde{\\mathbf{K}}^{(p)}$ is not p.s.d., we can enlarge $\\alpha$ to make the assumption hold. ", "page_idx": 5}, {"type": "text", "text": "Assumption 2 All $\\mathbf{K}^{(p)}$ are upper bounded. We denote b as the maximum of elements in all $\\mathbf{K}^{\\left(p\\right)}$ . ", "page_idx": 5}, {"type": "text", "text": "According to assumption 1, since all $\\tilde{\\mathbf{K}}^{(p)}$ are valid kernel matrices, $\\tilde{\\mathbf{K}}^{*}$ is also a valid kernel matrix. We define the corresponding kernel function of $\\tilde{\\mathbf{K}}^{*}$ as $\\tilde{\\cal K}^{*}(\\cdot,\\cdot)$ , and its kernel mapping function is $\\Phi_{\\gamma}(\\mathbf{x}_{i})=[\\gamma_{1}\\Phi_{1}(\\mathbf{x}_{i})^{T},\\ldots,\\gamma_{m}\\Phi_{m}(\\mathbf{x}_{i})^{T}]^{T}:\\mathbb{R}^{d}\\mapsto\\mathcal{H}$ , where $\\Phi_{1}(\\mathbf{x}_{i}),\\ldots,\\Phi_{m}(\\mathbf{x}_{i})$ are the induced kernel mapping function of $\\tilde{\\mathbf{K}}^{(1)},\\hdots,\\tilde{\\mathbf{K}}^{(m)}$ , respectively. Let $\\mathbf{M}=[\\mathbf{m}_{1},\\dots,\\mathbf{m}_{c}]$ denote the learned centroids matrix in the RKHS $\\mathcal{H}$ , where ${\\bf m}_{i}$ is the center of the $i$ -th cluster in $\\mathcal{H}$ . FMKKM aims to minimize the error: $\\begin{array}{r}{\\mathbb{E}\\left[\\operatorname*{min}_{\\mathbf{y}\\in\\{\\mathbf{e}_{1},\\dots,\\mathbf{e}_{c}\\}}\\|\\Phi_{\\gamma}(\\mathbf{x})-\\mathbf{M}\\mathbf{y}\\|_{\\mathcal{H}}^{2}\\right]}\\end{array}$ , where $[\\mathbf{e}_{1},\\ldots,\\mathbf{e}_{c}]$ are the standard orthonormal basis of $\\mathbb{R}^{c}$ space, i.e., $\\mathbf{e}_{i}$ is an all-zero vectors except that the $i$ -th element is 1. ", "page_idx": 5}, {"type": "text", "text": "Then, we define a function class as our hypothesis space: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{F}=\\left\\{f:\\mathbf{x}\\mapsto\\operatorname*{min}_{\\mathbf{y}\\in\\{\\mathbf{e}_{1},\\ldots,\\mathbf{e}_{c}\\}}\\left\\|\\Phi_{\\gamma}\\left(\\mathbf{x}\\right)-\\mathbf{M}\\mathbf{y}\\right\\|_{\\mathcal{H}}^{2}\\right\\vert\\left.\\gamma^{T}\\mathbf{1}=1,\\gamma_{p}\\geq0,\\mathbf{m}_{k}\\in\\mathcal{H}\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Similar to [21], we have the following Theorem to provide the generalization error bound: ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 Under Assumptions $^{\\,l}$ and 2, given training data $\\mathbf{X}=[\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}].$ , function class $\\mathcal{F}$ defined in Eq.(19), and any $\\delta\\geq0$ , with probability at least $1-\\delta$ , the following inequality holds for all $f\\in\\mathcal F$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}[f(\\mathbf{x})]\\leq\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}f\\left(\\mathbf{x}_{i}\\right)+\\frac{2\\sqrt{2\\pi}}{\\sqrt{n}}\\left[(1+c^{2})\\left(b+\\alpha\\right)-(1+\\frac{c^{2}}{t})\\lambda+c\\sqrt{2\\left(b+\\alpha-\\lambda\\right)\\left(b+\\alpha-\\frac{\\lambda}{t}\\right)}\\right]}\\\\ &{}&{\\quad\\quad+\\left(4\\left(b+\\alpha\\right)-2\\left(1+\\frac{1}{t}\\right)\\lambda\\right)\\sqrt{\\frac{\\log(1/\\delta)}{2n}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $t$ and c are the number of protected groups and clusters, respectively. ", "page_idx": 5}, {"type": "text", "text": "The first term in Eq.(20) is the empirical error. Notice that, we have $\\begin{array}{r l}{\\sum_{i=1}^{n}f\\left(\\mathbf{x}_{i}\\right)}&{{}=}\\end{array}$ $\\mathrm{Tr}\\left(\\tilde{\\bf K}^{*}\\left({\\bf I}-{\\bf Y}\\left({\\bf Y}^{T}{\\bf Y}\\right)^{-1}{\\bf Y}^{T}\\right)\\right)$ , which means our loss function is to minimize exactly this empirical error. However, in the two-step methods, which apply $\\mathbf{H}^{T}\\mathbf{H}=\\mathbf{I}$ where $\\mathbf{H}=\\mathbf{Y}\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{-\\frac{1}{2}}$ to replace ${\\bf Y}\\in I n d$ , they only optimize a continual approximation of the empirical error. ", "page_idx": 6}, {"type": "text", "text": "Besides, the second and third terms represent the gap between the generalization and empirical errors. Intuitively, the gap is the smaller the better. To decrease the gap, we wish $\\alpha$ to be as small as possible. However, Assumption 1 prevents $\\alpha$ being too small because $\\tilde{\\mathbf{K}}^{(p)}$ should be p.s.d., or Theorem 2 will not hold anymore. Now we can derive the lower bound of $\\alpha$ according to Assumption 1. Suppose $\\sigma_{m i n}$ as the smallest eigenvalue of $\\mathbf{K}^{(1)},\\ldots,\\mathbf{K}^{(p)}$ . Then, the smallest eigenvalue of $\\mathbf{K}^{\\left(p\\right)}+\\alpha\\mathbf{I}$ should be no smaller than $\\sigma_{m i n}+\\alpha$ . Notice that we have the following Lemma: ", "page_idx": 6}, {"type": "text", "text": "Lemma 1 Given two real symmetric matrices A and $\\mathbf{B}$ with the same size, where the smallest eigenvalue of A is $\\sigma_{A}$ and the largest eigenvalue of $\\mathbf{B}$ is $\\sigma_{B}$ . If $\\sigma_{A}\\geq\\sigma_{B}$ , then $\\mathbf{A}-\\mathbf{B}$ is p.s.d. ", "page_idx": 6}, {"type": "text", "text": "Proof 3 See Appendix C. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Denoting $\\sigma_{m a x}$ as the largest eigenvalue of $\\mathbf{GG}^{T}$ , it is easy to verify that $\\sigma_{m a x}=|\\mathcal{G}_{m a x}|$ , where $\\mathcal{G}_{m a x}$ is the protected group with the largest number of instances. According to Lemma 1, we have that if $\\sigma_{m i n}\\,+\\,\\alpha\\,-\\,\\lambda\\,*\\,|\\mathcal{G}_{m a x}|\\,\\geq\\,0,\\,\\tilde{\\mathbf{K}}^{(}$ $\\tilde{\\bf K}^{(p)}$ will be p.s.d. Therefore, $\\alpha$ has a lower bound $\\lambda*|\\mathcal{G}_{m a x}|-\\sigma_{m i n}$ . In practice, $\\sigma_{m i n}$ is often very small and close to 0. To avoid the time consuming to compute the eigenvalues of the kernels, we can approximately set $\\alpha=\\lambda*|\\mathcal{G}_{m a x}|$ . ", "page_idx": 6}, {"type": "text", "text": "Take $\\alpha=\\lambda*|\\mathcal{G}_{m a x}|$ back into the generalization error bound Eq.(20). We consider the gap between the generalization and empirical errors, i.e., the second and third terms: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{2\\sqrt{2\\pi}}{\\sqrt{n}}\\left[(1+c^{2})\\left(b+\\alpha\\right)-(1+\\frac{c^{2}}{t})\\lambda+c\\sqrt{2\\left(b+\\alpha-\\lambda\\right)\\left(b+\\alpha-\\frac{\\lambda}{t}\\right)}\\right]+\\left(4\\left(b+\\alpha\\right)-2\\left(1+\\frac{1}{t}\\right)\\lambda\\right)}\\\\ &{}&{\\frac{2\\sqrt{2\\pi}}{\\sqrt{n}}\\left[(1+c^{2})b+\\left(|\\mathcal{G}_{m a x}|-1+\\frac{c^{2}\\left(|\\mathcal{G}_{m a x}|t-1\\right)}{t}\\right)\\lambda+c\\sqrt{2(b+(|\\mathcal{G}_{m a x}|-1)\\lambda)\\left(b+\\frac{|\\mathcal{G}_{m a x}|t-1}{t}\\lambda\\right)}\\right.}\\\\ &{}&{\\left.+\\left(4b+4(|\\mathcal{G}_{m a x}|-1)\\lambda\\right)\\sqrt{\\frac{\\log(1/\\delta)}{2n}}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Notice that $|{\\mathcal{G}}_{m a x}|-1\\geq0$ and $|{\\mathcal{G}}_{m a x}|t-1\\geq0$ , and thus we have that the gap decreases with $\\lambda$ decreases. It means that smaller $\\lambda$ leads to a lower gap. Therefore, $\\lambda$ is a trade-off between the clustering performance and fairness. Increasing $\\lambda$ may enlarge the error bound, but obtain a fairer result. Based on this theoretical analysis, we provide a strategy to set $\\lambda$ by observing a fairness metric, which can be computed without the ground truth. In more detail, we gradually enlarge $\\lambda$ from 0, set $\\alpha=\\lambda*|\\mathcal{G}_{m a x}|$ , and observe the fairness metric. If it gets stable good fairness, we stop enlarging $\\lambda$ and set $\\lambda$ as the current value. This strategy does not need the ground truth, which is appropriate for unsupervised learning, and can obtain an as small as possible $\\lambda$ to achieve a good fairness result. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Data Sets and Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct experiments on benchmark data sets which are widely used in fair clustering, including D&S [2], HAR [3], Jaffe [29], MNIST-USPS [20], Credit Card [52] and K1b [53]. D&S is a human daily and sports activities data set including 8 participants. HAR is a human action recognition data set including 30 participants. In both D&S and HAR data sets, the data of each participant form a protected group. Jaffe is a face image data set. Following [20], the face images with the same expressions are put into a protected group. MNIST-USPS is an image data set containing images of handwritten digits from the subsets of MNIST and USPS data sets. Following [20], we randomly sample 2000 images from MNIST to form one protected group and randomly sample 1800 images from USPS to form the other protected group. Credit card is a data set that describes the customers\u2019 default payments and the data of males and females form two protected groups respectively. K1b is a text data set. Following [48], we randomly assign each text to a protected group with a Bernoulli distribution whose $p=0.5$ to form two protected groups. The statistical information of these data sets is shown in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "CehOqpvOxG/tmp/7cc2591f1ec2cb2cd50bbdbda07bd370958832060db8271af03c021f727f49e2.jpg", "table_caption": ["Table 1: Comparison results on the single kernel setting. The best and second best results are denoted in bold and underlined, respectively. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "CehOqpvOxG/tmp/406ef5cf9123b3be51c42dfe50d76494b7654d567e7be0b8ddb742f5f6f5bded.jpg", "img_caption": ["Figure 1: Fairness visualization results of FMKKM-f and FMKKM on D&S. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "In the single kernel setting, we compare our FKKM with K-means [13], Kernel K-means (KKM) [10], Spectral Clustering (SC) [33], and three state-of-the-art fair clustering methods, including SpFC [18], VFC [58], and FFC [34]. For the kernel meth\u221aods (i.e., our FKKM and KKM), we use a Gaussian kernel with a bandwidth parameter fixing to $\\sqrt{0.5}*D$ , where $D$ is the average distance between samples. In the multiple kernel setting, we compare our FMKKM with 9 state-of-the-art MKKM methods, including ONKC [25], MKCSS [55], DPMKKM [42], LFLKA [51], EMKC [38], OSLR [47], ASLR [46], CSAMKC [56], FAMKKM [41]. Detailed information of these compared methods is shown in Appendix E. Besides, for an ablation study, we also compare with the degeneration version of our method, which is without the fairness regularization term, denoted as FKKM-f (for single kernel version) and FMKKM-f (for multiple kernel version). ", "page_idx": 7}, {"type": "text", "text": "In the multiple kernel setting, following [11], we construct 12 kernels, including seven Gaussian kernels $\\begin{array}{r l r}{{\\bf K}\\left({\\bf x}_{i},{\\bf x}_{j}\\right)}&{{}\\!=\\!}&{\\exp\\left(-\\|{\\bf x}_{i}-{\\bf x}_{j}\\|_{2}^{2}/2\\epsilon^{2}\\right)}\\end{array}$ with $\\epsilon\\;\\;=\\;\\;\\sqrt{s}\\;*\\;D$ , where $s$ varies in the range of $\\{{\\textstyle\\frac{1}{8}},{\\textstyle\\frac{1}{4}},{\\textstyle\\frac{1}{2}},1,2,4,8\\}$ and $D$ is the average distance between samples; four polynomial kernels $\\mathbf{K}\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right)\\ =\\ \\left(a+\\mathbf{x}_{i}^{T}\\mathbf{x}_{j}\\right)^{b}$ with $a~=~\\{0,1\\}$ and $b~=~\\{2,4\\}$ ; and a cosine kernel $\\mathbf{K}\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right)\\ =\\ \\left(\\mathbf{x}_{i}^{T}\\mathbf{x}_{j}\\right)/\\left(\\left\\|\\mathbf{x}_{i}\\right\\|\\cdot\\left\\|\\mathbf{x}_{j}\\right\\|\\right)$ . Finally, all kernels have been normalized through $\\mathbf{K}\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right)/\\sqrt{\\mathbf{K}\\left(\\mathbf{x}_{i},\\mathbf{x}_{i}\\right)\\mathbf{K}\\left(\\mathbf{x}_{j},\\mathbf{x}_{j}\\right)}$ and then rescaled to $[0,1]$ . We use Accuracy (ACC) and Normalized Mutual Information (NMI) to evaluate the clustering performance. Besides, we also use balance (Bal) [20] and Minimal Normalized Conditional Entropy (MNCE) [49] to evaluate fairness. Specifically, Bal is defined as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname{Bal}\\left(\\mathcal{C}\\right)=\\operatorname*{min}_{k}\\left(\\frac{N_{k}^{\\mathrm{min}}}{N_{k}^{\\mathrm{max}}}\\right)\\in[0,1],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $N_{k}^{m i n}$ and $N_{k}^{m a x}$ represent the number of instances in the smallest and the largest (in size) protected groups in cluster $\\pi_{k}$ , respectively. MNCE is defined as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{MNCE}=\\frac{\\operatorname*{min}_{k}\\left(-\\sum_{i}\\frac{|\\mathcal{G}_{i}\\cap\\pi_{k}|}{|\\pi_{k}|}\\log\\frac{|\\mathcal{G}_{i}\\cap\\pi_{k}|}{|\\pi_{k}|}\\right)}{-\\sum_{i}\\frac{|\\mathcal{G}_{i}|}{n}\\log\\frac{|\\mathcal{G}_{i}|}{n}}\\in[0,1].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "All metrics are the larger the better. Based on previous analysis of hyper-parameter setting, we search $\\lambda$ as $\\lambda=1,2,\\ldots$ , by observing the corresponding MNCE. When the MNCE gets stable, i.e., the change of MNCE is smaller than 0.005, we stop the searching and use the current $\\lambda$ . For ", "page_idx": 7}, {"type": "table", "img_path": "CehOqpvOxG/tmp/cdb370180079728f29b959f9462c9812a47547cd228b65832032268b1f7a5d09.jpg", "table_caption": ["Table 2: Comparison results on the multiple kernel setting. The best and second best results are denoted in bold and underlined, respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "other comparison methods, we follow their recommended parameter configurations and search methodologies. All experiments are conducted on the 12th Gen Interl(R) Core(TM) i7-12700 with 32 GB RAM. All experiments are repeated 10 times and the average results are reported. ", "page_idx": 8}, {"type": "text", "text": "5.2 Experimental Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 1 shows the comparison results in the single kernel setting, where the best and second best results are denoted in bold and underlined, respectively. It can be seen that FKKM exhibits better fairness compared to K-means, KKM, SC, our ablation version (i.e., FKKM-f), and even the fair clustering methods, indicating the effectiveness of our fairness regularization term. When comparing w.r.t. clustering performance (i.e., ACC and NMI), FKKM still often achieves the best or the second-best results. ", "page_idx": 8}, {"type": "text", "text": "Table 2 presents the comparison results in the multiple kernel setting. FMKKM easily achieves the best fairness, due to the effectiveness of our fairness regularization term. Moreover, FMKKM often achieves better or at least comparable ACC and NMI. Notice that our method just simply modifies the original MKKM and can achieve competitive clustering performance, demonstrating that our method is simple yet effective. ", "page_idx": 8}, {"type": "text", "text": "Figure 1 shows the visualization results. It shows the number of instances of each protected group $\\mathcal{G}_{j}$ in each cluster $\\pi_{i}$ in the D&S data set obtained by FMKKM-f and FMKKM, respectively. As shown in Figure 1(a), in FMKKM-f, without the fairness regularization term, the numbers of data of each protected group in each cluster have a great difference, which means the result is unfair. Figure 1 (b) shows that the distribution of protected group in each cluster is more balanced, which means the result obtained by FMKKM is much fairer than FMKKM-f. It demonstrates the effectiveness of our fair regularization term. ", "page_idx": 8}, {"type": "text", "text": "5.3 Efficiency Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The convergence curves of our methods are shown in Appendix F. The results show that our methods often converge very fast. We also conduct experiments to compare the running time of our methods with other compared methods. Our method is faster than or at least comparable with other methods on many data sets. The detailed results are shown in Appendix F. ", "page_idx": 8}, {"type": "text", "text": "5.4 Parameter Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Figure 2 shows the effects of $\\lambda$ of FKKM and FMKKM on MNIST-USPS and Credit Cards data sets. The other results are similar. The red points denote the $\\lambda$ selected by our strategy. We can see that ", "page_idx": 8}, {"type": "image", "img_path": "CehOqpvOxG/tmp/ee9a402c1ca834212de309a13269c23f2040b5561498696d79e866dfe6d14754.jpg", "img_caption": ["(e) NMI on MNIST-(f) MNCE on MNIST-(g) NMI on Credit Card(h) MNCE on Credit USPS (FMKKM) USPS (FMKKM) (FMKKM) Card (FMKKM) "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 2: NMI and MNCE of our methods on MNIST-USPS and Credit Card data sets w.r.t. different values of $\\lambda$ . The red points represent the lambda that our algorithm automatically searches for. ", "page_idx": 9}, {"type": "text", "text": "with the increase of $\\lambda$ , the fairness grows and the clustering performance may decrease, which is consistent with our previous discussion. We can often achieve a good trade-off between fairness and performance at the red point, which shows the effectiveness of our hyper-parameter setting strategy. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we focused on the fairness issue in KKM and MKKM. We carefully designed a novel fairness regularization term, which can be seamlessly plugged into the KKM and MKKM framework. Equipped with this fairness regularization term, we proposed a novel FKKM and FMKKM method. We also provided a hyper-parameter setting strategy based on the theoretical analysis to make the methods easy to use. Extensive experiments demonstrated the effectiveness and superiority of our proposed FKKM and FMKKM methods. ", "page_idx": 9}, {"type": "text", "text": "Although the proposed methods achieve promising performance on fairness, they still have some limitations. For example, in our methods, the protected groups must be pre-given or decided by humans. An interesting question is how to automatically decide the protected groups without human intervention. In the future, we will focus on this problem. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the National Natural Science Foundation of China grants 62176001 and 62376146, and Natural Science Project of Anhui Provincial Education Department grants 2023AH030004. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] M. Abbasi, A. Bhaskara, and S. Venkatasubramanian. Fair clustering via equitable group representations. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 504\u2013514, 2021.   \n[2] K. Altun, B. Barshan, and O. Tun\u00e7el. Comparative study on classifying human activities with miniature inertial and magnetic sensors. Pattern Recognition, page 3605\u20133620, Oct 2010.   \n[3] D. Anguita, A. Ghio, L. Oneto, X. Parra, and J. Reyes-Ortiz. A public domain dataset for human activity recognition using smartphones. The European Symposium on Artificial Neural Networks,The European Symposium on Artificial Neural Networks, Jan 2013.   \n[4] A. Backurs, P. Indyk, K. Onak, B. Schieber, A. Vakilian, and T. Wagner. Scalable fair clustering. In International Conference on Machine Learning, pages 405\u2013413. PMLR, 2019. [5] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463\u2013482, 2002.   \n[6] S. Bera, D. Chakrabarty, N. Flores, and M. Negahbani. Fair algorithms for clustering. Advances in Neural Information Processing Systems, 32, 2019. [7] I. O. Bercea, M. Gro\u00df, S. Khuller, A. Kumar, C. R\u00f6sner, D. R. Schmidt, and M. Schmidt. On the cost of essentially fair clusterings. arXiv preprint arXiv:1811.10319, 2018. [8] A. Chhabra, P. Li, P. Mohapatra, and H. Liu. Robust fair clustering: A novel fairness attack and defense framework. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [9] F. Chierichetti, R. Kumar, S. Lattanzi, and S. Vassilvitskii. Fair clustering through fairlets. In I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5029\u20135037, 2017.   \n[10] I. S. Dhillon, Y. Guan, and B. Kulis. Kernel k-means: spectral clustering and normalized cuts. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 551\u2013556, 2004.   \n[11] L. Du, P. Zhou, L. Shi, H. Wang, M. Fan, W. Wang, and Y.-D. Shen. Robust multiple kernel kmeans using l21-norm. In Twenty-fourth international joint conference on artificial intelligence, 2015.   \n[12] M. Ghadiri, S. Samadi, and S. Vempala. Socially fair k-means clustering. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, Mar 2021.   \n[13] G. Hamerly and C. Elkan. Learning the k in k-means. Advances in neural information processing systems, 16, 2003.   \n[14] T. Handhayani and L. Hiryanto. Intelligent kernel $\\mathbf{k}$ -means for clustering gene expression. Procedia Computer Science, 59:171\u2013177, 2015.   \n[15] L. He and H. Zhang. Kernel k-means sampling for nystr\u00f6m approximation. IEEE Transactions on Image Processing, 27(5):2108\u20132120, 2018.   \n[16] D. S. Hochba. Approximation algorithms for np-hard problems. ACM Sigact News, 28(2):40\u201352, 1997.   \n[17] J. Huang, F. Nie, and H. Huang. Spectral rotation versus k-means in spectral clustering. Proceedings of the AAAI Conference on Artificial Intelligence, page 431\u2013437, Jun 2022.   \n[18] M. Kleindessner, S. Samadi, P. Awasthi, and J. Morgenstern. Guarantees for spectral clustering with fairness constraints. In International conference on machine learning, pages 3458\u20133467. PMLR, 2019.   \n[19] M. Li, Y. Zhang, S. Liu, Z. Liu, and X. Zhu. Simple multiple kernel k-means with kernel weight regularization. Information Fusion, 100:101902, 2023.   \n[20] P. Li, H. Zhao, and H. Liu. Deep fair clustering for visual learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9067\u20139076, June 2020.   \n[21] J. Liu, X. Liu, J. Xiong, Q. Liao, S. Zhou, S. Wang, and Y. Yang. Optimal neighborhood multiple kernel clustering with adaptive local kernels. IEEE Transactions on Knowledge and Data Engineering, 34(6):2872\u20132885, 2020.   \n[22] T. Liu, D. Tao, and D. Xu. Dimensionality-dependent generalization bounds for $\\boldsymbol{\\mathrm{k}}$ -dimensional coding schemes. Neural computation, 28(10):2213\u20132249, 2016.   \n[23] X. Liu. Simplemkkm: Simple multiple kernel k-means. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(4):5174\u20135186, 2022.   \n[24] X. Liu, S. Zhou, L. Liu, C. Tang, S. Wang, J. Liu, and Y. Zhang. Localized simple multiple kernel k-means. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9293\u20139301, 2021.   \n[25] X. Liu, S. Zhou, Y. Wang, M. Li, Y. Dou, E. Zhu, and J. Yin. Optimal neighborhood kernel clustering with multiple kernels. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017.   \n[26] X. Liu, X. Zhu, M. Li, L. Wang, E. Zhu, T. Liu, M. Kloft, D. Shen, J. Yin, and W. Gao. Multiple kernel $k$ k-means with incomplete kernels. IEEE transactions on pattern analysis and machine intelligence, 42(5):1191\u20131204, 2019.   \n[27] Y. Lu, L. Wang, J. Lu, J. Yang, and C. Shen. Multiple kernel clustering based on centered kernel alignment. Pattern Recognition, 47(11):3656\u20133664, 2014.   \n[28] Y. Lu, X. Zheng, J. Lu, R. Wang, F. Nie, and X. Li. Self-paced and discrete multiple kernel kmeans. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pages 4284\u20134288, 2022.   \n[29] M. J. Lyons, S. Akamatsu, M. Kamachi, J. Gyoba, and J. Budynek. The japanese female facial expression (jaffe) database. In Proceedings of third international conference on automatic face and gesture recognition, pages 14\u201316, 1998.   \n[30] A. Maurer and M. Pontil. $k$ -dimensional coding schemes in hilbert spaces. IEEE Transactions on Information Theory, 56(11):5839\u20135846, 2010.   \n[31] A. Maurer and M. Pontil. K -dimensional coding schemes in hilbert spaces. IEEE Trans. Inf. Theory, 56(11):5839\u20135846, 2010.   \n[32] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan. A survey on bias and fairness in machine learning. ACM computing surveys (CSUR), 54(6):1\u201335, 2021.   \n[33] A. Ng, M. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. Advances in neural information processing systems, 14, 2001.   \n[34] R. Pan and C. Zhong. Fairness first clustering: A multi-stage approach for mitigating bias. Electronics, page 2969, Jul 2023.   \n[35] Y. Pang, J. Xie, F. Nie, and X. Li. Spectral clustering by joint spectral embedding and spectral rotation. IEEE Transactions on Cybernetics, page 247\u2013258, Jan 2020.   \n[36] A. Saxena, G. Fletcher, and M. Pechenizkiy. Fairsna: Algorithmic fairness in social network analysis. ACM Computing Surveys, 2022.   \n[37] M. Schmidt, C. Schwiegelshohn, and C. Sohler. Fair coresets and streaming algorithms for fair k-means. In Approximation and Online Algorithms: 17th International Workshop, WAOA 2019, Munich, Germany, September 12\u201313, 2019, Revised Selected Papers 17, pages 232\u2013251. Springer, 2020.   \n[38] C. Tang, Z. Li, W. Yan, G. Yue, and W. Zhang. Efficient multiple kernel clustering via spectral perturbation. In Proceedings of the 30th ACM International Conference on Multimedia, pages 1603\u20131611, 2022.   \n[39] G. Tzortzis and A. Likas. The global kernel k-means clustering algorithm. In 2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence), pages 1977\u20131984. IEEE, 2008.   \n[40] B. Wang and I. Davidson. Towards fair deep clustering with multi-state protected variables. arXiv preprint arXiv:1901.10053, 2019.   \n[41] J. Wang, C. Tang, X. Zheng, X. Liu, W. Zhang, E. Zhu, and X. Zhu. Fast approximated multiple kernel k-means. IEEE Transactions on Knowledge and Data Engineering, 2023.   \n[42] R. Wang, J. Lu, Y. Lu, F. Nie, and X. Li. Discrete and parameter-free multiple kernel k-means. IEEE Transactions on Image Processing, 31:2796\u20132808, 2022.   \n[43] R. Wang, J. Lu, Y. Lu, F. Nie, and X. Li. Discrete and parameter-free multiple kernel k-means. IEEE Transactions on Image Processing, page 2796\u20132808, Jan 2022.   \n[44] D. Yan, L. Huang, and M. I. Jordan. Fast approximate spectral clustering. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 907\u2013916, 2009.   \n[45] D. Yin, R. Kannan, and P. Bartlett. Rademacher complexity for adversarially robust generalization. In International conference on machine learning, pages 7085\u20137094. PMLR, 2019.   \n[46] J. You, Z. Ren, Q. Sun, Y. Sun, and X. Li. Approximate shifted laplacian reconstruction for multiple kernel clustering. In Proceedings of the 30th ACM International Conference on Multimedia, pages 2862\u20132870, 2022.   \n[47] J. You, Z. Ren, F. R. Yu, and X. You. One-stage shifted laplacian refining for multiple kernel clustering. IEEE Transactions on Neural Networks and Learning Systems, 2023.   \n[48] M. B. Zafar, I. Valera, M. Gomez-Rodriguez, and K. P. Gummadi. Fairness constraints: A mechanism for fair classification. CoRR, abs/1507.05259, 2015.   \n[49] P. Zeng, Y. Li, P. Hu, D. Peng, J. Lv, and X. Peng. Deep fair clustering via maximizing and minimizing mutual information: Theory, algorithm and metric. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 23986\u201323995. IEEE, 2023.   \n[50] R. Zhang and A. I. Rudnicky. A large scale clustering scheme for kernel k-means. In 2002 International Conference on Pattern Recognition, volume 4, pages 289\u2013292. IEEE, 2002.   \n[51] T. Zhang, X. Liu, L. Gong, S. Wang, X. Niu, and L. Shen. Late fusion multiple kernel clustering with local kernel alignment maximization. IEEE Transactions on Multimedia, 25:993\u20131007, 2021.   \n[52] L. Zheng, Y. Zhu, and J. He. Fairness-aware multi-view clustering. In Proceedings of the 2023 SIAM International Conference on Data Mining (SDM), pages 856\u2013864. SIAM, 2023.   \n[53] S. Zhong and J. Ghosh. Generative model-based document clustering: a comparative study. Knowl. Inf. Syst., 8(3):374\u2013384, 2005.   \n[54] P. Zhou, L. Du, L. Shi, H. Wang, and Y. Shen. Recovery of corrupted multiple kernels for clustering. In Q. Yang and M. J. Wooldridge, editors, Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 4105\u20134111. AAAI Press, 2015.   \n[55] S. Zhou, X. Liu, M. Li, E. Zhu, L. Liu, C. Zhang, and J. Yin. Multiple kernel clustering with neighbor-kernel subspace segmentation. IEEE transactions on neural networks and learning systems, 31(4):1351\u20131362, 2019.   \n[56] S. Zhou, Q. Ou, X. Liu, S. Wang, L. Liu, S. Wang, E. Zhu, J. Yin, and X. Xu. Multiple kernel clustering with compressed subspace alignment. IEEE Transactions on Neural Networks and Learning Systems, 34(1):252\u2013263, 2021.   \n[57] I. M. Ziko, J. Yuan, E. Granger, and I. B. Ayed. Variational fair clustering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 11202\u201311209, 2021.   \n[58] I. M. Ziko, J. Yuan, E. Granger, and I. Ben Ayed. Variational fair clustering. Proceedings of the AAAI Conference on Artificial Intelligence, 35(12):11202\u201311209, Sep 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Pseudo-codes of FKKM and FMKKM ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Algorithms 1 and 2 show the pseudo-codes of our FKKM and FMKKM, respectively. ", "page_idx": 13}, {"type": "table", "img_path": "CehOqpvOxG/tmp/f915d80b3a239d8324d57d8422a85535709f4bba7ac43ba913f315d9702f4ac1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Algorithm 2 Fair Multiple Kernel K-means ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Input: Kernel matrices $\\{\\mathbf{K}^{\\left(p\\right)}\\}_{p=1}^{m}$ , protected groups $\\mathcal{G}_{1},\\cdot\\cdot\\cdot,\\mathcal{G}_{t}$ , fairness hyper-parameter $\\lambda$ . 1: Construct protected group indicator matrix $\\mathbf{G}$ and calculate $\\alpha$ as $\\alpha=|\\mathcal{G}_{m a x}|*\\lambda$ .   \n2: Construct the corresponding fair kernel by $\\tilde{\\mathbf{K}}^{(p)}=\\mathbf{K}^{(p)}+\\alpha\\mathbf{I}-\\lambda\\mathbf{G}\\mathbf{G}^{T}$ for each base kernel matrix K(p).   \n3: Initialize \u03b3 = m1 and $\\mathbf{Y}$ by running standard kernel k-means on $\\textstyle\\sum_{p=1}^{m}\\gamma_{p}^{2}\\mathbf{K}^{(p)}$ .   \n4: repeat   \n5: Update $\\mathbf{Y}$ row by row by solving Eq.(16).   \n6: Update $\\gamma$ by Eq.(18)   \n7: until Converges   \nOutput: The final partition matrix $\\mathbf{Y}$ . ", "page_idx": 13}, {"type": "text", "text": "B Proof of Theorem 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Denote $\\begin{array}{r}{\\hat{R}\\left(\\mathbf{M},\\gamma\\right)=\\frac{1}{n}\\sum_{i=1}^{n}\\operatorname*{min}_{\\mathbf{y}\\in\\left\\{\\mathbf{e}_{1},\\ldots,\\mathbf{e}_{c}\\right\\}}\\left\\|\\Phi_{\\gamma}\\left(\\mathbf{x}_{i}\\right)-\\mathbf{M}\\mathbf{y}\\right\\|_{\\mathcal{H}}^{2}}\\end{array}$ . Our goal is to bound: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f\\in\\mathcal{F}}\\left(\\mathbb{E}[f(\\mathbf{x})]-\\frac{1}{n}\\sum_{i=1}^{n}f\\left(\\mathbf{x}_{i}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "According to Assumption 2, we have the largest value of elements in $\\mathbf{K}^{(p)}$ is $b$ . Now, we consider $\\tilde{\\mathbf{K}}^{(p)}=\\bar{\\mathbf{K}}^{(p)}+\\alpha\\mathbf{I}-\\lambda\\mathbf{G}\\mathbf{G}^{T}$ . Since the diagonal elements of $\\mathbf{GG}^{T}$ are 1, given any $\\mathbf{x}_{i}$ , we have that $\\begin{array}{r}{\\Phi_{\\gamma}^{T}\\left(\\mathbf{x}_{i}\\right)\\Phi_{\\gamma}\\left(\\mathbf{x}_{i}\\right)=\\sum_{p=1}^{m}\\gamma_{p}^{2}\\Phi_{p}^{T}\\left(\\mathbf{x}_{i}\\right)\\Bar{\\Phi_{p}^{-}}(\\mathbf{x}_{i})\\le b+\\alpha-\\lambda,}\\end{array}$ Given two different instances $\\mathbf{x}_{i}$ and $\\mathbf{x}_{j}$ , if they belong to the same protected group, we have that the $(i,j)$ -th element in $\\mathbf{GG}^{T}$ is 1, and thus $\\begin{array}{r}{\\Phi_{\\gamma}^{T}\\left(\\mathbf{x}_{i}\\right)\\Phi_{\\gamma}\\left(\\mathbf{x}_{j}\\right)=\\sum_{p=1}^{m}\\gamma_{p}^{2}\\Phi_{p}^{T}\\left(\\mathbf{x}_{i}\\right)\\Phi_{p}\\left(\\mathbf{x}_{j}\\right)\\leq b-\\lambda.}\\end{array}$ . If $\\mathbf{x}_{i}$ and $\\mathbf{x}_{j}$ belong to different protected groups, we have that the $(i,j)$ -th element in $\\mathbf{GG}^{T}$ is 0, and thus $\\Phi_{\\gamma}^{T}\\left(\\mathbf{x}_{i}\\right)\\Phi_{\\gamma}\\left(\\mathbf{x}_{j}\\right)\\,=$ $\\begin{array}{r}{\\sum_{p=1}^{m}\\gamma_{p}^{2}\\Phi_{p}^{T}\\left(\\mathbf{x}_{i}\\right)\\Phi_{p}\\left(\\mathbf{x}_{j}\\right)\\leq b.}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "Then, notice that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{y}\\in\\left\\{\\mathbf{e}_{1},\\ldots,\\mathbf{e}_{c}\\right\\}}\\left\\|\\Phi_{\\gamma}\\left(\\mathbf{x}_{i}\\right)-\\mathbf{M}\\mathbf{y}\\right\\|_{\\mathcal{H}}^{2}=\\operatorname*{min}\\left\\{\\left\\|\\Phi_{\\gamma}\\left(\\mathbf{x}_{i}\\right)-\\mathbf{m}_{1}\\right\\|_{\\mathcal{H}}^{2},\\cdots,\\left\\|\\Phi_{\\gamma}\\left(\\mathbf{x}_{i}\\right)-\\mathbf{m}_{c}\\right\\|_{\\mathcal{H}}^{2}\\right\\},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where ${\\bf m}_{k}$ denotes the $k$ -th cluster centroid. Next, we denote $a_{i}=|\\pi_{k}\\cap\\mathcal{G}_{i}|$ to represent the number of instances in all protected groups in cluster $\\pi_{k}$ . We have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Phi_{r}\\left(\\mathbf{x}_{i}\\right)-\\mathbf{m}_{\\mathbf{i}}\\Big\\Vert_{\\mathcal{H}}^{2}=\\left\\Vert\\Phi_{r}(\\mathbf{x}_{i})-\\frac{1}{|\\pi|}\\sum_{k=1}^{\\infty}\\Phi_{\\mathbf{r}}(\\mathbf{x}_{i})\\right\\Vert_{\\mathcal{H}}^{2}}&{}\\\\ {\\leq2\\left(\\Phi_{r}^{T}(\\mathbf{x}_{i})\\Phi_{r}(\\mathbf{x}_{i})+\\frac{1}{|\\pi|}\\sum_{k=1}^{\\infty}\\sum_{1\\atop{k=1}}^{\\infty}\\Phi_{r}(\\mathbf{x}_{i})^{T}\\Phi_{r}(\\mathbf{x}_{i})\\right)}&{}\\\\ {\\leq2\\left((b+\\alpha-\\lambda)+\\frac{\\left(|\\pi_{k}|\\right)\\left(b+\\alpha-\\lambda\\right)+\\left(\\sum_{i=1}^{n}\\alpha_{i}^{2}-|\\pi_{k}|\\right)\\left(b-\\lambda\\right)+\\left(|\\pi_{k}|^{2}-\\alpha\\right)}{|\\pi_{k}|^{2}}\\right.}&{}\\\\ {=2\\left((b+\\alpha-\\lambda)+\\frac{|\\pi_{k}|-\\alpha-\\lambda\\sum_{i=1}^{n}\\alpha_{i}^{2}+b|\\pi_{k}|^{2}}{|\\pi_{k}|^{2}}\\right)}&{}\\\\ {\\leq2\\left(b+\\alpha-\\lambda+b-\\frac{\\lambda}{L}+\\frac{\\alpha}{|\\pi_{k}|}\\right)}&{}\\\\ {\\leq4(b+\\alpha)-2\\left(1+\\frac{1}{L}\\right)\\lambda}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The second to last inequality holds due to the Cauchy-Schwarz inequality  it=1 ai2 \u2265 ( it=t1 ai) and $\\begin{array}{r}{\\sum_{i=1}^{t}a_{i}=|\\pi_{k}|}\\end{array}$ . Therefore, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n0\\leq f\\left(x_{i}\\right)\\leq4\\left(b+\\alpha\\right)-2\\left(1+{\\frac{1}{t}}\\right)\\lambda.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "According to the Theorem 3.1 in [30], by utilizing McDiarmid\u2019s inequality, we have that for any $\\delta\\geq0$ , with probability at least $1-\\delta$ , for all $f\\in\\mathcal F$ , the following inequality holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(\\mathbf{x})]-\\frac{1}{n}\\sum_{i=1}^{n}f\\left(\\mathbf{x}_{i}\\right)\\leq2\\Re n(\\mathcal{F})+\\left(4\\left(b+\\alpha\\right)-2\\left(1+\\frac{1}{t}\\right)\\lambda\\right)\\sqrt{\\frac{\\log(1/\\delta)}{2n}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Re_{n}(\\mathcal{F})=\\frac{1}{n}\\mathbb{E}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{i=1}^{n}\\sigma_{i}f\\left(\\mathbf{x}_{i}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "represents the Rademacher complexity of $\\mathcal{F}$ [45]. $\\sigma_{1},\\ldots,\\sigma_{n}$ are Rademacher random variables uniformly distributed on $\\{-1,1\\}$ . ", "page_idx": 14}, {"type": "text", "text": "Next, we introduce the Gaussian complexity to provide an upper bound for $\\Re_{n}({\\mathcal{F}})$ [5]: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathfrak{G}_{n}(\\mathcal{F})=\\frac{1}{n}\\mathbb{E}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{i=1}^{n}\\beta_{i}f\\left(\\mathbf{x}_{i}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\beta_{1},\\ldots,\\beta_{n}$ are Gaussian random variables with zero mean and unit standard deviation. To bound the Rademacher complexity, we need the following two lemmas: ", "page_idx": 14}, {"type": "text", "text": "Lemma 2 $\\begin{array}{r}{[23J\\,\\mathfrak{R}_{n}(\\mathcal{F})\\leq\\sqrt{\\frac{\\pi}{2}}\\mathfrak{G}_{n}(\\mathcal{F})}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "Lemma 3 [23] Let $\\begin{array}{r}{G_{f}=\\sum_{i=1}^{n}\\beta_{i}G\\left(\\mathbf{x}_{i},f\\right)}\\end{array}$ and $\\begin{array}{r}{H_{f}=\\sum_{i=1}^{n}\\beta_{i}H\\left(\\mathbf{x}_{i},f\\right)}\\end{array}$ be two zero-mean separable Gaussian processes. If for all $f_{1},f_{2}\\in{\\mathcal{F}}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left(G_{f_{1}}-G_{f_{2}}\\right)^{2}\\right]\\le\\mathbb{E}\\left[\\left(H_{f_{1}}-H_{f_{2}}\\right)^{2}\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "then we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}G_{f}\\right]\\leq\\mathbb{E}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}H_{f}\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In our setting, we define: ", "page_idx": 15}, {"type": "equation", "text": "$$\nG\\left(\\mathbf{x}_{i},f\\right)=G_{\\mathbf{M},\\gamma}\\triangleq\\sum_{i=1}^{n}\\beta_{i}\\left(\\operatorname*{min}_{\\mathbf{y}\\in\\left\\{\\mathbf{e}_{1},\\ldots,\\mathbf{e}_{k}\\right\\}}\\left\\|\\Phi_{\\gamma}\\left(\\mathbf{x}_{i}\\right)-\\mathbf{M}\\mathbf{y}\\right\\|_{\\mathcal{H}}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, we aim to find $H_{f}$ (i.e., $H_{\\mathbf{M},\\gamma.}$ ) such that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\beta}\\left[\\left(G_{\\mathbf{M}_{1},\\gamma_{1}}-G_{\\mathbf{M}_{2},\\gamma_{2}}\\right)^{2}\\right]\\leq\\mathbb{E}_{\\beta}\\left[\\left(H_{\\mathbf{M}_{1},\\gamma_{1}}-H_{\\mathbf{M}_{2},\\gamma_{2}}\\right)^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Specifically, for any $f_{1},f_{2}\\in{\\mathcal{F}}$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(\\operatorname*{min}\\left\\{\\Phi_{j,x}\\left(x_{j}\\right)-\\Lambda_{j,x}(x_{j})\\right\\}_{j=j=0}^{n}+n\\alpha_{j}\\Lambda_{j,x}^{(n)}\\right)^{2}}\\\\ &{\\le\\left(n\\alpha_{j}\\left(\\Phi_{j,x}\\left(\\Lambda_{j}\\right),\\Lambda_{j}\\right)\\right)^{2}}\\\\ &{=\\left(\\left(\\Phi_{j,x}\\left(\\Lambda_{j}\\right)\\right)_{j=0}^{n}-[\\Phi_{j,x}(x_{j})]_{j=0}^{n}\\right)^{2}+n\\alpha_{j}\\left([\\Phi_{j,x}^{\\prime}\\left(\\Lambda_{j}\\right)\\Lambda_{j}-\\Phi_{j,x}^{\\prime}\\left(\\Lambda_{j}\\right)]_{j}\\right)^{2}+n^{2}\\alpha_{j}^{2}\\left([\\Lambda_{j}\\right)\\Lambda_{j}-\\Delta_{j,x}^{(n)}\\right)}\\\\ &{\\le\\left(\\left(\\Phi_{j,x}\\left(\\Lambda_{j}\\right)\\right)_{j=0}^{n}-[\\Phi_{j,x}(x_{j})]_{j}\\right)_{j}^{2}+n\\alpha_{j}^{2}\\left([\\Phi_{j,x}^{\\prime}\\left(\\Lambda_{j}\\right)\\Lambda_{j}-\\Phi_{j,x}^{\\prime}\\left(\\Lambda_{j}\\right)\\ M_{j}\\right)\\right)\\ge\\eta^{\\prime}\\left([\\Lambda_{j}]-\\Delta_{j,x}^{(n)}\\right)}\\\\ &{\\le\\left(\\left(\\Phi_{j,x}\\left(\\Lambda_{j}\\right)\\right)_{j=0}^{n}-[\\Phi_{j,x}(x_{j})]_{j}\\right)_{j}^{2}+n^{2}\\alpha_{j}^{2}\\left([\\Phi_{j,x}^{\\prime}\\left(\\Lambda_{j}\\right)\\Lambda_{j}-\\Phi_{j,x}^{\\prime}\\left(\\Lambda_{j}\\right)\\Lambda_{j}\\right)\\right)\\ge\\eta^{\\prime}\\left([\\Lambda_{j}]-\\Delta_{j,x}^{(n)}\\right)}\\\\ &{=\\left(\\left([\\Phi_{j,x}(x_{j})]_{j}\\right)_{j=0}^{n}-[\\Phi_{j,x}(x_{j})]_{j}\\right)_{j}^{2}+n^{2}\\alpha_{j}^{2}\\left([\\Phi_{j,x}^{\\prime}\\left(\\Lambda_{j}\\right)\\Lambda_{j}-\\Phi_{j,x}^{\\prime}\\left(\\Lambda_{j} \n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The final two inequalities hold due to $(a\\,+\\,b\\,+\\,c)^{2}\\:\\leq\\:4a^{2}\\,+\\,2b^{2}\\,+\\,4c^{2}$ , $\\scriptstyle\\sum_{r=1}^{c}y_{r}\\ =\\ 1$ , and $\\begin{array}{r}{\\sum_{r,s=1}^{c}y_{r}y_{s}=1}\\end{array}$ . Therefore, combining Eq.(33) and Eq.(35), we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\beta}\\left[\\left(G_{{\\bf A}_{1},\\gamma_{1}}-G_{{\\bf M}_{2},\\gamma_{2}}\\right)^{2}\\right]}\\\\ &{=\\!\\mathbb{E}_{\\beta}\\left[\\left(\\displaystyle\\sum_{i=1}^{n}\\beta_{i}\\left[\\operatorname*{min}_{|\\bf\\hat{x}_{\\beta}|}|\\mathbf{\\hat{x}}_{\\gamma_{1}}\\left({\\bf x}_{i}\\right)-{\\bf M}_{1}{\\bf y}|\\right]_{\\mathcal{H}}^{2}-\\operatorname*{min}_{|\\bf\\hat{y}_{\\beta}|}|\\boldsymbol{\\Phi}_{\\gamma_{2}}\\left({\\bf x}_{i}\\right)-{\\bf M}_{2}{\\bf y}|\\right|_{\\mathcal{H}}^{2}\\right]\\right)^{2}\\Bigg]}\\\\ &{=\\!\\displaystyle\\sum_{i=1}^{n}\\left(\\!\\operatorname*{min}_{y}{\\bf|\\hat{y}_{\\gamma_{1}}\\left({\\bf x}_{i}\\right)-{\\bf M}_{1}{\\bf y}|}\\!\\right)_{\\mathcal{H}}^{2}-\\operatorname*{min}_{\\bf y}{\\bf|\\hat{y}_{\\gamma_{2}}\\left({\\bf x}_{i}\\right)-{\\bf M}_{2}{\\bf y}|}\\!\\right)^{2}}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{n}\\left[4\\left(|\\Phi_{\\gamma_{1}}\\left({\\bf x}_{i}\\right)|_{\\mathcal{H}}^{2}-||\\Phi_{\\gamma_{2}}\\left({\\bf x}_{i}\\right)||_{\\mathcal{H}}^{2}\\right)^{2}+8\\displaystyle\\sum_{r=1}^{c}\\left(\\left(\\Phi_{\\gamma_{2}}^{T}\\left({\\bf x}_{i}\\right){\\bf M}_{2}-\\Phi_{\\gamma_{1}}^{T}\\left({\\bf x}_{i}\\right){\\bf M}_{1}\\right){\\bf e}_{r}\\right)^{2}\\right.}\\\\ &{\\quad\\left.+4\\displaystyle\\sum_{r=1}^{c}\\left(\\!\\left(\\!\\mathbf{e}_{r}^{T}\\left(\\mathbf{M}_{1}^{T}{\\bf M}_{1}-\\mathbf{M}_{2}^{T}{\\bf M}_{2}\\right){\\bf e}_{s}\\right)^{2}\\right]}\\\\ &{=\\!\\mathbb{E}_{\\beta}\\left[\\left(H{\\bf M}_{1,r_{1}}-H{\\bf M}_{2,r_{2}}\\right\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, we obtain $H_{\\mathbf{M},\\gamma}$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\nH_{\\mathbf{M},\\gamma}=2\\sum_{i=1}^{n}\\beta_{i}\\left\\Vert\\Phi_{\\gamma}^{T}\\left(\\mathbf{x}_{i}\\right)\\right\\Vert_{\\mathcal{H}}^{2}+2\\sqrt{2}\\sum_{i=1}^{n}\\sum_{r=1}^{c}\\beta_{i r}\\Phi_{\\gamma}^{T}\\left(\\mathbf{x}_{i}\\right)\\mathbf{M}\\mathbf{e}_{r}+2\\sum_{i=1}^{n}\\sum_{r,s=1}^{c}\\beta_{i r s}\\mathbf{e}_{r}^{T}\\mathbf{M}^{T}\\mathbf{M}\\mathbf{e}_{s}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To bound the expectation of $H_{\\mathbf{M},\\gamma}$ , we introduce the following Lemma from [31]: ", "page_idx": 16}, {"type": "text", "text": "Lemma 4 [31] Suppose that ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1) $\\mathbf{e}_{r}:1\\leq r\\leq c)$ is an orthonormal basis of $\\mathbb{R}^{c}$ ;   \n2) $\\mathcal{M}$ is the class of linear operators $\\mathbf{M}:\\mathbb{R}^{c}\\rightarrow H$ with $\\lVert\\mathbf{M}\\mathbf{e}_{r}\\rVert_{\\mathcal{H}}\\leq\\omega$   \n3) $(\\mathbf{x}_{i}:1\\leq i\\leq n)$ is a sequence in $H,\\|\\mathbf{x}_{i}\\|_{\\mathcal{H}}\\leq\\mu$ ;   \n4) $(\\beta_{i r}:1\\le i\\le n,1\\le r\\le c)$ and $(\\beta_{i r s}:1\\le i\\le n,1\\le r,s\\le r)$ are orthogaussian (independent   \nand $N(0,1)$ ) sequences. ", "page_idx": 16}, {"type": "text", "text": "Then the following three inequalities hold: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\beta}\\underset{\\mathbf{M}\\in\\mathcal{M}}{\\operatorname*{sup}}\\sum_{i=1}^{n}\\sum_{r=1}^{c}\\beta_{i r}\\left\\langle x_{i},\\mathbf{M}\\mathbf{e}_{r}\\right\\rangle\\leq\\omega\\mu c\\sqrt{n},}\\\\ {\\mathbb{E}_{\\beta}\\underset{\\mathbf{M}\\in\\mathcal{M}}{\\operatorname*{sup}}\\sum_{i=1}^{n}\\sum_{r=1}^{c}\\beta_{i r}\\left\\lVert\\mathbf{M}\\mathbf{e}_{r}\\right\\rVert_{\\mathcal{H}}^{2}\\leq\\omega^{2}c\\sqrt{n},}\\\\ {\\mathbb{E}_{\\beta}\\underset{\\mathbf{M}\\in\\mathcal{M}}{\\operatorname*{sup}}\\sum_{i=1}^{n}\\sum_{r,s=1}^{c}\\beta_{i r s}\\left\\langle\\mathbf{M}\\mathbf{e}_{r},\\mathbf{M}\\mathbf{e}_{s}\\right\\rangle\\leq\\omega^{2}c^{2}\\sqrt{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\langle\\cdot,\\cdot\\rangle$ denotes the inner production. ", "page_idx": 16}, {"type": "text", "text": "In our method, given an instance $\\mathbf{x}_{i}$ , we have that $\\begin{array}{r}{\\|\\Phi_{\\gamma}\\left(\\mathbf{x}_{i}\\right)\\|_{\\mathcal{H}}=\\Phi_{\\gamma}^{T}\\left(\\mathbf{x}_{i}\\right)\\Phi_{\\gamma}\\left(\\mathbf{x}_{i}\\right)\\leq b+\\alpha-\\lambda.}\\end{array}$ Moreover, we also have $\\lVert\\mathbf{Me}_{r}\\rVert_{\\mathcal{H}}\\leq\\sqrt{b+\\alpha-\\frac{\\lambda}{t}}$ according to Eq.(26). As a result, according to Lemma 4, the expectation of $H_{\\mathbf{M},\\gamma}$ can be be bounded as follows, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\beta}\\left[\\underset{\\ell\\in\\mathcal{F}_{i-1}}{\\operatorname*{sup}}H_{\\mathbf{M},\\tau}\\right]}\\\\ &{=\\mathbb{E}_{\\beta}\\left[\\underset{l\\in\\mathcal{F}_{i-1}}{\\operatorname*{sup}}2\\frac{n}{\\sum_{i=1}^{n}\\beta_{i}}\\left|\\left|\\Phi_{\\gamma}^{T}(\\mathbf{x}_{i})\\right|\\right|_{\\mathbb{H}}^{2}+2\\sqrt{2}\\sum_{i=1}^{n}\\underset{r=1}{\\overset{n}{\\sum}}\\beta_{i\\cdot\\Psi_{\\gamma}^{T}}(\\mathbf{x}_{i})\\,\\mathbf{M}\\mathbf{e}_{r}+2\\sum_{i=1}^{n}\\underset{r=1}{\\overset{c}{\\sum}}\\beta_{i\\cdot\\mathbf{e}_{r}}\\mathbf{\\Phi}_{r}^{T}\\mathbf{M}^{T}\\mathbf{M}\\mathbf{e}_{s}\\right]}\\\\ &{\\leq2\\mathbb{E}_{\\beta}\\left[\\underset{l\\in\\mathcal{F}_{i-1}}{\\operatorname*{sup}}\\frac{n}{\\sum_{i}\\beta_{i}}\\left|\\left|\\Phi_{\\gamma}^{T}(\\mathbf{x}_{i})\\right|\\right|_{\\mathbb{H}}^{2}\\right]+2\\sqrt{2}\\mathbb{E}_{\\beta}\\left[\\underset{l\\in\\mathcal{F}_{i-1}}{\\operatorname*{sup}}\\sum_{i=1}^{n}\\beta_{i\\cdot\\mathbf{r}}\\Phi_{\\gamma}^{T}(\\mathbf{x}_{i})\\,\\mathbf{M}\\mathbf{e}_{r}\\right]+2\\mathbb{E}_{\\beta}\\left[\\underset{l\\in\\mathcal{F}_{i-1}}{\\operatorname*{sup}}\\beta_{i\\cdot\\mathbf{r}}\\mathbf{e}_{r}^{T}\\right]}\\\\ &{\\leq2\\left(b+\\alpha-\\lambda\\right)\\sqrt{n}+2c\\sqrt{2\\left(b+\\alpha-\\lambda\\right)\\left(b+\\alpha-\\frac{\\lambda}{t}\\right)n}+2c^{2}\\left(b+\\alpha-\\frac{\\lambda}{t}\\right)\\sqrt{n}}\\\\ &{=2\\sqrt{n}\\left[(1+c^{2})\\left(b+\\alpha\\right)-(1+\\frac{c^{2}}{t})\\lambda+c\\sqrt{2\\left(b+\\alpha-\\lambda\\right)\\left(b+\\alpha-\\frac{\\lambda}{t}\\right)}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Last, we can bound $\\Re_{n}({\\mathcal{F}})$ with Lemma 2, Lemma 3, Eq.(29), Eq.(30), and Eq.(41): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{h}_{n}(\\mathcal{F})\\leq\\frac{1}{n}\\sqrt{\\pi/2}\\,\\mathbb{E}_{\\beta}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\,G_{\\mathrm{M},\\gamma}\\right]\\leq\\frac{1}{n}\\sqrt{\\pi/2}\\,\\mathbb{E}_{\\beta}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\,H_{\\mathrm{M},\\gamma}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\sqrt{2\\pi}}{\\sqrt{n}}\\left[(1+c^{2})\\,(b+\\alpha)-(1+\\frac{c^{2}}{t})\\lambda+c\\sqrt{2\\,(b+\\alpha-\\lambda)\\left(b+\\frac{c^{2}}{t}\\right)^{2}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Substituting Eq.(42) into Eq.(28), we finally obtain for any $\\delta\\geq0$ , with probability at least $1-\\delta$ , for all $f\\in\\mathcal F$ , the following holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\xi[f(\\mathbf{x})]\\le\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}f\\left(\\mathbf{x}_{i}\\right)+\\frac{2\\sqrt{2\\pi}}{\\sqrt{n}}\\left[(1+c^{2})\\left(b+\\alpha\\right)-(1+\\frac{c^{2}}{t})\\lambda+c\\sqrt{2\\left(b+\\alpha-\\lambda\\right)\\left(b+\\alpha-\\frac{\\lambda}{t}\\right)}\\right]}&\\\\ &{\\quad\\quad\\quad+\\left(4\\left(b+\\alpha\\right)-2\\left(1+\\frac{1}{t}\\right)\\lambda\\right)\\sqrt{\\frac{\\log(1/\\delta)}{2n}}.}&{(43)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "C Proof of Lemma 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Since $\\sigma_{A}$ is the smallest eigenvalue of $\\mathbf{A}$ , and $\\sigma_{B}$ is the largest eigenvalue of $\\mathbf{B}$ , we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{A}-\\sigma_{A}\\mathbf{I}\\succeq0,}\\\\ {\\sigma_{B}\\mathbf{I}-\\mathbf{B}\\succeq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Summing up Eq.(44) and Eq.(45), we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{A}-\\mathbf{B}-\\left(\\sigma_{A}-\\sigma_{B}\\right)\\mathbf{I}\\succeq0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Considering the smallest eigenvalue of $\\mathbf{A}-\\mathbf{B}$ , denoting as $\\sigma_{A-B}$ , and its corresponding eigenvector $\\mathbf{v}_{A-B}$ , we have $\\left(\\mathbf{A}-\\mathbf{B}\\right)\\mathbf{v}_{A-B}\\,=\\,\\sigma_{A-B}\\mathbf{v}_{A-B}$ . Multiplying the left-hand side of Eq.(46) with $\\mathbf{v}_{A-B}^{T}$ and $\\mathbf{v}_{A-B}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbf{v}_{A-B}^{T}\\left(\\mathbf{A}-\\mathbf{B}-\\left(\\sigma_{A}-\\sigma_{B}\\right)\\mathbf{I}\\right)\\mathbf{v}_{A-B}}\\\\ &{=\\mathbf{v}_{A-B}^{T}\\left(\\sigma_{A-B}-\\left(\\sigma_{A}-\\sigma_{B}\\right)\\right)\\mathbf{v}_{A-B}}\\\\ &{=\\left(\\sigma_{A-B}-\\left(\\sigma_{A}-\\sigma_{B}\\right)\\right)\\|\\mathbf{v}_{A-B}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Notice that $\\left(\\mathbf{A}-\\mathbf{B}-\\left(\\sigma_{A}-\\sigma_{B}\\right)\\mathbf{I}\\right)$ is positive semi-definite according to Eq.(46), which means $\\mathbf{v}_{A-B}^{T}\\left(\\mathbf{A}-\\mathbf{B}-\\left(\\sigma_{A}-\\sigma_{B}\\right)\\mathbf{I}\\right)\\mathbf{v}_{A-B}\\geq0$ . Therefore, $\\sigma_{A-B}-(\\sigma_{A}-\\sigma_{B})\\geq0$ , and thus $\\sigma_{A-B}\\geq$ $\\sigma_{A}-\\sigma_{B}\\geq0$ . This means that $\\mathbf{A}-\\mathbf{B}$ is positive semi-definite, which concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "D Statistical Information of Data Sets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We conduct experiments on benchmark data sets which are widely used in fair clustering, including D&S [2], HAR [3], Jaffe [29], MNIST-USPS [20], Credit Card [52] and K1b [53]. D&S is a human daily and sports activities data set including 8 participants. HAR is a human action recognition data set including 30 participants. In both D&S and HAR data sets, the data of each participant form a protected group. Jaffe is a face image data set. Following [20], the face images with the same expressions are put into a protected group. MNIST-USPS is an image data set containing images of handwritten digits from the subsets of MNIST and USPS data sets. Following [20], we randomly sample 2000 images from MNIST to form one protected group and randomly sample 1800 images from USPS to form the other protected group. Credit card is a data set that describes the customers\u2019 default payments and the data of males and females form two protected groups respectively. K1b is a text data set. Following [48], we randomly assign each text to a protected group with a Bernoulli distribution whose $p=0.5$ to form two protected groups. Details of the data sets are shown in Table 3. ", "page_idx": 17}, {"type": "text", "text": "E Introduction of Compared Methods ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To show the effectiveness of our method on clustering performance and fairness, we compare our method with some state-of-the-art fair clustering and multiple kernel $\\mathbf{k}$ -means methods, including: ", "page_idx": 17}, {"type": "text", "text": "\u2022 SpFC [18], which integrates fairness constraints into the Laplacian matrix of a graph.   \n\u2022 VFC [58], which is a universal variational fair clustering framework.   \n\u2022 FFC [34], which is a three-stage fair clustering method based on $\\boldsymbol{\\mathrm{k}}$ -means method.   \n\u2022 ONKC [25], which is an optimal neighborhood kernel clustering algorithm to enhance the representability of the optimal kernel.   \n\u2022 MKCSS [55], which is a simple yet effective neighbor-kernel-based MKC algorithm to consider the intrinsic neighborhood structure among base kernels.   \n\u2022 DPMKKM [42], which is a novel discrete multiple kernel $\\mathbf{k}$ -means by directly solving the clustering indicator matrix.   \n\u2022 LFLKA [51], whihc is a simple late fusion multiple kernel clustering with local kernel alignment maximisation approach.   \n\u2022 EMKC [38], which is effective multiple kernnel k-means by introducing spectral perturbation theory to laplacian matrix.   \n\u2022 OSLR [47], which is a one stage multiple kernel k-means by refining shifted laplacian matrix.   \n\u2022 ASLR [46], which is a effective multiple kernel $\\mathbf{k}$ -means by reconstructing the laplacian matrix.   \n\u2022 CSAMKC [56], which is a fast multiple kernel k-means by adopting a novel sampling strategy to improve the performance of MKC.   \n\u2022 FAMKKM [41], which is fast and innovative multiple kernel $\\mathbf{k}$ -means by incorporating two approximated partition matrices instead of the original individual partition matric for each base kernel. ", "page_idx": 17}, {"type": "table", "img_path": "CehOqpvOxG/tmp/5337354cb08d894dd1af5412029411eeeabb5832a7128a0d643b26d308a276cc.jpg", "table_caption": ["Table 3: Description of the data sets. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "F Efficiency Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Figures 3 and 4 show the convergence curves of FKKM and FMKKM, respectively. We can see that our methods converge very fast and they often converge within 5 iterations. ", "page_idx": 18}, {"type": "text", "text": "Figures 5 and 6 show the running time of all methods on single kernel setting and multiple kernel setting, respectively. For better comparison, we report the logarithm of the time (in seconds). From Figures 5 and 6, we can see that our FKKM and FMKKM are faster than or at least comparable with many state-of-the-art methods, which well demonstrates the efficiency of our methods. ", "page_idx": 18}, {"type": "image", "img_path": "CehOqpvOxG/tmp/bd34847ff3f9c2c43521e69e684dccd2c3a5543ec8268d0ace3de7b69756f3bd.jpg", "img_caption": ["Figure 3: Convergence curves of all data sets on single kernel setting. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "CehOqpvOxG/tmp/8ee63bb7d9911c1e6d794a1a0f92c08f9dca0c9b3c24512b98fe614f6c5a358e.jpg", "img_caption": ["Figure 4: Convergence curves of all data sets on multiple kernel setting. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "CehOqpvOxG/tmp/1aab3da1e82177ec7bd5b608660ec9e54b975f9b3fcbeda1ec8504e38d55f9cc.jpg", "img_caption": ["Figure 5: Running time of all methods on the single kernel setting. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "CehOqpvOxG/tmp/3b752b5e8276ed29d043d1b416c7d9a5bde97de2f3787a066c14ca221320f91d.jpg", "img_caption": ["Figure 6: Running time of all methods on the multiple kernel setting. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We discuss the limitations in Section Conclusion. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide the proofs of all theoretical results. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide the detailed experimental settings and the codes. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide the codes. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide the detailed experimental setting. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have computed the standard deviation but due to the space limit, we do not report this in the manuscript. We can provide it if requested. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide the detailed experimental setting. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We conform with the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We do not find any societal impact of this work. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All used assets are cited. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 27}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]