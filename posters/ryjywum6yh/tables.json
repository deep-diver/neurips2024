[{"figure_path": "rYjYwuM6yH/tables/tables_5_1.jpg", "caption": "Table 1: A summarization of three RoAd variants.", "description": "This table summarizes the three variants of the RoAd model, showing the number of trainable parameters for each variant.  RoAd1 uses a single scaling factor for all dimensions, RoAd2 uses two independent scaling factors for each pair of dimensions, and RoAd4 uses four separate scaling factors for each dimension. This results in different numbers of trainable parameters (d2, 2d2, and 4d2, respectively, where d2 is the number of dimensions in the original weight matrix). The table clarifies how the parameter sharing and independence differ across the variants of RoAd, reflecting the varying levels of parameter efficiency.", "section": "3.2 2D rotary adaptation"}, {"figure_path": "rYjYwuM6yH/tables/tables_6_1.jpg", "caption": "Table 2: Results on the held-out GLUE development set with RoBERTa as the backbone. We report matched accuracy for MNLI, Matthew\u2019s correlation for CoLA, Pearson correlation for STS-B and accuracy for other tasks. The best and second-best results are in bold and underlined, respectively, being the same for other tables. The percentage of trainable parameters is calculated without considering the classifier head. RoAd\u2081(fc1) means that we only insert the RoAd\u2081 module to the first feed-forward layer, to match the #Params. of RED and LoReFT. Results of methods denoted by * and are from Wu et al. [60] and Wu et al. [61], respectively. Otherwise, average results from three random runs are reported. Refer to Table C.4 for the standard deviation.", "description": "This table presents the results of different parameter-efficient fine-tuning (PEFT) methods on the GLUE benchmark using RoBERTa as the base model.  It compares the performance of RoAd with other PEFT methods like LORA, Adapter, and full finetuning across various tasks within the GLUE benchmark.  The table highlights RoAd's performance and parameter efficiency. It also includes the standard deviation of the results for RoAd.", "section": "4.1 Results on downstream tasks"}, {"figure_path": "rYjYwuM6yH/tables/tables_6_2.jpg", "caption": "Table 5: Score on AlpacaEval2.0 with LLaMA2-7B.", "description": "This table presents the results of instruction-following ability experiments on LLaMA2-7B using AlpacaEval2.0.  It compares the performance of different parameter-efficient fine-tuning (PEFT) methods, namely LoRA, LoReFT, and RoAd1, using two instruction-tuning datasets: 10K cleaned Alpaca and UltraFeedback. The 'Win Rate (%)' indicates the percentage of times the model's generated responses were judged to be superior to those of Text-davinci-003 by GPT-4. The table highlights the superior performance of RoAd1, achieving the highest win rate with minimal trainable parameters, demonstrating its effectiveness for instruction-following tasks.", "section": "4.1 Results on downstream tasks"}, {"figure_path": "rYjYwuM6yH/tables/tables_7_1.jpg", "caption": "Table 2: Results on the held-out GLUE development set with RoBERTa as the backbone. We report matched accuracy for MNLI, Matthew\u2019s correlation for CoLA, Pearson correlation for STS-B and accuracy for other tasks. The best and second-best results are in bold and underlined, respectively, being the same for other tables. The percentage of trainable parameters is calculated without considering the classifier head. RoAd\u2081(fc1) means that we only insert the RoAd\u2081 module to the first feed-forward layer, to match the #Params. of RED and LoReFT. Results of methods denoted by * and are from Wu et al. [60] and Wu et al. [61], respectively. Otherwise, average results from three random runs are reported. Refer to Table C.4 for the standard deviation.", "description": "This table presents the results of various parameter-efficient fine-tuning (PEFT) methods and full fine-tuning on the GLUE benchmark using RoBERTa-base and RoBERTa-large language models.  It compares the performance (accuracy and correlation scores) of different methods across multiple tasks, highlighting the parameter efficiency of RoAd1 in achieving results comparable to or better than other PEFT methods and even full fine-tuning, using significantly fewer trainable parameters.", "section": "4.1 Results on downstream tasks"}, {"figure_path": "rYjYwuM6yH/tables/tables_7_2.jpg", "caption": "Table 4: Accuracy of LLaMA on four arithmetic reasoning tasks. Results of methods denoted by *, and * are from [15], [61] and [27], respectively. Refer to Table C.7 for the standard deviation.", "description": "This table presents the results of several parameter-efficient fine-tuning (PEFT) methods and a full finetuning method on four arithmetic reasoning tasks using the LLaMA language model.  The tasks are AQuA, GSM8K, MAWPS, and SVAMP. The table shows the accuracy achieved by each method, along with the percentage of trainable parameters used.  The results highlight the performance of RoAd compared to other PEFT methods in this specific task.", "section": "4.1 Results on downstream tasks"}, {"figure_path": "rYjYwuM6yH/tables/tables_8_1.jpg", "caption": "Table 6: Visual instruction tuning results on LLaVA1.5-7B.", "description": "This table presents the results of visual instruction tuning on the LLaVA1.5-7B model using different parameter-efficient fine-tuning (PEFT) methods.  It shows the performance (accuracy) of each method across four different tasks: GQA, SQA, VQAT, and POPE, as well as the average performance across all four tasks.  The methods compared include LoRA, RoAd4 (a novel method proposed in the paper), and a combination of RoAd1 and LoRA. The table highlights the parameter efficiency of RoAd4, which achieves comparable performance to LoRA with significantly fewer parameters.", "section": "4.2 Efficiency results for batching"}, {"figure_path": "rYjYwuM6yH/tables/tables_22_1.jpg", "caption": "Table C.1: The data statistics and evaluation metrics of the GLUE benchmark. The valid and test sets are randomly split from the original development set. Following Wu et al. [60], only the matched development set of MNLI is used. For runs with different seeds, the samples in the valid and test sets are also different.", "description": "This table presents the details of the GLUE benchmark dataset used in the paper's experiments.  It provides the number of training, validation, and test samples for each of the nine tasks included in the GLUE benchmark.  It also specifies the evaluation metric used for each task (Accuracy, Pearson correlation, or Matthew's correlation). The table notes that the validation and test sets were randomly split from the original development set and that different random seeds resulted in different samples for the validation and test sets.", "section": "C.1 Natural language understanding (NLU)"}, {"figure_path": "rYjYwuM6yH/tables/tables_23_1.jpg", "caption": "Table 2: Results on the held-out GLUE development set with RoBERTa as the backbone. We report matched accuracy for MNLI, Matthew\u2019s correlation for CoLA, Pearson correlation for STS-B and accuracy for other tasks. The best and second-best results are in bold and underlined, respectively, being the same for other tables. The percentage of trainable parameters is calculated without considering the classifier head. RoAd\u2081(fc1) means that we only insert the RoAd\u2081 module to the first feed-forward layer, to match the #Params. of RED and LoReFT. Results of methods denoted by * and are from Wu et al. [60] and Wu et al. [61], respectively. Otherwise, average results from three random runs are reported. Refer to Table C.4 for the standard deviation.", "description": "This table presents the results of different parameter-efficient fine-tuning (PEFT) methods on the GLUE benchmark using RoBERTa as the base model.  It compares the performance of RoAd to other PEFT methods (LORA, Adapter, etc.) and full finetuning, showing the accuracy scores on various GLUE tasks. The table also shows the percentage of trainable parameters used by each method.", "section": "4.1 Results on downstream tasks"}, {"figure_path": "rYjYwuM6yH/tables/tables_23_2.jpg", "caption": "Table 2: Results on the held-out GLUE development set with RoBERTa as the backbone. We report matched accuracy for MNLI, Matthew\u2019s correlation for CoLA, Pearson correlation for STS-B and accuracy for other tasks. The best and second-best results are in bold and underlined, respectively, being the same for other tables. The percentage of trainable parameters is calculated without considering the classifier head. RoAd\u2081(fc1) means that we only insert the RoAd\u2081 module to the first feed-forward layer, to match the #Params. of RED and LoReFT. Results of methods denoted by * and  are from Wu et al. [60] and Wu et al. [61], respectively. Otherwise, average results from three random runs are reported. Refer to Table C.4 for the standard deviation.", "description": "This table presents the results of different parameter-efficient fine-tuning (PEFT) methods on the GLUE benchmark using RoBERTa as the base model.  It compares the performance (accuracy, correlation) of various methods, including RoAd, against full finetuning and other PEFT approaches like LoRA and adapters.  The table highlights RoAd's parameter efficiency, showing it achieves competitive results with significantly fewer trainable parameters.", "section": "4.1 Results on downstream tasks"}, {"figure_path": "rYjYwuM6yH/tables/tables_24_1.jpg", "caption": "Table 2: Results on the held-out GLUE development set with RoBERTa as the backbone. We report matched accuracy for MNLI, Matthew\u2019s correlation for CoLA, Pearson correlation for STS-B and accuracy for other tasks. The best and second-best results are in bold and underlined, respectively, being the same for other tables. The percentage of trainable parameters is calculated without considering the classifier head. RoAd\u2081(fc1) means that we only insert the RoAd\u2081 module to the first feed-forward layer, to match the #Params. of RED and LoReFT. Results of methods denoted by * and are from Wu et al. [60] and Wu et al. [61], respectively. Otherwise, average results from three random runs are reported. Refer to Table C.4 for the standard deviation.", "description": "This table presents the results of different parameter-efficient fine-tuning (PEFT) methods on the GLUE benchmark using RoBERTa as the base model.  It compares the performance of RoAd1 to other PEFT methods (e.g., LORA, Adapter, BitFit, RED, LoReFT) and full fine-tuning across various tasks in the GLUE benchmark.  The table highlights RoAd1's performance, particularly its efficiency in terms of trainable parameters while achieving comparable or superior accuracy to other methods.", "section": "4.1 Results on downstream tasks"}, {"figure_path": "rYjYwuM6yH/tables/tables_24_2.jpg", "caption": "Table C.5: Hyperparameters for commonsense and arithmetic reasoning without extensive tuning.", "description": "This table presents the hyperparameter settings used for both commonsense and arithmetic reasoning tasks.  The settings were chosen without extensive hyperparameter tuning. Note the differences in the number of epochs and the learning rate between the two reasoning tasks.  These differences reflect the different characteristics of the two types of tasks and their corresponding datasets.", "section": "C. Experimental details"}, {"figure_path": "rYjYwuM6yH/tables/tables_24_3.jpg", "caption": "Table 3: Accuracy of LLaMA on eight commonsense reasoning tasks. Results of methods denoted by *, and are from [15], [61] and [27], respectively. Otherwise, average results from three random runs are reported. Refer to Table C.6 for the standard deviation. Refer to Table D.2 for LLaMA2&3.", "description": "This table presents the results of evaluating various parameter-efficient fine-tuning (PEFT) methods and a full finetuning approach on eight commonsense reasoning tasks using the LLaMA language model.  The table shows the average accuracy across three random runs for each method and model size (LLaMA-7B and LLaMA-13B), along with the percentage of trainable parameters used.  The results highlight RoAd's performance compared to other PEFT methods like LoReFT, DORA, and LORA.", "section": "4.1 Results on downstream tasks"}, {"figure_path": "rYjYwuM6yH/tables/tables_25_1.jpg", "caption": "Table 4: Accuracy of LLaMA on four arithmetic reasoning tasks. Results of methods denoted by *, and * are from [15], [61] and [27], respectively. Refer to Table C.7 for the standard deviation.", "description": "This table presents the results of different parameter-efficient fine-tuning (PEFT) methods and full fine-tuning on four arithmetic reasoning tasks using the LLaMA language model.  The table shows the accuracy of each method on the AQuA, GSM8K, MAWPS, and SVAMP datasets.  The \"#Params.\" column indicates the percentage of trainable parameters used by each method.  The table also includes a comparison with three baselines reported in other studies.", "section": "4.1 Results on downstream tasks"}, {"figure_path": "rYjYwuM6yH/tables/tables_25_2.jpg", "caption": "Table D.1: Finetuning details of RoAds, OFT and BOFT on LLaMA-7B. The training setting here is: batch size = 1, maximum sequence length = 512, number of iterations = 100, 1 A100 80GB GPU.", "description": "This table presents the hyperparameters used for fine-tuning different models (RoAd1, RoAd2, RoAd4, OFT, and BOFT) on the LLaMA-7B model.  It shows the percentage of trainable parameters used (#Params.), the peak GPU memory consumed during training (Peak GPU memory (GB)), and the training time in seconds (Training time (s)). The table highlights the efficiency of RoAd methods compared to OFT and BOFT in terms of memory usage and training time.", "section": "D More results"}, {"figure_path": "rYjYwuM6yH/tables/tables_26_1.jpg", "caption": "Table 2: Results on the held-out GLUE development set with RoBERTa as the backbone. We report matched accuracy for MNLI, Matthew\u2019s correlation for CoLA, Pearson correlation for STS-B and accuracy for other tasks. The best and second-best results are in bold and underlined, respectively, being the same for other tables. The percentage of trainable parameters is calculated without considering the classifier head. RoAd\u2081(fc1) means that we only insert the RoAd\u2081 module to the first feed-forward layer, to match the #Params. of RED and LoReFT. Results of methods denoted by * and are from Wu et al. [60] and Wu et al. [61], respectively. Otherwise, average results from three random runs are reported. Refer to Table C.4 for the standard deviation.", "description": "This table presents the results of different parameter-efficient fine-tuning (PEFT) methods on the GLUE benchmark using RoBERTa as the base model.  It compares the performance of RoAd with other state-of-the-art PEFT techniques, highlighting RoAd's high accuracy while using significantly fewer trainable parameters.  The table includes results for both the RoBERTa-base and RoBERTa-large models.", "section": "4.1 Results on downstream tasks"}]