[{"figure_path": "rYjYwuM6yH/figures/figures_0_1.jpg", "caption": "Figure 1: Performance of various PEFT methods on the GLUE benchmark, eight commonsense reasoning tasks and four arithmetic reasoning tasks with RoBERTa-large or LLaMA-13B.", "description": "This figure compares the performance of different parameter-efficient fine-tuning (PEFT) methods on three types of tasks: GLUE benchmark, eight commonsense reasoning tasks, and four arithmetic reasoning tasks.  The PEFT methods compared are RoAd, Full FT (full finetuning), LoReFT, DORA, LORA, Adapters, and AdapterFFN. The x-axis represents the percentage of trainable parameters, and the y-axis represents the accuracy achieved on each task.  The results show that RoAd achieves high accuracy while using significantly fewer trainable parameters compared to other methods, particularly on GLUE and commonsense reasoning tasks. ", "section": "Abstract"}, {"figure_path": "rYjYwuM6yH/figures/figures_3_1.jpg", "caption": "Figure 2: Pilot study for the pretrained and finetuned representations. Left & Middle: The change in magnitude and angle of representations between pretrained and finetuned LLM using full finetuning or LORA. Right: The disentanglement experiment of magnitude and angle of pretrained representation.", "description": "This figure presents a pilot study on pretrained and finetuned representations to investigate the impact of magnitude and angular displacement on model adaptation. The left and middle panels show the relative change in magnitude (AM) and angular displacement (AD) between pretrained and finetuned representations using full finetuning and LoRA methods, respectively. The right panel demonstrates a disentanglement experiment that separately evaluates the effects of magnitude and angle on the model's ability to adapt to downstream tasks.", "section": "3.1 Pilot study"}, {"figure_path": "rYjYwuM6yH/figures/figures_4_1.jpg", "caption": "Figure 3: Overview of RoAd1.", "description": "This figure illustrates the architecture of RoAd1, a variant of the proposed 2D rotary adaptation method.  It shows how the input vector \\(h\\) is processed through a series of 2D rotations, represented by the matrices \\(R_i\\), before being combined with the pre-trained weights \\(W^0\\) to produce the final output \\(z\\). Each \\(R_i\\) rotates pairs of adjacent dimensions of \\(h\\), and the entire process is designed to be computationally efficient and parameter-sparse.", "section": "3.2 2D rotary adaptation"}, {"figure_path": "rYjYwuM6yH/figures/figures_8_1.jpg", "caption": "Figure 1: Performance of various PEFT methods on the GLUE benchmark, eight commonsense reasoning tasks and four arithmetic reasoning tasks with RoBERTa-large or LLaMA-13B.", "description": "The figure displays the accuracy achieved by various parameter-efficient fine-tuning (PEFT) methods and full finetuning on three different sets of tasks: GLUE benchmark, eight commonsense reasoning tasks, and four arithmetic reasoning tasks.  Two different large language models (LLMs) were used: RoBERTa-large and LLaMA-13B. The x-axis shows the percentage of trainable parameters used for each method, indicating its parameter efficiency. The y-axis displays the achieved accuracy for each task set.  RoAd consistently demonstrates high accuracy with significantly fewer trainable parameters compared to other PEFT methods.", "section": "Abstract"}, {"figure_path": "rYjYwuM6yH/figures/figures_27_1.jpg", "caption": "Figure 2: Pilot study for the pretrained and finetuned representations. Left & Middle: The change in magnitude and angle of representations between pretrained and finetuned LLM using full finetuning or LORA. Right: The disentanglement experiment of magnitude and angle of pretrained representation.", "description": "This figure presents a pilot study to investigate the impact of magnitude and angular changes on the representations of pretrained and finetuned LLMs.  The left and middle panels show how full finetuning and LoRA affect the magnitude and angle changes of representations. The right panel shows a disentanglement experiment evaluating the effect of isolating changes in magnitude vs. angle, and shows that angular adjustments have a more significant impact on finetuning.", "section": "3.1 Pilot study"}]