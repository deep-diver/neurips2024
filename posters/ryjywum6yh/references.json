{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-04", "reason": "This paper introduced the Transformer architecture, a foundational model for many large language models (LLMs), including those studied in the current research."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language Models are Few-Shot Learners", "publication_date": "2021-05-03", "reason": "This paper demonstrated the surprising capabilities of large language models to perform various downstream tasks with minimal finetuning, motivating parameter-efficient fine-tuning methods."}, {"fullname_first_author": "Edward J. Hu", "paper_title": "LoRA: Low-Rank Adaptation of Large Language Models", "publication_date": "2022-04-25", "reason": "This paper introduced LoRA, a highly influential parameter-efficient fine-tuning method that the current work builds upon and compares against."}, {"fullname_first_author": "Zhilin Wu", "paper_title": "ReFT: Representation Finetuning for Language Models", "publication_date": "2024-04-01", "reason": "This is a very recent and closely related paper introducing a similar approach to parameter-efficient fine-tuning, providing a strong comparison for the current work."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "publication_date": "2020-07-01", "reason": "This paper introduced a unified text-to-text transformer, a significant advance in the field of large language models, influencing subsequent research directions."}]}