[{"type": "text", "text": "How Molecules Impact Cells: Unlocking Contrastive PhenoMolecular Retrieval ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Philip Fradkin $^{1,2,*}$ , Puria Azadi1,3,\u2217, Karush $\\bf{S u r i^{1}}$ , Frederik Wenkel1, Ali Bashashati3, Maciej Sypetkowski1\u2020, Dominique Beaini1,4,\u2020 ", "page_idx": 0}, {"type": "text", "text": "1 Valence Labs, 2 University of Toronto, Vector Institute, 3 University of British Columbia, 4 Universit\u00e9 de Montr\u00e9al, Mila- Quebec AI Institute dominique@valencelabs.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Predicting molecular impact on cellular function is a core challenge in therapeutic design. Phenomic experiments, designed to capture cellular morphology, utilize microscopy based techniques and demonstrate a high throughput solution for uncovering molecular impact on the cell. In this work, we learn a joint latent space between molecular structures and microscopy phenomic experiments, aligning paired samples with contrastive learning. Specifically, we study the problem of Contrastive PhenoMolecular Retrieval, which consists of zero-shot molecular structure identification conditioned on phenomic experiments. We assess challenges in multi-modal learning of phenomics and molecular modalities such as experimental batch effect, inactive molecule perturbations, and encoding perturbation concentration. We demonstrate improved multi-modal learner retrieval through (1) a uni-modal pre-trained phenomics model, (2) a novel inter sample similarity aware loss, and (3) models conditioned on a representation of molecular concentration. Following this recipe, we propose MolPhenix, a molecular phenomics model. MolPhenix leverages a pre-trained phenomics model to demonstrate significant performance gains across perturbation concentrations, molecular scaffolds, and activity thresholds. In particular, we demonstrate an $8.1\\times$ improvement in zero shot molecular retrieval of active molecules over the previous state-of-the-art, reaching $77.33\\%$ in top- $1\\%$ accuracy. These results open the door for machine learning to be applied in virtual phenomics screening, which can significantly benefit drug discovery applications. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Quantifying cellular responses elicited by genetic and molecular perturbations represents a core challenge in medicinal research [4, 57]. Out of an approximate $1\\dot{0}^{60}$ druglike molecule designs, a small number are able to alter cellular properties to reverse the course of diseases [5, 27]. In recent years, microscopy-based cell morphology screening techniques, demonstrated potential for quantitative understanding of a molecule\u2019s biological effects. Experimental techniques such as cell-painting are used to capture cellular morphology, which correspond to physical and structural properties of the cell [6, 7]. Cells treated with molecular perturbations can change morphology, which is captured by staining and high throughput microscopy techniques. Perturbations with similar cellular impact induce analogous morphological changes, allowing to capture underlying biological effects in phenomic experiments. Identifying such perturbations with similar morphological changes can aid in discovery of novel therapeutic drug candidates [50, 29, 22]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Determining molecular impact on the cell can be formulated as a multi-modal learning problem, allowing us to build on a rich family of methods [43, 62, 53]. Similar to text-image models, paired data is collected from phenomic experiments along with molecules used to perturb the cells. Contrastive objectives have been used as an effective approach in aligning paired samples from different modalities [43, 32]. A model that has learned a cross-modal joint latent space must be able to retrieve a molecular perturbant conditioned on the phenomic experiment. We identify this problem as contrastive phenomolecular retrieval (see Figure 2). Addressing this problem can allow for identification of molecular impact on cellular function, however, this comes with its own set of challenges. [18, 2, 65]. ", "page_idx": 1}, {"type": "text", "text": "(1) Firstly, multi-modal paired phenomics molecular data suffers from lower overall dataset sizes and is subject to batch effects. Challenges with uniform processing and prohibitive costs associated with acquisition of paired data, leads to an order of magnitude fewer data points compared to text-image datasets [49, 11]. Furthermore, data is subject to random batch effects that capture non-biologically meaningful variation [33, 55]. (2) Paired phenomic-molecular data contains inactive perturbations that do not have a biological effect or do not perturb cellular morphology. It is difficult to infer a priori whether a molecule has a cellular effect, leading to the collection of paired molecular structures with unperturbed cells. These data-points are challenging to fliter out without an effective phenomic embedding, as morphological effects are rarely discernible. These samples can be interpreted as misannotated, under the assumption of all collected pairs having biologically meaningful interactions. (3) Finally, a complete solution for capturing molecular effects on cells must capture molecular concentration. The same molecule can have drastically different effects along its dose response curve, thus making concentration an essential component for learning molecular impact. ", "page_idx": 1}, {"type": "text", "text": "In this work, we explore the problem of contrastive phenomolecular retrieval by addressing the above challenges circumvented in prior works. Our key contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We demonstrate significantly higher phenomolecular retrieval rates by utilizing a pretrained unimodal phenomic encoder. Thus alleviating the data availability challenge, reducing the impact of batch effects, and identifying molecular activity levels.   \n\u2022 We propose a novel soft-weighted sigmoid locked loss (S2L) that addresses the effects of inactive molecules. This is done by leveraging distances computed in the phenomic embedding space to learn inter-sample similarities.   \n\u2022 We explore explicit and implicit methods to encode molecular concentration, assessing the model\u2019s ability to perform retrieval in an inter-concentration setting and generalize to unseen concentrations. ", "page_idx": 1}, {"type": "image", "img_path": "LQBlSGeOGm/tmp/4dde0a7e08853a6ee10fade0a8a453eab41f9aed6ba986f44f34c7d74255cb84.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Illustration of proposed guidelines when incorporated in our MolPhenix contrastive phenomolecular retrieval framework. We address challenges by utilizing uni-modal pretrained MAE & MPNN models, inter-sample weighting with a dosage aware S2L loss, undersampling inactive molecules, and encoding molecular concentration. ", "page_idx": 1}, {"type": "text", "text": "Following these principles, we build MolPhenix, a multi-modal molecular phenomics model addressing contrastive phenomolecular retrieval (Figure 1). MolPhenix demonstrates large and consistent improvements in the presence of batch effects, generalizing across different concentrations, molecules, and activity thresholds. Additionally, MolPhenix outperforms baseline methods in zero-shot setting, achieving $77.33\\%$ top- $\\cdot1\\%$ retrieval accuracies on active molecules, which corresponds to a ${\\bf8.1\\times}$ improvement over the previous state-of-the-art (SOTA) [48]. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Uni-modality Pretraining: Self-supervised methods have demonstrated success across a variety of domains such as computer vision, natural language processing and molecular representations [3, 44, 61]. In vision, contrastive methods have been used to minimize distance in the model\u2019s latent space of two views of the same sample [12, 51, 19, 21]. Reconstruction objectives have also permeated computer vision, such as masked autoencoders (MAE). MAEs typically utilize vision transformers to partition the image into learnable tokens and reconstruct masked patches [20, 17, 10, 14]. These methods have been extended to microscopy experimental data designed to capture cell morphology [60, 28]. Phenom1 utilizes a masked autoencoder with a ViT- $L/8+$ architecture and a custom Fourier domain reconstruction loss, yielding informative representations of phenomic experiments [28, 13]. From a representational perspective, Graph Neural Networks (GNN) have been used to predict molecular properties by reasoning over graph structures. A combination of reconstruction and supervised objectives have led to models generalizing to a diverse range of prediction tasks [36, 66, 56, 47]. Our work leverages uni-modal foundation models, which are used to generate embeddings of phenomic images and molecular graphs. ", "page_idx": 2}, {"type": "text", "text": "Multi-Modal Objectives: Multi-modal models combine samples from two or more domains, to learn rich representations and demonstrate flexible ways to predict sample properties [43, 1, 23]. Contrastive methods minimize distances between paired samples, traditionally in text-image domains. However, training these models is computationally expensive, requiring large datasets. Multiple contributions have allowed for a reduction in compute and data budgets by an order of magnitude. In $L i T$ , the authors demonstrate that utilizing uni-modal pretrained models for one or both modalities matches zero-shot performance with an order of magnitude fewer paired examples seen [63]. Zhai et al. (2023) demonstrate that by replacing the softmax operation over cosine similarities with an element wise sigmoid loss, allows contrastive learners to improve performance under label noise regime [62]. By using a uni-modal pre-trained modal to calculate similarities between samples from one of the modalities, Srinivasa et al. (2023) have demonstrated improved performance on zero-shot evaluation [53]. In our work, we build along these directions in molecular phenomic multi-modal training. ", "page_idx": 2}, {"type": "text", "text": "Molecular-Phenomic Contrastive Learning: Prior works in contrastive phenomic retrieval have utilized the InfoNCE objective as a pre-training technique to construct uni-modal representations [38, 64]. Nguyen et al. (2023) propose a multi-modal objective trained on hand-engineered visual features and a GNN molecular encoder. The work demonstrates improved molecular property prediction with no image encoder pre-training [37, 54]. Recent methods have attempted to improve retrieval by using the InfoLOOB objective [41]. Specifically, CLOOME utilizes the InfoLOOB loss with hopfield networks for zero-shot retrieval on unseen data samples [45, 48]. InfoCORE aims to mitigate batch effects in multimodal molecular representations, improving retrieval capabilities and property prediction by adaptively reweighting samples to minimize confounding from non-biological associations [59]. Our work is parallel to the above directions, demonstrating a significant increase in molecular-phenomic retrieval by building on algorithmic improvements from the multi-modality literature. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we explain key challenges facing phenomolecular retrieval and provide guidelines that are key methodological improvements behind the success of MolPhenix 1. ", "page_idx": 2}, {"type": "text", "text": "Preliminaries: Our setting studies the problem of learning multi-modal representations of molecules and phenomic experiments of treated cells [48]. The aim of this work is to learn a joint latent space which maps phenomic experiments of treated cells and the corresponding molecular perturbations into the same latent space. We consider a set of lab experiments $\\mathcal{E}$ defined as the tuple $({\\bf X},{\\bf M},{\\bf C},\\Psi)$ . Each experiment $\\epsilon\\in\\mathcal{E}$ consists of data samples $\\mathbf{x}_{i}\\in\\mathbf{X}$ (such as images) and perturbations $\\mathbf{m}_{i}\\in\\mathbf{M}$ (such as molecules) which are obtained at a specific dosage concentration $\\mathbf{c}_{i}\\in\\mathbf{C}$ , while $\\psi\\in\\Psi$ denotes molecular activity threshold. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Figure 2 describes the problem of contrastive phenomolecular retrieval, where for a single image $\\mathbf{x}_{i}$ , the challenge consists of identifying the matching perturbation, ${\\mathbf{m}}_{i}$ , and concentration, $\\mathbf{c}_{i}$ , used to induce morphological effects. This can be accomplished in a zero-shot way by generating embeddings for $(\\mathbf{m}_{1},\\mathbf{c}_{1}),\\ldots(\\mathbf{m}_{j},\\mathbf{c}_{j})$ and $\\mathbf{x}_{i}$ using functions $f_{\\theta_{m}}(\\mathbf{m},\\mathbf{c})$ , $f_{\\theta_{x}}(\\mathbf{x})$ which map samples into $\\mathbb{R}^{d}$ . Then, by defining a similarity metric between generated embeddings $\\mathbf{z}_{x_{i}}$ and ${\\bf z}_{m_{i}}$ , $f_{s i m}$ , we can rank $(\\mathbf{m}_{1},\\mathbf{c}_{1})...(\\mathbf{m}_{j},\\mathbf{c}_{j})$ based on computed similarities. An effective solution to the contrastive phenomolecular retrieval problem would learn $f_{\\theta_{m}}(\\mathbf{m},\\mathbf{c})$ and $f_{\\theta_{x}}(\\mathbf{x})$ that results in consistently high retrieval rates of $(\\mathbf{m}_{i},\\mathbf{c}_{i})$ used to perturb $\\mathbf{x}_{i}$ . ", "page_idx": 3}, {"type": "text", "text": "In practice, the image embeddings are generated using a phenomics microscopy foundation MAE model [28, 20]. We use phenomic embeddings to marginalize batch effects, infer inter-sample ", "page_idx": 3}, {"type": "image", "img_path": "LQBlSGeOGm/tmp/e95ee8f82ce1332f4d191ca9cbc9ed0c99fe2eeea40d34fc81f137c7e1bd0a06.jpg", "img_caption": ["Figure 2: Illustration of the contrastive phenomolecular retrieval challenge. Image $\\mathbf{x}_{i}$ and a set of molecules and corresponding concentrations $\\left(\\mathbf{m}_{k},\\mathbf{c}_{k}\\right)$ get mapped into a $\\mathbb{R}^{d}$ latent space. Their similarities get computed with $f_{s i m}$ and ranked to evaluate whether the paired perturbation appears in the top ${\\bf K}\\%$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "similarities, and undersample inactive molecules. Activity is determined using consistency of replicate measurements for a given perturbation. For each sample, a $p$ value cutoff $\\psi\\in\\Psi$ is used to quantify molecular activity. Only molecules below the $p$ value cutoff $\\psi$ are considered active. ", "page_idx": 3}, {"type": "text", "text": "Prior methods in multi-modal contrastive learning utilize the InfoNCE loss, and variants thereof [38] to maximize the joint likelihood of $\\mathbf{x}_{i}$ and ${\\mathbf{m}}_{i}$ . Given a set of $N\\times N$ random samples $(\\mathbf{x}_{1},\\mathbf{m}_{1},\\mathbf{c}_{1}),\\cdot\\cdot\\cdot\\mathbf{\\Phi},(\\mathbf{x}_{N},\\mathbf{m}_{N},\\mathbf{c}_{N})$ containing $N$ positive samples at $k^{\\mathrm{th}}$ index and $(N\\mathrm{~-~}1)\\,\\times\\,N$ negative samples, optimizing Equation 1 maximizes the likelihood of positive pairs while minimizing the likelihood of negative pairs: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{InfoNCE}}=-\\frac{1}{N}\\sum_{i=1}^{N}\\left[\\log\\frac{\\exp(\\langle\\mathbf{z}_{x_{i}},\\mathbf{z}_{m_{i}}\\rangle/\\tau)}{\\sum_{k=1}^{N}\\exp(\\langle\\mathbf{z}_{x_{i}},\\mathbf{z}_{m_{k}}\\rangle/\\tau)}+\\log\\frac{\\exp(\\langle\\mathbf{z}_{x_{i}},\\mathbf{z}_{m_{i}}\\rangle/\\tau)}{\\sum_{k=1}^{N}\\exp(\\langle\\mathbf{z}_{m_{i}},\\mathbf{z}_{x_{k}}\\rangle/\\tau)}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Where $\\mathbf{z}_{x},\\ \\mathbf{z}_{m}$ correspond to phenomics and molecular embeddings respectively, $\\tau$ is softmax temperature, and $\\langle\\cdot\\rangle$ corresponds to cosine similarity. ", "page_idx": 3}, {"type": "text", "text": "Challenge 1: Phenomic Pretraining and Generalization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We find that using a phenomics foundation model to embed microscopy images allows for mitigation of batch effects, reduces the required number of paired data points, and improves generalization in the process. While CLIP, a hallmark model in the field of text-image multi-modality, was trained on 400 million curated paired data points, there is an order of magnitude fewer paired molecular-phenomic molecule samples [43]. Cost and systematic pre-processing of data make large scale data generation efforts challenging, and resulting data is affected by experimental batch effects. Batch effects induce noise in the latent space as a result of random perturbations in the experimental process, while biologically meaningful variation remains unchanged [39, 52]. Limited dataset sizes and batch effects make it challenging for contrastive learners to capture molecular features affecting cell morphology, yielding low retrieval rates [48]. ", "page_idx": 3}, {"type": "text", "text": "We address data availability and generalization challenges by utilizing representations from a large unimodal pre-trained phenomic model, $\\theta_{\\mathrm{Ph}}$ , trained to capture representations of cellular morphology. $\\theta_{\\mathrm{Ph}}$ is pretrained on microscopy images using a Fourier modified MAE objective, utilizing the ViT-L/8 architecture with methodology similar to Kraus et al. (2024) [20, 14, 28]. For simplicity in future sections, we refer to this model as Phenom1. This pretrained model allows a drastic reduction in the required number of paired multi-modal samples [63]. In addition, using phenomic representations alleviates the challenge of batch effects by averaging samples, $\\mathbf{z}_{x}$ , generated with the same perturbation ${\\bf m}_{i}$ over multiple lab experiments $\\epsilon_{i}$ . Averaging model representations $\\scriptstyle{\\frac{1}{N}}\\sum_{i\\in N}^{1}\\mathbf{Z}_{x_{i}}$ allows marginalizing batch effect induced by individual experiments. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Guideline 1 Utilizing pre-trained uni-modal encoder, $\\theta_{P h}$ , can be used to reduce the number of paired data-points compared to training $\\theta$ without prior optimization. In addition, averaging phenomic embeddings $\\mathbf{z}_{x}$ from matched perturbations can alleviate batch effects. ", "page_idx": 4}, {"type": "text", "text": "To reason over molecular structures, we make use of features learned from GNNs trained on molecular property prediction [34]. We utilize a pretrained MPNN foundational model up to the order of 1B parameters for extracting molecular representations following a similar procedure to Sypetkowski et al. (2024) [56]. We refer to this model as MolGPS. ", "page_idx": 4}, {"type": "text", "text": "Challenge 2: Inactive Molecular Perturbations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The phenomics-molecular data collection process can result in pairing of molecular structures with unperturbed cells in cases where the molecule has no effect on cell morphology (Figure 3) ", "page_idx": 4}, {"type": "text", "text": "Since the morphological effects observed in cell $\\mathbf{x}_{i}$ is conditioned on the perturbation, in the absence of a molecular effect $P(\\mathbf{\\dot{x}_{i}}|\\mathbf{x}_{i}^{0},\\mathbf{c_{i}},\\mathbf{m_{i}})\\sim P(\\mathbf{x_{i}}|\\mathbf{x}_{i}^{0})$ . In these samples, phenomic data will be independent, from paired molecular data, which results in misannotation under the assumption of data-pairs having an underlying biological relationship. We demonstrate how utilizing Phenom1 to undersample inactive molecules and learn continuous similarities between samples can alleviate this challenge. ", "page_idx": 4}, {"type": "image", "img_path": "LQBlSGeOGm/tmp/25029295e3312bb17b8a5339770f102dd2351b9c5e1c1d4ae4c9d0d2a113970b.jpg", "img_caption": ["Figure 3: Data generation process of a phenomic experiment on cells $\\mathbf{x_{i}}$ with molecular perturbations ${\\bf{m_{i}}}$ and concentrations $\\mathbf{c_{i}}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "To undersample inactive molecules, we extract the embeddings from Phenom1 and calculate the relative activity of each perturbation $(\\mathbf{m}_{i},\\mathbf{c}_{i})\\in(\\mathbf{M},\\mathbf{C})$ . This is done using the rank of cosine similarities between technical replicates produced for a molecular perturbation against a null distribution. The null distribution is established by calculating cosine similarities from random pairs of Phenom1 embeddings generated with perturbation $(\\mathbf{m}_{j},\\mathbf{c}_{j}),(\\mathbf{m}_{k},\\mathbf{c}_{k})$ . Hence, we can compute a p-value and filter out samples likely to belong to the null distribution with an arbitrary threshold $\\psi$ . ", "page_idx": 4}, {"type": "text", "text": "In addition, by utilizing an inter-sample aware S2L training objective, the model can learn similarities between inactive molecules. S2L is grounded in previous work which demonstrates improved robustness to label noise (SigLip) and learnable inter-sample associations (CWCL) [62, 53]. Continuous Weighted Contrastive Loss (CWCL) provides better multi-modal alignment using a uni-modal pretrained model to suggest sample distances, relaxing the negative equidistant assumption present in InfoNCE [53]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{CWCL},\\,M\\rightarrow\\chi}=-\\frac{1}{N}\\sum_{i=1}^{N}\\left[\\frac{1}{\\sum_{j=1}^{N}\\mathbf{w}_{i,j}^{\\chi}}\\sum_{j=1}^{N}\\mathbf{w}_{i,j}^{\\chi}\\log\\frac{\\exp\\left(\\langle\\mathbf{z}_{x_{i}},\\mathbf{z}_{m_{j}}\\rangle/\\tau\\right)}{\\sum_{k=1}^{N}\\exp\\left(\\langle\\mathbf{z}_{x_{j}},\\mathbf{z}_{m_{k}}\\rangle/\\tau\\right)}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "CWCL weights logits with a continuous measure of similarity $\\mathbf{w}^{\\mathcal{X}}$ , resulting in better alignment of embeddings $\\mathbf{z}_{\\mathbf{x}_{i}}$ and $\\mathbf{z_{m_{j}}}$ across modalities. In equation 2, $\\mathbf{w}^{\\bar{\\mathcal{X}}}$ is computed using a within modality similarity function such as $\\mathbf{w}_{i,j}^{\\mathcal{X}}\\,=\\,\\langle z_{\\mathbf{x}_{i}},z_{\\mathbf{x}_{j}}\\rangle/2+0.5$ . Note, the above formula is used only for mapping samples from modality $\\mathcal{M}$ to $\\mathcal{X}$ for which a pre-trained model $\\theta_{\\mathrm{Ph}}$ is available. ", "page_idx": 4}, {"type": "text", "text": "Another work, SigLIP, demonstrates robustness to label noise and reduces computational requirements during contrastive training [62]. It does so by avoiding computation of a softmax over the entire set of in-batch samples, instead relying on element-wise sigmoid operation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{SigLP}}=-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\left[\\log\\frac{1}{1+\\exp\\left(\\mathbf{y}_{i,j}(-\\alpha\\left\\langle\\mathbf{z}_{\\mathbf{x}_{i}},\\mathbf{z}_{\\mathbf{m}_{j}}\\right\\rangle+b)\\right)}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/6911f6b5da997ef0019a68ae38793e09f4c8b489885c341f96d155d1cb5ca94a.jpg", "table_caption": ["Algorithm 1 S2L loss pseudo-implementation. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "In equation 3, $\\alpha$ and $b$ are learned, calibrating the model confidence conditioned on the ratio of positive to negative pairs. $\\mathbf{y}_{i,j}$ is set to 1 if $i=j$ and $^{-1}$ otherwise. ", "page_idx": 5}, {"type": "text", "text": "Inspired by prior works, we introduce S2L for molecular representation learning, which leverages inter-sample similarities and robustness to label noise to mitigate weak or inactive perturbations. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{S2L}}=-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\log\\left[\\frac{\\mathbf{w}_{i,j}^{\\mathcal{X}}}{1+\\exp\\left(-\\alpha(\\mathbf{z}_{\\mathbf{x}_{i}},\\mathbf{z}_{\\mathbf{m}_{j}})+b)\\right)}+\\frac{(1-\\mathbf{w}_{i,j}^{\\mathcal{X}})}{1+\\exp\\left(\\alpha\\langle\\mathbf{z}_{\\mathbf{x}_{i}},\\mathbf{z}_{\\mathbf{m}_{j}}\\rangle+b\\right))}\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the equation above, $\\mathbf{z}_{\\mathbf{x}_{i}}$ and $\\mathbf{z_{m_{j}}}$ correspond to latent representations of images and molecules, respectively. $\\alpha$ and $b$ correspond to learnable temperature and bias parameters for the calibrated sigmoid function. $\\mathbf{w}_{i j}^{\\mathcal{X}}$ is an inter-sample similarity function computed from images using the pretrained model $\\theta_{\\mathrm{Ph}}$ . To compute $\\mathbf{w}_{i,j}^{\\mathcal{X}}$ , we use the arctangent of L2 distance instead of cosine similarity, as was the case for Equation 2 (more details in Appendix D.3). Intuitively, S2L can be thought of as shifting from a multi-class classification to a soft multi-label problem. In our problem setting, the labels are continuous and determined by sample similarity in the phenomics space. ", "page_idx": 5}, {"type": "text", "text": "Guideline 2 When training a molecular-phenomic model, mitigating the effect of inactive molecules in training data distribution can be carried out by undersampling inactive molecules and using an inter-sample similarity aware, S2L loss (equation 4). ", "page_idx": 5}, {"type": "text", "text": "Challenge 3: Variable Concentrations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Perturbation effect on a cell is determined by both molecular structure and corresponding concentration [58]. A model capturing molecular impact on cell morphology must be able to generalize across different doses, since variable concentrations can correspond to different data distributions. ", "page_idx": 5}, {"type": "text", "text": "We note that providing concentrations $\\mathbf{c}_{i}$ as input to the model would benefit performance, as this would indicate the magnitude of molecular impact. However, we find that simply concatenating concentrations does not result in effective training due to its compressed dynamic range. To that end, we add concentration information in two separate ways: implicit and explicit formulations. ", "page_idx": 5}, {"type": "text", "text": "We add implicit concentration as molecular perturbation classes by using the S2L loss (Equation 4) to treat perturbation ${\\bf m}_{i}$ with concentrations $\\mathbf{c}_{i}$ and $\\mathbf{c}_{j}$ as distinct classes. This pushes samples apart in the latent space proportionally to similarities between phenomic experiments. ", "page_idx": 5}, {"type": "text", "text": "We add explicit concentration $c_{i}$ by passing it to the molecular encoder. We explore different formulation for dosage concentrations, $\\bar{\\mathbf{f^{\\prime}}}(c_{i})$ , where $\\mathbf{f^{\\prime}}$ maps $\\mathbf{c_{i}}\\rightarrow\\mathbb{R}$ . Encoded representations $\\mathbf{f}^{\\prime}(c_{i})$ ", "page_idx": 5}, {"type": "text", "text": "are concatenated at the initial layer of the model. We find simple functional encodings $\\mathbf{f^{\\prime}}$ (such as one-hot and logarithm) to work well in practice. ", "page_idx": 6}, {"type": "text", "text": "4 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we describe evaluation datasets used, and descriptions of the underlying data modalities. To assess phenomolecular retrieval, we use $1\\%$ recall metric unless stated otherwise, as it allows direct comparison between datasets with different number of samples. Additional implementation and evaluation details can be found in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "Datasets: Our training dataset consists of fluorescent microscopy images paired with molecular structures and concentrations, which are used as perturbants. We assess models\u2019 phenomolecular retrieval capabilities on three datasets of escalating generalization complexity. First dataset, consisting of unseen microscopy images and molecules present in the training dataset. Second, a dataset consisting of previously unseen phenomics experiments and molecules split by the corresponding molecular scaffold. Finally, we evaluate on an open source dataset with a different data generating distribution [16]. In the case of the latter two datasets, the model is required to perform zero-shot classification, as it has no access to those molecules in the training data. This requires the model to reason over molecular graphs to identify structures inducing corresponding cellular morphology changes. Using methodology described in guideline 2 we report retrieval results for all molecules as well as on an active subset. Finally, all datasets are comprised of molecular structures at multiple concentrations (.01, .1, 1.0, 10, etc.) Additional details regarding the datasets can be found in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Modality Representations: In our evaluations, we consider different representations for molecular perturbations and phenomic experiments and quantitatively evaluate their impact. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Images: Image encoders utilize 6-channel fluorescent microscopy images of cells representing phenomic experiments. Images are $2048\\times2048$ pixels, capturing cellular morphology changes post molecular perturbation. We downscale each image to $256\\times256$ using block mean downsampling. \u2022 Phenom1: We characterize phenomic experiments by embedding high resolution microscopy images in the latent space of a phenomics model $\\theta_{P h}$ as described in guideline 1. \u2022 Fingerprints: Molecular fingerprints utilize RDKIT [31], MACCS [30] and MORGAN3 [46] bit coding, which represent binary presence of molecular substructures. Additional information such as atomic identity, atomic radius and torsional angles are included in the fingerprint representations. \u2022 MolGPS: We generate molecular representations from a large pretrained GNN. Specifically, we obtain molecular embeddings from a 1B parameter MPNN [34]. ", "page_idx": 6}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/12a3dc4e9df7adca6573cf289bdd07f60815901fc656dfe44770fdf0375f8dda.jpg", "table_caption": ["Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an $\\mathbf{8.1\\times}$ improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom \\*). We note that MolPhenix\u2019s main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model. "], "table_footnote": ["MolPhenix\\* Phenom1 & MolGPS .9689 \u00b1 .0017 .7733 \u00b1 .0036 .5860 \u00b1 .0082 .5583 \u00b1 .0007 .3824 \u00b1 .0016 .2809 \u00b1 .0060 "], "page_idx": 6}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/ce244c8264861adb623b1a65a9abcfc50cd44609dfdee18e63efb9b886d91ece.jpg", "table_caption": ["Table 2: Top- $1\\%$ recall accuracy with use of the proposed MolPhenix guidelines, such as Phenom1 and embedding averaging. We omit explicit concentration from this experiment. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/a0d763ecb47e4a4954d97fd818c5923208786067b26d10929ba08ae1ebc53990.jpg", "table_caption": ["Table 3: Top- $\\cdot1\\%$ recall accuracy across different concentration encoding choices with use of the proposed MolPhenix guidelines, such as Phenom1 and embedding averaging. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5 Results and Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To evaluate the effectiveness of Guidelines 1, 2, and 3 we carry out evaluation in two different settings: (1) cumulative concentrations, and (2) held-out concentrations, testing the models\u2019 ability to generalize to new molecular doses. Finally, we perform comprehensive ablations testing model performance with varying data, model, and optimization parameters. The comprehensive set of results can be found in Tables 10, 11, 12, and 13. ", "page_idx": 7}, {"type": "text", "text": "5.1 Evaluation on cumulative concentrations: ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We demonstrate improvements in phenomolecular recall due to usage of a phenomics pre-trained foundation model, identify that MolPhenix set of design choices results in higher final performance, and more data efficient learning. Figure 4 demonstrates recall accuracy on all molecules and an active subset for CLOOME and MolPhenix models, as a function of training samples seen. ", "page_idx": 7}, {"type": "text", "text": "We observe a large performance gap between models trained on Phenom1 embeddings as opposed to images, emphasizing the utility of using a pre-trained encoder for microscopy images (Table 1). We note that provision of Phenom1 (CLOOME-Phenom1 Vs CLOOMEImages) significantly improves both active and all molecule retrieval by $\\mathbf{5.69\\times}$ and, $\\mathbf{4.75\\times}$ respectively (Table 1). ", "page_idx": 7}, {"type": "text", "text": "Furthermore, we identify that while all molecules retrieval stagnates throughout training, the performance on an active subset keeps improving, underscoring the importance of identification of the active subset. Finally, we com", "page_idx": 7}, {"type": "image", "img_path": "LQBlSGeOGm/tmp/0699712bc834a51510ae5397a74e01fbf3e4e2b94b57e2716145398cf43e57a8.jpg", "img_caption": ["Figure 4: Comparison of training phenomic encoder from scratch and utilizing pre-trained Phenom1 unseen dataset. X-axis plotted on logarithmic scale. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "pare CLOOME and MolPhenix trained using Phenom1 embeddings and find there is a consistent retrieval performance gap, throughout training, with a $\\mathbf{1.26\\times}$ final improvement (Figure 4, Table 1). Compared to CLOOME [48] trained directly on images, MolPhenix achieves an average improvement of $8.78\\times$ on active molecules on the unseen dataset. These results verify the effectiveness of Guideline 1 in accelerating training, and the importance of Guidelines 2 and 3 in recall improvements over CLOOME. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We evaluate the impact of different loss objectives on the proposed MolPhenix training framework. Table 2 presents top- $\\cdot1\\%$ retrieval accuracy across different contrastive losses utilized to train molecular-phenomics encoders on cumulative concentrations. Compared to prior methods, the proposed S2L loss demonstrates improved retrieval rates in cumulative concentration setting. Label noise and inter-sample similarity aware losses such as CWCL and SigLip also demonstrate improved performance. The effectiveness of S2L can be attributed to smoothed inter-sample similarities and implicit concentration information. ", "page_idx": 8}, {"type": "text", "text": "Finally, in Table 3, we observe recall improvements when considering both molecular structures and concentration. We note the importance of the addition of implicit concentration, further confirming the importance of considering molecular effects at different concentrations as different classes. Explicitly encoding molecular concentration with one-hot, logarithm and sigmoid yields improved recall performance, where one-hot performs the best in a cumulative concentration setting. These findings verify the efficacy of implicit and explicit concentration encoding outlined in Guideline 3. ", "page_idx": 8}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/fc35efc4bb6cc7ba1c54d796a3de1ffad341be39ac99f5dd12ba87572c97419f.jpg", "table_caption": ["Table 4: Top- $\\cdot1\\%$ recall accuracy of dif-Table 5: Top- $1\\%$ recall accuracy across different concentraferent loss objectives while using the tion encoding choices while using the proposed MolPhenix proposed MolPhenix guidelines, such as guidelines, such as Phenom1 and embedding averaging. Phenom1 and embedding averaging. "], "table_footnote": ["Results are averaged across experiments for each dropped concentration, and across three seeds. Recall is reported for active molecules, while the results for all molecules can be found in Table 13. "], "page_idx": 8}, {"type": "text", "text": "5.2 Evaluation on held-out concentrations: ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Next, we evaluate recall on held-out concentrations to obtain a measure of generalization performance. This evaluation allows us to capture the utility of our models for prediction of unseen concentrations, hence resembling in-silico testing. We omit concentrations from the training set and evaluate recall at the excluded data, where we observe a drop in retrieval performance for unseen concentrations. Similar to cumulative concentration results, we find that using S2L improves recall over other losses and outperforms CLOOME by up to $126\\%$ (Table 4). While one-hot encoding exhibits significant improvements in cumulative concentrations, its expressivity on unseen concentrations is limited (Table 5) and sigmoid encoding provides a sufficient representation of concentration. ", "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We assess the importance of our design decisions by conducting an ablation study over our proposed guidelines. Figure 5 presents the variation of top- $\\cdot1\\%$ recall accuracy across key components such as cutoff $p$ value, fingerprint type, and embedding averaging. We observe that employing a lower cutoff $p$ value yields improved generalization for unseen dataset, while employing a higher cutoff appears to be optimal for unseen images $^+$ unseen molecules. For molecular structure representations, we find that using embeddings from the large pretrained MPNN graph based model (e.g., MolGPS) surpasses traditional fingerprints. Finally, utilization of embedding averaging demonstrates improved recall. ", "page_idx": 8}, {"type": "image", "img_path": "LQBlSGeOGm/tmp/42848896b3e6864e24bb9d0f4b9a4e25d3183e01c6625d932dcf8f95d9f90069.jpg", "img_caption": ["Figure 5: Ablations of top- $1\\ \\%$ recall accuracy with (top-left) the size of embedding dimension, (top-center) number of parameters, (top-right) batch size, (bottom-left) cutoff $p$ value, (bottomcenter) fingerprint type, and (bottom-right) random batch averaging. Compact embedding sizes from pretrained models, larger number of parameters, larger batch sizes, lower cutoff ${\\bf p}$ -values, pretrained MolGPS fingerprints and presence of random batch averagin improving retrieval of our MolPhenix framework. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we investigate the problem of contrastive phenomolecular retrieval by constructing a joint multi-modal embedding of phenomic experiments and molecular structures. We identify a set of challenges afflicting molecular-phenomic training and proposed a set of guidelines for improving retrieval and generalization. Empirically, we observed that contrastive learners demonstrate higher retrieval rates when using representations from a high-capacity uni-modal pretrained model. Use of inter-sample similarities with a label noise resistant loss such as S2L allows us to tackle the challenge of inactive molecules. Finally, adding implicit and explicit concentrations allows models to generalize to previously unseen concentrations. MolPhenix demonstrates an $\\mathbf{8.1\\times}$ improvement in zero shot retrieval of active molecules over the previous state-of-the-art, reaching $77.33\\%$ in top- $1\\%$ accuracy. In addition, we conduct a preliminary investigation on MolPhenix\u2019s ability to uncover biologically meaningful properties (activity prediction, zero-shot biological perturbation matching, and molecular property prediction in Appendix E.1, E.2, and E.3, respectively.). We expect a wide range of applications for MolPhenix, particularly in drug discovery. While there\u2019s a remote chance of misuse for developing chemical weapons, such harm is unlikely, with our primary focus remaining on healthcare improvement. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Work: While our study covers challenges in phenomolecular recall, we leave three research directions for future work. (1) Future investigations could consider studying additional modalities such as text, genetic perturbations and chemical multi-compound interventions. (2) While we propose and evaluate our guidelines on previously conducted phenomic experiments, we note that a rigorous evaluation would evaluate model predictions in a wet-lab setting. (3) In addition, our work makes the assumption that the initial unperturbed cell state $\\boldsymbol{x}_{i}^{0}$ can be marginalized by utilizing a single cell line with an unperturbed genetic background. Future works can relax this assumption, aiming to capture innate intercellular variation. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank the broader Valence Labs team and Recursion Pharmaceuticals for support on the project. We thank Berton Earnshaw, Jason Hartford, Emmanuel Bengio, Oren Kraus, and Emmanuel Noutahi for providing valuable feedback on the manuscript. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan. Flamingo: a visual language model for few-shot learning, 2022.   \n[2] S. Albelwi. Survey on self-supervised learning: auxiliary pretext tasks and contrastive learning methods in imaging. Entropy, 24(4):551, 2022.   \n[3] R. Balestriero, M. Ibrahim, V. Sobal, A. Morcos, S. Shekhar, T. Goldstein, F. Bordes, A. Bardes, G. Mialon, Y. Tian, A. Schwarzschild, A. G. Wilson, J. Geiping, Q. Garrido, P. Fernandez, A. Bar, H. Pirsiavash, Y. LeCun, and M. Goldblum. A cookbook of self-supervised learning, 2023.   \n[4] C. Bock, P. Datlinger, F. Chardon, M. A. Coelho, M. B. Dong, K. A. Lawson, T. Lu, L. Maroc, T. M. Norman, B. Song, G. Stanley, S. Chen, M. Garnett, W. Li, J. Moffat, L. S. Qi, R. S. Shapiro, J. Shendure, J. S. Weissman, and X. Zhuang. High-content crispr screening. Nature Reviews Methods Primers, 2(1), Feb. 2022.   \n[5] R. S. Bohacek, C. McMartin, and W. C. Guida. The art and practice of structure-based drug design: A molecular modeling perspective. Medicinal Research Reviews, 16(1):3\u201350, Jan. 1996.   \n[6] M. Boutros, F. Heigwer, and C. Laufer. Microscopy-based high-content screening. Cell, 163(6):1314\u20131325, 2015.   \n[7] M.-A. Bray, S. Singh, H. Han, C. T. Davis, B. Borgeson, C. Hartland, M. Kost-Alimova, S. M. Gustafsdottir, C. C. Gibson, and A. E. Carpenter. Cell painting, a high-content imagebased assay for morphological profiling using multiplexed fluorescent dyes. Nature protocols, 11(9):1757\u20131774, 2016.   \n[8] M.-A. Bray, S. Singh, H. Han, C. T. Davis, B. Borgeson, C. Hartland, M. Kost-Alimova, S. M. Gustafsdottir, C. C. Gibson, and A. E. Carpenter. Cell painting, a high-content imagebased assay for morphological profiling using multiplexed fluorescent dyes. Nature protocols, 11(9):1757\u20131774, 2016.   \n[9] J. C. Caicedo, S. Cooper, F. Heigwer, S. Warchal, P. Qiu, C. Molnar, A. S. Vasilevich, J. D. Barry, H. S. Bansal, O. Kraus, et al. Data-analysis strategies for image-based cell profiling. Nature methods, 14(9):849\u2013863, 2017.   \n[10] S. Cao, P. Xu, and D. A. Clifton. How to understand masked autoencoders. arXiv preprint arXiv:2202.03670, 2022.   \n[11] S. N. Chandrasekaran, J. Ackerman, E. Alix, D. M. Ando, J. Arevalo, M. Bennion, N. Boisseau, A. Borowa, J. D. Boyd, L. Brino, P. J. Byrne, H. Ceulemans, C. Ch\u2019ng, B. A. Cimini, D.-A. Clevert, N. Deflaux, J. G. Doench, T. Dorval, R. Doyonnas, V. Dragone, O. Engkvist, P. W. Faloon, B. Fritchman, F. Fuchs, S. Garg, T. J. Gilbert, D. Glazer, D. Gnutt, A. Goodale, J. Grignard, J. Guenther, Y. Han, Z. Hanifehlou, S. Hariharan, D. Hernandez, S. R. Horman, G. Hormel, M. Huntley, I. Icke, M. Iida, C. B. Jacob, S. Jaensch, J. Khetan, M. Kost-Alimova, T. Krawiec, D. Kuhn, C.-H. Lardeau, A. Lembke, F. Lin, K. D. Little, K. R. Lofstrom, S. Lotf,i D. J. Logan, Y. Luo, F. Madoux, P. A. Marin Zapata, B. A. Marion, G. Martin, N. J. McCarthy, L. Mervin, L. Miller, H. Mohamed, T. Monteverde, E. Mouchet, B. Nicke, A. Ogier, A.-L. Ong, M. Osterland, M. Otrocka, P. J. Peeters, J. Pilling, S. Prechtl, C. Qian, K. Rataj, D. E. Root, S. K. Sakata, S. Scrace, H. Shimizu, D. Simon, P. Sommer, C. Spruiell, I. Sumia, S. E. Swalley, H. Terauchi, A. Thibaudeau, A. Unruh, J. Van de Waeter, M. Van Dyck, C. van Staden, M. Warcho\u0142, E. Weisbart, A. Weiss, N. Wiest-Daessle, G. Williams, S. Yu, B. Zapiec, M. Z\u02d9y\u0142a, S. Singh, and A. E. Carpenter. Jump cell painting dataset: morphological impact of 136,000 chemical and genetic perturbations. bioRxiv, 2023.   \n[12] T. Chen, S. Kornblith, K. Swersky, M. Norouzi, and G. E. Hinton. Big self-supervised models are strong semi-supervised learners. Advances in neural information processing systems, ", "page_idx": 10}, {"type": "text", "text": "33:22243\u201322255, 2020. ", "page_idx": 10}, {"type": "text", "text": "[13] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. P. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 7480\u20137512. PMLR, 2023.   \n[14] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[15] K. Dunn, A. Aotaki-Keen, F. Putkey, and L. Hjelmeland. Arpe-19, a human retinal pigment epithelial cell line with differentiated properties. Experimental eye research, 62(2):155\u2013170, 1996.   \n[16] M. M. Fay, O. Kraus, M. Victors, L. Arumugam, K. Vuggumudi, J. Urbanik, K. Hansen, S. Celik, N. Cernek, G. Jagannathan, et al. Rxrx3: Phenomics map of biology. Biorxiv, pages 2023\u201302, 2023.   \n[17] C. Feichtenhofer, Y. Li, K. He, et al. Masked autoencoders as spatiotemporal learners. Advances in neural information processing systems, 35:35946\u201335958, 2022.   \n[18] A. F\u00fcrst, E. Rumetshofer, J. Lehner, V. T. Tran, F. Tang, H. Ramsauer, D. Kreil, M. Kopp, G. Klambauer, A. Bitto, et al. Cloob: Modern hopfield networks with infoloob outperform clip. Advances in neural information processing systems, 35:20450\u201320468, 2022.   \n[19] J.-B. Grill, F. Strub, F. Altch\u00e9, C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch, B. A. Pires, Z. D. Guo, M. G. Azar, B. Piot, K. Kavukcuoglu, R. Munos, and M. Valko. Bootstrap your own latent: A new approach to self-supervised learning, 2020.   \n[20] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000\u201316009, 2022.   \n[21] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning, 2020.   \n[22] M. Hofmarcher, E. Rumetshofer, D.-A. Clevert, S. Hochreiter, and G. Klambauer. Accurate prediction of biological assays with high-throughput microscopy images and convolutional networks. Journal of chemical information and modeling, 59(3):1163\u20131171, 2019.   \n[23] S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, B. Patra, Q. Liu, K. Aggarwal, Z. Chi, J. Bjorck, V. Chaudhary, S. Som, X. Song, and F. Wei. Language is not all you need: Aligning perception with language models, 2023.   \n[24] W. E. Johnson, C. Li, and A. Rabinovic. Adjusting batch effects in microarray expression data using empirical bayes methods. Biostatistics, 8(1):118\u2013127, 2007.   \n[25] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.   \n[26] P. Kemmeren, K. Sameith, L. A. Van De Pasch, J. J. Benschop, T. L. Lenstra, T. Margaritis, E. O\u2019Duibhir, E. Apweiler, S. van Wageningen, C. W. Ko, et al. Large-scale genetic perturbations reveal regulatory networks and an abundance of gene-specific repressors. Cell, 157(3):740\u2013752, 2014.   \n[27] C. Knox, M. Wilson, C. M. Klinger, M. Franklin, E. Oler, A. Wilson, A. Pon, J. Cox, N. E. L. Chin, S. A. Strawbridge, M. Garcia-Patino, R. Kruger, A. Sivakumaran, S. Sanford, R. Doshi, N. Khetarpal, O. Fatokun, D. Doucet, A. Zubkowski, D. Y. Rayat, H. Jackson, K. Harford, A. Anjum, M. Zakir, F. Wang, S. Tian, B. Lee, J. Liigand, H. Peters, R. Q. R. Wang, T. Nguyen, D. So, M. Sharp, R. da Silva, C. Gabriel, J. Scantlebury, M. Jasinski, D. Ackerman, T. Jewison, T. Sajed, V. Gautam, and D. S. Wishart. Drugbank 6.0: the drugbank knowledgebase for 2024. Nucleic Acids Research, 52(D1):D1265\u2013D1275, Nov. 2023.   \n[28] O. Kraus, K. Kenyon-Dean, S. Saberian, M. Fallah, P. McLean, J. Leung, V. Sharma, A. Khan, J. Balakrishnan, S. Celik, et al. Masked autoencoders for microscopy are scalable learners of cellular biology. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11757\u201311768, 2024.   \n[29] O. Z. Kraus, B. T. Grys, J. Ba, Y. Chong, B. J. Frey, C. Boone, and B. J. Andrews. Automated analysis of high-content microscopy data with deep learning. Molecular systems biology, 13(4):924, 2017.   \n[30] H. Kuwahara and X. Gao. Analysis of the effects of related fingerprints on molecular similarity using an eigenvalue entropy approach. Journal of Cheminformatics, 13:1\u201312, 2021.   \n[31] G. Landrum et al. Rdkit: A software suite for cheminformatics, computational chemistry, and predictive modeling. Greg Landrum, 8(31.10):5281, 2013.   \n[32] F. Lanusse, L. Parker, S. Golkar, M. Cranmer, A. Bietti, M. Eickenberg, G. Krawezik, M. McCabe, R. Ohana, M. Pettee, B. R.-S. Blancard, T. Tesileanu, K. Cho, and S. Ho. Astroclip: Cross-modal pre-training for astronomical foundation models, 2023.   \n[33] J. T. Leek, R. B. Scharpf, H. C. Bravo, D. Simcha, B. Langmead, W. E. Johnson, D. Geman, K. Baggerly, and R. A. Irizarry. Tackling the widespread and critical impact of batch effects in high-throughput data. Nature Reviews Genetics, 11(10):733\u2013739, 2010.   \n[34] D. Masters, J. Dean, K. Klaser, Z. Li, S. Maddrell-Mander, A. Sanders, H. Helal, D. Beker, A. Fitzgibbon, S. Huang, et al. Gps $^{,++}$ : Reviving the art of message passing for molecular property prediction. arXiv preprint arXiv:2302.02947, 2023.   \n[35] D. Mendez, A. Gaulton, A. P. Bento, J. Chambers, M. De Veij, E. F\u00e9lix, M. P. Magari\u00f1os, J. F. Mosquera, P. Mutowo, M. Nowotka, et al. Chembl: towards direct deposition of bioassay data. Nucleic acids research, 47(D1):D930\u2013D940, 2019.   \n[36] O. M\u00e9ndez-Lucio, C. Nicolaou, and B. Earnshaw. Mole: a molecular foundation model for drug discovery, 2022.   \n[37] C. Q. Nguyen, D. Pertusi, and K. M. Branson. Molecule-morphology contrastive pretraining for transferable molecular representation. bioRxiv, pages 2023\u201305, 2023.   \n[38] A. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n[39] H. S. Parker and J. T. Leek. The practical effect of batch on genomic prediction. Statistical applications in genetics and molecular biology, 11(3), 2012.   \n[40] Z. Pincus and J. Theriot. Comparison of quantitative methods for cell-shape analysis. Journal of microscopy, 227(2):140\u2013156, 2007.   \n[41] B. Poole, S. Ozair, A. Van Den Oord, A. Alemi, and G. Tucker. On variational bounds of mutual information. In International Conference on Machine Learning, pages 5171\u20135180. PMLR, 2019.   \n[42] D. L. Purich. Enzyme kinetics: catalysis and control: a reference of theory and best-practice methods. Elsevier, 2010.   \n[43] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[44] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. 2019.   \n[45] H. Ramsauer, B. Sch\u00e4f,l J. Lehner, P. Seidl, M. Widrich, T. Adler, L. Gruber, M. Holzleitner, M. Pavlovi\u00b4c, G. K. Sandve, et al. Hopfield networks is all you need. arXiv preprint arXiv:2008.02217, 2020.   \n[46] D. Rogers and M. Hahn. Extended-connectivity fingerprints. Journal of chemical information and modeling, 50(5):742\u2013754, 2010.   \n[47] Y. Rong, Y. Bian, T. Xu, W. Xie, Y. Wei, W. Huang, and J. Huang. Self-supervised graph transformer on large-scale molecular data, 2020.   \n[48] A. Sanchez-Fernandez, E. Rumetshofer, S. Hochreiter, and G. Klambauer. Cloome: contrastive learning unlocks bioimaging databases for queries with chemical structures. Nature, 2023.   \n[49] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278\u201325294, 2022.   \n[50] J. Simm, G. Klambauer, A. Arany, M. Steijaert, J. K. Wegner, E. Gustin, V. Chupakhin, Y. T. Chong, J. Vialard, P. Buijnsters, et al. Repurposing high-throughput image assays enables biological activity prediction for drug discovery. Cell chemical biology, 25(5):611\u2013618, 2018.   \n[51] K. Sohn. Improved deep metric learning with multi-class n-pair loss objective. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS\u201916, page 1857\u20131865, Red Hook, NY, USA, 2016. Curran Associates Inc.   \n[52] C. Soneson, S. Gerster, and M. Delorenzi. Batch effect confounding leads to strong bias in performance estimates obtained by cross-validation. PloS one, 9(6):e100335, 2014.   \n[53] R. S. Srinivasa, J. Cho, C. Yang, Y. M. Saidutta, C.-H. Lee, Y. Shen, and H. Jin. Cwcl: Crossmodal transfer with continuously weighted contrastive loss. Advances in Neural Information Processing Systems, 36, 2023.   \n[54] D. R. Stirling, M. J. Swain-Bowden, A. M. Lucas, A. E. Carpenter, B. A. Cimini, and A. Goodman. Cellproflier 4: improvements in speed, utility and usability. BMC bioinformatics, 22:1\u201311, 2021.   \n[55] M. Sypetkowski, M. Rezanejad, S. Saberian, O. Kraus, J. Urbanik, J. Taylor, B. Mabey, M. Victors, J. Yosinski, A. R. Sereshkeh, et al. Rxrx1: A dataset for evaluating experimental batch correction methods. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4284\u20134293, 2023.   \n[56] M. Sypetkowski, F. Wenkel, , F. Poursafaei, N. Dickson, K. Suri, P. Fradkin, and D. Beaini. On the scalability of foundational models for molecular graphs. arxiv, 2024.   \n[57] F. Vincent, A. Nueda, J. Lee, M. Schenone, M. Prunotto, and M. Mercola. Phenotypic drug discovery: recent successes, lessons learned and new directions. Nature Reviews Drug Discovery, 21(12):899\u2013914, 2022.   \n[58] R. M. Walmsley and N. Billinton. How accurate is in vitro prediction of carcinogenicity? British Journal of Pharmacology, 162(6):1250\u20131258, Feb. 2011.   \n[59] C. Wang, S. Gupta, C. Uhler, and T. Jaakkola. Removing biases from molecular representations via information maximization, 2023.   \n[60] R. Xie, K. Pang, G. D. Bader, and B. Wang. Maester: Masked autoencoder guided segmentation at pixel resolution for accurate, self-supervised subcellular structure recognition. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, June 2023.   \n[61] S. Zaidi, M. Schaarschmidt, J. Martens, H. Kim, Y. W. Teh, A. Sanchez-Gonzalez, P. Battaglia, R. Pascanu, and J. Godwin. Pre-training via denoising for molecular property prediction, 2022.   \n[62] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pretraining. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11975\u201311986, 2023.   \n[63] X. Zhai, X. Wang, B. Mustafa, A. Steiner, D. Keysers, A. Kolesnikov, and L. Beyer. Lit: Zero-shot transfer with locked-image text tuning, 2022.   \n[64] S. Zheng, J. Rao, J. Zhang, L. Zhou, J. Xie, E. Cohen, W. Lu, C. Li, and Y. Yang. Cross-modal graph contrastive learning with cellular images. Advanced Science, 11(32), June 2024.   \n[65] Y. Zhong, H. Tang, J. Chen, J. Peng, and Y.-X. Wang. Is self-supervised learning more robust than supervised learning? arXiv preprint arXiv:2206.05259, 2022.   \n[66] G. Zhou, Z. Gao, Q. Ding, H. Zheng, H. Xu, Z. Wei, L. Zhang, and G. Ke. Uni-mol: A universal 3d molecular representation learning framework. In The Eleventh International Conference on Learning Representations, 2023. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "7 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: In the abstract, we claim that we build a multi-modal molecular-phenomics model and demonstrate improvements over prior works. This is done by taking using a uni-modal pre-trained phenomics model, tackling inactive molecules by undersampling and learning inter-sample similarities. In addition, we take into account concentration in our model training. We demonstrate comprehensive results supporting these claims. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: In the conclusion, we have a limitations subsection discussing future research directions and assumptions in our work. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 15}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: Our work does not contain proofs or theorems. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Our work documents our design decisions in detail and has comprehensive details about the underlying dataset. We document all our hyperparameter choices and model architectural decisions. Our evaluation is performed on a publicly accessible dataset RXRX3, allowing for benchmarking of other methods. To reproduce the pre-trained phenomics model, we base our architecture on the work from [28], for which they have also provided access to a snakker model, namely Phenom-Beta via a web platform hosted on the BioNeMo platform https://www.rxrx.ai/phenom. To reproduce the pre-trained molecular model, we based our architecture on [56], for which the authors provide all the code and data needed to reproduce it. We further note that the molecular model can be replaced by simple molecular fingerprints with only a slight drop in performance. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [No] ", "page_idx": 17}, {"type": "text", "text": "Justification: As part of the submission, we are unable to provide code to reproduce model training due to use of its proprietary nature. The training dataset is also an asset of a private institution, meaning that we are unable to be made publicly accessible. The unseen dataset RXRX3 is, however, open source and can be used by the community to evaluate public phenomics models. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We provide details regarding our hyperparameter choices in the Appendix C. In addition we document the use of scaffold splitting for Unseen Molecules & Images dataset. Unseen Dataset RXRX3 is publicly accessible. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Our reported results are averaged over 3 random seeds used to initialize the model and dictating stochasticity during model training. We report most standard deviations in the main text, and the remaining ones are all present in the Appendix E. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We provide details on compute time for each experiment in Appendix D.2. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The research described does not violate the NeurIPS Code of Ethics. Our experiments do not include human subjects, we follow fair use of data, privacy, and do not release model weights for mitigating impact measures. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Our work discusses the potential in which MolPhenix can have positive societal impact and we touch on the extenralities in our concluding statements. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: In our work we do not release model weights or the underlying code. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Assetts used are referenced and licenses checked or otherwise not released publicly. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing not human subject research. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing not human subject research. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}, {"type": "text", "text": "A Glossary ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Cell morphology: The examination and characterization of cellular structure, including shape, size, and organization, which can provide insights into cellular function and health [40]. ", "page_idx": 22}, {"type": "text", "text": "Cell line (ARPE-19): A specific immortalized human retinal pigment epithelial cell line, widely utilized in ophthalmic research due to its differentiated properties and stability [15]. ", "page_idx": 22}, {"type": "text", "text": "Molecular concentration: The quantitative measure of a specific molecule\u2019s abundance within a defined volume, typically expressed in molar units [42]. ", "page_idx": 22}, {"type": "text", "text": "Cell staining: A laboratory technique involving the application of dyes or fluorescent markers to enhance the visibility and differentiation of cellular components under microscopic examination [9]. ", "page_idx": 22}, {"type": "text", "text": "Molecular perturbations: Induced alterations in cellular molecular systems, often used to study cellular responses and regulatory networks [26]. ", "page_idx": 22}, {"type": "text", "text": "Inactive molecule perturbations: Cellular system alterations caused by molecules lacking significant biological activity. ", "page_idx": 22}, {"type": "text", "text": "Batch effects: Systematic non-biological variation between groups of samples in an experiment, resulting from technical or experimental factors rather than true biological differences. This phenomenon is commonly observed in high-throughput molecular biology experiments, such as microarray studies, mass spectrometry, and single-cell RNA sequencing [24]. ", "page_idx": 22}, {"type": "text", "text": "Molecular fingerprints: Distinctive patterns of molecular features that characterize specific cellular states or responses, often used for comparative analyses and classification [8]. ", "page_idx": 22}, {"type": "text", "text": "B Assumption of the Initial Cell State ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "There is an important distinction between phenomics - molecule and text - image contrastive training although there are initial similarities. In the text - image domain the two modalities are directly generated by the same latent variable which is the underlying semantic class. Whereas in phenomics - molecule, the observed phenomics variable is actually conditioned on molecular structure and the initial state. There are two important conclusions from this: (1) This indicates that if molecular structure has no effect on the initial cell state, there will not be a positive pairing between the molecular structure and morphological patterns captured by phenomics, making it indistinguishable from a control image. (2) There is an underlying assumption that the initial cell state $\\boldsymbol{x}_{i}^{0}$ is constant. In accordance with this assumption we utilize experiments with a fixed cell line, HUVEC-19, and a constant genetic background. Future works can relax this assumption by taking into account phenomics experiments of the cells prior to the perturbation. This can allow the models to generalize beyond a single cell line and to diverse genetic backgrounds. ", "page_idx": 22}, {"type": "text", "text": "C Dataset ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Models have been trained using our in house training set and we have conducted our evaluation on two novel datasets and an open-source molecule dataset [16]: ", "page_idx": 22}, {"type": "text", "text": "\u2022 Training Set: Our training dataset comprises 1,316,283 pairs of molecules and concentration concentration combinations, complemented by fluorescent microscopy images generated through over 2,150,000 phenomic experiments.   \n\u2022 Evaluation set 1 - Unseen Images $^+$ Seen Molecules: The first set consists of unseen images and seen molecules. Unseen microscopy images are associated with 15,058 pairs of molecules and concentrations from the training set and selected randomly.   \n\u2022 Evaluation set 2 - Unseen Images $^+$ Unseen Molecules: The second set includes previously unseen molecules, and images (consisting of 45,771 molecule and concentration pairs). Predicting molecular identities of previously unseen molecular perturbations corresponds to zeroshot prediction. Scaffold splitting was used to split this validation dataset from training ensuring minimal information leakage.   \n\u2022 Evaluation set 3 - Unseen Dataset: Finally, we utilize the RXRX3 dataset [16], an opensource out of distribution (OOD) dataset consisting of 6,549 novel molecule and concentration ", "page_idx": 22}, {"type": "text", "text": "pairs associated with phenomic experiments. The distribution of molecular structures differs from previous datasets, making this a challenging zero-shot prediction task. ", "page_idx": 23}, {"type": "text", "text": "C.1 Concentration Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Additional details regarding the number of molecules at significant concentrations of each evaluation set are available in Table 6. ", "page_idx": 23}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/1ea1b4d8ce0e0d16cc5ff2c22a9dc8614afce07e9a527385fd1a27b7997619c5.jpg", "table_caption": ["Table 6: Separated number of molecules for different concentrations at various pvalue cut-offs "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In our experiments we report the top $1\\%$ recall metric as it is agnostic to the size of the dataset used. Across different datasets, top 1 metric can correspond to varying levels of difficulty due to the number of negatives evaluated. Top $1\\%$ can be used to compare models with different batch sizes, datasets, and evaluations with different number of concentrations. ", "page_idx": 23}, {"type": "text", "text": "D.1 Hyperparameters ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our design choices and utilized hyperparameters for is presented in Table 7. We set batch size to 512 through experiments presented in top section of Table 1 and Figure 4 since training CLOOME model on images is not efficient compared to using pretrained models. In addition, results presented at bottom section of Table 1 are based on the best parameters found through described ablation studies (section E.5). ", "page_idx": 23}, {"type": "text", "text": "Table 7: Hyperparameter values utilized in our proposed MolPhenix training framework for MolGPS ersion. For non-MolGPS version $\\gamma\\,2.75\\,\\zeta$ is 1.0. ", "page_idx": 23}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/1f8f86b9273f388cb1227aa6bfb0f8d5fbbdb511df50f04f610919275a7a3d21.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "D.2 Resource Computation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We utilized an NVIDIA A100 GPU to train Molphenix using Phenom1 and MolGPS embeddings, which takes approximately ${\\sim}4.75$ hours each. For loss comparison experiments, we run each model using 3 different seeds and 8 different losses, resulting in a total of 114 hours of GPU processing time. For concentration experiments we train 7 runs, one for each concentration, with 3 seeds each totaling 21 runs per set of parameters. With 25 sets of parameters evaluated (13), that amounts to 2,500 A100 compute hours. Moreover, we employed 8 NVIDIA A100 GPUs to train CLOOME model on phenomics images, with an average of 40 hour usage per run. Across three seeds, that amounts to $\\sim1000$ hours of A100 GPU usage (8 GPUs for 40 hours 3 times). ", "page_idx": 23}, {"type": "image", "img_path": "LQBlSGeOGm/tmp/4e8d89a4506657c33ed3f6c0622e94961b9e096d60fbba741ea9305a4b823629.jpg", "img_caption": ["Figure 6: Plotted are cumulative densities of distance metrics for cosine similarity and arctangent of l2 distances between embeddings. Random mol corresponds to Phenom1 distances between random molecules, high pval corresponds to distances between molecules with high p-values, low pval corresponds to distances between active molecules with low p-values, finally high-low corresponds to distances between active and inactive molecules. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Note that, without accounting for the time to train Phenom1, MolPhenix is $8.4~\\times$ faster than the CLOOME baseline. ", "page_idx": 24}, {"type": "text", "text": "D.3 S2L Distance function ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To calculate inter sample distances, we utilize arctangent of l2 distances between Phenom1 embeddings. More specifically, we calculate distances with ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\arctan(\\lvert|z_{\\mathbf{x}_{i}}-z_{\\mathbf{x}_{j}}\\rvert|_{2}^{2}/c)*\\frac{4}{\\pi}-1,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $c$ is a constant indicating the median l2 distance between a null set of embeddings. Empirically, we\u2019ve found that setting similarities below a threshold $k$ to 0 improves model performance: $\\lceil w\\rceil^{k}$ . ", "page_idx": 24}, {"type": "text", "text": "Usage of arctan-l2 distances is motivated by an observation that cosine similarities do not effectively separate inactive molecules from other molecular pairs (Figure 6). To alleviate inactive molecule challenge, we require significant separation of CDF curves of inactive perturbations (p value $>.9$ ) and active molecules $({\\mathfrak{p}}<.01)$ . We observe that in both the plots using arctangent and cosine similarities achieves this purpose. However, if we compare high p-value curves with high-low, we find that in the case of cosine similarities they are almost identical. This indicates that the distribution of cosine similarities between active - inactive molecules is almost identical to that of inactive - inactive molecules. In contrast, when using arctangent similarities, we observe that the two CDF curves are well separated. ", "page_idx": 24}, {"type": "text", "text": "This property of l2 distances can inform our model training to identify inactive-inactive molecules. These results informed our decision to utilize arctangent of l2 distances to specify sample similarities for the S2L loss. ", "page_idx": 25}, {"type": "text", "text": "D.4 Intuition for S2L Loss ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section we aim to provide some additional intuition for the S2L loss and further relate it to previous works. We will first assess the conceptual similarities between InfoNCE and CWCL loss and then justify a similar extrapolation for the relationship between S2L and SigLIP losses. ", "page_idx": 25}, {"type": "text", "text": "InfoNCE can be considered a special case of the CWCL loss where $w_{i j}$ is set to 0 for all pairs of $i$ and $j$ unless $i=j$ . Conceptually this is equivalent to stating that all the negative pairs are equally distant from the reference sample. We will consider a uni directional loss CWCL, for identifying $\\mathcal{X}$ from $\\mathcal{M}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{CWCL,:}M\\rightarrow\\chi}=-\\frac{1}{N}\\sum_{i=1}^{N}\\left[\\frac{1}{\\sum_{j=1}^{N}\\mathbf{w}_{i,j}^{\\chi}}\\sum_{j=1}^{N}\\mathbf{w}_{i,j}^{\\chi}\\log\\frac{\\exp\\left(\\langle\\mathbf{z}_{x_{i}},\\mathbf{z}_{m_{j}}\\rangle/\\tau\\right)}{\\sum_{k=1}^{N}\\exp\\left(\\langle\\mathbf{z}_{x_{j}},\\mathbf{z}_{m_{k}}\\rangle/\\tau\\right)}\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "If we set $w_{i j}=0$ when $i\\neq j$ and 1 otherwise then the term $\\Sigma_{j=1}^{N}w_{i,j}^{X}$ evaluates to 1 and the above expression simplifies to: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{InfoNCE}}=-\\frac{1}{N}\\sum_{i=1}^{N}\\left[\\log\\frac{\\exp(\\langle\\mathbf{z}_{x_{i}},\\mathbf{z}_{m_{i}}\\rangle/\\tau)}{\\sum_{k=1}^{N}\\exp(\\langle\\mathbf{z}_{x_{i}},\\mathbf{z}_{m_{k}}\\rangle/\\tau)}\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In the case of CWCL, a non $0\\;w_{i,j}$ determined by a within modality similarity function informed by a pre-trained model, allows for an additional inductive bias. It is especially beneficial in a training setting with a limited dataset-size and in the presence of inactive negative molecules. ", "page_idx": 25}, {"type": "text", "text": "Similarly SigLip can be considered a special case of S2L when $w_{i,j}^{\\mathcal{X}}=0$ when $i\\neq j$ and $w_{i,j}^{\\mathcal{X}}=1$ in the case $i=j$ . This is the formulation of S2L ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{S2L}}=-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\log\\left[\\frac{\\mathbf{w}_{i,j}^{\\chi}}{1+\\exp\\left(-\\alpha\\langle\\mathbf{z}_{\\mathbf{x}_{i}}\\mathbf{z}_{\\mathbf{m}_{j}}\\rangle+b\\right)}+\\frac{(1-\\mathbf{w}_{i,j}^{\\chi})}{1+\\exp\\left(\\alpha\\langle\\mathbf{z}_{\\mathbf{x}_{i}}\\mathbf{z}_{\\mathbf{m}_{j}}\\rangle+b\\right)}\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It can be simplified to SigLIP by setting $w_{i,j}^{\\mathcal{X}}$ to 1 when $\\begin{array}{r l r}{y_{i,j}}&{{}=}&{1}\\end{array}$ thus setting the term $\\frac{(1-\\mathbf{w}_{i,j}^{\\mathcal{X}})}{1+\\exp\\left(\\alpha\\mathbf{z}_{\\mathbf{x}_{i}}\\cdot\\mathbf{z}_{\\mathbf{m}_{j}}+b\\right)}$ , corresponding to $i\\ne j$ , we set $w_{i,j}^{\\mathcal{X}}$ to 0 thus negating the first part of the $\\mathcal{L}_{S2L}$ loss, evaluating to: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{SigLP}}=-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\left[\\log\\frac{1}{1+\\exp\\left(\\mathbf{y}_{i,j}(-\\alpha\\langle\\mathbf{z}_{\\mathbf{x}_{i}},\\mathbf{z}_{\\mathbf{m}_{j}}\\rangle+b)\\right)}\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Having a $0\\leq w_{i,j}^{\\chi}\\leq1$ allows us to inform the training by going between discrete negative labels to continuous informed by some prior information. This information is given by a pre-trained encoder $\\theta_{P h i}$ , in our case but can be informed by any pre-trained model. ", "page_idx": 25}, {"type": "text", "text": "There are a lot of domain specific choices that can be made to inform the choice for setting $w_{i,j}^{\\mathcal{X}}$ which we discuss in the appendix. Briefly, we identify that a modified l2 loss is most effective for identifying inactive molecules. ", "page_idx": 25}, {"type": "text", "text": "E Additional Results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "E.1 Predicting molecular activity ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Given the significance of identifying active molecules, we evaluate the ability of the chemical encoder to predict molecular activity. To do so, we assessed whether embeddings generated from the chemical encoder can be used to predict calculated p-values for unseen molecules. We fti a logistic regression on molecular embeddings from the training set, classifying whether a molecular perturbation and concentration have a p-value below .01. We find that the trained logistic regression is capable of predicting molecular activity on two downstream datasets with a non-overlapping set of molecules, Figure 8. In addition, we provide a u-map of molecular embedding for the unseen dataset RXRX3, colored by p-value. We qualitatively observe a clustering of active molecules using a U-map (Figure 7). It demonstrates that predicting compounds activity is possible using MolPhenix chemical encoder as molecules representations are distinct, independent of the experimented dosage concentration. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "image", "img_path": "LQBlSGeOGm/tmp/da3e432b845ddf2d5d44a314fa1db0c14572f488096b58877b4a7db3de399498.jpg", "img_caption": ["UMAP Mol embeddings RXRX3 colored by p-values ", "Figure 7: U-map demonstrating dimensionality reduction of the chemical embeddings of unseen dataset RXRX3. First two dimensions are visualized and points are colored corresponding to their activity measured in phenomics experiments. Activity is evaluated using p-values calculated using technical replicability of Phenom1 embeddings. Top plot shows the u-map figure of all chemical embeddings, and bottom figure contains u-map figure of representations at specific concentrations. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "LQBlSGeOGm/tmp/35f5964b7fcc077d4ba388941bca797afd63c8421eef7165d8a51f92ccb8028e.jpg", "img_caption": ["Figure 8: Top left: ROC AUC of logistic regression predicting molecular activity on new dataset. Top right: ROC AUC of logistic regression predicting molecular activity on validation dataset with new molecules and new images. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "E.2 Zero Shot Biological Validation ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We conduct a preliminary investigation into whether MolPhenix can be used to identify biological relationships without the need for conducting the underlying experiments. To this end, we evaluate on a subset of ChEMBL with curated pairs of gene knockouts and molecular perturbants [35]. These pairs of perturbations were curated due to the similarity of their effects on cells, although these might not always be captured through phenomic experiments. Thus, there is maximum performance that can be reached through just phenomic data, which we assume to be achieved by experimental data embedded using Phenom1. ", "page_idx": 28}, {"type": "text", "text": "To evaluate MolPhenix\u2019s ability to identify previously known biological associations directly from data, we embed phenomics experiments from gene knockouts using the vision encoder. To perform in-silico screening, we then embed the molecular structures associated with positive pairs using the chemical encoder. Generating molecular embeddings and the corresponding concentrations does not utilize any experimental data. We then calculate cosine similarities between embeddings of phenomics experiments evaluating gene knockouts, and representations of the chemical representations along with encoded concentrations. Using the computed cosine similarities we are then able to assess whether MolPhenix is capable of identifying known associations between gene knockouts and molecular structures. Since there is no information on molecular concentration at which the cells must be treated with, we repeat the experiment across 4 concentrations. To get a null distribution of cosine similarities we take pairs of genes knockouts and molecules for which there are no annotated relationships. We calculate a cut-off for a low and high percentiles, and then evaluate what percentage of pairs of genes and molecules with known relationships exceed the set thresholds. ", "page_idx": 28}, {"type": "text", "text": "Figure 9 demonstrates that in-silico screening using MolPhenix Molecular encoder is capable of recovering a significant portion of known interactions. This is performed without the use of experimental data on the molecular encoder. It is difficult to estimate an upper bound on the expected performance due to uncertainty in the quality of curation of known pairs, presence of unknown associations between genes and molecules, and uncertainty regarding molecular concentration. There is a clear trend however that MolPhenix molecular encoder is capable of recovering a meaningful fraction of these interactions. ", "page_idx": 28}, {"type": "image", "img_path": "LQBlSGeOGm/tmp/7d44df4f6db8d0ae0ada24f022bff25ff14cc8468b9d1f5511cc366ef8484df4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 9: Evaluation of 0-shot ChEMBL identification of gene knockout and molecular phenomic similarities. On the $\\Chi$ axis are percentile ranges, at which points the threshold is computed for cosine similarities. On the y axis is plotted total recall of recovered known interactions. Grey $x$ plotted for each range indicate baseline recall. Orange line indicates MolPhenix-Molecular encoding of chemical compounds and MolPhenix-Vision for encoding gene knockout phenomics experiment. Blue line indicates Phenom1 encoding of phenomics experiments for both the molecular perturbation and gene knockouts. In-silico encoding of molecular perturbation, as well as the corresponding concentration, recovers a significant fraction of observed interactions. ", "page_idx": 29}, {"type": "text", "text": "E.3 Molecular Property Prediction ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We expand our evaluation with additional experiments supporting the utility of MolPhenix beyond retrieval. We conduct a KNN evaluation of the MolPhenix latent space, assessing the learned embedding on 35 molecular property prediction tasks across the Polaris and TDC datasets (Table 8 and 9). We find that MolPhenix trained with fingerprint embeddings consistently outperforms standalone input fingerprints, demonstrating that the MolPhenix latent space effectively clusters molecules according to their biological properties. We observed an interesting effect where prediction quality is positively correlated with implied dosage, indicating that MolPhenix learns dosage-specific effects. In addition, utilizing ", "page_idx": 29}, {"type": "text", "text": "Table 8: Comparison of a KNN applied on MolPhenix molecular embedding with traditional fingerprints on different tasks of TDC and Polaris datasets. Mean results for TDC, Polaris and together are available in the last three columns. Binary fingerprints use tanimoto similarity, while floating-point fingerprints use cosine similarity. ", "page_idx": 29}, {"type": "image", "img_path": "LQBlSGeOGm/tmp/70c15c6026ee5b2f28519fb6d2722905279cd798a469d68c61ef997ae3d73583.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Table 10: Evaluation on cumulative concentrations for active molecules: Average Top- $\\cdot1\\%$ and Top- ${}\\cdot5\\%$ recall accuracies of methods utilizing different contrastive learning loss functions and concentration encoding information. We evaluate all methods on unseen images, unseen images and unseen molecules and an unseen dataset for zero-shot retrieval. Entries in bold denote best performance when the loss function is fixed while entries in highlight denote best performance across all guidelines. ", "page_idx": 30}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/68e3b9c573e05d0d2d3f472f087f192b959983d7b0e3a1d50e40679ba51cf1d9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 9: Comparison of a KNN applied on MolPhenix molecular embedding with MolGPS on different tasks of TDC and Polaris datasets. Mean results for TDC, Polaris and together are available in the last three columns. ", "page_idx": 30}, {"type": "image", "img_path": "LQBlSGeOGm/tmp/8fcf527328962c82db919ca87908dd4480057bd4014e2ff72fbd1c6d154f96c5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "E.4 Addressing Challenges in Contrastive Phenomic Retrieval ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Table 10 and 12 show the complete Top $1\\%$ and $5\\%$ results of evaluation on cumulative concentrations on only active and all molecules, respectively. Similarly, Table 11 and 13 presents the full retrieval results of held-out concentrations experiments. In comparison to prior loss functions, S2L loss objective demonstrates consistent high retrieval rate in all tasks and molecular groups (i.e. all or active molecules), while using the same modality (Phenom1) and with or without explicit concentration information. ", "page_idx": 30}, {"type": "text", "text": "E.5 Ablation Studies ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Figure 10 and Table 15, 16, 17, 18 and 19 present top- $\\cdot1\\%$ recall accuracy across for the full ablation study on the variation of MolPhenix key components. We note that compact embedding sizes from pretrained models stabilize training. This indicates that embeddings are expressive and accurately capture intricate aspects of molecules. Larger batch sizes result in a greater number of negative samples, hence improving performance. This is in line with prior contrastive learning methods continuing to improve by increasing the batch size [12]. Increasing the number of parameters leads to more expressive models thereby enhancing retrieval performance. This result is in accordance with ", "page_idx": 30}, {"type": "text", "text": "Table 11: Evaluation on held-out concentration for active molecules: Average Top- $1\\%$ and Top- ${}.5\\%$ recall accuracies of methods utilizing different contrastive learning loss functions and concentration encoding information. We evaluate all methods on unseen images, unseen images and unseen molecules and an unseen dataset for zero-shot retrieval. Entries in bold denote highest performance when the loss function is fixed while entries in highlight denote highest performance across all guidelines. ", "page_idx": 31}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/920e20e33d21775683ae3ffd933741871bdeb7e55aba4c7aeb4a18204cfa1b7e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/9ac86afcb673ec06af6a545da9a02f6d9c7041a408abe07ba0cfb0e7328484ce.jpg", "table_caption": ["Table 12: Evaluation on cumulative concentrations for active and inactive perturbations Average Top- $\\cdot1\\%$ and Top- $.5\\%$ Recall accuracy of methods utilizing different contrastive learning methods. Best performing methods are highlighted in bold. "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/dc3749093b5eb4842009a273478e0642371a3c6eeb92379a34b1ab0d3e83a6fd.jpg", "table_caption": ["Table 13: Evaluation on held-out concentrations for active and inactive perturbations Average Top- $\\cdot1\\%$ and Top- ${.5\\%}$ Recall accuracy of methods utilizing different contrastive learning methods. Best performing methods are highlighted in bold. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/bafaf07cffb3764ee401748afbefaaf5344b9a13730363168596b9cc6ed92135.jpg", "table_caption": ["recent advances in language modelling and scaling laws across different data and compute budgets [25]. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/bc7979b9686a9fefacac0519417cfd5f16833db068a1167a61b0dc6f6a389dc4.jpg", "table_caption": ["Table 14: Ablations across different model sizes. Larger capacity models are found to be more expressive. ", "Table 15: Ablation across different batch sizes. Larger batch sizes benefit contrastive learning. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "E.6 Investigating Other Pre-trained Phenomic Encoders ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "To investigate the impact of pre-trained encoders, we perform additional experiments evaluating a supervised phenomic image encoder (Table 20). Instead of Phenom1, we trained Molphenix framework using AdaBN, a CNN-based supervised phenomic encoder, with an analogous implementation discussed in [55]. We find that the general trends between Phenom1 and AdaBN are consistent with a slight decrease in overall performance. These findings provide additional support to the generality of the proposed guidelines. ", "page_idx": 32}, {"type": "text", "text": "E.7 Integrating MolGPS Embeddings With Other Fingerprints ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Molphenix architecture is flexible, allowing that the proposed components be replaced by other phenomic or molecular pretrained models. We leveraged from MolGPS, which is a MPNN based ", "page_idx": 32}, {"type": "image", "img_path": "LQBlSGeOGm/tmp/8aded8e3e7db469be25e41356693b9778cf51d1898bbad44a35d30f9f495bd27.jpg", "img_caption": ["Figure 10: Ablations of top- $1\\ \\%$ recall accuracy with (top-left) the size of embedding dimension, (top-center) number of parameters, (top-right) batch size, (bottom-left) cutoff $p$ value, (bottomcenter) fingerprint type, and (bottom-right) random batch averaging. Compact embedding sizes from pretrained models, larger number of parameters, larger batch sizes, lower cutoff ${\\bf p}$ -values, pretrained MolGPS fingerprints and presence of random batch averagin improving retrieval of our MolPhenix framework. ", "Table 16: Ablation across different embedding dimensions. Compact embedding sizes capture more molecular information. "], "img_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/cde6226323dc47535fa8afa0e0d52bf972ea1ae4648580ebc897025f9bc9a0aa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "GNN model with 1B parameters which allows us to maximize architecture expressivity while minimizing the risk of overfitting [34, 56]. For additional investigation, we combine MolGPS molecular embeddings with RDKIT, MACCS, and Morgan fingerprints and show that they can provide Molphenix with richer molecular information and yields overall higher performance of MolPhenix in both cumulative and held-out concentration scenarios. Results for active and all molecules retrieval of Molphenix trained on the discussed combinational molecular embeddings are available in table 21 and 22. ", "page_idx": 33}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/28cc7f49a912dae00b703e828e9add1a587e79bd0c016bbf05f0862467cc64e2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/966ee9c213be91de14e2d4b5282ba284378526ab848716d304aeb5ab46572b56.jpg", "table_caption": ["Table 17: Ablation across different p-value cutoff threhsolds. p values < .1 benefti retrieval of active molecules. "], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "Table 18: Ablation across different fingerprint types. A combination of embeddings bootstrapped from Phenom1 and MolGPS significantly benefit retrieval. ", "page_idx": 34}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/a563d5050761374685b50be406cb94db9e30e866bfd38be28b70e3815bc9bed2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "Table 19: Ablation across random embedding averaging. Utilizing random batch averaging stabilizes training and benefits retrieval. ", "page_idx": 34}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/e7eac5579e9c7f5f8a89b1a4b5015b95fb7aaddd9dd74e5e0229fe68a18104eb.jpg", "table_caption": ["Table 20: Evaluation on cumulative concentrations while using AdaBN. Molphenix is trained on combination of RDKIT, MACCS, and Morgan fingerprints in this experiment "], "table_footnote": [], "page_idx": 34}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/7a2e8891907e6a4e8c803a572b3048956a3a915a862a0ce332f91b1fbd007a68.jpg", "table_caption": ["Table 21: Evaluation on cumulative concentrations while combining MolGPS, RDKIT, MACCS, and Morgan fingerprints. "], "table_footnote": [], "page_idx": 34}, {"type": "table", "img_path": "LQBlSGeOGm/tmp/f6f2d92c657deed1c149535f0167b92cf874eddbcb87670b0ac6c3075dbcf66c.jpg", "table_caption": ["Table 22: Evaluation on heldout concentrations while combining MolGPS, RDKIT, MACCS, and Morgan fingerprints. "], "table_footnote": [], "page_idx": 34}]