{"references": [{"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces a foundational multi-modal model, CLIP, which the authors adapt for their phenomolecular retrieval task, demonstrating the effectiveness of contrastive learning in aligning different modalities."}, {"fullname_first_author": "K. He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-06-01", "reason": "This paper introduces the Masked Autoencoder (MAE) framework, which the authors use for pretraining their phenomics model, showcasing the benefits of self-supervised learning in handling limited paired data."}, {"fullname_first_author": "O. Kraus", "paper_title": "Masked autoencoders for microscopy are scalable learners of cellular biology", "publication_date": "2024-06-01", "reason": "This paper introduces Phenoml, a large-scale pretrained model that is leveraged by the authors for phenomics embedding and forms a key component of their proposed method."}, {"fullname_first_author": "M. Sypetkowski", "paper_title": "Rxrx1: A dataset for evaluating experimental batch correction methods", "publication_date": "2023-06-01", "reason": "This paper introduces a dataset crucial for benchmarking the authors' method and evaluating its performance in handling batch effects."}, {"fullname_first_author": "A. Sanchez-Fernandez", "paper_title": "CLOOME: contrastive learning unlocks bioimaging databases for queries with chemical structures", "publication_date": "2023-07-01", "reason": "This paper proposes a prior state-of-the-art method for contrastive phenomolecular retrieval, which the authors improve upon in their work."}]}