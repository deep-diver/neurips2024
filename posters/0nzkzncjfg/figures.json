[{"figure_path": "0nzKznCjFG/figures/figures_7_1.jpg", "caption": "Figure 1: Performance before finetuning: We compare the test NLL loss before local personalization (finetuning) for baselines FedAvg, MAML, IFCA with our approach. NLL is uniformly averaged over users and each value is averaged over 50 random runs (error bars indicate 95% confidence intervals).", "description": "This figure compares the test negative log-likelihood (NLL) loss before local personalization across four methods: FedAvg, MAML, IFCA, and the authors' proposed approach.  The NLL, a measure of the model's prediction accuracy, is averaged across all users and multiple runs, showing the performance of each method on three datasets: Reddit, StackOverflow, and Amazon Reviews. Error bars illustrate the 95% confidence intervals.", "section": "Empirical evaluation"}, {"figure_path": "0nzKznCjFG/figures/figures_7_2.jpg", "caption": "Figure 2: Performance after finetuning: In the size-homogeneous (a-c), and size-heterogeneous (d-f) settings, we compare the test NLL loss for baselines FedAvg+FT, MAML+FT, IFCA+FT with our Alg. 3+FT, where FT is implemented by Eq. 3. Uniformly averaged over users, each value is averaged over 50 random runs (error bars indicate 95% confidence intervals).", "description": "This figure compares the test negative log-likelihood (NLL) loss after local finetuning for several baselines and the proposed algorithm in both size-homogeneous and size-heterogeneous settings.  The baselines include FedAvg+FT, MAML+FT, and IFCA+FT, while the proposed algorithm is Alg. 3+FT.  The finetuning step is implemented using Equation 3 from the paper.  Results are averaged across users and multiple runs, with error bars representing 95% confidence intervals.", "section": "Empirical evaluation"}, {"figure_path": "0nzKznCjFG/figures/figures_8_1.jpg", "caption": "Figure 3: Algorithmic design choices: We evaluate test NLL for Alg. 3 as we: (a) vary the number of clusters K; (b) use PrivateInit or randomly initialize cluster centers; and (c) use average of Good-Turing or empirical average to estimate cluster centers. In (d) we evaluate different finetuning methods applied to the FedAvg model.", "description": "This figure analyzes the impact of different algorithmic choices on the performance of the proposed personalized frequency estimation algorithm.  It shows the test negative log-likelihood (NLL) for four scenarios: varying the number of clusters (K), using the proposed PrivateInit vs. random initialization for cluster centers, comparing the use of Good-Turing vs. empirical averages for cluster center estimation, and comparing different local finetuning methods applied to the FedAvg model. Each subfigure helps assess the importance of a specific design choice, offering insights into the algorithm's effectiveness and efficiency.", "section": "Algorithmic design choices"}, {"figure_path": "0nzKznCjFG/figures/figures_9_1.jpg", "caption": "Figure 2: Performance after finetuning: In the size-homogeneous (a-c), and size-heterogenenous (d-f) settings, we compare the test NLL loss for baselines FedAvg+FT, MAML+FT, IFCA+FT with our Alg. 3+FT, where FT is implemented by Eq. 3. Uniformly averaged over users, each value is averaged over 50 random runs (error bars indicate 95% confidence intervals).", "description": "This figure compares the test negative log-likelihood (NLL) loss for several baselines (FedAvg+FT, MAML+FT, IFCA+FT) and the proposed algorithm (Alg. 3+FT) after a local finetuning step (FT).  The comparison is done in both size-homogeneous and size-heterogeneous settings across three datasets: Reddit, StackOverflow, and Amazon Reviews.  The results show the improvements achieved by the proposed method over the baselines in reducing the error, with error bars representing 95% confidence intervals.", "section": "Empirical evaluation"}, {"figure_path": "0nzKznCjFG/figures/figures_16_1.jpg", "caption": "Figure 5: Validating hyperparameter choice of K = 10 on Reddit dataset. We use the same value of K for the other two datasets as well.", "description": "This figure shows the result of validating the hyperparameter K (number of clusters) for the proposed algorithm. The x-axis represents different values of K, and the y-axis represents the test negative log-likelihood.  The plot shows that the test negative log-likelihood is minimized when K is around 10 for the Reddit dataset. This suggests that a value of K=10 is a good choice for the algorithm, and this value was used for the other two datasets as well.", "section": "5 Empirical evaluation"}]