[{"type": "text", "text": "Attention Interpolation for Text-to-Image Diffusion ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qiyuan He1 Jinghao Wang2 Ziwei Liu2 Angela Yao1, 1National University of Singapore 2S-Lab, Nanyang Technological University qhe@u.nus.edu.sg ayao@comp.nus.edu.sg {jinghao003, ziwei.liu}@ntu.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Conditional diffusion models can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or image is less understood. Common approaches interpolate linearly in the conditioning space but tend to result in inconsistent images with poor fidelity. This work introduces a novel trainingfree technique named Attention Interpolation via Diffusion (AID). AID has two key contributions: 1) a fused inner/outer interpolated attention layer to boost image consistency and fidelity; and 2) selection of interpolation coefficients via a beta distribution to increase smoothness. Additionally, we present an AID variant called Prompt-guided Attention Interpolation via Diffusion (PAID), which 3) treats interpolation as a condition-dependent generative process. Experiments demonstrate that our method achieves greater consistency, smoothness, and efficiency in condition-based interpolation, aligning closely with human preferences. Furthermore, PAID offers substantial benefits for compositional generation, controlled image editing, image morphing and image-controlled generation, all while remaining training-free. Our code and demo are available at https://qyh00.github.io/attention-interpolation-diffusion/. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Interpolation is a common operation applied to generative image models. It generates smoothly transitioning sequences of images from one seed to another within the latent space and facilitates applications in image attribute modification [40], data augmentation [38], and videos [46]. Interpolation has been investigated extensively [18, 43, 42] in VAEs [20], GANs [8], and diffusion models [13]. Text-to-image diffusion models [35, 37] are a new class of conditional generative models that generate high-quality images conditioned on textual descriptions. How to interpolate between distinct text conditions such as \u201ca truck\" and \u201ca cat\" (see Fig. 1 (d)) is relatively under-explored. This issue is, however, crucial for various downstream tasks, such as conditional generation with multiple conditions [6, 25, 51] or cross-modality conditions [54, 56], as well as for image editing [11, 48], where precise control over impact of different conditions is essential to achieve desired results. ", "page_idx": 0}, {"type": "text", "text": "This paper formulates the task of conditional interpolation and identifies three ideal properties for interpolating text-to-image diffusion models: thematic consistency, smooth visual transitions between adjacent images, and high-quality interpolated images. For instance, interpolating from \u201ca truck\u201d to \u201ca cat\u201d should avoid irrelevant transitions (e.g., via \u201ca bowl\u201d). The sequence should change between the two conditions gradually and feature high-quality and high-fidelity images (vs. e.g. simple overlays of the truck and cat). These properties directly motivate our quantitative evaluation metrics for conditional interpolation: consistency, smoothness, and fidelity. ", "page_idx": 0}, {"type": "text", "text": "A direct approach to traverse the conditioning space is interpolating in the text embedding itself [53, 55, 16]. Such an approach often has sub-optimal results (see the first row of Fig. 2). A closer analysis reveals that interpolating the text embedding is mathematically equivalent to interpolating the keys and values of the cross-attention module between the text and image space. Our analysis further $38\\mathrm{th}$ Conference on Neural Information Processing Systems (NeurIPS 2024). ", "page_idx": 0}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/eeabc52c25373f8460ff34e53a2f0129f756bce963b8391a45a3c6bf77695511.jpg", "img_caption": ["Figure 1: Our approach enables text-to-image diffusion models to generate nuanced spatial and conceptual interpolations between different conditions including text (a, c-e) and image (b), with seamless transitions in layout, conceptual blending, and user-specified prompts to guide the interpolation paths (f). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "reveals that the keys and values in self-attention impose a stronger influence than cross-attention, which may explain why text embedding interpolation fails to produce consistent results. ", "page_idx": 1}, {"type": "text", "text": "Based on our analysis, we introduce a novel framework: Attention Interpolation of Diffusion (AID) models for conditional interpolation. AID enhances interpolation quality with (1) a fused interpolated attention mechanism on both cross-attention and self-attention layers to improve consistency and fidelity and (2) a Beta-distribution-based sample selection along the interpolation path for interpolation smoothness. Additionally, we introduce (3) Prompt-guided Attention Interpolation of Diffusion (PAID) models to further guide the interpolation via a text description of the path itself. ", "page_idx": 1}, {"type": "text", "text": "Experiments on various state-of-the-art diffusion models [30, 35, 2] highlight our approach\u2019s effectiveness (see samples in Fig. 1 and more in Appx. H) without any additional training. ", "page_idx": 1}, {"type": "text", "text": "Human evaluators predominantly prefer our method over standard text embedding interpolation. We further show that our method can benefit various downstream tasks, such as composition generation, and boost the control ability of image editing. Our method is also compatible with image condition (see Fig. 1 (b)), which can be further used for more application such as image morphing and image-controlled generation. This underscores the practical impact of the problem of conditional interpolation and our proposed solution. Our main contributions are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Problem formulation for conditional interpolation within the text-to-image diffusion model context and proposing evaluation metrics for consistency, smoothness, and fidelity;   \n\u2022 A novel and effective training-free method AID for text-to-image interpolation. AID can be augmented with prompt-guided interpolation (PAID) to control specific paths between two conditions;   \n\u2022 Extensive experiments highlight AID\u2019s improvements for text-based image interpolation. AID substantially improves interpolation sequences, with significant enhancements in fidelity, consistency, and smoothness without any training. Human studies show a strong preference for the AID; ", "page_idx": 1}, {"type": "text", "text": "\u2022 We show that AID offers much better control ability for diffusion-based image editing, and it can be used for compositional generation with state-of-the-art performance. It is also compatible with image condition, enabling more applications such as image morphing or controlling the scale of additional image prompt. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion Models and Attention Manipulation. The emergence of diffusion models has significantly transformed the text-to-image synthesis domain, with higher quality and better alignment with textual descriptions [35, 37, 33]. Attention manipulation techniques have been instrumental in unlocking the potential of diffusion models, particularly in applications such as in-painting and compositional object generation. These applications benefit from refined control over the attention maps, aligning the modifier and the target object more closely to enhance image coherence [11, 1, 3, 51, 34]. Furthermore, cross-frame attention mechanisms have shown promise in augmenting visual consistency within video generation frameworks utilizing diffusion models [17, 31]. These works suggest that the visual closeness of two generated images may be reflected in the similarity of their attention maps and motivates us to study interpolation from an attention perspective. ", "page_idx": 2}, {"type": "text", "text": "Interpolation in Image Generative Models. Interpolation within the latent spaces of models such as GANs [8] and VAEs [20] has been studied extensively [43, 18, 46]. More recently, explorations of diffusion model latent spaces allow realistic interpolations between real-world images [38, 21]. Works to date, however, are limited to a single condition, and there is a lack of research focused on interpolation under varying conditions. Wang & Golland explored linear interpolation within text embedding to interpolate real-world images; however, this approach yields image sequence with diminished fidelity and smoothness. This gap underscores the need for further exploration of conditional interpolation in generative models. ", "page_idx": 2}, {"type": "text", "text": "3 Analysis of Conditional Interpolation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Text-to-Image Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Text-to-image diffusion models such as Stable Diffusion [35, 30] generate images from specified text. Consider the generation of an image for some specified text as an inference process denoted by $f(z^{T},c)$ . The function $f$ is an abstraction representing the denoising diffusion process, $c$ is a representation of the conditioning text and $z^{T}$ is a randomly sampled latent seed. Usually, $c$ is represented as a CLIP text embedding [32], while the $z$ \u2019s over the denoising time steps of the generation are sampled from a Gaussian distribution. More specifically, if the inference is carried out over $T$ denoising time steps, the latent $z^{t-1}$ can be sampled conditionally, based on $z^{t}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\theta}(z^{t-1}|z^{t})=\\textstyle\\mathcal{N}(z^{t-1};\\mu_{\\theta}(z^{t},c,t),\\Sigma_{\\theta}(z^{t},c,t)),\\quad\\mathrm{where~}z^{t}\\sim\\textstyle\\cal{N}(0,1),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $t$ represents the denoising time-step index, $\\mu_{\\theta}(z^{t},c,t)$ is estimated by a UNet [36], and $\\textstyle\\sum_{\\theta}(z^{t},c,t){\\bar{)}}$ is determined by a noise scheduler [13, 42]. After iterative sampling from $z^{T}$ to $z^{0}$ , the image is generated by decoder $D$ , as $D(z^{0})$ . ", "page_idx": 2}, {"type": "text", "text": "Attention is used in text-to-image diffusion models [35, 29, 37] in various forms. Cross-attention is commonly used as the link from the text condition to the image generation. Specifically, given a latent variable $z\\in\\mathbb{R}^{d_{z}}$ , text condition $c\\in\\mathbb{R}^{d_{c}}$ and the attention layer with matrices $W_{Q}\\in\\bar{\\mathbb{R}^{d_{z}\\times d_{q}}}$ , $W_{K}\\in\\mathbb{R}^{d_{c}\\times d_{k}}$ and $W_{V}\\in\\mathbb{R}^{d_{c}\\times d_{v}}$ , the cross-attention is computed as ", "page_idx": 2}, {"type": "equation", "text": "$$\nA(z,c)=\\operatorname{Attn}(Q,K,V)=\\operatorname{softmax}({\\frac{Q K^{\\top}}{\\sqrt{d_{k}}}})V,\\quad{\\mathrm{where~}}Q=W_{Q}^{\\top}z,\\ K=W_{K}^{\\top}c,\\ V=W_{V}^{\\top}c.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Self-attention is also commonly used in state-of-the-art text-to-diffusion models [30, 35, 2]. Selfattention is a special case of cross-attention and can also be computed with Eq. 2 as $A(z,z)$ . In this case, the key and values are defined as $K=W_{K}^{\\intercal}z,\\ V=W_{V}^{\\dagger}z$ respectively. For brevity, we abuse the notation to directly represent multi-head attention [47] and denote the attention layer as $\\mathsf{A t t n}(Q,K,V)$ in both cross-attention and self-attention scenarios. ", "page_idx": 2}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/875c53a8a4c50ce509e946414bcd608c3fd4a2699fef286788656385eeca305d.jpg", "img_caption": ["Figure 2: Results comparison between AID (the $1^{\\mathrm{st}}\\,\\mathbf{row}_{.}$ ) and text embedding interpolation (the $2^{\\mathbf{nd}}$ row). AID increases smoothness, consistency, and fidelity significantly. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Text Embedding Interpolation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this paper, we denote linear and spherical interpolation [49, 21] as $r_{l}(w;A,B)$ and $r_{s}(w;A,B)$ respectively, where $w\\in[0,1]$ is the interpolation coefficient, and $(A,B)$ are the interpolation anchors or end-points. Conditional interpolation differs from standard text-to-image generation in that there are two text conditions $c_{1}$ and $c_{m}$ . Each condition has its own respective latent seeds $z_{1}$ and ${z_{m}}^{1}$ . The objective of conditional interpolation is to generate a sequence of images $\\{I_{1:m}\\}=\\{I_{1},I_{2},...,I_{m}\\}$ . In this sequence, the source images are generated by the standard text-to-image generation, i.e, $I_{1}=f(z_{1}\\overline{{,}}\\,c_{1})$ , $I_{m}=f(z_{m},c_{m})$ as described in Sec. 3.1. ", "page_idx": 3}, {"type": "text", "text": "Existing literature has shown that similarity in input space, including the latent seed and the embedding of the condition reflects the similarity in the output pixel space [19, 49]. Directly interpolating the text embedding $c$ is therefore a straightforward approach that can be used to generate an interpolated image sequence [49, 16]. In text embedding interpolation, the text conditions $\\{c_{1},c_{m}\\}$ and their latent seeds $\\{z_{1},z_{m}\\}$ are used as endpoints for interpolation and images are generated accordingly: ", "page_idx": 3}, {"type": "equation", "text": "$$\n:z_{i}\\!=\\!r_{s}(w_{i};z_{1},z_{m}),\\,\\,c_{i}\\!=\\!r_{l}(w_{i};c_{1},c_{m}),\\,\\,\\mathrm{and}\\,\\,w_{i}\\!=\\!\\frac{i\\!-\\!1}{m\\!-\\!1}\\,\\,\\,\\mathrm{for}\\,i=\\{1\\ldots m\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that spherical interpolation is applied to estimate the latent seed $z_{i}$ to ensure that Gaussian noise properties are preserved [38]. In contrast, linear interpolation is applied to estimate the text condition [53, 49, 16, 24] $c_{i}$ . For both $z_{i}$ and $c_{i}$ , the interpolation coefficient $w_{i}$ sampled in uniform increments from 1 to $m$ . ", "page_idx": 3}, {"type": "text", "text": "Given that the text condition $c$ directly propagates into the key and value $K$ and $V$ (Eq. 2), interpolating between the text conditions $c_{1}$ and $c_{m}$ is equivalent to interpolating the associated keys and values in cross-attention. This is stated formally in the following proposition: ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Given query $Q$ from a latent variable $z$ , keys and values $\\{K_{1},V_{1}\\}$ and $\\{K_{m},V_{m}\\}$ from text conditions $\\{c_{1},c_{m}\\}$ and linearly interpolated text conditions $c_{i}$ , the resulting crossattention module $A(z,c_{i})$ is given by linearly interpolated keys and values $\\bar{K}_{i}$ and ${\\bar{V}}_{i}$ : ", "page_idx": 3}, {"type": "equation", "text": "$\\bar{V}_{i}=r_{l}(w_{i};V_{1},V_{m}),$ ", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $w_{i}$ is defined similarly as Eq. 3. ", "page_idx": 3}, {"type": "text", "text": "The proof for Proposition 1 is given in the Appx. A. This proposition gives insight into how text embedding interpolation can be viewed as manipulating the keys and values. Specifically, it is equivalent to interpolating the keys and values to generate the resulting interpolated image. It is worth noting that an analogous interpretation does not carry through for the query $Q$ even though it also depends on some interpolated latent seed $z^{t}$ . This is because $\\bar{z}_{i}^{t}$ is estimated as an interpolation between the latent seeds $z_{1}^{\\dot{t}}$ and $z_{m}^{t}$ , while the latent $z_{i}^{t}$ itself is progressively altered through the denoising process (see Eq. 1). ", "page_idx": 3}, {"type": "text", "text": "3.3 Measuring the Quality of Conditional Interpolation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Text embedding interpolations work well when the conditions are semantically related, e.g., \u201ca dog\u201d and \u201ca cat\u201d, but may lead to failures in less related cases. To better analyze the characteristics of the interpolated image sequences, we define three measures based on ideal interpolation qualities: consistency, smoothness, and image fidelity. ", "page_idx": 3}, {"type": "text", "text": "Perceptual Consistency. Ideally, the interpolated image sequence should transition from one source or endpoint to the other in a perceptually direct and therefore consistent path. Similar to [15], we use the average LPIPS metric [57] across all adjacent image pairs in the sequence to evaluate consistency. If $P$ denotes the LPIPS model, the consistency $C$ of a sequence $I_{1:m}$ is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nC(I_{1:m};P)=\\frac{1}{m-1}\\sum_{i=1}^{m-1}P(I_{i},I_{i+1}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For example, a consistent interpolation from \u201can apple\u201d to \u201ca bed\u201d may pass through \u201can apple and a bed\u201d but should not have intermediate stages like \u201ca messy sketch\u201d (see Fig. 2 (a)). ", "page_idx": 4}, {"type": "text", "text": "Perceptual Smoothness. A well-interpolated sequence should exhibit a gradual and smooth transition. We propose to apply Gini coefficients on the perceptual distance between each neighbouring pair of interpolated images to indicate smoothness. Gini coefficients [5] are a conventional indicator of data imbalance [45, 9, 10, 50] where higher coefficients indicate more imbalance. And the imbalance of perceptual distance of each neighbouring pair indicates low smoothness. Let $G(X)$ denote the Gini coefficient of a set $X\\,=\\,\\{x_{1},\\dot{x}_{2},...,x_{n}^{\\,\\bar{}}\\}$ . The smoothness $S$ of a sequence $I_{1:m}$ with $P$ denoting the LPIPS model is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nS(I_{1:m};P)=1-G(\\bigcup_{i=1}^{m-1}P(I_{i},I_{i+1})),\\quad G(X)=\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}|x_{i}-x_{j}|}{2n\\sum_{i=1}^{n}x_{i}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Fig. 2(b) shows how a smooth interpolation sequence exhibits a gradual transition on the visual content (from $^{\\ast\\epsilon}a$ lady wearing oxygen mask\u201d to \u201clion\u201d in the top row) instead of one source image or end-point dominating the sequence (the \u201clion\u201d in the bottom row). ", "page_idx": 4}, {"type": "text", "text": "Fidelity. Finally, any interpolated images should be of the same (high) quality conventionally generated images. Following [38, 49], we evaluate the fidelity of interpolated images with the Fr\u00e9chet Inception Distance (FID) [12]. Given $n$ interpolated sequences $\\{I_{1:m}^{(1)},I_{1:m}^{(2)},...,I_{1:m}^{(n)}\\}$ , the fidelity $F$ of the sequences is defined as the FID based on a visual inception model 2 The FID between the source images and the interpolated images is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nF(I_{1:m}^{(1)},I_{1:m}^{(2)},...,I_{1:m}^{(n)})=F I D_{M_{V}}\\left(\\bigcup_{j=1}^{n}\\{I_{1}^{(j)},I_{m}^{(j)}\\},\\bigcup_{j=1}^{n}\\{I_{i}^{(j)}|i\\not=1,i\\not=m\\}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For example, the interpolated sequence should have minimal artifacts (see Fig. 2(a)), where the top row clearly shows the appearance of the apple, whereas the bottom row does not. ", "page_idx": 4}, {"type": "text", "text": "3.4 Diagnosing Text Embedding Interpolation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Experimentally (see Sec. 5.1), we observe that text embedding interpolation sequences exhibit poor consistency and smoothness. The interpolated images are also commonly low in fidelity, with indirect and non-smooth transitions. Where do the failures of text embedding interpolation come from? We analyze the outputs from the perspective of spatial layouts and the selection of interpolation coefficients. ", "page_idx": 4}, {"type": "text", "text": "Spatial Layouts and Attention. Consistency is directly affected by the difference between the spatial layout of the source and interpolated images. One observation is that the spatial layout of interpolated images from text embedding interpolations is quite different from the source endpoints (see Fig. 2 (a) bottom row). Proposition 1 links the text embedding interpolation to the crossattention mechanism exclusively. However, the literature suggests that the spatial layout of the overall image is strongly linked to the self-attention mechanism [17, 31]. As such, we hypothesize that cross-attention does not pose enough spatial layout constraints stand-alone. Instead, there is a need for a stronger link of the interpolation to self-attention, to allow more consistent spatial transitions. ", "page_idx": 4}, {"type": "text", "text": "As a simple test, we swap the keys and values from two text-to-image generations. Consider two images $I$ and $I^{\\prime}$ generated from two text prompts $p$ and $p^{\\prime}$ . We replace the keys and values from either the cross-attention or self-attention layers in the generative process of $I$ with that of $I^{\\prime}$ to generate $I_{\\mathrm{cross}}$ and $I_{\\mathrm{self}}$ respectively. We then evaluate the mean squared error (MSE) of the lowfrequency components between $I^{\\prime}$ and $I_{\\mathrm{cross}}$ or $I_{\\mathrm{self}}$ . The results show that $I_{\\mathrm{self}}$ closely resembles $I^{\\prime}$ , while $I_{\\mathrm{cross}}$ does not. More details and results are shown in Appx. B. ", "page_idx": 4}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/a34e84716e90239762833b53aeb2eb7965bde6835357c6bc6353c87b0c2567c8.jpg", "img_caption": ["Figure 3: An overview of PAID: Prompt-guided Attention Interpolation of Diffusion. The main components include: (1) Replacing both cross-attention and self-attention when generating interpolated image by fused interpolated attention; (2) Selecting interpolation coefficients with Beta prior; (3) Inject prompt guidance in the fused interpolated cross-attention. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Selection of Interpolation Coefficients. Interpolation methods [52, 12, 38] commonly select uniformly spaced coefficients $w_{i}$ on the interpolation path. Yet an observation from Fig. 2 (b) shows that uniformly spaced points in the text-embedding space do not lead to uniformly spaced images with smooth transitions. Small visual transitions may occur over a large range of interpolation coefficients and vice versa, which we can show quantitatively by comparing the perceptual distances between adjacent pairs of uniformly spaced coefficients. This suggests that we should adopt nonuniform selection to ensure smoothness. More details and results are shown in the Appx. B. ", "page_idx": 5}, {"type": "text", "text": "4 AID: Attention Interpolation of Text-to-Image Diffusion ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The diagnosis in Sec. 3.4 directly leads us to make the following proposals for improving conditional interpolation. First, we are motivated to extend attention interpolation beyond cross-attention to selfattention as well (Sec. 4.1) and propose fused attention. Secondly, our diagnosis of the smoothness motivates us to adopt a non-uniform selection of interpolation coefficients to encourage more even transitions (Sec. 4.2). Combining these two techniques, we propose a AID: Attention Interpolation of text-to-image Diffusion. ", "page_idx": 5}, {"type": "text", "text": "Finally, in an effort to give more precise control over the interpolation path, we introduce the use of prompt guidance for interpolation (Sec. 4.3). This further enhances AID as Prompt-guidance AID (PAID). The full pipeline is shown in Fig. 3. ", "page_idx": 5}, {"type": "text", "text": "4.1 Fused Interpolated Attention Mechanism ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The analysis in Sec. 3.4 highlights that both cross-attention and self-attention likely play a role in interpolating spatially consistent images. Proposition 1 can be generalized to self-attention where the keys and values are derived from the latent $z$ instead of $c$ for enhancing spatial constraint. As such, we define a general form of inner-interpolated attention on the keys and values as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{Intp-Attn}_{I}\\Big(Q_{i},K_{1:m},V_{1:m};\\;w_{i}\\Big)=\\mathrm{Attn}\\Big(Q_{i},\\;(1-w_{i})K_{1}+w_{i}K_{m},\\;(1-w_{i})V_{1}+w_{i}V_{m}\\Big),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $Q_{i}$ is derived from $z_{i}$ . Note that Eq. 8 is equivalent to Eq. 4 if $\\{K_{1},K_{m}\\}$ and $\\{V_{1},V_{m}\\}$ are derived from $\\{c_{1},c_{m}\\}$ , i.e. as cross-attention; if they are derived from $\\{z_{1},z_{m}\\}$ , then it represents self-attention. ", "page_idx": 5}, {"type": "text", "text": "Instead of applying interpolation to the key and value, we can also interpolate the attention itself. We define this as outer-interpolated attention: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{Intp-Attn}_{O}\\Big(Q_{i},K_{1:m},V_{1:m};w_{i}\\Big)=(1-w_{i})\\cdot\\mathrm{Attn}\\Big(Q_{i},K_{1},V_{1}\\Big)+w_{i}\\cdot\\mathrm{Attn}\\Big(Q_{i},K_{m},V_{m}\\Big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Similarly, Eq. 9 can represent both cross- and self-attention, depending on if $\\{K_{1},K_{m}\\}$ and $\\{V_{1},V_{m}\\}$ are derived from $\\{c_{1},c_{m}\\}$ or $\\{z_{1},z_{m}\\}$ respectively. More details on the differences between inner and outer interpolation are given in the Appx. C. We denote the two versions as AID-I and AID-O for inner and outer interpolation respectively. ", "page_idx": 6}, {"type": "text", "text": "While applying interpolation as defined in Eqs. 8 and 9 for self-attention does lead to high spatial consistency, it also results in poor fidelity images. This is likely because directly replacing the selfattention mechanism with some interpolated version is too aggressive. Therefore, for self-attention, we maintain the source keys and values $K_{i}$ and $V_{i}$ from the interpolated $z_{i}$ and concatenate them with the interpolated keys and values, as shown in Fig. 3. Denoting concatenation as $[\\cdot,\\cdot]$ , we define fused attention interpolation, leading to a fused inner-interpolated attention: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{nup-Atun}_{I}^{F}\\Big(Q_{i},K_{1:m},V_{1:m};w_{i}\\Big)=\\mathsf{A t u n}\\Big(Q_{i},\\ \\big[(1\\!-\\!w_{i})K_{1}\\!+\\!w_{i}K_{m},\\ K_{i}\\big],\\ \\big[(1\\!-\\!w_{i})V_{1}\\!+\\!w_{i}V_{m},\\ K_{i}\\big]\\Big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For self-attention, as $K_{i}$ is derived from $z_{i}$ , $K_{i}\\neq(1-w_{i})K_{1}+w_{i}K_{m}$ ; the same holds for $V_{i}$ . For cross-attention, however, $K_{i}=(1-w_{i})K_{1}+w_{i}\\dot{K}_{m}$ , so fusing the two does not provide additional benefits. We note that there are opportunities for fusion with keys and values derived from other sources. We follow such a strategy in Sec. 4.3 to inject additional text-based guidance. ", "page_idx": 6}, {"type": "text", "text": "Analogous to Eq. 9, we define a fused outer-interpolated attention: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Intp-Attn}_{O}^{F}\\Big(Q_{i},K_{1:m},V_{1:m};w_{i}\\Big)=(1-w_{i})\\cdot\\mathrm{Attn}\\Big(Q_{i},[K_{1},K_{i}],[V_{1},V_{i}]\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+w_{i}\\cdot\\mathrm{Attn}(Q_{i},[K_{m},K_{i}],[V_{m},V_{i}]\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4.2 Non-Uniform Interpolation Coefficients ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The analysis in Sec. 3.4 shows that interpolation coefficients should not be selected uniformly adopted in previous methods [11, 16] on the interpolation path. For more flexibility, we apply a Beta distribution $p_{B}(t,\\alpha,\\beta)$ . Beta distributions are conveniently defined within the range of $[0,1]$ . When $\\alpha\\!=\\!1$ and $\\beta=1$ , $p_{B}$ degenerates to a uniform distribution, which reverts to the original setting. When $\\alpha\\,>\\,1$ and $\\beta\\,>\\,1$ , the distribution is concave (bell-shaped), with higher probabilities away from the end-points of 0 and 1, i.e. away from the source images. Finally, the selected points are adjustable based on alpha and beta values, to give higher preference towards one or the other source image (see Fig. 3). ", "page_idx": 6}, {"type": "text", "text": "Given the Beta prior represented as cumulative distribution function $F_{B}(w,\\alpha,\\beta)$ , we define a Betainterpolation $r_{B}(w;0,1)$ as $r(F^{-1}(w,\\alpha,\\beta))$ , where $w\\sim U(0,1)$ . Therefore, the distributed point with Beta prior becomes: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\{r(0),r(F_{B}^{-1}(\\frac{1}{m-1},\\alpha,\\beta)),...,r(F_{B}^{-1}(\\frac{m-2}{m-1},\\alpha,\\beta)),r(1)\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In practice, we employ a dynamic selection process to adjust the $\\alpha$ and $\\beta$ parameters of the Beta prior, and form the smoothest sequence from the explored coefficients. Further details are provided in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "4.3 Prompt Guided Conditional Interpolation (PAID) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Given two source inputs, the hypothesis space of interpolation paths is actually large and diverse. Yet most interpolation methods [52, 38] estimate one deterministic path. Can we control or specify the interpolation path? One possibility is to provide a (third) conditioning text, which we refer to as a guidance prompt. To connect the interpolated sequence with the text in the guidance prompt $g$ , we fuse the associated key $K_{g}=W_{K}^{\\mathsf{T}}g$ and value $\\bar{V_{g}}\\,\\bar{=}\\,{\\cal W}_{V}^{\\top}g$ instead of the original $K_{i}$ and $V_{i}$ in the fused inner-interpolated attention in Eq. 10 for cross-attention: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Guide\\mathrm{-}A t t h}_{I}^{F}\\Big(Q_{i},K_{1:m},V_{1:m};w_{i},K_{g},V_{g}\\Big)=}\\\\ &{\\qquad\\quad\\mathrm{Atth}\\Big(Q_{i},\\ \\big[(1-w_{i})K_{1}+w_{i}K_{m},\\ K_{g}\\big],\\ \\big[(1-w_{i})V_{1}+w_{i}V_{m},\\ V_{g}\\big]\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In practice, the guidance prompt is provided by users to choose the interpolation path conditioned on the text description as Fig. 1 (f) shows. We demonstrate that the prompt-guided attention interpolation dramatically boosts the ability of compositional generation in Sec. 5.2. ", "page_idx": 6}, {"type": "table", "img_path": "Nb5xlelV0C/tmp/e1912e1915414a7fbf0d7010630e398c29eedd24e5c66f6a41c9578294929b95.jpg", "table_caption": [], "table_footnote": ["Table 1: Quantitative results of conditional interpolation. Quantitative results where the best performance is marked as $(^{*})$ and the worst is marked as red. (a) Performance on CIFAR-10 and LAION-Aesthetics. AID-O and AID-I both show significant improvement over the Text Embedding Interpolation (TEI). Though Denoising Interpolation (DI) achieves relatively high fidelity, there is a trade-off with very bad performance on consistency (0.4295). AID-O boosts the performance in terms of consistency and fidelity while AID-I boosts the performance of smoothness; (b) Ablation studies on AID-O\u2019s components, showcase that the Beta prior enhances smoothness, attention interpolation heightens consistency, and self-attention fusion significantly elevates fidelity. "], "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Configuration and Settings. We evaluate quantitatively based on the three measures for conditional interpolation defined in Sec. 3.3 and user studies. Detailed experimental and application configurations are given in Appxs. F and G. ", "page_idx": 7}, {"type": "text", "text": "We use Stable Diffusion 1.4 [35] as the base model to implement our attention interpolation mechanism for quantitative evaluation. In all experiments, a $512\\times512$ image is generated with the DDIM Scheduler [42] and DPM Scheduler [26] within 25 timesteps. Additional qualitative results using other state-of-the-art text-to-image diffusion models [30, 23, 2] are given in Appx. H. ", "page_idx": 7}, {"type": "text", "text": "5.1 Conditional Interpolation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Protocol, Datasets & Comparison Methods. For experiments in each dataset, we run 5 trials each with $N=100$ iterations. In each iteration, we randomly select two conditions and generate an interpolation sequence with size $m=7$ . We report the mean of each metric of the interpolation sequences over all trials as the final result. Our proposed framework is evaluated using corpora from CIFAR-10 [22] and the LAION-Aesthetics dataset from the larger LAION-5B collection [39]. To the best of our knowledge, the only related method is the text-embedding interpolation (TEI) [49, 53, 55] (see Sec. 3.2). We also compare with Denoising Interpolation (DI), which interpolates along the denoising schedule; more details in DI are given in the Appx. F. ", "page_idx": 7}, {"type": "text", "text": "Results. We quantitatively evaluate our methods based on the evaluation protocol as shown in Tab. 1. AID-O significantly increases the performance of all the evaluation metrics. AID-I achieves higher smoothness, AID-O has significant improvements in consistency ${\\cdot20.3\\%}$ on CIFAR-10 and $-23.9\\%$ on LAION-Aesthetics) and fidelity (-66.62 on CIFAR-10 and -60.37 on LAION-Aesthetics). The fidelity of AID-I is poorer than AID-O and worse than Denoising Interpolation. However, AID-I achieves competitive qualitative results as shown by the user study. ", "page_idx": 7}, {"type": "text", "text": "Ablation Study. Tab. 1 shows ablations of the AID-O framework with CIFAR-10, focusing on three primary design elements: attention interpolation, self-attention fusion, and Beta-interpolation. Results show that attention interpolation improves consistency while Beta-interpolation contributes to improvements in smoothness and self-attention fusion to enhance image fidelity. While attention interpolation (without fusion with self-attention) with Beta-interpolation achieves the highest smoothness, it does so at the cost of fidelity. Similarly, AID without Beta interpolation achieves the strongest consistency but trades off smoothness (see Fig. 4). Fig. 4 (a) provides a qualitative comparison between different ablation settings. ", "page_idx": 7}, {"type": "text", "text": "User Study. Using Mechanical Turk, we check for human preferences on four types of text sources: 1) near objects, such as dogs and cats; 2) far objects, such as dragons and bananas; 3) scenes, such as waterfalls and sunsets; and 4) scene and object, such as a sea of flowers with a robot. This variety provides a comprehensive assessment for both concept and spatial interpolation. We conducted 320 trials in total; in each trial, an independent evaluator was asked to select their preferred interpolation result. Tab. 2 shows that our method is almost always preferred, though the preference is split across AID-I and AID-O depending on the type of text sources. ", "page_idx": 7}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/2de2a26dca40a751a11407bc6eef58ddac6164f5a8a986f8f0b81637bd09f598.jpg", "img_caption": ["Figure 4: Qualitative comparison of different ablation setting of AID. (a) Qualitative comparison between AID without fusion $\\mathrm{[^{\\,\\mathrm{st}}}$ row), AID with fusion $\\mathrm{2^{nd}}$ row), and AID with fusion and beta prior ( $3^{\\mathrm{rd}}$ row). Fusing interpolation with self-attention alleviates the artifacts of the interpolated image significantly, while beta prior increases smoothness based on AID with fusion. (b) CLIP score of different methods on composition generation. ", "Table 2: Human evaluation results. (a): Human preference ratio of each method in different categories of interpolation, AID-I, and AID-O are dominantly preferred by TEI; (b): Smoothness of different editing methods, combined with AID boosts the control ability on the editing level. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Nb5xlelV0C/tmp/f6aa0828a156ff8a628d93a375a05100d3f0bb298b445910b188e735a1ef2d4c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2 Application ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we firstly introduce how to adapt our methods into applications including image editing control and compositional generation. We further extend to cross-modality conditions including image prompt as well with IP-Adapter [54], which enables applications including image morphing and image-controlled generation. We provide more details in Appendix. G. ", "page_idx": 8}, {"type": "text", "text": "Image Editing Control. Text-based image editing tries to modify an image based on a textual description (see Fig. 5). Existing methods [16, 11, 48] rely on text embedding interpolation to control the editing level. Training-free methods [48, 11] struggle to control the editing level based on the text, while ours does not. We validate the control ability of our methods using Prompt-toPrompt [11] (P2P) for synthesized image editing and EDICT [48] for real image editing. ", "page_idx": 8}, {"type": "text", "text": "We evaluate the ability to control the editing level using the smoothness metric defined in Sec. 3.3 using the image editing dataset presented in [48]. Given an image with an editing level of 1 and the original image with an editing level of 0, we use either TEI or AID-O to interpolate edited images with levels ranging from $\\{\\textstyle{\\frac{1}{6}},{\\frac{7}{6}},\\dotsc,{\\frac{5}{6}}\\}$ and assess the smoothness of the edited image sequence. ", "page_idx": 8}, {"type": "text", "text": "Quantitative results are reported in Tab. 2 (b). Our method greatly improves the smoothness of the edited image sequence, aligning with different editing levels and thereby enhancing the control ability for editing. As shown in Fig. 5, P2P alone cannot effectively control the editing level but combining it with AID allows for precise level adjustments. ", "page_idx": 8}, {"type": "text", "text": "Compositional Text-to-Image Generation. Compositional generation is highly challenging for text-to-image diffusion models [25, 6, 7]. In our experiments, we focus on concept conjunction [6] - generating images that satisfy two given text conditions. For example, given the conditions \"a robot\" and \"a sea of flowers,\" the goal is to generate an image that aligns with both \"a robot\" and \"a sea of flowers.\" ", "page_idx": 8}, {"type": "text", "text": "For compositional generation, we use PAID to interpolate between conditions $c_{1}$ and $c_{2}$ with the prompt guidance $\"c_{1}$ AND $c_{2}\"$ . For quantitative evaluation, we use the same dataset for human evaluation as in Sec. 5.1 and CLIP scores [32] to evaluate if the generated images align with both conditions. We compare our methods with vanilla Stable Diffusion [35, 30] and two other state-ofthe-art training-free methods: Compositional Energy-based Model (CEBM) [25] and RRR [7]. ", "page_idx": 8}, {"type": "text", "text": "Fig. 4 (b) shows that the CLIP score of our method is higher than previous methods for both Stable Diffusion 1.4 [35] and SDXL [30]. Moreover, our method produces fewer artifacts such as merging the two objects together, as illustrated in Fig. 6. ", "page_idx": 8}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/4e1aa90dcee2df05c4899cad081ed4e3416cad139e09ed691c04f41a6b03f57e.jpg", "img_caption": ["Figure 5: Results of image editing control. Our method boosts the controlling ability over editing. The first row of (a) and (b) is generated by $\\mathrm{P}2\\mathrm{P}+$ AID while the second row is $\\mathrm{P2P+TEI}$ . "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/4892d81d7b0551516eaf423f13383ade80cea06ad455ca2deb2f6f2eb3008315.jpg", "img_caption": ["Figure 6: Results of compositional generation. Images on the left are generated with \"a deer\" and \"a plane\" based on SD 1.4 [35] and images on the right are generated with \"a robot\" and \"a sea of flowers\" based on SDXL [30]. Compared to other methods, PAID-O properly captures both conditions with higher fidelity. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/4cb4ea50474d858930dd50b54d1e7914867712f93200960912aa73b2edb5cae0.jpg", "img_caption": ["Figure 7: Results of AID with image conditions. Our method is compatible with IP-Adapter for imageconditioned generation (a). In both global image prompt (b) and composition image prompt (c), from left to right the scale of additional image prompt slowly increases. The first row illustrates results controlled by AID, while the second row shows results achieved using the scale setting provided by IP-Adapter. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Image Morphing and Image-Controlled Generation. Image morphing finds transitions between two images, while image-controlled generation creates images based on a text prompt with an additional image prompt. To enable generation with image condition, we adapt AID on IP-Adapter [54]. IP-Adapter integrates image embeddings into cross-attention layers, allowing diffusion models to incorporate image prompts. For morphing, we use an empty text prompt and apply AID for smooth interpolation between image conditions. In image-controlled generation, AID adjusts the image prompt scale across endpoints, enhancing control. ", "page_idx": 9}, {"type": "text", "text": "Our method enables effective image interpolation (Fig. 7 (a)) and offers finer control than IPAdapter. As shown in Fig. 7 (b), AID maintains both text and image alignment, while in Fig. 7 (c), it better preserves identity while following compositional references. Further comparisons are provided in Appendix G. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduce a novel task: conditional interpolation within a diffusion model, along with its evaluation metrics, which include consistency, smoothness, and fidelity. We present a novel approach, referred to as AID and PAID, designed to produce interpolations between images under varying conditions. This method significantly surpasses the baseline in performance without training, as demonstrated through both qualitative and quantitative analysis. Our method is training-free and broadens the scope of generative model interpolation, paving the way for new opportunities in various applications, such as compositional generation and image editing control. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18392\u201318402, 2023. [2] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Zhongdao Wang, James T. Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart- $\\alpha$ : Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR, 2024.   \n[3] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, and William W Cohen. Subject-driven text-to-image generation via apprenticeship learning. Advances in Neural Information Processing Systems, 36, 2024. [4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Conference on Computer Vision and Pattern Recognition, pp. 248\u2013255. Ieee, 2009. [5] Robert Dorfman. A formula for the gini coefficient. The review of economics and statistics, pp. 146\u2013149, 1979. [6] Yilun Du, Shuang Li, and Igor Mordatch. Compositional visual generation with energy-based models. Advances in Neural Information Processing Systems, 33:6637\u20136647, 2020.   \n[7] Yilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and MCMC. In ICML, volume 202 of Proceedings of Machine Learning Research, pp. 8489\u20138510. PMLR, 2023. [8] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint arXiv:1710.09412, 2014. [9] Jinjin Gu and Chao Dong. Interpreting super-resolution networks with local attribution maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9199\u20139208, 2021.   \n[10] Qiyuan He, Linlin Yang, Kerui Gu, Qiuxia Lin, and Angela Yao. Analyzing and diagnosing pose estimation with attributions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4821\u20134830, 2023.   \n[11] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross-attention control. In ICLR. OpenReview.net, 2023.   \n[12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in Neural Information Processing Systems, 30, 2017.   \n[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \n[14] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022.   \n[15] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4401\u20134410, 2019.   \n[16] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6007\u20136017, 2023.   \n[17] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023.   \n[18] Siavash Khodadadeh, Sharare Zehtabian, Saeed Vahidian, Weijia Wang, Bill Lin, and Ladislau B\u00f6l\u00f6ni. Unsupervised meta-learning through latent-space interpolation in generative models. arXiv preprint arXiv:2006.10236, 2020.   \n[19] Valentin Khrulkov, Gleb Ryzhakov, Andrei Chertkov, and Ivan Oseledets. Understanding ddpm latent codes through optimal transport. arXiv preprint arXiv:2202.07477, 2022.   \n[20] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[21] Tetta Kondo, Shumpei Takezaki, Daichi Haraguchi, and Seiichi Uchida. Font style interpolation with diffusion models. arXiv preprint arXiv:2402.14311, 2024.   \n[22] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[23] Cagliostro Research Lab. Animagine xl 3.1. https://huggingface.co/cagliostrolab/ animagine-xl-3.1, 2024.   \n[24] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. Advances in Neural Information Processing Systems, 35:17612\u201317625, 2022.   \n[25] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B. Tenenbaum. Compositional visual generation with composable diffusion models. In ECCV, 2022.   \n[26] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775\u20135787, 2022.   \n[27] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-instruct: A universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36, 2024.   \n[28] OpenAI. GPT-4. https://openai.com/gpt-4, 2023.   \n[29] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4195\u20134205, 2023.   \n[30] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024.   \n[31] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. arXiv preprint arXiv:2303.09535, 2023.   \n[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, Proceedings of Machine Learning Research, 2021.   \n[33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.   \n[34] Royi Rassin, Eran Hirsch, Daniel Glickman, Shauli Ravfogel, Yoav Goldberg, and Gal Chechik. Linguistic binding in diffusion models: Enhancing attribute correspondence through attention map alignment. Advances in Neural Information Processing Systems, 36, 2024.   \n[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684\u201310695, 2022.   \n[36] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pp. 234\u2013241. Springer, 2015.   \n[37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022.   \n[38] Dvir Samuel, Rami Ben-Ari, Nir Darshan, Haggai Maron, and Gal Chechik. Norm-guided latent space exploration for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024.   \n[39] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278\u201325294, 2022.   \n[40] Yujun Shen, Ceyuan Yang, Xiaoou Tang, and Bolei Zhou. Interfacegan: Interpreting the disentangled face representation learned by gans. IEEE transactions on pattern analysis and machine intelligence, 44(4):2004\u20132018, 2020.   \n[41] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.   \n[42] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[43] \u0141ukasz Struski, Jacek Tabor, Igor Podolak, Aleksandra Nowak, and Krzysztof Maziarz. Realism index: Interpolation in generative models with arbitrary prior. arXiv preprint arXiv:1904.03445, 2019.   \n[44] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818\u20132826, 2016.   \n[45] Suryakanthi Tangirala. Evaluating the impact of gini index and information gain on classification using decision tree classifier algorithm. International Journal of Advanced Computer Science and Applications, 11(2):612\u2013619, 2020.   \n[46] Quang Nhat Tran and Shih-Hsuan Yang. Efficient video frame interpolation using generative adversarial networks. Applied Sciences, 10(18):6245, 2020.   \n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.   \n[48] Bram Wallace, Akash Gokul, and Nikhil Naik. EDICT: exact diffusion inversion via coupled transformations. In CVPR, pp. 22532\u201322541. IEEE, 2023.   \n[49] Clinton J Wang and Polina Golland. Interpolating between images with diffusion models. arXiv e-prints, pp. arXiv\u20132307, 2023.   \n[50] Jinghao Wang, Zhengyu Wen, Xiangtai Li, Zujin Guo, Jingkang Yang, and Ziwei Liu. Pair then relation: Pair-net for panoptic scene graph generation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n[51] Ruichen Wang, Zekang Chen, Chen Chen, Jian Ma, Haonan Lu, and Xiaodong Lin. Compositional text-to-image synthesis with attention map control of diffusion models. arXiv preprint arXiv:2305.13921, 2023.   \n[52] Cheng Yang, Lijing Liang, and Zhixun Su. Real-world denoising via diffusion model. arXiv preprint arXiv:2305.04457, 2023.   \n[53] Zhaoyuan Yang, Zhengyang Yu, Zhiwei Xu, Jaskirat Singh, Jing Zhang, Dylan Campbell, Peter H. Tu, and Richard Hartley. IMPUS: image morphing with perceptually-uniform sampling using diffusion models. In ICLR. OpenReview.net, 2024.   \n[54] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023.   \n[55] Kaiwen Zhang, Yifan Zhou, Xudong Xu, Bo Dai, and Xingang Pan. Diffmorpher: Unleashing the capability of diffusion models for image morphing. In CVPR, pp. 7912\u20137921. IEEE, 2024.   \n[56] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3836\u20133847, 2023.   \n[57] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 586\u2013595, 2018.   \n[58] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: A unified predictorcorrector framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Preliminaries and formulation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Linear / Spherical Interpolation. Given tensor $A$ and tensor $B$ , the linear interpolation path $r_{l}(w)$ where $w\\in[0,1]$ is defined as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nr_{l}(w;A,B)=(1-w)A+w B\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The spherical interpolation is defined as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nr_{s}(w;A,B)=\\frac{\\sin(1-w)\\theta}{\\sin\\theta}A+\\frac{\\sin w\\theta}{\\sin\\theta}B,\\quad\\theta=a r c o s\\frac{A\\cdot B}{||A|||B||}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Distinction on the Discrete Sequence and Continuous Path. Our formulation diverges from previous studies by concentrating on the assessment of discrete samples, referred to as the interpolation sequence, instead of the continuous interpolation path. This is crucial because the quality of the interpolation sequence is determined not only by the interpolation path\u2019s quality but also by how to select the exact sample along the interpolation path, which previous methods overlook. Additionally, the size of an interpolation sequence is often low in practical usage [38, 52]. As a result, our evaluation framework is specifically designed to cater to interpolation sequences. ", "page_idx": 14}, {"type": "text", "text": "This distinction is significant when evaluating smoothness and consistency as Fig. 8 shows. While Perceptual Path Length (PPL) [15] indicates both smoothness and consistency on the continuous path, where the PPL of the blue path is shorter than the green path, this does not hold in discrete sequences. The sequence can have bad smoothness even if it lies on a smooth interpolation path (see the blue triangle). ", "page_idx": 14}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/b4c884473a5642ee0660deec37da4460897921d68bd3b0b35a2b0230554806ac.jpg", "img_caption": ["Figure 8: Difference between smoothness and consistency in measurement of discrete sequence. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Proof of Proposition 1. Proposition 1 indicates that interpolating text embedding linearly is equivalent to interpolating key and value in the cross-attention mechanism. The proof is straightforward by decomposing the formula of the attention layer as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A(z_{i},c_{i})=A t t n(Q_{i},K_{i},V_{i})}\\\\ &{\\qquad\\qquad=A t t n(Q_{i},W_{K}^{T}c_{i},W_{V}^{T}c_{i})}\\\\ &{\\qquad\\quad=A t t n(Q_{i},W_{K}^{T}r_{l}(\\frac{i-1}{m-1};c_{1},c_{m}),W_{V}^{T}r_{l}(\\frac{i-1}{m-1};c_{1},c_{m}))}\\\\ &{\\qquad\\quad=A t t n(Q_{i},r_{l}(\\frac{i-1}{m-1};W_{K}^{T}c_{1},W_{K}^{T}c_{m}),r_{l}(\\frac{i-1}{m-1};W_{V}^{T}c_{1},W_{V}^{T}c_{m}))}\\\\ &{\\qquad\\qquad=A t t n(Q_{i},r_{l}(\\frac{i-1}{m-1};K_{1},K_{m}),r_{l}(\\frac{i-1}{m-1};V_{1},V_{m}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B Diagnosis of Text Embedding Interpolation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Controlled Experiments on the Key and Value of Attention. To conduct the analysis on the key and value of self-attention and cross-attention, we analyze the effect by replacement experiments. Specifically, given two conditions $c$ and $c^{\\prime}$ , we first generate $I$ and $I^{\\prime}$ accordingly. They replace all the key and value of either cross-attention or self-attention during the generation of $I^{\\prime}$ to the key and value computed from $I$ , which incurs two new generated images including $I_{c r o s s}^{\\prime}$ and $I_{s e l f}^{\\prime}$ . If self-attention is more important to constraint the spatial layout, The images obtained by replacing self-attention $I_{s e l f}^{\\prime}$ should be more similar to $I$ compared to $I_{c r o s s}$ . ", "page_idx": 14}, {"type": "text", "text": "To quantitatively verify this, we consider two images sharing more similar spatial layouts should have lower differences in the low-frequency information. Therefore, we evaluate the difference in the spatial layout of the two images by directly evaluating the L2 loss on the low-pass images. Specifically, it can be written as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{s l}(I,I^{\\prime};\\sigma)=\\frac{1}{2}||G(I;\\sigma)-G(I^{\\prime};\\sigma)||^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/852962ed06efe92c593044fc0c2da3503b2f47e3858678f0e0958d3f38337c8d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 9: Diagnosis of text embedding interpolation on spatial layout (a - e) and adjacent distance (f). (a) Image generated by \u201ca cat wearing sunglasses\u201d; (b) Image generated by \u201ca dog wearing sunglasses\u201d; (c) Replacing the cross-attention during generation of (b) by (a); (d) Replacing the self-attention during generation of (b) by (a); (e) Box plot of $D_{s l}(I,I_{c r o s s}^{\\prime})$ and $D_{s l}(I,I_{s e l f}^{\\prime})$ . When fixing a query, the key and value in self-attention mostly determine the output of pixel space compared to cross-attention. (f) The maximum adjacent distance and the average of other adjacent pairs. ", "page_idx": 15}, {"type": "text", "text": "where $G(\\cdot;\\sigma)$ represents Gaussian blurring kernel with parameter $\\sigma$ . We conduct our experiments based on the corpus in the form of class names of CIFAR-10 [22], which we introduce in Sec. 5.1. We run 100 trials and generate two images at each trial then compare the difference between $D_{s l}(I,I_{s e l f}^{\\prime})$ and $\\bar{D}_{s l}\\bar{(I,I_{c r o s s}^{\\prime})}$ . ", "page_idx": 15}, {"type": "text", "text": "Based on our empirical verification shown in Fig. 9 (e), $D_{s l}(I,I_{s e l f}^{\\prime})\\approx0$ , which indicates that the spatial layout of $I_{s e l f}^{\\prime}$ is almost the same as $I$ , while $D_{s l}(I,I_{s e l f}^{\\prime})\\,>>\\,D_{s l}(I,I_{c r o s s}^{\\prime})$ indicating key and value of self-attention impulses much stronger spatial constraints on the generation than cross-attention. ", "page_idx": 15}, {"type": "text", "text": "Non-smooth Distance Among Adjacent Pairs. Selecting uniformly distributed interpolation coefficients in text embedding interpolation, commonly does not result in uniform visual transition in the pixel space. Instead, we found that small visual transitions may occur over a large range of interpolation coefficients, and vice versa. To quantitatively verify this, we randomly draw two text conditions from the same corpus of CIFAR-10 [22] and apply text embedding interpolation with uniformly distributed coefficients $\\{0,0.25,0.5,0.75,1\\}$ to generate interpolated images. Then we evaluate our observation by comparing the maximum distance of four adjacent pairs and the average of other distances. As Fig. 9 (f) shows, the maximum distance is often much larger than the average distance of other adjacent pairs, indicating that abrupt visual transition occurs in a short range of interpolation coefficients transition. ", "page_idx": 15}, {"type": "text", "text": "C Outer vs. Inner Attention Interpolation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Mathematical Induction. We start by comparing the formula of outer interpolated attention and inner interpolated attention. We expand the inner interpolated attention defined in Eq. 9 as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Intp.Attn}_{I}(Q_{i},K_{1:m},V_{1:m};t_{i})}\\\\ {=}&{\\mathrm{Attn}(Q_{i},\\,(1-t_{i})K_{1}+t_{i}K_{m},(1-t_{i})V_{1}+t_{i}V_{m})}\\\\ {=}&{\\mathrm{softmax}(\\frac{Q_{i}\\left[(1-t_{i})K_{1}+t_{i}K_{m}\\right]^{T}}{\\sqrt{d_{k}}})[(1-t_{i})V_{1}+t_{i}V_{m})]}\\\\ {=}&{(1-t_{i})\\cdot\\mathrm{softmax}(\\frac{Q_{i}\\left[(1-t_{i})K_{1}+t_{i}K_{m}\\right]^{T}}{\\sqrt{d_{k}}})V_{1}}\\\\ &{+t_{i}\\cdot\\mathrm{softmax}(\\frac{Q_{i}\\left[(1-t_{i})K_{1}+t_{i}K_{m}\\right]^{T}}{\\sqrt{d_{k}}})V_{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, we expand the outer interpolated attention defined in Eq. 10 ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Intp-Attn}_{O}(Q_{i},K_{1:m},V_{1:m};t_{i})}\\\\ {=}&{(1-t_{i})\\cdot\\mathrm{Attn}(Q_{i},K_{1},V_{1})+t_{i}\\cdot\\mathrm{Attn}(Q_{i},K_{m},V_{m})}\\\\ {=}&{(1-t_{i})\\cdot\\mathrm{softmax}(\\frac{Q_{i}K_{1}^{T}}{\\sqrt{d_{k}}})V_{1}+t_{i}\\cdot\\mathrm{softmax}(\\frac{Q_{i}K_{m}^{T}}{\\sqrt{d_{k}}})V_{m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Comparing Eq. 18 and Eq. 19 above, the essential difference is: while inner attention interpolation uses the same attention map softmax $\\begin{array}{r}{\\frac{\\cdot Q_{i}[(1-t_{i})K_{1}+t_{i}K_{m}]^{T}}{\\sqrt{d_{k}}})}\\end{array}$ fusing source keys $K_{1}$ and $K_{m}$ for ", "page_idx": 15}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/cd090cae4bcd674d12255c001178e045258174bee7c19bcefb0c1ce0e7fd626c.jpg", "img_caption": ["(a) \u201cFox - Watercolor Art Print\u201d to \u201cMerry-Christmas-from-AnnetteFunicello1960.jpeg\u201d "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/1994968c8148c4e3a4e26f0a24f680c1414760b63b6a3cd83bc2f19592feb751.jpg", "img_caption": ["(b) \u201cLouis Henry Sullivan (September 3, 1856 \u2013 April 14, 1924) was an American architect...\u201d to \u201cModern Landscape Painting - Zion by Johnathan Harris\u201d "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 10: Qualitative results from LAION-Aesthetics. For each pair of prompts, the first row is the Input Interpolation, the second row is AID-O and the third row is AID-I. Our methods provide direct and smooth interpolation in spatial layout and style, with high fidelity. ", "page_idx": 16}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/b5d1ac1d88aa025f02fc02789c6c11f5c376420776585467f6ede3982b364e8b.jpg", "img_caption": ["Figure 11: Qualitative comparison between AID-O (the 1st row) and AID-I (the 2nd row). While AID-O prefers keeping the spatial layout, AID-I prefers interpolating the concept and style. Comparing the 4th column in (b), AID-I properly captures \u201cpen in the shape of banana\u201d while AID-O provides a banana but the spatial layout is the same as the pen. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "different source value $V_{1}$ and $V_{m}$ , outer attention interpolation, on the other hand, using different attention maps for different source key and value. This may answer why the AID-I tends to conceptual interpolation fusing the characteristics of two concepts into one target but AID-O tends to spatial layout interpolation allowing the simultaneous existence of two concepts in the interpolated image. ", "page_idx": 16}, {"type": "text", "text": "Qualitative Results. We observe that AID-I prefers interpolation on the concept or style. On the other hand, AID-O strongly enhances perceptual consistency and encourages interpolations in the spatial layout of images, as Fig. 11 shows. Even when interpolating between two very long prompts, both methods can achieve direct and smooth interpolations with high fidelity as Fig. 10 shows. ", "page_idx": 16}, {"type": "text", "text": "D Selection with Beta Prior ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Intuition behind Beta Prior ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Based on our analysis in Sec. 3.4 and Sec. B, the transition often occurs abruptly in a small range of interpolation coefficients. This indicates that we need to select more points in that small range rather than uniformly select coefficients between [0, 1]. ", "page_idx": 16}, {"type": "text", "text": "We hypothesize that this is because: different from interpolation in the latent space, which is only introduced in the initial denoising steps, the diffusion model incorporates the text embedding for multiple denoising steps. This may amplify the influence of the source latent variable with higher coefficients. Therefore, when $t$ is close to 0 or 1, $r^{\\prime}(t)$ is closer to 0, leading to the intuition that we want to sample more mid-range $t$ . ", "page_idx": 16}, {"type": "text", "text": "Based on our heuristics above and the empirical observation in Sec. B, we apply Beta prior, which is a bell-shaped distribution when $\\alpha$ and $\\beta$ are both larger than 1, to encourage more coefficients in a smaller range of interpolation coefficients. Furthermore, we can de-bias the visual transition towards one endpoint to make it smoother by adjusting $\\alpha>\\beta$ , or vice versa. ", "page_idx": 16}, {"type": "text", "text": "D.2 Dynamic Selection ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The main challenge of the entire selection procedure for interpolation coefficients lies in the time cost. Firstly, the ideal interpolation coefficients can vary depending on different combinations of conditions and even different latent seeds. Secondly, exploring each new interpolation coefficient ", "page_idx": 16}, {"type": "text", "text": "Input: Exploration size $n$ , the initial Gaussian Noise $z_{1},z_{n}$ , two conditions $c_{1},c_{n}$ , a generative process represented as $f(z_{1},z_{n},c_{1},c_{n};w_{i})$ , perceptual distance function $P(\\cdot,\\cdot)$ , CDF of Beta distribution F (B\u03b1 $F_{B}^{(\\alpha,\\beta)}$ ", "page_idx": 17}, {"type": "text", "text": "Output: An image sequence I ", "page_idx": 17}, {"type": "text", "text": "Initialize the list of explored coefficients: $\\mathbf{w}\\gets[0,1]$ Initialize image sequence $\\mathbf{I}=[f(z_{1},z_{n},c_{1},c_{n};\\bar{0}),f(z_{1},z_{n},c_{1},c_{n};1)]$ Initialize the list of distance of each neighbouring pair: $\\mathbf{d}\\leftarrow[P(f(z_{1},z_{n},c_{1},c_{n};0),f(z_{1},z_{n},\\bar{c_{1}},c_{n};\\bar{1}))]$ ] Initialize the hyperparameter: $\\alpha\\gets1$ and $\\beta\\gets1$ , iteration number $i\\leftarrow0$ $\\mathbf{k}=\\mathrm{argmax}(d)$ \u25b7Search for the current largest distance and get its index $\\triangleright$ Select coefficient uniformly separate the distance under Beta prior ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\nw^{\\prime}\\gets F_{B}^{(\\alpha,\\beta)^{-1}}(F_{B}^{(\\alpha,\\beta)}(\\mathbf{w}_{k})+F_{B}^{(\\alpha,\\beta)}(\\mathbf{w}_{k+1})/2)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "$\\triangleright$ Update exploration state ", "page_idx": 17}, {"type": "text", "text": "Remove ${\\bf d}_{k}$ from d   \nd.inser $\\langle k,P(f(z_{1},z_{n},c_{1},c_{n};{\\bf w}_{k}),f(z_{1},z_{n},c_{1},c_{n};w^{p}r i m e)))$   \nd.insert $[k+1$ , $P(f(z_{1},z_{n},c_{1},c_{n};w^{\\prime}f(z_{1},z_{n},c_{1},c_{n};\\mathbf{w}_{k+1})))$   \nw.insert $(k,w^{\\prime})$   \nI.insert $\\langle\\boldsymbol{k},f(\\boldsymbol{z}_{1},\\boldsymbol{z}_{n},\\boldsymbol{c}_{1},\\boldsymbol{c}_{n};\\boldsymbol{w}^{\\prime}))$   \nGet target points: $\\hat{\\mathbf{w}}\\leftarrow\\mathbf{d}/\\mathbf{d}.s u m()$ , w\u02c6 $\\leftarrow$ accumulate( w\u02c6)   \nCurve fit: \u03b1, \u03b2 \u2190argmax\u03b1,\u03b2MLE(F (B\u03b1, \u03b2) , w, w\u02c6)   \n$i+=1$ ", "page_idx": 17}, {"type": "text", "text": "\u25b7Update Beta prior ", "page_idx": 17}, {"type": "text", "text": "end for return I ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "requires re-running the image generation process. To address these issues, we introduce a Beta-based dynamic selection method to efficiently select satisfactory interpolation coefficients. This selection procedure is divided into two stages as shown in Alg. 1 and Alg. 2. In the first stage, we explore different coefficients based on a Beta prior and observations of perceptual distances. In the second stage, we search for a smooth image interpolation sequence from the images generated with the explored coefficients. Below, we refer to the exploration size as $n$ , and the size of the interpolation sequence as $m$ . ", "page_idx": 17}, {"type": "text", "text": "Exploration. During the exploration procedure, we maintain the following: 1) the currently explored coefficients w; 2) the hyperparameters of the Beta distribution $\\alpha$ and $\\beta;3$ ) the list of perceptual distances $\\mathbf{d}$ of each neighboring image pair generated by the explored coefficients; and 4) the generated images I. ", "page_idx": 17}, {"type": "text", "text": "In each iteration, we first select the neighboring image pair with the highest perceptual distance and explore a new coefficient located between the selected two coefficients according to the Beta distribution. We then generate a new image with the chosen coefficient and compute its perceptual distance with both images in the selected pair. After that, we update $\\alpha$ and $\\beta$ based on the new observations. Specifically, we aim to adjust the coefficients so that their differences are proportional to the perceptual distances between neighboring images, which is our target. Using the currently explored coefficients and their corresponding target coefficients, we update $\\alpha$ and $\\beta$ by fitting the cumulative distribution function. By repeating this process, we obtain a set of generated images to be used in the second stage. ", "page_idx": 17}, {"type": "text", "text": "Search. In the second stage, we first compute the perceptual distance between each pair (not only neighboring pairs), represented as a weight matrix $W$ . We reformulate the task as a directed graph, where each image represents a node $I_{i}$ with $i\\;\\in\\;\\{1,2,\\ldots,n\\}$ , and an edge $E_{i j}$ exists for any $1\\,\\leq\\,i\\,<\\,j\\,\\leq\\,n$ . Our goal is to find a path starting from $I_{1}$ to $I_{n}$ with a fixed path length $m$ , maximizing smoothness. ", "page_idx": 17}, {"type": "text", "text": "To solve this problem efficiently, we use a heuristic indicator\u2014the difference between the maximum and minimum weights, i.e, the range of weight along the path\u2014which reflects smoothness. This ", "page_idx": 17}, {"type": "text", "text": "Algorithm 2 Search smoothest sequence ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Input: Image sequence I, interpolation size $m$ , perceptual distance $P(\\cdot,\\cdot)$ , threshold $\\epsilon$   \nOutput: Smooth interpolation sequence $\\mathbf{\\cal{I}^{\\prime}}$   \nInitialize graph $G=(V,E)$ , where $V\\gets\\mathbf{I}$ and $E_{i j}\\leftarrow P(\\mathbf{I}_{i},\\mathbf{I}_{j}),\\forall i<j$   \nSet binary search bounds: $l\\leftarrow0$ , $h\\leftarrow\\operatorname*{max}(E)-\\operatorname*{min}(E)$   \nwhile $h-l>\\epsilon$ do $D\\leftarrow(h+l)/2$ Initialize $D P$ with size $(n,m)$ , where $D P_{1,1}\\leftarrow(-\\infty,\\infty,[1])$ ) $\\triangleright$ Each item includes (max,   \nmin, path) for $s=1$ to $m-1$ do $\\triangleright$ Start dynamic programming for $i=1$ to $n-1$ do Let $(w_{\\mathrm{max}},w_{\\mathrm{min}},\\hat{I})\\gets D P_{i,s}$ for $j=i+1$ to $n$ do if $w_{\\mathrm{min}}<E_{i j}<w_{\\mathrm{max}}$ then Update $\\dot{w_{\\mathrm{max}}}\\gets\\operatorname*{max}(w_{\\mathrm{max}},E_{i j})$ , $w_{\\mathrm{min}}\\leftarrow\\mathrm{min}(w_{\\mathrm{min}},E_{i j})$ if $w_{\\mathrm{max}}-w_{\\mathrm{min}}<D$ and better path found then $D P_{j,s+1}\\gets(w_{\\operatorname*{max}},w_{\\operatorname*{min}},\\hat{I}+[j])$ end if end if end for end for end for if $D P_{n,m}$ exists then $h\\leftarrow D$ , $I^{\\prime}\\gets D P_{n,m}$ [3] else $l\\leftarrow D$ end if   \nend while   \nreturn $I^{\\prime}$ ", "page_idx": 18}, {"type": "text", "text": "indicator is bounded by the difference between the maximum and minimum weights of the entire graph, allowing us to perform a binary search to find the lowest possible difference. Specifically, given a value of such a difference, we search for a path fulfilling the requirement with length $m$ using dynamic programming. The algorithm is guaranteed to find the path with the minimal difference between the maximum and minimum weights. This is equivalent to finding an image interpolation sequence with the minimal difference between the maximum and minimum perceptual distances among all neighboring pairs. ", "page_idx": 18}, {"type": "text", "text": "The computation complexity of the search algorithm is $O(n^{2}m\\cdot l o g(c))$ , where $c$ is the range of the perceptual distance. In practice, we choose the exploration size $n=1.5m$ , which can already achieve very smooth results and the overhead is neglible compared to the cost of inference with diffusion model. ", "page_idx": 18}, {"type": "text", "text": "E Trade-off between Consistency and Fidelity via Warm-up Steps ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We observe that early steps in denoising are essential to determine the spatial layout of the generated image. Thus, we can trade off between the effect of interpolation and prompt guidance by setting the number of warm-up steps. After several warm-up steps, we transform the attention interpolation into a simple generation process. ", "page_idx": 18}, {"type": "text", "text": "This design is based on the observation that early denoising steps of the generative model can determine the image content to a large extent as Fig. 12 shows. With only 5 initial steps (over a total of 25 denoising steps) using \u201cdog\u201d as guidance (the 6th image in Fig. 12, the image content is already fixed as \u201cdog\u201d, which means the influence of later denoising steps using \u201ccar\u201d has very low influence to the image content generation. ", "page_idx": 18}, {"type": "text", "text": "Therefore, we can utilize this characteristic of the diffusion model to constrain spatial layout with AID in the early stage of denoising and then transit to self-generation with the guided prompt to refine the details. ", "page_idx": 18}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/c58eae3c8f00e6e0458378ddef18e906b6b6dfe491130e452b7ab6d4d0f70710.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 12: Effect of early denoising steps. The images are generated using 25 denoising steps. For the ith image shown in the row from left to right, it is generated by using \u201cA photo of dog, best quality, extremely detailed\u201d in the first $i-1$ denoising steps, then generated by using $^{\\epsilon\\epsilon}A$ photo of car, best quality, extremely detailed\u201d for the rest denoising steps. ", "page_idx": 19}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/a3df1a4ec2cf38bfea398e44f74b2cf9cb7b6696d0336ffbb5cbf3e0b4242c9c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 13: Screenshot of the survey layout. The user is prompted to choose the best interpolation sequence with high smoothness, consistency, and fidelity. ", "page_idx": 19}, {"type": "text", "text": "F Auxiliary Experiments Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Hardware Environments. All quantitative and qualitative experiments presented in this work are conducted on a single H100 GPU and Float16 precision. ", "page_idx": 19}, {"type": "text", "text": "Perceptual Model Used in Evaluation Metrics. For consistency and smoothness, we follow conventional settings and choose VGG16 [41] to compute LPIPS [57]. For fidelity, we adapt the Google v3 Inception Model [44] following previous literature to compute FID between source images and interpolated images. ", "page_idx": 19}, {"type": "text", "text": "Datasets. We introduce the details of CIFAR-10 and LAION-Aesthetics used for evaluating conditional interpolation here. ", "page_idx": 19}, {"type": "text", "text": "\u2022 CIFAR-10: The CIFAR-10 dataset [22] comprises $60{,}000\\;32\\mathrm{x}32$ color images distributed across 10 classes. This dataset is commonly used to benchmark classification algorithms. In our context, we utilize the class names as prompts to generate images corresponding to specific categories. The CIFAR-10 corpus aids in assessing the effectiveness of our framework, PAID, in handling brief prompts that describe clear-cut concepts. ", "page_idx": 19}, {"type": "text", "text": "\u2022 LAION-Aesthetics: We sample the LAION-Aesthetics dataset from the larger LAION5B collection [39] with aesthetics score over 6, curated for its high visual quality. Unlike CIFAR-10, this dataset provides extensive ground truth captions for images, encompassing lengthy and less direct descriptions. These characteristics present more complex challenges for text-based analysis. We employ the dataset to test our framework\u2019s interpolation capabilities in more demanding scenarios. ", "page_idx": 19}, {"type": "text", "text": "Selection Configuration. In terms of Bayesian optimization on $\\alpha$ and $\\beta$ in the beta prior to applying our selection approach, we set the smoothness of the interpolation sequence as the objective target, [1, 15] as the range of both hyperparameters, 9 fixed exploration where $\\alpha$ and $\\beta$ are chosen from $\\{10,\\bar{1}2,14\\}$ , and 15 iterations to optimize. ", "page_idx": 19}, {"type": "text", "text": "Denoising Interpolation. Denoising interpolation interpolates the images along the schedule. Specifically, given prompt A and prompt B and the number of denoising steps $N$ , for an interpolation coefficient $t$ we guide the generation with prompt A for the first $\\lfloor t N\\rfloor$ steps and guide with prompt B for the rest of steps. ", "page_idx": 19}, {"type": "text", "text": "Human Evaluation Details. To minimize bias towards a particular style, we included an equal number of photorealistic and artistic prompts for each category. We conducted 320 trials in total. In each trial, an independent rater from Amazon Mechanical Turk evaluated the results and chose the best one among AID-I, AID-O, and text embedding interpolation (TEI). We show the layout of the human study survey in Fig. 13. For near object, the prompt is sampled from: $\\{[^{\\leftarrow}\\mathrm{a}~\\mathrm{dog}^{\\ast}$ , \u201ca cat\u201d], [\u201ca jeep\u201d, \u201ca sports car\u201d], [\u201ca lion\u201d, \u201ca tiger\u201d], [\u201ca boy with blone hair\u201d, \u201ca boy with black hair\u201d]}; ", "page_idx": 19}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/2549ab0a4c0f87dbc07b8c7421436485ab543fe32515b572df8eb5d7574eb60f.jpg", "img_caption": ["Figure 14: Our method combined with the inversion method [48] or IP-Adapter [54] can be further applied to several downstream tasks including image editing, image-control generation and image morphing. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "For far object, the prompt is sampled from: {[\u201can astronaut\u201d, \u201ca horse\u201d], [\u201ca girl\u201d, \u201ca ballon\u201d], [\u201ca dragon\u201d, \u201ca banana\u201d], [\u201ca computer\u201d, \u201ca ship\u201d], [\u201ca deer\u201d, \u201can airplane\u201d]}; For scene, the prompt is sampled from: {[\u201csunset\u201d, \u201cmoonlit night\u201d], [\u2019moonlit night\u2019, \u2019forest\u2019], [\u201cforest\u201d, \u201clake\u201d], [\u2019lake\u2019, \u2019sunset\u2019]}; For scene and object, the prompt is sampled from: {[\u201ca robot\u201d, \u201csea of flowers\u201d], [\u201ca deer\u201d, \u201curban street\u201d], [\u201csea of flowers\u201d, \u201ca deer\u201d], [\u201curban street\u201d, \u201ca robot\u201d]}. ", "page_idx": 20}, {"type": "text", "text": "G Details of Applications ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we introduce to adapt our methods for four applications including composition generation (interpolation between text conditions), image editing (interpolation from image condition to multi-modal condition), image morphing (interpolation between image conditions) and imageconditioned generation (interpolation from text condition to multi-modal condition), as Fig. 14 shows, where the content in orange box represents the input, and the content in blue box represents the output. We combine our method with IP-Adapter [54] for the application of image-control generation and image morphing. ", "page_idx": 20}, {"type": "text", "text": "G.1 Composition Generation: Text to Text ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "CEBM [25] interprets diffusion models as energy-based models in which the data distributions defined by the energy functions can be combined, and generate compositional images by considering the generation from each condition separately and combining them at each denoising step. ", "page_idx": 20}, {"type": "text", "text": "RRR [7] concludes that the sampler (not the model) is responsible for the failure in compositional generation and proposes new samplers, inspired by MCMC, which enables successful compositional generation, with an energy-based parameterization of diffusion models which enables Metropoliscorrected samplers. ", "page_idx": 20}, {"type": "text", "text": "Datasets. We use the same dataset for human evaluation introduced in Sec. F. ", "page_idx": 20}, {"type": "text", "text": "G.2 Image Editing Control: Image to $\\{\\mathbf{Text+Image}\\}$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "P2P [11]. controls the prompt-to-prompt editing, i.e., editing on synthesized images while keeping spatial layout by borrowing the cross-attention map, i.e., query and key, from the generation of the edited image. P2P enables editing the focus content while keeping the spatial layout the same for synthesized images. ", "page_idx": 20}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/bd8f67410e14146e082ce4fdc4cc3a13cddb95f2544cad56cb03825f1119f441.jpg", "img_caption": ["Figure 15: Results of image morphing. We compare our method (first row) with IMPUS [53] (second row) and the method by Wang et al. [49] (third row). Our approach achieves comparable or superior performance to IMPUS, but with only 1/100th of the time cost. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "$\\mathbf{P2P+AID}.$ . For a given source prompt generating source images, we view another generation trajectory of edited images as a whole and apply AID on the two-generation trajectory. Specifically, with P2P, the interpolation of AID between cross-attention only happens at the value vector while other components remain the same as the original method. ", "page_idx": 21}, {"type": "text", "text": "EDICT [48]. re-conceptualizes the denoising process through a series of coupled transformation layers, with each inversion process mirrored as such transformations. We denote the comprehensive process in EDICT of generating image $I$ from latent $z$ under prompt $C$ as $E_{f}(z,C)$ , and its inverse as $z=E_{i}(I,C)$ . AID is applied within these coupled layers during the denoising phase. We explore two applications: editing control and video frame interpolation. ", "page_idx": 21}, {"type": "text", "text": "EDICT $^+$ AID. For a given image $I_{1}$ with source prompt $C_{1}$ and target prompt $C_{t}$ , we first derive its latent representation $z_{1}=E_{i}(I_{1},C_{1})$ by EDICT. To interpolate $m$ images between $C_{1}$ and $C_{t}$ , we replicate $z_{1}$ across $z_{1:m}$ and employ AID for sequence generation. ", "page_idx": 21}, {"type": "text", "text": "Dataset. We follow the same dataset presented in [48] for quantitative evaluation. For synthesized images, each data is presented as a source prompt and an editing prompt. For real images, each data is presented in a source image and editing prompt. Specifically, images of five classes \u201cAfrican Elephant, Ram, Egyptian Cat, Brown Bear, and Norfolk Terrier\u201d from ImageNet [4] are taken and then we conduct four types of experiments: one involves editing \u201ca photo of $\\{\\mathrm{animal\\;1\\}^{\\circ}$ to \u201ca photo of $\\{\\mathrm{animal}\\ 2\\}^{\\ast}$ (resulting in 20 species editing pairs in total); two involve contextual changes (\u201ca photo of {animal} in the snow\u201d and \u201ca photo of {animal} in a parking lot\u201d); and one involves a stylistic change (\u201can impressionistic painting of the {animal}\u201d). When this is applied to synthesized images, we use \u201ca photo of the {animal}\u201d as the source prompt. ", "page_idx": 21}, {"type": "text", "text": "G.3 Image Morphing: Image to Image ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Image morphing involves smoothly transitioning from one image to another by blending and aligning key features. Traditional approaches often require fine-tuning pretrained text-to-image diffusion models on individual samples [53, 55], which can be computationally intensive and time-consuming. In contrast, our method provides a training-free solution, allowing seamless image transitions without the need for fine-tuning. We compare our approach with [51, 53]. ", "page_idx": 21}, {"type": "text", "text": "AID for Image Morphing. We build on IP-Adapter [54], which adapts text-to-image diffusion models to accommodate multi-modal conditions, including image and text inputs. IP-Adapter manages image prompts through cross-attention layers, enabling our method to naturally extend to image morphing without additional training. For each morphing task, the provided image serves as the prompt in IP-Adapter, with a null text input. Unlike approaches that rely on model fine-tuning [55, 53], our method operates entirely training-free, making it a more efficient alternative while delivering high-quality results. ", "page_idx": 21}, {"type": "text", "text": "Preliminary Results. As shown in the first row of Fig. 15, our method produces smooth, consistent transitions for real-world images. In comparison to the previous training-free approach by [49] (third row in Fig. 15), our method achieves improved fidelity in the interpolated images. Furthermore, compared to IMPUS [53] (second row in Fig. 15), which requires fine-tuning Stable Diffusion with LoRA [14]\u2014taking approximately one hour per sample on a single A100 GPU\u2014our approach is training-free, generating the entire interpolation sequence in about six minutes, achieving competitive performance with significantly lower computational cost. ", "page_idx": 21}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/5395f3439439a7706253c3e7272f852823a18f5861ade45b4ece393aef098dd9.jpg", "img_caption": ["Figure 16: Results of image-conditioned generation. IP-Adapter (second row) has difficulty properly scaling the influence of the additional image condition (see the last column in (a) and (b)). Our method (first row) achieves smoother control and greater subject consistency, particularly evident in the statue\u2019s hair in (a). "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "G.4 Image-Conditioned Generation: Text to $\\mathbf{\\{Text+Image\\}}$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Image-conditioned generation has emerged as an approach for guiding text-to-image models using supplementary control signals that are challenging to express through text alone, such as layout, subject, and style. Existing methods, including ControlNet [56] and IP-Adapter [54], often struggle to balance these additional conditions effectively, particularly when scaling their influence. Our method addresses these limitations, specifically improving upon IP-Adapter, which frequently fails to maintain a balance between the image and text inputs when both are used as conditions. ", "page_idx": 22}, {"type": "text", "text": "AID for Image-Conditioned Generation. Our approach utilizes the IP-Adapter integrated with the AID framework, similarly to its use in image morphing. For image-conditioned generation, however, we begin with a null image prompt alongside a text prompt, and at the interpolation endpoint, we incorporate both the image and text prompts. ", "page_idx": 22}, {"type": "text", "text": "Preliminary Results. Fig. 16 compares our method (first row) with IP-Adapter (second row). The leftmost images are generated without the additional image condition, while the rightmost images show the maximum influence of the image condition. IP-Adapter fails to maintain subject consistency, as shown by the statue\u2019s inconsistent hair in (a), and struggles to scale the image condition effectively, especially in (b). In contrast, our method offers more consistent and smoother control over the subject, ensuring a better balance between image and text conditions. ", "page_idx": 22}, {"type": "text", "text": "H Auxiliary Qualitative Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We show more qualitative results here using prompt guidance with inner attention interpolation. In this section, the results are obtained with Stable Diffusion 1.5 [35] and UniPCMultistepScheduler [58]. To enhance the visual ability, we use the negative prompt \u201cmonochrome, lowres, bad anatomy, worst quality, low quality\u201d. To trade-off between perceptual consistency and effectiveness of prompt guidance, we use the first 10 denoising steps over 50 total denoising steps of Uni for warming up. As Fig. 17 and Fig. 18 show, our methods can generate image interpolation on different concepts and paintings. We provide more examples in Fig. 19. And we provide more results obtained by SDXL [30] and Animagine 3.0 [23] in Fig. 20. ", "page_idx": 22}, {"type": "text", "text": "I Distinction with Concurrent Works on Image Morphing ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "There are two concurrent works [55, 53] that also focus on deep interpolation but the objective is different where their objective is on real-world image morphing where the main challenge is to make the interpolation between real images is as good as the interpolation between generated images. On the contrary, we focus on improving the quality of generative interpolation, which can be further used in their framework. ", "page_idx": 22}, {"type": "text", "text": "IMPUS [53] specializes in generating image morphing through uniform perceptual sampling. The process begins with applying textual inversion to derive text embeddings, followed by fine-tuning the model with LoRA [14] based on specific image content. The generation of image sequences is executed sequentially, ensuring the images are uniformly distributed. ", "page_idx": 22}, {"type": "text", "text": "DiffMorpher [27] initiates its process by training with LoRA, utilizing both prompts and source images. The method extends beyond simple interpolation of text embedding and latent space by also interpolating within LoRA parameters. They also explore attention interpolation, specifically within the realm of inner attention. They observe that attention interpolation across all denoising steps can introduce artifacts, which makes them adopt it with interpolating with other multiple components. On the contrary, we find that combining outer attention interpolation with self-attention fusing can significantly address this problem, and the performance is boosted without any fine-tuning, which emphasizes the difference. ", "page_idx": 23}, {"type": "text", "text": "IMPUS and DiffMorpher lack control over the specific interpolation path due to their reliance on interpolated text embeddings. Conversely, our method can plug in to allow precise control over the interpolation path. ", "page_idx": 23}, {"type": "text", "text": "Furthermore, IMPUS and DiffMorpher necessitate fine-tuning during the testing phase to achieve optimal performance to tackle the challenges from real images, requiring thousands of iterations to optimize LoRA or text embeddings for each interpolation sequence. Our method is more efficient for downstream tasks such as editing and video frame interpolation in a training-free manner. ", "page_idx": 23}, {"type": "text", "text": "J Limitation and Social Impact ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Limitation. Our method is post-hoc performed on a text-to-image diffusion model and the results are dependent on the ability of the base model. ", "page_idx": 23}, {"type": "text", "text": "Social Impact. Our method offers control over training-free image editing methods which initially have nearly no such ability. This is impactful to the practical usage of the text-to-image diffusion model. However, our method also increases the compositional generation ability of the text-to-image model, which may make deepfake harder to detect. ", "page_idx": 23}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/a91e3770cc86d21512c884f8f4c30de252f190b4f1f137413d900a67681e6ce8.jpg", "img_caption": ["Figure 17: Qualitative results of interpolation between animal concepts. For an animal, we use \u201cA photo of $\\{a n i m a l\\_n a m e\\}$ , high quality, extremely detailed\u201d to generate the corresponding source images. The guidance prompt is formulated as \u201cA photo of an animal called $\\{a n i m a l\\_n a m e\\_A\\}$ -{animal_name $.B\\}$ , high quality, extremely detailed\u201d. PAID enables a strong ability to create compositional objects. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/85b603f58d3373bb8cd900cf1838e7e8a19d16849ec25efe5b2fbbac777ae8ad.jpg", "img_caption": ["Figure 18: Qualitative results of interpolation between different paintings. For a painting, we use \u201cA painting of painting_name, high quality, extremely detailed\u201d to generate the source images. The guided prompt is generated by GPT-4 [28] given description of source images, e.g., the guided prompt for the second row is \u201cA painting of Mona Lisa under Starry Night, high quality, extremely detailed\u201d. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/e3bff7c473a044c94ee4bfa7fc0fe220049d0fb241e48be1f64c8c3e2c6c9005.jpg", "img_caption": ["Figure 19: More qualitative results generated by SD 1.5. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "Nb5xlelV0C/tmp/e1c056f5111f9a7177a00dbf9a6f4320742511f7553c83e1040fe1535772fd6b.jpg", "img_caption": ["Figure 20: More qualitative results generated by Animagine 3.0 [23] (the 1st row) and SDXL (from 2nd to 9th rows). "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 27}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 27}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 27}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 27}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. IMPORTANT, please: ", "page_idx": 27}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We discuss the limitation in the Appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide the full experiments setting only relying on open-source data and models in the Appendix. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We provide detailed experiment details in Sec. 5.1 and Appendix ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 29}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide the full settings in Sec. 5.3 and the Appendix for hyper-parameter selection and inference details. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide the compute resource in the Appendix ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We discuss the impacts in the Appendix ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 31}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] Justification: [NA] ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] Justification: [NA] ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] Justification: [NA] Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]