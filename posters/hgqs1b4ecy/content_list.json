[{"type": "text", "text": "Inferring Neural Signed Distance Functions by Overfitting on Single Noisy Point Clouds through Finetuning Data-Driven based Priors ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chao Chen1 Yu-Shen Liu1\u2217 Zhizhong Han2 ", "page_idx": 0}, {"type": "text", "text": "1School of Software, Tsinghua University, Beijing, China   \n2Department of Computer Science, Wayne State University, Detroit, USA   \nchenchao19@tsinghua.org.cn liuyushen@mails.tsinghua.edu.cn h312h@wayne.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "It is important to estimate an accurate signed distance function (SDF) from a point cloud in many computer vision applications. The latest methods learn neural SDFs using either a data-driven based or an overftiting-based strategy. However, these two kinds of methods are with either poor generalization or slow convergence, which limits their capability under challenging scenarios like highly noisy point clouds. To resolve this issue, we propose a method to promote pros of both data-driven based and overftiting-based methods for better generalization, faster inference, and higher accuracy in learning neural SDFs. We introduce a novel statistical reasoning algorithm in local regions which is able to finetune data-driven based priors without signed distance supervision, clean point cloud, or point normals. This helps our method start with a good initialization, and converge to a minimum in a much faster way. Our numerical and visual comparisons with the state-of-the-art methods show our superiority over these methods in surface reconstruction and point cloud denoising on widely used shape and scene benchmarks. The code is available at https://github.com/chenchao15/LocalN2NM. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "It is an important task to estimate an implicit function from a point cloud in computer graphics, computer vision, and robotics. An implicit function, such as a signed distance function (SDF), describes a continuous 3D distance field to indicate distances to the nearest surfaces at arbitrary locations. Since point clouds are easy to obtain, they are widely used as an information source to estimate SDFs, particularly without using normals that are not available for most scenarios. The challenge for SDF estimation mainly comes from the difficulty of bridging the gap between the discreteness of point clouds and the continuity of implicit functions. ", "page_idx": 0}, {"type": "text", "text": "Recent methods [62, 64, 29, 14, 95, 80, 58, 74] overcome this challenge using either a data-driven based or an overftiting-based strategy. To map a point cloud to a signed distance field, the data-driven based methods [60, 27, 36, 45, 81, 79, 22, 42, 92, 83] rely on a prior learned with signed distance supervision from a large-scale dataset, while the overfitting-based methods [28, 1, 102, 2, 99, 4, 21, 50, 18, 88] do not need signed distance supervision and just use the point cloud to infer a signed distance field. However, both of the two kinds of methods have pros and cons. The data-driven based methods can do inference fast but suffers from the need of large-scale training samples and poor generalization to instances that are unseen during training. Although the overfitting-based methods have a better generalization ability and do not need the large-scale signed distance supervision, they usually require a much longer time to converge during inference. The cons of these two kinds of methods dramatically limit the performance of learning neural SDFs under challenging scenarios like highly noisy point clouds. Therefore, beyond pursuing higher accuracy of SDFs, how to balance the generalization ability and the convergence efficiency is also a significant issue. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To resolve this issue, we propose to learn an SDF from a single point cloud by finetuning data-driven based priors. Our key idea is to promote the advantages of both the data-driven based and the overftiting-based strategy to pursue better generalization, faster inference, and higher accuracy. Our method overfits a neural network on a single point cloud to estimate an SDF with a novel loss without using signed distance supervision, clean point, or point normals, where the neural network was pretrained as a data-driven based prior from large-scale signed distance supervision. With finetuning priors, our method can generalize better on unseen instances than the data-driven based methods, and also converge much more accurate SDFs in a much faster way than the overftiting-based methods. Moreover, our novel loss for finetuning the data-driven based prior can conduct a statistical reasoning in a local region which can recover more accurate and sharper underlying surface from noisy points. We report numerical and visual comparisons with the state-of-the-art methods and show our superiority over these methods in surface reconstruction and point cloud denoising on widely used shape and scene benchmarks. Our contributions are summarized below, ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce a method which is capable of funetuning a data-driven based prior by minimizing an overfitting-based loss without signed distance supervision, leading to neural SDFs with better generalization, faster inference, and higher accuracy.   \n\u2022 The proposed overftiting-based loss can conduct a novel statistical reasoning in local regions, which improves the accuracy of neural SDFs inferred from noisy point clouds.   \n\u2022 Our method produces the state-of-the-art results in surface reconstruction and point cloud denoising on the widely used benchmarks. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Learning implicit functions has achieved promising performance in various tasks [62, 64, 29, 14, 95, 80, 58, 74, 30, 31, 33]. We can learn neural implicit representations from different supervision including 3D supervision [61, 69, 59, 17], multi-view images [78, 44, 38, 101, 46, 94, 63, 41, 98, 97, 25, 86, 100, 89, 84, 85], and point clouds [92, 43, 60, 27]. We briefly review the existing methods related to point clouds below. ", "page_idx": 1}, {"type": "text", "text": "2.1 Data-driven based Methods ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In 3D supervision, many techniques utilize a data-driven approach to learning priors, and then apply these learned priors to infer implicit models for unseen point clouds. Some strategies focus on acquiring global priors [60, 27, 36, 45, 81, 79, 22, 42] at the shape level, whereas others aim to boost the generalization of these priors by learning local priors [92, 83, 11, 37, 6, 51] at the component or patch level. These learned priors facilitate the marching cubes algorithm [47] to reconstruct surfaces from implicit fields. The effectiveness of these methods often rely on extensive datasets, but they may not generalize well when facing with unseen point clouds that significantly deviate in geometry from training samples. ", "page_idx": 1}, {"type": "text", "text": "2.2 Overfitting-based Methods ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In an effort to enhance generalization, some methods concentrate on precisely ftiting neural networks to single point clouds. These methods incorporate innovative constraints [28, 1, 102, 2, 99, 4, 21], utilize gradients [50, 18, 88], employ differentiable Poisson solvers [70], or apply specially tailored priors [51, 54] to learn either signed [50, 28, 1, 102, 2, 15, 56, 13] or unsigned distance functions [18, 104, 103]. Despite achieving significant advances, these approaches typically require clean point clouds to accurately determine distance or occupancy fields around the point clouds. ", "page_idx": 1}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/980f67b121d9518e4af7e1a3ce6a5452b8858e2a28eb1148969b158da9f5b8cb.jpg", "img_caption": ["Figure 1: The overview of our method. We learn the data-driven based prior by learning a neural implicit function $f^{\\prime}$ with a condition $c^{\\prime}$ on a clean dataset. During inference, we employ a novel statistical reasoning algorithm to infer a neural SDF $f$ for a noisy point cloud $M$ with learned prior (average code and learned parameter). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2.3 Learning from Noisy Point Clouds ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The key to accurately reconstructing surfaces on noisy point clouds is to minimize the effect of noise in inferring implicit functions. PointCleanNet [73] was developed to fliter out noise from point clouds through a data-driven approach. GPDNet [72] incorporated graph convolution based on dynamically generated neighborhood graphs to enhance noise reduction. Some other methods leveraged point cloud convolution [6], alternating latent topology [90, 57], semi-supervised strategy [106, 19], dual and integrated latent [76], or neural kernel field [91, 35] to reduce noise from point clouds. On the unsupervised front, TotalDenoising [10] adopts principles similar to Noise2Noise [40], utilizing a spatial prior suitable for unordered point clouds. DiGS [3] employs a soft constraint for unoriented point clouds. Noise2NoiseMapping [52] leverage statistical reasoning among multiple noisy point clouds with specially designed losses. Some methods using downsample-upsample frameworks [48], gradient fields [49, 9, 16, 68, 65], convolution-free intrinsic occupancy network [67], intra-shape regularization [66], eikonal equation [96, 23], neural Galerkin [34] and neural splines [93] have been implemented to further diminish noise in point clouds. Our method falls in this category, but we aim to promote the advantages of both the data-driven based and the overftiting-based strategy to pursue better generalization, faster inference, and higher accuracy. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Overview. We aim to infer a neural SDF $f$ from a single point cloud with noises $M$ . Our method includes two stages shown in Fig. 1, one is to learn a prior $f^{\\prime}$ in a data-driven manner, the other is to infer a neural SDF $f$ on unseen noisy point cloud $M$ . At the first stage, we learn a prior by training a neural SDF using ground truth signed distances of clean meshes indicated by embeddings $\\pmb{c}_{j}^{\\prime}$ . At the second stage, we finetune the learned prior $f^{\\prime}$ to infer a neural SDF $f$ of $M$ using our proposed local noise to noise mapping, where the embedding $^c$ indicating $M$ is also learned. We can use the marching cubes algorithm [47] to extract the zero-level set of $f$ as the mesh surface of $M$ . ", "page_idx": 2}, {"type": "text", "text": "Neural Signed Distance Function. We leverage an SDF $f$ to represent the geometry of a shape. An SDF $f$ is an implicit function that can predict a signed distance $s$ for an arbitrary location $q$ , i.e., $s\\,=\\,f(q)$ . The latest methods usually train a neural network to approximate an SDF from signed distance supervision or infer an SDF from 3D point clouds or multi-view images. A level set is an iso-surface formed by the points with the same signed distance value. For instance, zero-level set is a special level set, which is formed by points with a signed distance of 0. On the zero-level set, the gradient $\\nabla f(q)$ of the SDF $f$ at an arbitrary location $q$ is also the surface normal at $q$ . ", "page_idx": 2}, {"type": "text", "text": "Data-driven Based Prior. As shown in Fig. 1, we employ an auto-decoder similar to DeepSDF [69] for learning a prior $f^{\\prime}$ in a data-driven manner and inferring a neural SDF $f$ for single point clouds with noises, respectively. We employ a data-driven strategy to learn a prior $f^{\\prime}$ from clean meshes first. Specifically, we learn $f^{\\prime}$ with an embedding $\\pmb{c}_{j}^{\\prime}$ as a condition of queries. For each shape, we sample queries $q$ around a shape represented by $\\pmb{c}_{j}^{\\prime}$ , and establish the signed distance supervision by recording the signed distance $s$ to the ground truth mesh. Thus, we learn the prior $f^{\\prime}$ by minimizing the prediction errors to the ground truth signed distances, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f^{\\prime},\\{c_{i}^{\\prime}\\}}\\sum_{i=1}^{I}\\sum_{j=1}^{J}\\|s_{i}^{j}-f^{\\prime}(q_{j},c_{i}^{\\prime})\\|_{2}^{2}+\\alpha\\sum_{i=1}^{I}\\|{\\pmb{c}}_{i}^{\\prime}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pmb{c}_{i}^{\\prime}$ is a learnable condition for the $i$ -th training shape, $q_{j}$ is the $j$ -th query that is randomly sampled around the $i$ -th shape, and $s_{i}^{j}$ is the ground truth signed distance. We also add a regularization term on the learned embeddings $\\pmb{c}_{i}^{\\prime}$ , and $\\alpha$ is the balance weight. ", "page_idx": 3}, {"type": "text", "text": "Signed Distance Inference. With the learned prior $f^{\\prime}$ , we infer a neural SDF $f$ for a single point cloud with noises $M$ . We do not require ground truth signed distances, clean point clouds, or even point normal during the inference of $f$ . Specifically, we infer $f$ by finetuning parameters of $f^{\\prime}$ with a learnable embedding $^c$ indicating the single point cloud with noises. The finetuning relies on a novel statistical reasoning algorithm on local regions. ", "page_idx": 3}, {"type": "text", "text": "The advantage of our method lies in the capability of conducting the statistical reasoning in local regions. Comparing to the global reasoning method [52], our method is able to not only infer more accurate geometry but also significantly improve the efficiency. Our method starts from randomly sampling a local region $m_{n}$ on the shape $M$ . We randomly select one point on $M$ as the center of $m_{n}$ , and set up its $K$ nearest noisy points as a local region $m_{n}$ . Then, we randomly sample $U$ queries $\\{\\bar{q}_{u}\\}_{u=1}^{U}$ around $m_{n}$ , and also randomly select $U$ noisy points $\\{p_{v}\\}_{v=1}^{U}$ out of $m_{n}$ for statistically reasoning the surface in each iteration. ", "page_idx": 3}, {"type": "text", "text": "Our key idea of inferring a neural SDF $f$ is to estimate a mean zero-level set that is consistent to all points in the local region $m_{n}$ . To this end, we use the $U$ sampled queries $\\left\\{\\bar{q}_{u}\\right\\}$ to represent the zero-level set in this area using $f$ , and minimize the distances of the $U$ noisy points $\\{p_{v}\\}$ to the zero-level set in each iteration. Statistically, the expectation of the zero-level set should have the minimum distance to all the noisy point splitting in region $m_{n}$ . ", "page_idx": 3}, {"type": "text", "text": "Specifically, we first project the $U$ sampled queries $\\left\\{\\bar{q}_{u}\\right\\}$ onto the zero-level set of $f$ using a differentiable pulling operation [50]. For each query $\\bar{q}_{u}$ , its projection on the zero-level set is, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{q}_{u}^{\\prime}=\\bar{q}_{u}-s*\\nabla f(\\bar{q}_{u},c)/|\\nabla f(\\bar{q}_{u},c)|,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\bar{q}_{u}^{\\prime}$ is the projection of $\\bar{q}_{u}$ on the zero-level set, $s=f(\\bar{q}_{u},c),\\nabla f(\\bar{q}_{u},c)$ is the gradient of $f$ at the location $\\bar{q}_{u}$ , and $^c$ is the learnable embedding that represents the noisy point cloud $M$ . ", "page_idx": 3}, {"type": "text", "text": "With the pulling operation, we can use projections $\\{\\bar{q}_{u}^{\\prime}\\}$ of queries $\\left\\{\\bar{q}_{u}\\right\\}$ to approximate the zero-level set in region $m_{n}$ . With a coarse zero-level set estimation, we expect this zero-level set can be consistent to various subsets of noises $\\{p_{v}\\}$ sampled from $m_{n}$ . Thus, we minimize the errors between the $\\{\\bar{q}_{u}^{\\prime}\\}_{u=1}^{U}$ and a subset of points $\\{p_{v}\\}_{v=1}^{U}$ on area $m_{n}$ in each optimization iteration, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f,c}\\mathbb{E}_{m_{n}\\sim M,\\bar{q}_{u}\\sim m_{n},p_{v}\\sim m_{n}}E M D(\\{\\bar{q}_{u}^{\\prime}\\},\\{p_{v}\\})+\\beta||c||_{2}^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we learn $f$ through finetuning the prior $f^{\\prime}$ and learning the embedding $^c$ representing the noisy point cloud $M$ . The expectation is over the local regions $m_{n}$ that randomly sampled from the noisy point cloud $M$ , and the subset patch $p_{v}$ randomly sampled from each $m_{n}$ . We follow the method [52] to use the EMD to evaluate the distance between the two sets of points, which leads the neural SDF $f$ to converge on the specific noisy point cloud $M$ . ", "page_idx": 3}, {"type": "text", "text": "Initialization. The network architecture of $f$ is the same to the one of prior $f^{\\prime}$ . We learn $f$ with the parameters of $f^{\\prime}$ as the initialization, representing the prior that we learned. For the embedding $^c$ that represents $M$ , we initialize $^c$ as the center of the embedding space learned by the prior $f^{\\prime}$ in Eq. 1, i.e., $\\begin{array}{r}{\\pmb{c}=1/I\\sum_{i=1}^{I}\\pmb{c}_{i}^{\\prime}}\\end{array}$ . This initialization is important for the accuracy and efficiency of learning $f$ for single no isy point cloud . This finetuning of parameters of $f^{\\prime}$ also shows advantages over the auto-decoding [69] in terms of generalization and efficiency. We will justify these advantages in our experiments. ", "page_idx": 3}, {"type": "text", "text": "Implementation Details. We randomly select one point from noisy point cloud $M$ as a center, and select its $K=1000$ nearest points to form a local region $m_{n}$ . We also randomly sample $U=1000$ queries around the $K$ noisy points for statistically reasoning. Specifically, we adopt a method introduced by NeuralPull [50] to sample queries around each one of the $K$ noisy points. We use a Gaussian distribution centered at each point and set the standard deviation as the distance to the 51th nearest neighbor in the point cloud. We run the marching cubes for surface reconstruction at a resolution of 256 for shapes, and 512 for large-scale scenes. ", "page_idx": 3}, {"type": "table", "img_path": "Hgqs1b4ECy/tmp/0e22d5c70981679c67fd6ff26482d25853870957888a48b43e5fcb225c1d6552.jpg", "table_caption": [], "table_footnote": ["Table 1: Numerical Comparisons on ShapeNet dataset in terms of $\\overline{{C D_{L1}\\times10}}$ , NC and F-Score. "], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "The length of the embedding $^c$ or $c^{\\prime}$ is set to 256. We use Adam optimizer for learning a neural implicit network, which is an auto-decoder similar to DeepSDF [69]. For training, we use an initial embedding learning rate of 0.0005 for updating embeddings and an auto-decoder learning rate of 0.001 for optimizing the prior network. Both learning rates are decreased by 0.5 for every 500 epochs. We train the prior network $f^{\\prime}$ for 2000 epochs. For inference, we finetune the network $f^{\\prime}$ for each noisy point cloud in 4000 iterations with a learning rate of 0.0001. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments and Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We compare our method with the latest methods in terms of numerical and visual results on synthetic point clouds and real scans in surface reconstruction. ", "page_idx": 4}, {"type": "text", "text": "Datasets and Metric. We use eight datasets including shapes and scenes in the evaluations. For shapes, we conduct experiments under five datasets including ShapeNet [12], ABC [22], FAMOUS [22], Surface Reconstruction Benchmark (SRB) [92] and D-FAUST [5]. For scenes, we conduct experiments under three real scan datasets including 3D Scene [105], KITTI [26], Paris-rueMadame [75], and nuScenes [8]. We leverage L1 Chamfer Distance $(C D_{L1})$ , L2 Chamfer Distance $(C D_{L2})$ to evaluate the error between the reconstructed surface and ground truth. We also use Normal Consistency (NC) [59] and F-Score [82] with a threshold of $1\\%$ to evaluate the normal accuracy of the reconstructed surface. In the ablation study, we also report time consumption to highlight the superiority of our data-driven based prior. For KITTI and Paris-rue-Madame datasets, due to their lack of ground truth meshes, we only report visual comparisons. ", "page_idx": 4}, {"type": "text", "text": "4.1 Surface Reconstruction for Shapes ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Evaluation on ShapeNet. We first report our results on shapes from ShapeNet. We report evaluations by comparing our method with the latest prior-based and overfitting-based methods in Tab 1. For prior-based methods, we compare our method with PSG [24], R2N2 [20], COcc [71], OCNN [87], IMLS [45], POCO [7], and ALTO [90]. All of these methods are pretrained to learn priors using shapes with noises in training set of ShapeNet. We also follow these meth", "page_idx": 4}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/e97310fdd44fd6c201bb37566880fcc3370907644e910ceb026eb481ffaf9e84.jpg", "img_caption": ["Figure 2: Comparison in surface reconstruction on ShapeNet. More visual results are provided in the appendix. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "ods to use the same set of training shapes to learn our prior. For overfitting-based methods, we compare our method with PSR [39], SAP [70], and N2NM [52]. These methods did not need to learn a prior, and have the ability of inferring neural implicit functions on each shape in the testing set. We also follow these methods and report our results by finetuning our prior through overfitting on each testing shape. All the shapes for testing are corrupted with noises with a variance of 0.005. ", "page_idx": 4}, {"type": "table", "img_path": "Hgqs1b4ECy/tmp/64f25c28387df821e5afccdba7c7b6bb3af4ab7108add448503bdde0dfae5ad4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "The comparisons in Tab. 1 indicate that our method can infer much more accurate neural implicit functions than the prior-based methods. The improvement comes from the ability of conducting test time optimization with the learned prior and inferring signed distances using the local noise to noise mapping. Moreover, our ", "page_idx": 5}, {"type": "text", "text": "Table 2: Time consumption on ShapeNet dataset with overfitting-based methods. ", "page_idx": 5}, {"type": "text", "text": "local statistical reasoning not only achieves better ability of recovering geometry from noisy points than overftiting-based methods but also significantly reduces the time complexity during the test time overftiting procedure with our prior. Different from prior-based methods, our ability of conducting testtime optimization with our local statistical reasoning loss can significantly improve the generalization ability on unseen shapes. Tab. 2 shows that our method can infer neural implicit functions on single shapes much faster than the overftiting-based methods. We also demonstrate our advantages in visual ", "page_idx": 5}, {"type": "text", "text": "Evaluation on ABC. We also report our evaluations on ABC dataset in Tab. 3. We learn priors from shapes in training set, and finetune this prior for each single shape in the testing set. The numerical comparisons are conducted on the testing set of ABC dataset released by P2S [22]. It includes two versions with different ", "page_idx": 5}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/7c74d20ecbcff04287098f3be2c8e5f469ea03158a2a5b0b13f09432cd310dce.jpg", "img_caption": ["Figure 3: Comparison in surface reconstruction on ABC. More visual results are provided in the appendix. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "noise levels. Similarly, we also report comparisons with prior-based methods and overfitting-based methods. With our local noise to noise mapping, we achieve the best performance over all baselines. Compared to prior-based methods, such as P2S [22], COcc [71], and POCO [7], our loss can infer more accurate geometry during the test time overftiting procedure. Also, the ability of finetuning the prior can also provide a coarse estimation and a good start for inferring neural implicit from single noisy points. Besides the accuracy, we also observe improvements on efficiency. Fig. 3 demonstrates the improvements over the baselines in terms of surface completeness and edge sharpness. ", "page_idx": 5}, {"type": "text", "text": "Evaluation on SRB. We report previous experiments using man-made objects in ShapeNet and ABC dataset, We also report our results on real scans on SRB dataset [92]. Since there is no training samples on SRB, we use the prior learned from the ShapeNet as the prior for real scans. Although the shapes in ShapeNet are not similar to shapes in SRB, we found the prior can also work well with the scans on SRB. Different from the man-made objects, real 41 ", "page_idx": 5}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/79688143872342065eaf56563f1c3b5f25a33e7667f6c4d4e088f390db1c8770.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 4: Comparison in surface reconstruction on SRB.   \nMore visual results are provided in the appendix. ", "page_idx": 5}, {"type": "text", "text": "evaluations with the prior-based and overftiting-based methods in Tab. 4 and Fig. 4. The comparisons show that our method achieves the best performance in implicit surface reconstruction. Under the same experimental settings, our method can infer more accurate geometry details with our local noise to noise mapping. ", "page_idx": 5}, {"type": "text", "text": "Evaluation on FAMOUS. We report evaluations on more complex shapes on FAMOUS dataset. Similar to SRB, we also use the prior learned from ShapeNet. We evaluate the performance on two kinds of noises in Tab. 5. We can see that our method can recover more geometry details and achieve higher accuracy and smoother surfaces. We also report visual comparisons in Fig. 5, which also highlights our improvements in terms of accuracy, smoothness, completeness, and recovered sharp edges. ", "page_idx": 5}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/48c3fd628c1ae5502efd4a299251a97fb4aa5a0e0db77057f978eafd8a60c3be.jpg", "img_caption": ["Figure 5: Comparison in surface reconstruction on FAMOUS. More visual results are provided in the appendix. "], "img_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "Hgqs1b4ECy/tmp/79782cdd941bad34a60746dadf8186f52b2c8b85563c63b1c66c69c0946597c0.jpg", "table_caption": ["Table 4: Numerical Comparisons on SRB dataset in terms of $C D_{L1}\\times10$ and F-Score. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Evaluation on D-FAUST. Finally, we report our results on non-rigid shapes, i.e., humans. Different from rigid shapes in the previous experiments, humans are with more complex poses. We learn a prior from the training set, and finetuning the prior on unseen humans with different poses. We mainly compare our method with overfittingbased methods in Tab. 6. We can see that our method achieves the best performance in CD, F-Score, and comparable performance to N2NM [52] but with faster inference speed. We further show the visual comparison in Fig. 6. We can see that our method can recover more accurate geometry and poses. ", "page_idx": 6}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/716c55c10646de553a90b00aa91867c43a5469d841d1290a56c95d001fd3acbd.jpg", "img_caption": ["Figure 6: Comparison in surface reconstruction on D-FAUST. More visual results are provided in the appendix. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Surface Reconstruction for Scenes ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Since we have a limited number of scenes for training, we use the prior learned from ShapeNet as the pretrained prior in our experiments on scenes. Specifically, we conduct experiments on four different scene datasets: 3D Scene [105], KITTI [26], Paris-rue-Madame [75] and nuScenes [8], where the results on nuScenes are reported in the appendix. ", "page_idx": 6}, {"type": "text", "text": "Evaluation on 3D Scene. We further evaluate our method in surface reconstruction for scenes in 3D Scene [105]. We follow previous methods LIG [37] to randomly sample 1000 points per $m^{2}$ . We compare our method with the latest methods including COcc [71] and LIG [37], DeepLS [11], NeuralPull (NP) [50] and Noise2NoiseMapping (N2NM) [52]. For prior-based methods COcc [71] and LIG [37], we leverage their released pretrained models to produce the results, and we also provide them with the ground truth point normals. For overftiting-based methods DeepLS [11], NP [50] and N2NM [52], we overfti them to produce results with the same noisy point clouds. We follow LIG [37] to report $C D_{L1}$ , $C D_{L2}$ and NC for evaluation. We report the comparisons in Tab. 7. The results demonstrate that our method outperforms both kinds of methods with learned priors such as LIG [37] and overftiting-based N2NM [52]. The visual comparisons in Fig. 7 show that our method can reveal more geometry details on real scans, which justifies our capability of handling noise in point clouds. ", "page_idx": 6}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/595343f335f2162a7b5f0a99625202054d87bf965abb905b29332e8d6a97805b.jpg", "img_caption": ["Figure 7: Comparison in surface reconstruction on 3D Scene. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "Hgqs1b4ECy/tmp/747d3bffc4ea39b921e8208c453373522f647df76252275c4d615ff04a718eac.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 6: Accuracy of reconstruction on D-FAUST dataset in terms of $C D_{L1}$ , NC and F-Score. ", "page_idx": 7}, {"type": "text", "text": "Evaluation on KITTI. Following GridPull [16], we further evaluate our method on KITTI [26] odometry dataset (Sequence 00, frame 3000 to 4000), which contains about 13.8 million points, which are split into 15 chunks. We reconstruct each of them and concatenate them together for visualization. We compare our method with the latest methods SAP [70] and GridPull [16]. As shown in Fig. 8, our method is robust to noise in real scans, successfully generalizes to large-scale scenes, and achieves visual-appealing reconstructions with more details. ", "page_idx": 7}, {"type": "text", "text": "Evaluation on Paris-rue-Madame. Following N2NM [52], we further evaluate our method on Paris-rue-Madame [75], which contains much noises. We split the 10 million points into 50 chunks each of which is used to learn a neural implicit function. We compare our method with LIG [37] and N2NM [52]. For LIG [37], we produce the results for each chunk with released pretrained models. For N2NM [52], we overfit on all chunks until convergence. As shown in Fig. 9, we achieve better performance over LIG [37] and N2NM [52] in large-scale surface reconstruction, which highlight our advantages in reconstructing complete and detailed surfaces from noisy scene point clouds. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct ablation studies on the ABC dataset [22] to justify each module of our method. ", "page_idx": 7}, {"type": "text", "text": "Embedding Size. We evaluate our performance on differen sizes of embedding c. We try several sizes $\\{128,256,512\\}$ to infer the signed distance functions from a noisy point cloud. ", "page_idx": 7}, {"type": "table", "img_path": "Hgqs1b4ECy/tmp/349f63a93d657058711d06e5ae9a57847d1768b9cb18d4b31616c38ae7b04863.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "The numerical comparison in Tab. 8 shows that the optimal Table 8: Effect of the embedding size. result is obtained with a size of 256. Deviations from this value, either longer or shorter dimensions, leads to worse results with the current number of training samples. ", "page_idx": 7}, {"type": "text", "text": "Prior. We conduct experiments to explore the importance of data-driven based prior. We first replace our learned embedding $^c$ and parameter ", "page_idx": 7}, {"type": "table", "img_path": "Hgqs1b4ECy/tmp/3c4e7eef31273835c56420f206b75c7426ae0085ba3395f44cfc33a3476880b9.jpg", "table_caption": ["Table 9: Effect of the prior. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "with randomly initialized embedding and parameter, or only replace $^c$ with randomly initialized embedding. As shown in Tab. 9, The degenerated result of \u201cWithout Prior\u201d and \u201cWithout Embed\u201d indicates that directly inferring implicit functions without our prior or learned embedding makes it difficult to accurately learn the surfaces of the noisy point clouds, and also slows the convergence. Then we fix the learned parameters and only optimize the embedding $^c$ , similar to auto-decoding. The results also get worse, as shown in \u201cFixed Param\u201d. ", "page_idx": 7}, {"type": "text", "text": "Local Region Splitting. We further validate the effectiveness of local region splitting strategies. We employ three different splitting strategies in Tab. 10. We first ", "page_idx": 7}, {"type": "table", "img_path": "Hgqs1b4ECy/tmp/f314a0d898f382e26c7c2f80b7e8677b156859966439868bb9e176e9044bef91.jpg", "table_caption": ["Table 10: Effect of splitting strategies. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "split the whole space where the noisy point cloud is located uniformly into multiple voxel blocks, as shown by the result of \u201cVoxel\u201d. The severely degenerated results indicate that this splitting strategy is even worse than the global method N2NM [52], as it results in many empty voxel blocks. Then we randomly select a point from the noisy point cloud as a center to sample all points within a radius of 0.1 as a local region. The result of \u201cSphere (Fixed Size)\u201d slightly degenerates due to some of the spheres containing too few points. In contrast, our splitting strategy, as shown by the result of \u201cSphere (KNN)\u201d, ensures that each local region has enough points to help achieve superior performance. ", "page_idx": 7}, {"type": "table", "img_path": "Hgqs1b4ECy/tmp/f821bcc5ad1373e3e7782009cfab536e095951a38de50be52048ccf1653d3d03.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 7: Numerical Comparisons on 3D Scene dataset in terms of $\\overline{{C D_{L1}}}$ , $\\overline{{C D_{L2}}}$ and NC. Detailed comparisons for each scene are provided in the appendix. ", "page_idx": 7}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/6a09babb3e2d5959c62f03537add1e4d498db90bde63781e623bd9b9eff33ba0.jpg", "img_caption": ["Figure 8: Comparison in surface reconstruction on KITTI. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/386fa13058e61703db5e70b0408e21cbf74933a63c9984a801a667392b539819.jpg", "img_caption": ["Figure 9: Comparison in surface reconstruction on Paris-rue-Madame. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Global and Local. With our learned prior, we compare our performance in global and local mappings with finetuning the priors. We report results obtained with the local noise to noise mapping or the global one during the finetuning. As shown in Tab. 11, the numerical comparison shows that the global ", "page_idx": 8}, {"type": "table", "img_path": "Hgqs1b4ECy/tmp/065ff0b0f229bc8cafd81a883896f7130d8eba4c26eaffba86fd5e160156ac42.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 11: Effect of local mapping. ", "page_idx": 8}, {"type": "text", "text": "mapping struggles to infer local details from noisy point clouds. Moreover, our local prior also converges faster than the global statistical reasoning. ", "page_idx": 8}, {"type": "text", "text": "Local Region Size. We further validate the effectiveness of local region sizes (points number in a local region) in Tab. 12. We use different local region sizes including $\\{500,1000,3000,5000\\}$ . The results show that 1000 is the best. ", "page_idx": 8}, {"type": "table", "img_path": "Hgqs1b4ECy/tmp/50a859f965d88801f3f6faa2c20579ed0595555942cbaa35bebf40f8624b5648.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "SDF initialization. We further validate the effectiveness of different SDF initializations in Tab. 13 and Fig. 10, including random initialization, geometry initialization [1], initialization to a simple square shape, and ours. ", "page_idx": 8}, {"type": "text", "text": "We can see our prior can reconstruct more accurate surfaces from single noisy point clouds in much shorter time than any other initializations. ", "page_idx": 8}, {"type": "text", "text": "Table 12: Effect of local region size. ", "page_idx": 8}, {"type": "table", "img_path": "Hgqs1b4ECy/tmp/fa8b319f70435e4e4f444722578c2c592a246662568543d33f5e2c92cf09f01f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 13: The effect of SDF initialization. ", "page_idx": 8}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/c8d861d340d7517accbd08cbc6b478d7f6ff6d98176ccb500570039e0b5d7448.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 10: Comparison with different SDF initializations. ", "page_idx": 8}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/49288052ce968383d3710d5450d05afe27bc2f6152fb756798a4c3680fa89989.jpg", "img_caption": ["Figure 11: Visual results with nonuniform noises. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Noise Type. We report our performance with various noise types, i.e., impulse noise, quantization noise, Laplacian noise, and Gaussian noise. Visual comparison in Fig. 12 justifies that we can also handle other types of noise quite well. Moreover, we also tried more challenging cases with nonuniform noises which do not have a zero expectation across a shape, like a shape with only a half of points having noises or a shape with several patches having noises. The result in Fig. 11 shows that our method can also handle nonuniform no ", "page_idx": 8}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/c01b0b229a1c127dae578c1671d4be1b89fa213080c9640d6ad0dcfc36c88e60.jpg", "img_caption": ["Figure 12: Visual results with different noise types. ses well. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Noise Level. We report our performance on point clouds with different levels of noise. As shown in Tab. 14, the noise levels of middle and max come from the ABC dataset [22]. The middle indicates noises ", "page_idx": 9}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/e2434d53e4dec2999871d3e3d9ec7c824d99f04e5e7d1f160752bc621ae53c37.jpg", "img_caption": ["Figure 13: Visual comparison with different noise levels. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "with a variance of $0.01L$ , where $L$ is the longest edge of the bounding box. The max indicates noises with a variance of $0.05L$ . Our extreme noise comes with a variance of $0.07L$ . ", "page_idx": 9}, {"type": "text", "text": "The $C D_{L2}$ comparison shows that our results slightly degenerate with max and extreme noise, but still outperform N2NM [52]. The visual results in Fig. 13 indicates that our method is more robust to noises even when the noise variance is as large as $7\\%$ . ", "page_idx": 9}, {"type": "text", "text": "Table 14: Effect of noise level. ", "page_idx": 9}, {"type": "table", "img_path": "Hgqs1b4ECy/tmp/5c4995f661f771ec8723961224e39b95f512c857fbee8ef6ab70da50f424e929.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Sparsity. We report the effect of the sparsity of noisy point clouds. We downsample the noisy point clouds to $25\\%$ and $50\\%$ of their original size to validate the impact of sparsity. The $C D_{L2}$ results in Tab. 15 and visual comparisons in Fig. 14 indicate that our method can handle sparsity in noisy point clouds better than N2NM [52]. Since our data-driven based prior can help to learn a more complete surface and reduce the impacts brought by the sparsity. ", "page_idx": 9}, {"type": "table", "img_path": "Hgqs1b4ECy/tmp/c1c0842d0d8a09feca4051b123e5ac193339f680bf7b45c0bfdc74c87345d315.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/6dcfe1e8d8b751732219358d71bc039a2b6c487b15d5c0aba360b4a30485a7e0.jpg", "img_caption": ["Figure 14: Visual comparison with different point numbers. ", "Table 15: Effect of sparsity. "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "Hgqs1b4ECy/tmp/e14a5a36a34124839e9ec8b75a1001c3d97ad8647a423e3e73ec8b3821f90078.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Time Consumption. Since our method can handle sparsity and require less time as the point number decreases, we conduct an experiment with downsampled noisy points in Tab. 16. Fig. 14 indicates that we can ", "page_idx": 9}, {"type": "text", "text": "Table 16: The comparison of time consumption with different point numbers. ", "page_idx": 9}, {"type": "text", "text": "work well on much fewer points, and also provide an alternative of improving efficiency. ", "page_idx": 9}, {"type": "text", "text": "Optimization. ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We visualize the optimization process in Fig. 15. We reconstruct meshes using the neural SDF $f$ learned in different iterations. We ", "page_idx": 9}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/ca3207ecd9248634a4524a60acdccdecf699383d05bfdc3dfe07cfffa2f50474.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "see that the shape is updated progressively to the ground truth shapes. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose a method to resolve the key problem in inferring SDFs from a single noisy point cloud. Our method can effectively use a data-driven based prior as an initialization, and infer a neural SDF by overfitting on a single noisy point cloud. The novel statistical reasoning successfully infers an accurate and smooth signed distance field around the single noisy point cloud with the data-driven based prior. By finetuning data-driven based priors with statistical reasoning, our method significantly improves the robustness, the scalability, the efficiency, and the accuracy in inferring SDFs from single point clouds. Our experimental results and ablations studies show our superiority and justify the effectiveness of the proposed modules. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Matan Atzmon and Yaron Lipman. SAL: Sign agnostic learning of shapes from raw data. In IEEE Conference on Computer Vision and Pattern Recognition, 2020.   \n[2] Matan Atzmon and yaron Lipman. SALD: sign agnostic learning with derivatives. In International Conference on Learning Representations, 2021.   \n[3] Yizhak Ben-Shabat, Chamin Hewa Koneputugodage, and Stephen Gould. DiGS: Divergence guided shape implicit neural representation for unoriented point clouds. In IEEE Conference on Computer Vision and Pattern Recognition, 2022.   \n[4] Yizhak Ben-Shabat, Chamin Hewa Koneputugodage, and Stephen Gould. DiGS $:$ Divergence guided shape implicit neural representation for unoriented point clouds. CoRR, abs/2106.10811, 2021.   \n[5] Federica Bogo, Javier Romero, Gerard Pons-Moll, and Michael J. Black. Dynamic FAUST: Registering human bodies in motion. In IEEE Computer Vision and Pattern Recognition, 2017.   \n[6] Alexandre Boulch and Renaud Marlet. POCO: Point convolution for surface reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6302\u20136314, June 2022.   \n[7] Alexandre Boulch and Renaud Marlet. Poco: Point convolution for surface reconstruction. In IEEE Conference on Computer Vision and Pattern Recognition, 2022.   \n[8] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.   \n[9] Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, and Bharath Hariharan. Learning gradient fields for shape generation. In European Conference on Computer Vision, 2020.   \n[10] Pedro Hermosilla Casajus, Tobias Ritschel, and Timo Ropinski. Total denoising: Unsupervised learning of 3d point cloud cleaning. In IEEE International Conference on Computer Vision, pages 52\u201360, 2019.   \n[11] Rohan Chabra, Jan Eric Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove, and Richard A. Newcombe. Deep local shapes: Learning local SDF priors for detailed 3D reconstruction. In European Conference on Computer Vision, volume 12374, pages 608\u2013625, 2020.   \n[12] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University \u2014 Princeton University \u2014 Toyota Technological Institute at Chicago, 2015.   \n[13] Chao Chen, Zhizhong Han, and Yu-Shen Liu. Unsupervised inference of signed distance functions from single sparse point clouds without learning priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17712\u201317723, 2023.   \n[14] Chao Chen, Zhizhong Han, Yu-Shen Liu, and Matthias Zwicker. Unsupervised learning of fine structure generation for 3D point clouds by 2D projections matching. In IEEE International Conference on Computer Vision, 2021.   \n[15] Chao Chen, Yu-Shen Liu, and Zhizhong Han. Latent partition implicit with surface codes for 3d representation. In European Conference on Computer Vision, 2022.   \n[16] Chao Chen, Yu-Shen Liu, and Zhizhong Han. GridPull: Towards scalability in learning implicit representations from 3d point clouds. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 18322\u201318334, 2023.   \n[17] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. IEEE Conference on Computer Vision and Pattern Recognition, 2019.   \n[18] Julian Chibane, Aymen Mir, and Gerard Pons-Moll. Neural unsigned distance fields for implicit function learning. arXiv, 2010.13938, 2020.   \n[19] Gene Chou, Ilya Chugunov, and Felix Heide. GenSDF: Two-stage learning of generalizable signed distance functions. In Advances in Neural Information Processing Systems, pages 24905\u201324919, 2022.   \n[20] Christopher B. Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3D-r2n2: A unified approach for single and multi-view 3d object reconstruction. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, European Conference on Computer Vision, volume 9912, pages 628\u2013644, 2016.   \n[21] Angela Dai and Matthias Nie\u00dfner. Neural Poisson: Indicator functions for neural fields. arXiv preprint arXiv:2211.14249, 2022.   \n[22] Philipp Erler, Paul Guerrero, Stefan Ohrhallinger, Niloy J. Mitra, and Michael Wimmer. Points2Surf: Learning implicit surfaces from point clouds. In European Conference on Computer Vision, 2020.   \n[23] Miguel Fainstein, Viviana Siless, and Emmanuel Iarussi. DUDF: Differentiable unsigned distance fields with hyperbolic scaling. arXiv preprint arXiv:2402.08876, 2024.   \n[24] Haoqiang Fan, Hao Su, and Leonidas J. Guibas. A point set generation network for 3D object reconstruction from a single image. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, pages 2463\u20132471, 2017.   \n[25] Qiancheng Fu, Qingshan Xu, Yew-Soon Ong, and Wenbing Tao. Geo-Neus: Geometry-consistent neural implicit surfaces learning for multi-view reconstruction. 2022.   \n[26] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Computer Vision and Pattern Recognition, 2012.   \n[27] Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna, William T. Freeman, and Thomas Funkhouser. Learning shape templates with structured implicit functions. In International Conference on Computer Vision, 2019.   \n[28] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regularization for learning shapes. In International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 3789\u20133799, 2020.   \n[29] Zhizhong Han, Chao Chen, Yu-Shen Liu, and Matthias Zwicker. DRWR: A differentiable renderer without rendering for unsupervised 3D structure learning from silhouette images. In International Conference on Machine Learning, 2020.   \n[30] Zhizhong Han, Chao Chen, Yu-Shen Liu, and Matthias Zwicker. ShapeCaptioner: Generative caption network for 3D shapes by learning a mapping from parts detected in multiple views to sentences. In ACM International Conference on Multimedia, 2020.   \n[31] Zhizhong Han, Xiyang Wang, Yu-Shen Liu, and Matthias Zwicker. Hierarchical view predictor: Unsupervised 3d global feature learning through hierarchical prediction among unordered views. In Proceedings of the 29th ACM International Conference on Multimedia, pages 3862\u2013\u20133871, 2021.   \n[32] Rana Hanocka, Gal Metzer, Raja Giryes, and Daniel Cohen-Or. Point2mesh: a self-prior for deformable meshes. ACM Transactions on Graphics, 39(4):126, 2020.   \n[33] Pengchong Hu and Zhizhong Han. Learning neural implicit through volume rendering with attentive depth fusion priors. In Advances in Neural Information Processing Systems, 2023.   \n[34] Jiahui Huang, Hao-Xiang Chen, and Shi-Min Hu. A neural galerkin solver for accurate surface reconstruction. ACM Trans. Graph., 41(6), 2022.   \n[35] Jiahui Huang, Zan Gojcic, Matan Atzmon, Or Litany, Sanja Fidler, and Francis Williams. Neural kernel surface reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4369\u20134379, 2023.   \n[36] Meng Jia and Matthew Kyan. Learning occupancy function from point clouds for surface reconstruction. arXiv, 2010.11378, 2020.   \n[37] Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nie\u00dfner, and Thomas Funkhouser. Local implicit grid representations for 3D scenes. In IEEE Conference on Computer Vision and Pattern Recognition, 2020.   \n[38] Yue Jiang, Dantong Ji, Zhizhong Han, and Matthias Zwicker. SDFDiff: Differentiable rendering of signed distance fields for 3D shape optimization. In IEEE Conference on Computer Vision and Pattern Recognition, 2020.   \n[39] Michael M. Kazhdan and Hugues Hoppe. Screened poisson surface reconstruction. ACM Transactions on Graphics, 32(3):29:1\u201329:13, 2013.   \n[40] Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and Timo Aila. Noise2noise: Learning image restoration without clean data. In Jennifer G. Dy and Andreas Krause, editors, International Conference on Machine Learning, volume 80, pages 2971\u20132980, 2018.   \n[41] Chen-Hsuan Lin, Chaoyang Wang, and Simon Lucey. SDF-SRN: Learning signed distance 3D object reconstruction from static images. In Advances in Neural Information Processing Systems, 2020.   \n[42] Cheng Lin, Changjian Li, Yuan Liu, Nenglun Chen, Yi-King Choi, and Wenping Wang. Point2Skeleton: Learning skeletal representations from point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4277\u20134286, 2021.   \n[43] Minghua Liu, Xiaoshuai Zhang, and Hao Su. Meshing point clouds with predicted intrinsic-extrinsic ratio guidance. In European Conference on Computer vision, 2020.   \n[44] Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys, and Zhaopeng Cui. DIST: Rendering deep implicit signed distance function with differentiable sphere tracing. In IEEE Conference on Computer Vision and Pattern Recognition, 2020.   \n[45] Shi-Lin Liu, Hao-Xiang Guo, Hao Pan, Pengshuai Wang, Xin Tong, and Yang Liu. Deep implicit moving least-squares functions for 3D reconstruction. In IEEE Conference on Computer Vision and Pattern Recognition, 2021.   \n[46] Shichen Liu, Shunsuke Saito, Weikai Chen, and Hao Li. Learning to infer implicit surfaces without 3D supervision. In Advances in Neural Information Processing Systems, 2019.   \n[47] William E. Lorensen and Harvey E. Cline. Marching cubes: A high resolution 3D surface construction algorithm. Computer Graphics, 21(4):163\u2013169, 1987.   \n[48] Shitong Luo and Wei Hu. Differentiable manifold reconstruction for point cloud denoising. In ACM International Conference on Multimedia, pages 1330\u20131338. ACM, 2020.   \n[49] Shitong Luo and Wei Hu. Score-based point cloud denoising. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4583\u20134592, 2021.   \n[50] Baorui Ma, Zhizhong Han, Yu-Shen Liu, and Matthias Zwicker. Neural-pull: Learning signed distance functions from point clouds by learning to pull space onto surfaces. In International Conference on Machine Learning, 2021.   \n[51] Baorui Ma, Yu-Shen Liu, and Zhizhong Han. Reconstructing surfaces for sparse point clouds with on-surface priors. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6305\u20136315, 2022.   \n[52] Baorui Ma, Yu-Shen Liu, and Zhizhong Han. Learning signed distance functions from noisy 3D point clouds via noise to noise mapping. In International Conference on Machine Learning, pages 23338\u201323357. PMLR, 2023.   \n[53] Baorui Ma, Yu-Shen Liu, Matthias Zwicker, and Zhizhong Han. Reconstructing surfaces for sparse point clouds with on-surface priors. In IEEE Conference on Computer Vision and Pattern Recognition, 2022.   \n[54] Baorui Ma, Yu-Shen Liu, Matthias Zwicker, and Zhizhong Han. Surface reconstruction from point clouds by learning predictive context priors. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6316\u20136327, 2022.   \n[55] Baorui Ma, Yu-Shen Liu, Matthias Zwicker, and Zhizhong Han. Surface reconstruction from point clouds by learning predictive context priors. In IEEE Conference on Computer Vision and Pattern Recognition, 2022.   \n[56] Baorui Ma, Junsheng Zhou, Yu-Shen Liu, and Zhizhong Han. Towards better gradient consistency for neural signed distance functions via level set alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17724\u201317734, 2023.   \n[57] Aihua Mao, Biao Yan, Zijing Ma, and Ying He. Denoising point clouds in latent space via graph convolution and invertible neural network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5768\u20135777, June 2024.   \n[58] Julien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro, and Gordon Wetzstein. ACORN: adaptive coordinate networks for neural scene representation. CoRR, abs/2105.02788, 2021.   \n[59] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3D reconstruction in function space. In IEEE Conference on Computer Vision and Pattern Recognition, 2019.   \n[60] Zhenxing Mi, Yiming Luo, and Wenbing Tao. SSRNet: Scalable 3D surface reconstruction network. In IEEE Conference on Computer Vision and Pattern Recognition, 2020.   \n[61] Mateusz Michalkiewicz, Jhony K. Pontes, Dominic Jack, Mahsa Baktashmotlagh, and Anders P. Eriksson. Deep level sets: Implicit surface representations for 3D shape inference. CoRR, abs/1901.06802, 2019.   \n[62] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision, 2020.   \n[63] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learning implicit 3D representations without 3D supervision. In IEEE Conference on Computer Vision and Pattern Recognition, 2020.   \n[64] Michael Oechsle, Songyou Peng, and Andreas Geiger. UNISURF: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In International Conference on Computer Vision, 2021.   \n[65] Amine Ouasfi and Adnane Boukhayma. Few \u2019zero level set\u2019-shot learning of shape signed distance functions in feature space. In European Conference on Computer Vision, 2022.   \n[66] Amine Ouasfi and Adnane Boukhayma. Robustifying generalizable implicit shape networks with a tunable non-parametric model. In Advances in Neural Information Processing Systems, 2023.   \n[67] Amine Ouasfi and Adnane Boukhayma. Mixing-denoising generalizable occupancy networks. In International Conference on 3D Vision, 2024.   \n[68] Amine Ouasf iand Adnane Boukhayma. Unsupervised occupancy learning from sparse point cloud. In IEEE Conference on Computer Vision and Pattern Recognition, 2024.   \n[69] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. DeepSDF: Learning continuous signed distance functions for shape representation. In IEEE Conference on Computer Vision and Pattern Recognition, 2019.   \n[70] Songyou Peng, Chiyu \"Max\" Jiang, Yiyi Liao, Michael Niemeyer, Marc Pollefeys, and Andreas Geiger. Shape as points: A differentiable poisson solver. In Advances in Neural Information Processing Systems, 2021.   \n[71] Songyou Peng, Michael Niemeyer, Lars M. Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In European Conference on Computer Vision, volume 12348, pages 523\u2013540, 2020.   \n[72] Francesca Pistilli, Giulia Fracastoro, Diego Valsesia, and Enrico Magli. Learning graph-convolutional representations for point cloud denoising. In European Conference on Computer Vision, volume 12365, pages 103\u2013118, 2020.   \n[73] Marie-Julie Rakotosaona, Vittorio La Barbera, Paul Guerrero, Niloy J. Mitra, and Maks Ovsjanikov. Pointcleannet: Learning to denoise and remove outliers from dense point clouds. Computer Graphics Forum, 39(1):185\u2013203, 2020.   \n[74] Konstantinos Rematas, Ricardo Martin-Brualla, and Vittorio Ferrari. Sharf: Shape-conditioned radiance fields from a single view. In International Conference on Machine Learning, 2021.   \n[75] Andr\u00e9s Serna, Beatriz Marcotegui, Fran\u00e7ois Goulette, and Jean-Emmanuel Deschaud. Paris-rue-madame database - A 3D mobile laser scanner dataset for benchmarking urban detection, segmentation and classification methods. In International Conference on Pattern Recognition Applications and Methods, pages 819\u2013824, 2014.   \n[76] Jaehyeok Shim and Kyungdon Joo. DITTO: Dual and integrated latent topologies for implicit 3d reconstruction. arXiv preprint arXiv:2403.05005, 2024.   \n[77] Vincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In Advances in Neural Information Processing Systems, 2020.   \n[78] Vincent Sitzmann, Michael Zollh\u00f6fer, and Gordon Wetzstein. Scene representation networks: Continuous 3D-structure-aware neural scene representations. In Advances in Neural Information Processing Systems, 2019.   \n[79] Peng Songyou, Niemeyer Michael, Mescheder Lars, Pollefeys Marc, and Geiger Andreas. Convolutional occupancy networks. In European Conference on Computer Vision, 2020.   \n[80] Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Neural geometric level of detail: Real-time rendering with implicit 3D shapes. In IEEE Conference on Computer Vision and Pattern Recognition, 2021.   \n[81] Jiapeng Tang, Jiabao Lei, Dan Xu, Feiying Ma, Kui Jia, and Lei Zhang. SA-ConvONet: Sign-agnostic optimization of convolutional occupancy networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.   \n[82] Maxim Tatarchenko, Stephan R. Richter, Rene Ranftl, Zhuwen Li, Vladlen Koltun, and Thomas Brox. What do single-view 3D reconstruction networks learn? In The IEEE Conference on Computer Vision and Pattern Recognition, 2019.   \n[83] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollh\u00f6fer, Carsten Stoll, and Christian Theobalt. PatchNets: Patch-Based Generalizable Deep Implicit 3D Shape Representations. European Conference on Computer Vision, 2020.   \n[84] Delio Vicini, S\u00e9bastien Speierer, and Wenzel Jakob. Differentiable signed distance function rendering. ACM Transactions on Graphics, 41(4):125:1\u2013125:18, 2022.   \n[85] Jiepeng Wang, Peng Wang, Xiaoxiao Long, Christian Theobalt, Taku Komura, Lingjie Liu, and Wenping Wang. NeuRIS: Neural reconstruction of indoor scenes using normal priors. In European Conference on Computer Vision, 2022.   \n[86] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. NeuS: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In Advances in Neural Information Processing Systems, pages 27171\u201327183, 2021.   \n[87] Peng-Shuai Wang, Yang Liu, and Xin Tong. Deep octree-based cnns with output-guided skip connections for 3d shape and scene completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 266\u2013267, 2020.   \n[88] Ruian Wang, Zixiong Wang, Yunxiao Zhang, Shuangmin Chen, Shiqing Xin, Changhe Tu, and Wenping Wang. Aligning gradient and hessian for neural signed distance function. In Advances in Neural Information Processing Systems, volume 36, pages 63515\u201363528, 2023.   \n[89] Yiqun Wang, Ivan Skorokhodov, and Peter Wonka. HF-NeuS: Improved surface reconstruction using high-frequency details. 2022.   \n[90] Zhen Wang, Shijie Zhou, Jeong Joon Park, Despoina Paschalidou, Suya You, Gordon Wetzstein, Leonidas Guibas, and Achuta Kadambi. Alto: Alternating latent topologies for implicit 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 259\u2013270, 2023.   \n[91] Francis Williams, Zan Gojcic, Sameh Khamis, Denis Zorin, Joan Bruna, Sanja Fidler, and Or Litany. Neural fields as learnable kernels for 3D reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18500\u201318510, 2022.   \n[92] Francis Williams, Teseo Schneider, Claudio Silva, Denis Zorin, Joan Bruna, and Daniele Panozzo. Deep geometric prior for surface reconstruction. In IEEE Conference on Computer Vision and Pattern Recognition, 2019.   \n[93] Francis Williams, Matthew Trager, Joan Bruna, and Denis Zorin. Neural splines: Fitting 3D surfaces with infinitely-wide neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, pages 9949\u20139958, 2021.   \n[94] Yunjie Wu and Zhengxing Sun. DFR: differentiable function rendering for learning 3D generation from images. Computer Graphics Forum, 39(5):241\u2013252, 2020.   \n[95] Peng Xiang, Xin Wen, Yu-Shen Liu, Yan-Pei Cao, Pengfei Wan, Wen Zheng, and Zhizhong Han. SnowflakeNet: Point cloud completion by snowflake point deconvolution with skip-transformer. In IEEE International Conference on Computer Vision, 2021.   \n[96] Huizong Yang, Yuxin Sun, Ganesh Sundaramoorthi, and Anthony Yezzi. StEik: Stabilizing the optimization of neural signed distance functions and finer shape representation. In Advances in Neural Information Processing Systems, 2023.   \n[97] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. In Advances in Neural Information Processing Systems, 2021.   \n[98] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. Advances in Neural Information Processing Systems, 33, 2020.   \n[99] Wang Yifan, Shihao Wu, Cengiz Oztireli, and Olga Sorkine-Hornung. Iso-Points: Optimizing neural implicit surfaces with hybrid representations. CoRR, abs/2012.06434, 2020.   \n[100] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. MonoSDF: Exploring monocular geometric cues for neural implicit surface reconstruction. 2022.   \n[101] Sergey Zakharov, Wadim Kehl, Arjun Bhargava, and Adrien Gaidon. Autolabeling 3D objects with differentiable rendering of sdf shape priors. In IEEE Conference on Computer Vision and Pattern Recognition, 2020.   \n[102] Wenbin Zhao, Jiabao Lei, Yuxin Wen, Jianguo Zhang, and Kui Jia. Sign-agnostic implicit learning of surface self-similarities for shape modeling and reconstruction from raw point clouds. CoRR, abs/2012.07498, 2020.   \n[103] Junsheng Zhou, Baorui Ma, Shujuan Li, Yu-Shen Liu, and Zhizhong Han. Learning a more continuous zero level set in unsigned distance fields through level set projection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3181\u20133192, 2023.   \n[104] Junsheng Zhou, Baorui Ma, Yu-Shen Liu, Yi Fang, and Zhizhong Han. Learning consistency-aware unsigned distance functions progressively from raw point clouds. In Advances in Neural Information Processing Systems, 2022.   \n[105] Qian-Yi Zhou and Vladlen Koltun. Dense scene reconstruction with points of interest. ACM Transactions on Graphics, 32(4):112:1\u2013112:8, 2013.   \n[106] Runsong Zhu, Di Kang, Ka-Hei Hui, Yue Qian, Shi Qiu, Zhen Dong, Linchao Bao, Pheng-Ann Heng, and Chi-Wing Fu. Ssp: Semi-signed prioritized neural ftiting for surface reconstruction from unoriented point clouds. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 3769\u20133778, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our method is still limited to too large noises. For noises that corrupted shapes too much, our method still produces bad results. One direction for our future work is to improve our prior, so that we could have a better sense of a shape even under large noises. ", "page_idx": 15}, {"type": "text", "text": "A.2 Detailed Comparisons on 3D Scene ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We detail our evaluations on each scene in 3D scene dataset in Tab. 17. The comparisons highlight our advantages in each scene. ", "page_idx": 15}, {"type": "table", "img_path": "Hgqs1b4ECy/tmp/a1ce08a88103d6896adf982ef9a1204f4aa394caff279d7014b95bb7b12d55e8.jpg", "table_caption": [], "table_footnote": ["Table 17: Numerical Comparisons on 3D Scene dataset in terms of $C D_{L1}$ , $C D_{L2}$ and NC. "], "page_idx": 15}, {"type": "text", "text": "A.3 More Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We visualize more surface reconstruction results under ShapeNet [12], ABC [22], Surface Reconstruction Benchmark (SRB) [92], FAMOUS [22], D-FAUST [5] and nuScenes [8] in Fig. 16, Fig. 17, Fig. 18, Fig. 19, Fig. 20 and Fig. 21. ", "page_idx": 15}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/a7de407c9838d0d67265ba60020a77fe7a3e0d886761c1cd9e4df197bc47363c.jpg", "img_caption": ["Figure 16: Comparison in surface reconstruction on ShapeNet. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/d2fb7963c9bd40929f69a8a67fe30abe2817dae6c24f1b84fd770b6956edd0b1.jpg", "img_caption": ["Figure 17: Comparison in surface reconstruction on ABC. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/dd683131ba045f669fe59097ccb0854966d9dd007d4cda4a0a6dba59ef7291ff.jpg", "img_caption": ["Figure 18: Comparison in surface reconstruction on SRB. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/86f07b0b1a12c14eb2a40b3c37aec99854cb7bd09a2c2716b38fc89241062cfe.jpg", "img_caption": ["Figure 19: Comparison in surface reconstruction on FAMOUS. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/de684efdb689e3f24965d00507ed6969512fe176924beffbab51114a7787833d.jpg", "img_caption": ["Figure 20: Comparison in surface reconstruction on D-FAUST. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "Hgqs1b4ECy/tmp/9c9a39e98ee5a2bc9b94ab7bb9397510855afc2e51e86ecedc7d9ca30754c6d0.jpg", "img_caption": ["Figure 21: Comparison in surface reconstruction on nuScenes. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We discuss the limitations in the Appendix A.1. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: We describe in Method one of the core contributions of the local noise-tonoise mapping, and although there is no theory or theorem in it, we verify its validity and reasonableness in our experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide detailed information in reproducing our methods in Implementation Details of Section 3. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide our demonstration code as a part of our supplementary materials.   \nWe will release our source code, data and sufficient instructions upon acceptance. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We provide all the training and test details for shapes and scenes in Section 4. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: We report the average performance in terms of several metrics as the experimental results. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We report our inference time with other methods in the experiments. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The research conducted in this paper conforms in all respects to the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We discuss the application and potential positive impact of our method in the introduction. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: There is no such risk to the paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We use open-source datasets and code under their licence. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]