[{"figure_path": "UPxFYvHsyN/figures/figures_1_1.jpg", "caption": "Figure 1: Existing dynamic-NeRF models struggle to generate plausible 3D reconstructions for generic dynamic scenes featuring humans and objects engaged in complex interactions. In this work, we introduce a Neural Radiance Field model designed for 3D reconstruction of such generic scenes, captured using a sparse/single-view video, capable of producing plausible geometry for each semantic element within the scene. In this figure, A: Input RGB, B: predicted normal map, C: predicted semantic reconstruction, and D: predicted skinning weight.", "description": "This figure demonstrates the shortcomings of existing dynamic-NeRF models in reconstructing complex, interactive dynamic scenes.  It highlights the challenge of accurately representing the geometry of interacting entities (humans and objects) from limited video input. The proposed TFS-NeRF model aims to address these limitations by generating a more realistic and semantically consistent 3D reconstruction using sparse/single-view RGB videos, achieving plausible geometry for each individual semantic element.", "section": "Introduction"}, {"figure_path": "UPxFYvHsyN/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of the system. A: To produce a semantically separable reconstruction of each element, first, we perform a semantic-aware ray sampling. Given a 2D semantic segmentation mask, we shoot two sets of rays and sample two sets of 3D points for differentiating the deformable and non-deformable entities of the scene, {xd}i, {xnd}i under interactions. B: Next, each set of points is transformed from the deformed/view space (input frame) to its respective canonical space by inverse warping enabled by the learned forward LBS (Details are presented in Fig. 3. C: Then the individual geometry is predicted at the canonical space in the form of canonical SDFs by two independent SDF prediction networks Fd->(\u03b8d), Fnd->(\u03b8nd) for the deformable and non-deformable entities denoted as j \u2208 {d, nd}. D: Finally, the output SDFs are used to predict a composite scene rendering. Both these branches are optimized jointly using the RGB reconstruction loss.", "description": "This figure illustrates the proposed TFS-NeRF framework. It consists of four stages: semantic-aware ray sampling, inverse warping, SDF prediction, and volume rendering.  The framework uses semantic segmentation to separate interacting entities and learns forward Linear Blend Skinning (LBS) via an invertible neural network (INN).  It generates semantically separable 3D reconstructions from sparse/single-view RGB videos, effectively handling dynamic scenes with interacting objects.", "section": "3 Methodology"}, {"figure_path": "UPxFYvHsyN/figures/figures_4_1.jpg", "caption": "Figure 3: Overview of the transformation from view space to canonical space.", "description": "This figure illustrates the process of transforming points from view space to canonical space using an Invertible Neural Network (INN). The INN takes the deformed space points (x<sub>v</sub>) and the pose of the 3D skeleton (J<sub>p</sub>) as input. An initial transformation is applied to estimate the canonical space points (x<sub>init</sub>), which are then used as input to the INN. The INN learns a bijective mapping between view space and canonical space. The skinning weights (w<sub>s</sub>) are predicted by a separate Multilayer Perceptron (MLP) that takes the transformed canonical points as input. Finally, the canonical points are transformed back to the deformed space using inverse warping.", "section": "3 Methodology"}, {"figure_path": "UPxFYvHsyN/figures/figures_8_1.jpg", "caption": "Figure 5: Qualitative comparison with SoTA methods on BEHAVE dataset.", "description": "This figure compares the qualitative results of the proposed TFS-NeRF method against other state-of-the-art methods on the BEHAVE dataset.  The top row shows input images, while the subsequent rows display 3D reconstructions generated by different methods for three distinct interaction scenarios involving humans and objects (a person standing on a box, a person interacting with a ball, and a person holding a basketball). The figure visually demonstrates the superior quality and accuracy of the TFS-NeRF reconstructions compared to the alternatives, particularly in handling complex interactions and object deformations.", "section": "Experiments"}, {"figure_path": "UPxFYvHsyN/figures/figures_8_2.jpg", "caption": "Figure 7: INN vs Broyden formulation.", "description": "This figure compares the convergence speed of the proposed INN-based method with the Broyden method for learning the forward LBS (Linear Blend Skinning).  The x-axis represents training time in hours, and the y-axis shows the Chamfer distance (a metric for evaluating the accuracy of 3D reconstruction). The graph clearly demonstrates that the INN method (W/o Broyden) converges significantly faster than the Broyden method (W Broyden), achieving lower Chamfer distance in a shorter period. This highlights the efficiency gain of the proposed INN-based approach.", "section": "Ablation studies"}, {"figure_path": "UPxFYvHsyN/figures/figures_8_3.jpg", "caption": "Figure 1: Existing dynamic-NeRF models struggle to generate plausible 3D reconstructions for generic dynamic scenes featuring humans and objects engaged in complex interactions. In this work, we introduce a Neural Radiance Field model designed for 3D reconstruction of such generic scenes, captured using a sparse/single-view video, capable of producing plausible geometry for each semantic element within the scene. In this figure, A: Input RGB, B: predicted normal map, C: predicted semantic reconstruction, and D: predicted skinning weight.", "description": "This figure shows a comparison between the results of existing dynamic-NeRF models and the proposed model. The existing models struggle to generate plausible 3D reconstructions of dynamic scenes with interacting humans and objects. The proposed model, on the other hand, is able to generate plausible geometry for each semantic element in the scene, even when there are complex interactions between the entities.", "section": "1 Introduction"}, {"figure_path": "UPxFYvHsyN/figures/figures_9_1.jpg", "caption": "Figure 1: Existing dynamic-NeRF models struggle to generate plausible 3D reconstructions for generic dynamic scenes featuring humans and objects engaged in complex interactions. In this work, we introduce a Neural Radiance Field model designed for 3D reconstruction of such generic scenes, captured using a sparse/single-view video, capable of producing plausible geometry for each semantic element within the scene. In this figure, A: Input RGB, B: predicted normal map, C: predicted semantic reconstruction, and D: predicted skinning weight.", "description": "This figure demonstrates the limitations of existing dynamic-NeRF models in reconstructing complex dynamic scenes with interacting entities.  It highlights the challenge of generating accurate 3D reconstructions, especially when dealing with both humans and objects interacting. The figure shows an example of input RGB images and compares them with the output from existing models, showcasing limitations in generating plausible geometries and semantic separation.  The authors' proposed model addresses these shortcomings and produces better results as described in the paper.", "section": "Introduction"}]