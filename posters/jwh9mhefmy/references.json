{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational to RLHF and introduces a method for training language models to follow instructions using human feedback, directly impacting the current study's reward model training."}, {"fullname_first_author": "Leo Gao", "paper_title": "Scaling laws for reward model overoptimization", "publication_date": "2023-07-01", "reason": "This paper introduces the concept of reward model overoptimization, a key problem that the current research addresses through regularization."}, {"fullname_first_author": "Nisan Stiennon", "paper_title": "Learning to summarize with human feedback", "publication_date": "2020-12-01", "reason": "This paper is a crucial foundation for RLHF, demonstrating the effectiveness of training language models using reinforcement learning from human feedback, a technique that this work builds upon."}, {"fullname_first_author": "Daniel M Ziegler", "paper_title": "Fine-tuning language models from human preferences", "publication_date": "2019-09-01", "reason": "This paper is among the earliest to explore using human preferences to fine-tune language models, setting the stage for RLHF techniques used in the current study."}, {"fullname_first_author": "Ananya Kumar", "paper_title": "Fine-tuning can distort pretrained features and underperform out-of-distribution", "publication_date": "2022-02-01", "reason": "This paper provides the theoretical basis for addressing the OOD generalization problem, a core issue that this work aims to solve through regularization, directly influencing the current research's approach."}]}