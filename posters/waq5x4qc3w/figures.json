[{"figure_path": "waQ5X4qc3W/figures/figures_1_1.jpg", "caption": "Figure 1: (a): Linear probe and class-unconditional generation performance of different methods trained and evaluated on ImageNet-1K. (b): Class-conditional generation performance of different methods on ImageNet-1k. DIGIT achieves SOTA performance in linear probing and establishes a new SOTA in image generation within a single model.", "description": "This figure compares different image generation models based on their performance in linear probing (measuring image understanding) and class-unconditional/conditional image generation.  The size of each bubble corresponds to the number of model parameters. Subfigure (a) shows class-unconditional performance (FID and Top-1 Accuracy), and subfigure (b) shows class-conditional performance (FID and Inception Score).  The key takeaway is that DiGIT achieves state-of-the-art (SOTA) results in both linear probing and image generation.", "section": "1 Introduction"}, {"figure_path": "waQ5X4qc3W/figures/figures_5_1.jpg", "caption": "Figure 2: The architecture of DIGIT.", "description": "The figure illustrates the architecture of the DIGIT model.  An image is divided into patches, which are fed into a frozen discriminative self-supervised learning (SSL) model, such as DINOv2. This model extracts visual features from each patch.  These features are then clustered using K-Means, creating a codebook of discriminative tokens.  The resulting tokens are then fed into the DIGIT autoregressive model. The DIGIT model is a transformer architecture containing multiple transformer blocks which utilizes the next-token prediction principle for generating an image from the sequence of tokens, starting with a beginning-of-sequence token. The model is trained in a way that the discriminative SSL model's parameters are frozen, and the DiGIT transformer's parameters are trainable.", "section": "3 Stabilize the Latent Space with Self-supervised Learning Model"}, {"figure_path": "waQ5X4qc3W/figures/figures_8_1.jpg", "caption": "Figure 3: Ablation study of DiGIT. (a) The comparison of tokenizer, training steps, and model size in the image generation task. (b) Linear-probe accuracy from different layers in the pre-trained DiGIT-base with different number of K-Means clusters.", "description": "This figure shows the ablation study of DiGIT, a discriminative image tokenizer for autoregressive image generation.  Part (a) displays the results of image generation experiments comparing various model configurations: the baseline VQ Tokenizer, the addition of the discriminative tokenizer, longer training (400 epochs), and scaling up the model size (732M). It demonstrates improvements in both FID (Fr\u00e9chet Inception Distance) and IS (Inception Score) metrics as more advanced techniques are incorporated. Part (b) presents the results of a linear probe analysis on the pre-trained DiGIT-base model with different numbers of K-means clusters (8192, 4096, and 1024). It illustrates how the linear-probe accuracy changes across different transformer layers, suggesting an optimal number of clusters for improved image understanding.", "section": "4 Experiments"}, {"figure_path": "waQ5X4qc3W/figures/figures_16_1.jpg", "caption": "Figure 5: FID and Inception Score as a function of top-k, top-p sampling on the image generation task with DiGIT-base. The decoding temperature is fixed to 1.0. The \"stage2\" denotes the autoregressive model for pixel rendering.", "description": "This figure shows the impact of top-k and top-p sampling techniques on the FID and Inception Score metrics during image generation using the DiGIT-base model.  The decoding temperature is held constant at 1.0.  The results show how different sampling strategies affect the quality of the generated images, with \"stage2\" referring to a separate autoregressive model used for pixel rendering.", "section": "4 Experiments"}, {"figure_path": "waQ5X4qc3W/figures/figures_16_2.jpg", "caption": "Figure 6: Toy example of PCA and LDA.", "description": "This figure shows a comparison of Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) on a toy dataset with two classes. The leftmost panel displays the original two-dimensional data, clearly showing two overlapping clusters. The middle panels show the one-dimensional projections obtained using PCA and LDA, respectively.  The PCA projection attempts to maximize variance, leading to a considerable amount of overlap between the two classes.  The LDA projection, on the other hand, prioritizes maximizing the separation between the classes. The rightmost panel illustrates this clearly by showing classification accuracy under increasing levels of added Gaussian noise to the data; LDA consistently outperforms PCA in terms of classification accuracy as noise increases.", "section": "2.2 Stability of the Latent space"}, {"figure_path": "waQ5X4qc3W/figures/figures_17_1.jpg", "caption": "Figure 7: Class-unconditional image generation results on ImageNet 256x256 by DiGIT.", "description": "This figure showcases the results of class-unconditional image generation using the DiGIT model on the ImageNet dataset. The images are generated at a resolution of 256x256 pixels. The figure displays a grid of diverse images generated by the model, demonstrating its ability to generate a wide range of objects and scenes.", "section": "A.4 Qualitative Cases"}, {"figure_path": "waQ5X4qc3W/figures/figures_18_1.jpg", "caption": "Figure 8: Class-conditional image generation results on ImageNet 256x256 by DiGIT, where the images in the same row share the same class label.", "description": "This figure shows the results of class-conditional image generation using the DiGIT model on the ImageNet dataset.  Each row displays images generated from the same class label.  It demonstrates the model's ability to generate diverse and realistic images within a given class.", "section": "A.4 Qualitative Cases"}]